<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Seg4Diff Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d457104edf5e48858abc5b8cf10a6a4a')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-24-æ›´æ–°"><a href="#2025-09-24-æ›´æ–°" class="headerlink" title="2025-09-24 æ›´æ–°"></a>2025-09-24 æ›´æ–°</h1><h2 id="Seg4Diff-Unveiling-Open-Vocabulary-Segmentation-in-Text-to-Image-Diffusion-Transformers"><a href="#Seg4Diff-Unveiling-Open-Vocabulary-Segmentation-in-Text-to-Image-Diffusion-Transformers" class="headerlink" title="Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers"></a>Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers</h2><p><strong>Authors:Chaehyun Kim, Heeseong Shin, Eunbeen Hong, Heeji Yoon, Anurag Arnab, Paul Hongsuck Seo, Sunghwan Hong, Seungryong Kim</strong></p>
<p>Text-to-image diffusion models excel at translating language prompts into photorealistic images by implicitly grounding textual concepts through their cross-modal attention mechanisms. Recent multi-modal diffusion transformers extend this by introducing joint self-attention over concatenated image and text tokens, enabling richer and more scalable cross-modal alignment. However, a detailed understanding of how and where these attention maps contribute to image generation remains limited. In this paper, we introduce Seg4Diff (Segmentation for Diffusion), a systematic framework for analyzing the attention structures of MM-DiT, with a focus on how specific layers propagate semantic information from text to image. Through comprehensive analysis, we identify a semantic grounding expert layer, a specific MM-DiT block that consistently aligns text tokens with spatially coherent image regions, naturally producing high-quality semantic segmentation masks. We further demonstrate that applying a lightweight fine-tuning scheme with mask-annotated image data enhances the semantic grouping capabilities of these layers and thereby improves both segmentation performance and generated image fidelity. Our findings demonstrate that semantic grouping is an emergent property of diffusion transformers and can be selectively amplified to advance both segmentation and generation performance, paving the way for unified models that bridge visual perception and generation. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹é€šè¿‡å…¶è·¨æ¨¡æ€æ³¨æ„æœºåˆ¶éšå¼åœ°å»ºç«‹æ–‡æœ¬æ¦‚å¿µï¼Œä»è€Œå°†è¯­è¨€æç¤ºç¿»è¯‘æˆå…·æœ‰å…‰æ„Ÿçš„å›¾åƒã€‚æœ€è¿‘çš„å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨é€šè¿‡åœ¨è¿æ¥çš„å›¾åƒå’Œæ–‡æœ¬ç¬¦å·ä¸Šå¼•å…¥è”åˆè‡ªæ³¨æ„åŠ›ï¼Œä½¿å…¶æ‰©å±•ï¼Œä»è€Œå®ç°æ›´ä¸°å¯Œå’Œæ›´å¯æ‰©å±•çš„è·¨æ¨¡æ€å¯¹é½ã€‚ç„¶è€Œï¼Œå¯¹äºè¿™äº›æ³¨æ„åŠ›å›¾å¦‚ä½•åœ¨å›¾åƒç”Ÿæˆä¸­å‘æŒ¥ä½œç”¨ä»¥åŠå‘æŒ¥ä½œç”¨çš„ä½ç½®ï¼Œæˆ‘ä»¬çš„äº†è§£ä»ç„¶æœ‰é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18096v1">PDF</a> NeurIPS 2025. Project page: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/Seg4Diff/">https://cvlab-kaist.github.io/Seg4Diff/</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶éšå¼åœ°å°†æ–‡æœ¬æ¦‚å¿µæ¥åœ°ï¼Œä»è€Œæ“…é•¿å°†è¯­è¨€æç¤ºè½¬åŒ–ä¸ºé€¼çœŸçš„å›¾åƒã€‚æœ€è¿‘çš„å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨é€šè¿‡å¼•å…¥è”åˆè‡ªæ³¨æ„åŠ›ï¼Œå®ç°å¯¹ä¸²è”çš„å›¾åƒå’Œæ–‡æœ¬ç¬¦å·çš„æ›´ä¸°å¯Œå’Œå¯æ‰©å±•çš„è·¨æ¨¡æ€å¯¹é½ã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›æ³¨æ„åŠ›å›¾å¦‚ä½•ä»¥åŠåœ¨ä½•å¤„åŠ©åŠ›å›¾åƒç”Ÿæˆä»å­˜åœ¨è¯¦ç»†çš„äº†è§£ç©ºç™½ã€‚æœ¬æ–‡æå‡ºSeg4Diffï¼ˆç”¨äºæ‰©æ•£çš„åˆ†å‰²ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿåœ°åˆ†æMM-DiTæ³¨æ„åŠ›ç»“æ„çš„æ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨ç‰¹å®šå±‚å¦‚ä½•å°†è¯­ä¹‰ä¿¡æ¯ä»æ–‡æœ¬ä¼ æ’­åˆ°å›¾åƒã€‚é€šè¿‡ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†è¯­ä¹‰æ¥åœ°ä¸“å®¶å±‚ï¼Œè¿™æ˜¯ä¸€ä¸ªç‰¹å®šçš„MM-DiTå—ï¼Œèƒ½å¤ŸæŒç»­åœ°å°†æ–‡æœ¬ç¬¦å·ä¸ç©ºé—´è¿è´¯çš„å›¾åƒåŒºåŸŸå¯¹é½ï¼Œè‡ªç„¶äº§ç”Ÿé«˜è´¨é‡è¯­ä¹‰åˆ†å‰²æ©æ¨¡ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è¯æ˜ï¼Œä½¿ç”¨å¸¦æ©ç æ³¨é‡Šçš„å›¾åƒæ•°æ®è¿›è¡Œè½»é‡çº§å¾®è°ƒå¯ä»¥å¢å¼ºè¿™äº›å±‚çš„è¯­ä¹‰åˆ†ç»„èƒ½åŠ›ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½å’Œç”Ÿæˆçš„å›¾åƒä¿çœŸåº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰åˆ†ç»„æ˜¯æ‰©æ•£å˜å‹å™¨çš„ä¸€ç§æ–°å…´å±æ€§ï¼Œå¯é€‰æ‹©æ€§å¢å¼ºä»¥æé«˜åˆ†å‰²å’Œç”Ÿæˆæ€§èƒ½ï¼Œä¸ºèåˆè§†è§‰æ„ŸçŸ¥å’Œç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹é“ºå¹³é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å°†è¯­è¨€æç¤ºè½¬åŒ–ä¸ºé€¼çœŸçš„å›¾åƒã€‚</li>
<li>å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨é€šè¿‡å¼•å…¥è”åˆè‡ªæ³¨æ„åŠ›å®ç°æ›´ä¸°å¯Œçš„è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>Seg4Diffæ¡†æ¶ç”¨äºåˆ†æMM-DiTçš„æ³¨æ„åŠ›ç»“æ„ï¼Œæ¢ç©¶ä¿¡æ¯ä»æ–‡æœ¬åˆ°å›¾åƒçš„ä¼ æ’­è¿‡ç¨‹ã€‚</li>
<li>è¯­ä¹‰æ¥åœ°ä¸“å®¶å±‚æ˜¯MM-DiTä¸­çš„ä¸€ä¸ªç‰¹å®šå—ï¼Œèƒ½å°†æ–‡æœ¬ç¬¦å·ä¸ç©ºé—´è¿è´¯çš„å›¾åƒåŒºåŸŸå¯¹é½ï¼Œäº§ç”Ÿé«˜è´¨é‡çš„è¯­ä¹‰åˆ†å‰²æ©æ¨¡ã€‚</li>
<li>ä½¿ç”¨å¸¦æ©ç æ³¨é‡Šçš„å›¾åƒæ•°æ®è¿›è¡Œå¾®è°ƒå¯ä»¥å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰åˆ†ç»„èƒ½åŠ›ã€‚</li>
<li>è¯­ä¹‰åˆ†ç»„æ˜¯æ‰©æ•£å˜å‹å™¨çš„ä¸€ç§æ–°å…´å±æ€§ï¼Œèƒ½å¤Ÿæé«˜åˆ†å‰²å’Œç”Ÿæˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc90ce97d62b099517dca1500596c5f9" align="middle">
<img src="https://picx.zhimg.com/v2-905d146530463f810ceb0355959f4a1c" align="middle">
<img src="https://picx.zhimg.com/v2-9ff53a5bb9c591a24799357c02110509" align="middle">
<img src="https://picx.zhimg.com/v2-1c4070877be921eba4ab45966c349581" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GraDeT-HTR-A-Resource-Efficient-Bengali-Handwritten-Text-Recognition-System-utilizing-Grapheme-based-Tokenizer-and-Decoder-only-Transformer"><a href="#GraDeT-HTR-A-Resource-Efficient-Bengali-Handwritten-Text-Recognition-System-utilizing-Grapheme-based-Tokenizer-and-Decoder-only-Transformer" class="headerlink" title="GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition   System utilizing Grapheme-based Tokenizer and Decoder-only Transformer"></a>GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition   System utilizing Grapheme-based Tokenizer and Decoder-only Transformer</h2><p><strong>Authors:Md. Mahmudul Hasan, Ahmed Nesar Tahsin Choudhury, Mahmudul Hasan, Md. Mosaddek Khan</strong></p>
<p>Despite Bengali being the sixth most spoken language in the world, handwritten text recognition (HTR) systems for Bengali remain severely underdeveloped. The complexity of Bengali scriptâ€“featuring conjuncts, diacritics, and highly variable handwriting stylesâ€“combined with a scarcity of annotated datasets makes this task particularly challenging. We present GraDeT-HTR, a resource-efficient Bengali handwritten text recognition system based on a Grapheme-aware Decoder-only Transformer architecture. To address the unique challenges of Bengali script, we augment the performance of a decoder-only transformer by integrating a grapheme-based tokenizer and demonstrate that it significantly improves recognition accuracy compared to conventional subword tokenizers. Our model is pretrained on large-scale synthetic data and fine-tuned on real human-annotated samples, achieving state-of-the-art performance on multiple benchmark datasets. </p>
<blockquote>
<p>å°½ç®¡å­ŸåŠ æ‹‰è¯­æ˜¯ä¸–ç•Œä¸Šç¬¬å…­å¤§è¯­è¨€ï¼Œä½†å­ŸåŠ æ‹‰è¯­çš„æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰ç³»ç»Ÿä»ç„¶ä¸¥é‡ç¼ºä¹å¼€å‘ã€‚å­ŸåŠ æ‹‰è¯­è„šæœ¬çš„å¤æ‚æ€§â€”â€”åŒ…æ‹¬è¿å­—ã€å˜éŸ³ç¬¦å·å’Œé«˜åº¦å¯å˜çš„ä¹¦å†™é£æ ¼â€”â€”ä¸æ³¨é‡Šæ•°æ®é›†çš„åŒ®ä¹ç›¸ç»“åˆï¼Œä½¿è¿™ä¸€ä»»åŠ¡ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†GraDeT-HTRï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºéŸ³ç´ æ„ŸçŸ¥è§£ç å™¨Transformeræ¶æ„çš„èµ„æºé«˜æ•ˆå­ŸåŠ æ‹‰è¯­æ‰‹å†™æ–‡æœ¬è¯†åˆ«ç³»ç»Ÿã€‚ä¸ºäº†è§£å†³å­ŸåŠ æ‹‰è¯­è„šæœ¬çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é€šè¿‡é›†æˆéŸ³ç´ æ„ŸçŸ¥çš„åˆ‡è¯å™¨æ¥æé«˜åªå«è§£ç å™¨çš„Transformerçš„æ€§èƒ½ï¼Œå¹¶è¯æ˜ä¸ä¼ ç»Ÿçš„å­è¯åˆ‡è¯å™¨ç›¸æ¯”ï¼Œå®ƒåœ¨è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢æœ‰äº†æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤§å‹åˆæˆæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œåœ¨çœŸå®çš„äººä¸ºæ³¨é‡Šæ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18081v1">PDF</a> 7 pages. Accepted at the 2025 Conference on Empirical Methods in   Natural Language Processing (EMNLP) System Demonstrations. Equal   Contribution: Md. Mahmudul Hasan and Ahmed Nesar Tahsin Choudhury</p>
<p><strong>æ€»ç»“</strong></p>
<p>è™½ç„¶å­ŸåŠ æ‹‰è¯­æ˜¯å…¨çƒç¬¬å…­å¤§è¯­è¨€ï¼Œä½†é’ˆå¯¹å­ŸåŠ æ‹‰è¯­çš„æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰ç³»ç»Ÿä»ç„¶ä¸¥é‡ç¼ºä¹å‘å±•ã€‚å­ŸåŠ æ‹‰è¯­è„šæœ¬çš„å¤æ‚æ€§ï¼Œå¦‚è¿å­—ç¬¦ã€å˜éŸ³ç¬¦å·å’Œé«˜åº¦å¯å˜çš„ä¹¦å†™é£æ ¼ï¼ŒåŠ ä¸Šç¼ºä¹æ³¨é‡Šæ•°æ®é›†ï¼Œä½¿å¾—è¿™ä¸€ä»»åŠ¡ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†GraDeT-HTRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºGraphemeæ„ŸçŸ¥è§£ç å™¨Transformeræ¶æ„çš„èµ„æºèŠ‚çº¦å‹å­ŸåŠ æ‹‰è¯­æ‰‹å†™æ–‡æœ¬è¯†åˆ«ç³»ç»Ÿã€‚ä¸ºè§£å†³å­ŸåŠ æ‹‰è¯­è„šæœ¬çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é€šè¿‡é›†æˆåŸºäºéŸ³ç´ çš„æ ‡è®°å™¨å¢å¼ºäº†ä»…è§£ç å™¨Transformerçš„æ€§èƒ½ï¼Œå¹¶è¯æ˜å…¶åœ¨è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å­è¯æ ‡è®°å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤§è§„æ¨¡åˆæˆæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œåœ¨çœŸå®çš„äººå·¥æ³¨é‡Šæ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å­ŸåŠ æ‹‰è¯­æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰ç³»ç»Ÿå‘å±•ä¸è¶³ï¼Œé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>å­ŸåŠ æ‹‰è¯­è„šæœ¬çš„å¤æ‚æ€§ï¼ŒåŒ…æ‹¬è¿å­—ç¬¦ã€å˜éŸ³ç¬¦å·å’Œæ‰‹å†™é£æ ¼çš„å¤šæ ·æ€§ã€‚</li>
<li>ç¼ºä¹æ³¨é‡Šæ•°æ®é›†ä½¿å¾—å¼€å‘HTRç³»ç»Ÿæ›´åŠ å›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºGraphemeæ„ŸçŸ¥è§£ç å™¨Transformeræ¶æ„çš„GraDeT-HTRç³»ç»Ÿã€‚</li>
<li>é€šè¿‡é›†æˆåŸºäºéŸ³ç´ çš„æ ‡è®°å™¨æé«˜äº†ä»…è§£ç å™¨Transformerçš„æ€§èƒ½ã€‚</li>
<li>GraDeT-HTRåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2eafd2d4423907468513d447bd03ac9" align="middle">
<img src="https://picx.zhimg.com/v2-73cc2c9920c484fa3be3d4d5d15ce905" align="middle">
<img src="https://picx.zhimg.com/v2-8d76b3f869d59194ea1a992288412567" align="middle">
<img src="https://picx.zhimg.com/v2-eb154fea2d9818bbf0159ada99721085" align="middle">
<img src="https://picx.zhimg.com/v2-18d1f3d93ca9299ac83e77083954b8a0" align="middle">
<img src="https://picx.zhimg.com/v2-dbce32151da0b9726fde66e632dbfdbf" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs"><a href="#TempSamp-R1-Effective-Temporal-Sampling-with-Reinforcement-Fine-Tuning-for-Video-LLMs" class="headerlink" title="TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs"></a>TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning   for Video LLMs</h2><p><strong>Authors:Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (<a href="mailto:&#82;&#49;&#x40;&#48;&#x2e;&#55;">&#82;&#49;&#x40;&#48;&#x2e;&#55;</a>: 52.9%, +2.7%), ActivityNet Captions (<a href="mailto:&#x52;&#x31;&#64;&#x30;&#x2e;&#x35;">&#x52;&#x31;&#64;&#x30;&#x2e;&#x35;</a>: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1">https://github.com/HVision-NKU/TempSamp-R1</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TempSamp-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–ç²¾ç»†è°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘æ—¶é—´å®šä½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¦‚ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä¾èµ–äºç­–ç•¥æ›´æ–°çš„åœ¨ç­–ç•¥é‡‡æ ·ã€‚ç„¶è€Œï¼Œåœ¨å…·æœ‰å¤§æ—¶é—´æœç´¢ç©ºé—´çš„ä»»åŠ¡ä¸­ï¼Œè¿™ç§ç­–ç•¥å˜å¾—æ•ˆç‡ä½ä¸‹ä¸”æ€§èƒ½å—é™ï¼Œå› ä¸ºå®ƒç»å¸¸æ— æ³•æ‰¾åˆ°æ—¶é—´å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTempSamp-R1åˆ©ç”¨çœŸå®æ ‡æ³¨ä½œä¸ºç¦»çº¿ç›‘ç£æ¥æä¾›ç²¾ç¡®çš„æ—¶é—´æŒ‡å¯¼ï¼Œæœ‰æ•ˆåœ°è¡¥å¿äº†åœ¨ç­–ç•¥è§£å†³æ–¹æ¡ˆä¸­çš„ç¨€ç–æ€§å’Œä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–åŸºäºå¥–åŠ±çš„æ›´æ–°å¹¶å‡å°‘æ–¹å·®ï¼ŒTempSamp-R1æä¾›äº†ä¸€ç§éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸å¯¹ç§°è½¬æ¢åŠ¨æ€åœ°é‡å¡‘å¥–åŠ±åé¦ˆã€‚é€šè¿‡é‡‡ç”¨æ··åˆçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è®­ç»ƒèŒƒå¼ï¼ŒTempSamp-R1ä¼˜åŒ–äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¥æ”¯æŒCoTå’ŒéCoTæ¨ç†æ¨¡å¼ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†ä¸åŒæ¨ç†å¤æ‚åº¦çš„æŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTempSamp-R1ä¼˜äºåŸºäºGRPOçš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼šCharades-STAï¼ˆ<a href="mailto:&#82;&#x31;&#64;&#x30;&#46;&#55;">&#82;&#x31;&#64;&#x30;&#46;&#55;</a>: 52.9%ï¼Œ+2.7%ï¼‰ï¼ŒActivityNet Captionsï¼ˆ<a href="mailto:&#82;&#49;&#x40;&#x30;&#x2e;&#x35;">&#82;&#49;&#x40;&#x30;&#x2e;&#x35;</a>: 56.0%ï¼Œ+5.3%ï¼‰å’ŒQVHighlightsï¼ˆmAP: 30.0%ï¼Œ+3.0%ï¼‰ã€‚æ­¤å¤–ï¼ŒTempSamp-R1åœ¨æœ‰é™æ•°æ®ä¸‹æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å°‘é‡æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/TempSamp-R1">https://github.com/HVision-NKU/TempSamp-R1</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18056v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTempSamp-R1çš„æ–°å‹å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­çš„æ•ˆç‡ã€‚é’ˆå¯¹ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹æ—¶åºæœç´¢ç©ºé—´ä¸­çš„å±€é™æ€§å’Œä¸è¶³ï¼ŒTempSamp-R1åˆ©ç”¨çœŸå®æ ‡æ³¨ä½œä¸ºç¦»çº¿ç›‘ç£ï¼Œæä¾›ç²¾ç¡®çš„æ—¶é—´æŒ‡å¯¼ï¼Œå¹¶å¼•å…¥éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ä»¥ç¨³å®šè®­ç»ƒå’Œå‡å°‘å¥–åŠ±æ›´æ–°çš„æ–¹å·®ã€‚æ­¤å¤–ï¼ŒTempSamp-R1é‡‡ç”¨æ··åˆçš„Chain-of-Thoughtï¼ˆCoTï¼‰è®­ç»ƒèŒƒå¼ï¼Œä¼˜åŒ–å•ä¸€æ¨¡å‹ä»¥æ”¯æŒCoTå’ŒéCoTæ¨ç†æ¨¡å¼ï¼Œå¹¶å±•ç¤ºå‡ºè‰²çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTempSamp-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—çªç ´æ€§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬Charades-STAã€ActivityNet Captionså’ŒQVHighlightsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TempSamp-R1æ˜¯ä¸€ä¸ªæ–°çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ—¶åºå®šä½ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹æ—¶åºæœç´¢ç©ºé—´ä¸­è¡¨ç°ä¸ä½³ï¼ŒTempSamp-R1é€šè¿‡å¼•å…¥çœŸå®æ ‡æ³¨ä½œä¸ºç¦»çº¿ç›‘ç£æ¥å…‹æœè¿™ä¸€é—®é¢˜ã€‚</li>
<li>TempSamp-R1é‡‡ç”¨éçº¿æ€§è½¯ä¼˜åŠ¿è®¡ç®—æ–¹æ³•ï¼Œä»¥ç¨³å®šè®­ç»ƒå’Œå‡å°‘å¥–åŠ±æ›´æ–°çš„æ–¹å·®ã€‚</li>
<li>TempSamp-R1ç»“åˆChain-of-Thoughtï¼ˆCoTï¼‰è®­ç»ƒèŒƒå¼ï¼Œæ”¯æŒå¤šç§æ¨ç†æ¨¡å¼ã€‚</li>
<li>TempSamp-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬Charades-STAã€ActivityNet Captionså’ŒQVHighlightsã€‚</li>
<li>TempSamp-R1å…·æœ‰å¼ºå¤§çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-585eeb01e1b5036bc615aaedb3aea715" align="middle">
<img src="https://picx.zhimg.com/v2-a02c9f82f60723021b14d04507190943" align="middle">
<img src="https://picx.zhimg.com/v2-a763b12df7eed6a48c0395e058a35dec" align="middle">
<img src="https://picx.zhimg.com/v2-f63fbf7d12ced746bb82be62d064abd6" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="V2V-GoT-Vehicle-to-Vehicle-Cooperative-Autonomous-Driving-with-Multimodal-Large-Language-Models-and-Graph-of-Thoughts"><a href="#V2V-GoT-Vehicle-to-Vehicle-Cooperative-Autonomous-Driving-with-Multimodal-Large-Language-Models-and-Graph-of-Thoughts" class="headerlink" title="V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with   Multimodal Large Language Models and Graph-of-Thoughts"></a>V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with   Multimodal Large Language Models and Graph-of-Thoughts</h2><p><strong>Authors:Hsu-kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen, Stephen F. Smith</strong></p>
<p>Current state-of-the-art autonomous vehicles could face safety-critical situations when their local sensors are occluded by large nearby objects on the road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed as a means of addressing this problem, and one recently introduced framework for cooperative autonomous driving has further adopted an approach that incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative perception and planning processes. However, despite the potential benefit of applying graph-of-thoughts reasoning to the MLLM, this idea has not been considered by previous cooperative autonomous driving research. In this paper, we propose a novel graph-of-thoughts framework specifically designed for MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our proposed novel ideas of occlusion-aware perception and planning-aware prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for training and testing the cooperative driving graph-of-thoughts. Our experimental results show that our method outperforms other baselines in cooperative perception, prediction, and planning tasks. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œå½“å®ƒä»¬çš„å±€éƒ¨ä¼ æ„Ÿå™¨è¢«é“è·¯ä¸Šçš„å¤§å‹é™„è¿‘ç‰©ä½“é®æŒ¡æ—¶ï¼Œå¯èƒ½ä¼šé¢ä¸´å®‰å…¨å…³é”®çš„æƒ…å†µã€‚è½¦å¯¹è½¦ï¼ˆV2Vï¼‰ååŒè‡ªåŠ¨é©¾é©¶å·²è¢«æè®®ä½œä¸ºè§£å†³æ­¤é—®é¢˜çš„ä¸€ç§æ‰‹æ®µï¼Œè€Œæœ€è¿‘å¼•å…¥çš„ååŒè‡ªåŠ¨é©¾é©¶æ¡†æ¶è¿›ä¸€æ­¥é‡‡ç”¨äº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–¹æ³•ï¼Œä»¥æ•´åˆååŒæ„ŸçŸ¥å’Œè§„åˆ’è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå°½ç®¡å°†å›¾çŠ¶æ€ç»´æ¨ç†åº”ç”¨äºMLLMå…·æœ‰æ½œåœ¨ä¼˜åŠ¿ï¼Œä½†è¿™ä¸€æƒ³æ³•å¹¶æœªè¢«ä¹‹å‰çš„ååŒè‡ªåŠ¨é©¾é©¶ç ”ç©¶è€ƒè™‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨ç”¨äºåŸºäºMLLMçš„ååŒè‡ªåŠ¨é©¾é©¶çš„å›¾çŠ¶æ€ç»´æ¡†æ¶ã€‚æˆ‘ä»¬çš„å›¾çŠ¶æ€ç»´åŒ…æ‹¬æˆ‘ä»¬æå‡ºçš„é®æŒ¡æ„ŸçŸ¥æ„ŸçŸ¥å’Œè§„åˆ’æ„ŸçŸ¥é¢„æµ‹çš„æ–°æ€æƒ³ã€‚æˆ‘ä»¬æ•´ç†äº†V2V-GoT-QAæ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†V2V-GoTæ¨¡å‹ï¼Œç”¨äºè®­ç»ƒå’Œæµ‹è¯•ååŒé©¾é©¶çš„å›¾çŠ¶æ€ç»´ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ååŒæ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18053v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ€æ–°è‡ªä¸»é©¾é©¶è½¦è¾†æŠ€æœ¯åœ¨å±€éƒ¨ä¼ æ„Ÿå™¨å—åˆ°é“è·¯å¤§å‹ç‰©ä½“çš„é®æŒ¡æ—¶å¯èƒ½é¢ä¸´å®‰å…¨éšæ‚£ã€‚æœ¬æ–‡æå‡ºé‡‡ç”¨è½¦å¯¹è½¦ååŒè‡ªåŠ¨é©¾é©¶æ¥åº”å¯¹æ­¤é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ•´åˆåˆä½œæ„ŸçŸ¥å’Œè§„åˆ’æµç¨‹ã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°å°†è¿™ä¸€æƒ³æ³•ç”¨äºå®é™…çš„åˆä½œé©¾é©¶è¿‡ç¨‹ï¼Œæœ¬æ–‡å°†ç»“åˆä¸€ç§æ–°çš„æ€è€ƒå›¾è°±æ¨¡å‹æ„å»ºç›¸åº”çš„æ¡†æ¶ï¼Œå¹¶å¼•å…¥é®æŒ¡æ„ŸçŸ¥å’Œè§„åˆ’é¢„æµ‹çš„æ–°æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ„å»ºäº†V2V-GoTæ•°æ®é›†å¹¶å¼€å‘äº†V2V-GoTæ¨¡å‹ï¼Œä»¥å®ç°åˆä½œé©¾é©¶æ€è€ƒå›¾è°±çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åˆä½œæ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡æ–¹é¢ï¼Œæœ¬æ–‡çš„æ–¹æ³•è¾ƒåŸºçº¿æœ‰æ›´ä½³è¡¨ç°ã€‚</p>
<p><strong>å…³é”®å‘ç°</strong></p>
<ol>
<li>æœ€æ–°è‡ªä¸»é©¾é©¶æŠ€æœ¯åœ¨é¢å¯¹å¤§å‹ç‰©ä½“é®æŒ¡å±€éƒ¨ä¼ æ„Ÿå™¨æ—¶å­˜åœ¨å®‰å…¨éšæ‚£ã€‚</li>
<li>æå‡ºä½¿ç”¨è½¦å¯¹è½¦ååŒè‡ªåŠ¨é©¾é©¶ä½œä¸ºè§£å†³æ‰‹æ®µï¼Œå¹¶é‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ•´åˆæ„ŸçŸ¥å’Œè§„åˆ’è¿‡ç¨‹ã€‚</li>
<li>æå‡ºç»“åˆæ€è€ƒå›¾è°±æ¨¡å‹æ„å»ºæ–°çš„ååŒè‡ªåŠ¨é©¾é©¶æ¡†æ¶ã€‚å¼•å…¥æ–°æ¦‚å¿µåŒ…æ‹¬é®æŒ¡æ„ŸçŸ¥å’Œè§„åˆ’é¢„æµ‹ï¼Œä¼˜åŒ–è¿‡ç¨‹æ›´é€‚ç”¨äºç°å®æƒ…å†µã€‚</li>
<li>æ„å»ºæ–°çš„V2V-GoTæ•°æ®é›†ä»¥è®­ç»ƒå¹¶æµ‹è¯•åˆä½œé©¾é©¶æ€è€ƒå›¾è°±æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åˆ›æ–°æ–¹æ³•ä»¥è§£å†³å› ç‰©ä½“é®æŒ¡å¯¼è‡´è‡ªä¸»è½¦è¾†æ‰€é¢ä¸´çš„å®‰å…¨éšæ‚£é—®é¢˜ã€‚ </li>
<li>æ–°æ„å»ºçš„æ¡†æ¶æœ‰æ•ˆé›†æˆäº†è½¦è¾†é—´åˆä½œçš„æ„ŸçŸ¥å’Œè§„åˆ’æµç¨‹ï¼Œæé«˜äº†ååŒè‡ªåŠ¨é©¾é©¶çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-328339f05f8bcac949591f75d4d86409" align="middle">
<img src="https://picx.zhimg.com/v2-46157e9dbf638a75a7b93a8cc54048d5" align="middle">
<img src="https://picx.zhimg.com/v2-bda55b6729b86da34e032259d4ac8282" align="middle">
<img src="https://picx.zhimg.com/v2-fddbec75d1f555dfb44e8a07d2295684" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Beyond-Diagnosis-Evaluating-Multimodal-LLMs-for-Pathology-Localization-in-Chest-Radiographs"><a href="#Beyond-Diagnosis-Evaluating-Multimodal-LLMs-for-Pathology-Localization-in-Chest-Radiographs" class="headerlink" title="Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization   in Chest Radiographs"></a>Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization   in Chest Radiographs</h2><p><strong>Authors:Advait Gosai, Arun Kavishwar, Stephanie L. McNamara, Soujanya Samineni, Renato Umeton, Alexander Chowdhury, William Lotter</strong></p>
<p>Recent work has shown promising performance of frontier large language models (LLMs) and their multimodal counterparts in medical quizzes and diagnostic tasks, highlighting their potential for broad clinical utility given their accessible, general-purpose nature. However, beyond diagnosis, a fundamental aspect of medical image interpretation is the ability to localize pathological findings. Evaluating localization not only has clinical and educational relevance but also provides insight into a modelâ€™s spatial understanding of anatomy and disease. Here, we systematically assess two general-purpose MLLMs (GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to localize pathologies on chest radiographs, using a prompting pipeline that overlays a spatial grid and elicits coordinate-based predictions. Averaged across nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a localization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%), all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark (80.1%). Despite modest performance, error analysis revealed that GPT-5â€™s predictions were largely in anatomically plausible regions, just not always precisely localized. GPT-4 performed well on pathologies with fixed anatomical locations, but struggled with spatially variable findings and exhibited anatomically implausible predictions more frequently. MedGemma demonstrated the lowest performance on all pathologies, showing limited capacity to generalize to this novel task. Our findings highlight both the promise and limitations of current MLLMs in medical imaging and underscore the importance of integrating them with task-specific tools for reliable use. </p>
<blockquote>
<p>æœ€è¿‘çš„å·¥ä½œæ˜¾ç¤ºï¼Œå‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠå…¶å¤šæ¨¡æ€å¯¹åº”æ¨¡å‹åœ¨åŒ»å­¦æµ‹éªŒå’Œè¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œçªå‡ºäº†å®ƒä»¬ç”±äºæ˜“äºè®¿é—®ã€é€šç”¨æ€§å¼ºè€Œå…·æœ‰çš„å¹¿æ³›ä¸´åºŠåº”ç”¨çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œé™¤äº†è¯Šæ–­ä¹‹å¤–ï¼ŒåŒ»å­¦å›¾åƒè§£è¯»çš„ä¸€ä¸ªåŸºæœ¬èƒ½åŠ›æ˜¯å®šä½ç—…ç†å‘ç°ã€‚è¯„ä¼°å®šä½ä¸ä»…å…·æœ‰ä¸´åºŠå’Œæ•™è‚²æ„ä¹‰ï¼Œè€Œä¸”æä¾›äº†æ¨¡å‹å¯¹è§£å‰–ç»“æ„å’Œç–¾ç—…ç©ºé—´ç†è§£æ´å¯Ÿã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨æç¤ºç®¡é“ï¼ˆè¯¥ç®¡é“å åŠ äº†ä¸€ä¸ªç©ºé—´ç½‘æ ¼å¹¶æ¿€å‘åŸºäºåæ ‡çš„é¢„æµ‹ï¼‰ï¼Œç³»ç»Ÿè¯„ä¼°äº†ä¸¤ç§é€šç”¨MLLMï¼ˆGPT-4å’ŒGPT-5ï¼‰å’Œä¸€ä¸ªç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼ˆMedGemmaï¼‰åœ¨èƒ¸éƒ¨Xå…‰ç‰‡ä¸Šå®šä½ç—…ç†çš„èƒ½åŠ›ã€‚åœ¨CheXlocalizeæ•°æ®é›†ä¸Šçš„ä¹ç§ç—…ç†ä¸­ï¼ŒGPT-5çš„å®šä½å‡†ç¡®åº¦ä¸º49.7%ï¼Œå…¶æ¬¡æ˜¯GPT-4ï¼ˆ39.1%ï¼‰ï¼Œè€ŒMedGemmaï¼ˆ17.7%ï¼‰åˆ™ä½äºç‰¹å®šçš„CNNåŸºçº¿ï¼ˆ59.9%ï¼‰å’Œæ”¾å°„ç§‘åŒ»ç”ŸåŸºå‡†ï¼ˆ80.1%ï¼‰ã€‚å°½ç®¡è¡¨ç°å¹³å¹³ï¼Œä½†é”™è¯¯åˆ†ææ˜¾ç¤ºGPT-5çš„é¢„æµ‹å¤§å¤šåœ¨è§£å‰–ä¸Šåˆç†çš„åŒºåŸŸï¼Œåªæ˜¯å¹¶ä¸æ€»æ˜¯ç²¾ç¡®å®šä½ã€‚GPT-4åœ¨å…·æœ‰å›ºå®šè§£å‰–ä½ç½®çš„ç—…ç†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç©ºé—´ä¸Šå¯å˜çš„å‘ç°ä¸Šè¡¨ç°å›°éš¾ï¼Œå¹¶ä¸”æ›´é¢‘ç¹åœ°å‡ºç°è§£å‰–ä¸Šä¸åˆç†çš„é¢„æµ‹ã€‚MedGemmaåœ¨æ‰€æœ‰ç—…ç†ä¸Šçš„è¡¨ç°æœ€ä½ï¼Œæ˜¾ç¤ºå‡ºå¯¹è¿™é¡¹æ–°ä»»åŠ¡çš„æœ‰é™é€šç”¨åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å½“å‰MLLMåœ¨åŒ»å­¦æˆåƒæ–¹é¢çš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†å°†å®ƒä»¬ä¸ç‰¹å®šä»»åŠ¡å·¥å…·é›†æˆä»¥å¯é ä½¿ç”¨çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18015v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€åŒ»ç–—æµ‹è¯•ä»»åŠ¡ä¸­æœ‰æ½œåŠ›è¡¨ç°è‰¯å¥½ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸‰ç§æ¨¡å‹GPT-4ã€GPT-5ä»¥åŠé¢†åŸŸç‰¹å®šæ¨¡å‹MedGemmaåœ¨èƒ¸éƒ¨æ”¾å°„å›¾åƒä¸Šå®šä½ç—…ç†çš„èƒ½åŠ›ï¼Œå‘ç°GPT-5çš„å®šä½ç²¾åº¦ä¸º49.7%ï¼Œè¡¨ç°æœ€ä½³ï¼Œä½†ä»ä½äºä»»åŠ¡ç‰¹å®šCNNåŸºçº¿ï¼ˆ59.9%ï¼‰å’Œæ”¾å°„ç§‘åŒ»ç”ŸåŸºå‡†ï¼ˆ80.1%ï¼‰ã€‚å°½ç®¡æ€§èƒ½æ¸©å’Œï¼Œä½†åˆ†æè¡¨æ˜GPT-5çš„é¢„æµ‹å¤§å¤šåœ¨è§£å‰–ä¸Šåˆç†ï¼Œä½†å¹¶éæ€»æ˜¯ç²¾ç¡®ã€‚GPT-4åœ¨å›ºå®šè§£å‰–ä½ç½®çš„ç—…ç†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç©ºé—´ä¸Šå˜åŒ–è¾ƒå¤§çš„ç—…ç†ä¸Šé‡åˆ°å›°éš¾ï¼Œä¸”å¶å°”å‡ºç°è§£å‰–ä¸Šä¸åˆé€»è¾‘çš„é¢„æµ‹ã€‚MedGemmaåœ¨æ‰€æœ‰ç—…ç†ä¸Šè¡¨ç°æœ€ä½ï¼Œè¡¨æ˜å…¶åœ¨æ¨å¹¿æ­¤ç±»ä»»åŠ¡æ—¶èƒ½åŠ›æœ‰é™ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å½“å‰LLMsåœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†å°†å…¶ä¸ç‰¹å®šä»»åŠ¡å·¥å…·ç»“åˆä½¿ç”¨çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦è¯Šæ–­ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>åœ¨èƒ¸éƒ¨æ”¾å°„å›¾åƒä¸Šå®šä½ç—…ç†çš„èƒ½åŠ›è¯„ä¼°ä¸­ï¼ŒGPT-5è¡¨ç°æœ€ä½³ï¼Œä½†ç²¾åº¦ä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>GPT-4åœ¨å¤„ç†ç©ºé—´å˜åŒ–è¾ƒå¤§çš„ç—…ç†ä¸Šé‡åˆ°å›°éš¾ã€‚</li>
<li>MedGemmaåœ¨å®šä½ç—…ç†ä¸Šçš„è¡¨ç°æœ€ä½ï¼Œè¡¨æ˜å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>LLMsåœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›å’Œå±€é™æ€§å¹¶å­˜ï¼Œéœ€ç»“åˆç‰¹å®šä»»åŠ¡å·¥å…·ä»¥æé«˜å¯é æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06be026a638366b7b72d58ec6400cdd2" align="middle">
<img src="https://picx.zhimg.com/v2-9606c8985796318d7ca1c90be1e86eb2" align="middle">
<img src="https://picx.zhimg.com/v2-b72fb363bfdb1e0748a1cc31cbdb510d" align="middle">
<img src="https://picx.zhimg.com/v2-ef30f9c7c73106b9845ed7b3b8513ffc" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Adaptive-Kernel-Design-for-Bayesian-Optimization-Is-a-Piece-of-CAKE-with-LLMs"><a href="#Adaptive-Kernel-Design-for-Bayesian-Optimization-Is-a-Piece-of-CAKE-with-LLMs" class="headerlink" title="Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with   LLMs"></a>Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with   LLMs</h2><p><strong>Authors:Richard Cornelius Suwandi, Feng Yin, Juntao Wang, Renjie Li, Tsung-Hui Chang, Sergios Theodoridis</strong></p>
<p>The efficiency of Bayesian optimization (BO) relies heavily on the choice of the Gaussian process (GP) kernel, which plays a central role in balancing exploration and exploitation under limited evaluation budgets. Traditional BO methods often rely on fixed or heuristic kernel selection strategies, which can result in slow convergence or suboptimal solutions when the chosen kernel is poorly suited to the underlying objective function. To address this limitation, we propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO with large language models (LLMs). Concretely, CAKE leverages LLMs as the crossover and mutation operators to adaptively generate and refine GP kernels based on the observed data throughout the optimization process. To maximize the power of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to select the most effective kernel through balancing the model fit measured by the Bayesian information criterion (BIC) with the expected improvement at each iteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO method consistently outperforms established baselines across a range of real-world tasks, including hyperparameter optimization, controller tuning, and photonic chip design. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/cake4bo/cake">https://github.com/cake4bo/cake</a>. </p>
<blockquote>
<p>è´å¶æ–¯ä¼˜åŒ–ï¼ˆBOï¼‰çš„æ•ˆç‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºé«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰æ ¸çš„é€‰æ‹©ï¼Œè¿™åœ¨æœ‰é™çš„è¯„ä¼°é¢„ç®—ä¸‹å¯¹äºå¹³è¡¡æ¢ç´¢å’Œå¼€å‘èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚ä¼ ç»Ÿçš„BOæ–¹æ³•é€šå¸¸ä¾èµ–äºå›ºå®šæˆ–å¯å‘å¼æ ¸é€‰æ‹©ç­–ç•¥ï¼Œå½“æ‰€é€‰çš„æ ¸ä¸é€‚åˆåº•å±‚ç›®æ ‡å‡½æ•°æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ”¶æ•›é€Ÿåº¦æ…¢æˆ–å¾—åˆ°éæœ€ä¼˜è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‡ºç‚‰çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ ¸è¿›åŒ–ï¼ˆCAKEï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºBOã€‚å…·ä½“æ¥è¯´ï¼ŒCAKEåˆ©ç”¨LLMä½œä¸ºäº¤å‰å’Œå˜å¼‚ç®—å­ï¼ŒåŸºäºä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è§‚æµ‹æ•°æ®è‡ªé€‚åº”åœ°ç”Ÿæˆå’Œç»†åŒ–GPæ ¸ã€‚ä¸ºäº†æœ€å¤§åŒ–CAKEçš„å¨åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†BICè·å–æ ¸æ’åï¼ˆBAKERï¼‰ï¼Œé€šè¿‡å¹³è¡¡è´å¶æ–¯ä¿¡æ¯å‡†åˆ™ï¼ˆBICï¼‰æ‰€è¡¡é‡çš„æ¨¡å‹æ‹Ÿåˆä¸BOæ¯æ¬¡è¿­ä»£çš„é¢„æœŸæ”¹è¿›ï¼Œæ¥é€‰æ‹©æœ€æœ‰æ•ˆçš„æ ¸ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬å…¨æ–°çš„åŸºäºCAKEçš„BOæ–¹æ³•åœ¨ä¸€ç³»åˆ—çœŸå®ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºæ—¢å®šçš„åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬è¶…å‚æ•°ä¼˜åŒ–ã€æ§åˆ¶å™¨è°ƒä¼˜å’Œå…‰å­èŠ¯ç‰‡è®¾è®¡ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/cake4bo/cake%E3%80%82">https://github.com/cake4bo/cakeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17998v1">PDF</a> Accepted as Poster at NeurIPS 2025</p>
<p><strong>Summary</strong>ï¼šè´å¶æ–¯ä¼˜åŒ–çš„æ•ˆç‡å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜æ–¯è¿‡ç¨‹å†…æ ¸çš„é€‰æ‹©ï¼Œå†…æ ¸åœ¨æœ‰é™çš„è¯„ä¼°é¢„ç®—ä¸‹å¹³è¡¡æ¢ç´¢ä¸å¼€å‘èµ·ç€å…³é”®ä½œç”¨ã€‚ä¼ ç»Ÿçš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•å¸¸å¸¸ä¾èµ–å›ºå®šæˆ–å¯å‘å¼å†…æ ¸é€‰æ‹©ç­–ç•¥ï¼Œå½“é€‰æ‹©çš„å†…æ ¸ä¸é€‚åˆåº•å±‚ç›®æ ‡å‡½æ•°æ—¶ï¼Œå¯èƒ½å¯¼è‡´æ”¶æ•›é€Ÿåº¦æ…¢æˆ–å¾—åˆ°æ¬¡ä¼˜è§£ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å†…æ ¸è¿›åŒ–ï¼ˆCAKEï¼‰æ–¹æ³•å¢å¼ºè´å¶æ–¯ä¼˜åŒ–ã€‚CAKEåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºäº¤å‰å’Œå˜å¼‚è¿ç®—ç¬¦ï¼Œæ ¹æ®ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è§‚æµ‹æ•°æ®è‡ªé€‚åº”ç”Ÿæˆå’Œç»†åŒ–é«˜æ–¯è¿‡ç¨‹å†…æ ¸ã€‚ä¸ºæœ€å¤§åŒ–CAKEçš„å¨åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºBICé‡‡é›†å†…æ ¸æ’åï¼ˆBAKERï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¹³è¡¡æ¨¡å‹æ‹Ÿåˆçš„è´å¶æ–¯ä¿¡æ¯å‡†åˆ™ä¸è´å¶æ–¯ä¼˜åŒ–æ¯æ¬¡è¿­ä»£çš„é¢„æœŸæ”¹è¿›æ¥é€‰æ‹©æœ€æœ‰æ•ˆçš„å†…æ ¸ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒåŸºäºCAKEçš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•åœ¨ä¸€ç³»åˆ—çœŸå®ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬è¶…å‚æ•°ä¼˜åŒ–ã€æ§åˆ¶å™¨è°ƒä¼˜å’Œå…‰å­èŠ¯ç‰‡è®¾è®¡ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è´å¶æ–¯ä¼˜åŒ–çš„æ•ˆç‡ä¾èµ–äºé«˜æ–¯è¿‡ç¨‹å†…æ ¸çš„é€‰æ‹©ï¼Œå†…æ ¸åœ¨å¹³è¡¡æ¢ç´¢ä¸å¼€å‘ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ä¼ ç»Ÿè´å¶æ–¯ä¼˜åŒ–æ–¹æ³•å­˜åœ¨å†…æ ¸é€‰æ‹©é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´æ”¶æ•›é€Ÿåº¦æ…¢æˆ–å¾—åˆ°æ¬¡ä¼˜è§£ã€‚</li>
<li>ä¸Šä¸‹æ–‡æ„ŸçŸ¥å†…æ ¸è¿›åŒ–ï¼ˆCAKEï¼‰æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªé€‚åº”ç”Ÿæˆå’Œç»†åŒ–é«˜æ–¯è¿‡ç¨‹å†…æ ¸ã€‚</li>
<li>CAKEé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºäº¤å‰å’Œå˜å¼‚è¿ç®—ç¬¦å¢å¼ºè´å¶æ–¯ä¼˜åŒ–ã€‚</li>
<li>BICé‡‡é›†å†…æ ¸æ’åï¼ˆBAKERï¼‰æ–¹æ³•é€šè¿‡å¹³è¡¡æ¨¡å‹æ‹Ÿåˆçš„è´å¶æ–¯ä¿¡æ¯å‡†åˆ™ä¸é¢„æœŸæ”¹è¿›æ¥é€‰æ‹©æœ€æœ‰æ•ˆçš„å†…æ ¸ã€‚</li>
<li>åŸºäºCAKEçš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•åœ¨å¤šç§çœŸå®ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬è¶…å‚æ•°ä¼˜åŒ–ã€æ§åˆ¶å™¨è°ƒä¼˜å’Œå…‰å­èŠ¯ç‰‡è®¾è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c21f60fe8ddd5ee9f7b85cd3100a1d2" align="middle">
<img src="https://picx.zhimg.com/v2-52503da08329ad891220b830f426a37e" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ConfClip-Confidence-Weighted-and-Clipped-Reward-for-Reinforcement-Learning-in-LLMs"><a href="#ConfClip-Confidence-Weighted-and-Clipped-Reward-for-Reinforcement-Learning-in-LLMs" class="headerlink" title="ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement   Learning in LLMs"></a>ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement   Learning in LLMs</h2><p><strong>Authors:Bonan Zhang, Zhongqi Chen, Bowen Song, Qinya Li, Fan Wu, Guihai Chen</strong></p>
<p>Reinforcement learning (RL) has become a standard paradigm for refining large language models (LLMs) beyond pre-training and instruction tuning. A prominent line of work is RL with verifiable rewards (RLVR), which leverages automatically verifiable outcomes (e.g., correctness or executability) to generate reward signals. While efficient, this framework faces two key limitations: First, its binary feedback is too sparse to capture the quality of the reasoning process. Second, its coarse-grained rewards potentially lead to vanishing gradients. Inspired by observations from human learning, we introduce a RL technique that integrates verifiable outcomes with the modelâ€™s own confidence estimates. This joint design enriches the reward signal, providing finer-grained feedback and implicitly supervising the reasoning process. Experimental results demonstrate that our proposed method enhances RL performance across multiple datasets and reduces token consumption during inference, while incurring negligible additional training cost. Moreover, it can be used as a plug-in module to enhance other state-of-the-art RL methods. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºåœ¨é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒä¹‹å¤–ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç²¾ç»†è°ƒæ•´çš„æ ‡å‡†èŒƒå¼ã€‚ä¸€æ¡çªå‡ºçš„å·¥ä½œçº¿æ˜¯å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ï¼Œå®ƒåˆ©ç”¨å¯è‡ªåŠ¨éªŒè¯çš„ç»“æœï¼ˆä¾‹å¦‚æ­£ç¡®æ€§æˆ–å¯æ‰§è¡Œæ€§ï¼‰æ¥ç”Ÿæˆå¥–åŠ±ä¿¡å·ã€‚å°½ç®¡æ•ˆç‡å¾ˆé«˜ï¼Œä½†è¿™ä¸ªæ¡†æ¶é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šé¦–å…ˆï¼Œå…¶åé¦ˆæ˜¯äºŒè¿›åˆ¶çš„ï¼Œè¿‡äºç¨€ç–ï¼Œæ— æ³•æ•æ‰åˆ°æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚å…¶æ¬¡ï¼Œå…¶ç²—ç²’åº¦çš„å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚å—äººç±»å­¦ä¹ çš„è§‚å¯Ÿå¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç»“åˆäº†å¯éªŒè¯çš„ç»“æœä¸æ¨¡å‹æœ¬èº«çš„ä¿¡å¿ƒä¼°è®¡ã€‚è¿™ç§è”åˆè®¾è®¡ä¸°å¯Œäº†å¥–åŠ±ä¿¡å·ï¼Œæä¾›äº†æ›´ç²¾ç»†çš„åé¦ˆï¼Œå¹¶éšå«åœ°ç›‘ç£äº†æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæé«˜äº†å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡å°‘äº†æ ‡è®°ç¬¦çš„æ¶ˆè€—ï¼ŒåŒæ—¶åªäº§ç”Ÿäº†å¯ä»¥å¿½ç•¥çš„é¢å¤–è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥ä½œä¸ºä¸€ä¸ªæ’ä»¶æ¨¡å—ï¼Œç”¨äºå¢å¼ºå…¶ä»–æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17730v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å‡†èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒä¹‹åã€‚RLVRï¼ˆå¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼‰åˆ©ç”¨å¯è‡ªåŠ¨éªŒè¯çš„ç»“æœï¼ˆå¦‚æ­£ç¡®æ€§ï¼‰ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œå°½ç®¡æ•ˆç‡è¾ƒé«˜ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ã€‚é¦–å…ˆï¼Œå…¶åé¦ˆç¨€ç–ï¼Œæ— æ³•æ•æ‰æ¨ç†è¿‡ç¨‹çš„è´¨é‡ï¼›å…¶æ¬¡ï¼Œå…¶ç²—ç²’åº¦çš„å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚æœ¬ç ”ç©¶å—äººç±»å­¦ä¹ çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¯éªŒè¯ç»“æœä¸æ¨¡å‹è‡ªèº«ç½®ä¿¡åº¦ä¼°è®¡çš„RLæŠ€æœ¯ã€‚è¿™ç§è”åˆè®¾è®¡ä¸°å¯Œäº†å¥–åŠ±ä¿¡å·ï¼Œæä¾›äº†æ›´ç²¾ç»†çš„åé¦ˆå¹¶éšå«åœ°ç›‘ç£äº†æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†å¤šä¸ªæ•°æ®é›†ä¸Šçš„RLæ€§èƒ½ï¼Œé™ä½äº†æ¨ç†æ—¶çš„ä»¤ç‰Œæ¶ˆè€—ï¼Œä¸”å‡ ä¹ä¸å¢åŠ é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥ä½œä¸ºæ’ä»¶æ¨¡å—å¢å¼ºå…¶ä»–å…ˆè¿›çš„RLæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ‡å‡†æ–¹æ³•ã€‚</li>
<li>RLVRåˆ©ç”¨è‡ªåŠ¨éªŒè¯çš„ç»“æœç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œä½†å­˜åœ¨åé¦ˆç¨€ç–å’Œç²—ç²’åº¦å¥–åŠ±çš„é—®é¢˜ã€‚</li>
<li>ç»“åˆå¯éªŒè¯ç»“æœä¸æ¨¡å‹è‡ªèº«ç½®ä¿¡åº¦ä¼°è®¡çš„RLæŠ€æœ¯å¯ä»¥æé«˜å¥–åŠ±ä¿¡å·çš„ä¸°å¯Œæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†å¤šä¸ªæ•°æ®é›†ä¸Šçš„RLæ€§èƒ½ï¼Œå¹¶é™ä½äº†æ¨ç†æ—¶çš„ä»¤ç‰Œæ¶ˆè€—ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•å‡ ä¹ä¸å¢åŠ é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥ä½œä¸ºæ’ä»¶æ¨¡å—å¢å¼ºå…¶ä»–å…ˆè¿›çš„RLæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96dd758acc93017237f01cbf29c1dbaf" align="middle">
<img src="https://picx.zhimg.com/v2-420ab5b94445ef28080cf0e83df8cbd2" align="middle">
<img src="https://picx.zhimg.com/v2-39284dc37134a99e27728d0e4f8b3385" align="middle">
<img src="https://picx.zhimg.com/v2-5e1c3a1df3b1cbef2daee129dd547529" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CorefInst-Leveraging-LLMs-for-Multilingual-Coreference-Resolution"><a href="#CorefInst-Leveraging-LLMs-for-Multilingual-Coreference-Resolution" class="headerlink" title="CorefInst: Leveraging LLMs for Multilingual Coreference Resolution"></a>CorefInst: Leveraging LLMs for Multilingual Coreference Resolution</h2><p><strong>Authors:TuÄŸba Pamay Arslan, Emircan Erol, GÃ¼lÅŸen EryiÄŸit</strong></p>
<p>Coreference Resolution (CR) is a crucial yet challenging task in natural language understanding, often constrained by task-specific architectures and encoder-based language models that demand extensive training and lack adaptability. This study introduces the first multilingual CR methodology which leverages decoder-only LLMs to handle both overt and zero mentions. The article explores how to model the CR task for LLMs via five different instruction sets using a controlled inference method. The approach is evaluated across three LLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when instruction-tuned with a suitable instruction set, can surpass state-of-the-art task-specific architectures. Specifically, our best model, a fully fine-tuned Llama 3.1 for multilingual CR, outperforms the leading multilingual CR model (i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages in the CorefUD v1.2 dataset collection. </p>
<blockquote>
<p>æ ¸å¿ƒå¼•ç”¨è§£æï¼ˆCoreference Resolutionï¼Œç®€ç§°CRï¼‰æ˜¯è‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„ä¸€ä¸ªé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å®ƒé€šå¸¸å—åˆ°ç‰¹å®šä»»åŠ¡æ¶æ„å’ŒåŸºäºç¼–ç å™¨çš„è¯­è¨€æ¨¡å‹çš„é™åˆ¶ï¼Œè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡çš„è®­ç»ƒä¸”ç¼ºä¹é€‚åº”æ€§ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†é¦–ä¸ªå¤šè¯­è¨€CRæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä»…è§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å¤„ç†æ˜¾æ€§å’Œé›¶å¼•ç”¨ã€‚æ–‡ç« æ¢è®¨äº†å¦‚ä½•é€šè¿‡äº”ç§ä¸åŒçš„æŒ‡ä»¤é›†ï¼Œä½¿ç”¨å—æ§æ¨ç†æ–¹æ³•ï¼Œä¸ºLLMsæ„å»ºCRä»»åŠ¡æ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ç§LLMsä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬Llama 3.1ã€Gemma 2å’ŒMistral 0.3ã€‚ç»“æœè¡¨æ˜ï¼Œå½“LLMsé€šè¿‡åˆé€‚çš„æŒ‡ä»¤é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ—¶ï¼Œå®ƒä»¬å¯ä»¥è¶…è¶Šæœ€å…ˆè¿›çš„ç‰¹å®šä»»åŠ¡æ¶æ„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹æ˜¯ä¸“ä¸ºå¤šè¯­è¨€CRè¿›è¡Œå®Œå…¨å¾®è°ƒè¿‡çš„Llama 3.1ï¼Œåœ¨CorefUD v1.2æ•°æ®é›†é›†åˆçš„æ‰€æœ‰è¯­è¨€ä¸­ï¼Œå¹³å‡è¡¨ç°ä¼˜äºé¢†å…ˆçš„å¤šè¯­è¨€CRæ¨¡å‹ï¼ˆå³Corpipe 24å•é˜¶æ®µå˜ä½“ï¼‰2ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17505v1">PDF</a> Accepted for publication in Transactions of the Association for   Computational Linguistics (TACL) (2025 August). Submission: March, 2025.   Revision: July, 2025. Acceptance: August, 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨è§£ç å™¨ä»…å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†æ˜¾æ€§å’Œé›¶æåŠçš„æ ¸å¿ƒå¼•ç”¨è§£æï¼ˆCRï¼‰çš„å¤šè¯­è¨€æ–¹æ³•ã€‚æ–‡ç« æ¢è®¨äº†å¦‚ä½•é€šè¿‡äº”ç§ä¸åŒçš„æŒ‡ä»¤é›†ä¸ºLLMså»ºæ¨¡CRä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨äº†å—æ§æ¨ç†æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ç§LLMsï¼ˆLlama 3.1ã€Gemma 2å’ŒMistral 0.3ï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œå½“LLMsé€šè¿‡é€‚å½“çš„æŒ‡ä»¤é›†è¿›è¡Œè°ƒä¼˜æ—¶ï¼Œå¯ä»¥è¶…è¶Šç‰¹å®šçš„ä»»åŠ¡æ¶æ„ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹â€”â€”é’ˆå¯¹å¤šè¯­è¨€CRå…¨é¢è°ƒä¼˜çš„Llama 3.1ï¼Œåœ¨CorefUD v1.2æ•°æ®é›†ä¸Šå¹³å‡æ¯”é¢†å…ˆçš„å¤šè¯­è¨€CRæ¨¡å‹ï¼ˆå³Corpipe 24å•é˜¶æ®µå˜ä½“ï¼‰é«˜å‡º2ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§å¤šè¯­è¨€çš„æ ¸å¿ƒå¼•ç”¨è§£æï¼ˆCRï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§£ç å™¨ä»…å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†æ˜¾æ€§å’Œé›¶æåŠã€‚</li>
<li>é€šè¿‡äº”ç§ä¸åŒçš„æŒ‡ä»¤é›†æ¢ç´¢äº†LLMsçš„CRä»»åŠ¡å»ºæ¨¡æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨å—æ§æ¨ç†æ–¹æ³•å¯¹LLMsè¿›è¡ŒCRä»»åŠ¡è¯„ä¼°ã€‚</li>
<li>åœ¨ä¸‰ç§LLMsä¸Šè¿›è¡Œäº†å®éªŒï¼šLlama 3.1ã€Gemma 2å’ŒMistral 0.3ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€‚å½“æŒ‡ä»¤é›†è°ƒä¼˜çš„LLMså¯ä»¥è¶…è¶Šç‰¹å®šä»»åŠ¡æ¶æ„ã€‚</li>
<li>æœ€ä½³æ¨¡å‹Llama 3.1åœ¨å¤šè¯­è¨€CRä¸Šçš„è¡¨ç°è¶…è¿‡äº†é¢†å…ˆçš„æ¨¡å‹Corpipe 24å•é˜¶æ®µå˜ä½“ã€‚</li>
<li>åœ¨CorefUD v1.2æ•°æ®é›†ä¸Šï¼ŒLlama 3.1å¹³å‡æ¯”Corpipe 24é«˜å‡º2ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24b9a36fbf10e6712706434c04926991" align="middle">
<img src="https://picx.zhimg.com/v2-8c1afdcdd054ff4db5613465fe07b298" align="middle">
<img src="https://picx.zhimg.com/v2-46213e68ac926153fc6162ff48456f36" align="middle">
<img src="https://picx.zhimg.com/v2-8dde8db0ef893ca56bba793352771d89" align="middle">
<img src="https://picx.zhimg.com/v2-dc3879c37b94b5f858ac7da42bd4eab3" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Transformer-Gather-Fuzzy-Reconsider-A-Scalable-Hybrid-Framework-for-Entity-Resolution"><a href="#Transformer-Gather-Fuzzy-Reconsider-A-Scalable-Hybrid-Framework-for-Entity-Resolution" class="headerlink" title="Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for   Entity Resolution"></a>Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for   Entity Resolution</h2><p><strong>Authors:Mohammadreza Sharifi, Danial Ahmadzadeh</strong></p>
<p>Entity resolution plays a significant role in enterprise systems where data integrity must be rigorously maintained. Traditional methods often struggle with handling noisy data or semantic understanding, while modern methods suffer from computational costs or the excessive need for parallel computation. In this study, we introduce a scalable hybrid framework, which is designed to address several important problems, including scalability, noise robustness, and reliable results. We utilized a pre-trained language model to encode each structured data into corresponding semantic embedding vectors. Subsequently, after retrieving a semantically relevant subset of candidates, we apply a syntactic verification stage using fuzzy string matching techniques to refine classification on the unlabeled data. This approach was applied to a real-world entity resolution task, which exposed a linkage between a central user management database and numerous shared hosting server records. Compared to other methods, this approach exhibits an outstanding performance in terms of both processing time and robustness, making it a reliable solution for a server-side product. Crucially, this efficiency does not compromise results, as the system maintains a high retrieval recall of approximately 0.97. The scalability of the framework makes it deployable on standard CPU-based infrastructure, offering a practical and effective solution for enterprise-level data integrity auditing. </p>
<blockquote>
<p>å®ä½“è§£æåœ¨ä¼ä¸šç³»ç»Ÿä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œé‚£é‡Œéœ€è¦ä¸¥æ ¼ç»´æŠ¤æ•°æ®å®Œæ•´æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€éš¾ä»¥å¤„ç†å˜ˆæ‚çš„æ•°æ®æˆ–ç†è§£è¯­ä¹‰ï¼Œè€Œç°ä»£æ–¹æ³•åˆ™é¢ä¸´è®¡ç®—æˆæœ¬é«˜æ˜‚æˆ–éœ€è¦å¤§é‡å¹¶è¡Œè®¡ç®—çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ…æ‹¬å¯æ‰©å±•æ€§ã€å™ªå£°é²æ£’æ€§å’Œå¯é ç»“æœç­‰é‡è¦é—®é¢˜ã€‚æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å°†æ¯ä¸ªç»“æ„åŒ–æ•°æ®ç¼–ç ä¸ºç›¸åº”çš„è¯­ä¹‰åµŒå…¥å‘é‡ã€‚åœ¨æ£€ç´¢åˆ°è¯­ä¹‰ç›¸å…³çš„å€™é€‰å­é›†åï¼Œæˆ‘ä»¬åº”ç”¨åŸºäºæ¨¡ç³Šå­—ç¬¦ä¸²åŒ¹é…æŠ€æœ¯çš„å¥æ³•éªŒè¯é˜¶æ®µï¼Œå¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œç²¾ç»†åŒ–åˆ†ç±»ã€‚è¯¥æ–¹æ³•åº”ç”¨äºç°å®ä¸–ç•Œä¸­çš„å®ä½“è§£æä»»åŠ¡ï¼Œæ¶‰åŠä¸­å¤®ç”¨æˆ·ç®¡ç†æ•°æ®åº“å’Œä¼—å¤šå…±äº«ä¸»æœºæœåŠ¡å™¨è®°å½•ä¹‹é—´çš„å…³è”ã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†æ—¶é—´å’Œç¨³å¥æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæˆä¸ºæœåŠ¡å™¨ç«¯äº§å“çš„å¯é è§£å†³æ–¹æ¡ˆã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ç§æ•ˆç‡å¹¶ä¸ä¼šå½±å“ç»“æœï¼Œå› ä¸ºç³»ç»Ÿçš„æ£€ç´¢å¬å›ç‡ä¿æŒåœ¨é«˜æ°´å¹³çš„0.97å·¦å³ã€‚è¯¥æ¡†æ¶çš„å¯æ‰©å±•æ€§ä½¿å…¶å¯ä»¥éƒ¨ç½²åœ¨åŸºäºæ ‡å‡†CPUçš„åŸºç¡€è®¾æ–½ä¸Šï¼Œä¸ºä¼ä¸šçº§æ•°æ®å®Œæ•´æ€§å®¡è®¡æä¾›äº†å®ç”¨æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17470v1">PDF</a> Accepted at ICCKE 2025 Conference. 6 tables, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•çš„æ··åˆæ¡†æ¶ï¼Œç”¨äºè§£å†³ä¼ä¸šç³»ç»Ÿä¸­çš„å®ä½“è§£æé—®é¢˜ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³å¯æ‰©å±•æ€§ã€å™ªå£°é²æ£’æ€§å’Œå¯é ç»“æœç­‰æ–¹é¢çš„é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å°†ç»“æ„åŒ–æ•°æ®ç¼–ç ä¸ºç›¸åº”çš„è¯­ä¹‰åµŒå…¥å‘é‡ï¼Œå¹¶åº”ç”¨æ¨¡ç³Šå­—ç¬¦ä¸²åŒ¹é…æŠ€æœ¯è¿›è¡Œå¥æ³•éªŒè¯ï¼Œä»¥åœ¨æœªæ ‡è®°æ•°æ®ä¸Šè¿›è¡Œç²¾ç»†åŒ–åˆ†ç±»ã€‚è¯¥æ¡†æ¶åœ¨å®é™…çš„ç”¨æˆ·ç®¡ç†æ•°æ®åº“ä¸å…±äº«ä¸»æœºæœåŠ¡å™¨è®°å½•é“¾æ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰é«˜æ•ˆçš„å¤„ç†èƒ½åŠ›å’Œå¼ºå¤§çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å¬å›ç‡ã€‚æ¡†æ¶çš„å¯æ‰©å±•æ€§ä½¿å…¶èƒ½å¤Ÿåœ¨æ ‡å‡†CPUåŸºç¡€è®¾æ–½ä¸Šéƒ¨ç½²ï¼Œä¸ºä¼ä¸šçº§æ•°æ®å®Œæ•´æ€§å®¡è®¡æä¾›äº†å®ç”¨æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ä½“è§£æåœ¨ä¼ä¸šç³»ç»Ÿä¸­éå¸¸é‡è¦ï¼Œéœ€è¦ç»´æŠ¤æ•°æ®å®Œæ•´æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å˜ˆæ‚æ•°æ®æˆ–è¯­ä¹‰ç†è§£æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç°ä»£æ–¹æ³•é¢ä¸´è®¡ç®—æˆæœ¬æˆ–å¹¶è¡Œè®¡ç®—éœ€æ±‚è¿‡é«˜çš„æŒ‘æˆ˜ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•çš„æ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å®ä½“è§£æä¸­çš„å¯æ‰©å±•æ€§ã€å™ªå£°é²æ£’æ€§å’Œå¯é ç»“æœé—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å°†ç»“æ„åŒ–æ•°æ®ç¼–ç ä¸ºè¯­ä¹‰åµŒå…¥å‘é‡ã€‚</li>
<li>æ¡†æ¶ä½¿ç”¨æ¨¡ç³Šå­—ç¬¦ä¸²åŒ¹é…æŠ€æœ¯è¿›è¡Œå¥æ³•éªŒè¯ï¼Œä»¥åœ¨æœªæ ‡è®°æ•°æ®ä¸Šè¿›è¡Œç²¾ç»†åŒ–åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07906695c45d0bb194e3f6b30698dd59" align="middle">
<img src="https://picx.zhimg.com/v2-ae46145597caedc9c7fb64457b72a08a" align="middle">
<img src="https://picx.zhimg.com/v2-7fc969ffcb737dc83360cd992f9b2572" align="middle">
<img src="https://picx.zhimg.com/v2-3091ac6b64f2f52cd5e7ba6bd40fc44d" align="middle">
<img src="https://picx.zhimg.com/v2-5dea090331dc7fcb19a93eeb8d33969a" align="middle">
<img src="https://picx.zhimg.com/v2-1053f6a670c673dc590cb1584617a560" align="middle">
<img src="https://picx.zhimg.com/v2-7df2ec646a063d337ed5636216499a57" align="middle">
<img src="https://picx.zhimg.com/v2-4947ef0f62e348cb06ce144eb8c635e8" align="middle">
<img src="https://picx.zhimg.com/v2-fa4e510966d4ab93a144d2867d134bb2" align="middle">
<img src="https://picx.zhimg.com/v2-805c8b93e9d247bb452d31894457b00c" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Comparing-Model-vs-Agentic-Level-Red-Teaming-with-Action-Graph-Observability-on-GPT-OSS-20B"><a href="#Mind-the-Gap-Comparing-Model-vs-Agentic-Level-Red-Teaming-with-Action-Graph-Observability-on-GPT-OSS-20B" class="headerlink" title="Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with   Action-Graph Observability on GPT-OSS-20B"></a>Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with   Action-Graph Observability on GPT-OSS-20B</h2><p><strong>Authors:Ilham Wicaksono, Zekun Wu, Rahul Patel, Theo King, Adriano Koshiyama, Philip Treleaven</strong></p>
<p>As the industry increasingly adopts agentic AI systems, understanding their unique vulnerabilities becomes critical. Prior research suggests that security flaws at the model level do not fully capture the risks present in agentic deployments, where models interact with tools and external environments. This paper investigates this gap by conducting a comparative red teaming analysis of GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability framework AgentSeer to deconstruct agentic systems into granular actions and components, we apply iterative red teaming attacks with harmful objectives from HarmBench at two distinct levels: the standalone model and the model operating within an agentic loop. Our evaluation reveals fundamental differences between model level and agentic level vulnerability profiles. Critically, we discover the existence of agentic-only vulnerabilities, attack vectors that emerge exclusively within agentic execution contexts while remaining inert against standalone models. Agentic level iterative attacks successfully compromise objectives that completely failed at the model level, with tool-calling contexts showing 24% higher vulnerability than non-tool contexts. Conversely, certain model-specific exploits work exclusively at the model level and fail when transferred to agentic contexts, demonstrating that standalone model vulnerabilities do not always generalize to deployed systems. </p>
<blockquote>
<p>éšç€ä¸šç•Œè¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨æ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œç†è§£å®ƒä»¬ç‹¬ç‰¹çš„æ¼æ´å˜å¾—è‡³å…³é‡è¦ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹å±‚é¢çš„å®‰å…¨æ¼æ´å¹¶ä¸èƒ½å®Œå…¨æ•æ‰æ™ºèƒ½ä½“éƒ¨ç½²ä¸­çš„é£é™©ï¼Œå³åœ¨æ¨¡å‹ä¸­ä¸å·¥å…·å’Œå¤–éƒ¨ç¯å¢ƒè¿›è¡Œäº¤äº’çš„éƒ¨åˆ†ã€‚æœ¬æ–‡é€šè¿‡å¯¹æ¯”ç ”ç©¶GPT-OSS-20Bï¼ˆä¸€ä¸ªæ‹¥æœ‰20äº¿å‚æ•°çš„å¼€æºæ¨¡å‹ï¼‰çš„çº¢é˜Ÿåˆ†ææ¥æ¢è®¨è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬ä½¿ç”¨å¯è§‚å¯Ÿæ€§æ¡†æ¶AgentSeeræ¥å°†æ™ºèƒ½ä½“ç³»ç»Ÿåˆ†è§£ä¸ºé¢—ç²’çŠ¶çš„åŠ¨ä½œå’Œç»„ä»¶ï¼Œä»¥åœ¨ä¸¤ä¸ªä¸åŒå±‚é¢åº”ç”¨å…·æœ‰æœ‰å®³ç›®æ ‡çš„è¿­ä»£çº¢é˜Ÿæ”»å‡»ï¼šç‹¬ç«‹æ¨¡å‹å’Œè¿è¡Œäºæ™ºèƒ½ä½“å¾ªç¯ä¸­çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†æ¨¡å‹å±‚é¢å’Œæ™ºèƒ½ä½“å±‚é¢æ¼æ´åˆ†å¸ƒçš„æ ¹æœ¬å·®å¼‚ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº›ä»…å­˜åœ¨äºæ™ºèƒ½ä½“å±‚é¢çš„æ¼æ´ï¼Œè¿™äº›æ¼æ´å‘é‡ä»…åœ¨æ™ºèƒ½ä½“æ‰§è¡Œç¯å¢ƒä¸­å‡ºç°ï¼Œè€Œå¯¹ç‹¬ç«‹æ¨¡å‹åˆ™æ— æ•ˆã€‚æ™ºèƒ½ä½“å±‚é¢çš„è¿­ä»£æ”»å‡»æˆåŠŸç ´åäº†ä¹‹å‰æ¨¡å‹å±‚é¢æ— æ³•è¾¾æˆçš„ç›®æ ‡ï¼Œå¹¶ä¸”åœ¨å·¥å…·è°ƒç”¨ç¯å¢ƒä¸‹çš„æ¼æ´æ¯”éå·¥å…·ç¯å¢ƒä¸‹é«˜å‡º24%ã€‚ç›¸åï¼ŒæŸäº›ç‰¹å®šæ¨¡å‹çš„æ”»å‡»ç­–ç•¥åªèƒ½åœ¨æ¨¡å‹å±‚é¢ä¸Šå¥æ•ˆï¼Œå½“è½¬ç§»åˆ°æ™ºèƒ½ä½“ç¯å¢ƒæ—¶åˆ™æ— æ•ˆï¼Œè¿™è¡¨æ˜ç‹¬ç«‹æ¨¡å‹çš„æ¼æ´å¹¶ä¸æ€»æ˜¯é€‚ç”¨äºå·²éƒ¨ç½²çš„ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17259v1">PDF</a> Winner of the OpenAI GPT-OSS-20B Red Teaming Challenge (Kaggle, 2025)</p>
<p><strong>Summary</strong></p>
<p>éšç€ä¸šç•Œè¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨æ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œç†è§£å…¶ç‰¹æœ‰çš„æ¼æ´å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†GPT-OSS-20Bæ¨¡å‹åœ¨æ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æ¼æ´æƒ…å†µã€‚é€šè¿‡çº¢é˜Ÿæ”»å‡»æµ‹è¯•å’ŒAgentSeeræ¡†æ¶ï¼Œå‘ç°æ¨¡å‹çº§å’Œæ™ºèƒ½ä½“çº§çš„æ¼æ´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚å­˜åœ¨ä»…å­˜åœ¨äºæ™ºèƒ½ä½“æ‰§è¡Œä¸Šä¸‹æ–‡ä¸­çš„æ¼æ´ï¼Œå·¥å…·è°ƒç”¨ç¯å¢ƒçš„æ¼æ´æ›´æ˜¯é«˜å‡ºéå·¥å…·ç¯å¢ƒ24%ã€‚è¿™æ„å‘³ç€å•çº¯ä¾é æ¨¡å‹çº§çš„å®‰å…¨æµ‹è¯•ä¸è¶³ä»¥ä¿éšœæ•´ä¸ªæ™ºèƒ½ä½“ç³»ç»Ÿçš„å®‰å…¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±å…¥ç†è§£æ™ºèƒ½ä½“AIç³»ç»Ÿçš„è„†å¼±æ€§è‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™å…³ä¹å…¶å®é™…éƒ¨ç½²çš„å®‰å…¨æ€§ã€‚</li>
<li>GPT-OSS-20Bæ¨¡å‹åœ¨æ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„æ¼æ´ç ”ç©¶æ˜¯å¿…è¦çš„ã€‚</li>
<li>æ¨¡å‹çº§å’Œæ™ºèƒ½ä½“çº§çš„æ¼æ´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œéœ€è¦è¿›è¡Œé’ˆå¯¹æ€§çš„å®‰å…¨æµ‹è¯•ã€‚</li>
<li>å­˜åœ¨ä»…å­˜åœ¨äºæ™ºèƒ½ä½“æ‰§è¡Œä¸Šä¸‹æ–‡ä¸­çš„æ¼æ´ï¼Œè¿™äº›è¢«ç§°ä¸ºâ€œæ™ºèƒ½ä½“çº§ç‹¬æœ‰æ¼æ´â€ã€‚</li>
<li>å·¥å…·è°ƒç”¨ç¯å¢ƒçš„æ¼æ´é«˜äºéå·¥å…·ç¯å¢ƒï¼Œéœ€ç‰¹åˆ«å…³æ³¨ã€‚</li>
<li>å•çº¯ä¾èµ–æ¨¡å‹çº§çš„å®‰å…¨æµ‹è¯•å¹¶ä¸è¶³ä»¥ä¿éšœæ™ºèƒ½ä½“ç³»ç»Ÿçš„å…¨é¢å®‰å…¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cd5c38a7e51784f8b9515240e345524" align="middle">
<img src="https://picx.zhimg.com/v2-bbe25c2af2e08ba005549b7d219dc58b" align="middle">
<img src="https://picx.zhimg.com/v2-a8f55cfc458b9a864db93c2090ad8afc" align="middle">
<img src="https://picx.zhimg.com/v2-d457104edf5e48858abc5b8cf10a6a4a" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Interpretable-Audio-Editing-Evaluation-via-Chain-of-Thought-Difference-Commonality-Reasoning-with-Multimodal-LLMs"><a href="#Interpretable-Audio-Editing-Evaluation-via-Chain-of-Thought-Difference-Commonality-Reasoning-with-Multimodal-LLMs" class="headerlink" title="Interpretable Audio Editing Evaluation via Chain-of-Thought   Difference-Commonality Reasoning with Multimodal LLMs"></a>Interpretable Audio Editing Evaluation via Chain-of-Thought   Difference-Commonality Reasoning with Multimodal LLMs</h2><p><strong>Authors:Yuhang Jia, Xu Zhang, Yang Chen, Hui Wang, Enzhi Wang, Yong Qin</strong></p>
<p>Automatic mean opinion score (MOS) prediction provides a more perceptual alternative to objective metrics, offering deeper insights into the evaluated models. With the rapid progress of multimodal large language models (MLLMs), their enhanced perceptual and reasoning abilities enable more comprehensive and interpretable audio quality assessment. In this work, we tackle the challenging task of audio editing evaluation and propose the first natural language-based automated evaluation framework built on MLLMs. Our approach introduces two fine-tuning tasks to boost multi-audio understanding, combined with Chain-of-Thought prompting, and lightweight instruction tuning, to enhance step-by-step reasoning. Experiment demonstrate that our framework delivers accurate, interpretable, and text-based editing evaluation, closely aligning with human judgments and objective metrics while substantially improving over baselines. The code and demo are available at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Eval_Reasoning">https://github.com/NKU-HLT/Eval_Reasoning</a>. </p>
<blockquote>
<p>è‡ªåŠ¨ä¸»è§‚è¯„åˆ†ï¼ˆMOSï¼‰é¢„æµ‹ä¸ºå®¢è§‚æŒ‡æ ‡æä¾›äº†ä¸€ç§æ›´æ„ŸçŸ¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºæ‰€è¯„ä¼°çš„æ¨¡å‹æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ã€‚éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶å¢å¼ºçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›èƒ½å¤Ÿæä¾›æ›´å…¨é¢å’Œå¯è§£é‡Šçš„éŸ³é¢‘è´¨é‡è¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†éŸ³é¢‘ç¼–è¾‘è¯„ä¼°è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¹¶åŸºäºMLLMsæå‡ºäº†ç¬¬ä¸€ä¸ªè‡ªç„¶è¯­è¨€é©±åŠ¨çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå¾®è°ƒä»»åŠ¡æ¥æå‡å¤šéŸ³é¢‘ç†è§£ï¼Œç»“åˆé“¾å¼æ€ç»´æç¤ºå’Œè½»é‡çº§æŒ‡ä»¤å¾®è°ƒï¼Œä»¥å¢å¼ºé€æ­¥æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæä¾›å‡†ç¡®ã€å¯è§£é‡Šã€åŸºäºæ–‡æœ¬çš„è§†é¢‘ç¼–è¾‘è¯„ä¼°ï¼Œä¸äººç±»åˆ¤æ–­å’Œå®¢è§‚æŒ‡æ ‡ç´§å¯†å¯¹é½ï¼ŒåŒæ—¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¶…è¶Šäº†åŸºçº¿ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Eval_Reasoning">https://github.com/NKU-HLT/Eval_Reasoning</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨æ„ä¹‰è¯„åˆ†ï¼ˆMOSï¼‰é¢„æµ‹ä¸ºå®¢è§‚æŒ‡æ ‡æä¾›äº†æ›´æ„ŸçŸ¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºè¯„ä¼°æ¨¡å‹æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ã€‚éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶å¢å¼ºçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä¸ºéŸ³é¢‘è´¨é‡è¯„ä¼°æä¾›äº†æ›´å…¨é¢å’Œå¯è§£é‡Šçš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶è‡´åŠ›äºè§£å†³éŸ³é¢‘ç¼–è¾‘è¯„ä¼°çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œå¹¶é¦–æ¬¡æå‡ºäº†åŸºäºMLLMçš„è‡ªç„¶è¯­è¨€è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå¾®è°ƒä»»åŠ¡æ¥æå‡å¤šéŸ³é¢‘ç†è§£ï¼Œç»“åˆé“¾å¼æ€ç»´æç¤ºå’Œè½»é‡çº§æŒ‡ä»¤å¾®è°ƒï¼Œä»¥å¢å¼ºé€æ­¥æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›å‡†ç¡®ã€å¯è§£é‡Šã€åŸºäºæ–‡æœ¬çš„ç¼–è¾‘è¯„ä¼°ï¼Œä¸äººç±»åˆ¤æ–­å’Œå®¢è§‚æŒ‡æ ‡ç´§å¯†å¯¹é½ï¼Œå¹¶ä¸”åœ¨åŸºå‡†æµ‹è¯•ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Eval_Reasoning%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/Eval_Reasoningä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨æ„ä¹‰è¯„åˆ†ï¼ˆMOSï¼‰é¢„æµ‹ä¸ºå®¢è§‚æŒ‡æ ‡æä¾›äº†æ›´æ„ŸçŸ¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨éŸ³é¢‘è´¨é‡è¯„ä¼°ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†åŸºäºMLLMçš„è‡ªç„¶è¯­è¨€è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºéŸ³é¢‘ç¼–è¾‘è¯„ä¼°ã€‚</li>
<li>å¼•å…¥ä¸¤ä¸ªå¾®è°ƒä»»åŠ¡æå‡å¤šéŸ³é¢‘ç†è§£ã€‚</li>
<li>ç»“åˆé“¾å¼æ€ç»´æç¤ºå’Œè½»é‡çº§æŒ‡ä»¤å¾®è°ƒå¢å¼ºé€æ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶çš„è¯„ä¼°ç»“æœå‡†ç¡®ã€å¯è§£é‡Šï¼Œä¸äººç±»åˆ¤æ–­å’Œå®¢è§‚æŒ‡æ ‡ç´§å¯†å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3c7262854f689941a79c559eb39f10b" align="middle">
<img src="https://picx.zhimg.com/v2-2a4c8bcad4ebdd5021da337c7c309957" align="middle">
<img src="https://picx.zhimg.com/v2-2f0e76807e2bee2c478ca8d83a19aa70" align="middle">
<img src="https://picx.zhimg.com/v2-568fbf97cc572c22256f6f1d5ac83e6b" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Mental-Multi-class-Classification-on-Social-Media-Benchmarking-Transformer-Architectures-against-LSTM-Models"><a href="#Mental-Multi-class-Classification-on-Social-Media-Benchmarking-Transformer-Architectures-against-LSTM-Models" class="headerlink" title="Mental Multi-class Classification on Social Media: Benchmarking   Transformer Architectures against LSTM Models"></a>Mental Multi-class Classification on Social Media: Benchmarking   Transformer Architectures against LSTM Models</h2><p><strong>Authors:Khalid Hasan, Jamil Saquer, Yifan Zhang</strong></p>
<p>Millions of people openly share mental health struggles on social media, providing rich data for early detection of conditions such as depression, bipolar disorder, etc. However, most prior Natural Language Processing (NLP) research has focused on single-disorder identification, leaving a gap in understanding the efficacy of advanced NLP techniques for distinguishing among multiple mental health conditions. In this work, we present a large-scale comparative study of state-of-the-art transformer versus Long Short-Term Memory (LSTM)-based models to classify mental health posts into exclusive categories of mental health conditions. We first curate a large dataset of Reddit posts spanning six mental health conditions and a control group, using rigorous filtering and statistical exploratory analysis to ensure annotation quality. We then evaluate five transformer architectures (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against several LSTM variants (with or without attention, using contextual or static embeddings) under identical conditions. Experimental results show that transformer models consistently outperform the alternatives, with RoBERTa achieving 91-99% F1-scores and accuracies across all classes. Notably, attention-augmented LSTMs with BERT embeddings approach transformer performance (up to 97% F1-score) while training 2-3.5 times faster, whereas LSTMs using static embeddings fail to learn useful signals. These findings represent the first comprehensive benchmark for multi-class mental health detection, offering practical guidance on model selection and highlighting an accuracy-efficiency trade-off for real-world deployment of mental health NLP systems. </p>
<blockquote>
<p>æˆåƒä¸Šä¸‡çš„äººåœ¨ç¤¾äº¤åª’ä½“ä¸Šå…¬å¼€åˆ†äº«å¿ƒç†å¥åº·é—®é¢˜ï¼Œä¸ºæ—©æœŸæ£€æµ‹æŠ‘éƒç—‡ã€åŒç›¸æƒ…æ„Ÿéšœç¢ç­‰ç–¾ç—…æä¾›äº†ä¸°å¯Œæ•°æ®ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€ç–¾ç—…çš„è¯†åˆ«ä¸Šï¼Œå¯¹äºä½¿ç”¨å…ˆè¿›çš„NLPæŠ€æœ¯æ¥åŒºåˆ†å¤šç§å¿ƒç†å¥åº·ç–¾ç—…çš„æœ‰æ•ˆæ€§æ–¹é¢ä»å­˜åœ¨ç©ºç™½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹æœ€æ–°å˜å‹å™¨ä¸åŸºäºé•¿çŸ­æ—¶è®°å¿†ï¼ˆLSTMï¼‰çš„æ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡æ¯”è¾ƒç ”ç©¶ï¼Œä»¥å°†å¿ƒç†å¥åº·å¸–å­åˆ†ç±»ä¸ºå¤šç§å¿ƒç†å¥åº·ç–¾ç—…çš„ä¸“å±ç±»åˆ«ã€‚æˆ‘ä»¬é¦–å…ˆç­›é€‰äº†ä¸€ä¸ªæ¶µç›–å…­ç§å¿ƒç†å¥åº·ç–¾ç—…å’Œä¸€ç»„å¯¹ç…§çš„Redditå¸–å­å¤§å‹æ•°æ®é›†ï¼Œé€šè¿‡ä¸¥æ ¼çš„è¿‡æ»¤å’Œç»Ÿè®¡æ¢ç´¢æ€§åˆ†ææ¥ç¡®ä¿æ³¨é‡Šè´¨é‡ã€‚ç„¶åæˆ‘ä»¬åœ¨ç›¸åŒæ¡ä»¶ä¸‹è¯„ä¼°äº†äº”ç§å˜å‹å™¨æ¶æ„ï¼ˆBERTã€RoBERTaã€DistilBERTã€ALBERTå’ŒELECTRAï¼‰å’Œå‡ ç§LSTMå˜ä½“ï¼ˆå¸¦æˆ–ä¸å¸¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡åµŒå…¥æˆ–é™æ€åµŒå…¥ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå˜å‹å™¨æ¨¡å‹å§‹ç»ˆä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒRoBERTaåœ¨æ‰€æœ‰ç±»åˆ«ä¸­è¾¾åˆ°91-99ï¼…çš„F1åˆ†æ•°å’Œå‡†ç¡®ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»“åˆäº†BERTåµŒå…¥çš„å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰åœ¨æ¥è¿‘å˜å‹å™¨æ€§èƒ½çš„åŒæ—¶ï¼ˆé«˜è¾¾97ï¼…çš„F1åˆ†æ•°ï¼‰ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†2-3.5å€ï¼Œè€Œä½¿ç”¨é™æ€åµŒå…¥çš„LSTMåˆ™æ— æ³•å­¦ä¹ æœ‰ç”¨çš„ä¿¡å·ã€‚è¿™äº›å‘ç°ä»£è¡¨äº†é¦–æ¬¡è¿›è¡Œçš„å¤šç±»å¿ƒç†å¥åº·æ£€æµ‹çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œä¸ºæ¨¡å‹é€‰æ‹©æä¾›äº†å®é™…æŒ‡å¯¼ï¼Œå¹¶çªå‡ºäº†å¿ƒç†å¥åº·NLPç³»ç»Ÿåœ¨å®é™…éƒ¨ç½²ä¸­çš„å‡†ç¡®æ€§æ•ˆç‡æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16542v1">PDF</a> 24th IEEE International Conference on Machine Learning and   Applications, ICMLA 2025 (camera-ready)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¤¾äº¤åª’ä½“ä¸Šå…¬å¼€åˆ†äº«å¿ƒç†å¥åº·é—®é¢˜çš„äººæ•°æ—¥ç›Šå¢å¤šï¼Œä¸ºæ—©æœŸè¯†åˆ«æŠ‘éƒç—‡ã€åŒç›¸æƒ…æ„Ÿéšœç¢ç­‰ç–¾ç—…æä¾›äº†ä¸°å¯Œçš„æ•°æ®ã€‚ç„¶è€Œï¼Œå¤§éƒ¨åˆ†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç ”ç©¶èšç„¦äºå•ä¸€ç–¾ç—…çš„è¯†åˆ«ï¼Œå¯¹å¤šç§å¿ƒç†å¥åº·çŠ¶å†µçš„åŒºåˆ†ç ”ç©¶å­˜åœ¨ç©ºç™½ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¯”è¾ƒå½“å‰å…ˆè¿›çš„è½¬æ¢æ¨¡å‹ä¸åŸºäºé•¿çŸ­æ—¶è®°å¿†ï¼ˆLSTMï¼‰çš„æ¨¡å‹åœ¨åˆ†ç±»å¿ƒç†å¥åº·å¸–å­æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬ç­›é€‰äº†Redditä¸Šçš„å¸–å­ï¼Œæ¶µç›–å…­ç§å¿ƒç†å¥åº·çŠ¶å†µåŠå¯¹ç…§ç»„ï¼Œè¿›è¡Œä¸¥æ ¼çš„è¿‡æ»¤å’Œç»Ÿè®¡åˆ†æä»¥ç¡®ä¿æ³¨é‡Šè´¨é‡ã€‚è¯„ä¼°äº†äº”ç§è½¬æ¢å™¨æ¶æ„ä¸å‡ ç§LSTMå˜ç§ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè½¬æ¢å™¨æ¨¡å‹è¡¨ç°æ›´ä¼˜ç§€ï¼ŒRoBERTaåœ¨æ‰€æœ‰ç±»åˆ«ä¸­çš„F1åˆ†æ•°å’Œå‡†ç¡®ç‡å‡è¾¾åˆ°91%~99%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»“åˆæ³¨æ„åŠ›æœºåˆ¶çš„LSTMåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´ä¸ºå¿«é€Ÿï¼ˆé€Ÿåº¦åŠ å¿«çº¦ä¸¤å€ï¼‰ï¼Œä¸”åœ¨å¸¦æœ‰BERTåµŒå…¥çš„æƒ…å†µä¸‹èƒ½å¤Ÿæ¥è¿‘Transformeræ€§èƒ½ï¼ˆè¾¾åˆ°97%çš„F1åˆ†æ•°ï¼‰ï¼Œè€Œä½¿ç”¨é™æ€åµŒå…¥çš„LSTMåˆ™æ— æ³•å­¦ä¹ æœ‰æ•ˆä¿¡å·ã€‚è¿™äº›å‘ç°ä»£è¡¨äº†é¦–ä¸ªå…¨é¢çš„å¤šç±»å¿ƒç†å¥åº·æ£€æµ‹åŸºå‡†æµ‹è¯•ï¼Œä¸ºæ¨¡å‹é€‰æ‹©æä¾›äº†å®é™…æŒ‡å¯¼ï¼Œå¹¶å¼ºè°ƒäº†å¿ƒç†å¥åº·NLPç³»ç»Ÿåœ¨å®é™…éƒ¨ç½²ä¸­çš„å‡†ç¡®æ€§-æ•ˆç‡æƒè¡¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“æä¾›äº†å¤§é‡å¿ƒç†å¥åº·ç›¸å…³çš„æ•°æ®ï¼Œå¯ç”¨äºæ—©æœŸç–¾ç—…æ£€æµ‹ã€‚</li>
<li>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åœ¨åŒºåˆ†å¤šç§å¿ƒç†å¥åº·çŠ¶å†µæ–¹é¢çš„ç ”ç©¶å­˜åœ¨ç©ºç™½ã€‚</li>
<li>æœ¬ç ”ç©¶æ¯”è¾ƒäº†å…ˆè¿›çš„Transformeræ¨¡å‹å’ŒåŸºäºLSTMçš„æ¨¡å‹åœ¨åˆ†ç±»å¿ƒç†å¥åº·å¸–å­æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>RoBERTaåœ¨æ‰€æœ‰ç±»åˆ«ä¸­çš„è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡æé«˜ã€‚</li>
<li>ç»“åˆæ³¨æ„åŠ›æœºåˆ¶çš„LSTMè®­ç»ƒé€Ÿåº¦å¿«ï¼Œä¸”ä½¿ç”¨BERTåµŒå…¥æ—¶çš„æ€§èƒ½æ¥è¿‘Transformeræ¨¡å‹ã€‚</li>
<li>å¯¹æ¯”æ¨¡å‹åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„æ€§èƒ½æœ‰åŠ©äºè¿›è¡Œå®é™…åº”ç”¨æ—¶çš„æ¨¡å‹é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f73b9938a697ef419608d45e7744a9b4" align="middle">
<img src="https://picx.zhimg.com/v2-25e7a186ee6ca680303f9aa4204a6618" align="middle">
<img src="https://picx.zhimg.com/v2-5c330f9b5fdc45816b04651e840a174a" align="middle">
<img src="https://picx.zhimg.com/v2-3b044290585662c2ed6a406e1c057883" align="middle">
<img src="https://picx.zhimg.com/v2-9af4ce454fe709ca3aea7263e4eef997" align="middle">
<img src="https://picx.zhimg.com/v2-2bdc51dbea6c61d78baad4381defb1d7" align="middle">
<img src="https://picx.zhimg.com/v2-826b1f97e2669be282ba2fe39f71a9ec" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Empowered-Decision-Transformer-for-UAV-Enabled-Data-Collection"><a href="#Large-Language-Model-Empowered-Decision-Transformer-for-UAV-Enabled-Data-Collection" class="headerlink" title="Large Language Model-Empowered Decision Transformer for UAV-Enabled Data   Collection"></a>Large Language Model-Empowered Decision Transformer for UAV-Enabled Data   Collection</h2><p><strong>Authors:Zhixion Chen, Jiangzhou Wang, Hyundong Shin, Arumugam Nallanathan</strong></p>
<p>The deployment of unmanned aerial vehicles (UAVs) for reliable and energy-efficient data collection from spatially distributed devices holds great promise in supporting diverse Internet of Things (IoT) applications. Nevertheless, the limited endurance and communication range of UAVs necessitate intelligent trajectory planning. While reinforcement learning (RL) has been extensively explored for UAV trajectory optimization, its interactive nature entails high costs and risks in real-world environments. Offline RL mitigates these issues but remains susceptible to unstable training and heavily rely on expert-quality datasets. To address these challenges, we formulate a joint UAV trajectory planning and resource allocation problem to maximize energy efficiency of data collection. The resource allocation subproblem is first transformed into an equivalent linear programming formulation and solved optimally with polynomial-time complexity. Then, we propose a large language model (LLM)-empowered critic-regularized decision transformer (DT) framework, termed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we incorporate critic networks to regularize the DT model training, thereby integrating the sequence modeling capabilities of DT with critic-based value guidance to enable learning effective policies from suboptimal datasets. Furthermore, to mitigate the data-hungry nature of transformer models, we employ a pre-trained LLM as the transformer backbone of the DT model and adopt a parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid adaptation to UAV control tasks with small-scale dataset and low computational overhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark online and offline RL methods, achieving up to 36.7% higher energy efficiency than the current state-of-the-art DT approaches. </p>
<blockquote>
<p>æ— äººæœºï¼ˆUAVï¼‰çš„éƒ¨ç½²åœ¨å¯é ä¸”èƒ½æºé«˜æ•ˆåœ°ä»ç©ºé—´åˆ†å¸ƒè®¾å¤‡æ”¶é›†æ•°æ®æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä¸ºæ”¯æŒå¤šæ ·åŒ–çš„ç‰©è”ç½‘ï¼ˆIoTï¼‰åº”ç”¨æä¾›äº†å¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼Œæ— äººæœºçš„ç»­èˆªå’Œé€šä¿¡èŒƒå›´æœ‰é™ï¼Œéœ€è¦è¿›è¡Œæ™ºèƒ½è½¨è¿¹è§„åˆ’ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ— äººæœºè½¨è¿¹ä¼˜åŒ–æ–¹é¢å·²æœ‰å¹¿æ³›ç ”ç©¶ï¼Œä½†å…¶äº¤äº’æ€§è´¨åœ¨å®é™…ç¯å¢ƒä¸­å¸¦æ¥é«˜æ˜‚çš„æˆæœ¬å’Œé£é™©ã€‚ç¦»çº¿RLå‡è½»äº†è¿™äº›é—®é¢˜ï¼Œä½†ä»ç„¶å®¹æ˜“å—åˆ°è®­ç»ƒä¸ç¨³å®šçš„å½±å“ï¼Œå¹¶ä¸¥é‡ä¾èµ–äºä¸“å®¶çº§æ•°æ®é›†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ¶å®šäº†è”åˆæ— äººæœºè½¨è¿¹è§„åˆ’å’Œèµ„æºåˆ†é…é—®é¢˜ï¼Œä»¥æœ€å¤§åŒ–æ•°æ®æ”¶é›†çš„èƒ½æ•ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†èµ„æºåˆ†é…å­é—®é¢˜è½¬åŒ–ä¸ºç­‰æ•ˆçš„çº¿æ€§è§„åˆ’å½¢å¼ï¼Œå¹¶ä»¥å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦æœ€ä¼˜åœ°è§£å†³ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹èƒ½çš„è¯„è®ºå®¶æ­£åˆ™åŒ–å†³ç­–è½¬æ¢å™¨ï¼ˆDTï¼‰æ¡†æ¶ï¼Œç§°ä¸ºLLM-CRDTï¼Œç”¨äºå­¦ä¹ æœ‰æ•ˆçš„æ— äººæœºæ§åˆ¶ç­–ç•¥ã€‚åœ¨LLM-CRDTä¸­ï¼Œæˆ‘ä»¬ç»“åˆè¯„è®ºå®¶ç½‘ç»œæ¥è§„èŒƒDTæ¨¡å‹çš„è®­ç»ƒï¼Œä»è€Œå°†DTçš„åºåˆ—å»ºæ¨¡èƒ½åŠ›ä¸åŸºäºè¯„è®ºå®¶çš„ä»·å€¼æŒ‡å¯¼ç›¸ç»“åˆï¼Œä½¿èƒ½å¤Ÿä»æ¬¡ä¼˜æ•°æ®é›†ä¸­å­¦ä¹ æœ‰æ•ˆç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»è½¬æ¢å™¨æ¨¡å‹çš„æ•°æ®æ¸´æ±‚æ€§è´¨ï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒçš„LLMä½œä¸ºDTæ¨¡å‹çš„è½¬æ¢å™¨éª¨å¹²ï¼Œå¹¶é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œå³LoRAï¼Œèƒ½å¤Ÿå®ç°ç”¨å°è§„æ¨¡æ•°æ®é›†å¿«é€Ÿé€‚åº”æ— äººæœºæ§åˆ¶ä»»åŠ¡ä¸”é™ä½è®¡ç®—å¼€é”€ã€‚å¤§é‡æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼ŒLLM-CRDTä¼˜äºåŸºå‡†åœ¨çº¿å’Œç¦»çº¿RLæ–¹æ³•ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„DTæ–¹æ³•ç›¸æ¯”ï¼Œèƒ½æ•ˆæé«˜äº†é«˜è¾¾36.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13934v2">PDF</a> 14pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨æ— äººæœºï¼ˆUAVsï¼‰è¿›è¡Œå¯é ä¸”èƒ½æºé«˜æ•ˆçš„æ•°æ®é‡‡é›†ä»¥æ”¯æŒç‰©è”ç½‘ï¼ˆIoTï¼‰åº”ç”¨çš„å‰æ™¯ã€‚é’ˆå¯¹æ— äººæœºæœ‰é™çš„ç»­èˆªå’Œé€šä¿¡èŒƒå›´é—®é¢˜ï¼Œéœ€è¦æ™ºèƒ½è½¨è¿¹è§„åˆ’ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºæ— äººæœºè½¨è¿¹ä¼˜åŒ–ï¼Œä½†å…¶äº¤äº’æ€§è´¨å¯¼è‡´åœ¨å®é™…ç¯å¢ƒä¸­çš„æˆæœ¬é«˜å’Œé£é™©å¤§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè”åˆæ— äººæœºè½¨è¿¹è§„åˆ’å’Œèµ„æºåˆ†é…é—®é¢˜ï¼Œä»¥æœ€å¤§åŒ–æ•°æ®æ”¶é›†çš„èƒ½æºæ•ˆç‡ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹èƒ½çš„æ‰¹è¯„æ­£åˆ™åŒ–å†³ç­–è½¬æ¢å™¨ï¼ˆLLM-CRDTï¼‰æ¡†æ¶ï¼Œè§£å†³äº†èµ„æºåˆ†é…å­é—®é¢˜å¹¶å°†å…¶è½¬åŒ–ä¸ºç­‰æ•ˆçš„çº¿æ€§è§„åˆ’å½¢å¼ï¼Œä»¥æœ€ä¼˜æ–¹å¼è§£å†³å¹¶å…·æœ‰å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é‡‡ç”¨äº†é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä½œä¸ºå†³ç­–è½¬æ¢æ¨¡å‹çš„å˜å‹å™¨éª¨å¹²ï¼Œå¹¶é‡‡ç”¨LoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€‚åº”å°è§„æ¨¡æ•°æ®é›†å’Œä½è®¡ç®—å¼€é”€çš„æ— äººæœºæ§åˆ¶ä»»åŠ¡ã€‚æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼ŒLLM-CRDTç›¸è¾ƒäºåŸºå‡†åœ¨çº¿å’Œç¦»çº¿RLæ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„èƒ½æºæ•ˆç‡ï¼Œç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„å†³ç­–è½¬æ¢æ–¹æ³•ï¼Œèƒ½æºæ•ˆç‡æé«˜äº†é«˜è¾¾36.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— äººæœºåœ¨ç‰©è”ç½‘åº”ç”¨ä¸­å…·æœ‰æ•°æ®æ”¶é›†çš„æ½œåŠ›ã€‚</li>
<li>æ— äººæœºè½¨è¿¹è§„åˆ’å’Œèµ„æºåˆ†é…æ˜¯é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºæ— äººæœºè½¨è¿¹ä¼˜åŒ–é¢ä¸´é«˜æˆæœ¬å’Œé£é™©é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è”åˆè½¨è¿¹è§„åˆ’å’Œèµ„æºåˆ†é…æ–¹æ³•ä»¥æé«˜èƒ½æºæ•ˆç‡ã€‚</li>
<li>èµ„æºåˆ†é…é—®é¢˜è½¬åŒ–ä¸ºçº¿æ€§è§„åˆ’å½¢å¼ä»¥å®ç°æœ€ä¼˜è§£å†³ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹èµ‹èƒ½å†³ç­–è½¬æ¢å™¨æ¡†æ¶LLM-CRDTå­¦ä¹ æœ‰æ•ˆçš„æ— äººæœºæ§åˆ¶ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fcb5c83ba8d3885ca0f0643b88214b92" align="middle">
<img src="https://picx.zhimg.com/v2-150e6584a659eadfd969ce10943651db" align="middle">
<img src="https://picx.zhimg.com/v2-3751732572b8cda29f9a81fd7c8ac17e" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MALTA-An-Automated-CGRA-Design-Framework"><a href="#MALTA-An-Automated-CGRA-Design-Framework" class="headerlink" title="MALTA: An Automated CGRA Design Framework"></a>MALTA: An Automated CGRA Design Framework</h2><p><strong>Authors:Zesong Jiang, Yuqi Sun, Qing Zhong, Mahathi Krishna, Deepak Patil, Cheng Tan, Sriram Krishnamoorthy, Jeff Zhang</strong></p>
<p>Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing architecture that can deliver high-performance, energy-efficient acceleration across diverse domains. By supporting reconfiguration at the functional unit level, CGRAs efficiently adapt to varying computational patterns and optimize resource utilization. However, designing CGRAs is highly challenging due to the vast design space, independent architectural parameters, and the time-consuming nature of manual design. Fortunately, the rapid advancement of large language models (LLMs) presents new opportunities to automate this process.   In this work, we propose MALTAâ€“ an open-source multi-agent LLM-based framework for Hardware&#x2F;Software (HW&#x2F;SW) co-design of CGRAs. The framework employs LLM reasoning to generate CGRAs across four stages: HW&#x2F;SW co-design, Design error correction, Best design selection, and Evaluation &amp; Feedback. Furthermore, MALTA iteratively optimizes the generated CGRAs, leveraging agent reasoning and feedback to achieve higher PPA (that is, power, performance, and area) design points for a given domain. In addition, we introduce an LLM self-learning mechanism that employs LLM-driven decision making to select the optimal CGRA to accelerate the design process.   We evaluate the framework with state-of-the-art LLM-based methods and manual CGRA design, in terms of performance, power consumption, and area. Experimental results show that MALTA efficiently generates high-quality CGRA architectures, significantly reducing manual design effort and demonstrating the potential of our framework for real-world CGRA design. </p>
<blockquote>
<p>ç²—ç²’åº¦å¯é‡æ„é˜µåˆ—ï¼ˆCGRAsï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è®¡ç®—æ¶æ„ï¼Œå¯ä»¥åœ¨ä¸åŒçš„é¢†åŸŸæä¾›é«˜æ€§èƒ½ã€èŠ‚èƒ½çš„åŠ é€Ÿã€‚é€šè¿‡æ”¯æŒåŠŸèƒ½å•å…ƒçº§åˆ«çš„é‡æ„ï¼ŒCGRAsèƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”ä¸åŒçš„è®¡ç®—æ¨¡å¼å¹¶ä¼˜åŒ–èµ„æºåˆ©ç”¨ã€‚ç„¶è€Œï¼Œè®¾è®¡CGRAsé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè®¾è®¡ç©ºé—´å¤§ã€æ¶æ„å‚æ•°ç‹¬ç«‹ä»¥åŠæ‰‹åŠ¨è®¾è®¡çš„è€—æ—¶æ€§ã€‚å¹¸è¿çš„æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºè‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹æä¾›äº†æ–°çš„æœºä¼šã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MALTAâ€”â€”ä¸€ä¸ªå¼€æºçš„å¤šæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºç¡¬ä»¶&#x2F;è½¯ä»¶ï¼ˆHW&#x2F;SWï¼‰ååŒè®¾è®¡çš„CGRAsæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨LLMæ¨ç†åœ¨å››ä¸ªé˜¶æ®µç”ŸæˆCGRAsï¼šHW&#x2F;SWååŒè®¾è®¡ã€è®¾è®¡é”™è¯¯ä¿®æ­£ã€æœ€ä½³è®¾è®¡é€‰æ‹©ä»¥åŠè¯„ä¼°å’Œåé¦ˆã€‚æ­¤å¤–ï¼ŒMALTAé€šè¿‡æ™ºèƒ½ä½“æ¨ç†å’Œåé¦ˆæ¥è¿­ä»£ä¼˜åŒ–ç”Ÿæˆçš„CGRAsï¼Œä»¥å®ç°ç»™å®šé¢†åŸŸçš„æ›´é«˜PPAï¼ˆå³åŠŸç‡ã€æ€§èƒ½å’Œé¢ç§¯ï¼‰è®¾è®¡ç‚¹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§LLMè‡ªæˆ‘å­¦ä¹ æœºåˆ¶ï¼Œé‡‡ç”¨LLMé©±åŠ¨çš„å†³ç­–é€‰æ‹©æ¥åŠ é€Ÿè®¾è®¡è¿‡ç¨‹ä¸­çš„æœ€ä½³CGRAé€‰æ‹©ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13557v4">PDF</a> Due to certain confidentiality requirements, this article needs to be   withdrawn</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç²—ç²’åº¦å¯é‡æ„é˜µåˆ—ï¼ˆCGRAsï¼‰æ˜¯ä¸€ç§æœ‰å‰é€”çš„è®¡ç®—æ¶æ„ï¼Œå¯ä»¥åœ¨ä¸åŒçš„é¢†åŸŸæä¾›é«˜æ€§èƒ½ã€èŠ‚èƒ½çš„åŠ é€Ÿã€‚é€šè¿‡æ”¯æŒåŠŸèƒ½å•å…ƒçº§åˆ«çš„é‡æ„ï¼ŒCGRAså¯ä»¥çµæ´»åœ°é€‚åº”ä¸åŒçš„è®¡ç®—æ¨¡å¼å¹¶ä¼˜åŒ–èµ„æºåˆ©ç”¨ã€‚ç„¶è€Œï¼Œè®¾è®¡CGRAsé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¾è®¡ç©ºé—´å¤§ã€ç‹¬ç«‹çš„æ¶æ„å‚æ•°ä»¥åŠæ‰‹åŠ¨è®¾è®¡çš„æ—¶é—´æ¶ˆè€—ã€‚å¹¸è¿çš„æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºè‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹æä¾›äº†æ–°çš„æœºä¼šã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MALTAâ€”â€”ä¸€ä¸ªç”¨äºCGRAsè½¯ç¡¬ä»¶ååŒè®¾è®¡çš„å¼€æºå¤šæ™ºèƒ½ä½“LLMæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨LLMæ¨ç†ç”ŸæˆCGRAsçš„å››ä¸ªé˜¶æ®µï¼šè½¯ç¡¬ä»¶ååŒè®¾è®¡ã€è®¾è®¡é”™è¯¯ä¿®æ­£ã€æœ€ä½³è®¾è®¡é€‰æ‹©ã€è¯„ä¼°ä¸åé¦ˆã€‚æ­¤å¤–ï¼ŒMALTAé€šè¿‡åˆ©ç”¨æ™ºèƒ½ä½“æ¨ç†å’Œåé¦ˆæ¥è¿­ä»£ä¼˜åŒ–ç”Ÿæˆçš„CGRAsï¼Œä»¥å®ç°ç»™å®šé¢†åŸŸçš„æ›´é«˜åŠŸç‡æ€§èƒ½é¢ç§¯æ¯”ï¼ˆPPAï¼‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§LLMè‡ªå­¦ä¹ æœºåˆ¶ï¼Œé‡‡ç”¨LLMé©±åŠ¨çš„å†³ç­–é€‰æ‹©æœ€ä½³CGRAä»¥åŠ é€Ÿè®¾è®¡è¿‡ç¨‹ã€‚é€šè¿‡æœ€å…ˆè¿›çš„LLMæ–¹æ³•å’Œæ‰‹åŠ¨CGRAè®¾è®¡è¯„ä¼°æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ€§èƒ½ã€åŠŸè€—å’Œé¢ç§¯æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMALTAèƒ½æœ‰æ•ˆç”Ÿæˆé«˜è´¨é‡çš„CGRAæ¶æ„ï¼Œæ˜¾è‘—å‡å°‘æ‰‹åŠ¨è®¾è®¡å·¥ä½œé‡ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨ç°å®ä¸–ç•ŒCGRAè®¾è®¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CGRAsæ˜¯ä¸€ç§å…·æœ‰é«˜æ€§èƒ½å’Œèƒ½æºæ•ˆç‡çš„è®¡ç®—æ¶æ„ï¼Œå¯ä»¥åœ¨ä¸åŒçš„é¢†åŸŸæä¾›åŠ é€Ÿï¼Œå¹¶æ”¯æŒåŠŸèƒ½å•å…ƒçº§åˆ«çš„é‡æ„ã€‚</li>
<li>è®¾è®¡CGRAså…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè®¾è®¡ç©ºé—´å¤§ã€ç‹¬ç«‹çš„æ¶æ„å‚æ•°ä»¥åŠæ‰‹åŠ¨è®¾è®¡çš„æ—¶é—´æ¶ˆè€—ã€‚</li>
<li>LLMsçš„å¿«é€Ÿå‘å±•ä¸ºè‡ªåŠ¨åŒ–CGRAè®¾è®¡è¿‡ç¨‹æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>MALTAæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“LLMæ¡†æ¶ï¼Œç”¨äºCGRAsçš„è½¯ç¡¬ä»¶ååŒè®¾è®¡ï¼ŒåŒ…æ‹¬è®¾è®¡ã€ä¿®æ­£ã€é€‰æ‹©å’Œè¯„ä¼°é˜¶æ®µã€‚</li>
<li>MALTAåˆ©ç”¨LLMæ¨ç†å’Œè‡ªå­¦ä¹ æœºåˆ¶æ¥ä¼˜åŒ–ç”Ÿæˆçš„CGRAsï¼Œæé«˜åŠŸç‡æ€§èƒ½é¢ç§¯æ¯”ï¼ˆPPAï¼‰ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMALTAåœ¨ç”Ÿæˆé«˜è´¨é‡CGRAæ¶æ„æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½æ˜¾è‘—å‡å°‘æ‰‹åŠ¨è®¾è®¡å·¥ä½œé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d75fbbbb2c9d438d3ecd8032ffc890dd" align="middle">
<img src="https://picx.zhimg.com/v2-d713ff856fd303fa7232416402f2b580" align="middle">
<img src="https://picx.zhimg.com/v2-98f3eb3c506ec6a862753693eefb45ff" align="middle">
<img src="https://picx.zhimg.com/v2-59da966001b279fc40fdb8f4a991b2ab" align="middle">
<img src="https://picx.zhimg.com/v2-531ab1f54008eaa323eea0a9035dd710" align="middle">
<img src="https://picx.zhimg.com/v2-f843341f90bc7a406fe3cb5eac0fe9d0" align="middle">
<img src="https://picx.zhimg.com/v2-8ed06d000d4cfc4d411cb5c03cd5ddc6" align="middle">
<img src="https://picx.zhimg.com/v2-b6c5deecd32d6f4f68800d627cadbf07" align="middle">
<img src="https://picx.zhimg.com/v2-fd1c1817509a213f76307983acbc02bf" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-Thinking-Therapist-Training-Large-Language-Models-to-Deliver-Acceptance-and-Commitment-Therapy-using-Supervised-Fine-Tuning-and-Odds-Ratio-Policy-Optimization"><a href="#The-Thinking-Therapist-Training-Large-Language-Models-to-Deliver-Acceptance-and-Commitment-Therapy-using-Supervised-Fine-Tuning-and-Odds-Ratio-Policy-Optimization" class="headerlink" title="The Thinking Therapist: Training Large Language Models to Deliver   Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio   Policy Optimization"></a>The Thinking Therapist: Training Large Language Models to Deliver   Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio   Policy Optimization</h2><p><strong>Authors:Talha Tahir</strong></p>
<p>Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral therapy with emerging evidence of efficacy in several psychiatric conditions. This study investigates the impact of post-training methodology and explicit reasoning on the ability of a small open-weight large language model (LLM) to deliver ACT. Using synthetic ACT transcripts generated by Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches, supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each with and without an explicit chain-of-thought (COT) reasoning step. Performance was evaluated by comparing these four post-trained variants against the base Instruct model. These models were benchmarked in simulated therapy sessions, with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM) and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned on human evaluations. Our findings demonstrate that the ORPO-trained models significantly outperformed both their SFT and Instruct counterparts on ACT fidelity ($\chi^2(5) &#x3D; 185.15, p &lt; .001$) and therapeutic empathy ($\chi^2(5) &#x3D; 140.37, p &lt; .001$). The effect of COT was conditional as it provided a significant benefit to SFT models, improving ACT-FM scores by an average of 2.68 points ($p &lt; .001$), while offering no discernible advantage to the superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO stems from its ability to learn the therapeutic <code>process&#39; over imitating </code>content,â€™ a key aspect of ACT, while COT acts as a necessary scaffold for models trained only via imitation. This study establishes that preference-aligned policy optimization can effectively instill ACT competencies in small LLMs, and that the utility of explicit reasoning is highly dependent on the underlying training paradigm. </p>
<blockquote>
<p>æ¥çº³æ‰¿è¯ºç–—æ³•ï¼ˆACTï¼‰æ˜¯ä¸€ç§æ–°å…´çš„ã€åœ¨å¤šç§ç²¾ç¥ç–¾ç—…ä¸­ç–—æ•ˆæ˜¾è‘—çš„ç¬¬ä¸‰ä»£è®¤çŸ¥è¡Œä¸ºç–—æ³•ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åŸ¹è®­åçš„æ–¹æ³•å’Œæ˜ç¡®æ¨ç†å¯¹å°å‹å¼€æ”¾æƒé‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰§è¡ŒACTèƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬åˆ©ç”¨Mistral-Largeç”Ÿæˆçš„åˆæˆACTè½¬å½•æœ¬ï¼Œé‡‡ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•è®­ç»ƒäº†Llama-3.2-3b-Instructæ¨¡å‹ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œä¼˜åŠ¿æ¯”ç­–ç•¥ä¼˜åŒ–ï¼ˆORPOï¼‰ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰å’Œæ²¡æœ‰æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCOTï¼‰æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬é€šè¿‡å°†è¿™å››ä¸ªç»è¿‡è®­ç»ƒåçš„æ¨¡å‹ä¸åŸºç¡€æŒ‡ä»¤æ¨¡å‹è¿›è¡Œæ¯”è¾ƒæ¥è¯„ä¼°æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹åœ¨æ¨¡æ‹Ÿæ²»ç–—ä¼šè¯ä¸­è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶é€šè¿‡ACTå¿ è¯šåº¦åº¦é‡ï¼ˆACT-FMï¼‰å’Œç–—æ„ˆå¸ˆåŒç†å¿ƒé‡è¡¨ï¼ˆTESï¼‰è¿›è¡Œå®šé‡è¯„ä¼°ï¼Œè¯„ä¼°ç”±ç»è¿‡äººç±»è¯„ä¼°ç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„å§”è¿›è¡Œã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒORPOè®­ç»ƒçš„æ¨¡å‹åœ¨ACTå¿ è¯šåº¦ï¼ˆÏ‡Â²ï¼ˆ5ï¼‰&#x3D; 185.15ï¼Œp &lt; .001ï¼‰å’Œæ²»ç–—åŒç†å¿ƒï¼ˆÏ‡Â²ï¼ˆ5ï¼‰&#x3D; 140.37ï¼Œp &lt; .001ï¼‰æ–¹é¢æ˜¾è‘—ä¼˜äºSFTå’ŒæŒ‡ä»¤æ¨¡å‹ã€‚æ€ç»´é“¾çš„å½±å“æ˜¯æœ‰æ¡ä»¶çš„ï¼Œå› ä¸ºå®ƒå¯¹SFTæ¨¡å‹æœ‰æ˜æ˜¾çš„å¸®åŠ©ï¼ŒACT-FMå¾—åˆ†å¹³å‡æé«˜äº†2.68åˆ†ï¼ˆp &lt; .001ï¼‰ï¼Œè€Œå¯¹äºæ›´é«˜çº§çš„ORPOæˆ–æŒ‡ä»¤è°ƒæ•´å˜ä½“åˆ™æ²¡æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚æˆ‘ä»¬è®¤ä¸ºORPOçš„ä¼˜è¶Šæ€§æºäºå…¶å­¦ä¹ æ²»ç–—â€œè¿‡ç¨‹â€è€Œéä»…ä»…æ¨¡ä»¿â€œå†…å®¹â€çš„èƒ½åŠ›ï¼Œè¿™æ˜¯ACTçš„ä¸€ä¸ªå…³é”®æ–¹é¢ï¼Œè€Œæ€ç»´é“¾å¯¹äºä»…é€šè¿‡æ¨¡ä»¿è®­ç»ƒçš„æ¨¡å‹æ¥è¯´æ˜¯å¿…è¦çš„æ”¯æ¶ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œåå¥½å¯¹é½çš„ç­–ç•¥ä¼˜åŒ–å¯ä»¥æœ‰æ•ˆåœ°åœ¨å°å‹LLMä¸­çŒè¾“ACTèƒ½åŠ›ï¼Œè€Œæ˜ç¡®æ¨ç†çš„å®ç”¨æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºåº•å±‚è®­ç»ƒèŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09712v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨å°å‹å¼€æ”¾å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­åº”ç”¨æ¥çº³æ‰¿è¯ºç–—æ³•ï¼ˆACTï¼‰çš„æ•ˆæœã€‚å®éªŒæ¶‰åŠåˆæˆACTæ¼”è®²ç¨¿ï¼Œå¯¹Lama-3.2-3b-Instructæ¨¡å‹ä½¿ç”¨ä¸¤ç§è®­ç»ƒæ–¹æ³•ï¼ˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œä¼˜åŠ¿æ¯”ç­–ç•¥ä¼˜åŒ–ï¼ˆORPOï¼‰ï¼‰ç»“åˆä¸åŒçš„æ€ç»´æ–¹å¼ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒORPOè®­ç»ƒæ¨¡å‹åœ¨ACTçš„å¿ å®åº¦ä¸ç–—æ„ˆå…±æƒ…æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚æ€ç»´é€»è¾‘å¯¹äºæŸäº›æ¨¡å‹èµ·åˆ°äº†è¾…åŠ©çš„ä½œç”¨ï¼Œè€Œå¯¹ä¼˜åŠ¿è¾ƒå¤§çš„æ¨¡å‹å¹¶æ— æ˜¾è‘—ä¼˜åŠ¿ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒORPOä¹‹æ‰€ä»¥å…·æœ‰ä¼˜åŠ¿ï¼Œæ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿå­¦ä¹ æ²»ç–—è¿‡ç¨‹è€Œéå•çº¯æ¨¡ä»¿å†…å®¹ï¼Œè¿™æ˜¯ACTçš„æ ¸å¿ƒè¦ç´ ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿè¯æ˜äº†åå¥½å¯¹é½ç­–ç•¥ä¼˜åŒ–å¯ä»¥æœ‰æ•ˆåœ°å°†ACTæŠ€èƒ½èå…¥å°å‹LLMä¸­ï¼Œè€Œæ˜ç¡®æ¨ç†çš„å®ç”¨æ€§å–å†³äºåŸºç¡€è®­ç»ƒèŒƒå¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶è°ƒæŸ¥äº†ä¸åŒçš„è®­ç»ƒæ–¹æ³•å¯¹LLMåœ¨æä¾›ACTèƒ½åŠ›æ–¹é¢çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å¯¹å°å‹è¯­è¨€æ¨¡å‹çš„æ•ˆæœè¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚</li>
<li>å®éªŒåˆ©ç”¨åˆæˆACTæ¼”è®²ç¨¿æ¥è®­ç»ƒLLMæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨å››ç§ä¸åŒçš„è®­ç»ƒç»„åˆè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>ORPOè®­ç»ƒæ¨¡å‹åœ¨ACTå¿ å®åº¦å’Œç–—æ„ˆå…±æƒ…æ–¹é¢çš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…¶ä¼˜åŠ¿æºäºå¯¹æ²»ç–—è¿‡ç¨‹çš„å­¦ä¹ è€Œéå•çº¯æ¨¡ä»¿å†…å®¹ã€‚</li>
<li>æ€ç»´é€»è¾‘å¯¹éƒ¨åˆ†æ¨¡å‹çš„æ€§èƒ½èµ·åˆ°äº†ä¿ƒè¿›ä½œç”¨ï¼Œä½†å¯¹å·²ç»ç»è¿‡ä¼˜åŠ¿è®­ç»ƒçš„æ¨¡å‹å¹¶æ— é¢å¤–ä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶å‘ç°åå¥½å¯¹é½ç­–ç•¥ä¼˜åŒ–å¯ä»¥æœ‰æ•ˆåœ°å°†ACTæŠ€èƒ½èå…¥å°å‹LLMä¸­ã€‚</li>
<li>æ˜ç¡®æ¨ç†çš„å®ç”¨æ€§å–å†³äºåŸºç¡€è®­ç»ƒèŒƒå¼ï¼Œä¸æ¨¡å‹çš„è®­ç»ƒæ–¹å¼å’Œå½“å‰çŠ¶æ€æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4bf21ddfd6ff8de8c541376c57159740" align="middle">
<img src="https://picx.zhimg.com/v2-d2bfc314ffd0b1828fe612c296ff61e2" align="middle">
<img src="https://picx.zhimg.com/v2-7bc6b2c3079500d5d601eec1cec5962d" align="middle">
<img src="https://picx.zhimg.com/v2-57c5f84e4250e3b491d6713e7e360b2a" align="middle">
<img src="https://picx.zhimg.com/v2-efc791a2153cd672a68a644ad544efef" align="middle">
<img src="https://picx.zhimg.com/v2-a132a63581a8f3458669f333285790d3" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Measuring-Scalar-Constructs-in-Social-Science-with-LLMs"><a href="#Measuring-Scalar-Constructs-in-Social-Science-with-LLMs" class="headerlink" title="Measuring Scalar Constructs in Social Science with LLMs"></a>Measuring Scalar Constructs in Social Science with LLMs</h2><p><strong>Authors:Hauke Licht, Rupak Sarkar, Patrick Y. Wu, Pranav Goel, Niklas Stoehr, Elliott Ash, Alexander Miserlis Hoyle</strong></p>
<p>Many constructs that characterize language, like its complexity or emotionality, have a naturally continuous semantic structure; a public speech is not just â€œsimpleâ€ or â€œcomplex,â€ but exists on a continuum between extremes. Although large language models (LLMs) are an attractive tool for measuring scalar constructs, their idiosyncratic treatment of numerical outputs raises questions of how to best apply them. We address these questions with a comprehensive evaluation of LLM-based approaches to scalar construct measurement in social science. Using multiple datasets sourced from the political science literature, we evaluate four approaches: unweighted direct pointwise scoring, aggregation of pairwise comparisons, token-probability-weighted pointwise scoring, and finetuning. Our study finds that pairwise comparisons made by LLMs produce better measurements than simply prompting the LLM to directly output the scores, which suffers from bunching around arbitrary numbers. However, taking the weighted mean over the token probability of scores further improves the measurements over the two previous approaches. Finally, finetuning smaller models with as few as 1,000 training pairs can match or exceed the performance of prompted LLMs. </p>
<blockquote>
<p>è¯­è¨€çš„è®¸å¤šç‰¹å¾ï¼Œå¦‚å¤æ‚æ€§æˆ–æƒ…æ„Ÿæ€§ï¼Œéƒ½å…·æœ‰è‡ªç„¶è¿ç»­çš„è¯­ä¹‰ç»“æ„ï¼›å…¬å…±æ¼”è®²ä¸ä»…ä»…æ˜¯â€œç®€å•â€æˆ–â€œå¤æ‚â€ï¼Œè€Œæ˜¯å­˜åœ¨äºæç«¯ä¹‹é—´çš„è¿ç»­ä½“ä¸­ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯æµ‹é‡æ ‡é‡ç»“æ„çš„å¸å¼•äººçš„å·¥å…·ï¼Œä½†å®ƒä»¬å¯¹æ•°å€¼è¾“å‡ºçš„ç‹¬ç‰¹å¤„ç†æ–¹å¼æå‡ºäº†å¦‚ä½•æœ€å¥½åœ°åº”ç”¨å®ƒä»¬çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢è¯„ä¼°LLMåœ¨ç¤¾ä¼šç§‘å­¦ä¸­æµ‹é‡æ ‡é‡ç»“æ„çš„æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬åˆ©ç”¨æ¥è‡ªæ”¿æ²»ç§‘å­¦æ–‡çŒ®çš„å¤šä¸ªæ•°æ®é›†ï¼Œè¯„ä¼°äº†å››ç§æ–¹æ³•ï¼šæ— æƒé‡ç›´æ¥é€ç‚¹è®¡åˆ†ã€æˆå¯¹æ¯”è¾ƒèšåˆã€ä»¤ç‰Œæ¦‚ç‡åŠ æƒé€ç‚¹è®¡åˆ†å’Œå¾®è°ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒLLMè¿›è¡Œçš„æˆå¯¹æ¯”è¾ƒäº§ç”Ÿçš„æµ‹é‡å€¼ä¼˜äºç®€å•åœ°æç¤ºLLMç›´æ¥è¾“å‡ºåˆ†æ•°ï¼Œåè€…å¾ˆå®¹æ˜“èšé›†åœ¨ä»»æ„æ•°å­—å‘¨å›´ã€‚ç„¶è€Œï¼Œé€šè¿‡å¯¹ä»¤ç‰Œæ¦‚ç‡çš„å¾—åˆ†è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›å‰ä¸¤ä¸ªæ–¹æ³•çš„æµ‹é‡å€¼ã€‚æœ€åï¼Œé€šè¿‡å¾®è°ƒåªæœ‰1000ä¸ªè®­ç»ƒå¯¹çš„è¾ƒå°æ¨¡å‹ï¼Œå¯ä»¥åŒ¹é…æˆ–è¶…è¿‡æç¤ºLLMçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03116v2">PDF</a> Accepted to EMNLP 2025 (Main)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹é‡è¯­è¨€ç‰¹å¾å¦‚å¤æ‚åº¦å’Œæƒ…æ„Ÿæ—¶ï¼Œå­˜åœ¨æ•°å€¼è¾“å‡ºçš„ç‰¹å®šå¤„ç†æ¨¡å¼ï¼Œå¦‚ä½•åº”ç”¨æˆä¸ºå…³é”®é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡å…¨é¢è¯„ä¼°LLMåœ¨ç¤¾ä¼šç§‘å­¦é¢†åŸŸè¿›è¡Œæ ‡é‡ç‰¹å¾æµ‹é‡çš„å››ç§æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼šæ— åŠ æƒç›´æ¥ç‚¹å€¼è¯„åˆ†ã€æˆå¯¹æ¯”è¾ƒèšåˆã€åŸºäºtokenæ¦‚ç‡çš„åŠ æƒç‚¹å€¼è¯„åˆ†å’Œå¾®è°ƒã€‚ç ”ç©¶å‘ç°ï¼ŒLLMçš„æˆå¯¹æ¯”è¾ƒæµ‹é‡ä¼˜äºç›´æ¥è¾“å‡ºå¾—åˆ†çš„ç®€å•æç¤ºæ³•ï¼Œåè€…å®¹æ˜“äº§ç”Ÿéšæœºæ•°å€¼å †ç§¯é—®é¢˜ã€‚è¿›ä¸€æ­¥ä»¥tokenæ¦‚ç‡åŠ æƒå¹³å‡å€¼çš„æ–¹æ³•èƒ½å¤Ÿè¿›ä¸€æ­¥ä¼˜åŒ–æµ‹é‡æ•ˆæœã€‚æœ€ç»ˆï¼Œé€šè¿‡å¯¹å°å‹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ•°é‡è¾¾æ•°åƒçš„è®­ç»ƒå¯¹è¶³ä»¥ä¸LLMçš„æç¤ºæ•ˆæœç›¸åª²ç¾æˆ–è¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­è¨€ç‰¹å¾çš„è¿ç»­æ€§è¯­ä¹‰ç»“æ„ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹é‡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>LLMåœ¨æµ‹é‡æ ‡é‡ç‰¹å¾æ—¶å­˜åœ¨æ•°å€¼è¾“å‡ºçš„ç‰¹å®šå¤„ç†æ–¹å¼ï¼Œéœ€è¦åˆç†åº”ç”¨ã€‚</li>
<li>é€šè¿‡å››ç§æ–¹æ³•è¯„ä¼°LLMåœ¨ç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„æ ‡é‡ç‰¹å¾æµ‹é‡ï¼šæ— åŠ æƒç›´æ¥ç‚¹å€¼è¯„åˆ†ã€æˆå¯¹æ¯”è¾ƒèšåˆç­‰ã€‚</li>
<li>LLMçš„æˆå¯¹æ¯”è¾ƒæµ‹é‡æ³•ä¼˜äºç›´æ¥è¾“å‡ºå¾—åˆ†çš„ç®€å•æç¤ºæ³•ã€‚</li>
<li>åŸºäºtokenæ¦‚ç‡çš„åŠ æƒå¹³å‡å€¼æ–¹æ³•èƒ½å¤Ÿè¿›ä¸€æ­¥ä¼˜åŒ–æµ‹é‡ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec77653128f91a743dd0231bffc8885a" align="middle">
<img src="https://picx.zhimg.com/v2-b194781350b849076aca4e2e135e5342" align="middle">
<img src="https://picx.zhimg.com/v2-c3e70c88d1dc1425580549d2a09d4732" align="middle">
<img src="https://picx.zhimg.com/v2-03f087719a9aa67700f12ab3abaed652" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMsâ€™-Moral-Values-Understanding"><a href="#Beyond-Human-Judgment-A-Bayesian-Evaluation-of-LLMsâ€™-Moral-Values-Understanding" class="headerlink" title="Beyond Human Judgment: A Bayesian Evaluation of LLMsâ€™ Moral Values   Understanding"></a>Beyond Human Judgment: A Bayesian Evaluation of LLMsâ€™ Moral Values   Understanding</h2><p><strong>Authors:Maciej Skorski, Alina Landowska</strong></p>
<p>How do Large Language Models understand moral dimensions compared to humans?   This first large-scale Bayesian evaluation of market-leading language models provides the answer. In contrast to prior work using deterministic ground truth (majority or inclusion rules), we model annotator disagreements to capture both aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty (model domain sensitivity). We evaluated the best language models (Claude Sonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from nearly 700 annotators in 100K+ texts spanning social networks, news and forums.   Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing that AI models typically rank among the top 25% of human annotators, performing much better than average balanced accuracy. Importantly, we find that AI produces far fewer false negatives than humans, highlighting their more sensitive moral detection capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»ç›¸æ¯”æ˜¯å¦‚ä½•ç†è§£é“å¾·ç»´åº¦çš„ï¼Ÿè¿™ç¯‡å…³äºå¸‚åœºé¢†å…ˆè¯­è¨€æ¨¡å‹çš„é¦–ä¸ªå¤§è§„æ¨¡è´å¶æ–¯è¯„ä¼°æä¾›äº†ç­”æ¡ˆã€‚ä¸ä¹‹å‰ä½¿ç”¨ç¡®å®šæ€§çœŸå®å€¼ï¼ˆå¤šæ•°æˆ–åŒ…å«è§„åˆ™ï¼‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬å¯¹æ³¨é‡Šå™¨ä¹‹é—´çš„åˆ†æ­§è¿›è¡Œå»ºæ¨¡ï¼Œä»¥æ•æ‰éšæœºä¸ç¡®å®šæ€§ï¼ˆå›ºæœ‰çš„äººç±»åˆ†æ­§ï¼‰å’ŒçŸ¥è¯†ä¸ç¡®å®šæ€§ï¼ˆæ¨¡å‹é¢†åŸŸæ•æ„Ÿæ€§ï¼‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€ä½³è¯­è¨€æ¨¡å‹ï¼ˆClaude Sonnet 4ã€DeepSeek-V3ã€Llama 4 Maverickï¼‰ï¼Œè¿™äº›æ¨¡å‹åŸºäºæ¥è‡ªè¿‘700åæ³¨é‡Šè€…åœ¨è¶…è¿‡10ä¸‡ä¸ªæ–‡æœ¬ï¼ˆæ¶µç›–ç¤¾äº¤ç½‘ç»œã€æ–°é—»å’Œè®ºå›ï¼‰ä¸­çš„25ä¸‡å¤šä¸ªæ³¨é‡Šè¿›è¡Œäº†è¯„ä»·ã€‚æˆ‘ä»¬çš„GPUä¼˜åŒ–è´å¶æ–¯æ¡†æ¶å¤„ç†äº†è¶…è¿‡1ç™¾ä¸‡ä¸ªæ¨¡å‹æŸ¥è¯¢ï¼Œç»“æœè¡¨æ˜AIæ¨¡å‹é€šå¸¸æ’åœ¨äººç±»æ³¨é‡Šå™¨çš„å‰25%ï¼Œå…¶å¹³å‡å¹³è¡¡å‡†ç¡®ç‡è¿œé«˜äºäººç±»ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°AIäº§ç”Ÿçš„å‡é˜´æ€§è¿œå°‘äºäººç±»ï¼Œè¿™å‡¸æ˜¾äº†å®ƒä»¬æ›´æ•æ„Ÿçš„é“å¾·æ£€æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13804v2">PDF</a> Appears in UncertaiNLP@EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å¯¹äººç±»é“å¾·ç»´åº¦çš„ç†è§£ç¨‹åº¦å¦‚ä½•ï¼Ÿä¸€é¡¹å¤§è§„æ¨¡è´å¶æ–¯è¯„ä¼°æä¾›äº†ç­”æ¡ˆã€‚è¯¥ç ”ç©¶é‡‡ç”¨GPUä¼˜åŒ–çš„è´å¶æ–¯æ¡†æ¶ï¼Œå¯¹å¸‚åœºä¸Šé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä»·ï¼ŒåŒ…æ‹¬Claude Sonnet 4ã€DeepSeek-V3ã€Llama 4 Maverickç­‰ã€‚é€šè¿‡å¯¹è¿‘700åæ ‡æ³¨è€…åœ¨è¶…è¿‡ç™¾ä¸‡æ–‡æœ¬ä¸­çš„è¿‘ç™¾ä¸‡æ ‡æ³¨æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°AIæ¨¡å‹é€šå¸¸æ’ååœ¨å‰ç™¾åˆ†ä¹‹äºŒåäº”çš„äººç±»æ ‡æ³¨è€…ä¸­ï¼Œè¡¨ç°å‡ºè¶…è¶Šå¹³å‡æ°´å¹³çš„å‡†ç¡®æ€§ã€‚å°¤å…¶æ˜¯AIäº§ç”Ÿçš„å‡é˜´æ€§è¿œä½äºäººç±»ï¼Œæ˜¾ç¤ºå‡ºå…¶æ›´çµæ•çš„é“å¾·æ£€æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£é“å¾·ç»´åº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€šå¸¸æ’ååœ¨äººç±»æ ‡æ³¨è€…çš„å‰ç™¾åˆ†ä¹‹äºŒåäº”ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å¤§è§„æ¨¡ä½¿ç”¨è´å¶æ–¯è¯„ä¼°æ³•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä»·ã€‚</li>
<li>ä¸ä»¥å‰çš„ç ”ç©¶ä¸åŒï¼Œæœ¬ç ”ç©¶é‡‡ç”¨GPUä¼˜åŒ–çš„è´å¶æ–¯æ¡†æ¶æ¥å¤„ç†è¶…è¿‡ç™¾ä¸‡æ¬¡çš„æ¨¡å‹æŸ¥è¯¢ã€‚</li>
<li>æœ¬ç ”ç©¶åŒæ—¶è€ƒè™‘äº†äººç±»çš„ä¸¤ç±»ä¸ç¡®å®šæ€§ï¼šå†…åœ¨çš„ä¸å¯ç¡®å®šæ€§ï¼ˆéšæœºæ€§ä¸ç¡®å®šï¼‰å’ŒçŸ¥è¯†çš„ä¸ç¡®å®šæ€§ï¼ˆæ¨¡å‹é¢†åŸŸæ•æ„Ÿæ€§ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2eb9d1967fe8302de735dab6ce37c0db" align="middle">
<img src="https://picx.zhimg.com/v2-fb882c613fec774a15e78ed66ebb39fb" align="middle">
<img src="https://picx.zhimg.com/v2-a91693539deb2c9461fd2f180a40242b" align="middle">
<img src="https://picx.zhimg.com/v2-9a550768e096dd196fdd9e0412777129" align="middle">
<img src="https://picx.zhimg.com/v2-32101644cee3892179da6be4c1952055" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Advanced-Financial-Reasoning-at-Scale-A-Comprehensive-Evaluation-of-Large-Language-Models-on-CFA-Level-III"><a href="#Advanced-Financial-Reasoning-at-Scale-A-Comprehensive-Evaluation-of-Large-Language-Models-on-CFA-Level-III" class="headerlink" title="Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of   Large Language Models on CFA Level III"></a>Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of   Large Language Models on CFA Level III</h2><p><strong>Authors:Pranam Shetty, Abhisek Upadhayaya, Parth Mitesh Shah, Srikanth Jagabathula, Shilpi Nayak, Anna Joo Fee</strong></p>
<p>As financial institutions increasingly adopt Large Language Models (LLMs), rigorous domain-specific evaluation becomes critical for responsible deployment. This paper presents a comprehensive benchmark evaluating 23 state-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam - the gold standard for advanced financial reasoning. We assess both multiple-choice questions (MCQs) and essay-style responses using multiple prompting strategies including Chain-of-Thought and Self-Discover. Our evaluation reveals that leading models demonstrate strong capabilities, with composite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA Level III. These results, achieved under a revised, stricter essay grading methodology, indicate significant progress in LLM capabilities for high-stakes financial applications. Our findings provide crucial guidance for practitioners on model selection and highlight remaining challenges in cost-effective deployment and the need for nuanced interpretation of performance against professional benchmarks. </p>
<blockquote>
<p>éšç€é‡‘èæœºæ„è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸ºäº†è´Ÿè´£ä»»çš„éƒ¨ç½²ï¼Œä¸¥æ ¼çš„é¢†åŸŸç‰¹å®šè¯„ä¼°å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†23ä¸ªæœ€æ–°LLMåœ¨ç‰¹è®¸é‡‘èåˆ†æå¸ˆï¼ˆCFAï¼‰ä¸‰çº§è€ƒè¯•ä¸Šçš„è¡¨ç°â€”â€”è¿™æ˜¯é«˜çº§é‡‘èæ¨ç†çš„é‡‘æ ‡å‡†ã€‚æˆ‘ä»¬è¯„ä¼°äº†é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰å’Œè®ºæ–‡å¼å›ç­”ï¼Œé‡‡ç”¨äº†å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬æ€ç»´é“¾å’Œè‡ªå‘ç°ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œé¢†å…ˆæ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¦‚åœ¨CFAä¸‰çº§è€ƒè¯•ä¸­çš„ç»¼åˆå¾—åˆ†ç‡é«˜è¾¾79.1%ï¼ˆo4-miniï¼‰å’Œ77.3%ï¼ˆåŒå­åº§2.5é—ªå­˜ï¼‰ã€‚è¿™äº›ç»“æœæ˜¯åœ¨ä¿®è®¢åæ›´ä¸¥æ ¼çš„è®ºæ–‡è¯„åˆ†æ–¹æ³•ä¸‹å–å¾—çš„ï¼Œè¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜é£é™©é‡‘èåº”ç”¨æ–¹é¢çš„èƒ½åŠ›å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºä»ä¸šè€…æä¾›äº†å…³äºæ¨¡å‹é€‰æ‹©çš„å®è´µæŒ‡å¯¼ï¼Œå¹¶å¼ºè°ƒäº†ä½æˆæœ¬éƒ¨ç½²çš„å‰©ä½™æŒ‘æˆ˜ä»¥åŠä¸ä¸“ä¸šåŸºå‡†ç›¸å¯¹æ€§èƒ½ç»†è‡´è§£è¯»çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02954v2">PDF</a> Accepted at FinLLM @ IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†é‡‘èé¢†åŸŸå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ä¸è¯„ä¼°ã€‚é€šè¿‡å¯¹åŒ…æ‹¬CFAä¸‰çº§è€ƒè¯•åœ¨å†…çš„ä¸“ä¸šé‡‘èè€ƒè¯•è¿›è¡Œæ¨¡æ‹Ÿæµ‹è¯•ï¼Œå‘ç°é¡¶çº§LLMæ¨¡å‹åœ¨é«˜çº§é‡‘èæ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚è¿™ä¸€ç ”ç©¶ä¸ºä»ä¸šè€…æä¾›äº†é‡è¦æ¨¡å‹é€‰æ‹©æŒ‡å¯¼ï¼Œå¹¶æŒ‡å‡ºäº†æˆæœ¬æ•ˆç›Šéƒ¨ç½²çš„æŒ‘æˆ˜ä»¥åŠä¸“ä¸šåŸºå‡†æ€§èƒ½è¯„ä¼°çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æå–çš„å…³é”®è¦ç‚¹ï¼š</p>
<ol>
<li>é‡‘èæœºæ„å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡‡ç”¨æ­£åœ¨å¢åŠ ï¼Œå› æ­¤è¿›è¡Œé¢†åŸŸç‰¹å®šçš„ä¸¥æ ¼è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬ç ”ç©¶å¯¹23ç§æœ€æ–°LLMæ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•è¯„ä¼°ï¼Œä½¿ç”¨CFAä¸‰çº§è€ƒè¯•ä½œä¸ºé«˜çº§é‡‘èæ¨ç†çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>é€šè¿‡å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬Chain-of-Thoughtå’ŒSelf-Discoverï¼Œè¯„ä¼°äº†å¤šé€‰å’Œè®ºè¿°å¼å›ç­”çš„æ€§èƒ½ã€‚</li>
<li>é¡¶çº§æ¨¡å‹åœ¨CFAä¸‰çº§è€ƒè¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¦‚o4-miniçš„å¤åˆåˆ†æ•°è¾¾åˆ°79.1%ï¼ŒGemini 2.5 Flashçš„å¤åˆåˆ†æ•°ä¸º77.3%ã€‚</li>
<li>è¿™äº›ç»“æœæ˜¯åœ¨ä¿®è®¢çš„æ›´ä¸¥æ ¼çš„è®ºè¿°è¯„åˆ†æ–¹æ³•ä¸‹è·å¾—çš„ï¼Œè¡¨æ˜LLMåœ¨é«˜é£é™©é‡‘èåº”ç”¨ä¸­çš„èƒ½åŠ›å·²å–å¾—äº†é‡å¤§è¿›å±•ã€‚</li>
<li>ç ”ç©¶ç»“æœä¸ºä»ä¸šè€…æä¾›äº†å…³é”®æ¨¡å‹é€‰æ‹©æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†å¯¹æ¨¡å‹æˆæœ¬æ•ˆç›Šéƒ¨ç½²çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02954">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-433372cd569411f9f2860506ba364a2e" align="middle">
<img src="https://picx.zhimg.com/v2-d9840810e0b1fa362a77334251d1a07d" align="middle">
<img src="https://picx.zhimg.com/v2-43bd47c1005e0f370b8f67e47397ba40" align="middle">
<img src="https://picx.zhimg.com/v2-d64ad7a8b5de2e8614c9855d07ace6db" align="middle">
<img src="https://picx.zhimg.com/v2-10261184394ed10697da97d5c1e30fa0" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Med-PRM-Medical-Reasoning-Models-with-Stepwise-Guideline-verified-Process-Rewards"><a href="#Med-PRM-Medical-Reasoning-Models-with-Stepwise-Guideline-verified-Process-Rewards" class="headerlink" title="Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified   Process Rewards"></a>Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified   Process Rewards</h2><p><strong>Authors:Jaehoon Yun, Jiwoong Sohn, Jungwoo Park, Hyunjae Kim, Xiangru Tang, Yanjun Shao, Yonghoe Koo, Minhyeok Ko, Qingyu Chen, Mark Gerstein, Michael Moor, Jaewoo Kang</strong></p>
<p>Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: <a target="_blank" rel="noopener" href="https://med-prm.github.io/">https://med-prm.github.io/</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠå†³ç­–ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹çš„ç‰¹å®šæ­¥éª¤ä¸­éš¾ä»¥å®šä½å’Œçº æ­£é”™è¯¯ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œè¿™ä¸€å±€é™æ€§è‡³å…³é‡è¦ï¼Œå› ä¸ºå‘ç°å’Œè§£å†³æ¨ç†é”™è¯¯å¯¹äºå‡†ç¡®è¯Šæ–­å’Œæ²»ç–—æ‚£è€…è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†Med-PRMï¼Œè¿™æ˜¯ä¸€ç§è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¢å¼ºæ£€ç´¢ç”Ÿæˆçš„æ–¹æ³•ï¼Œæ ¹æ®å»ºç«‹åŒ»å­¦çŸ¥è¯†åº“æ¥éªŒè¯æ¯ä¸€æ­¥çš„æ¨ç†ã€‚é€šè¿‡ä»ä¸´åºŠæŒ‡å—å’Œæ–‡çŒ®ä¸­æ£€ç´¢çš„è¯æ®éªŒè¯ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç²¾ç»†åœ°è¯„ä¼°æ¨ç†è´¨é‡ã€‚åœ¨äº”ä¸ªåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªå¼€æ”¾è¯Šæ–­ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMed-PRMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½¿ç”¨Med-PRMæé«˜åŸºç¡€æ¨¡å‹çš„æ€§èƒ½é«˜è¾¾13.5%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶ä¸å¼ºå¤§çš„æ”¿ç­–æ¨¡å‹ï¼ˆå¦‚Meerkatï¼‰ä»¥æ’ä»¶æ–¹å¼é›†æˆï¼Œå±•ç¤ºäº†Med-PRMçš„æ™®éæ€§ï¼Œé¦–æ¬¡åœ¨å°è§„æ¨¡æ¨¡å‹ï¼ˆ8äº¿å‚æ•°ï¼‰ä¸Šå®ç°äº†è¶…è¿‡80%çš„MedQAå‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://med-prm.github.io/%E6%9F%A5%E7%9C%8B%E3%80%82">https://med-prm.github.io/æŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11474v2">PDF</a> Accepted to EMNLP 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠå†³ç­–ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥å¯¹æ¨ç†è¿‡ç¨‹ä¸­çš„å…·ä½“æ­¥éª¤è¿›è¡Œå®šä½å’Œçº é”™ã€‚æˆ‘ä»¬æå‡ºMed-PRMï¼Œä¸€ç§åˆ©ç”¨æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯çš„è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡å¯¹ç…§åŒ»å­¦çŸ¥è¯†åº“éªŒè¯æ¯ä¸€æ­¥æ¨ç†ï¼Œä»¥ç²¾ç»†çš„æ–¹å¼è¯„ä¼°æ¨ç†è´¨é‡ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒMed-PRMåœ¨äº”ä¸ªåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªå¼€æ”¾å¼è¯Šæ–­ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¾ƒåŸºç¡€æ¨¡å‹æ€§èƒ½æå‡æœ€å¤šè¾¾13.5%ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†Med-PRMä¸å¼ºå¤§ç­–ç•¥æ¨¡å‹ï¼ˆå¦‚Meerkatï¼‰ä»¥æ’ä»¶æ–¹å¼é›†æˆï¼Œæˆ‘ä»¬åœ¨å°è§„æ¨¡æ¨¡å‹ï¼ˆ8äº¿å‚æ•°ï¼‰ä¸Šé¦–æ¬¡å®ç°äº†è¶…è¿‡80%çš„MedQAå‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠå†³ç­–ä¸­å…·å¤‡æ½œåŠ›ï¼Œä½†å­˜åœ¨å®šä½åŠçº æ­£æ¨ç†é”™è¯¯çš„å…·ä½“æ­¥éª¤çš„å›°éš¾ã€‚</li>
<li>Med-PRMæ¡†æ¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯éªŒè¯æ¯ä¸€æ­¥æ¨ç†ã€‚</li>
<li>Med-PRMèƒ½ç²¾ç»†åœ°è¯„ä¼°æ¨ç†è´¨é‡ï¼Œé€šè¿‡å¯¹ç…§åŒ»å­¦çŸ¥è¯†åº“è¿›è¡ŒéªŒè¯ã€‚</li>
<li>Med-PRMåœ¨åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒåŸºç¡€æ¨¡å‹æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>Med-PRMå¯ä¸å¼ºå¤§ç­–ç•¥æ¨¡å‹é›†æˆï¼Œå®ç°é«˜å‡†ç¡®ç‡ã€‚</li>
<li>Med-PRMæä¾›æ’ä»¶å¼é›†æˆæ–¹å¼ï¼Œé€‚ç”¨äºä¸åŒæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-274e15ebb150de54697ebea217c09fcc" align="middle">
<img src="https://picx.zhimg.com/v2-79853e691393a5b4c57985cc0ed062b5" align="middle">
<img src="https://picx.zhimg.com/v2-0d24ce0d232b97b8cd6098fb134313e4" align="middle">
<img src="https://picx.zhimg.com/v2-b0335dc522e5b1836655b99a56f55613" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TurnaboutLLM-A-Deductive-Reasoning-Benchmark-from-Detective-Games"><a href="#TurnaboutLLM-A-Deductive-Reasoning-Benchmark-from-Detective-Games" class="headerlink" title="TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games"></a>TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games</h2><p><strong>Authors:Yuan Yuan, Muyu He, Muhammad Adil Shahid, Jiani Huang, Ziyang Li, Li Zhang</strong></p>
<p>This paper introduces TurnaboutLLM, a novel framework and dataset for evaluating the deductive reasoning abilities of Large Language Models (LLMs) by leveraging the interactive gameplay of detective games Ace Attorney and Danganronpa. The framework tasks LLMs with identifying contradictions between testimonies and evidences within long narrative contexts, a challenging task due to the large answer space and diverse reasoning types presented by its questions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at limitations of popular strategies for enhancing deductive reasoning such as extensive thinking and Chain-of-Thought prompting. The results also suggest varying effects of context size, the number of reasoning step and answer space size on model performance. Overall, TurnaboutLLM presents a substantial challenge for LLMsâ€™ deductive reasoning abilities in complex, narrative-rich environments. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TurnaboutLLMï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ä¾¦æ¢æ¸¸æˆã€Šé€†è½¬è£åˆ¤ã€‹å’Œã€Šå¼¹ä¸¸è®ºç ´ã€‹çš„äº’åŠ¨æ¸¸æˆç¯èŠ‚æ¥è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›çš„æ–°å‹æ¡†æ¶å’Œæ•°æ®é›†ã€‚è¯¥æ¡†æ¶çš„ä»»åŠ¡æ˜¯è¦æ±‚LLMåœ¨é•¿ç¯‡å™äº‹ç¯å¢ƒä¸­è¯†åˆ«è¯è¯å’Œè¯æ®ä¹‹é—´çš„çŸ›ç›¾ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå…¶ä¸­çš„é—®é¢˜å…·æœ‰è¾ƒå¤§çš„ç­”æ¡ˆç©ºé—´å’Œå¤šæ ·çš„æ¨ç†ç±»å‹ã€‚æˆ‘ä»¬åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°äº†åäºŒä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæš—ç¤ºäº†æé«˜æ¨ç†èƒ½åŠ›ï¼ˆå¦‚æ·±å…¥æ€è€ƒåŠæ€ç»´é“¾æç¤ºï¼‰çš„æµè¡Œç­–ç•¥çš„å±€é™æ€§ã€‚ç»“æœè¿˜è¡¨æ˜ä¸Šä¸‹æ–‡å¤§å°ã€æ¨ç†æ­¥éª¤çš„æ•°é‡å’Œç­”æ¡ˆç©ºé—´å¤§å°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“å„ä¸ç›¸åŒã€‚æ€»ä½“è€Œè¨€ï¼ŒTurnaboutLLMåœ¨å¤æ‚çš„å™äº‹ä¸°å¯Œç¯å¢ƒä¸­å¯¹LLMçš„æ¨ç†èƒ½åŠ›æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15712v2">PDF</a> In EMNLP 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºTurnaboutLLMçš„æ–°æ¡†æ¶å’Œæ•°æ®é›†ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä¾¦æ¢æ¸¸æˆAce Attorneyå’ŒDanganronpaçš„äº¤äº’å¼æ¸¸æˆç©æ³•æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¼”ç»æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶çš„ä»»åŠ¡æ˜¯ä½¿LLMèƒ½å¤Ÿåœ¨é•¿ç¯‡å™äº‹ç¯å¢ƒä¸­è¯†åˆ«è¯è¯å’Œè¯æ®ä¹‹é—´çš„çŸ›ç›¾ï¼Œè¿™ä¸€ä»»åŠ¡ç”±äºå…¶é—®é¢˜çš„å·¨å¤§ç­”æ¡ˆç©ºé—´å’Œå¤šæ ·çš„æ¨ç†ç±»å‹è€Œå˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¯¹åäºŒç§æœ€å…ˆè¿›çš„LLMè¿›è¡Œäº†æ•°æ®é›†è¯„ä¼°ï¼Œæ­ç¤ºäº†å¢å¼ºæ¼”ç»æ¨ç†çš„æµè¡Œç­–ç•¥ï¼ˆå¦‚æ·±åº¦æ€è€ƒå’ŒChain-of-Thoughtæç¤ºï¼‰çš„å±€é™æ€§ã€‚ç»“æœè¿˜è¡¨æ˜ï¼Œä¸Šä¸‹æ–‡å¤§å°ã€æ¨ç†æ­¥éª¤æ•°é‡å’Œç­”æ¡ˆç©ºé—´å¤§å°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“å„ä¸ç›¸åŒã€‚æ€»ä½“è€Œè¨€ï¼ŒTurnaboutLLMåœ¨å¤æ‚çš„å™äº‹ä¸°å¯Œç¯å¢ƒä¸­å¯¹LLMçš„æ¼”ç»æ¨ç†èƒ½åŠ›æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TurnaboutLLMæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMæ¼”ç»æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶å’Œæ•°æ®é›†ã€‚</li>
<li>å®ƒé€šè¿‡ä¾¦æ¢æ¸¸æˆçš„äº¤äº’å¼ç©æ³•æ¥æ¨¡æ‹Ÿå¤æ‚çš„æ¨ç†åœºæ™¯ã€‚</li>
<li>LLMé¢ä¸´è¯†åˆ«é•¿ç¯‡å™äº‹ä¸­çš„çŸ›ç›¾è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæµè¡Œçš„å¢å¼ºæ¼”ç»æ¨ç†ç­–ç•¥ï¼ˆå¦‚æ·±åº¦æ€è€ƒå’ŒChain-of-Thoughtæç¤ºï¼‰å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ä¸Šä¸‹æ–‡å¤§å°ã€æ¨ç†æ­¥éª¤æ•°é‡å’Œç­”æ¡ˆç©ºé—´å¤§å°å¯¹LLMæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>TurnaboutLLMä¸ºLLMåœ¨å¤æ‚å™äº‹ç¯å¢ƒä¸­çš„æ¼”ç»æ¨ç†èƒ½åŠ›è®¾å®šäº†é«˜æŒ‘æˆ˜æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d722e9506ceda4fdae50455df387cdf" align="middle">
<img src="https://picx.zhimg.com/v2-8ff9cebf4ae4823de731a0e1de972c1b" align="middle">
<img src="https://picx.zhimg.com/v2-738d068a03cdf3cd5048d1cd1fd7c3da" align="middle">
<img src="https://picx.zhimg.com/v2-06a380914184e0a24940b495dd7566b2" align="middle">
<img src="https://picx.zhimg.com/v2-82e1f3fe0015a10410985cc522e9b186" align="middle">
<img src="https://picx.zhimg.com/v2-6eb969b114dee5f3e546067363c991d4" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96335630fadeba55c4326db30a6c49f9" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  The STAR-XAI Protocol An Interactive Framework for Inducing   Second-Order Agency in AI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-585eeb01e1b5036bc615aaedb3aea715" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  UniPixel Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
