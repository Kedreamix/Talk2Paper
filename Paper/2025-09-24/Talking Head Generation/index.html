<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-09-24  Beat on Gaze Learning Stylized Generation of Gaze and Head Dynamics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-2728ea1bfc3efad0d6c082b65169f3e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684145&auth_key=1760684145-0-0-470bcc0fbc27a2bc8e4ee5fe453f0bca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    22 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-24-更新"><a href="#2025-09-24-更新" class="headerlink" title="2025-09-24 更新"></a>2025-09-24 更新</h1><h2 id="Beat-on-Gaze-Learning-Stylized-Generation-of-Gaze-and-Head-Dynamics"><a href="#Beat-on-Gaze-Learning-Stylized-Generation-of-Gaze-and-Head-Dynamics" class="headerlink" title="Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics"></a>Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics</h2><p><strong>Authors:Chengwei Shi, Chong Cao, Xin Tong, Xukun Shen</strong></p>
<p>Head and gaze dynamics are crucial in expressive 3D facial animation for conveying emotion and intention. However, existing methods frequently address facial components in isolation, overlooking the intricate coordination between gaze, head motion, and speech. The scarcity of high-quality gaze-annotated datasets hinders the development of data-driven models capable of capturing realistic, personalized gaze control. To address these challenges, we propose StyGazeTalk, an audio-driven method that generates synchronized gaze and head motion styles. We extract speaker-specific motion traits from gaze-head sequences with a multi-layer LSTM structure incorporating a style encoder, enabling the generation of diverse animation styles. We also introduce a high-precision multimodal dataset comprising eye-tracked gaze, audio, head pose, and 3D facial parameters, providing a valuable resource for training and evaluating head and gaze control models. Experimental results demonstrate that our method generates realistic, temporally coherent, and style-aware head-gaze motions, significantly advancing the state-of-the-art in audio-driven facial animation. </p>
<blockquote>
<p>头部和目光动态在传达情感与意图的表现力十足的3D面部动画中至关重要。然而，现有的方法往往孤立地处理面部组件，忽略了目光、头部运动和语音之间的复杂协调。高质量目光注释数据集的稀缺阻碍了数据驱动模型的发展，而这些模型能够捕捉真实、个性化的目光控制。为了解决这些挑战，我们提出了StyGazeTalk，这是一种音频驱动的方法，可以生成同步的目光和头部运动风格。我们从目光-头部序列中提取出与说话者相关的运动特征，采用多层LSTM结构并融入风格编码器，从而能够生成多样化的动画风格。我们还引入了一个高精度多模式数据集，包括眼动追踪目光、音频、头部姿态和3D面部参数，为训练和评估头部和目光控制模型提供了宝贵的资源。实验结果表明，我们的方法能够生成真实、时间连贯且风格明显的头部-目光运动，在音频驱动的面部动画方面显著提升了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17168v1">PDF</a> arXiv submission</p>
<p><strong>Summary</strong></p>
<p>本文提出一种音频驱动的方法，用于生成同步的凝视和头部运动风格。文章强调了头部和凝视动力学在表达3D面部动画中的重要性，并针对现有方法的局限性（孤立处理面部组件，忽视凝视、头部运动和语音之间的复杂协调）提出了创新的解决方案。为了捕捉真实的个性化凝视控制，引入了高质量凝视注释数据集。文章采用多层LSTM结构结合风格编码器，从凝视头部序列中提取说话人特定的运动特征，生成多样化的动画风格。此外，还引入了一个高精度多模式数据集，包含眼动追踪凝视、音频、头部姿势和3D面部参数，为训练和评估头部和凝视控制模型提供了宝贵资源。实验结果表明，该方法生成的头部凝视动作真实、时间连贯、风格鲜明。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强调了头部和凝视动力学在表达3D面部动画中的重要性。</li>
<li>现有方法常常孤立地处理面部组件，忽略了凝视、头部运动和语音之间的复杂协调。</li>
<li>高质量凝视注释数据集的缺乏阻碍了数据驱动模型的发展，这些模型能够捕捉真实的个性化凝视控制。</li>
<li>提出了 StyGazeTalk 方法，这是一种音频驱动的方法，用于生成同步的凝视和头部运动风格。</li>
<li>采用多层LSTM结构和风格编码器，从凝视头部序列中提取说话人特定的运动特征。</li>
<li>引入了一个包含眼动追踪凝视、音频、头部姿势和3D面部参数的高精度多模式数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17168">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-0d5b98f5984b41426083a6817253aa2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684153&auth_key=1760684153-0-0-0e0b25501c559cab92ea09fd2f6ed7b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-715e86ae7a05bf144f44d9923671a549~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684161&auth_key=1760684161-0-0-db4c8eee505a7ff75042e8b55716a18a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0aed3a14ef9dbed77d55574d8ebd1180~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684168&auth_key=1760684168-0-0-e01806af2caae7e79b4c27495de1764e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2f46be3dc63404e62d16d218295fa7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684175&auth_key=1760684175-0-0-9c537c1170dc511b40765daec67e60fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-367674a20c8fa40f79a4469036809de4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684182&auth_key=1760684182-0-0-8b139f9ab39cd1d7166c211507455e05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PGSTalker-Real-Time-Audio-Driven-Talking-Head-Generation-via-3D-Gaussian-Splatting-with-Pixel-Aware-Density-Control"><a href="#PGSTalker-Real-Time-Audio-Driven-Talking-Head-Generation-via-3D-Gaussian-Splatting-with-Pixel-Aware-Density-Control" class="headerlink" title="PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D   Gaussian Splatting with Pixel-Aware Density Control"></a>PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D   Gaussian Splatting with Pixel-Aware Density Control</h2><p><strong>Authors:Tianheng Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng</strong></p>
<p>Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment. </p>
<blockquote>
<p>音频驱动的说话人头部生成对于虚拟现实、数字化身和电影制作中的应用至关重要。虽然基于NeRF的方法能够实现高保真重建，但它们存在渲染效率低下和音视频同步不佳的问题。本研究提出了PGSTalker，一个基于3D高斯拼贴（3DGS）的实时音频驱动说话人头部合成框架。为了提高渲染性能，我们提出了一种像素感知密度控制策略，该策略能够自适应地分配点密度，从而在动态面部区域增强细节，同时减少其他地方的冗余。此外，我们还引入了一个轻量级的多模式门控融合模块，以有效地融合音频和空间特征，从而提高高斯变形预测的准确性。在公共数据集上的大量实验表明，PGSTalker在渲染质量、唇同步精度和推理速度方面优于现有的NeRF和3DGS方法。我们的方法表现出强大的泛化能力和在实际部署中的实用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16922v1">PDF</a> Main paper (15 pages). Accepted for publication by ICONIP(   International Conference on Neural Information Processing) 2025</p>
<p><strong>Summary</strong></p>
<p>基于音频驱动的说话人头部生成技术在虚拟现实、数字分身和电影制作中具有广泛应用。本研究提出PGSTalker，一个基于3D高斯平铺（3DGS）的实时音频驱动说话人头部合成框架。为提升渲染性能，研究采用像素感知密度控制策略，自适应分配点密度，在动态面部区域增强细节的同时减少冗余。此外，研究还引入轻量级多模态门控融合模块，有效融合音频和空间特征，从而提高高斯变形预测的准确性。在公共数据集上的广泛实验表明，PGSTalker在渲染质量、唇同步精度和推理速度上优于现有的NeRF和3DGS方法。该方法具有良好的通用性和实际应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频驱动的说话人头部生成技术在多个领域有重要应用。</li>
<li>PGSTalker是一个基于3D高斯平铺的实时音频驱动说话头合成框架。</li>
<li>像素感知密度控制策略提升了渲染性能，通过自适应分配点密度优化细节和冗余。</li>
<li>引入的多模态门控融合模块有效融合了音频和空间特征。</li>
<li>PGSTalker在渲染质量、唇同步精度和推理速度上优于现有方法。</li>
<li>该方法具有良好的通用性，可应用于多种场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16922">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4bee1be9c769726296148800b2e1d994~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684190&auth_key=1760684190-0-0-600ef3aaba9ad04d15ea50a80ca92a50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2728ea1bfc3efad0d6c082b65169f3e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684197&auth_key=1760684197-0-0-1515fbfdf184000dc6c071b2639fc849&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Deep-Dubbing-End-to-End-Auto-Audiobook-System-with-Text-to-Timbre-and-Context-Aware-Instruct-TTS"><a href="#Deep-Dubbing-End-to-End-Auto-Audiobook-System-with-Text-to-Timbre-and-Context-Aware-Instruct-TTS" class="headerlink" title="Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and   Context-Aware Instruct-TTS"></a>Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and   Context-Aware Instruct-TTS</h2><p><strong>Authors:Ziqi Dai, Yiting Chen, Jiacheng Xu, Liufei Xie, Yuchen Wang, Zhenchuan Yang, Bingsong Bai, Yangsheng Gao, Wenjiang Zhou, Weifeng Zhao, Ruohua Zhou</strong></p>
<p>The pipeline for multi-participant audiobook production primarily consists of three stages: script analysis, character voice timbre selection, and speech synthesis. Among these, script analysis can be automated with high accuracy using NLP models, whereas character voice timbre selection still relies on manual effort. Speech synthesis uses either manual dubbing or text-to-speech (TTS). While TTS boosts efficiency, it struggles with emotional expression, intonation control, and contextual scene adaptation. To address these challenges, we propose DeepDubbing, an end-to-end automated system for multi-participant audiobook production. The system comprises two main components: a Text-to-Timbre (TTT) model and a Context-Aware Instruct-TTS (CA-Instruct-TTS) model. The TTT model generates role-specific timbre embeddings conditioned on text descriptions. The CA-Instruct-TTS model synthesizes expressive speech by analyzing contextual dialogue and incorporating fine-grained emotional instructions. This system enables the automated generation of multi-participant audiobooks with both timbre-matched character voices and emotionally expressive narration, offering a novel solution for audiobook production. </p>
<blockquote>
<p>针对多参与者有声书生产流程主要涵盖三个阶段：剧本分析、角色嗓音音质的选取以及语音合成。其中，剧本分析可借助NLP模型实现高度自动化，而角色嗓音音质的选取仍需要人工操作。语音合成则采用人工配音或文本到语音（TTS）技术。虽然TTS技术能提高效率，但在情感表达、语调控制和上下文场景适应方面仍存在困难。为了应对这些挑战，我们提出了DeepDubbing这一端到端的多参与者有声书生产自动化系统。该系统主要包括两个组件：文本到音质（TTT）模型和上下文感知指令语音合成（CA-Instruct-TTS）模型。TTT模型根据文本描述生成特定角色的音质嵌入。CA-Instruct-TTS模型通过分析上下文对话并融入精细的情感指令，合成富有表现力的语音。该系统能够实现多参与者有声书的自动化生成，既匹配角色嗓音音质，又具备情感丰富的旁白，为有声书生产提供全新解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15845v1">PDF</a> Submitted to ICASSP 2026.Copyright 2026 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses, including reprinting&#x2F;republishing, creating new collective works, for   resale or redistribution to servers or lists, or reuse of any copyrighted   component of this work. DOI will be added upon IEEE Xplore publication</p>
<p><strong>Summary</strong><br>文本介绍了多参与者有声书生产的流程，包括脚本分析、角色声音音色选择和语音合成三个阶段。其中，脚本分析可以使用NLP模型实现自动化且高准确率；音色选择仍需要人工操作。语音合成采用人工配音或TTS技术，但TTS技术在情感表达、语调控制和场景适应方面存在挑战。为此，提出DeepDubbing系统，包括Text-to-Timbre模型和Context-Aware Instruct-TTS模型，实现多参与者有声书的自动化生成，具有音色匹配的角色声音和情感丰富的叙述。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多参与者有声书生产流程包括脚本分析、角色声音音色选择和语音合成三个阶段。</li>
<li>脚本分析可以使用NLP模型实现自动化且高准确率。</li>
<li>音色选择目前仍需要人工操作。</li>
<li>语音合成采用人工配音或TTS技术，但TTS在情感表达、语调控制和场景适应方面有挑战。</li>
<li>DeepDubbing系统包括Text-to-Timbre模型和Context-Aware Instruct-TTS模型，可实现多参与者有声书的自动化生成。</li>
<li>Text-to-Timbre模型根据文本描述生成角色特定的音色嵌入。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15845">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bc8dafef98971565ae9adc22e95dab18~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684226&auth_key=1760684226-0-0-3c5be6963670fab506f19a4966856a8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9e921cf4dc3ef153927978d6dcaca319~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684233&auth_key=1760684233-0-0-484d9dd3c2ac6559eb8ae50a40998f28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-702380b2f46364b0769482b714ff400a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684240&auth_key=1760684240-0-0-3d397144f9551f52f06c21901ce6ebf6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-82d6a5148fa18eeebc5840c8018e8128~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684247&auth_key=1760684247-0-0-55116c7c3b6e62e4945de60779f83b47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d2fc2d216d00d09c4c42c42190ac8516~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684253&auth_key=1760684253-0-0-70dc4583118cc80404215bbaa8eecbcd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Tiny-is-not-small-enough-High-quality-low-resource-facial-animation-models-through-hybrid-knowledge-distillation"><a href="#Tiny-is-not-small-enough-High-quality-low-resource-facial-animation-models-through-hybrid-knowledge-distillation" class="headerlink" title="Tiny is not small enough: High-quality, low-resource facial animation   models through hybrid knowledge distillation"></a>Tiny is not small enough: High-quality, low-resource facial animation   models through hybrid knowledge distillation</h2><p><strong>Authors:Zhen Han, Mattias Teye, Derek Yadgaroff, Judith Bütepage</strong></p>
<p>The training of high-quality, robust machine learning models for speech-driven 3D facial animation requires a large, diverse dataset of high-quality audio-animation pairs. To overcome the lack of such a dataset, recent work has introduced large pre-trained speech encoders that are robust to variations in the input audio and, therefore, enable the facial animation model to generalize across speakers, audio quality, and languages. However, the resulting facial animation models are prohibitively large and lend themselves only to offline inference on a dedicated machine. In this work, we explore on-device, real-time facial animation models in the context of game development. We overcome the lack of large datasets by using hybrid knowledge distillation with pseudo-labeling. Given a large audio dataset, we employ a high-performing teacher model to train very small student models. In contrast to the pre-trained speech encoders, our student models only consist of convolutional and fully-connected layers, removing the need for attention context or recurrent updates. In our experiments, we demonstrate that we can reduce the memory footprint to up to 3.4 MB and required future audio context to up to 81 ms while maintaining high-quality animations. This paves the way for on-device inference, an important step towards realistic, model-driven digital characters. </p>
<blockquote>
<p>对于高质量、稳健的机器学习模型进行语音驱动的3D面部动画训练，需要高质量音频动画对的大规模、多样化的数据集。为了克服缺乏这样的数据集，近期的工作引入了大型预训练语音编码器，它对输入音频的变化具有鲁棒性，因此，使得面部动画模型能够在发言人、音频质量和语言方面实现泛化。然而，由此产生的面部动画模型过于庞大，只能在专用机器上进行离线推理。在这项工作中，我们在游戏开发的背景下探索了设备上的实时面部动画模型。通过使用混合知识蒸馏和伪标签技术，我们克服了缺乏大规模数据集的难题。给定一个大型音频数据集，我们利用高性能的教师模型来训练非常小的的学生模型。与预训练的语音编码器不同，我们的学生模型只包含卷积层和全连接层，无需注意力上下文或递归更新。在我们的实验中，我们证明可以将内存占用减少到最多3.4MB，并将未来音频上下文的需求减少到最多81毫秒，同时保持高质量的动画效果。这为设备端推理铺平了道路，是朝着逼真、模型驱动的数字角色迈进的重要一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18352v2">PDF</a> Accepted to ACM TOG 2025 (SIGGRAPH journal track); Project page:   <a target="_blank" rel="noopener" href="https://electronicarts.github.io/tiny-voice2face/">https://electronicarts.github.io/tiny-voice2face/</a></p>
<p><strong>Summary</strong></p>
<p>本文研究了面向游戏开发的实时面部动画模型。为解决高质量音频动画对大型数据集的需求，采用混合知识蒸馏与伪标签技术，利用高性能教师模型训练小型学生模型。学生模型仅包含卷积层和全连接层，无需注意力上下文或递归更新，降低了内存占用并维持了高质量动画。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高质量、稳健的机器学习任务驱动3D面部动画需要大型、多样化的高质量音频动画数据集。</li>
<li>近期工作引入大型预训练语音编码器，对输入音频的变异具有鲁棒性，使面部动画模型能够跨说话人、音频质量和语言进行推广。</li>
<li>现有面部动画模型通常过于庞大，仅限于离线推理，不适用于游戏开发等场景。</li>
<li>提出使用混合知识蒸馏与伪标签技术，解决缺乏大型数据集的问题。</li>
<li>利用高性能教师模型训练小型学生模型，学生模型仅包含卷积和完全连接层。</li>
<li>实验表明，在维持高质量动画的同时，能减少内存占用高达3.4MB，并将未来音频上下文需求减少到81ms。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18352">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a488ddfa0737851e3b84abd16a1488ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684262&auth_key=1760684262-0-0-d1cfb61fe4312ea0442f88f8c9fd0a25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3773e9beda8873cf0d8c68a5f4e2f250~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684270&auth_key=1760684270-0-0-8e2c564801c8a9f8c282b284561d0723&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed3fe0ca6e849b2b78f9bd8b305180d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684277&auth_key=1760684277-0-0-3ec5b5c15fab0b44cc1759c19203c7bd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1be340f59a9634cc7cb9e84496bb6c44~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684284&auth_key=1760684284-0-0-fe20f29e99546f30630cb522d6296112&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Revisiting-Speech-Lip-Alignment-A-Phoneme-Aware-Speech-Encoder-for-Robust-Talking-Head-Synthesis"><a href="#Revisiting-Speech-Lip-Alignment-A-Phoneme-Aware-Speech-Encoder-for-Robust-Talking-Head-Synthesis" class="headerlink" title="Revisiting Speech-Lip Alignment: A Phoneme-Aware Speech Encoder for   Robust Talking Head Synthesis"></a>Revisiting Speech-Lip Alignment: A Phoneme-Aware Speech Encoder for   Robust Talking Head Synthesis</h2><p><strong>Authors:Yihuan Huang, Jiajun Liu, Yanzhen Ren, Wuyang Liu, Zongkun Sun</strong></p>
<p>Speech-driven talking head synthesis tasks commonly use general acoustic features as guided speech features. However, we discovered that these features suffer from phoneme-viseme alignment ambiguity, which refers to the uncertainty and imprecision in matching phonemes with visemes. To overcome this limitation, we propose a phoneme-aware speech encoder (PASE) that explicitly enforces accurate phoneme-viseme correspondence. PASE first captures fine-grained speech and visual features, then introduces a prediction-reconstruction task to improve robustness under noise and modality absence. Furthermore, a phoneme-level alignment module guided by phoneme embeddings and contrastive learning ensures discriminative audio and visual alignment. Experimental results show that PASE achieves state-of-the-art performance in both NeRF and 3DGS rendering models. Its lip sync accuracy improves by 13.7% and 14.2% compared to the acoustic feature, producing results close to the ground truth videos. </p>
<blockquote>
<p>语音驱动说话人头部合成任务通常使用通用声学特征作为引导语音特征。然而，我们发现这些特征存在音素-面部动画参数对齐模糊的问题，即音素与面部动画参数匹配时存在不确定性和不精确性。为了克服这一局限性，我们提出了一种音素感知语音编码器（PASE），它显式地强制执行准确的音素-面部动画参数对应关系。PASE首先捕获精细的语音和视觉特征，然后引入预测重建任务，以提高噪声和模态缺失情况下的稳健性。此外，一个由音素嵌入和对比学习引导的音素级对齐模块确保了区分性的音频和视觉对齐。实验结果表明，PASE在NeRF和3DGS渲染模型中都达到了最新性能。与声学特征相比，其唇同步精度提高了13.7%和14.2%，生成的结果接近真实视频。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05803v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>该文介绍了语音驱动的头部合成任务中常见的以通用声学特征作为引导语音特征的问题。研究发现，由于语音与视觉之间的音素-姿态对应模糊，即音素与姿态匹配的不确定性和不精确性，这种方法存在局限性。为了克服这一问题，提出了一种音素感知语音编码器（PASE），它能准确地对应音素和姿态。PASE首先捕捉精细的语音和视觉特征，然后引入预测重建任务，以提高噪声和模态缺失下的稳健性。此外，通过音素嵌入和对比学习引导的音素级对齐模块确保了音频和视觉对齐的鉴别性。实验结果表明，PASE在NeRF和3DGS渲染模型上均达到了最新技术水平，唇同步精度分别提高了13.7%和14.2%，与声学特征相比，生成的结果更接近真实视频。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>当前语音驱动头部合成任务依赖于通用声学特征作为引导语音特征。</li>
<li>存在音素与姿态匹配的不确定性和不精确性问题。</li>
<li>提出了一种音素感知语音编码器（PASE）来准确对应音素和姿态。</li>
<li>PASE能捕捉精细的语音和视觉特征。</li>
<li>预测重建任务提高了噪声和模态缺失下的稳健性。</li>
<li>音素级对齐模块通过音素嵌入和对比学习增强了音频和视觉对齐的鉴别性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fd77ef9769c5e4c199cb986c416bc8c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684292&auth_key=1760684292-0-0-bd955328389e77046f9d0e456538dd28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ef509e3cf7f109302bdea3a1caa09141~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684299&auth_key=1760684299-0-0-3d0c9715434f07b6f76e6ed4b9418bf6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9c9459803817f097638172f2585854d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684306&auth_key=1760684306-0-0-d7dcca016e757f4161531331383b55f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9bf48199e1f393ea598bd58bc29caed3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684312&auth_key=1760684312-0-0-ca294420f46e590c32ff681ae2ccab5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d0f52e712ded4568efbc615bf817ced~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684319&auth_key=1760684319-0-0-be2b059b4eb5ed86f7458d58625cf900&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait"><a href="#FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait" class="headerlink" title="FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait"></a>FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait</h2><p><strong>Authors:Taekyung Ki, Dongchan Min, Gyeongsu Chae</strong></p>
<p>With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. Instead of a pixel-based latent space, we take advantage of a learned orthogonal motion latent space, enabling efficient generation and editing of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with an effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency. </p>
<blockquote>
<p>随着基于扩散的生成模型的快速发展，肖像图像动画已经取得了显著成果。然而，由于其迭代采样的性质，它在时序一致的视频生成和快速采样方面仍然面临挑战。本文提出了FLOAT，这是一种基于流匹配生成模型的音频驱动式说话肖像视频生成方法。我们利用学习到的正交运动潜在空间，而不是基于像素的潜在空间，实现了高效且时序一致的运动生成和编辑。为此，我们引入了一个基于变压器的矢量场预测器，并设计了一个有效的帧条件机制。此外，我们的方法支持语音驱动的情感增强，能够实现表达性运动的自然融合。大量实验表明，我们的方法在视觉质量、运动保真度和效率方面优于最先进的音频驱动式说话肖像方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01064v5">PDF</a> ICCV 2025. Project page:   <a target="_blank" rel="noopener" href="https://deepbrainai-research.github.io/float/">https://deepbrainai-research.github.io/float/</a></p>
<p><strong>Summary</strong></p>
<p>随着扩散生成模型的快速发展，肖像图像动画已经取得了显著成果。然而，它仍面临因迭代采样产生的视频生成时间一致性和快速采样挑战。本文提出了基于流匹配生成模型的音频驱动肖像视频生成方法FLOAT。我们利用学习到的正交运动潜在空间，而非基于像素的潜在空间，实现了高效的时间一致运动生成和编辑。为此，我们引入基于变换器的向量场预测器，并配备了有效的帧条件机制。此外，我们的方法支持语音驱动的情感增强，能够自然地融入表达性动作。实验证明，我们的方法在视觉质量、运动保真度和效率方面超越了现有音频驱动的肖像动画方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FLOAT是一种基于流匹配生成模型的音频驱动肖像视频生成方法。</li>
<li>利用正交运动潜在空间，实现高效的时间一致运动生成和编辑。</li>
<li>引入基于变换器的向量场预测器，配备有效的帧条件机制。</li>
<li>支持语音驱动的情感增强，融入表达性动作。</li>
<li>方法在视觉质量、运动保真度和效率方面超越现有音频驱动的肖像动画方法。</li>
<li>面对挑战：迭代采样带来的时间一致视频生成和快速采样问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01064">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-31e85e1d61a000eda461b783ae612dd7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684327&auth_key=1760684327-0-0-75a0da99dd916202792af1339b5fa5f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16292bb7e83296bca7145627a7591bdc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684336&auth_key=1760684336-0-0-27f99246ddc57643d0a9a94e64ea481c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1bf23e979ccafd415927201321dc284~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684343&auth_key=1760684343-0-0-02c7c7e7518500ebb6c9d1613feb440c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e664574875172447d0fa281156a519d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760684350&auth_key=1760684350-0-0-3636e5ebc5d1b50751422576e9c62ad4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-585eeb01e1b5036bc615aaedb3aea715~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138220&auth_key=1760138220-0-0-d288918137d258a6fddff352b51887e8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-24  UniPixel Unified Object Referring and Segmentation for Pixel-Level   Visual Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-5eb7fc7256b42dcd3bfff81dba874c60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760138127&auth_key=1760138127-0-0-f5ae0a650a4f59d3cbad86dd74e8abc4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-21  DICE Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
