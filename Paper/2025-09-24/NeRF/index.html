<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2025-09-24  From Restoration to Reconstruction Rethinking 3D Gaussian Splatting for   Underwater Scenes">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15123v2/page_3_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-24-更新"><a href="#2025-09-24-更新" class="headerlink" title="2025-09-24 更新"></a>2025-09-24 更新</h1><h2 id="From-Restoration-to-Reconstruction-Rethinking-3D-Gaussian-Splatting-for-Underwater-Scenes"><a href="#From-Restoration-to-Reconstruction-Rethinking-3D-Gaussian-Splatting-for-Underwater-Scenes" class="headerlink" title="From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for   Underwater Scenes"></a>From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for   Underwater Scenes</h2><p><strong>Authors:Guoxi Huang, Haoran Wang, Zipeng Qi, Wenjun Lu, David Bull, Nantheera Anantrasirichai</strong></p>
<p>Underwater image degradation poses significant challenges for 3D reconstruction, where simplified physical models often fail in complex scenes. We propose \textbf{R-Splatting}, a unified framework that bridges underwater image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both rendering quality and geometric fidelity. Our method integrates multiple enhanced views produced by diverse UIR models into a single reconstruction pipeline. During inference, a lightweight illumination generator samples latent codes to support diverse yet coherent renderings, while a contrastive loss ensures disentangled and stable illumination representations. Furthermore, we propose \textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models opacity as a stochastic function to regularize training. This suppresses abrupt gradient responses triggered by illumination variation and mitigates overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong baselines in both rendering quality and geometric accuracy. </p>
<blockquote>
<p>水下图像退化给3D重建带来了重大挑战，复杂的场景中简单的物理模型往往无法胜任。我们提出了<strong>R-Splatting</strong>框架，它将水下图像恢复（UIR）与3D高斯拼贴（3DGS）相结合，以提高渲染质量和几何保真度。我们的方法将多种由不同UIR模型生成的增强视图集成到单个重建流程中。在推理过程中，轻量级照明生成器对潜在代码进行采样，以支持多样且连贯的渲染，而对比损失则确保去耦和稳定的照明表示。此外，我们提出了<em>不确定性感知不透明度优化（UAOO）</em>，它将不透明度建模为随机函数以规范训练。这抑制了由照明变化触发的突然梯度响应，并减轻了对噪声或特定视图伪影的过拟合。在Seathru-NeRF和我们新的BlueCoral3D数据集上的实验表明，R-Splatting在渲染质量和几何准确性方面均优于强大的基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17789v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为R-Splatting的统一框架，它将水下图像恢复（UIR）与三维高斯拼贴（3DGS）相结合，旨在解决水下图像退化对三维重建带来的挑战。通过整合多种由不同UIR模型生成的水下图像增强视图，提高渲染质量和几何保真度。同时，采用轻量级照明生成器支持多样且连贯的渲染，并通过对比损失确保照明表示的去纠缠和稳定性。此外，还提出了不确定性感知不透明度优化（UAOO），将不透明度建模为随机函数以进行训练正则化，抑制由照明变化触发的突然梯度响应，并减轻对噪声或特定视图伪影的过拟合。在Seathru-NeRF和新的BlueCoral3D数据集上的实验表明，R-Splatting在渲染质量和几何精度方面优于强大的基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出R-Splatting框架，整合水下图像恢复（UIR）与三维高斯拼贴（3DGS），应对水下图像退化对三维重建的挑战。</li>
<li>通过多种UIR模型生成增强视图，并整合到单一重建流程中，提升渲染质量与几何保真度。</li>
<li>采用轻量级照明生成器支持多样且连贯的渲染，对比损失确保照明表示的稳定性与去纠缠。</li>
<li>引入不确定性感知不透明度优化（UAOO），将不透明度建模为随机函数，进行训练正则化，抑制突然梯度响应，减轻对噪声或特定视图伪影的过拟合。</li>
<li>R-Splatting框架在Seathru-NeRF和BlueCoral3D数据集上的实验表现优异，优于现有基线方法。</li>
<li>R-Splatting能提高渲染质量和几何精度。</li>
<li>框架具有处理复杂场景下水下图像退化的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17789">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17789v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17789v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17789v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17789v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17789v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GAN-Based-Multi-Microphone-Spatial-Target-Speaker-Extraction"><a href="#GAN-Based-Multi-Microphone-Spatial-Target-Speaker-Extraction" class="headerlink" title="GAN-Based Multi-Microphone Spatial Target Speaker Extraction"></a>GAN-Based Multi-Microphone Spatial Target Speaker Extraction</h2><p><strong>Authors:Shrishti Saha Shetu, Emanuël A. P. Habets, Andreas Brendel</strong></p>
<p>Spatial target speaker extraction isolates a desired speaker’s voice in multi-speaker environments using spatial information, such as the direction of arrival (DoA). Although recent deep neural network (DNN)-based discriminative methods have shown significant performance improvements, the potential of generative approaches, such as generative adversarial networks (GANs), remains largely unexplored for this problem. In this work, we demonstrate that a GAN can effectively leverage both noisy mixtures and spatial information to extract and generate the target speaker’s speech. By conditioning the GAN on intermediate features of a discriminative spatial filtering model in addition to DoA, we enable steerable target extraction with high spatial resolution of 5 degrees, outperforming state-of-the-art discriminative methods in perceptual quality-based objective metrics. </p>
<blockquote>
<p>空间目标说话人提取技术利用空间信息，如到达方向（DoA），在多说话人环境中分离出目标说话人的声音。尽管基于深度神经网络（DNN）的判别方法最近取得了显著的性能改进，但生成方法（如生成对抗网络（GAN））的潜力在很大程度上仍未被探索用于解决这个问题。在这项工作中，我们证明了GAN可以有效地利用噪声混合和空间信息来提取和生成目标说话人的语音。除了DoA之外，通过对GAN进行条件化设置，使其依赖于判别性空间滤波模型的中间特征，我们实现了具有高达5度的高空间分辨率的可控目标提取，在基于感知质量的客观度量方面优于最先进的判别方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17741v1">PDF</a> </p>
<p><strong>Summary</strong><br>空间目标说话人提取利用空间信息，如到达方向（DoA），在多说话人环境中分离出目标说话人的声音。虽然基于深度神经网络（DNN）的判别方法已取得了显著的性能改进，但生成式方法，如生成对抗网络（GANs）在此问题上的潜力尚未得到充分探索。本研究展示了GAN如何利用嘈杂的混合和空度信息有效地提取并生成目标说话人的语音。通过在GAN上增加基于判别性空间滤波模型的中间特征以及DoA的条件，我们实现了具有高达5度的高空间分辨率的可控目标提取，在基于感知质量的客观度量上超越了最先进的判别方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>空间目标说话人提取利用空间信息如到达方向（DoA）来分离多说话环境中的目标说话人的声音。</li>
<li>虽然判别方法已经取得了显著进展，但生成对抗网络（GANs）在此问题上的潜力尚未被充分探索。</li>
<li>GAN可以有效地利用嘈杂的混合和空度信息来提取并生成目标说话人的语音。</li>
<li>通过在GAN上增加基于判别性空间滤波模型的中间特征的条件，提高了性能。</li>
<li>结合DoA信息，实现了具有高达5度的高空间分辨率的可控目标提取。</li>
<li>在基于感知质量的客观度量上，所提出的方法超越了现有的最先进的判别方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17741">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17741v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17741v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17741v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17741v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DT-NeRF-A-Diffusion-and-Transformer-Based-Optimization-Approach-for-Neural-Radiance-Fields-in-3D-Reconstruction"><a href="#DT-NeRF-A-Diffusion-and-Transformer-Based-Optimization-Approach-for-Neural-Radiance-Fields-in-3D-Reconstruction" class="headerlink" title="DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for   Neural Radiance Fields in 3D Reconstruction"></a>DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for   Neural Radiance Fields in 3D Reconstruction</h2><p><strong>Authors:Bo Liu, Runlong Li, Li Zhou, Yan Zhou</strong></p>
<p>This paper proposes a Diffusion Model-Optimized Neural Radiance Field (DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency in 3D scene reconstruction. By combining diffusion models with Transformers, DT-NeRF effectively restores details under sparse viewpoints and maintains high accuracy in complex geometric scenes. Experimental results demonstrate that DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art methods on the Matterport3D and ShapeNet datasets, particularly in metrics such as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further confirm the critical role of the diffusion and Transformer modules in the model’s performance, with the removal of either module leading to a decline in performance. The design of DT-NeRF showcases the synergistic effect between modules, providing an efficient and accurate solution for 3D scene reconstruction. Future research may focus on further optimizing the model, exploring more advanced generative models and network architectures to enhance its performance in large-scale dynamic scenes. </p>
<blockquote>
<p>本文提出了一种优化扩散模型的神经辐射场（DT-NeRF）方法，旨在提高三维场景重建中的细节恢复和多视图一致性。通过将扩散模型与Transformer相结合，DT-NeRF在稀疏视角下有效地恢复了细节，并在复杂几何场景中保持了高精度。实验结果表明，DT-NeRF在Matterport3D和ShapeNet数据集上显著优于传统NeRF和其他最新方法，特别是在PSNR、SSIM、Chamfer距离和保真度等指标上。消融实验进一步证实了扩散和Transformer模块对模型性能的关键作用，移除任何一个模块都会导致性能下降。DT-NeRF的设计展示了模块之间的协同作用，为三维场景重建提供了高效且准确的解决方案。未来的研究可能会集中在进一步优化模型，探索更先进的生成模型和网络架构，以提高其在大型动态场景中的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17232v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为DT-NeRF的扩散模型优化神经辐射场方法，旨在提高三维场景重建中的细节恢复和多视角一致性。结合扩散模型和Transformer，DT-NeRF在稀疏视角下能有效恢复细节，并在复杂几何场景中保持高精度。实验结果在Matterport3D和ShapeNet数据集上表明，DT-NeRF较传统NeRF和其他先进方法有明显优势，特别是在PSNR、SSIM、Chamfer距离和保真度等指标上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DT-NeRF结合了扩散模型和Transformer，优化了神经辐射场方法。</li>
<li>该方法旨在提高三维场景重建中的细节恢复和多视角一致性。</li>
<li>实验结果显示DT-NeRF在多个数据集上较传统方法性能更优。</li>
<li>DT-NeRF在稀疏视角下能有效恢复细节，并在复杂几何场景中保持高精度。</li>
<li>消融实验证实了扩散模型和Transformer模块的关键作用。</li>
<li>DT-NeRF的设计展示了模块间的协同作用，为三维场景重建提供了高效准确的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17232">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17232v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17232v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17232v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17232v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis"><a href="#HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis" class="headerlink" title="HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel   View Synthesis"></a>HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel   View Synthesis</h2><p><strong>Authors:Zipeng Wang, Dan Xu</strong></p>
<p>Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at <a target="_blank" rel="noopener" href="https://wzpscott.github.io/hyrf/">https://wzpscott.github.io/hyrf/</a>. </p>
<blockquote>
<p>最近，3D高斯采样（3DGS）作为一种强大的NeRF替代方法出现，它通过明确的、可优化的3D高斯实现实时高质量的新视角合成。然而，由于3DGS依赖于高斯参数来模拟视角相关的效果和各向异性形状，它存在较大的内存开销。尽管最近的研究提出了用神经网络场来压缩3DGS，但这些方法在捕捉高斯属性的高频空间变化方面存在困难，导致精细细节的重建退化。我们提出了混合辐射场（HyRF），这是一种结合显式高斯和神经网络场优点的新型场景表示方法。HyRF将场景分解为（1）一组紧凑的显式高斯，只存储关键的高频参数；（2）基于网格的神经网络场，用于预测其余属性。为了提高表示能力，我们引入了一个解耦的神经网络场架构，分别建模几何（尺度、不透明度、旋转）和视角相关的颜色。此外，我们提出了一种混合渲染方案，将高斯采样与神经网络场预测的背景进行合成，解决了远距离场景表示的局限性。实验表明，HyRF达到了最先进的渲染质量，与3DGS相比，模型大小减少了20倍以上，同时保持了实时性能。我们的项目页面可在[<a target="_blank" rel="noopener" href="https://wzpscott.github.io/hyrf/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://wzpscott.github.io/hyrf/]上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17083v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了Hybrid Radiance Fields（HyRF）这一新型场景表示方法，结合了显式高斯和神经网络的优势。HyRF将场景分解为两组元素：关键的显式高斯用于存储高频参数，以及基于网格的神经网络用于预测其余属性。同时引入了解耦神经网络架构和混合渲染方案，以提升模型表现力和渲染质量。实验证明，HyRF达到了最先进的渲染质量，同时实现了与3DGS相比超过20倍的模型大小缩减，并保持实时性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D Gaussian Splatting（3DGS）虽然可以实现实时高质量的新型视图合成，但存在内存开销大的问题。</li>
<li>Hybrid Radiance Fields（HyRF）结合了显式高斯和神经网络的优势，旨在解决3DGS的问题。</li>
<li>HyRF将场景分解为关键的显式高斯和其他基于网格的神经网络预测属性。</li>
<li>解耦神经网络架构被引入以分别建模几何和视觉依赖颜色，增强了模型的代表性。</li>
<li>提出了一种混合渲染方案，通过结合高斯和神经网络预测的背景，解决了远距离场景表示的限制。</li>
<li>实验证明，HyRF在保持实时性能的同时，实现了高质量的渲染，并显著减小了模型大小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17083">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17083v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17083v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17083v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.17083v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PGSTalker-Real-Time-Audio-Driven-Talking-Head-Generation-via-3D-Gaussian-Splatting-with-Pixel-Aware-Density-Control"><a href="#PGSTalker-Real-Time-Audio-Driven-Talking-Head-Generation-via-3D-Gaussian-Splatting-with-Pixel-Aware-Density-Control" class="headerlink" title="PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D   Gaussian Splatting with Pixel-Aware Density Control"></a>PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D   Gaussian Splatting with Pixel-Aware Density Control</h2><p><strong>Authors:Tianheng Zhu, Yinfeng Yu, Liejun Wang, Fuchun Sun, Wendong Zheng</strong></p>
<p>Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment. </p>
<blockquote>
<p>音频驱动的说话人头部生成对于虚拟现实、数字化身和电影制作等应用至关重要。尽管基于NeRF的方法能够实现高保真重建，但它们存在渲染效率低下和视听同步不佳的问题。本研究提出了基于3D高斯拼贴（3DGS）的实时音频驱动说话人头部合成框架PGSTalker。为了提高渲染性能，我们提出了一种像素感知密度控制策略，该策略能够自适应地分配点密度，从而在动态面部区域增强细节，同时减少其他区域的冗余。此外，我们还引入了一个轻量级的多模式门控融合模块，以有效地融合音频和空间特征，从而提高高斯变形预测的准确性。在公开数据集上的大量实验表明，PGSTalker在渲染质量、唇同步精度和推理速度方面优于现有的基于NeRF和3DGS的方法。我们的方法表现出强大的泛化能力和实际部署的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16922v1">PDF</a> Main paper (15 pages). Accepted for publication by ICONIP(   International Conference on Neural Information Processing) 2025</p>
<p><strong>Summary</strong><br>PGSTalker是一个基于实时音频驱动的说话人头部合成框架，采用三维高斯喷绘技术（3DGS）。为提高渲染性能，提出像素感知密度控制策略，自适应分配点密度，同时引入轻量级多模态门融合模块，有效融合音频和空间特征，提高高斯变形预测的准确性。在公共数据集上的实验表明，PGSTalker在渲染质量、唇同步精度和推理速度方面优于现有的NeRF和3DGS方法，具有强大的泛化能力和实际应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PGSTalker是一个音频驱动的说话人头部生成框架，基于三维高斯喷绘技术（3DGS）。</li>
<li>框架中存在渲染性能问题，因此提出像素感知密度控制策略以提高性能。</li>
<li>引入轻量级多模态门融合模块来融合音频和空间特征，提高变形预测准确性。</li>
<li>在公共数据集上的实验显示PGSTalker在渲染质量、唇同步精度和推理速度方面的优越性。</li>
<li>该方法具有较强的泛化能力，可用于各种虚拟角色、电影制作等应用。</li>
<li>该技术具有实际应用潜力，可为虚拟现实、数字化身等领域带来革新。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16922">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.16922v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.16922v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="QWD-GAN-Quality-aware-Wavelet-driven-GAN-for-Unsupervised-Medical-Microscopy-Images-Denoising"><a href="#QWD-GAN-Quality-aware-Wavelet-driven-GAN-for-Unsupervised-Medical-Microscopy-Images-Denoising" class="headerlink" title="QWD-GAN: Quality-aware Wavelet-driven GAN for Unsupervised Medical   Microscopy Images Denoising"></a>QWD-GAN: Quality-aware Wavelet-driven GAN for Unsupervised Medical   Microscopy Images Denoising</h2><p><strong>Authors:Qijun Yang, Yating Huang, Lintao Xiang, Hujun Yin</strong></p>
<p>Image denoising plays a critical role in biomedical and microscopy imaging, especially when acquiring wide-field fluorescence-stained images. This task faces challenges in multiple fronts, including limitations in image acquisition conditions, complex noise types, algorithm adaptability, and clinical application demands. Although many deep learning-based denoising techniques have demonstrated promising results, further improvements are needed in preserving image details, enhancing algorithmic efficiency, and increasing clinical interpretability. We propose an unsupervised image denoising method based on a Generative Adversarial Network (GAN) architecture. The approach introduces a multi-scale adaptive generator based on the Wavelet Transform and a dual-branch discriminator that integrates difference perception feature maps with original features. Experimental results on multiple biomedical microscopy image datasets show that the proposed model achieves state-of-the-art denoising performance, particularly excelling in the preservation of high-frequency information. Furthermore, the dual-branch discriminator is seamlessly compatible with various GAN frameworks. The proposed quality-aware, wavelet-driven GAN denoising model is termed as QWD-GAN. </p>
<blockquote>
<p>图像去噪在生物医学和显微镜成像中扮演着至关重要的角色，尤其是在获取宽场荧光染色图像时。此任务面临着多方面的挑战，包括图像采集条件的限制、复杂的噪声类型、算法适应性和临床应用需求。尽管许多基于深度学习的去噪技术在细节保留、算法效率提升和临床可解释性方面表现出巨大潜力，但仍需进一步改进。我们提出了一种基于生成对抗网络（GAN）架构的无监督图像去噪方法。该方法引入了一种基于小波变换的多尺度自适应生成器，以及一个集成了差异感知特征映射和原始特征的双重分支鉴别器。在多个生物医学显微镜图像数据集上的实验结果表明，该模型取得了最先进的去噪性能，尤其在高频信息保护方面表现出色。此外，双重分支鉴别器能够无缝兼容各种GAN框架。所提出的具有质量感知和小波驱动的去噪GAN模型被称为QWD-GAN。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15814v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于生成对抗网络（GAN）架构的QWD-GAN模型，提出一种无监督图像去噪方法。该方法引入基于小波变换的多尺度自适应生成器与结合差异感知特征图的双分支鉴别器。实验证明，该模型在生物医学显微镜图像数据集上去噪性能达到先进水平，特别擅长保留高频信息，且与各种GAN框架兼容。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>去噪在生物医学和显微镜成像中至关重要，特别是在获取宽场荧光染色图像时。</li>
<li>图像去噪面临多重挑战，包括图像采集条件限制、复杂噪声类型、算法适应性和临床应用需求。</li>
<li>尽管深度学习去噪技术已展现潜力，但仍需改进以保留图像细节、提高算法效率和临床可解释性。</li>
<li>提出了一种基于生成对抗网络（GAN）架构的无监督图像去噪方法。</li>
<li>该方法引入多尺度自适应生成器与双分支鉴别器，结合差异感知特征图。</li>
<li>实验证明该模型在多个生物医学显微镜图像数据集上达到先进去噪性能，尤其擅长保留高频信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15814">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15814v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15814v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15814v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MS-GS-Multi-Appearance-Sparse-View-3D-Gaussian-Splatting-in-the-Wild"><a href="#MS-GS-Multi-Appearance-Sparse-View-3D-Gaussian-Splatting-in-the-Wild" class="headerlink" title="MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild"></a>MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild</h2><p><strong>Authors:Deming Li, Kaiwen Jiang, Yutao Tang, Ravi Ramamoorthi, Rama Chellappa, Cheng Peng</strong></p>
<p>In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis. Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting. In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS. To address the lack of support due to sparse initializations, our approach is built on the geometric priors elicited from monocular depth estimations. The key lies in extracting and utilizing local semantic regions with a Structure-from-Motion (SfM) points anchored algorithm for reliable alignment and geometry cues. Then, to introduce multi-view constraints, we propose a series of geometry-guided supervision at virtual views in a fine-grained and coarse scheme to encourage 3D consistency and reduce overfitting. We also introduce a dataset and an in-the-wild experiment setting to set up more realistic benchmarks. We demonstrate that MS-GS achieves photorealistic renderings under various challenging sparse-view and multi-appearance conditions and outperforms existing approaches significantly across different datasets. </p>
<blockquote>
<p>野外照片集通常包含有限的图像数量，并且呈现多种外观，例如不同的时间或季节拍摄的照片，给场景重建和新颖视图合成带来重大挑战。尽管最近的神经辐射场（NeRF）和三维高斯溅出（3DGS）的改进已经在这一领域有所提升，但它们往往过于平滑并且容易过度拟合。在本文中，我们提出了MS-GS，这是一个在稀疏视图场景中利用3DGS的多外观能力设计的新型框架。为了解决由于稀疏初始化而导致的支持不足的问题，我们的方法建立在从单目深度估计中引发的几何先验之上。关键在于提取和利用局部语义区域，采用结构从运动（SfM）点锚定算法实现可靠的对齐和几何线索。然后，为了引入多视图约束，我们提出了一种在精细粒度和粗略方案中在虚拟视图中进行几何指导的监督的一系列方法，以鼓励三维一致性并减少过度拟合。我们还引入了一个数据集和野外实验设置，以建立更现实的基准测试。我们证明，MS-GS在各种具有挑战性的稀疏视图和多外观条件下实现了逼真的渲染，并在不同数据集上显著优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15548v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了在野外照片采集场景中的重建与合成问题，特别是在不同时间和季节下的多外观挑战。针对现有NeRF和3DGS方法存在的局限性和问题，提出一个结合多外观能力、应用于稀疏视角下的全新框架MS-GS。该框架利用单目深度估计的几何先验信息，通过结构从运动（SfM）点锚定算法提取并利用局部语义区域，实现可靠对齐和几何线索。此外，通过引入多视角约束和精细粒度的几何引导监督，减少了过度拟合，提高了3D一致性。实验证明，MS-GS在多种具有挑战性的稀疏视角和多外观条件下实现了逼真的渲染效果，并在不同数据集上显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在野外照片采集场景中存在多个外观挑战，需要解决场景重建和新型视角合成的问题。</li>
<li>现有NeRF和3DGS方法存在过度平滑和过度拟合的问题。</li>
<li>提出一种全新的框架MS-GS，该框架结合多外观能力并应用于稀疏视角场景中使用3DGS。</li>
<li>MS-GS利用单目深度估计的几何先验信息，并通过结构从运动（SfM）点锚定算法实现可靠对齐和几何线索的提取和利用。</li>
<li>通过引入多视角约束和精细粒度的几何引导监督，提高3D一致性并减少过度拟合。</li>
<li>MS-GS在多种挑战性的稀疏视角和多外观条件下实现了逼真的渲染效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15548">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15548v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15548v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15548v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15548v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes"><a href="#RGB-Only-Supervised-Camera-Parameter-Optimization-in-Dynamic-Scenes" class="headerlink" title="RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes"></a>RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes</h2><p><strong>Authors:Fang Li, Hao Zhang, Narendra Ahuja</strong></p>
<p>Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video, dubbed ROS-Cam. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision. </p>
<blockquote>
<p>尽管COLMAP长期以来一直是静态场景摄像机参数优化的主要方法，但它受到运行时间长和依赖于真实运动遮罩应用于动态场景的制约。许多尝试通过引入更多先验知识进行监督来改善它，如真实焦距、运动遮罩、3D点云、相机姿态和度量深度，然而，这些通常在随意捕获的RGB视频中并不可用。在本文中，我们提出了一种新的方法，仅通过单个RGB视频进行监督，实现动态场景中更准确高效的摄像机参数优化，称为ROS-Cam。我们的方法由三个关键组件组成：（1）斑块追踪滤波器，用于在RGB视频上建立稳健且尽可能稀疏的铰接关系。（2）异常值感知联合优化，通过自适应降低移动异常值的权重，实现高效的摄像机参数优化，无需依赖运动先验。（3）两阶段优化策略，通过在损失中的Softplus限制和凸极小值之间进行权衡，以提高稳定性和优化速度。我们通过视觉和数值评估我们的摄像机估计。为了进一步提高准确性，我们将摄像机估计值输入到4D重建方法，并评估所得的3D场景、渲染的2D RGB和深度图。我们在4个真实世界数据集（NeRF-DS、DAVIS、iPhone和TUM-dynamics）和1个合成数据集（MPI-Sintel）上进行了实验，结果表明，我们的方法使用单个RGB视频作为唯一的监督，能更高效、更准确地估计摄像机参数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15123v2">PDF</a> NeurIPS 2025 Spotlight</p>
<p><strong>摘要</strong></p>
<p>尽管COLMAP长期以来一直是静态场景摄像机参数优化的主要方法，但其受到运行时间长和对地面真实运动掩膜的依赖的制约，难以应用于动态场景。本文提出了一种仅通过单目RGB视频实现更准确高效的摄像机参数优化方法，称为ROS-Cam。该方法包括三个关键组件：基于块追踪滤波器建立稳健且尽可能稀疏的枢纽关系；采用异常值感知联合优化，自适应减轻运动异常值影响，无需依赖运动先验，实现摄像机参数高效优化；采用两阶段优化策略，通过损失函数的Softplus极限和凸极小值之间的权衡提高稳定性和优化速度。我们通过视觉和数值评估了摄像机参数估计的准确性。为进一步验证准确性，将估算的摄像机参数输入到四维重建方法中进行三维场景重建，并评估生成的二维RGB图和深度图。在4个真实数据集（NeRF-DS、DAVIS、iPhone和TUM-dynamics）和1个合成数据集（MPI-Sintel）上进行的实验表明，我们的方法使用单一RGB视频作为监督，能更精准高效地估算摄像机参数。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>COLMAP虽然在静态场景摄像机参数优化中占主导地位，但在处理动态场景时存在运行时间长和依赖地面真实运动掩膜的制约。</li>
<li>提出的ROS-Cam方法通过结合三个关键组件：块追踪滤波器、异常值感知联合优化以及两阶段优化策略，实现了更准确且高效的摄像机参数优化。</li>
<li>ROS-Cam方法在单一RGB视频的监督下工作，无需额外的先验信息，如地面真实焦距、运动掩膜、三维点云、摄像机姿态和深度度量等。</li>
<li>通过视觉和数值评估了摄像机参数估计的准确性，并通过四维重建方法验证了结果的准确性。</li>
<li>在多个真实和合成数据集上的实验表明，ROS-Cam方法比传统方法更精准高效。</li>
<li>块追踪滤波器有助于建立稳健且稀疏的枢纽关系，从而提高优化效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15123">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15123v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15123v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15123v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15123v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15123v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Revisiting-Speech-Lip-Alignment-A-Phoneme-Aware-Speech-Encoder-for-Robust-Talking-Head-Synthesis"><a href="#Revisiting-Speech-Lip-Alignment-A-Phoneme-Aware-Speech-Encoder-for-Robust-Talking-Head-Synthesis" class="headerlink" title="Revisiting Speech-Lip Alignment: A Phoneme-Aware Speech Encoder for   Robust Talking Head Synthesis"></a>Revisiting Speech-Lip Alignment: A Phoneme-Aware Speech Encoder for   Robust Talking Head Synthesis</h2><p><strong>Authors:Yihuan Huang, Jiajun Liu, Yanzhen Ren, Wuyang Liu, Zongkun Sun</strong></p>
<p>Speech-driven talking head synthesis tasks commonly use general acoustic features as guided speech features. However, we discovered that these features suffer from phoneme-viseme alignment ambiguity, which refers to the uncertainty and imprecision in matching phonemes with visemes. To overcome this limitation, we propose a phoneme-aware speech encoder (PASE) that explicitly enforces accurate phoneme-viseme correspondence. PASE first captures fine-grained speech and visual features, then introduces a prediction-reconstruction task to improve robustness under noise and modality absence. Furthermore, a phoneme-level alignment module guided by phoneme embeddings and contrastive learning ensures discriminative audio and visual alignment. Experimental results show that PASE achieves state-of-the-art performance in both NeRF and 3DGS rendering models. Its lip sync accuracy improves by 13.7% and 14.2% compared to the acoustic feature, producing results close to the ground truth videos. </p>
<blockquote>
<p>语音驱动的人头合成任务通常使用一般声学特征作为引导语音特征。然而，我们发现这些特征存在音素-面部动作对齐模糊的问题，即音素与面部动作匹配的不确定性和不精确性。为了克服这一局限性，我们提出了一种音素感知语音编码器（PASE），它显式地强制准确的音素-面部动作对应关系。PASE首先捕捉精细的语音和视觉特征，然后引入预测重建任务，以提高噪声和模态缺失下的稳健性。此外，一个由音素嵌入和对比学习引导的音素级对齐模块确保了具有鉴别力的音频和视觉对齐。实验结果表明，PASE在NeRF和3DGS渲染模型中均达到了最先进的性能。与声学特征相比，其唇同步精度提高了13.7%和14.2%，产生的结果接近真实视频。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05803v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音驱动的人头合成任务中普遍使用的一般声学特征作为引导语音特征，但存在音素-面部动态匹配模糊的问题。为解决此问题，提出了音素感知语音编码器（PASE），能明确实现音素与面部动态的准确对应。PASE首先捕捉精细的语音和视觉特征，然后通过预测-重建任务提高在噪声和模态缺失下的稳健性。此外，以音素嵌入和对比学习引导的音素级别对齐模块确保了音频和视觉的区分对齐。实验结果显示，PASE在NeRF和3DGS渲染模型中达到最佳性能，与声学特征相比，唇同步精度分别提高了13.7%和14.2%，结果接近真实视频。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音驱动的人头合成任务普遍使用一般声学特征，但存在音素-面部动态匹配模糊的问题。</li>
<li>为解决音素-面部动态匹配模糊问题，提出了音素感知语音编码器（PASE）。</li>
<li>PASE可以捕捉精细的语音和视觉特征。</li>
<li>PASE通过预测-重建任务提高在噪声和模态缺失下的稳健性。</li>
<li>PASE通过音素嵌入和对比学习实现音频和视觉的区分对齐。</li>
<li>实验结果显示PASE在NeRF和3DGS渲染模型中表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2504.05803v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2504.05803v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2504.05803v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2504.05803v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2504.05803v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16986v1/page_1_0.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-09-24  Seg4Diff Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/3DGS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_3DGS/2509.11003v2/page_5_1.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-09-24  GeoSVR Taming Sparse Voxels for Geometrically Accurate Surface   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
