<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-3c12af6654f2baf1c3434fa8fd3e9d9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683503&auth_key=1760683503-0-0-28c2af450822dfb9448918206b3438ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    46 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-24-æ›´æ–°"><a href="#2025-09-24-æ›´æ–°" class="headerlink" title="2025-09-24 æ›´æ–°"></a>2025-09-24 æ›´æ–°</h1><h2 id="Degradation-Aware-All-in-One-Image-Restoration-via-Latent-Prior-Encoding"><a href="#Degradation-Aware-All-in-One-Image-Restoration-via-Latent-Prior-Encoding" class="headerlink" title="Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding"></a>Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding</h2><p><strong>Authors:S M A Sharif, Abdur Rehman, Fayaz Ali Dharejo, Radu Timofte, Rizwan Ali Naqvi</strong></p>
<p>Real-world images often suffer from spatially diverse degradations such as haze, rain, snow, and low-light, significantly impacting visual quality and downstream vision tasks. Existing all-in-one restoration (AIR) approaches either depend on external text prompts or embed hand-crafted architectural priors (e.g., frequency heuristics); both impose discrete, brittle assumptions that weaken generalization to unseen or mixed degradations. To address this limitation, we propose to reframe AIR as learned latent prior inference, where degradation-aware representations are automatically inferred from the input without explicit task cues. Based on latent priors, we formulate AIR as a structured reasoning paradigm: (1) which features to route (adaptive feature selection), (2) where to restore (spatial localization), and (3) what to restore (degradation semantics). We design a lightweight decoding module that efficiently leverages these latent encoded cues for spatially-adaptive restoration. Extensive experiments across six common degradation tasks, five compound settings, and previously unseen degradations demonstrate that our method outperforms state-of-the-art (SOTA) approaches, achieving an average PSNR improvement of 1.68 dB while being three times more efficient. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä¸­çš„å›¾åƒç»å¸¸å—åˆ°ç©ºé—´å¤šæ ·åŒ–çš„é€€åŒ–å½±å“ï¼Œå¦‚é›¾éœ¾ã€é›¨æ°´ã€é›ªå’Œæš—å…‰ï¼Œè¿™ä¼šå¯¹å›¾åƒè´¨é‡å’Œä¸‹æ¸¸è§†è§‰ä»»åŠ¡äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚ç°æœ‰çš„å…¨èƒ½æ¢å¤ï¼ˆAIRï¼‰æ–¹æ³•è¦ä¹ˆä¾èµ–äºå¤–éƒ¨æ–‡æœ¬æç¤ºï¼Œè¦ä¹ˆåµŒå…¥æ‰‹å·¥æ„å»ºçš„ç»“æ„å…ˆéªŒï¼ˆå¦‚é¢‘ç‡å¯å‘å¼ï¼‰ï¼›ä¸¤è€…éƒ½æ–½åŠ ç¦»æ•£ã€è„†å¼±çš„å‡è®¾ï¼Œå‰Šå¼±äº†å…¶å¯¹æœªè§æˆ–æ··åˆé€€åŒ–çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºå°†AIRé‡æ–°æ„å»ºä¸ºå­¦ä¹ æ½œåœ¨å…ˆéªŒæ¨ç†ï¼Œå…¶ä¸­é€€åŒ–æ„ŸçŸ¥è¡¨ç¤ºè‡ªåŠ¨ä»è¾“å…¥ä¸­æ¨æ–­å‡ºæ¥ï¼Œæ— éœ€æ˜ç¡®çš„ä»»åŠ¡æç¤ºã€‚åŸºäºæ½œåœ¨å…ˆéªŒï¼Œæˆ‘ä»¬å°†AIRåˆ¶å®šä¸ºç»“æ„åŒ–æ¨ç†æ¨¡å¼ï¼šï¼ˆ1ï¼‰é€‰æ‹©å“ªäº›ç‰¹å¾è¿›è¡Œè·¯ç”±ï¼ˆè‡ªé€‚åº”ç‰¹å¾é€‰æ‹©ï¼‰ã€ï¼ˆ2ï¼‰åœ¨å“ªé‡Œæ¢å¤ï¼ˆç©ºé—´å®šä½ï¼‰å’Œï¼ˆ3ï¼‰æ¢å¤ä»€ä¹ˆï¼ˆé€€åŒ–è¯­ä¹‰ï¼‰ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„è§£ç æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿé«˜æ•ˆåˆ©ç”¨è¿™äº›æ½œåœ¨ç¼–ç çº¿ç´¢è¿›è¡Œç©ºé—´è‡ªé€‚åº”æ¢å¤ã€‚åœ¨å…­ä¸ªå¸¸è§çš„é€€åŒ–ä»»åŠ¡ã€äº”ä¸ªå¤åˆåœºæ™¯å’Œä¹‹å‰æœªè§çš„é€€åŒ–æƒ…å†µä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ï¼Œå¹³å‡PSNRå€¼æé«˜äº†1.68åˆ†è´ï¼ŒåŒæ—¶æ•ˆç‡æé«˜äº†ä¸‰å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17792v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç©ºé—´å¤šæ ·é€€åŒ–å¦‚é›¾éœ¾ã€é›¨æ°´ã€é›ªå’Œæš—å…‰ç­‰ç°å®é—®é¢˜å¯¹å›¾åƒè´¨é‡å’Œè§†è§‰ä»»åŠ¡äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚ç°æœ‰çš„ä¸€ç«™å¼æ¢å¤æ–¹æ³•ä¾èµ–å¤–éƒ¨æ–‡æœ¬æç¤ºæˆ–æ‰‹å·¥æ¶æ„å…ˆéªŒï¼Œå­˜åœ¨é€šç”¨æ€§å¼±çš„ç¼ºç‚¹ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡æ½œåœ¨å…ˆéªŒæ¨ç†æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè‡ªåŠ¨ä»è¾“å…¥ä¸­æ¨æ–­é€€åŒ–æ„ŸçŸ¥è¡¨ç¤ºï¼Œå°†å…¶åˆ¶å®šä¸ºç»“æ„åŒ–æ¨ç†æ¨¡å¼ï¼ŒåŒ…æ‹¬ç‰¹å¾è·¯ç”±ã€æ¢å¤ä½ç½®å’Œé€€åŒ–è¯­ä¹‰ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„è§£ç æ¨¡å—ï¼Œèƒ½å¤Ÿé«˜æ•ˆåˆ©ç”¨è¿™äº›æ½œåœ¨çº¿ç´¢è¿›è¡Œç©ºé—´è‡ªé€‚åº”æ¢å¤ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…­ç§å¸¸è§é€€åŒ–ä»»åŠ¡ã€äº”ç§å¤åˆè®¾ç½®å’ŒæœªçŸ¥é€€åŒ–ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹³å‡PSNRæé«˜äº†1.68dBï¼Œä¸”æ•ˆç‡æé«˜äº†ä¸‰å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°å®é—®é¢˜ä¸­çš„å›¾åƒç»å¸¸é¢ä¸´å¤šç§ç©ºé—´é€€åŒ–æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰çš„ä¸€ç«™å¼å›¾åƒæ¢å¤æ–¹æ³•å­˜åœ¨é€šç”¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºé€šè¿‡æ½œåœ¨å…ˆéªŒæ¨ç†æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè‡ªåŠ¨ä»è¾“å…¥ä¸­æ¨æ–­é€€åŒ–æ„ŸçŸ¥è¡¨ç¤ºã€‚</li>
<li>å°†å›¾åƒæ¢å¤åˆ¶å®šä¸ºç»“æ„åŒ–æ¨ç†æ¨¡å¼ï¼ŒåŒ…æ‹¬ç‰¹å¾è·¯ç”±ã€æ¢å¤ä½ç½®å’Œé€€åŒ–è¯­ä¹‰ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„è§£ç æ¨¡å—ï¼Œåˆ©ç”¨æ½œåœ¨çº¿ç´¢è¿›è¡Œç©ºé—´è‡ªé€‚åº”æ¢å¤ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§é€€åŒ–ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æé«˜æ¢å¤è´¨é‡çš„åŒæ—¶ï¼Œä¹Ÿæé«˜äº†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0ae2e82c633939e2846978fc719fdec9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424741&auth_key=1760424741-0-0-8264620e7efd158e5ff8bf1ec7c7aea0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95b81c2f9325b33de9c575d4d6160325~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424748&auth_key=1760424748-0-0-a3b5b6143a0f15d4bc39fa2a3cf06ba7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85cece4f199e59ebb2409e55bd91c95f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424755&auth_key=1760424755-0-0-616ad50cc8a859399b1b5ecdf1dc80f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dual-View-Alignment-Learning-with-Hierarchical-Prompt-for-Class-Imbalance-Multi-Label-Classification"><a href="#Dual-View-Alignment-Learning-with-Hierarchical-Prompt-for-Class-Imbalance-Multi-Label-Classification" class="headerlink" title="Dual-View Alignment Learning with Hierarchical-Prompt for   Class-Imbalance Multi-Label Classification"></a>Dual-View Alignment Learning with Hierarchical-Prompt for   Class-Imbalance Multi-Label Classification</h2><p><strong>Authors:Sheng Huang, Jiexuan Yan, Beiyan Liu, Bo Liu, Richang Hong</strong></p>
<p>Real-world datasets often exhibit class imbalance across multiple categories, manifesting as long-tailed distributions and few-shot scenarios. This is especially challenging in Class-Imbalanced Multi-Label Image Classification (CI-MLIC) tasks, where data imbalance and multi-object recognition present significant obstacles. To address these challenges, we propose a novel method termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which leverages multi-modal knowledge from vision-language pretrained (VLP) models to mitigate the class-imbalance problem in multi-label settings. Specifically, HP-DVAL employs dual-view alignment learning to transfer the powerful feature representation capabilities from VLP models by extracting complementary features for accurate image-text alignment. To better adapt VLP models for CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes global and local prompts to learn task-specific and context-related prior knowledge. Additionally, we design a semantic consistency loss during prompt tuning to prevent learned prompts from deviating from general knowledge embedded in VLP models. The effectiveness of our approach is validated on two CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results demonstrate the superiority of our method over SOTA approaches, achieving mAP improvements of 10.0% and 5.2% on the long-tailed multi-label image classification task, and 6.8% and 2.9% on the multi-label few-shot image classification task. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œçš„æ•°æ®é›†é€šå¸¸åœ¨å¤šä¸ªç±»åˆ«ä¹‹é—´è¡¨ç°å‡ºç±»åˆ«ä¸å¹³è¡¡ï¼Œè¡¨ç°ä¸ºé•¿å°¾åˆ†å¸ƒå’Œå°‘é‡æ ·æœ¬åœºæ™¯ã€‚è¿™åœ¨ç±»åˆ«ä¸å¹³è¡¡çš„å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ï¼ˆCI-MLICï¼‰ä»»åŠ¡ä¸­å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå…¶ä¸­æ•°æ®ä¸å¹³è¡¡å’Œå¤šç›®æ ‡è¯†åˆ«æ„æˆäº†é‡å¤§éšœç¢ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºå±‚æ¬¡æç¤ºçš„åŒè§†å›¾å¯¹é½å­¦ä¹ ï¼ˆHP-DVALï¼‰ï¼Œå®ƒåˆ©ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æ¨¡å‹çš„å¤šæ¨¡æ€çŸ¥è¯†æ¥ç¼“è§£å¤šæ ‡ç­¾è®¾ç½®ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒHP-DVALé‡‡ç”¨åŒè§†å›¾å¯¹é½å­¦ä¹ ï¼Œé€šè¿‡æå–å›¾åƒå’Œæ–‡æœ¬å¯¹é½çš„äº’è¡¥ç‰¹å¾ï¼Œä»VLPæ¨¡å‹è½¬ç§»å¼ºå¤§çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚ä¸ºäº†æ›´å¥½åœ°é€‚åº”CI-MLICä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å±‚æ¬¡åŒ–çš„æç¤ºè°ƒæ•´ç­–ç•¥ï¼Œåˆ©ç”¨å…¨å±€å’Œå±€éƒ¨æç¤ºæ¥å­¦ä¹ ç‰¹å®šä»»åŠ¡å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æç¤ºè°ƒæ•´è¿‡ç¨‹ä¸­è®¾è®¡äº†ä¸€ç§è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥é˜²æ­¢å­¦ä¹ åˆ°çš„æç¤ºåç¦»VLPæ¨¡å‹ä¸­çš„é€šç”¨çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨MS-COCOå’ŒVOC2007ä¸¤ä¸ªCI-MLICåŸºå‡†æµ‹è¯•ä¸Šçš„æœ‰æ•ˆæ€§å¾—åˆ°äº†éªŒè¯ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨é•¿å°¾å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šæé«˜äº†10.0%å’Œ5.2%çš„mAPï¼Œåœ¨å¤šæ ‡ç­¾å°‘é‡æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šæé«˜äº†6.8%å’Œ2.9%çš„mAPã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17747v1">PDF</a> accepted by IEEE Transactions on Image Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ï¼ˆCI-MLICï¼‰ä»»åŠ¡çš„æ–°çš„æŒ‘æˆ˜ï¼Œä»‹ç»äº†ä¸€ç§åä¸ºHP-DVALçš„åŒè§†å›¾å¯¹é½å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æ¨¡å‹çš„å¤šæ¨¡æ€çŸ¥è¯†ï¼Œé€šè¿‡åŒè§†å›¾å¯¹é½å­¦ä¹ æå–äº’è¡¥ç‰¹å¾ï¼Œå®ç°å›¾åƒä¸æ–‡æœ¬å‡†ç¡®å¯¹é½ã€‚ä¸ºé€‚åº”CI-MLICä»»åŠ¡ï¼Œå¼•å…¥å±‚æ¬¡åŒ–æç¤ºè°ƒæ•´ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨å…¨å±€å’Œå±€éƒ¨æç¤ºå­¦ä¹ ç‰¹å®šä»»åŠ¡å’Œä¸Šä¸‹æ–‡ç›¸å…³çŸ¥è¯†ã€‚åœ¨MS-COCOå’ŒVOC2007ä¸¤ä¸ªCI-MLICåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨é•¿å°¾å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šæé«˜äº†mAPå€¼10.0%å’Œ5.2%ï¼Œåœ¨å¤šæ ‡ç­¾å°æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šæé«˜äº†mAPå€¼6.8%å’Œ2.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç±»åˆ«ä¸å¹³è¡¡å’Œå¤šç›®æ ‡è¯†åˆ«åœ¨CI-MLICä»»åŠ¡ä¸­æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†HP-DVALåŒè§†å›¾å¯¹é½å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>HP-DVALé€šè¿‡åŒè§†å›¾å¯¹é½å­¦ä¹ å®ç°å›¾åƒä¸æ–‡æœ¬å‡†ç¡®å¯¹é½ï¼Œæå–äº’è¡¥ç‰¹å¾ã€‚</li>
<li>å¼•å…¥å±‚æ¬¡åŒ–æç¤ºè°ƒæ•´ç­–ç•¥ï¼Œåˆ©ç”¨å…¨å±€å’Œå±€éƒ¨æç¤ºå­¦ä¹ ç‰¹å®šä»»åŠ¡å’Œä¸Šä¸‹æ–‡ç›¸å…³çŸ¥è¯†ã€‚</li>
<li>è®¾è®¡äº†è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ï¼Œé˜²æ­¢å­¦ä¹ åˆ°çš„æç¤ºåç¦»é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„é€šç”¨çŸ¥è¯†ã€‚</li>
<li>åœ¨MS-COCOå’ŒVOC2007åŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9c268144593345a945c8035686eb0f32~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424762&auth_key=1760424762-0-0-fa365e390039775ade3900a5e92e38bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-db4b1a44c8612ffca7cad4e4dd594d70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683510&auth_key=1760683510-0-0-f3b3874b093ad253ec6d8039e37e5f78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8ddc625a6cbc42293c27a849cac472d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683517&auth_key=1760683517-0-0-232b27f0b5ebb9b1a41117a2b4685f4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Visual-Instruction-Pretraining-for-Domain-Specific-Foundation-Models"><a href="#Visual-Instruction-Pretraining-for-Domain-Specific-Foundation-Models" class="headerlink" title="Visual Instruction Pretraining for Domain-Specific Foundation Models"></a>Visual Instruction Pretraining for Domain-Specific Foundation Models</h2><p><strong>Authors:Yuxuan Li, Yicheng Zhang, Wenhao Tang, Yimian Dai, Ming-Ming Cheng, Xiang Li, Jian Yang</strong></p>
<p>Modern computer vision is converging on a closed loop in which perception, reasoning and generation mutually reinforce each other. However, this loop remains incomplete: the top-down influence of high-level reasoning on the foundational learning of low-level perceptual features is not yet underexplored. This paper addresses this gap by proposing a new paradigm for pretraining foundation models in downstream domains. We introduce Visual insTruction Pretraining (ViTP), a novel approach that directly leverages reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT) backbone within a Vision-Language Model and pretrains it end-to-end using a rich corpus of visual instruction data curated from target downstream domains. ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels the ViT to learn robust and domain-relevant features from a sparse set of visual tokens. Extensive experiments on 16 challenging remote sensing and medical imaging benchmarks demonstrate that ViTP establishes new state-of-the-art performance across a diverse range of downstream tasks. The code is available at github.com&#x2F;zcablii&#x2F;ViTP. </p>
<blockquote>
<p>ç°ä»£è®¡ç®—æœºè§†è§‰æ­£åœ¨å½¢æˆä¸€ä¸ªé—­ç¯ï¼Œæ„ŸçŸ¥ã€æ¨ç†å’Œç”Ÿæˆåœ¨å…¶ä¸­ç›¸äº’åŠ å¼ºã€‚ç„¶è€Œï¼Œè¿™ä¸ªå¾ªç¯ä»ç„¶ä¸å®Œæ•´ï¼šé«˜çº§æ¨ç†å¯¹ä½çº§æ„ŸçŸ¥ç‰¹å¾åŸºç¡€å­¦ä¹ çš„è‡ªä¸Šè€Œä¸‹å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡ä¸ºä¸‹æ¸¸é¢†åŸŸçš„åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒæå‡ºä¸€ç§æ–°çš„èŒƒå¼æ¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬å¼•å…¥äº†è§†è§‰æŒ‡ä»¤é¢„è®­ç»ƒï¼ˆViTPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ¨ç†æ¥ç›´æ¥å¢å¼ºæ„ŸçŸ¥çš„æ–°å‹æ–¹æ³•ã€‚ViTPå°†è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸»å¹²åµŒå…¥åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¹¶ä½¿ç”¨ä»ç›®æ ‡ä¸‹æ¸¸é¢†åŸŸç²¾å¿ƒæŒ‘é€‰çš„ä¸°å¯Œè§†è§‰æŒ‡ä»¤æ•°æ®å¯¹å…¶è¿›è¡Œç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒã€‚ViTPç”±æˆ‘ä»¬æå‡ºçš„è§†è§‰ç¨³å¥æ€§å­¦ä¹ ï¼ˆVRLï¼‰èµ‹èƒ½ï¼Œå®ƒä¿ƒä½¿ViTä»å°‘é‡çš„è§†è§‰æ ‡è®°ä¸­å­¦ä¹ ç¨³å¥å’Œä¸é¢†åŸŸç›¸å…³çš„ç‰¹å¾ã€‚åœ¨16ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é¥æ„ŸåŒ»ç–—æˆåƒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒViTPåœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨github.com&#x2F;zcablii&#x2F;ViTPæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17562v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰Transformeråœ¨ä¸‹æ¸¸é¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹çš„æ–°èŒƒå¼ã€‚æå‡ºViTPï¼ˆè§†è§‰æŒ‡ä»¤é¢„è®­ç»ƒï¼‰æ–¹æ³•ï¼Œç›´æ¥åˆ©ç”¨æ¨ç†å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡ViTï¼ˆè§†è§‰Transformerï¼‰éª¨å¹²ç½‘åµŒå…¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ä»ç›®æ ‡ä¸‹æ¸¸é¢†åŸŸæ”¶é›†çš„å¤§é‡è§†è§‰æŒ‡ä»¤æ•°æ®è¿›è¡Œç«¯åˆ°ç«¯çš„é¢„è®­ç»ƒã€‚é€šè¿‡VRLï¼ˆè§†è§‰ç¨³å¥å­¦ä¹ ï¼‰ä¿ƒä½¿ViTä»ç¨€ç–çš„è§†è§‰ä»¤ç‰Œé›†ä¸­å­¦ä¹ ç¨³å¥ä¸”ä¸é¢†åŸŸç›¸å…³çš„ç‰¹å¾ã€‚åœ¨è¿œç¨‹æ„Ÿåº”å’ŒåŒ»å­¦å½±åƒç­‰é¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ä»£ç å·²ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°ä»£è®¡ç®—æœºè§†è§‰æ­£åœ¨å½¢æˆä¸€ä¸ªæ„ŸçŸ¥ã€æ¨ç†å’Œç”Ÿæˆç›¸äº’åŠ å¼ºçš„é—­ç¯ï¼Œä½†é«˜çº§æ¨ç†å¯¹åŸºç¡€æ„ŸçŸ¥ç‰¹å¾å­¦ä¹ çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºViTPæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥æ¨ç†æ¥å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ï¼Œå¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚</li>
<li>ViTPåˆ©ç”¨è§†è§‰Transformerï¼ˆViTï¼‰éª¨å¹²ç½‘åµŒå…¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè¿›è¡Œä¸‹æ¸¸é¢†åŸŸçš„é¢„è®­ç»ƒã€‚</li>
<li>ViTPä½¿ç”¨ä¸°å¯Œçš„è§†è§‰æŒ‡ä»¤æ•°æ®æ¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™äº›æ•°æ®ä»ç›®æ ‡ä¸‹æ¸¸é¢†åŸŸæ”¶é›†ã€‚</li>
<li>å¼•å…¥çš„VRLæ–¹æ³•ä¿ƒä½¿ViTä»ç¨€ç–çš„è§†è§‰ä»¤ç‰Œä¸­å­¦ä¹ ç¨³å¥ä¸”ä¸é¢†åŸŸç›¸å…³çš„ç‰¹å¾ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒViTPè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿œç¨‹æ„Ÿåº”å’ŒåŒ»å­¦å½±åƒé¢†åŸŸã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2cba936a5b30df661558e8468c4df58b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683525&auth_key=1760683525-0-0-07848290c05c79e434bafe271b72748a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0f8fb84e1d1a220fb86a6017fbf75d01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683533&auth_key=1760683533-0-0-08e6a9e9dffe655c2560036691cd6549&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad796353ce0469810ae106e7cb95d820~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683540&auth_key=1760683540-0-0-5e701337784dfa0480ad34163b0d0f89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0a6a5ba992320873c1c330e6f41cc43~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683547&auth_key=1760683547-0-0-799aebaee1c73c2157a4c8f83de3db97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Informative-Text-Image-Alignment-for-Visual-Affordance-Learning-with-Foundation-Models"><a href="#Informative-Text-Image-Alignment-for-Visual-Affordance-Learning-with-Foundation-Models" class="headerlink" title="Informative Text-Image Alignment for Visual Affordance Learning with   Foundation Models"></a>Informative Text-Image Alignment for Visual Affordance Learning with   Foundation Models</h2><p><strong>Authors:Qian Zhang, Lin Zhang, Xing Fang, Mingxin Zhang, Zhiyuan Wei, Ran Song, Wei Zhang</strong></p>
<p>Visual affordance learning is crucial for robots to understand and interact effectively with the physical world. Recent advances in this field attempt to leverage pre-trained knowledge of vision-language foundation models to learn affordance properties with limited training data, providing a novel paradigm for visual affordance learning. However, these methods overlook the significance of maintaining feature alignment between visual images and language descriptions for identifying affordance areas with textual guidance, and thus may lead to suboptimal results. In this paper, we present an informative framework for text-guided affordance learning, which involves information-based constraints to achieve text-image alignment at feature level. Specifically, we design an affordance mutual information constraint that helps learn appropriate textual prompts and task-oriented visual features simultaneously by maximizing the mutual information between the features of the affordance areas in the input images and the corresponding textual prompts. In addition, we propose an object-level information constraint that maximizes the mutual information between the visual features of a given object and the text features of the category it belongs to. This enables the model to capture high-quality representations for the object, providing more reliable semantic priors for identifying affordance regions. Experimental results on the AGD20K dataset show that the proposed method outperforms existing approaches and achieves the new state-of-the-art in one-shot affordance learning. </p>
<blockquote>
<p>è§†è§‰åŠ¨ä½œå­¦ä¹ å¯¹äºæœºå™¨äººæœ‰æ•ˆåœ°ç†è§£å’Œä¸ç‰©ç†ä¸–ç•Œäº¤äº’è‡³å…³é‡è¦ã€‚è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•è¯•å›¾åˆ©ç”¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒçŸ¥è¯†æ¥å­¦ä¹ åŠ¨ä½œå±æ€§ï¼Œä»¥æœ‰é™çš„è®­ç»ƒæ•°æ®è¿›è¡ŒåŠ¨ä½œå±æ€§çš„å­¦ä¹ ï¼Œä¸ºè§†è§‰åŠ¨ä½œå­¦ä¹ æä¾›äº†æ–°é¢–çš„æ¨¡å¼ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¿½è§†äº†åœ¨æ–‡æœ¬æŒ‡å¯¼ä¸‹ç»´æŒè§†è§‰å›¾åƒå’Œè¯­è¨€æè¿°ä¹‹é—´ç‰¹å¾å¯¹é½çš„é‡è¦æ€§ï¼Œä»è€Œå¯èƒ½æ— æ³•æœ‰æ•ˆåœ°è¯†åˆ«åŠ¨ä½œåŒºåŸŸå¹¶å¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¿¡æ¯ä¸°å¯Œçš„æ–‡æœ¬å¼•å¯¼åŠ¨ä½œå­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¶‰åŠåŸºäºä¿¡æ¯çš„çº¦æŸæ¥å®ç°æ–‡æœ¬å›¾åƒåœ¨ç‰¹å¾å±‚é¢çš„å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠ¨ä½œäº’ä¿¡æ¯çº¦æŸï¼Œé€šè¿‡æœ€å¤§åŒ–è¾“å…¥å›¾åƒä¸­çš„åŠ¨ä½œåŒºåŸŸç‰¹å¾ä¸ç›¸åº”çš„æ–‡æœ¬æç¤ºä¹‹é—´çš„äº’ä¿¡æ¯ï¼ŒåŒæ—¶å­¦ä¹ é€‚å½“çš„æ–‡æœ¬æç¤ºå’Œä»»åŠ¡å¯¼å‘çš„è§†è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯¹è±¡çº§åˆ«çš„ä¿¡æ¯çº¦æŸï¼Œé€šè¿‡æœ€å¤§åŒ–ç»™å®šå¯¹è±¡çš„è§†è§‰ç‰¹å¾ä¸å…¶æ‰€å±ç±»åˆ«çš„æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¯¹è±¡çš„é«˜è´¨é‡è¡¨ç¤ºï¼Œä¸ºè¯†åˆ«åŠ¨ä½œåŒºåŸŸæä¾›æ›´å¯é çš„è¯­ä¹‰å…ˆéªŒã€‚åœ¨AGD20Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•å¹¶å®ç°äº†å•æ¬¡åŠ¨ä½œå­¦ä¹ çš„æ–°æŠ€æœ¯çŠ¶æ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17074v1">PDF</a> Submitted to the IEEE International Conference on Robotics and   Automation (ICRA) 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬æŒ‡å¯¼çš„è§†è§‰è¡¨ç°å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ä¿¡æ¯çº¦æŸåœ¨ç‰¹å¾å±‚é¢å®ç°æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½ã€‚è®¾è®¡äº†ä¸€ç§è¡¨ç°äº’æƒ ä¿¡æ¯çº¦æŸï¼Œé€šè¿‡æœ€å¤§åŒ–è¾“å…¥å›¾åƒä¸­çš„è¡¨ç°åŒºåŸŸç‰¹å¾ä¸ç›¸åº”æ–‡æœ¬æç¤ºä¹‹é—´çš„äº’æƒ ä¿¡æ¯ï¼ŒåŒæ—¶å­¦ä¹ é€‚å½“çš„æ–‡æœ¬æç¤ºå’Œä»»åŠ¡å¯¼å‘çš„è§†è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¯¹è±¡çº§åˆ«çš„ä¿¡æ¯çº¦æŸï¼Œèƒ½å¤Ÿæœ€å¤§åŒ–ç»™å®šå¯¹è±¡çš„è§†è§‰ç‰¹å¾ä¸å…¶æ‰€å±ç±»åˆ«æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„äº’æƒ ä¿¡æ¯ï¼Œä¸ºè¯†åˆ«è¡¨ç°åŒºåŸŸæä¾›å¯é çš„è¯­ä¹‰å…ˆéªŒã€‚åœ¨AGD20Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ä¸€ç«™å¼è¡¨ç°å­¦ä¹ ä¸­è¾¾åˆ°æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¡¨ç°å­¦ä¹ å¯¹äºæœºå™¨äººç†è§£å’Œæœ‰æ•ˆä¸ç‰©ç†ä¸–ç•Œäº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶å°è¯•åˆ©ç”¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒçŸ¥è¯†æ¥å­¦ä¹ è¡¨ç°å±æ€§ï¼Œä»¥å‡å°‘è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†åœ¨è¯†åˆ«è¡¨ç°åŒºåŸŸæ—¶ï¼Œä¿æŒè§†è§‰å›¾åƒå’Œè¯­è¨€æè¿°ä¹‹é—´ç‰¹å¾å¯¹é½çš„é‡è¦æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬æŒ‡å¯¼çš„è§†è§‰è¡¨ç°å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä¿¡æ¯çº¦æŸå®ç°æ–‡æœ¬ä¸å›¾åƒåœ¨ç‰¹å¾å±‚é¢çš„å¯¹é½ã€‚</li>
<li>å¼•å…¥è¡¨ç°äº’æƒ ä¿¡æ¯çº¦æŸï¼Œé€šè¿‡æœ€å¤§åŒ–è¡¨ç°åŒºåŸŸç‰¹å¾ä¸æ–‡æœ¬æç¤ºä¹‹é—´çš„äº’æƒ ä¿¡æ¯ï¼ŒåŒæ—¶å­¦ä¹ æ–‡æœ¬æç¤ºä¸è§†è§‰ç‰¹å¾ã€‚</li>
<li>æå‡ºå¯¹è±¡çº§åˆ«çš„ä¿¡æ¯çº¦æŸï¼Œèƒ½å¤Ÿæ•æ‰å¯¹è±¡çš„é«˜è´¨é‡è¡¨ç¤ºï¼Œä¸ºè¯†åˆ«è¡¨ç°åŒºåŸŸæä¾›æ›´å¯é çš„è¯­ä¹‰å…ˆéªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-92384243d2fdb7df4d4aa6dadd3a15cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683555&auth_key=1760683555-0-0-d4b77605fe15eafdcef885f92f2e77d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5680f4802b4796c86131b66435c0a4b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683562&auth_key=1760683562-0-0-8470a629de70fa17a8e4c20a50d6a059&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b148aa33b464873db1a7615f85f81b41~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683569&auth_key=1760683569-0-0-0f62745df092015c4d9ac0f5813e8375&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2c0e21d1bf75b4e55d97518e184e3fb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683575&auth_key=1760683575-0-0-82078bc079b86d559f0c4bbe73807c14&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dd21d284673a682d51588391ecfa231e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683582&auth_key=1760683582-0-0-a371a7c87079d1d7d06baf9a3297f0d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a69b26f19868dc69828076115f24f7d9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683589&auth_key=1760683589-0-0-667a5e4f0a68cbfa472fcb4a583f7338&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="When-Color-Space-Decoupling-Meets-Diffusion-for-Adverse-Weather-Image-Restoration"><a href="#When-Color-Space-Decoupling-Meets-Diffusion-for-Adverse-Weather-Image-Restoration" class="headerlink" title="When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image   Restoration"></a>When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image   Restoration</h2><p><strong>Authors:Wenxuan Fang, Jili Fan, Chao Wang, Xiantao Hu, Jiangwei Weng, Ying Tai, Jian Yang, Jun Li</strong></p>
<p>Adverse Weather Image Restoration (AWIR) is a highly challenging task due to the unpredictable and dynamic nature of weather-related degradations. Traditional task-specific methods often fail to generalize to unseen or complex degradation types, while recent prompt-learning approaches depend heavily on the degradation estimation capabilities of vision-language models, resulting in inconsistent restorations. In this paper, we propose \textbf{LCDiff}, a novel framework comprising two key components: \textit{Lumina-Chroma Decomposition Network} (LCDN) and \textit{Lumina-Guided Diffusion Model} (LGDM). LCDN processes degraded images in the YCbCr color space, separately handling degradation-related luminance and degradation-invariant chrominance components. This decomposition effectively mitigates weather-induced degradation while preserving color fidelity. To further enhance restoration quality, LGDM leverages degradation-related luminance information as a guiding condition, eliminating the need for explicit degradation prompts. Additionally, LGDM incorporates a \textit{Dynamic Time Step Loss} to optimize the denoising network, ensuring a balanced recovery of both low- and high-frequency features in the image. Finally, we present DriveWeather, a comprehensive all-weather driving dataset designed to enable robust evaluation. Extensive experiments demonstrate that our approach surpasses state-of-the-art methods, setting a new benchmark in AWIR. The dataset and code are available at: <a target="_blank" rel="noopener" href="https://github.com/fiwy0527/LCDiff">https://github.com/fiwy0527/LCDiff</a>. </p>
<blockquote>
<p>æ¶åŠ£å¤©æ°”å›¾åƒæ¢å¤ï¼ˆAWIRï¼‰æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå¤©æ°”ç›¸å…³çš„é€€åŒ–æ˜¯ä¸å¯é¢„æµ‹å’ŒåŠ¨æ€çš„ã€‚ä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ–¹æ³•å¾€å¾€ä¸èƒ½æ¨å¹¿åˆ°æœªè§è¿‡çš„æˆ–å¤æ‚çš„é€€åŒ–ç±»å‹ï¼Œè€Œæœ€è¿‘çš„æç¤ºå­¦ä¹ çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºè§†è§‰è¯­è¨€æ¨¡å‹çš„é€€åŒ–ä¼°è®¡èƒ½åŠ›ï¼Œå¯¼è‡´æ¢å¤ç»“æœä¸ä¸€è‡´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶LCDiffï¼Œå®ƒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šäº®åº¦-è‰²åº¦åˆ†è§£ç½‘ç»œï¼ˆLCDNï¼‰å’Œäº®åº¦å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆLGDMï¼‰ã€‚LCDNåœ¨YCbCré¢œè‰²ç©ºé—´å¤„ç†é€€åŒ–å›¾åƒï¼Œåˆ†åˆ«å¤„ç†ä¸é€€åŒ–ç›¸å…³çš„äº®åº¦å’Œé€€åŒ–ä¸å˜çš„è‰²åº¦ç»„ä»¶ã€‚è¿™ç§åˆ†è§£æœ‰æ•ˆåœ°å‡è½»äº†å¤©æ°”å¼•èµ·çš„é€€åŒ–ï¼ŒåŒæ—¶ä¿æŒé¢œè‰²ä¿çœŸåº¦ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹å–„æ¢å¤è´¨é‡ï¼ŒLGDMåˆ©ç”¨ä¸é€€åŒ–ç›¸å…³çš„äº®åº¦ä¿¡æ¯ä½œä¸ºæŒ‡å¯¼æ¡ä»¶ï¼Œæ¶ˆé™¤äº†å¯¹æ˜ç¡®é€€åŒ–æç¤ºçš„éœ€æ±‚ã€‚æ­¤å¤–ï¼ŒLGDMè¿˜é‡‡ç”¨äº†ä¸€ç§åŠ¨æ€æ—¶é—´æ­¥é•¿æŸå¤±æ¥ä¼˜åŒ–å»å™ªç½‘ç»œï¼Œç¡®ä¿å›¾åƒä¸­ä½é¢‘å’Œé«˜é¢‘ç‰¹å¾çš„å¹³è¡¡æ¢å¤ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†DriveWeatherï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å…¨å¤©å€™é©¾é©¶æ•°æ®é›†ï¼Œæ—¨åœ¨å®ç°ç¨³å¥çš„è¯„ä¼°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨AWIRä¸­æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/fiwy0527/LCDiff%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/fiwy0527/LCDiffè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17024v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å›¾åƒæ¢å¤æ¡†æ¶LCDiffï¼Œç”¨äºåº”å¯¹æ¶åŠ£å¤©æ°”ä¸‹çš„å›¾åƒæ¢å¤ï¼ˆAWIRï¼‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šLumina-Chromaåˆ†è§£ç½‘ç»œï¼ˆLCDNï¼‰å’ŒLumina-Guidedæ‰©æ•£æ¨¡å‹ï¼ˆLGDMï¼‰ã€‚LCDNåœ¨YCbCrè‰²å½©ç©ºé—´å¤„ç†é€€åŒ–å›¾åƒï¼Œåˆ†åˆ«å¤„ç†ä¸é€€åŒ–ç›¸å…³çš„äº®åº¦å’Œä¸å˜è‰²åº¦æˆåˆ†ï¼Œæœ‰æ•ˆå‡è½»å¤©æ°”å¼•èµ·çš„é€€åŒ–ï¼ŒåŒæ—¶ä¿æŒè‰²å½©ä¿çœŸã€‚LGDMåˆ©ç”¨ä¸é€€åŒ–ç›¸å…³çš„äº®åº¦ä¿¡æ¯ä½œä¸ºæŒ‡å¯¼æ¡ä»¶ï¼Œè¿›ä¸€æ­¥æé«˜æ¢å¤è´¨é‡ï¼Œå¹¶å¼•å…¥åŠ¨æ€æ—¶é—´æ­¥é•¿æŸå¤±æ¥ä¼˜åŒ–å»å™ªç½‘ç»œï¼Œç¡®ä¿å›¾åƒä¸­ä½é¢‘å’Œé«˜é¢‘ç‰¹å¾çš„å¹³è¡¡æ¢å¤ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†DriveWeatherå…¨å¤©æ°”é©¾é©¶æ•°æ®é›†ï¼Œç”¨äºç¨³å¥è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºAWIRè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LCDiffæ¡†æ¶è¢«æå‡ºï¼Œç”¨äºåº”å¯¹æ¶åŠ£å¤©æ°”å›¾åƒæ¢å¤ï¼ˆAWIRï¼‰çš„æŒ‘æˆ˜ã€‚</li>
<li>LCDiffåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šLCDNå’ŒLGDMã€‚</li>
<li>LCDNåœ¨YCbCrè‰²å½©ç©ºé—´å¤„ç†å›¾åƒï¼Œåˆ†ç¦»äº®åº¦å’Œè‰²åº¦æˆåˆ†ï¼Œä»¥æé«˜æ¢å¤æ•ˆæœã€‚</li>
<li>LGDMåˆ©ç”¨äº®åº¦ä¿¡æ¯ä½œä¸ºæŒ‡å¯¼æ¡ä»¶ï¼ŒåŠ¨æ€æ—¶é—´æ­¥é•¿æŸå¤±ä¼˜åŒ–å»å™ªç½‘ç»œã€‚</li>
<li>DriveWeatheræ•°æ®é›†çš„æ¨å‡ºä¸ºAWIRæä¾›äº†ç¨³å¥çš„è¯„ä¼°å¹³å°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLCDiffæ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3c12af6654f2baf1c3434fa8fd3e9d9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683597&auth_key=1760683597-0-0-ce8e7bd132e2eccaba04420112837de1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2802c1439406ff38c25928c310d7279b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683605&auth_key=1760683605-0-0-6ac8fc9b7f26f635788a799b565c2457&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0674382a98f59ff82601a3ba3056a1af~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683612&auth_key=1760683612-0-0-c675fa913b75d67ddb7c1c6f985b17f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7558fe016ce85a286e1e084c171fc3ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683619&auth_key=1760683619-0-0-76904cf3b57d2ccff9792f54a6115991&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a8d35976f7cc8afb0d6ce3526029d601~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683626&auth_key=1760683626-0-0-2fc28030506d05445024268bca64a828&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="V-CECE-Visual-Counterfactual-Explanations-via-Conceptual-Edits"><a href="#V-CECE-Visual-Counterfactual-Explanations-via-Conceptual-Edits" class="headerlink" title="V-CECE: Visual Counterfactual Explanations via Conceptual Edits"></a>V-CECE: Visual Counterfactual Explanations via Conceptual Edits</h2><p><strong>Authors:Nikolaos Spanos, Maria Lymperaiou, Giorgos Filandrianos, Konstantinos Thomas, Athanasios Voulodimos, Giorgos Stamou</strong></p>
<p>Recent black-box counterfactual generation frameworks fail to take into account the semantic content of the proposed edits, while relying heavily on training to guide the generation process. We propose a novel, plug-and-play black-box counterfactual generation framework, which suggests step-by-step edits based on theoretical guarantees of optimal edits to produce human-level counterfactual explanations with zero training. Our framework utilizes a pre-trained image editing diffusion model, and operates without access to the internals of the classifier, leading to an explainable counterfactual generation process. Throughout our experimentation, we showcase the explanatory gap between human reasoning and neural model behavior by utilizing both Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision Language Model (LVLM) classifiers, substantiated through a comprehensive human evaluation. </p>
<blockquote>
<p>æœ€è¿‘çš„é»‘ç›’åäº‹å®ç”Ÿæˆæ¡†æ¶å¿½ç•¥äº†æè®®ç¼–è¾‘çš„è¯­ä¹‰å†…å®¹ï¼ŒåŒæ—¶ä¸¥é‡ä¾èµ–äºè®­ç»ƒæ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ã€å³æ’å³ç”¨çš„é»‘ç›’åäº‹å®ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºæœ€ä¼˜ç¼–è¾‘çš„ç†è®ºä¿è¯ï¼Œæå‡ºé€æ­¥ç¼–è¾‘å»ºè®®ï¼Œä»¥é›¶è®­ç»ƒç”Ÿæˆäººç±»æ°´å¹³çš„åäº‹å®è§£é‡Šã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨é¢„å…ˆè®­ç»ƒçš„å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€è®¿é—®åˆ†ç±»å™¨çš„å†…éƒ¨ï¼Œä»è€Œå®ç°å¯è§£é‡Šçš„åäº‹å®ç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†äººç±»æ¨ç†å’Œç¥ç»ç½‘ç»œæ¨¡å‹è¡Œä¸ºä¹‹é—´çš„è§£é‡Šå·®è·ï¼Œè¿™é€šè¿‡ç»¼åˆäººç±»è¯„ä¼°å¾—åˆ°è¯å®ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åˆ†ç±»å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16567v1">PDF</a> Accepted in NeurIPS 2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºä¸€ç§æ–°çš„é›¶è®­ç»ƒçš„é»‘ç›’å¯¹æŠ—æ€§ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç†è®ºä¿è¯æœ€ä¼˜ç¼–è¾‘ï¼Œå¹¶åŸºäºé¢„è®­ç»ƒçš„å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹è¿›è¡Œé€æ­¥ç¼–è¾‘å»ºè®®ï¼Œç”Ÿæˆäººç±»çº§åˆ«çš„å¯¹æŠ—æ€§è§£é‡Šï¼Œæ— éœ€è®¿é—®åˆ†ç±»å™¨çš„å†…éƒ¨ä¿¡æ¯ã€‚è¯¥æ¡†æ¶å¡«è¡¥äº†äººç±»æ¨ç†å’Œç¥ç»ç½‘ç»œæ¨¡å‹è¡Œä¸ºä¹‹é—´çš„è§£é‡Šé¸¿æ²Ÿï¼Œå¹¶é€šè¿‡CNNã€Vision Transformerå’ŒLVLMåˆ†ç±»å™¨çš„ç»¼åˆäººç±»è¯„ä¼°å¾—åˆ°è¯å®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰é»‘ç›’å¯¹æŠ—ç”Ÿæˆæ¡†æ¶å¿½è§†äº†æè®®ç¼–è¾‘çš„è¯­ä¹‰å†…å®¹ï¼Œè€Œä¾èµ–å¤§é‡è®­ç»ƒæ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹é›¶è®­ç»ƒé»‘ç›’å¯¹æŠ—ç”Ÿæˆæ¡†æ¶ï¼ŒåŸºäºç†è®ºä¿è¯è¿›è¡Œæœ€ä¼˜ç¼–è¾‘å»ºè®®ï¼Œç”Ÿæˆäººç±»çº§åˆ«çš„å¯¹æŠ—è§£é‡Šã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹è¿›è¡Œæ“ä½œï¼Œæ— éœ€è®¿é—®åˆ†ç±»å™¨çš„å†…éƒ¨ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¡†æ¶å¡«è¡¥äº†äººç±»æ¨ç†å’Œç¥ç»ç½‘ç»œæ¨¡å‹è¡Œä¸ºä¹‹é—´çš„è§£é‡Šé¸¿æ²Ÿã€‚</li>
<li>é€šè¿‡CNNã€Vision Transformerå’ŒLVLMåˆ†ç±»å™¨çš„ç»¼åˆäººç±»è¯„ä¼°éªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¡†æ¶å…·æœ‰å¯æ’æ‹”æ€§ï¼Œå¯è½»æ¾é›†æˆåˆ°å…¶ä»–ç³»ç»Ÿä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0202361d300ec0a896fc59d97a312a78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683633&auth_key=1760683633-0-0-2b2f6a022760e5219c9a9623ee9b919a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9bd83b98a062f2456ceb98eabcee0ffa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683640&auth_key=1760683640-0-0-39e08ba5d1b06a8536d58924ac14c1c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Which-Direction-to-Choose-An-Analysis-on-the-Representation-Power-of-Self-Supervised-ViTs-in-Downstream-Tasks"><a href="#Which-Direction-to-Choose-An-Analysis-on-the-Representation-Power-of-Self-Supervised-ViTs-in-Downstream-Tasks" class="headerlink" title="Which Direction to Choose? An Analysis on the Representation Power of   Self-Supervised ViTs in Downstream Tasks"></a>Which Direction to Choose? An Analysis on the Representation Power of   Self-Supervised ViTs in Downstream Tasks</h2><p><strong>Authors:Yannis Kaltampanidis, Alexandros Doumanoglou, Dimitrios Zarpalas</strong></p>
<p>Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently demonstrated considerable potential as a pre-training strategy for a variety of computer vision tasks, including image classification and segmentation, both in standard and few-shot downstream contexts. Two pre-training objectives dominate the landscape of SSL techniques: Contrastive Learning and Masked Image Modeling. Features (or tokens) extracted from the final transformer attention block â€“ specifically, the keys, queries, and values â€“ as well as features obtained after the final blockâ€™s feed-forward layer, have become a common foundation for addressing downstream tasks. However, in many existing approaches, these pre-trained ViT features are further processed through additional transformation layers, often involving lightweight heads or combined with distillation, to achieve superior task performance. Although such methods can improve task outcomes, to the best of our knowledge, a comprehensive analysis of the intrinsic representation capabilities of unaltered ViT features has yet to be conducted. This study aims to bridge this gap by systematically evaluating the use of these unmodified features across image classification and segmentation tasks, in both standard and few-shot contexts. The classification and segmentation rules that we use are either hyperplane based (as in logistic regression) or cosine-similarity based, both of which rely on the presence of interpretable directions in the ViTâ€™s latent space. Based on the previous rules and without the use of additional feature transformations, we conduct an analysis across token types, tasks, and pre-trained ViT models. This study provides insights into the optimal choice for token type and decision rule based on the task, context, and the pre-training objective, while reporting detailed findings on two widely-used datasets. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å¯¹äºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰å·²ç»æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½œä¸ºä¸€ç§é¢„è®­ç»ƒç­–ç•¥ï¼Œå®ƒå¯ä»¥åº”ç”¨äºå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ ‡å‡†å’Œå°‘æ ·æœ¬ä¸‹æ¸¸ç¯å¢ƒä¸­çš„å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ã€‚åœ¨SSLæŠ€æœ¯ä¸­ï¼Œä¸¤ç§é¢„è®­ç»ƒç›®æ ‡å æ®ä¸»å¯¼åœ°ä½ï¼šå¯¹æ¯”å­¦ä¹ å’Œæ©æ¨¡å›¾åƒå»ºæ¨¡ã€‚ä»æœ€ç»ˆçš„è½¬æ¢å™¨æ³¨æ„åŠ›å—ä¸­æå–çš„ç‰¹å¾ï¼ˆæˆ–ä»¤ç‰Œï¼‰ï¼Œç‰¹åˆ«æ˜¯é”®ã€æŸ¥è¯¢å’Œå€¼ï¼Œä»¥åŠä»æœ€åä¸€ä¸ªå—çš„å‰é¦ˆå±‚ä¹‹åè·å¾—çš„ç‰¹å¾ï¼Œå·²ç»æˆä¸ºè§£å†³ä¸‹æ¸¸ä»»åŠ¡çš„å¸¸è§åŸºç¡€ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šç°æœ‰æ–¹æ³•ä¸­ï¼Œè¿™äº›é¢„è®­ç»ƒçš„ViTç‰¹å¾ä¼šç»è¿‡é¢å¤–çš„è½¬æ¢å±‚è¿›è¡Œå¤„ç†ï¼Œé€šå¸¸æ¶‰åŠè½»é‡çº§å¤´éƒ¨æˆ–ä¸è’¸é¦ç›¸ç»“åˆï¼Œä»¥å®ç°å“è¶Šçš„ä»»åŠ¡æ€§èƒ½ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å¯ä»¥æ”¹å–„ä»»åŠ¡ç»“æœï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°šæœªå¯¹æœªç»ä¿®æ”¹çš„ViTç‰¹å¾çš„å†…åœ¨è¡¨ç¤ºèƒ½åŠ›è¿›è¡Œå…¨é¢åˆ†æã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›æœªä¿®æ”¹çš„ç‰¹å¾åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨æ¥å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæ— è®ºæ˜¯åœ¨æ ‡å‡†è¿˜æ˜¯åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸­çš†æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬ä½¿ç”¨çš„åˆ†ç±»å’Œåˆ†å‰²è§„åˆ™æ˜¯åŸºäºè¶…å¹³é¢çš„ï¼ˆå¦‚é€»è¾‘å›å½’ï¼‰ï¼Œæˆ–æ˜¯åŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„ï¼Œä¸¤è€…éƒ½ä¾èµ–äºViTæ½œåœ¨ç©ºé—´ä¸­å¯è§£é‡Šæ–¹å‘çš„å­˜åœ¨ã€‚åŸºäºä¹‹å‰çš„è§„åˆ™å’Œæ²¡æœ‰ä½¿ç”¨é¢å¤–çš„ç‰¹å¾è½¬æ¢ï¼Œæˆ‘ä»¬å¯¹ä»¤ç‰Œç±»å‹ã€ä»»åŠ¡å’Œé¢„è®­ç»ƒçš„ViTæ¨¡å‹è¿›è¡Œäº†åˆ†æã€‚æœ¬ç ”ç©¶ä¸ºåŸºäºä»»åŠ¡ã€ä¸Šä¸‹æ–‡å’Œé¢„è®­ç»ƒç›®æ ‡çš„ä»¤ç‰Œç±»å‹å’Œå†³ç­–è§„åˆ™çš„æœ€ä½³é€‰æ‹©æä¾›äº†è§è§£ï¼Œå¹¶åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸ŠæŠ¥å‘Šäº†è¯¦ç»†çš„å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15272v1">PDF</a> 24 pages, XAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>     è§†è§‰Transformerçš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä½œä¸ºå¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œå·²å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ï¼Œä»¥åŠæ ‡å‡†å’Œå°‘é•œå¤´ä¸‹æ¸¸ç¯å¢ƒã€‚SSLæŠ€æœ¯çš„ä¸»è¦é¢„è®­ç»ƒç›®æ ‡åŒ…æ‹¬å¯¹æ¯”å­¦ä¹ å’Œæ©æ¨¡å›¾åƒå»ºæ¨¡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°æœªç»ä¿®æ”¹çš„ç‰¹å¾åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­çš„ä½¿ç”¨ï¼Œæ¶‰åŠæ ‡è®°ç±»å‹ã€ä»»åŠ¡å’Œé¢„è®­ç»ƒè§†è§‰Transformeræ¨¡å‹çš„é€‰æ‹©ã€‚åŸºäºä»¥å‰çš„è§„å®šï¼Œä¸ä½¿ç”¨é¢å¤–çš„ç‰¹å¾è½¬æ¢ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒè¯­å¢ƒä¸‹çš„ä»»åŠ¡é€‰æ‹©å¯¹äºæ ‡è®°ç±»å‹å’Œå†³ç­–è§„åˆ™çš„æœ€ä¼˜é€‰æ‹©ï¼ŒåŒæ—¶æŠ¥å‘Šäº†åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨æ•°æ®é›†ä¸Šçš„è¯¦ç»†å‘ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶æœ‰åŠ©äºäº†è§£è§†è§‰Transformerçš„å†…åœ¨è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å·²æˆä¸ºè§†è§‰Transformerï¼ˆViTï¼‰é¢„è®­ç»ƒçš„æœ‰æ•ˆç­–ç•¥ï¼Œé€‚ç”¨äºå¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚</li>
<li>SSLæŠ€æœ¯çš„ä¸»è¦é¢„è®­ç»ƒç›®æ ‡åŒ…æ‹¬å¯¹æ¯”å­¦ä¹ å’Œæ©æ¨¡å›¾åƒå»ºæ¨¡ã€‚</li>
<li>æœªç»ä¿®æ”¹çš„è§†è§‰Transformerç‰¹å¾åœ¨å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸åŒæ ‡è®°ç±»å‹ã€ä»»åŠ¡å’Œé¢„è®­ç»ƒæ¨¡å‹çš„é€‰æ‹©ã€‚</li>
<li>åˆ†æè¡¨æ˜ï¼Œåœ¨å†³ç­–è¿‡ç¨‹ä¸­åº”è€ƒè™‘åˆ°ä»»åŠ¡ã€ä¸Šä¸‹æ–‡å’Œé¢„è®­ç»ƒç›®æ ‡æ¥é€‰æ‹©æœ€ä½³çš„æ ‡è®°ç±»å‹å’Œå†³ç­–è§„åˆ™ã€‚</li>
<li>ç ”ç©¶æŠ¥å‘Šäº†åœ¨ä¸¤ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šçš„è¯¦ç»†å‘ç°ï¼Œä¸ºç†è§£è§†è§‰Transformerçš„å†…åœ¨è¡¨ç¤ºèƒ½åŠ›æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cfed9f114595e1e5edfd1c8dd32f7e01~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683648&auth_key=1760683648-0-0-646c9d47ce1cf270880dad2689e3e112&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-25554e8d3f95e4324616c8848b2de98c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683655&auth_key=1760683655-0-0-1b3b4f1ed9e93377aad0de9edf8ca805&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ViSpec-Accelerating-Vision-Language-Models-with-Vision-Aware-Speculative-Decoding"><a href="#ViSpec-Accelerating-Vision-Language-Models-with-Vision-Aware-Speculative-Decoding" class="headerlink" title="ViSpec: Accelerating Vision-Language Models with Vision-Aware   Speculative Decoding"></a>ViSpec: Accelerating Vision-Language Models with Vision-Aware   Speculative Decoding</h2><p><strong>Authors:Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen</strong></p>
<p>Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (&lt;1.5x). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft modelâ€™s attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target modelâ€™s hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KangJialiang/ViSpec">https://github.com/KangJialiang/ViSpec</a>. </p>
<blockquote>
<p>æ¨æµ‹è§£ç æ˜¯åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ï¼Œç„¶è€Œå®ƒåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œç°æœ‰æ–¹æ³•åªèƒ½å®ç°é€‚åº¦çš„åŠ é€Ÿï¼ˆ&lt;1.5å€ï¼‰ã€‚éšç€å¤šæ¨¡æ€èƒ½åŠ›æˆä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„æ ¸å¿ƒï¼Œè¿™ä¸€å·®è·å˜å¾—è¶Šæ¥è¶Šæ˜¾è‘—ã€‚æˆ‘ä»¬å‡è®¾å¤§å‹VLMå¯ä»¥é€å±‚æœ‰æ•ˆåœ°è¿‡æ»¤æ‰å†—ä½™çš„å›¾åƒä¿¡æ¯ï¼Œè€Œä¸æŸå®³æ–‡æœ¬ç†è§£ï¼Œè€Œè¾ƒå°çš„è‰ç¨¿æ¨¡å‹åˆ™å¾ˆéš¾åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹VLMè®¾è®¡çš„å…¨æ–°æ¡†æ¶â€”â€”è§†è§‰æ„ŸçŸ¥æ¨æµ‹è§£ç ï¼ˆViSpecï¼‰ã€‚ViSpecé‡‡ç”¨è½»é‡çº§çš„è§†è§‰é€‚é…å™¨æ¨¡å—ï¼Œå°†å›¾åƒä»¤ç‰Œå‹ç¼©æˆç´§å‡‘çš„è¡¨ç¤ºå½¢å¼ï¼Œæ— ç¼é›†æˆåˆ°è‰ç¨¿æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªè¾“å…¥å›¾åƒæå–å…¨å±€ç‰¹å¾å‘é‡ï¼Œå¹¶å°†å…¶å¢å¼ºåˆ°éšåçš„æ‰€æœ‰æ–‡æœ¬ä»¤ç‰Œä¸­ï¼Œä»¥æé«˜å¤šæ¨¡æ€ä¸€è‡´æ€§ã€‚ä¸ºäº†å…‹æœç¼ºä¹å¸¦æœ‰é•¿åŠ©ç†å“åº”çš„å¤šæ¨¡æ€æ•°æ®é›†çš„é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡é‡æ–°åˆ©ç”¨ç°æœ‰æ•°æ®é›†å¹¶ä½¿ç”¨ç›®æ ‡VLMç”Ÿæˆæ‰©å±•è¾“å‡ºæ¥ä¸“é—¨åˆ¶ä½œä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„è®­ç»ƒç­–ç•¥å‡è½»äº†è‰ç¨¿æ¨¡å‹ç›´æ¥è®¿é—®ç›®æ ‡æ¨¡å‹çš„éšè—çŠ¶æ€çš„é£é™©ï¼Œå¦‚æœä»…åœ¨ç›®æ ‡æ¨¡å‹è¾“å‡ºä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½å¯¼è‡´æ·å¾„å­¦ä¹ ã€‚å¤§é‡å®éªŒéªŒè¯äº†ViSpecçš„æœ‰æ•ˆæ€§ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯VLMæ¨æµ‹è§£ç æ–¹é¢çš„é¦–æ¬¡å®è´¨æ€§åŠ é€Ÿã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KangJialiang/ViSpec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KangJialiang/ViSpecæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15235v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨æµ‹è§£ç æŠ€æœ¯å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œç°æœ‰æ–¹æ³•çš„é€Ÿåº¦æå‡æœ‰é™ï¼ˆå°äº1.5å€ï¼‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹VLMsçš„å®šåˆ¶æ¡†æ¶â€”â€”Vision-Aware Speculative Decodingï¼ˆViSpecï¼‰ã€‚å®ƒé€šè¿‡è½»é‡çº§è§†è§‰é€‚é…å™¨æ¨¡å—å‹ç¼©å›¾åƒæ ‡è®°ï¼Œå°†å…¶æ— ç¼é›†æˆåˆ°è‰æ¡ˆæ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä¸ºæ¯ä¸ªè¾“å…¥å›¾åƒæå–å…¨å±€ç‰¹å¾å‘é‡ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°æ‰€æœ‰åç»­æ–‡æœ¬æ ‡è®°ä¸­ï¼Œä»¥å¢å¼ºå¤šåª’ä½“æ¨¡æ€çš„ä¸€è‡´æ€§ã€‚è¯¥è®ºæ–‡é€šè¿‡å®éªŒéªŒè¯äº†ViSpecçš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†å¯¹VLMæ¨æµ‹è§£ç çš„é¦–æ¬¡å®è´¨æ€§åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨æµ‹è§£ç æŠ€æœ¯åº”ç”¨å­˜åœ¨ç©ºç™½ï¼Œç°æœ‰æ–¹æ³•é€Ÿåº¦æå‡æœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹VLMsçš„å®šåˆ¶æ¡†æ¶â€”â€”Vision-Aware Speculative Decodingï¼ˆViSpecï¼‰ã€‚</li>
<li>ViSpecä½¿ç”¨è½»é‡çº§è§†è§‰é€‚é…å™¨æ¨¡å—å‹ç¼©å›¾åƒæ ‡è®°ä»¥åŠ é€Ÿæ¨æ–­ã€‚</li>
<li>ViSpecä¿ç•™äº†åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ï¼Œå¹¶å°†å…¶é›†æˆåˆ°è‰æ¡ˆæ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚</li>
<li>é€šè¿‡ä¸ºæ¯ä¸ªè¾“å…¥å›¾åƒæå–å…¨å±€ç‰¹å¾å‘é‡å¹¶æ·»åŠ åˆ°æ–‡æœ¬æ ‡è®°ä¸­ï¼Œå¢å¼ºå¤šåª’ä½“æ¨¡æ€çš„ä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨ç‰¹æ®Šè®­ç»ƒæ•°æ®é›†å…‹æœè®­ç»ƒæ—¶çš„æŒ‘æˆ˜ï¼Œé¿å…æ¨¡å‹åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„éšè—çŠ¶æ€å¯¼è‡´çš„æ·å¾„å­¦ä¹ ã€‚</li>
<li>å®éªŒéªŒè¯äº†ViSpecçš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†å¯¹VLMæ¨æµ‹è§£ç çš„é¦–æ¬¡å®è´¨æ€§åŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-461483c2c8157fde55f2bb2370a5dfc9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683663&auth_key=1760683663-0-0-3c7b7e12662c01e2dd08984ab7d0cd61&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-235428602e43a141b7ca45b7c6f867fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683670&auth_key=1760683670-0-0-3ca0363ed745a2ddde805a39a0c58c85&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4188160b4159f71ce59eee4ce462e16~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683677&auth_key=1760683677-0-0-8b58e8e46e49ccb4a976153d2a0c8dbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e17e2f836736fc394f63c0c0d8038f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683684&auth_key=1760683684-0-0-8a6b5507961df09912fe7dc726a1a812&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SAIL-VL2-Technical-Report"><a href="#SAIL-VL2-Technical-Report" class="headerlink" title="SAIL-VL2 Technical Report"></a>SAIL-VL2 Technical Report</h2><p><strong>Authors:Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng</strong></p>
<p>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Its effectiveness is driven by three core innovations. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SAIL-VL2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆLVMï¼‰ï¼Œç”¨äºå…¨é¢çš„å¤šæ¨¡å¼ç†è§£å’Œæ¨ç†ã€‚ä½œä¸ºSAIL-VLçš„åç»­äº§å“ï¼ŒSAIL-VL2åœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„2Bå’Œ8Bä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å¤šç§å›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†ä»ç²¾ç»†ç²’åº¦æ„ŸçŸ¥åˆ°å¤æ‚æ¨ç†çš„å¼ºå¤§èƒ½åŠ›ã€‚å®ƒçš„æœ‰æ•ˆæ€§æºäºä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ã€‚é¦–å…ˆï¼Œé‡‡ç”¨å¤§è§„æ¨¡æ•°æ®æ•´ç†ç®¡é“ï¼Œç»“åˆè¯„åˆ†å’Œè¿‡æ»¤ç­–ç•¥ï¼Œæé«˜äº†æè¿°ã€OCRã€é—®ç­”å’Œè§†é¢‘æ•°æ®çš„è´¨é‡å’Œåˆ†å¸ƒï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å…¶æ¬¡ï¼Œæ¸è¿›å¼è®­ç»ƒæ¡†æ¶ä»ä¸€ä¸ªå¼ºå¤§çš„é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼ˆSAIL-ViTï¼‰å¼€å§‹ï¼Œé€šè¿‡å¤šæ¨¡å¼é¢„è®­ç»ƒï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ªç³»ç»Ÿå¼ºåŒ–æ¨¡å‹èƒ½åŠ›çš„æ€ç»´èåˆSFT-RLæ··åˆèŒƒå¼ã€‚ç¬¬ä¸‰ï¼Œæ¶æ„ä¸Šçš„è¿›æ­¥è¶…è¶Šäº†å¯†é›†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨äº†é«˜æ•ˆçš„ç¨€ç–ä¸“å®¶æ··åˆï¼ˆMoEï¼‰è®¾è®¡ã€‚é€šè¿‡è¿™äº›è´¡çŒ®ï¼ŒSAIL-VL2åœ¨106ä¸ªæ•°æ®é›†ä¸Šå±•ç°äº†ç«äº‰åŠ›ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MMMUå’ŒMathVistaï¼‰ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚æ­¤å¤–ï¼Œåœ¨OpenCompassæ’è¡Œæ¦œä¸Šï¼ŒSAIL-VL2-2Båœ¨å®˜æ–¹å‘å¸ƒçš„å¼€æºæ¨¡å‹ä¸­ä½åˆ—4Bå‚æ•°è§„æ¨¡çš„ç¬¬ä¸€åï¼Œæˆä¸ºå¼€æºå¤šæ¨¡å¼ç¤¾åŒºé«˜æ•ˆä¸”å¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14033v2">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>SAIL-VL2æ˜¯ä¸€æ¬¾ç”¨äºå…¨é¢å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„å¼€æ”¾è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆLVMï¼‰ã€‚ç›¸è¾ƒäºå‰ä»£æ¨¡å‹ï¼ŒSAIL-VL2åœ¨2Bå’Œ8Bå‚æ•°è§„æ¨¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œåœ¨å„ç§å›¾åƒå’Œè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä»ç²¾ç»†ç²’åº¦æ„ŸçŸ¥åˆ°å¤æ‚æ¨ç†ã€‚å…¶æœ‰æ•ˆæ€§æºäºä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šå¤§è§„æ¨¡æ•°æ®æ•´ç†ç®¡é“ã€æ¸è¿›å¼è®­ç»ƒæ¡†æ¶å’Œæ¶æ„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAIL-VL2æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„å¼€æ”¾è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆLVMï¼‰ã€‚</li>
<li>ä¸å‰ä»£æ¨¡å‹ç›¸æ¯”ï¼ŒSAIL-VL2åœ¨å‚æ•°è§„æ¨¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ã€‚</li>
<li>SAIL-VL2é€šè¿‡å¤§è§„æ¨¡æ•°æ®æ•´ç†ç®¡é“æé«˜äº†æ•°æ®è´¨é‡å’Œåˆ†å¸ƒï¼Œæ”¹è¿›äº†æ ‡æ³¨ã€OCRã€QAå’Œè§†é¢‘æ•°æ®çš„è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>æ¸è¿›å¼è®­ç»ƒæ¡†æ¶ä»å¼ºå¤§çš„é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å¼€å§‹ï¼Œé€šè¿‡å¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ªç³»ç»Ÿå¼ºåŒ–æ¨¡å‹èƒ½åŠ›çš„æ€è€ƒèåˆSFT-RLæ··åˆèŒƒå¼ã€‚</li>
<li>SAIL-VL2çš„æ¶æ„è¿›æ­¥è¶…è¶Šäº†å¯†é›†çš„LLMsï¼Œé‡‡ç”¨äº†é«˜æ•ˆçš„ç¨€ç–Mixture-of-Expertsï¼ˆMoEï¼‰è®¾è®¡ã€‚</li>
<li>SAIL-VL2åœ¨106ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶åœ¨MMMUå’ŒMathVistaç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€ä½³çŠ¶æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-88a787f79e735213e58a71c2e42a9293~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683692&auth_key=1760683692-0-0-6ff69ea2775fc558c660b5fe5f460f34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ce3e7aa843e8cff6b73d50d74660c13~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683700&auth_key=1760683700-0-0-c32472469defbbe9d4badb8c4361db98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ba80a4c219cd68fdb4be088705b3a70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683706&auth_key=1760683706-0-0-fd2c1334ca48938123444b6c2a7856fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2613522288dfbd84a874b2443072545f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683713&auth_key=1760683713-0-0-0cfa8c49b430df2cfe746c0446acac4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-980f27df19d0dc7f0d9b2c9fb41a9fa2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683720&auth_key=1760683720-0-0-c800dc0383e5357f69d630d30fa3efaa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Look-Focus-Act-Efficient-and-Robust-Robot-Learning-via-Human-Gaze-and-Foveated-Vision-Transformers"><a href="#Look-Focus-Act-Efficient-and-Robust-Robot-Learning-via-Human-Gaze-and-Foveated-Vision-Transformers" class="headerlink" title="Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and   Foveated Vision Transformers"></a>Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and   Foveated Vision Transformers</h2><p><strong>Authors:Ian Chuang, Jinyu Zou, Andrew Lee, Dechen Gao, Iman Soltani</strong></p>
<p>Human vision is a highly active process driven by gaze, which directs attention to task-relevant regions through foveation, dramatically reducing visual processing. In contrast, robot learning systems typically rely on passive, uniform processing of raw camera images. In this work, we explore how incorporating human-like active gaze into robotic policies can enhance efficiency and robustness. We develop GIAVA (Gaze Integrated Active-Vision ALOHA), a robot vision system that emulates human head and neck movement, and gaze adjustment for foveated processing. Extending the AV-ALOHA robot platform, we introduce a framework for simultaneously collecting eye-tracking, perspective control, and robot manipulation demonstration data from a human operator. We also open-source a simulation benchmark and dataset for training robot policies that incorporate human gaze. Inspired by recent work in foveated image segmentation and given the widespread use of Vision Transformers (ViTs) in robot learning, we integrate gaze information into ViTs using a foveated patch tokenization scheme. Compared to uniform patch tokenization, this significantly reduces the number of tokens, and thus computation. Our results show that our method for foveated robot vision drastically reduces computational overhead, and enhances robustness to background distractors. Notably, on certain high-precision tasks, foveated vision also improves performance, as reflected in higher success rates. Together, these findings suggest that human-inspired foveated visual processing offers untapped potential and should be further considered as a useful inductive bias in robotic vision systems. <a target="_blank" rel="noopener" href="https://ian-chuang.github.io/gaze-av-aloha/">https://ian-chuang.github.io/gaze-av-aloha/</a> </p>
<blockquote>
<p>äººç±»çš„è§†è§‰æ˜¯ä¸€ä¸ªç”±æ³¨è§†é©±åŠ¨çš„é«˜åº¦æ´»è·ƒçš„è¿‡ç¨‹ï¼Œé€šè¿‡æ³¨è§†å°†æ³¨æ„åŠ›é›†ä¸­åœ¨ä¸ä»»åŠ¡ç›¸å…³çš„åŒºåŸŸï¼Œä»è€Œæå¤§åœ°å‡å°‘è§†è§‰å¤„ç†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœºå™¨äººå­¦ä¹ ç³»ç»Ÿé€šå¸¸ä¾èµ–äºå¯¹åŸå§‹ç›¸æœºå›¾åƒçš„è¢«åŠ¨å‡åŒ€å¤„ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å°†äººç±»ç±»ä¼¼çš„ä¸»åŠ¨æ³¨è§†èå…¥æœºå™¨äººç­–ç•¥å¦‚ä½•å¢å¼ºæ•ˆç‡å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†GIAVAï¼ˆé›†æˆæ³¨è§†çš„ä¸»åŠ¨è§†è§‰ALOHAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿäººç±»å¤´éƒ¨å’Œé¢ˆéƒ¨è¿åŠ¨ä»¥åŠæ³¨è§†è°ƒæ•´è¿›è¡Œç„¦ç‚¹å¤„ç†çš„æœºå™¨äººè§†è§‰ç³»ç»Ÿã€‚åœ¨AV-ALOHAæœºå™¨äººå¹³å°çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶ä»äººç±»æ“ä½œè€…é‚£é‡Œæ”¶é›†çœ¼åŠ¨è¿½è¸ªã€è§†è§’æ§åˆ¶å’Œæœºå™¨äººæ“ä½œæ¼”ç¤ºæ•°æ®ã€‚æˆ‘ä»¬è¿˜å¼€æºäº†ä¸€ä¸ªæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•å’Œæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒèå…¥äººç±»æ³¨è§†çš„æœºå™¨äººç­–ç•¥ã€‚å—æœ€è¿‘å…³äºç„¦ç‚¹å›¾åƒåˆ†å‰²å·¥ä½œçš„å¯å‘ï¼Œä»¥åŠVision Transformersï¼ˆViTsï¼‰åœ¨æœºå™¨äººå­¦ä¹ ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç§ç„¦ç‚¹è¡¥ä¸æ ‡è®°æ–¹æ¡ˆå°†æ³¨è§†ä¿¡æ¯èå…¥ViTsã€‚ä¸å‡åŒ€çš„è¡¥ä¸æ ‡è®°ç›¸æ¯”ï¼Œè¿™æ˜¾è‘—å‡å°‘äº†æ ‡è®°çš„æ•°é‡ï¼Œä»è€Œå‡å°‘äº†è®¡ç®—é‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç„¦ç‚¹æœºå™¨äººè§†è§‰æ–¹æ³•æå¤§åœ°å‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œå¹¶å¢å¼ºäº†å¯¹æŠ—èƒŒæ™¯å¹²æ‰°é¡¹çš„ç¨³å¥æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æŸäº›é«˜ç²¾åº¦ä»»åŠ¡ä¸Šï¼Œç„¦ç‚¹è§†è§‰ä¹Ÿæé«˜äº†æ€§èƒ½ï¼Œä½“ç°åœ¨æ›´é«˜çš„æˆåŠŸç‡ä¸Šã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™äº›å‘ç°è¡¨æ˜äººç±»å¯å‘çš„ç„¦ç‚¹è§†è§‰å¤„ç†å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶åº”è¿›ä¸€æ­¥è€ƒè™‘å°†å…¶ä½œä¸ºæœºå™¨äººè§†è§‰ç³»ç»Ÿä¸­çš„ä¸€ä¸ªæœ‰ç”¨çš„å½’çº³åè§ã€‚<a target="_blank" rel="noopener" href="https://ian-chuang.github.io/gaze-av-aloha/">https://ian-chuang.github.io/gaze-av-aloha/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15833v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ian-chuang.github.io/gaze-av-aloha/">https://ian-chuang.github.io/gaze-av-aloha/</a></p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢ç´¢äº†å°†äººç±»å¼çš„ä¸»åŠ¨è§†çº¿èå…¥æœºå™¨äººç­–ç•¥ï¼Œä»¥æé«˜æœºå™¨äººè§†è§‰ç³»ç»Ÿçš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†GIAVAï¼ˆè§†çº¿é›†æˆä¸»åŠ¨è§†è§‰ALOHAï¼‰ï¼Œä¸€ä¸ªæ¨¡ä»¿äººç±»å¤´éƒ¨å’Œé¢ˆéƒ¨è¿åŠ¨ä»¥åŠè§†çº¿è°ƒæ•´çš„æœºå™¨äººè§†è§‰ç³»ç»Ÿã€‚è¯¥ç ”ç©¶å°†è§†çº¿ä¿¡æ¯èå…¥Vision Transformersï¼ˆViTsï¼‰ï¼Œé‡‡ç”¨æ³¨è§†è¡¥ä¸æ ‡è®°æ–¹æ¡ˆï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å‡åŒ€è¡¥ä¸æ ‡è®°ï¼Œæ˜¾è‘—å‡å°‘äº†æ ‡è®°æ•°é‡ï¼Œé™ä½äº†è®¡ç®—é‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ³¨è§†æœºå™¨äººè§†è§‰æ–¹æ³•å¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ï¼Œæé«˜äº†å¯¹èƒŒæ™¯å¹²æ‰°å› ç´ çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨æŸäº›é«˜ç²¾åº¦ä»»åŠ¡ä¸Šæå‡äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»è§†è§‰æ˜¯ä¸€ä¸ªç”±è§†çº¿é©±åŠ¨çš„æ´»åŠ¨è¿‡ç¨‹ï¼Œé€šè¿‡æ³¨è§†ä»»åŠ¡ç›¸å…³åŒºåŸŸå®ç°è§†è§‰å¤„ç†çš„æ˜¾è‘—å‡å°‘ã€‚</li>
<li>æœºå™¨äººå­¦ä¹ ç³»ç»Ÿé€šå¸¸ä¾èµ–äºå¯¹åŸå§‹ç›¸æœºå›¾åƒçš„è¢«åŠ¨å‡åŒ€å¤„ç†ï¼Œè€Œæœ¬ç ”ç©¶æ¢ç´¢äº†å°†äººç±»å¼çš„ä¸»åŠ¨è§†çº¿èå…¥æœºå™¨äººç­–ç•¥ä»¥å¢å¼ºæ•ˆç‡å’Œç¨³å¥æ€§ã€‚</li>
<li>å¼€å‘äº†GIAVAï¼ˆè§†çº¿é›†æˆä¸»åŠ¨è§†è§‰ALOHAï¼‰ç³»ç»Ÿï¼Œæ¨¡ä»¿äººç±»å¤´éƒ¨å’Œé¢ˆéƒ¨è¿åŠ¨ä»¥åŠè§†çº¿è°ƒæ•´ï¼Œå®ç°æ³¨è§†å¤„ç†ã€‚</li>
<li>å¼•å…¥äº†åŒæ—¶æ”¶é›†äººç±»æ“ä½œå‘˜çš„çœ¼åŠ¨è¿½è¸ªã€è§†è§’æ§åˆ¶å’Œæœºå™¨äººæ“ä½œæ¼”ç¤ºæ•°æ®çš„æ¡†æ¶ã€‚</li>
<li>å¼€æ”¾äº†ä¸€ä¸ªç”¨äºè®­ç»ƒèå…¥äººç±»è§†çº¿çš„æœºå™¨äººç­–ç•¥ä»¿çœŸåŸºå‡†æµ‹è¯•å’Œæ•°æ®é›†ã€‚</li>
<li>èåˆæ³¨è§†ä¿¡æ¯åˆ°Vision Transformersï¼ˆViTsï¼‰ä¸­ï¼Œé€šè¿‡æ³¨è§†è¡¥ä¸æ ‡è®°æ–¹æ¡ˆæ˜¾è‘—å‡å°‘äº†æ ‡è®°æ•°é‡å’Œè®¡ç®—é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e180010a26b490ffb85d3dee2673337f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683727&auth_key=1760683727-0-0-78375014d1f80b1b53a333565b41ed56&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce8c936a1bcd449b5eccd3188479fbb2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683735&auth_key=1760683735-0-0-c88e3cf291fdb85594c565f7f4a2229c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-855f2fb3102d6349def39464ba8bc4a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683741&auth_key=1760683741-0-0-d26d2806fb76bef7f5e31213bc246266&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-08bd6f98e97a4b3467e40e67b4f8f7f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683748&auth_key=1760683748-0-0-b2f4cb55318290b3d99b556148b2aa04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a8bb990707f79aebc597668f8848362~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683755&auth_key=1760683755-0-0-653335e768b30fed1f5454526553288e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4915c3cd7abcc51f6631b2391967308d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683762&auth_key=1760683762-0-0-dca8fcaeeb2382505b04a284ebc8c771&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2012d36149e6d7076bf5b13516fb2da8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683768&auth_key=1760683768-0-0-6deaadb4e460605c7674fb180297971b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Image-Quality-Assessment-via-Adaptation-of-Vision-Language-Models"><a href="#Few-Shot-Image-Quality-Assessment-via-Adaptation-of-Vision-Language-Models" class="headerlink" title="Few-Shot Image Quality Assessment via Adaptation of Vision-Language   Models"></a>Few-Shot Image Quality Assessment via Adaptation of Vision-Language   Models</h2><p><strong>Authors:Xudong Li, Zihao Huang, Yan Zhang, Yunhang Shen, Ke Li, Xiawu Zheng, Liujuan Cao, Rongrong Ji</strong></p>
<p>Image Quality Assessment (IQA) remains an unresolved challenge in computer vision due to complex distortions, diverse image content, and limited data availability. Existing Blind IQA (BIQA) methods largely rely on extensive human annotations, which are labor-intensive and costly due to the demanding nature of creating IQA datasets. To reduce this dependency, we propose the Gradient-Regulated Meta-Prompt IQA Framework (GRMP-IQA), designed to efficiently adapt the visual-language pre-trained model, CLIP, to IQA tasks, achieving high accuracy even with limited data. GRMP-IQA consists of two core modules: (i) Meta-Prompt Pre-training Module and (ii) Quality-Aware Gradient Regularization. The Meta Prompt Pre-training Module leverages a meta-learning paradigm to pre-train soft prompts with shared meta-knowledge across different distortions, enabling rapid adaptation to various IQA tasks. On the other hand, the Quality-Aware Gradient Regularization is designed to adjust the update gradients during fine-tuning, focusing the modelâ€™s attention on quality-relevant features and preventing overfitting to semantic information. Extensive experiments on standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods under limited data setting. Notably, utilizing just 20% of the training data, GRMP-IQA is competitive with most existing fully supervised BIQA approaches. </p>
<blockquote>
<p>å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ä»ç„¶æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªæœªè§£å†³çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºå¤æ‚çš„å¤±çœŸã€å¤šæ ·åŒ–çš„å›¾åƒå†…å®¹å’Œæ•°æ®å¯ç”¨æ€§æœ‰é™æ‰€å¯¼è‡´çš„ã€‚ç°æœ‰çš„ç›²å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆBIQAï¼‰æ–¹æ³•å¤§å¤šä¾èµ–äºå¤§é‡çš„äººå·¥æ ‡æ³¨ï¼Œç”±äºåˆ›å»ºIQAæ•°æ®é›†çš„æ€§è´¨è¦æ±‚ä¸¥æ ¼ï¼Œå› æ­¤åŠ³åŠ¨å¯†é›†ä¸”æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†å‡å°‘è¿™ç§ä¾èµ–æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ¢¯åº¦è°ƒèŠ‚å…ƒæç¤ºå›¾åƒè´¨é‡è¯„ä¼°æ¡†æ¶ï¼ˆGRMP-IQAï¼‰ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æœ‰æ•ˆåœ°é€‚åº”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹CLIPï¼Œå³ä½¿åœ¨æœ‰é™æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°é«˜å‡†ç¡®æ€§ã€‚GRMP-IQAç”±ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼šï¼ˆiï¼‰å…ƒæç¤ºé¢„è®­ç»ƒæ¨¡å—å’Œï¼ˆiiï¼‰è´¨é‡æ„ŸçŸ¥æ¢¯åº¦æ­£åˆ™åŒ–ã€‚å…ƒæç¤ºé¢„è®­ç»ƒæ¨¡å—åˆ©ç”¨å…ƒå­¦ä¹ èŒƒå¼å¯¹ä¸åŒå¤±çœŸè¿›è¡Œå…±äº«å…ƒçŸ¥è¯†çš„è½¯æç¤ºé¢„è®­ç»ƒï¼Œä»è€Œèƒ½å¤Ÿè¿…é€Ÿé€‚åº”å„ç§IQAä»»åŠ¡ã€‚å¦ä¸€æ–¹é¢ï¼Œè´¨é‡æ„ŸçŸ¥æ¢¯åº¦æ­£åˆ™åŒ–è®¾è®¡ç”¨äºå¾®è°ƒæ—¶è°ƒæ•´æ›´æ–°æ¢¯åº¦ï¼Œä½¿æ¨¡å‹çš„æ³¨æ„åŠ›é›†ä¸­åœ¨è´¨é‡ç›¸å…³ç‰¹å¾ä¸Šï¼Œé˜²æ­¢è¿‡åº¦æ‹Ÿåˆè¯­ä¹‰ä¿¡æ¯ã€‚åœ¨æ ‡å‡†BIQAæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨æœ‰é™æ•°æ®è®¾ç½®ä¸‹ï¼ŒGRMP-IQAçš„æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„BIQAæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…ä½¿ç”¨20%çš„è®­ç»ƒæ•°æ®ï¼ŒGRMP-IQAå°±ä¸å¤§å¤šæ•°ç°æœ‰çš„å®Œå…¨ç›‘ç£BIQAæ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05381v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰çš„æŒ‘æˆ˜ä»¥åŠç°æœ‰BIQAæ–¹æ³•çš„ä¸è¶³ã€‚ä¸ºå‡å°‘å¯¹æ•°æ®æ ‡æ³¨çš„ä¾èµ–ï¼Œæå‡ºäº†Gradient-Regulated Meta-Prompt IQA Frameworkï¼ˆGRMP-IQAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹CLIPï¼Œåœ¨æœ‰é™æ•°æ®ä¸‹å®ç°é«˜å‡†ç¡®ç‡çš„IQAä»»åŠ¡ã€‚å…¶æ ¸å¿ƒåŒ…æ‹¬ä¸¤å¤§æ¨¡å—ï¼šMeta-Prompt Pre-training Moduleå’ŒQuality-Aware Gradient Regularizationã€‚å‰è€…é€šè¿‡å…ƒå­¦ä¹ èŒƒå¼é¢„è®­ç»ƒè½¯æç¤ºï¼Œåè€…ç”¨äºå¾®è°ƒæ—¶çš„æ¢¯åº¦è°ƒæ•´ï¼Œé‡ç‚¹å…³æ³¨è´¨é‡ç›¸å…³ç‰¹å¾å¹¶é˜²æ­¢å¯¹è¯­ä¹‰ä¿¡æ¯çš„è¿‡åº¦æ‹Ÿåˆã€‚åœ¨æ ‡å‡†BIQAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨æœ‰é™æ•°æ®è®¾ç½®ä¸‹ï¼ŒGRMP-IQAçš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„BIQAæ–¹æ³•ã€‚ä»…ä½¿ç”¨20%çš„è®­ç»ƒæ•°æ®ï¼ŒGRMP-IQAçš„è¡¨ç°åœ¨å¤§å¤šæ•°ç°æœ‰å…¨ç›‘ç£BIQAæ–¹æ³•ä¸­éƒ½å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªç”±äºå¤æ‚å¤±çœŸã€å¤šæ ·åŒ–çš„å›¾åƒå†…å®¹å’Œæœ‰é™çš„æ•°æ®å¯ç”¨æ€§è€Œæœªèƒ½å®Œå…¨è§£å†³çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰çš„ç›²å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆBIQAï¼‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå¤§é‡çš„äººå·¥æ³¨é‡Šï¼Œè¿™æ—¢è€—è´¹åŠ³åŠ›åˆæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºäº†Gradient-Regulated Meta-Prompt IQA Frameworkï¼ˆGRMP-IQAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹CLIPæ¥è§£å†³IQAä»»åŠ¡ï¼Œå¹¶åœ¨æœ‰é™æ•°æ®ä¸‹å®ç°é«˜å‡†ç¡®ç‡ã€‚</li>
<li>GRMP-IQAæ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šMeta-Prompt Pre-training Moduleå’ŒQuality-Aware Gradient Regularizationã€‚</li>
<li>Meta-Prompt Pre-training Moduleé€šè¿‡å…ƒå­¦ä¹ èŒƒå¼è¿›è¡Œè½¯æç¤ºçš„é¢„è®­ç»ƒï¼Œä»¥åº”å¯¹ä¸åŒçš„å¤±çœŸï¼Œå¹¶åŠ é€Ÿå¯¹IQAä»»åŠ¡çš„é€‚åº”ã€‚</li>
<li>Quality-Aware Gradient Regularizationæ¨¡å—ç”¨äºå¾®è°ƒæ—¶çš„æ¢¯åº¦è°ƒæ•´ï¼Œä¸“æ³¨äºè´¨é‡ç›¸å…³ç‰¹å¾ï¼Œå¹¶é˜²æ­¢æ¨¡å‹å¯¹è¯­ä¹‰ä¿¡æ¯çš„è¿‡åº¦æ‹Ÿåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-31f17dd3a95d9767b2065689897ba3e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683776&auth_key=1760683776-0-0-c978b853562db9de589b15a557c60b7f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-18753b768098955f63874e7d27393054~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683784&auth_key=1760683784-0-0-fb86b38ca6b69ee014c345ca57ba743e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3e7a29ce985236334425024adb5228b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683790&auth_key=1760683790-0-0-5fcfc904a768b4c5bc89010ef20e5b92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f3ce1f63ae5a2696feb9d5abe2ad0a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683800&auth_key=1760683800-0-0-de95a74ab935d53a3a32c7e2c612091b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-39a272d1a6996c7909eceb952b972cec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683807&auth_key=1760683807-0-0-696beffb78ca90a72f3c11eb9469aa3b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  An Empirical Study on the Robustness of YOLO Models for Underwater   Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-478b037a8aac58cbad625b712dd4ef7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424653&auth_key=1760424653-0-0-6bead7ffdf2481265958b595f3f770cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  ChronoForge-RL Chronological Forging through Reinforcement Learning for   Enhanced Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
