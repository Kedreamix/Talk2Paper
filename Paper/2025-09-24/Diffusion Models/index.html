<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Seg4Diff Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16986v1/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-24-æ›´æ–°"><a href="#2025-09-24-æ›´æ–°" class="headerlink" title="2025-09-24 æ›´æ–°"></a>2025-09-24 æ›´æ–°</h1><h2 id="Seg4Diff-Unveiling-Open-Vocabulary-Segmentation-in-Text-to-Image-Diffusion-Transformers"><a href="#Seg4Diff-Unveiling-Open-Vocabulary-Segmentation-in-Text-to-Image-Diffusion-Transformers" class="headerlink" title="Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers"></a>Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image   Diffusion Transformers</h2><p><strong>Authors:Chaehyun Kim, Heeseong Shin, Eunbeen Hong, Heeji Yoon, Anurag Arnab, Paul Hongsuck Seo, Sunghwan Hong, Seungryong Kim</strong></p>
<p>Text-to-image diffusion models excel at translating language prompts into photorealistic images by implicitly grounding textual concepts through their cross-modal attention mechanisms. Recent multi-modal diffusion transformers extend this by introducing joint self-attention over concatenated image and text tokens, enabling richer and more scalable cross-modal alignment. However, a detailed understanding of how and where these attention maps contribute to image generation remains limited. In this paper, we introduce Seg4Diff (Segmentation for Diffusion), a systematic framework for analyzing the attention structures of MM-DiT, with a focus on how specific layers propagate semantic information from text to image. Through comprehensive analysis, we identify a semantic grounding expert layer, a specific MM-DiT block that consistently aligns text tokens with spatially coherent image regions, naturally producing high-quality semantic segmentation masks. We further demonstrate that applying a lightweight fine-tuning scheme with mask-annotated image data enhances the semantic grouping capabilities of these layers and thereby improves both segmentation performance and generated image fidelity. Our findings demonstrate that semantic grouping is an emergent property of diffusion transformers and can be selectively amplified to advance both segmentation and generation performance, paving the way for unified models that bridge visual perception and generation. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹é€šè¿‡å…¶è·¨æ¨¡æ€æ³¨æ„æœºåˆ¶éšå¼åœ°å°†æ–‡æœ¬æ¦‚å¿µä¸å›¾åƒç”Ÿæˆç›¸è”ç³»ï¼Œä»è€Œæ“…é•¿å°†è¯­è¨€æç¤ºè½¬åŒ–ä¸ºé€¼çœŸçš„å›¾åƒã€‚æœ€è¿‘çš„å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨é€šè¿‡åœ¨è¿æ¥çš„å›¾åƒå’Œæ–‡æœ¬æ ‡è®°ä¸Šå¼•å…¥è”åˆè‡ªæ³¨æ„åŠ›ï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†è¿™ä¸€åŠŸèƒ½ï¼Œå®ç°äº†æ›´ä¸°å¯Œã€æ›´å¯æ‰©å±•çš„è·¨æ¨¡æ€å¯¹é½ã€‚ç„¶è€Œï¼Œå¯¹äºè¿™äº›æ³¨æ„åŠ›å›¾å¦‚ä½•ä»¥åŠåœ¨ä½•å¤„æœ‰åŠ©äºå›¾åƒç”Ÿæˆçš„è¯¦ç»†ç†è§£ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Seg4Diffï¼ˆç”¨äºæ‰©æ•£çš„åˆ†å‰²ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆ†æMM-DiTæ³¨æ„åŠ›ç»“æ„çš„ç³»ç»Ÿæ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨ç‰¹å®šå±‚å¦‚ä½•å°†è¯­ä¹‰ä¿¡æ¯ä»æ–‡æœ¬ä¼ æ’­åˆ°å›¾åƒã€‚é€šè¿‡ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†è¯­ä¹‰å®šä½ä¸“å®¶å±‚ï¼Œè¿™æ˜¯ä¸€ä¸ªç‰¹å®šçš„MM-DiTå—ï¼Œèƒ½å¤ŸæŒç»­åœ°å°†æ–‡æœ¬æ ‡è®°ä¸ç©ºé—´è¿è´¯çš„å›¾åƒåŒºåŸŸå¯¹é½ï¼Œè‡ªç„¶åœ°äº§ç”Ÿé«˜è´¨é‡è¯­ä¹‰åˆ†å‰²æ©ç ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œä½¿ç”¨å¸¦æ©ç æ ‡æ³¨çš„å›¾åƒæ•°æ®è¿›è¡Œè½»é‡çº§å¾®è°ƒæ–¹æ¡ˆå¯ä»¥å¢å¼ºè¿™äº›å±‚çš„è¯­ä¹‰åˆ†ç»„èƒ½åŠ›ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½å’Œç”Ÿæˆçš„å›¾åƒä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰åˆ†ç»„æ˜¯æ‰©æ•£å˜å‹å™¨çš„å›ºæœ‰å±æ€§ï¼Œå¯ä»¥é’ˆå¯¹æ€§åœ°è¿›è¡Œå¢å¼ºä»¥æé«˜åˆ†å‰²å’Œç”Ÿæˆæ€§èƒ½ï¼Œä¸ºå»ºç«‹èåˆè§†è§‰æ„ŸçŸ¥å’Œç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18096v1">PDF</a> NeurIPS 2025. Project page: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/Seg4Diff/">https://cvlab-kaist.github.io/Seg4Diff/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶éšå¼åœ°å°†æ–‡æœ¬æ¦‚å¿µè½¬åŒ–ä¸ºå›¾åƒï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æœ€è¿‘çš„å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨é€šè¿‡å¼•å…¥è”åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹å›¾åƒå’Œæ–‡æœ¬æ ‡è®°è¿›è¡Œä¸²è”ï¼Œå®ç°äº†æ›´ä¸°å¯Œå’Œæ›´å¯æ‰©å±•çš„è·¨æ¨¡æ€å¯¹é½ã€‚ç„¶è€Œï¼Œå…³äºæ³¨æ„åŠ›å›¾å¦‚ä½•ä»¥åŠåœ¨å“ªé‡Œå¯¹å›¾åƒç”Ÿæˆåšå‡ºè´¡çŒ®çš„è¯¦ç»†ç†è§£ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºSeg4Diffï¼ˆç”¨äºæ‰©æ•£çš„åˆ†å‰²ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†æMM-DiTæ³¨æ„åŠ›ç»“æ„çš„ç³»ç»Ÿæ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨ç‰¹å®šå±‚å¦‚ä½•å°†è¯­ä¹‰ä¿¡æ¯ä»æ–‡æœ¬ä¼ æ’­åˆ°å›¾åƒã€‚é€šè¿‡ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†è¯­ä¹‰å®šä½ä¸“å®¶å±‚ï¼Œè¿™æ˜¯ä¸€ä¸ªç‰¹å®šçš„MM-DiTå—ï¼Œèƒ½å¤ŸæŒç»­åœ°å°†æ–‡æœ¬æ ‡è®°ä¸ç©ºé—´è¿è´¯çš„å›¾åƒåŒºåŸŸå¯¹é½ï¼Œè‡ªç„¶åœ°äº§ç”Ÿé«˜è´¨é‡è¯­ä¹‰åˆ†å‰²æ©æ¨¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œä½¿ç”¨å¸¦æ©ç æ³¨é‡Šçš„å›¾åƒæ•°æ®è¿›è¡Œè½»é‡çº§å¾®è°ƒå¯ä»¥å¢å¼ºè¿™äº›å±‚çš„è¯­ä¹‰åˆ†ç»„èƒ½åŠ›ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½å’Œç”Ÿæˆçš„å›¾åƒä¿çœŸåº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯­ä¹‰åˆ†ç»„æ˜¯æ‰©æ•£å˜å‹å™¨çš„å›ºæœ‰å±æ€§ï¼Œå¯ä»¥é’ˆå¯¹æ€§åœ°è¿›è¡Œå¢å¼ºä»¥æé«˜åˆ†å‰²å’Œç”Ÿæˆæ€§èƒ½ï¼Œä¸ºç»Ÿä¸€æ¨¡å‹é“ºå¹³é“è·¯ï¼Œå®ç°è§†è§‰æ„ŸçŸ¥å’Œç”Ÿæˆçš„èåˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å°†æ–‡æœ¬æ¦‚å¿µè½¬åŒ–ä¸ºå›¾åƒã€‚</li>
<li>å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨é€šè¿‡å¼•å…¥è”åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>Seg4Diffæ¡†æ¶ç”¨äºåˆ†æMM-DiTçš„æ³¨æ„åŠ›ç»“æ„ï¼Œæ­ç¤ºè¯­ä¹‰ä¿¡æ¯ä»æ–‡æœ¬åˆ°å›¾åƒçš„ä¼ æ’­è¿‡ç¨‹ã€‚</li>
<li>è¯†åˆ«å‡ºè¯­ä¹‰å®šä½ä¸“å®¶å±‚ï¼Œè¯¥å±‚èƒ½å°†æ–‡æœ¬æ ‡è®°ä¸ç©ºé—´è¿è´¯çš„å›¾åƒåŒºåŸŸå¯¹é½ã€‚</li>
<li>è¯­ä¹‰åˆ†ç»„æ˜¯æ‰©æ•£å˜å‹å™¨çš„å›ºæœ‰å±æ€§ï¼Œé€šè¿‡è½»é‡çº§å¾®è°ƒå¯æé«˜åˆ†å‰²å’Œç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>å¸¦æ©ç æ³¨é‡Šçš„å›¾åƒæ•°æ®å¯ç”¨äºå¢å¼ºæ¨¡å‹çš„è¯­ä¹‰åˆ†ç»„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18096v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18096v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18096v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18096v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ComposeMe-Attribute-Specific-Image-Prompts-for-Controllable-Human-Image-Generation"><a href="#ComposeMe-Attribute-Specific-Image-Prompts-for-Controllable-Human-Image-Generation" class="headerlink" title="ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image   Generation"></a>ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image   Generation</h2><p><strong>Authors:Guocheng Gordon Qian, Daniil Ostashev, Egor Nemchinov, Avihay Assouline, Sergey Tulyakov, Kuan-Chieh Jackson Wang, Kfir Aberman</strong></p>
<p>Generating high-fidelity images of humans with fine-grained control over attributes such as hairstyle and clothing remains a core challenge in personalized text-to-image synthesis. While prior methods emphasize identity preservation from a reference image, they lack modularity and fail to provide disentangled control over specific visual attributes. We introduce a new paradigm for attribute-specific image prompting, in which distinct sets of reference images are used to guide the generation of individual aspects of human appearance, such as hair, clothing, and identity. Our method encodes these inputs into attribute-specific tokens, which are injected into a pre-trained text-to-image diffusion model. This enables compositional and disentangled control over multiple visual factors, even across multiple people within a single image. To promote natural composition and robust disentanglement, we curate a cross-reference training dataset featuring subjects in diverse poses and expressions, and propose a multi-attribute cross-reference training strategy that encourages the model to generate faithful outputs from misaligned attribute inputs while adhering to both identity and textual conditioning. Extensive experiments show that our method achieves state-of-the-art performance in accurately following both visual and textual prompts. Our framework paves the way for more configurable human image synthesis by combining visual prompting with text-driven generation. Webpage is available at: <a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/">https://snap-research.github.io/composeme/</a>. </p>
<blockquote>
<p>åœ¨äººç±»é«˜ä¿çœŸå›¾åƒç”Ÿæˆä¸­ï¼Œå¯¹äºå‘å‹å’Œæœè£…ç­‰å±æ€§çš„ç²¾ç»†æ§åˆ¶ä»æ˜¯ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è™½ç„¶ä¹‹å‰çš„æ–¹æ³•å¼ºè°ƒä»å‚è€ƒå›¾åƒä¸­ä¿ç•™èº«ä»½ï¼Œä½†å®ƒä»¬ç¼ºä¹æ¨¡å—åŒ–ï¼Œæ— æ³•æä¾›å¯¹ç‰¹å®šè§†è§‰å±æ€§çš„åˆ†ç¦»æ§åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å±æ€§ç‰¹å®šå›¾åƒæç¤ºèŒƒå¼ï¼Œå…¶ä¸­ä½¿ç”¨ä¸åŒçš„å‚è€ƒå›¾åƒæ¥æŒ‡å¯¼äººç±»å¤–è§‚çš„å„ä¸ªæ–¹é¢ï¼Œå¦‚å¤´å‘ã€æœè£…å’Œèº«ä»½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¿™äº›è¾“å…¥ç¼–ç ä¸ºå±æ€§ç‰¹å®šä»¤ç‰Œï¼Œå¹¶æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ã€‚è¿™å®ç°å¯¹å¤šä¸ªè§†è§‰å› ç´ çš„ç»„åˆå’Œåˆ†ç¦»æ§åˆ¶ï¼Œç”šè‡³åœ¨å•ä¸ªå›¾åƒä¸­çš„å¤šäººä¹‹é—´ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†ä¿ƒè¿›è‡ªç„¶æ„å›¾å’Œç¨³å¥çš„åˆ†ç¦»ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªè·¨å‚è€ƒè®­ç»ƒæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å„ç§å§¿åŠ¿å’Œè¡¨æƒ…çš„ä¸»ä½“ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¤šå±æ€§è·¨å‚è€ƒè®­ç»ƒç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹åœ¨èº«ä»½å’Œæ–‡æœ¬æ¡ä»¶ä¸‹ä»é”™ä½å±æ€§è¾“å…¥ç”Ÿæˆå¿ å®è¾“å‡ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®éµå¾ªè§†è§‰å’Œæ–‡æœ¬æç¤ºæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ç»“åˆè§†è§‰æç¤ºå’Œæ–‡æœ¬é©±åŠ¨ç”Ÿæˆï¼Œä¸ºäººåƒåˆæˆé“ºå¹³äº†æ›´å¤šå¯é…ç½®çš„é“è·¯ã€‚ç½‘é¡µåœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/">https://snap-research.github.io/composeme/ã€‚</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18092v1">PDF</a> Accepted to SIGGRAPH Asia 2025, webpage:   <a target="_blank" rel="noopener" href="https://snap-research.github.io/composeme/">https://snap-research.github.io/composeme/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„å±æ€§ç‰¹å®šå›¾åƒæç¤ºæ–¹æ³•ï¼Œé€šè¿‡é‡‡ç”¨ä¸åŒå‚è€ƒå›¾åƒæ¥æŒ‡å¯¼äººç±»å¤–è§‚çš„å„ä¸ªæ–¹é¢ç”Ÿæˆï¼Œå¦‚å‘å‹ã€æœè£…å’Œèº«ä»½ç­‰ã€‚è¯¥æ–¹æ³•å°†è¾“å…¥ç¼–ç ä¸ºç‰¹å®šå±æ€§çš„æ ‡è®°ï¼Œå¹¶æ³¨å…¥é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ã€‚æ­¤æ–¹æ³•å®ç°äº†å¯¹å¤šä¸ªè§†è§‰å› ç´ çš„ç»„åˆå’Œè§£è€¦æ§åˆ¶ï¼Œç”šè‡³å¯ä»¥åœ¨å•ä¸ªå›¾åƒä¸­å¯¹å¤šä¸ªäººè¿›è¡Œæ­¤ç§æ§åˆ¶ã€‚é€šè¿‡æå‡ºä¸€ç§å¤šå±æ€§è·¨å‚è€ƒè®­ç»ƒç­–ç•¥ï¼Œæ¨¡å‹åœ¨äº§ç”Ÿå¯¹ä¸ä¸€è‡´å±æ€§è¾“å…¥å¿ å®å“åº”çš„åŒæ—¶ï¼Œä¹Ÿç¬¦åˆèº«ä»½å’Œæ–‡æœ¬æ¡ä»¶çš„è¦æ±‚ã€‚è¯¥æ¡†æ¶å®ç°äº†é«˜åº¦å¯é…ç½®çš„äººç±»å›¾åƒåˆæˆï¼Œé€šè¿‡è§†è§‰æç¤ºä¸æ–‡æœ¬é©±åŠ¨ç”Ÿæˆç›¸ç»“åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å±æ€§ç‰¹å®šå›¾åƒæç¤ºæ–¹æ³•ï¼Œä½¿ç”¨ä¸åŒå‚è€ƒå›¾åƒæ¥æŒ‡å¯¼äººç±»å¤–è§‚ä¸åŒæ–¹é¢çš„ç”Ÿæˆï¼ˆå‘å‹ã€æœè£…ã€èº«ä»½ç­‰ï¼‰ã€‚</li>
<li>å°†è¾“å…¥ç¼–ç ä¸ºç‰¹å®šå±æ€§çš„æ ‡è®°ï¼Œå¹¶æ³¨å…¥é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°ç»„åˆä¸è§£è€¦æ§åˆ¶å¤šä¸ªè§†è§‰å› ç´ çš„èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•å¯ä»¥åœ¨å•ä¸ªå›¾åƒä¸­å¯¹å¤šä¸ªäººè¿›è¡Œå±æ€§æ§åˆ¶ã€‚</li>
<li>é€šè¿‡æå‡ºä¸€ç§å¤šå±æ€§è·¨å‚è€ƒè®­ç»ƒç­–ç•¥ï¼Œæ¨¡å‹èƒ½åœ¨å¯¹ä¸ä¸€è‡´å±æ€§è¾“å…¥äº§ç”Ÿå¿ å®å“åº”çš„åŒæ—¶ï¼Œä¹Ÿç¬¦åˆèº«ä»½å’Œæ–‡æœ¬æ¡ä»¶çš„è¦æ±‚ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†é«˜åº¦å¯é…ç½®çš„äººç±»å›¾åƒåˆæˆã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†è§†è§‰æç¤ºä¸æ–‡æœ¬é©±åŠ¨ç”Ÿæˆï¼Œæé«˜äº†å›¾åƒç”Ÿæˆçš„çµæ´»æ€§å’Œé€¼çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18092v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18092v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18092v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18092v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18092v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.18092v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="StableGuard-Towards-Unified-Copyright-Protection-and-Tamper-Localization-in-Latent-Diffusion-Models"><a href="#StableGuard-Towards-Unified-Copyright-Protection-and-Tamper-Localization-in-Latent-Diffusion-Models" class="headerlink" title="StableGuard: Towards Unified Copyright Protection and Tamper   Localization in Latent Diffusion Models"></a>StableGuard: Towards Unified Copyright Protection and Tamper   Localization in Latent Diffusion Models</h2><p><strong>Authors:Haoxin Yang, Bangzhen Liu, Xuemiao Xu, Cheng Xu, Yuyang Yu, Zikai Huang, Yi Wang, Shengfeng He</strong></p>
<p>The advancement of diffusion models has enhanced the realism of AI-generated content but also raised concerns about misuse, necessitating robust copyright protection and tampering localization. Although recent methods have made progress toward unified solutions, their reliance on post hoc processing introduces considerable application inconvenience and compromises forensic reliability. We propose StableGuard, a novel framework that seamlessly integrates a binary watermark into the diffusion generation process, ensuring copyright protection and tampering localization in Latent Diffusion Models through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE) by equipping a pretrained Variational Autoencoder (VAE) with a lightweight latent residual-based adapter, enabling the generation of paired watermarked and watermark-free images. These pairs, fused via random masks, create a diverse dataset for training a tampering-agnostic forensic network. To further enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic Network (MoE-GFN) that dynamically integrates holistic watermark patterns, local tampering traces, and frequency-domain cues for precise watermark verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly optimized in a self-supervised, end-to-end manner, fostering a reciprocal training between watermark embedding and forensic accuracy. Extensive experiments demonstrate that StableGuard consistently outperforms state-of-the-art methods in image fidelity, watermark verification, and tampering localization. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æé«˜äº†äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„çœŸå®æ€§ï¼Œä½†ä¹Ÿå¼•å‘äº†å…³äºè¯¯ç”¨çš„æ‹…å¿§ï¼Œè¿™éœ€è¦è¿›è¡Œå¼ºæœ‰åŠ›çš„ç‰ˆæƒä¿æŠ¤å’Œç¯¡æ”¹å®šä½ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•åœ¨ç»Ÿä¸€è§£å†³æ–¹æ¡ˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬å¯¹äº‹åå¤„ç†çš„ä¾èµ–å¸¦æ¥äº†ç›¸å½“å¤§çš„åº”ç”¨ä¸ä¾¿å’Œé‰´å®šå¯é æ€§çš„å¦¥åã€‚æˆ‘ä»¬æå‡ºäº†StableGuardï¼Œè¿™æ˜¯ä¸€ä¸ªæ— ç¼é›†æˆäºŒè¿›åˆ¶æ°´å°åˆ°æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹çš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ç«¯åˆ°ç«¯è®¾è®¡ï¼Œç¡®ä¿åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ç‰ˆæƒä¿æŠ¤å’Œç¯¡æ”¹å®šä½ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šè·¯å¤ç”¨æ°´å°VAEï¼ˆMPW-VAEï¼‰ï¼Œå®ƒé€šè¿‡ä¸ºé¢„è®­ç»ƒçš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰é…å¤‡è½»é‡çº§çš„åŸºäºæ½œåœ¨æ®‹ç•™çš„é€‚é…å™¨ï¼Œèƒ½å¤Ÿç”Ÿæˆé…å¯¹çš„æ°´å°å’Œæ— æ°´å°å›¾åƒã€‚è¿™äº›é…å¯¹å›¾åƒé€šè¿‡éšæœºæ©æ¨¡èåˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå¯¹ç¯¡æ”¹æ— å…³çš„æ³•ç½‘ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºæ³•ç½‘ååŒä½œç”¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ··åˆä¸“å®¶æŒ‡å¯¼æ³•ç½‘ï¼ˆMoE-GFNï¼‰ï¼Œå®ƒåŠ¨æ€åœ°æ•´åˆæ•´ä½“æ°´å°æ¨¡å¼ã€å±€éƒ¨ç¯¡æ”¹ç—•è¿¹å’Œé¢‘åŸŸçº¿ç´¢ï¼Œè¿›è¡Œç²¾ç¡®çš„æ°´å°éªŒè¯å’Œç¯¡æ”¹åŒºåŸŸæ£€æµ‹ã€‚MPW-VAEå’ŒMoE-GFNä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ä¿ƒè¿›æ°´å°åµŒå…¥ä¸é‰´å®šå‡†ç¡®æ€§ä¹‹é—´çš„äº’æƒ è®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒStableGuardåœ¨å›¾åƒä¿çœŸåº¦ã€æ°´å°éªŒè¯å’Œç¯¡æ”¹å®šä½æ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17993v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æé«˜äº†AIç”Ÿæˆå†…å®¹çš„é€¼çœŸåº¦ï¼Œä½†ä¹Ÿå¼•å‘äº†å…³äºè¯¯ç”¨çš„æ‹…å¿§ï¼Œéœ€è¦å¼ºå¤§çš„ç‰ˆæƒä¿æŠ¤å’Œå¹²æ‰°å®šä½ã€‚æˆ‘ä»¬æå‡ºStableGuardï¼Œä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå°†äºŒè¿›åˆ¶æ°´å°æ— ç¼é›†æˆåˆ°æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ç«¯åˆ°ç«¯è®¾è®¡ç¡®ä¿æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç‰ˆæƒä¿æŠ¤å’Œå¹²æ‰°å®šä½ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šè·¯å¤ç”¨æ°´å°å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMPW-VAEï¼‰ï¼Œå®ƒé€šè¿‡ä¸ºé¢„è®­ç»ƒçš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰é…å¤‡è½»é‡çº§çš„æ½œåœ¨æ®‹å·®é€‚é…å™¨ï¼Œç”Ÿæˆé…å¯¹çš„æ°´å°å’Œæ— æ°´å°å›¾åƒã€‚è¿™äº›é…å¯¹é€šè¿‡éšæœºæ©æ¨¡èåˆï¼Œåˆ›å»ºä¸€ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå¯¹å¹²æ‰°æ— å…³çš„æ³•ç½‘ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºæ³•ç½‘ååŒä½œç”¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“å®¶å¼•å¯¼æ³•ç½‘æ··åˆç‰©ï¼ˆMoE-GFNï¼‰ï¼Œå®ƒåŠ¨æ€é›†æˆæ•´ä½“æ°´å°æ¨¡å¼ã€å±€éƒ¨å¹²æ‰°ç—•è¿¹å’Œé¢‘åŸŸçº¿ç´¢ï¼Œè¿›è¡Œç²¾ç¡®çš„æ°´å°éªŒè¯å’Œå¹²æ‰°åŒºåŸŸæ£€æµ‹ã€‚MPW-VAEå’ŒMoE-GFNä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œç«¯åˆ°ç«¯åœ°ä¿ƒè¿›æ°´å°åµŒå…¥ä¸æ³•ç½‘å‡†ç¡®æ€§çš„ç›¸äº’è®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒStableGuardåœ¨å›¾åƒä¿çœŸåº¦ã€æ°´å°éªŒè¯å’Œå¹²æ‰°å®šä½æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æé«˜äº†AIç”Ÿæˆå†…å®¹çš„çœŸå®æ„Ÿï¼Œä½†ä¹Ÿéœ€è¦åº”å¯¹è¯¯ç”¨é—®é¢˜ï¼ŒåŒ…æ‹¬ç‰ˆæƒä¿æŠ¤å’Œå¹²æ‰°å®šä½çš„éœ€æ±‚ã€‚</li>
<li>StableGuardæ¡†æ¶é€šè¿‡ç«¯åˆ°ç«¯è®¾è®¡æ— ç¼é›†æˆäºŒè¿›åˆ¶æ°´å°åˆ°æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚</li>
<li>MPW-VAEé€šè¿‡é…å¤‡è½»é‡çº§æ½œåœ¨æ®‹å·®é€‚é…å™¨ç”Ÿæˆé…å¯¹çš„æ°´å°å’Œæ— æ°´å°å›¾åƒã€‚</li>
<li>é€šè¿‡éšæœºæ©æ¨¡èåˆç”Ÿæˆå¤šæ ·åŒ–æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå¯¹å¹²æ‰°æ— å…³çš„æ³•ç½‘ã€‚</li>
<li>MoE-GFNç»“åˆæ•´ä½“æ°´å°æ¨¡å¼ã€å±€éƒ¨å¹²æ‰°ç—•è¿¹å’Œé¢‘åŸŸçº¿ç´¢è¿›è¡Œç²¾ç¡®æ°´å°éªŒè¯å’Œå¹²æ‰°åŒºåŸŸæ£€æµ‹ã€‚</li>
<li>MPW-VAEå’ŒMoE-GFNä»¥è‡ªæˆ‘ç›‘ç£æ–¹å¼è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œä¿ƒè¿›æ°´å°åµŒå…¥ä¸æ³•ç½‘å‡†ç¡®æ€§çš„ç›¸äº’æå‡ã€‚</li>
<li>StableGuardåœ¨å›¾åƒä¿çœŸåº¦ã€æ°´å°éªŒè¯å’Œå¹²æ‰°å®šä½æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17993v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17993v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17993v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantic-and-Visual-Crop-Guided-Diffusion-Models-for-Heterogeneous-Tissue-Synthesis-in-Histopathology"><a href="#Semantic-and-Visual-Crop-Guided-Diffusion-Models-for-Heterogeneous-Tissue-Synthesis-in-Histopathology" class="headerlink" title="Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous   Tissue Synthesis in Histopathology"></a>Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous   Tissue Synthesis in Histopathology</h2><p><strong>Authors:Saghir Alfasly, Wataru Uegami, MD Enamul Hoq, Ghazal Alabtah, H. R. Tizhoosh</strong></p>
<p>Synthetic data generation in histopathology faces unique challenges: preserving tissue heterogeneity, capturing subtle morphological features, and scaling to unannotated datasets. We present a latent diffusion model that generates realistic heterogeneous histopathology images through a novel dual-conditioning approach combining semantic segmentation maps with tissue-specific visual crops. Unlike existing methods that rely on text prompts or abstract visual embeddings, our approach preserves critical morphological details by directly incorporating raw tissue crops from corresponding semantic regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we introduce a self-supervised extension that clusters whole-slide images into 100 tissue types using foundation model embeddings, automatically generating pseudo-semantic maps for training. Our method synthesizes high-fidelity images with precise region-wise annotations, achieving superior performance on downstream segmentation tasks. When evaluated on annotated datasets, models trained on our synthetic data show competitive performance to those trained on real data, demonstrating the utility of controlled heterogeneous tissue generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within 1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA whole-slide images without manual annotations, our framework offers a practical solution for an urgent need for generating diverse, annotated histopathology data, addressing a critical bottleneck in computational pathology. </p>
<blockquote>
<p>åœ¨ç—…ç†å­¦ä¸­å¯¹åˆæˆæ•°æ®ç”Ÿæˆé¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå¦‚ä¿æŒç»„ç»‡å¼‚è´¨æ€§ã€æ•æ‰å¾®å¦™çš„å½¢æ€ç‰¹å¾å’Œæ‰©å±•åˆ°æœªæ ‡æ³¨çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¸€ç§æ–°é¢–çš„åŒé‡æ¡ä»¶æ–¹æ³•ç»“åˆè¯­ä¹‰åˆ†å‰²åœ°å›¾å’Œç‰¹å®šç»„ç»‡çš„è§†è§‰è£å‰ªï¼Œç”Ÿæˆé€¼çœŸçš„å¼‚è´¨æ€§ç—…ç†å­¦å›¾åƒã€‚ä¸åŒäºä¾èµ–æ–‡æœ¬æç¤ºæˆ–æŠ½è±¡è§†è§‰åµŒå…¥çš„ç°æœ‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç›´æ¥ç»“åˆæ¥è‡ªç›¸åº”è¯­ä¹‰åŒºåŸŸçš„åŸå§‹ç»„ç»‡è£å‰ªï¼Œä¿ç•™äº†å…³é”®çš„å½¢æ€ç»†èŠ‚ã€‚å¯¹äºå·²æ ‡æ³¨çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚Camelyon16å’ŒPandaï¼‰ï¼Œæˆ‘ä»¬æå–æ–‘å—ï¼Œç¡®ä¿20-80%çš„ç»„ç»‡å¼‚è´¨æ€§ã€‚å¯¹äºæœªæ ‡æ³¨çš„æ•°æ®ï¼ˆä¾‹å¦‚TCGAï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªç›‘ç£æ‰©å±•ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹åµŒå…¥å°†æ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒèšç±»ä¸º100ç§ç»„ç»‡ç±»å‹ï¼Œè‡ªåŠ¨ç”Ÿæˆä¼ªè¯­ä¹‰åœ°å›¾è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆæˆé«˜ä¿çœŸå›¾åƒï¼Œå…·æœ‰ç²¾ç¡®çš„åŒºåŸŸæ³¨é‡Šï¼Œå¹¶åœ¨ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚åœ¨å·²æ ‡æ³¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç»æˆ‘ä»¬çš„åˆæˆæ•°æ®è®­ç»ƒæ¨¡å‹çš„è¡¨ç°ä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œè¯æ˜äº†å¯æ§å¼‚è´¨æ€§ç»„ç»‡ç”Ÿæˆçš„å®ç”¨æ€§ã€‚åœ¨å®šé‡è¯„ä¼°ä¸­ï¼Œæç¤ºå¼•å¯¼çš„åˆæˆå°†Camelyon16ä¸Šçš„Frechetè·ç¦»å‡å°‘äº†é«˜è¾¾6å€ï¼ˆä»430.1é™è‡³72.0ï¼‰ï¼Œå¹¶ä¸”åœ¨Pandaå’ŒTCGAä¸Šçš„FDé™ä½äº†2-3å€ã€‚ä»…ç»åˆæˆæ•°æ®è®­ç»ƒçš„ä¸‹æ¸¸DeepLabv3+æ¨¡å‹åœ¨Camelyon16å’ŒPandaä¸Šçš„æµ‹è¯•IoUè¾¾åˆ°0.71å’Œ0.95ï¼Œä¸çœŸå®æ•°æ®åŸºå‡†æµ‹è¯•å€¼ï¼ˆåˆ†åˆ«ä¸º0.72å’Œ0.96ï¼‰ç›¸å·®ä»…1-2%ã€‚é€šè¿‡æ‰©å±•åˆ°æ²¡æœ‰æ‰‹åŠ¨æ ‡æ³¨çš„TCGAçš„11,765å¼ å…¨å¹»ç¯ç‰‡å›¾åƒï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ºè§£å†³è¿«åˆ‡éœ€æ±‚çš„ç”Ÿæˆå¤šæ ·ã€æ ‡æ³¨çš„ç—…ç†å­¦æ•°æ®æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†è®¡ç®—ç—…ç†å­¦ä¸­çš„å…³é”®ç“¶é¢ˆé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17847v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºç”ŸæˆçœŸå®ä¸”å¼‚è´¨æ€§çš„ç—…ç†å›¾åƒã€‚è¯¥æ–¹æ³•ç»“åˆäº†è¯­ä¹‰åˆ†å‰²å›¾å’Œç‰¹å®šç»„ç»‡çš„è§†è§‰è£å‰ªï¼Œé€šè¿‡ä¸€ç§æ–°çš„åŒé‡æ¡ä»¶æ–¹æ³•ç”Ÿæˆå›¾åƒã€‚å¯¹äºå¸¦æ³¨é‡Šçš„æ•°æ®é›†ï¼Œé€šè¿‡æå–åŒ…å«20-80%ç»„ç»‡å¼‚è´¨æ€§çš„è¡¥ä¸è¿›è¡Œè®­ç»ƒï¼›å¯¹äºæœªæ³¨é‡Šçš„æ•°æ®é›†ï¼Œåˆ™ä½¿ç”¨è‡ªç›‘ç£æ‰©å±•æ–¹æ³•è‡ªåŠ¨äº§ç”Ÿä¼ªè¯­ä¹‰å›¾è¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•åˆæˆçš„å›¾åƒå…·æœ‰é«˜ä¿çœŸåº¦å’Œç²¾ç¡®çš„åŒºåŸŸæ³¨é‡Šï¼Œå¹¶åœ¨ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç»è¿‡è¯„ä¼°ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨å¸¦æ³¨é‡Šæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚è¿™ä¸ºæ€¥éœ€ç”Ÿæˆå¤šæ ·åŒ–å’Œå¸¦æ³¨é‡Šçš„ç—…ç†æ•°æ®æä¾›äº†ä¸€ä¸ªå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ç—…ç†å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿç”ŸæˆçœŸå®ä¸”å¼‚è´¨æ€§çš„ç—…ç†å›¾åƒã€‚</li>
<li>é€šè¿‡ç»“åˆè¯­ä¹‰åˆ†å‰²å›¾å’Œç‰¹å®šç»„ç»‡çš„è§†è§‰è£å‰ªï¼Œé‡‡ç”¨åŒé‡æ¡ä»¶æ–¹æ³•ç”Ÿæˆå›¾åƒï¼Œä¿ç•™äº†å…³é”®çš„å½¢æ€ç»†èŠ‚ã€‚</li>
<li>æ–¹æ³•å¯ä»¥å¤„ç†å¸¦æ³¨é‡Šå’Œæœªæ³¨é‡Šçš„æ•°æ®é›†ï¼Œå¯¹äºæœªæ³¨é‡Šçš„æ•°æ®é›†ï¼Œä½¿ç”¨è‡ªç›‘ç£æ‰©å±•æ–¹æ³•è‡ªåŠ¨äº§ç”Ÿä¼ªè¯­ä¹‰å›¾è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åˆæˆå›¾åƒå…·æœ‰é«˜ä¿çœŸåº¦å’Œç²¾ç¡®çš„åŒºåŸŸæ³¨é‡Šï¼Œåœ¨ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç»è¿‡è¯„ä¼°ï¼Œåˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨å¸¦æ³¨é‡Šæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†è®¡ç®—ç—…ç†å­¦ä¸­æ€¥éœ€ç”Ÿæˆå¤šæ ·åŒ–å’Œå¸¦æ³¨é‡Šçš„ç—…ç†æ•°æ®çš„ç“¶é¢ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17847v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17847v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17847v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17847v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Elucidating-the-Design-Space-of-FP4-training"><a href="#Elucidating-the-Design-Space-of-FP4-training" class="headerlink" title="Elucidating the Design Space of FP4 training"></a>Elucidating the Design Space of FP4 training</h2><p><strong>Authors:Robert Hu, Carlo Luschi, Paul Balanca</strong></p>
<p>The increasing computational demands of foundation models have spurred research into low-precision training, with 4-bit floating-point (\texttt{FP4}) formats emerging as a frontier for maximizing hardware throughput. While numerous techniques have been proposed to stabilize \texttt{FP4} training, they often present isolated solutions with varying, and not always clear, computational overheads. This paper aims to provide a unified view of the design space of \texttt{FP4} training. We introduce a comprehensive, quantisation gradient-based framework for microscaling quantization that allows for a theoretical analysis of the computational costs associated with different stabilization methods on both the forward and backward passes. Using a simulator built on this framework, we conduct an extensive empirical study across a wide range of machine learning tasks, including regression, image classification, diffusion models, and language models. By systematically evaluating thousands of combinations of techniques, such as novel gradient approximations, rounding strategies, and scaling methods, we identify which configurations offer the most favourable performance-to-overhead trade-off. We find that the techniques enabling the best trade-off involve carefully combining Hadamard transformations, tensor scaling and stochastic rounding. We further find that using \texttt{UE5M3} as a scaling factor potentially offers a good compromise between range and precision with manageable computational overhead. </p>
<blockquote>
<p>éšç€åŸºç¡€æ¨¡å‹è®¡ç®—éœ€æ±‚çš„ä¸æ–­å¢åŠ ï¼Œä½ç²¾åº¦è®­ç»ƒç ”ç©¶åº”è¿è€Œç”Ÿï¼Œè€Œ4ä½æµ®ç‚¹æ•°ï¼ˆFP4ï¼‰æ ¼å¼ä½œä¸ºæœ€å¤§åŒ–ç¡¬ä»¶ååé‡çš„å‰æ²¿æŠ€æœ¯å´­éœ²å¤´è§’ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šæŠ€æœ¯æ¥ç¨³å®šFP4è®­ç»ƒï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯å­¤ç«‹çš„è§£å†³æ–¹æ¡ˆï¼Œè®¡ç®—å¼€é”€å„ä¸ç›¸åŒï¼Œå¹¶ä¸æ€»æ˜¯æ¸…æ™°æ˜äº†ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºFP4è®­ç»ƒçš„è®¾è®¡ç©ºé—´æä¾›ä¸€ä¸ªç»Ÿä¸€è§†å›¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„ã€åŸºäºé‡åŒ–æ¢¯åº¦çš„å¾®å°ºåº¦é‡åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å¯¹æ­£å‘å’Œåå‘ä¼ é€’ä¸­ä¸åŒç¨³å®šæ–¹æ³•ä¸è®¡ç®—æˆæœ¬è¿›è¡Œç†è®ºåˆ†æã€‚ä½¿ç”¨åŸºäºæ­¤æ¡†æ¶æ„å»ºçš„æ¨¡æ‹Ÿå™¨ï¼Œæˆ‘ä»¬å¯¹ä¸€ç³»åˆ—å¹¿æ³›çš„æœºå™¨å­¦ä¹ ä»»åŠ¡è¿›è¡Œäº†å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼ŒåŒ…æ‹¬å›å½’ã€å›¾åƒåˆ†ç±»ã€æ‰©æ•£æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°æ•°åƒç§æŠ€æœ¯ç»„åˆï¼Œä¾‹å¦‚æ–°å‹æ¢¯åº¦é€¼è¿‘ã€èˆå…¥ç­–ç•¥å’Œè°ƒæ•´æ–¹æ³•ï¼Œæˆ‘ä»¬ç¡®å®šäº†å“ªäº›é…ç½®æä¾›äº†æœ€æœ‰åˆ©äºæ€§èƒ½ä¸å¼€é”€ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬å‘ç°ï¼Œèƒ½å¤Ÿå®ç°æœ€ä½³æƒè¡¡çš„æŠ€æœ¯æ¶‰åŠç²¾å¿ƒç»“åˆå“ˆè¾¾ç›å˜æ¢ã€å¼ é‡ç¼©æ”¾å’Œéšæœºèˆå…¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°ï¼Œä½¿ç”¨UE5M3ä½œä¸ºè°ƒæ•´å› å­å¯èƒ½åœ¨èŒƒå›´å’Œç²¾åº¦ä¹‹é—´æä¾›è‰¯å¥½çš„å¹³è¡¡ï¼ŒåŒæ—¶è®¡ç®—å¼€é”€å¯æ§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17791v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½ç²¾åº¦è®­ç»ƒä¸­çš„FP4æ ¼å¼ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé‡åŒ–æ¢¯åº¦çš„å¾®å°ºåº¦é‡åŒ–ç»¼åˆæ¡†æ¶ï¼Œç”¨äºå¯¹å‰å‘å’Œåå‘ä¼ æ’­ä¸­ä¸åŒç¨³å®šæ–¹æ³•çš„è®¡ç®—æˆæœ¬è¿›è¡Œç†è®ºåˆ†æã€‚é€šè¿‡æ¨¡æ‹Ÿå™¨å¯¹å›å½’ã€å›¾åƒåˆ†ç±»ã€æ‰©æ•£æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ç­‰å¹¿æ³›æœºå™¨å­¦ä¹ ä»»åŠ¡è¿›è¡Œå¤§é‡å®è¯ç ”ç©¶ï¼Œç³»ç»Ÿè¯„ä¼°äº†å„ç§æŠ€æœ¯ç»„åˆçš„æ€§èƒ½ä¸å¼€é”€æƒè¡¡ï¼Œå‘ç°æœ€ä½³çš„ç»„åˆç­–ç•¥åŒ…æ‹¬ç»“åˆHadamardå˜æ¢ã€å¼ é‡å°ºåº¦å’Œéšæœºèˆå…¥ç­‰æŠ€æœ¯ï¼Œå¹¶ä½¿ç”¨UE5M3ä½œä¸ºå°ºåº¦å› å­åœ¨èŒƒå›´å’Œç²¾åº¦ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†ä½ç²¾åº¦è®­ç»ƒä¸­çš„FP4æ ¼å¼ï¼Œè¿™æ˜¯ä¸ºäº†åº”å¯¹åŸºç¡€æ¨¡å‹å¯¹è®¡ç®—èƒ½åŠ›çš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºé‡åŒ–æ¢¯åº¦çš„å¾®å°ºåº¦é‡åŒ–ç»¼åˆæ¡†æ¶ï¼Œç”¨äºç†è®ºåˆ†æä¸åŒç¨³å®šæ–¹æ³•çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿå™¨è¿›è¡Œäº†å¤§é‡å®è¯ç ”ç©¶ï¼Œæ¶µç›–äº†å¤šç§æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°äº†åŒ…æ‹¬æ–°å‹æ¢¯åº¦è¿‘ä¼¼ã€èˆå…¥ç­–ç•¥åŠå°ºåº¦æ–¹æ³•ç­‰åœ¨å†…çš„æŠ€æœ¯ç»„åˆã€‚</li>
<li>å‘ç°æœ€ä½³çš„ç»„åˆç­–ç•¥æ¶‰åŠç»“åˆç‰¹å®šæŠ€æœ¯ï¼Œå¦‚Hadamardå˜æ¢ã€å¼ é‡å°ºåº¦å’Œéšæœºèˆå…¥ã€‚</li>
<li>ä½¿ç”¨UE5M3ä½œä¸ºå°ºåº¦å› å­åœ¨èŒƒå›´å’Œç²¾åº¦ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17791v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17791v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17791v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17791v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SISMA-Semantic-Face-Image-Synthesis-with-Mamba"><a href="#SISMA-Semantic-Face-Image-Synthesis-with-Mamba" class="headerlink" title="SISMA: Semantic Face Image Synthesis with Mamba"></a>SISMA: Semantic Face Image Synthesis with Mamba</h2><p><strong>Authors:Filippo Botti, Alex Ergasti, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati</strong></p>
<p>Diffusion Models have become very popular for Semantic Image Synthesis (SIS) of human faces. Nevertheless, their training and inference is computationally expensive and their computational requirements are high due to the quadratic complexity of attention layers. In this paper, we propose a novel architecture called SISMA, based on the recently proposed Mamba. SISMA generates high quality samples by controlling their shape using a semantic mask at a reduced computational demand. We validated our approach through comprehensive experiments with CelebAMask-HQ, revealing that our architecture not only achieves a better FID score yet also operates at three times the speed of state-of-the-art architectures. This indicates that the proposed design is a viable, lightweight substitute to transformer-based models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰å·²ç»åœ¨äººç±»é¢éƒ¨çš„è¯­ä¹‰å›¾åƒåˆæˆï¼ˆSISï¼‰é¢†åŸŸå˜å¾—éå¸¸å—æ¬¢è¿ã€‚ç„¶è€Œï¼Œç”±äºæ³¨æ„åŠ›å±‚çš„äºŒæ¬¡å¤æ‚æ€§ï¼Œå…¶è®­ç»ƒå’Œæ¨ç†çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ä¸”è®¡ç®—è¦æ±‚ä¹Ÿå¾ˆé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°æå‡ºçš„Mambaçš„æ–°å‹æ¶æ„ï¼Œç§°ä¸ºSISMAã€‚SISMAé€šè¿‡åˆ©ç”¨è¯­ä¹‰æ©è†œæ§åˆ¶å½¢çŠ¶ç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ï¼Œå¹¶åœ¨å‡å°‘è®¡ç®—éœ€æ±‚çš„æƒ…å†µä¸‹è¿›è¡Œã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨CelebAMask-HQè¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¶æ„ä¸ä»…å®ç°äº†æ›´å¥½çš„FIDå¾—åˆ†ï¼Œè€Œä¸”è¿è¡Œé€Ÿåº¦æ˜¯ç°æœ‰æŠ€æœ¯æ¶æ„çš„ä¸‰å€ã€‚è¿™è¡¨æ˜æ‰€æå‡ºçš„è®¾è®¡æ˜¯ä¸€ä¸ªå¯è¡Œçš„ã€è½»é‡çº§çš„æ›¿ä»£åŸºäºå˜å‹å™¨æ¨¡å‹çš„é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17651v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºMambaçš„æ–°å‹æ¶æ„SISMAï¼Œç”¨äºè¯­ä¹‰å›¾åƒåˆæˆï¼ˆSISï¼‰ä¸­çš„äººè„¸ç”Ÿæˆã€‚SISMAé€šè¿‡è¯­ä¹‰æ©è†œæ§åˆ¶å½¢çŠ¶ï¼Œé™ä½è®¡ç®—éœ€æ±‚ï¼Œç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒSISMAä¸ä»…å®ç°äº†æ›´å¥½çš„FIDå¾—åˆ†ï¼Œè€Œä¸”è¿è¡Œé€Ÿåº¦æ˜¯ç°æœ‰æŠ€æœ¯æ¶æ„çš„ä¸‰å€ï¼Œæˆä¸ºå¯è¡Œçš„è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Modelsåœ¨äººè„¸è¯­ä¹‰å›¾åƒåˆæˆï¼ˆSISï¼‰ä¸­å¾ˆå—æ¬¢è¿ã€‚</li>
<li>ç°æœ‰Diffusion Modelsç”±äºæ³¨æ„åŠ›å±‚çš„äºŒæ¬¡å¤æ‚æ€§ï¼Œå…¶è®­ç»ƒå’Œæ¨ç†è®¡ç®—é‡å¤§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„SISMAï¼ŒåŸºäºMambaï¼Œç”¨äºé«˜è´¨é‡æ ·æœ¬ç”Ÿæˆã€‚</li>
<li>SISMAé€šè¿‡è¯­ä¹‰æ©è†œæ§åˆ¶å½¢çŠ¶ï¼Œé™ä½è®¡ç®—éœ€æ±‚ã€‚</li>
<li>SISMAåœ¨CelebAMask-HQä¸Šçš„å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>SISMAå®ç°äº†æ›´å¥½çš„FIDå¾—åˆ†ï¼Œè¿è¡Œé€Ÿåº¦æ˜¯ç°æœ‰æŠ€æœ¯æ¶æ„çš„ä¸‰å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17651v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17651v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17651v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CARINOX-Inference-time-Scaling-with-Category-Aware-Reward-based-Initial-Noise-Optimization-and-Exploration"><a href="#CARINOX-Inference-time-Scaling-with-Category-Aware-Reward-based-Initial-Noise-Optimization-and-Exploration" class="headerlink" title="CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial   Noise Optimization and Exploration"></a>CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial   Noise Optimization and Exploration</h2><p><strong>Authors:Seyed Amir Kasaei, Ali Aghayari, Arash Marioriyad, Niki Sepasian, Shayan Baghayi Nejad, MohammadAmin Fazli, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban</strong></p>
<p>Text-to-image diffusion models, such as Stable Diffusion, can produce high-quality and diverse images but often fail to achieve compositional alignment, particularly when prompts describe complex object relationships, attributes, or spatial arrangements. Recent inference-time approaches address this by optimizing or exploring the initial noise under the guidance of reward functions that score text-image alignment without requiring model fine-tuning. While promising, each strategy has intrinsic limitations when used alone: optimization can stall due to poor initialization or unfavorable search trajectories, whereas exploration may require a prohibitively large number of samples to locate a satisfactory output. Our analysis further shows that neither single reward metrics nor ad-hoc combinations reliably capture all aspects of compositionality, leading to weak or inconsistent guidance. To overcome these challenges, we present Category-Aware Reward-based Initial Noise Optimization and Exploration (CARINOX), a unified framework that combines noise optimization and exploration with a principled reward selection procedure grounded in correlation with human judgments. Evaluations on two complementary benchmarks covering diverse compositional challenges show that CARINOX raises average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS benchmark, consistently outperforming state-of-the-art optimization and exploration-based methods across all major categories, while preserving image quality and diversity. The project page is available at <a target="_blank" rel="noopener" href="https://amirkasaei.com/carinox/%7Bthis">https://amirkasaei.com/carinox/{this</a> URL}. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå¦‚Stable Diffusionï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„å›¾åƒï¼Œä½†å¾€å¾€éš¾ä»¥å®ç°ç»„æˆå¯¹é½ï¼Œç‰¹åˆ«æ˜¯åœ¨æç¤ºæè¿°å¤æ‚çš„å¯¹è±¡å…³ç³»ã€å±æ€§æˆ–ç©ºé—´æ’åˆ—æ—¶ã€‚æœ€è¿‘çš„æ¨ç†æ—¶é—´æ–¹æ³•é€šè¿‡ä¼˜åŒ–æˆ–æ¢ç´¢åˆå§‹å™ªå£°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨å¥–åŠ±å‡½æ•°çš„æŒ‡å¯¼ä¸‹è¯„åˆ†æ–‡æœ¬å›¾åƒå¯¹é½ï¼Œè€Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è™½ç„¶å‰æ™¯å¹¿é˜”ï¼Œä½†æ¯ç§ç­–ç•¥å•ç‹¬ä½¿ç”¨æ—¶éƒ½æœ‰å›ºæœ‰çš„å±€é™æ€§ï¼šä¼˜åŒ–å¯èƒ½ä¼šå› åˆå§‹åŒ–ä¸ä½³æˆ–æœç´¢è½¨è¿¹ä¸åˆ©è€Œé™·å…¥åƒµå±€ï¼Œè€Œæ¢ç´¢å¯èƒ½éœ€è¦å¤§é‡çš„æ ·æœ¬æ‰èƒ½æ‰¾åˆ°æ»¡æ„çš„è¾“å‡ºã€‚æˆ‘ä»¬çš„è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæ— è®ºæ˜¯å•ä¸€å¥–åŠ±æŒ‡æ ‡è¿˜æ˜¯ä¸´æ—¶ç»„åˆï¼Œéƒ½æ— æ³•å¯é åœ°æ•è·æ‰€æœ‰æ–¹é¢çš„ç»„åˆæ€§ï¼Œå¯¼è‡´æŒ‡å¯¼ä¸åŠ›æˆ–ä¸ä¸€è‡´ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç±»åˆ«çš„å¥–åŠ±åˆå§‹å™ªå£°ä¼˜åŒ–å’Œæ¢ç´¢ï¼ˆCARINOXï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å™ªå£°ä¼˜åŒ–å’Œæ¢ç´¢çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé‡‡ç”¨åŸºäºä¸äººç±»åˆ¤æ–­ç›¸å…³çš„åŸåˆ™æ€§å¥–åŠ±é€‰æ‹©ç¨‹åºã€‚åœ¨ä¸¤ä¸ªäº’è¡¥çš„åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ï¼Œæ¶µç›–äº†å„ç§ç»„æˆæŒ‘æˆ˜ï¼Œè¡¨æ˜CARINOXåœ¨T2I-CompBench++ä¸Šå¹³å‡å¯¹é½å¾—åˆ†æé«˜äº†+16%ï¼Œåœ¨HRSåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†+11%ï¼Œåœ¨æ‰€æœ‰ä¸»è¦ç±»åˆ«ä¸­ä¸€è‡´åœ°ä¼˜äºæœ€æ–°çš„ä¼˜åŒ–å’Œæ¢ç´¢æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://amirkasaei.com/carinox/">æ­¤URL</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17458v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°çš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å¤æ‚å¯¹è±¡å…³ç³»ã€å±æ€§æˆ–ç©ºé—´æ’åˆ—çš„æç¤ºæ—¶ï¼Œå¾€å¾€éš¾ä»¥å®ç°ç»„æˆå¯¹é½ã€‚æœ€æ–°æ¨æ–­æ—¶é—´æ–¹æ³•é€šè¿‡ä¼˜åŒ–æˆ–æ¢ç´¢åˆå§‹å™ªå£°æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œåœ¨æ— éœ€æ¨¡å‹å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è¯„åˆ†æ–‡æœ¬å›¾åƒå¯¹é½çš„å¥–åŠ±å‡½æ•°æ¥æŒ‡å¯¼ã€‚ç„¶è€Œï¼Œæ¯ç§ç­–ç•¥éƒ½æœ‰å…¶å±€é™æ€§ï¼šä¼˜åŒ–å¯èƒ½å› ä¸è‰¯åˆå§‹åŒ–æˆ–ä¸åˆ©æœç´¢è½¨è¿¹è€Œé™·å…¥åƒµå±€ï¼Œè€Œæ¢ç´¢å¯èƒ½éœ€è¦å¤§é‡æ ·æœ¬æ‰èƒ½æ‰¾åˆ°æ»¡æ„çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œå•ä¸€çš„å¥–åŠ±æŒ‡æ ‡æˆ–ä¸´æ—¶ç»„åˆæ— æ³•å…¨é¢æ•æ‰ç»„æˆæ€§çš„å„ä¸ªæ–¹é¢ï¼Œå¯¼è‡´æŒ‡å¯¼ä¸åŠ›æˆ–ä¸ä¸€è‡´ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºç±»åˆ«çš„å¥–åŠ±åˆå§‹å™ªå£°ä¼˜åŒ–å’Œæ¢ç´¢ï¼ˆCARINOXï¼‰çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç»“åˆä¼˜åŒ–å’Œæ¢ç´¢ï¼Œå¹¶åŸºäºä¸äººç±»åˆ¤æ–­ç›¸å…³çš„å¥–åŠ±é€‰æ‹©ç¨‹åºã€‚è¯„ä¼°è¡¨æ˜ï¼ŒCARINOXåœ¨T2I-CompBench++å’ŒHRSåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å¯¹é½å¾—åˆ†åˆ†åˆ«æé«˜äº†+16%å’Œ+11%ï¼Œåœ¨æ‰€æœ‰ä¸»è¦ç±»åˆ«ä¸­ä¸€è‡´åœ°ä¼˜äºæœ€æ–°ä¼˜åŒ–å’Œæ¢ç´¢æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡å’Œå¤šæ ·æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionè™½èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨å¤„ç†å¤æ‚å¯¹è±¡å…³ç³»ã€å±æ€§åŠç©ºé—´æ’åˆ—æ—¶éš¾ä»¥å®ç°ç»„æˆå¯¹é½ã€‚</li>
<li>ç°æœ‰æ¨æ–­æ—¶é—´æ–¹æ³•é€šè¿‡ä¼˜åŒ–åˆå§‹å™ªå£°æˆ–æ¢ç´¢æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å®ƒä»¬å„è‡ªå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å•ä¸€çš„å¥–åŠ±æŒ‡æ ‡æ— æ³•å…¨é¢æ•æ‰ç»„æˆæ€§çš„å„ä¸ªæ–¹é¢ï¼Œå¯¼è‡´æŒ‡å¯¼æ•ˆæœä¸ä½³ã€‚</li>
<li>CARINOXæ¡†æ¶ç»“åˆäº†ä¼˜åŒ–å’Œæ¢ç´¢ç­–ç•¥ï¼Œé€šè¿‡åŸºäºç±»åˆ«çš„å¥–åŠ±é€‰æ‹©ç¨‹åºæ¥æé«˜æ–‡æœ¬å›¾åƒå¯¹é½çš„å¾—åˆ†ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼ŒCARINOXåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†å¹³å‡å¯¹é½å¾—åˆ†ã€‚</li>
<li>CARINOXåœ¨ä¿æŒå›¾åƒè´¨é‡å’Œå¤šæ ·æ€§çš„åŒæ—¶ï¼Œä¸€è‡´åœ°ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17458v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17458v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17458v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17458v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Graph-Signal-Generative-Diffusion-Models"><a href="#Graph-Signal-Generative-Diffusion-Models" class="headerlink" title="Graph Signal Generative Diffusion Models"></a>Graph Signal Generative Diffusion Models</h2><p><strong>Authors:Yigit Berkay Uslu, Samar Hadou, Sergio Rozada, Shirin Saeedi Bidokhti, Alejandro Ribeiro</strong></p>
<p>We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) for stochastic graph signal generation using denoising diffusion processes. The architecture learns node features at different resolutions with skip connections between the encoder and decoder paths, analogous to the convolutional U-Net for image generation. The U-GNN is prominent for a pooling operation that leverages zero-padding and avoids arbitrary graph coarsening, with graph convolutions layered on top to capture local dependencies. This technique permits learning feature embeddings for sampled nodes at deeper levels of the architecture that remain convolutional with respect to the original graph. Applied to stock price prediction â€“ where deterministic forecasts struggle to capture uncertainties and tail events that are paramount â€“ we demonstrate the effectiveness of the diffusion model in probabilistic forecasting of stock prices. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥Uå‹ç¼–ç å™¨-è§£ç å™¨å›¾ç¥ç»ç½‘ç»œï¼ˆU-GNNsï¼‰ï¼Œåˆ©ç”¨å»å™ªæ‰©æ•£è¿‡ç¨‹è¿›è¡Œéšæœºå›¾ä¿¡å·ç”Ÿæˆã€‚è¯¥æ¶æ„é€šè¿‡ç¼–ç å™¨ä¸è§£ç å™¨è·¯å¾„ä¹‹é—´çš„è·³è¿‡è¿æ¥å­¦ä¹ ä¸åŒåˆ†è¾¨ç‡çš„èŠ‚ç‚¹ç‰¹å¾ï¼Œç±»ä¼¼äºç”¨äºå›¾åƒç”Ÿæˆçš„å·ç§¯U-Netã€‚U-GNNçš„çªå‡ºä¹‹å¤„åœ¨äºå®ƒçš„æ± åŒ–æ“ä½œï¼Œè¯¥æ“ä½œåˆ©ç”¨é›¶å¡«å……å¹¶é¿å…ä»»æ„çš„å›¾ç®€åŒ–ï¼Œå¹¶åœ¨å…¶ä¸Šå åŠ å›¾å·ç§¯ä»¥æ•è·å±€éƒ¨ä¾èµ–æ€§ã€‚è¿™é¡¹æŠ€æœ¯å…è®¸åœ¨æ¶æ„çš„æ›´æ·±å±‚æ¬¡ä¸Šï¼Œå¯¹é‡‡æ ·èŠ‚ç‚¹è¿›è¡Œç‰¹å¾åµŒå…¥å­¦ä¹ ï¼Œç›¸å¯¹äºåŸå§‹å›¾ä¿æŒå·ç§¯ã€‚å°†å…¶åº”ç”¨äºè‚¡ç¥¨ä»·æ ¼é¢„æµ‹â€”â€”ç¡®å®šæ€§é¢„æµ‹å¾ˆéš¾æ•æ‰ä¸ç¡®å®šæ€§å’Œæç«¯äº‹ä»¶ï¼Œè€Œè¿™äº›å› ç´ è‡³å…³é‡è¦â€”â€”æˆ‘ä»¬å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨æ¦‚ç‡é¢„æµ‹è‚¡ç¥¨ä»·æ ¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17250v1">PDF</a> Submitted to 2026 IEEE International Conference on Acoustics, Speech,   and Signal Processing (ICASSP 2026)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹çš„éšæœºå›¾ä¿¡å·ç”Ÿæˆï¼Œæˆ‘ä»¬å¼•å…¥äº†Uå‹ç¼–ç å™¨-è§£ç å™¨å›¾ç¥ç»ç½‘ç»œï¼ˆU-GNNsï¼‰ã€‚æ¶æ„é€šè¿‡è·³è¿‡è¿æ¥å­¦ä¹ ä¸åŒåˆ†è¾¨ç‡çš„èŠ‚ç‚¹ç‰¹å¾ï¼Œç±»ä¼¼äºç”¨äºå›¾åƒç”Ÿæˆçš„å·ç§¯U-Netã€‚U-GNNçš„ä¸€ä¸ªçªå‡ºç‰¹ç‚¹æ˜¯å®ƒçš„æ± åŒ–æ“ä½œåˆ©ç”¨é›¶å¡«å……é¿å…äº†ä»»æ„å›¾ç²—åŒ–ï¼Œé¡¶å±‚ä½¿ç”¨å›¾å·ç§¯æ•æ‰å±€éƒ¨ä¾èµ–æ€§ã€‚è¯¥æŠ€æœ¯å…è®¸åœ¨æ¶æ„çš„æ›´æ·±å±‚æ¬¡å­¦ä¹ é‡‡æ ·èŠ‚ç‚¹çš„ç‰¹å¾åµŒå…¥ï¼Œè¿™äº›åµŒå…¥ç›¸å¯¹äºåŸå§‹å›¾æ˜¯å·ç§¯çš„ã€‚åœ¨è‚¡ç¥¨é¢„æµ‹æ–¹é¢ï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ‰©æ•£æ¨¡å‹åœ¨æ¦‚ç‡é¢„æµ‹è‚¡ç¥¨ä»·æ ¼ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç¡®å®šæ€§é¢„æµ‹å¾ˆéš¾æ•æ‰åˆ°ä¸ç¡®å®šæ€§å’Œé‡è¦çš„äº‹ä»¶å°¾éƒ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Uå‹ç¼–ç å™¨-è§£ç å™¨å›¾ç¥ç»ç½‘ç»œï¼ˆU-GNNsï¼‰è¢«å¼•å…¥ç”¨äºåŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹çš„éšæœºå›¾ä¿¡å·ç”Ÿæˆã€‚</li>
<li>U-GNNæ¶æ„é€šè¿‡è·³è¿‡è¿æ¥å­¦ä¹ ä¸åŒåˆ†è¾¨ç‡çš„èŠ‚ç‚¹ç‰¹å¾ã€‚</li>
<li>U-GNNçš„æ± åŒ–æ“ä½œåˆ©ç”¨é›¶å¡«å……é¿å…ä»»æ„å›¾ç²—åŒ–ï¼Œæ•æ‰å±€éƒ¨ä¾èµ–æ€§ã€‚</li>
<li>è¯¥æŠ€æœ¯å…è®¸å­¦ä¹ é‡‡æ ·èŠ‚ç‚¹çš„ç‰¹å¾åµŒå…¥ï¼Œè¿™äº›åµŒå…¥ç›¸å¯¹äºåŸå§‹å›¾æ˜¯å·ç§¯çš„ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ¦‚ç‡é¢„æµ‹è‚¡ç¥¨ä»·æ ¼æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17250v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17250v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Stencil-Subject-Driven-Generation-with-Context-Guidance"><a href="#Stencil-Subject-Driven-Generation-with-Context-Guidance" class="headerlink" title="Stencil: Subject-Driven Generation with Context Guidance"></a>Stencil: Subject-Driven Generation with Context Guidance</h2><p><strong>Authors:Gordon Chen, Ziqi Huang, Cheston Tan, Ziwei Liu</strong></p>
<p>Recent text-to-image diffusion models can generate striking visuals from text prompts, but they often fail to maintain subject consistency across generations and contexts. One major limitation of current fine-tuning approaches is the inherent trade-off between quality and efficiency. Fine-tuning large models improves fidelity but is computationally expensive, while fine-tuning lightweight models improves efficiency but compromises image fidelity. Moreover, fine-tuning pre-trained models on a small set of images of the subject can damage the existing priors, resulting in suboptimal results. To this end, we present Stencil, a novel framework that jointly employs two diffusion models during inference. Stencil efficiently fine-tunes a lightweight model on images of the subject, while a large frozen pre-trained model provides contextual guidance during inference, injecting rich priors to enhance generation with minimal overhead. Stencil excels at generating high-fidelity, novel renditions of the subject in less than a minute, delivering state-of-the-art performance and setting a new benchmark in subject-driven generation. </p>
<blockquote>
<p>è¿‘æœŸçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥ä»æ–‡æœ¬æç¤ºç”Ÿæˆå¼•äººæ³¨ç›®çš„å›¾åƒï¼Œä½†å®ƒä»¬åœ¨è·¨ä¸–ä»£å’Œä¸Šä¸‹æ–‡æ—¶å¾€å¾€æ— æ³•ä¿æŒä¸»é¢˜ä¸€è‡´æ€§ã€‚å½“å‰å¾®è°ƒæ–¹æ³•çš„ä¸€ä¸ªä¸»è¦å±€é™æ˜¯è´¨é‡ä¸æ•ˆç‡ä¹‹é—´çš„å›ºæœ‰æƒè¡¡ã€‚å¾®è°ƒå¤§å‹æ¨¡å‹å¯ä»¥æé«˜ä¿çœŸåº¦ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè€Œå¾®è°ƒè½»å‹æ¨¡å‹åˆ™æé«˜æ•ˆç‡ä½†ä¼šç‰ºç‰²å›¾åƒä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œåœ¨å°‘é‡æœ‰å…³ä¸»é¢˜å›¾åƒä¸Šå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒå¯èƒ½ä¼šç ´åç°æœ‰å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´ç»“æœä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Stencilï¼Œä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŒæ—¶é‡‡ç”¨ä¸¤ç§æ‰©æ•£æ¨¡å‹ã€‚Stencilåœ¨æœ‰å…³ä¸»é¢˜çš„å›¾åƒä¸Šæœ‰æ•ˆåœ°å¾®è°ƒè½»å‹æ¨¡å‹ï¼Œè€Œä¸€ä¸ªå¤§å‹å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹åˆ™åœ¨æ¨ç†è¿‡ç¨‹ä¸­æä¾›ä¸Šä¸‹æ–‡æŒ‡å¯¼ï¼Œæ³¨å…¥ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ä»¥å¢å¼ºç”Ÿæˆï¼Œä¸”å‡ ä¹ä¸äº§ç”Ÿé¢å¤–å¼€é”€ã€‚Stencilæ“…é•¿åœ¨ä¸åˆ°ä¸€åˆ†é’Ÿå†…ç”Ÿæˆé«˜ä¿çœŸã€æ–°é¢–çš„ä¸»é¢˜å‘ˆç°ï¼Œå®ç°å“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨ä¸»é¢˜é©±åŠ¨ç”Ÿæˆæ–¹é¢æ ‘ç«‹æ–°æ ‡æ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17120v1">PDF</a> Accepted as Spotlight at ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿä»æ–‡æœ¬æç¤ºç”Ÿæˆå¼•äººæ³¨ç›®çš„å›¾åƒï¼Œä½†åœ¨ä¸åŒä¸–ä»£å’ŒèƒŒæ™¯ä¸‹ç»´æŒä¸»é¢˜ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚ç°æœ‰å¾®è°ƒæ–¹æ³•é¢ä¸´è´¨é‡ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚å¤§å‹æ¨¡å‹çš„å¾®è°ƒæé«˜äº†ä¿çœŸåº¦ä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè€Œè½»é‡çº§æ¨¡å‹çš„å¾®è°ƒæé«˜äº†æ•ˆç‡ä½†ç‰ºç‰²äº†å›¾åƒä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œå¯¹ä¸»é¢˜å°‘é‡å›¾åƒè¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒä¼šç ´åç°æœ‰å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´ç»“æœä¸å°½äººæ„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Stencilæ¡†æ¶ï¼Œå®ƒåœ¨æ¨ç†è¿‡ç¨‹ä¸­è”åˆé‡‡ç”¨ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹ã€‚Stencilé«˜æ•ˆåœ°å¯¹å…³äºä¸»é¢˜çš„å›¾åƒè¿›è¡Œè½»é‡çº§æ¨¡å‹çš„å¾®è°ƒï¼Œè€Œä¸€ä¸ªå¤§å‹å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­æä¾›ä¸Šä¸‹æ–‡æŒ‡å¯¼ï¼Œæ³¨å…¥ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ä»¥å¢å¼ºç”Ÿæˆæ•ˆæœï¼Œä¸”å‡ ä¹ä¸å¢åŠ è®¡ç®—å¼€é”€ã€‚Stencilæ“…é•¿åœ¨ä¸åˆ°ä¸€åˆ†é’Ÿå†…ç”Ÿæˆé«˜ä¿çœŸã€æ–°é¢–çš„ä¸»é¢˜æ¸²æŸ“ï¼Œå®ç°å“è¶Šæ€§èƒ½ï¼Œåœ¨ä¸»é¢˜é©±åŠ¨ç”Ÿæˆæ–¹é¢æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç»´æŒä¸»é¢˜ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å¾®è°ƒæ–¹æ³•åœ¨è´¨é‡å’Œæ•ˆç‡ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚</li>
<li>å¯¹ä¸»é¢˜å°‘é‡å›¾åƒè¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒå¯èƒ½ç ´åç°æœ‰å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>Stencilæ¡†æ¶é€šè¿‡è”åˆé‡‡ç”¨ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>Stencilèƒ½é«˜æ•ˆåœ°å¯¹å…³äºä¸»é¢˜çš„å›¾åƒè¿›è¡Œè½»é‡çº§æ¨¡å‹çš„å¾®è°ƒã€‚</li>
<li>ä¸€ä¸ªå¤§å‹å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸ºStencilæä¾›ä¸Šä¸‹æ–‡æŒ‡å¯¼å’Œä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17120">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17120v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17120v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17120v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17120v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.17120v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VCE-Safe-Autoregressive-Image-Generation-via-Visual-Contrast-Exploitation"><a href="#VCE-Safe-Autoregressive-Image-Generation-via-Visual-Contrast-Exploitation" class="headerlink" title="VCE: Safe Autoregressive Image Generation via Visual Contrast   Exploitation"></a>VCE: Safe Autoregressive Image Generation via Visual Contrast   Exploitation</h2><p><strong>Authors:Feng Han, Chao Gong, Zhipeng Wei, Jingjing Chen, Yu-Gang Jiang</strong></p>
<p>Recently, autoregressive image generation models have wowed audiences with their remarkable capability in creating surprisingly realistic images. Models such as GPT-4o and LlamaGen can not only produce images that faithfully mimic renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also potentially generate Not-Safe-For-Work (NSFW) content, raising significant concerns regarding copyright infringement and ethical use. Despite these concerns, methods to safeguard autoregressive text-to-image models remain underexplored. Previous concept erasure methods, primarily designed for diffusion models that operate in denoising latent space, are not directly applicable to autoregressive models that generate images token by token. To address this critical gap, we propose Visual Contrast Exploitation (VCE), a novel framework comprising: (1) an innovative contrastive image pair construction paradigm that precisely decouples unsafe concepts from their associated content semantics, and (2) a sophisticated DPO-based training approach that enhances the modelâ€™s ability to identify and leverage visual contrastive features from image pairs, enabling precise concept erasure. Our comprehensive experiments across three challenging tasks-artist style erasure, explicit content erasure, and object removal-demonstrate that our method effectively secures the model, achieving state-of-the-art results while erasing unsafe concepts and maintaining the integrity of unrelated safe concepts. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/Maplebb/VCE">https://github.com/Maplebb/VCE</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹å‡­å€Ÿå…¶åˆ›é€ æƒŠäººé€¼çœŸå›¾åƒçš„èƒ½åŠ›å¸å¼•äº†è§‚ä¼—ã€‚ä¾‹å¦‚GPT-4oå’ŒLlamaGenç­‰æ¨¡å‹ä¸ä»…èƒ½å¤Ÿå¿ å®åœ°æ¨¡ä»¿å‰åœåˆ©ã€æ¢µé«˜æˆ–æ¯•åŠ ç´¢ç­‰è‘—åè‰ºæœ¯é£æ ¼ç”Ÿæˆå›¾åƒï¼Œè€Œä¸”è¿˜å¯èƒ½ç”Ÿæˆä¸é€‚åˆå·¥ä½œåœºåˆï¼ˆNSFWï¼‰çš„å†…å®¹ï¼Œå¼•å‘äº†å…³äºç‰ˆæƒä¾µçŠ¯å’Œé“å¾·ä½¿ç”¨ç­‰æ–¹é¢çš„ä¸¥é‡å…³æ³¨ã€‚å°½ç®¡å­˜åœ¨è¿™äº›æ‹…å¿§ï¼Œä½†ä¿æŠ¤è‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ–¹æ³•ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚ä¹‹å‰çš„æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•ä¸»è¦è®¾è®¡ç”¨äºåœ¨é™å™ªæ½œåœ¨ç©ºé—´æ“ä½œçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä¸ç›´æ¥é€‚ç”¨äºç”Ÿæˆå›¾åƒçš„è‡ªå›å½’æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å…³é”®ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰å¯¹æ¯”åˆ©ç”¨ï¼ˆVCEï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒåŒ…å«ï¼šï¼ˆ1ï¼‰ä¸€ç§åˆ›æ–°æ€§çš„å¯¹æ¯”å›¾åƒå¯¹æ„å»ºèŒƒå¼ï¼Œèƒ½å¤Ÿç²¾ç¡®åœ°å°†ä¸å®‰å…¨æ¦‚å¿µä¸å…¶å…³è”çš„å†…å®¹è¯­ä¹‰åˆ†ç¦»ï¼›ï¼ˆ2ï¼‰ä¸€ç§åŸºäºDPOçš„å…ˆè¿›è®­ç»ƒæ–¹æ³•ï¼Œæé«˜æ¨¡å‹ä»å›¾åƒå¯¹ä¸­è¯†åˆ«å’Œåˆ©ç”¨è§†è§‰å¯¹æ¯”ç‰¹å¾çš„èƒ½åŠ›ï¼Œä»è€Œå®ç°ç²¾ç¡®çš„æ¦‚å¿µæ¶ˆé™¤ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ˆè‰ºæœ¯å®¶é£æ ¼æ¶ˆé™¤ã€æ˜ç¡®å†…å®¹æ¶ˆé™¤å’Œå¯¹è±¡ç§»é™¤ï¼‰ä¸Šè¿›è¡Œçš„å…¨é¢å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ä¿æŠ¤äº†æ¨¡å‹ï¼Œåœ¨æ¶ˆé™¤ä¸å®‰å…¨æ¦‚å¿µçš„åŒæ—¶ä¿æŒæ— å…³å®‰å…¨æ¦‚å¿µçš„å®Œæ•´æ€§ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Maplebb/VCE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Maplebb/VCEè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16986v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸï¼Œè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹å¦‚GPT-4oå’ŒLlamaGenèƒ½é€¼çœŸåœ°ç”Ÿæˆå›¾åƒï¼Œå¼•èµ·è§‚ä¼—æƒŠå¹ã€‚è¿™äº›æ¨¡å‹ä¸ä»…èƒ½æ¨¡ä»¿è‘—åè‰ºæœ¯é£æ ¼ï¼Œè¿˜å¯èƒ½ç”Ÿæˆä¸é€‚å®œå·¥ä½œç¯å¢ƒçš„ï¼ˆNSFWï¼‰å†…å®¹ï¼Œå¼•å‘ç‰ˆæƒå’Œä¼¦ç†ä½¿ç”¨æ–¹é¢çš„æ‹…å¿§ã€‚å°½ç®¡å­˜åœ¨æ‹…å¿§ï¼Œä½†é’ˆå¯¹è‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ä¿æŠ¤æªæ–½ä»è¢«å¿½è§†ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºVisual Contrast Exploitationï¼ˆVCEï¼‰æ¡†æ¶ï¼ŒåŒ…å«å¯¹æ¯”å›¾åƒå¯¹æ„å»ºæ¨¡å¼å’ŒåŸºäºDPOçš„è®­ç»ƒæ–¹æ³•ï¼Œèƒ½ç²¾ç¡®å‰¥ç¦»ä¸å®‰å…¨æ¦‚å¿µä¸å…¶ç›¸å…³å†…å®¹è¯­ä¹‰ï¼Œæå‡æ¨¡å‹è¯†åˆ«å’Œåˆ©ç”¨å¯¹æ¯”ç‰¹å¾çš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é£æ ¼æ¶ˆé™¤ã€æ˜ç¡®å†…å®¹æ¶ˆé™¤å’Œå¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸Šè¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œæœ‰æ•ˆä¿æŠ¤æ¨¡å‹å¹¶ç»´æŒå®‰å…¨æ¦‚å¿µçš„å®Œæ•´æ€§ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/Maplebb/VCE%E3%80%82">https://github.com/Maplebb/VCEã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹å…·æœ‰åˆ›å»ºé€¼çœŸå›¾åƒçš„èƒ½åŠ›ï¼Œå¹¶èƒ½æ¨¡ä»¿è‘—åè‰ºæœ¯é£æ ¼ï¼Œå¼•å‘å…³æ³¨ã€‚</li>
<li>è¿™äº›æ¨¡å‹å­˜åœ¨ç”Ÿæˆä¸é€‚å®œå·¥ä½œç¯å¢ƒçš„ï¼ˆNSFWï¼‰å†…å®¹çš„æ½œåœ¨é£é™©ï¼Œå¼•å‘ç‰ˆæƒå’Œä¼¦ç†ä½¿ç”¨æ–¹é¢çš„æ‹…å¿§ã€‚</li>
<li>é’ˆå¯¹è‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ä¿æŠ¤æªæ–½ç›®å‰ä»è¢«å¿½è§†ï¼Œå­˜åœ¨ç ”ç©¶ç©ºç™½ã€‚</li>
<li>æå‡ºçš„Visual Contrast Exploitationï¼ˆVCEï¼‰æ¡†æ¶åŒ…å«åˆ›æ–°å¯¹æ¯”å›¾åƒå¯¹æ„å»ºæ¨¡å¼å’ŒåŸºäºDPOçš„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>VCEæ¡†æ¶èƒ½ç²¾ç¡®å‰¥ç¦»ä¸å®‰å…¨æ¦‚å¿µä¸å…¶ç›¸å…³å†…å®¹è¯­ä¹‰ï¼Œæé«˜æ¨¡å‹çš„è¯†åˆ«å’Œå¯¹æ¯”ç‰¹å¾åˆ©ç”¨èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒVCEæ¡†æ¶åœ¨é£æ ¼æ¶ˆé™¤ã€æ˜ç¡®å†…å®¹æ¶ˆé™¤å’Œå¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16986v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16986v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16986v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16986v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16986v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DiffEye-Diffusion-Based-Continuous-Eye-Tracking-Data-Generation-Conditioned-on-Natural-Images"><a href="#DiffEye-Diffusion-Based-Continuous-Eye-Tracking-Data-Generation-Conditioned-on-Natural-Images" class="headerlink" title="DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation   Conditioned on Natural Images"></a>DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation   Conditioned on Natural Images</h2><p><strong>Authors:Ozgur Kara, Harris Nisar, James M. Rehg</strong></p>
<p>Numerous models have been developed for scanpath and saliency prediction, which are typically trained on scanpaths, which model eye movement as a sequence of discrete fixation points connected by saccades, while the rich information contained in the raw trajectories is often discarded. Moreover, most existing approaches fail to capture the variability observed among human subjects viewing the same image. They generally predict a single scanpath of fixed, pre-defined length, which conflicts with the inherent diversity and stochastic nature of real-world visual attention. To address these challenges, we propose DiffEye, a diffusion-based training framework designed to model continuous and diverse eye movement trajectories during free viewing of natural images. Our method builds on a diffusion model conditioned on visual stimuli and introduces a novel component, namely Corresponding Positional Embedding (CPE), which aligns spatial gaze information with the patch-based semantic features of the visual input. By leveraging raw eye-tracking trajectories rather than relying on scanpaths, DiffEye captures the inherent variability in human gaze behavior and generates high-quality, realistic eye movement patterns, despite being trained on a comparatively small dataset. The generated trajectories can also be converted into scanpaths and saliency maps, resulting in outputs that more accurately reflect the distribution of human visual attention. DiffEye is the first method to tackle this task on natural images using a diffusion model while fully leveraging the richness of raw eye-tracking data. Our extensive evaluation shows that DiffEye not only achieves state-of-the-art performance in scanpath generation but also enables, for the first time, the generation of continuous eye movement trajectories. Project webpage: <a target="_blank" rel="noopener" href="https://diff-eye.github.io/">https://diff-eye.github.io/</a> </p>
<blockquote>
<p>é’ˆå¯¹æ‰«æè·¯å¾„å’Œæ˜¾è‘—æ€§é¢„æµ‹ï¼Œå·²ç»å¼€å‘äº†è®¸å¤šæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šå¸¸æ˜¯åœ¨æ‰«æè·¯å¾„ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå°†çœ¼çƒè¿åŠ¨æ¨¡æ‹Ÿä¸ºä¸€ç³»åˆ—ç”±çœ¼è·³è¿æ¥çš„ç¦»æ•£æ³¨è§†ç‚¹ï¼Œè€ŒåŸå§‹è½¨è¿¹ä¸­åŒ…å«çš„ä¸°å¯Œä¿¡æ¯é€šå¸¸è¢«ä¸¢å¼ƒã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•æ— æ³•æ•æ‰åŒä¸€å›¾åƒä¸­ä¸åŒäººç±»ä¸»ä½“ä¹‹é—´çš„å·®å¼‚æ€§ã€‚å®ƒä»¬é€šå¸¸é¢„æµ‹å›ºå®šé¢„å®šä¹‰é•¿åº¦çš„å•ä¸€æ‰«æè·¯å¾„ï¼Œè¿™ä¸çœŸå®ä¸–ç•Œè§†è§‰æ³¨æ„åŠ›çš„å†…åœ¨å¤šæ ·æ€§å’Œéšæœºæ€§ç›¸å†²çªã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiffEyeï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿåœ¨è‡ªç„¶å›¾åƒè‡ªç”±è§‚çœ‹æ—¶çš„è¿ç»­å’Œå¤šæ ·çš„çœ¼çƒè¿åŠ¨è½¨è¿¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºè§†è§‰åˆºæ¿€çš„æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ä¸Šæ„å»ºï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåä¸ºç›¸åº”ä½ç½®åµŒå…¥ï¼ˆCPEï¼‰çš„æ–°ç»„ä»¶ï¼Œå®ƒå°†ç©ºé—´å‡è§†ä¿¡æ¯ä¸è§†è§‰è¾“å…¥çš„åŸºäºè¡¥ä¸çš„è¯­ä¹‰ç‰¹å¾å¯¹é½ã€‚DiffEyeé€šè¿‡åˆ©ç”¨åŸå§‹çš„çœ¼åŠ¨è½¨è¿¹ï¼Œè€Œä¸æ˜¯ä¾èµ–äºæ‰«æè·¯å¾„ï¼Œæ•æ‰äº†äººç±»å‡è§†è¡Œä¸ºçš„å†…åœ¨å·®å¼‚ï¼Œå¹¶ç”Ÿæˆäº†é«˜è´¨é‡çš„ã€ç°å®çš„çœ¼çƒè¿åŠ¨æ¨¡å¼ï¼Œå°½ç®¡å®ƒæ˜¯åœ¨ç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€‚ç”Ÿæˆçš„è½¨è¿¹è¿˜å¯ä»¥è½¬æ¢ä¸ºæ‰«æè·¯å¾„å’Œæ˜¾è‘—æ€§åœ°å›¾ï¼Œä»è€Œè¾“å‡ºæ›´å‡†ç¡®åæ˜ äººç±»è§†è§‰æ³¨æ„åŠ›åˆ†å¸ƒçš„ç»“æœã€‚DiffEyeæ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸Šå®Œæˆæ­¤ä»»åŠ¡çš„æ–¹æ³•ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨äº†åŸå§‹çš„çœ¼åŠ¨è¿½è¸ªæ•°æ®çš„ä¸°å¯Œæ€§ã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDiffEyeä¸ä»…åœ¨æ‰«æè·¯å¾„ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œè€Œä¸”é¦–æ¬¡å®ç°äº†è¿ç»­çœ¼åŠ¨è½¨è¿¹çš„ç”Ÿæˆã€‚é¡¹ç›®ç½‘é¡µï¼š<a target="_blank" rel="noopener" href="https://diff-eye.github.io/">https://diff-eye.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16767v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰æ³¨è§†è·¯å¾„é¢„æµ‹çš„æ–°æ¨¡å‹DiffEyeã€‚è¯¥æ¨¡å‹åŸºäºæ‰©æ•£æ¨¡å‹è®­ç»ƒï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿè¿ç»­ä¸”å¤šæ ·çš„çœ¼åŠ¨è½¨è¿¹ï¼Œåº”å¯¹è‡ªç„¶å›¾åƒçš„è‡ªç”±è§‚çœ‹ã€‚DiffEyeåˆ©ç”¨CPEï¼ˆå¯¹åº”ä½ç½®åµŒå…¥ï¼‰å¯¹é½ç©ºé—´æ³¨è§†ä¿¡æ¯ä¸è§†è§‰è¾“å…¥çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨åŸå§‹çœ¼åŠ¨è½¨è¿¹æ•°æ®è®­ç»ƒï¼Œæå‡äº†æ³¨è§†è·¯å¾„ç”Ÿæˆè´¨é‡ï¼Œå‡†ç¡®åæ˜ äººç±»è§†è§‰æ³¨æ„åˆ†å¸ƒã€‚å®ƒè§£å†³äº†ä½¿ç”¨scanpathä½œä¸ºé¢„æµ‹åŸºç¡€çš„æ¨¡å‹çš„å±€é™æ€§ï¼Œå¦‚ä¿¡æ¯ä¸¢å¤±å’Œé¢„æµ‹å•ä¸€åŒ–é—®é¢˜ã€‚DiffEyeçš„æ€§èƒ½è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œå¹¶é¦–æ¬¡å®ç°äº†è¿ç»­çœ¼åŠ¨è½¨è¿¹çš„ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffEyeæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿè‡ªç„¶å›¾åƒè§‚çœ‹æ—¶çš„è¿ç»­å’Œå¤šæ ·çš„çœ¼åŠ¨è½¨è¿¹ã€‚</li>
<li>DiffEyeåˆ©ç”¨CPEæŠ€æœ¯å¯¹é½ç©ºé—´æ³¨è§†ä¿¡æ¯ä¸è§†è§‰åˆºæ¿€çš„è¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨åŸå§‹çœ¼åŠ¨è½¨è¿¹æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ•æ‰äººç±»æ³¨è§†è¡Œä¸ºçš„å†…åœ¨å˜åŒ–ã€‚</li>
<li>DiffEyeè§£å†³äº†ä½¿ç”¨scanpathä½œä¸ºé¢„æµ‹åŸºç¡€çš„æ¨¡å‹çš„å±€é™æ€§ï¼Œå¦‚ä¿¡æ¯ä¸¢å¤±å’Œå¯¹çœŸå®ä¸–ç•Œè§†è§‰æ³¨æ„åŠ›å¤šæ ·æ€§çš„å¿½è§†ã€‚</li>
<li>DiffEyeåœ¨ç”Ÿæˆæ³¨è§†è·¯å¾„å’Œçœ¼åŠ¨è½¨è¿¹æ–¹é¢è¾¾åˆ°äº†å“è¶Šçš„æ€§èƒ½æ°´å¹³ã€‚</li>
<li>è¯¥æ¨¡å‹é¦–æ¬¡å®ç°äº†åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¯¹è‡ªç„¶å›¾åƒè¿›è¡Œçœ¼åŠ¨è½¨è¿¹çš„ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16767v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16767v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16767v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16767v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16767v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="InstanceAssemble-Layout-Aware-Image-Generation-via-Instance-Assembling-Attention"><a href="#InstanceAssemble-Layout-Aware-Image-Generation-via-Instance-Assembling-Attention" class="headerlink" title="InstanceAssemble: Layout-Aware Image Generation via Instance Assembling   Attention"></a>InstanceAssemble: Layout-Aware Image Generation via Instance Assembling   Attention</h2><p><strong>Authors:Qiang Xiang, Shuang Sun, Binglei Li, Dejia Song, Huaxia Li, Nemo Chen, Xu Tang, Yao Hu, Junping Zhang</strong></p>
<p>Diffusion models have demonstrated remarkable capabilities in generating high-quality images. Recent advancements in Layout-to-Image (L2I) generation have leveraged positional conditions and textual descriptions to facilitate precise and controllable image synthesis. Despite overall progress, current L2I methods still exhibit suboptimal performance. Therefore, we propose InstanceAssemble, a novel architecture that incorporates layout conditions via instance-assembling attention, enabling position control with bounding boxes (bbox) and multimodal content control including texts and additional visual content. Our method achieves flexible adaption to existing DiT-based T2I models through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image benchmark, Denselayout, a comprehensive benchmark for layout-to-image generation, containing 5k images with 90k instances in total. We further introduce Layout Grounding Score (LGS), an interpretable evaluation metric to more precisely assess the accuracy of L2I generation. Experiments demonstrate that our InstanceAssemble method achieves state-of-the-art performance under complex layout conditions, while exhibiting strong compatibility with diverse style LoRA modules. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚æœ€è¿‘åœ¨Layout-to-Imageï¼ˆL2Iï¼‰ç”Ÿæˆæ–¹é¢çš„è¿›å±•åˆ©ç”¨äº†ä½ç½®æ¡ä»¶å’Œæ–‡æœ¬æè¿°ï¼Œä¿ƒè¿›äº†ç²¾ç¡®å¯æ§çš„å›¾åƒåˆæˆã€‚å°½ç®¡æ•´ä½“æœ‰æ‰€è¿›å±•ï¼Œä½†å½“å‰çš„L2Iæ–¹æ³•ä»ç„¶è¡¨ç°å‡ºæ¬¡ä¼˜æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†InstanceAssembleï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œé€šè¿‡å®ä¾‹ç»„è£…æ³¨æ„åŠ›èå…¥å¸ƒå±€æ¡ä»¶ï¼Œå®ç°åˆ©ç”¨è¾¹ç•Œæ¡†ï¼ˆbboxï¼‰è¿›è¡Œä½ç½®æ§åˆ¶ä»¥åŠåŒ…æ‹¬æ–‡æœ¬å’Œé¢å¤–è§†è§‰å†…å®¹çš„å¤šæ¨¡æ€å†…å®¹æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è½»é‡çº§çš„LoRAæ¨¡å—ï¼Œçµæ´»é€‚åº”ç°æœ‰çš„åŸºäºDiTçš„T2Iæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†Layout-to-ImageåŸºå‡†æµ‹è¯•Denselayoutï¼Œè¿™æ˜¯å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆçš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«5000å¼ å›¾åƒï¼Œæ€»è®¡9ä¸‡ä¸ªå®ä¾‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†å¸ƒå±€æ¥åœ°å¾—åˆ†ï¼ˆLGSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯è§£é‡Šçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¯ä»¥æ›´ç²¾ç¡®åœ°è¯„ä¼°L2Iç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„InstanceAssembleæ–¹æ³•åœ¨å¤æ‚çš„å¸ƒå±€æ¡ä»¶ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸å¤šç§é£æ ¼çš„LoRAæ¨¡å—å…·æœ‰å¾ˆå¼ºçš„å…¼å®¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16691v1">PDF</a> Accepted in NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚æœ€è¿‘çš„Layout-to-Imageï¼ˆL2Iï¼‰ç”ŸæˆæŠ€æœ¯é€šè¿‡ä½ç½®æ¡ä»¶å’Œæ–‡æœ¬æè¿°ï¼Œä¿ƒè¿›äº†ç²¾ç¡®å¯æ§çš„å›¾åƒåˆæˆã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œä½†å½“å‰çš„L2Iæ–¹æ³•æ€§èƒ½ä»ä¸ç†æƒ³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†InstanceAssembleï¼Œä¸€ç§é€šè¿‡å®ä¾‹ç»„è£…æ³¨æ„åŠ›æœºåˆ¶èå…¥å¸ƒå±€æ¡ä»¶çš„æ–°å‹æ¶æ„ï¼Œå®ç°é€šè¿‡è¾¹ç•Œæ¡†ï¼ˆbboxï¼‰è¿›è¡Œä½ç½®æ§åˆ¶ï¼Œä»¥åŠåŒ…æ‹¬æ–‡æœ¬å’Œé¢å¤–è§†è§‰å†…å®¹çš„å¤šæ¨¡æ€å†…å®¹æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è½»é‡çº§çš„LoRAæ¨¡å—ï¼Œçµæ´»é€‚åº”ç°æœ‰çš„DiT-based T2Iæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†Layout-to-ImageåŸºå‡†æµ‹è¯•Denselayoutï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5000å¼ å›¾åƒã€æ€»è®¡9ä¸‡ä¸ªå®ä¾‹çš„å…¨é¢å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†å¸ƒå±€æ¥åœ°åˆ†æ•°ï¼ˆLGSï¼‰ï¼Œä¸€ä¸ªå¯è§£é‡Šçš„è¯„ä»·æŒ‡æ ‡ï¼Œä»¥æ›´ç²¾ç¡®åœ°è¯„ä¼°L2Iç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„InstanceAssembleæ–¹æ³•åœ¨å¤æ‚çš„å¸ƒå±€æ¡ä»¶ä¸‹è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ä¸å„ç§é£æ ¼çš„LoRAæ¨¡å—è¡¨ç°å‡ºå¼ºå¤§çš„å…¼å®¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
<li>Layout-to-Imageï¼ˆL2Iï¼‰ç”ŸæˆæŠ€æœ¯é€šè¿‡ä½ç½®æ¡ä»¶å’Œæ–‡æœ¬æè¿°ä¿ƒè¿›äº†å›¾åƒåˆæˆçš„ç²¾ç¡®æ€§å’Œå¯æ§æ€§ã€‚</li>
<li>å½“å‰L2Iæ–¹æ³•æ€§èƒ½ä»æœ‰å¾…æå‡ã€‚</li>
<li>InstanceAssembleæ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œé€šè¿‡å®ä¾‹ç»„è£…æ³¨æ„åŠ›æœºåˆ¶å®ç°çµæ´»çš„ä½ç½®å’Œå†…å®¹æ§åˆ¶ã€‚</li>
<li>InstanceAssembleæ–¹æ³•å…¼å®¹ç°æœ‰çš„DiT-based T2Iæ¨¡å‹ï¼Œå¹¶é€šè¿‡è½»é‡çº§çš„LoRAæ¨¡å—è¿›è¡Œé€‚åº”ã€‚</li>
<li>Denselayoutæ˜¯å…¨é¢çš„Layout-to-Imageç”ŸæˆåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16691">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16691v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16691v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16691v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16691v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16691v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Novel-Metric-for-Detecting-Memorization-in-Generative-Models-for-Brain-MRI-Synthesis"><a href="#A-Novel-Metric-for-Detecting-Memorization-in-Generative-Models-for-Brain-MRI-Synthesis" class="headerlink" title="A Novel Metric for Detecting Memorization in Generative Models for Brain   MRI Synthesis"></a>A Novel Metric for Detecting Memorization in Generative Models for Brain   MRI Synthesis</h2><p><strong>Authors:Antonio Scardace, Lemuel Puglisi, Francesco Guarnera, Sebastiano Battiato, Daniele RavÃ¬</strong></p>
<p>Deep generative models have emerged as a transformative tool in medical imaging, offering substantial potential for synthetic data generation. However, recent empirical studies highlight a critical vulnerability: these models can memorize sensitive training data, posing significant risks of unauthorized patient information disclosure. Detecting memorization in generative models remains particularly challenging, necessitating scalable methods capable of identifying training data leakage across large sets of generated samples. In this work, we propose DeepSSIM, a novel self-supervised metric for quantifying memorization in generative models. DeepSSIM is trained to: i) project images into a learned embedding space and ii) force the cosine similarity between embeddings to match the ground-truth SSIM (Structural Similarity Index) scores computed in the image space. To capture domain-specific anatomical features, training incorporates structure-preserving augmentations, allowing DeepSSIM to estimate similarity reliably without requiring precise spatial alignment. We evaluate DeepSSIM in a case study involving synthetic brain MRI data generated by a Latent Diffusion Model (LDM) trained under memorization-prone conditions, using 2,195 MRI scans from two publicly available datasets (IXI and CoRR). Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior performance, improving F1 scores by an average of +52.03% over the best existing method. Code and data of our approach are publicly available at the following link: <a target="_blank" rel="noopener" href="https://github.com/brAIn-science/DeepSSIM">https://github.com/brAIn-science/DeepSSIM</a>. </p>
<blockquote>
<p>æ·±åº¦ç”Ÿæˆæ¨¡å‹ä½œä¸ºåŒ»å­¦å½±åƒä¸­çš„å˜é©æ€§å·¥å…·ï¼Œå…·æœ‰ç”Ÿæˆåˆæˆæ•°æ®çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„å®è¯ç ”ç©¶çªå‡ºäº†ä¸€ä¸ªå…³é”®æ¼æ´ï¼šè¿™äº›æ¨¡å‹ä¼šè®°å¿†æ•æ„Ÿçš„è®­ç»ƒæ•°æ®ï¼Œå­˜åœ¨æœªç»æˆæƒçš„æ³„éœ²æ‚£è€…ä¿¡æ¯çš„é£é™©ã€‚åœ¨ç”Ÿæˆæ¨¡å‹ä¸­æ£€æµ‹è®°å¿†åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦èƒ½å¤Ÿåœ¨å¤§é‡ç”Ÿæˆçš„æ ·æœ¬ä¸­è¯†åˆ«è®­ç»ƒæ•°æ®æ³„éœ²çš„å¯æ‰©å±•æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DeepSSIMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªæˆ‘ç›‘ç£æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–ç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†åŒ–ã€‚DeepSSIMç»è¿‡è®­ç»ƒèƒ½å¤Ÿï¼ši)å°†å›¾åƒæŠ•å½±åˆ°å­¦ä¹ åˆ°çš„åµŒå…¥ç©ºé—´ï¼›ii)å¼ºåˆ¶åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§ä¸å›¾åƒç©ºé—´ä¸­è®¡ç®—çš„çœŸå®SSIMï¼ˆç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼‰åˆ†æ•°ç›¸åŒ¹é…ã€‚ä¸ºäº†æ•æ‰ç‰¹å®šé¢†åŸŸçš„è§£å‰–ç‰¹å¾ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†ä¿æŒç»“æ„å¢å¼ºçš„æ–¹æ³•ï¼Œå…è®¸DeepSSIMåœ¨ä¸éœ€è¦ç²¾ç¡®ç©ºé—´å¯¹é½çš„æƒ…å†µä¸‹å¯é åœ°ä¼°è®¡ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬åœ¨ä¸€é¡¹æ¶‰åŠç”±æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨æ˜“äºè®°å¿†æ¡ä»¶ä¸‹è®­ç»ƒçš„åˆæˆå¤§è„‘MRIæ•°æ®çš„æ¡ˆä¾‹ç ”ç©¶ä¸­è¯„ä¼°äº†DeepSSIMï¼Œè¯¥ç ”ç©¶ä½¿ç”¨äº†ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ï¼ˆIXIå’ŒCoRRï¼‰çš„2,195ä¸ªMRIæ‰«æã€‚ä¸æœ€æ–°çš„è®°å¿†åŒ–æŒ‡æ ‡ç›¸æ¯”ï¼ŒDeepSSIMæ€§èƒ½å“è¶Šï¼Œå¹³å‡å°†F1åˆ†æ•°æé«˜äº†+52.03%ï¼Œè¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/brAIn-science/DeepSSIM%E3%80%82">https://github.com/brAIn-science/DeepSSIMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16582v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦æˆåƒé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå…·æœ‰ç”Ÿæˆåˆæˆæ•°æ®çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œæœ€æ–°å®è¯ç ”ç©¶æ­ç¤ºäº†å…¶å…³é”®æ¼æ´ï¼šè¿™äº›æ¨¡å‹ä¼šè®°å¿†æ•æ„Ÿçš„è®­ç»ƒæ•°æ®ï¼Œå­˜åœ¨æœªç»æˆæƒæŠ«éœ²æ‚£è€…ä¿¡æ¯çš„é£é™©ã€‚æ£€æµ‹ç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†åŒ–é¢‡å…·æŒ‘æˆ˜ï¼Œéœ€è¦èƒ½å¤Ÿåœ¨å¤§é‡ç”Ÿæˆæ ·æœ¬ä¸­è¯†åˆ«è®­ç»ƒæ•°æ®æ³„éœ²çš„å¯æ‰©å±•æ–¹æ³•ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹è‡ªç›‘ç£åº¦é‡æ–¹æ³•DeepSSIMï¼Œç”¨äºé‡åŒ–ç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†åŒ–ã€‚DeepSSIMç»è¿‡è®­ç»ƒï¼Œèƒ½å°†å›¾åƒæŠ•å½±åˆ°å­¦ä¹ åµŒå…¥ç©ºé—´ï¼Œå¹¶é€šè¿‡å¼ºåˆ¶åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§ä¸å›¾åƒç©ºé—´ä¸­è®¡ç®—çš„SSIMï¼ˆç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼‰å¾—åˆ†åŒ¹é…ã€‚ä¸ºæ•æ‰ç‰¹å®šé¢†åŸŸçš„è§£å‰–ç‰¹å¾ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­èå…¥äº†ç»“æ„ä¿ç•™å¢å¼ºæŠ€æœ¯ï¼Œä½¿DeepSSIMåœ¨æ— éœ€ç²¾ç¡®ç©ºé—´å¯¹é½çš„æƒ…å†µä¸‹å¯é åœ°ä¼°è®¡ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠç”±æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨æ˜“è®°å¿†æ¡ä»¶ä¸‹è®­ç»ƒçš„åˆæˆè„‘MRIæ•°æ®çš„æ¡ˆä¾‹ç ”ç©¶ä¸­è¯„ä¼°äº†DeepSSIMã€‚ä¸æœ€å…ˆè¿›çš„è®°å¿†åŒ–åº¦é‡æ–¹æ³•ç›¸æ¯”ï¼ŒDeepSSIMè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹³å‡å°†F1åˆ†æ•°æé«˜äº†+52.03%ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»£ç å’Œæ•°æ®å¯åœ¨ä»¥ä¸‹é“¾æ¥å…¬å¼€è·å–ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å­˜åœ¨è®°å¿†æ•æ„Ÿè®­ç»ƒæ•°æ®çš„é£é™©ã€‚</li>
<li>æ£€æµ‹ç”Ÿæˆæ¨¡å‹ä¸­çš„è®°å¿†åŒ–éœ€è¦å¯æ‰©å±•æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è‡ªç›‘ç£åº¦é‡DeepSSIMï¼Œç”¨äºé‡åŒ–ç”Ÿæˆæ¨¡å‹çš„è®°å¿†åŒ–ã€‚</li>
<li>DeepSSIMé€šè¿‡æŠ•å½±å›¾åƒåˆ°åµŒå…¥ç©ºé—´å¹¶åŒ¹é…SSIMå¾—åˆ†æ¥å·¥ä½œã€‚</li>
<li>DeepSSIMåœ¨ç»“æ„ä¿ç•™å¢å¼ºæŠ€æœ¯çš„å¸®åŠ©ä¸‹ï¼Œèƒ½æ•æ‰ç‰¹å®šé¢†åŸŸçš„è§£å‰–ç‰¹å¾ã€‚</li>
<li>åœ¨åˆæˆè„‘MRIæ•°æ®çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼ŒDeepSSIMè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16582v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16582v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16582v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16582v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16582v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="V-CECE-Visual-Counterfactual-Explanations-via-Conceptual-Edits"><a href="#V-CECE-Visual-Counterfactual-Explanations-via-Conceptual-Edits" class="headerlink" title="V-CECE: Visual Counterfactual Explanations via Conceptual Edits"></a>V-CECE: Visual Counterfactual Explanations via Conceptual Edits</h2><p><strong>Authors:Nikolaos Spanos, Maria Lymperaiou, Giorgos Filandrianos, Konstantinos Thomas, Athanasios Voulodimos, Giorgos Stamou</strong></p>
<p>Recent black-box counterfactual generation frameworks fail to take into account the semantic content of the proposed edits, while relying heavily on training to guide the generation process. We propose a novel, plug-and-play black-box counterfactual generation framework, which suggests step-by-step edits based on theoretical guarantees of optimal edits to produce human-level counterfactual explanations with zero training. Our framework utilizes a pre-trained image editing diffusion model, and operates without access to the internals of the classifier, leading to an explainable counterfactual generation process. Throughout our experimentation, we showcase the explanatory gap between human reasoning and neural model behavior by utilizing both Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision Language Model (LVLM) classifiers, substantiated through a comprehensive human evaluation. </p>
<blockquote>
<p>æœ€è¿‘çš„é»‘ç›’åäº‹å®ç”Ÿæˆæ¡†æ¶æœªèƒ½è€ƒè™‘åˆ°æè®®ç¼–è¾‘çš„è¯­ä¹‰å†…å®¹ï¼ŒåŒæ—¶å´ä¸¥é‡ä¾èµ–äºè®­ç»ƒæ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–å³æ’å³ç”¨çš„é»‘ç›’åäº‹å®ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºæœ€ä¼˜ç¼–è¾‘çš„ç†è®ºä¿è¯ï¼Œæå‡ºé€æ­¥ç¼–è¾‘ï¼Œä»¥äº§ç”Ÿæ— éœ€è®­ç»ƒå³å¯è¾¾åˆ°äººç±»æ°´å¹³çš„åäº‹å®è§£é‡Šã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€è®¿é—®åˆ†ç±»å™¨çš„å†…éƒ¨ç»„ä»¶ï¼Œä»è€Œå®ç°å¯è§£é‡Šçš„åäº‹å®ç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†äººç±»æ¨ç†å’Œç¥ç»ç½‘ç»œæ¨¡å‹è¡Œä¸ºä¹‹é—´çš„è§£é‡Šå·®è·ï¼Œä½¿ç”¨çš„åˆ†ç±»å™¨åŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ï¼Œå¹¶é€šè¿‡å…¨é¢çš„äººç±»è¯„ä¼°å¾—åˆ°äº†è¯å®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16567v1">PDF</a> Accepted in NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸé»‘ç›’å¼åäº‹å®ç”Ÿæˆæ¡†æ¶å¿½è§†äº†å»ºè®®ç¼–è¾‘çš„è¯­ä¹‰å†…å®¹ï¼Œè€Œè¿‡åº¦ä¾èµ–è®­ç»ƒæ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å³æ’å³ç”¨é»‘ç›’å¼åäº‹å®ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºç†è®ºä¸Šçš„æœ€ä½³ç¼–è¾‘ä¿è¯ï¼Œèƒ½å¤Ÿé€æ­¥è¿›è¡Œç¼–è¾‘å»ºè®®ï¼Œæ— éœ€è®­ç»ƒå³å¯ç”Ÿæˆäººç±»æ°´å¹³çš„åäº‹å®è§£é‡Šã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹è¿›è¡Œæ“ä½œï¼Œæ— éœ€è®¿é—®åˆ†ç±»å™¨çš„å†…éƒ¨ä¿¡æ¯ï¼Œä»è€Œå®ç°äº†å¯è§£é‡Šçš„åäº‹å®ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å®éªŒå±•ç¤ºäº†äººç±»æ¨ç†ä¸ç¥ç»ç½‘ç»œæ¨¡å‹è¡Œä¸ºä¹‹é—´çš„è§£é‡Šé¸¿æ²Ÿï¼Œè¿™å¾—åˆ°äº†åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œå¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åˆ†ç±»å™¨çš„ç»¼åˆäººç±»è¯„ä¼°çš„è¯å®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„é»‘ç›’åäº‹å®ç”Ÿæˆæ¡†æ¶å¿½ç•¥äº†ç¼–è¾‘çš„è¯­ä¹‰å†…å®¹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹é»‘ç›’åäº‹å®ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤ŸåŸºäºç†è®ºä¸Šçš„æœ€ä½³ç¼–è¾‘ä¿è¯è¿›è¡Œé€æ­¥ç¼–è¾‘å»ºè®®ã€‚</li>
<li>è¯¥æ¡†æ¶æ— éœ€è®­ç»ƒå³å¯ç”Ÿæˆäººç±»æ°´å¹³çš„åäº‹å®è§£é‡Šã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹è¿›è¡Œæ“ä½œã€‚</li>
<li>æ— éœ€è®¿é—®åˆ†ç±»å™¨çš„å†…éƒ¨ä¿¡æ¯ï¼Œå®ç°å¯è§£é‡Šçš„åäº‹å®ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>å®éªŒå±•ç¤ºäº†äººç±»æ¨ç†ä¸ç¥ç»ç½‘ç»œæ¨¡å‹è¡Œä¸ºä¹‹é—´çš„è§£é‡Šé¸¿æ²Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16567v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16567v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dynamic-Classifier-Free-Diffusion-Guidance-via-Online-Feedback"><a href="#Dynamic-Classifier-Free-Diffusion-Guidance-via-Online-Feedback" class="headerlink" title="Dynamic Classifier-Free Diffusion Guidance via Online Feedback"></a>Dynamic Classifier-Free Diffusion Guidance via Online Feedback</h2><p><strong>Authors:Pinelopi Papalampidi, Olivia Wiles, Ira Ktena, Aleksandar Shtedritski, Emanuele Bugliarello, Ivana Kajic, Isabela Albuquerque, Aida Nematzadeh</strong></p>
<p>Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion models, yet its effectiveness is limited by the use of static guidance scales. This â€œone-size-fits-allâ€ approach fails to adapt to the diverse requirements of different prompts; moreover, prior solutions like gradient-based correction or fixed heuristic schedules introduce additional complexities and fail to generalize. In this work, we challeng this static paradigm by introducing a framework for dynamic CFG scheduling. Our method leverages online feedback from a suite of general-purpose and specialized small-scale latent-space evaluations, such as CLIP for alignment, a discriminator for fidelity and a human preference reward model, to assess generation quality at each step of the reverse diffusion process. Based on this feedback, we perform a greedy search to select the optimal CFG scale for each timestep, creating a unique guidance schedule tailored to every prompt and sample. We demonstrate the effectiveness of our approach on both small-scale models and the state-of-the-art Imagen 3, showing significant improvements in text alignment, visual quality, text rendering and numerical reasoning. Notably, when compared against the default Imagen 3 baseline, our method achieves up to 53.8% human preference win-rate for overall preference, a figure that increases up to to 55.5% on prompts targeting specific capabilities like text rendering. Our work establishes that the optimal guidance schedule is inherently dynamic and prompt-dependent, and provides an efficient and generalizable framework to achieve it. </p>
<blockquote>
<p>æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰æ˜¯æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒï¼Œä½†å…¶æœ‰æ•ˆæ€§å—åˆ°é™æ€å¼•å¯¼å°ºåº¦ä½¿ç”¨çš„é™åˆ¶ã€‚â€œä¸€åˆ€åˆ‡â€çš„æ–¹æ³•æ— æ³•æ»¡è¶³ä¸åŒæç¤ºçš„å¤šæ ·åŒ–éœ€æ±‚ï¼›æ­¤å¤–ï¼Œå…ˆå‰çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚åŸºäºæ¢¯åº¦çš„æ ¡æ­£æˆ–å›ºå®šçš„å¯å‘å¼æ—¶é—´è¡¨ï¼Œå¼•å…¥äº†é¢å¤–çš„å¤æ‚æ€§ï¼Œå¹¶ä¸”æ— æ³•æ¨å¹¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥åŠ¨æ€CFGè°ƒåº¦æ¡†æ¶æ¥æŒ‘æˆ˜è¿™ä¸€é™æ€æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸€ç³»åˆ—é€šç”¨å’Œä¸“ç”¨å°å‹æ½œåœ¨ç©ºé—´è¯„ä¼°çš„åœ¨çº¿åé¦ˆï¼Œå¦‚CLIPå¯¹é½ã€é‰´åˆ«å™¨ä¿çœŸåº¦ä»¥åŠäººç±»åå¥½å¥–åŠ±æ¨¡å‹ï¼Œæ¥è¯„ä¼°åå‘æ‰©æ•£è¿‡ç¨‹æ¯ä¸€æ­¥çš„ç”Ÿæˆè´¨é‡ã€‚åŸºäºè¿™äº›åé¦ˆï¼Œæˆ‘ä»¬æ‰§è¡Œè´ªå©ªæœç´¢æ¥é€‰æ‹©æ¯ä¸ªæ—¶é—´æ­¥çš„æœ€ä½³CFGå°ºåº¦ï¼Œä¸ºæ¯ä¸ªæç¤ºå’Œæ ·æœ¬åˆ›å»ºç‹¬ç‰¹çš„æŒ‡å¯¼æ—¶é—´è¡¨ã€‚æˆ‘ä»¬åœ¨å°å‹æ¨¡å‹ä»¥åŠæœ€å…ˆè¿›çš„Imagen 3ä¸Šéƒ½è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ–‡æœ¬å¯¹é½ã€è§†è§‰è´¨é‡ã€æ–‡æœ¬æ¸²æŸ“å’Œæ•°å€¼æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯ä¸é»˜è®¤çš„Imagen 3åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€»ä½“åå¥½ä¸Šè¾¾åˆ°äº†53.8%çš„äººç±»åå¥½èƒœç‡ï¼Œåœ¨é’ˆå¯¹ç‰¹å®šèƒ½åŠ›ï¼ˆå¦‚æ–‡æœ¬æ¸²æŸ“ï¼‰çš„æç¤ºä¸Šï¼Œè¿™ä¸€æ•°å­—æœ€é«˜å¯è¾¾55.5%ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œæœ€ä½³çš„å¼•å¯¼æ—¶é—´è¡¨æœ¬è´¨ä¸Šæ˜¯åŠ¨æ€çš„ï¼Œå¹¶ä¸”ä¾èµ–äºæç¤ºï¼Œå¹¶æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ¨å¹¿çš„æ¡†æ¶æ¥å®ç°å®ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16131v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºåŠ¨æ€åˆ†ç±»å™¨æŒ‡å¯¼è°ƒåº¦æ¡†æ¶çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨çº¿åé¦ˆè¯„ä¼°ç”Ÿæˆè´¨é‡ï¼Œæ ¹æ®åé¦ˆè¿›è¡Œè´ªå©ªæœç´¢ï¼Œä¸ºæ¯ä¸ªæ—¶é—´æ­¥é€‰æ‹©æœ€ä½³çš„CFGè§„æ¨¡ï¼Œä¸ºæ¯ä¸€ä¸ªæç¤ºå’Œæ ·æœ¬é‡èº«å®šåˆ¶ç‹¬ç‰¹çš„æŒ‡å¯¼è®¡åˆ’ã€‚åœ¨å°å‹æ¨¡å‹ä»¥åŠæœ€æ–°Imagen 3æ¨¡å‹ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æé«˜äº†æ–‡æœ¬å¯¹é½ã€è§†è§‰è´¨é‡ã€æ–‡æœ¬æ¸²æŸ“å’Œæ•°å€¼æ¨ç†çš„æ•ˆæœã€‚ç›¸è¾ƒäºé»˜è®¤çš„Imagen 3åŸºçº¿æ¨¡å‹ï¼Œæ–°æ–¹æ³•å–å¾—äº†æœ€é«˜è¾¾åˆ°äººç±»åå¥½ç‡æé«˜è‡³çº¦åŠæ•°ä»¥ä¸Šï¼ˆè¿‘æœ€é«˜è¾¾åˆ°çš„ç”šè‡³è¾¾åˆ°äº†äº”åˆ†ä¹‹ä¸‰ä»¥ä¸Šï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œè¯å®äº†æœ€ä½³æŒ‡å¯¼è®¡åˆ’æœ¬è´¨ä¸Šæ˜¯åŠ¨æ€çš„å¹¶ä¸”ä¾èµ–äºæç¤ºçš„ï¼Œæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ¨å¹¿çš„æ¡†æ¶æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†ç±»å™¨æŒ‡å¯¼ï¼ˆCFGï¼‰æ˜¯æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒï¼Œä½†å…¶æ•ˆæœå—é™äºé™æ€æŒ‡å¯¼è§„æ¨¡çš„ä½¿ç”¨ã€‚</li>
<li>å½“å‰æ–¹æ³•å¦‚åŸºäºæ¢¯åº¦çš„æ ¡æ­£æˆ–å›ºå®šå¯å‘å¼æ—¶é—´è¡¨å¢åŠ äº†å¤æ‚æ€§ä¸”éš¾ä»¥æ¨å¹¿ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€CFGè°ƒåº¦çš„æ¡†æ¶ï¼Œåˆ©ç”¨åœ¨çº¿åé¦ˆè¯„ä¼°ç”Ÿæˆè´¨é‡ã€‚</li>
<li>é€šè¿‡è´ªå©ªæœç´¢é€‰æ‹©æ¯ä¸ªæ—¶é—´æ­¥çš„æœ€ä½³CFGè§„æ¨¡ï¼Œä¸ºæ¯æ¬¡æç¤ºå’Œæ ·æœ¬é‡èº«å®šåˆ¶ç‹¬ç‰¹æŒ‡å¯¼è®¡åˆ’ã€‚</li>
<li>åœ¨å°å‹æ¨¡å‹å’Œæœ€æ–°Imagen 3æ¨¡å‹ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨æ–‡æœ¬å¯¹é½ã€è§†è§‰è´¨é‡ã€æ–‡æœ¬æ¸²æŸ“å’Œæ•°å€¼æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç›¸è¾ƒäºé»˜è®¤åŸºçº¿æ¨¡å‹ï¼Œäººç±»åå¥½ç‡æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16131v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.16131v2/page_1_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Image-to-Brain-Signal-Generation-for-Visual-Prosthesis-with-CLIP-Guided-Multimodal-Diffusion-Models"><a href="#Image-to-Brain-Signal-Generation-for-Visual-Prosthesis-with-CLIP-Guided-Multimodal-Diffusion-Models" class="headerlink" title="Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided   Multimodal Diffusion Models"></a>Image-to-Brain Signal Generation for Visual Prosthesis with CLIP Guided   Multimodal Diffusion Models</h2><p><strong>Authors:Ganxi Xu, Jinyi Long, Jia Zhang</strong></p>
<p>Visual prostheses hold great promise for restoring vision in blind individuals. While researchers have successfully utilized M&#x2F;EEG signals to evoke visual perceptions during the brain decoding stage of visual prostheses, the complementary process of converting images into M&#x2F;EEG signals in the brain encoding stage remains largely unexplored, hindering the formation of a complete functional pipeline. In this work, we present, to our knowledge, the first image-to-brain signal framework that generates M&#x2F;EEG from images by leveraging denoising diffusion probabilistic models enhanced with cross-attention mechanisms. Specifically, the proposed framework comprises two key components: a pretrained CLIP visual encoder that extracts rich semantic representations from input images, and a cross-attention enhanced U-Net diffusion model that reconstructs brain signals through iterative denoising. Unlike conventional generative models that rely on simple concatenation for conditioning, our cross-attention modules capture the complex interplay between visual features and brain signal representations, enabling fine-grained alignment during generation. We evaluate the framework on two multimodal benchmark datasets and demonstrate that it generates biologically plausible brain signals. We also present visualizations of M&#x2F;EEG topographies across all subjects in both datasets, providing intuitive demonstrations of intra-subject and inter-subject variations in brain signals. </p>
<blockquote>
<p>è§†è§‰å‡ä½“å¯¹æ¢å¤ç›²äººçš„è§†åŠ›æœ‰ç€å·¨å¤§æ½œåŠ›ã€‚è™½ç„¶ç ”ç©¶è€…å·²æˆåŠŸåœ¨è§†è§‰å‡ä½“çš„è„‘è§£ç é˜¶æ®µåˆ©ç”¨M&#x2F;EEGä¿¡å·æ¥æ¿€å‘è§†è§‰æ„ŸçŸ¥ï¼Œä½†å¤§è„‘ç¼–ç é˜¶æ®µå°†å›¾åƒè½¬æ¢ä¸ºM&#x2F;EEGä¿¡å·è¿™ä¸€äº’è¡¥è¿‡ç¨‹ä»è¢«å¤§å¤§å¿½è§†ï¼Œé˜»ç¢äº†å®Œæ•´åŠŸèƒ½ç®¡é“çš„å½¢æˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ˆç°äº†ï¼ˆæ®æˆ‘ä»¬æ‰€çŸ¥ï¼‰é¦–ä¸ªå›¾åƒåˆ°è„‘ä¿¡å·çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å€ŸåŠ©å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹å¹¶å¢å¼ºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿä»å›¾åƒç”ŸæˆM&#x2F;EEGä¿¡å·ã€‚å…·ä½“æ¥è¯´ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨ï¼Œç”¨äºä»è¾“å…¥å›¾åƒä¸­æå–ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºï¼›å¢å¼ºäº¤å‰æ³¨æ„åŠ›çš„U-Netæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è¿­ä»£å»å™ªé‡å»ºè„‘ä¿¡å·ã€‚ä¸åŒäºä¾èµ–ç®€å•æ‹¼æ¥è¿›è¡Œæ¡ä»¶è®¾ç½®çš„ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬çš„äº¤å‰æ³¨æ„åŠ›æ¨¡å—æ•æ‰è§†è§‰ç‰¹å¾å’Œè„‘ä¿¡å·è¡¨ç¤ºä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œä»è€Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°ç²¾ç»†å¯¹é½ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¤šæ¨¡å¼åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œå¹¶è¯æ˜å…¶èƒ½å¤Ÿç”Ÿæˆç”Ÿç‰©ä¸Šåˆç†çš„è„‘ä¿¡å·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸¤ä¸ªæ•°æ®é›†ä¸­æ‰€æœ‰å—è¯•è€…çš„M&#x2F;EEGåœ°å½¢å›¾å¯è§†åŒ–ï¼Œç›´è§‚åœ°å±•ç¤ºäº†è„‘ä¿¡å·çš„å—è¯•è€…å†…å’Œå—è¯•è€…é—´å˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00787v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ä¸äº¤å‰æ³¨æ„æœºåˆ¶çš„å›¾åƒåˆ°è„‘ä¿¡å·æ¡†æ¶ï¼Œç”¨äºå°†å›¾åƒè½¬åŒ–ä¸ºM&#x2F;EEGä¿¡å·ï¼Œä¸ºè§†è§‰å‡ä½“çš„ç ”ç©¶æä¾›äº†é‡è¦è¿›å±•ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬é¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨å’Œå¢å¼ºå‹U-Netæ‰©æ•£æ¨¡å‹ä¸¤éƒ¨åˆ†ï¼Œèƒ½å¤Ÿæå–å›¾åƒä¸­çš„ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºå¹¶é‡å»ºè„‘ä¿¡å·ã€‚é€šè¿‡äº¤å‰æ³¨æ„æ¨¡å—æ•æ‰è§†è§‰ç‰¹å¾ä¸è„‘ä¿¡å·è¡¨ç¤ºä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œå®ç°äº†ç²¾ç»†å¯¹é½ç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨ä¸¤ä¸ªå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆç”Ÿç‰©å­¦ä¸Šåˆç†çš„è„‘ä¿¡å·ï¼Œå¹¶å‘ˆç°ä¸åŒæ•°æ®é›†å†…ä¸ªä½“å’Œä¸ªä½“é—´M&#x2F;EEGåœ°å½¢å›¾çš„å¯è§†åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜åˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ä¸äº¤å‰æ³¨æ„æœºåˆ¶æå‡ºäº†é¦–ä¸ªå›¾åƒåˆ°è„‘ä¿¡å·çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨è§†è§‰å‡ä½“çš„ç ”ç©¶ä¸Šå–å¾—äº†é‡è¦è¿›å±•ã€‚</li>
<li>æ¡†æ¶åŒ…å«é¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨å’Œå¢å¼ºå‹U-Netæ‰©æ•£æ¨¡å‹ä¸¤éƒ¨åˆ†ï¼Œå‰è€…ç”¨äºæå–å›¾åƒè¯­ä¹‰ä¿¡æ¯ï¼Œåè€…åˆ™é€šè¿‡è¿­ä»£å»å™ªé‡å»ºè„‘ä¿¡å·ã€‚</li>
<li>äº¤å‰æ³¨æ„æ¨¡å—èƒ½å¤Ÿæ•æ‰è§†è§‰ç‰¹å¾ä¸è„‘ä¿¡å·ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œå®ç°æ›´ç²¾ç»†çš„ç”Ÿæˆå¯¹é½ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ä¸¤ä¸ªå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶ç”Ÿæˆç”Ÿç‰©å­¦ä¸Šåˆç†è„‘ä¿¡å·çš„èƒ½åŠ›ã€‚</li>
<li>æä¾›äº†ä¸åŒæ•°æ®é›†å†…ä¸ªä½“å’Œä¸ªä½“é—´M&#x2F;EEGåœ°å½¢å›¾çš„ç›´è§‚å¯è§†åŒ–å±•ç¤ºã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè§†è§‰å‡ä½“çš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†é‡è¦æ”¯æŒï¼Œæœ‰æœ›ä¸ºç›²äººçš„è§†è§‰æ¢å¤å¸¦æ¥å®è´¨æ€§è¿›å±•ã€‚</li>
<li>ç›®å‰è¯¥è¿‡ç¨‹ä»å¤„äºæ¢ç´¢é˜¶æ®µï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›ä»¥å®ç°å®é™…åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.00787v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.00787v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.00787v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.00787v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.00787v3/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2509.00787v3/page_5_2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Subjective-Camera-1-0-Bridging-Human-Cognition-and-Visual-Reconstruction-through-Sequence-Aware-Sketch-Guided-Diffusion"><a href="#Subjective-Camera-1-0-Bridging-Human-Cognition-and-Visual-Reconstruction-through-Sequence-Aware-Sketch-Guided-Diffusion" class="headerlink" title="Subjective Camera 1.0: Bridging Human Cognition and Visual   Reconstruction through Sequence-Aware Sketch-Guided Diffusion"></a>Subjective Camera 1.0: Bridging Human Cognition and Visual   Reconstruction through Sequence-Aware Sketch-Guided Diffusion</h2><p><strong>Authors:Haoyang Chen, Dongfang Sun, Caoyuan Ma, Shiqin Wang, Kewei Zhang, Zheng Wang, Zhixiang Wang</strong></p>
<p>We introduce the concept of a subjective camera to reconstruct meaningful moments that physical cameras fail to capture. We propose Subjective Camera 1.0, a framework for reconstructing real-world scenes from readily accessible subjective readouts, i.e., textual descriptions and progressively drawn rough sketches. Built on optimization-based alignment of diffusion models, our approach avoids large-scale paired training data and mitigates generalization issues. To address the challenge of integrating multiple abstract concepts in real-world scenarios, we design a Sequence-Aware Sketch-Guided Diffusion framework with three loss terms for concept-wise sequential optimization, following the natural order of subjective readouts. Experiments on two datasets demonstrate that our method achieves state-of-the-art performance in image quality as well as spatial and semantic alignment with target scenes. User studies with 40 participants further confirm that our approach is consistently preferred. Our project page is at: subjective-camera.github.io </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸»è§‚ç›¸æœºçš„æ¦‚å¿µï¼Œä»¥é‡å»ºç‰©ç†ç›¸æœºæœªèƒ½æ•æ‰çš„æœ‰æ„ä¹‰æ—¶åˆ»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸»è§‚ç›¸æœº1.0ï¼Œè¿™æ˜¯ä¸€ä¸ªä»æ˜“äºè·å–çš„ä¸»è§‚è¯»æ•°ï¼ˆå¦‚æ–‡æœ¬æè¿°å’Œé€æ­¥ç»˜åˆ¶çš„ç²—ç•¥è‰å›¾ï¼‰é‡å»ºç°å®åœºæ™¯çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨åŸºäºä¼˜åŒ–çš„æ‰©æ•£æ¨¡å‹å¯¹é½åŸºç¡€ä¸Šï¼Œé¿å…äº†å¤§è§„æ¨¡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œå¹¶ç¼“è§£äº†æ³›åŒ–é—®é¢˜ã€‚ä¸ºäº†è§£å†³åœ¨ç°å®åœºæ™¯ä¸­æ•´åˆå¤šä¸ªæŠ½è±¡æ¦‚å¿µçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåºåˆ—æ„ŸçŸ¥è‰å›¾å¼•å¯¼æ‰©æ•£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªç”¨äºæ¦‚å¿µçº§é¡ºåºä¼˜åŒ–çš„æŸå¤±é¡¹ï¼Œéµå¾ªä¸»è§‚è¯»æ•°çš„è‡ªç„¶é¡ºåºã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒè´¨é‡ä»¥åŠç©ºé—´å’Œç›®æ ‡åœºæ™¯è¯­ä¹‰å¯¹é½æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æœ‰40åå‚ä¸è€…å‚ä¸çš„ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä¸ºï¼š[ä¸»è§‚ç›¸æœºçš„github.ioé¡µé¢]ï¼ˆsubjective-camera.github.ioï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23711v3">PDF</a> </p>
<p><strong>Summary</strong><br>ä¸»è§‚ç›¸æœºçš„æ¦‚å¿µè¢«å¼•å…¥ä»¥é‡å»ºç‰©ç†ç›¸æœºæ— æ³•æ•æ‰çš„æœ‰æ„ä¹‰æ—¶åˆ»ã€‚æˆ‘ä»¬æå‡ºä¸»è§‚ç›¸æœº1.0ï¼Œè¿™æ˜¯ä¸€ä¸ªä»æ˜“äºè·å–çš„ä¸»è§‚è¯»æ•°ï¼ˆå¦‚æ–‡æœ¬æè¿°å’Œæ¸è¿›å¼ç²—ç•¥è‰å›¾ï¼‰é‡å»ºç°å®åœºæ™¯çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¼˜åŒ–å¯¹é½ï¼Œé¿å…äº†å¤§è§„æ¨¡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œå¹¶ç¼“è§£äº†æ³›åŒ–é—®é¢˜ã€‚ä¸ºäº†è§£å†³å°†å¤šä¸ªæŠ½è±¡æ¦‚å¿µèå…¥ç°å®åœºæ™¯çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåºåˆ—æ„ŸçŸ¥è‰å›¾å¼•å¯¼æ‰©æ•£æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªç”¨äºæ¦‚å¿µçº§é¡ºåºä¼˜åŒ–çš„æŸå¤±é¡¹ï¼Œéµå¾ªä¸»è§‚è¯»æ•°çš„è‡ªç„¶é¡ºåºã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒè´¨é‡ä»¥åŠç©ºé—´å’Œç›®æ ‡åœºæ™¯è¯­ä¹‰å¯¹é½æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ç”±40åå‚ä¸è€…è¿›è¡Œçš„ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸»è§‚ç›¸æœºçš„æ¦‚å¿µï¼Œç”¨ä»¥é‡å»ºç‰©ç†ç›¸æœºæœªèƒ½æ•æ‰çš„æœ‰æ„ä¹‰æ—¶åˆ»ã€‚</li>
<li>æå‡ºäº†Subjective Camera 1.0æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½é€šè¿‡æ–‡æœ¬æè¿°å’Œç²—ç•¥è‰å›¾é‡å»ºç°å®åœºæ™¯ã€‚</li>
<li>åˆ©ç”¨ä¼˜åŒ–å¯¹é½çš„æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¤§è§„æ¨¡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œå¹¶å‡è½»æ³›åŒ–é—®é¢˜ã€‚</li>
<li>è®¾è®¡äº†åºåˆ—æ„ŸçŸ¥è‰å›¾å¼•å¯¼æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªæŸå¤±é¡¹è¿›è¡Œæ¦‚å¿µçº§çš„é¡ºåºä¼˜åŒ–ã€‚</li>
<li>æ¡†æ¶éµå¾ªä¸»è§‚è¯»æ•°çš„è‡ªç„¶é¡ºåºï¼Œä»¥æ›´å¥½åœ°èå…¥å¤šä¸ªæŠ½è±¡æ¦‚å¿µäºç°å®åœºæ™¯ã€‚</li>
<li>åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡åŠç©ºé—´ã€è¯­ä¹‰å¯¹é½æ–¹é¢è¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.23711v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.23711v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.23711v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.23711v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.23711v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Single-step-Diffusion-for-Image-Compression-at-Ultra-Low-Bitrates"><a href="#Single-step-Diffusion-for-Image-Compression-at-Ultra-Low-Bitrates" class="headerlink" title="Single-step Diffusion for Image Compression at Ultra-Low Bitrates"></a>Single-step Diffusion for Image Compression at Ultra-Low Bitrates</h2><p><strong>Authors:Chanung Park, Joo Chan Lee, Jong Hwan Ko</strong></p>
<p>Although there have been significant advancements in image compression techniques, such as standard and learned codecs, these methods still suffer from severe quality degradation at extremely low bits per pixel. While recent diffusion-based models provided enhanced generative performance at low bitrates, they often yields limited perceptual quality and prohibitive decoding latency due to multiple denoising steps. In this paper, we propose the single-step diffusion model for image compression that delivers high perceptual quality and fast decoding at ultra-low bitrates. Our approach incorporates two key innovations: (i) Vector-Quantized Residual (VQ-Residual) training, which factorizes a structural base code and a learned residual in latent space, capturing both global geometry and high-frequency details; and (ii) rate-aware noise modulation, which tunes denoising strength to match the desired bitrate. Extensive experiments show that ours achieves comparable compression performance to state-of-the-art methods while improving decoding speed by about 50x compared to prior diffusion-based methods, greatly enhancing the practicality of generative codecs. </p>
<blockquote>
<p>å°½ç®¡å›¾åƒå‹ç¼©æŠ€æœ¯ï¼ˆå¦‚æ ‡å‡†å’ŒåŸºäºå­¦ä¹ çš„ç¼–ç æŠ€æœ¯ï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¿™äº›æ–¹æ³•ä»ç„¶é¢ä¸´æä½æ¯”ç‰¹ç‡ä¸‹çš„ä¸¥é‡è´¨é‡ä¸‹é™é—®é¢˜ã€‚è™½ç„¶æœ€è¿‘çš„åŸºäºæ‰©æ•£çš„æ¨¡å‹åœ¨ä½æ¯”ç‰¹ç‡ä¸‹æä¾›äº†å¢å¼ºçš„ç”Ÿæˆæ€§èƒ½ï¼Œä½†ç”±äºå¤šæ¬¡å»å™ªæ­¥éª¤ï¼Œå®ƒä»¬é€šå¸¸äº§ç”Ÿæœ‰é™çš„æ„ŸçŸ¥è´¨é‡å’Œç¦æ­¢çš„è§£ç å»¶è¿Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå›¾åƒå‹ç¼©çš„å•æ­¥æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹æä¾›é«˜æ„ŸçŸ¥è´¨é‡å’Œå¿«é€Ÿè§£ç ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆiï¼‰å‘é‡é‡åŒ–æ®‹å·®ï¼ˆVQ-Residualï¼‰è®­ç»ƒï¼Œå®ƒåœ¨æ½œåœ¨ç©ºé—´ä¸­åˆ†è§£ç»“æ„åŸºç¡€ä»£ç å’Œå­¦ä¹ çš„æ®‹å·®ï¼Œæ•è·å…¨å±€å‡ ä½•å’Œé«˜é¢‘ç»†èŠ‚ï¼›ï¼ˆiiï¼‰é€Ÿç‡æ„ŸçŸ¥å™ªå£°è°ƒåˆ¶ï¼Œå®ƒè°ƒæ•´å»å™ªå¼ºåº¦ä»¥åŒ¹é…æ‰€éœ€çš„æ¯”ç‰¹ç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸å…ˆå‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œè§£ç é€Ÿåº¦æé«˜äº†çº¦50å€ï¼Œå¤§å¤§æé«˜äº†ç”Ÿæˆç¼–ç å™¨çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16572v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå•æ­¥æ‰©æ•£æ¨¡å‹çš„å›¾åƒå‹ç¼©æ–¹æ³•ï¼Œå¯åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹å®ç°é«˜æ„ŸçŸ¥è´¨é‡å’Œå¿«é€Ÿè§£ç ã€‚è¯¥æ–¹æ³•é€šè¿‡VQ-Residualè®­ç»ƒå’Œé€Ÿç‡æ„ŸçŸ¥å™ªå£°è°ƒåˆ¶ä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼Œå®ç°äº†ç»“æ„åŸºç¡€ç å’Œå­¦å¾—æ®‹å·®çš„å› å­åˆ†è§£ï¼Œæ•æ‰äº†å…¨å±€å‡ ä½•ç»“æ„å’Œé«˜é¢‘ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‹ç¼©æ€§èƒ½ä¸Šä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ï¼Œè§£ç é€Ÿåº¦æ¯”å…ˆå‰çš„æ‰©æ•£æ–¹æ³•æé«˜äº†çº¦50å€ï¼Œæå¤§åœ°æé«˜äº†ç”Ÿæˆç å®çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå‹ç¼©é¢†åŸŸçš„åº”ç”¨è¢«ä»‹ç»ï¼Œå°¤å…¶æ˜¯å…¶å¢å¼ºç”Ÿæˆæ€§èƒ½å’Œåœ¨ä½æ¯”ç‰¹ç‡ä¸‹çš„è¡¨ç°ã€‚</li>
<li>å½“å‰å›¾åƒå‹ç¼©æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬è´¨é‡é™è§£å’Œè§£ç å»¶è¿Ÿã€‚</li>
<li>æå‡ºçš„å•æ­¥æ‰©æ•£æ¨¡å‹æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°äº†é«˜æ„ŸçŸ¥è´¨é‡å’Œå¿«é€Ÿè§£ç ã€‚</li>
<li>VQ-Residualè®­ç»ƒæ˜¯å…³é”®æŠ€æœ¯ä¹‹ä¸€ï¼Œå®ç°äº†ç»“æ„åŸºç¡€ç å’Œå­¦å¾—æ®‹å·®çš„å› å­åˆ†è§£ã€‚</li>
<li>é€Ÿç‡æ„ŸçŸ¥å™ªå£°è°ƒåˆ¶æŠ€æœ¯ç”¨äºè°ƒæ•´å»å™ªå¼ºåº¦ä»¥åŒ¹é…æ‰€éœ€çš„æ¯”ç‰¹ç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‹ç¼©æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.16572v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.16572v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.16572v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.16572v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.16572v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.16572v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2506.16572v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models"><a href="#QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models" class="headerlink" title="QVGen: Pushing the Limit of Quantized Video Generative Models"></a>QVGen: Pushing the Limit of Quantized Video Generative Models</h2><p><strong>Authors:Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, Jun Zhang</strong></p>
<p>Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\Phi$, we propose a rank-decay strategy that progressively eliminates $\Phi$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\mathbf{\gamma}$ to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench. </p>
<blockquote>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„è§†é¢‘åˆæˆã€‚ç„¶è€Œï¼Œå…¶å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚å¯¹å®é™…éƒ¨ç½²å¸¦æ¥äº†ä¸¥å³»æŒ‘æˆ˜ï¼Œå³ä½¿åœ¨é«˜ç«¯GPUä¸Šä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä½œä¸ºä¸€ç§å¸¸ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œé‡åŒ–åœ¨é™ä½å›¾åƒDMçš„æˆæœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œè€Œç›´æ¥åº”ç”¨äºè§†é¢‘DMåˆ™ä»ç„¶æ— æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†QVGenï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æç«¯ä½æ¯”ç‰¹é‡åŒ–ï¼ˆä¾‹å¦‚4ä½åŠä»¥ä¸‹ï¼‰ä¸‹é«˜æ€§èƒ½å’Œæ¨ç†æ•ˆç‡çš„è§†é¢‘DMé‡èº«å®šåˆ¶çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œç†è®ºåˆ†æï¼Œè¯æ˜é™ä½æ¢¯åº¦èŒƒæ•°å¯¹äºä¿ƒè¿›QATçš„æ”¶æ•›è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¾…åŠ©æ¨¡å—ï¼ˆÎ¦ï¼‰æ¥ç¼“è§£å¤§é‡åŒ–è¯¯å·®ï¼Œä»è€Œæ˜¾ç€å¢å¼ºæ”¶æ•›æ€§ã€‚ä¸ºäº†æ¶ˆé™¤Î¦çš„æ¨ç†å¼€é”€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ’åè¡°å‡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€æ­¥æ¶ˆé™¤Î¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åå¤ä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å’Œæå‡ºçš„åŸºäºæ’åçš„æ­£åˆ™åŒ–Î³æ¥è¯†åˆ«å’Œè¡°å‡ä½è´¡çŒ®æˆåˆ†ã€‚æ­¤ç­–ç•¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ¶ˆé™¤äº†æ¨ç†å¼€é”€ã€‚åœ¨4ç§æœ€å…ˆè¿›çš„è§†é¢‘DMsä¸Šçš„å¹¿æ³›å®éªŒï¼Œå‚æ•°å¤§å°ä»1.3Båˆ°14Bä¸ç­‰ï¼Œè¡¨æ˜QVGené¦–æ¬¡åœ¨4ä½è®¾ç½®ä¸‹è¾¾åˆ°å…¨ç²¾åº¦ç›¸å½“çš„è´¨é‡ã€‚è€Œä¸”ï¼Œå®ƒæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„3ä½CogVideoX-2Båœ¨VBenchä¸Šçš„åŠ¨æ€åº¦æé«˜äº†+25.28ï¼Œåœºæ™¯ä¸€è‡´æ€§æé«˜äº†+8.43ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11497v3">PDF</a> Our code will be released upon acceptance</p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è™½ç„¶å¯ä»¥å®ç°é«˜è´¨é‡çš„è§†é¢‘åˆæˆï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚é˜»ç¢äº†å®é™…åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç«¯GPUä¸Šçš„åº”ç”¨ã€‚é‡åŒ–æ˜¯ä¸€ç§å¹¿æ³›é‡‡ç”¨çš„æ–¹æ³•ï¼Œèƒ½å¤ŸæˆåŠŸé™ä½å›¾åƒDMçš„æˆæœ¬ï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘DMåˆ™æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡æå‡ºQVGenï¼Œä¸€ä¸ªé’ˆå¯¹é«˜æ€§èƒ½å’Œæ¨ç†æ•ˆç‡çš„è§†é¢‘DMçš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¡†æ¶ï¼Œå¯åœ¨æä½æ¯”ç‰¹é‡åŒ–ï¼ˆå¦‚4ä½åŠä»¥ä¸‹ï¼‰ä¸‹å·¥ä½œã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œå‡å°‘æ¢¯åº¦èŒƒæ•°æ˜¯ä¿ƒè¿›QATæ”¶æ•›çš„å…³é”®ã€‚é€šè¿‡å¼•å…¥è¾…åŠ©æ¨¡å—æ¥å‡è½»å¤§é‡é‡åŒ–è¯¯å·®ï¼Œæ˜¾è‘—æé«˜äº†æ”¶æ•›æ€§ã€‚ä¸ºäº†æ¶ˆé™¤è¾…åŠ©æ¨¡å—çš„æ¨ç†å¼€é”€ï¼Œæå‡ºäº†ä¸€ç§æ’åè¡°å‡ç­–ç•¥ï¼Œé€šè¿‡é€æ­¥æ¶ˆé™¤è¾…åŠ©æ¨¡å—å®ç°æ€§èƒ½ä¿ç•™å¹¶æ¶ˆé™¤æ¨ç†å¼€é”€ã€‚å®éªŒè¯æ˜ï¼ŒQVGenåœ¨4ä½è®¾ç½®ä¸‹é¦–æ¬¡è¾¾åˆ°å…¨ç²¾åº¦å¯æ¯”è´¨é‡ï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„3ä½CogVideoX-2Båœ¨VBenchä¸Šçš„åŠ¨æ€åº¦å’Œåœºæ™¯ä¸€è‡´æ€§åˆ†åˆ«æé«˜äº†+25.28å’Œ+8.43ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰é¢ä¸´ç€å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚æŒ‘æˆ˜ã€‚</li>
<li>é‡åŒ–æ˜¯ä¸€ç§æœ‰æ•ˆçš„é™ä½å›¾åƒDMæˆæœ¬çš„æ–¹æ³•ï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘DMæ•ˆæœä¸ä½³ã€‚</li>
<li>QVGenæ˜¯ä¸€ä¸ªé’ˆå¯¹è§†é¢‘DMçš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¡†æ¶ï¼Œé€‚ç”¨äºæä½æ¯”ç‰¹é‡åŒ–è®¾ç½®ã€‚</li>
<li>ç†è®ºåˆ†æè¡¨æ˜å‡å°‘æ¢¯åº¦èŒƒæ•°æ˜¯ä¿ƒè¿›QATæ”¶æ•›çš„å…³é”®ã€‚</li>
<li>å¼•å…¥è¾…åŠ©æ¨¡å—æ¥å‡è½»é‡åŒ–è¯¯å·®ï¼Œæé«˜æ”¶æ•›æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ’åè¡°å‡ç­–ç•¥ï¼Œé€æ­¥æ¶ˆé™¤è¾…åŠ©æ¨¡å—ï¼Œæ¶ˆé™¤æ¨ç†å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2505.11497v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2505.11497v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2505.11497v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2505.11497v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2505.11497v3/page_4_1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DiCo-Revitalizing-ConvNets-for-Scalable-and-Efficient-Diffusion-Modeling"><a href="#DiCo-Revitalizing-ConvNets-for-Scalable-and-Efficient-Diffusion-Modeling" class="headerlink" title="DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion   Modeling"></a>DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion   Modeling</h2><p><strong>Authors:Yuang Ai, Qihang Fan, Xuefeng Hu, Zhenheng Yang, Ran He, Huaibo Huang</strong></p>
<p>Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns-highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet generation benchmarks, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53 at 512x512, with a 2.7x and 3.1x speedup over DiT-XL&#x2F;2, respectively. Furthermore, experimental results on MS-COCO demonstrate that the purely convolutional DiCo exhibits strong potential for text-to-image generation. Code: <a target="_blank" rel="noopener" href="https://github.com/shallowdream204/DiCo">https://github.com/shallowdream204/DiCo</a>. </p>
<blockquote>
<p>æ‰©æ•£Transformerï¼ˆDiTï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§†è§‰ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œå®ƒè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœï¼Œä½†å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¯¹é¢„è®­ç»ƒçš„DiTæ¨¡å‹çš„åˆ†æè¡¨æ˜ï¼Œå…¨å±€è‡ªæ³¨æ„åŠ›é€šå¸¸æ˜¯å†—ä½™çš„ï¼Œä¸»è¦æ•æ‰å±€éƒ¨æ¨¡å¼ï¼Œè¿™çªæ˜¾äº†æ›´é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†å·ç§¯ä½œä¸ºæ„å»ºé«˜æ•ˆä¸”è¡¨è¾¾æ€§æ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ„å»ºå—ã€‚ç„¶è€Œï¼Œå¤©çœŸåœ°ç”¨å·ç§¯æ›¿æ¢è‡ªæ³¨æ„åŠ›é€šå¸¸ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å°†è¿™ä¸€æ€§èƒ½å·®è·å½’å› äºä¸Transformerç›¸æ¯”ï¼ŒConvNetä¸­çš„é€šé“å†—ä½™åº¦æ›´é«˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç´§å‡‘çš„é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¿ƒè¿›äº†æ›´å¤šä¸åŒé€šé“çš„æ¿€æ´»ï¼Œä»è€Œå¢å¼ºäº†ç‰¹å¾å¤šæ ·æ€§ã€‚è¿™å¯¼è‡´äº†å®Œå…¨ç”±æ ‡å‡†ConvNetæ¨¡å—æ„å»ºçš„æ‰©æ•£æ¨¡å‹å®¶æ—â€”â€”æ‰©æ•£å·ç§¯ç½‘ç»œï¼ˆDiCoï¼‰ã€‚åœ¨æ¡ä»¶ImageNetç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDiCo-XLåœ¨256x256åˆ†è¾¨ç‡ä¸‹å®ç°äº†FIDä¸º2.05ï¼Œåœ¨512x512åˆ†è¾¨ç‡ä¸‹å®ç°äº†2.53ï¼Œç›¸å¯¹äºDiT-XL&#x2F;2åˆ†åˆ«å®ç°äº†2.7xå’Œ3.1xçš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œåœ¨MS-COCOä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå®Œå…¨å·ç§¯çš„DiCoåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/shallowdream204/DiCo%E3%80%82">https://github.com/shallowdream204/DiCoã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11196v2">PDF</a> NeurIPS 2025 Spotlight</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diffusion Transformerï¼ˆDiTï¼‰åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æ·±å…¥ç ”ç©¶é¢„è®­ç»ƒçš„DiTæ¨¡å‹ï¼Œå‘ç°å…¨å±€è‡ªæ³¨æ„åŠ›å¸¸å¸¸æ˜¯å†—ä½™çš„ï¼Œä¸»è¦æ•æ‰çš„æ˜¯å±€éƒ¨æ¨¡å¼ã€‚äºæ˜¯å¼•å…¥å·ç§¯ä½œä¸ºæ„å»ºé«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£ç»„ä»¶ï¼Œæå‡ºäº†Diffusion ConvNetï¼ˆDiCoï¼‰ã€‚ä¸ºäº†å¼¥è¡¥å·ç§¯ç½‘ç»œé€šé“å†—ä½™çš„é—®é¢˜ï¼Œå¼•å…¥äº†ç´§å‡‘é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†ç‰¹å¾å¤šæ ·æ€§ã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒDiCoå®ç°äº†å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½å’Œæ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚åœ¨ImageNetå’ŒMS-COCOæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDiCoå…·æœ‰å¾ˆå¼ºçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ½œåŠ›ã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹æˆ–å…³é”®ä¿¡æ¯ç‚¹ï¼š</p>
<ul>
<li>Diffusion Transformerï¼ˆDiTï¼‰åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚</li>
<li>é¢„è®­ç»ƒçš„DiTæ¨¡å‹åˆ†ææ˜¾ç¤ºå…¨å±€è‡ªæ³¨æ„åŠ›å¸¸å¸¸å†—ä½™ï¼Œä¸»è¦æ•æ‰å±€éƒ¨æ¨¡å¼ã€‚</li>
<li>å¼•å…¥å·ç§¯ä½œä¸ºæ„å»ºé«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£ç»„ä»¶ã€‚</li>
<li>ä»…ä½¿ç”¨å·ç§¯ç½‘ç»œæ„å»ºçš„Diffusion ConvNetï¼ˆDiCoï¼‰å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>ç´§å‡‘é€šé“æ³¨æ„åŠ›æœºåˆ¶æé«˜äº†ç‰¹å¾å¤šæ ·æ€§å¹¶è§£å†³äº†å·ç§¯ç½‘ç»œé€šé“å†—ä½™çš„é—®é¢˜ã€‚</li>
<li>DiCoåœ¨ImageNetå’ŒMS-COCOæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½å¼ºå¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2505.11196v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2505.11196v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2505.11196v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Diffusion Models/2505.11196v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_åŒ»å­¦å›¾åƒ/2509.17726v1/page_4_0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Towards Seeing Bones at Radio Frequency
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_NeRF/2509.15123v2/page_3_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  From Restoration to Reconstruction Rethinking 3D Gaussian Splatting for   Underwater Scenes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
