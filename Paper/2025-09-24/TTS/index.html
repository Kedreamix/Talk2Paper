<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-09-24  TMD-TTS A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for   Ü-Tsang, Amdo and Kham Speech Dataset Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18004v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-24-更新"><a href="#2025-09-24-更新" class="headerlink" title="2025-09-24 更新"></a>2025-09-24 更新</h1><h2 id="TMD-TTS-A-Unified-Tibetan-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation"><a href="#TMD-TTS-A-Unified-Tibetan-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation" class="headerlink" title="TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for   Ü-Tsang, Amdo and Kham Speech Dataset Generation"></a>TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for   Ü-Tsang, Amdo and Kham Speech Dataset Generation</h2><p><strong>Authors:Yutong Liu, Ziyue Zhang, Ban Ma-bao, Renzeng Duojie, Yuqing Cai, Yongbin Yu, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi</strong></p>
<p>Tibetan is a low-resource language with limited parallel speech corpora spanning its three major dialects (&quot;U-Tsang, Amdo, and Kham), limiting progress in speech modeling. To address this issue, we propose TMD-TTS, a unified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes parallel dialectal speech from explicit dialect labels. Our method features a dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects. Extensive objective and subjective evaluations demonstrate that TMD-TTS significantly outperforms baselines in dialectal expressiveness. We further validate the quality and utility of the synthesized speech through a challenging Speech-to-Speech Dialect Conversion (S2SDC) task. </p>
<blockquote>
<p>藏语是一种资源有限的语种，其三大方言（乌斯藏、安多和康巴）的平行语音语料库有限，限制了语音建模的进展。为了解决这一问题，我们提出了TMD-TTS，这是一个统一的藏语多方言文本到语音（TTS）框架，它可以根据明确的方言标签合成平行方言语音。我们的方法具有方言融合模块和方言专用动态路由网络（DSDR-Net），可以捕捉不同方言之间的细微声音和语言变化。大量的客观和主观评估表明，在方言表现力方面，TMD-TTS显著优于基线。我们进一步通过具有挑战性的语音到语音方言转换（S2SDC）任务验证了合成语音的质量和实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18060v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种针对藏语的统一多方言文本转语音（TTS）框架TMD-TTS，解决了藏语资源匮乏的问题。该框架能够合成包含明确方言标签的并行方言语音。通过采用方言融合模块和方言专用动态路由网络（DSDR-Net），该框架能够捕捉不同方言之间的细微声音和语言学差异。客观和主观评估均表明，TMD-TTS在方言表现力方面显著优于基准模型。此外，通过挑战性的语音到语音方言转换（S2SDC）任务验证了合成语音的质量和实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>藏语是一种资源匮乏的语言，其三大方言（U-Tsang、Amdo和Kham）的平行语音语料库有限，限制了语音建模的进展。</li>
<li>提出了一种统一的藏语多方言文本转语音（TTS）框架TMD-TTS，能够合成包含明确方言标签的并行方言语音。</li>
<li>TMD-TTS框架包含方言融合模块和方言专用动态路由网络（DSDR-Net），用于捕捉不同方言之间的细微声音和语言学差异。</li>
<li>客观和主观评估表明，TMD-TTS在方言表现力方面优于基准模型。</li>
<li>合成语音的质量和实用性通过挑战性的语音到语音方言转换（S2SDC）任务得到验证。</li>
<li>TMD-TTS框架有助于推动藏语方言的语音研究和应用发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18060">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18060v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18060v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18060v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18060v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18060v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18060v1/page_3_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WenetSpeech-Chuan-A-Large-Scale-Sichuanese-Corpus-with-Rich-Annotation-for-Dialectal-Speech-Processing"><a href="#WenetSpeech-Chuan-A-Large-Scale-Sichuanese-Corpus-with-Rich-Annotation-for-Dialectal-Speech-Processing" class="headerlink" title="WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation   for Dialectal Speech Processing"></a>WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation   for Dialectal Speech Processing</h2><p><strong>Authors:Yuhang Dai, Ziyu Zhang, Shuai Wang, Longhao Li, Zhao Guo, Tianlun Zuo, Shuiyuan Wang, Hongfei Xue, Chengyou Wang, Qing Wang, Xin Xu, Hui Bu, Jie Li, Jian Kang, Binbin Zhang, Lei Xie</strong></p>
<p>The scarcity of large-scale, open-source data for dialects severely hinders progress in speech technology, a challenge particularly acute for the widely spoken Sichuanese dialects of Chinese. To address this critical gap, we introduce WenetSpeech-Chuan, a 10,000-hour, richly annotated corpus constructed using our novel Chuan-Pipeline, a complete data processing framework for dialectal speech. To facilitate rigorous evaluation and demonstrate the corpus’s effectiveness, we also release high-quality ASR and TTS benchmarks, WenetSpeech-Chuan-Eval, with manually verified transcriptions. Experiments show that models trained on WenetSpeech-Chuan achieve state-of-the-art performance among open-source systems and demonstrate results comparable to commercial services. As the largest open-source corpus for Sichuanese dialects, WenetSpeech-Chuan not only lowers the barrier to research in dialectal speech processing but also plays a crucial role in promoting AI equity and mitigating bias in speech technologies. The corpus, benchmarks, models, and receipts are publicly available on our project page. </p>
<blockquote>
<p>方言大规模开源数据的稀缺严重阻碍了语音技术的进步，对于广泛使用的四川方言来说，这一挑战尤为严峻。为了解决这一关键空白，我们推出了WenetSpeech-Chuan，这是一个使用我们新型Chuan-Pipeline构建而成的10000小时丰富注释语料库，这是一个完整的方言语音数据处理框架。为了促进严格评估并证明语料库的有效性，我们还发布了高质量的ASR和TTS基准测试WenetSpeech-Chuan-Eval，其中包含手动验证的转录。实验表明，在WenetSpeech-Chuan上训练的模型在开源系统中处于最新技术性能水平，并且显示的结果与商业服务相当。作为四川方言最大的开源语料库，WenetSpeech-Chuan不仅降低了方言语音处理的研究壁垒，而且在促进人工智能公平和缓解语音技术中的偏见方面也发挥了关键作用。语料库、基准测试、模型和收据均可在我们的项目页面上公开获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.18004v1">PDF</a> 4 pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>这是一篇关于WenetSpeech-Chuan的介绍，它是一个针对四川话的大型、开源语音数据语料库。该语料库采用创新的Chuan-Pipeline数据处理框架构建而成，包括丰富的注释。此外，还发布了高质量的自评估报告和语音合成报告来评估语料库的有效性。实验表明，基于WenetSpeech-Chuan训练的模型在开放系统中表现卓越，与商业服务相比也有竞争力。该语料库不仅降低了方言语音处理的门槛，还促进了人工智能的公平性并减轻了语音技术中的偏见问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WenetSpeech-Chuan是一个大型、开源的四川话语音数据语料库，用于解决方言语音数据的稀缺问题。</li>
<li>Chuan-Pipeline是一种完整的方言语音数据处理框架，用于构建WenetSpeech-Chuan语料库。</li>
<li>WenetSpeech-Chuan-Eval是发布的高质量自评估报告和语音合成报告，用于评估语料库的有效性。</li>
<li>基于WenetSpeech-Chuan训练的模型在开放系统中表现卓越，与商业服务相比也有竞争力。</li>
<li>WenetSpeech-Chuan语料库促进了方言语音处理的进一步研究。</li>
<li>该语料库的公开使用有助于促进人工智能的公平性，并有助于减轻语音技术中的偏见问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18004">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18004v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18004v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18004v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18004v1/page_1_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18004v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18004v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18004v1/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.18004v1/page_2_3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Audiobook-CC-Controllable-Long-context-Speech-Generation-for-Multicast-Audiobook"><a href="#Audiobook-CC-Controllable-Long-context-Speech-Generation-for-Multicast-Audiobook" class="headerlink" title="Audiobook-CC: Controllable Long-context Speech Generation for Multicast   Audiobook"></a>Audiobook-CC: Controllable Long-context Speech Generation for Multicast   Audiobook</h2><p><strong>Authors:Min Liu, JingJing Yin, Xiang Zhang, Siyu Hao, Yanni Hu, Bin Lin, Yuan Feng, Hongbin Zhou, Jianhao Ye</strong></p>
<p>Existing text-to-speech systems predominantly focus on single-sentence synthesis and lack adequate contextual modeling as well as fine-grained performance control capabilities for generating coherent multicast audiobooks. To address these limitations, we propose a context-aware and emotion controllable speech synthesis framework specifically engineered for multicast audiobooks with three key innovations: a context mechanism for contextual consistency, a disentanglement paradigm to decouple style control from speech prompts for semantic consistency, and self-distillation to boost emotional expressiveness and instruction controllability. Experimental results show superior performance across the generation of narration, dialogue, and the whole chapter, significantly outperforming existing baselines. Ablation studies are conducted to validate the effectiveness of our proposed methods. Demo samples can be found in <a target="_blank" rel="noopener" href="https://everest-ai.github.io/">https://everest-ai.github.io/</a>. </p>
<blockquote>
<p>现有的文本转语音系统主要集中在单句合成上，缺乏足够的上下文建模和生成连贯的多播有声书所需的精细性能控制功能。为了解决这些局限性，我们提出了一种专门针对多播有声书的上下文感知和情感可控的语音合成框架，包含三项关键创新：一种用于上下文一致性的上下文机制，一种将风格控制与语音提示解耦以实现语义一致性的分离范式，以及自我蒸馏技术来提升情感表现力和指令可控性。实验结果表明，无论是在叙述、对话还是整章的生成方面，该框架的性能都优于现有基线。消融研究已验证了我们提出方法的有效性。演示样本可在<a target="_blank" rel="noopener" href="https://everest-ai.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://everest-ai.github.io/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17516v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本针对现有文本转语音系统在多句子合成方面的局限性，提出了一种面向多媒体音频书的语境感知和情感可控的语音合成框架。该框架具有三项关键创新：语境机制确保语境一致性，解耦风格控制与语音提示以维持语义一致性，以及通过自我提炼提升情感表现力和指令控制性。实验结果证明该框架在叙事、对话和整章生成方面表现卓越，显著优于现有基线模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有文本转语音系统在处理多句子合成时存在局限性，缺乏足够的语境建模和精细的性能控制。</li>
<li>提出的语音合成框架具有三项关键创新，包括语境机制、解耦风格控制和自我提炼技术。</li>
<li>语境机制确保生成的语音在语境上保持一致性。</li>
<li>解耦风格控制使语音提示与风格控制分离，维持语义一致性。</li>
<li>自我提炼技术提升了语音的情感表现力和指令控制性。</li>
<li>实验结果表明，该框架在叙事、对话和整章生成方面表现卓越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17516">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.17516v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.17516v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.17516v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.17516v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.17516v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VoXtream-Full-Stream-Text-to-Speech-with-Extremely-Low-Latency"><a href="#VoXtream-Full-Stream-Text-to-Speech-with-Extremely-Low-Latency" class="headerlink" title="VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency"></a>VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency</h2><p><strong>Authors:Nikita Torgashov, Gustav Eje Henter, Gabriel Skantze</strong></p>
<p>We present VoXtream, a fully autoregressive, zero-shot streaming text-to-speech (TTS) system for real-time use that begins speaking from the first word. VoXtream directly maps incoming phonemes to audio tokens using a monotonic alignment scheme and a dynamic look-ahead that does not delay onset. Built around an incremental phoneme transformer, a temporal transformer predicting semantic and duration tokens, and a depth transformer producing acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay among publicly available streaming TTS: 102 ms on GPU. Despite being trained on a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several metrics, while delivering competitive quality in both output- and full-streaming settings. Demo and code are available at <a target="_blank" rel="noopener" href="https://herimor.github.io/voxtream">https://herimor.github.io/voxtream</a>. </p>
<blockquote>
<p>我们推出了VoXtream，这是一个完全自回归的零样本流式文本到语音（TTS）系统，适用于实时使用，从第一个字开始说话。VoXtream直接使用单调对齐方案和不会延迟发音的动态前瞻，将传入的音素直接映射到音频令牌。VoXtream建立在增量音素变换器周围，一个预测语义和持续时间令牌的时间变换器，以及一个产生声音令牌的深度变换器。据我们所知，VoXtream的初始延迟是公众可用的流式TTS中最短的：GPU上为102毫秒。尽管它仅在中等规模的9k小时语料库上进行训练，但它能在多个指标上达到或超过更大的基线水平，同时在输出和全流式设置中都提供了具有竞争力的质量。Demo和代码可以在 <a target="_blank" rel="noopener" href="https://herimor.github.io/voxtream">https://herimor.github.io/voxtream</a> 上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15969v1">PDF</a> 5 pages, 1 figure, submitted to IEEE ICASSP 2026</p>
<p><strong>摘要</strong></p>
<p>VoXtream是一款全自回归、零射击流文本到语音（TTS）系统，适用于实时使用，可从第一个字开始发声。VoXtream直接使用单调对齐方案和无需延迟的动态前瞻，将传入的音素直接映射到音频令牌。通过构建增量音素转换器、预测语义和持续时间标记的时间转换器以及产生声音标记的深度转换器，VoXtream达到了我们已知的公开可用流媒体TTS中最低的首个延迟：GPU上为102毫秒。尽管它是在中等规模的9k小时语料库上进行训练的，但在多个指标上，它达到或超过了更大的基线系统的表现，同时在输出和流式传输环境中都提供了竞争力的质量。相关演示和代码可访问herimor.github.io&#x2F;voxtream。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>VoXtream是一个用于实时使用的全自回归、零射击流文本到语音（TTS）系统。</li>
<li>系统可将传入的音素直接映射到音频令牌，具有动态前瞻和单调对齐方案。</li>
<li>VoXtream通过增量音素转换器、预测语义和持续时间的时间转换器以及产生声音标记的深度转换器实现高效性能。</li>
<li>VoXtream在GPU上的首个延迟达到了前所未有的低水平，为102毫秒。</li>
<li>该系统在中等规模的语料库上训练，但在多个指标上达到或超过了更大的基线系统的表现。</li>
<li>VoXtream在输出和流式传输环境中都提供了高质量的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15969">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15969v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15969v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15969v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15969v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15969v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15969v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Deep-Dubbing-End-to-End-Auto-Audiobook-System-with-Text-to-Timbre-and-Context-Aware-Instruct-TTS"><a href="#Deep-Dubbing-End-to-End-Auto-Audiobook-System-with-Text-to-Timbre-and-Context-Aware-Instruct-TTS" class="headerlink" title="Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and   Context-Aware Instruct-TTS"></a>Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and   Context-Aware Instruct-TTS</h2><p><strong>Authors:Ziqi Dai, Yiting Chen, Jiacheng Xu, Liufei Xie, Yuchen Wang, Zhenchuan Yang, Bingsong Bai, Yangsheng Gao, Wenjiang Zhou, Weifeng Zhao, Ruohua Zhou</strong></p>
<p>The pipeline for multi-participant audiobook production primarily consists of three stages: script analysis, character voice timbre selection, and speech synthesis. Among these, script analysis can be automated with high accuracy using NLP models, whereas character voice timbre selection still relies on manual effort. Speech synthesis uses either manual dubbing or text-to-speech (TTS). While TTS boosts efficiency, it struggles with emotional expression, intonation control, and contextual scene adaptation. To address these challenges, we propose DeepDubbing, an end-to-end automated system for multi-participant audiobook production. The system comprises two main components: a Text-to-Timbre (TTT) model and a Context-Aware Instruct-TTS (CA-Instruct-TTS) model. The TTT model generates role-specific timbre embeddings conditioned on text descriptions. The CA-Instruct-TTS model synthesizes expressive speech by analyzing contextual dialogue and incorporating fine-grained emotional instructions. This system enables the automated generation of multi-participant audiobooks with both timbre-matched character voices and emotionally expressive narration, offering a novel solution for audiobook production. </p>
<blockquote>
<p>多参与者有声书生产流程主要包括三个阶段：剧本分析、角色嗓音音质选择和语音合成。其中，剧本分析可以使用NLP模型实现高度自动化，而角色嗓音音质选择仍然需要人工操作。语音合成采用人工配音或文本到语音（TTS）技术。虽然TTS技术提高了效率，但在情感表达、语调控制和上下文场景适应方面仍存在困难。为了解决这些挑战，我们提出了DeepDubbing，这是一个用于多参与者有声书生产的端到端自动化系统。该系统主要包括两个组件：文本到音质（TTT）模型和上下文感知指令TTS（CA-Instruct-TTS）模型。TTT模型根据文本描述生成特定角色的音质嵌入。CA-Instruct-TTS模型通过分析上下文对话并融入精细的情感指令，合成富有表现力的语音。该系统能够实现多参与者有声书的自动生成，既有匹配的音质角色声音，又有情感丰富的旁白，为有声书生产提供了新颖的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15845v1">PDF</a> Submitted to ICASSP 2026.Copyright 2026 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses, including reprinting&#x2F;republishing, creating new collective works, for   resale or redistribution to servers or lists, or reuse of any copyrighted   component of this work. DOI will be added upon IEEE Xplore publication</p>
<p><strong>Summary</strong><br>文本主要介绍了多参与者有声书生产的流程，包括脚本分析、角色声音音色的选择以及语音合成三个阶段。其中，脚本分析可以通过NLP模型实现自动化且准确率高；角色声音音色的选择仍然需要人工操作。为提高效率，提出DeepDubbing系统，包括Text-to-Timbre模型和Context-Aware Instruct-TTS模型，实现多参与者有声书的自动化生成，具有音色匹配和富有情感表达的旁白功能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多参与者有声书生产流程包括脚本分析、角色声音音色选择和语音合成三个阶段。</li>
<li>脚本分析可以通过NLP模型实现自动化且准确率高。</li>
<li>角色声音音色的选择仍然需要人工操作。</li>
<li>DeepDubbing系统是实现多参与者有声书自动化生成的关键。</li>
<li>DeepDubbing系统包括Text-to-Timbre模型和Context-Aware Instruct-TTS模型两个主要组件。</li>
<li>Text-to-Timbre模型根据文本描述生成角色特定的音色嵌入。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15845">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15845v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15845v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15845v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15845v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15845v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Video-to-SFX-Video-to-Audio-Synthesis-with-Environmentally-Aware-Speech"><a href="#Beyond-Video-to-SFX-Video-to-Audio-Synthesis-with-Environmentally-Aware-Speech" class="headerlink" title="Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware   Speech"></a>Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware   Speech</h2><p><strong>Authors:Xinlei Niu, Jianbo Ma, Dylan Harper-Harris, Xiangyu Zhang, Charles Patrick Martin, Jing Zhang</strong></p>
<p>The generation of realistic, context-aware audio is important in real-world applications such as video game development. While existing video-to-audio (V2A) methods mainly focus on Foley sound generation, they struggle to produce intelligible speech. Meanwhile, current environmental speech synthesis approaches remain text-driven and fail to temporally align with dynamic video content. In this paper, we propose Beyond Video-to-SFX (BVS), a method to generate synchronized audio with environmentally aware intelligible speech for given videos. We introduce a two-stage modeling method: (1) stage one is a video-guided audio semantic (V2AS) model to predict unified audio semantic tokens conditioned on phonetic cues; (2) stage two is a video-conditioned semantic-to-acoustic (VS2A) model that refines semantic tokens into detailed acoustic tokens. Experiments demonstrate the effectiveness of BVS in scenarios such as video-to-context-aware speech synthesis and immersive audio background conversion, with ablation studies further validating our design. Our demonstration is available at~\href{<a target="_blank" rel="noopener" href="https://xinleiniu.github.io/BVS-demo/%7D%7BBVS-Demo%7D">https://xinleiniu.github.io/BVS-demo/}{BVS-Demo}</a>. </p>
<blockquote>
<p>在现实世界的应用（如游戏开发）中，生成真实、具有语境意识的音频至关重要。现有的视频到音频（V2A）方法主要集中在音效声音生成上，但在生成可理解的语音方面遇到了困难。与此同时，当前的环境语音合成方法仍然是文本驱动的，并且不能与动态视频内容在时间上进行对齐。在本文中，我们提出了超越视频到音效（BVS）的方法，该方法可以为给定的视频生成具有环境意识的同步可理解语音的音频。我们引入了一种两阶段的建模方法：（1）第一阶段是视频引导音频语义（V2AS）模型，根据语音线索预测统一的音频语义标记；（2）第二阶段是视频条件语义到声音（VS2A）模型，它将语义标记细化为详细的声学标记。实验表明，BVS在视频到上下文感知语音合成和沉浸式音频背景转换等场景中非常有效，消融研究进一步验证了我们的设计。我们的演示可在<a target="_blank" rel="noopener" href="https://xinleiniu.github.io/BVS-demo/">BVS-Demo</a>查看。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15492v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本提出一种名为Beyond Video-to-SFX（BVS）的方法，旨在生成与视频同步的具有环境意识的可识别语音的音频。该方法采用两阶段建模方法，首先通过视频引导音频语义（V2AS）模型预测基于语音特征的统一音频语义令牌，然后通过视频条件语义到声音（VS2A）模型细化语义令牌为详细的声学令牌。此方法能有效应用于视频到语境感知语音合成和沉浸式音频背景转换等场景。具体演示请参考提供的链接。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>文本介绍了一种新的视频到音频生成方法Beyond Video-to-SFX（BVS）。</li>
<li>该方法旨在生成具有环境意识的可识别语音的音频，适用于视频游戏开发等现实应用。</li>
<li>BVS采用两阶段建模，包括视频引导音频语义（V2AS）模型和视频条件语义到声音（VS2A）模型。</li>
<li>V2AS模型基于语音特征预测统一音频语义令牌。</li>
<li>VS2A模型将语义令牌细化为详细的声学令牌，实现音频的精细合成。</li>
<li>BVS方法在视频到语境感知语音合成和沉浸式音频背景转换等场景具有应用价值。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15492">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15492v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15492v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15492v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15492v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR"><a href="#Frustratingly-Easy-Data-Augmentation-for-Low-Resource-ASR" class="headerlink" title="Frustratingly Easy Data Augmentation for Low-Resource ASR"></a>Frustratingly Easy Data Augmentation for Low-Resource ASR</h2><p><strong>Authors:Katsumi Ibaraki, David Chiang</strong></p>
<p>This paper introduces three self-contained data augmentation methods for low-resource Automatic Speech Recognition (ASR). Our techniques first generate novel text–using gloss-based replacement, random replacement, or an LLM-based approach–and then apply Text-to-Speech (TTS) to produce synthetic audio. We apply these methods, which leverage only the original annotated data, to four languages with extremely limited resources (Vatlongos, Nashta, Shinekhen Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a combination of the original audio and generated synthetic data yields significant performance gains, including a 14.3% absolute WER reduction for Nashta. The methods prove effective across all four low-resource languages and also show utility for high-resource languages like English, demonstrating their broad applicability. </p>
<blockquote>
<p>本文介绍了三种用于低资源自动语音识别（ASR）的自主数据增强方法。我们的技术首先使用基于术语替换、随机替换或基于大型语言模型（LLM）的方法生成新的文本，然后应用文本到语音（TTS）技术生成合成音频。我们只在有限资源的四种语言（瓦特隆戈斯语、纳斯塔语、谢内肯布里亚特语和卡卡贝语）中应用这些方法使用原始标注数据。通过对预训练的Wav2Vec2-XLSR-53模型进行微调，结合原始音频和生成的合成数据，取得了显著的性能提升，其中纳斯塔语的绝对字词错误率（WER）降低了14.3%。这些方法在四种低资源语言中都证明了其有效性，并且在高资源语言如英语中也显示出实用性，证明了其广泛的应用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15373v1">PDF</a> 5 pages, 2 figures, 2 tables, submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了三种为低资源语音识别（ASR）自我完善的数据增强方法。这些方法首先利用基于词义的替代、随机替代或大型语言模型（LLM）的方法生成新文本，然后应用文本转语音（TTS）技术生成合成音频。这些方法仅利用原始标注数据，应用于四种资源极度匮乏的语言（Vatlongos、Nashta、Shinekhen Buryat和Kakabe）。通过预训练的Wav2Vec2-XLSR-53模型对原始音频和生成合成数据的组合进行微调，取得了显著的性能提升，其中Nashta语言的绝对词错误率（WER）降低了14.3%。这些方法在四种低资源语言中都表现出有效性和实用性，并且在高资源语言如英语中也显示了其应用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍三种自我完善的数据增强方法用于低资源语音识别。</li>
<li>方法包括基于词义的替代、随机替代或大型语言模型生成新文本。</li>
<li>使用文本转语音技术将新文本转化为合成音频。</li>
<li>这些方法仅利用原始标注数据，适用于多种语言。</li>
<li>在四种资源极度匮乏的语言中有显著的性能提升。</li>
<li>通过对预训练模型的微调，实现了词错误率的显著降低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15373">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15373v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15373v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15373v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15373v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.15373v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems"><a href="#Do-You-Hear-What-I-Mean-Quantifying-the-Instruction-Perception-Gap-in-Instruction-Guided-Expressive-Text-To-Speech-Systems" class="headerlink" title="Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in   Instruction-Guided Expressive Text-To-Speech Systems"></a>Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in   Instruction-Guided Expressive Text-To-Speech Systems</h2><p><strong>Authors:Yi-Cheng Lin, Huang-Cheng Chou, Tzu-Chieh Wei, Kuan-Yu Chen, Hung-yi Lee</strong></p>
<p>Instruction-guided text-to-speech (ITTS) enables users to control speech generation through natural language prompts, offering a more intuitive interface than traditional TTS. However, the alignment between user style instructions and listener perception remains largely unexplored. This work first presents a perceptual analysis of ITTS controllability across two expressive dimensions (adverbs of degree and graded emotion intensity) and collects human ratings on speaker age and word-level emphasis attributes. To comprehensively reveal the instruction-perception gap, we provide a data collection with large-scale human evaluations, named Expressive VOice Control (E-VOC) corpus. Furthermore, we reveal that (1) gpt-4o-mini-tts is the most reliable ITTS model with great alignment between instruction and generated utterances across acoustic dimensions. (2) The 5 analyzed ITTS systems tend to generate Adult voices even when the instructions ask to use child or Elderly voices. (3) Fine-grained control remains a major challenge, indicating that most ITTS systems have substantial room for improvement in interpreting slightly different attribute instructions. </p>
<blockquote>
<p>指令引导型文本转语音（ITTS）允许用户通过自然语言提示来控制语音生成，为用户提供了一个比传统TTS更直观的界面。然而，用户风格指令和听众感知之间的对齐仍然很大程度上未被探索。这项工作首先针对ITTS在两个表达维度（程度副词和分级情绪强度）上的可控性进行了感知分析，并对说话人的年龄和单词级别的强调属性进行了人类评分。为了全面揭示指令感知差距，我们提供了一个大规模人类评估的数据收集，名为表现力语音控制（E-VOC）语料库。此外，我们揭示：（1）gpt-4o-mini-tts是最可靠的ITTS模型，在声学维度上，指令和生成话语之间的对齐效果很好。（2）这五个分析的ITTS系统倾向于生成成年人声音，即使指令要求使用儿童或老年人声音。（3）精细控制仍然是一个主要挑战，这表明大多数ITTS系统在解释略有不同的属性指令方面仍有很大的改进空间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13989v3">PDF</a> Submission to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>指令引导式文本转语音（ITTS）允许用户通过自然语言提示控制语音生成，相较于传统TTS提供了更直观的操作界面。然而，用户指令风格与听众感知之间的对齐程度尚未得到充分研究。本研究首先针对ITTS的可控性进行感知分析，涵盖程度副词和分级情感强度两个表达维度，并收集人类评价者关于说话人年龄和单词级别强调属性的评分。为了全面揭示指令与感知之间的差距，我们提供了大规模人类评估的数据收集，名为表达性语音控制（E-VOC）语料库。研究发现，GPT-4o-mini-tts是可靠度最高的ITTS模型，指令与生成的语音片段在声学维度上的对齐度良好。但存在系统生成的声音与实际指令要求的儿童或老年声音差异较大，精细控制仍是主要挑战，意味着大多数ITTS系统在解读细微属性指令方面仍有很大的提升空间。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ITTS允许用户通过自然语言提示控制语音生成，其界面相较于传统TTS更为直观。</li>
<li>指令引导式文本转语音在感知分析方面仍有许多未知领域，特别是关于用户指令风格与听众感知的对齐。</li>
<li>E-VOC语料库用于全面揭示指令与感知之间的差距，通过大规模人类评估的数据收集来实现。</li>
<li>GPT-4o-mini-tts是可靠度最高的ITTS模型，其在声学维度上的指令与生成语音的对齐度表现最佳。</li>
<li>ITTS系统在生成特定声音（如儿童或老年声音）时存在偏差，实际生成的往往偏向于成年声音。</li>
<li>ITTS在精细控制方面存在挑战，即解读细微属性指令的能力有待提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13989">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.13989v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.13989v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.13989v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.13989v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.13989v3/page_3_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LTA-thinker-Latent-Thought-Augmented-Training-Framework-for-Large-Language-Models-on-Complex-Reasoning"><a href="#LTA-thinker-Latent-Thought-Augmented-Training-Framework-for-Large-Language-Models-on-Complex-Reasoning" class="headerlink" title="LTA-thinker: Latent Thought-Augmented Training Framework for Large   Language Models on Complex Reasoning"></a>LTA-thinker: Latent Thought-Augmented Training Framework for Large   Language Models on Complex Reasoning</h2><p><strong>Authors:Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang, Yuantao Han, Cangyi Yang, jiaxu Zhang, Feiliang Ren</strong></p>
<p>Complex Reasoning in Large Language Models can be dynamically optimized using Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut, SoftCoT and its variant are effective in continuous latent space inference, the core bottleneck still lies in the efficient generation and utilization of high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger variance in the generated Latent Thought distribution more closely approximates the golden truth distribution, we propose a Latent Thought-Augmented Training Framework–LTA-Thinker, which improves distributional variance and enhances reasoning performance from two perspectives. First, LTA-Thinker constructs a Latent Thought generation architecture based on a learnable prior. This architecture aims to increase the variance distribution of generated Latent Thought Vectors in order to simplify the overall structure and raise the performance ceiling. Second, LTA-Thinker introduces a distribution-based directional optimization paradigm that jointly constrains both distribution locality and distribution scale. This mechanism improves information efficiency and computational cost through a multi-objective co-training strategy, which combines standard Supervised Fine-Tuning (SFT) loss with two novel losses: Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent Thought is highly relevant to the semantics of the question; Reasoning Focus Loss, which utilizes a contrastive learning mechanism to guide the model to focus on the most critical reasoning steps. Experiments show that LTA-thinker achieves state-of-the-art (SOTA) performance among various baselines and demonstrates a higher performance ceiling and better scaling effects. </p>
<blockquote>
<p>在大语言模型的复杂推理中，可以通过测试时间缩放（TTS）进行动态优化，以减轻过度思考。椰子数、SoftCoT及其变种等方法在连续潜在空间推理中效果显著，但核心瓶颈仍在于高效生成和利用高质量的潜在思维。根据SoftCoT++的理论，生成的潜在思维分布在更大的方差下更接近真实分布。因此，我们提出了一个基于潜在思维的训练框架——LTA-Thinker，从两个方面提高分布方差并增强推理性能。首先，LTA-Thinker构建了一个基于可学习先验的潜在思维生成架构。该架构旨在增加生成的潜在思维向量的方差分布，以简化整体结构并提高性能上限。其次，LTA-Thinker引入了一种基于分布的方向优化范式，同时约束分布局部性和分布规模。该机制通过多目标联合训练策略提高了信息效率和计算成本，结合标准监督微调（SFT）损失与两种新型损失：语义对齐损失，利用KL散度确保潜在思维与问题的语义高度相关；推理焦点损失，利用对比学习机制引导模型关注最关键的推理步骤。实验表明，LTA-thinker在各种基准测试中达到了最新水平（SOTA）的性能，并展示了更高的性能上限和更好的扩展效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12875v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了在大语言模型中利用测试时间缩放（TTS）进行动态优化以减少过度思考的方法。文章介绍了Coconut、SoftCoT及其变体等方法在连续潜在空间推理中的应用，并提出了一种潜在思维增强训练框架LTA-Thinker，旨在提高分布方差并增强推理性能。LTA-Thinker从两个方面进行改进：构建基于可学习先验的潜在思维生成架构，增加生成潜在思维向量的方差分布；引入基于分布的方向优化范式，联合约束分布局部性和分布规模。实验表明，LTA-Thinker在多种基线方法中取得了最佳性能，具有更高的性能上限和更好的扩展效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型中的复杂推理可以通过测试时间缩放（TTS）进行动态优化，以减轻过度思考的问题。</li>
<li>SoftCoT++理论表明，更大的潜在思维分布方差更接近真实分布。</li>
<li>LTA-Thinker提出了一种潜在思维增强训练框架，旨在提高分布方差并增强推理性能。</li>
<li>LTA-Thinker构建了一个基于可学习先验的潜在思维生成架构。</li>
<li>LTA-Thinker引入了基于分布的方向优化范式，联合约束分布局部性和分布规模。</li>
<li>LTA-Thinker采用多目标联合训练策略，结合标准监督微调（SFT）损失与两种新损失：语义对齐损失和推理焦点损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12875">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.12875v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.12875v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.12875v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Building-Coding-Agents-via-Entropy-Enhanced-Multi-Turn-Preference-Optimization"><a href="#Building-Coding-Agents-via-Entropy-Enhanced-Multi-Turn-Preference-Optimization" class="headerlink" title="Building Coding Agents via Entropy-Enhanced Multi-Turn Preference   Optimization"></a>Building Coding Agents via Entropy-Enhanced Multi-Turn Preference   Optimization</h2><p><strong>Authors:Jiahao Yu, Zelei Cheng, Xian Wu, Xinyu Xing</strong></p>
<p>Software engineering presents complex, multi-step challenges for Large Language Models (LLMs), requiring reasoning over large codebases and coordinated tool use. The difficulty of these tasks is exemplified by benchmarks like SWE-bench, where current LLMs still struggle to resolve real-world issues.   A promising approach to enhance performance is test-time scaling (TTS), but its gains are heavily dependent on the diversity of model outputs.   While standard alignment methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs with human preferences, this process can come at the cost of reduced diversity, limiting the effectiveness of TTS.   Additionally, existing preference optimization algorithms are typically designed for single-turn tasks and do not fully address the complexities of multi-turn reasoning and tool integration required for interactive coding agents.   To bridge this gap, we introduce EntroPO, an entropy-enhanced framework that adapts existing preference optimization algorithms to the multi-turn, tool-assisted setting.   EntroPO augments the preference objective to explicitly preserve policy entropy and generalizes learning to optimize over multi-turn interactions rather than single-turn responses.   We validate EntroPO by fine-tuning a diverse suite of models from different families and sizes (up to 106B parameters).   To maximize performance gains from TTS, we further propose a hybrid best-trajectory selection scheme combining a learned verifier model with model free approaches.   On the \swebench leaderboard, our approach establishes new state-of-the-art results among open-weight models. A 30B parameter model trained with EntroPO ranks 1st on \lite and 4th on \verified on the open-weight leaderboard, surpassed only by models with over 10x more parameters(\eg$&gt;$350B). </p>
<blockquote>
<p>软件工程为大型语言模型（LLM）带来了复杂、多步骤的挑战，需要处理大型代码库并进行工具协调。这些任务的难度体现在如SWE-bench等基准测试中，当前LLM在解决现实世界问题时仍面临困难。一种提高性能的有前途的方法是测试时缩放（TTS），但其收益在很大程度上取决于模型输出的多样性。虽然直接偏好优化（DPO）和卡内曼-特沃斯基优化（KTO）等标准对齐方法可以有效地将模型输出与人类的偏好对齐，但这个过程可能会导致多样性降低，从而限制了TTS的有效性。此外，现有的偏好优化算法通常是为单轮任务设计的，并不能完全解决多轮推理和工具集成所需的交互编码代理的复杂性。为了弥补这一差距，我们引入了EntroPO，这是一种基于熵增强的框架，它适应了现有的偏好优化算法，用于多轮、工具辅助的环境。EntroPO通过增强偏好目标来显式地保留策略熵，并将学习推广到优化多轮交互而不是单轮响应。我们通过微调来自不同家族和规模的多样化模型（参数高达106B）来验证EntroPO的有效性。为了最大限度地提高TTS的性能收益，我们进一步提出了一种混合的最佳轨迹选择方案，该方案结合了学习验证模型和无模型方法。在Swebench排行榜上，我们的方法树立了开放权重模型中的最新技术标杆。使用EntroPO训练的参数为30B的模型在开放权重排行榜上的\lite排名第一，在\verified排名第四，仅被参数超过我们十倍以上的模型（例如大于350B参数的模型）超越。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12434v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了软件工程对于大型语言模型（LLM）的复杂挑战，包括在大规模代码库上进行推理和协调工具的使用。为提升性能，一种有前途的方法是测试时间缩放（TTS），但其收益高度依赖于模型输出的多样性。虽然直接偏好优化（DPO）和卡内曼-特维尔斯基优化（KTO）等标准对齐方法能够有效地对齐模型输出与人类偏好，这一过程可能会降低多样性，限制了TTS的有效性。为了解决这一问题，引入EntroPO框架，该框架适应于多回合工具辅助设置，通过优化偏好目标来明确保留策略熵，并优化多回合交互而非单回合响应。实验验证了EntroPO的有效性，在软件工程技术竞赛中获得领先水平。使用EntroPO训练的参数为三十亿的大型语言模型在公开权重排行榜上取得了优异成绩。总的来说，该文关注大型语言模型在解决软件工程挑战方面的进展以及通过改进的测试时间缩放策略提高其性能的方法。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是七个关键要点：</p>
<ol>
<li>大型语言模型（LLM）在解决软件工程挑战时面临推理和协调工具使用的复杂问题。</li>
<li>测试时间缩放（TTS）是提高模型性能的一种有前途的方法，但其成功取决于模型输出的多样性。</li>
<li>标准对齐方法如直接偏好优化（DPO）和卡内曼-特维尔斯基优化（KTO）可能降低模型输出的多样性。</li>
<li>EntroPO框架通过适应多回合工具辅助设置来解决这一问题，旨在保留策略熵并优化多回合交互。</li>
<li>EntroPO框架在软件工程技术竞赛中表现优异，展示了其有效性。</li>
<li>使用EntroPO训练的参数为三十亿的大型语言模型在公开权重排行榜上名列前茅。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12434">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.12434v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2509.12434v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="P2VA-Converting-Persona-Descriptions-into-Voice-Attributes-for-Fair-and-Controllable-Text-to-Speech"><a href="#P2VA-Converting-Persona-Descriptions-into-Voice-Attributes-for-Fair-and-Controllable-Text-to-Speech" class="headerlink" title="P2VA: Converting Persona Descriptions into Voice Attributes for Fair and   Controllable Text-to-Speech"></a>P2VA: Converting Persona Descriptions into Voice Attributes for Fair and   Controllable Text-to-Speech</h2><p><strong>Authors:Yejin Lee, Jaehoon Kang, Kyuhong Shim</strong></p>
<p>While persona-driven large language models (LLMs) and prompt-based text-to-speech (TTS) systems have advanced significantly, a usability gap arises when users attempt to generate voices matching their desired personas from implicit descriptions. Most users lack specialized knowledge to specify detailed voice attributes, which often leads TTS systems to misinterpret their expectations. To address these gaps, we introduce Persona-to-Voice-Attribute (P2VA), the first framework enabling voice generation automatically from persona descriptions. Our approach employs two strategies: P2VA-C for structured voice attributes, and P2VA-O for richer style descriptions. Evaluation shows our P2VA-C reduces WER by 5% and improves MOS by 0.33 points. To the best of our knowledge, P2VA is the first framework to establish a connection between persona and voice synthesis. In addition, we discover that current LLMs embed societal biases in voice attributes during the conversion process. Our experiments and findings further provide insights into the challenges of building persona-voice systems. </p>
<blockquote>
<p>虽然个性驱动的大型语言模型（LLM）和基于提示的文本到语音（TTS）系统已经取得了显著的进步，但在用户尝试从隐式描述生成匹配其所需个性的声音时，就会出现可用性差距。大多数用户缺乏指定详细语音属性的专业知识，这往往导致TTS系统误解他们的期望。为了解决这些差距，我们引入了Person-to-Voice属性（P2VA），这是第一个能够从个性描述中自动生成语音的框架。我们的方法采用两种策略：P2VA-C用于结构化的语音属性，P2VA 揭示了个人特质的重要性并将其嵌入语音表达之中。评价表明我们的P2VA-C降低了字错误率（WER）5%，并提高了平均意见得分（MOS）0.33分。据我们所知，P2VA是第一个建立个性与语音合成之间联系的框架。此外，我们发现当前的大型语言模型在转换过程中嵌入语音属性的社会偏见。我们的实验和发现为进一步构建个性化语音系统提供了深入见解和启示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17093v2">PDF</a> </p>
<p><strong>总结</strong></p>
<p>基于文本的描述，提出了一个名为P2VA的框架，旨在从人物描述中自动生成语音。该框架采用了两种策略：P2VA-C用于结构化语音属性，P2VA-O用于更丰富风格描述。评价显示，P2VA-C降低了字错误率（WER）5%，并提高了平均意见分数（MOS）0.33分。这是首个建立人物与语音合成之间联系的框架。此外，还发现当前的大型语言模型（LLMs）在转换过程中会嵌入语音属性中的社会偏见。</p>
<p><strong>要点</strong></p>
<ol>
<li>P2VA框架实现从人物描述自动生成语音。</li>
<li>框架包含两种策略：P2VA-C用于结构化语音属性，P2VA-O用于更丰富风格描述。</li>
<li>P2VA-C可降低字错误率（WER）5%，提高平均意见分数（MOS）0.33分。</li>
<li>该框架是首个建立人物与语音合成联系的框架。</li>
<li>当前LLMs在语音属性转换过程中会嵌入社会偏见。</li>
<li>用户缺乏详细语音属性知识，导致TTS系统误解用户期望。</li>
<li>P2VA框架有助于解决这一问题，提高TTS系统的用户体验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2505.17093v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2505.17093v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2505.17093v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2505.17093v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2505.17093v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TT-DF-A-Large-Scale-Diffusion-Based-Dataset-and-Benchmark-for-Human-Body-Forgery-Detection"><a href="#TT-DF-A-Large-Scale-Diffusion-Based-Dataset-and-Benchmark-for-Human-Body-Forgery-Detection" class="headerlink" title="TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human   Body Forgery Detection"></a>TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human   Body Forgery Detection</h2><p><strong>Authors:Wenkui Yang, Zhida Zhang, Xiaoqiang Zhou, Junxian Duan, Jie Cao</strong></p>
<p>The emergence and popularity of facial deepfake methods spur the vigorous development of deepfake datasets and facial forgery detection, which to some extent alleviates the security concerns about facial-related artificial intelligence technologies. However, when it comes to human body forgery, there has been a persistent lack of datasets and detection methods, due to the later inception and complexity of human body generation methods. To mitigate this issue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale diffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic frames, specifically tailored for body forgery detection. TT-DF offers a wide variety of forgery methods, involving multiple advanced human image animation models utilized for manipulation, two generative configurations based on the disentanglement of identity and pose information, as well as different compressed versions. The aim is to simulate any potential unseen forged data in the wild as comprehensively as possible, and we also furnish a benchmark on TT-DF. Additionally, we propose an adapted body forgery detection model, Temporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal inconsistencies and optical flow distribution differences between natural data and forged data. Our experiments demonstrate that TOF-Net achieves favorable performance on TT-DF, outperforming current state-of-the-art extendable facial forgery detection models. For our TT-DF dataset, please refer to <a target="_blank" rel="noopener" href="https://github.com/HashTAG00002/TT-DF">https://github.com/HashTAG00002/TT-DF</a>. </p>
<blockquote>
<p>面部深度伪造方法的出现和流行刺激了深度伪造数据集和面部伪造检测技术的蓬勃发展，这在一定程度上减轻了与面部相关的人工智能技术的安全担忧。然而，当涉及到人体伪造时，由于人体生成方法的起步较迟和复杂性，数据集和检测方法的缺乏一直存在。为了缓解这个问题，我们推出了TikTok-DeepFake（TT-DF），这是一个新型的大规模扩散基础数据集，包含6,120个伪造视频和1,378,857个合成帧，专门用于人体伪造检测。TT-DF提供了多种伪造方法，涉及多种先进的人体图像动画模型用于操作，两种基于身份和姿势信息解耦的生成配置，以及不同的压缩版本。我们的目标是尽可能全面地模拟野外任何潜在未见过的伪造数据，我们还为TT-DF提供了一个基准。此外，我们提出了一种适应性的人体伪造检测模型——Temporal Optical Flow Network（TOF-Net），它利用自然数据和伪造数据之间的时空不一致性和光流分布差异。我们的实验表明，TOF-Net在TT-DF上表现良好，优于当前先进的可扩展面部伪造检测模型。有关我们的TT-DF数据集，请参阅<a target="_blank" rel="noopener" href="https://github.com/HashTAG00002/TT-DF%E3%80%82">https://github.com/HashTAG00002/TT-DF。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08437v2">PDF</a> Accepted by PRCV 2024</p>
<p><strong>Summary</strong><br>     推出TikTok-DeepFake数据集，包含6,120个伪造视频和1,378,857个合成帧，旨在解决人体伪造检测缺乏数据集和方法的问题。同时提出Temporal Optical Flow Network (TOF-Net)模型，能利用时空不一致性和光学流分布差异进行身体伪造检测，表现优于当前先进的面部伪造检测模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TikTok-DeepFake（TT-DF）数据集是为了解决人体伪造检测缺乏数据集的问题而推出的，包含大量伪造视频和合成帧。</li>
<li>TT-DF数据集提供了多种伪造方法，包括多种先进的人体图像动画模型、基于身份和姿势信息解耦的两种生成配置以及不同的压缩版本。</li>
<li>TOF-Net模型被提出来检测身体伪造，它能利用时空不一致性和光学流分布差异进行识别。</li>
<li>TOF-Net模型在TT-DF数据集上的表现优于当前先进的面部伪造检测模型。</li>
<li>TT-DF数据集的目标是模拟野外任何潜在未见的伪造数据，并提供了相应的基准测试。</li>
<li>数据集和模型的详细信息可在<a target="_blank" rel="noopener" href="https://github.com/HashTAG00002/TT-DF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HashTAG00002/TT-DF找到。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08437">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2505.08437v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2505.08437v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_TTS/2505.08437v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Interactive/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_3_2.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-09-24  DA-Mamba Dialogue-aware selective state-space model for multimodal   engagement estimation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_医学图像/2509.17726v1/page_4_0.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-24  Towards Seeing Bones at Radio Frequency
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
