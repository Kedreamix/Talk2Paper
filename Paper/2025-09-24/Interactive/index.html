<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive 方向最新论文已更新，请持续关注 Update in 2025-09-24  DA-Mamba Dialogue-aware selective state-space model for multimodal   engagement estimation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_3_2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-24-更新"><a href="#2025-09-24-更新" class="headerlink" title="2025-09-24 更新"></a>2025-09-24 更新</h1><h2 id="DA-Mamba-Dialogue-aware-selective-state-space-model-for-multimodal-engagement-estimation"><a href="#DA-Mamba-Dialogue-aware-selective-state-space-model-for-multimodal-engagement-estimation" class="headerlink" title="DA-Mamba: Dialogue-aware selective state-space model for multimodal   engagement estimation"></a>DA-Mamba: Dialogue-aware selective state-space model for multimodal   engagement estimation</h2><p><strong>Authors:Shenwei Kang, Xin Zhang, Wen Liu, Bin Li, Yujie Liu, Bo Gao</strong></p>
<p>Human engagement estimation in conversational scenarios is essential for applications such as adaptive tutoring, remote healthcare assessment, and socially aware human–computer interaction. Engagement is a dynamic, multimodal signal conveyed by facial expressions, speech, gestures, and behavioral cues over time. In this work we introduce DA-Mamba, a dialogue-aware multimodal architecture that replaces attention-heavy dialogue encoders with Mamba-based selective state-space processing to achieve linear time and memory complexity while retaining expressive cross-modal reasoning. We design a Mamba dialogue-aware selective state-space model composed of three core modules: a Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group Fusion and Partner-Group Fusion, these modules achieve expressive dialogue understanding. Extensive experiments on three standard benchmarks (NoXi, NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art (SOTA) methods in concordance correlation coefficient (CCC), while reducing training time and peak memory; these gains enable processing much longer sequences and facilitate real-time deployment in resource-constrained, multi-party conversational settings. The source code will be available at: <a target="_blank" rel="noopener" href="https://github.com/kksssssss-ssda/MMEA">https://github.com/kksssssss-ssda/MMEA</a>. </p>
<blockquote>
<p>对话场景中的用户参与度估计是自适应辅导、远程健康评估和社交感知人机交互等应用的关键。参与度是一个动态的多模式信号，通过面部表情、语音、手势和行为线索随时间传递。在这项工作中，我们推出了DA-Mamba，这是一个基于对话的多模式架构，它用Mamba选择性状态空间处理替代了注意力密集的对话编码器，实现了线性时间和内存复杂度，同时保留了跨模态推理的表达力。我们设计了一个由三个核心模块组成的Mamba对话选择性状态空间模型：对话感知编码器和两种基于Mamba的融合机制：模态组融合和伙伴组融合。这些模块实现了富有表现力的对话理解。在三个标准基准测试（NoXi、NoXi-Add和MPIIGI）上的大量实验表明，DA-Mamba在协同相关系数（CCC）上超越了现有的最新方法，同时减少了训练时间和峰值内存；这些优势能够实现更长的序列处理，并在资源受限的多方对话环境中进行实时部署。源代码将发布在：<a target="_blank" rel="noopener" href="https://github.com/kksssssss-ssda/MMEA%E3%80%82">https://github.com/kksssssss-ssda/MMEA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17711v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了DA-Mamba这一对话感知的多模态架构，该架构采用基于Mamba的选择性状态空间处理，以线性时间和内存复杂度实现了跨模态推理。通过设计包括对话感知编码器和两种基于Mamba的融合机制（模态组融合和伙伴组融合）的核心模块，实现精准对话理解。在三个标准基准测试上的实验显示，DA-Mamba在提高一致性相关系数（CCC）的同时减少了训练时间和峰值内存。此进步使处理更长的序列和在资源受限的多方对话环境中进行实时部署成为可能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DA-Mamba是一个对话感知的多模态架构，适用于对话场景中的用户参与度评估。</li>
<li>该架构采用基于Mamba的选择性状态空间处理，实现线性时间和内存复杂度。</li>
<li>通过设计包括对话感知编码器和模态组融合、伙伴组融合在内的核心模块，DA-Mamba实现了精准的对话理解。</li>
<li>在标准基准测试上，DA-Mamba超过现有最佳方法，提高了一致性相关系数（CCC）。</li>
<li>DA-Mamba减少了训练时间和峰值内存，使处理更长的序列成为可能。</li>
<li>该架构适用于资源受限的环境和多方对话设置中的实时部署。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17711">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17711v1/page_3_2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PRINCIPLES-Synthetic-Strategy-Memory-for-Proactive-Dialogue-Agents"><a href="#PRINCIPLES-Synthetic-Strategy-Memory-for-Proactive-Dialogue-Agents" class="headerlink" title="PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents"></a>PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents</h2><p><strong>Authors:Namyoung Kim, Kai Tzu-iunn Ong, Yeonjun Hwang, Minseok Kang, Iiseo Jihn, Gayoung Kim, Minju Kim, Jinyoung Yeo</strong></p>
<p>Dialogue agents based on large language models (LLMs) have shown promising performance in proactive dialogue, which requires effective strategy planning. However, existing approaches to strategy planning for proactive dialogue face several limitations: limited strategy coverage, preference bias in planning, and reliance on costly additional training. To address these, we propose PRINCIPLES: a synthetic strategy memory for proactive dialogue agents. PRINCIPLES is derived through offline self-play simulations and serves as reusable knowledge that guides strategy planning during inference, eliminating the need for additional training and data annotation. We evaluate PRINCIPLES in both emotional support and persuasion domains, demonstrating consistent improvements over strong baselines. Furthermore, PRINCIPLES maintains its robustness across extended and more diverse evaluation settings. See our project page at <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/kimnamssya/Principles">https://huggingface.co/spaces/kimnamssya/Principles</a>. </p>
<blockquote>
<p>基于大型语言模型（LLM）的对话代理人在主动性对话中表现出有前景的性能，这需要进行有效的策略规划。然而，现有的主动性对话策略规划方法面临一些局限性：策略覆盖面有限、规划中的偏好偏见以及依赖昂贵的额外训练。为了解决这些问题，我们提出了PRINCIPLES：一种用于主动性对话代理人的合成策略记忆。PRINCIPLES是通过离线自我游戏模拟得出的，作为一种可重复使用的知识，在推理过程中指导策略规划，无需额外的训练和数据标注。我们在情感支持和劝说领域对PRINCIPLES进行了评估，与强大的基线相比，表现出了持续的一致性改进。此外，PRINCIPLES在扩展和更多样化的评估环境中也保持了其稳健性。请访问我们的项目页面<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/kimnamssya/Principles%E4%BA%86%E8%A7%A3%E8%AF%A6%E6%83%85%E3%80%82">https://huggingface.co/spaces/kimnamssya/Principles了解详情。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17459v1">PDF</a> Accepted to EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的对话代理在主动对话中表现出良好的性能，这要求有效的策略规划。然而，现有主动对话策略规划方法存在策略覆盖面有限、规划偏好偏见和依赖成本高昂的额外训练等局限性。为解决这些问题，我们提出了PRINCIPLES：一种主动对话代理的合成策略记忆。PRINCIPLES通过离线自我游戏模拟得出，作为可重复使用的知识，在推理过程中指导策略规划，无需额外的训练和数据标注。我们在情感支持和劝说领域对PRINCIPLES进行了评估，证明其在强基线之上具有持续改进的能力。此外，PRINCIPLES在扩展和更复杂的评估环境中保持了其稳健性。欲了解更多详情，请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/kimnamssya/Principles">网址链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于大型语言模型的对话代理在主动对话中表现良好，需要有效的策略规划来提升性能。</li>
<li>现有主动对话策略规划方法存在局限性，如策略覆盖面有限、规划偏好和额外训练成本。</li>
<li>PRINCIPLES是一种合成策略记忆，通过离线自我游戏模拟来指导主动对话代理的策略规划。</li>
<li>PRINCIPLES可消除对额外训练和数据标注的需求。</li>
<li>在情感支持和劝说领域，PRINCIPLES相较于强基线有显著改善。</li>
<li>PRINCIPLES在多种评估环境中保持稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17459">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17459v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17459v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17459v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.17459v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Leveraging-Multiple-Speech-Enhancers-for-Non-Intrusive-Intelligibility-Prediction-for-Hearing-Impaired-Listeners"><a href="#Leveraging-Multiple-Speech-Enhancers-for-Non-Intrusive-Intelligibility-Prediction-for-Hearing-Impaired-Listeners" class="headerlink" title="Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility   Prediction for Hearing-Impaired Listeners"></a>Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility   Prediction for Hearing-Impaired Listeners</h2><p><strong>Authors:Boxuan Cao, Linkai Li, Hanlin Yu, Changgeng Mo, Haoshuai Zhou, Shan Xiang Wang</strong></p>
<p>Speech intelligibility evaluation for hearing-impaired (HI) listeners is essential for assessing hearing aid performance, traditionally relying on listening tests or intrusive methods like HASPI. However, these methods require clean reference signals, which are often unavailable in real-world conditions, creating a gap between lab-based and real-world assessments. To address this, we propose a non-intrusive intelligibility prediction framework that leverages speech enhancers to provide a parallel enhanced-signal pathway, enabling robust predictions without reference signals. We evaluate three state-of-the-art enhancers and demonstrate that prediction performance depends on the choice of enhancer, with ensembles of strong enhancers yielding the best results. To improve cross-dataset generalization, we introduce a 2-clips augmentation strategy that enhances listener-specific variability, boosting robustness on unseen datasets. Our approach consistently outperforms the non-intrusive baseline, CPC2 Champion across multiple datasets, highlighting the potential of enhancer-guided non-intrusive intelligibility prediction for real-world applications. </p>
<blockquote>
<p>对于听力受损（HI）的听众来说，进行语音清晰度评估是评估助听器性能的关键。传统上，这依赖于听力测试或像HASPI这样的侵入性方法。然而，这些方法需要干净的参考信号，在真实世界中往往无法获得，从而在实验室评估和真实世界评估之间存在差距。为了解决这个问题，我们提出了一种非侵入性的清晰度预测框架，它利用语音增强器提供并行增强信号通路，能够在没有参考信号的情况下进行稳健的预测。我们评估了三种最先进的增强器，并证明预测性能取决于增强器的选择，强大的增强器组合能带来最佳结果。为了提高跨数据集泛化能力，我们引入了一种2片段增强策略，以增强听众特定变化，提高未见数据集上的稳健性。我们的方法在多数据集上始终优于非侵入性基线CPC2冠军，突显了增强器引导的非侵入性清晰度预测在现实应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16979v1">PDF</a> </p>
<p><strong>Summary</strong><br>听力受损者的语音清晰度评估对于评估助听器性能至关重要，传统方法依赖听力测试或如HASPI之类的侵入式方法。但现实世界中的场景常常无法获取纯净的参考信号，于是研究采用非侵入性的语音清晰度预测框架来解决这一问题。该框架利用语音增强器提供并行增强信号路径，无需参考信号即可进行稳健预测。研究评估了三种先进的增强器并发现预测性能取决于增强器的选择，强增强器组合表现最佳。为了改善跨数据集泛化能力，研究引入了增强听者特定变异性的2剪辑增强策略，在未见数据集上增强了稳健性。该研究提出的非侵入性方法表现优于CPC2冠军模型，突显了增强器引导的非侵入性语音清晰度预测在现实世界应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>听力受损者的语音清晰度评估对评估助听器性能非常重要。</li>
<li>传统评估方法依赖听力测试或侵入式方法，不适用于现实世界的多变场景。</li>
<li>提出的非侵入性语音清晰度预测框架解决了无纯净参考信号的评估问题。</li>
<li>采用语音增强器作为框架的核心工具，提供了无需参考信号的稳健预测。</li>
<li>增强器的选择对预测性能有显著影响，强增强器组合表现最佳。</li>
<li>为提高跨数据集泛化能力，引入了增强听者特定变异性的策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16979v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16979v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16979v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16979v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16979v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evaluating-Behavioral-Alignment-in-Conflict-Dialogue-A-Multi-Dimensional-Comparison-of-LLM-Agents-and-Humans"><a href="#Evaluating-Behavioral-Alignment-in-Conflict-Dialogue-A-Multi-Dimensional-Comparison-of-LLM-Agents-and-Humans" class="headerlink" title="Evaluating Behavioral Alignment in Conflict Dialogue: A   Multi-Dimensional Comparison of LLM Agents and Humans"></a>Evaluating Behavioral Alignment in Conflict Dialogue: A   Multi-Dimensional Comparison of LLM Agents and Humans</h2><p><strong>Authors:Deuksin Kwon, Kaleen Shrestha, Bin Han, Elena Hayoung Lee, Gale Lucas</strong></p>
<p>Large Language Models (LLMs) are increasingly deployed in socially complex, interaction-driven tasks, yet their ability to mirror human behavior in emotionally and strategically complex contexts remains underexplored. This study assesses the behavioral alignment of personality-prompted LLMs in adversarial dispute resolution by simulating multi-turn conflict dialogues that incorporate negotiation. Each LLM is guided by a matched Five-Factor personality profile to control for individual variation and enhance realism. We evaluate alignment across three dimensions: linguistic style, emotional expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the closest alignment with humans in linguistic style and emotional dynamics, while Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial alignment gaps persist. Our findings establish a benchmark for alignment between LLMs and humans in socially complex interactions, underscoring both the promise and the limitations of personality conditioning in dialogue modeling. </p>
<blockquote>
<p>大型语言模型（LLMs）越来越多地被部署在社会复杂、交互驱动的任务中，然而，它们在情感和战略复杂的背景下反映人类行为的能力仍被低估。本研究通过模拟多轮冲突对话中的谈判，评估人格驱动型LLMs在对抗性争议解决中的行为一致性。每个LLM都由匹配的五因素人格剖面图引导，以控制个体差异并增强现实性。我们从三个维度评估一致性：语言风格、情感表达（例如愤怒动态）和战略行为。GPT-4.1在语言风格和情感动态方面与人类最接近，而Claude-3.7-Sonnet最能反映战略行为。然而，仍存在显著的一致性差距。我们的研究建立了LLMs和人类在社会复杂交互中的一致性基准，强调了人格条件在对话建模中的前景和局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16394v1">PDF</a> Accepted to EMNLP 2025 (Main Conference)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在社会复杂、互动驱动的任务中越来越受欢迎，但它们能否在情绪化和策略性复杂的情境中模仿人类行为尚待探索。本研究通过模拟多轮冲突对话（融入谈判）评估人格引导的LLMs在对抗性争议解决中的行为一致性。每个LLM都由匹配的五因素人格剖面图引导，以控制个体差异并增强现实性。我们评估了三个维度的对齐情况：语言风格、情感表达和策略行为。GPT-4.1在语言风格和情感动力上最接近人类对齐，而Claude-3.7-Sonnet在策略行为方面反映最佳。然而，仍存在显著的对齐差距。我们的研究为LLMs在复杂社会互动中的与人类对齐建立了基准，强调了人格条件在对话建模中的潜力及局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在模拟人类行为方面仍存在较大差距，尤其是在复杂社会场景下的互动任务中。</li>
<li>本研究通过模拟多轮冲突对话评估了人格引导的LLMs在争议解决中的行为一致性。</li>
<li>LLMs的行为一致性评估包括语言风格、情感表达和策略行为三个维度。</li>
<li>GPT-4.1在语言风格和情感动力方面与人类对齐程度最高，而Claude-3.7-Sonnet在策略行为方面表现最佳。</li>
<li>尽管有一定程度的对齐，但仍存在显著的对齐差距，这表明需要进一步提高LLMs在复杂情境中的模拟人类行为的能力。</li>
<li>研究结果强调了人格条件在对话建模中的重要性，不仅有助于控制个体差异，还能增强模拟对话的现实性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16394">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16394v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16394v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Rethinking-Molecule-Synthesizability-with-Chain-of-Reaction"><a href="#Rethinking-Molecule-Synthesizability-with-Chain-of-Reaction" class="headerlink" title="Rethinking Molecule Synthesizability with Chain-of-Reaction"></a>Rethinking Molecule Synthesizability with Chain-of-Reaction</h2><p><strong>Authors:Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Saee Paliwal, Weili Nie, Arash Vahdat</strong></p>
<p>A well-known pitfall of molecular generative models is that they are not guaranteed to generate synthesizable molecules. There have been considerable attempts to address this problem, but given the exponentially large combinatorial space of synthesizable molecules, existing methods have shown limited coverage of the space and poor molecular optimization performance. To tackle these problems, we introduce ReaSyn, a generative framework for synthesizable projection where the model explores the neighborhood of given molecules in the synthesizable space by generating pathways that result in synthesizable analogs. To fully utilize the chemical knowledge contained in the synthetic pathways, we propose a novel perspective that views synthetic pathways akin to reasoning paths in large language models (LLMs). Specifically, inspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the chain-of-reaction (CoR) notation that explicitly states reactants, reaction types, and intermediate products for each step in a pathway. With the CoR notation, ReaSyn can get dense supervision in every reaction step to explicitly learn chemical reaction rules during supervised training and perform step-by-step reasoning. In addition, to further enhance the reasoning capability of ReaSyn, we propose reinforcement learning (RL)-based finetuning and goal-directed test-time compute scaling tailored for synthesizable projection. ReaSyn achieves the highest reconstruction rate and pathway diversity in synthesizable molecule reconstruction and the highest optimization performance in synthesizable goal-directed molecular optimization, and significantly outperforms previous synthesizable projection methods in synthesizable hit expansion. These results highlight ReaSyn’s superior ability to navigate combinatorially-large synthesizable chemical space. </p>
<blockquote>
<p>分子生成模型的一个众所周知的陷阱是它们不能保证生成可合成的分子。虽然已有大量尝试解决此问题，但考虑到可合成分子的组合空间呈指数级增长，现有方法的覆盖空间有限，分子优化性能较差。为了解决这些问题，我们引入了ReaSyn，这是一个可合成的投影生成框架，该模型通过生成产生可合成类似物的路径，探索给定分子在可合成空间中的邻域。为了充分利用合成路径中的化学知识，我们提出了一种新的视角，将合成路径视为大型语言模型（LLM）中的推理路径。具体来说，我们受到大型语言模型中思维链（CoT）推理的启发，引入了反应链（CoR）符号，该符号明确说明了路径每一步的反应物、反应类型和中间产物。借助CoR符号，ReaSyn可以在每个反应步骤中获得密集的监督，以在监督训练期间明确学习化学反应规则并进行逐步推理。此外，为了进一步增强ReaSyn的推理能力，我们提出了基于强化学习（RL）的微调以及针对可合成投影的目标导向测试时间计算缩放。ReaSyn在可合成分子重建中实现了最高的重建率和路径多样性，在可合成目标导向分子优化中取得了最高的优化性能，并在可合成命中扩展的可合成投影方法中显著优于以前的方法。这些结果突出了ReaSyn在组合式大型可合成化学空间中的卓越导航能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16084v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了分子生成模型的一个重要缺陷，即无法保证生成可合成的分子。为解决这一问题，本文提出了ReaSyn框架，该框架通过生成路径来探索给定分子的合成空间邻域，生成可合成的类似物。ReaSyn利用合成路径中的化学知识，借鉴大型语言模型的推理路径概念，提出了反应链（CoR）标记法。此外，为提高ReaSyn的推理能力，还引入了强化学习微调及针对合成投影的目标导向测试时间计算缩放方法。ReaSyn在可合成分子重建和可合成目标导向分子优化方面表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReaSyn是一个用于合成投影的生成框架，解决了分子生成模型可能无法生成可合成分子的问题。</li>
<li>ReaSyn通过生成路径探索给定分子的合成空间邻域，生成可合成的类似物。</li>
<li>借鉴大型语言模型的推理路径概念，ReaSyn提出了反应链（CoR）标记法，以利用合成路径中的化学知识。</li>
<li>ReaSyn通过强化学习微调增强了其推理能力。</li>
<li>ReaSyn实现了高的重建率和路径多样性，在可合成分子重建方面表现出卓越性能。</li>
<li>ReaSyn在可合成目标导向分子优化方面表现出最高的优化性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16084">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16084v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16084v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.16084v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue"><a href="#Ask-to-Clarify-Resolving-Instruction-Ambiguity-through-Multi-turn-Dialogue" class="headerlink" title="Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn   Dialogue"></a>Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn   Dialogue</h2><p><strong>Authors:Xingyao Lin, Xinghao Zhu, Tianyi Lu, Sicheng Xie, Hui Zhang, Xipeng Qiu, Zuxuan Wu, Yu-Gang Jiang</strong></p>
<p>The ultimate goal of embodied agents is to create collaborators that can interact with humans, not mere executors that passively follow instructions. This requires agents to communicate, coordinate, and adapt their actions based on human feedback. Recently, advances in VLAs have offered a path toward this goal. However, most current VLA-based embodied agents operate in a one-way mode: they receive an instruction and execute it without feedback. This approach fails in real-world scenarios where instructions are often ambiguous. In this paper, we address this problem with the Ask-to-Clarify framework. Our framework first resolves ambiguous instructions by asking questions in a multi-turn dialogue. Then it generates low-level actions end-to-end. Specifically, the Ask-to-Clarify framework consists of two components, one VLM for collaboration and one diffusion for action. We also introduce a connection module that generates conditions for the diffusion based on the output of the VLM. This module adjusts the observation by instructions to create reliable conditions. We train our framework with a two-stage knowledge-insulation strategy. First, we fine-tune the collaboration component using ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the action component while freezing the collaboration one. This preserves the interaction abilities while fine-tuning the diffusion to generate actions. The training strategy guarantees our framework can first ask questions, then generate actions. During inference, a signal detector functions as a router that helps our framework switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it outperforms existing state-of-the-art VLAs. The results suggest that our proposed framework, along with the training strategy, provides a path toward collaborative embodied agents. </p>
<blockquote>
<p>最终目标是创建能够与人类互动的合作者，而非仅仅是被动执行指令的执行者。这要求智能体进行通信、协调和根据人类反馈调整其行动。最近，视觉语言算法的进步为实现这一目标提供了途径。然而，当前大多数基于视觉语言算法的实体智能体都处于单向模式：它们接收指令并执行，无需反馈。这种方法在处理指令通常模糊不清的现实世界场景时就会失效。在本文中，我们通过“请求澄清”框架来解决这个问题。“请求澄清”框架首先通过多轮对话提出问题来解决模糊指令，然后生成端到端的低级动作。具体来说，“请求澄清”框架包含两个组件：一个用于协作的视觉语言模型和另一个用于动作生成的扩散模型。我们还引入了一个连接模块，该模块根据视觉语言模型的输出生成扩散的条件。该模块通过调整指令的观察结果来创建可靠的条件。我们使用两阶段的知识隔离策略来训练我们的框架。首先，我们使用解决模糊对话的数据微调协作组件，以处理模糊性。然后，我们整合动作组件，同时冻结协作组件。这既保留了交互能力，又可以在微调中生成动作。这种训练策略保证我们的框架能够先提问，然后生成动作。在推理过程中，信号检测器充当路由器，帮助我们的框架在提问和采取行动之间切换。我们在8个真实任务中评估了“请求澄清”框架的性能，它在现有最先进的视觉语言算法中表现出色。结果表明，我们提出的框架及其训练策略为实现协作式实体智能体提供了途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15061v2">PDF</a> 9 pages, 4 figures, 7 tables</p>
<p><strong>Summary</strong><br>     实现与人类协作的体感智能体，要求智能体能与人沟通、协调并根据人类反馈调整行动。本文提出了一个解决现实问题——基于视觉感知与语言理解的体感智能体通常是一维模式操作的问题，即只能接收指令并执行，缺乏反馈机制。为解决此问题，本文提出了“问询澄清框架”，它能在多轮对话中解决模糊指令问题并生成低层次行动。该框架包括两个组件：一个用于协作的视觉语言模型和用于行动扩散模型，同时引入一个连接模块，基于视觉语言模型的输出为扩散模型生成条件。采用两阶段知识绝缘策略训练框架，先微调协作组件处理模糊性，再整合行动组件。评估结果显示，在八个真实任务中，“问询澄清框架”优于现有最先进的视觉感知与语言理解技术，为构建协作式体感智能体提供了途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>体感智能体的目标是创建能与人类互动的协作者，而非仅执行指令的被动实体。</li>
<li>当前基于视觉感知与语言理解的体感智能体存在操作模式单一的问题，缺乏反馈机制。</li>
<li>“问询澄清框架”能解决模糊指令问题，通过多轮对话实现指令明确化。</li>
<li>框架包含视觉语言模型、扩散模型和连接模块三个关键组件。</li>
<li>采用两阶段知识绝缘策略训练框架，确保智能体能先提问后行动。</li>
<li>在八个真实任务中，“问询澄清框架”展现出优越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15061">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.15061v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.15061v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.15061v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.15061v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.15061v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.15061v2/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2509.15061v2/page_5_2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MEDAL-A-Framework-for-Benchmarking-LLMs-as-Multilingual-Open-Domain-Dialogue-Evaluators"><a href="#MEDAL-A-Framework-for-Benchmarking-LLMs-as-Multilingual-Open-Domain-Dialogue-Evaluators" class="headerlink" title="MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain   Dialogue Evaluators"></a>MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain   Dialogue Evaluators</h2><p><strong>Authors:John Mendonça, Alon Lavie, Isabel Trancoso</strong></p>
<p>Evaluating the quality of open-domain chatbots has become increasingly reliant on LLMs acting as automatic judges. However, existing meta-evaluation benchmarks are static, outdated, and lacking in multilingual coverage, limiting their ability to fully capture subtle weaknesses in evaluation. We introduce MEDAL, an automated multi-agent framework for curating more representative and diverse open-domain dialogue evaluation benchmarks. Our approach leverages several state-of-the-art LLMs to generate user-chatbot multilingual dialogues, conditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a multidimensional analysis of the performance of the chatbots, uncovering noticeable cross-lingual performance differences. Guided by this large-scale evaluation, we curate a new meta-evaluation multilingual benchmark and human-annotate samples with nuanced quality judgments. This benchmark is then used to assess the ability of several reasoning and non-reasoning LLMs to act as evaluators of open-domain dialogues. Using MEDAL, we uncover that state-of-the-art judges fail to reliably detect nuanced issues such as lack of empathy, commonsense, or relevance. </p>
<blockquote>
<p>评估开放领域聊天机器人的质量越来越依赖于作为自动裁判的大型语言模型（LLMs）。然而，现有的元评估基准测试是静态的、过时的，并且缺乏多语言覆盖，限制了它们全面捕捉评估中细微弱点的能力。我们介绍了 MEDAL，这是一个自动化的多代理框架，用于创建更具代表性和多样性的开放领域对话评估基准测试。我们的方法利用若干先进的大型语言模型来生成基于不同种子语境的用户聊天机器人多语言对话。然后，使用强大的大型语言模型（GPT-4.1）对聊天机器人的性能进行多维分析，发现显著的语言间性能差异。在这一大规模评估的指导下，我们创建了一个新的元评估多语言基准测试，并对样本进行了微妙的品质判断进行人工标注。此基准测试然后用于评估多个理性与非理性的大型语言模型在作为开放领域对话评估器时的能力。使用 MEDAL，我们发现最先进的裁判在可靠检测诸如缺乏同理心、常识或关联性等微妙问题上存在不足。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22777v3">PDF</a> October ARR</p>
<p><strong>Summary</strong></p>
<p>本文介绍了评估开放领域聊天机器人质量的新方法MEDAL。该方法利用多语言大型语言模型（LLM）生成用户与聊天机器人的对话，并进行多维度的分析评估聊天机器人的性能。研究发现，现有的评估基准测试存在局限性，而新的多语言基准测试能够发现显著的语言间性能差异。此外，还发现当前顶尖的大型语言模型在检测某些细微问题（如缺乏同理心、常识或相关性）方面存在不足。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在开放域聊天机器人评估中扮演着越来越重要的角色。</li>
<li>现有评估基准测试存在局限性，需要更代表性和多样性的评估方法。</li>
<li>MEDAL框架利用多语言大型语言模型生成用户与聊天机器人的对话。</li>
<li>GPT-4.1用于多维度分析聊天机器人性能。</li>
<li>存在显著的语言间性能差异。</li>
<li>新的多语言基准测试能够评估聊天机器人的细微问题，如缺乏同理心、常识或相关性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22777">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2505.22777v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2505.22777v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2505.22777v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2505.22777v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Pursuit-of-Empathy-Evaluating-Small-Language-Models-for-PTSD-Dialogue-Support"><a href="#The-Pursuit-of-Empathy-Evaluating-Small-Language-Models-for-PTSD-Dialogue-Support" class="headerlink" title="The Pursuit of Empathy: Evaluating Small Language Models for PTSD   Dialogue Support"></a>The Pursuit of Empathy: Evaluating Small Language Models for PTSD   Dialogue Support</h2><p><strong>Authors:Suhas BN, Yash Mahajan, Dominik Mattioli, Andrew M. Sherrill, Rosa I. Arriaga, Chris W. Wiese, Saeed Abdullah</strong></p>
<p>This paper investigates the capacity of small language models (0.5B-5B parameters) to generate empathetic responses for individuals with PTSD. We introduce Trauma-Informed Dialogue for Empathy (TIDE), a novel dataset comprising 10,000 two-turn conversations across 500 diverse, clinically-grounded PTSD personas (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/yenopoya/TIDE">https://huggingface.co/datasets/yenopoya/TIDE</a>). Using frontier model outputs as ground truth, we evaluate eight small LLMs in zero-shot settings and after fine-tuning. Fine-tuning enhances empathetic capabilities, improving cosine similarity and perceived empathy, although gains vary across emotional scenarios and smaller models exhibit a “knowledge transfer ceiling.” As expected, Claude Sonnet 3.5 consistently outperforms all models, but surprisingly, the smaller models often approach human-rated empathy levels. Demographic analyses showed that older adults favored responses that validated distress before offering support (p &#x3D; .004), while graduate-educated users preferred emotionally layered replies in specific scenarios. Gender-based differences were minimal (p &gt; 0.15), suggesting the feasibility of broadly empathetic model designs. This work offers insights into building resource-efficient, emotionally intelligent systems for mental health support. </p>
<blockquote>
<p>本文探讨了小型语言模型（0.5B-5B参数）在生成针对患有PTSD个体的共情响应方面的能力。我们引入了基于创伤的共情对话（TIDE），这是一个新型数据集，包含500个多样且基于临床的PTSD人物角色的1万次两轮对话（<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/yenopoya/TIDE%EF%BC%89%E3%80%82%E4%BB%A5%E5%89%8D%E6%B2%BF%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E4%BD%9C%E4%B8%BA%E5%9F%BA%E5%87%86%EF%BC%8C%E6%88%91%E4%BB%AC%E8%AF%84%E4%BC%B0%E4%BA%86%E5%85%AB%E7%A7%8D%E5%B0%8F%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E9%9D%9E%E5%8D%B3%E6%97%B6%E6%83%85%E5%86%B5%E4%B8%8B%E7%9A%84%E9%9B%B6%E9%95%9C%E5%A4%B4%E8%A1%A8%E7%8E%B0%E5%92%8C%E8%B0%83%E6%95%B4%E5%90%8E%E7%9A%84%E8%A1%A8%E7%8E%B0%E3%80%82%E5%BE%AE%E8%B0%83%E6%8F%90%E9%AB%98%E4%BA%86%E5%85%B1%E6%83%85%E8%83%BD%E5%8A%9B%EF%BC%8C%E6%94%B9%E8%BF%9B%E4%BA%86%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%92%8C%E6%84%9F%E7%9F%A5%E5%88%B0%E7%9A%84%E5%85%B1%E6%83%85%E6%B0%B4%E5%B9%B3%EF%BC%8C%E5%B0%BD%E7%AE%A1%E4%B8%8D%E5%90%8C%E6%83%85%E7%BB%AA%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E6%94%B6%E7%9B%8A%E6%9C%89%E6%89%80%E4%B8%8D%E5%90%8C%EF%BC%8C%E8%BE%83%E5%B0%8F%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%8E%B0%E5%87%BA%E2%80%9C%E7%9F%A5%E8%AF%86%E8%BD%AC%E7%A7%BB%E4%B8%8A%E9%99%90%E2%80%9D%E3%80%82%E6%AD%A3%E5%A6%82%E9%A2%84%E6%9C%9F%E7%9A%84%E9%82%A3%E6%A0%B7%EF%BC%8CClaude">https://huggingface.co/datasets/yenopoya/TIDE）。以前沿模型输出作为基准，我们评估了八种小型语言模型在非即时情况下的零镜头表现和调整后的表现。微调提高了共情能力，改进了余弦相似性和感知到的共情水平，尽管不同情绪场景下的收益有所不同，较小的模型表现出“知识转移上限”。正如预期的那样，Claude</a> Sonnet 3.5始终表现优于所有其他模型，但令人惊讶的是，较小的模型往往接近人类水平的共情能力。人口统计分析显示，成年人更喜欢在提供支持之前验证痛苦（p &#x3D; .004），而高学历用户则在特定场景中偏爱情感丰富的回复。基于性别的差异最小（p &gt; 0.15），这表明广泛共情模型设计的可行性。这项工作为构建资源高效、情感智能的心理健康支持系统提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15065v2">PDF</a> 23 pages, 3 figures. Accepted for Oral presentation at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了小型语言模型（参数范围在0.5B到5B之间）在生成针对患有PTSD个体的同理心回应方面的能力。文章介绍了一种名为Trauma-Informed Dialogue for Empathy（TIDE）的新型数据集，其中包含1万组针对500个不同且基于临床的PTSD人物的二轮对话。文章对八种小型LLM进行了零样本评估和微调后的评估，发现微调可以提高模型的同理心能力，但在不同情感场景下效果各异，小型模型存在“知识转移上限”。Claude Sonnet 3.5表现最佳，但令人惊讶的是，小型模型往往也能达到人类水平的同理心。人口统计分析显示，成年人更喜欢在提供支持前先验证痛苦（p &#x3D; .004），而高学历用户在特定场景下的情感回复更丰富。性别差异较小（p &gt; 0.15），表明广泛设计具有同理心的模型是可行的。本文为构建资源高效、情感智能的心理健康支持系统提供了见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了Trauma-Informed Dialogue for Empathy（TIDE）数据集，包含多样化的PTSD人物对话。</li>
<li>对小型LLM进行了零样本和微调后的评估，发现微调能提高模型的同理心能力。</li>
<li>Claude Sonnet 3.5表现最佳，但小型模型在某些情况下也能展现高同理心。</li>
<li>成年人更喜欢先验证痛苦再提供支持，而高学历用户需要更丰富的情感回复。</li>
<li>性别差异在模型表现中较小，表明广泛设计具有同理心的模型的可行性。</li>
<li>文章强调了构建资源高效、情感智能的心理健康支持系统的必要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15065">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2505.15065v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2505.15065v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2505.15065v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2505.15065v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="How-Real-Are-Synthetic-Therapy-Conversations-Evaluating-Fidelity-in-Prolonged-Exposure-Dialogues"><a href="#How-Real-Are-Synthetic-Therapy-Conversations-Evaluating-Fidelity-in-Prolonged-Exposure-Dialogues" class="headerlink" title="How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in   Prolonged Exposure Dialogues"></a>How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in   Prolonged Exposure Dialogues</h2><p><strong>Authors:Suhas BN, Dominik Mattioli, Saeed Abdullah, Rosa I. Arriaga, Chris W. Wiese, Andrew M. Sherrill</strong></p>
<p>Synthetic data adoption in healthcare is driven by privacy concerns, data access limitations, and high annotation costs. We explore synthetic Prolonged Exposure (PE) therapy conversations for PTSD as a scalable alternative for training clinical models. We systematically compare real and synthetic dialogues using linguistic, structural, and protocol-specific metrics like turn-taking and treatment fidelity. We introduce and evaluate PE-specific metrics, offering a novel framework for assessing clinical fidelity beyond surface fluency. Our findings show that while synthetic data successfully mitigates data scarcity and protects privacy, capturing the most subtle therapeutic dynamics remains a complex challenge. Synthetic dialogues successfully replicate key linguistic features of real conversations, for instance, achieving a similar Readability Score (89.2 vs. 88.1), while showing differences in some key fidelity markers like distress monitoring. This comparison highlights the need for fidelity-aware metrics that go beyond surface fluency to identify clinically significant nuances. Our model-agnostic framework is a critical tool for developers and clinicians to benchmark generative model fidelity before deployment in sensitive applications. Our findings help clarify where synthetic data can effectively complement real-world datasets, while also identifying areas for future refinement. </p>
<blockquote>
<p>医疗保健领域采用合成数据是出于隐私担忧、数据访问限制和高标注成本的驱动。我们探索了合成延长暴露（PE）疗法对话作为训练临床模型的可扩展替代方案，用于治疗创伤后应激障碍（PTSD）。我们系统地使用语言、结构和协议特定指标（如轮流和治疗效果的忠实度）比较真实和合成的对话。我们引入并评估了PE特定指标，为评估临床忠实度提供了一个超越表面流利的新框架。我们的研究发现，虽然合成数据成功地缓解了数据稀缺问题并保护了隐私，但捕捉最微妙的疗法动态仍然是一个复杂的挑战。合成对话成功复制了真实对话的主要语言特征，例如，达到了类似的阅读得分（89.2比88.1），在某些关键忠实度标记（如压力监测）方面显示出差异。这种比较凸显了忠实度感知指标的需要，这些指标超越表面流利度，能够识别临床上重要的细微差别。我们的模型无关框架是开发者和临床医生在敏感应用部署之前评估生成模型忠实度的关键工具。我们的发现有助于澄清合成数据在哪些情况下可以有效地补充真实世界数据集，同时也确定了未来改进的领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21800v4">PDF</a> 10 pages, 5 tables. Accepted for Poster presentation at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>合成数据在医疗保健领域的应用主要受隐私担忧、数据访问限制和高标注成本等因素的驱动。本文探索了合成延长暴露（PE）疗法对话作为训练临床模型的可扩展替代方案。通过语言、结构和协议特定指标（如轮流发言和治疗忠实度）系统地比较真实和合成的对话。本文介绍了PE特定指标，提供了一个评估临床忠实度的新框架，而不仅仅局限于表面流利度。研究发现，合成数据虽然成功缓解了数据稀缺性和保护隐私的问题，但捕捉最微妙的治疗动态仍然是一个复杂的挑战。合成对话成功复制了真实对话的主要语言特征，例如达到类似的阅读分数（89.2比88.1），而在一些关键忠实度指标（如压力监测）方面存在差异。对比结果凸显了在应用超过表面流利度的临床敏感应用中，忠实度感知指标对识别具有临床意义的细微差别的需求。本文的模型无关框架为开发者和临床医生提供了在部署前评估生成模型忠实度的关键工具。研究结果有助于明确合成数据在哪些情况下能有效地补充真实数据集，同时也确定了未来改进的领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>合成数据在医疗保健领域的采用受到隐私担忧、数据访问限制和高标注成本的影响。</li>
<li>合成延长暴露（PE）疗法对话作为一种可扩展的替代方案被探索，用于训练临床模型。</li>
<li>系统比较了真实和合成对话的多种方式，包括语言、结构和协议特定指标。</li>
<li>引入并评估了PE特定指标，以评估临床忠实度，超越了表面流利度的框架。</li>
<li>合成数据虽然成功解决了数据稀缺性和隐私保护问题，但在捕捉治疗细微动态方面仍存在挑战。</li>
<li>合成对话成功复制了真实对话的主要语言特征，但关键忠实度指标如压力监测等方面存在差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2504.21800v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2504.21800v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2504.21800v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2504.21800v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Interactive/2504.21800v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-28/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-28\./crop_Talking Head Generation/2509.20961v1/page_5_1.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-09-28  Unlocking Financial Insights An advanced Multimodal Summarization with   Multimodal Output Framework for Financial Advisory Videos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-5f14ccd83f750a5754c5f964febf07cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760428787&auth_key=1760428787-0-0-7ca7786b12d509fcb9018c3e26f103b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-09-24  TMD-TTS A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for   Ü-Tsang, Amdo and Kham Speech Dataset Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
