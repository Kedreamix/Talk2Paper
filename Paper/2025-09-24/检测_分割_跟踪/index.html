<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  An Empirical Study on the Robustness of YOLO Models for Underwater   Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-39a272d1a6996c7909eceb952b972cec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683807&auth_key=1760683807-0-0-696beffb78ca90a72f3c11eb9469aa3b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    53 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-24-æ›´æ–°"><a href="#2025-09-24-æ›´æ–°" class="headerlink" title="2025-09-24 æ›´æ–°"></a>2025-09-24 æ›´æ–°</h1><h2 id="An-Empirical-Study-on-the-Robustness-of-YOLO-Models-for-Underwater-Object-Detection"><a href="#An-Empirical-Study-on-the-Robustness-of-YOLO-Models-for-Underwater-Object-Detection" class="headerlink" title="An Empirical Study on the Robustness of YOLO Models for Underwater   Object Detection"></a>An Empirical Study on the Robustness of YOLO Models for Underwater   Object Detection</h2><p><strong>Authors:Edwine Nabahirwa, Wei Song, Minghua Zhang, Shufan Chen</strong></p>
<p>Underwater object detection (UOD) remains a critical challenge in computer vision due to underwater distortions which degrade low-level features and compromise the reliability of even state-of-the-art detectors. While YOLO models have become the backbone of real-time object detection, little work has systematically examined their robustness under these uniquely challenging conditions. This raises a critical question: Are YOLO models genuinely robust when operating under the chaotic and unpredictable conditions of underwater environments? In this study, we present one of the first comprehensive evaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated underwater environments. Using a unified dataset of 10,000 annotated images from DUO and Roboflow100, we not only benchmark model robustness but also analyze how distortions affect key low-level features such as texture, edges, and color. Our findings show that (1) YOLOv12 delivers the strongest overall performance but is highly vulnerable to noise, and (2) noise disrupts edge and texture features, explaining the poor detection performance in noisy images. Class imbalance is a persistent challenge in UOD. Experiments revealed that (3) image counts and instance frequency primarily drive detection performance, while object appearance exerts only a secondary influence. Finally, we evaluated lightweight training-aware strategies: noise-aware sample injection, which improves robustness in both noisy and real-world conditions, and fine-tuning with advanced enhancement, which boosts accuracy in enhanced domains but slightly lowers performance in original data, demonstrating strong potential for domain adaptation, respectively. Together, these insights provide practical guidance for building resilient and cost-efficient UOD systems. </p>
<blockquote>
<p>æ°´ä¸‹ç›®æ ‡æ£€æµ‹ï¼ˆUODï¼‰ä»ç„¶æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œç”±äºæ°´ä¸‹å¤±çœŸä¼šç ´åä½çº§ç‰¹å¾ï¼Œç”šè‡³å½±å“æœ€å…ˆè¿›æ£€æµ‹å™¨çš„å¯é æ€§ã€‚è™½ç„¶YOLOæ¨¡å‹å·²æˆä¸ºå®æ—¶ç›®æ ‡æ£€æµ‹çš„æ ¸å¿ƒï¼Œä½†å¾ˆå°‘æœ‰å·¥ä½œç³»ç»Ÿåœ°ç ”ç©¶å®ƒä»¬åœ¨è¿™äº›ç‹¬ç‰¹æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šYOLOæ¨¡å‹åœ¨æ°´ä¸‹ç¯å¢ƒæ··ä¹±å’Œä¸å¯é¢„æµ‹çš„æ¡ä»¶ä¸‹çœŸæ­£ç¨³å¥å—ï¼Ÿåœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹æœ€è¿‘çš„YOLOå˜ä½“ï¼ˆYOLOv8-YOLOv12ï¼‰è¿›è¡Œäº†é¦–æ¬¡å…¨é¢çš„è¯„ä¼°ä¹‹ä¸€ï¼Œè·¨è¶Šå…­ç§æ¨¡æ‹Ÿæ°´ä¸‹ç¯å¢ƒã€‚æˆ‘ä»¬ä½¿ç”¨DUOå’ŒRoboflow100çš„ç»Ÿä¸€æ•°æ®é›†ï¼Œä¸ä»…è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œè¿˜åˆ†æå¤±çœŸå¦‚ä½•å½±å“çº¹ç†ã€è¾¹ç¼˜å’Œé¢œè‰²ç­‰å…³é”®ä½çº§ç‰¹å¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼šï¼ˆ1ï¼‰YOLOv12æ€»ä½“æ€§èƒ½æœ€å¼ºï¼Œä½†å¯¹å™ªå£°é«˜åº¦æ•æ„Ÿï¼›ï¼ˆ2ï¼‰å™ªå£°ä¼šæ‰°ä¹±è¾¹ç¼˜å’Œçº¹ç†ç‰¹å¾ï¼Œè¿™è§£é‡Šäº†å™ªå£°å›¾åƒä¸­æ£€æµ‹æ€§èƒ½ä¸ä½³çš„åŸå› ã€‚ç±»åˆ«ä¸å¹³è¡¡æ˜¯UODä¸­çš„æŒä¹…æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼Œï¼ˆ3ï¼‰å›¾åƒè®¡æ•°å’Œå®ä¾‹é¢‘ç‡ä¸»è¦é©±åŠ¨æ£€æµ‹æ€§èƒ½ï¼Œè€Œå¯¹è±¡å¤–è§‚çš„å½±å“è¾ƒå°ã€‚æœ€åï¼Œæˆ‘ä»¬è¯„ä¼°äº†è½»é‡çº§è®­ç»ƒæ„ŸçŸ¥ç­–ç•¥ï¼šå™ªå£°æ„ŸçŸ¥æ ·æœ¬æ³¨å…¥ï¼Œè¯¥ç­–ç•¥æé«˜äº†å™ªå£°å’Œç°å®ä¸–ç•Œæ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼›ä»¥åŠä½¿ç”¨å…ˆè¿›å¢å¼ºæŠ€æœ¯çš„å¾®è°ƒï¼Œè¿™æé«˜äº†å¢å¼ºé¢†åŸŸçš„å‡†ç¡®æ€§ï¼Œä½†ç•¥å¾®é™ä½äº†åŸå§‹æ•°æ®çš„æ€§èƒ½ï¼Œåˆ†åˆ«æ˜¾ç¤ºå‡ºåŸŸé€‚åº”çš„å¼ºå¤§æ½œåŠ›ã€‚è¿™äº›è§è§£å…±åŒä¸ºæ„å»ºå…·æœ‰å¼¹æ€§å’Œæˆæœ¬æ•ˆç›Šçš„æ°´ä¸‹ç›®æ ‡æ£€æµ‹ç³»ç»Ÿæä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17561v1">PDF</a> 28 Pages, 12 Figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ°´ä¸‹ç›®æ ‡æ£€æµ‹ï¼ˆUODï¼‰çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ¢è®¨äº†YOLOæ¨¡å‹åœ¨æ°´ä¸‹ç¯å¢ƒä¸­çš„ç¨³å¥æ€§ã€‚ç ”ç©¶é€šè¿‡å¯¹YOLOçš„ä¸åŒå˜ä½“ï¼ˆYOLOv8è‡³YOLOv12ï¼‰åœ¨å…­ä¸ªæ¨¡æ‹Ÿæ°´ä¸‹ç¯å¢ƒä¸­çš„ç»¼åˆè¯„ä¼°ï¼Œå‘ç°YOLOv12æ•´ä½“æ€§èƒ½æœ€ä½³ä½†æ˜“å—å™ªå£°å½±å“ã€‚åŒæ—¶ï¼Œç ”ç©¶åˆ†æäº†æ°´ä¸‹å¤±çœŸå¯¹çº¹ç†ã€è¾¹ç¼˜å’Œé¢œè‰²ç­‰å…³é”®ä½å±‚æ¬¡ç‰¹å¾çš„å½±å“ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜å›¾åƒæ•°é‡å’Œå®ä¾‹é¢‘ç‡å¯¹æ£€æµ‹æ€§èƒ½æœ‰ä¸»è¦å½±å“ï¼Œè€Œç‰©ä½“å¤–è§‚çš„å½±å“è¾ƒå°ã€‚æœ€åï¼Œç ”ç©¶è¯„ä¼°äº†è½»é‡çº§è®­ç»ƒæ„ŸçŸ¥ç­–ç•¥ï¼Œå¦‚å™ªå£°æ„ŸçŸ¥æ ·æœ¬æ³¨å…¥å’Œé«˜çº§å¢å¼ºå¾®è°ƒç­‰ï¼Œä¸ºæé«˜æ°´ä¸‹ç›®æ ‡æ£€æµ‹çš„é²æ£’æ€§å’Œæ•ˆç‡æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>YOLOv12åœ¨æ°´ä¸‹ç¯å¢ƒä¸­æ•´ä½“æ€§èƒ½æœ€ä½³ï¼Œä½†åœ¨å™ªå£°å¹²æ‰°æ–¹é¢å­˜åœ¨è¾ƒé«˜è„†å¼±æ€§ã€‚</li>
<li>å™ªå£°å¯¹è¾¹ç¼˜å’Œçº¹ç†ç‰¹å¾çš„ç ´åå½±å“äº†æ°´ä¸‹ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨æ°´ä¸‹ç›®æ ‡æ£€æµ‹ä¸­ï¼Œå›¾åƒæ•°é‡å’Œå®ä¾‹é¢‘ç‡å¯¹æ£€æµ‹æ€§èƒ½èµ·ä¸»è¦ä½œç”¨ã€‚</li>
<li>ç‰©ä½“å¤–è§‚å¯¹æ£€æµ‹æ€§èƒ½çš„å½±å“ç›¸å¯¹æ¬¡è¦ã€‚</li>
<li>å™ªå£°æ„ŸçŸ¥æ ·æœ¬æ³¨å…¥ç­–ç•¥æé«˜äº†æ¨¡å‹åœ¨å™ªå£°å’ŒçœŸå®ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>é‡‡ç”¨é«˜çº§å¢å¼ºå¾®è°ƒç­–ç•¥åœ¨å¢å¼ºé¢†åŸŸæé«˜äº†ç²¾åº¦ï¼Œä½†ç•¥å¾®é™ä½äº†åŸå§‹æ•°æ®çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3e824e43d63e542fac35b3b35ba198c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424810&auth_key=1760424810-0-0-4b35ce907f8c4e49a7713e06f8d1f609&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-48bf43c00b759d4d4d657d8accc62746~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424825&auth_key=1760424825-0-0-a7f0d18e3e98e96cfa0b52168d89b466&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0e8a9ef570b27c3a2731cfa8ac75bb4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424831&auth_key=1760424831-0-0-5ca37d483a6d43033202431f97971b37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-83f0bb519b8fc79aed4a0a6c5c7c2ace~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424838&auth_key=1760424838-0-0-312f1cfb85f0ed601a682f964bb1ea74&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DepTR-MOT-Unveiling-the-Potential-of-Depth-Informed-Trajectory-Refinement-for-Multi-Object-Tracking"><a href="#DepTR-MOT-Unveiling-the-Potential-of-Depth-Informed-Trajectory-Refinement-for-Multi-Object-Tracking" class="headerlink" title="DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory   Refinement for Multi-Object Tracking"></a>DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory   Refinement for Multi-Object Tracking</h2><p><strong>Authors:Buyin Deng, Lingxin Huang, Kai Luo, Fei Teng, Kailun Yang</strong></p>
<p>Visual Multi-Object Tracking (MOT) is a crucial component of robotic perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D cues, such as bounding boxes and motion modeling, which struggle under occlusions and close-proximity interactions. Trackers relying on these 2D cues are particularly unreliable in robotic environments, where dense targets and frequent occlusions are common. While depth information has the potential to alleviate these issues, most existing MOT datasets lack depth annotations, leading to its underexploited role in the domain. To unveil the potential of depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based detector enhanced with instance-level depth information. Specifically, we propose two key innovations: (i) foundation model-based instance-level soft depth label supervision, which refines depth prediction, and (ii) the distillation of dense depth maps to maintain global depth consistency. These strategies enable DepTR-MOT to output instance-level depth during inference, without requiring foundation models and without additional computational cost. By incorporating depth cues, our method enhances the robustness of the TBD paradigm, effectively resolving occlusion and close-proximity challenges. Experiments on both the QuadTrack and DanceTrack datasets demonstrate the effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47, respectively. In particular, results on QuadTrack, a robotic platform MOT dataset, highlight the advantages of our method in handling occlusion and close-proximity challenges in robotic tracking. The source code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/warriordby/DepTR-MOT">https://github.com/warriordby/DepTR-MOT</a>. </p>
<blockquote>
<p>è§†è§‰å¤šç›®æ ‡è·Ÿè¸ªï¼ˆMOTï¼‰æ˜¯æœºå™¨äººæ„ŸçŸ¥çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†ç°æœ‰çš„è·Ÿè¸ªæ£€æµ‹ï¼ˆTBDï¼‰æ–¹æ³•å¾€å¾€ä¾èµ–äºäºŒç»´çº¿ç´¢ï¼Œå¦‚è¾¹ç•Œæ¡†å’Œè¿åŠ¨å»ºæ¨¡ï¼Œè¿™åœ¨é®æŒ¡å’Œè¿‘è·ç¦»äº¤äº’çš„æƒ…å†µä¸‹å­˜åœ¨å›°éš¾ã€‚ä¾èµ–è¿™äº›äºŒç»´çº¿ç´¢çš„è·Ÿè¸ªå™¨åœ¨æœºå™¨äººç¯å¢ƒä¸­å°¤å…¶ä¸å¯é ï¼Œå…¶ä¸­å¯†é›†ç›®æ ‡å’Œé¢‘ç¹é®æŒ¡å¾ˆå¸¸è§ã€‚è™½ç„¶æ·±åº¦ä¿¡æ¯æœ‰æ½œåŠ›è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„MOTæ•°æ®é›†ç¼ºä¹æ·±åº¦æ³¨é‡Šï¼Œå¯¼è‡´å…¶åœ¨è¯¥é¢†åŸŸçš„ä½œç”¨è¢«ä½ä¼°ã€‚ä¸ºäº†æ­ç¤ºæ·±åº¦ä¿¡æ¯è½¨è¿¹ä¿®æ­£çš„æ½œåŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†DepTR-MOTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºDETRçš„æ¢æµ‹å™¨ï¼Œå¢å¼ºäº†å®ä¾‹çº§åˆ«çš„æ·±åº¦ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šï¼ˆiï¼‰åŸºäºåŸºç¡€æ¨¡å‹çš„å®ä¾‹çº§è½¯æ·±åº¦æ ‡ç­¾ç›‘ç£ï¼Œè¿™å¯ä»¥ä¼˜åŒ–æ·±åº¦é¢„æµ‹ï¼›ï¼ˆiiï¼‰å¯†é›†æ·±åº¦å›¾çš„è’¸é¦ä»¥ä¿æŒå…¨å±€æ·±åº¦ä¸€è‡´æ€§ã€‚è¿™äº›ç­–ç•¥ä½¿DepTR-MOTèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¾“å‡ºå®ä¾‹çº§åˆ«çš„æ·±åº¦ï¼Œè€Œæ— éœ€åŸºç¡€æ¨¡å‹ä¸”æ— éœ€é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚é€šè¿‡èå…¥æ·±åº¦çº¿ç´¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†TBDèŒƒå¼çš„ç¨³å¥æ€§ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†é®æŒ¡å’Œè¿‘è·ç¦»æŒ‘æˆ˜ã€‚åœ¨QuadTrackå’ŒDanceTrackæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåˆ†åˆ«å®ç°äº†HOTAåˆ†æ•°ä¸º27.59å’Œ44.47ã€‚ç‰¹åˆ«æ˜¯åœ¨QuadTrackæœºå™¨äººå¹³å°MOTæ•°æ®é›†ä¸Šçš„ç»“æœï¼Œçªæ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœºå™¨äººè·Ÿè¸ªä¸­å¤„ç†é®æŒ¡å’Œè¿‘è·ç¦»æŒ‘æˆ˜çš„ä¼˜åŠ¿ã€‚æºä»£ç å°†å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/warriordby/DepTR-MOT%E3%80%82">https://github.com/warriordby/DepTR-MOTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17323v1">PDF</a> The source code will be made publicly available at   <a target="_blank" rel="noopener" href="https://github.com/warriordby/DepTR-MOT">https://github.com/warriordby/DepTR-MOT</a></p>
<p><strong>Summary</strong><br>    åŸºäºæ·±åº¦ä¿¡æ¯åŠ å¼ºçš„Detrè§†è§‰å¤šç›®æ ‡è·Ÿè¸ªå™¨åœ¨è§£å†³æœºå™¨äººç¯å¢ƒä¸­ç‰©ä½“é®æŒ¡ä¸è¿‘è·ç¦»äº¤äº’çš„è·Ÿè¸ªé—®é¢˜æ–¹é¢å…·æœ‰å“è¶Šæ•ˆæœã€‚å®ƒé€šè¿‡å¼•å…¥å®ä¾‹çº§æ·±åº¦ä¿¡æ¯æå‡è½¨è¿¹ç²¾ç»†åº¦ï¼Œå¹¶é€šè¿‡åŸºç¡€æ¨¡å‹è¾…åŠ©çš„å®ä¾‹çº§è½¯æ·±åº¦æ ‡ç­¾ç›‘ç£å’Œæ·±åº¦å›¾çš„è’¸é¦æ–¹æ³•ç»´æŒå…¨å±€æ·±åº¦ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨QuadTrackå’ŒDanceTrackæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†è§‰å¤šç›®æ ‡è·Ÿè¸ªï¼ˆMOTï¼‰æ˜¯æœºå™¨äººæ„ŸçŸ¥çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç°æœ‰çš„Tracking-By-Detectionï¼ˆTBDï¼‰æ–¹æ³•ä¸»è¦ä¾èµ–äºŒç»´çº¿ç´¢å¦‚è¾¹ç•Œæ¡†å’Œè¿åŠ¨å»ºæ¨¡ï¼Œè¿™åœ¨é®æŒ¡å’Œè¿‘è·ç¦»äº¤äº’æƒ…å†µä¸‹å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦ä¿¡æ¯èƒ½å¢å¼ºè·Ÿè¸ªå™¨çš„æ€§èƒ½ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„MOTæ•°æ®é›†ç¼ºä¹æ·±åº¦æ ‡æ³¨ï¼Œé™åˆ¶äº†å…¶åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>DepTR-MOTæ˜¯ä¸€ç§åŸºäºDETRçš„æ£€æµ‹å™¨ï¼Œå¼•å…¥äº†å®ä¾‹çº§æ·±åº¦ä¿¡æ¯æ¥æå‡è½¨è¿¹ç²¾ç»†åº¦ã€‚å…¶ä¸¤å¤§å…³é”®åˆ›æ–°ä¸ºåŸºç¡€æ¨¡å‹è¾…åŠ©çš„å®ä¾‹çº§è½¯æ·±åº¦æ ‡ç­¾ç›‘ç£å’Œç”¨äºç»´æŒå…¨å±€æ·±åº¦ä¸€è‡´æ€§çš„æ·±åº¦å›¾è’¸é¦æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-798538978cb6b4c6272f06a9587ea207~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424845&auth_key=1760424845-0-0-e1a571afe72b05de6ef0a693f075ba94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-206a32985e4e36f037d071f42e5c8067~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424852&auth_key=1760424852-0-0-96ca93c5e3f523535cd5a454ce0b8abf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b82042d50cbf467b32dc6a71bd7871f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424859&auth_key=1760424859-0-0-0e2e3afebc3543f5d760deb0f4787869&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e332fcf295aebb21bc9906b9434c712b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424866&auth_key=1760424866-0-0-89564f6fdc095a844fd5f1d5529e1237&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d1da33ae1ba2a3f19e2284f4d1d2893~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424873&auth_key=1760424873-0-0-0d4815cb7a3b95484f0dc4660f28677f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhanced-Detection-of-Tiny-Objects-in-Aerial-Images"><a href="#Enhanced-Detection-of-Tiny-Objects-in-Aerial-Images" class="headerlink" title="Enhanced Detection of Tiny Objects in Aerial Images"></a>Enhanced Detection of Tiny Objects in Aerial Images</h2><p><strong>Authors:Kihyun Kim, Michalis Lazarou, Tania Stathaki</strong></p>
<p>While one-stage detectors like YOLOv8 offer fast training speed, they often under-perform on detecting small objects as a trade-off. This becomes even more critical when detecting tiny objects in aerial imagery due to low-resolution targets and cluttered backgrounds. To address this, we introduce three enhancement strategies â€“ input image resolution adjustment, data augmentation, and attention mechanisms â€“ that can be easily implemented on YOLOv8. We demonstrate that image size enlargement and the proper use of augmentation can lead to enhancement. Additionally, we designed a Mixture of Orthogonal Neural-modules Network (MoonNet) pipeline which consists of attention-augmented CNNs. Two well-known attention modules, the Squeeze-and-Excitation Block (SE Block) and the Convolutional Block Attention Module (CBAM), were integrated into the backbone of YOLOv8 with an increased number of channels, and the MoonNet backbone obtained improved detection accuracy compared to the original YOLOv8. MoonNet further proved its adaptability and potential by achieving state-of-the-art performance on a tiny-object benchmark when integrated with the YOLC model. Our codes are available at: <a target="_blank" rel="noopener" href="https://github.com/Kihyun11/MoonNet">https://github.com/Kihyun11/MoonNet</a> </p>
<blockquote>
<p>å°½ç®¡åƒYOLOv8è¿™æ ·çš„å•é˜¶æ®µæ£€æµ‹å™¨æä¾›äº†å¿«é€Ÿçš„è®­ç»ƒé€Ÿåº¦ï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨æ£€æµ‹å°ç‰©ä½“æ—¶è¡¨ç°ä¸ä½³ã€‚è¿™åœ¨æ£€æµ‹ç©ºä¸­å›¾åƒä¸­çš„å¾®å°ç‰©ä½“æ—¶å˜å¾—å°¤ä¸ºå…³é”®ï¼Œå› ä¸ºç›®æ ‡åˆ†è¾¨ç‡ä½ä¸”èƒŒæ™¯æ‚ä¹±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§å¢å¼ºç­–ç•¥ï¼Œå³è¾“å…¥å›¾åƒåˆ†è¾¨ç‡è°ƒæ•´ã€æ•°æ®å¢å¼ºå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨YOLOv8ä¸Šå®ç°ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå›¾åƒå°ºå¯¸æ”¾å¤§å’Œé€‚å½“çš„å¢å¼ºå¯ä»¥å¯¼è‡´æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ··åˆæ­£äº¤ç¥ç»ç½‘ç»œï¼ˆMoonNetï¼‰ç®¡é“ï¼Œå®ƒç”±å¢å¼ºæ³¨æ„åŠ›çš„å·ç§¯ç¥ç»ç½‘ç»œç»„æˆã€‚æˆ‘ä»¬æ•´åˆäº†ä¸¤ä¸ªçŸ¥åçš„æ³¨æ„åŠ›æ¨¡å—ï¼Œå³æŒ¤å‹-æ¿€å‘å—ï¼ˆSEå—ï¼‰å’Œå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰ï¼Œå¢åŠ äº†YOLOv8çš„é€šé“æ•°ï¼Œå¹¶ä¸”ä¸åŸå§‹YOLOv8ç›¸æ¯”ï¼ŒMoonNetéª¨å¹²ç½‘åœ¨æ£€æµ‹ç²¾åº¦ä¸Šæœ‰æ‰€æé«˜ã€‚å½“ä¸YOLCæ¨¡å‹é›†æˆæ—¶ï¼ŒMoonNetåœ¨å¾®å°ç‰©ä½“åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶é€‚åº”æ€§å’Œæ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Kihyun11/MoonNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Kihyun11/MoonNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17078v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹YOLOv8åœ¨æ£€æµ‹å°ç›®æ ‡æ—¶çš„ä¸è¶³ï¼Œæå‡ºä¸‰ç§å¢å¼ºç­–ç•¥ï¼šè°ƒæ•´è¾“å…¥å›¾åƒåˆ†è¾¨ç‡ã€æ•°æ®å¢å¼ºå’Œæ³¨æ„åŠ›æœºåˆ¶ã€‚è®¾è®¡äº†ä¸€ç§åä¸ºMoonNetçš„ç¥ç»ç½‘ç»œç®¡é“ï¼Œç”±å¢å¼ºæ³¨æ„åŠ›CNNç»„æˆã€‚å°†ä¸¤ç§çŸ¥åæ³¨æ„åŠ›æ¨¡å—â€”â€”å‹ç¼©æ¿€åŠ±å—ï¼ˆSE Blockï¼‰å’Œå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰â€”â€”èå…¥YOLOv8çš„éª¨å¹²ç½‘ç»œï¼Œæé«˜æ£€æµ‹ç²¾åº¦ã€‚MoonNetä¸YOLCæ¨¡å‹ç»“åˆåï¼Œåœ¨å°ç›®æ ‡åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>YOLOv8è™½ç„¶è®­ç»ƒé€Ÿåº¦å¿«ï¼Œä½†åœ¨æ£€æµ‹å°ç›®æ ‡æ—¶æ€§èƒ½è¾ƒå·®ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸‰ç§å¢å¼ºç­–ç•¥ï¼šè°ƒæ•´è¾“å…¥å›¾åƒåˆ†è¾¨ç‡ã€æ•°æ®å¢å¼ºå’Œå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>é€šè¿‡å¢å¤§å›¾åƒå°ºå¯¸å’Œé€‚å½“çš„æ•°æ®å¢å¼ºï¼Œå¯ä»¥å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>è®¾è®¡äº†åä¸ºMoonNetçš„ç¥ç»ç½‘ç»œç®¡é“ï¼Œç”±å¢å¼ºæ³¨æ„åŠ›çš„CNNç»„æˆã€‚</li>
<li>å°†SE Blockå’ŒCBAMä¸¤ç§æ³¨æ„åŠ›æ¨¡å—èå…¥YOLOv8çš„éª¨å¹²ç½‘ç»œã€‚</li>
<li>MoonNetéª¨å¹²ç½‘ç»œç›¸è¾ƒäºåŸå§‹YOLOv8ï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17078">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-da654ed2ed7c8e3044415eb978b8d1ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424880&auth_key=1760424880-0-0-d8aa5bc638092264dcad9d1d5eb90ebb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a4c324ba914067a0b9b99810f92e982~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424888&auth_key=1760424888-0-0-c4213600dd4f3162495454b8da47f35f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa52f1dc628378d6f9ac7e9de5963f65~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424896&auth_key=1760424896-0-0-3afe03acaa9d778878544bcfeb5cfec3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c340ea08f5720ca50d02e6da2e26d1af~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424902&auth_key=1760424902-0-0-57d380913fe68c99c2212d52224834e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prototype-Based-Pseudo-Label-Denoising-for-Source-Free-Domain-Adaptation-in-Remote-Sensing-Semantic-Segmentation"><a href="#Prototype-Based-Pseudo-Label-Denoising-for-Source-Free-Domain-Adaptation-in-Remote-Sensing-Semantic-Segmentation" class="headerlink" title="Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation   in Remote Sensing Semantic Segmentation"></a>Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation   in Remote Sensing Semantic Segmentation</h2><p><strong>Authors:Bin Wang, Fei Deng, Zeyu Chen, Zhicheng Yu, Yiguang Liu</strong></p>
<p>Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic segmentation of Remote Sensing Images (RSIs) using only a well-trained source model and unlabeled target domain data. However, the lack of ground-truth labels in the target domain often leads to the generation of noisy pseudo-labels. Such noise impedes the effective mitigation of domain shift (DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA framework. It employs prototype-weighted pseudo-labels to facilitate reliable self-training (ST) under pseudo-labels noise. We, in addition, introduce a prototype-contrast strategy that encourages the aggregation of features belonging to the same class, enabling the model to learn discriminative target domain representations without relying on ground-truth supervision. Extensive experiments show that our approach substantially outperforms existing methods. </p>
<blockquote>
<p>æ— æºåŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰æŠ€æœ¯ä»…åˆ©ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æºæ¨¡å‹å’Œæœªæ ‡è®°çš„ç›®æ ‡åŸŸæ•°æ®ï¼Œå®ç°äº†é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„åŸŸè‡ªé€‚åº”ã€‚ç„¶è€Œï¼Œç›®æ ‡åŸŸä¸­ç¼ºä¹çœŸå®æ ‡ç­¾é€šå¸¸ä¼šå¯¼è‡´ç”Ÿæˆå¸¦æœ‰å™ªå£°çš„ä¼ªæ ‡ç­¾ã€‚è¿™ç§å™ªå£°é˜»ç¢äº†åŸŸåç§»ï¼ˆDSï¼‰çš„æœ‰æ•ˆç¼“è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ProSFDAï¼Œä¸€ç§åŸå‹å¼•å¯¼çš„æ— æºåŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰æ¡†æ¶ã€‚å®ƒé‡‡ç”¨åŸå‹åŠ æƒä¼ªæ ‡ç­¾ï¼Œåœ¨ä¼ªæ ‡ç­¾å™ªå£°ä¸‹å®ç°å¯é çš„è‡ªè®­ç»ƒï¼ˆSTï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸå‹å¯¹æ¯”ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é¼“åŠ±å±äºåŒä¸€ç±»çš„ç‰¹å¾è¿›è¡Œèšåˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸ä¾èµ–çœŸå®æ ‡ç­¾ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œå­¦ä¹ å…·æœ‰åŒºåˆ†åº¦çš„ç›®æ ‡åŸŸè¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16942v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æºè‡ªç”±åŸŸé€‚é…æŠ€æœ¯ï¼ˆSFDAï¼‰åœ¨é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ProSFDAæ¡†æ¶ï¼Œé‡‡ç”¨åŸå‹å¼•å¯¼çš„ä¼ªæ ‡ç­¾åŠ æƒæ–¹æ³•ï¼Œå®ç°å¯é çš„è‡ªè®­ç»ƒï¼ˆSTï¼‰ï¼ŒåŒæ—¶å¼•å…¥åŸå‹å¯¹æ¯”ç­–ç•¥ï¼Œé¼“åŠ±åŒç±»ç‰¹å¾çš„èšåˆï¼Œå­¦ä¹ å…·æœ‰é‰´åˆ«åŠ›çš„ç›®æ ‡åŸŸè¡¨ç¤ºã€‚è¯¥æ¡†æ¶åœ¨ä¸ä¾èµ–çœŸå®æ ‡ç­¾ç›‘ç£çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æºè‡ªç”±åŸŸé€‚é…æŠ€æœ¯ï¼ˆSFDAï¼‰èƒ½å¤Ÿå®ç°é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„åŸŸé€‚åº”ã€‚</li>
<li>æ— æ ‡ç­¾ç›®æ ‡åŸŸæ•°æ®äº§ç”Ÿçš„ä¼ªæ ‡ç­¾å™ªå£°æ˜¯åŸŸé€‚åº”ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ProSFDAæ¡†æ¶é‡‡ç”¨åŸå‹å¼•å¯¼çš„ä¼ªæ ‡ç­¾åŠ æƒæ–¹æ³•ï¼Œå®ç°å¯é è‡ªè®­ç»ƒï¼ˆSTï¼‰ã€‚</li>
<li>åŸå‹å¯¹æ¯”ç­–ç•¥æœ‰åŠ©äºåŒç±»ç‰¹å¾çš„èšåˆï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®æ ‡ç­¾ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ å…·æœ‰é‰´åˆ«åŠ›çš„ç›®æ ‡åŸŸè¡¨ç¤ºã€‚</li>
<li>ProSFDAæ¡†æ¶åœ¨å¤§é‡å®éªŒä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-adfc36263950de294f096cc3593abbcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424910&auth_key=1760424910-0-0-50208dbe48f3feb06cba61aae0923730&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2de603a97f45333ee5feed12863bac5f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424917&auth_key=1760424917-0-0-301bf6f931775168b8cc120c595040fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b555b2d501afc28bfde25a71b2592c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424924&auth_key=1760424924-0-0-78a2a21364090999b2e84376600d67c1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SegDINO3D-3D-Instance-Segmentation-Empowered-by-Both-Image-Level-and-Object-Level-2D-Features"><a href="#SegDINO3D-3D-Instance-Segmentation-Empowered-by-Both-Image-Level-and-Object-Level-2D-Features" class="headerlink" title="SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and   Object-Level 2D Features"></a>SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and   Object-Level 2D Features</h2><p><strong>Authors:Jinyuan Qu, Hongyang Li, Xingyu Chen, Shilong Liu, Yukai Shi, Tianhe Ren, Ruitao Jing, Lei Zhang</strong></p>
<p>In this paper, we present SegDINO3D, a novel Transformer encoder-decoder framework for 3D instance segmentation. As 3D training data is generally not as sufficient as 2D training images, SegDINO3D is designed to fully leverage 2D representation from a pre-trained 2D detection model, including both image-level and object-level features, for improving 3D representation. SegDINO3D takes both a point cloud and its associated 2D images as input. In the encoder stage, it first enriches each 3D point by retrieving 2D image features from its corresponding image views and then leverages a 3D encoder for 3D context fusion. In the decoder stage, it formulates 3D object queries as 3D anchor boxes and performs cross-attention from 3D queries to 2D object queries obtained from 2D images using the 2D detection model. These 2D object queries serve as a compact object-level representation of 2D images, effectively avoiding the challenge of keeping thousands of image feature maps in the memory while faithfully preserving the knowledge of the pre-trained 2D model. The introducing of 3D box queries also enables the model to modulate cross-attention using the predicted boxes for more precise querying. SegDINO3D achieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D instance segmentation benchmarks. Notably, on the challenging ScanNet200 dataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAP on the validation and hidden test sets, respectively, demonstrating its superiority. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„Transformerç¼–ç å™¨-è§£ç å™¨æ¡†æ¶SegDINO3Dï¼Œç”¨äº3Då®ä¾‹åˆ†å‰²ã€‚ç”±äº3Dè®­ç»ƒæ•°æ®é€šå¸¸ä¸åƒ2Dè®­ç»ƒå›¾åƒé‚£æ ·å……è¶³ï¼ŒSegDINO3Dçš„è®¾è®¡ç›®çš„æ˜¯å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„2Dæ£€æµ‹æ¨¡å‹çš„2Dè¡¨ç¤ºï¼ŒåŒ…æ‹¬å›¾åƒçº§åˆ«å’Œå¯¹è±¡çº§åˆ«çš„ç‰¹å¾ï¼Œä»¥æ”¹è¿›3Dè¡¨ç¤ºã€‚SegDINO3Dä»¥ç‚¹äº‘åŠå…¶å…³è”çš„2Då›¾åƒä½œä¸ºè¾“å…¥ã€‚åœ¨ç¼–ç é˜¶æ®µï¼Œå®ƒé¦–å…ˆé€šè¿‡ä»ç›¸åº”çš„å›¾åƒè§†å›¾ä¸­æ£€ç´¢2Då›¾åƒç‰¹å¾æ¥ä¸°å¯Œæ¯ä¸ª3Dç‚¹ï¼Œç„¶ååˆ©ç”¨3Dç¼–ç å™¨è¿›è¡Œ3Dä¸Šä¸‹æ–‡èåˆã€‚åœ¨è§£ç é˜¶æ®µï¼Œå®ƒå°†3Då¯¹è±¡æŸ¥è¯¢åˆ¶å®šä¸º3Dé”šæ¡†ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„2Dæ£€æµ‹æ¨¡å‹ä»3DæŸ¥è¯¢åˆ°2Då¯¹è±¡æŸ¥è¯¢è¿›è¡Œäº¤å‰æ³¨æ„åŠ›å¤„ç†ã€‚è¿™äº›2Då¯¹è±¡æŸ¥è¯¢ä½œä¸º2Då›¾åƒçš„ç´§å‡‘å¯¹è±¡çº§åˆ«è¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°é¿å…äº†åœ¨å†…å­˜ä¸­ä¿ç•™æ•°åƒä¸ªå›¾åƒç‰¹å¾å›¾çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶å¿ å®ä¿ç•™äº†é¢„è®­ç»ƒçš„2Dæ¨¡å‹çš„çŸ¥è¯†ã€‚å¼•å…¥çš„3Dæ¡†æŸ¥è¯¢è¿˜ä½¿æ¨¡å‹èƒ½å¤Ÿä½¿ç”¨é¢„æµ‹çš„æ¡†å¯¹äº¤å‰æ³¨æ„åŠ›è¿›è¡Œè°ƒåˆ¶ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„æŸ¥è¯¢ã€‚SegDINO3Dåœ¨ScanNetV2å’ŒScanNet200 3Då®ä¾‹åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ScanNet200æ•°æ®é›†ä¸Šï¼ŒSegDINO3Dåœ¨éªŒè¯é›†å’Œéšè—æµ‹è¯•é›†ä¸Šçš„mAPåˆ†åˆ«æ¯”ç°æœ‰æ–¹æ³•é«˜å‡º+8.7å’Œ+6.8ï¼Œæ˜¾ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16098v1">PDF</a> </p>
<p><strong>Summary</strong><br>SegDINO3Dæ˜¯ä¸€ä¸ªç”¨äºä¸‰ç»´å®ä¾‹åˆ†å‰²çš„æ–°å‹Transformerç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ã€‚å®ƒå……åˆ†åˆ©ç”¨äºŒç»´æ£€æµ‹æ¨¡å‹çš„å›¾åƒçº§å’Œå¯¹è±¡çº§ç‰¹å¾æ¥æé«˜ä¸‰ç»´è¡¨ç°ã€‚æ¡†æ¶æ¥å—ç‚¹äº‘å’Œç›¸å…³çš„äºŒç»´å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå°†æ¯ä¸ªä¸‰ç»´ç‚¹ä¸°å¯Œä¸æ¥è‡ªç›¸åº”å›¾åƒè§†å›¾çš„äºŒç»´å›¾åƒç‰¹å¾ï¼Œå¹¶é€šè¿‡ä¸‰ç»´ç¼–ç å™¨è¿›è¡Œä¸‰ç»´ä¸Šä¸‹æ–‡èåˆã€‚è§£ç é˜¶æ®µåˆ¶å®šäº†ä¸‰ç»´å¯¹è±¡æŸ¥è¯¢ä½œä¸ºä¸‰ç»´é”šç®±ï¼Œå¹¶ä»ä¸‰ç»´æŸ¥è¯¢åˆ°äºŒç»´å¯¹è±¡æŸ¥è¯¢æ‰§è¡Œäº¤å‰æ³¨æ„åŠ›ï¼Œä½¿ç”¨äºŒç»´æ£€æµ‹æ¨¡å‹ä»äºŒç»´å›¾åƒä¸­è·å–è¿™äº›æŸ¥è¯¢ã€‚SegDINO3Dåœ¨ScanNetV2å’ŒScanNet200ä¸‰ç»´å®ä¾‹åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ScanNet200æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SegDINO3Dæ˜¯ä¸€ä¸ªåŸºäºTransformerçš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œç”¨äºä¸‰ç»´å®ä¾‹åˆ†å‰²ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨å……åˆ†åˆ©ç”¨äºŒç»´æ£€æµ‹æ¨¡å‹çš„å›¾åƒçº§å’Œå¯¹è±¡çº§ç‰¹å¾æ¥æé«˜ä¸‰ç»´è¡¨ç°ã€‚</li>
<li>SegDINO3Dæ¥å—ç‚¹äº‘å’Œç›¸å…³çš„äºŒç»´å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶é€šè¿‡ä¸‰ç»´ç¼–ç å™¨è¿›è¡Œä¸Šä¸‹æ–‡èåˆã€‚</li>
<li>è§£ç é˜¶æ®µåˆ©ç”¨ä¸‰ç»´å¯¹è±¡æŸ¥è¯¢å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ç»“åˆäºŒç»´å¯¹è±¡æŸ¥è¯¢ï¼Œé¿å…ä¿æŒå¤§é‡å›¾åƒç‰¹å¾å›¾åœ¨å†…å­˜ä¸­çš„æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cc912215b631467dc6ae4a0cbd77b6f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424931&auth_key=1760424931-0-0-44ce5a11c22f6ef1eb38c64bf93317a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-194d3764109d4541acc1b2d4aa041190~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424939&auth_key=1760424939-0-0-63ac68e4ab41ea261b4cd7a18f79cafe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d4f23031e3268cc44a3d8ab30ac7761e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424946&auth_key=1760424946-0-0-45dafa3a32432d8081ef7a7d7172adc5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-Missing-Piece-A-Case-for-Pre-Training-in-3D-Medical-Object-Detection"><a href="#The-Missing-Piece-A-Case-for-Pre-Training-in-3D-Medical-Object-Detection" class="headerlink" title="The Missing Piece: A Case for Pre-Training in 3D Medical Object   Detection"></a>The Missing Piece: A Case for Pre-Training in 3D Medical Object   Detection</h2><p><strong>Authors:Katharina Eckstein, Constantin Ulrich, Michael Baumgartner, Jessica KÃ¤chele, Dimitrios Bounias, Tassilo Wald, Ralf Floca, Klaus H. Maier-Hein</strong></p>
<p>Large-scale pre-training holds the promise to advance 3D medical object detection, a crucial component of accurate computer-aided diagnosis. Yet, it remains underexplored compared to segmentation, where pre-training has already demonstrated significant benefits. Existing pre-training approaches for 3D object detection rely on 2D medical data or natural image pre-training, failing to fully leverage 3D volumetric information. In this work, we present the first systematic study of how existing pre-training methods can be integrated into state-of-the-art detection architectures, covering both CNNs and Transformers. Our results show that pre-training consistently improves detection performance across various tasks and datasets. Notably, reconstruction-based self-supervised pre-training outperforms supervised pre-training, while contrastive pre-training provides no clear benefit for 3D medical object detection. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/nnDetection-finetuning">https://github.com/MIC-DKFZ/nnDetection-finetuning</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæœ‰æœ›æ¨åŠ¨ä¸‰ç»´åŒ»å­¦ç›®æ ‡æ£€æµ‹çš„å‘å±•ï¼Œè¿™æ˜¯å®ç°è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å‡†ç¡®æ€§çš„å…³é”®éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œä¸åˆ†å‰²ç›¸æ¯”ï¼Œé¢„è®­ç»ƒçš„ç ”ç©¶ä»ç„¶è¿œè¿œä¸å¤Ÿæ·±å…¥ï¼Œåœ¨åˆ†å‰²é¢†åŸŸï¼Œé¢„è®­ç»ƒå·²ç»æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç›Šå¤„ã€‚ç°æœ‰çš„ä¸‰ç»´ç›®æ ‡æ£€æµ‹é¢„è®­ç»ƒæ–¹æ³•ä¾èµ–äºäºŒç»´åŒ»å­¦æ•°æ®æˆ–è‡ªç„¶å›¾åƒé¢„è®­ç»ƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ä¸‰ç»´ä½“ç§¯ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡ç³»ç»Ÿåœ°ç ”ç©¶äº†å¦‚ä½•å°†ç°æœ‰é¢„è®­ç»ƒæ–¹æ³•é›†æˆåˆ°æœ€å…ˆè¿›çš„æ£€æµ‹æ¶æ„ä¸­ï¼Œæ¶µç›–å·ç§¯ç¥ç»ç½‘ç»œå’ŒTransformerã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒåœ¨å„ç§ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šå§‹ç»ˆæé«˜äº†æ£€æµ‹æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºé‡å»ºçš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒä¼˜äºç›‘ç£é¢„è®­ç»ƒï¼Œè€Œå¯¹æ¯”é¢„è®­ç»ƒå¯¹äºä¸‰ç»´åŒ»å­¦ç›®æ ‡æ£€æµ‹æ²¡æœ‰æ˜ç¡®çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/nnDetection-finetuning%E3%80%82]">https://github.com/MIC-DKFZ/nnDetection-finetuningã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15947v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæœ‰æœ›æ¨åŠ¨3DåŒ»ç–—ç›®æ ‡æ£€æµ‹çš„å‘å±•ï¼Œè¿™æ˜¯è®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œä¸åˆ†å‰²ç›¸æ¯”ï¼Œé¢„è®­ç»ƒåœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢çš„åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ï¼Œå°½ç®¡åœ¨åˆ†å‰²ä»»åŠ¡ä¸­é¢„è®­ç»ƒå·²ç»æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚ç°æœ‰çš„é¢„è®­ç»ƒç­–ç•¥ä¸»è¦ä¾èµ–äºäºŒç»´åŒ»å­¦æ•°æ®æˆ–è‡ªç„¶å›¾åƒé¢„è®­ç»ƒï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ä¸‰ç»´ä½“ç§¯ä¿¡æ¯ã€‚æœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿåœ°æ¢è®¨äº†å¦‚ä½•å°†ç°æœ‰é¢„è®­ç»ƒæŠ€æœ¯èå…¥å…ˆè¿›çš„æ£€æµ‹æ¶æ„ä¸­ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œå’ŒTransformerã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒåœ¨å„ç§ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šéƒ½èƒ½æé«˜æ£€æµ‹æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºé‡å»ºçš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•è¡¨ç°ä¼˜äºç›‘ç£é¢„è®­ç»ƒï¼Œè€Œå¯¹æ¯”é¢„è®­ç»ƒå¯¹3DåŒ»ç–—ç›®æ ‡æ£€æµ‹æ²¡æœ‰æ˜ç¡®çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/nnDetection-finetuning%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/MIC-DKFZ/nnDetection-finetuningä¸Šå¯ç”¨ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒå¯¹æå‡3DåŒ»ç–—ç›®æ ‡æ£€æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>ä¸åˆ†å‰²ä»»åŠ¡ç›¸æ¯”ï¼Œé¢„è®­ç»ƒåœ¨ç›®æ ‡æ£€æµ‹ä¸­çš„åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚</li>
<li>å½“å‰é¢„è®­ç»ƒç­–ç•¥ä¸»è¦ä¾èµ–äºŒç»´åŒ»å­¦æ•°æ®æˆ–è‡ªç„¶å›¾åƒï¼Œæœªå……åˆ†åˆ©ç”¨ä¸‰ç»´ä½“ç§¯ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†é¢„è®­ç»ƒæŠ€æœ¯ä¸å…ˆè¿›æ£€æµ‹æ¶æ„çš„ç»“åˆã€‚</li>
<li>é¢„è®­ç»ƒèƒ½æé«˜å„ç§ä»»åŠ¡å’Œæ•°æ®é›†çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åŸºäºé‡å»ºçš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•è¡¨ç°ä¼˜äºç›‘ç£é¢„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-434bf67d3db70b992cca8e61beb8e6ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424953&auth_key=1760424953-0-0-00efc18873c83f2cc700bbb79ea81893&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b91f23cc0c84b94216e5dcaaf9175f78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424961&auth_key=1760424961-0-0-7aa4641f242938ebb61761b12f484b6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aac281accc9f6ec1b33ac8e6de43a00e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424967&auth_key=1760424967-0-0-45e1cbe267b58361a96bfab0f95d4534&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MCOD-The-First-Challenging-Benchmark-for-Multispectral-Camouflaged-Object-Detection"><a href="#MCOD-The-First-Challenging-Benchmark-for-Multispectral-Camouflaged-Object-Detection" class="headerlink" title="MCOD: The First Challenging Benchmark for Multispectral Camouflaged   Object Detection"></a>MCOD: The First Challenging Benchmark for Multispectral Camouflaged   Object Detection</h2><p><strong>Authors:Yang Li, Tingfa Xu, Shuyan Bai, Peifu Liu, Jianan Li</strong></p>
<p>Camouflaged Object Detection (COD) aims to identify objects that blend seamlessly into natural scenes. Although RGB-based methods have advanced, their performance remains limited under challenging conditions. Multispectral imagery, providing rich spectral information, offers a promising alternative for enhanced foreground-background discrimination. However, existing COD benchmark datasets are exclusively RGB-based, lacking essential support for multispectral approaches, which has impeded progress in this area. To address this gap, we introduce MCOD, the first challenging benchmark dataset specifically designed for multispectral camouflaged object detection. MCOD features three key advantages: (i) Comprehensive challenge attributes: It captures real-world difficulties such as small object sizes and extreme lighting conditions commonly encountered in COD tasks. (ii) Diverse real-world scenarios: The dataset spans a wide range of natural environments to better reflect practical applications. (iii) High-quality pixel-level annotations: Each image is manually annotated with precise object masks and corresponding challenge attribute labels. We benchmark eleven representative COD methods on MCOD, observing a consistent performance drop due to increased task difficulty. Notably, integrating multispectral modalities substantially alleviates this degradation, highlighting the value of spectral information in enhancing detection robustness. We anticipate MCOD will provide a strong foundation for future research in multispectral camouflaged object detection. The dataset is publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/yl2900260-bit/MCOD">https://github.com/yl2900260-bit/MCOD</a>. </p>
<blockquote>
<p>éšè”½ç‰©ä½“æ£€æµ‹ï¼ˆCODï¼‰æ—¨åœ¨è¯†åˆ«æ— ç¼èå…¥è‡ªç„¶åœºæ™¯ä¸­çš„ç‰©ä½“ã€‚å°½ç®¡åŸºäºRGBçš„æ–¹æ³•å·²ç»å–å¾—è¿›å±•ï¼Œä½†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ï¼Œå…¶æ€§èƒ½ä»ç„¶å—åˆ°é™åˆ¶ã€‚å¤šå…‰è°±æˆåƒæä¾›äº†ä¸°å¯Œçš„å…‰è°±ä¿¡æ¯ï¼Œä¸ºå¢å¼ºå‰æ™¯èƒŒæ™¯é‰´åˆ«æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„CODåŸºå‡†æ•°æ®é›†éƒ½æ˜¯åŸºäºRGBçš„ï¼Œç¼ºä¹å¤šå…‰è°±æ–¹æ³•æ‰€å¿…éœ€çš„æ”¯æŒï¼Œè¿™é˜»ç¢äº†è¯¥é¢†åŸŸçš„è¿›æ­¥ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†MCODï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºå¤šå…‰è°±éšè”½ç‰©ä½“æ£€æµ‹è®¾è®¡çš„é¦–ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æ•°æ®é›†ã€‚MCODå…·æœ‰ä¸‰ä¸ªä¸»è¦ä¼˜ç‚¹ï¼šï¼ˆiï¼‰å…¨é¢çš„æŒ‘æˆ˜å±æ€§ï¼šå®ƒæ•æ‰äº†ç°å®ä¸–ç•Œä¸­çš„éš¾ç‚¹ï¼Œä¾‹å¦‚å¸¸è§çš„éšè”½ç‰©ä½“æ£€æµ‹ä»»åŠ¡ä¸­å°ç‰©ä½“å°ºå¯¸å’Œæç«¯ç…§æ˜æ¡ä»¶ã€‚ï¼ˆiiï¼‰ä¸°å¯Œçš„ç°å®ä¸–ç•Œåœºæ™¯ï¼šæ•°æ®é›†æ¶µç›–äº†å¹¿æ³›è‡ªç„¶ç¯å¢ƒåœºæ™¯ä»¥æ›´å¥½åœ°åæ˜ å®é™…åº”ç”¨ã€‚ï¼ˆiiiï¼‰é«˜è´¨é‡çš„åƒç´ çº§æ³¨é‡Šï¼šæ¯å¼ å›¾åƒéƒ½ä¼šé€šè¿‡ç²¾ç¡®çš„ç‰©ä½“æ©è†œå’Œç›¸åº”çš„æŒ‘æˆ˜å±æ€§æ ‡ç­¾è¿›è¡Œæ‰‹åŠ¨æ³¨é‡Šã€‚æˆ‘ä»¬åœ¨MCODä¸ŠåŸºå‡†æµ‹è¯•äº†åä¸€ç§ä»£è¡¨æ€§çš„CODæ–¹æ³•ï¼Œè§‚å¯Ÿåˆ°ç”±äºä»»åŠ¡éš¾åº¦å¢åŠ è€Œå¯¼è‡´çš„æ€§èƒ½æ™®éä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé›†æˆå¤šå…‰è°±æ¨¡å¼å¤§å¤§å‡è½»äº†è¿™ç§é€€åŒ–ï¼Œçªæ˜¾äº†å…‰è°±ä¿¡æ¯åœ¨å¢å¼ºæ£€æµ‹ç¨³å¥æ€§æ–¹é¢çš„ä»·å€¼ã€‚æˆ‘ä»¬é¢„è®¡MCODå°†ä¸ºæœªæ¥å¤šå…‰è°±éšè”½ç‰©ä½“æ£€æµ‹çš„ç ”ç©¶æä¾›åšå®çš„åŸºç¡€ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yl2900260-bit/MCOD%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/yl2900260-bit/MCODå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15753v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹ä¼ªè£…ç‰©ä½“æ£€æµ‹çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œå¼•å…¥äº†é¦–ä¸ªä¸“ä¸ºå¤šå…‰è°±ä¼ªè£…ç‰©ä½“æ£€æµ‹è®¾è®¡çš„åŸºå‡†æ•°æ®é›†MCODã€‚MCODå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šå…¨é¢çš„æŒ‘æˆ˜å±æ€§ã€å¤šæ ·åŒ–çš„ç°å®ä¸–ç•Œåœºæ™¯ä»¥åŠé«˜è´¨é‡åƒç´ çº§åˆ«çš„æ ‡æ³¨ã€‚ç ”ç©¶ä¸­è§‚å¯Ÿåˆ°ç°æœ‰CODæ–¹æ³•åœ¨MCODä¸Šçš„æ€§èƒ½ä¸‹é™ï¼Œè€Œå¤šå…‰è°±æ¨¡æ€çš„æ•´åˆæœ‰åŠ©äºæé«˜æ£€æµ‹çš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>CODçš„ç›®æ ‡ï¼šè¯†åˆ«èå…¥è‡ªç„¶åœºæ™¯ä¸­çš„ç‰©ä½“ã€‚</li>
<li>RGBæ–¹æ³•åœ¨é¢ä¸´æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹æ€§èƒ½å—é™ï¼Œéœ€å¯»æ‰¾å…¶ä»–é€”å¾„æ”¹è¿›å‰æ™¯èƒŒæ™¯è¯†åˆ«æŠ€æœ¯ã€‚</li>
<li>å¤šå…‰è°±æˆåƒå¯ä»¥æä¾›ä¸°å¯Œçš„å…‰è°±ä¿¡æ¯ï¼Œæœ‰åŠ©äºå¢å¼ºå‰æ™¯èƒŒæ™¯é‰´åˆ«èƒ½åŠ›ã€‚</li>
<li>å½“å‰CODåŸºå‡†æ•°æ®é›†ç¼ºä¹é’ˆå¯¹å¤šå…‰è°±æ–¹æ³•çš„æ”¯æŒï¼Œé™åˆ¶äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚</li>
<li>MCODæ•°æ®é›†çš„ç‰¹ç‚¹åŒ…æ‹¬å…¨é¢çš„æŒ‘æˆ˜å±æ€§ã€å¤šæ ·åŒ–çš„ç°å®ä¸–ç•Œåœºæ™¯å’Œé«˜è´¨é‡åƒç´ çº§åˆ«çš„æ ‡æ³¨ã€‚</li>
<li>MCODå¯¹ä»£è¡¨CODæ–¹æ³•è¿›è¡Œè¯„ä¼°æ—¶ï¼Œè§‚å¯Ÿåˆ°æ€§èƒ½ä¸‹é™ï¼Œå¤šå…‰è°±æ¨¡æ€æ•´åˆå¯ç¼“è§£æ­¤é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-eee4a71e83d6fbb334fcaabcc407c2fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424975&auth_key=1760424975-0-0-94588438f92d0fe9b601442b0dc8ca75&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-18f80e264b4ec28c5efc3535253d48b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424985&auth_key=1760424985-0-0-a2856ae874dd4cb179e51df1e4162c74&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-60f682a91b335bc279c831717fcee137~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424992&auth_key=1760424992-0-0-2373352793f518a28a404d206a3d7e19&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-26672ebec1959430f4b543eb197a1d16~resize:0:q75.jpg?source=1f5c5e47&expiration=1760424999&auth_key=1760424999-0-0-c5d8e9dba608e30201f428fcdb78955c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-39a272d1a6996c7909eceb952b972cec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425006&auth_key=1760425006-0-0-6e3e46e7eaab5de00d829dff3b16aa2c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cad2f1c416fb4b13cbca4e631b68d0e1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425012&auth_key=1760425012-0-0-7220b112574d209a777993bb693b3d3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f231edd9f5122ce04b5c7c607c2da622~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425019&auth_key=1760425019-0-0-c50e9b22e4602c7187c687a2ab472296&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cb2d6e8ca6420322b5ce3006ccd52c4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425026&auth_key=1760425026-0-0-bb3c7897cc671e7eac8648f998885235&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Size-invariant-Salient-Object-Detection-A-Generic-Evaluation-and-Optimization-Approach"><a href="#Towards-Size-invariant-Salient-Object-Detection-A-Generic-Evaluation-and-Optimization-Approach" class="headerlink" title="Towards Size-invariant Salient Object Detection: A Generic Evaluation   and Optimization Approach"></a>Towards Size-invariant Salient Object Detection: A Generic Evaluation   and Optimization Approach</h2><p><strong>Authors:Shilong Bao, Qianqian Xu, Feiran Li, Boyu Han, Zhiyong Yang, Xiaochun Cao, Qingming Huang</strong></p>
<p>This paper investigates a fundamental yet underexplored issue in Salient Object Detection (SOD): the size-invariant property for evaluation protocols, particularly in scenarios when multiple salient objects of significantly different sizes appear within a single image. We first present a novel perspective to expose the inherent size sensitivity of existing widely used SOD metrics. Through careful theoretical derivations, we show that the evaluation outcome of an image under current SOD metrics can be essentially decomposed into a sum of several separable terms, with the contribution of each term being directly proportional to its corresponding region size. Consequently, the prediction errors would be dominated by the larger regions, while smaller yet potentially more semantically important objects are often overlooked, leading to biased performance assessments and practical degradation. To address this challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed. The core idea is to evaluate each separable component individually and then aggregate the results, thereby effectively mitigating the impact of size imbalance across objects. Building upon this, we further develop a dedicated optimization framework (SIOpt), which adheres to the size-invariant principle and significantly enhances the detection of salient objects across a broad range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly integrated with a wide range of SOD backbones. Theoretically, we also present generalization analysis of SOD methods and provide evidence supporting the validity of our new evaluation protocols. Finally, comprehensive experiments speak to the efficacy of our proposed approach. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/SI-SOD">https://github.com/Ferry-Li/SI-SOD</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰ä¸­ä¸€ä¸ªåŸºç¡€ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„é—®é¢˜ï¼šè¯„ä¼°åè®®ä¸­çš„å°ºå¯¸ä¸å˜ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å•ä¸ªå›¾åƒä¸­å‡ºç°å¤šä¸ªå¤§å°æ˜¾è‘—ä¸åŒçš„ç›®æ ‡æ—¶çš„åœºæ™¯ã€‚æˆ‘ä»¬é¦–å…ˆä»ä¸€ä¸ªæ–°é¢–çš„è§’åº¦æ­ç¤ºäº†ç°æœ‰å¹¿æ³›ä½¿ç”¨çš„SODæŒ‡æ ‡å›ºæœ‰çš„å°ºå¯¸æ•æ„Ÿæ€§ã€‚é€šè¿‡ä»”ç»†çš„ç†è®ºæ¨å¯¼ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨å½“å‰SODæŒ‡æ ‡ä¸‹å¯¹å›¾åƒçš„è¯„ä¼°ç»“æœå¯ä»¥åŸºæœ¬åˆ†è§£ä¸ºå‡ ä¸ªå¯åˆ†é‡çš„å’Œï¼Œæ¯ä¸ªåˆ†é‡çš„è´¡çŒ®ä¸å…¶ç›¸åº”çš„åŒºåŸŸå¤§å°ç›´æ¥æˆæ­£æ¯”ã€‚å› æ­¤ï¼Œé¢„æµ‹è¯¯å·®ä¸»è¦ç”±è¾ƒå¤§çš„åŒºåŸŸä¸»å¯¼ï¼Œè€Œè¾ƒå°ä½†å¯èƒ½åœ¨è¯­ä¹‰ä¸Šæ›´é‡è¦çš„ç›®æ ‡å¾€å¾€è¢«å¿½è§†ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½è¯„ä¼°åå·®å’Œå®é™…æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨çš„å°ºå¯¸ä¸å˜è¯„ä¼°ï¼ˆSIEvaï¼‰æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å¯¹æ¯ä¸ªå¯åˆ†ç¦»ç»„ä»¶è¿›è¡Œå•ç‹¬è¯„ä¼°ï¼Œç„¶åæ±‡æ€»ç»“æœï¼Œä»è€Œæœ‰æ•ˆåœ°å‡è½»å¯¹è±¡ä¹‹é—´å°ºå¯¸ä¸å¹³è¡¡çš„å½±å“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªä¸“ç”¨çš„ä¼˜åŒ–æ¡†æ¶ï¼ˆSIOptï¼‰ï¼Œå®ƒéµå¾ªå°ºå¯¸ä¸å˜åŸåˆ™ï¼Œå¹¶æ˜¾è‘—æé«˜äº†å„ç§å°ºå¯¸ç›®æ ‡çš„æ£€æµ‹æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSIOptä¸æ¨¡å‹æ— å…³ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°å„ç§SODä¸»å¹²ç½‘ç»œä¸­ã€‚åœ¨ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¿˜å¯¹SODæ–¹æ³•è¿›è¡Œäº†æ³›åŒ–åˆ†æï¼Œå¹¶ä¸ºæˆ‘ä»¬çš„æ–°è¯„ä¼°åè®®çš„æœ‰æ•ˆæ€§æä¾›äº†è¯æ®æ”¯æŒã€‚æœ€åï¼Œç»¼åˆå®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/SI-SOD%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ferry-Li/SI-SODä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15573v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ˜¾è‘—ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰ä¸­ä¸€ä¸ªåŸºç¡€ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„é—®é¢˜ï¼šè¯„ä¼°åè®®ä¸­çš„å°ºå¯¸ä¸å˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å•ä¸ªå›¾åƒä¸­å‡ºç°å¤šä¸ªå¤§å°æ˜¾è‘—ä¸åŒçš„ç›®æ ‡æ—¶çš„åœºæ™¯ã€‚æ–‡ç« é¦–å…ˆæå‡ºä¸€ç§æ–°çš„è§†è§’æ¥æ­ç¤ºç°æœ‰å¹¿æ³›ä½¿ç”¨çš„SODæŒ‡æ ‡å›ºæœ‰çš„å°ºå¯¸æ•æ„Ÿæ€§ã€‚é€šè¿‡ä»”ç»†çš„ç†è®ºæ¨å¯¼ï¼Œæ–‡ç« è¡¨æ˜å½“å‰SODæŒ‡æ ‡ä¸‹çš„å›¾åƒè¯„ä¼°ç»“æœå¯ä»¥åˆ†è§£ä¸ºå‡ ä¸ªå¯åˆ†ç¦»é¡¹çš„å’Œï¼Œæ¯ä¸ªé¡¹çš„è´¡çŒ®ä¸å…¶ç›¸åº”åŒºåŸŸçš„å¤§å°ç›´æ¥æˆæ¯”ä¾‹ã€‚å› æ­¤ï¼Œé¢„æµ‹è¯¯å·®ä¸»è¦ç”±è¾ƒå¤§çš„åŒºåŸŸä¸»å¯¼ï¼Œè€Œè¾ƒå°ä½†å¯èƒ½æ›´å…·è¯­ä¹‰é‡è¦æ€§çš„å¯¹è±¡å¾€å¾€è¢«å¿½è§†ï¼Œå¯¼è‡´æ€§èƒ½è¯„ä¼°åå·®å’Œå®é™…æ€§èƒ½ä¸‹é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†é€šç”¨çš„å°ºå¯¸ä¸å˜è¯„ä¼°ï¼ˆSIEvaï¼‰æ¡†æ¶ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å¯¹æ¯ä¸ªå¯åˆ†ç¦»ç»„ä»¶è¿›è¡Œä¸ªåˆ«è¯„ä¼°ï¼Œç„¶åæ±‡æ€»ç»“æœï¼Œä»è€Œæœ‰æ•ˆç¼“è§£å¯¹è±¡ä¹‹é—´å°ºå¯¸ä¸å¹³è¡¡çš„å½±å“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ–‡ç« è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªä¸“ç”¨çš„ä¼˜åŒ–æ¡†æ¶ï¼ˆSIOptï¼‰ï¼Œå®ƒéµå¾ªå°ºå¯¸ä¸å˜åŸåˆ™ï¼Œå¹¶æ˜¾è‘—æé«˜äº†å„ç§å°ºå¯¸ä¸‹æ˜¾è‘—ç›®æ ‡çš„æ£€æµ‹æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSIOptä¸æ¨¡å‹æ— å…³ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°å¹¿æ³›çš„SODä¸»å¹²ç½‘ç»œä¸­ã€‚æœ€åï¼Œå¤§é‡çš„å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æŒ‡å‡ºäº†ç°æœ‰æ˜¾è‘—ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰è¯„ä¼°æŒ‡æ ‡åœ¨å°ºå¯¸æ•æ„Ÿæ€§æ–¹é¢å­˜åœ¨çš„é—®é¢˜ã€‚</li>
<li>æ–‡ç« é€šè¿‡ç†è®ºæ¨å¯¼æ­ç¤ºäº†å½“å‰SODæŒ‡æ ‡åœ¨è¯„ä¼°æ—¶çš„å°ºå¯¸åå·®ç°è±¡ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„å°ºå¯¸ä¸å˜è¯„ä¼°ï¼ˆSIEvaï¼‰æ¡†æ¶æ¥è§£å†³å°ºå¯¸ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>SIEvaæ¡†æ¶èƒ½æœ‰æ•ˆè¯„ä¼°æ¯ä¸ªå¯åˆ†ç¦»ç»„ä»¶å¹¶æ±‡æ€»ç»“æœï¼Œå‡å°‘å°ºå¯¸å¯¹è¯„ä¼°çš„å½±å“ã€‚</li>
<li>æ–‡ç« è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªä¸“ç”¨ä¼˜åŒ–æ¡†æ¶ï¼ˆSIOptï¼‰ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å„ç§å°ºå¯¸ä¸‹æ˜¾è‘—ç›®æ ‡çš„æ£€æµ‹æ•ˆæœã€‚</li>
<li>SIOptä¸å¤šç§SODæ¨¡å‹å…¼å®¹ï¼Œå¯å¹¿æ³›é›†æˆåˆ°å„ç§SODæ–¹æ³•ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9dcf65a7d12d3d142b951bed5c6ad4c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425033&auth_key=1760425033-0-0-440f6392df40afe8a29e1e03e169103b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5d5ef4ac1dc377f4b4b91a60d1bb588~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425041&auth_key=1760425041-0-0-52aa645a6bb0e4e83a98ff3c303488fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85c65875fbc9e5a85bc825af87d3a3cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425048&auth_key=1760425048-0-0-2d495302539f48bcd7d22e3448b49cd9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GLSim-Detecting-Object-Hallucinations-in-LVLMs-via-Global-Local-Similarity"><a href="#GLSim-Detecting-Object-Hallucinations-in-LVLMs-via-Global-Local-Similarity" class="headerlink" title="GLSim: Detecting Object Hallucinations in LVLMs via Global-Local   Similarity"></a>GLSim: Detecting Object Hallucinations in LVLMs via Global-Local   Similarity</h2><p><strong>Authors:Seongheon Park, Yixuan Li</strong></p>
<p>Object hallucination in large vision-language models presents a significant challenge to their safe deployment in real-world applications. Recent works have proposed object-level hallucination scores to estimate the likelihood of object hallucination; however, these methods typically adopt either a global or local perspective in isolation, which may limit detection reliability. In this paper, we introduce GLSim, a novel training-free object hallucination detection framework that leverages complementary global and local embedding similarity signals between image and text modalities, enabling more accurate and reliable hallucination detection in diverse scenarios. We comprehensively benchmark existing object hallucination detection methods and demonstrate that GLSim achieves superior detection performance, outperforming competitive baselines by a significant margin. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¯¹è±¡å¹»è§‰ç°è±¡å¯¹å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨éƒ¨ç½²æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œå·²ç»æå‡ºäº†å¯¹è±¡çº§åˆ«çš„å¹»è§‰è¯„åˆ†æ¥ä¼°è®¡å‡ºç°å¯¹è±¡å¹»è§‰çš„å¯èƒ½æ€§ï¼›ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­¤ç«‹åœ°é‡‡ç”¨å…¨å±€æˆ–å±€éƒ¨è§†è§’ï¼Œå¯èƒ½ä¼šé™åˆ¶æ£€æµ‹å¯é æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GLSimï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒå¯¹è±¡å¹»è§‰æ£€æµ‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´å…¨å±€å’Œå±€éƒ¨åµŒå…¥ç›¸ä¼¼æ€§çš„äº’è¡¥ä¿¡å·ï¼Œèƒ½å¤Ÿåœ¨å„ç§åœºæ™¯ä¸­å®ç°æ›´å‡†ç¡®ã€æ›´å¯é åœ°å¹»è§‰æ£€æµ‹ã€‚æˆ‘ä»¬å¯¹ç°æœ‰çš„å¯¹è±¡å¹»è§‰æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯æ˜GLSimçš„æ£€æµ‹æ€§èƒ½ä¼˜è¶Šï¼Œæ˜¾è‘—ä¼˜äºç«äº‰å¯¹æ‰‹çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19972v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†å¯¹è±¡å¹»è§†åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»å…¨å±€æˆ–å±€éƒ¨å•ä¸€è§’åº¦è¯„ä¼°å¯¹è±¡å¹»è§†çš„å¯èƒ½æ€§ï¼Œå¯èƒ½é™åˆ¶äº†æ£€æµ‹çš„å¯é æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒå¯¹è±¡å¹»è§†æ£€æµ‹æ¡†æ¶GLSimï¼Œå®ƒåˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å…¨å±€å’Œå±€éƒ¨åµŒå…¥ç›¸ä¼¼æ€§ä¿¡å·ï¼Œæé«˜äº†åœ¨å„ç§åœºæ™¯ä¸‹çš„æ£€æµ‹å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒGLSimåœ¨å¯¹è±¡å¹»è§†æ£€æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è±¡å¹»è§†åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå½±å“å…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨çš„å®‰å…¨æ€§ã€‚</li>
<li>ç°æœ‰å¯¹è±¡å¹»è§†æ£€æµ‹æ–¹æ³•ä¸»è¦ä»å…¨å±€æˆ–å±€éƒ¨å•ä¸€è§’åº¦è¿›è¡Œè¯„ä¼°ï¼Œå­˜åœ¨æ£€æµ‹å¯é æ€§é—®é¢˜ã€‚</li>
<li>GLSimæ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒå¯¹è±¡å¹»è§†æ£€æµ‹æ¡†æ¶ï¼Œç»“åˆäº†å…¨å±€å’Œå±€éƒ¨åµŒå…¥ç›¸ä¼¼æ€§ä¿¡å·ï¼Œæé«˜äº†æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>GLSimé€šè¿‡ç»¼åˆè¯„ä¼°ç°æœ‰å¯¹è±¡å¹»è§†æ£€æµ‹æ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>GLSimåœ¨å¤šç§åœºæ™¯ä¸‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºå„ç§è§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-13064bb5eff988fa83d6c9b59055605a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425056&auth_key=1760425056-0-0-0f918447df696fe0ac89f60f366a3497&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07c1c72849709cc7ee66072bba0b7a6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425063&auth_key=1760425063-0-0-2f9cc46f3ec86685b39fc307678b355b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62a4d14cbf026c7ef643db9a8cfe3605~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425070&auth_key=1760425070-0-0-a094d477631c5675f3a69b9bc74673d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Semantic-Segmentation-Algorithm-for-Pleural-Effusion-Based-on-DBIF-AUNet"><a href="#A-Semantic-Segmentation-Algorithm-for-Pleural-Effusion-Based-on-DBIF-AUNet" class="headerlink" title="A Semantic Segmentation Algorithm for Pleural Effusion Based on   DBIF-AUNet"></a>A Semantic Segmentation Algorithm for Pleural Effusion Based on   DBIF-AUNet</h2><p><strong>Authors:Ruixiang Tang, Mingda Zhang, Jianglong Qin, Yan Song, Yi Wu, Wei Wu</strong></p>
<p>Pleural effusion semantic segmentation can significantly enhance the accuracy and timeliness of clinical diagnosis and treatment by precisely identifying disease severity and lesion areas. Currently, semantic segmentation of pleural effusion CT images faces multiple challenges. These include similar gray levels between effusion and surrounding tissues, blurred edges, and variable morphology. Existing methods often struggle with diverse image variations and complex edges, primarily because direct feature concatenation causes semantic gaps. To address these challenges, we propose the Dual-Branch Interactive Fusion Attention model (DBIF-AUNet). This model constructs a densely nested skip-connection network and innovatively refines the Dual-Domain Feature Disentanglement module (DDFD). The DDFD module orthogonally decouples the functions of dual-domain modules to achieve multi-scale feature complementarity and enhance characteristics at different levels. Concurrently, we design a Branch Interaction Attention Fusion module (BIAF) that works synergistically with the DDFD. This module dynamically weights and fuses global, local, and frequency band features, thereby improving segmentation robustness. Furthermore, we implement a nested deep supervision mechanism with hierarchical adaptive hybrid loss to effectively address class imbalance. Through validation on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results outperform state-of-the-art medical image segmentation models U-Net++ and Swin-UNet by 5.7%&#x2F;2.7% and 2.2%&#x2F;1.5% respectively, demonstrating significant optimization in segmentation accuracy for complex pleural effusion CT images. </p>
<blockquote>
<p>èƒ¸è…”ç§¯æ¶²è¯­ä¹‰åˆ†å‰²èƒ½å¤Ÿç²¾ç¡®è¯†åˆ«ç–¾ç—…ä¸¥é‡ç¨‹åº¦å’Œç—…å˜åŒºåŸŸï¼Œä»è€Œæ˜¾è‘—æé«˜ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—å‡†ç¡®æ€§å’ŒåŠæ—¶æ€§ã€‚ç›®å‰ï¼Œèƒ¸è…”ç§¯æ¶²CTå›¾åƒçš„è¯­ä¹‰åˆ†å‰²é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç§¯æ¶²ä¸å‘¨å›´ç»„ç»‡çš„ç°åº¦ç›¸ä¼¼ã€è¾¹ç¼˜æ¨¡ç³Šä»¥åŠå½¢æ€å¤šå˜ç­‰ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åº”å¯¹å›¾åƒå¤šæ ·æ€§å’Œå¤æ‚è¾¹ç¼˜æƒ…å†µï¼Œä¸»è¦æ˜¯å› ä¸ºç›´æ¥ç‰¹å¾æ‹¼æ¥ä¼šé€ æˆè¯­ä¹‰é¸¿æ²Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒåˆ†æ”¯äº¤äº’èåˆæ³¨æ„åŠ›æ¨¡å‹ï¼ˆDBIF-AUNetï¼‰ã€‚è¯¥æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªå¯†é›†åµŒå¥—è·³è·ƒè¿æ¥ç½‘ç»œï¼Œå¹¶åˆ›æ–°åœ°æ”¹è¿›äº†åŒåŸŸç‰¹å¾åˆ†è§£æ¨¡å—ï¼ˆDDFDï¼‰ã€‚DDFDæ¨¡å—æ­£äº¤åœ°è§£è€¦äº†åŒåŸŸæ¨¡å—çš„åŠŸèƒ½ï¼Œä»¥å®ç°å¤šå°ºåº¦ç‰¹å¾äº’è¡¥å¹¶å¢å¼ºä¸åŒçº§åˆ«çš„ç‰¹å¾ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†åˆ†æ”¯äº¤äº’æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ˆBIAFï¼‰ï¼Œå®ƒä¸DDFDååŒå·¥ä½œã€‚è¯¥æ¨¡å—åŠ¨æ€åŠ æƒå¹¶èåˆå…¨å±€ã€å±€éƒ¨å’Œé¢‘å¸¦ç‰¹å¾ï¼Œæé«˜åˆ†å‰²ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†å…·æœ‰åˆ†å±‚è‡ªé€‚åº”æ··åˆæŸå¤±çš„åµŒå¥—æ·±åº¦ç›‘ç£æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚é€šè¿‡å¯¹è¥¿å—åŒ»é™¢1622å¼ èƒ¸è…”ç§¯æ¶²CTå›¾åƒçš„éªŒè¯ï¼ŒDBIF-AUNetçš„IoUå’ŒDiceå¾—åˆ†åˆ†åˆ«ä¸º80.1%å’Œ89.0%ï¼Œç›¸æ¯”æœ€å…ˆè¿›çš„åŒ»ç–—å›¾åƒåˆ†å‰²æ¨¡å‹U-Net++å’ŒSwin-UNetåˆ†åˆ«é«˜å‡º5.7%&#x2F;2.7%å’Œ2.2%&#x2F;1.5%ã€‚è¿™è¯æ˜äº†DBIF-AUNetåœ¨å¤æ‚èƒ¸è…”ç§¯æ¶²CTå›¾åƒåˆ†å‰²ç²¾åº¦ä¸Šçš„æ˜¾è‘—ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06191v2">PDF</a> 12 pages, 6 figures, 2 tables</p>
<p><strong>Summary</strong>ï¼š<br>èƒ¸è†œç§¯æ¶²è¯­ä¹‰åˆ†å‰²èƒ½å¤Ÿç²¾ç¡®è¯†åˆ«ç–¾ç—…ä¸¥é‡ç¨‹åº¦å’Œç—…ç¶åŒºåŸŸï¼Œä»è€Œæé«˜ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—å‡†ç¡®æ€§å’ŒåŠæ—¶æ€§ã€‚é’ˆå¯¹èƒ¸è†œç§¯æ¶²CTå›¾åƒè¯­ä¹‰åˆ†å‰²æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç°åº¦ç›¸ä¼¼ã€è¾¹ç¼˜æ¨¡ç³Šå’Œå½¢æ€å¤šå˜ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒåˆ†æ”¯äº¤äº’èåˆæ³¨æ„åŠ›æ¨¡å‹ï¼ˆDBIF-AUNetï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡æ”¹è¿›ç‰¹å¾åˆ†è§£å’Œèåˆæ–¹å¼ï¼Œå®ç°äº†å¤šå°ºåº¦ç‰¹å¾äº’è¡¥å’Œä¸åŒå±‚æ¬¡ç‰¹å¾å¢å¼ºã€‚åœ¨è¥¿å—åŒ»é™¢çš„1622å¼ èƒ¸è†œç§¯æ¶²CTå›¾åƒä¸Šè¿›è¡ŒéªŒè¯ï¼ŒDBIF-AUNetçš„IoUå’ŒDiceå¾—åˆ†åˆ†åˆ«ä¸º80.1%å’Œ89.0%ï¼Œè¾ƒå…¶ä»–å…ˆè¿›åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>èƒ¸è†œç§¯æ¶²è¯­ä¹‰åˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—å…·æœ‰é‡è¦æ„ä¹‰ï¼Œèƒ½ç²¾ç¡®è¯†åˆ«ç–¾ç—…ä¸¥é‡ç¨‹åº¦å’Œç—…ç¶åŒºåŸŸã€‚</li>
<li>èƒ¸è†œç§¯æ¶²CTå›¾åƒè¯­ä¹‰åˆ†å‰²é¢ä¸´ç°åº¦ç›¸ä¼¼ã€è¾¹ç¼˜æ¨¡ç³Šå’Œå½¢æ€å¤šå˜ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸å¸¸éš¾ä»¥åº”å¯¹å¤šæ ·çš„å›¾åƒå˜åŒ–å’Œå¤æ‚çš„è¾¹ç¼˜ï¼Œä¸»è¦åŸå› æ˜¯ç›´æ¥ç‰¹å¾æ‹¼æ¥ä¼šå¯¼è‡´è¯­ä¹‰é¸¿æ²Ÿã€‚</li>
<li>åŒåˆ†æ”¯äº¤äº’èåˆæ³¨æ„åŠ›æ¨¡å‹ï¼ˆDBIF-AUNetï¼‰è¢«æå‡ºä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ”¹è¿›çš„ç‰¹å¾åˆ†è§£å’Œèåˆæ–¹å¼ã€‚</li>
<li>DBIF-AUNeté€šè¿‡æ„å»ºå¯†é›†åµŒå¥—è·³è·ƒè¿æ¥ç½‘ç»œå’Œåˆ›æ–°åœ°æ”¹è¿›åŒåŸŸç‰¹å¾åˆ†è§£æ¨¡å—ï¼ˆDDFDï¼‰ï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾äº’è¡¥å’Œä¸åŒå±‚æ¬¡ç‰¹å¾å¢å¼ºã€‚</li>
<li>DBIF-AUNetè¿˜è®¾è®¡äº†åˆ†æ”¯äº¤äº’æ³¨æ„åŠ›èåˆæ¨¡å—ï¼ˆBIAFï¼‰ï¼Œèƒ½å¤ŸåŠ¨æ€åŠ æƒå’Œèåˆå…¨å±€ã€å±€éƒ¨å’Œé¢‘å¸¦ç‰¹å¾ï¼Œæé«˜åˆ†å‰²ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-52bbc776cdce54af1ab0304ad1fd6899~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425077&auth_key=1760425077-0-0-5f85a58e6859086b3c75ffa46f96c3a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-13ddac17662bf75e0d7f8754a608fa4c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425085&auth_key=1760425085-0-0-62ec6b4e50af046f786cc551e5c13b29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e656edeaf5ad76621bc515cb935886f2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425092&auth_key=1760425092-0-0-4b7d8c7bfc70f065677f36cbb4160f2b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc17139d5ace2ece84770a6918f43938~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425098&auth_key=1760425098-0-0-9b95c167283037a9d08489133b6633ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DAOcc-3D-Object-Detection-Assisted-Multi-Sensor-Fusion-for-3D-Occupancy-Prediction"><a href="#DAOcc-3D-Object-Detection-Assisted-Multi-Sensor-Fusion-for-3D-Occupancy-Prediction" class="headerlink" title="DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy   Prediction"></a>DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy   Prediction</h2><p><strong>Authors:Zhen Yang, Yanpeng Dong, Jiayu Wang, Heng Wang, Lichao Ma, Zijian Cui, Qi Liu, Haoran Pei, Kexin Zhang, Chao Zhang</strong></p>
<p>Multi-sensor fusion significantly enhances the accuracy and robustness of 3D semantic occupancy prediction, which is crucial for autonomous driving and robotics. However, most existing approaches depend on high-resolution images and complex networks to achieve top performance, hindering their deployment in practical scenarios. Moreover, current multi-sensor fusion approaches mainly focus on improving feature fusion while largely neglecting effective supervision strategies for those features. To address these issues, we propose DAOcc, a novel multi-modal occupancy prediction framework that leverages 3D object detection supervision to assist in achieving superior performance, while using a deployment-friendly image backbone and practical input resolution. In addition, we introduce a BEV View Range Extension strategy to mitigate performance degradation caused by lower image resolution. Extensive experiments demonstrate that DAOcc achieves new state-of-the-art results on both the Occ3D-nuScenes and Occ3D-Waymo benchmarks, and outperforms previous state-of-the-art methods by a significant margin using only a ResNet-50 backbone and 256*704 input resolution. With TensorRT optimization, DAOcc reaches 104.9 FPS while maintaining 54.2 mIoU on an NVIDIA RTX 4090 GPU. Code is available at <a target="_blank" rel="noopener" href="https://github.com/AlphaPlusTT/DAOcc">https://github.com/AlphaPlusTT/DAOcc</a>. </p>
<blockquote>
<p>å¤šä¼ æ„Ÿå™¨èåˆæ˜¾è‘—æé«˜äº†3Dè¯­ä¹‰å ç”¨é¢„æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œè¿™å¯¹äºè‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚ç½‘ç»œæ¥å®ç°æœ€ä½³æ€§èƒ½ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨å®è·µåœºæ™¯ä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œå½“å‰çš„å¤šä¼ æ„Ÿå™¨èåˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›ç‰¹å¾èåˆä¸Šï¼Œè€Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†è¿™äº›ç‰¹å¾çš„æœ‰æ•ˆç›‘ç£ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DAOccï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ¨¡å¼å ç”¨é¢„æµ‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨3Då¯¹è±¡æ£€æµ‹ç›‘ç£æ¥å¸®åŠ©å®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨éƒ¨ç½²å‹å¥½çš„å›¾åƒä¸»å¹²å’Œå®ç”¨çš„è¾“å…¥åˆ†è¾¨ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†BEV View Range Extensionç­–ç•¥ï¼Œä»¥ç¼“è§£å› è¾ƒä½å›¾åƒåˆ†è¾¨ç‡å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDAOccåœ¨Occ3D-nuSceneså’ŒOcc3D-WaymoåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œå¹¶ä¸”ä»…ä½¿ç”¨ResNet-50ä¸»å¹²å’Œ256*704è¾“å…¥åˆ†è¾¨ç‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚é€šè¿‡TensorRTä¼˜åŒ–ï¼ŒDAOccåœ¨NVIDIA RTX 4090 GPUä¸Šè¾¾åˆ°äº†104.9 FPSçš„å¸§ç‡ï¼ŒåŒæ—¶ä¿æŒäº†54.2 mIoUçš„æ€§èƒ½ã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/AlphaPlusTT/DAOcc%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AlphaPlusTT/DAOccè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19972v4">PDF</a> TCSVT Accepted version (not the final published version)</p>
<p><strong>Summary</strong><br>å¤šä¼ æ„Ÿå™¨èåˆèƒ½æé«˜ä¸‰ç»´è¯­ä¹‰å ç”¨é¢„æµ‹çš„å‡†ç¡®æ€§åŠé²æ£’æ€§ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯è‡³å…³é‡è¦ã€‚ä½†ç°æœ‰æ–¹æ³•ä¾èµ–é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚ç½‘ç»œå®ç°æœ€ä½³æ€§èƒ½ï¼Œé˜»ç¢äº†å®é™…åº”ç”¨åœºæ™¯çš„éƒ¨ç½²ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DAOccæ¡†æ¶ï¼Œåˆ©ç”¨ä¸‰ç»´ç‰©ä½“æ£€æµ‹çš„ç›‘æ§å¸®åŠ©è·å¾—å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶é‡‡ç”¨éƒ¨ç½²å‹å¥½çš„å›¾åƒéª¨å¹²ç½‘åŠå®ç”¨è¾“å…¥åˆ†è¾¨ç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒDAOccåœ¨Occ3D-nuSceneså’ŒOcc3D-Waymoä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šéƒ½è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä»…ä½¿ç”¨ResNet-50éª¨å¹²ç½‘å’Œ256*704è¾“å…¥åˆ†è¾¨ç‡ä¾¿å¤§å¹…è¶…è¶Šå…ˆå‰æŠ€æœ¯ã€‚ç»TensorRTä¼˜åŒ–åï¼ŒDAOccåœ¨NVIDIA RTX 4090 GPUä¸Šè¾¾åˆ°æ¯ç§’104.9å¸§çš„å¤„ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒ54.2 mIoUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä¼ æ„Ÿå™¨èåˆèƒ½æé«˜ä¸‰ç»´è¯­ä¹‰å ç”¨é¢„æµ‹çš„å‡†ç¡®æ€§åŠé²æ£’æ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚ç½‘ç»œï¼Œå½±å“å®é™…åº”ç”¨éƒ¨ç½²ã€‚</li>
<li>DAOccæ¡†æ¶åˆ©ç”¨ä¸‰ç»´ç‰©ä½“æ£€æµ‹ç›‘æ§æ¥æå‡æ€§èƒ½ï¼Œå¹¶é‡‡ç”¨éƒ¨ç½²å‹å¥½çš„å›¾åƒéª¨å¹²ç½‘åŠå®ç”¨è¾“å…¥åˆ†è¾¨ç‡ã€‚</li>
<li>DAOccåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šå…ˆå‰æŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨ResNet-50éª¨å¹²ç½‘å’Œ256*704è¾“å…¥åˆ†è¾¨ç‡å³å¯å®ç°è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>DAOccç»TensorRTä¼˜åŒ–åå¤„ç†é€Ÿåº¦æå‡ã€‚</li>
<li>DAOccåœ¨NVIDIA RTX 4090 GPUä¸Šè¾¾åˆ°æ¯ç§’104.9å¸§å¤„ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒé«˜mIoUå€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a1d883212c20e2da7db198ed04564cda~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425106&auth_key=1760425106-0-0-dc60aceffb0746e0a4e84dfb15947556&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-35c2818e99aa901c89fe73bf6dfdcbf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425113&auth_key=1760425113-0-0-afc82e6af57b1c419fc418d16b921e82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-530d062f41b26c70f2483a47c95935e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425121&auth_key=1760425121-0-0-617fd3838462e6fa55c126fb923085c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6916f4becdf3e86e04f4fcfa4a5b5142~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425128&auth_key=1760425128-0-0-7f4bf4fbe93b8c5f9fdbd1e376757a65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab0814fd4fe8efccbe7f5590ceb1481f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425135&auth_key=1760425135-0-0-25eb334de6784c7f2c5f2a3aa69af00d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2baac4dd357f11ea49beb4bd4105eef1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425142&auth_key=1760425142-0-0-5b8c87007066b2997bb1b08f94b56637&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Investigating-Long-term-Training-for-Remote-Sensing-Object-Detection"><a href="#Investigating-Long-term-Training-for-Remote-Sensing-Object-Detection" class="headerlink" title="Investigating Long-term Training for Remote Sensing Object Detection"></a>Investigating Long-term Training for Remote Sensing Object Detection</h2><p><strong>Authors:JongHyun Park, Yechan Kim, Moongu Jeon</strong></p>
<p>Recently, numerous methods have achieved impressive performance in remote sensing object detection, relying on convolution or transformer architectures. Such detectors typically have a feature backbone to extract useful features from raw input images. A common practice in current detectors is initializing the backbone with pre-trained weights available online. Fine-tuning the backbone is typically required to generate features suitable for remote-sensing images. While the prolonged training could lead to over-fitting, hindering the extraction of basic visual features, it can enable models to gradually extract deeper insights and richer representations from remote sensing data. Striking a balance between these competing factors is critical for achieving optimal performance. In this study, we aim to investigate the performance and characteristics of remote sensing object detection models under very long training schedules, and propose a novel method named Dynamic Backbone Freezing (DBF) for feature backbone fine-tuning on remote sensing object detection under long-term training. Our method addresses the dilemma of whether the backbone should extract low-level generic features or possess specific knowledge of the remote sensing domain, by introducing a module called â€˜Freezing Schedulerâ€™ to manage the update of backbone features during long-term training dynamically. Extensive experiments on DOTA and DIOR-R show that our approach enables more accurate model learning while substantially reducing computational costs in long-term training. Besides, it can be seamlessly adopted without additional effort due to its straightforward design. The code is available at <a target="_blank" rel="noopener" href="https://github.com/unique-chan/dbf">https://github.com/unique-chan/dbf</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè®¸å¤šæ–¹æ³•å·²åœ¨é¥æ„Ÿç›®æ ‡æ£€æµ‹æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå·ç§¯æˆ–è½¬æ¢å™¨æ¶æ„ã€‚æ­¤ç±»æ£€æµ‹å™¨é€šå¸¸å…·æœ‰ç‰¹å¾ä¸»å¹²ï¼Œå¯ä»åŸå§‹è¾“å…¥å›¾åƒä¸­æå–æœ‰ç”¨ç‰¹å¾ã€‚å½“å‰æ£€æµ‹å™¨ä¸­çš„ä¸€ç§å¸¸è§åšæ³•æ˜¯ä½¿ç”¨åœ¨çº¿å¯ç”¨çš„é¢„è®­ç»ƒæƒé‡æ¥åˆå§‹åŒ–ä¸»å¹²ã€‚é€šå¸¸éœ€è¦å¯¹ä¸»å¹²è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆé€‚ç”¨äºé¥æ„Ÿå›¾åƒçš„ç‰¹å¾ã€‚è™½ç„¶é•¿æ—¶é—´çš„è®­ç»ƒå¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä»è€Œé˜»ç¢åŸºæœ¬è§†è§‰ç‰¹å¾çš„æå–ï¼Œä½†å®ƒå¯ä»¥ä½¿æ¨¡å‹é€æ¸ä»é¥æ„Ÿæ•°æ®ä¸­æå–æ›´æ·±å…¥çš„è§è§£å’Œæ›´ä¸°å¯Œçš„è¡¨ç¤ºã€‚åœ¨è¿™ä¸¤ç§ç›¸äº’ç«äº‰çš„å› ç´ ä¹‹é—´å–å¾—å¹³è¡¡å¯¹äºå®ç°æœ€ä½³æ€§èƒ½è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨ç ”ç©¶é¥æ„Ÿç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨éå¸¸é•¿çš„è®­ç»ƒè®¡åˆ’ä¸‹çš„æ€§èƒ½å’Œç‰¹ç‚¹ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€ä¸»å¹²å†»ç»“ï¼ˆDBFï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨é•¿æœŸè®­ç»ƒä¸­å¯¹é¥æ„Ÿç›®æ ‡æ£€æµ‹çš„ç‰¹å¾ä¸»å¹²è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ä¸ªåä¸ºâ€œå†»ç»“è°ƒåº¦ç¨‹åºâ€çš„æ¨¡å—æ¥ç®¡ç†é•¿æœŸè®­ç»ƒæœŸé—´ä¸»å¹²ç‰¹å¾çš„æ›´æ–°ï¼Œè§£å†³äº†ä¸»å¹²åº”è¯¥æå–ä½çº§é€šç”¨ç‰¹å¾è¿˜æ˜¯å…·å¤‡é¥æ„Ÿé¢†åŸŸçš„ç‰¹å®šçŸ¥è¯†çš„é—®é¢˜ã€‚åœ¨DOTAå’ŒDIOR-Rä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿æ¨¡å‹å­¦ä¹ æ›´åŠ å‡†ç¡®ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†é•¿æœŸè®­ç»ƒçš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œç”±äºå…¶ç®€å•ç›´è§‚çš„è®¾è®¡ï¼Œå®ƒå¯ä»¥æ— ç¼é‡‡ç”¨ï¼Œæ— éœ€é¢å¤–åŠªåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/unique-chan/dbf%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/unique-chan/dbfæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15143v3">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨é¥æ„Ÿç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨é•¿æœŸè®­ç»ƒä¸‹çš„æ€§èƒ½ä¸ç‰¹æ€§ï¼Œå¹¶æå‡ºä¸€ç§åä¸ºåŠ¨æ€ä¸»å¹²å†»ç»“ï¼ˆDBFï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨é•¿æœŸè®­ç»ƒä¸­å¯¹é¥æ„Ÿç›®æ ‡æ£€æµ‹çš„ç‰¹å¾ä¸»å¹²è¿›è¡Œå¾®è°ƒã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åä¸ºâ€œå†»ç»“è°ƒåº¦å™¨â€çš„æ¨¡å—ï¼Œå®ç°ä¸»å¹²ç‰¹å¾æ›´æ–°çš„åŠ¨æ€ç®¡ç†ï¼Œè§£å†³äº†ä¸»å¹²åº”æå–ä½å±‚æ¬¡é€šç”¨ç‰¹å¾è¿˜æ˜¯å…·å¤‡é¥æ„Ÿé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„é—®é¢˜ã€‚åœ¨DOTAå’ŒDIOR-Rä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ›´å‡†ç¡®åœ°å­¦ä¹ æ¨¡å‹ï¼ŒåŒæ—¶å¤§å¹…é™ä½é•¿æœŸè®­ç»ƒçš„è®¡ç®—æˆæœ¬ï¼Œä¸”è®¾è®¡ç®€æ´ï¼Œå¯æ— ç¼é‡‡ç”¨è€Œæ— éœ€é¢å¤–åŠªåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¥æ„Ÿç›®æ ‡æ£€æµ‹è¿‘æœŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä¸»è¦ä¾èµ–äºå·ç§¯æˆ–transformeræ¶æ„ã€‚</li>
<li>å½“å‰æ£€æµ‹å™¨é€šå¸¸ä½¿ç”¨åœ¨çº¿é¢„è®­ç»ƒæƒé‡æ¥åˆå§‹åŒ–ä¸»å¹²ï¼Œå¹¶è¿›è¡Œå¾®è°ƒä»¥ç”Ÿæˆé€‚åˆé¥æ„Ÿå›¾åƒçš„ç‰¹å¾ã€‚</li>
<li>é•¿æœŸè®­ç»ƒå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå½±å“åŸºæœ¬è§†è§‰ç‰¹å¾çš„æå–ï¼Œä½†ä¹Ÿèƒ½ä½¿æ¨¡å‹é€æ¸æå–æ›´æ·±å…¥ã€æ›´ä¸°å¯Œçš„é¥æ„Ÿæ•°æ®è¡¨ç¤ºã€‚</li>
<li>ç ”ç©¶æ—¨åœ¨è§£å†³é¥æ„Ÿç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨é•¿æœŸè®­ç»ƒä¸‹çš„æ€§èƒ½é—®é¢˜ï¼Œå¹¶æå‡ºåŠ¨æ€ä¸»å¹²å†»ç»“ï¼ˆDBFï¼‰æ–¹æ³•ã€‚</li>
<li>DBFæ–¹æ³•é€šè¿‡å¼•å…¥â€œå†»ç»“è°ƒåº¦å™¨â€æ¨¡å—ï¼Œå®ç°ä¸»å¹²ç‰¹å¾æ›´æ–°çš„åŠ¨æ€ç®¡ç†ï¼Œåœ¨æå–é€šç”¨ä½å±‚æ¬¡ç‰¹å¾å’Œé¥æ„Ÿé¢†åŸŸç‰¹å®šçŸ¥è¯†ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-446c31b083d3359ef894e30295dd21cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425149&auth_key=1760425149-0-0-e630615ee0134b0e93e69e8dbecd68bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-00947253f4ccd084d63f621901644cc9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425156&auth_key=1760425156-0-0-a9fec89a01e199584a6c4bda2e45158c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bea7915d4f304d73ecc70a3752e3bd0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425162&auth_key=1760425162-0-0-f65d158197854eebb2c56c866f9084bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-re-calibration-method-for-object-detection-with-multi-modal-alignment-bias-in-autonomous-driving"><a href="#A-re-calibration-method-for-object-detection-with-multi-modal-alignment-bias-in-autonomous-driving" class="headerlink" title="A re-calibration method for object detection with multi-modal alignment   bias in autonomous driving"></a>A re-calibration method for object detection with multi-modal alignment   bias in autonomous driving</h2><p><strong>Authors:Zhihang Song, Dingyi Yao, Ruibo Ming, Lihui Peng, Danya Yao, Yi Zhang</strong></p>
<p>Multi-modal object detection in autonomous driving has achieved great breakthroughs due to the usage of fusing complementary information from different sensors. The calibration in fusion between sensors such as LiDAR and camera was always supposed to be precise in previous work. However, in reality, calibration matrices are fixed when the vehicles leave the factory, but mechanical vibration, road bumps, and data lags may cause calibration bias. As there is relatively limited research on the impact of calibration on fusion detection performance, multi-sensor detection methods with flexible calibration dependency have remained a key objective. In this paper, we systematically evaluate the sensitivity of the SOTA EPNet++ detection framework and prove that even slight bias on calibration can reduce the performance seriously. To address this vulnerability, we propose a re-calibration model to re-calibrate the misalignment in detection tasks. This model integrates LiDAR point cloud, camera image, and initial calibration matrix as inputs, generating re-calibrated bias through semantic segmentation guidance and a tailored loss function design. The re-calibration model can operate with existing detection algorithms, enhancing both robustness against calibration bias and overall object detection performance. Our approach establishes a foundational methodology for maintaining reliability in multi-modal perception systems under real-world calibration uncertainties. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç‰©ä½“æ£€æµ‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå·²ç»å–å¾—äº†é‡å¤§çªç ´ï¼Œè¿™æ˜¯ç”±äºèåˆäº†ä¸åŒä¼ æ„Ÿå™¨çš„äº’è¡¥ä¿¡æ¯ã€‚ä»¥å‰çš„å·¥ä½œæ€»æ˜¯è¦æ±‚æ¿€å…‰é›·è¾¾å’Œç›¸æœºç­‰ä¼ æ„Ÿå™¨ä¹‹é—´çš„èåˆæ ¡å‡†è¦ç²¾ç¡®ã€‚ç„¶è€Œï¼Œåœ¨å®é™…æƒ…å†µä¸‹ï¼Œè½¦è¾†åœ¨å‡ºå‚æ—¶æ ¡å‡†çŸ©é˜µæ˜¯å›ºå®šçš„ï¼Œä½†æœºæ¢°æŒ¯åŠ¨ã€è·¯é¢é¢ ç°¸å’Œæ•°æ®å»¶è¿Ÿå¯èƒ½ä¼šå¯¼è‡´æ ¡å‡†åå·®ã€‚ç”±äºå…³äºæ ¡å‡†å¯¹èåˆæ£€æµ‹æ€§èƒ½å½±å“çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ï¼Œå…·æœ‰çµæ´»æ ¡å‡†ä¾èµ–æ€§çš„å¤šä¼ æ„Ÿå™¨æ£€æµ‹æ–¹æ³•ä»æ˜¯ä¸€ä¸ªå…³é”®ç›®æ ‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†å…ˆè¿›æ£€æµ‹æ¡†æ¶EPNet++çš„æ•æ„Ÿæ€§ï¼Œå¹¶è¯æ˜å³ä½¿è½»å¾®çš„æ ¡å‡†åå·®ä¹Ÿä¼šä¸¥é‡é™ä½æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ¼æ´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡æ–°æ ¡å‡†æ¨¡å‹ï¼Œå¯¹æ£€æµ‹ä»»åŠ¡ä¸­çš„ä¸å¯¹é½è¿›è¡Œé‡æ–°æ ¡å‡†ã€‚è¯¥æ¨¡å‹å°†æ¿€å…‰é›·è¾¾ç‚¹äº‘ã€ç›¸æœºå›¾åƒå’Œåˆå§‹æ ¡å‡†çŸ©é˜µä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡è¯­ä¹‰åˆ†å‰²æŒ‡å¯¼å’Œå®šåˆ¶çš„æŸå¤±å‡½æ•°è®¾è®¡æ¥ç”Ÿæˆé‡æ–°æ ¡å‡†çš„åå·®ã€‚é‡æ–°æ ¡å‡†æ¨¡å‹å¯ä»¥ä¸ç°æœ‰çš„æ£€æµ‹ç®—æ³•ä¸€èµ·ä½¿ç”¨ï¼Œæé«˜äº†å¯¹æ ¡å‡†åå·®çš„é²æ£’æ€§å’Œæ€»ä½“ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºåœ¨ç°å®ä¸–ç•Œçš„æ ¡å‡†ä¸ç¡®å®šæ€§ä¸‹ï¼Œä¿æŒå¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿçš„å¯é æ€§å»ºç«‹äº†åŸºç¡€æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16848v3">PDF</a> Accepted for publication in IST 2025. Official IEEE Xplore entry will   be available once published</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªä¸»é©¾é©¶ä¸­å¤šæ¨¡æ€ç‰©ä½“æ£€æµ‹çš„çªç ´ï¼Œç‰¹åˆ«æ˜¯åœ¨èåˆä¸åŒä¼ æ„Ÿå™¨ä¿¡æ¯æ–¹é¢çš„åº”ç”¨ã€‚ç”±äºè½¦è¾†å‡ºå‚æ—¶æ ¡å‡†çŸ©é˜µæ˜¯å›ºå®šçš„ï¼Œä½†ç°å®ç¯å¢ƒä¸­çš„æœºæ¢°æŒ¯åŠ¨ã€è·¯é¢é¢ ç°¸å’Œæ•°æ®å»¶è¿Ÿå¯èƒ½å¯¼è‡´æ ¡å‡†åå·®ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¸­å…³äºæ ¡å‡†å¯¹èåˆæ£€æµ‹æ€§èƒ½å½±å“çš„ç ”ç©¶ç›¸å¯¹æœ‰é™ï¼Œæœ¬æ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†å½“å‰å…ˆè¿›çš„EPNet++æ£€æµ‹æ¡†æ¶çš„æ•æ„Ÿæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§é‡æ–°æ ¡å‡†æ¨¡å‹æ¥è§£å†³æ£€æµ‹ä»»åŠ¡ä¸­çš„ä¸å¯¹é½é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡è¯­ä¹‰åˆ†å‰²æŒ‡å¯¼å’Œå®šåˆ¶çš„æŸå¤±å‡½æ•°è®¾è®¡ï¼Œå°†æ¿€å…‰é›·è¾¾ç‚¹äº‘ã€ç›¸æœºå›¾åƒå’Œåˆå§‹æ ¡å‡†çŸ©é˜µä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆé‡æ–°æ ¡å‡†çš„åå·®ã€‚é‡æ–°æ ¡å‡†æ¨¡å‹å¯ä»¥ä¸ç°æœ‰çš„æ£€æµ‹ç®—æ³•ä¸€èµ·æ“ä½œï¼Œå¢å¼ºäº†å¯¹æŠ—æ ¡å‡†åå·®çš„é²æ£’æ€§å’Œæ•´ä½“ç‰©ä½“æ£€æµ‹æ€§èƒ½ã€‚æœ¬æ–‡çš„æ–¹æ³•ä¸ºç°å®ä¸–ç•Œä¸­æ ¡å‡†ä¸ç¡®å®šæ€§ä¸‹çš„å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿä¿æŒå¯é æ€§å»ºç«‹äº†åŸºç¡€æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ç‰©ä½“æ£€æµ‹åœ¨è‡ªä¸»é©¾é©¶ä¸­å–å¾—çªç ´ï¼Œå¾—ç›Šäºèåˆä¸åŒä¼ æ„Ÿå™¨çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>ç°å®ä¸­ï¼Œè½¦è¾†å‡ºå‚æ—¶çš„å›ºå®šæ ¡å‡†çŸ©é˜µå¯èƒ½å—åˆ°æœºæ¢°æŒ¯åŠ¨ã€è·¯é¢é¢ ç°¸å’Œæ•°æ®å»¶è¿Ÿç­‰å› ç´ çš„å½±å“ï¼Œå¯¼è‡´æ ¡å‡†åå·®ã€‚</li>
<li>å½“å‰ç ”ç©¶å¯¹æ ¡å‡†å¯¹èåˆæ£€æµ‹æ€§èƒ½çš„å½±å“æœ‰é™ï¼Œå› æ­¤å¤šä¼ æ„Ÿå™¨æ£€æµ‹æ–¹æ³•éœ€è¦å…·å¤‡çµæ´»çš„æ ¡å‡†ä¾èµ–æ€§ã€‚</li>
<li>æœ¬æ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†EPNet++æ£€æµ‹æ¡†æ¶çš„æ•æ„Ÿæ€§ï¼Œå¹¶æŒ‡å‡ºè½»å¾®æ ¡å‡†åå·®å³å¯ä¸¥é‡å½±å“æ€§èƒ½ã€‚</li>
<li>é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†é‡æ–°æ ¡å‡†æ¨¡å‹ï¼Œé€šè¿‡è¯­ä¹‰åˆ†å‰²æŒ‡å¯¼å’Œå®šåˆ¶æŸå¤±å‡½æ•°è®¾è®¡æ¥è§£å†³æ£€æµ‹ä»»åŠ¡ä¸­çš„ä¸å¯¹é½é—®é¢˜ã€‚</li>
<li>é‡æ–°æ ¡å‡†æ¨¡å‹å¯ä»¥ä¸ç°æœ‰æ£€æµ‹ç®—æ³•ç»“åˆï¼Œå¢å¼ºå¯¹æ ¡å‡†åå·®çš„é²æ£’æ€§å’Œæ•´ä½“ç‰©ä½“æ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-948a207ee6705aa0878c9a8e206e55db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425170&auth_key=1760425170-0-0-433ac2816a0af62efc31ce3a657b171c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2c6007e8ffcef4c62b7d69eee080caed~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425178&auth_key=1760425178-0-0-c1e32e722e95fdab43972f46da465ced&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b4351723e2ef306965d19e3e52ceaddc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425185&auth_key=1760425185-0-0-fb3ff8d397574c26f219c4a5fa9335cf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63cf51a7fd765541c387815086c94d49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425191&auth_key=1760425191-0-0-98f01f167607bbc94c28e36e8dc07f41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a5e5fa325a9d86d53e56f6ff345b668~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425198&auth_key=1760425198-0-0-e043d658ceae1b8e0353bf849a5fd065&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa92b92ed4a516ff74d3d5362333cf02~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425205&auth_key=1760425205-0-0-5a7464b8dd8d1bde82f70e52aa13415b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af265b9191f6a67f745f427928b6f8eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425212&auth_key=1760425212-0-0-e3ec9eef83d421b77affb9c18dfd71e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-abb428bc4298c36e840a7732a8425dd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425218&auth_key=1760425218-0-0-22caad2331371ff792853ac7e28bc898&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-f4d6ca190ffea9ea27fa48f23675a116~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425226&auth_key=1760425226-0-0-a39a72d090c2f4046b1fe751ec43fd02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  FROQ Observing Face Recognition Models for Efficient Quality Assessment
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-3c12af6654f2baf1c3434fa8fd3e9d9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760683503&auth_key=1760683503-0-0-28c2af450822dfb9448918206b3438ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
