<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="无监督/半监督/对比学习">
    <meta name="description" content="无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-09-24  TS-P$^2$CL Plug-and-Play Dual Contrastive Learning for Vision-Guided   Medical Time Series Classification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>无监督/半监督/对比学习 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-a93e3776597773699674e91737962a70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425271&auth_key=1760425271-0-0-7f5f8ae73b74295bc67e35512a20b371&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">无监督/半监督/对比学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">无监督/半监督/对比学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                无监督/半监督/对比学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-24-更新"><a href="#2025-09-24-更新" class="headerlink" title="2025-09-24 更新"></a>2025-09-24 更新</h1><h2 id="TS-P-2-CL-Plug-and-Play-Dual-Contrastive-Learning-for-Vision-Guided-Medical-Time-Series-Classification"><a href="#TS-P-2-CL-Plug-and-Play-Dual-Contrastive-Learning-for-Vision-Guided-Medical-Time-Series-Classification" class="headerlink" title="TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided   Medical Time Series Classification"></a>TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided   Medical Time Series Classification</h2><p><strong>Authors:Qi’ao Xu, Pengfei Wang, Bo Zhong, Tianwen Qian, Xiaoling Wang, Ye Wang, Hong Yu</strong></p>
<p>Medical time series (MedTS) classification is pivotal for intelligent healthcare, yet its efficacy is severely limited by poor cross-subject generation due to the profound cross-individual heterogeneity. Despite advances in architectural innovations and transfer learning techniques, current methods remain constrained by modality-specific inductive biases that limit their ability to learn universally invariant representations. To overcome this, we propose TS-P$^2$CL, a novel plug-and-play framework that leverages the universal pattern recognition capabilities of pre-trained vision models. We introduce a vision-guided paradigm that transforms 1D physiological signals into 2D pseudo-images, establishing a bridge to the visual domain. This transformation enables implicit access to rich semantic priors learned from natural images. Within this unified space, we employ a dual-contrastive learning strategy: intra-modal consistency enforces temporal coherence, while cross-modal alignment aligns time-series dynamics with visual semantics, thereby mitigating individual-specific biases and learning robust, domain-invariant features. Extensive experiments on six MedTS datasets demonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both subject-dependent and subject-independent settings. </p>
<blockquote>
<p>医疗时间序列（MedTS）分类对于智能医疗至关重要，但其效率受到个体间巨大异质性导致的跨主题生成不良的限制。尽管在建筑创新和迁移学习技术方面取得了进展，当前的方法仍然受到特定于模态的归纳偏置的约束，这限制了它们学习普遍不变表示的能力。为了克服这一局限性，我们提出了TS-P$^2$CL，这是一个新型即插即用框架，利用预训练视觉模型的通用模式识别能力。我们引入了一种视觉引导范式，将1D生理信号转换为2D伪图像，建立了与视觉领域的桥梁。这种转换能够隐式地访问从自然图像中学习到的丰富的语义先验。在这个统一的空间中，我们采用了一种双重对比学习策略：模态内一致性加强时间连贯性，而模态间对齐将时间序列动态与视觉语义对齐，从而减轻个体特定偏见并学习稳健的、领域不变的特征。在六个MedTS数据集上的广泛实验表明，TS-P$^2$CL在主体依赖和主体独立的环境中始终优于十四种方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17802v1">PDF</a> 12 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为TS-P$^2$CL的新型即插即用框架，用于解决医疗时间序列分类中的跨主题生成问题。该框架利用预训练视觉模型的通用模式识别能力，通过转换一维生理信号为二维伪图像，建立与视觉领域的桥梁，从而访问从自然图像中学习到的丰富语义先验。在统一的空间内，采用双对比学习策略，即模态内一致性保证时间连贯性，模态间对齐将时间序列动态与视觉语义对齐，从而减轻个体特异性偏见并学习稳健的、领域不变的特征。在六个医疗时间序列数据集上的实验表明，TS-P$^2$CL在主体依赖和主体独立设置中均优于十四种方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗时间序列分类在智能医疗中至关重要，但受到跨主题生成不佳的限制，主要原因是跨个体之间存在巨大差异。</li>
<li>当前方法受到模态特定归纳偏见的影响，无法学习普遍的不变表示。</li>
<li>TS-P$^2$CL框架利用预训练的视觉模型的通用模式识别能力来解决这一问题。</li>
<li>通过将一维生理信号转换为二维伪图像，建立与视觉领域的桥梁，访问丰富的语义先验。</li>
<li>采用双对比学习策略，确保时间连贯性和模态间对齐，从而减轻个体特异性偏见。</li>
<li>在多个医疗时间序列数据集上，TS-P$^2$CL表现出优异的性能，优于其他方法。</li>
<li>TS-P$^2$CL框架具有广泛的应用前景，可推广到其他需要跨模态学习的领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17802">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1919a3547a8616cd924a4122c9d5319f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425279&auth_key=1760425279-0-0-0dea0d9af0dffd3aeed7bfa293db5347&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3d92c003607a4c1acfa054dd1eeb03e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425287&auth_key=1760425287-0-0-17846143d92244d35d040289dda57697&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VCE-Safe-Autoregressive-Image-Generation-via-Visual-Contrast-Exploitation"><a href="#VCE-Safe-Autoregressive-Image-Generation-via-Visual-Contrast-Exploitation" class="headerlink" title="VCE: Safe Autoregressive Image Generation via Visual Contrast   Exploitation"></a>VCE: Safe Autoregressive Image Generation via Visual Contrast   Exploitation</h2><p><strong>Authors:Feng Han, Chao Gong, Zhipeng Wei, Jingjing Chen, Yu-Gang Jiang</strong></p>
<p>Recently, autoregressive image generation models have wowed audiences with their remarkable capability in creating surprisingly realistic images. Models such as GPT-4o and LlamaGen can not only produce images that faithfully mimic renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also potentially generate Not-Safe-For-Work (NSFW) content, raising significant concerns regarding copyright infringement and ethical use. Despite these concerns, methods to safeguard autoregressive text-to-image models remain underexplored. Previous concept erasure methods, primarily designed for diffusion models that operate in denoising latent space, are not directly applicable to autoregressive models that generate images token by token. To address this critical gap, we propose Visual Contrast Exploitation (VCE), a novel framework comprising: (1) an innovative contrastive image pair construction paradigm that precisely decouples unsafe concepts from their associated content semantics, and (2) a sophisticated DPO-based training approach that enhances the model’s ability to identify and leverage visual contrastive features from image pairs, enabling precise concept erasure. Our comprehensive experiments across three challenging tasks-artist style erasure, explicit content erasure, and object removal-demonstrate that our method effectively secures the model, achieving state-of-the-art results while erasing unsafe concepts and maintaining the integrity of unrelated safe concepts. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/Maplebb/VCE">https://github.com/Maplebb/VCE</a>. </p>
<blockquote>
<p>最近，自回归图像生成模型凭借其创造惊人逼真图像的能力吸引了观众的目光。诸如GPT-4o和LlamaGen等模型不仅能够产生忠实模仿吉卜力、梵高或毕加索等著名艺术风格的图像，而且还可能生成不适合工作场合（NSFW）的内容，这引发了关于版权侵犯和道德使用方面的担忧。尽管存在这些担忧，但保护自回归文本到图像模型的方法仍然未被充分探索。以前的概念消除方法主要设计用于在降噪潜在空间操作的扩散模型，并不直接适用于逐令牌生成图像的自回归模型。为了解决这一关键空白，我们提出了视觉对比利用（VCE），这是一个新的框架，包括：（1）一种创新的对比图像对构建范式，该范式精确地将从不安全概念与其关联的内容语义中分离出来；（2）一种基于DPO的先进训练方法，提高模型从图像对中识别和利用视觉对比特征的能力，从而实现精确的概念消除。我们在三个具有挑战性的任务（艺术家风格消除、明确内容消除和对象移除）上进行的综合实验表明，我们的方法有效地保护了模型，在消除不安全概念的同时保持无关安全概念的完整性，并实现了最先进的结果。相关代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/Maplebb/VCE%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Maplebb/VCE上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16986v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了近期出现的基于文本生成图像的自回归模型（如GPT-4o和LlamaGen），这些模型可以生成逼真的图像并模仿不同的艺术风格，但同时也存在生成不适合工作场合（NSFW）内容的隐患，引发版权和伦理问题。为解决此问题，文章提出了一种名为视觉对比利用（VCE）的新框架，该框架包括构建对比图像对和基于DPO的训练方法，可精确地将不安全概念与其相关内容语义分离，实现精确的概念消除。实验证明，该方法在消除不安全概念的同时保持了安全概念的完整性，取得了业界最佳效果。代码和模型已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自回归图像生成模型具有模仿多种艺术风格的能力，并能生成逼真的图像。</li>
<li>这些模型可能生成不适合工作场合（NSFW）的内容，引发版权和伦理问题。</li>
<li>当前缺乏针对自回归文本到图像模型的保护措施。</li>
<li>提出的视觉对比利用（VCE）框架包括构建对比图像对和基于DPO的训练方法。</li>
<li>VCE框架可以精确地将不安全概念与其相关内容语义分离。</li>
<li>实验证明，VCE框架在消除不安全概念的同时保持了安全概念的完整性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16986">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3e18db0f1bd3bef3190459f429f43845~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425294&auth_key=1760425294-0-0-6439acfe78fb3fc9d2c98d654a1832d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fdd002b1f358f9ec11a677b8fa5daa60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425302&auth_key=1760425302-0-0-5e084ed0940322d21e38cc5dfb3ef430&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9841d872da1088a6c00776cf97bdc2e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425308&auth_key=1760425308-0-0-7a732dff15d0157be10bcce67305d245&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-42c251441d071fd505cb1deca1c75ef7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425315&auth_key=1760425315-0-0-6ea440c685b503b1573e2fd0af301582&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8a4808d19a283dc94b24f45c0416e0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425322&auth_key=1760425322-0-0-6f98125f4bbdf6105b6627bece262c5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding"><a href="#RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding" class="headerlink" title="RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding"></a>RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding</h2><p><strong>Authors:Tianchen Fang, Guiru Liu</strong></p>
<p>Medical image understanding plays a crucial role in enabling automated diagnosis and data-driven clinical decision support. However, its progress is impeded by two primary challenges: the limited availability of high-quality annotated medical data and an overreliance on global image features, which often miss subtle but clinically significant pathological regions. To address these issues, we introduce RegionMed-CLIP, a region-aware multimodal contrastive learning framework that explicitly incorporates localized pathological signals along with holistic semantic representations. The core of our method is an innovative region-of-interest (ROI) processor that adaptively integrates fine-grained regional features with the global context, supported by a progressive training strategy that enhances hierarchical multimodal alignment. To enable large-scale region-level representation learning, we construct MedRegion-500k, a comprehensive medical image-text corpus that features extensive regional annotations and multilevel clinical descriptions. Extensive experiments on image-text retrieval, zero-shot classification, and visual question answering tasks demonstrate that RegionMed-CLIP consistently exceeds state-of-the-art vision language models by a wide margin. Our results highlight the critical importance of region-aware contrastive pre-training and position RegionMed-CLIP as a robust foundation for advancing multimodal medical image understanding. </p>
<blockquote>
<p>医学图像理解在促进自动化诊断和治疗决策支持中发挥着至关重要的作用。然而，其进展受到两个主要挑战的限制：高质量标注医学数据的有限可用性，以及对全局图像特征的过度依赖，这往往会导致微妙的但临床上重要的病理区域被遗漏。为了解决这些问题，我们引入了RegionMed-CLIP，这是一个区域感知的多模式对比学习框架，它显式地结合了局部病理信号和整体语义表示。我们的方法的核心是一个创新的兴趣区域（ROI）处理器，它自适应地集成了精细的局部特征与全局上下文，并得到了一种增强层次多模式对齐的渐进训练策略的支持。为了进行大规模的区域级别表示学习，我们构建了MedRegion-500k，这是一个以区域注释和多层次临床描述为特色的医学图像-文本语料库。在图像-文本检索、零样本分类和视觉问答任务上的大量实验表明，RegionMed-CLIP始终大幅超越了最先进的视觉语言模型。我们的研究结果强调了区域感知对比预训练的关键重要性，并将RegionMed-CLIP定位为推动多模式医学图像理解发展的稳健基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05244v2">PDF</a> Upon further review, we identified that our dataset requires   optimization to ensure research reliability and accuracy. Additionally,   considering the target journal’s latest submission policies, we believe   comprehensive manuscript revisions are necessary</p>
<p><strong>Summary</strong></p>
<p>RegionMed-CLIP是一种针对医疗图像理解的新方法，通过结合局部病理信号和整体语义表示，解决高质量标注医疗数据有限和过度依赖全局图像特征的问题。该方法通过自适应集成细粒度区域特征与全局上下文的核心区域处理器，以及增强层次化多模态对齐的渐进训练策略，实现了有效的学习。为支持大规模区域级别表示学习，构建了MedRegion-500k医疗图像文本语料库，包含丰富的区域标注和多层临床描述。实验表明，RegionMed-CLIP在图像文本检索、零样本分类和视觉问答任务上均大幅超越当前先进的视觉语言模型，凸显了区域感知对比预训练的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RegionMed-CLIP解决了医疗图像理解中的两大挑战：高质量标注医疗数据有限和过度依赖全局图像特征。</li>
<li>RegionMed-CLIP通过结合局部病理信号和整体语义表示，提高了医疗图像理解的准确性。</li>
<li>该方法通过自适应集成细粒度区域特征与全局上下文的区域处理器实现有效学习。</li>
<li>渐进训练策略增强了层次化多模态对齐，提高了模型的性能。</li>
<li>为支持大规模区域级别表示学习，构建了MedRegion-500k医疗图像文本语料库。</li>
<li>RegionMed-CLIP在多个任务上表现优异，证明了其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05244">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4409389a475b3efc3467f6322ecaf107~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425330&auth_key=1760425330-0-0-0f8cfc5cf8322ca22934e0b24e67bd42&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0038387d1731557539231913673eb74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425337&auth_key=1760425337-0-0-a8d6abf8a31477d4f17c2a08e03c5109&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b048fbad3f03f4ac0c39114b067eb10~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425344&auth_key=1760425344-0-0-0f159d4ed1b1f4f9ddc222c68ac23fd9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e98bdf7cef64cadde7f2ee8c5e2dc8cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425351&auth_key=1760425351-0-0-46e5778ccf493b240406ca415376cfce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a93e3776597773699674e91737962a70~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425358&auth_key=1760425358-0-0-abe1e60fd52032cbe391f256eb6f3b27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CLIPTTA-Robust-Contrastive-Vision-Language-Test-Time-Adaptation"><a href="#CLIPTTA-Robust-Contrastive-Vision-Language-Test-Time-Adaptation" class="headerlink" title="CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation"></a>CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation</h2><p><strong>Authors:Marc Lafon, Gustavo Adolfo Vargas Hakim, Clément Rambour, Christian Desrosier, Nicolas Thome</strong></p>
<p>Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities but often fail to generalize under distribution shifts. Test-time adaptation (TTA) allows models to update at inference time without labeled data, typically via entropy minimization. However, this objective is fundamentally misaligned with the contrastive image-text training of VLMs, limiting adaptation performance and introducing failure modes such as pseudo-label drift and class collapse. We propose CLIPTTA, a new gradient-based TTA method for vision-language models that leverages a soft contrastive loss aligned with CLIP’s pre-training objective. We provide a theoretical analysis of CLIPTTA’s gradients, showing how its batch-aware design mitigates the risk of collapse. We further extend CLIPTTA to the open-set setting, where both in-distribution (ID) and out-of-distribution (OOD) samples are encountered, using an Outlier Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75 datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms entropy-based objectives and is highly competitive with state-of-the-art TTA methods, outperforming them on a large number of datasets and exhibiting more stable performance across diverse shifts. </p>
<blockquote>
<p>视觉语言模型（如CLIP）展现出强大的零样本能力，但在分布变化下通常难以推广。测试时间适应（TTA）允许模型在推理时间进行无需标记数据的更新，通常通过熵最小化实现。然而，这一目标根本上与CLIP等视觉语言模型的对比图像文本训练相悖，限制了适应性能并引入了伪标签漂移和类别崩溃等失败模式。我们提出了CLIPTTA，这是一种基于梯度的新TTA方法，适用于视觉语言模型，它利用与CLIP预训练目标对齐的软对比损失。我们对CLIPTTA的梯度进行了理论分析，展示了其批量感知设计如何缓解崩溃风险。我们进一步将CLIPTTA扩展到开放集设置，在此设置中遇到的是分布内（ID）和分布外（OOD）样本，使用异常值对比曝光（OCE）损失来改善OOD检测。在涵盖多种分布变化的7.数据集上评估，CLIPTTA始终优于基于熵的目标，并在最新TTA方法中表现出高度竞争力，在大量数据集上表现优于它们并在各种变化中展现出更稳定的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14312v2">PDF</a> </p>
<p><strong>Summary</strong>：<br>本文提出一种基于梯度的测试时自适应方法CLIPTTA，用于提高视觉语言模型（如CLIP）在不同分布下的泛化能力。CLIPTTA利用与CLIP预训练目标一致的软对比损失，解决了传统熵最小化目标存在的伪标签漂移和类别崩溃问题。通过理论分析和实验验证，CLIPTTA在多种数据集上的表现均优于基于熵的目标，并与当前先进的测试时自适应方法竞争。此外，它还扩展到开放集设置，使用异常值对比曝光（OCE）损失提高异常值检测性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>CLIPTTA是一种针对视觉语言模型的测试时自适应方法，旨在提高模型在不同分布下的泛化能力。</li>
<li>CLIPTTA利用软对比损失解决伪标签漂移和类别崩溃问题，这与传统的熵最小化目标不同。</li>
<li>CLIPTTA的理论分析显示其批次感知设计能够降低类别崩溃的风险。</li>
<li>CLIPTTA扩展到开放集设置，通过引入Outlier Contrastive Exposure（OCE）损失来提高异常值检测性能。</li>
<li>在多个数据集上的实验结果表明，CLIPTTA优于基于熵的目标并与其他先进的测试时自适应方法竞争。</li>
<li>CLIPTTA在多种分布变化下的表现稳定且具有良好的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14312">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8286638fd717818e79f901f10efd80b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425366&auth_key=1760425366-0-0-0cc4e9913ab32b1f279c53820d179d22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89c86ca2211e847902091c6792246ad9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425374&auth_key=1760425374-0-0-60279c7e72a2b994ed7eccb754565f1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27d6212facaa51880c719d69069881d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425402&auth_key=1760425402-0-0-8d73288038fd68324ec8740fbe2d5c54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Test-Time-Multimodal-Backdoor-Detection-by-Contrastive-Prompting"><a href="#Test-Time-Multimodal-Backdoor-Detection-by-Contrastive-Prompting" class="headerlink" title="Test-Time Multimodal Backdoor Detection by Contrastive Prompting"></a>Test-Time Multimodal Backdoor Detection by Contrastive Prompting</h2><p><strong>Authors:Yuwei Niu, Shuo He, Qi Wei, Zongyu Wu, Feng Liu, Lei Feng</strong></p>
<p>While multimodal contrastive learning methods (e.g., CLIP) can achieve impressive zero-shot classification performance, recent research has revealed that these methods are vulnerable to backdoor attacks. To defend against backdoor attacks on CLIP, existing defense methods focus on either the pre-training stage or the fine-tuning stage, which would unfortunately cause high computational costs due to numerous parameter updates and are not applicable in black-box settings. In this paper, we provide the first attempt at a computationally efficient backdoor detection method to defend against backdoored CLIP in the \emph{inference} stage. We empirically find that the visual representations of backdoored images are \emph{insensitive} to \emph{benign} and \emph{malignant} changes in class description texts. Motivated by this observation, we propose BDetCLIP, a novel test-time backdoor detection method based on contrastive prompting. Specifically, we first prompt a language model (e.g., GPT-4) to produce class-related description texts (benign) and class-perturbed random texts (malignant) by specially designed instructions. Then, the distribution difference in cosine similarity between images and the two types of class description texts can be used as the criterion to detect backdoor samples. Extensive experiments validate that our proposed BDetCLIP is superior to state-of-the-art backdoor detection methods, in terms of both effectiveness and efficiency. Our codes are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Purshow/BDetCLIP">https://github.com/Purshow/BDetCLIP</a>. </p>
<blockquote>
<p>尽管多模态对比学习方法（例如CLIP）可以实现令人印象深刻的零样本分类性能，但最近的研究表明这些方法容易受到后门攻击的影响。为了防御CLIP的后门攻击，现有的防御方法主要关注预训练阶段或微调阶段，这会导致由于大量参数更新而产生的高计算成本，并且不适用于黑盒设置。在本文中，我们首次尝试了一种计算高效的后门检测方法，在推理阶段防御被后门控制的CLIP。我们实证发现，被后门控制的图像视觉表示对良性（benign）和恶性（malignant）变化的类别描述文本并不敏感。基于这一观察，我们提出了BDetCLIP，这是一种新的测试时间后门检测方法，基于对比提示。具体来说，我们首先通过特定设计的指令提示语言模型（例如GPT-4）生成与类别相关的描述文本（良性）和类别扰动随机文本（恶性）。然后，图像与这两种类别描述文本之间余弦相似性的分布差异可以用作检测后门样本的准则。大量实验验证了我们的BDetCLIP在有效性和效率方面优于当前先进的后门检测方法。我们的代码公开在：<a target="_blank" rel="noopener" href="https://github.com/Purshow/BDetCLIP%E3%80%82">https://github.com/Purshow/BDetCLIP。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15269v3">PDF</a> Accepted to ICML2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种针对CLIP模型在推理阶段的抗后门攻击方法BDetCLIP。研究发现，被后门攻击的图像在视觉表示上对良性及恶性文本描述变化不敏感。基于此观察，BDetCLIP利用对比提示在测试时进行后门样本检测。该方法通过语言模型产生与类别相关的良性描述和类别随机扰动下的恶性描述文本，计算图像与这两类描述文本间的余弦相似度分布差异作为检测标准。实验证实，相较于现有技术，BDetCLIP在检测效果和效率上表现优越。相关代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态对比学习方法的潜在威胁：近期研究揭示，CLIP等多媒体对比学习方法虽具备出色的零样本分类性能，但仍存在后门攻击风险。</li>
<li>现有防御策略的挑战：当前防御策略集中在预训练或微调阶段，涉及大量参数更新和高昂的计算成本，且在黑盒环境中不适用。</li>
<li>新颖的反向检测策略：首次提出在计算效率高的推理阶段进行后门检测的方法。</li>
<li>图像与文本描述间的敏感性观察：研究发现被后门攻击的图像对良性及恶性文本描述变化视觉上不敏感。</li>
<li>BDetCLIP方法介绍：基于对比提示和余弦相似度分布差异进行后门样本检测。</li>
<li>实验验证：对比实验证明BDetCLIP在检测效果与效率上超越现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15269">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-cb530a8ce239652586c06dda746fa169~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425409&auth_key=1760425409-0-0-49e6aba99da991dc6205b4fc4301f0a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-26c906f77a1e6f02f728add97f93266c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425417&auth_key=1760425417-0-0-165ae858432cb66bdb6c002301c6c341&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d027a0a0ddfc4a4f16482053e4f5f9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425446&auth_key=1760425446-0-0-f6e57bb255428fe3917b58354f28210d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-668c1b026800d7079c2cec03d224f8cf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425454&auth_key=1760425454-0-0-602c27bedf7713d815b396197eb2977f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Superpixel-Graph-Contrastive-Clustering-with-Semantic-Invariant-Augmentations-for-Hyperspectral-Images"><a href="#Superpixel-Graph-Contrastive-Clustering-with-Semantic-Invariant-Augmentations-for-Hyperspectral-Images" class="headerlink" title="Superpixel Graph Contrastive Clustering with Semantic-Invariant   Augmentations for Hyperspectral Images"></a>Superpixel Graph Contrastive Clustering with Semantic-Invariant   Augmentations for Hyperspectral Images</h2><p><strong>Authors:Jianhan Qi, Yuheng Jia, Hui Liu, Junhui Hou</strong></p>
<p>Hyperspectral images (HSI) clustering is an important but challenging task. The state-of-the-art (SOTA) methods usually rely on superpixels, however, they do not fully utilize the spatial and spectral information in HSI 3-D structure, and their optimization targets are not clustering-oriented. In this work, we first use 3-D and 2-D hybrid convolutional neural networks to extract the high-order spatial and spectral features of HSI through pre-training, and then design a superpixel graph contrastive clustering (SPGCC) model to learn discriminative superpixel representations. Reasonable augmented views are crucial for contrastive clustering, and conventional contrastive learning may hurt the cluster structure since different samples are pushed away in the embedding space even if they belong to the same class. In SPGCC, we design two semantic-invariant data augmentations for HSI superpixels: pixel sampling augmentation and model weight augmentation. Then sample-level alignment and clustering-center-level contrast are performed for better intra-class similarity and inter-class dissimilarity of superpixel embeddings. We perform clustering and network optimization alternatively. Experimental results on several HSI datasets verify the advantages of the proposed SPGCC compared to SOTA methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/jhqi/spgcc">https://github.com/jhqi/spgcc</a>. </p>
<blockquote>
<p>高光谱图像（HSI）聚类是一项重要且具有挑战性的任务。目前先进的方法通常依赖于超像素，但它们没有充分利用HSI三维结构中的空间和光谱信息，且其优化目标并非针对聚类。在这项工作中，我们首先使用三维和二维混合卷积神经网络通过预训练提取HSI的高阶空间和光谱特征，然后设计超像素图对比聚类（SPGCC）模型来学习判别超像素表示。合理的增强视图对于对比聚类至关重要，而传统的对比学习可能会损害聚类结构，因为不同样本即使在属于同一类别的情况下也会在嵌入空间中相互推开。在SPGCC中，我们为HSI超像素设计了两种语义不变的数据增强方法：像素采样增强和模型权重增强。然后对超像素嵌入进行样本级对齐和聚类中心级对比，以更好地实现类内相似性和类间差异性。我们交替执行聚类和网络优化。在多个HSI数据集上的实验结果验证了与先进方法相比，所提出SPGCC的优势。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/jhqi/spgcc%E3%80%82">https://github.com/jhqi/spgcc。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.01799v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出了基于三维和二维混合卷积神经网络的高光谱图像聚类方法。通过预训练提取高光谱图像的高阶空间和光谱特征，然后设计超像素图对比聚类模型（SPGCC），学习具有区分性的超像素表示。合理的数据增强对比聚类至关重要，传统的对比学习可能损害聚类结构，因为同一类的不同样本在嵌入空间中会被推开。在SPGCC中，为HSI超像素设计了两种语义不变的数据增强方法：像素采样增强和模型权重增强。然后进行样本级对齐和聚类中心级对比，以改善超像素嵌入的类内相似性和类间差异性。通过交替进行聚类和网络优化，在几个高光谱图像数据集上的实验结果验证了所提出的SPGCC相较于最新方法具有优势。</p>
<p><strong>要点摘要</strong></p>
<ol>
<li>提出了一种基于三维和二维混合卷积神经网络的高光谱图像聚类方法。</li>
<li>通过预训练提取高光谱图像的高阶空间和光谱特征。</li>
<li>设计了超像素图对比聚类模型（SPGCC）来学习区分性的超像素表示。</li>
<li>引入合理的数据增强对比聚类，解决了传统对比学习可能损害聚类结构的问题。</li>
<li>针对HSI超像素设计两种语义不变的数据增强方法：像素采样增强和模型权重增强。</li>
<li>实现样本级对齐和聚类中心级对比，提高超像素嵌入的类内相似性和类间差异性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.01799">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4526a930b353088910cbe17310213a8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425462&auth_key=1760425462-0-0-76f98341a940e7ba0d949100305e7c20&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-791816d2439234f0096810ca3b2f8b8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425469&auth_key=1760425469-0-0-675cfb77cac1933dc7a5623bb77f3c04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8cf97f1f97719d6c90e635a68c648826~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425476&auth_key=1760425476-0-0-66420a4bd42b258fe9074c01ae04f96c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4885cb50e7327dfefd795a7a8a377d3d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425483&auth_key=1760425483-0-0-e12e14b3de2954a51ce2e6cfe972549f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">无监督/半监督/对比学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-af3d3c2296e160340e135672856ebb89~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425489&auth_key=1760425489-0-0-eb9acc1505499bff5d4e0992469bdf35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-09-24  Accurate Thyroid Cancer Classification using a Novel Binary Pattern   Driven Local Discrete Cosine Transform Descriptor
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-f4d6ca190ffea9ea27fa48f23675a116~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425226&auth_key=1760425226-0-0-a39a72d090c2f4046b1fe751ec43fd02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-09-24  FROQ Observing Face Recognition Models for Efficient Quality Assessment
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
