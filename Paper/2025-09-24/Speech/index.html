<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Audio Super-Resolution with Latent Bridge Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-ff95bb8195609367c1e67f9d54057052~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425533&auth_key=1760425533-0-0-dedb1cbc397c38c5239c24e45f06eab6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-24-æ›´æ–°"><a href="#2025-09-24-æ›´æ–°" class="headerlink" title="2025-09-24 æ›´æ–°"></a>2025-09-24 æ›´æ–°</h1><h2 id="Audio-Super-Resolution-with-Latent-Bridge-Models"><a href="#Audio-Super-Resolution-with-Latent-Bridge-Models" class="headerlink" title="Audio Super-Resolution with Latent Bridge Models"></a>Audio Super-Resolution with Latent Bridge Models</h2><p><strong>Authors:Chang Li, Zehua Chen, Liyuan Wang, Jun Zhu</strong></p>
<p>Audio super-resolution (SR), i.e., upsampling the low-resolution (LR) waveform to the high-resolution (HR) version, has recently been explored with diffusion and bridge models, while previous methods often suffer from sub-optimal upsampling quality due to their uninformative generation prior. Towards high-quality audio super-resolution, we present a new system with latent bridge models (LBMs), where we compress the audio waveform into a continuous latent space and design an LBM to enable a latent-to-latent generation process that naturally matches the LR-toHR upsampling process, thereby fully exploiting the instructive prior information contained in the LR waveform. To further enhance the training results despite the limited availability of HR samples, we introduce frequency-aware LBMs, where the prior and target frequency are taken as model input, enabling LBMs to explicitly learn an any-to-any upsampling process at the training stage. Furthermore, we design cascaded LBMs and present two prior augmentation strategies, where we make the first attempt to unlock the audio upsampling beyond 48 kHz and empower a seamless cascaded SR process, providing higher flexibility for audio post-production. Comprehensive experimental results evaluated on the VCTK, ESC-50, Song-Describer benchmark datasets and two internal testsets demonstrate that we achieve state-of-the-art objective and perceptual quality for any-to-48kHz SR across speech, audio, and music signals, as well as setting the first record for any-to-192kHz audio SR. Demo at <a target="_blank" rel="noopener" href="https://audiolbm.github.io/">https://AudioLBM.github.io/</a>. </p>
<blockquote>
<p>éŸ³é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰å³å°†ä»ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰æ³¢å½¢ä¸Šé‡‡æ ·åˆ°é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰ç‰ˆæœ¬ï¼Œæœ€è¿‘å·²ä½¿ç”¨æ‰©æ•£æ¨¡å‹å’Œæ¡¥æ¢æ¨¡å‹è¿›è¡Œäº†æ¢ç´¢ã€‚è€Œä¹‹å‰çš„æ–¹æ³•ç”±äºå…¶ç¼ºä¹ä¿¡æ¯ç”Ÿæˆå…ˆéªŒï¼Œå¾€å¾€é­å—æ¬¡ä¼˜ä¸Šé‡‡æ ·è´¨é‡çš„å›°æ‰°ã€‚ä¸ºäº†è·å¾—é«˜è´¨é‡éŸ³é¢‘è¶…åˆ†è¾¨ç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ½œå¼æ¡¥æ¢æ¨¡å‹ï¼ˆLBMï¼‰ç³»ç»Ÿï¼Œæˆ‘ä»¬å°†éŸ³é¢‘æ³¢å½¢å‹ç¼©åˆ°è¿ç»­æ½œåœ¨ç©ºé—´ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªLBMï¼Œä»¥å®ç°æ½œåœ¨åˆ°æ½œåœ¨çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œè‡ªç„¶åœ°åŒ¹é…LRåˆ°HRçš„ä¸Šé‡‡æ ·è¿‡ç¨‹ï¼Œä»è€Œå……åˆ†åˆ©ç”¨LRæ³¢å½¢ä¸­åŒ…å«çš„æŒ‡ç¤ºæ€§å…ˆéªŒä¿¡æ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è®­ç»ƒç»“æœï¼Œå°½ç®¡é«˜è´¨é‡æ ·æœ¬çš„å¯ç”¨æ€§æœ‰é™ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥LBMï¼Œå°†å…ˆéªŒå’Œç›®æ ‡é¢‘ç‡ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œä½¿LBMèƒ½å¤Ÿåœ¨è®­ç»ƒé˜¶æ®µæ˜¾å¼åœ°å­¦ä¹ ä»»ä½•åˆ°ä»»ä½•çš„ä¸Šé‡‡æ ·è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†çº§è”LBMï¼Œå¹¶æä¾›äº†ä¸¤ç§å…ˆéªŒå¢å¼ºç­–ç•¥ï¼Œæˆ‘ä»¬é¦–æ¬¡å°è¯•è§£é”è¶…è¿‡48kHzçš„éŸ³é¢‘ä¸Šé‡‡æ ·ï¼Œå¹¶ä¸ºæ— ç¼çº§è”SRè¿‡ç¨‹æä¾›æ”¯æŒï¼Œä¸ºéŸ³é¢‘åæœŸåˆ¶ä½œæä¾›æ›´é«˜çš„çµæ´»æ€§ã€‚åœ¨VCTKã€ESC-50ã€Song-DescriberåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªå†…éƒ¨æµ‹è¯•é›†ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·çš„ä»»ä½•åˆ°48kHz SRæ–¹é¢è¾¾åˆ°äº†æœ€æ–°çš„å®¢è§‚å’Œæ„ŸçŸ¥è´¨é‡ï¼Œå¹¶é¦–æ¬¡åˆ›é€ äº†ä»»ä½•åˆ°192kHzéŸ³é¢‘SRçš„è®°å½•ã€‚æ¼”ç¤ºç½‘ç«™ä¸ºï¼š<a target="_blank" rel="noopener" href="https://audiolbm.github.io/%E3%80%82">https://AudioLBM.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17609v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong><br>éŸ³é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯è¿‘å¹´æ¥å·²ä½¿ç”¨æ‰©æ•£æ¨¡å‹å’Œæ¡¥æ¢æ¨¡å‹è¿›è¡Œäº†æ¢ç´¢ã€‚ä»¥å‰çš„æ–¹æ³•ç”±äºç”Ÿæˆå…ˆéªŒä¿¡æ¯ä¸å……è¶³ï¼Œå¾€å¾€å¯¼è‡´ä¸Šé‡‡æ ·è´¨é‡ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ½œåœ¨æ¡¥æ¢æ¨¡å‹ï¼ˆLBMsï¼‰çš„ç³»ç»Ÿï¼Œå°†éŸ³é¢‘æ³¢å½¢å‹ç¼©åˆ°è¿ç»­æ½œåœ¨ç©ºé—´ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªLBMæ¥æ‰§è¡Œæ½œåœ¨åˆ°æ½œåœ¨çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œè‡ªç„¶åœ°åŒ¹é…LRåˆ°HRçš„ä¸Šé‡‡æ ·è¿‡ç¨‹ï¼Œä»è€Œå……åˆ†åˆ©ç”¨LRæ³¢å½¢ä¸­çš„æŒ‡å¯¼æ€§å…ˆéªŒä¿¡æ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è®­ç»ƒç»“æœï¼Œå³ä½¿åœ¨é«˜åˆ†è¾¨ç‡æ ·æœ¬æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥LBMsï¼Œå°†å…ˆéªŒå’Œç›®æ ‡é¢‘ç‡ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œä½¿LBMsèƒ½å¤Ÿåœ¨è®­ç»ƒé˜¶æ®µæ˜¾å¼å­¦ä¹ ä»»ä½•åˆ°ä»»ä½•çš„ä¸Šé‡‡æ ·è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†çº§è”LBMsï¼Œå¹¶æä¾›äº†ä¸¤ç§å…ˆéªŒå¢å¼ºç­–ç•¥ã€‚æˆ‘ä»¬é¦–æ¬¡å°è¯•è§£é”è¶…è¿‡48kHzçš„éŸ³é¢‘ä¸Šé‡‡æ ·ï¼Œå¹¶å®ç°æ— ç¼çº§è”SRè¿‡ç¨‹ï¼Œä¸ºéŸ³é¢‘åæœŸåˆ¶ä½œæä¾›æ›´é«˜çš„çµæ´»æ€§ã€‚åœ¨VCTKã€ESC-50ã€Song-DescriberåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªå†…éƒ¨æµ‹è¯•é›†ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨ä»»ä½•åˆ°48kHzçš„SRæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„å®¢è§‚å’Œæ„ŸçŸ¥è´¨é‡ï¼Œå¹¶åœ¨ä»»ä½•åˆ°192kHzçš„éŸ³é¢‘SRæ–¹é¢åˆ›é€ äº†æ–°çºªå½•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ½œåœ¨æ¡¥æ¢æ¨¡å‹ï¼ˆLBMsï¼‰çš„éŸ³é¢‘è¶…åˆ†è¾¨ç‡ç³»ç»Ÿï¼Œç”¨äºå‹ç¼©éŸ³é¢‘æ³¢å½¢å¹¶æ‰§è¡Œæ½œåœ¨ç©ºé—´çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå®ç°LRåˆ°HRçš„è‡ªç„¶ä¸Šé‡‡æ ·ã€‚</li>
<li>å¼•å…¥é¢‘ç‡æ„ŸçŸ¥LBMsä»¥æé«˜è®­ç»ƒæ•ˆæœï¼Œé€šè¿‡åˆ©ç”¨å…ˆéªŒå’Œç›®æ ‡é¢‘ç‡ä¿¡æ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä»»ä½•åˆ°ä»»ä½•çš„ä¸Šé‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>è®¾è®¡äº†çº§è”LBMså’Œå…ˆéªŒå¢å¼ºç­–ç•¥ï¼Œä¸ºéŸ³é¢‘åæœŸåˆ¶ä½œæä¾›æ›´é«˜çš„çµæ´»æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•è§£é”äº†è¶…è¿‡48kHzçš„éŸ³é¢‘ä¸Šé‡‡æ ·èƒ½åŠ›ï¼Œå®ç°äº†æ— ç¼çº§è”SRè¿‡ç¨‹ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·çš„ä»»ä½•åˆ°48kHz SRæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®¢è§‚å’Œæ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•é¦–æ¬¡åœ¨ä»»ä½•åˆ°192kHzçš„éŸ³é¢‘SRæ–¹é¢åˆ›é€ äº†æ–°çºªå½•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7a08db5cb92d04a59314f0c37823a48e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425541&auth_key=1760425541-0-0-63d646fc9d47f44a7216603e317ebba5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5212b599d29df06da4a25ae5461868c7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425549&auth_key=1760425549-0-0-90ebff1ac88861b6b97f38db70df95ce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-804c5150d6f030b936a277ca31e039b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425556&auth_key=1760425556-0-0-ea8da24dea4272efed4f9b4171d66a33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21f088fbd5bb25115c5413bb438cdc48~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425562&auth_key=1760425562-0-0-614d82b402686d6e2410721d0df526f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Leveraging-Audio-Visual-Data-to-Reduce-the-Multilingual-Gap-in-Self-Supervised-Speech-Models"><a href="#Leveraging-Audio-Visual-Data-to-Reduce-the-Multilingual-Gap-in-Self-Supervised-Speech-Models" class="headerlink" title="Leveraging Audio-Visual Data to Reduce the Multilingual Gap in   Self-Supervised Speech Models"></a>Leveraging Audio-Visual Data to Reduce the Multilingual Gap in   Self-Supervised Speech Models</h2><p><strong>Authors:MarÃ­a Andrea Cruz BlandÃ³n, Zakaria Aldeneh, Jie Chi, Maureen de Seyssel</strong></p>
<p>Self-supervised learning (SSL) has made significant advances in speech representation learning. Models like wav2vec 2.0 and HuBERT have achieved state-of-the-art results in tasks such as speech recognition, particularly in monolingual settings. However, multilingual SSL models tend to underperform their monolingual counterparts on each individual language, especially in multilingual scenarios with few languages such as the bilingual setting. In this work, we investigate a novel approach to reduce this performance gap by introducing limited visual grounding into bilingual speech SSL models. Our results show that visual grounding benefits both monolingual and bilingual models, with especially pronounced gains for the latter, reducing the multilingual performance gap on zero-shot phonetic discrimination from 31.5% for audio-only models to 8.04% with grounding. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨è¯­éŸ³è¡¨å¾å­¦ä¹ æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚åƒwav2vec 2.0å’ŒHuBERTè¿™æ ·çš„æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å•è¯­ç¯å¢ƒä¸‹ã€‚ç„¶è€Œï¼Œå¤šè¯­ç§SSLæ¨¡å‹å¾€å¾€åœ¨æ¯ä¸ªå•ç‹¬çš„è¯­è¨€ä¸Šçš„è¡¨ç°ä¸å¦‚å•è¯­ç§æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒè¯­ç­‰å°‘æ•°è¯­ç§åœºæ™¯ä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§é€šè¿‡å¼•å…¥æœ‰é™è§†è§‰å®šä½æ¥å‡å°‘è¿™ç§æ€§èƒ½å·®è·çš„æ–°æ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºåŒè¯­è¯­éŸ³SSLæ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè§†è§‰å®šä½å¯¹å•è¯­ç§å’ŒåŒè¯­ç§æ¨¡å‹éƒ½æœ‰å¥½å¤„ï¼Œç‰¹åˆ«æ˜¯å¯¹åè€…æ¥è¯´ï¼Œå…¶åœ¨é›¶è¯­éŸ³å‘éŸ³è¾¨è¯†çš„å¤šè¯­ç§æ€§èƒ½å·®è·ä»åªæœ‰éŸ³é¢‘æ¨¡å‹çš„31.5%å‡å°‘åˆ°åŠ å…¥å®šä½åçš„8.04%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17523v1">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨è¯­éŸ³è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯wav2vec 2.0å’ŒHuBERTç­‰æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨å¤šè¯­è¨€SSLæ¨¡å‹ä¸­ï¼Œé’ˆå¯¹æ¯ç§è¯­è¨€çš„æ€§èƒ½å¾€å¾€ä¸åŠå•è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒè¯­ç¯å¢ƒä¸­ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥æœ‰é™çš„è§†è§‰å®šä½æ¥è§£å†³è¿™ä¸€æ€§èƒ½å·®è·é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼Œè§†è§‰å®šä½ä¸ä»…å¯¹å•è¯­å’ŒåŒè¯­æ¨¡å‹éƒ½æœ‰å¥½å¤„ï¼Œè€Œä¸”å¯¹åè€…çš„æ•ˆç›Šæ›´ä¸ºæ˜¾è‘—ï¼Œå°†ä»…ä½¿ç”¨éŸ³é¢‘æ¨¡å‹çš„é›¶å°„è¯­éŸ³è¾¨è¯†å¤šè¯­è¨€æ€§èƒ½å·®è·ä»31.5%å‡å°‘åˆ°ä½¿ç”¨å®šä½çš„8.04%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨è¯­éŸ³è¡¨ç¤ºå­¦ä¹ ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>wav2vec 2.0å’ŒHuBERTç­‰æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>å¤šè¯­è¨€SSLæ¨¡å‹åœ¨æ¯ç§è¯­è¨€ä¸Šçš„è¡¨ç°é€šå¸¸ä¸åŠå•è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åœ¨åŒè¯­è¯­éŸ³SSLæ¨¡å‹ä¸­å¼•å…¥æœ‰é™çš„è§†è§‰å®šä½å¯ä»¥å‡å°‘æ€§èƒ½å·®è·ã€‚</li>
<li>è§†è§‰å®šä½å¯¹å•è¯­å’ŒåŒè¯­æ¨¡å‹éƒ½æœ‰å¥½å¤„ã€‚</li>
<li>è§†è§‰å®šä½å¯¹å‡å°‘é›¶å°„è¯­éŸ³è¾¨è¯†å¤šè¯­è¨€æ€§èƒ½å·®è·æ•ˆæœæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dfa62a2ea8cf510193d985640de18658~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425570&auth_key=1760425570-0-0-2707c0078250720e81a7bbc74a2a71ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5556ab3624916ec8531b9544477727d9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425577&auth_key=1760425577-0-0-868fe4ec6c057359f9b30cd82faf93f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a9bf70efa4da85a2e95bc446fc02ac7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425583&auth_key=1760425583-0-0-04024cff9f6ebdefc44f3c1697742fa9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7969c4a42c05b2f0d1903d034b9752bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425591&auth_key=1760425591-0-0-dd3dc8bddbba070255fa8b303ced6e94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="STAR-Speech-to-Audio-Generation-via-Representation-Learning"><a href="#STAR-Speech-to-Audio-Generation-via-Representation-Learning" class="headerlink" title="STAR: Speech-to-Audio Generation via Representation Learning"></a>STAR: Speech-to-Audio Generation via Representation Learning</h2><p><strong>Authors:Zeyu Xie, Xuenan Xu, Yixuan Li, Mengyue Wu, Yuexian Zou</strong></p>
<p>This work presents STAR, the first end-to-end speech-to-audio generation framework, designed to enhance efficiency and address error propagation inherent in cascaded systems. Unlike prior approaches relying on text or vision, STAR leverages speech as it constitutes a natural modality for interaction. As an initial step to validate the feasibility of the system, we demonstrate through representation learning experiments that spoken sound event semantics can be effectively extracted from raw speech, capturing both auditory events and scene cues. Leveraging the semantic representations, STAR incorporates a bridge network for representation mapping and a two-stage training strategy to achieve end-to-end synthesis. With a 76.9% reduction in speech processing latency, STAR demonstrates superior generation performance over the cascaded systems. Overall, STAR establishes speech as a direct interaction signal for audio generation, thereby bridging representation learning and multimodal synthesis. Generated samples are available at <a target="_blank" rel="noopener" href="https://zeyuxie29.github.io/STAR">https://zeyuxie29.github.io/STAR</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†STARï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°éŸ³é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å¹¶è§£å†³çº§è”ç³»ç»Ÿä¸­å›ºæœ‰çš„é”™è¯¯ä¼ æ’­é—®é¢˜ã€‚ä¸åŒäºä»¥å¾€ä¾èµ–æ–‡æœ¬æˆ–è§†è§‰çš„æ–¹æ³•ï¼ŒSTARåˆ©ç”¨è¯­éŸ³ä½œä¸ºäº¤äº’çš„è‡ªç„¶æ¨¡å¼ã€‚ä½œä¸ºéªŒè¯ç³»ç»Ÿå¯è¡Œæ€§çš„åˆæ­¥æ­¥éª¤ï¼Œæˆ‘ä»¬é€šè¿‡è¡¨ç¤ºå­¦ä¹ å®éªŒè¯æ˜ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»åŸå§‹è¯­éŸ³ä¸­æå–è¯­éŸ³å£°éŸ³äº‹ä»¶è¯­ä¹‰ï¼Œæ•è·å¬è§‰äº‹ä»¶å’Œåœºæ™¯çº¿ç´¢ã€‚åˆ©ç”¨è¯­ä¹‰è¡¨ç¤ºï¼ŒSTARç»“åˆäº†ä¸€ä¸ªç”¨äºè¡¨ç¤ºæ˜ å°„çš„æ¡¥æ¢ç½‘ç»œä»¥åŠä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥å®ç°ç«¯åˆ°ç«¯çš„åˆæˆã€‚STARå°†è¯­éŸ³å¤„ç†å»¶è¿Ÿå‡å°‘äº†76.9%ï¼Œåœ¨çº§è”ç³»ç»Ÿä¸Šå±•ç°äº†æ›´ä¼˜è¶Šç”Ÿæˆæ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼ŒSTARç¡®ç«‹äº†è¯­éŸ³ä½œä¸ºéŸ³é¢‘ç”Ÿæˆçš„ç›´æ¥äº¤äº’ä¿¡å·ï¼Œä»è€Œè¿æ¥äº†è¡¨ç¤ºå­¦ä¹ å’Œå¤šæ¨¡æ€åˆæˆã€‚ç”Ÿæˆçš„æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://zeyuxie29.github.io/STAR%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://zeyuxie29.github.io/STARä¸ŠæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17164v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬é¡¹ç›®æå‡ºäº†STARï¼Œé¦–ä¸ªç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°éŸ³é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å’Œè§£å†³çº§è”ç³»ç»Ÿä¸­çš„è¯¯å·®ä¼ æ’­é—®é¢˜ã€‚ä¸åŒäºä¾èµ–æ–‡æœ¬æˆ–è§†è§‰çš„å…ˆå‰æ–¹æ³•ï¼ŒSTARåˆ©ç”¨è¯­éŸ³ä½œä¸ºäº¤äº’çš„è‡ªç„¶åª’ä»‹ã€‚é€šè¿‡è¡¨ç¤ºå­¦ä¹ å®éªŒéªŒè¯äº†ç³»ç»Ÿçš„å¯è¡Œæ€§ï¼Œå¯æœ‰æ•ˆæå–å£è¯­å£°éŸ³äº‹ä»¶è¯­ä¹‰ï¼ŒåŒ…æ‹¬å¬è§‰äº‹ä»¶å’Œåœºæ™¯çº¿ç´¢ã€‚å€ŸåŠ©è¯­ä¹‰è¡¨ç¤ºï¼ŒSTARæ„å»ºäº†ä¸€ä¸ªæ¡¥æ¢ç½‘ç»œç”¨äºè¡¨ç¤ºæ˜ å°„ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å®ç°ç«¯åˆ°ç«¯åˆæˆã€‚ç›¸è¾ƒäºçº§è”ç³»ç»Ÿï¼ŒSTARå°†è¯­éŸ³å¤„ç†å»¶è¿Ÿé™ä½äº†76.9%ï¼Œå±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼ŒSTARç¡®ç«‹äº†è¯­éŸ³ä½œä¸ºéŸ³é¢‘ç”Ÿæˆçš„ç›´æ¥äº¤äº’ä¿¡å·ï¼Œä»è€Œå®ç°äº†è¡¨ç¤ºå­¦ä¹ å’Œå¤šæ¨¡æ€åˆæˆçš„æ¡¥æ¢ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>STARæ˜¯é¦–ä¸ªç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°éŸ³é¢‘ç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨æé«˜æ•ˆç‡å’Œè§£å†³çº§è”ç³»ç»Ÿä¸­çš„è¯¯å·®ä¼ æ’­é—®é¢˜ã€‚</li>
<li>ä¸ä¾èµ–æ–‡æœ¬æˆ–è§†è§‰çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒSTARåˆ©ç”¨è¯­éŸ³çš„è‡ªç„¶äº¤äº’ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡è¡¨ç¤ºå­¦ä¹ å®éªŒéªŒè¯äº†ç³»ç»Ÿçš„å¯è¡Œæ€§ã€‚</li>
<li>ç³»ç»Ÿå¯ä»¥æå–å£è¯­å£°éŸ³äº‹ä»¶è¯­ä¹‰ï¼ŒåŒ…æ‹¬å¬è§‰äº‹ä»¶å’Œåœºæ™¯çº¿ç´¢ã€‚</li>
<li>STARåˆ©ç”¨æ¡¥æ¢ç½‘ç»œå’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å®ç°é«˜æ•ˆçš„ç«¯åˆ°ç«¯åˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-579639b4a0265bac671a9f1aa7ffb56a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425633&auth_key=1760425633-0-0-a9506d832aea38ca4c00985737a0ae7c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea5e25864df2bfd6a133db4161043f79~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425641&auth_key=1760425641-0-0-e11dd9c4b328b06a87ca165e7f9a9131&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5218fabe45c0c9c75058378d41869cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425647&auth_key=1760425647-0-0-071961b01db6d303b011f32c06bfa9b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b08412a15a83139e109dba953ac9c62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425688&auth_key=1760425688-0-0-4fa9dc515fca878ae6585a6fa6665ac4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Leveraging-Multiple-Speech-Enhancers-for-Non-Intrusive-Intelligibility-Prediction-for-Hearing-Impaired-Listeners"><a href="#Leveraging-Multiple-Speech-Enhancers-for-Non-Intrusive-Intelligibility-Prediction-for-Hearing-Impaired-Listeners" class="headerlink" title="Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility   Prediction for Hearing-Impaired Listeners"></a>Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility   Prediction for Hearing-Impaired Listeners</h2><p><strong>Authors:Boxuan Cao, Linkai Li, Hanlin Yu, Changgeng Mo, Haoshuai Zhou, Shan Xiang Wang</strong></p>
<p>Speech intelligibility evaluation for hearing-impaired (HI) listeners is essential for assessing hearing aid performance, traditionally relying on listening tests or intrusive methods like HASPI. However, these methods require clean reference signals, which are often unavailable in real-world conditions, creating a gap between lab-based and real-world assessments. To address this, we propose a non-intrusive intelligibility prediction framework that leverages speech enhancers to provide a parallel enhanced-signal pathway, enabling robust predictions without reference signals. We evaluate three state-of-the-art enhancers and demonstrate that prediction performance depends on the choice of enhancer, with ensembles of strong enhancers yielding the best results. To improve cross-dataset generalization, we introduce a 2-clips augmentation strategy that enhances listener-specific variability, boosting robustness on unseen datasets. Our approach consistently outperforms the non-intrusive baseline, CPC2 Champion across multiple datasets, highlighting the potential of enhancer-guided non-intrusive intelligibility prediction for real-world applications. </p>
<blockquote>
<p>é’ˆå¯¹å¬åŠ›å—æŸï¼ˆHIï¼‰å¬ä¼—çš„è¯­éŸ³æ¸…æ™°åº¦è¯„ä¼°å¯¹äºè¯„ä¼°åŠ©å¬å™¨æ€§èƒ½è‡³å…³é‡è¦ï¼Œä¼ ç»Ÿä¸Šä¾èµ–äºå¬åŠ›æµ‹è¯•æˆ–åƒHASPIè¿™æ ·çš„ä¾µå…¥æ€§æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¹²å‡€çš„å‚è€ƒä¿¡å·ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„æ¡ä»¶ä¸‹å¾€å¾€ä¸å¯ç”¨ï¼Œä»è€Œåœ¨åŸºäºå®éªŒå®¤å’Œç°å®ä¸–ç•Œè¯„ä¼°ä¹‹é—´å­˜åœ¨å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éä¾µå…¥æ€§çš„æ¸…æ™°åº¦é¢„æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è¯­éŸ³å¢å¼ºå™¨æä¾›å¹¶è¡Œå¢å¼ºä¿¡å·é€šè·¯ï¼Œä»è€Œåœ¨ä¸ä½¿ç”¨å‚è€ƒä¿¡å·çš„æƒ…å†µä¸‹è¿›è¡Œç¨³å¥çš„é¢„æµ‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§æœ€å…ˆè¿›çš„å¢å¼ºå™¨ï¼Œå¹¶è¯æ˜é¢„æµ‹æ€§èƒ½å–å†³äºå¢å¼ºå™¨çš„é€‰æ‹©ï¼Œå¼ºå¤§çš„å¢å¼ºå™¨ç»„åˆå–å¾—äº†æœ€ä½³ç»“æœã€‚ä¸ºäº†æé«˜è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§2å‰ªè¾‘å¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¢å¼ºäº†å¬ä¼—ç‰¹å®šçš„å·®å¼‚æ€§ï¼Œåœ¨æœªè§è¿‡æ•°æ®é›†ä¸Šæé«˜äº†é²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºéä¾µå…¥æ€§åŸºçº¿CPC2å† å†›ï¼Œçªæ˜¾äº†å¢å¼ºå™¨å¼•å¯¼çš„éä¾µå…¥æ€§æ¸…æ™°åº¦é¢„æµ‹åœ¨ç°å®åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16979v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¬åŠ›éšœç¢è€…çš„è¯­éŸ³æ¸…æ™°åº¦è¯„ä¼°å¯¹äºè¯„ä¼°åŠ©å¬å™¨æ€§èƒ½è‡³å…³é‡è¦ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¬åŠ›æµ‹è¯•æˆ–HASPIç­‰ä¾µå…¥æ€§æ–¹æ³•ï¼Œä½†ç°å®ç¯å¢ƒä¸­å¾€å¾€æ— æ³•è·å¾—æ¸…æ™°çš„å‚è€ƒä¿¡å·ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§éä¾µå…¥æ€§çš„æ¸…æ™°åº¦é¢„æµ‹æ¡†æ¶ï¼Œåˆ©ç”¨è¯­éŸ³å¢å¼ºå™¨æä¾›å¹¶è¡Œå¢å¼ºä¿¡å·è·¯å¾„ï¼Œæ— éœ€å‚è€ƒä¿¡å·å³å¯è¿›è¡Œç¨³å¥é¢„æµ‹ã€‚è¯„ä¼°äº†ä¸‰ç§æœ€æ–°å¢å¼ºå™¨å¹¶å‘ç°é¢„æµ‹æ€§èƒ½å–å†³äºå¢å¼ºå™¨çš„é€‰æ‹©ï¼Œå¼ºå¢å¼ºå™¨ç»„åˆè¡¨ç°æœ€ä½³ã€‚ä¸ºæé«˜è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œå¼•å…¥äº†ä¸€ç§2ç‰‡æ®µå¢å¼ºç­–ç•¥ï¼Œå¢å¼ºå¬è€…ç‰¹å¼‚æ€§å˜åŒ–ï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šæé«˜ç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºéä¾µå…¥æ€§åŸºçº¿CPC2å† å†›ï¼Œçªæ˜¾äº†å¢å¼ºå™¨å¼•å¯¼çš„éä¾µå…¥æ€§æ¸…æ™°åº¦é¢„æµ‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¬åŠ›éšœç¢è€…çš„è¯­éŸ³æ¸…æ™°åº¦è¯„ä¼°æ˜¯è¯„ä¼°åŠ©å¬å™¨æ€§èƒ½çš„å…³é”®ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ä¾èµ–æ¸…æ™°å‚è€ƒä¿¡å·ï¼Œä½†åœ¨ç°å®ç¯å¢ƒä¸­å¸¸å¸¸æ— æ³•è·å¾—ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§éä¾µå…¥æ€§çš„æ¸…æ™°åº¦é¢„æµ‹æ¡†æ¶ï¼Œåˆ©ç”¨è¯­éŸ³å¢å¼ºå™¨è¿›è¡Œç¨³å¥é¢„æµ‹ï¼Œæ— éœ€å‚è€ƒä¿¡å·ã€‚</li>
<li>é¢„æµ‹æ€§èƒ½å–å†³äºæ‰€é€‰æ‹©çš„è¯­éŸ³å¢å¼ºå™¨ï¼Œå¼ºå¢å¼ºå™¨ç»„åˆè¡¨ç°æœ€ä½³ã€‚</li>
<li>ä¸ºæé«˜è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œé‡‡ç”¨äº†2ç‰‡æ®µå¢å¼ºç­–ç•¥ï¼Œå¢å¼ºäº†å¬è€…ç‰¹å¼‚æ€§å˜åŒ–ã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰éä¾µå…¥æ€§æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-863aa328dc2bf3100a1c326605a7c352~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425696&auth_key=1760425696-0-0-eadffcc5315dcd8ae0d320ed6d74f190&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8742102adda5bee8b15f159ce839212e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425778&auth_key=1760425778-0-0-cd004a8184fbc791afe2b6168b14ea11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46deececdec4ac081394ccc98cb824b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425785&auth_key=1760425785-0-0-0330b14cdd9bb59929fcf90d2831636d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d30a6717830d0b5174e6aff5b5610b35~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425793&auth_key=1760425793-0-0-5c604ddddcdffe93071f28b7b95b11da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4bd1f5379072f3873b857d2b7e0fff1f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425799&auth_key=1760425799-0-0-bd7b78e3b0561d29bc0eba94fff6d346&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Idiosyncratic-Versus-Normative-Modeling-of-Atypical-Speech-Recognition-Dysarthric-Case-Studies"><a href="#Idiosyncratic-Versus-Normative-Modeling-of-Atypical-Speech-Recognition-Dysarthric-Case-Studies" class="headerlink" title="Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition:   Dysarthric Case Studies"></a>Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition:   Dysarthric Case Studies</h2><p><strong>Authors:Vishnu Raja, Adithya V Ganesan, Anand Syamkumar, Ritwik Banerjee, H Andrew Schwartz</strong></p>
<p>State-of-the-art automatic speech recognition (ASR) models like Whisper, perform poorly on atypical speech, such as that produced by individuals with dysarthria. Past works for atypical speech have mostly investigated fully personalized (or idiosyncratic) models, but modeling strategies that can both generalize and handle idiosyncracy could be more effective for capturing atypical speech. To investigate this, we compare four strategies: (a) $\textit{normative}$ models trained on typical speech (no personalization), (b) $\textit{idiosyncratic}$ models completely personalized to individuals, (c) $\textit{dysarthric-normative}$ models trained on other dysarthric speakers, and (d) $\textit{dysarthric-idiosyncratic}$ models which combine strategies by first modeling normative patterns before adapting to individual speech. In this case study, we find the dysarthric-idiosyncratic model performs better than idiosyncratic approach while requiring less than half as much personalized data (36.43 WER with 128 train size vs 36.99 with 256). Further, we found that tuning the speech encoder alone (as opposed to the LM decoder) yielded the best results reducing word error rate from 71% to 32% on average. Our findings highlight the value of leveraging both normative (cross-speaker) and idiosyncratic (speaker-specific) patterns to improve ASR for underrepresented speech populations. </p>
<blockquote>
<p>æœ€å…ˆè¿›çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œå¦‚Whisperï¼Œåœ¨éå…¸å‹è¯­éŸ³ä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œæ¯”å¦‚å‘éŸ³éšœç¢è€…äº§ç”Ÿçš„è¯­éŸ³ã€‚è¿‡å»å¯¹äºéå…¸å‹è¯­éŸ³çš„ç ”ç©¶å¤§å¤šé›†ä¸­åœ¨å®Œå…¨ä¸ªæ€§åŒ–çš„ï¼ˆæˆ–ç‰¹æ®Šçš„ï¼‰æ¨¡å‹ä¸Šï¼Œä½†èƒ½å¤ŸåŒæ—¶å®ç°é€šç”¨æ€§å’Œå¤„ç†ç‰¹æ®Šæ€§çš„å»ºæ¨¡ç­–ç•¥å¯¹äºæ•æ‰éå…¸å‹è¯­éŸ³å¯èƒ½æ›´ä¸ºæœ‰æ•ˆã€‚ä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å››ç§ç­–ç•¥ï¼šï¼ˆaï¼‰åœ¨å…¸å‹è¯­éŸ³ä¸Šè®­ç»ƒçš„$\textit{è§„èŒƒæ€§}$æ¨¡å‹ï¼ˆæ— ä¸ªæ€§åŒ–ï¼‰ï¼Œï¼ˆbï¼‰å®Œå…¨ä¸ªæ€§åŒ–åˆ°ä¸ªä½“çš„$\textit{ç‰¹æ®Šæ€§}$æ¨¡å‹ï¼Œï¼ˆcï¼‰åœ¨å…¶ä»–å‘éŸ³éšœç¢è€…èº«ä¸Šè®­ç»ƒçš„$\textit{å‘éŸ³éšœç¢è§„èŒƒæ€§}$æ¨¡å‹ï¼Œä»¥åŠï¼ˆdï¼‰ç»“åˆä¸¤ç§ç­–ç•¥çš„$\textit{å‘éŸ³éšœç¢ç‰¹æ®Šæ€§}$æ¨¡å‹ï¼Œé¦–å…ˆå»ºç«‹è§„èŒƒæ€§æ¨¡å¼ï¼Œç„¶åé€‚åº”ä¸ªäººè¯­éŸ³ã€‚åœ¨æœ¬ä¸ªæ¡ˆç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‘ç°å‘éŸ³éšœç¢ç‰¹æ®Šæ€§æ¨¡å‹åœ¨è¡¨ç°ä¸Šä¼˜äºç‰¹æ®Šæ€§æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ‰€éœ€ä¸ªæ€§åŒ–æ•°æ®ä¸åˆ°ä¸€åŠçš„æƒ…å†µä¸‹å³å¯è¾¾åˆ°æ­¤æ•ˆæœï¼ˆè®­ç»ƒé›†å¤§å°ä¸º128æ—¶çš„WERä¸º36.43%ï¼Œè€Œè®­ç»ƒé›†å¤§å°ä¸º256æ—¶ä¸º36.99%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä»…è°ƒæ•´è¯­éŸ³ç¼–ç å™¨ï¼ˆè€Œä¸æ˜¯LMè§£ç å™¨ï¼‰å¾—åˆ°äº†æœ€ä½³ç»“æœï¼Œå¹³å‡å°†å•è¯é”™è¯¯ç‡ä»71%é™ä½åˆ°32%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨è§„èŒƒæ€§ï¼ˆè·¨å‘è¨€äººï¼‰å’Œç‰¹æ®Šæ€§ï¼ˆç‰¹å®šå‘è¨€äººï¼‰æ¨¡å¼æ¥æé«˜ä»£è¡¨æ€§ä¸è¶³çš„è¯­éŸ³ç¾¤ä½“çš„ASRä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16718v1">PDF</a> Will appear in EMNLP 2025 Main Proceedings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸åŒæ¨¡å‹ç­–ç•¥åœ¨å¯¹ä¸å…¸å‹è¯­éŸ³å¦‚å‘éŸ³éšœç¢äººç¾¤çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸Šçš„è¡¨ç°ã€‚å¯¹æ¯”äº†å››ç§ç­–ç•¥ï¼Œå‘ç°ç»“åˆè§„èŒƒæ€§å’Œä¸ªæ€§åŒ–ç‰¹å¾çš„æ¨¡å‹ç­–ç•¥è¡¨ç°æœ€å¥½ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ ·æœ¬éœ€æ±‚é‡ä¸Šä¹Ÿæ›´çœã€‚æœ€ä½³ç»“æœæ˜¯é€šè¿‡è°ƒæ•´è¯­éŸ³ç¼–ç å™¨å®ç°çš„ï¼Œæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ã€‚ç ”ç©¶å¼ºè°ƒäº†ç»“åˆè·¨è¯´è¯äººçš„è§„èŒƒæ¨¡å¼å’Œè¯´è¯äººç‰¹å®šçš„ä¸ªæ€§åŒ–æ¨¡å¼åœ¨æé«˜ä»£è¡¨æ€§ä¸è¶³çš„è¯­éŸ³ç¾¤ä½“çš„ASRä»·å€¼ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤„ç†ä¸å…¸å‹è¯­éŸ³å¦‚å‘éŸ³éšœç¢äººç¾¤çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ—¶ï¼Œç°æœ‰æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚</li>
<li>å¯¹æ¯”äº†å››ç§ä¸åŒçš„æ¨¡å‹ç­–ç•¥ï¼ŒåŒ…æ‹¬è§„èŒƒæ€§æ¨¡å‹ã€ä¸ªæ€§åŒ–æ¨¡å‹ã€å…¶ä»–å‘éŸ³éšœç¢è€…çš„æ¨¡å‹ä»¥åŠç»“åˆè§„èŒƒæ€§å’Œä¸ªæ€§åŒ–çš„æ¨¡å‹ã€‚</li>
<li>ç»“åˆè§„èŒƒæ€§å’Œä¸ªæ€§åŒ–ç‰¹å¾çš„æ¨¡å‹è¡¨ç°æœ€å¥½ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ ·æœ¬éœ€æ±‚é‡ä¸Šæ›´çœã€‚</li>
<li>è°ƒæ•´è¯­éŸ³ç¼–ç å™¨æ˜¯è·å¾—æœ€ä½³ç»“æœçš„å…³é”®æ­¥éª¤ï¼Œæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œå¯¹äºä»£è¡¨æ€§ä¸è¶³çš„è¯­éŸ³ç¾¤ä½“ï¼Œç»“åˆè·¨è¯´è¯äººçš„è§„èŒƒæ¨¡å¼å’Œè¯´è¯äººç‰¹å®šçš„ä¸ªæ€§åŒ–æ¨¡å¼æœ‰åŠ©äºæé«˜ASRçš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹äºæ”¹å–„ä¸å…¸å‹è¯­éŸ³çš„ASRå…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ï¼Œæœ‰åŠ©äºæ¨åŠ¨ASRæŠ€æœ¯åœ¨æ›´å¤šåœºæ™¯å’Œäººç¾¤ä¸­çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-59daca70dc1b61720c6226cc2b216fc9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425806&auth_key=1760425806-0-0-f7db2314db54dd2443455260e5ebf2de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7de4fc25469408e417665a2f6481fc85~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425814&auth_key=1760425814-0-0-2778ea873175bc2b208ce209203ff039&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0f2f2f71d4d14b68953105b1873deac~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425821&auth_key=1760425821-0-0-91ba95259329ffc3884aca749af0c908&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Audio-Conditioned-Diffusion-LLMs-for-ASR-and-Deliberation-Processing"><a href="#Audio-Conditioned-Diffusion-LLMs-for-ASR-and-Deliberation-Processing" class="headerlink" title="Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing"></a>Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing</h2><p><strong>Authors:Mengqi Wang, Zhan Liu, Zengrui Jin, Guangzhi Sun, Chao Zhang, Philip C. Woodland</strong></p>
<p>Diffusion-based large language models (DLLMs) have recently attracted growing interest as an alternative to autoregressive decoders. In this work, we present an empirical study on using the diffusion-based large language model LLaDA for automatic speech recognition (ASR). We first investigate its use as an external deliberation-based processing module for Whisper-LLaMA transcripts. By leveraging the bidirectional attention and denoising capabilities of LLaDA, we explore random masking, low-confidence masking, and semi-autoregressive strategies, showing that Whisper-LLaDA substantially reduces WER compared with the baseline. On LibriSpeech, the best cascade system achieves 2.25%&#x2F;4.94% WER on test-clean&#x2F;test-other, representing a 12.3% relative improvement over the Whisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA without acoustic features fails to improve accuracy, highlighting the importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA as a standalone decoder for ASR with diffusion-based and semi-autoregressive decoding. Most experimental configurations achieve faster inference than the Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These findings offer an empirical view of diffusion-based LLMs for ASR and point to promising directions for improvements. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆDLLMsï¼‰æœ€è¿‘ä½œä¸ºè‡ªå›å½’è§£ç å™¨çš„æ›¿ä»£æ–¹æ¡ˆè€Œå¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨åŸºäºæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹LLaDAè¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆç ”ç©¶äº†å…¶åœ¨åŸºäºå¤–éƒ¨æ·±æ€å¤„ç†çš„æ¨¡å—Whisper-LLaMAè½¬å½•ä¸­çš„åº”ç”¨ã€‚é€šè¿‡åˆ©ç”¨LLaDAçš„åŒå‘æ³¨æ„åŠ›å’Œé™å™ªèƒ½åŠ›ï¼Œæˆ‘ä»¬æ¢ç´¢äº†éšæœºæ©è”½ã€ä½ç½®ä¿¡æ©è”½å’ŒåŠè‡ªå›å½’ç­–ç•¥ï¼Œè¡¨æ˜Whisper-LLaDAä¸åŸºçº¿ç›¸æ¯”å¤§å¹…é™ä½äº†WERã€‚åœ¨LibriSpeechä¸Šï¼Œæœ€ä½³çº§è”ç³»ç»Ÿåœ¨æµ‹è¯•æ¸…æ´&#x2F;æµ‹è¯•å…¶ä»–æƒ…å†µä¸‹çš„WERè¾¾åˆ°2.25%&#x2F;4.94%ï¼Œç›¸å¯¹äºWhisper-LLaMAåŸºçº¿åœ¨æµ‹è¯•å…¶ä»–åˆ†å‰²ä¸Šå®ç°äº†12.3%çš„ç›¸å¯¹æ”¹è¿›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ²¡æœ‰å£°å­¦ç‰¹å¾çš„çº¯æ–‡æœ¬LLaDAæœªèƒ½æé«˜å‡†ç¡®æ€§ï¼Œä»è€Œçªå‡ºäº†éŸ³é¢‘æ¡ä»¶åµŒå…¥çš„é‡è¦æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯„ä¼°äº†å°†Whisper-LLaDAä½œä¸ºç‹¬ç«‹äºASRçš„æ‰©æ•£å’ŒåŠè‡ªå›å½’è§£ç çš„è§£ç å™¨ã€‚å¤§å¤šæ•°å®éªŒé…ç½®å®ç°äº†æ¯”Whisper-LLaMAåŸºçº¿æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå°½ç®¡è¯†åˆ«å‡†ç¡®ç‡ç•¥æœ‰ä¸‹é™ã€‚è¿™äº›å‘ç°ä¸ºåŸºäºæ‰©æ•£çš„LLMåœ¨ASRæ–¹é¢çš„åº”ç”¨æä¾›äº†å®è¯è§‚ç‚¹ï¼Œå¹¶æŒ‡å‡ºäº†æ”¹è¿›çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16622v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ‰©æ•£æ¨¡å‹è¯­è¨€æ¨¡å‹ï¼ˆDLLMsï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„åº”ç”¨è¿‘æœŸå¤‡å—å…³æ³¨ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨LLaDAæ¨¡å‹è¿›è¡ŒASRçš„å®è¯æƒ…å†µã€‚é€šè¿‡ä½œä¸ºå¤–éƒ¨æ€è€ƒå¤„ç†æ¨¡å—åº”ç”¨äºWhisper-LLaMAè½¬å½•ï¼Œåˆ©ç”¨LLaDAçš„åŒå‘æ³¨æ„åŠ›å’Œå»å™ªèƒ½åŠ›ï¼Œç ”ç©¶éšæœºæ©ç ã€ä½ç½®ä¿¡æ©ç å’ŒåŠè‡ªå›å½’ç­–ç•¥ã€‚ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼ŒWhisper-LLaDAå¤§å¹…å‡å°‘äº†WERã€‚åœ¨LibriSpeechä¸Šï¼Œæœ€ä½³çº§è”ç³»ç»Ÿæµ‹è¯•æ¸…æ´&#x2F;å…¶ä»–æµ‹è¯•é›†è¾¾åˆ°2.25%&#x2F;4.94%çš„WERï¼Œç›¸è¾ƒäºWhisper-LLaMAåŸºçº¿æ¨¡å‹åœ¨å…¶ä»–æµ‹è¯•é›†ä¸Šå®ç°12.3%çš„ç›¸å¯¹æ”¹è¿›ã€‚ç„¶è€Œï¼Œä¸ä½¿ç”¨å£°å­¦ç‰¹å¾çš„çº¯æ–‡æœ¬LLaDAæ¨¡å‹æ— æ³•æé«˜å‡†ç¡®æ€§ï¼Œå‡¸æ˜¾éŸ³é¢‘æ¡ä»¶åµŒå…¥çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œå°†Whisper-LLaDAä½œä¸ºç‹¬ç«‹çš„ASRè§£ç å™¨è¿›è¡Œè¯„ä¼°ï¼Œå¤§å¤šæ•°å®éªŒé…ç½®å®ç°äº†æ¯”Whisper-LLaMAåŸºçº¿æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå°½ç®¡è¯†åˆ«å‡†ç¡®ç‡ç•¥æœ‰ä¸‹é™ã€‚æœ¬ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹åœ¨ASRä¸­çš„åº”ç”¨æä¾›äº†å®è¯è§‚ç‚¹ï¼Œå¹¶ä¸ºæ”¹è¿›æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLaDAæ¨¡å‹ä½œä¸ºå¤–éƒ¨æ€è€ƒå¤„ç†æ¨¡å—ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚</li>
<li>LLaDAçš„åŒå‘æ³¨æ„åŠ›å’Œå»å™ªèƒ½åŠ›åœ¨éšæœºæ©ç ã€ä½ç½®ä¿¡æ©ç å’ŒåŠè‡ªå›å½’ç­–ç•¥ä¸­å¾—åˆ°æ¢ç´¢å’Œåº”ç”¨ã€‚</li>
<li>Whisper-LLaDAåœ¨LibriSpeechä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>æœ€å¥½çš„çº§è”ç³»ç»Ÿå®ç°äº†å¯¹æµ‹è¯•æ¸…æ´å’Œå…¶ä»–æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æå‡ã€‚</li>
<li>çº¯æ–‡æœ¬LLaDAæ¨¡å‹æœªèƒ½æé«˜å‡†ç¡®æ€§ï¼Œå¼ºè°ƒéŸ³é¢‘æ¡ä»¶åµŒå…¥çš„é‡è¦æ€§ã€‚</li>
<li>Whisper-LLaDAä½œä¸ºç‹¬ç«‹çš„ASRè§£ç å™¨è¡¨ç°è‰¯å¥½ï¼Œå®éªŒé…ç½®é€šå¸¸å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4763d1492f2b054fa7b29ad08b88ccb0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425829&auth_key=1760425829-0-0-897f1534d71de7224e2e938e713a1e2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-676f451fc950dc066994d0bd7dab4683~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425837&auth_key=1760425837-0-0-94842fc1d0b29f4350544ff7fd38b478&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5af576cacd857cc23f3a3b763f36df56~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425843&auth_key=1760425843-0-0-0df04dccf1c9b0ef49cdce7d5a4daf3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e05e8329ebf3f8b915e8b152ce16eec2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425850&auth_key=1760425850-0-0-1918ca2facd40c36025b44098b70d900&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Investigating-Polyglot-Speech-Foundation-Models-for-Learning-Collective-Emotion-from-Crowds"><a href="#Investigating-Polyglot-Speech-Foundation-Models-for-Learning-Collective-Emotion-from-Crowds" class="headerlink" title="Investigating Polyglot Speech Foundation Models for Learning Collective   Emotion from Crowds"></a>Investigating Polyglot Speech Foundation Models for Learning Collective   Emotion from Crowds</h2><p><strong>Authors:Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Panchal Nayak, Priyabrata Mallick, Swarup Ranjan Behera, Parabattina Bhagath, Pailla Balakrishna Reddy, Arun Balaji Buduru</strong></p>
<p>This paper investigates the polyglot (multilingual) speech foundation models (SFMs) for Crowd Emotion Recognition (CER). We hypothesize that polyglot SFMs, pre-trained on diverse languages, accents, and speech patterns, are particularly adept at navigating the noisy and complex acoustic environments characteristic of crowd settings, thereby offering a significant advantage for CER. To substantiate this, we perform a comprehensive analysis, comparing polyglot, monolingual, and speaker recognition SFMs through extensive experiments on a benchmark CER dataset across varying audio durations (1 sec, 500 ms, and 250 ms). The results consistently demonstrate the superiority of polyglot SFMs, outperforming their counterparts across all audio lengths and excelling even with extremely short-duration inputs. These findings pave the way for adaptation of SFMs in setting up new benchmarks for CER. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†ç”¨äºç¾¤ä½“æƒ…ç»ªè¯†åˆ«ï¼ˆCERï¼‰çš„å¤šè¯­ç§ï¼ˆå¤šè¯­è¨€ï¼‰è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰ã€‚æˆ‘ä»¬å‡è®¾åœ¨å¤šç§è¯­è¨€ã€å£éŸ³å’Œè¯­éŸ³æ¨¡å¼ä¸Šé¢„å…ˆè®­ç»ƒçš„å¤šè¯­ç§SFMç‰¹åˆ«æ“…é•¿å¤„ç†ç¾¤ä½“è®¾ç½®ä¸­æ‰€ç‰¹æœ‰çš„å˜ˆæ‚å’Œå¤æ‚çš„å£°å­¦ç¯å¢ƒï¼Œä»è€Œä¸ºCERæä¾›æ˜¾è‘—ä¼˜åŠ¿ã€‚ä¸ºäº†è¯å®è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œé€šè¿‡åŸºå‡†CERæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒå¯¹å¤šè¯­ç§ã€å•è¯­ç§å’Œè¯´è¯äººè¯†åˆ«SFMè¿›è¡Œäº†æ¯”è¾ƒï¼Œå®éªŒæ¶‰åŠä¸åŒçš„éŸ³é¢‘æŒç»­æ—¶é—´ï¼ˆå¦‚1ç§’ã€500æ¯«ç§’å’Œ250æ¯«ç§’ï¼‰ã€‚ç»“æœä¸€è‡´è¡¨æ˜å¤šè¯­ç§SFMçš„ä¼˜è¶Šæ€§ï¼Œåœ¨æ‰€æœ‰éŸ³é¢‘é•¿åº¦ä¸Šå‡ä¼˜äºå…¶åŒç±»æ¨¡å‹ï¼Œå³ä½¿åœ¨æçŸ­çš„è¾“å…¥æ—¶é—´å†…ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚è¿™äº›å‘ç°ä¸ºSFMåœ¨å»ºç«‹æ–°çš„CERåŸºå‡†æ–¹é¢æä¾›äº†æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16329v1">PDF</a> Accepted to APSIPA-ASC 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šè¯­è¨€è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰åœ¨ç¾¤ä½“æƒ…ç»ªè¯†åˆ«ï¼ˆCERï¼‰ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‡è®¾åŸºäºå¤šç§è¯­è¨€ã€å£éŸ³å’Œè¯­éŸ³æ¨¡å¼è¿›è¡Œé¢„è®­ç»ƒçš„å¤šè¯­è¨€SFMsåœ¨å¤„ç†å…·æœ‰å™ªéŸ³å’Œå¤æ‚ç‰¹æ€§çš„ç¾¤ä½“ç¯å¢ƒæ—¶å…·æœ‰ä¼˜åŠ¿ï¼Œå› æ­¤ç‰¹åˆ«é€‚åˆCERã€‚é€šè¿‡åŸºå‡†CERæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼Œå¯¹æ¯”å¤šè¯­è¨€ã€å•è¯­å’Œè¯­éŸ³è¯†åˆ«SFMsåœ¨ä¸åŒéŸ³é¢‘æ—¶é•¿ï¼ˆ1ç§’ã€500æ¯«ç§’å’Œ250æ¯«ç§’ï¼‰çš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºå¤šè¯­è¨€SFMsè¡¨ç°å“è¶Šï¼Œåœ¨æ‰€æœ‰éŸ³é¢‘é•¿åº¦ä¸Šå‡ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå³ä½¿åœ¨æçŸ­çš„è¾“å…¥æ—¶é•¿ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚è¿™ä¸ºSFMsåœ¨è®¾ç«‹æ–°çš„CERåŸºå‡†ä¸Šæä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ç ”ç©¶äº†å¤šè¯­è¨€è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰åœ¨ç¾¤ä½“æƒ…ç»ªè¯†åˆ«ï¼ˆCERï¼‰é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>å‡è®¾å¤šè¯­è¨€SFMsèƒ½å¾ˆå¥½åœ°å¤„ç†å…·æœ‰å™ªéŸ³å’Œå¤æ‚ç‰¹æ€§çš„ç¾¤ä½“ç¯å¢ƒï¼Œå¯¹CERæœ‰ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†å¤šè¯­è¨€SFMsåœ¨åŸºå‡†CERæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå¯¹æ¯”äº†å¤šè¯­è¨€ã€å•è¯­å’Œè¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šè¯­è¨€SFMsåœ¨æ‰€æœ‰éŸ³é¢‘é•¿åº¦ä¸Šå‡è¡¨ç°å“è¶Šï¼Œå°¤å…¶åœ¨æçŸ­çš„è¾“å…¥æ—¶é•¿ä¸­ã€‚</li>
<li>å¤šè¯­è¨€SFMsçš„ä¼˜è¶Šæ€§ä½“ç°åœ¨å…¶èƒ½å¤Ÿè¯†åˆ«ä¸åŒè¯­è¨€å’Œå£éŸ³çš„è¯­éŸ³æ¨¡å¼ã€‚</li>
<li>ç ”ç©¶ä¸ºSFMsåœ¨è®¾ç«‹æ–°çš„CERåŸºå‡†ä¸Šæä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b3be2eff4b71f018e37aa9e7b57680ed~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425859&auth_key=1760425859-0-0-e7762e976cfc6fe9d4309bb15c804951&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d0cfaacf67344c3459e637721de3fdc9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425866&auth_key=1760425866-0-0-20a30b49c9ac1e50f5be67fbae737df4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f1346e1ee3eff2e6e7b045eeb6bbd58~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425874&auth_key=1760425874-0-0-c794bc012ee6dbb84e8850227f8546d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-599595d20f48e60a94e38676e5f1d756~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425881&auth_key=1760425881-0-0-71027e4623f3c1d54588be60344db724&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-333bb3f0e6856e5b7f88262d308ff561~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425888&auth_key=1760425888-0-0-69fdf229e02fa65adede7a6939630bbe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ded8b388b88e3f8d91ffcd78411631ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425894&auth_key=1760425894-0-0-1e2700ae25c7748cb063f2e04a3ab03d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FocalCodec-Stream-Streaming-Low-Bitrate-Speech-Coding-via-Causal-Distillation"><a href="#FocalCodec-Stream-Streaming-Low-Bitrate-Speech-Coding-via-Causal-Distillation" class="headerlink" title="FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal   Distillation"></a>FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal   Distillation</h2><p><strong>Authors:Luca Della Libera, Cem Subakan, Mirco Ravanelli</strong></p>
<p>Neural audio codecs are a fundamental component of modern generative audio pipelines. Although recent codecs achieve strong low-bitrate reconstruction and provide powerful representations for downstream tasks, most are non-streamable, limiting their use in real-time applications. We present FocalCodec-Stream, a hybrid codec based on focal modulation that compresses speech into a single binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our approach combines multi-stage causal distillation of WavLM with targeted architectural improvements, including a lightweight refiner module that enhances quality under latency constraints. Experiments show that FocalCodec-Stream outperforms existing streamable codecs at comparable bitrates, while preserving both semantic and acoustic information. The result is a favorable trade-off between reconstruction quality, downstream task performance, latency, and efficiency. Code and checkpoints will be released at <a target="_blank" rel="noopener" href="https://github.com/lucadellalib/focalcodec">https://github.com/lucadellalib/focalcodec</a>. </p>
<blockquote>
<p>ç¥ç»éŸ³é¢‘ç¼–ç æ˜¯ç°ä»£ç”ŸæˆéŸ³é¢‘ç®¡é“çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡æœ€è¿‘çš„ç¼–ç æŠ€æœ¯å®ç°äº†å¼ºå¤§çš„ä½æ¯”ç‰¹ç‡é‡å»ºï¼Œå¹¶ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›äº†å¼ºå¤§çš„è¡¨ç°ï¼Œä½†å¤§å¤šæ•°éƒ½æ˜¯éæµå¼çš„ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†FocalCodec-Streamï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç„¦ç‚¹è°ƒåˆ¶çš„æ··åˆç¼–ç æŠ€æœ¯ï¼Œå®ƒå°†è¯­éŸ³å‹ç¼©åˆ°ä¸€ä¸ªå•ä¸€çš„äºŒè¿›åˆ¶ç¼–ç æœ¬ä¸­ï¼Œä»¥0.55-0.8 kbpsçš„æ¯”ç‰¹ç‡è¿è¡Œï¼Œç†è®ºå»¶è¿Ÿä¸º80æ¯«ç§’ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤šé˜¶æ®µå› æœè’¸é¦çš„WavLMæŠ€æœ¯ï¼Œå¹¶é’ˆå¯¹æ¶æ„è¿›è¡Œäº†æ”¹è¿›ï¼ŒåŒ…æ‹¬ä¸€ä¸ªè½»é‡çº§çš„ç²¾ç‚¼æ¨¡å—ï¼Œåœ¨å»¶è¿Ÿé™åˆ¶ä¸‹æé«˜è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¯æ¯”çš„æ¯”ç‰¹ç‡ä¸‹ï¼ŒFocalCodec-Streamçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æµå¼ç¼–ç æŠ€æœ¯ï¼ŒåŒæ—¶ä¿ç•™äº†è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯ã€‚å…¶ç»“æœæ˜¯é‡å»ºè´¨é‡ã€ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€å»¶è¿Ÿå’Œæ•ˆç‡ä¹‹é—´çš„æœ‰åˆ©æƒè¡¡ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lucadellalib/focalcodec%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/lucadellalib/focalcodecå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16195v1">PDF</a> 5 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–è§£ç å™¨æ˜¯ç°ä»£ç”Ÿæˆå¼éŸ³é¢‘ç®¡é“çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚ç°æœ‰ç¼–è§£ç å™¨è™½ç„¶èƒ½å¤Ÿåœ¨ä½ç ç‡ä¸‹å®ç°é«˜è´¨é‡çš„é‡å»ºï¼Œå¹¶ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›å¼ºå¤§çš„è¡¨å¾ï¼Œä½†å¤§å¤šéæµå¼ä¼ è¾“ï¼Œé™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç„¦ç‚¹è°ƒåˆ¶çš„æ··åˆç¼–è§£ç å™¨FocalCodec-Streamï¼Œä»¥0.55~0.8kbpsçš„ç ç‡å°†è¯­éŸ³å‹ç¼©åˆ°ä¸€ä¸ªå•ä¸€çš„äºŒè¿›åˆ¶ä»£ç ç°¿ä¸­ï¼Œç†è®ºå»¶è¿Ÿä¸º80æ¯«ç§’ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤šé˜¶æ®µå› æœè’¸é¦çš„WavLMæŠ€æœ¯ï¼Œå¹¶è¿›è¡Œäº†æœ‰é’ˆå¯¹æ€§çš„æ¶æ„æ”¹è¿›ï¼ŒåŒ…æ‹¬ä¸€ä¸ªè½»é‡çº§çš„ç²¾ç‚¼æ¨¡å—ï¼Œä»¥æé«˜åœ¨å»¶è¿Ÿé™åˆ¶ä¸‹çš„è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒFocalCodec-Streamåœ¨å¯æ¯”çš„ç ç‡ä¸‹ä¼˜äºç°æœ‰çš„æµå¼ç¼–è§£ç å™¨ï¼ŒåŒæ—¶ä¿ç•™äº†è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯ã€‚ç»“æœæ˜¯åœ¨é‡å»ºè´¨é‡ã€ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€å»¶è¿Ÿå’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†æœ‰åˆ©çš„å¹³è¡¡ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lucadellalib/focalcodec%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/lucadellalib/focalcodecå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–è§£ç å™¨åœ¨ç°ä»£ç”Ÿæˆå¼éŸ³é¢‘ç®¡é“ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>ç°æœ‰ç¼–è§£ç å™¨è™½ç„¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†å¤§å¤šæ•°ä¸é€‚ç”¨äºå®æ—¶æµå¼ä¼ è¾“åº”ç”¨ã€‚</li>
<li>FocalCodec-Streamæ˜¯ä¸€ç§åŸºäºç„¦ç‚¹è°ƒåˆ¶çš„æ··åˆç¼–è§£ç å™¨ï¼Œå¯åœ¨ä½ç ç‡ä¸‹å®ç°è¯­éŸ³å‹ç¼©ã€‚</li>
<li>FocalCodec-Streamå…·æœ‰ç†è®ºå»¶è¿Ÿä½çš„ä¼˜ç‚¹ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å¤šé˜¶æ®µå› æœè’¸é¦æŠ€æœ¯å’Œé’ˆå¯¹æ€§çš„æ¶æ„æ”¹è¿›ã€‚</li>
<li>FocalCodec-Streamåœ¨æ¯”è¾ƒå®éªŒä¸­è¡¨ç°å‡ºä¼˜è‰¯çš„æ€§èƒ½ï¼Œä¼˜äºç°æœ‰çš„æµå¼ç¼–è§£ç å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16195">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-38a55d1cc4ef4135a954d88c7008939d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425903&auth_key=1760425903-0-0-a8f58aa0c8d825b885b404df819bca86&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff9280e5b23e129c5f32bf44d9589573~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425911&auth_key=1760425911-0-0-0d2e700cdad62de90be32c96fe75481c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-74d7ec4f10f3277372f040ffabfbfd8d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425918&auth_key=1760425918-0-0-b09decb1a23d972cc3bf6cd4ca51378a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-18ca2b2d1d2ded0e8739099fe356ec87~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425925&auth_key=1760425925-0-0-c8557bcdb6a86bb64bba720d303c5cf9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Are-Multimodal-Foundation-Models-All-That-Is-Needed-for-Emofake-Detection"><a href="#Are-Multimodal-Foundation-Models-All-That-Is-Needed-for-Emofake-Detection" class="headerlink" title="Are Multimodal Foundation Models All That Is Needed for Emofake   Detection?"></a>Are Multimodal Foundation Models All That Is Needed for Emofake   Detection?</h2><p><strong>Authors:Mohd Mujtaba Akhtar,  Girish, Orchid Chetia Phukan, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru</strong></p>
<p>In this work, we investigate multimodal foundation models (MFMs) for EmoFake detection (EFD) and hypothesize that they will outperform audio foundation models (AFMs). MFMs due to their cross-modal pre-training, learns emotional patterns from multiple modalities, while AFMs rely only on audio. As such, MFMs can better recognize unnatural emotional shifts and inconsistencies in manipulated audio, making them more effective at distinguishing real from fake emotional expressions. To validate our hypothesis, we conduct a comprehensive comparative analysis of state-of-the-art (SOTA) MFMs (e.g. LanguageBind) alongside AFMs (e.g. WavLM). Our experiments confirm that MFMs surpass AFMs for EFD. Beyond individual foundation models (FMs) performance, we explore FMs fusion, motivated by findings in related research areas such synthetic speech detection and speech emotion recognition. To this end, we propose SCAR, a novel framework for effective fusion. SCAR introduces a nested cross-attention mechanism, where representations from FMs interact at two stages sequentially to refine information exchange. Additionally, a self-attention refinement module further enhances feature representations by reinforcing important cross-FM cues while suppressing noise. Through SCAR with synergistic fusion of MFMs, we achieve SOTA performance, surpassing both standalone FMs and conventional fusion approaches and previous works on EFD. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”¨äºæƒ…ç»ªä¼ªé€ æ£€æµ‹ï¼ˆEFDï¼‰çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆMFMsï¼‰ï¼Œå¹¶å‡è®¾å®ƒä»¬å°†ä¼˜äºéŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆAFMsï¼‰ã€‚MFMsç”±äºå…¶è·¨æ¨¡æ€çš„é¢„è®­ç»ƒï¼Œå¯ä»¥ä»å¤šä¸ªæ¨¡æ€å­¦ä¹ æƒ…æ„Ÿæ¨¡å¼ï¼Œè€ŒAFMsä»…ä¾èµ–äºéŸ³é¢‘ã€‚å› æ­¤ï¼ŒMFMsèƒ½å¤Ÿæ›´å¥½åœ°è¯†åˆ«æ“çºµéŸ³é¢‘ä¸­çš„ä¸è‡ªç„¶æƒ…æ„Ÿå˜åŒ–å’Œä¸ä¸€è‡´æ€§ï¼Œä½¿å…¶åœ¨åŒºåˆ†çœŸå®å’Œè™šå‡æƒ…æ„Ÿè¡¨è¾¾æ–¹é¢æ›´åŠ æœ‰æ•ˆã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„å‡è®¾ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚LanguageBindï¼‰å’ŒéŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚WavLMï¼‰è¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒåˆ†æã€‚æˆ‘ä»¬çš„å®éªŒè¯å®ï¼Œå¯¹äºEFDï¼ŒMFMsçš„è¡¨ç°ä¼˜äºAFMsã€‚é™¤äº†å•ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„æ€§èƒ½å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†åŸºç¡€æ¨¡å‹çš„èåˆï¼Œè¿™å—åˆ°äº†åˆæˆè¯­éŸ³æ£€æµ‹å’Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç­‰ç›¸å…³ç ”ç©¶é¢†åŸŸçš„å‘ç°çš„å¯å‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SCARï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„èåˆæ¡†æ¶ã€‚SCARå¼•å…¥äº†ä¸€ç§åµŒå¥—çš„äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œå…¶ä¸­æ¥è‡ªåŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºåœ¨ä¸¤ä¸ªé˜¶æ®µä¸Šé¡ºåºäº¤äº’ï¼Œä»¥ç²¾ç‚¼ä¿¡æ¯äº¤æ¢ã€‚æ­¤å¤–ï¼Œè‡ªæ³¨æ„ç²¾ç‚¼æ¨¡å—é€šè¿‡åŠ å¼ºé‡è¦çš„è·¨åŸºç¡€æ¨¡å‹çº¿ç´¢å¹¶æŠ‘åˆ¶å™ªå£°ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡SCARååŒèåˆMFMsï¼Œæˆ‘ä»¬è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å•ç‹¬çš„FMså’Œä¼ ç»Ÿèåˆæ–¹æ³•ä»¥åŠä¹‹å‰åœ¨EFDä¸Šçš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16193v1">PDF</a> Accepted to APSIPA-ASC 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆMFMsï¼‰åœ¨EmoFakeæ£€æµ‹ï¼ˆEFDï¼‰ä¸­çš„åº”ç”¨ï¼Œå¹¶å‡è®¾å®ƒä»¬çš„è¡¨ç°ä¼šä¼˜äºéŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆAFMsï¼‰ã€‚MFMsç”±äºè·¨æ¨¡æ€é¢„è®­ç»ƒï¼Œèƒ½ä»å¤šç§æ¨¡æ€å­¦ä¹ æƒ…æ„Ÿæ¨¡å¼ï¼Œè€ŒAFMsä»…ä¾èµ–éŸ³é¢‘ã€‚å› æ­¤ï¼ŒMFMsèƒ½æ›´å¥½åœ°è¯†åˆ«æ“çºµéŸ³é¢‘ä¸­çš„ä¸è‡ªç„¶æƒ…æ„Ÿå˜åŒ–å’Œä¸ä¸€è‡´æ€§ï¼Œæ›´æœ‰æ•ˆåœ°åŒºåˆ†çœŸå®å’Œè™šå‡çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚ä¸ºéªŒè¯å‡è®¾ï¼Œä½œè€…å¯¹æœ€æ–°MFMsï¼ˆå¦‚LanguageBindï¼‰å’ŒAFMsï¼ˆå¦‚WavLMï¼‰è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒåˆ†æã€‚å®éªŒè¯å®MFMsåœ¨EFDæ–¹é¢è¶…è¶ŠAFMsã€‚é™¤äº†å•ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„æ€§èƒ½å¤–ï¼Œä½œè€…è¿˜æ¢è®¨äº†FMsèåˆï¼Œå—åˆ°ç›¸å…³ç ”ç©¶é¢†åŸŸå¦‚åˆæˆè¯­éŸ³æ£€æµ‹å’Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«å‘ç°çš„å¯å‘ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†SCARï¼Œä¸€ç§æœ‰æ•ˆçš„èåˆæ¡†æ¶ã€‚SCARå¼•å…¥äº†ä¸€ç§åµŒå¥—äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œå…¶ä¸­æ¥è‡ªFMsçš„è¡¨ç¤ºåœ¨ä¸¤ä¸ªé˜¶æ®µé¡ºåºäº¤äº’ï¼Œä»¥æ”¹è¿›ä¿¡æ¯äº¤æ¢ã€‚æ­¤å¤–ï¼Œè‡ªæ³¨æ„ç»†åŒ–æ¨¡å—é€šè¿‡åŠ å¼ºé‡è¦çš„è·¨FMçº¿ç´¢åŒæ—¶æŠ‘åˆ¶å™ªå£°ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡SCARååŒèåˆMFMsï¼Œä½œè€…å®ç°äº†æœ€æ–°æ€§èƒ½ï¼Œè¶…è¶Šäº†å•ç‹¬çš„FMså’Œä¼ ç»Ÿèåˆæ–¹æ³•ä»¥åŠä¹‹å‰çš„EFDå·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆMFMsï¼‰åœ¨EmoFakeæ£€æµ‹ä¸­çš„åº”ç”¨ï¼Œå¹¶å‡è®¾å®ƒä»¬ç›¸è¾ƒäºéŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆAFMsï¼‰ä¼šæœ‰æ›´å¥½çš„è¡¨ç°ã€‚</li>
<li>MFMsç”±äºèƒ½è·¨æ¨¡æ€å­¦ä¹ æƒ…æ„Ÿæ¨¡å¼ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è¯†åˆ«ä¸è‡ªç„¶æƒ…æ„Ÿå˜åŒ–å’Œæ“çºµéŸ³é¢‘ä¸­çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†MFMsåœ¨EFDæ–¹é¢çš„æ€§èƒ½è¶…è¶Šäº†AFMsã€‚</li>
<li>é™¤äº†å•ä¸ªåŸºç¡€æ¨¡å‹æ€§èƒ½å¤–ï¼Œè¿˜æ¢è®¨äº†åŸºç¡€æ¨¡å‹èåˆï¼Œå—åˆ°åˆæˆè¯­éŸ³æ£€æµ‹å’Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶çš„å¯å‘ã€‚</li>
<li>æå‡ºäº†SCARï¼Œä¸€ç§æœ‰æ•ˆçš„èåˆæ¡†æ¶ï¼Œé€šè¿‡åµŒå¥—äº¤å‰æ³¨æ„æœºåˆ¶å’Œè‡ªæ³¨æ„ç»†åŒ–æ¨¡å—å®ç°ç‰¹å¾è¡¨ç¤ºçš„å¢å¼ºå’Œä¿¡æ¯çš„ç²¾ç»†äº¤æ¢ã€‚</li>
<li>SCARååŒèåˆMFMsè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè¶…è¶Šäº†å•ç‹¬çš„FMsã€ä¼ ç»Ÿèåˆæ–¹æ³•ä»¥åŠä¹‹å‰çš„EFDå·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bf310f8b8ba0d5eb866df09a1c44002e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425933&auth_key=1760425933-0-0-70379cf9b6c42fcbf117ef81ffd2d004&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-adcc082842a2841b87b5578f072dff51~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425940&auth_key=1760425940-0-0-ec6667e9cb3cedc1318f529507e8e374&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-827623dcab9a2ed1480e355cb0d23630~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425948&auth_key=1760425948-0-0-87ba2252e51bd378a78e26a544a0a814&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Rethinking-Cross-Corpus-Speech-Emotion-Recognition-Benchmarking-Are-Paralinguistic-Pre-Trained-Representations-Sufficient"><a href="#Rethinking-Cross-Corpus-Speech-Emotion-Recognition-Benchmarking-Are-Paralinguistic-Pre-Trained-Representations-Sufficient" class="headerlink" title="Rethinking Cross-Corpus Speech Emotion Recognition Benchmarking: Are   Paralinguistic Pre-Trained Representations Sufficient?"></a>Rethinking Cross-Corpus Speech Emotion Recognition Benchmarking: Are   Paralinguistic Pre-Trained Representations Sufficient?</h2><p><strong>Authors:Orchid Chetia Phukan, Mohd Mujtaba Akhtar,  Girish, Swarup Ranjan Behera, Parabattina Bhagath, Pailla Balakrishna Reddy, Arun Balaji Buduru</strong></p>
<p>Recent benchmarks evaluating pre-trained models (PTMs) for cross-corpus speech emotion recognition (SER) have overlooked PTM pre-trained for paralinguistic speech processing (PSP), raising concerns about their reliability, since SER is inherently a paralinguistic task. We hypothesize that PSP-focused PTM will perform better in cross-corpus SER settings. To test this, we analyze state-of-the-art PTMs representations including paralinguistic, monolingual, multilingual, and speaker recognition. Our results confirm that TRILLsson (a paralinguistic PTM) outperforms others, reinforcing the need to consider PSP-focused PTMs in cross-corpus SER benchmarks. This study enhances benchmark trustworthiness and guides PTMs evaluations for reliable cross-corpus SER. </p>
<blockquote>
<p>æœ€è¿‘å¯¹è·¨è¯­æ–™åº“è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPTMï¼‰çš„åŸºå‡†æµ‹è¯•å¿½ç•¥äº†é’ˆå¯¹å‰¯è¯­è¨€è¯­éŸ³å¤„ç†ï¼ˆPSPï¼‰çš„PTMé¢„è®­ç»ƒï¼Œè¿™å¼•å‘äº†å¯¹å…¶å¯é æ€§çš„æ‹…å¿§ï¼Œå› ä¸ºSERæœ¬è´¨ä¸Šæ˜¯ä¸€é¡¹å‰¯è¯­è¨€ä»»åŠ¡ã€‚æˆ‘ä»¬å‡è®¾ä»¥PSPä¸ºé‡ç‚¹çš„PTMå°†åœ¨è·¨è¯­æ–™åº“SERè®¾ç½®ä¸­è¡¨ç°æ›´å¥½ã€‚ä¸ºäº†æµ‹è¯•è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ†æäº†åŒ…æ‹¬å‰¯è¯­è¨€ã€å•è¯­ã€å¤šè¯­å’Œè¯­éŸ³è¯†åˆ«åœ¨å†…çš„æœ€æ–°PTMè¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç»“æœè¯å®ï¼ŒTRILLssonï¼ˆä¸€ç§å‰¯è¯­è¨€PTMï¼‰è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¿™å¼ºåŒ–äº†éœ€è¦åœ¨è·¨è¯­æ–™åº“SERåŸºå‡†æµ‹è¯•ä¸­è€ƒè™‘ä»¥PSPä¸ºé‡ç‚¹çš„PTMçš„å¿…è¦æ€§ã€‚æœ¬ç ”ç©¶æé«˜äº†åŸºå‡†æµ‹è¯•çš„å¯é æ€§ï¼Œå¹¶ä¸ºå¯é çš„è·¨è¯­æ–™åº“SERæä¾›äº†PTMè¯„ä¼°æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16182v1">PDF</a> Accepted to APSIPA-ASC 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPTMsï¼‰åœ¨è·¨è¯­æ–™åº“è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„è¡¨ç°ï¼Œå¹¶æŒ‡å‡ºå½“å‰åŸºå‡†æµ‹è¯•å¿½ç•¥äº†é’ˆå¯¹å‰¯è¯­è¨€è¯­éŸ³å¤„ç†ï¼ˆPSPï¼‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™å¼•å‘äº†å¯¹å…¶å¯é æ€§çš„æ‹…å¿§ã€‚ç ”ç©¶å‡è®¾é’ˆå¯¹PSPçš„PTMåœ¨è·¨è¯­æ–™åº“SERè®¾ç½®ä¸­è¡¨ç°æ›´å¥½ã€‚é€šè¿‡å¯¹æ¯”åˆ†æåŒ…æ‹¬å‰¯è¯­è¨€ã€å•è¯­ã€å¤šè¯­å’Œè¯­éŸ³è¯†åˆ«åœ¨å†…çš„æœ€æ–°PTMè¡¨ç¤ºï¼Œå‘ç°TRILLssonï¼ˆä¸€ç§å‰¯è¯­è¨€PTMï¼‰è¡¨ç°æœ€ä½³ï¼Œå¼ºè°ƒåœ¨è·¨è¯­æ–™åº“SERåŸºå‡†æµ‹è¯•ä¸­éœ€è¦è€ƒè™‘é’ˆå¯¹PSPçš„PTMã€‚è¯¥ç ”ç©¶æé«˜äº†åŸºå‡†æµ‹è¯•çš„å¯é æ€§ï¼Œå¹¶ä¸ºPTMsåœ¨è·¨è¯­æ–™åº“SERä¸­çš„è¯„ä¼°æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPTMsï¼‰è¿›è¡Œè·¨è¯­æ–™åº“è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ—¶ï¼Œå¿½è§†äº†é’ˆå¯¹å‰¯è¯­è¨€è¯­éŸ³å¤„ç†ï¼ˆPSPï¼‰çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å‡è®¾æŒ‡å‡ºï¼Œé’ˆå¯¹PSPçš„PTMåœ¨è·¨è¯­æ–™åº“SERè®¾ç½®ä¸­çš„è¡¨ç°ä¼šæ›´å¥½ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”åˆ†æï¼Œå‘ç°TRILLssonï¼ˆä¸€ç§å‰¯è¯­è¨€PTMï¼‰åœ¨è·¨è¯­æ–™åº“SERä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨è¿›è¡Œè·¨è¯­æ–™åº“SERçš„åŸºå‡†æµ‹è¯•æ—¶ï¼Œéœ€è¦è€ƒè™‘å‰¯è¯­è¨€è¯­éŸ³å¤„ç†çš„é‡è¦æ€§ã€‚</li>
<li>æ­¤ç ”ç©¶æé«˜äº†åŸºå‡†æµ‹è¯•çš„å¯é æ€§ï¼Œä¸ºè¯„ä¼°PTMsåœ¨è·¨è¯­æ–™åº“SERä¸­çš„æ€§èƒ½æä¾›äº†æ›´å‡†ç¡®çš„æŒ‡å¯¼ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ï¼Œé¼“åŠ±æ›´å¤šç ”ç©¶è€…å…³æ³¨å‰¯è¯­è¨€è¯­éŸ³å¤„ç†åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ff95bb8195609367c1e67f9d54057052~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425956&auth_key=1760425956-0-0-45380008fef660c7896b796099ade672&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b61d3e6fdfbe88716b2e3d894785c53~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425965&auth_key=1760425965-0-0-2c64e45ce5df30a0bd588fde901faa0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f58223a7c3cb176df087a3bd7706b30f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425972&auth_key=1760425972-0-0-c5a5ee5254964f79f448d3e2f27f7ce3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Think-Verbalize-then-Speak-Bridging-Complex-Thoughts-and-Comprehensible-Speech"><a href="#Think-Verbalize-then-Speak-Bridging-Complex-Thoughts-and-Comprehensible-Speech" class="headerlink" title="Think, Verbalize, then Speak: Bridging Complex Thoughts and   Comprehensible Speech"></a>Think, Verbalize, then Speak: Bridging Complex Thoughts and   Comprehensible Speech</h2><p><strong>Authors:Sang Hoon Woo, Sehun Lee, Kang-wook Kim, Gunhee Kim</strong></p>
<p>Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose Think-Verbalize-Speak, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is verbalizing, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce ReVerT, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a target="_blank" rel="noopener" href="https://yhytoto12.github.io/TVS-ReVerT">https://yhytoto12.github.io/TVS-ReVerT</a> </p>
<blockquote>
<p>å£è¯­å¯¹è¯ç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥åˆ©ç”¨å®ƒä»¬çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥å°†LLMåº”ç”¨äºå£è¯­äº¤æµå¾€å¾€ä¼šäº§ç”Ÿä¸ç†æƒ³çš„ç»“æœï¼Œè¿™æ˜¯ç”±äºæœ€ä½³æ–‡æœ¬è¾“å‡ºå’Œå£è¯­è¡¨è¾¾ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ã€‚è™½ç„¶ç°æœ‰çš„æ–¹æ³•ä½¿LLMé€‚åº”äº§ç”Ÿå‹å¥½çš„å£è¯­è¾“å‡ºï¼Œä½†å®ƒä»¬å¯¹æ¨ç†æ€§èƒ½çš„å½±å“ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Think-Verbalize-Speakæ¡†æ¶ï¼Œé€šè¿‡å°†æ¨ç†ä¸å£è¯­è¡¨è¾¾åˆ†ç¦»æ¥ä¿ç•™LLMçš„å®Œæ•´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯â€œå£è¯­åŒ–â€ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†æ€æƒ³ç¿»è¯‘æˆè‡ªç„¶ã€å‡†å¤‡å£è¯­çš„æ–‡æœ¬çš„ä¸­é—´æ­¥éª¤ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ReVerTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¢é‡å’Œå¼‚æ­¥æ‘˜è¦çš„å»¶è¿Ÿæ•ˆç‡é«˜çš„å£è¯­åŒ–å·¥å…·ã€‚è·¨å¤šä¸ªåŸºå‡†çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¸å½±å“æ¨ç†çš„æƒ…å†µä¸‹æé«˜å£è¯­çš„è‡ªç„¶åº¦å’Œç®€æ´æ€§ã€‚æ•°æ®é›†å’Œé¡¹ç›®é¡µé¢ä»¥åŠæºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://yhytoto12.github.io/TVS-ReVerT%E6%9F%A5%E7%9C%8B%E3%80%82">https://yhytoto12.github.io/TVS-ReVerTæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16028v1">PDF</a> EMNLP 2025 Main. Project page: <a target="_blank" rel="noopener" href="https://yhytoto12.github.io/TVS-ReVerT">https://yhytoto12.github.io/TVS-ReVerT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†ç›´æ¥åº”ç”¨LLMsåœ¨å£è¯­äº¤æµä¸­çš„ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†Think-Verbalize-Speakæ¡†æ¶ï¼Œå°†æ¨ç†ä¸å£è¯­è¡¨è¾¾åˆ†ç¦»ï¼Œä»¥ä¿ç•™LLMsçš„å®Œæ•´æ¨ç†èƒ½åŠ›ã€‚å…¶ä¸­ï¼Œæ ¸å¿ƒæ˜¯â€œè¨€è¯­åŒ–â€è¿™ä¸€ä¸­é—´æ­¥éª¤ï¼Œå°†æ€æƒ³è½¬åŒ–ä¸ºè‡ªç„¶ã€é€‚åˆå£è¯­çš„æ–‡æœ¬ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†åŸºäºå¢é‡å’Œå¼‚æ­¥æ‘˜è¦çš„å»¶è¿Ÿæ•ˆç‡é«˜çš„è¨€è¯­åŒ–å·¥å…·ReVerTã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜å£è¯­è‡ªç„¶æ€§å’Œç®€æ´æ€§çš„åŒæ—¶ï¼Œå¯¹æ¨ç†èƒ½åŠ›çš„å½±å“è¾ƒå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†ç›´æ¥åº”ç”¨äºå£è¯­äº¤æµä¼šäº§ç”Ÿä¸ç†æƒ³çš„ç»“æœã€‚</li>
<li>ç°æœ‰æ–¹æ³•è™½ç„¶å°è¯•å°†LLMsé€‚åº”äºå£è¯­è¾“å‡ºï¼Œä½†å…¶å¯¹æ¨ç†æ€§èƒ½çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Think-Verbalize-Speakæ¡†æ¶ï¼Œå°†æ¨ç†ä¸å£è¯­è¡¨è¾¾åˆ†ç¦»ï¼Œä»¥å……åˆ†åˆ©ç”¨LLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>â€œè¨€è¯­åŒ–â€æ˜¯æ¡†æ¶çš„æ ¸å¿ƒï¼Œèƒ½å°†æ€æƒ³è½¬åŒ–ä¸ºè‡ªç„¶ã€é€‚åˆå£è¯­çš„æ–‡æœ¬ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è¨€è¯­åŒ–å·¥å…·ReVerTï¼ŒåŸºäºå¢é‡å’Œå¼‚æ­¥æ‘˜è¦ï¼Œå…·æœ‰è¾ƒä½çš„å»¶è¿Ÿã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¢å¼ºå£è¯­è‡ªç„¶æ€§å’Œç®€æ´æ€§çš„åŒæ—¶ï¼Œå¯¹æ¨ç†èƒ½åŠ›çš„å½±å“è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cca5cf992ba4db3186280bf10c522ae2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425980&auth_key=1760425980-0-0-5121110195d3b29db45cbfccbf77ca41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc2842dc2ced19ea40554e59ab00cbf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425988&auth_key=1760425988-0-0-ecf1ac66a5bc4f9bb9e7e078d26761ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9bc3bef5648519a7a80f39473963775d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425995&auth_key=1760425995-0-0-da21619f47fa7919207d8abc1eab3e72&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2fd89a8e79ebc2868056c9f42e6517b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426002&auth_key=1760426002-0-0-2a8f55e018e33e28c94fc37510858707&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Interpreting-the-Role-of-Visemes-in-Audio-Visual-Speech-Recognition"><a href="#Interpreting-the-Role-of-Visemes-in-Audio-Visual-Speech-Recognition" class="headerlink" title="Interpreting the Role of Visemes in Audio-Visual Speech Recognition"></a>Interpreting the Role of Visemes in Audio-Visual Speech Recognition</h2><p><strong>Authors:Aristeidis Papadopoulos, Naomi Harte</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) models have surpassed their audio-only counterparts in terms of performance. However, the interpretability of AVSR systems, particularly the role of the visual modality, remains under-explored. In this paper, we apply several interpretability techniques to examine how visemes are encoded in AV-HuBERT a state-of-the-art AVSR model. First, we use t-distributed Stochastic Neighbour Embedding (t-SNE) to visualize learned features, revealing natural clustering driven by visual cues, which is further refined by the presence of audio. Then, we employ probing to show how audio contributes to refining feature representations, particularly for visemes that are visually ambiguous or under-represented. Our findings shed light on the interplay between modalities in AVSR and could point to new strategies for leveraging visual information to improve AVSR performance. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ¨¡å‹çš„æ€§èƒ½å·²ç»è¶…è¶Šäº†ä»…ä¾èµ–éŸ³é¢‘çš„æ¨¡å‹ã€‚ç„¶è€Œï¼ŒAVSRç³»ç»Ÿçš„å¯è§£é‡Šæ€§ï¼Œç‰¹åˆ«æ˜¯è§†è§‰æ¨¡å¼çš„ä½œç”¨ï¼Œä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†å‡ ç§å¯è§£é‡Šæ€§æŠ€æœ¯æ¥æ£€æŸ¥AV-HuBERTè¿™ä¸€å…ˆè¿›çš„AVSRæ¨¡å‹ä¸­æ˜¯å¦‚ä½•ç¼–ç éŸ³ç´ çš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨tåˆ†å¸ƒéšæœºé‚»åŸŸåµŒå…¥ï¼ˆt-SNEï¼‰æ¥å¯è§†åŒ–å­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œæ­ç¤ºç”±è§†è§‰çº¿ç´¢é©±åŠ¨çš„å¤©ç„¶èšç±»ï¼Œè¿™ç§èšç±»åœ¨éŸ³é¢‘å­˜åœ¨çš„æƒ…å†µä¸‹å¾—åˆ°äº†è¿›ä¸€æ­¥çš„ä¼˜åŒ–ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡æ¢æµ‹æ¥å±•ç¤ºéŸ³é¢‘æ˜¯å¦‚ä½•å¯¹ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºåšå‡ºè´¡çŒ®çš„ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè§†è§‰ä¸Šæ¨¡ç³Šæˆ–è¡¨ç¤ºä¸è¶³çš„éŸ³ç´ ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†AVSRä¸­æ¨¡æ€ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶å¯èƒ½æŒ‡å‘åˆ©ç”¨è§†è§‰ä¿¡æ¯æé«˜AVSRæ€§èƒ½çš„æ–°ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16023v1">PDF</a> Accepted into Automatic Speech Recognition and Understanding- ASRU   2025</p>
<p><strong>æ€»ç»“</strong><br>    AVSRæ¨¡å‹æ€§èƒ½ä¼˜äºä»…ä¾èµ–éŸ³é¢‘çš„æ¨¡å‹ï¼Œä½†å…¶å¯è§£é‡Šæ€§ï¼Œç‰¹åˆ«æ˜¯è§†è§‰æ¨¡å¼çš„ä½œç”¨ï¼Œä»å¾…æ¢ç´¢ã€‚æœ¬æ–‡é‡‡ç”¨å¤šç§è§£é‡ŠæŠ€æœ¯æ¥ç ”ç©¶AV-HuBERTè¿™ä¸€å…ˆè¿›çš„AVSRæ¨¡å‹ä¸­æ˜¯å¦‚ä½•ç¼–ç è§†ä½çš„ã€‚é€šè¿‡t-SNEå¯è§†åŒ–å­¦ä¹ ç‰¹å¾ï¼Œæ­ç¤ºç”±è§†è§‰çº¿ç´¢é©±åŠ¨çš„å¤©ç„¶èšç±»ï¼Œå¹¶å¯é€šè¿‡éŸ³é¢‘è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚é€šè¿‡æ¢æµ‹å±•ç¤ºéŸ³é¢‘å¦‚ä½•æ”¹å–„ç‰¹å¾è¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è§†è§‰ä¸Šæ¨¡ç³Šæˆ–è¡¨ç¤ºä¸è¶³çš„è§†ä½ã€‚æœ¬æ–‡æ­ç¤ºäº†AVSRä¸­æ¨¡æ€ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶å¯èƒ½æŒ‡å‘åˆ©ç”¨è§†è§‰ä¿¡æ¯æé«˜AVSRæ€§èƒ½çš„æ–°ç­–ç•¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AVSRæ¨¡å‹æ€§èƒ½ä¼˜äºéŸ³é¢‘æ¨¡å‹ã€‚</li>
<li>å¯è§£é‡Šæ€§æ˜¯AVSRæ¨¡å‹çš„ä¸€ä¸ªé‡è¦ä½†å°šæœªå®Œå…¨æ¢ç´¢çš„æ–¹é¢ã€‚</li>
<li>ä½¿ç”¨t-SNEå¯è§†åŒ–å­¦ä¹ ç‰¹å¾æ­ç¤ºè‡ªç„¶èšç±»ï¼Œè¿™äº›èšç±»æ˜¯ç”±è§†è§‰çº¿ç´¢é©±åŠ¨çš„ã€‚</li>
<li>éŸ³é¢‘å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ç”±è§†è§‰çº¿ç´¢é©±åŠ¨çš„èšç±»ã€‚</li>
<li>é€šè¿‡æ¢æµ‹æ˜¾ç¤ºéŸ³é¢‘å¦‚ä½•æ”¹å–„ç‰¹å¾è¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯å¯¹æ¨¡ç³Šæˆ–è¡¨ç¤ºä¸è¶³çš„è§†ä½ã€‚</li>
<li>æ­ç¤ºäº†AVSRä¸­æ¨¡æ€ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0ba17a15bddf45ed391403705a1b7b3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426010&auth_key=1760426010-0-0-b13e4e87475fd5e2ce4b6b82f29db4ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0037a1103b6b12fdf19fa9ab16a8c292~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426017&auth_key=1760426017-0-0-9f49a2f2a7b0ac289152a32f132f2207&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d43946dcb4c2ca33e1f3029815d421fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426024&auth_key=1760426024-0-0-8d8e320f0f21d826ae8706e56261cd8a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-43fb9a4d13fe431cfed3bb3bb76b5336~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426031&auth_key=1760426031-0-0-ba29a4fff5b5eac4d9638be81cc1d9ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ab64a3dde188b3d125991deb33ad53d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426038&auth_key=1760426038-0-0-1e60a20985f2fff06386b9d329f717ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10d9b61397674957bd155047be51d936~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426044&auth_key=1760426044-0-0-76c28c48624ada7d5dcd64e32823e206&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c0244df2d10a4885cb203eae59adb4c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426051&auth_key=1760426051-0-0-1452f03166272019c979eeee34df7d8e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Compose-Yourself-Average-Velocity-Flow-Matching-for-One-Step-Speech-Enhancement"><a href="#Compose-Yourself-Average-Velocity-Flow-Matching-for-One-Step-Speech-Enhancement" class="headerlink" title="Compose Yourself: Average-Velocity Flow Matching for One-Step Speech   Enhancement"></a>Compose Yourself: Average-Velocity Flow Matching for One-Step Speech   Enhancement</h2><p><strong>Authors:Gang Yang, Yue Lei, Wenxin Tai, Jin Wu, Jia Chen, Ting Zhong, Fan Zhou</strong></p>
<p>Diffusion and flow matching (FM) models have achieved remarkable progress in speech enhancement (SE), yet their dependence on multi-step generation is computationally expensive and vulnerable to discretization errors. Recent advances in one-step generative modeling, particularly MeanFlow, provide a promising alternative by reformulating dynamics through average velocity fields. In this work, we present COSE, a one-step FM framework tailored for SE. To address the high training overhead of Jacobian-vector product (JVP) computations in MeanFlow, we introduce a velocity composition identity to compute average velocity efficiently, eliminating expensive computation while preserving theoretical consistency and achieving competitive enhancement quality. Extensive experiments on standard benchmarks show that COSE delivers up to 5x faster sampling and reduces training cost by 40%, all without compromising speech quality. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ICDM-UESTC/COSE">https://github.com/ICDM-UESTC/COSE</a>. </p>
<blockquote>
<p>æ‰©æ•£å’ŒæµåŒ¹é…ï¼ˆFMï¼‰æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬å¯¹å¤šæ­¥ç”Ÿæˆçš„ä¾èµ–åœ¨è®¡ç®—ä¸Šæˆæœ¬é«˜æ˜‚ï¼Œå¹¶å®¹æ˜“å—åˆ°ç¦»æ•£åŒ–è¯¯å·®çš„å½±å“ã€‚æœ€è¿‘ä¸€æ­¥ç”Ÿæˆå»ºæ¨¡çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯MeanFlowï¼Œé€šè¿‡å¹³å‡é€Ÿåº¦åœºé‡æ–°è¡¨è¿°åŠ¨æ€ï¼Œæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†COSEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºSEè®¾è®¡çš„ä¸€æ­¥FMæ¡†æ¶ã€‚ä¸ºäº†è§£å†³MeanFlowä¸­é›…å¯æ¯”å‘é‡ç§¯ï¼ˆJVPï¼‰è®¡ç®—çš„é«˜è®­ç»ƒå¼€é”€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé€Ÿåº¦ç»„åˆæ’ç­‰å¼æ¥æœ‰æ•ˆåœ°è®¡ç®—å¹³å‡é€Ÿåº¦ï¼Œè¿™æ—¢æ¶ˆé™¤äº†æ˜‚è´µçš„è®¡ç®—ï¼Œåˆä¿æŒäº†ç†è®ºçš„ä¸€è‡´æ€§ï¼Œå¹¶å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„å¢å¼ºè´¨é‡ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCOSEå®ç°äº†é«˜è¾¾5å€çš„æ›´å¿«é‡‡æ ·é€Ÿåº¦ï¼Œå¹¶å°†è®­ç»ƒæˆæœ¬é™ä½äº†40%ï¼Œæ‰€æœ‰è¿™ä¸€åˆ‡éƒ½ä¸å½±å“è¯­éŸ³è´¨é‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ICDM-UESTC/COSE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ICDM-UESTC/COSEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15952v2">PDF</a> 5 pages, 2 figures, submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹è¯­éŸ³å¢å¼ºçš„æ–°é¢–ä¸€æ­¥æ‰©æ•£ä¸æµåŒ¹é…ï¼ˆFMï¼‰æ¡†æ¶COSEã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ä¼ ç»Ÿçš„å¤šæ­¥ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬å’Œç¦»æ•£åŒ–è¯¯å·®é—®é¢˜ã€‚é€šè¿‡å¼•å…¥é€Ÿåº¦ç»„æˆæ’ç­‰å¼ï¼ŒCOSEèƒ½å¤Ÿé«˜æ•ˆè®¡ç®—å¹³å‡é€Ÿåº¦ï¼Œå‡å°‘æ˜‚è´µçš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™ç†è®ºä¸€è‡´æ€§å¹¶è¾¾åˆ°æœ‰ç«äº‰åŠ›çš„å¢å¼ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCOSEå¯å®ç°é«˜è¾¾5å€çš„å¿«é€Ÿé‡‡æ ·å¹¶é™ä½è®­ç»ƒæˆæœ¬40%ï¼ŒåŒæ—¶ä¸æŸå®³è¯­éŸ³è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>COSEæ˜¯ä¸€ä¸ªé’ˆå¯¹è¯­éŸ³å¢å¼ºçš„ä¸€æ­¥FMæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šæ­¥ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„é«˜è®¡ç®—æˆæœ¬å’Œç¦»æ•£åŒ–è¯¯å·®é—®é¢˜ã€‚</li>
<li>COSEå¼•å…¥é€Ÿåº¦ç»„æˆæ’ç­‰å¼ï¼Œé«˜æ•ˆè®¡ç®—å¹³å‡é€Ÿåº¦ï¼Œå‡å°‘è®¡ç®—æˆæœ¬ã€‚</li>
<li>COSEä¿ç•™äº†ç†è®ºä¸€è‡´æ€§å¹¶å®ç°äº†æœ‰ç«äº‰åŠ›çš„å¢å¼ºè´¨é‡ã€‚</li>
<li>å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCOSEèƒ½å¤Ÿå®ç°å¿«é€Ÿé‡‡æ ·å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>COSEåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæœ€é«˜å¯è¾¾åˆ°5å€æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦ã€‚</li>
<li>COSEå¯é™ä½è®­ç»ƒæˆæœ¬è¾¾40%ï¼ŒåŒæ—¶ä¸æŸå®³è¯­éŸ³è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5ba7ee4d549c66d2c8303880c4a3357d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426060&auth_key=1760426060-0-0-7843ca4df21bcd770c4bfcc9b79f91e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-99314b075d05422ac07fadd4d93044e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426067&auth_key=1760426067-0-0-d5e0ef540c15d407ed98b384ae73aa08&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a544e5b8b8a2b79991381b2500b5b963~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426074&auth_key=1760426074-0-0-8d4ac2ca5ce48baa18d60cd71c99e119&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DISPATCH-Distilling-Selective-Patches-for-Speech-Enhancement"><a href="#DISPATCH-Distilling-Selective-Patches-for-Speech-Enhancement" class="headerlink" title="DISPATCH: Distilling Selective Patches for Speech Enhancement"></a>DISPATCH: Distilling Selective Patches for Speech Enhancement</h2><p><strong>Authors:Dohwan Kim, Jung-Woo Choi</strong></p>
<p>In speech enhancement, knowledge distillation (KD) compresses models by transferring a high-capacity teacherâ€™s knowledge to a compact student. However, conventional KD methods train the student to mimic the teacherâ€™s output entirely, which forces the student to imitate the regions where the teacher performs poorly and to apply distillation to the regions where the student already performs well, which yields only marginal gains. We propose Distilling Selective Patches (DISPatch), a KD framework for speech enhancement that applies the distillation loss to spectrogram patches where the teacher outperforms the student, as determined by a Knowledge Gap Score. This approach guides optimization toward areas with the most significant potential for student improvement while minimizing the influence of regions where the teacher may provide unreliable instruction. Furthermore, we introduce Multi-Scale Selective Patches (MSSP), a frequency-dependent method that uses different patch sizes across low- and high-frequency bands to account for spectral heterogeneity. We incorporate DISPatch into conventional KD methods and observe consistent gains in compact students. Moreover, integrating DISPatch and MSSP into a state-of-the-art frequency-dependent KD method considerably improves performance across all metrics. </p>
<blockquote>
<p>åœ¨è¯­éŸ³å¢å¼ºé¢†åŸŸï¼ŒçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰é€šè¿‡è½¬ç§»é«˜å®¹é‡æ•™å¸ˆçš„çŸ¥è¯†åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹æ¥å‹ç¼©æ¨¡å‹ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦æ–¹æ³•è®­ç»ƒå­¦ç”Ÿæ¨¡å‹å»å®Œå…¨æ¨¡ä»¿æ•™å¸ˆçš„è¾“å‡ºï¼Œè¿™å¯¼è‡´å­¦ç”Ÿæ¨¡å‹è¢«è¿«æ¨¡ä»¿æ•™å¸ˆè¡¨ç°ä¸ä½³çš„åŒºåŸŸï¼Œå¹¶å¯¹å·²ç»è¡¨ç°è‰¯å¥½çš„åŒºåŸŸåº”ç”¨è’¸é¦ï¼Œåªå¸¦æ¥äº†å¾®å°çš„æ”¶ç›Šã€‚æˆ‘ä»¬æå‡ºäº†é’ˆå¯¹è¯­éŸ³å¢å¼ºçš„çŸ¥è¯†è’¸é¦æ¡†æ¶â€”â€”è’¸é¦é€‰æ‹©æ€§è¡¥ä¸ï¼ˆDISPatchï¼‰ï¼Œè¯¥æ¡†æ¶å°†è’¸é¦æŸå¤±åº”ç”¨äºæ•™å¸ˆè¡¨ç°ä¼˜äºå­¦ç”Ÿçš„é¢‘è°±å›¾è¡¥ä¸ä¸Šï¼Œé€šè¿‡çŸ¥è¯†å·®è·åˆ†æ•°æ¥ç¡®å®šã€‚è¿™ç§æ–¹æ³•å¼•å¯¼ä¼˜åŒ–èµ°å‘å¯¹å­¦ç”Ÿæ”¹è¿›æ½œåŠ›æœ€å¤§çš„åŒºåŸŸï¼ŒåŒæ—¶æœ€å°åŒ–æ•™å¸ˆå¯èƒ½åœ¨ä¸å¯é åŒºåŸŸæä¾›æŒ‡å¯¼çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå°ºåº¦é€‰æ‹©æ€§è¡¥ä¸ï¼ˆMSSPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é¢‘ç‡ä¾èµ–çš„æ–¹æ³•ï¼Œä½¿ç”¨ä¸åŒå¤§å°çš„è¡¥ä¸æ¥é€‚åº”ä½é¢‘å’Œé«˜é¢‘å¸¦ï¼Œä»¥åº”å¯¹é¢‘è°±å¼‚è´¨æ€§ã€‚æˆ‘ä»¬å°†DISPatchçº³å…¥ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œå¹¶è§‚å¯Ÿåˆ°ç´§å‡‘å­¦ç”Ÿæ¨¡å‹çš„æŒç»­æ”¶ç›Šã€‚æ­¤å¤–ï¼Œå°†DISPatchå’ŒMSSPé›†æˆåˆ°æœ€å…ˆè¿›çš„é¢‘ç‡ä¾èµ–çŸ¥è¯†è’¸é¦æ–¹æ³•ä¸­ï¼Œå¯å…¨é¢æé«˜æ‰€æœ‰æŒ‡æ ‡çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15922v1">PDF</a> submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰åœ¨è¯­éŸ³å¢å¼ºä¸­é€šè¿‡è½¬ç§»é«˜æ€§èƒ½æ•™å¸ˆçš„çŸ¥è¯†æ¥å‹ç¼©æ¨¡å‹ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„KDæ–¹æ³•å®Œå…¨è®­ç»ƒå­¦ç”Ÿæ¨¡å‹æ¨¡ä»¿æ•™å¸ˆçš„è¾“å‡ºï¼Œè¿™å¯¼è‡´å­¦ç”Ÿåœ¨æ•™å¸ˆè¡¨ç°ä¸ä½³çš„åŒºåŸŸè¿›è¡Œæ¨¡ä»¿ï¼Œå¹¶åœ¨å­¦ç”Ÿå·²ç»è¡¨ç°è‰¯å¥½çš„åŒºåŸŸåº”ç”¨è’¸é¦ï¼Œåªäº§ç”Ÿå¾®å°çš„æ”¶ç›Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘è¯­éŸ³å¢å¼ºçš„çŸ¥è¯†è’¸é¦æ¡†æ¶â€”â€”é€‰æ‹©æ€§è’¸é¦è¡¥ä¸ï¼ˆDISPatchï¼‰ï¼Œè¯¥æ¡†æ¶å°†è’¸é¦æŸå¤±åº”ç”¨äºæ•™å¸ˆåœ¨å­¦ç”Ÿæ¨¡å‹ä¸Šè¡¨ç°æ›´å¥½çš„é¢‘è°±å›¾è¡¥ä¸ä¸Šï¼Œé€šè¿‡çŸ¥è¯†å·®è·åˆ†æ•°è¿›è¡Œç¡®å®šã€‚æ­¤æ–¹æ³•å¼•å¯¼ä¼˜åŒ–æœç€å­¦ç”Ÿæ¨¡å‹æ½œåŠ›æœ€å¤§çš„åŒºåŸŸå‘å±•ï¼ŒåŒæ—¶æœ€å°åŒ–æ•™å¸ˆå¯èƒ½åœ¨ä¸å¯é åŒºåŸŸæä¾›æŒ‡å¯¼çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šå°ºåº¦é€‰æ‹©æ€§è¡¥ä¸ï¼ˆMSSPï¼‰ï¼Œä¸€ç§é¢‘ç‡ä¾èµ–çš„æ–¹æ³•ï¼Œä½¿ç”¨ä½å’Œé«˜é¢‘å¸¦çš„ä¸åŒè¡¥ä¸å¤§å°æ¥å¤„ç†é¢‘è°±å¼‚è´¨æ€§ã€‚æˆ‘ä»¬å°†DISPatchçº³å…¥ä¼ ç»ŸKDæ–¹æ³•ï¼Œè§‚å¯Ÿåˆ°ç´§å‡‘å­¦ç”Ÿæ¨¡å‹çš„æŒç»­æ”¶ç›Šã€‚è€Œä¸”ï¼Œå°†DISPatchå’ŒMSSPé›†æˆåˆ°æœ€å…ˆè¿›çš„é¢‘ç‡ä¾èµ–KDæ–¹æ³•ä¸­ï¼Œå¯å…¨é¢æ”¹å–„æ‰€æœ‰æŒ‡æ ‡çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ç”¨äºè¯­éŸ³å¢å¼ºä¸­çš„æ¨¡å‹å‹ç¼©ï¼Œé€šè¿‡è½¬ç§»æ•™å¸ˆçš„çŸ¥è¯†ã€‚</li>
<li>ä¼ ç»ŸKDæ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šå­¦ç”Ÿæ¨¡å‹å®Œå…¨æ¨¡ä»¿æ•™å¸ˆè¾“å‡ºï¼Œå¯¼è‡´åœ¨æ•™å¸ˆå¯¹åŒºåŸŸè¡¨ç°ä¸ä½³æ—¶å­¦ä¹ æ•ˆæœé™ä½ã€‚</li>
<li>æå‡ºäº†DISPatchæ¡†æ¶ï¼Œå°†è’¸é¦æŸå¤±åº”ç”¨äºæ•™å¸ˆåœ¨å­¦ç”Ÿä¸Šè¡¨ç°æ›´å¥½çš„é¢‘è°±å›¾è¡¥ä¸ä¸Šã€‚</li>
<li>DISPatchæ¡†æ¶èƒ½å¼•å¯¼ä¼˜åŒ–æœå‘å­¦ç”Ÿæ¨¡å‹æ½œåŠ›æœ€å¤§çš„åŒºåŸŸå‘å±•ï¼Œå¹¶å‡å°‘æ•™å¸ˆä¸å¯é æŒ‡å¯¼çš„å½±å“ã€‚</li>
<li>å¼•å…¥äº†MSSPæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é¢‘ç‡ä¾èµ–çš„æ–¹æ³•ï¼Œåœ¨ä¸åŒé¢‘å¸¦ä½¿ç”¨ä¸åŒçš„è¡¥ä¸å¤§å°ï¼Œä»¥å¤„ç†é¢‘è°±å¼‚è´¨æ€§ã€‚</li>
<li>å°†DISPatchçº³å…¥ä¼ ç»ŸKDæ–¹æ³•å¯ä»¥æé«˜ç´§å‡‘å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1a668efc7c4f8c7cba121cc28787f2e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426103&auth_key=1760426103-0-0-8d7b31b093151a3e06000f207291a765&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-37709d498dd77e4cbee740d4d676d878~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426111&auth_key=1760426111-0-0-2d706e3212ea51a24d924d5b072b035b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ddef2be4cfe81fae715517d27a32015d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426117&auth_key=1760426117-0-0-c214376994c0dc2f9f6f029d956e5fce&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ebd386ecce5be8de3aac4a7e3bd7e8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426124&auth_key=1760426124-0-0-20defdc0e28fb79ab0ba96f384d03943&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-Curious-Case-of-Visual-Grounding-Different-Effects-for-Speech-and-Text-based-Language-Encoders"><a href="#The-Curious-Case-of-Visual-Grounding-Different-Effects-for-Speech-and-Text-based-Language-Encoders" class="headerlink" title="The Curious Case of Visual Grounding: Different Effects for Speech- and   Text-based Language Encoders"></a>The Curious Case of Visual Grounding: Different Effects for Speech- and   Text-based Language Encoders</h2><p><strong>Authors:Adrian Sauter, Willem Zuidema, Marianne de Heer Kloots</strong></p>
<p>How does visual information included in training affect language processing in audio- and text-based deep learning models? We explore how such visual grounding affects model-internal representations of words, and find substantially different effects in speech- vs. text-based language encoders. Firstly, global representational comparisons reveal that visual grounding increases alignment between representations of spoken and written language, but this effect seems mainly driven by enhanced encoding of word identity rather than meaning. We then apply targeted clustering analyses to probe for phonetic vs. semantic discriminability in model representations. Speech-based representations remain phonetically dominated with visual grounding, but in contrast to text-based representations, visual grounding does not improve semantic discriminability. Our findings could usefully inform the development of more efficient methods to enrich speech-based models with visually-informed semantics. </p>
<blockquote>
<p>åŒ…å«è§†è§‰ä¿¡æ¯çš„è®­ç»ƒå¯¹éŸ³é¢‘å’Œæ–‡å­—æ·±åº¦å­¦ä¹ ä¸­æ¨¡å‹çš„è¯­è¨€å¤„ç†å¦‚ä½•äº§ç”Ÿå½±å“ï¼Ÿæˆ‘ä»¬æ¢ç´¢è¿™ç§è§†è§‰é”šå®šå¯¹å•è¯å†…éƒ¨æ¨¡å‹è¡¨ç¤ºçš„å½±å“ï¼Œå‘ç°è¯­éŸ³å’Œæ–‡å­—è¯­è¨€ç¼–ç å™¨ä¸­çš„ä¸åŒå½±å“ã€‚é¦–å…ˆï¼Œå…¨å±€ä»£è¡¨æ€§æ¯”è¾ƒæ˜¾ç¤ºï¼Œè§†è§‰é”šå®šå¢åŠ äº†å£è¯­å’Œä¹¦é¢è¯­è¨€è¡¨è¿°ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼Œä½†è¿™ç§æ•ˆæœä¼¼ä¹ä¸»è¦ç”±å¢å¼ºè¯èº«ä»½ç¼–ç æ‰€è‡´ï¼Œè€Œéè¯ä¹‰ç¼–ç ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åº”ç”¨æœ‰é’ˆå¯¹æ€§çš„èšç±»åˆ†ææ¥æ¢æŸ¥æ¨¡å‹è¡¨ç¤ºçš„è¯­éŸ³ä¸è¯­ä¹‰åŒºåˆ†èƒ½åŠ›ã€‚åœ¨è¯­éŸ³æ¨¡å‹ä¸­ï¼Œè§†è§‰é”šå®šä»ç„¶ä»¥è¯­éŸ³ä¸ºä¸»å¯¼ï¼Œä½†ä¸æ–‡å­—æ¨¡å‹ä¸åŒï¼Œè§†è§‰é”šå®šä¸ä¼šæé«˜è¯­ä¹‰åŒºåˆ†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯ä»¥ä¸ºå¼€å‘æ›´å¤šåˆ©ç”¨è§†è§‰è¯­ä¹‰ä¸°å¯Œè¯­éŸ³æ¨¡å‹çš„æ–¹æ³•æä¾›æœ‰ç›Šçš„å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15837v1">PDF</a> 5 pages, 3 figures, Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰ä¿¡æ¯å¯¹éŸ³é¢‘å’Œæ–‡æœ¬æ·±åº¦å­¦ä¹ ä¸­è¯­è¨€å¤„ç†çš„å½±å“ç ”ç©¶ã€‚ç ”ç©¶å‘ç°è§†è§‰æ¥åœ°å¯ä»¥å¢å¼ºå£è¯­å’Œä¹¦é¢è¯­è¨€è¡¨ç¤ºçš„å¯¹é½ï¼Œä½†å¯¹è¯­éŸ³ä¸ºåŸºç¡€çš„è¯­è¨€ç¼–ç å™¨çš„æ•ˆæœå’Œå¯¹æ„ä¹‰ç¼–ç çš„å½±å“ç›¸å¯¹æœ‰é™ã€‚æ­¤å¤–ï¼Œè™½ç„¶è¯­éŸ³è¡¨å¾ä¾ç„¶ä¿æŒéŸ³éŸµä¸ºä¸»çš„ç‰¹ç‚¹ï¼Œä½†è§†è§‰æ¥åœ°å¯¹è¯­éŸ³æ¨¡å‹çš„éŸ³éŸµé‰´åˆ«èƒ½åŠ›å½±å“ä¸å¤§ã€‚ç ”ç©¶ç»“æœä¸ºå¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨è§†è§‰ä¿¡æ¯ä¸°å¯Œè¯­éŸ³æ¨¡å‹æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰ä¿¡æ¯å¯¹è¯­è¨€å¤„ç†çš„å½±å“åœ¨éŸ³é¢‘å’Œæ–‡æœ¬æ·±åº¦å­¦ä¹ ä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>è§†è§‰æ¥åœ°å¯ä»¥å¢å¼ºå£è¯­å’Œä¹¦é¢è¯­è¨€è¡¨ç¤ºçš„å¯¹é½ã€‚</li>
<li>è§†è§‰æ¥åœ°ä¸»è¦é€šè¿‡å¢å¼ºå•è¯èº«ä»½çš„ç¼–ç å®ç°è¿™ä¸€æ•ˆæœã€‚</li>
<li>è¯­éŸ³è¡¨å¾ä»¥éŸ³éŸµä¸ºä¸»ï¼Œè§†è§‰æ¥åœ°å¯¹è¯­éŸ³æ¨¡å‹çš„éŸ³éŸµé‰´åˆ«èƒ½åŠ›å½±å“æœ‰é™ã€‚</li>
<li>åœ¨æ–‡æœ¬è¡¨å¾ä¸­ï¼Œè§†è§‰æ¥åœ°ä¸æ”¹å–„è¯­ä¹‰é‰´åˆ«èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹äºå¦‚ä½•æ›´æœ‰æ•ˆåœ°ç»“åˆè§†è§‰ä¿¡æ¯æ¥ä¸°å¯Œè¯­éŸ³æ¨¡å‹å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-55da84d7a0dd7201de99cd9dbe5c58e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426132&auth_key=1760426132-0-0-e06da5c71fa1044f93cbbba7fd0c68d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1eda3b8e1f0424feba4ce1a75a651bb0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426139&auth_key=1760426139-0-0-986230f0632fe59fa505db8464417526&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eaa5a204035dec179ba2ab475583d722~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426146&auth_key=1760426146-0-0-33332d4107a60cd39f70004d703dbead&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CompSpoof-A-Dataset-and-Joint-Learning-Framework-for-Component-Level-Audio-Anti-spoofing-Countermeasures"><a href="#CompSpoof-A-Dataset-and-Joint-Learning-Framework-for-Component-Level-Audio-Anti-spoofing-Countermeasures" class="headerlink" title="CompSpoof: A Dataset and Joint Learning Framework for Component-Level   Audio Anti-spoofing Countermeasures"></a>CompSpoof: A Dataset and Joint Learning Framework for Component-Level   Audio Anti-spoofing Countermeasures</h2><p><strong>Authors:Xueping Zhang, Liwei Jin, Yechen Wang, Linxi Li, Ming Li</strong></p>
<p>Component-level audio Spoofing (Comp-Spoof) targets a new form of audio manipulation where only specific components of a signal, such as speech or environmental sound, are forged or substituted while other components remain genuine. Existing anti-spoofing datasets and methods treat an utterance or a segment as entirely bona fide or entirely spoofed, and thus cannot accurately detect component-level spoofing. To address this, we construct a new dataset, CompSpoof, covering multiple combinations of bona fide and spoofed speech and environmental sound. We further propose a separation-enhanced joint learning framework that separates audio components apart and applies anti-spoofing models to each one. Joint learning is employed, preserving information relevant for detection. Extensive experiments demonstrate that our method outperforms the baseline, highlighting the necessity of separate components and the importance of detecting spoofing for each component separately. Datasets and code are available at: <a target="_blank" rel="noopener" href="https://github.com/XuepingZhang/CompSpoof">https://github.com/XuepingZhang/CompSpoof</a>. </p>
<blockquote>
<p>éŸ³é¢‘ç»„ä»¶çº§æ¬ºéª—ï¼ˆComp-Spoofï¼‰é’ˆå¯¹çš„æ˜¯ä¸€ç§æ–°å‹éŸ³é¢‘æ“ä½œå½¢å¼ï¼Œåœ¨è¿™ç§å½¢å¼ä¸­ï¼Œåªæœ‰ä¿¡å·ä¸­çš„ç‰¹å®šç»„ä»¶ï¼ˆå¦‚è¯­éŸ³æˆ–ç¯å¢ƒå£°éŸ³ï¼‰è¢«ä¼ªé€ æˆ–æ›¿æ¢ï¼Œè€Œå…¶ä»–ç»„ä»¶åˆ™ä¿æŒçœŸå®ã€‚ç°æœ‰çš„é˜²æ¬ºéª—æ•°æ®é›†å’Œæ–¹æ³•å°†æ•´ä¸ªè¯è¯­æˆ–ç‰‡æ®µè§†ä¸ºå®Œå…¨çœŸå®æˆ–å®Œå…¨æ¬ºéª—ï¼Œå› æ­¤æ— æ³•å‡†ç¡®æ£€æµ‹ç»„ä»¶çº§åˆ«çš„æ¬ºéª—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†CompSpoofï¼Œæ¶µç›–äº†çœŸå®å’Œæ¬ºéª—è¯­éŸ³ä»¥åŠç¯å¢ƒå£°éŸ³çš„å¤šç§ç»„åˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå¢å¼ºåˆ†ç¦»èƒ½åŠ›çš„è”åˆå­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†éŸ³é¢‘ç»„ä»¶åˆ†å¼€ï¼Œå¹¶å¯¹æ¯ä¸ªç»„ä»¶åº”ç”¨åæ¬ºéª—æ¨¡å‹ã€‚é‡‡ç”¨è”åˆå­¦ä¹ çš„æ–¹æ³•ï¼Œä¿ç•™ä¸æ£€æµ‹ç›¸å…³çš„ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œçªæ˜¾äº†å•ç‹¬ç»„ä»¶çš„å¿…è¦æ€§ä»¥åŠä¸ºæ¯ä¸ªç»„ä»¶å•ç‹¬æ£€æµ‹æ¬ºéª—çš„é‡è¦æ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/XuepingZhang/CompSpoof">https://github.com/XuepingZhang/CompSpoof</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15804v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éŸ³é¢‘ç»„ä»¶çº§æ¬ºéª—ï¼ˆComp-Spoofï¼‰æ˜¯ä¸€ç§æ–°å‹éŸ³é¢‘æ“ä½œå½¢å¼ï¼Œå…¶ä¸­åªå¯¹éŸ³é¢‘ä¿¡å·ä¸­çš„ç‰¹å®šéƒ¨åˆ†ï¼ˆå¦‚è¯­éŸ³æˆ–ç¯å¢ƒå£°éŸ³ï¼‰è¿›è¡Œä¼ªé€ æˆ–æ›¿æ¢ï¼Œè€Œå…¶ä»–éƒ¨åˆ†ä¿æŒçœŸå®ã€‚ç°æœ‰çš„åæ¬ºéª—æ•°æ®é›†å’Œæ–¹æ³•å°†æ•´ä¸ªè¯­å¥æˆ–ç‰‡æ®µè§†ä¸ºå®Œå…¨çœŸå®æˆ–å®Œå…¨æ¬ºéª—ï¼Œå› æ­¤æ— æ³•å‡†ç¡®æ£€æµ‹ç»„ä»¶çº§åˆ«çš„æ¬ºéª—ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†æ–°çš„CompSpoofæ•°æ®é›†ï¼Œæ¶µç›–å¤šç§çœŸå®å’Œæ¬ºéª—è¯­éŸ³å’Œç¯å¢ƒå£°éŸ³çš„æ··åˆç»„åˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†åˆ†ç¦»å¢å¼ºè”åˆå­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†éŸ³é¢‘ç»„ä»¶åˆ†å¼€å¹¶å¯¹æ¯ä¸ªç»„ä»¶åº”ç”¨åæ¬ºéª—æ¨¡å‹è¿›è¡Œå•ç‹¬æ£€æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œçªå‡ºäº†åˆ†ç¦»ç»„ä»¶çš„å¿…è¦æ€§ä»¥åŠå•ç‹¬å¯¹æ¯ä¸ªç»„ä»¶è¿›è¡Œæ¬ºéª—æ£€æµ‹çš„å®ç”¨æ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨æ­¤å¤„è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/XuepingZhang/CompSpoof">https://github.com/XuepingZhang/CompSpoof</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘ç»„ä»¶çº§æ¬ºéª—æ˜¯ä¸€ç§æ–°å‡ºç°çš„éŸ³é¢‘æ“ä½œæ–¹å¼ï¼Œç‰¹å®šé’ˆå¯¹éŸ³é¢‘ä¿¡å·çš„éƒ¨åˆ†ç»„ä»¶è¿›è¡Œä¼ªé€ æˆ–æ›¿æ¢ã€‚</li>
<li>ç°æœ‰åæ¬ºéª—æ•°æ®é›†å’Œæ–¹æ³•æ— æ³•å‡†ç¡®æ£€æµ‹ç»„ä»¶çº§åˆ«çš„æ¬ºéª—ï¼Œå› ä¸ºå®ƒä»¬å°†æ•´ä¸ªè¯­å¥æˆ–ç‰‡æ®µè§†ä¸ºå®Œå…¨çœŸå®æˆ–å®Œå…¨æ¬ºéª—ã€‚</li>
<li>ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæ„å»ºäº†åä¸ºCompSpoofçš„æ–°æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§çœŸå®å’Œæ¬ºéª—è¯­éŸ³åŠç¯å¢ƒå£°éŸ³çš„æ··åˆç»„åˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ†ç¦»å¢å¼ºè”åˆå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåˆ†ç¦»éŸ³é¢‘ç»„ä»¶å¹¶å¯¹æ¯ä¸ªç»„ä»¶è¿›è¡Œåæ¬ºéª—æ¨¡å‹æ£€æµ‹ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è”åˆå­¦ä¹ ä¿ç•™ä¸æ£€æµ‹ç›¸å…³çš„ä¿¡æ¯ï¼Œä»¥æé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ–°æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¼ºè°ƒäº†å¯¹æ¯ä¸ªç»„ä»¶è¿›è¡Œå•ç‹¬æ£€æµ‹å’Œåˆ†ç¦»ç»„ä»¶çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c051983cd6778bf787a96ac89b1f2f29~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426155&auth_key=1760426155-0-0-356e41665ab89113ad7a5f26589ca2fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d9b5caf01d6742ec5bff6d448106ee7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426163&auth_key=1760426163-0-0-eeda1663ecd84c96873ba115d1a374dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-96e2917ea60e71237d173d0a91874441~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426170&auth_key=1760426170-0-0-2d785a3c26288c8db8faf945ffd64795&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bbb7494512e0c1bdd5e9ac1265e0add4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426177&auth_key=1760426177-0-0-7e9b549a06446ba8916cf2c60edee12f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a8fd468b4372852927a275424d51a6f8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426183&auth_key=1760426183-0-0-f60b27bf9e0e8dd73f587900c15802d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EmoQ-Speech-Emotion-Recognition-via-Speech-Aware-Q-Former-and-Large-Language-Model"><a href="#EmoQ-Speech-Emotion-Recognition-via-Speech-Aware-Q-Former-and-Large-Language-Model" class="headerlink" title="EmoQ: Speech Emotion Recognition via Speech-Aware Q-Former and Large   Language Model"></a>EmoQ: Speech Emotion Recognition via Speech-Aware Q-Former and Large   Language Model</h2><p><strong>Authors:Yiqing Yang, Man-Wai Mak</strong></p>
<p>The performance of speech emotion recognition (SER) is limited by the insufficient emotion information in unimodal systems and the feature alignment difficulties in multimodal systems. Recently, multimodal large language models (MLLMs) have made progress in SER. However, MLLMs still suffer from hallucination and misclassification problems in complex emotion reasoning. To address these problems, we propose an MLLM-based framework called EmoQ, which generates query embeddings that fuse multimodal information through an EmoQ-Former and uses multi-objective affective learning (MAL) to achieve co-optimization. The framework also provides a soft-prompt injection strategy to inject multimodal representations into the LLM. This end-to-end architecture achieves state-of-the-art performance on the IEMOCAP and MELD datasets, providing a new multimodal fusion paradigm for SER. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰çš„æ€§èƒ½å—é™äºå•æ¨¡æ€ç³»ç»Ÿä¸­æƒ…æ„Ÿä¿¡æ¯çš„ä¸è¶³å’Œå¤šæ¨¡æ€ç³»ç»Ÿä¸­ç‰¹å¾å¯¹é½çš„å›°éš¾ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨SERæ–¹é¢å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼ŒMLLMåœ¨å¤æ‚æƒ…ç»ªæ¨ç†ä¸­ä»å­˜åœ¨å¹»è§‰å’Œè¯¯åˆ†ç±»é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMLLMçš„æ¡†æ¶ï¼Œåä¸ºEmoQã€‚è¯¥æ¡†æ¶é€šè¿‡EmoQ-Formerç”Ÿæˆèåˆå¤šæ¨¡æ€ä¿¡æ¯çš„æŸ¥è¯¢åµŒå…¥ï¼Œå¹¶ä½¿ç”¨å¤šç›®æ ‡æƒ…æ„Ÿå­¦ä¹ ï¼ˆMALï¼‰å®ç°ååŒä¼˜åŒ–ã€‚è¯¥æ¡†æ¶è¿˜æä¾›äº†ä¸€ç§è½¯æç¤ºæ³¨å…¥ç­–ç•¥ï¼Œå°†å¤šæ¨¡æ€è¡¨ç¤ºæ³¨å…¥LLMä¸­ã€‚è¿™ç§ç«¯åˆ°ç«¯çš„æ¶æ„åœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºSERæä¾›äº†æ–°çš„å¤šæ¨¡æ€èåˆèŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15775v1">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸­é’ˆå¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¡†æ¶EmoQã€‚è¯¥æ¡†æ¶èåˆäº†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œé‡‡ç”¨EmoQ-Formerç”ŸæˆæŸ¥è¯¢åµŒå…¥ï¼Œå¹¶åˆ©ç”¨å¤šç›®æ ‡æƒ…æ„Ÿå­¦ä¹ ï¼ˆMALï¼‰å®ç°ååŒä¼˜åŒ–ï¼Œä»¥è§£å†³ä¼ ç»Ÿç³»ç»Ÿä¸­æƒ…æ„Ÿä¿¡æ¯ä¸è¶³å’Œç‰¹å¾å¯¹é½å›°éš¾çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æä¾›äº†ä¸€ç§è½¯æç¤ºæ³¨å…¥ç­–ç•¥ï¼Œå°†å¤šæ¨¡æ€è¡¨ç¤ºæ³¨å…¥åˆ°LLMä¸­ã€‚è¯¥æ¡†æ¶åœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸ºSERæä¾›äº†æ–°çš„å¤šæ¨¡æ€èåˆèŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å—é™äºå•æ¨¡æ€ç³»ç»Ÿä¸­æƒ…æ„Ÿä¿¡æ¯çš„ä¸è¶³å’Œå¤šæ¨¡æ€ç³»ç»Ÿä¸­ç‰¹å¾å¯¹é½çš„å›°éš¾ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨SERæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä»å­˜åœ¨æ¨ç†å¤æ‚æƒ…ç»ªæ—¶çš„å¹»è§‰å’Œè¯¯åˆ†ç±»é—®é¢˜ã€‚</li>
<li>æå‡ºçš„EmoQæ¡†æ¶åŸºäºMLLMsï¼Œé€šè¿‡EmoQ-Formerç”ŸæˆæŸ¥è¯¢åµŒå…¥ï¼Œèåˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚</li>
<li>EmoQæ¡†æ¶é‡‡ç”¨å¤šç›®æ ‡æƒ…æ„Ÿå­¦ä¹ ï¼ˆMALï¼‰å®ç°ååŒä¼˜åŒ–ã€‚</li>
<li>æ¡†æ¶ä¸­çš„è½¯æç¤ºæ³¨å…¥ç­–ç•¥èƒ½å¤Ÿå°†å¤šæ¨¡æ€è¡¨ç¤ºæ³¨å…¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚</li>
<li>EmoQæ¡†æ¶åœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8c73fb90e09501149b7f7f2414433629~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426191&auth_key=1760426191-0-0-58f3fccfa181f59071c0e6ec11151358&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-deb6b2803e922ad4934ee068091a5267~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426198&auth_key=1760426198-0-0-d79e3f339419b3d279ef322d23021412&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a090fed71bf5eb4d404edfdc54a774b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426206&auth_key=1760426206-0-0-dbf6cb0f3b1a4a7dcc61b98bee7b9927&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Interpretable-Modeling-of-Articulatory-Temporal-Dynamics-from-real-time-MRI-for-Phoneme-Recognition"><a href="#Interpretable-Modeling-of-Articulatory-Temporal-Dynamics-from-real-time-MRI-for-Phoneme-Recognition" class="headerlink" title="Interpretable Modeling of Articulatory Temporal Dynamics from real-time   MRI for Phoneme Recognition"></a>Interpretable Modeling of Articulatory Temporal Dynamics from real-time   MRI for Phoneme Recognition</h2><p><strong>Authors:Jay Park, Hong Nguyen, Sean Foley, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan</strong></p>
<p>Real-time Magnetic Resonance Imaging (rtMRI) visualizes vocal tract action, offering a comprehensive window into speech articulation. However, its signals are high dimensional and noisy, hindering interpretation. We investigate compact representations of spatiotemporal articulatory dynamics for phoneme recognition from midsagittal vocal tract rtMRI videos. We compare three feature types: (1) raw video, (2) optical flow, and (3) six linguistically-relevant regions of interest (ROIs) for articulator movements. We evaluate models trained independently on each representation, as well as multi-feature combinations. Results show that multi-feature models consistently outperform single-feature baselines, with the lowest phoneme error rate (PER) of 0.34 obtained by combining ROI and raw video. Temporal fidelity experiments demonstrate a reliance on fine-grained articulatory dynamics, while ROI ablation studies reveal strong contributions from tongue and lips. Our findings highlight how rtMRI-derived features provide accuracy and interpretability, and establish strategies for leveraging articulatory data in speech processing. </p>
<blockquote>
<p>å®æ—¶ç£å…±æŒ¯æˆåƒï¼ˆrtMRIï¼‰èƒ½å¤Ÿå¯è§†åŒ–å£°è…”åŠ¨ä½œï¼Œä¸ºè¯­éŸ³å‘éŸ³æä¾›å…¨é¢çš„è§‚å¯Ÿçª—å£ã€‚ç„¶è€Œï¼Œå…¶ä¿¡å·å…·æœ‰é«˜ç»´åº¦å’Œå™ªå£°å¤§çš„ç‰¹ç‚¹ï¼Œé˜»ç¢äº†å¯¹å…¶çš„è§£è¯»ã€‚æˆ‘ä»¬ç ”ç©¶ä»å£°è…”ä¸­éƒ¨çŸ¢çŠ¶rtMRIè§†é¢‘ä¸­è¯†åˆ«éŸ³ç´ çš„æ—¶ç©ºå‘éŸ³åŠ¨æ€ç´§å‡‘è¡¨ç¤ºã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§ç‰¹å¾ç±»å‹ï¼šï¼ˆ1ï¼‰åŸå§‹è§†é¢‘ã€ï¼ˆ2ï¼‰å…‰æµï¼Œä»¥åŠï¼ˆ3ï¼‰å…­ä¸ªä¸è¯­è¨€ç›¸å…³çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„å‘éŸ³è¿åŠ¨ã€‚æˆ‘ä»¬è¯„ä¼°äº†é’ˆå¯¹æ¯ç§è¡¨ç¤ºç‹¬ç«‹è®­ç»ƒçš„æ¨¡å‹ï¼Œä»¥åŠå¤šç‰¹å¾ç»„åˆã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤šç‰¹å¾æ¨¡å‹å§‹ç»ˆä¼˜äºå•ç‰¹å¾åŸºçº¿ï¼Œé€šè¿‡ç»“åˆROIå’ŒåŸå§‹è§†é¢‘è·å¾—æœ€ä½çš„éŸ³ç´ é”™è¯¯ç‡ï¼ˆPERï¼‰ä¸º0.34ã€‚æ—¶é—´ä¿çœŸå®éªŒè¯æ˜äº†ç²¾ç»†å‘éŸ³åŠ¨æ€çš„ä¾èµ–æ€§ï¼Œè€ŒROIæ¶ˆèç ”ç©¶æ­ç¤ºäº†èˆŒå¤´å’Œå˜´å”‡çš„å¼ºçƒˆè´¡çŒ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒrtMRIè¡ç”Ÿçš„ç‰¹å¾å¦‚ä½•æä¾›å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå¹¶å»ºç«‹äº†åœ¨è¯­éŸ³å¤„ç†ä¸­åˆ©ç”¨å‘éŸ³æ•°æ®çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15689v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å®æ—¶ç£å…±æŒ¯æˆåƒï¼ˆrtMRIï¼‰èƒ½å¤Ÿå¯è§†åŒ–è¯­éŸ³é€šé“çš„è¿ä½œï¼Œä¸ºè¯­éŸ³å‘éŸ³æä¾›å…¨é¢çš„è§‚å¯Ÿçª—å£ã€‚ç„¶è€Œï¼Œå…¶ä¿¡å·é«˜ç»´åº¦ä¸”å˜ˆæ‚ï¼Œé˜»ç¢äº†è§£è¯»ã€‚æœ¬ç ”ç©¶æ¢è®¨ä»çŸ¢çŠ¶é¢rtMRIè§†é¢‘ä¸­è¯†åˆ«éŸ³ç´ çš„æ—¶ç©ºå‘éŸ³åŠ¨æ€ç®€æ´è¡¨ç¤ºã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§ç‰¹å¾ç±»å‹ï¼šï¼ˆ1ï¼‰åŸå§‹è§†é¢‘ã€ï¼ˆ2ï¼‰å…‰æµï¼Œä»¥åŠï¼ˆ3ï¼‰ä¸å‘éŸ³ç›¸å…³çš„å…­ä¸ªæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†é’ˆå¯¹æ¯ç§è¡¨ç¤ºç‹¬ç«‹è®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠå¤šç‰¹å¾ç»„åˆã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤šç‰¹å¾æ¨¡å‹å§‹ç»ˆä¼˜äºå•ç‰¹å¾åŸºçº¿ï¼Œé€šè¿‡ç»“åˆROIå’ŒåŸå§‹è§†é¢‘è·å¾—æœ€ä½çš„éŸ³ç´ é”™è¯¯ç‡ï¼ˆPERï¼‰ä¸º0.34ã€‚æ—¶é—´ä¿çœŸåº¦å®éªŒè¡¨æ˜å¯¹ç²¾ç»†å‘éŸ³åŠ¨æ€çš„ä¾èµ–ï¼ŒROIæ¶ˆèç ”ç©¶æ­ç¤ºäº†èˆŒå¤´å’Œå˜´å”‡çš„å¼ºçƒˆè´¡çŒ®ã€‚æœ¬ç ”ç©¶ç»“æœçªæ˜¾äº†rtMRIè¡ç”Ÿçš„ç‰¹å¾åœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶ä¸ºåˆ©ç”¨å‘éŸ³æ•°æ®åœ¨è¯­éŸ³å¤„ç†ä¸­åˆ¶å®šäº†ç­–ç•¥ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>å®æ—¶ç£å…±æŒ¯æˆåƒï¼ˆrtMRIï¼‰èƒ½å¤Ÿå¯è§†åŒ–è¯­éŸ³é€šé“çš„è¿ä½œï¼Œæä¾›å…¨é¢çš„è¯­éŸ³è§‚å¯Ÿã€‚</li>
<li>rtMRIä¿¡å·é«˜ç»´åº¦ä¸”å˜ˆæ‚ï¼Œå¸¦æ¥è§£è¯»å›°éš¾ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†ä¸‰ç§ç‰¹å¾ç±»å‹ï¼šåŸå§‹è§†é¢‘ã€å…‰æµå’Œæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ã€‚</li>
<li>å¤šç‰¹å¾æ¨¡å‹è¡¨ç°ä¼˜äºå•ç‰¹å¾æ¨¡å‹ã€‚</li>
<li>ç»“åˆROIå’ŒåŸå§‹è§†é¢‘è·å¾—äº†æœ€ä½éŸ³ç´ é”™è¯¯ç‡ï¼ˆPERï¼‰ã€‚</li>
<li>æ—¶é—´ä¿çœŸåº¦å®éªŒæ˜¾ç¤ºå¯¹ç²¾ç»†å‘éŸ³åŠ¨æ€çš„ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bd9fbf06aa7ea542aefed183db0cea3c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426214&auth_key=1760426214-0-0-55e79d46320a5c67266f132f78577ea6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d6711e672c67aa97131f2d8701ad73eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426222&auth_key=1760426222-0-0-0c661c5b952f06c4410f0dd204d0e272&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-31a4ede35b4310aa4f57450cdb05b2b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426229&auth_key=1760426229-0-0-94eaf7737747647ca304bfa6befecb1f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1be2775ce1cc456deaf7d12e14cbaf4b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426236&auth_key=1760426236-0-0-6a427b2f9a7c27f104b85640d9b914a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7aeb3f35d5b8c7e3ffe3901f18df74b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426242&auth_key=1760426242-0-0-c61d8800f5691a3e525fb8aad308f4ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VOX-KRIKRI-Unifying-Speech-and-Language-through-Continuous-Fusion"><a href="#VOX-KRIKRI-Unifying-Speech-and-Language-through-Continuous-Fusion" class="headerlink" title="VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion"></a>VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion</h2><p><strong>Authors:Dimitrios Damianos, Leon Voukoutis, Georgios Paraskevopoulos, Vassilis Katsouros</strong></p>
<p>We present a multimodal fusion framework that bridges pre-trained decoder-based large language models (LLM) and acoustic encoder-decoder architectures such as Whisper, with the aim of building speech-enabled LLMs. Instead of directly using audio embeddings, we explore an intermediate audio-conditioned text space as a more effective mechanism for alignment. Our method operates fully in continuous text representation spaces, fusing Whisperâ€™s hidden decoder states with those of an LLM through cross-modal attention, and supports both offline and streaming modes. We introduce \textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that our approach effectively aligns representations across modalities. These results highlight continuous space fusion as a promising path for multilingual and low-resource speech LLMs, while achieving state-of-the-art results for Automatic Speech Recognition in Greek, providing an average $\sim20%$ relative improvement across benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºè¯­éŸ³åŠŸèƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¯¥æ¡†æ¶å°†é¢„è®­ç»ƒçš„åŸºäºè§£ç å™¨çš„LLMä¸è¯¸å¦‚whisperè¿™æ ·çš„å£°å­¦ç¼–ç å™¨-è§£ç å™¨æ¶æ„è¿æ¥èµ·æ¥ã€‚æˆ‘ä»¬å¹¶ä¸ç›´æ¥ä½¿ç”¨éŸ³é¢‘åµŒå…¥ï¼Œè€Œæ˜¯æ¢ç´¢ä¸€ä¸ªä¸­é—´çš„éŸ³é¢‘æ¡ä»¶æ–‡æœ¬ç©ºé—´ï¼Œä½œä¸ºä¸€ç§æ›´æœ‰æ•ˆçš„å¯¹é½æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®Œå…¨åœ¨è¿ç»­çš„æ–‡æœ¬è¡¨ç¤ºç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å°†whisperçš„éšè—è§£ç å™¨çŠ¶æ€ä¸LLMçš„çŠ¶æ€ç›¸èåˆï¼Œå¹¶æ”¯æŒç¦»çº¿æ¨¡å¼å’Œæµå¼å¤„ç†æ¨¡å¼ã€‚æˆ‘ä»¬æ¨å‡ºäº†ç¬¬ä¸€ä¸ªå¸Œè…Šè¯­è¯­éŸ³LLMâ€”â€”VoxKrikriï¼Œé€šè¿‡åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å®ç°äº†è·¨æ¨¡æ€çš„è¡¨ç¤ºå¯¹é½ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†è¿ç»­ç©ºé—´èåˆåœ¨å¤šè¯­ç§å’Œä½èµ„æºè¯­éŸ³LLMæ–¹é¢çš„å‰æ™¯ï¼ŒåŒæ—¶åœ¨å¸Œè…Šè¯­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡çº¦20%çš„ç›¸å¯¹æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºè¯­éŸ³é©±åŠ¨çš„é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶èåˆé¢„è®­ç»ƒçš„è§£ç å™¨å‹LLMå’Œè¯­éŸ³ç¼–ç å™¨è§£ç å™¨æ¶æ„ï¼ˆå¦‚Whisperï¼‰ï¼Œå¹¶ä¸ç›´æ¥ä½¿ç”¨éŸ³é¢‘åµŒå…¥ï¼Œè€Œæ˜¯æ¢ç´¢ä¸­é—´éŸ³é¢‘è°ƒèŠ‚æ–‡æœ¬ç©ºé—´ä½œä¸ºæ›´æœ‰æ•ˆçš„å¯¹é½æœºåˆ¶ã€‚è¯¥æ–¹æ³•å®Œå…¨åœ¨è¿ç»­æ–‡æœ¬è¡¨ç¤ºç©ºé—´ä¸­è¿è¡Œï¼Œæ”¯æŒç¦»çº¿æ¨¡å¼å’Œæµå¼å¤„ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†å¸Œè…Šè¯­éŸ³LLMâ€”â€”VoxKriKriï¼Œå¹¶é€šè¿‡åˆ†æè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å®ç°è·¨æ¨¡æ€è¡¨ç¤ºå¯¹é½ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå¤šè¯­è¨€å’Œä½èµ„æºè¯­éŸ³LLMæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è·¯å¾„ï¼Œå¹¶åœ¨å¸Œè…Šè¯­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡çº¦20%çš„ç›¸å¯¹æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œæ—¨åœ¨æ„å»ºè¯­éŸ³é©±åŠ¨çš„é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶èåˆé¢„è®­ç»ƒè§£ç å™¨å‹LLMå’Œè¯­éŸ³ç¼–ç å™¨è§£ç å™¨æ¶æ„ã€‚</li>
<li>æ¢ç´¢ä¸­é—´éŸ³é¢‘è°ƒèŠ‚æ–‡æœ¬ç©ºé—´ä½œä¸ºæ›´æœ‰æ•ˆçš„å¯¹é½æœºåˆ¶ï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨éŸ³é¢‘åµŒå…¥ã€‚</li>
<li>æ¡†æ¶å®Œå…¨åœ¨è¿ç»­æ–‡æœ¬è¡¨ç¤ºç©ºé—´ä¸­è¿è¡Œï¼Œæ”¯æŒç¦»çº¿æ¨¡å¼å’Œæµå¼å¤„ç†æ¨¡å¼ã€‚</li>
<li>æ¨å‡ºå¸Œè…Šè¯­éŸ³LLMâ€”â€”VoxKriKriã€‚</li>
<li>è·¨æ¨¡æ€å¯¹é½æœ‰æ•ˆå®ç°å¹¶æå‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d69d31585328c97a698a75c79fab998b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426250&auth_key=1760426250-0-0-c6b8bc41068c69b963b713fa5aaddfc5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d753bc2ac5d554d471e3a1559517194d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426257&auth_key=1760426257-0-0-67cb3bf456412ea6ffb2b3ca225277ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6eeccdcf32413ed86f9b95a70e79374c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426264&auth_key=1760426264-0-0-14365e0a5eae25f666808f7bf55fe7c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fdb49e9f3631d6696c284831e09176b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426271&auth_key=1760426271-0-0-02d2fe2566031b4a56e761095e9a2c4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-660d7698fed32253c7002e40df703538~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426277&auth_key=1760426277-0-0-0f433cb594a502aba838da5abb2042d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TISDiSS-A-Training-Time-and-Inference-Time-Scalable-Framework-for-Discriminative-Source-Separation"><a href="#TISDiSS-A-Training-Time-and-Inference-Time-Scalable-Framework-for-Discriminative-Source-Separation" class="headerlink" title="TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation"></a>TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation</h2><p><strong>Authors:Yongsheng Feng, Yuetonghui Xu, Jiehui Luo, Hongjia Liu, Xiaobing Li, Feng Yu, Wei Li</strong></p>
<p>Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly large networks, inflating training and deployment costs. Motivated by recent advances in inference-time scaling for generative modeling, we propose Training-Time and Inference-Time Scalable Discriminative Source Separation (TISDiSS), a unified framework that integrates early-split multi-loss supervision, shared-parameter design, and dynamic inference repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting inference depth without retraining additional models. We further provide systematic analyses of architectural and training choices and show that training with more inference repetitions improves shallow-inference performance, benefiting low-latency applications. Experiments on standard speech separation benchmarks demonstrate state-of-the-art performance with a reduced parameter count, establishing TISDiSS as a scalable and practical framework for adaptive source separation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/WingSingFung/TISDiSS">https://github.com/WingSingFung/TISDiSS</a>. </p>
<blockquote>
<p>æºåˆ†ç¦»æ˜¯è¯­éŸ³ã€éŸ³ä¹ã€éŸ³é¢‘å¤„ç†ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå®ƒä¹Ÿä¸ºè®­ç»ƒç”Ÿæˆæ¨¡å‹æä¾›äº†æ›´å¹²å‡€ã€æ›´å¤§çš„æ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­æé«˜åˆ†ç¦»æ€§èƒ½å¾€å¾€å–å†³äºç½‘ç»œè§„æ¨¡çš„æ‰©å¤§ï¼Œä»è€Œå¯¼è‡´è®­ç»ƒå’Œéƒ¨ç½²æˆæœ¬çš„å¢åŠ ã€‚å—ç”Ÿæˆæ¨¡å‹æ¨ç†æ—¶é—´ç¼©æ”¾æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†è®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´å¯ä¼¸ç¼©çš„åˆ¤åˆ«æºåˆ†ç¦»ï¼ˆTISDiSSï¼‰è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒé›†æˆäº†æ—©æœŸåˆ†è£‚å¤šæŸå¤±ç›‘ç£ã€å…±äº«å‚æ•°è®¾è®¡å’ŒåŠ¨æ€æ¨ç†é‡å¤ã€‚TISDiSSé€šè¿‡è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå°±èƒ½å®ç°çµæ´»çš„é€Ÿåº¦æ€§èƒ½æƒè¡¡ã€‚æˆ‘ä»¬è¿˜å¯¹æ¶æ„å’Œè®­ç»ƒé€‰æ‹©è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œå¹¶è¡¨æ˜ä½¿ç”¨æ›´å¤šæ¨ç†é‡å¤è¿›è¡Œè®­ç»ƒå¯ä»¥æé«˜æµ…å±‚æ¨ç†æ€§èƒ½ï¼Œæœ‰åˆ©äºä½å»¶è¿Ÿåº”ç”¨ç¨‹åºã€‚åœ¨æ ‡å‡†è¯­éŸ³åˆ†ç¦»åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†å…¶å“è¶Šçš„æ€§èƒ½å’Œå‡å°‘çš„å‚æ•°è®¡æ•°ï¼Œç¡®ç«‹äº†TISDiSSä½œä¸ºè‡ªé€‚åº”æºåˆ†ç¦»çš„å®ç”¨å’Œå¯æ‰©å±•æ¡†æ¶ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/WingSingFung/TISDiSS">https://github.com/WingSingFung/TISDiSS</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15666v2">PDF</a> Submitted to ICASSP 2026.(C) 2025 IEEE. Personal use of this material   is permitted. Permission from IEEE must be obtained for all other uses, in   any current or future media, including reprinting&#x2F;republishing this material   for advertising or promotional purposes, creating new collective works, for   resale or redistribution to servers or lists, or reuse of any copyrighted   component of this work</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´å¯ä¼¸ç¼©çš„åˆ¤åˆ«æºåˆ†ç¦»ï¼ˆTISDiSSï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†æ—©æœŸåˆ†è£‚å¤šæŸå¤±ç›‘ç£ã€å…±äº«å‚æ•°è®¾è®¡å’ŒåŠ¨æ€æ¨ç†é‡å¤æœºåˆ¶ã€‚TISDiSSé€šè¿‡è°ƒæ•´æ¨ç†æ·±åº¦å®ç°çµæ´»çš„é€Ÿåº¦æ€§èƒ½æƒè¡¡ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ ‡å‡†è¯­éŸ³åˆ†ç¦»åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†å‚æ•°æ•°é‡ï¼Œæˆä¸ºè‡ªé€‚åº”æºåˆ†ç¦»çš„å®ç”¨æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TISDiSSæ¡†æ¶é›†æˆäº†å¤šç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ—©æœŸåˆ†è£‚å¤šæŸå¤±ç›‘ç£ã€å…±äº«å‚æ•°è®¾è®¡å’ŒåŠ¨æ€æ¨ç†é‡å¤ï¼Œä»¥æé«˜æºåˆ†ç¦»æ€§èƒ½ã€‚</li>
<li>TISDiSSé€šè¿‡è°ƒæ•´æ¨ç†æ·±åº¦å®ç°çµæ´»çš„é€Ÿåº¦æ€§èƒ½æƒè¡¡ï¼Œè¿™æ˜¯å…¶ä¸€å¤§ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆæœ¬çš„å‰æä¸‹ï¼Œå®ç°æ€§èƒ½çš„ä¼˜åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTISDiSSåœ¨æ ‡å‡†è¯­éŸ³åˆ†ç¦»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>TISDiSSæ¡†æ¶å‡å°‘äº†å‚æ•°æ•°é‡ï¼Œä½¿å¾—æ¨¡å‹æ›´åŠ å®ç”¨å’Œé«˜æ•ˆã€‚</li>
<li>TISDiSSæ¡†æ¶æä¾›äº†ç³»ç»Ÿåˆ†æï¼Œæ¢è®¨äº†æ¶æ„å’Œè®­ç»ƒé€‰æ‹©çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d0045eae2b4c51d2fd26947b54e93381~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426285&auth_key=1760426285-0-0-314ef41d9ecf1f985a7a682f5972c6ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16843ca755599b236d6398969862cf12~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426293&auth_key=1760426293-0-0-a14beace2e9c01a414e8da940c217c04&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d52afb50ded9b4ec2a2c302ecfa318e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426300&auth_key=1760426300-0-0-acffbc7ce3e6d9bce8e011deb63c7b07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-64825d64f7856a6fbe0e6af3e75637ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426307&auth_key=1760426307-0-0-24cc30cca9d1fc850ce52e8373b34551&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-366739aa7237c4e4e22ab24c9b163422~resize:0:q75.jpg?source=1f5c5e47&expiration=1760426314&auth_key=1760426314-0-0-25e80c595e858fdcb9a2675213bd9803&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  ContextFlow Training-Free Video Object Editing via Adaptive Context   Enrichment
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-af3d3c2296e160340e135672856ebb89~resize:0:q75.jpg?source=1f5c5e47&expiration=1760425489&auth_key=1760425489-0-0-eb9acc1505499bff5d4e0992469bdf35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-24  Accurate Thyroid Cancer Classification using a Novel Binary Pattern   Driven Local Discrete Cosine Transform Descriptor
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
