<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-09-24  Audio Super-Resolution with Latent Bridge Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16182v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    72 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-24-更新"><a href="#2025-09-24-更新" class="headerlink" title="2025-09-24 更新"></a>2025-09-24 更新</h1><h2 id="Audio-Super-Resolution-with-Latent-Bridge-Models"><a href="#Audio-Super-Resolution-with-Latent-Bridge-Models" class="headerlink" title="Audio Super-Resolution with Latent Bridge Models"></a>Audio Super-Resolution with Latent Bridge Models</h2><p><strong>Authors:Chang Li, Zehua Chen, Liyuan Wang, Jun Zhu</strong></p>
<p>Audio super-resolution (SR), i.e., upsampling the low-resolution (LR) waveform to the high-resolution (HR) version, has recently been explored with diffusion and bridge models, while previous methods often suffer from sub-optimal upsampling quality due to their uninformative generation prior. Towards high-quality audio super-resolution, we present a new system with latent bridge models (LBMs), where we compress the audio waveform into a continuous latent space and design an LBM to enable a latent-to-latent generation process that naturally matches the LR-toHR upsampling process, thereby fully exploiting the instructive prior information contained in the LR waveform. To further enhance the training results despite the limited availability of HR samples, we introduce frequency-aware LBMs, where the prior and target frequency are taken as model input, enabling LBMs to explicitly learn an any-to-any upsampling process at the training stage. Furthermore, we design cascaded LBMs and present two prior augmentation strategies, where we make the first attempt to unlock the audio upsampling beyond 48 kHz and empower a seamless cascaded SR process, providing higher flexibility for audio post-production. Comprehensive experimental results evaluated on the VCTK, ESC-50, Song-Describer benchmark datasets and two internal testsets demonstrate that we achieve state-of-the-art objective and perceptual quality for any-to-48kHz SR across speech, audio, and music signals, as well as setting the first record for any-to-192kHz audio SR. Demo at <a target="_blank" rel="noopener" href="https://audiolbm.github.io/">https://AudioLBM.github.io/</a>. </p>
<blockquote>
<p>音频超分辨率（SR）即将从低分辨率（LR）波形上采样到高分辨率（HR）版本，最近已使用扩散模型和桥梁模型进行了探索。而之前的方法由于其缺乏信息生成先验，往往遭受次优上采样质量的困扰。为了获得高质量音频超分辨率，我们提出了一种新的潜式桥梁模型（LBM）系统，我们将音频波形压缩到连续潜在空间，并设计了一个LBM，以实现潜在到潜在的生成过程，自然地匹配LR到HR的上采样过程，从而充分利用LR波形中包含的指示性先验信息。为了进一步提高训练结果，尽管高质量样本的可用性有限，我们引入了频率感知LBM，将先验和目标频率作为模型输入，使LBM能够在训练阶段显式地学习任何到任何的上采样过程。此外，我们设计了级联LBM，并提供了两种先验增强策略，我们首次尝试解锁超过48kHz的音频上采样，并为无缝级联SR过程提供支持，为音频后期制作提供更高的灵活性。在VCTK、ESC-50、Song-Describer基准数据集和两个内部测试集上的综合实验结果表明，我们在语音、音频和音乐信号的任何到48kHz SR方面达到了最新的客观和感知质量，并首次创造了任何到192kHz音频SR的记录。演示网站为：<a target="_blank" rel="noopener" href="https://audiolbm.github.io/%E3%80%82">https://AudioLBM.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17609v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>摘要</strong><br>音频超分辨率（SR）技术近年来已使用扩散模型和桥梁模型进行了探索。以前的方法由于生成先验信息不充足，往往导致上采样质量不佳。我们提出了一种新的基于潜在桥梁模型（LBMs）的系统，将音频波形压缩到连续潜在空间，并设计一个LBM来执行潜在到潜在的生成过程，自然地匹配LR到HR的上采样过程，从而充分利用LR波形中的指导性先验信息。为了进一步提高训练结果，即使在高分辨率样本有限的情况下，我们引入了频率感知LBMs，将先验和目标频率作为模型输入，使LBMs能够在训练阶段显式学习任何到任何的上采样过程。此外，我们设计了级联LBMs，并提供了两种先验增强策略。我们首次尝试解锁超过48kHz的音频上采样，并实现无缝级联SR过程，为音频后期制作提供更高的灵活性。在VCTK、ESC-50、Song-Describer基准数据集和两个内部测试集上的综合实验结果表明，我们在任何到48kHz的SR方面实现了最先进的客观和感知质量，并在任何到192kHz的音频SR方面创造了新纪录。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了基于潜在桥梁模型（LBMs）的音频超分辨率系统，用于压缩音频波形并执行潜在空间的生成过程，实现LR到HR的自然上采样。</li>
<li>引入频率感知LBMs以提高训练效果，通过利用先验和目标频率信息，使模型能够学习任何到任何的上采样过程。</li>
<li>设计了级联LBMs和先验增强策略，为音频后期制作提供更高的灵活性。</li>
<li>该方法解锁了超过48kHz的音频上采样能力，实现了无缝级联SR过程。</li>
<li>在多个基准数据集上的实验结果表明，该方法在语音、音频和音乐信号的任何到48kHz SR方面达到了最先进的客观和感知质量。</li>
<li>该方法首次在任何到192kHz的音频SR方面创造了新纪录。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17609">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17609v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17609v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17609v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17609v1/page_4_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Leveraging-Audio-Visual-Data-to-Reduce-the-Multilingual-Gap-in-Self-Supervised-Speech-Models"><a href="#Leveraging-Audio-Visual-Data-to-Reduce-the-Multilingual-Gap-in-Self-Supervised-Speech-Models" class="headerlink" title="Leveraging Audio-Visual Data to Reduce the Multilingual Gap in   Self-Supervised Speech Models"></a>Leveraging Audio-Visual Data to Reduce the Multilingual Gap in   Self-Supervised Speech Models</h2><p><strong>Authors:María Andrea Cruz Blandón, Zakaria Aldeneh, Jie Chi, Maureen de Seyssel</strong></p>
<p>Self-supervised learning (SSL) has made significant advances in speech representation learning. Models like wav2vec 2.0 and HuBERT have achieved state-of-the-art results in tasks such as speech recognition, particularly in monolingual settings. However, multilingual SSL models tend to underperform their monolingual counterparts on each individual language, especially in multilingual scenarios with few languages such as the bilingual setting. In this work, we investigate a novel approach to reduce this performance gap by introducing limited visual grounding into bilingual speech SSL models. Our results show that visual grounding benefits both monolingual and bilingual models, with especially pronounced gains for the latter, reducing the multilingual performance gap on zero-shot phonetic discrimination from 31.5% for audio-only models to 8.04% with grounding. </p>
<blockquote>
<p>自监督学习（SSL）在语音表征学习方面取得了重大进展。像wav2vec 2.0和HuBERT这样的模型在语音识别等任务上达到了最先进的水平，特别是在单语环境下。然而，多语种SSL模型往往在每个单独的语言上的表现不如单语种模型，特别是在双语等少数语种场景下。在这项工作中，我们研究了一种通过引入有限视觉定位来减少这种性能差距的新方法，并将其应用于双语语音SSL模型。我们的结果表明，视觉定位对单语种和双语种模型都有好处，特别是对后者来说，其在零语音发音辨识的多语种性能差距从只有音频模型的31.5%减少到加入定位后的8.04%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17523v1">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了自我监督学习（SSL）在语音表示学习方面的最新进展，特别是wav2vec 2.0和HuBERT等模型在语音识别任务上的表现。然而，在多语言SSL模型中，针对每种语言的性能往往不及单语言模型，特别是在双语环境中。本研究通过引入有限的视觉定位来解决这一性能差距问题。结果显示，视觉定位不仅对单语和双语模型都有好处，而且对后者的效益更为显著，将仅使用音频模型的零射语音辨识多语言性能差距从31.5%减少到使用定位的8.04%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自我监督学习（SSL）在语音表示学习中取得了显著进展。</li>
<li>wav2vec 2.0和HuBERT等模型在语音识别任务上具有最先进的性能。</li>
<li>多语言SSL模型在每种语言上的表现通常不及单语言模型。</li>
<li>在双语语音SSL模型中引入有限的视觉定位可以减少性能差距。</li>
<li>视觉定位对单语和双语模型都有好处。</li>
<li>视觉定位对减少零射语音辨识多语言性能差距效果显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17523">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17523v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17523v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17523v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17523v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="STAR-Speech-to-Audio-Generation-via-Representation-Learning"><a href="#STAR-Speech-to-Audio-Generation-via-Representation-Learning" class="headerlink" title="STAR: Speech-to-Audio Generation via Representation Learning"></a>STAR: Speech-to-Audio Generation via Representation Learning</h2><p><strong>Authors:Zeyu Xie, Xuenan Xu, Yixuan Li, Mengyue Wu, Yuexian Zou</strong></p>
<p>This work presents STAR, the first end-to-end speech-to-audio generation framework, designed to enhance efficiency and address error propagation inherent in cascaded systems. Unlike prior approaches relying on text or vision, STAR leverages speech as it constitutes a natural modality for interaction. As an initial step to validate the feasibility of the system, we demonstrate through representation learning experiments that spoken sound event semantics can be effectively extracted from raw speech, capturing both auditory events and scene cues. Leveraging the semantic representations, STAR incorporates a bridge network for representation mapping and a two-stage training strategy to achieve end-to-end synthesis. With a 76.9% reduction in speech processing latency, STAR demonstrates superior generation performance over the cascaded systems. Overall, STAR establishes speech as a direct interaction signal for audio generation, thereby bridging representation learning and multimodal synthesis. Generated samples are available at <a target="_blank" rel="noopener" href="https://zeyuxie29.github.io/STAR">https://zeyuxie29.github.io/STAR</a>. </p>
<blockquote>
<p>本文介绍了STAR，这是第一个端到端的语音到音频生成框架，旨在提高效率并解决级联系统中固有的错误传播问题。不同于以往依赖文本或视觉的方法，STAR利用语音作为交互的自然模式。作为验证系统可行性的初步步骤，我们通过表示学习实验证明，可以有效地从原始语音中提取语音声音事件语义，捕获听觉事件和场景线索。利用语义表示，STAR结合了一个用于表示映射的桥梁网络以及一个两阶段训练策略来实现端到端的合成。STAR将语音处理延迟减少了76.9%，在级联系统上展现了更优越生成性能。总的来说，STAR确立了语音作为音频生成的直接交互信号，从而连接了表示学习和多模态合成。生成的样本可在<a target="_blank" rel="noopener" href="https://zeyuxie29.github.io/STAR%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://zeyuxie29.github.io/STAR上查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17164v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本项目提出了STAR，首个端到端的语音到音频生成框架，旨在提高效率和解决级联系统中的误差传播问题。不同于依赖文本或视觉的先前方法，STAR利用语音作为交互的自然媒介。通过表示学习实验验证了系统的可行性，可有效提取口语声音事件语义，包括听觉事件和场景线索。借助语义表示，STAR构建了一个桥梁网络用于表示映射，并采用两阶段训练策略实现端到端合成。相较于级联系统，STAR将语音处理延迟降低了76.9%，展现出更优越的性能。总体而言，STAR确立了语音作为音频生成的直接交互信号，从而实现了表示学习和多模态合成的桥梁作用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>STAR是首个端到端的语音到音频生成框架。</li>
<li>该框架旨在提高效率和解决级联系统中的误差传播问题。</li>
<li>与依赖文本或视觉的先前方法不同，STAR利用语音的自然交互特性。</li>
<li>通过表示学习实验验证了系统的可行性。</li>
<li>系统可以提取口语声音事件语义，包括听觉事件和场景线索。</li>
<li>STAR利用桥梁网络和两阶段训练策略实现高效的端到端合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17164v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17164v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17164v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.17164v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Leveraging-Multiple-Speech-Enhancers-for-Non-Intrusive-Intelligibility-Prediction-for-Hearing-Impaired-Listeners"><a href="#Leveraging-Multiple-Speech-Enhancers-for-Non-Intrusive-Intelligibility-Prediction-for-Hearing-Impaired-Listeners" class="headerlink" title="Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility   Prediction for Hearing-Impaired Listeners"></a>Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility   Prediction for Hearing-Impaired Listeners</h2><p><strong>Authors:Boxuan Cao, Linkai Li, Hanlin Yu, Changgeng Mo, Haoshuai Zhou, Shan Xiang Wang</strong></p>
<p>Speech intelligibility evaluation for hearing-impaired (HI) listeners is essential for assessing hearing aid performance, traditionally relying on listening tests or intrusive methods like HASPI. However, these methods require clean reference signals, which are often unavailable in real-world conditions, creating a gap between lab-based and real-world assessments. To address this, we propose a non-intrusive intelligibility prediction framework that leverages speech enhancers to provide a parallel enhanced-signal pathway, enabling robust predictions without reference signals. We evaluate three state-of-the-art enhancers and demonstrate that prediction performance depends on the choice of enhancer, with ensembles of strong enhancers yielding the best results. To improve cross-dataset generalization, we introduce a 2-clips augmentation strategy that enhances listener-specific variability, boosting robustness on unseen datasets. Our approach consistently outperforms the non-intrusive baseline, CPC2 Champion across multiple datasets, highlighting the potential of enhancer-guided non-intrusive intelligibility prediction for real-world applications. </p>
<blockquote>
<p>针对听力受损（HI）听众的语音清晰度评估对于评估助听器性能至关重要，传统上依赖于听力测试或像HASPI这样的侵入性方法。然而，这些方法需要干净的参考信号，这在现实世界的条件下往往不可用，从而在基于实验室和现实世界评估之间存在差距。为了解决这一问题，我们提出了一种非侵入性的清晰度预测框架，该框架利用语音增强器提供并行增强信号通路，从而在不使用参考信号的情况下进行稳健的预测。我们评估了三种最先进的增强器，并证明预测性能取决于增强器的选择，强大的增强器组合取得了最佳结果。为了提高跨数据集泛化能力，我们引入了一种2剪辑增强策略，该策略增强了听众特定的差异性，在未见过数据集上提高了鲁棒性。我们的方法在多数据集上始终优于非侵入性基线CPC2冠军，突显了增强器引导的非侵入性清晰度预测在现实应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16979v1">PDF</a> </p>
<p><strong>Summary</strong><br>听力障碍者的语音清晰度评估对于评估助听器性能至关重要，传统方法依赖于听力测试或HASPI等侵入性方法，但现实环境中往往无法获得清晰的参考信号。本研究提出了一种非侵入性的清晰度预测框架，利用语音增强器提供并行增强信号路径，无需参考信号即可进行稳健预测。评估了三种最新增强器并发现预测性能取决于增强器的选择，强增强器组合表现最佳。为提高跨数据集泛化能力，引入了一种2片段增强策略，增强听者特异性变化，在未见数据集上提高稳健性。该研究在多个数据集上始终优于非侵入性基线CPC2冠军，突显了增强器引导的非侵入性清晰度预测在现实世界应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>听力障碍者的语音清晰度评估是评估助听器性能的关键。</li>
<li>传统评估方法依赖清晰参考信号，但在现实环境中常常无法获得。</li>
<li>本研究提出了一种非侵入性的清晰度预测框架，利用语音增强器进行稳健预测，无需参考信号。</li>
<li>预测性能取决于所选择的语音增强器，强增强器组合表现最佳。</li>
<li>为提高跨数据集泛化能力，采用了2片段增强策略，增强了听者特异性变化。</li>
<li>该研究在多个数据集上的表现优于现有非侵入性方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16979v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16979v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16979v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16979v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16979v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Idiosyncratic-Versus-Normative-Modeling-of-Atypical-Speech-Recognition-Dysarthric-Case-Studies"><a href="#Idiosyncratic-Versus-Normative-Modeling-of-Atypical-Speech-Recognition-Dysarthric-Case-Studies" class="headerlink" title="Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition:   Dysarthric Case Studies"></a>Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition:   Dysarthric Case Studies</h2><p><strong>Authors:Vishnu Raja, Adithya V Ganesan, Anand Syamkumar, Ritwik Banerjee, H Andrew Schwartz</strong></p>
<p>State-of-the-art automatic speech recognition (ASR) models like Whisper, perform poorly on atypical speech, such as that produced by individuals with dysarthria. Past works for atypical speech have mostly investigated fully personalized (or idiosyncratic) models, but modeling strategies that can both generalize and handle idiosyncracy could be more effective for capturing atypical speech. To investigate this, we compare four strategies: (a) $\textit{normative}$ models trained on typical speech (no personalization), (b) $\textit{idiosyncratic}$ models completely personalized to individuals, (c) $\textit{dysarthric-normative}$ models trained on other dysarthric speakers, and (d) $\textit{dysarthric-idiosyncratic}$ models which combine strategies by first modeling normative patterns before adapting to individual speech. In this case study, we find the dysarthric-idiosyncratic model performs better than idiosyncratic approach while requiring less than half as much personalized data (36.43 WER with 128 train size vs 36.99 with 256). Further, we found that tuning the speech encoder alone (as opposed to the LM decoder) yielded the best results reducing word error rate from 71% to 32% on average. Our findings highlight the value of leveraging both normative (cross-speaker) and idiosyncratic (speaker-specific) patterns to improve ASR for underrepresented speech populations. </p>
<blockquote>
<p>最先进的自动语音识别（ASR）模型，如Whisper，在非典型语音上的表现较差，比如发音障碍者产生的语音。过去对于非典型语音的研究大多集中在完全个性化的（或特殊的）模型上，但能够同时实现通用性和处理特殊性的建模策略对于捕捉非典型语音可能更为有效。为了调查这一点，我们比较了四种策略：（a）在典型语音上训练的$\textit{规范性}$模型（无个性化），（b）完全个性化到个体的$\textit{特殊性}$模型，（c）在其他发音障碍者身上训练的$\textit{发音障碍规范性}$模型，以及（d）结合两种策略的$\textit{发音障碍特殊性}$模型，首先建立规范性模式，然后适应个人语音。在本个案研究中，我们发现发音障碍特殊性模型在表现上优于特殊性方法，并且在所需个性化数据不到一半的情况下即可达到此效果（训练集大小为128时的WER为36.43%，而训练集大小为256时为36.99%）。此外，我们发现仅调整语音编码器（而不是LM解码器）得到了最佳结果，平均将单词错误率从71%降低到32%。我们的研究结果表明，利用规范性（跨发言人）和特殊性（特定发言人）模式来提高代表性不足的语音群体的ASR价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16718v1">PDF</a> Will appear in EMNLP 2025 Main Proceedings</p>
<p><strong>Summary</strong></p>
<p>本文研究了不同模型策略在对不典型语音如发音障碍人群的自动语音识别（ASR）上的表现。对比了四种策略，发现结合规范性和个性化特征的模型策略表现最好，并且在训练样本需求量上也更省。最佳结果是通过调整语音编码器实现的，显著降低了词错误率。研究强调了结合跨说话人的规范模式和说话人特定的个性化模式在提高代表性不足的语音群体的ASR价值中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在处理不典型语音如发音障碍人群的自动语音识别（ASR）时，现有模型表现不佳。</li>
<li>对比了四种不同的模型策略，包括规范性模型、个性化模型、其他发音障碍者的模型以及结合规范性和个性化的模型。</li>
<li>结合规范性和个性化特征的模型表现最好，并且在训练样本需求量上更省。</li>
<li>调整语音编码器是获得最佳结果的关键步骤，显著降低了词错误率。</li>
<li>研究发现，对于代表性不足的语音群体，结合跨说话人的规范模式和说话人特定的个性化模式有助于提高ASR的性能。</li>
<li>该研究对于改善不典型语音的ASR具有重要的实际意义，有助于推动ASR技术在更多场景和人群中的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16718">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16718v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16718v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16718v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Audio-Conditioned-Diffusion-LLMs-for-ASR-and-Deliberation-Processing"><a href="#Audio-Conditioned-Diffusion-LLMs-for-ASR-and-Deliberation-Processing" class="headerlink" title="Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing"></a>Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing</h2><p><strong>Authors:Mengqi Wang, Zhan Liu, Zengrui Jin, Guangzhi Sun, Chao Zhang, Philip C. Woodland</strong></p>
<p>Diffusion-based large language models (DLLMs) have recently attracted growing interest as an alternative to autoregressive decoders. In this work, we present an empirical study on using the diffusion-based large language model LLaDA for automatic speech recognition (ASR). We first investigate its use as an external deliberation-based processing module for Whisper-LLaMA transcripts. By leveraging the bidirectional attention and denoising capabilities of LLaDA, we explore random masking, low-confidence masking, and semi-autoregressive strategies, showing that Whisper-LLaDA substantially reduces WER compared with the baseline. On LibriSpeech, the best cascade system achieves 2.25%&#x2F;4.94% WER on test-clean&#x2F;test-other, representing a 12.3% relative improvement over the Whisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA without acoustic features fails to improve accuracy, highlighting the importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA as a standalone decoder for ASR with diffusion-based and semi-autoregressive decoding. Most experimental configurations achieve faster inference than the Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These findings offer an empirical view of diffusion-based LLMs for ASR and point to promising directions for improvements. </p>
<blockquote>
<p>基于扩散的大型语言模型（DLLMs）最近作为自回归解码器的替代方案而引起了越来越多的关注。在这项工作中，我们对使用基于扩散的大型语言模型LLaDA进行自动语音识别（ASR）进行了实证研究。我们首先研究了其在基于外部深思处理的模块Whisper-LLaMA转录中的应用。通过利用LLaDA的双向注意力和降噪能力，我们探索了随机掩蔽、低置信掩蔽和半自回归策略，表明Whisper-LLaDA与基线相比大幅降低了WER。在LibriSpeech上，最佳级联系统在测试清洁&#x2F;测试其他情况下的WER达到2.25%&#x2F;4.94%，相对于Whisper-LLaMA基线在测试其他分割上实现了12.3%的相对改进。相比之下，没有声学特征的纯文本LLaDA未能提高准确性，从而突出了音频条件嵌入的重要性。我们进一步评估了将Whisper-LLaDA作为独立于ASR的扩散和半自回归解码的解码器。大多数实验配置实现了比Whisper-LLaMA基线更快的推理速度，尽管识别准确率略有下降。这些发现为基于扩散的LLM在ASR方面的应用提供了实证观点，并指出了改进的有希望的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16622v1">PDF</a> </p>
<p><strong>摘要</strong><br>    扩散模型语言模型（DLLMs）在自动语音识别（ASR）中的应用近期备受关注。本研究探讨了使用LLaDA模型进行ASR的实证情况。通过作为外部思考处理模块应用于Whisper-LLaMA转录，利用LLaDA的双向注意力和去噪能力，研究随机掩码、低置信掩码和半自回归策略。相较于基线模型，Whisper-LLaDA大幅减少了WER。在LibriSpeech上，最佳级联系统测试清洁&#x2F;其他测试集达到2.25%&#x2F;4.94%的WER，相较于Whisper-LLaMA基线模型在其他测试集上实现12.3%的相对改进。然而，不使用声学特征的纯文本LLaDA模型无法提高准确性，凸显音频条件嵌入的重要性。此外，将Whisper-LLaDA作为独立的ASR解码器进行评估，大多数实验配置实现了比Whisper-LLaMA基线更快的推理速度，尽管识别准确率略有下降。本研究为扩散模型在ASR中的应用提供了实证观点，并为改进指明了有前景的方向。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLaDA模型作为外部思考处理模块用于自动语音识别（ASR）。</li>
<li>LLaDA的双向注意力和去噪能力在随机掩码、低置信掩码和半自回归策略中得到探索和应用。</li>
<li>Whisper-LLaDA在LibriSpeech上的表现优于基线模型，显著降低了词错误率（WER）。</li>
<li>最好的级联系统实现了对测试清洁和其他测试集上的性能提升。</li>
<li>纯文本LLaDA模型未能提高准确性，强调音频条件嵌入的重要性。</li>
<li>Whisper-LLaDA作为独立的ASR解码器表现良好，实验配置通常实现更快的推理速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16622v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16622v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16622v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16622v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Investigating-Polyglot-Speech-Foundation-Models-for-Learning-Collective-Emotion-from-Crowds"><a href="#Investigating-Polyglot-Speech-Foundation-Models-for-Learning-Collective-Emotion-from-Crowds" class="headerlink" title="Investigating Polyglot Speech Foundation Models for Learning Collective   Emotion from Crowds"></a>Investigating Polyglot Speech Foundation Models for Learning Collective   Emotion from Crowds</h2><p><strong>Authors:Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Panchal Nayak, Priyabrata Mallick, Swarup Ranjan Behera, Parabattina Bhagath, Pailla Balakrishna Reddy, Arun Balaji Buduru</strong></p>
<p>This paper investigates the polyglot (multilingual) speech foundation models (SFMs) for Crowd Emotion Recognition (CER). We hypothesize that polyglot SFMs, pre-trained on diverse languages, accents, and speech patterns, are particularly adept at navigating the noisy and complex acoustic environments characteristic of crowd settings, thereby offering a significant advantage for CER. To substantiate this, we perform a comprehensive analysis, comparing polyglot, monolingual, and speaker recognition SFMs through extensive experiments on a benchmark CER dataset across varying audio durations (1 sec, 500 ms, and 250 ms). The results consistently demonstrate the superiority of polyglot SFMs, outperforming their counterparts across all audio lengths and excelling even with extremely short-duration inputs. These findings pave the way for adaptation of SFMs in setting up new benchmarks for CER. </p>
<blockquote>
<p>本文研究了用于群体情绪识别（CER）的多语种（多语言）语音基础模型（SFM）。我们假设在多种语言、口音和语音模式上预先训练的多语种SFM特别擅长处理群体设置中所特有的嘈杂和复杂的声学环境，从而为CER提供显著优势。为了证实这一点，我们进行了全面的分析，通过基准CER数据集上的大量实验对多语种、单语种和说话人识别SFM进行了比较，实验涉及不同的音频持续时间（如1秒、500毫秒和250毫秒）。结果一致表明多语种SFM的优越性，在所有音频长度上均优于其同类模型，即使在极短的输入时间内也表现出色。这些发现为SFM在建立新的CER基准方面提供了方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16329v1">PDF</a> Accepted to APSIPA-ASC 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了多语言语音基础模型（SFMs）在群体情绪识别（CER）中的应用。研究假设基于多种语言、口音和语音模式进行预训练的多语言SFMs在处理具有噪音和复杂特性的群体环境时具有优势，因此特别适合CER。通过基准CER数据集上的实验验证，对比多语言、单语和语音识别SFMs在不同音频时长（1秒、500毫秒和250毫秒）的表现，结果显示多语言SFMs表现卓越，在所有音频长度上均优于其他模型，即使在极短的输入时长中也表现出色。这为SFMs在设立新的CER基准上提供了方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文研究了多语言语音基础模型（SFMs）在群体情绪识别（CER）领域的应用。</li>
<li>假设多语言SFMs能很好地处理具有噪音和复杂特性的群体环境，对CER有优势。</li>
<li>通过实验验证了多语言SFMs在基准CER数据集上的表现，对比了多语言、单语和语音识别模型。</li>
<li>实验结果表明，多语言SFMs在所有音频长度上均表现卓越，尤其在极短的输入时长中。</li>
<li>多语言SFMs的优越性体现在其能够识别不同语言和口音的语音模式。</li>
<li>研究为SFMs在设立新的CER基准上提供了方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16329">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16329v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16329v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16329v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16329v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16329v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16329v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FocalCodec-Stream-Streaming-Low-Bitrate-Speech-Coding-via-Causal-Distillation"><a href="#FocalCodec-Stream-Streaming-Low-Bitrate-Speech-Coding-via-Causal-Distillation" class="headerlink" title="FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal   Distillation"></a>FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal   Distillation</h2><p><strong>Authors:Luca Della Libera, Cem Subakan, Mirco Ravanelli</strong></p>
<p>Neural audio codecs are a fundamental component of modern generative audio pipelines. Although recent codecs achieve strong low-bitrate reconstruction and provide powerful representations for downstream tasks, most are non-streamable, limiting their use in real-time applications. We present FocalCodec-Stream, a hybrid codec based on focal modulation that compresses speech into a single binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our approach combines multi-stage causal distillation of WavLM with targeted architectural improvements, including a lightweight refiner module that enhances quality under latency constraints. Experiments show that FocalCodec-Stream outperforms existing streamable codecs at comparable bitrates, while preserving both semantic and acoustic information. The result is a favorable trade-off between reconstruction quality, downstream task performance, latency, and efficiency. Code and checkpoints will be released at <a target="_blank" rel="noopener" href="https://github.com/lucadellalib/focalcodec">https://github.com/lucadellalib/focalcodec</a>. </p>
<blockquote>
<p>神经音频编码是现代生成音频管道的基本组成部分。尽管最近的编码技术实现了强大的低比特率重建，并为下游任务提供了强大的表现，但大多数都是非流式的，限制了它们在实时应用中的使用。我们提出了FocalCodec-Stream，这是一种基于焦点调制的混合编码技术，它将语音压缩到一个单一的二进制编码本中，以0.55-0.8 kbps的比特率运行，理论延迟为80毫秒。我们的方法结合了多阶段因果蒸馏的WavLM技术，并针对架构进行了改进，包括一个轻量级的精炼模块，在延迟限制下提高质量。实验表明，在可比的比特率下，FocalCodec-Stream的性能优于现有的流式编码技术，同时保留了语义和声音信息。其结果是重建质量、下游任务性能、延迟和效率之间的有利权衡。代码和检查点将在<a target="_blank" rel="noopener" href="https://github.com/lucadellalib/focalcodec%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/lucadellalib/focalcodec发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16195v1">PDF</a> 5 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>神经网络音频编解码器是现代生成式音频管道的基本组成部分。现有编解码器虽然能够在低码率下实现高质量的重建，并为下游任务提供强大的表征，但大多非流式传输，限制了其在实时应用中的使用。本文提出一种基于焦点调制的混合编解码器FocalCodec-Stream，以0.55~0.8kbps的码率将语音压缩到一个单一的二进制代码簿中，理论延迟为80毫秒。我们的方法结合了多阶段因果蒸馏的WavLM技术，并进行了有针对性的架构改进，包括一个轻量级的精炼模块，以提高在延迟限制下的质量。实验表明，FocalCodec-Stream在可比的码率下优于现有的流式编解码器，同时保留了语义和声音信息。结果是在重建质量、下游任务性能、延迟和效率之间取得了有利的平衡。代码和检查点将在<a target="_blank" rel="noopener" href="https://github.com/lucadellalib/focalcodec%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/lucadellalib/focalcodec发布。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络音频编解码器在现代生成式音频管道中扮演重要角色。</li>
<li>现有编解码器虽然功能强大，但大多数不适用于实时流式传输应用。</li>
<li>FocalCodec-Stream是一种基于焦点调制的混合编解码器，可在低码率下实现语音压缩。</li>
<li>FocalCodec-Stream具有理论延迟低的优点。</li>
<li>该方法结合了多阶段因果蒸馏技术和针对性的架构改进。</li>
<li>FocalCodec-Stream在比较实验中表现出优良的性能，优于现有的流式编解码器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16195">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16195v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16195v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16195v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16195v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Are-Multimodal-Foundation-Models-All-That-Is-Needed-for-Emofake-Detection"><a href="#Are-Multimodal-Foundation-Models-All-That-Is-Needed-for-Emofake-Detection" class="headerlink" title="Are Multimodal Foundation Models All That Is Needed for Emofake   Detection?"></a>Are Multimodal Foundation Models All That Is Needed for Emofake   Detection?</h2><p><strong>Authors:Mohd Mujtaba Akhtar,  Girish, Orchid Chetia Phukan, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru</strong></p>
<p>In this work, we investigate multimodal foundation models (MFMs) for EmoFake detection (EFD) and hypothesize that they will outperform audio foundation models (AFMs). MFMs due to their cross-modal pre-training, learns emotional patterns from multiple modalities, while AFMs rely only on audio. As such, MFMs can better recognize unnatural emotional shifts and inconsistencies in manipulated audio, making them more effective at distinguishing real from fake emotional expressions. To validate our hypothesis, we conduct a comprehensive comparative analysis of state-of-the-art (SOTA) MFMs (e.g. LanguageBind) alongside AFMs (e.g. WavLM). Our experiments confirm that MFMs surpass AFMs for EFD. Beyond individual foundation models (FMs) performance, we explore FMs fusion, motivated by findings in related research areas such synthetic speech detection and speech emotion recognition. To this end, we propose SCAR, a novel framework for effective fusion. SCAR introduces a nested cross-attention mechanism, where representations from FMs interact at two stages sequentially to refine information exchange. Additionally, a self-attention refinement module further enhances feature representations by reinforcing important cross-FM cues while suppressing noise. Through SCAR with synergistic fusion of MFMs, we achieve SOTA performance, surpassing both standalone FMs and conventional fusion approaches and previous works on EFD. </p>
<blockquote>
<p>在这项工作中，我们研究了用于情绪伪造检测（EFD）的多模态基础模型（MFMs），并假设它们将优于音频基础模型（AFMs）。MFMs由于其跨模态的预训练，可以从多个模态学习情感模式，而AFMs仅依赖于音频。因此，MFMs能够更好地识别操纵音频中的不自然情感变化和不一致性，使其在区分真实和虚假情感表达方面更加有效。为了验证我们的假设，我们对最先进的多模态基础模型（例如LanguageBind）和音频基础模型（例如WavLM）进行了全面的比较分析。我们的实验证实，对于EFD，MFMs的表现优于AFMs。除了单个基础模型（FMs）的性能外，我们还探索了基础模型的融合，这受到了合成语音检测和语音情感识别等相关研究领域的发现的启发。为此，我们提出了SCAR，这是一个有效的融合框架。SCAR引入了一种嵌套的交叉注意机制，其中来自基础模型的表示在两个阶段上顺序交互，以精炼信息交换。此外，自注意精炼模块通过加强重要的跨基础模型线索并抑制噪声，进一步增强了特征表示。通过SCAR协同融合MFMs，我们达到了最先进的性能，超越了单独的FMs和传统融合方法以及之前在EFD上的工作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16193v1">PDF</a> Accepted to APSIPA-ASC 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了多模态基础模型（MFMs）在EmoFake检测（EFD）中的应用，并假设它们的表现会优于音频基础模型（AFMs）。MFMs由于跨模态预训练，能从多种模态学习情感模式，而AFMs仅依赖音频。因此，MFMs能更好地识别操纵音频中的不自然情感变化和不一致性，更有效地区分真实和虚假的情感表达。为验证假设，作者对最新MFMs（如LanguageBind）和AFMs（如WavLM）进行了全面比较分析。实验证实MFMs在EFD方面超越AFMs。除了单个基础模型（FMs）的性能外，作者还探讨了FMs融合，受到相关研究领域如合成语音检测和语音情感识别发现的启发。为此，作者提出了SCAR，一种有效的融合框架。SCAR引入了一种嵌套交叉注意机制，其中来自FMs的表示在两个阶段顺序交互，以改进信息交换。此外，自注意细化模块通过加强重要的跨FM线索同时抑制噪声，进一步增强了特征表示。通过SCAR协同融合MFMs，作者实现了最新性能，超越了单独的FMs和传统融合方法以及之前的EFD工作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文研究了多模态基础模型（MFMs）在EmoFake检测中的应用，并假设它们相较于音频基础模型（AFMs）会有更好的表现。</li>
<li>MFMs由于能跨模态学习情感模式，可以更有效地识别不自然情感变化和操纵音频中的不一致性。</li>
<li>通过实验验证了MFMs在EFD方面的性能超越了AFMs。</li>
<li>除了单个基础模型性能外，还探讨了基础模型融合，受到合成语音检测和语音情感识别研究的启发。</li>
<li>提出了SCAR，一种有效的融合框架，通过嵌套交叉注意机制和自注意细化模块实现特征表示的增强和信息的精细交换。</li>
<li>SCAR协同融合MFMs达到了最新性能水平，超越了单独的FMs、传统融合方法以及之前的EFD工作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16193">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16193v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16193v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16193v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Rethinking-Cross-Corpus-Speech-Emotion-Recognition-Benchmarking-Are-Paralinguistic-Pre-Trained-Representations-Sufficient"><a href="#Rethinking-Cross-Corpus-Speech-Emotion-Recognition-Benchmarking-Are-Paralinguistic-Pre-Trained-Representations-Sufficient" class="headerlink" title="Rethinking Cross-Corpus Speech Emotion Recognition Benchmarking: Are   Paralinguistic Pre-Trained Representations Sufficient?"></a>Rethinking Cross-Corpus Speech Emotion Recognition Benchmarking: Are   Paralinguistic Pre-Trained Representations Sufficient?</h2><p><strong>Authors:Orchid Chetia Phukan, Mohd Mujtaba Akhtar,  Girish, Swarup Ranjan Behera, Parabattina Bhagath, Pailla Balakrishna Reddy, Arun Balaji Buduru</strong></p>
<p>Recent benchmarks evaluating pre-trained models (PTMs) for cross-corpus speech emotion recognition (SER) have overlooked PTM pre-trained for paralinguistic speech processing (PSP), raising concerns about their reliability, since SER is inherently a paralinguistic task. We hypothesize that PSP-focused PTM will perform better in cross-corpus SER settings. To test this, we analyze state-of-the-art PTMs representations including paralinguistic, monolingual, multilingual, and speaker recognition. Our results confirm that TRILLsson (a paralinguistic PTM) outperforms others, reinforcing the need to consider PSP-focused PTMs in cross-corpus SER benchmarks. This study enhances benchmark trustworthiness and guides PTMs evaluations for reliable cross-corpus SER. </p>
<blockquote>
<p>最近对跨语料库语音情感识别（SER）的预训练模型（PTM）的基准测试忽略了针对副语言语音处理（PSP）的PTM预训练，这引发了对其可靠性的担忧，因为SER本质上是一项副语言任务。我们假设以PSP为重点的PTM将在跨语料库SER设置中表现更好。为了测试这一点，我们分析了包括副语言、单语、多语和语音识别在内的最新PTM表示。我们的结果证实，TRILLsson（一种副语言PTM）表现优于其他模型，这强化了需要在跨语料库SER基准测试中考虑以PSP为重点的PTM的必要性。本研究提高了基准测试的可靠性，并为可靠的跨语料库SER提供了PTM评估指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16182v1">PDF</a> Accepted to APSIPA-ASC 2025</p>
<p><strong>Summary</strong></p>
<p>本文关注预训练模型（PTMs）在跨语料库语音情感识别（SER）中的表现，并指出当前基准测试忽略了针对副语言语音处理（PSP）的预训练模型，这引发了对其可靠性的担忧。研究假设针对PSP的PTM在跨语料库SER设置中表现更好。通过对比分析包括副语言、单语、多语和语音识别在内的最新PTM表示，发现TRILLsson（一种副语言PTM）表现最佳，强调在跨语料库SER基准测试中需要考虑针对PSP的PTM。该研究提高了基准测试的可靠性，并为PTMs在跨语料库SER中的评估提供了指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前基准测试在评估预训练模型（PTMs）进行跨语料库语音情感识别（SER）时，忽视了针对副语言语音处理（PSP）的预训练模型。</li>
<li>研究假设指出，针对PSP的PTM在跨语料库SER设置中的表现会更好。</li>
<li>通过对比分析，发现TRILLsson（一种副语言PTM）在跨语料库SER中的表现优于其他模型。</li>
<li>研究结果强调了在进行跨语料库SER的基准测试时，需要考虑副语言语音处理的重要性。</li>
<li>此研究提高了基准测试的可靠性，为评估PTMs在跨语料库SER中的性能提供了更准确的指导。</li>
<li>该研究为未来的研究提供了方向，鼓励更多研究者关注副语言语音处理在语音情感识别中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16182">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16182v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16182v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16182v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Think-Verbalize-then-Speak-Bridging-Complex-Thoughts-and-Comprehensible-Speech"><a href="#Think-Verbalize-then-Speak-Bridging-Complex-Thoughts-and-Comprehensible-Speech" class="headerlink" title="Think, Verbalize, then Speak: Bridging Complex Thoughts and   Comprehensible Speech"></a>Think, Verbalize, then Speak: Bridging Complex Thoughts and   Comprehensible Speech</h2><p><strong>Authors:Sang Hoon Woo, Sehun Lee, Kang-wook Kim, Gunhee Kim</strong></p>
<p>Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose Think-Verbalize-Speak, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is verbalizing, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce ReVerT, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at <a target="_blank" rel="noopener" href="https://yhytoto12.github.io/TVS-ReVerT">https://yhytoto12.github.io/TVS-ReVerT</a> </p>
<blockquote>
<p>口语对话系统越来越多地采用大型语言模型（LLM）来利用它们的先进推理能力。然而，直接将LLM应用于口语交流往往会产生不理想的结果，这是由于最佳文本输出和口语表达之间存在不匹配。虽然现有的方法使LLM适应产生友好的口语输出，但它们对推理性能的影响仍未得到充分探索。在这项工作中，我们提出了Think-Verbalize-Speak框架，通过将推理与口语表达分离来保留LLM的完整推理能力。我们的方法的核心是“口语化”，这是一个将思想翻译成自然、准备口语的文本的中间步骤。我们还引入了ReVerT，这是一种基于增量和异步摘要的延迟效率高的口语化工具。跨多个基准的实验表明，我们的方法可以在不影响推理的情况下提高口语的自然度和简洁性。数据集和项目页面以及源代码可在<a target="_blank" rel="noopener" href="https://yhytoto12.github.io/TVS-ReVerT%E6%9F%A5%E7%9C%8B%E3%80%82">https://yhytoto12.github.io/TVS-ReVerT查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16028v1">PDF</a> EMNLP 2025 Main. Project page: <a target="_blank" rel="noopener" href="https://yhytoto12.github.io/TVS-ReVerT">https://yhytoto12.github.io/TVS-ReVerT</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLMs）在口语对话系统中的应用，并指出了直接应用LLMs在口语交流中的不足。为解决这个问题，文章提出了Think-Verbalize-Speak框架，将推理与口语表达分离，以保留LLMs的完整推理能力。其中，核心是“言语化”这一中间步骤，将思想转化为自然、适合口语的文本。同时，文章还介绍了基于增量和异步摘要的延迟效率高的言语化工具ReVerT。实验表明，该方法在提高口语自然性和简洁性的同时，对推理能力的影响较小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在口语对话系统中的应用日益广泛，但直接应用于口语交流会产生不理想的结果。</li>
<li>现有方法虽然尝试将LLMs适应于口语输出，但其对推理性能的影响尚未得到充分探索。</li>
<li>本文提出了Think-Verbalize-Speak框架，将推理与口语表达分离，以充分利用LLMs的推理能力。</li>
<li>“言语化”是框架的核心，能将思想转化为自然、适合口语的文本。</li>
<li>引入了一种新的言语化工具ReVerT，基于增量和异步摘要，具有较低的延迟。</li>
<li>实验表明，该方法在增强口语自然性和简洁性的同时，对推理能力的影响较小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16028">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16028v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16028v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16028v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16028v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Interpreting-the-Role-of-Visemes-in-Audio-Visual-Speech-Recognition"><a href="#Interpreting-the-Role-of-Visemes-in-Audio-Visual-Speech-Recognition" class="headerlink" title="Interpreting the Role of Visemes in Audio-Visual Speech Recognition"></a>Interpreting the Role of Visemes in Audio-Visual Speech Recognition</h2><p><strong>Authors:Aristeidis Papadopoulos, Naomi Harte</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) models have surpassed their audio-only counterparts in terms of performance. However, the interpretability of AVSR systems, particularly the role of the visual modality, remains under-explored. In this paper, we apply several interpretability techniques to examine how visemes are encoded in AV-HuBERT a state-of-the-art AVSR model. First, we use t-distributed Stochastic Neighbour Embedding (t-SNE) to visualize learned features, revealing natural clustering driven by visual cues, which is further refined by the presence of audio. Then, we employ probing to show how audio contributes to refining feature representations, particularly for visemes that are visually ambiguous or under-represented. Our findings shed light on the interplay between modalities in AVSR and could point to new strategies for leveraging visual information to improve AVSR performance. </p>
<blockquote>
<p>视听语音识别（AVSR）模型的性能已经超越了仅依赖音频的模型。然而，AVSR系统的可解释性，特别是视觉模式的作用，仍然被探索得不够深入。在本文中，我们应用了几种可解释性技术来检查AV-HuBERT这一先进的AVSR模型中是如何编码音素的。首先，我们使用t分布随机邻域嵌入（t-SNE）来可视化学习到的特征，揭示由视觉线索驱动的天然聚类，这种聚类在音频存在的情况下得到了进一步的优化。然后，我们通过探测来展示音频是如何对优化特征表示做出贡献的，特别是对于视觉上模糊或表示不足的音素。我们的研究揭示了AVSR中模态之间的相互作用，并可能指向利用视觉信息提高AVSR性能的新策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.16023v1">PDF</a> Accepted into Automatic Speech Recognition and Understanding- ASRU   2025</p>
<p><strong>总结</strong><br>    AVSR模型性能优于仅依赖音频的模型，但其可解释性，特别是视觉模式的作用，仍待探索。本文采用多种解释技术来研究AV-HuBERT这一先进的AVSR模型中是如何编码视位的。通过t-SNE可视化学习特征，揭示由视觉线索驱动的天然聚类，并可通过音频进一步优化。通过探测展示音频如何改善特征表示，特别是针对视觉上模糊或表示不足的视位。本文揭示了AVSR中模态之间的相互作用，并可能指向利用视觉信息提高AVSR性能的新策略。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>AVSR模型性能优于音频模型。</li>
<li>可解释性是AVSR模型的一个重要但尚未完全探索的方面。</li>
<li>使用t-SNE可视化学习特征揭示自然聚类，这些聚类是由视觉线索驱动的。</li>
<li>音频可以进一步优化由视觉线索驱动的聚类。</li>
<li>通过探测显示音频如何改善特征表示，特别是对模糊或表示不足的视位。</li>
<li>揭示了AVSR中模态之间的相互作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16023">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16023v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16023v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16023v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16023v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16023v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16023v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.16023v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Compose-Yourself-Average-Velocity-Flow-Matching-for-One-Step-Speech-Enhancement"><a href="#Compose-Yourself-Average-Velocity-Flow-Matching-for-One-Step-Speech-Enhancement" class="headerlink" title="Compose Yourself: Average-Velocity Flow Matching for One-Step Speech   Enhancement"></a>Compose Yourself: Average-Velocity Flow Matching for One-Step Speech   Enhancement</h2><p><strong>Authors:Gang Yang, Yue Lei, Wenxin Tai, Jin Wu, Jia Chen, Ting Zhong, Fan Zhou</strong></p>
<p>Diffusion and flow matching (FM) models have achieved remarkable progress in speech enhancement (SE), yet their dependence on multi-step generation is computationally expensive and vulnerable to discretization errors. Recent advances in one-step generative modeling, particularly MeanFlow, provide a promising alternative by reformulating dynamics through average velocity fields. In this work, we present COSE, a one-step FM framework tailored for SE. To address the high training overhead of Jacobian-vector product (JVP) computations in MeanFlow, we introduce a velocity composition identity to compute average velocity efficiently, eliminating expensive computation while preserving theoretical consistency and achieving competitive enhancement quality. Extensive experiments on standard benchmarks show that COSE delivers up to 5x faster sampling and reduces training cost by 40%, all without compromising speech quality. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ICDM-UESTC/COSE">https://github.com/ICDM-UESTC/COSE</a>. </p>
<blockquote>
<p>扩散和流匹配（FM）模型在语音增强（SE）方面取得了显著的进步，但它们对多步生成的依赖在计算上成本高昂，并容易受到离散化误差的影响。最近一步生成建模的进展，特别是MeanFlow，通过平均速度场重新表述动态，提供了一个有前景的替代方案。在这项工作中，我们提出了COSE，这是一个专为SE设计的一步FM框架。为了解决MeanFlow中雅可比向量积（JVP）计算的高训练开销问题，我们引入了一个速度组合恒等式来有效地计算平均速度，这既消除了昂贵的计算，又保持了理论的一致性，并实现了具有竞争力的增强质量。在标准基准测试上的大量实验表明，COSE实现了高达5倍的更快采样速度，并将训练成本降低了40%，所有这一切都不影响语音质量。代码可在<a target="_blank" rel="noopener" href="https://github.com/ICDM-UESTC/COSE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ICDM-UESTC/COSE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15952v2">PDF</a> 5 pages, 2 figures, submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对语音增强的新颖一步扩散与流匹配（FM）框架COSE。该框架旨在解决传统的多步生成扩散模型的高计算成本和离散化误差问题。通过引入速度组成恒等式，COSE能够高效计算平均速度，减少昂贵的计算成本，同时保留理论一致性并达到有竞争力的增强质量。实验结果表明，COSE可实现高达5倍的快速采样并降低训练成本40%，同时不损害语音质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>COSE是一个针对语音增强的一步FM框架，旨在解决多步生成扩散模型的高计算成本和离散化误差问题。</li>
<li>COSE引入速度组成恒等式，高效计算平均速度，减少计算成本。</li>
<li>COSE保留了理论一致性并实现了有竞争力的增强质量。</li>
<li>广泛的实验结果表明，COSE能够实现快速采样并降低训练成本。</li>
<li>COSE在标准基准测试上表现优异，最高可达到5倍更快的采样速度。</li>
<li>COSE可降低训练成本达40%，同时不损害语音质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15952">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15952v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15952v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15952v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DISPATCH-Distilling-Selective-Patches-for-Speech-Enhancement"><a href="#DISPATCH-Distilling-Selective-Patches-for-Speech-Enhancement" class="headerlink" title="DISPATCH: Distilling Selective Patches for Speech Enhancement"></a>DISPATCH: Distilling Selective Patches for Speech Enhancement</h2><p><strong>Authors:Dohwan Kim, Jung-Woo Choi</strong></p>
<p>In speech enhancement, knowledge distillation (KD) compresses models by transferring a high-capacity teacher’s knowledge to a compact student. However, conventional KD methods train the student to mimic the teacher’s output entirely, which forces the student to imitate the regions where the teacher performs poorly and to apply distillation to the regions where the student already performs well, which yields only marginal gains. We propose Distilling Selective Patches (DISPatch), a KD framework for speech enhancement that applies the distillation loss to spectrogram patches where the teacher outperforms the student, as determined by a Knowledge Gap Score. This approach guides optimization toward areas with the most significant potential for student improvement while minimizing the influence of regions where the teacher may provide unreliable instruction. Furthermore, we introduce Multi-Scale Selective Patches (MSSP), a frequency-dependent method that uses different patch sizes across low- and high-frequency bands to account for spectral heterogeneity. We incorporate DISPatch into conventional KD methods and observe consistent gains in compact students. Moreover, integrating DISPatch and MSSP into a state-of-the-art frequency-dependent KD method considerably improves performance across all metrics. </p>
<blockquote>
<p>在语音增强领域，知识蒸馏（KD）通过转移高容量教师的知识到紧凑的学生模型来压缩模型。然而，传统的知识蒸馏方法训练学生模型去完全模仿教师的输出，这导致学生模型被迫模仿教师表现不佳的区域，并对已经表现良好的区域应用蒸馏，只带来了微小的收益。我们提出了针对语音增强的知识蒸馏框架——蒸馏选择性补丁（DISPatch），该框架将蒸馏损失应用于教师表现优于学生的频谱图补丁上，通过知识差距分数来确定。这种方法引导优化走向对学生改进潜力最大的区域，同时最小化教师可能在不可靠区域提供指导的影响。此外，我们引入了多尺度选择性补丁（MSSP），这是一种频率依赖的方法，使用不同大小的补丁来适应低频和高频带，以应对频谱异质性。我们将DISPatch纳入传统知识蒸馏方法，并观察到紧凑学生模型的持续收益。此外，将DISPatch和MSSP集成到最先进的频率依赖知识蒸馏方法中，可全面提高所有指标的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15922v1">PDF</a> submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>知识蒸馏（KD）在语音增强中通过转移高性能教师的知识来压缩模型。然而，传统的KD方法完全训练学生模型模仿教师的输出，这导致学生在教师表现不佳的区域进行模仿，并在学生已经表现良好的区域应用蒸馏，只产生微小的收益。为此，我们提出了面向语音增强的知识蒸馏框架——选择性蒸馏补丁（DISPatch），该框架将蒸馏损失应用于教师在学生模型上表现更好的频谱图补丁上，通过知识差距分数进行确定。此方法引导优化朝着学生模型潜力最大的区域发展，同时最小化教师可能在不可靠区域提供指导的影响。此外，我们还引入了多尺度选择性补丁（MSSP），一种频率依赖的方法，使用低和高频带的不同补丁大小来处理频谱异质性。我们将DISPatch纳入传统KD方法，观察到紧凑学生模型的持续收益。而且，将DISPatch和MSSP集成到最先进的频率依赖KD方法中，可全面改善所有指标的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>知识蒸馏（KD）用于语音增强中的模型压缩，通过转移教师的知识。</li>
<li>传统KD方法存在缺陷：学生模型完全模仿教师输出，导致在教师对区域表现不佳时学习效果降低。</li>
<li>提出了DISPatch框架，将蒸馏损失应用于教师在学生上表现更好的频谱图补丁上。</li>
<li>DISPatch框架能引导优化朝向学生模型潜力最大的区域发展，并减少教师不可靠指导的影响。</li>
<li>引入了MSSP方法，这是一种频率依赖的方法，在不同频带使用不同的补丁大小，以处理频谱异质性。</li>
<li>将DISPatch纳入传统KD方法可以提高紧凑学生模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15922">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15922v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15922v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15922v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15922v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-Curious-Case-of-Visual-Grounding-Different-Effects-for-Speech-and-Text-based-Language-Encoders"><a href="#The-Curious-Case-of-Visual-Grounding-Different-Effects-for-Speech-and-Text-based-Language-Encoders" class="headerlink" title="The Curious Case of Visual Grounding: Different Effects for Speech- and   Text-based Language Encoders"></a>The Curious Case of Visual Grounding: Different Effects for Speech- and   Text-based Language Encoders</h2><p><strong>Authors:Adrian Sauter, Willem Zuidema, Marianne de Heer Kloots</strong></p>
<p>How does visual information included in training affect language processing in audio- and text-based deep learning models? We explore how such visual grounding affects model-internal representations of words, and find substantially different effects in speech- vs. text-based language encoders. Firstly, global representational comparisons reveal that visual grounding increases alignment between representations of spoken and written language, but this effect seems mainly driven by enhanced encoding of word identity rather than meaning. We then apply targeted clustering analyses to probe for phonetic vs. semantic discriminability in model representations. Speech-based representations remain phonetically dominated with visual grounding, but in contrast to text-based representations, visual grounding does not improve semantic discriminability. Our findings could usefully inform the development of more efficient methods to enrich speech-based models with visually-informed semantics. </p>
<blockquote>
<p>包含视觉信息的训练对音频和文字深度学习中模型的语言处理如何产生影响？我们探索这种视觉锚定对单词内部模型表示的影响，发现语音和文字语言编码器中的不同影响。首先，全局代表性比较显示，视觉锚定增加了口语和书面语言表述之间的对齐程度，但这种效果似乎主要由增强词身份编码所致，而非词义编码。接着，我们应用有针对性的聚类分析来探查模型表示的语音与语义区分能力。在语音模型中，视觉锚定仍然以语音为主导，但与文字模型不同，视觉锚定不会提高语义区分能力。我们的研究可以为开发更多利用视觉语义丰富语音模型的方法提供有益的启示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15837v1">PDF</a> 5 pages, 3 figures, Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>视觉信息对音频和文本深度学习中语言处理的影响研究。研究发现视觉接地可以增强口语和书面语言表示的对齐，但对语音为基础的语言编码器的效果和对意义编码的影响相对有限。此外，虽然语音表征依然保持音韵为主的特点，但视觉接地对语音模型的音韵鉴别能力影响不大。研究结果为如何更好地利用视觉信息丰富语音模型提供了启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉信息对语言处理的影响在音频和文本深度学习中受到关注。</li>
<li>视觉接地可以增强口语和书面语言表示的对齐。</li>
<li>视觉接地主要通过增强单词身份的编码实现这一效果。</li>
<li>语音表征以音韵为主，视觉接地对语音模型的音韵鉴别能力影响有限。</li>
<li>在文本表征中，视觉接地不改善语义鉴别能力。</li>
<li>研究结果对于如何更有效地结合视觉信息来丰富语音模型具有指导意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15837">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15837v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15837v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15837v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CompSpoof-A-Dataset-and-Joint-Learning-Framework-for-Component-Level-Audio-Anti-spoofing-Countermeasures"><a href="#CompSpoof-A-Dataset-and-Joint-Learning-Framework-for-Component-Level-Audio-Anti-spoofing-Countermeasures" class="headerlink" title="CompSpoof: A Dataset and Joint Learning Framework for Component-Level   Audio Anti-spoofing Countermeasures"></a>CompSpoof: A Dataset and Joint Learning Framework for Component-Level   Audio Anti-spoofing Countermeasures</h2><p><strong>Authors:Xueping Zhang, Liwei Jin, Yechen Wang, Linxi Li, Ming Li</strong></p>
<p>Component-level audio Spoofing (Comp-Spoof) targets a new form of audio manipulation where only specific components of a signal, such as speech or environmental sound, are forged or substituted while other components remain genuine. Existing anti-spoofing datasets and methods treat an utterance or a segment as entirely bona fide or entirely spoofed, and thus cannot accurately detect component-level spoofing. To address this, we construct a new dataset, CompSpoof, covering multiple combinations of bona fide and spoofed speech and environmental sound. We further propose a separation-enhanced joint learning framework that separates audio components apart and applies anti-spoofing models to each one. Joint learning is employed, preserving information relevant for detection. Extensive experiments demonstrate that our method outperforms the baseline, highlighting the necessity of separate components and the importance of detecting spoofing for each component separately. Datasets and code are available at: <a target="_blank" rel="noopener" href="https://github.com/XuepingZhang/CompSpoof">https://github.com/XuepingZhang/CompSpoof</a>. </p>
<blockquote>
<p>音频组件级欺骗（Comp-Spoof）针对的是一种新型音频操作形式，在这种形式中，只有信号中的特定组件（如语音或环境声音）被伪造或替换，而其他组件则保持真实。现有的防欺骗数据集和方法将整个话语或片段视为完全真实或完全欺骗，因此无法准确检测组件级别的欺骗。为了解决这一问题，我们构建了一个新的数据集CompSpoof，涵盖了真实和欺骗语音以及环境声音的多种组合。我们还提出了一个增强分离能力的联合学习框架，该框架将音频组件分开，并对每个组件应用反欺骗模型。采用联合学习的方法，保留与检测相关的信息。大量实验表明，我们的方法优于基线方法，突显了单独组件的必要性以及为每个组件单独检测欺骗的重要性。数据集和代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/XuepingZhang/CompSpoof">https://github.com/XuepingZhang/CompSpoof</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15804v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>音频组件级欺骗（Comp-Spoof）是一种新型音频操作形式，其中只对音频信号中的特定部分（如语音或环境声音）进行伪造或替换，而其他部分保持真实。现有的反欺骗数据集和方法将整个语句或片段视为完全真实或完全欺骗，因此无法准确检测组件级别的欺骗。为解决此问题，我们构建了新的CompSpoof数据集，涵盖多种真实和欺骗语音和环境声音的混合组合。我们进一步提出了分离增强联合学习框架，该框架将音频组件分开并对每个组件应用反欺骗模型进行单独检测。大量实验表明，我们的方法优于基线方法，突出了分离组件的必要性以及单独对每个组件进行欺骗检测的实用性。数据集和代码可在此处获取：<a target="_blank" rel="noopener" href="https://github.com/XuepingZhang/CompSpoof">https://github.com/XuepingZhang/CompSpoof</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频组件级欺骗是一种新出现的音频操作方式，特定针对音频信号的部分组件进行伪造或替换。</li>
<li>现有反欺骗数据集和方法无法准确检测组件级别的欺骗，因为它们将整个语句或片段视为完全真实或完全欺骗。</li>
<li>为应对这一挑战，构建了名为CompSpoof的新数据集，涵盖多种真实和欺骗语音及环境声音的混合组合。</li>
<li>提出了一种分离增强联合学习框架，用于分离音频组件并对每个组件进行反欺骗模型检测。</li>
<li>该框架通过联合学习保留与检测相关的信息，以提高检测准确性。</li>
<li>实验结果表明，新方法优于基线方法，强调了对每个组件进行单独检测和分离组件的必要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15804">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15804v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15804v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15804v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15804v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15804v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EmoQ-Speech-Emotion-Recognition-via-Speech-Aware-Q-Former-and-Large-Language-Model"><a href="#EmoQ-Speech-Emotion-Recognition-via-Speech-Aware-Q-Former-and-Large-Language-Model" class="headerlink" title="EmoQ: Speech Emotion Recognition via Speech-Aware Q-Former and Large   Language Model"></a>EmoQ: Speech Emotion Recognition via Speech-Aware Q-Former and Large   Language Model</h2><p><strong>Authors:Yiqing Yang, Man-Wai Mak</strong></p>
<p>The performance of speech emotion recognition (SER) is limited by the insufficient emotion information in unimodal systems and the feature alignment difficulties in multimodal systems. Recently, multimodal large language models (MLLMs) have made progress in SER. However, MLLMs still suffer from hallucination and misclassification problems in complex emotion reasoning. To address these problems, we propose an MLLM-based framework called EmoQ, which generates query embeddings that fuse multimodal information through an EmoQ-Former and uses multi-objective affective learning (MAL) to achieve co-optimization. The framework also provides a soft-prompt injection strategy to inject multimodal representations into the LLM. This end-to-end architecture achieves state-of-the-art performance on the IEMOCAP and MELD datasets, providing a new multimodal fusion paradigm for SER. </p>
<blockquote>
<p>语音识别情绪识别（SER）的性能受限于单模态系统中情感信息的不足和多模态系统中特征对齐的困难。最近，多模态大型语言模型（MLLM）在SER方面取得了进展。然而，MLLM在复杂情绪推理中仍存在幻觉和误分类问题。为了解决这些问题，我们提出了一种基于MLLM的框架，名为EmoQ。该框架通过EmoQ-Former生成融合多模态信息的查询嵌入，并使用多目标情感学习（MAL）实现协同优化。该框架还提供了一种软提示注入策略，将多模态表示注入LLM中。这种端到端的架构在IEMOCAP和MELD数据集上达到了最先进的性能，为SER提供了新的多模态融合范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15775v1">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>文本中针对语音情感识别（SER）的问题，提出了一个基于多模态大型语言模型（MLLMs）的框架EmoQ。该框架融合了多模态信息，采用EmoQ-Former生成查询嵌入，并利用多目标情感学习（MAL）实现协同优化，以解决传统系统中情感信息不足和特征对齐困难的问题。此外，该框架还提供了一种软提示注入策略，将多模态表示注入到LLM中。该框架在IEMOCAP和MELD数据集上取得了最先进的性能表现，为SER提供了新的多模态融合范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音情感识别（SER）受限于单模态系统中情感信息的不足和多模态系统中特征对齐的困难。</li>
<li>多模态大型语言模型（MLLMs）在SER方面取得了进展，但仍存在推理复杂情绪时的幻觉和误分类问题。</li>
<li>提出的EmoQ框架基于MLLMs，通过EmoQ-Former生成查询嵌入，融合多模态信息。</li>
<li>EmoQ框架采用多目标情感学习（MAL）实现协同优化。</li>
<li>框架中的软提示注入策略能够将多模态表示注入到大型语言模型（LLM）中。</li>
<li>EmoQ框架在IEMOCAP和MELD数据集上实现了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15775v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15775v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15775v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Interpretable-Modeling-of-Articulatory-Temporal-Dynamics-from-real-time-MRI-for-Phoneme-Recognition"><a href="#Interpretable-Modeling-of-Articulatory-Temporal-Dynamics-from-real-time-MRI-for-Phoneme-Recognition" class="headerlink" title="Interpretable Modeling of Articulatory Temporal Dynamics from real-time   MRI for Phoneme Recognition"></a>Interpretable Modeling of Articulatory Temporal Dynamics from real-time   MRI for Phoneme Recognition</h2><p><strong>Authors:Jay Park, Hong Nguyen, Sean Foley, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan</strong></p>
<p>Real-time Magnetic Resonance Imaging (rtMRI) visualizes vocal tract action, offering a comprehensive window into speech articulation. However, its signals are high dimensional and noisy, hindering interpretation. We investigate compact representations of spatiotemporal articulatory dynamics for phoneme recognition from midsagittal vocal tract rtMRI videos. We compare three feature types: (1) raw video, (2) optical flow, and (3) six linguistically-relevant regions of interest (ROIs) for articulator movements. We evaluate models trained independently on each representation, as well as multi-feature combinations. Results show that multi-feature models consistently outperform single-feature baselines, with the lowest phoneme error rate (PER) of 0.34 obtained by combining ROI and raw video. Temporal fidelity experiments demonstrate a reliance on fine-grained articulatory dynamics, while ROI ablation studies reveal strong contributions from tongue and lips. Our findings highlight how rtMRI-derived features provide accuracy and interpretability, and establish strategies for leveraging articulatory data in speech processing. </p>
<blockquote>
<p>实时磁共振成像（rtMRI）能够可视化声腔动作，为语音发音提供全面的观察窗口。然而，其信号具有高维度和噪声大的特点，阻碍了对其的解读。我们研究从声腔中部矢状rtMRI视频中识别音素的时空发音动态紧凑表示。我们比较了三种特征类型：（1）原始视频、（2）光流，以及（3）六个与语言相关的感兴趣区域（ROI）的发音运动。我们评估了针对每种表示独立训练的模型，以及多特征组合。结果显示，多特征模型始终优于单特征基线，通过结合ROI和原始视频获得最低的音素错误率（PER）为0.34。时间保真实验证明了精细发音动态的依赖性，而ROI消融研究揭示了舌头和嘴唇的强烈贡献。我们的研究结果表明，rtMRI衍生的特征如何提供准确性和可解释性，并建立了在语音处理中利用发音数据的策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15689v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>实时磁共振成像（rtMRI）能够可视化语音通道的运作，为语音发音提供全面的观察窗口。然而，其信号高维度且嘈杂，阻碍了解读。本研究探讨从矢状面rtMRI视频中识别音素的时空发音动态简洁表示。我们比较了三种特征类型：（1）原始视频、（2）光流，以及（3）与发音相关的六个感兴趣区域（ROI）。我们评估了针对每种表示独立训练模型，以及多特征组合。结果显示，多特征模型始终优于单特征基线，通过结合ROI和原始视频获得最低的音素错误率（PER）为0.34。时间保真度实验表明对精细发音动态的依赖，ROI消融研究揭示了舌头和嘴唇的强烈贡献。本研究结果突显了rtMRI衍生的特征在准确性和可解释性方面的作用，并为利用发音数据在语音处理中制定了策略。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>实时磁共振成像（rtMRI）能够可视化语音通道的运作，提供全面的语音观察。</li>
<li>rtMRI信号高维度且嘈杂，带来解读困难。</li>
<li>研究对比了三种特征类型：原始视频、光流和感兴趣区域（ROI）。</li>
<li>多特征模型表现优于单特征模型。</li>
<li>结合ROI和原始视频获得了最低音素错误率（PER）。</li>
<li>时间保真度实验显示对精细发音动态的依赖。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15689">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15689v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15689v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15689v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15689v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15689v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VOX-KRIKRI-Unifying-Speech-and-Language-through-Continuous-Fusion"><a href="#VOX-KRIKRI-Unifying-Speech-and-Language-through-Continuous-Fusion" class="headerlink" title="VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion"></a>VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion</h2><p><strong>Authors:Dimitrios Damianos, Leon Voukoutis, Georgios Paraskevopoulos, Vassilis Katsouros</strong></p>
<p>We present a multimodal fusion framework that bridges pre-trained decoder-based large language models (LLM) and acoustic encoder-decoder architectures such as Whisper, with the aim of building speech-enabled LLMs. Instead of directly using audio embeddings, we explore an intermediate audio-conditioned text space as a more effective mechanism for alignment. Our method operates fully in continuous text representation spaces, fusing Whisper’s hidden decoder states with those of an LLM through cross-modal attention, and supports both offline and streaming modes. We introduce \textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that our approach effectively aligns representations across modalities. These results highlight continuous space fusion as a promising path for multilingual and low-resource speech LLMs, while achieving state-of-the-art results for Automatic Speech Recognition in Greek, providing an average $\sim20%$ relative improvement across benchmarks. </p>
<blockquote>
<p>我们提出了一种多模态融合框架，旨在构建语音功能的大型语言模型（LLM），该框架将预训练的基于解码器的LLM与诸如whisper这样的声学编码器-解码器架构连接起来。我们并不直接使用音频嵌入，而是探索一个中间的音频条件文本空间，作为一种更有效的对齐机制。我们的方法完全在连续的文本表示空间中进行操作，通过跨模态注意力将whisper的隐藏解码器状态与LLM的状态相融合，并支持离线模式和流式处理模式。我们推出了第一个希腊语语音LLM——VoxKrikri，通过分析表明，我们的方法有效地实现了跨模态的表示对齐。这些结果强调了连续空间融合在多语种和低资源语音LLM方面的前景，同时在希腊语的自动语音识别方面达到了最新水平，在基准测试中实现了平均约20%的相对改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种多模态融合框架，旨在构建语音驱动的预训练大型语言模型（LLM）。该框架通过跨模态注意力机制融合预训练的解码器型LLM和语音编码器解码器架构（如Whisper），并不直接使用音频嵌入，而是探索中间音频调节文本空间作为更有效的对齐机制。该方法完全在连续文本表示空间中运行，支持离线模式和流式处理模式。此外，还推出了希腊语音LLM——VoxKriKri，并通过分析证明了该方法可以有效地实现跨模态表示对齐。该研究为构建多语言和低资源语音LLM提供了一个有前景的路径，并在希腊语的自动语音识别任务上取得了最先进的成果，在多个基准测试中实现了平均约20%的相对改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了多模态融合框架，旨在构建语音驱动的预训练大型语言模型（LLM）。</li>
<li>通过跨模态注意力机制融合预训练解码器型LLM和语音编码器解码器架构。</li>
<li>探索中间音频调节文本空间作为更有效的对齐机制，而不是直接使用音频嵌入。</li>
<li>框架完全在连续文本表示空间中运行，支持离线模式和流式处理模式。</li>
<li>推出希腊语音LLM——VoxKriKri。</li>
<li>跨模态对齐有效实现并提升自动语音识别任务性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15667v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15667v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15667v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15667v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15667v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TISDiSS-A-Training-Time-and-Inference-Time-Scalable-Framework-for-Discriminative-Source-Separation"><a href="#TISDiSS-A-Training-Time-and-Inference-Time-Scalable-Framework-for-Discriminative-Source-Separation" class="headerlink" title="TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation"></a>TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation</h2><p><strong>Authors:Yongsheng Feng, Yuetonghui Xu, Jiehui Luo, Hongjia Liu, Xiaobing Li, Feng Yu, Wei Li</strong></p>
<p>Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly large networks, inflating training and deployment costs. Motivated by recent advances in inference-time scaling for generative modeling, we propose Training-Time and Inference-Time Scalable Discriminative Source Separation (TISDiSS), a unified framework that integrates early-split multi-loss supervision, shared-parameter design, and dynamic inference repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting inference depth without retraining additional models. We further provide systematic analyses of architectural and training choices and show that training with more inference repetitions improves shallow-inference performance, benefiting low-latency applications. Experiments on standard speech separation benchmarks demonstrate state-of-the-art performance with a reduced parameter count, establishing TISDiSS as a scalable and practical framework for adaptive source separation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/WingSingFung/TISDiSS">https://github.com/WingSingFung/TISDiSS</a>. </p>
<blockquote>
<p>源分离是语音、音乐、音频处理中的一项基本任务，它也为训练生成模型提供了更干净、更大的数据。然而，在实践中提高分离性能往往取决于网络规模的扩大，从而导致训练和部署成本的增加。受生成模型推理时间缩放最新进展的启发，我们提出了训练时间和推理时间可伸缩的判别源分离（TISDiSS）这一统一框架，它集成了早期分裂多损失监督、共享参数设计和动态推理重复。TISDiSS通过调整推理深度，无需重新训练其他模型，就能实现灵活的速度性能权衡。我们还对架构和训练选择进行了系统分析，并表明使用更多推理重复进行训练可以提高浅层推理性能，有利于低延迟应用程序。在标准语音分离基准测试上的实验证明了其卓越的性能和减少的参数计数，确立了TISDiSS作为自适应源分离的实用和可扩展框架。代码可在 <a target="_blank" rel="noopener" href="https://github.com/WingSingFung/TISDiSS">https://github.com/WingSingFung/TISDiSS</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15666v2">PDF</a> Submitted to ICASSP 2026.(C) 2025 IEEE. Personal use of this material   is permitted. Permission from IEEE must be obtained for all other uses, in   any current or future media, including reprinting&#x2F;republishing this material   for advertising or promotional purposes, creating new collective works, for   resale or redistribution to servers or lists, or reuse of any copyrighted   component of this work</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种训练时间和推理时间可伸缩的判别源分离（TISDiSS）框架，该框架融合了早期分裂多损失监督、共享参数设计和动态推理重复机制。TISDiSS通过调整推理深度实现灵活的速度性能权衡，无需重新训练模型。实验表明，该框架在标准语音分离基准测试中实现了卓越的性能，同时减少了参数数量，成为自适应源分离的实用框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TISDiSS框架集成了多种技术，包括早期分裂多损失监督、共享参数设计和动态推理重复，以提高源分离性能。</li>
<li>TISDiSS通过调整推理深度实现灵活的速度性能权衡，这是其一大优势。</li>
<li>该框架能够在不增加额外训练成本的前提下，实现性能的优化。</li>
<li>实验结果表明，TISDiSS在标准语音分离基准测试中表现出卓越的性能。</li>
<li>TISDiSS框架减少了参数数量，使得模型更加实用和高效。</li>
<li>TISDiSS框架提供了系统分析，探讨了架构和训练选择的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15666">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15666v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15666v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15666v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Speech/2509.15666v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-24/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_Face Swapping/2509.17818v1/page_1_0.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-09-24  ContextFlow Training-Free Video Object Editing via Adaptive Context   Enrichment
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-24/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-24\./crop_医学影像_Breast Ultrasound/2509.16382v1/page_5_0.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-09-24  Accurate Thyroid Cancer Classification using a Novel Binary Pattern   Driven Local Discrete Cosine Transform Descriptor
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
