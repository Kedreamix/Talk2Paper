<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-07  A self-supervised cyclic neural-analytic approach for novel view   synthesis and 3D reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ff31b8c293a9847af10bba02cef2b473.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    80 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-07-更新"><a href="#2025-03-07-更新" class="headerlink" title="2025-03-07 更新"></a>2025-03-07 更新</h1><h2 id="A-self-supervised-cyclic-neural-analytic-approach-for-novel-view-synthesis-and-3D-reconstruction"><a href="#A-self-supervised-cyclic-neural-analytic-approach-for-novel-view-synthesis-and-3D-reconstruction" class="headerlink" title="A self-supervised cyclic neural-analytic approach for novel view   synthesis and 3D reconstruction"></a>A self-supervised cyclic neural-analytic approach for novel view   synthesis and 3D reconstruction</h2><p><strong>Authors:Dragos Costea, Alina Marcu, Marius Leordeanu</strong></p>
<p>Generating novel views from recorded videos is crucial for enabling autonomous UAV navigation. Recent advancements in neural rendering have facilitated the rapid development of methods capable of rendering new trajectories. However, these methods often fail to generalize well to regions far from the training data without an optimized flight path, leading to suboptimal reconstructions. We propose a self-supervised cyclic neural-analytic pipeline that combines high-quality neural rendering outputs with precise geometric insights from analytical methods. Our solution improves RGB and mesh reconstructions for novel view synthesis, especially in undersampled areas and regions that are completely different from the training dataset. We use an effective transformer-based architecture for image reconstruction to refine and adapt the synthesis process, enabling effective handling of novel, unseen poses without relying on extensive labeled datasets. Our findings demonstrate substantial improvements in rendering views of novel and also 3D reconstruction, which to the best of our knowledge is a first, setting a new standard for autonomous navigation in complex outdoor environments. </p>
<blockquote>
<p>从录制的视频中生成新型视角对于实现自主无人机导航至关重要。神经网络渲染的近期进展促进了能够呈现新轨迹的方法的快速发展。然而，这些方法在没有优化飞行路径的情况下，往往不能很好地推广到远离训练数据的区域，从而导致重建效果不理想。我们提出了一种自监督循环神经分析管道，该管道结合了高质量神经渲染输出和来自分析方法的精确几何洞察力。我们的解决方案改进了用于合成新型视角的RGB和网格重建，特别是在欠采样区域和与训练数据集完全不同的区域。我们采用有效的基于转换器的架构进行图像重建，以优化和适应合成过程，能够有效处理新型、未见姿态，而无需依赖大量标记数据集。我们的研究发现在新型视角渲染和3D重建方面取得了显著改进，据我们所知，这是第一次实现，为复杂室外环境中的自主导航设定了新的标准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03543v1">PDF</a> Published in BMVC 2024, 10 pages, 4 figures</p>
<p><strong>Summary</strong><br>视频生成新技术能提升无人机自主导航能力。新方法结合了高质量神经渲染输出和精确几何分析，改善未知视角的合成和三维重建，尤其适用于采样不足和与训练集差异大的区域。采用有效的转换器架构进行图像重建，可精细调整并适应合成过程，有效处理新颖、未见姿态，无需依赖大量标注数据集。此技术革新了复杂户外环境的自主导航标准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自主无人机导航中，从录制视频生成新颖视角至关重要。</li>
<li>神经渲染技术的最新进展促进了新型渲染方法的发展。</li>
<li>这些方法在没有优化飞行路径的情况下，难以很好地推广到远离训练数据的区域，导致重建效果不理想。</li>
<li>提出了一种自我监督的循环神经分析管道，结合了高质量神经渲染和精确几何分析。</li>
<li>该解决方案改进了RGB和网格重建用于新颖视角的合成，特别是在采样不足和与训练集完全不同的区域。</li>
<li>采用有效的转换器架构进行图像重建，提高合成过程的精细度和适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03543">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a7e069fe4103a1ebbbd3272cd86ec607.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-403e794c0baf4be1ca229e72e2e0f6bf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Top-K-Maximum-Intensity-Projection-Priors-for-3D-Liver-Vessel-Segmentation"><a href="#Top-K-Maximum-Intensity-Projection-Priors-for-3D-Liver-Vessel-Segmentation" class="headerlink" title="Top-K Maximum Intensity Projection Priors for 3D Liver Vessel   Segmentation"></a>Top-K Maximum Intensity Projection Priors for 3D Liver Vessel   Segmentation</h2><p><strong>Authors:Xiaotong Zhang, Alexander Broersen, Gonnie CM van Erp, Silvia L. Pintea, Jouke Dijkstra</strong></p>
<p>Liver-vessel segmentation is an essential task in the pre-operative planning of liver resection. State-of-the-art 2D or 3D convolution-based methods focusing on liver vessel segmentation on 2D CT cross-sectional views, which do not take into account the global liver-vessel topology. To maintain this global vessel topology, we rely on the underlying physics used in the CT reconstruction process, and apply this to liver-vessel segmentation. Concretely, we introduce the concept of top-k maximum intensity projections, which mimics the CT reconstruction by replacing the integral along each projection direction, with keeping the top-k maxima along each projection direction. We use these top-k maximum projections to condition a diffusion model and generate 3D liver-vessel trees. We evaluate our 3D liver-vessel segmentation on the 3D-ircadb-01 dataset, and achieve the highest Dice coefficient, intersection-over-union (IoU), and Sensitivity scores compared to prior work. </p>
<blockquote>
<p>肝脏血管分割是肝脏切除术前规划中的一项重要任务。目前最先进的2D或3D卷积方法主要关注于在2D CT横截面视图上的肝脏血管分割，这些方法并没有考虑到全局肝脏血管拓扑结构。为了保持这种全局血管拓扑结构，我们依赖于CT重建过程中使用的底层物理原理，并将其应用于肝脏血管分割。具体来说，我们引入了前k最大强度投影的概念，通过保留每个投影方向上的前k个最大值来模拟CT重建过程。我们使用这些前k最大投影来设定扩散模型并生成三维肝脏血管树。我们在三维肝脏数据集上进行评估3D-ircadb-01，与先前的工作相比，我们取得了最高的Dice系数、交并比（IoU）和敏感性得分。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03367v1">PDF</a> Accepted in 2025 IEEE International Symposium on Biomedical Imaging   (ISBI 2025)</p>
<p><strong>Summary</strong></p>
<p>肝脏血管分割是肝脏切除术前规划的重要任务。当前方法主要关注二维CT横截面上的肝脏血管分割，忽略了全局血管拓扑结构。本研究利用CT重建过程中的物理原理，引入top-k最大强度投影的概念，生成三维肝脏血管树。在3D-ircadb-01数据集上评估该三维肝脏血管分割方法，较前期工作有更高的Dice系数、交并比（IoU）和敏感性得分。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>肝脏血管分割是肝脏切除术前规划的重要部分。</li>
<li>当前方法主要关注二维CT横截面上的肝脏血管分割，忽略了全局血管拓扑结构的影响。</li>
<li>研究利用CT重建的物理原理来维护全局血管拓扑结构。</li>
<li>引入top-k最大强度投影的概念，模仿CT重建过程。</li>
<li>通过top-k最大强度投影来调控扩散模型，生成三维肝脏血管树。</li>
<li>在3D-ircadb-01数据集上评估该三维肝脏血管分割方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03367">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-104c55ee80957c4a28eed0849f95466c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f571ba2c58bd210436ce3c345ed24dce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-247a1b522cf2fdf73b121e58c46c4f2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8e48530ffc84de4b3478595c4b238d9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TopoMortar-A-dataset-to-evaluate-image-segmentation-methods-focused-on-topology-accuracy"><a href="#TopoMortar-A-dataset-to-evaluate-image-segmentation-methods-focused-on-topology-accuracy" class="headerlink" title="TopoMortar: A dataset to evaluate image segmentation methods focused on   topology accuracy"></a>TopoMortar: A dataset to evaluate image segmentation methods focused on   topology accuracy</h2><p><strong>Authors:Juan Miguel Valverde, Motoya Koga, Nijihiko Otsuka, Anders Bjorholm Dahl</strong></p>
<p>We present TopoMortar, a brick wall dataset that is the first dataset specifically designed to evaluate topology-focused image segmentation methods, such as topology loss functions. TopoMortar enables to investigate in two ways whether methods incorporate prior topological knowledge. First, by eliminating challenges seen in real-world data, such as small training set, noisy labels, and out-of-distribution test-set images, that, as we show, impact the effectiveness of topology losses. Second, by allowing to assess in the same dataset topology accuracy across dataset challenges, isolating dataset-related effects from the effect of incorporating prior topological knowledge. In these two experiments, it is deliberately difficult to improve topology accuracy without actually using topology information, thus, permitting to attribute an improvement in topology accuracy to the incorporation of prior topological knowledge. To this end, TopoMortar includes three types of labels (accurate, noisy, pseudo-labels), two fixed training sets (large and small), and in-distribution and out-of-distribution test-set images. We compared eight loss functions on TopoMortar, and we found that clDice achieved the most topologically accurate segmentations, Skeleton Recall loss performed best particularly with noisy labels, and the relative advantageousness of the other loss functions depended on the experimental setting. Additionally, we show that simple methods, such as data augmentation and self-distillation, can elevate Cross entropy Dice loss to surpass most topology loss functions, and that those simple methods can enhance topology loss functions as well. clDice and Skeleton Recall loss, both skeletonization-based loss functions, were also the fastest to train, making this type of loss function a promising research direction. TopoMortar and our code can be found at <a target="_blank" rel="noopener" href="https://github.com/jmlipman/TopoMortar">https://github.com/jmlipman/TopoMortar</a> </p>
<blockquote>
<p>我们推出了TopoMortar，这是一款专门用于评估以拓扑为重点的图像分割方法（如拓扑损失函数）的砖墙数据集。TopoMortar能够通过两种方式研究方法是否融入了先验拓扑知识。首先，通过消除真实数据中的挑战，例如小训练集、标签噪声和超出分布范围的测试集图像，这些挑战如我们所展示的，会影响拓扑损失的有效性。其次，它允许在同一数据集中评估不同数据集挑战下的拓扑精度，从而区分数据集相关的影响与融入先验拓扑知识的影响。在这两项实验中，如果不实际使用拓扑信息，很难提高拓扑精度，因此，可以将拓扑精度的提高归因于融入的先验拓扑知识。为此，TopoMortar包括三种标签（准确、噪声、伪标签）、两个固定训练集（大型和小型）以及符合分布和超出分布的测试集图像。我们在TopoMortar上比较了八种损失函数，发现clDice获得了最准确的拓扑分割，特别是在带有噪声标签的情况下Skeleton Recall损失表现最佳，其他损失函数的相对优势取决于实验设置。此外，我们还证明了一些简单的方法（如数据增强和自我蒸馏）可以将Cross entropy Dice损失提升到超越大多数拓扑损失函数，并且这些简单的方法还可以增强拓扑损失函数的效果。clDice和Skeleton Recall损失都是基于骨架化的损失函数，也是训练速度最快的，这使得这种类型的损失函数成为一个有前景的研究方向。TopoMortar和我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/jmlipman/TopoMortar%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jmlipman/TopoMortar找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03365v1">PDF</a> </p>
<p><strong>摘要</strong><br>    TopoMortar数据集专为评估以拓扑为中心的图像分割方法而设计，如拓扑损失函数。该数据集通过消除真实数据中的挑战，如小训练集、标签噪声和超出分布范围的测试集图像，来探究方法是否融入了先验拓扑知识。同时，它允许在同一数据集中评估拓扑精度，以隔离数据集相关效应与融入先验拓扑知识的影响。为此，TopoMortar包含三种标签、两个固定训练集以及符合和超出分布的测试集图像。在TopoMortar上比较了八种损失函数，发现clDice实现最拓扑准确的分割，特别是带有噪声标签时Skeleton Recall损失表现最佳，其他损失函数的优势取决于实验设置。此外，简单的方法如数据增强和自我蒸馏可使Cross entropy Dice损失超越大多数拓扑损失函数，这些方法也能增强拓扑损失函数的效果。clDice和Skeleton Recall损失是基于骨架化的损失函数，训练速度最快，成为有前景的研究方向。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>TopoMortar数据集专为评估拓扑聚焦的图像分割方法而设计，例如拓扑损失函数。</li>
<li>该数据集消除了真实数据中的挑战，如小训练集、标签噪声和超出分布范围的测试集图像。</li>
<li>TopoMortar允许在同一数据集中评估拓扑精度，区分数据集相关效应与融入先验拓扑知识的影响。</li>
<li>在此数据集中比较了八种损失函数，发现clDice实现最拓扑准确的分割。</li>
<li>在处理带有噪声的标签时，Skeleton Recall损失表现最佳。</li>
<li>简单的方法（如数据增强和自我蒸馏）能显著提高损失函数的表现，尤其是基于骨架化的损失函数训练速度非常快。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03365">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e2641f851ca0a676bdaee0927f928801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50fd8f7bb93c9a828ff46af3254ce86e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f02ba5d08485166d150bf52e944583d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92ad5c7967d931956233aa84866cb801.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-Based-Diffusion-MRI-Tractography-Integrating-Spatial-and-Anatomical-Information"><a href="#Deep-Learning-Based-Diffusion-MRI-Tractography-Integrating-Spatial-and-Anatomical-Information" class="headerlink" title="Deep Learning-Based Diffusion MRI Tractography: Integrating Spatial and   Anatomical Information"></a>Deep Learning-Based Diffusion MRI Tractography: Integrating Spatial and   Anatomical Information</h2><p><strong>Authors:Yiqiong Yang, Yitian Yuan, Baoxing Ren, Ye Wu, Yanqiu Feng, Xinyuan Zhang</strong></p>
<p>Diffusion MRI tractography technique enables non-invasive visualization of the white matter pathways in the brain. It plays a crucial role in neuroscience and clinical fields by facilitating the study of brain connectivity and neurological disorders. However, the accuracy of reconstructed tractograms has been a longstanding challenge. Recently, deep learning methods have been applied to improve tractograms for better white matter coverage, but often comes at the expense of generating excessive false-positive connections. This is largely due to their reliance on local information to predict long range streamlines. To improve the accuracy of streamline propagation predictions, we introduce a novel deep learning framework that integrates image-domain spatial information and anatomical information along tracts, with the former extracted through convolutional layers and the later modeled via a Transformer-decoder. Additionally, we employ a weighted loss function to address fiber class imbalance encountered during training. We evaluate the proposed method on the simulated ISMRM 2015 Tractography Challenge dataset, achieving a valid streamline rate of 66.2%, white matter coverage of 63.8%, and successfully reconstructing 24 out of 25 bundles. Furthermore, on the multi-site Tractoinferno dataset, the proposed method demonstrates its ability to handle various diffusion MRI acquisition schemes, achieving a 5.7% increase in white matter coverage and a 4.1% decrease in overreach compared to RNN-based methods. </p>
<blockquote>
<p>扩散磁共振成像（MRI）纤维追踪技术能够实现大脑白质通路无创可视化。该技术对于神经科学和临床领域至关重要，能够促进脑连接和神经障碍的研究。然而，重建纤维追踪图的准确性一直是一个挑战。最近，深度学习方法被应用于提高纤维追踪图的覆盖范围，但往往会产生过多的假阳性连接。这主要是因为它们依赖局部信息来预测长距离流线。为了提高流线传播预测的准确度，我们引入了一种新型的深度学习框架，该框架结合了图像域的空间信息和沿纤维束的解剖信息。其中，前者通过卷积层提取，后者通过Transformer解码器建模。此外，我们还采用加权损失函数来解决训练过程中遇到的纤维类别不平衡问题。我们在模拟的ISMRM 2015年纤维追踪挑战赛数据集上评估了所提出的方法，实现了有效的流线率为66.2%，白质覆盖率为63.8%，成功重建了24个纤维束中的25个。在多站点Tractoinferno数据集上，该方法展示了其处理各种扩散MRI采集方案的能力，与基于RNN的方法相比，白质覆盖率提高了5.7%，超出率降低了4.1%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03329v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了一种结合图像域空间信息和解剖信息的新型深度学习框架，用于改善扩散MRI追踪图的准确性。该框架通过卷积层和Transformer解码器进行建模，使用加权损失函数解决训练过程中的纤维类别不平衡问题。在模拟的ISMRM 2015追踪图挑战赛数据集上，该方法实现了有效流线率为66.2%，白质覆盖率为63.8%，成功重建了24个束中的25个。在多站点Tractoinferno数据集上，该方法可处理各种扩散MRI采集方案，与RNN方法相比，白质覆盖率提高了5.7%，过度到达率降低了4.1%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散MRI追踪技术可实现脑内白质通路的无创可视化。</li>
<li>在神经科学和临床领域，该技术对于研究脑连接和神经疾病至关重要。</li>
<li>深度学习已应用于改进追踪图以提高白质覆盖率，但存在生成过多假阳性连接的问题。</li>
<li>提出的新型深度学习框架结合了图像域空间信息和解剖信息，以提高流线传播预测的准确性。</li>
<li>该框架使用卷积层和Transformer解码器进行建模，并采用了加权损失函数来解决纤维类别不平衡问题。</li>
<li>在模拟数据集上，该方法的流线率和白质覆盖率表现良好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03329">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f00f4f249a8a72941cfc02e1562c85e9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Interactive-Segmentation-and-Report-Generation-for-CT-Images"><a href="#Interactive-Segmentation-and-Report-Generation-for-CT-Images" class="headerlink" title="Interactive Segmentation and Report Generation for CT Images"></a>Interactive Segmentation and Report Generation for CT Images</h2><p><strong>Authors:Yannian Gu, Wenhui Lei, Hanyu Chen, Xiaofan Zhang, Shaoting Zhang</strong></p>
<p>Automated CT report generation plays a crucial role in improving diagnostic accuracy and clinical workflow efficiency. However, existing methods lack interpretability and impede patient-clinician understanding, while their static nature restricts radiologists from dynamically adjusting assessments during image review. Inspired by interactive segmentation techniques, we propose a novel interactive framework for 3D lesion morphology reporting that seamlessly generates segmentation masks with comprehensive attribute descriptions, enabling clinicians to generate detailed lesion profiles for enhanced diagnostic assessment. To our best knowledge, we are the first to integrate the interactive segmentation and structured reports in 3D CT medical images. Experimental results across 15 lesion types demonstrate the effectiveness of our approach in providing a more comprehensive and reliable reporting system for lesion segmentation and capturing. The source code will be made publicly available following paper acceptance. </p>
<blockquote>
<p>自动化CT报告生成在提高诊断准确性和临床工作流程效率方面起着至关重要的作用。然而，现有方法缺乏解释性，阻碍医患之间的理解，而其静态性质限制了放射科医生在图像审查过程中动态调整评估的能力。受交互式分割技术的启发，我们提出了一种用于3D病变形态报告的新型交互式框架，该框架可以无缝生成带有综合属性描述的分割掩模，使临床医生能够生成详细的病变特征，从而提高诊断评估水平。据我们所知，我们是首次将交互式分割和结构化报告整合到3D CT医学图像中。跨越15种病变类型的实验结果表明，我们的方法在提供病变分割和捕获的更全面、可靠的报告系统方面非常有效。论文被接受后，源代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03294v1">PDF</a> </p>
<p><strong>Summary</strong><br>在CT影像诊断中，自动化报告生成在提高诊断准确性和临床工作效率方面发挥着重要作用。然而，现有方法缺乏可解释性，阻碍医患沟通，同时静态的特性限制了放射科医生在查看图像时的动态评估能力。受交互式分割技术的启发，我们提出了一种新颖的交互式框架，用于生成包含综合属性描述的分割掩膜，使临床医生能够生成详细的病灶图谱，提高诊断评估的准确性。据我们所知，我们是首次将交互式分割和结构化报告整合到三维CT医学图像中。跨越十五种病灶类型的实验结果表明，我们的方法提供了更全面可靠的病灶分割报告系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动化CT报告生成在影像诊断中能提高诊断准确性和临床工作效率。</li>
<li>当前的方法缺乏可解释性，阻碍了医患沟通。</li>
<li>交互式分割技术为改进现有方法提供了新的思路。</li>
<li>提出了一种新颖的交互式框架，用于生成包含综合属性描述的分割掩膜。</li>
<li>该框架使临床医生能够生成详细的病灶图谱，提高诊断评估的准确性。</li>
<li>首次将交互式分割和结构化报告整合到三维CT医学图像中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03294">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5897ac05fbe37ae49f74c129a581beea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6648b945257a926889eb107c3e636c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56fc74e0df9566a0a037ae60251f0829.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UnPuzzle-A-Unified-Framework-for-Pathology-Image-Analysis"><a href="#UnPuzzle-A-Unified-Framework-for-Pathology-Image-Analysis" class="headerlink" title="UnPuzzle: A Unified Framework for Pathology Image Analysis"></a>UnPuzzle: A Unified Framework for Pathology Image Analysis</h2><p><strong>Authors:Dankai Liao, Sicheng Chen, Nuwa Xi, Qiaochu Xue, Jieyu Li, Lingxuan Hou, Zeyu Liu, Chang Han Low, Yufeng Wu, Yiling Liu, Yanqin Jiang, Dandan Li, Yueming Jin, Shangqing Lyu</strong></p>
<p>Pathology image analysis plays a pivotal role in medical diagnosis, with deep learning techniques significantly advancing diagnostic accuracy and research. While numerous studies have been conducted to address specific pathological tasks, the lack of standardization in pre-processing methods and model&#x2F;database architectures complicates fair comparisons across different approaches. This highlights the need for a unified pipeline and comprehensive benchmarks to enable consistent evaluation and accelerate research progress. In this paper, we present UnPuzzle, a novel and unified framework for pathological AI research that covers a broad range of pathology tasks with benchmark results. From high-level to low-level, upstream to downstream tasks, UnPuzzle offers a modular pipeline that encompasses data pre-processing, model composition,taskconfiguration,andexperimentconduction.Specifically, it facilitates efficient benchmarking for both Whole Slide Images (WSIs) and Region of Interest (ROI) tasks. Moreover, the framework supports variouslearningparadigms,includingself-supervisedlearning,multi-task learning,andmulti-modallearning,enablingcomprehensivedevelopment of pathology AI models. Through extensive benchmarking across multiple datasets, we demonstrate the effectiveness of UnPuzzle in streamlining pathology AI research and promoting reproducibility. We envision UnPuzzle as a cornerstone for future advancements in pathology AI, providing a more accessible, transparent, and standardized approach to model evaluation. The UnPuzzle repository is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Puzzle-AI/UnPuzzle">https://github.com/Puzzle-AI/UnPuzzle</a>. </p>
<blockquote>
<p>病理学图像分析在医学诊断中扮演着至关重要的角色，深度学习技术显著提高了诊断准确性和研究水平。虽然已进行了大量研究来解决特定的病理任务，但在预处理方法和模型&#x2F;数据库架构方面缺乏标准化，这使得不同的方法之间难以进行公平的比较。这强调了需要一个统一的流程和综合基准测试，以实现一致的评价并加速研究进展。在本文中，我们提出了UnPuzzle，这是一个用于病理人工智能研究的新型统一框架，涵盖广泛的病理任务并提供基准测试结果。从高级到低级，从上游到下游任务，UnPuzzle提供了一个模块化流程，包括数据预处理、模型组合、任务配置和实验执行。具体来说，它便于对整个幻灯片图像（WSIs）和感兴趣区域（ROI）任务进行高效基准测试。此外，该框架支持多种学习范式，包括自我监督学习、多任务学习和多模式学习，从而实现病理人工智能模型的全面发展。通过多个数据集的大量基准测试，我们证明了UnPuzzle在简化病理人工智能研究并促进可重复性方面的有效性。我们期望UnPuzzle能成为未来病理人工智能发展的基石，提供一种更可访问、透明和标准化的模型评估方法。UnPuzzle仓库可在<a target="_blank" rel="noopener" href="https://github.com/Puzzle-AI/UnPuzzle">https://github.com/Puzzle-AI/UnPuzzle</a>公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03152v1">PDF</a> 11 pages,2 figures</p>
<p><strong>Summary</strong><br>     病理学图像分析在医学诊断中扮演重要角色，深度学习技术显著提高了诊断准确性和研究水平。当前缺乏标准化预处理方法和模型&#x2F;数据库架构，导致不同方法之间的公平比较变得复杂。本文提出了UnPuzzle，一个用于病理学人工智能研究的新型统一框架，涵盖广泛的任务，并提供模块化管道，包括数据处理、模型构建、任务配置和实验执行。它支持多种学习范式，并通过跨多个数据集的大量基准测试证明了其在简化病理学人工智能研究和促进可重复性的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>病理学图像分析在医学诊断中起关键作用，深度学习技术提高了诊断准确性和研究水平。</li>
<li>当前缺乏统一的预处理方法和模型&#x2F;数据库架构，需要更标准化的研究评估流程。</li>
<li>UnPuzzle是一个用于病理学人工智能研究的统一框架，涵盖多种任务并提供模块化管道。</li>
<li>UnPuzzle支持多种学习范式，包括自监督学习、多任务学习和多模态学习。</li>
<li>UnPuzzle通过跨多个数据集的基准测试证明了其有效性。</li>
<li>UnPuzzle旨在成为病理学人工智能未来进步的基石，提供更易于访问、透明和标准化的模型评估方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03152">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-354dfd6d39a9d41570b08ecee6f23bf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e743da5759513c05a4a336103f14321a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Implicit-U-KAN2-0-Dynamic-Efficient-and-Interpretable-Medical-Image-Segmentation"><a href="#Implicit-U-KAN2-0-Dynamic-Efficient-and-Interpretable-Medical-Image-Segmentation" class="headerlink" title="Implicit U-KAN2.0: Dynamic, Efficient and Interpretable Medical Image   Segmentation"></a>Implicit U-KAN2.0: Dynamic, Efficient and Interpretable Medical Image   Segmentation</h2><p><strong>Authors:Chun-Wun Cheng, Yining Zhao, Yanqi Cheng, Javier Montoya, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero</strong></p>
<p>Image segmentation is a fundamental task in both image analysis and medical applications. State-of-the-art methods predominantly rely on encoder-decoder architectures with a U-shaped design, commonly referred to as U-Net. Recent advancements integrating transformers and MLPs improve performance but still face key limitations, such as poor interpretability, difficulty handling intrinsic noise, and constrained expressiveness due to discrete layer structures, often lacking a solid theoretical foundation.In this work, we introduce Implicit U-KAN 2.0, a novel U-Net variant that adopts a two-phase encoder-decoder structure. In the SONO phase, we use a second-order neural ordinary differential equation (NODEs), called the SONO block, for a more efficient, expressive, and theoretically grounded modeling approach. In the SONO-MultiKAN phase, we integrate the second-order NODEs and MultiKAN layer as the core computational block to enhance interpretability and representation power. Our contributions are threefold. First, U-KAN 2.0 is an implicit deep neural network incorporating MultiKAN and second order NODEs, improving interpretability and performance while reducing computational costs. Second, we provide a theoretical analysis demonstrating that the approximation ability of the MultiKAN block is independent of the input dimension. Third, we conduct extensive experiments on a variety of 2D and a single 3D dataset, demonstrating that our model consistently outperforms existing segmentation networks. </p>
<blockquote>
<p>图像分割是图像分析和医学应用中的基本任务。最先进的方法主要依赖于编码器-解码器架构，并且具有U形设计，通常称为U-Net。最近融合了变压器和多层感知机的进展提高了性能，但仍然面临关键局限，如解释性差、处理内在噪声困难、由于离散层结构导致的表达能力受限，并且通常缺乏坚实理论基础。</p>
</blockquote>
<p>在这项工作中，我们引入了Implicit U-KAN 2.0，这是一种采用两阶段编码器-解码器结构的新型U-Net变体。在SONO阶段，我们使用称为SONO块的二阶神经常微分方程（NODEs），以更有效、更具表现力和理论基础的建模方法。在SONO-MultiKAN阶段，我们将二阶NODEs和MultiKAN层作为核心计算块进行集成，以提高可解释性和表示能力。我们的贡献有三点。首先，U-KAN 2.0是一个隐式深度神经网络，结合了MultiKAN和二阶NODEs，在提高可解释性和性能的同时降低了计算成本。其次，我们提供了理论分析，证明MultiKAN块的逼近能力与输入维度无关。最后，我们在多种二维和单个三维数据集上进行了广泛实验，结果表明我们的模型始终优于现有分割网络。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03141v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本工作提出了Implicit U-KAN 2.0，一种新型U-Net变体，采用两阶段编码器-解码器结构。引入SONO块和SONO-MultiKAN阶段，提高模型效率、表达力、可解释性和理论支撑。在多种数据集上的实验表明，该模型在图像分割任务上优于现有网络。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Implicit U-KAN 2.0是一种新型的U-Net变体，采用两阶段编码器-解码器结构。</li>
<li>该模型引入SONO块和SONO-MultiKAN阶段，以提高模型效率和表达力。</li>
<li>SONO块采用二阶神经常微分方程（NODEs），提高模型的理论支撑。</li>
<li>MultiKAN块具有独立输入维度的近似能力，增强模型的可解释性。</li>
<li>在多种二维和单一三维数据集上进行的实验表明，该模型的性能优于现有分割网络。</li>
<li>该模型具有较低的计算成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03141">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7d2caf536d01e2d978d6f1b53d70acac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff31b8c293a9847af10bba02cef2b473.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Linking-quantum-mechanical-features-to-structural-phase-transformation-in-inorganic-solids"><a href="#Linking-quantum-mechanical-features-to-structural-phase-transformation-in-inorganic-solids" class="headerlink" title="Linking quantum mechanical features to structural phase-transformation   in inorganic solids"></a>Linking quantum mechanical features to structural phase-transformation   in inorganic solids</h2><p><strong>Authors:Prashant Singh, Anis Biswas, Alexander Thayer, Yaroslav Mudryk</strong></p>
<p>We present a new descriptor, i.e., local lattice distortion, to predict structural phase transformation in inorganic compounds containing lanthanides and transition metals. The descriptor utilizes local lattice and angular distortions obtained from structural optimization of experimentally known crystalline phases within state-of-the-art density-functional theory method. The predictive power of the descriptor was tested on lanthanide based RE2In (RE&#x3D;rare-earth) compounds known for a variety of phase transformations. We show that the inclusion of quantum-mechanical effects through local-charge, bonding, symmetry, and electronic-structure enhances the robustness of the descriptor in predicting structural phase transformation. To gain further insights, we analyzed phononic and electronic behavior of Y2In, and show that experimentally observed phase transformation can only be predicted when atomic strains are included. The descriptor was used to predict structural phase change in couple of new compounds, i.e., (Yb1-xErx)2In and Gd2(In1-xAlx), which was validated by X-ray powder diffraction measurements. Finally, we demonstrated the generality of the proposed descriptor by predicting phase transformation behavior in different classes of compounds indicating the usefulness of our approach in mapping desired phase changes in novel functional materials. </p>
<blockquote>
<p>我们提出了一种新的描述符，即局部晶格畸变，用于预测含稀土元素和过渡金属的无机化合物的结构相变。该描述符利用局部晶格和角度畸变，这些畸变是通过最新发展的密度泛函理论方法对已知晶体结构进行优化而获得的。该描述符的预测能力在基于稀土元素的RE2In（RE&#x3D;稀土元素）化合物上得到了测试，这些化合物具有多种相变。我们表明，通过局部电荷、键合、对称性和电子结构融入量子力学效应，增强了该描述符在预测结构相变方面的稳健性。为了深入了解，我们分析了Y2In的声学和电子行为，并表明只有当包含原子应变时，才能预测实验观察到的相变。该描述符被用于预测几种新化合物的结构相变，如（Yb1-xErx）2In和Gd2（In1-xAlx），并通过X射线粉末衍射测量验证了其预测结果。最后，我们通过预测不同类别化合物的相变行为来展示所提出描述符的普遍性，这证明了我们方法在新型功能材料中映射所需相变的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03120v1">PDF</a> 25 page, 8 figures, 78 references</p>
<p><strong>Summary</strong></p>
<p>一种新型描述符——局部晶格畸变被提出来预测含稀土元素和过渡金属的无机化合物的结构相变。该描述符利用局部晶格和角度畸变，通过先进的密度泛函理论方法对已知晶体结构进行优化得到。通过测试在稀土元素RE₂In（RE&#x3D;稀土元素）化合物中的预测能力，显示出该描述符在预测结构相变方面具有良好的鲁棒性，并且量子机械效应包括局部电荷、键合、对称和电子结构的加入增强了预测的准确性。对新化合物（Yb¹ₓEr）₂In和Gd₂（In¹ₓAl）进行了预测，并通过X射线粉末衍射测量验证了预测的相变行为。最终证明了该描述符的通用性，在预测不同化合物的相变行为方面表现出良好的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新型描述符——局部晶格畸变，用于预测无机化合物的结构相变。</li>
<li>描述符结合了先进的密度泛函理论方法来优化已知晶体结构，并利用局部晶格和角度畸变进行预测。</li>
<li>描述符在稀土元素RE₂In化合物中的预测能力得到了测试验证。</li>
<li>描述符在考虑量子机械效应（如局部电荷、键合、对称和电子结构）的情况下，更能准确预测结构相变。</li>
<li>对新化合物（Yb¹ₓEr）₂In和Gd₂（In¹ₓAl）的相变行为进行了预测，并通过实验验证了其准确性。</li>
<li>描述符具有良好的通用性，能够预测不同化合物的相变行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03120">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-23d640fabcc6049f158b6f08b6a49fea.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-Diffusion-Models-Provide-Rigorous-Uncertainty-Quantification-for-Bayesian-Inverse-Problems"><a href="#Can-Diffusion-Models-Provide-Rigorous-Uncertainty-Quantification-for-Bayesian-Inverse-Problems" class="headerlink" title="Can Diffusion Models Provide Rigorous Uncertainty Quantification for   Bayesian Inverse Problems?"></a>Can Diffusion Models Provide Rigorous Uncertainty Quantification for   Bayesian Inverse Problems?</h2><p><strong>Authors:Evan Scope Crafts, Umberto Villa</strong></p>
<p>In recent years, the ascendance of diffusion modeling as a state-of-the-art generative modeling approach has spurred significant interest in their use as priors in Bayesian inverse problems. However, it is unclear how to optimally integrate a diffusion model trained on the prior distribution with a given likelihood function to obtain posterior samples. While algorithms that have been developed for this purpose can produce high-quality, diverse point estimates of the unknown parameters of interest, they are often tested on problems where the prior distribution is analytically unknown, making it difficult to assess their performance in providing rigorous uncertainty quantification. In this work, we introduce a new framework, Bayesian Inverse Problem Solvers through Diffusion Annealing (BIPSDA), for diffusion model based posterior sampling. The framework unifies several recently proposed diffusion model based posterior sampling algorithms and contains novel algorithms that can be realized through flexible combinations of design choices. Algorithms within our framework were tested on model problems with a Gaussian mixture prior and likelihood functions inspired by problems in image inpainting, x-ray tomography, and phase retrieval. In this setting, approximate ground-truth posterior samples can be obtained, enabling principled evaluation of the performance of the algorithms. The results demonstrate that BIPSDA algorithms can provide strong performance on the image inpainting and x-ray tomography based problems, while the challenging phase retrieval problem, which is difficult to sample from even when the posterior density is known, remains outside the reach of the diffusion model based samplers. </p>
<blockquote>
<p>近年来，扩散模型作为一种最先进的生成建模方法逐渐兴起，这激发了对其在贝叶斯反问题中作为先验使用的极大兴趣。然而，对于如何将训练的扩散模型与给定的似然函数相结合以获得后验样本的最优方法尚不清楚。尽管为此目的而开发的算法可以产生高质量的未知参数估计值，但这些估计值往往是多样化的。但这些算法经常在先验分布分析未知的问题上接受测试，这使得它们在提供严格的不确定性量化方面的性能难以评估。在这项工作中，我们引入了基于扩散退火的贝叶斯反问题求解器（BIPSDA）的新框架，用于基于扩散模型的采样。该框架统一了最近提出的几种基于扩散模型的采样算法，并包含了可以通过灵活组合设计选择来实现的新算法。在我们的框架内的算法是在具有高斯混合先验和受图像修复、X射线断层扫描和相位检索问题启发的似然函数模型问题上进行的测试。在此设置中，可以获得近似真实的后验样本，这能够按照原则评估算法的性能。结果表明，BIPSDA算法在图像修复和基于X射线断层扫描的问题上表现出强大的性能，而对于即使已知后验密度也很难采样的具有挑战性的相位检索问题，基于扩散模型的采样器仍无法解决。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03007v1">PDF</a> </p>
<p><strong>Summary</strong><br>    本文介绍了通过扩散退火解决贝叶斯反问题的新框架BIPSDA，该框架整合了多种扩散模型后采样算法，包括新颖算法，可通过灵活的设计选择实现。在具有高斯混合先验和受图像补全、X射线断层扫描和相位检索问题启发的似然函数的模型问题上测试了该框架的算法，可以获得近似真实的后验样本，从而可以原则性地评估算法性能。结果表明，BIPSDA算法在图像补全和基于X射线断层扫描的问题上表现强劲，而在采样困难的相位检索问题上则有待进一步突破。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型作为最先进的生成建模方法，在贝叶斯反问题中被广泛用作先验。</li>
<li>现有算法在将扩散模型与给定的似然函数结合以获取后验样本方面存在不确定性。</li>
<li>引入了一个新框架BIPSDA，用于基于扩散模型的后验采样。</li>
<li>BIPSDA框架整合了多种扩散模型后采样算法，包括新颖算法。</li>
<li>在具有高斯混合先验和特定似然函数的模型问题上测试了BIPSDA框架的算法。</li>
<li>BIPSDA算法在图像补全和基于X射线断层扫描的问题上表现良好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5abfbbcaa66c0c0eb7e0b994e32a29c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Personalizing-the-meshed-SPL-NAC-Brain-Atlas-for-patient-specific-scientific-computing-using-SynthMorph"><a href="#Personalizing-the-meshed-SPL-NAC-Brain-Atlas-for-patient-specific-scientific-computing-using-SynthMorph" class="headerlink" title="Personalizing the meshed SPL&#x2F;NAC Brain Atlas for patient-specific   scientific computing using SynthMorph"></a>Personalizing the meshed SPL&#x2F;NAC Brain Atlas for patient-specific   scientific computing using SynthMorph</h2><p><strong>Authors:Andy Huynh, Benjamin Zwick, Michael Halle, Adam Wittek, Karol Miller</strong></p>
<p>Developing personalized computational models of the human brain remains a challenge for patient-specific clinical applications and neuroscience research. Efficient and accurate biophysical simulations rely on high-quality personalized computational meshes derived from patient’s segmented anatomical MRI scans. However, both automatic and manual segmentation are particularly challenging for tissues with limited visibility or low contrast. In this work, we present a new method to create personalized computational meshes of the brain, streamlining the development of computational brain models for clinical applications and neuroscience research. Our method uses SynthMorph, a state-of-the-art anatomy-aware, learning-based medical image registration approach, to morph a comprehensive hexahedral mesh of the open-source SPL&#x2F;NAC Brain Atlas to patient-specific MRI scans. Each patient-specific mesh includes over 300 labeled anatomical structures, more than any existing manual or automatic methods. Our registration-based method takes approximately 20 minutes, significantly faster than current state-of-the-art mesh generation pipelines, which can take up to two hours. We evaluated several state-of-the-art medical image registration methods, including SynthMorph, to determine the most optimal registration method to morph our meshed anatomical brain atlas to patient MRI scans. Our results demonstrate that SynthMorph achieved high DICE similarity coefficients and low Hausdorff Distance metrics between anatomical structures, while maintaining high mesh element quality. These findings demonstrate that our registration-based method efficiently and accurately produces high-quality, comprehensive personalized brain meshes, representing an important step toward clinical translation. </p>
<blockquote>
<p>构建个性化的计算化人类大脑模型仍然是针对患者特定临床应用和神经科学研究的挑战。高效且准确的生物物理模拟依赖于由患者分割的MRI扫描图像生成的高质量个性化计算网格。然而，对于可见度有限或对比度较低的组织，自动和手动分割都具有特别的挑战性。在这项工作中，我们提出了一种创建个性化计算大脑网格的新方法，以简化计算大脑模型的临床应用和神经科学研究开发。我们的方法使用SynthMorph（一种基于学习的、先进的解剖学意识医学图像配准方法），将开源SPL&#x2F;NAC脑图谱的全面六面体网格变形为针对患者的MRI扫描图像。每个患者特定的网格包括超过300个标记的解剖结构，比现有的任何手动或自动方法都要多。我们的基于配准的方法大约需要20分钟的时间，这大大快于当前的先进网格生成管道（可能需要长达两个小时的时间）。我们评估了几种先进的医学图像配准方法，包括SynthMorph，以确定将我们网格化的解剖脑图谱变形为患者MRI扫描图像的最优配准方法。我们的结果表明，SynthMorph在解剖结构之间实现了高的DICE相似性系数和低的Hausdorff距离指标，同时保持了高的网格元素质量。这些发现表明，我们的基于配准的方法能够高效且准确地产生高质量、全面的个性化大脑网格，这是向临床应用翻译的重要一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00931v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于注册的新方法，用于创建个性化的脑计算网格模型。该方法利用最新的解剖学意识图像注册技术，将开源SPL&#x2F;NAC Brain Atlas的综合六面体网格形态化至患者特定的MRI扫描。新方法能高效、准确地生成个性化的脑网格模型，有望促进其在临床应用和神经科学研究中的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用SynthMorph注册技术创建个性化脑计算网格模型。</li>
<li>将开源SPL&#x2F;NAC Brain Atlas的网格形态化至患者特定MRI扫描。</li>
<li>每个个性化网格包含超过300个标记的解剖结构，超越现有手动或自动方法。</li>
<li>注册方法耗时约20分钟，显著快于当前最先进的网格生成流程。</li>
<li>对比评估了多种先进的医学图像注册方法，确定SynthMorph为最佳选择。</li>
<li>SynthMorph在解剖结构间实现了高DICE相似系数和低Hausdorff距离指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00931">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-be4a0591fceba9b2009c28be1bae8452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a56ec83cdc322e3abf91627df4c67583.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-524d61db3f2046edd0da1ba7ecf408ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93f700137f58b61a47aa18af928bb1e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5894bfde4d44944a58038fa84f27a529.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8409496dfed810eb5a71d927213af23d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multimodal-Distillation-Driven-Ensemble-Learning-for-Long-Tailed-Histopathology-Whole-Slide-Images-Analysis"><a href="#Multimodal-Distillation-Driven-Ensemble-Learning-for-Long-Tailed-Histopathology-Whole-Slide-Images-Analysis" class="headerlink" title="Multimodal Distillation-Driven Ensemble Learning for Long-Tailed   Histopathology Whole Slide Images Analysis"></a>Multimodal Distillation-Driven Ensemble Learning for Long-Tailed   Histopathology Whole Slide Images Analysis</h2><p><strong>Authors:Xitong Ling, Yifeng Ping, Jiawen Li, Jing Peng, Yuxuan Chen, Minxi Ouyang, Yizhi Wang, Yonghong He, Tian Guan, Xiaoping Liu, Lianghui Zhu</strong></p>
<p>Multiple Instance Learning (MIL) plays a significant role in computational pathology, enabling weakly supervised analysis of Whole Slide Image (WSI) datasets. The field of WSI analysis is confronted with a severe long-tailed distribution problem, which significantly impacts the performance of classifiers. Long-tailed distributions lead to class imbalance, where some classes have sparse samples while others are abundant, making it difficult for classifiers to accurately identify minority class samples. To address this issue, we propose an ensemble learning method based on MIL, which employs expert decoders with shared aggregators and consistency constraints to learn diverse distributions and reduce the impact of class imbalance on classifier performance. Moreover, we introduce a multimodal distillation framework that leverages text encoders pre-trained on pathology-text pairs to distill knowledge and guide the MIL aggregator in capturing stronger semantic features relevant to class information. To ensure flexibility, we use learnable prompts to guide the distillation process of the pre-trained text encoder, avoiding limitations imposed by specific prompts. Our method, MDE-MIL, integrates multiple expert branches focusing on specific data distributions to address long-tailed issues. Consistency control ensures generalization across classes. Multimodal distillation enhances feature extraction. Experiments on Camelyon+-LT and PANDA-LT datasets show it outperforms state-of-the-art methods. </p>
<blockquote>
<p>多实例学习（MIL）在计算病理学中扮演着重要角色，它能够对全切片图像（WSI）数据集进行弱监督分析。WSI分析领域面临着一个严重的长尾分布问题，这严重影响了分类器的性能。长尾分布导致类别不平衡，其中一些类别的样本稀少，而其他类别则很丰富，这使得分类器难以准确识别出少数类别的样本。为了解决这个问题，我们提出了一种基于MIL的集成学习方法，该方法使用带有共享聚合器和一致性约束的专家解码器来学习各种分布，并减少类别不平衡对分类器性能的影响。此外，我们引入了一个多模态蒸馏框架，该框架利用在病理学文本对上预训练的文本编码器进行知识蒸馏，并指导MIL聚合器捕获与类别信息相关的更强语义特征。为了确保灵活性，我们使用可学习的提示来引导预训练文本编码器的蒸馏过程，避免特定提示所带来的限制。我们的方法MDE-MIL集成了多个专注于特定数据分布的专家分支，以解决长尾问题。一致性控制确保跨类别的泛化。多模态蒸馏增强了特征提取。在Camelyon+-LT和PANDA-LT数据集上的实验表明，该方法优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00915v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在计算病理学领域中，多实例学习（MIL）在处理全幻灯片图像（WSI）数据集时的应用。针对WSI分析面临的严重长尾分布问题，提出了一种基于MIL的集成学习方法。该方法采用具有共享聚合器和一致性约束的专家解码器，以学习各种分布并减少类别不平衡对分类器性能的影响。此外，引入了一种多模态蒸馏框架，利用在病理学文本对上预训练的文本编码器进行知识蒸馏，引导MIL聚合器捕获与类别信息相关的更强语义特征。实验结果表明，该方法在Camelyon+-LT和PANDA-LT数据集上的性能优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多实例学习（MIL）在计算病理学中的重要作用，特别是在全幻灯片图像（WSI）数据集上的弱监督分析。</li>
<li>WSI分析面临的长尾分布问题及其对分类器性能的影响。</li>
<li>基于MIL的集成学习方法，通过采用专家解码器和共享聚合器来解决类别不平衡问题。</li>
<li>引入多模态蒸馏框架，利用预训练的文本编码器进行知识蒸馏，提高MIL的性能。</li>
<li>使用一致性控制确保跨类别的泛化能力。</li>
<li>MDE-MIL方法通过集成多个专注于特定数据分布的专家分支来解决长尾问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00915">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d60d767db296844d98ea64b6c9530403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60531e5f223daac98a010491288030f5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MR-EIT-Multi-Resolution-Reconstruction-for-Electrical-Impedance-Tomography-via-Data-Driven-and-Unsupervised-Dual-Mode-Neural-Networks"><a href="#MR-EIT-Multi-Resolution-Reconstruction-for-Electrical-Impedance-Tomography-via-Data-Driven-and-Unsupervised-Dual-Mode-Neural-Networks" class="headerlink" title="MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance   Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks"></a>MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance   Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks</h2><p><strong>Authors:Fangming Shi, Jinzhen Liu, Xiangqian Meng, Yapeng Zhou, Hui Xiong</strong></p>
<p>This paper presents a multi-resolution reconstruction method for Electrical Impedance Tomography (EIT), referred to as MR-EIT, which is capable of operating in both supervised and unsupervised learning modes. MR-EIT integrates an ordered feature extraction module and an unordered coordinate feature expression module. The former achieves the mapping from voltage to two-dimensional conductivity features through pre-training, while the latter realizes multi-resolution reconstruction independent of the order and size of the input sequence by utilizing symmetric functions and local feature extraction mechanisms. In the data-driven mode, MR-EIT reconstructs high-resolution images from low-resolution data of finite element meshes through two stages of pre-training and joint training, and demonstrates excellent performance in simulation experiments. In the unsupervised learning mode, MR-EIT does not require pre-training data and performs iterative optimization solely based on measured voltages to rapidly achieve image reconstruction from low to high resolution. It shows robustness to noise and efficient super-resolution reconstruction capabilities in both simulation and real water tank experiments. Experimental results indicate that MR-EIT outperforms the comparison methods in terms of Structural Similarity (SSIM) and Relative Image Error (RIE), especially in the unsupervised learning mode, where it can significantly reduce the number of iterations and improve image reconstruction quality. </p>
<blockquote>
<p>本文提出了一种用于电阻抗断层扫描（EIT）的多分辨率重建方法，简称为MR-EIT。该方法既可在有监督学习模式，也可在无监督学习模式下运行。MR-EIT结合了有序特征提取模块和无序坐标特征表达式模块。前者通过预训练实现电压到二维导电率特征的映射，后者则利用对称函数和局部特征提取机制，实现了独立于输入序列顺序和大小的多分辨率重建。在数据驱动模式下，MR-EIT通过两个阶段（预训练和联合训练）从有限元网格的低分辨率数据中重建高分辨率图像，并在模拟实验中表现出卓越的性能。在无监督学习模式下，MR-EIT无需预训练数据，仅基于测量的电压进行迭代优化，从而迅速实现从低分辨率到高分辨率的图像重建。该方法对噪声具有鲁棒性，并且在模拟和实际水箱实验中均表现出高效的超分辨率重建能力。实验结果表明，MR-EIT在结构相似性（SSIM）和相对图像误差（RIE）方面优于比较方法，特别是在无监督学习模式下，可以显著减少迭代次数并提高图像重建质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00762v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种多分辨率重建方法，用于电气阻抗层析成像（EIT），称为MR-EIT。该方法可在有监督和无监督学习模式下运行，集成了有序特征提取模块和无序坐标特征表达模块。通过预训练实现电压到二维导电特征的映射，利用对称函数和局部特征提取机制实现独立于输入序列顺序和大小的多分辨率重建。在数据驱动模式下，MR-EIT通过两个阶段进行高分辨率图像重建，模拟实验表现优异。无监督模式下，MR-EIT无需预训练数据，基于测量电压进行迭代优化，实现快速低分辨率至高分辨率的图像重建，对噪声具有鲁棒性，仿真和真实水箱实验中的超分辨率重建能力出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MR-EIT是一种多分辨率重建方法，用于电气阻抗层析成像（EIT）。</li>
<li>MR-EIT可在有监督和无监督学习模式下运行。</li>
<li>MR-EIT集成了有序特征提取模块与无序坐标特征表达模块。</li>
<li>通过预训练实现电压到二维导电特征的映射。</li>
<li>MR-EIT利用对称函数和局部特征提取机制进行多分辨率重建，该机制独立于输入序列的顺序和大小。</li>
<li>在数据驱动模式下，MR-EIT通过两个阶段进行高分辨率图像重建，表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00762">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c0928ebdcf6c80a1ef54a3a63858fea4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa553d81777faf2e5939374754334994.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deef2b1af68fc5a6e508a399966d8e42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-649abe2380ab6f4aa442c89754e246ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d48e596daf4ccb2d51368814418799c3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Geodesic-Diffusion-Models-for-Medical-Image-to-Image-Generation"><a href="#Geodesic-Diffusion-Models-for-Medical-Image-to-Image-Generation" class="headerlink" title="Geodesic Diffusion Models for Medical Image-to-Image Generation"></a>Geodesic Diffusion Models for Medical Image-to-Image Generation</h2><p><strong>Authors:Teng Zhang, Hongxu Jiang, Kuang Gong, Wei Shao</strong></p>
<p>Diffusion models transform an unknown data distribution into a Gaussian prior by progressively adding noise until the data become indistinguishable from pure noise. This stochastic process traces a path in probability space, evolving from the original data distribution (considered as a Gaussian with near-zero variance) to an isotropic Gaussian. The denoiser then learns to reverse this process, generating high-quality samples from random Gaussian noise. However, standard diffusion models, such as the Denoising Diffusion Probabilistic Model (DDPM), do not ensure a geodesic (i.e., shortest) path in probability space. This inefficiency necessitates the use of many intermediate time steps, leading to high computational costs in training and sampling. To address this limitation, we propose the Geodesic Diffusion Model (GDM), which defines a geodesic path under the Fisher-Rao metric with a variance-exploding noise scheduler. This formulation transforms the data distribution into a Gaussian prior with minimal energy, significantly improving the efficiency of diffusion models. We trained GDM by continuously sampling time steps from 0 to 1 and using as few as 15 evenly spaced time steps for model sampling. We evaluated GDM on two medical image-to-image generation tasks: CT image denoising and MRI image super-resolution. Experimental results show that GDM achieved state-of-the-art performance while reducing training time by a 50-fold compared to DDPM and 10-fold compared to Fast-DDPM, with 66 times faster sampling than DDPM and a similar sampling speed to Fast-DDPM. These efficiency gains enable rapid model exploration and real-time clinical applications. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/mirthAI/GDM-VE">https://github.com/mirthAI/GDM-VE</a>. </p>
<blockquote>
<p>扩散模型通过逐步添加噪声将未知数据分布转变为高斯先验，直到数据无法与纯噪声区分开。这一随机过程在概率空间中追踪一条路径，从原始数据分布（被视为方差接近零的高斯分布）演变到各向同性高斯。然后，去噪器学习反转这一过程，从随机高斯噪声中生成高质量样本。然而，标准扩散模型，如降噪扩散概率模型（DDPM），并不能确保概率空间中的测地线（即最短）路径。这种低效性需要使用许多中间时间步，导致训练和采样中的计算成本高昂。为了解决这一局限性，我们提出了测地扩散模型（GDM），它在Fisher-Rao度量下定义了一个测地线路径，并配备了一个方差爆炸噪声调度器。这种表述将数据分布转变为高斯先验，以最小的能量消耗，显著提高了扩散模型的效率。我们通过从0到1持续采样时间步，并使用最多15个均匀间隔的时间步来训练GDM模型进行采样。我们在两项医学图像到图像生成任务上评估了GDM：CT图像去噪和MRI图像超分辨率。实验结果表明，GDM达到了最先进的性能，与DDPM相比，训练时间减少了50倍，与Fast-DDPM相比减少了10倍。与DDPM相比，GDM的采样速度快66倍，与Fast-DDPM的采样速度相似。这些效率提升使得模型能够快速探索并应用于实时临床。我们的代码公开在：<a target="_blank" rel="noopener" href="https://github.com/mirthAI/GDM-VE">https://github.com/mirthAI/GDM-VE</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00745v1">PDF</a> </p>
<p><strong>摘要</strong><br>    扩散模型通过逐步添加噪声将未知数据分布转化为高斯先验，直到数据变得与纯噪声无法区分。在概率空间中，它从原始数据分布（被视为具有近零方差的高斯分布）演化到一个等距高斯。去噪器学习逆转这一过程，从随机高斯噪声中生成高质量样本。然而，标准扩散模型（如去噪扩散概率模型DDPM）并不确保概率空间中的最短路径（即测地线）。这种低效需要大量中间时间步骤，导致训练和采样中的高计算成本。为解决这一问题，我们提出了Geodesic Diffusion Model（GDM），在Fisher-Rao度量下定义了测地线路径，并采用方差爆炸噪声调度器。这种表述将数据分布转换为高斯先验，以最小的能量，显著提高了扩散模型的效率。我们在两个医学图像到图像生成任务上评估了GDM：CT图像去噪和MRI图像超分辨率。实验结果表明，GDM在减少训练时间的同时实现了最先进的性能，相较于DDPM减少50倍，相较于Fast-DDPM减少10倍；相较于DDPM采样速度提高66倍，与Fast-DDPM采样速度相似。这些效率提升使模型快速探索及实时临床应用成为可能。我们的代码公开在：<a target="_blank" rel="noopener" href="https://github.com/mirthAI/GDM-VE">https://github.com/mirthAI/GDM-VE</a>。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型通过逐步添加噪声将数据分布转化为高斯先验。</li>
<li>标准扩散模型在概率空间中并不总是采取最短的测地线路径，导致训练和采样的高计算成本。</li>
<li>提出的Geodesic Diffusion Model（GDM）在Fisher-Rao度量下定义测地线路径，提高扩散模型的效率。</li>
<li>GDM在医学图像生成任务上表现卓越，如CT图像去噪和MRI图像超分辨率。</li>
<li>GDM相较于其他模型大幅减少了训练时间和采样时间。</li>
<li>GDM的实现代码已公开，便于研究者和开发者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00745">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f24cbc4dacac9887ac1e485629e24d70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13180e16fc67eaaf4a8de963818f223e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Flow-Matching-for-Medical-Image-Synthesis-Bridging-the-Gap-Between-Speed-and-Quality"><a href="#Flow-Matching-for-Medical-Image-Synthesis-Bridging-the-Gap-Between-Speed-and-Quality" class="headerlink" title="Flow Matching for Medical Image Synthesis: Bridging the Gap Between   Speed and Quality"></a>Flow Matching for Medical Image Synthesis: Bridging the Gap Between   Speed and Quality</h2><p><strong>Authors:Milad Yazdani, Yasamin Medghalchi, Pooria Ashrafian, Ilker Hacihaliloglu, Dena Shahriari</strong></p>
<p>Deep learning models have emerged as a powerful tool for various medical applications. However, their success depends on large, high-quality datasets that are challenging to obtain due to privacy concerns and costly annotation. Generative models, such as diffusion models, offer a potential solution by synthesizing medical images, but their practical adoption is hindered by long inference times. In this paper, we propose the use of an optimal transport flow matching approach to accelerate image generation. By introducing a straighter mapping between the source and target distribution, our method significantly reduces inference time while preserving and further enhancing the quality of the outputs. Furthermore, this approach is highly adaptable, supporting various medical imaging modalities, conditioning mechanisms (such as class labels and masks), and different spatial dimensions, including 2D and 3D. Beyond image generation, it can also be applied to related tasks such as image enhancement. Our results demonstrate the efficiency and versatility of this framework, making it a promising advancement for medical imaging applications. Code with checkpoints and a synthetic dataset (beneficial for classification and segmentation) is now available on: <a target="_blank" rel="noopener" href="https://github.com/milad1378yz/MOTFM">https://github.com/milad1378yz/MOTFM</a>. </p>
<blockquote>
<p>深度学习模型在各种医疗应用中已崭露头角。然而，它们的成功依赖于难以获得的大规模高质量数据集，这主要是由于隐私担忧和昂贵的标注成本所致。生成模型，如扩散模型，通过合成医疗图像提供了一个潜在的解决方案，但它们由于漫长的推理时间而阻碍了实际应用。在本文中，我们提出了一种使用最优传输流匹配方法来加速图像生成的方法。通过引入源和目标分布之间的更直接映射，我们的方法在保持和进一步提高输出质量的同时，显著减少了推理时间。此外，这种方法具有高度适应性，支持各种医学成像模式、调节机制（如类别标签和掩码），以及不同的空间维度，包括二维和三维。除了图像生成，它还可以应用于图像增强等相关任务。我们的结果证明了该框架的高效率和多功能性，使其成为医疗成像应用中的一项有前途的进步。关于检查点和合成数据集的代码现在可以在以下链接找到：<a target="_blank" rel="noopener" href="https://github.com/milad1378yz/MOTFM%EF%BC%88%E8%BF%99%E4%B8%AA%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86%E5%AF%B9%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E6%9C%89%E7%9B%8A%EF%BC%89%E3%80%82">https://github.com/milad1378yz/MOTFM（这个合成数据集对分类和分割任务有益）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00266v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像生成领域，提出一种基于最优传输流匹配（OTFM）的加速图像生成方法。该方法通过源与目标分布之间的更直接映射，显著减少推理时间，同时保证输出质量。方法高度灵活，适用于多种医学成像模式、条件机制和空间维度。代码及数据集已在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>最优传输流匹配（OTFM）方法被应用于医学图像生成，旨在加速推理过程。</li>
<li>OTFM方法通过创建源和目标分布之间的直接映射，提高了图像生成的效率。</li>
<li>该方法在保证图像质量的同时，显著减少了推理时间。</li>
<li>OTFM方法高度灵活，能够适应不同的医学成像模式、条件机制和空间维度（如2D和3D）。</li>
<li>该方法不仅适用于图像生成，还可以应用于相关任务，如图像增强。</li>
<li>公开的代码和合成数据集可用于分类和分割任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00266">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-078a674888586c3da5787c01a0e64810.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d53910d522c54d2babf4e4bc2418d99f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a589c25241dd817cb0ca279ab605cd2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PRISM-High-Resolution-Precise-Counterfactual-Medical-Image-Generation-using-Language-guided-Stable-Diffusion"><a href="#PRISM-High-Resolution-Precise-Counterfactual-Medical-Image-Generation-using-Language-guided-Stable-Diffusion" class="headerlink" title="PRISM: High-Resolution &amp; Precise Counterfactual Medical Image Generation   using Language-guided Stable Diffusion"></a>PRISM: High-Resolution &amp; Precise Counterfactual Medical Image Generation   using Language-guided Stable Diffusion</h2><p><strong>Authors:Amar Kumar, Anita Kriz, Mohammad Havaei, Tal Arbel</strong></p>
<p>Developing reliable and generalizable deep learning systems for medical imaging faces significant obstacles due to spurious correlations, data imbalances, and limited text annotations in datasets. Addressing these challenges requires architectures robust to the unique complexities posed by medical imaging data. The rapid advancements in vision-language foundation models within the natural image domain prompt the question of how they can be adapted for medical imaging tasks. In this work, we present PRISM, a framework that leverages foundation models to generate high-resolution, language-guided medical image counterfactuals using Stable Diffusion. Our approach demonstrates unprecedented precision in selectively modifying spurious correlations (the medical devices) and disease features, enabling the removal and addition of specific attributes while preserving other image characteristics. Through extensive evaluation, we show how PRISM advances counterfactual generation and enables the development of more robust downstream classifiers for clinically deployable solutions. To facilitate broader adoption and research, we make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/Amarkr1/PRISM">https://github.com/Amarkr1/PRISM</a>. </p>
<blockquote>
<p>在医学图像领域，开发可靠且可推广的深度学习系统面临着重大挑战，这主要是由于数据中的虚假关联、数据不平衡以及文本注释有限。要解决这些挑战，需要构建能够应对医学成像数据独特复杂性的架构。自然图像领域的视觉语言基础模型的快速发展引发了一个问题，即如何将这些模型适应于医学成像任务。在这项工作中，我们提出了PRISM框架，该框架利用基础模型，使用Stable Diffusion生成高分辨率的、受语言引导的医疗图像反事实。我们的方法以前所未有的精度选择性地修改虚假关联（医疗器材）和疾病特征，能够在保留其他图像特征的同时删除和添加特定属性。通过广泛的评估，我们展示了PRISM在反事实生成方面的进展，并证明其能够开发更稳健的下游分类器，用于临床部署解决方案。为了促进更广泛的采用和研究，我们在<a target="_blank" rel="noopener" href="https://github.com/Amarkr1/PRISM">https://github.com/Amarkr1/PRISM</a>上公开了我们的代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00196v1">PDF</a> Under Review for MIDL 2025</p>
<p><strong>Summary</strong><br>     针对医学成像领域的深度学习系统面临诸多挑战，如虚假关联、数据不平衡和标注文本有限等。为解决这些问题，需要适应医学成像数据独特复杂性的稳健架构。本研究提出PRISM框架，利用基础模型生成高分辨率、语言引导的医疗图像反事实，利用Stable Diffusion进行选择性地修改虚假关联和疾病特征，实现特定属性的添加和移除，同时保留其他图像特征。PRISM推动反事实生成技术的发展，并有助于开发更稳健的下游分类器，为临床部署解决方案提供可能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学成像领域的深度学习系统面临诸多挑战，包括虚假关联、数据不平衡和标注文本有限等。</li>
<li>PRISM框架利用基础模型生成语言引导的高分辨率医疗图像反事实。</li>
<li>PRISM能够选择性地修改医学图像中的虚假关联和疾病特征。</li>
<li>PRISM通过添加和移除特定属性，同时保留其他图像特征，实现精准修改。</li>
<li>PRISM框架推动了反事实生成技术的发展。</li>
<li>PRISM有助于开发更稳健的下游分类器，为临床部署解决方案提供支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00196">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-04dae0498b203c0b9df097bfaf49e45b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f91729dbb831971a8f62fd3f1ab6c0da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a44f50a53eda689dd0f401684ffa8bdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e39f1904c80cc6822d2def7e029ceaf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7f6d825d0701082ffabc72abfc6b636.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Tool-or-Tutor-Experimental-evidence-from-AI-deployment-in-cancer-diagnosis"><a href="#Tool-or-Tutor-Experimental-evidence-from-AI-deployment-in-cancer-diagnosis" class="headerlink" title="Tool or Tutor? Experimental evidence from AI deployment in cancer   diagnosis"></a>Tool or Tutor? Experimental evidence from AI deployment in cancer   diagnosis</h2><p><strong>Authors:Vivianna Fang He, Sihan Li, Phanish Puranam</strong></p>
<p>Professionals increasingly use Artificial Intelligence (AI) to enhance their capabilities and assist with task execution. While prior research has examined these uses separately, their potential interaction remains underexplored. We propose that AI-driven training (“tutor” effect) and AI-assisted task completion (“tool” effect) can be complementary and test this hypothesis in the context of lung cancer diagnosis. In a field experiment with 336 medical students, we manipulated AI deployment in training, in practice, and in both. Our findings reveal that while AI-integrated training and AI assistance independently improved diagnostic performance, their combination yielded the highest accuracy. These results underscore AI’s dual role in enhancing human performance through both learning and real-time support, offering insights into AI deployment in professional settings where human expertise remains essential. </p>
<blockquote>
<p>专业人员越来越多地使用人工智能（AI）来增强自身能力并辅助执行任务。虽然之前的研究已经分别研究了这些用途，但它们之间的潜在相互作用仍未得到充分探索。我们提出AI驱动的培训（“导师”效应）和AI辅助任务完成（“工具”效应）可以互补，并在肺癌诊断的情境中检验这一假设。在一项有336名医学学生参与的实验中，我们操作了培训、实践和两者都涉及的AI部署情况。我们的研究发现，虽然AI集成培训和AI辅助独立改善了诊断性能，但它们的结合却获得了最高的准确率。这些结果强调了人工智能通过学习和实时支持增强人类性能的双面角色，为在专业环境中部署人工智能提供了见解，在这些环境中，人类专业知识仍然至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16411v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>人工智能（AI）在医学图像诊断领域正被越来越多的专业人士用来提升能力和辅助任务执行。本研究探讨了AI在训练（“导师效应”）和完成医学任务（“工具效应”）方面的潜力互补性，并在肺癌诊断的情境下进行了测试。实验发现，结合AI训练和AI辅助的独立提高诊断性能的同时，两者的结合能产生最高的准确度。这表明AI通过学习和实时支持两方面增强人类表现，对于在专业人士仍需发挥重要作用的专业环境中部署AI具有启示意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI在医学图像诊断领域得到广泛应用，能够提升专业人士的能力和辅助任务执行。</li>
<li>AI在训练和任务完成方面的潜力可以互补。</li>
<li>在肺癌诊断的情境下，AI训练和AI辅助的结合产生了最高的诊断准确度。</li>
<li>AI通过学习和实时支持两方面增强人类表现。</li>
<li>AI部署对于在专业人士仍需发挥重要作用的专业环境中具有启示意义。</li>
<li>实验结果表明，AI的“导师效应”和“工具效应”共同作用可最大化提高诊断准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16411">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7f96b606dd3bd2c28496904c562fbd61.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis"><a href="#MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis" class="headerlink" title="MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis"></a>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis</h2><p><strong>Authors:Wei Dai, Jun Liu</strong></p>
<p>Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the &#96;&#96;Mamba’’ model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba’s potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models. </p>
<blockquote>
<p>高效评估三维（3D）医学图像对医疗诊断和治疗实践至关重要。近年来，深度学习和计算机视觉在医学图像分析和解释方面的应用有所增长。传统方法，如卷积神经网络（CNNs）和视觉转换器（ViTs），面临重大的计算挑战，促使架构发展的需求。最近的努力导致了新型架构的出现，如作为替代传统CNN或ViT的解决方案的“Mamba”模型。Mamba模型在处理一维数据的线性处理方面表现出色且计算需求较低。然而，Mamba在3D医学图像分析方面的潜力尚未得到充分探索，随着维度的增加可能会面临重大的计算挑战。本文提出了MobileViM，这是一个用于高效分割3D医学图像的流线化架构。在MobileViM网络中，我们发明了一种新的维度独立机制和一种双向遍历方法与基于视觉Mamba的框架相结合。MobileViM还采用跨尺度桥接技术，以提高不同医学成像模式的效率和准确性。通过这些增强功能，MobileViM在单个图形处理单元（即NVIDIA RTX 4090）上实现了超过每秒90帧（FPS）的分割速度。此性能比使用相同计算资源的最新深度学习模型处理3D图像的速度快24 FPS以上。此外，实验评估表明，MobileViM的性能卓越，在PENGWIN、BraTS2024、ATLAS和Toothfairy2数据集上的Dice相似度得分分别达到92.72%、86.69%、80.46%和77.43%，显著超越了现有模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13524v3">PDF</a> </p>
<p><strong>Summary</strong><br>    提出一种高效的三维医学图像分割架构MobileViM，采用维度独立机制、双向遍历方法和跨尺度桥接技术，实现快速而精确的医学图像分析，速度超过现有模型，并在多个数据集上取得优异性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MobileViM是一个针对三维医学图像分割的流线型架构。</li>
<li>该架构采用新的维度独立机制和双向遍历方法，结合视觉Mamba基础框架。</li>
<li>MobileViM的跨尺度桥接技术提高了在各种医学成像模式中的效率和准确性。</li>
<li>MobileViM实现了在单个图形处理单元上的每秒超过90帧的分割速度，比现有模型快24帧以上。</li>
<li>MobileViM在多个数据集上的性能表现优于现有模型，Dice相似度得分分别为92.72%、86.69%、80.46%和77.43%。</li>
<li>该架构解决了传统卷积神经网络和视觉变压器在三维医学图像分析中的计算挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13524">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a19da823a39016902a1c548a5c88342b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b8d590b505813b80d6e6b5e2bad344e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5082a48f077c477e48276a57b6fe17b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d40734e73903ff1d635e972ebebfdebd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Vision-based-Geo-Localization-of-Future-Mars-Rotorcraft-in-Challenging-Illumination-Conditions"><a href="#Vision-based-Geo-Localization-of-Future-Mars-Rotorcraft-in-Challenging-Illumination-Conditions" class="headerlink" title="Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging   Illumination Conditions"></a>Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging   Illumination Conditions</h2><p><strong>Authors:Dario Pisanti, Robert Hewitt, Roland Brockers, Georgios Georgakis</strong></p>
<p>Planetary exploration using aerial assets has the potential for unprecedented scientific discoveries on Mars. While NASA’s Mars helicopter Ingenuity proved flight in Martian atmosphere is possible, future Mars rotocrafts will require advanced navigation capabilities for long-range flights. One such critical capability is Map-based Localization (MbL) which registers an onboard image to a reference map during flight in order to mitigate cumulative drift from visual odometry. However, significant illumination differences between rotocraft observations and a reference map prove challenging for traditional MbL systems, restricting the operational window of the vehicle. In this work, we investigate a new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model for image registration that is more robust under large illumination differences than prior models. The system is supported by a custom simulation framework that uses real orbital maps to produce large amounts of realistic images of the Martian terrain. Comprehensive evaluations show that our proposed system outperforms prior MbL efforts in terms of localization accuracy under significant lighting and scale variations. Furthermore, we demonstrate the validity of our approach across a simulated Martian day. </p>
<blockquote>
<p>利用空中资源进行行星探索具有在火星上获得前所未有的科学发现的潜力。虽然美国宇航局的火星直升机“机智号”证明了在火星大气中飞行是可能的，但未来的火星旋翼机将需要先进的导航能力以进行远程飞行。其中一项关键能力是地图定位（MbL），它在飞行过程中将机载图像注册到参考地图上，以减少视觉里程计的累积漂移。然而，旋翼机观测与参考地图之间的显著照明差异给传统MbL系统带来了挑战，限制了车辆的作业窗口。在这项工作中，我们调查了一种新的MbL系统，并提出Geo-LoFTR，这是一种辅助几何的深度学习图像注册模型，与先前模型相比，它在较大的照明差异下更为稳健。该系统由一个自定义仿真框架支持，该框架使用真实的轨道地图生成大量真实的火星地形图像。综合评估表明，我们提出的系统在照明和比例尺变化较大的情况下，在定位精度方面优于先前的MbL工作。此外，我们在模拟的火星日周期中验证了我们的方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09795v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在火星上，利用空中资源进行行星探索具有巨大的科学发现潜力。NASA的火星直升机“机智号”证明了在火星大气中飞行的可行性，但未来的火星旋翼飞行器需要先进的导航能力以实现远程飞行。其中一项关键能力是基于地图的定位（MbL），它在飞行过程中将机上图像注册到参考地图上，以减轻由视觉里程计引起的累积漂移问题。然而，旋翼飞行器观测和参考地图之间的显著照明差异对传统的MbL系统构成挑战，限制了车辆的操作窗口。本研究调查了一种新型的MbL系统，并提出Geo-LoFTR，这是一种辅助几何的深度学习模型，用于图像注册，能够在较大的照明差异下比先前模型表现出更强的稳健性。该系统得到了一个自定义仿真框架的支持，该框架使用真实的轨道地图生成大量逼真的火星地形图像。综合评估表明，我们的系统在照明和比例变化较大的情况下，定位精度优于先前的MbL努力。此外，我们在模拟的火星日周期内验证了我们的方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>行星探索利用空中资源具有巨大的科学发现潜力，特别是在火星上。</li>
<li>NASA的火星直升机“机智号”证明了在火星大气中飞行的可行性。</li>
<li>未来的火星旋翼飞行器需要先进的导航能力，其中关键之一是基于地图的定位（MbL）。</li>
<li>旋翼飞行器观测和参考地图之间的照明差异对传统的MbL系统构成挑战。</li>
<li>Geo-LoFTR是一种新型的MbL系统，通过辅助几何的深度学习模型进行图像注册，表现更强的稳健性。</li>
<li>自定义仿真框架支持Geo-LoFTR系统，使用真实的轨道地图生成大量逼真的火星地形图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09795">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a9b3653f319014b25fbc5baf04ea8212.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc42af83f315c318560f2893e6708db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0360e432cb5a466240dc46b94a446dc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5a62e710921343120fbafc1d981bfaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ccb5b3aa052a1b9d722071b87f439dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e757a27bbb268b509dec655634c3b03.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Rethinking-High-speed-Image-Reconstruction-Framework-with-Spike-Camera"><a href="#Rethinking-High-speed-Image-Reconstruction-Framework-with-Spike-Camera" class="headerlink" title="Rethinking High-speed Image Reconstruction Framework with Spike Camera"></a>Rethinking High-speed Image Reconstruction Framework with Spike Camera</h2><p><strong>Authors:Kang Chen, Yajing Zheng, Tiejun Huang, Zhaofei Yu</strong></p>
<p>Spike cameras, as innovative neuromorphic devices, generate continuous spike streams to capture high-speed scenes with lower bandwidth and higher dynamic range than traditional RGB cameras. However, reconstructing high-quality images from the spike input under low-light conditions remains challenging. Conventional learning-based methods often rely on the synthetic dataset as the supervision for training. Still, these approaches falter when dealing with noisy spikes fired under the low-light environment, leading to further performance degradation in the real-world dataset. This phenomenon is primarily due to inadequate noise modelling and the domain gap between synthetic and real datasets, resulting in recovered images with unclear textures, excessive noise, and diminished brightness. To address these challenges, we introduce a novel spike-to-image reconstruction framework SpikeCLIP that goes beyond traditional training paradigms. Leveraging the CLIP model’s powerful capability to align text and images, we incorporate the textual description of the captured scene and unpaired high-quality datasets as the supervision. Our experiments on real-world low-light datasets U-CALTECH and U-CIFAR demonstrate that SpikeCLIP significantly enhances texture details and the luminance balance of recovered images. Furthermore, the reconstructed images are well-aligned with the broader visual features needed for downstream tasks, ensuring more robust and versatile performance in challenging environments. </p>
<blockquote>
<p>脉冲相机作为一种创新的神经形态设备，能够产生连续的脉冲流，以低于传统RGB相机的带宽和更高的动态范围来捕捉高速场景。然而，在低光照条件下从脉冲输入重建高质量图像仍然具有挑战性。传统的基于学习的方法通常依赖于合成数据集作为训练过程的监督。然而，这些方法在处理低光环境下产生的噪声脉冲时常常表现不佳，导致在实际数据集上的性能进一步下降。这种现象的主要原因是噪声建模不足以及合成数据集和真实数据集之间的域差距，导致恢复后的图像纹理不清晰、噪声过多以及亮度降低。为了解决这些挑战，我们引入了一种新型的脉冲到图像重建框架SpikeCLIP，它超越了传统的训练模式。我们借助CLIP模型的强大文本和图像对齐能力，将捕获场景的文本描述和未配对的高质量数据集作为监督信息。我们在实际低光数据集U-CALTECH和U-CIFAR上的实验表明，SpikeCLIP显著提高了恢复图像的纹理细节和亮度平衡。此外，重建的图像与下游任务所需的更广泛的视觉特征对齐，确保在具有挑战性的环境中表现出更稳健和多功能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04477v2">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>这是一篇关于脉冲相机在低光照环境下图像重建的研究。该研究引入了SpikeCLIP框架，利用CLIP模型的文本与图像对齐能力，结合场景文本描述和高质量数据集进行监督，解决了传统学习方法在面对噪声脉冲时的性能下降问题。实验证明，SpikeCLIP能够显著提高低光照环境下重建图像的纹理细节和亮度平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>脉冲相机通过连续脉冲流捕捉高速场景，相比传统RGB相机具有更低的带宽和更高的动态范围。</li>
<li>低光照环境下从脉冲输入重建高质量图像具有挑战性。</li>
<li>常规的学习型方法依赖于合成数据集进行训练，但在面对低光照环境下噪声脉冲时表现不佳。</li>
<li>SpikeCLIP框架超越了传统训练模式，利用CLIP模型的文本与图像对齐能力。</li>
<li>SpikeCLIP结合场景文本描述和未配对的高质量数据集进行监督。</li>
<li>在真实世界的低光照数据集U-CALTECH和U-CIFAR上的实验证明，SpikeCLIP能显著提高重建图像的纹理细节和亮度平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04477">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4fce9329c288cddf767380a28522750c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff285dcbe3c326a4b4963101d2742db8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a10e4159d1b7fa4bbfea0d90e79d465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83d8f95fe9a364f186a41fc9477dff9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee32d7c171f4e38e519776b99054cd60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed5bb3a3df28409141efaa8c545f9b76.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder"><a href="#XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder" class="headerlink" title="XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction   Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder"></a>XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction   Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder</h2><p><strong>Authors:Shenghao Zhu, Yifei Chen, Shuo Jiang, Weihong Chen, Chang Liu, Yuanhan Wang, Xu Chen, Yifan Ke, Feiwei Qin, Changmiao Wang, Zhu Zhu</strong></p>
<p>Neurogliomas are among the most aggressive forms of cancer, presenting considerable challenges in both treatment and monitoring due to their unpredictable biological behavior. Magnetic resonance imaging (MRI) is currently the preferred method for diagnosing and monitoring gliomas. However, the lack of specific imaging techniques often compromises the accuracy of tumor segmentation during the imaging process. To address this issue, we introduce the XLSTM-HVED model. This model integrates a hetero-modal encoder-decoder framework with the Vision XLSTM module to reconstruct missing MRI modalities. By deeply fusing spatial and temporal features, it enhances tumor segmentation performance. The key innovation of our approach is the Self-Attention Variational Encoder (SAVE) module, which improves the integration of modal features. Additionally, it optimizes the interaction of features between segmentation and reconstruction tasks through the Squeeze-Fusion-Excitation Cross Awareness (SFECA) module. Our experiments using the BraTS 2024 dataset demonstrate that our model significantly outperforms existing advanced methods in handling cases where modalities are missing. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED">https://github.com/Quanato607/XLSTM-HVED</a>. </p>
<blockquote>
<p>神经胶质瘤是最具侵袭性的癌症之一，由于其不可预测的生物行为，为治疗和监测带来了相当大的挑战。目前，磁共振成像（MRI）是诊断和治疗胶质瘤的首选方法。然而，缺乏特定的成像技术往往会影响成像过程中肿瘤分割的准确性。为了解决这一问题，我们引入了XLSTM-HVED模型。该模型结合了异模式编码器-解码器框架和Vision XLSTM模块，以重建缺失的MRI模式。通过深度融合空间和时间特征，提高了肿瘤分割的性能。我们方法的关键创新点是自注意力变分编码器（SAVE）模块，它改进了模式特征的融合。此外，它通过Squeeze-Fusion-Excitation Cross Awareness（SFECA）模块优化了分割和重建任务之间的特征交互。我们使用BraTS 2024数据集进行的实验表明，我们的模型在处理缺失模式的情况时显著优于现有的高级方法。我们的源代码可在[<a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/Quanato607/XLSTM-HVED找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07804v3">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了神经胶质瘤的治疗和监测挑战，以及磁共振成像（MRI）在诊断与监测中的作用。为解决MRI成像过程中因缺乏特定成像技术而影响肿瘤分割准确性的问题，提出了XLSTM-HVED模型。该模型结合了异模态编码器-解码器框架与Vision XLSTM模块，可重建缺失的MRI模态，通过深度融合时空特征提高肿瘤分割性能。其关键创新在于自注意力变分编码器（SAVE）模块，可改进模态特征的整合。同时，通过Squeeze-Fusion-Excitation Cross Awareness（SFECA）模块优化分割与重建任务之间的特征交互。使用BraTS 2024数据集的实验表明，该模型在处理缺失模态的情况时显著优于现有先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经胶质瘤治疗与监测存在挑战，MRI是目前首选的诊断与监测方法。</li>
<li>缺乏特定成像技术会影响MRI成像过程中的肿瘤分割准确性。</li>
<li>XLSTM-HVED模型通过结合异模态编码器-解码器框架与Vision XLSTM模块，旨在解决这一问题。</li>
<li>该模型可重建缺失的MRI模态，并通过深度融合时空特征提高肿瘤分割性能。</li>
<li>自注意力变分编码器（SAVE）模块是模型的关键创新，可改进模态特征的整合。</li>
<li>SFECA模块优化分割与重建任务之间的特征交互。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07804">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-07bf78a820d9382ff8bae4344379d0fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da60db8443b675a68c3c47ba18158641.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-970e1b1c8819dbfa82c0b6a5bd066932.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ac994a06936a3646fed8efd39998301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-078694a99b64803abd171ca0ae8cce30.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-51b6b444117a3bd724c529fa9d923b9d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-03-07  Good practices for evaluation of synthesized speech
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-07  DualDiff+ Dual-Branch Diffusion for High-Fidelity Video Generation with   Reward Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
