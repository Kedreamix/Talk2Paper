<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  A self-supervised cyclic neural-analytic approach for novel view   synthesis and 3D reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ff31b8c293a9847af10bba02cef2b473.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-07-æ›´æ–°"><a href="#2025-03-07-æ›´æ–°" class="headerlink" title="2025-03-07 æ›´æ–°"></a>2025-03-07 æ›´æ–°</h1><h2 id="A-self-supervised-cyclic-neural-analytic-approach-for-novel-view-synthesis-and-3D-reconstruction"><a href="#A-self-supervised-cyclic-neural-analytic-approach-for-novel-view-synthesis-and-3D-reconstruction" class="headerlink" title="A self-supervised cyclic neural-analytic approach for novel view   synthesis and 3D reconstruction"></a>A self-supervised cyclic neural-analytic approach for novel view   synthesis and 3D reconstruction</h2><p><strong>Authors:Dragos Costea, Alina Marcu, Marius Leordeanu</strong></p>
<p>Generating novel views from recorded videos is crucial for enabling autonomous UAV navigation. Recent advancements in neural rendering have facilitated the rapid development of methods capable of rendering new trajectories. However, these methods often fail to generalize well to regions far from the training data without an optimized flight path, leading to suboptimal reconstructions. We propose a self-supervised cyclic neural-analytic pipeline that combines high-quality neural rendering outputs with precise geometric insights from analytical methods. Our solution improves RGB and mesh reconstructions for novel view synthesis, especially in undersampled areas and regions that are completely different from the training dataset. We use an effective transformer-based architecture for image reconstruction to refine and adapt the synthesis process, enabling effective handling of novel, unseen poses without relying on extensive labeled datasets. Our findings demonstrate substantial improvements in rendering views of novel and also 3D reconstruction, which to the best of our knowledge is a first, setting a new standard for autonomous navigation in complex outdoor environments. </p>
<blockquote>
<p>ä»å½•åˆ¶çš„è§†é¢‘ä¸­ç”Ÿæˆæ–°å‹è§†è§’å¯¹äºå®ç°è‡ªä¸»æ— äººæœºå¯¼èˆªè‡³å…³é‡è¦ã€‚ç¥ç»ç½‘ç»œæ¸²æŸ“çš„è¿‘æœŸè¿›å±•ä¿ƒè¿›äº†èƒ½å¤Ÿå‘ˆç°æ–°è½¨è¿¹çš„æ–¹æ³•çš„å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ²¡æœ‰ä¼˜åŒ–é£è¡Œè·¯å¾„çš„æƒ…å†µä¸‹ï¼Œå¾€å¾€ä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°è¿œç¦»è®­ç»ƒæ•°æ®çš„åŒºåŸŸï¼Œä»è€Œå¯¼è‡´é‡å»ºæ•ˆæœä¸ç†æƒ³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å¾ªç¯ç¥ç»åˆ†æç®¡é“ï¼Œè¯¥ç®¡é“ç»“åˆäº†é«˜è´¨é‡ç¥ç»æ¸²æŸ“è¾“å‡ºå’Œæ¥è‡ªåˆ†ææ–¹æ³•çš„ç²¾ç¡®å‡ ä½•æ´å¯ŸåŠ›ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ”¹è¿›äº†ç”¨äºåˆæˆæ–°å‹è§†è§’çš„RGBå’Œç½‘æ ¼é‡å»ºï¼Œç‰¹åˆ«æ˜¯åœ¨æ¬ é‡‡æ ·åŒºåŸŸå’Œä¸è®­ç»ƒæ•°æ®é›†å®Œå…¨ä¸åŒçš„åŒºåŸŸã€‚æˆ‘ä»¬é‡‡ç”¨æœ‰æ•ˆçš„åŸºäºè½¬æ¢å™¨çš„æ¶æ„è¿›è¡Œå›¾åƒé‡å»ºï¼Œä»¥ä¼˜åŒ–å’Œé€‚åº”åˆæˆè¿‡ç¨‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æ–°å‹ã€æœªè§å§¿æ€ï¼Œè€Œæ— éœ€ä¾èµ–å¤§é‡æ ‡è®°æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°åœ¨æ–°å‹è§†è§’æ¸²æŸ“å’Œ3Dé‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡å®ç°ï¼Œä¸ºå¤æ‚å®¤å¤–ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03543v1">PDF</a> Published in BMVC 2024, 10 pages, 4 figures</p>
<p><strong>Summary</strong><br>è§†é¢‘ç”Ÿæˆæ–°æŠ€æœ¯èƒ½æå‡æ— äººæœºè‡ªä¸»å¯¼èˆªèƒ½åŠ›ã€‚æ–°æ–¹æ³•ç»“åˆäº†é«˜è´¨é‡ç¥ç»æ¸²æŸ“è¾“å‡ºå’Œç²¾ç¡®å‡ ä½•åˆ†æï¼Œæ”¹å–„æœªçŸ¥è§†è§’çš„åˆæˆå’Œä¸‰ç»´é‡å»ºï¼Œå°¤å…¶é€‚ç”¨äºé‡‡æ ·ä¸è¶³å’Œä¸è®­ç»ƒé›†å·®å¼‚å¤§çš„åŒºåŸŸã€‚é‡‡ç”¨æœ‰æ•ˆçš„è½¬æ¢å™¨æ¶æ„è¿›è¡Œå›¾åƒé‡å»ºï¼Œå¯ç²¾ç»†è°ƒæ•´å¹¶é€‚åº”åˆæˆè¿‡ç¨‹ï¼Œæœ‰æ•ˆå¤„ç†æ–°é¢–ã€æœªè§å§¿æ€ï¼Œæ— éœ€ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®é›†ã€‚æ­¤æŠ€æœ¯é©æ–°äº†å¤æ‚æˆ·å¤–ç¯å¢ƒçš„è‡ªä¸»å¯¼èˆªæ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»æ— äººæœºå¯¼èˆªä¸­ï¼Œä»å½•åˆ¶è§†é¢‘ç”Ÿæˆæ–°é¢–è§†è§’è‡³å…³é‡è¦ã€‚</li>
<li>ç¥ç»æ¸²æŸ“æŠ€æœ¯çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†æ–°å‹æ¸²æŸ“æ–¹æ³•çš„å‘å±•ã€‚</li>
<li>è¿™äº›æ–¹æ³•åœ¨æ²¡æœ‰ä¼˜åŒ–é£è¡Œè·¯å¾„çš„æƒ…å†µä¸‹ï¼Œéš¾ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°è¿œç¦»è®­ç»ƒæ•°æ®çš„åŒºåŸŸï¼Œå¯¼è‡´é‡å»ºæ•ˆæœä¸ç†æƒ³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªæˆ‘ç›‘ç£çš„å¾ªç¯ç¥ç»åˆ†æç®¡é“ï¼Œç»“åˆäº†é«˜è´¨é‡ç¥ç»æ¸²æŸ“å’Œç²¾ç¡®å‡ ä½•åˆ†æã€‚</li>
<li>è¯¥è§£å†³æ–¹æ¡ˆæ”¹è¿›äº†RGBå’Œç½‘æ ¼é‡å»ºç”¨äºæ–°é¢–è§†è§’çš„åˆæˆï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‡æ ·ä¸è¶³å’Œä¸è®­ç»ƒé›†å®Œå…¨ä¸åŒçš„åŒºåŸŸã€‚</li>
<li>é‡‡ç”¨æœ‰æ•ˆçš„è½¬æ¢å™¨æ¶æ„è¿›è¡Œå›¾åƒé‡å»ºï¼Œæé«˜åˆæˆè¿‡ç¨‹çš„ç²¾ç»†åº¦å’Œé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7e069fe4103a1ebbbd3272cd86ec607.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-403e794c0baf4be1ca229e72e2e0f6bf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Top-K-Maximum-Intensity-Projection-Priors-for-3D-Liver-Vessel-Segmentation"><a href="#Top-K-Maximum-Intensity-Projection-Priors-for-3D-Liver-Vessel-Segmentation" class="headerlink" title="Top-K Maximum Intensity Projection Priors for 3D Liver Vessel   Segmentation"></a>Top-K Maximum Intensity Projection Priors for 3D Liver Vessel   Segmentation</h2><p><strong>Authors:Xiaotong Zhang, Alexander Broersen, Gonnie CM van Erp, Silvia L. Pintea, Jouke Dijkstra</strong></p>
<p>Liver-vessel segmentation is an essential task in the pre-operative planning of liver resection. State-of-the-art 2D or 3D convolution-based methods focusing on liver vessel segmentation on 2D CT cross-sectional views, which do not take into account the global liver-vessel topology. To maintain this global vessel topology, we rely on the underlying physics used in the CT reconstruction process, and apply this to liver-vessel segmentation. Concretely, we introduce the concept of top-k maximum intensity projections, which mimics the CT reconstruction by replacing the integral along each projection direction, with keeping the top-k maxima along each projection direction. We use these top-k maximum projections to condition a diffusion model and generate 3D liver-vessel trees. We evaluate our 3D liver-vessel segmentation on the 3D-ircadb-01 dataset, and achieve the highest Dice coefficient, intersection-over-union (IoU), and Sensitivity scores compared to prior work. </p>
<blockquote>
<p>è‚è„è¡€ç®¡åˆ†å‰²æ˜¯è‚è„åˆ‡é™¤æœ¯å‰è§„åˆ’ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ã€‚ç›®å‰æœ€å…ˆè¿›çš„2Dæˆ–3Då·ç§¯æ–¹æ³•ä¸»è¦å…³æ³¨äºåœ¨2D CTæ¨ªæˆªé¢è§†å›¾ä¸Šçš„è‚è„è¡€ç®¡åˆ†å‰²ï¼Œè¿™äº›æ–¹æ³•å¹¶æ²¡æœ‰è€ƒè™‘åˆ°å…¨å±€è‚è„è¡€ç®¡æ‹“æ‰‘ç»“æ„ã€‚ä¸ºäº†ä¿æŒè¿™ç§å…¨å±€è¡€ç®¡æ‹“æ‰‘ç»“æ„ï¼Œæˆ‘ä»¬ä¾èµ–äºCTé‡å»ºè¿‡ç¨‹ä¸­ä½¿ç”¨çš„åº•å±‚ç‰©ç†åŸç†ï¼Œå¹¶å°†å…¶åº”ç”¨äºè‚è„è¡€ç®¡åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‰kæœ€å¤§å¼ºåº¦æŠ•å½±çš„æ¦‚å¿µï¼Œé€šè¿‡ä¿ç•™æ¯ä¸ªæŠ•å½±æ–¹å‘ä¸Šçš„å‰kä¸ªæœ€å¤§å€¼æ¥æ¨¡æ‹ŸCTé‡å»ºè¿‡ç¨‹ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›å‰kæœ€å¤§æŠ•å½±æ¥è®¾å®šæ‰©æ•£æ¨¡å‹å¹¶ç”Ÿæˆä¸‰ç»´è‚è„è¡€ç®¡æ ‘ã€‚æˆ‘ä»¬åœ¨ä¸‰ç»´è‚è„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°3D-ircadb-01ï¼Œä¸å…ˆå‰çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬å–å¾—äº†æœ€é«˜çš„Diceç³»æ•°ã€äº¤å¹¶æ¯”ï¼ˆIoUï¼‰å’Œæ•æ„Ÿæ€§å¾—åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03367v1">PDF</a> Accepted in 2025 IEEE International Symposium on Biomedical Imaging   (ISBI 2025)</p>
<p><strong>Summary</strong></p>
<p>è‚è„è¡€ç®¡åˆ†å‰²æ˜¯è‚è„åˆ‡é™¤æœ¯å‰è§„åˆ’çš„é‡è¦ä»»åŠ¡ã€‚å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨äºŒç»´CTæ¨ªæˆªé¢ä¸Šçš„è‚è„è¡€ç®¡åˆ†å‰²ï¼Œå¿½ç•¥äº†å…¨å±€è¡€ç®¡æ‹“æ‰‘ç»“æ„ã€‚æœ¬ç ”ç©¶åˆ©ç”¨CTé‡å»ºè¿‡ç¨‹ä¸­çš„ç‰©ç†åŸç†ï¼Œå¼•å…¥top-kæœ€å¤§å¼ºåº¦æŠ•å½±çš„æ¦‚å¿µï¼Œç”Ÿæˆä¸‰ç»´è‚è„è¡€ç®¡æ ‘ã€‚åœ¨3D-ircadb-01æ•°æ®é›†ä¸Šè¯„ä¼°è¯¥ä¸‰ç»´è‚è„è¡€ç®¡åˆ†å‰²æ–¹æ³•ï¼Œè¾ƒå‰æœŸå·¥ä½œæœ‰æ›´é«˜çš„Diceç³»æ•°ã€äº¤å¹¶æ¯”ï¼ˆIoUï¼‰å’Œæ•æ„Ÿæ€§å¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚è„è¡€ç®¡åˆ†å‰²æ˜¯è‚è„åˆ‡é™¤æœ¯å‰è§„åˆ’çš„é‡è¦éƒ¨åˆ†ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨äºŒç»´CTæ¨ªæˆªé¢ä¸Šçš„è‚è„è¡€ç®¡åˆ†å‰²ï¼Œå¿½ç•¥äº†å…¨å±€è¡€ç®¡æ‹“æ‰‘ç»“æ„çš„å½±å“ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨CTé‡å»ºçš„ç‰©ç†åŸç†æ¥ç»´æŠ¤å…¨å±€è¡€ç®¡æ‹“æ‰‘ç»“æ„ã€‚</li>
<li>å¼•å…¥top-kæœ€å¤§å¼ºåº¦æŠ•å½±çš„æ¦‚å¿µï¼Œæ¨¡ä»¿CTé‡å»ºè¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡top-kæœ€å¤§å¼ºåº¦æŠ•å½±æ¥è°ƒæ§æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆä¸‰ç»´è‚è„è¡€ç®¡æ ‘ã€‚</li>
<li>åœ¨3D-ircadb-01æ•°æ®é›†ä¸Šè¯„ä¼°è¯¥ä¸‰ç»´è‚è„è¡€ç®¡åˆ†å‰²æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-104c55ee80957c4a28eed0849f95466c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f571ba2c58bd210436ce3c345ed24dce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-247a1b522cf2fdf73b121e58c46c4f2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8e48530ffc84de4b3478595c4b238d9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TopoMortar-A-dataset-to-evaluate-image-segmentation-methods-focused-on-topology-accuracy"><a href="#TopoMortar-A-dataset-to-evaluate-image-segmentation-methods-focused-on-topology-accuracy" class="headerlink" title="TopoMortar: A dataset to evaluate image segmentation methods focused on   topology accuracy"></a>TopoMortar: A dataset to evaluate image segmentation methods focused on   topology accuracy</h2><p><strong>Authors:Juan Miguel Valverde, Motoya Koga, Nijihiko Otsuka, Anders Bjorholm Dahl</strong></p>
<p>We present TopoMortar, a brick wall dataset that is the first dataset specifically designed to evaluate topology-focused image segmentation methods, such as topology loss functions. TopoMortar enables to investigate in two ways whether methods incorporate prior topological knowledge. First, by eliminating challenges seen in real-world data, such as small training set, noisy labels, and out-of-distribution test-set images, that, as we show, impact the effectiveness of topology losses. Second, by allowing to assess in the same dataset topology accuracy across dataset challenges, isolating dataset-related effects from the effect of incorporating prior topological knowledge. In these two experiments, it is deliberately difficult to improve topology accuracy without actually using topology information, thus, permitting to attribute an improvement in topology accuracy to the incorporation of prior topological knowledge. To this end, TopoMortar includes three types of labels (accurate, noisy, pseudo-labels), two fixed training sets (large and small), and in-distribution and out-of-distribution test-set images. We compared eight loss functions on TopoMortar, and we found that clDice achieved the most topologically accurate segmentations, Skeleton Recall loss performed best particularly with noisy labels, and the relative advantageousness of the other loss functions depended on the experimental setting. Additionally, we show that simple methods, such as data augmentation and self-distillation, can elevate Cross entropy Dice loss to surpass most topology loss functions, and that those simple methods can enhance topology loss functions as well. clDice and Skeleton Recall loss, both skeletonization-based loss functions, were also the fastest to train, making this type of loss function a promising research direction. TopoMortar and our code can be found at <a target="_blank" rel="noopener" href="https://github.com/jmlipman/TopoMortar">https://github.com/jmlipman/TopoMortar</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†TopoMortarï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ç”¨äºè¯„ä¼°ä»¥æ‹“æ‰‘ä¸ºé‡ç‚¹çš„å›¾åƒåˆ†å‰²æ–¹æ³•ï¼ˆå¦‚æ‹“æ‰‘æŸå¤±å‡½æ•°ï¼‰çš„ç –å¢™æ•°æ®é›†ã€‚TopoMortarèƒ½å¤Ÿé€šè¿‡ä¸¤ç§æ–¹å¼ç ”ç©¶æ–¹æ³•æ˜¯å¦èå…¥äº†å…ˆéªŒæ‹“æ‰‘çŸ¥è¯†ã€‚é¦–å…ˆï¼Œé€šè¿‡æ¶ˆé™¤çœŸå®æ•°æ®ä¸­çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚å°è®­ç»ƒé›†ã€æ ‡ç­¾å™ªå£°å’Œè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æµ‹è¯•é›†å›¾åƒï¼Œè¿™äº›æŒ‘æˆ˜å¦‚æˆ‘ä»¬æ‰€å±•ç¤ºçš„ï¼Œä¼šå½±å“æ‹“æ‰‘æŸå¤±çš„æœ‰æ•ˆæ€§ã€‚å…¶æ¬¡ï¼Œå®ƒå…è®¸åœ¨åŒä¸€æ•°æ®é›†ä¸­è¯„ä¼°ä¸åŒæ•°æ®é›†æŒ‘æˆ˜ä¸‹çš„æ‹“æ‰‘ç²¾åº¦ï¼Œä»è€ŒåŒºåˆ†æ•°æ®é›†ç›¸å…³çš„å½±å“ä¸èå…¥å…ˆéªŒæ‹“æ‰‘çŸ¥è¯†çš„å½±å“ã€‚åœ¨è¿™ä¸¤é¡¹å®éªŒä¸­ï¼Œå¦‚æœä¸å®é™…ä½¿ç”¨æ‹“æ‰‘ä¿¡æ¯ï¼Œå¾ˆéš¾æé«˜æ‹“æ‰‘ç²¾åº¦ï¼Œå› æ­¤ï¼Œå¯ä»¥å°†æ‹“æ‰‘ç²¾åº¦çš„æé«˜å½’å› äºèå…¥çš„å…ˆéªŒæ‹“æ‰‘çŸ¥è¯†ã€‚ä¸ºæ­¤ï¼ŒTopoMortaråŒ…æ‹¬ä¸‰ç§æ ‡ç­¾ï¼ˆå‡†ç¡®ã€å™ªå£°ã€ä¼ªæ ‡ç­¾ï¼‰ã€ä¸¤ä¸ªå›ºå®šè®­ç»ƒé›†ï¼ˆå¤§å‹å’Œå°å‹ï¼‰ä»¥åŠç¬¦åˆåˆ†å¸ƒå’Œè¶…å‡ºåˆ†å¸ƒçš„æµ‹è¯•é›†å›¾åƒã€‚æˆ‘ä»¬åœ¨TopoMortarä¸Šæ¯”è¾ƒäº†å…«ç§æŸå¤±å‡½æ•°ï¼Œå‘ç°clDiceè·å¾—äº†æœ€å‡†ç¡®çš„æ‹“æ‰‘åˆ†å‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¸¦æœ‰å™ªå£°æ ‡ç­¾çš„æƒ…å†µä¸‹Skeleton RecallæŸå¤±è¡¨ç°æœ€ä½³ï¼Œå…¶ä»–æŸå¤±å‡½æ•°çš„ç›¸å¯¹ä¼˜åŠ¿å–å†³äºå®éªŒè®¾ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ä¸€äº›ç®€å•çš„æ–¹æ³•ï¼ˆå¦‚æ•°æ®å¢å¼ºå’Œè‡ªæˆ‘è’¸é¦ï¼‰å¯ä»¥å°†Cross entropy DiceæŸå¤±æå‡åˆ°è¶…è¶Šå¤§å¤šæ•°æ‹“æ‰‘æŸå¤±å‡½æ•°ï¼Œå¹¶ä¸”è¿™äº›ç®€å•çš„æ–¹æ³•è¿˜å¯ä»¥å¢å¼ºæ‹“æ‰‘æŸå¤±å‡½æ•°çš„æ•ˆæœã€‚clDiceå’ŒSkeleton RecallæŸå¤±éƒ½æ˜¯åŸºäºéª¨æ¶åŒ–çš„æŸå¤±å‡½æ•°ï¼Œä¹Ÿæ˜¯è®­ç»ƒé€Ÿåº¦æœ€å¿«çš„ï¼Œè¿™ä½¿å¾—è¿™ç§ç±»å‹çš„æŸå¤±å‡½æ•°æˆä¸ºä¸€ä¸ªæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚TopoMortarå’Œæˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/jmlipman/TopoMortar%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jmlipman/TopoMortaræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03365v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    TopoMortaræ•°æ®é›†ä¸“ä¸ºè¯„ä¼°ä»¥æ‹“æ‰‘ä¸ºä¸­å¿ƒçš„å›¾åƒåˆ†å‰²æ–¹æ³•è€Œè®¾è®¡ï¼Œå¦‚æ‹“æ‰‘æŸå¤±å‡½æ•°ã€‚è¯¥æ•°æ®é›†é€šè¿‡æ¶ˆé™¤çœŸå®æ•°æ®ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚å°è®­ç»ƒé›†ã€æ ‡ç­¾å™ªå£°å’Œè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æµ‹è¯•é›†å›¾åƒï¼Œæ¥æ¢ç©¶æ–¹æ³•æ˜¯å¦èå…¥äº†å…ˆéªŒæ‹“æ‰‘çŸ¥è¯†ã€‚åŒæ—¶ï¼Œå®ƒå…è®¸åœ¨åŒä¸€æ•°æ®é›†ä¸­è¯„ä¼°æ‹“æ‰‘ç²¾åº¦ï¼Œä»¥éš”ç¦»æ•°æ®é›†ç›¸å…³æ•ˆåº”ä¸èå…¥å…ˆéªŒæ‹“æ‰‘çŸ¥è¯†çš„å½±å“ã€‚ä¸ºæ­¤ï¼ŒTopoMortaråŒ…å«ä¸‰ç§æ ‡ç­¾ã€ä¸¤ä¸ªå›ºå®šè®­ç»ƒé›†ä»¥åŠç¬¦åˆå’Œè¶…å‡ºåˆ†å¸ƒçš„æµ‹è¯•é›†å›¾åƒã€‚åœ¨TopoMortarä¸Šæ¯”è¾ƒäº†å…«ç§æŸå¤±å‡½æ•°ï¼Œå‘ç°clDiceå®ç°æœ€æ‹“æ‰‘å‡†ç¡®çš„åˆ†å‰²ï¼Œç‰¹åˆ«æ˜¯å¸¦æœ‰å™ªå£°æ ‡ç­¾æ—¶Skeleton RecallæŸå¤±è¡¨ç°æœ€ä½³ï¼Œå…¶ä»–æŸå¤±å‡½æ•°çš„ä¼˜åŠ¿å–å†³äºå®éªŒè®¾ç½®ã€‚æ­¤å¤–ï¼Œç®€å•çš„æ–¹æ³•å¦‚æ•°æ®å¢å¼ºå’Œè‡ªæˆ‘è’¸é¦å¯ä½¿Cross entropy DiceæŸå¤±è¶…è¶Šå¤§å¤šæ•°æ‹“æ‰‘æŸå¤±å‡½æ•°ï¼Œè¿™äº›æ–¹æ³•ä¹Ÿèƒ½å¢å¼ºæ‹“æ‰‘æŸå¤±å‡½æ•°çš„æ•ˆæœã€‚clDiceå’ŒSkeleton RecallæŸå¤±æ˜¯åŸºäºéª¨æ¶åŒ–çš„æŸå¤±å‡½æ•°ï¼Œè®­ç»ƒé€Ÿåº¦æœ€å¿«ï¼Œæˆä¸ºæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>TopoMortaræ•°æ®é›†ä¸“ä¸ºè¯„ä¼°æ‹“æ‰‘èšç„¦çš„å›¾åƒåˆ†å‰²æ–¹æ³•è€Œè®¾è®¡ï¼Œä¾‹å¦‚æ‹“æ‰‘æŸå¤±å‡½æ•°ã€‚</li>
<li>è¯¥æ•°æ®é›†æ¶ˆé™¤äº†çœŸå®æ•°æ®ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚å°è®­ç»ƒé›†ã€æ ‡ç­¾å™ªå£°å’Œè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æµ‹è¯•é›†å›¾åƒã€‚</li>
<li>TopoMortarå…è®¸åœ¨åŒä¸€æ•°æ®é›†ä¸­è¯„ä¼°æ‹“æ‰‘ç²¾åº¦ï¼ŒåŒºåˆ†æ•°æ®é›†ç›¸å…³æ•ˆåº”ä¸èå…¥å…ˆéªŒæ‹“æ‰‘çŸ¥è¯†çš„å½±å“ã€‚</li>
<li>åœ¨æ­¤æ•°æ®é›†ä¸­æ¯”è¾ƒäº†å…«ç§æŸå¤±å‡½æ•°ï¼Œå‘ç°clDiceå®ç°æœ€æ‹“æ‰‘å‡†ç¡®çš„åˆ†å‰²ã€‚</li>
<li>åœ¨å¤„ç†å¸¦æœ‰å™ªå£°çš„æ ‡ç­¾æ—¶ï¼ŒSkeleton RecallæŸå¤±è¡¨ç°æœ€ä½³ã€‚</li>
<li>ç®€å•çš„æ–¹æ³•ï¼ˆå¦‚æ•°æ®å¢å¼ºå’Œè‡ªæˆ‘è’¸é¦ï¼‰èƒ½æ˜¾è‘—æé«˜æŸå¤±å‡½æ•°çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åŸºäºéª¨æ¶åŒ–çš„æŸå¤±å‡½æ•°è®­ç»ƒé€Ÿåº¦éå¸¸å¿«ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2641f851ca0a676bdaee0927f928801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50fd8f7bb93c9a828ff46af3254ce86e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f02ba5d08485166d150bf52e944583d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92ad5c7967d931956233aa84866cb801.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-Based-Diffusion-MRI-Tractography-Integrating-Spatial-and-Anatomical-Information"><a href="#Deep-Learning-Based-Diffusion-MRI-Tractography-Integrating-Spatial-and-Anatomical-Information" class="headerlink" title="Deep Learning-Based Diffusion MRI Tractography: Integrating Spatial and   Anatomical Information"></a>Deep Learning-Based Diffusion MRI Tractography: Integrating Spatial and   Anatomical Information</h2><p><strong>Authors:Yiqiong Yang, Yitian Yuan, Baoxing Ren, Ye Wu, Yanqiu Feng, Xinyuan Zhang</strong></p>
<p>Diffusion MRI tractography technique enables non-invasive visualization of the white matter pathways in the brain. It plays a crucial role in neuroscience and clinical fields by facilitating the study of brain connectivity and neurological disorders. However, the accuracy of reconstructed tractograms has been a longstanding challenge. Recently, deep learning methods have been applied to improve tractograms for better white matter coverage, but often comes at the expense of generating excessive false-positive connections. This is largely due to their reliance on local information to predict long range streamlines. To improve the accuracy of streamline propagation predictions, we introduce a novel deep learning framework that integrates image-domain spatial information and anatomical information along tracts, with the former extracted through convolutional layers and the later modeled via a Transformer-decoder. Additionally, we employ a weighted loss function to address fiber class imbalance encountered during training. We evaluate the proposed method on the simulated ISMRM 2015 Tractography Challenge dataset, achieving a valid streamline rate of 66.2%, white matter coverage of 63.8%, and successfully reconstructing 24 out of 25 bundles. Furthermore, on the multi-site Tractoinferno dataset, the proposed method demonstrates its ability to handle various diffusion MRI acquisition schemes, achieving a 5.7% increase in white matter coverage and a 4.1% decrease in overreach compared to RNN-based methods. </p>
<blockquote>
<p>æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çº¤ç»´è¿½è¸ªæŠ€æœ¯èƒ½å¤Ÿå®ç°å¤§è„‘ç™½è´¨é€šè·¯æ— åˆ›å¯è§†åŒ–ã€‚è¯¥æŠ€æœ¯å¯¹äºç¥ç»ç§‘å­¦å’Œä¸´åºŠé¢†åŸŸè‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿä¿ƒè¿›è„‘è¿æ¥å’Œç¥ç»éšœç¢çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œé‡å»ºçº¤ç»´è¿½è¸ªå›¾çš„å‡†ç¡®æ€§ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•è¢«åº”ç”¨äºæé«˜çº¤ç»´è¿½è¸ªå›¾çš„è¦†ç›–èŒƒå›´ï¼Œä½†å¾€å¾€ä¼šäº§ç”Ÿè¿‡å¤šçš„å‡é˜³æ€§è¿æ¥ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–å±€éƒ¨ä¿¡æ¯æ¥é¢„æµ‹é•¿è·ç¦»æµçº¿ã€‚ä¸ºäº†æé«˜æµçº¿ä¼ æ’­é¢„æµ‹çš„å‡†ç¡®åº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å›¾åƒåŸŸçš„ç©ºé—´ä¿¡æ¯å’Œæ²¿çº¤ç»´æŸçš„è§£å‰–ä¿¡æ¯ã€‚å…¶ä¸­ï¼Œå‰è€…é€šè¿‡å·ç§¯å±‚æå–ï¼Œåè€…é€šè¿‡Transformerè§£ç å™¨å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨åŠ æƒæŸå¤±å‡½æ•°æ¥è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°çš„çº¤ç»´ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿçš„ISMRM 2015å¹´çº¤ç»´è¿½è¸ªæŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œå®ç°äº†æœ‰æ•ˆçš„æµçº¿ç‡ä¸º66.2%ï¼Œç™½è´¨è¦†ç›–ç‡ä¸º63.8%ï¼ŒæˆåŠŸé‡å»ºäº†24ä¸ªçº¤ç»´æŸä¸­çš„25ä¸ªã€‚åœ¨å¤šç«™ç‚¹Tractoinfernoæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº†å…¶å¤„ç†å„ç§æ‰©æ•£MRIé‡‡é›†æ–¹æ¡ˆçš„èƒ½åŠ›ï¼Œä¸åŸºäºRNNçš„æ–¹æ³•ç›¸æ¯”ï¼Œç™½è´¨è¦†ç›–ç‡æé«˜äº†5.7%ï¼Œè¶…å‡ºç‡é™ä½äº†4.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03329v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆå›¾åƒåŸŸç©ºé—´ä¿¡æ¯å’Œè§£å‰–ä¿¡æ¯çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ”¹å–„æ‰©æ•£MRIè¿½è¸ªå›¾çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å·ç§¯å±‚å’ŒTransformerè§£ç å™¨è¿›è¡Œå»ºæ¨¡ï¼Œä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­çš„çº¤ç»´ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨æ¨¡æ‹Ÿçš„ISMRM 2015è¿½è¸ªå›¾æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ‰æ•ˆæµçº¿ç‡ä¸º66.2%ï¼Œç™½è´¨è¦†ç›–ç‡ä¸º63.8%ï¼ŒæˆåŠŸé‡å»ºäº†24ä¸ªæŸä¸­çš„25ä¸ªã€‚åœ¨å¤šç«™ç‚¹Tractoinfernoæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å¯å¤„ç†å„ç§æ‰©æ•£MRIé‡‡é›†æ–¹æ¡ˆï¼Œä¸RNNæ–¹æ³•ç›¸æ¯”ï¼Œç™½è´¨è¦†ç›–ç‡æé«˜äº†5.7%ï¼Œè¿‡åº¦åˆ°è¾¾ç‡é™ä½äº†4.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£MRIè¿½è¸ªæŠ€æœ¯å¯å®ç°è„‘å†…ç™½è´¨é€šè·¯çš„æ— åˆ›å¯è§†åŒ–ã€‚</li>
<li>åœ¨ç¥ç»ç§‘å­¦å’Œä¸´åºŠé¢†åŸŸï¼Œè¯¥æŠ€æœ¯å¯¹äºç ”ç©¶è„‘è¿æ¥å’Œç¥ç»ç–¾ç—…è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å·²åº”ç”¨äºæ”¹è¿›è¿½è¸ªå›¾ä»¥æé«˜ç™½è´¨è¦†ç›–ç‡ï¼Œä½†å­˜åœ¨ç”Ÿæˆè¿‡å¤šå‡é˜³æ€§è¿æ¥çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ç»“åˆäº†å›¾åƒåŸŸç©ºé—´ä¿¡æ¯å’Œè§£å‰–ä¿¡æ¯ï¼Œä»¥æé«˜æµçº¿ä¼ æ’­é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨å·ç§¯å±‚å’ŒTransformerè§£ç å™¨è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é‡‡ç”¨äº†åŠ æƒæŸå¤±å‡½æ•°æ¥è§£å†³çº¤ç»´ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•çš„æµçº¿ç‡å’Œç™½è´¨è¦†ç›–ç‡è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f00f4f249a8a72941cfc02e1562c85e9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Interactive-Segmentation-and-Report-Generation-for-CT-Images"><a href="#Interactive-Segmentation-and-Report-Generation-for-CT-Images" class="headerlink" title="Interactive Segmentation and Report Generation for CT Images"></a>Interactive Segmentation and Report Generation for CT Images</h2><p><strong>Authors:Yannian Gu, Wenhui Lei, Hanyu Chen, Xiaofan Zhang, Shaoting Zhang</strong></p>
<p>Automated CT report generation plays a crucial role in improving diagnostic accuracy and clinical workflow efficiency. However, existing methods lack interpretability and impede patient-clinician understanding, while their static nature restricts radiologists from dynamically adjusting assessments during image review. Inspired by interactive segmentation techniques, we propose a novel interactive framework for 3D lesion morphology reporting that seamlessly generates segmentation masks with comprehensive attribute descriptions, enabling clinicians to generate detailed lesion profiles for enhanced diagnostic assessment. To our best knowledge, we are the first to integrate the interactive segmentation and structured reports in 3D CT medical images. Experimental results across 15 lesion types demonstrate the effectiveness of our approach in providing a more comprehensive and reliable reporting system for lesion segmentation and capturing. The source code will be made publicly available following paper acceptance. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–CTæŠ¥å‘Šç”Ÿæˆåœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠå·¥ä½œæµç¨‹æ•ˆç‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹è§£é‡Šæ€§ï¼Œé˜»ç¢åŒ»æ‚£ä¹‹é—´çš„ç†è§£ï¼Œè€Œå…¶é™æ€æ€§è´¨é™åˆ¶äº†æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨å›¾åƒå®¡æŸ¥è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´è¯„ä¼°çš„èƒ½åŠ›ã€‚å—äº¤äº’å¼åˆ†å‰²æŠ€æœ¯çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äº3Dç—…å˜å½¢æ€æŠ¥å‘Šçš„æ–°å‹äº¤äº’å¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ— ç¼ç”Ÿæˆå¸¦æœ‰ç»¼åˆå±æ€§æè¿°çš„åˆ†å‰²æ©æ¨¡ï¼Œä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„ç—…å˜ç‰¹å¾ï¼Œä»è€Œæé«˜è¯Šæ–­è¯„ä¼°æ°´å¹³ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡å°†äº¤äº’å¼åˆ†å‰²å’Œç»“æ„åŒ–æŠ¥å‘Šæ•´åˆåˆ°3D CTåŒ»å­¦å›¾åƒä¸­ã€‚è·¨è¶Š15ç§ç—…å˜ç±»å‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æä¾›ç—…å˜åˆ†å‰²å’Œæ•è·çš„æ›´å…¨é¢ã€å¯é çš„æŠ¥å‘Šç³»ç»Ÿæ–¹é¢éå¸¸æœ‰æ•ˆã€‚è®ºæ–‡è¢«æ¥å—åï¼Œæºä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03294v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨CTå½±åƒè¯Šæ–­ä¸­ï¼Œè‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆåœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠå·¥ä½œæ•ˆç‡æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œé˜»ç¢åŒ»æ‚£æ²Ÿé€šï¼ŒåŒæ—¶é™æ€çš„ç‰¹æ€§é™åˆ¶äº†æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨æŸ¥çœ‹å›¾åƒæ—¶çš„åŠ¨æ€è¯„ä¼°èƒ½åŠ›ã€‚å—äº¤äº’å¼åˆ†å‰²æŠ€æœ¯çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„äº¤äº’å¼æ¡†æ¶ï¼Œç”¨äºç”ŸæˆåŒ…å«ç»¼åˆå±æ€§æè¿°çš„åˆ†å‰²æ©è†œï¼Œä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„ç—…ç¶å›¾è°±ï¼Œæé«˜è¯Šæ–­è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡å°†äº¤äº’å¼åˆ†å‰²å’Œç»“æ„åŒ–æŠ¥å‘Šæ•´åˆåˆ°ä¸‰ç»´CTåŒ»å­¦å›¾åƒä¸­ã€‚è·¨è¶Šåäº”ç§ç—…ç¶ç±»å‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†æ›´å…¨é¢å¯é çš„ç—…ç¶åˆ†å‰²æŠ¥å‘Šç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–CTæŠ¥å‘Šç”Ÿæˆåœ¨å½±åƒè¯Šæ–­ä¸­èƒ½æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œä¸´åºŠå·¥ä½œæ•ˆç‡ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œé˜»ç¢äº†åŒ»æ‚£æ²Ÿé€šã€‚</li>
<li>äº¤äº’å¼åˆ†å‰²æŠ€æœ¯ä¸ºæ”¹è¿›ç°æœ‰æ–¹æ³•æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„äº¤äº’å¼æ¡†æ¶ï¼Œç”¨äºç”ŸæˆåŒ…å«ç»¼åˆå±æ€§æè¿°çš„åˆ†å‰²æ©è†œã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„ç—…ç¶å›¾è°±ï¼Œæé«˜è¯Šæ–­è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
<li>é¦–æ¬¡å°†äº¤äº’å¼åˆ†å‰²å’Œç»“æ„åŒ–æŠ¥å‘Šæ•´åˆåˆ°ä¸‰ç»´CTåŒ»å­¦å›¾åƒä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5897ac05fbe37ae49f74c129a581beea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6648b945257a926889eb107c3e636c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56fc74e0df9566a0a037ae60251f0829.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UnPuzzle-A-Unified-Framework-for-Pathology-Image-Analysis"><a href="#UnPuzzle-A-Unified-Framework-for-Pathology-Image-Analysis" class="headerlink" title="UnPuzzle: A Unified Framework for Pathology Image Analysis"></a>UnPuzzle: A Unified Framework for Pathology Image Analysis</h2><p><strong>Authors:Dankai Liao, Sicheng Chen, Nuwa Xi, Qiaochu Xue, Jieyu Li, Lingxuan Hou, Zeyu Liu, Chang Han Low, Yufeng Wu, Yiling Liu, Yanqin Jiang, Dandan Li, Yueming Jin, Shangqing Lyu</strong></p>
<p>Pathology image analysis plays a pivotal role in medical diagnosis, with deep learning techniques significantly advancing diagnostic accuracy and research. While numerous studies have been conducted to address specific pathological tasks, the lack of standardization in pre-processing methods and model&#x2F;database architectures complicates fair comparisons across different approaches. This highlights the need for a unified pipeline and comprehensive benchmarks to enable consistent evaluation and accelerate research progress. In this paper, we present UnPuzzle, a novel and unified framework for pathological AI research that covers a broad range of pathology tasks with benchmark results. From high-level to low-level, upstream to downstream tasks, UnPuzzle offers a modular pipeline that encompasses data pre-processing, model composition,taskconfiguration,andexperimentconduction.Specifically, it facilitates efficient benchmarking for both Whole Slide Images (WSIs) and Region of Interest (ROI) tasks. Moreover, the framework supports variouslearningparadigms,includingself-supervisedlearning,multi-task learning,andmulti-modallearning,enablingcomprehensivedevelopment of pathology AI models. Through extensive benchmarking across multiple datasets, we demonstrate the effectiveness of UnPuzzle in streamlining pathology AI research and promoting reproducibility. We envision UnPuzzle as a cornerstone for future advancements in pathology AI, providing a more accessible, transparent, and standardized approach to model evaluation. The UnPuzzle repository is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Puzzle-AI/UnPuzzle">https://github.com/Puzzle-AI/UnPuzzle</a>. </p>
<blockquote>
<p>ç—…ç†å­¦å›¾åƒåˆ†æåœ¨åŒ»å­¦è¯Šæ–­ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯æ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œç ”ç©¶æ°´å¹³ã€‚è™½ç„¶å·²è¿›è¡Œäº†å¤§é‡ç ”ç©¶æ¥è§£å†³ç‰¹å®šçš„ç—…ç†ä»»åŠ¡ï¼Œä½†åœ¨é¢„å¤„ç†æ–¹æ³•å’Œæ¨¡å‹&#x2F;æ•°æ®åº“æ¶æ„æ–¹é¢ç¼ºä¹æ ‡å‡†åŒ–ï¼Œè¿™ä½¿å¾—ä¸åŒçš„æ–¹æ³•ä¹‹é—´éš¾ä»¥è¿›è¡Œå…¬å¹³çš„æ¯”è¾ƒã€‚è¿™å¼ºè°ƒäº†éœ€è¦ä¸€ä¸ªç»Ÿä¸€çš„æµç¨‹å’Œç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œä»¥å®ç°ä¸€è‡´çš„è¯„ä»·å¹¶åŠ é€Ÿç ”ç©¶è¿›å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UnPuzzleï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç—…ç†äººå·¥æ™ºèƒ½ç ”ç©¶çš„æ–°å‹ç»Ÿä¸€æ¡†æ¶ï¼Œæ¶µç›–å¹¿æ³›çš„ç—…ç†ä»»åŠ¡å¹¶æä¾›åŸºå‡†æµ‹è¯•ç»“æœã€‚ä»é«˜çº§åˆ°ä½çº§ï¼Œä»ä¸Šæ¸¸åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼ŒUnPuzzleæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹ç»„åˆã€ä»»åŠ¡é…ç½®å’Œå®éªŒæ‰§è¡Œã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä¾¿äºå¯¹æ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰å’Œæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ä»»åŠ¡è¿›è¡Œé«˜æ•ˆåŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒå¤šç§å­¦ä¹ èŒƒå¼ï¼ŒåŒ…æ‹¬è‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ å’Œå¤šæ¨¡å¼å­¦ä¹ ï¼Œä»è€Œå®ç°ç—…ç†äººå·¥æ™ºèƒ½æ¨¡å‹çš„å…¨é¢å‘å±•ã€‚é€šè¿‡å¤šä¸ªæ•°æ®é›†çš„å¤§é‡åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯æ˜äº†UnPuzzleåœ¨ç®€åŒ–ç—…ç†äººå·¥æ™ºèƒ½ç ”ç©¶å¹¶ä¿ƒè¿›å¯é‡å¤æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æœŸæœ›UnPuzzleèƒ½æˆä¸ºæœªæ¥ç—…ç†äººå·¥æ™ºèƒ½å‘å±•çš„åŸºçŸ³ï¼Œæä¾›ä¸€ç§æ›´å¯è®¿é—®ã€é€æ˜å’Œæ ‡å‡†åŒ–çš„æ¨¡å‹è¯„ä¼°æ–¹æ³•ã€‚UnPuzzleä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Puzzle-AI/UnPuzzle">https://github.com/Puzzle-AI/UnPuzzle</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03152v1">PDF</a> 11 pages,2 figures</p>
<p><strong>Summary</strong><br>     ç—…ç†å­¦å›¾åƒåˆ†æåœ¨åŒ»å­¦è¯Šæ–­ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯æ˜¾è‘—æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œç ”ç©¶æ°´å¹³ã€‚å½“å‰ç¼ºä¹æ ‡å‡†åŒ–é¢„å¤„ç†æ–¹æ³•å’Œæ¨¡å‹&#x2F;æ•°æ®åº“æ¶æ„ï¼Œå¯¼è‡´ä¸åŒæ–¹æ³•ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒå˜å¾—å¤æ‚ã€‚æœ¬æ–‡æå‡ºäº†UnPuzzleï¼Œä¸€ä¸ªç”¨äºç—…ç†å­¦äººå·¥æ™ºèƒ½ç ”ç©¶çš„æ–°å‹ç»Ÿä¸€æ¡†æ¶ï¼Œæ¶µç›–å¹¿æ³›çš„ä»»åŠ¡ï¼Œå¹¶æä¾›æ¨¡å—åŒ–ç®¡é“ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€æ¨¡å‹æ„å»ºã€ä»»åŠ¡é…ç½®å’Œå®éªŒæ‰§è¡Œã€‚å®ƒæ”¯æŒå¤šç§å­¦ä¹ èŒƒå¼ï¼Œå¹¶é€šè¿‡è·¨å¤šä¸ªæ•°æ®é›†çš„å¤§é‡åŸºå‡†æµ‹è¯•è¯æ˜äº†å…¶åœ¨ç®€åŒ–ç—…ç†å­¦äººå·¥æ™ºèƒ½ç ”ç©¶å’Œä¿ƒè¿›å¯é‡å¤æ€§çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å­¦å›¾åƒåˆ†æåœ¨åŒ»å­¦è¯Šæ–­ä¸­èµ·å…³é”®ä½œç”¨ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œç ”ç©¶æ°´å¹³ã€‚</li>
<li>å½“å‰ç¼ºä¹ç»Ÿä¸€çš„é¢„å¤„ç†æ–¹æ³•å’Œæ¨¡å‹&#x2F;æ•°æ®åº“æ¶æ„ï¼Œéœ€è¦æ›´æ ‡å‡†åŒ–çš„ç ”ç©¶è¯„ä¼°æµç¨‹ã€‚</li>
<li>UnPuzzleæ˜¯ä¸€ä¸ªç”¨äºç—…ç†å­¦äººå·¥æ™ºèƒ½ç ”ç©¶çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ¶µç›–å¤šç§ä»»åŠ¡å¹¶æä¾›æ¨¡å—åŒ–ç®¡é“ã€‚</li>
<li>UnPuzzleæ”¯æŒå¤šç§å­¦ä¹ èŒƒå¼ï¼ŒåŒ…æ‹¬è‡ªç›‘ç£å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ å’Œå¤šæ¨¡æ€å­¦ä¹ ã€‚</li>
<li>UnPuzzleé€šè¿‡è·¨å¤šä¸ªæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>UnPuzzleæ—¨åœ¨æˆä¸ºç—…ç†å­¦äººå·¥æ™ºèƒ½æœªæ¥è¿›æ­¥çš„åŸºçŸ³ï¼Œæä¾›æ›´æ˜“äºè®¿é—®ã€é€æ˜å’Œæ ‡å‡†åŒ–çš„æ¨¡å‹è¯„ä¼°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-354dfd6d39a9d41570b08ecee6f23bf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e743da5759513c05a4a336103f14321a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Implicit-U-KAN2-0-Dynamic-Efficient-and-Interpretable-Medical-Image-Segmentation"><a href="#Implicit-U-KAN2-0-Dynamic-Efficient-and-Interpretable-Medical-Image-Segmentation" class="headerlink" title="Implicit U-KAN2.0: Dynamic, Efficient and Interpretable Medical Image   Segmentation"></a>Implicit U-KAN2.0: Dynamic, Efficient and Interpretable Medical Image   Segmentation</h2><p><strong>Authors:Chun-Wun Cheng, Yining Zhao, Yanqi Cheng, Javier Montoya, Carola-Bibiane SchÃ¶nlieb, Angelica I Aviles-Rivero</strong></p>
<p>Image segmentation is a fundamental task in both image analysis and medical applications. State-of-the-art methods predominantly rely on encoder-decoder architectures with a U-shaped design, commonly referred to as U-Net. Recent advancements integrating transformers and MLPs improve performance but still face key limitations, such as poor interpretability, difficulty handling intrinsic noise, and constrained expressiveness due to discrete layer structures, often lacking a solid theoretical foundation.In this work, we introduce Implicit U-KAN 2.0, a novel U-Net variant that adopts a two-phase encoder-decoder structure. In the SONO phase, we use a second-order neural ordinary differential equation (NODEs), called the SONO block, for a more efficient, expressive, and theoretically grounded modeling approach. In the SONO-MultiKAN phase, we integrate the second-order NODEs and MultiKAN layer as the core computational block to enhance interpretability and representation power. Our contributions are threefold. First, U-KAN 2.0 is an implicit deep neural network incorporating MultiKAN and second order NODEs, improving interpretability and performance while reducing computational costs. Second, we provide a theoretical analysis demonstrating that the approximation ability of the MultiKAN block is independent of the input dimension. Third, we conduct extensive experiments on a variety of 2D and a single 3D dataset, demonstrating that our model consistently outperforms existing segmentation networks. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²æ˜¯å›¾åƒåˆ†æå’ŒåŒ»å­¦åº”ç”¨ä¸­çš„åŸºæœ¬ä»»åŠ¡ã€‚æœ€å…ˆè¿›çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶ä¸”å…·æœ‰Uå½¢è®¾è®¡ï¼Œé€šå¸¸ç§°ä¸ºU-Netã€‚æœ€è¿‘èåˆäº†å˜å‹å™¨å’Œå¤šå±‚æ„ŸçŸ¥æœºçš„è¿›å±•æé«˜äº†æ€§èƒ½ï¼Œä½†ä»ç„¶é¢ä¸´å…³é”®å±€é™ï¼Œå¦‚è§£é‡Šæ€§å·®ã€å¤„ç†å†…åœ¨å™ªå£°å›°éš¾ã€ç”±äºç¦»æ•£å±‚ç»“æ„å¯¼è‡´çš„è¡¨è¾¾èƒ½åŠ›å—é™ï¼Œå¹¶ä¸”é€šå¸¸ç¼ºä¹åšå®ç†è®ºåŸºç¡€ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Implicit U-KAN 2.0ï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨ä¸¤é˜¶æ®µç¼–ç å™¨-è§£ç å™¨ç»“æ„çš„æ–°å‹U-Netå˜ä½“ã€‚åœ¨SONOé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ç§°ä¸ºSONOå—çš„äºŒé˜¶ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆNODEsï¼‰ï¼Œä»¥æ›´æœ‰æ•ˆã€æ›´å…·è¡¨ç°åŠ›å’Œç†è®ºåŸºç¡€çš„å»ºæ¨¡æ–¹æ³•ã€‚åœ¨SONO-MultiKANé˜¶æ®µï¼Œæˆ‘ä»¬å°†äºŒé˜¶NODEså’ŒMultiKANå±‚ä½œä¸ºæ ¸å¿ƒè®¡ç®—å—è¿›è¡Œé›†æˆï¼Œä»¥æé«˜å¯è§£é‡Šæ€§å’Œè¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸‰ç‚¹ã€‚é¦–å…ˆï¼ŒU-KAN 2.0æ˜¯ä¸€ä¸ªéšå¼æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç»“åˆäº†MultiKANå’ŒäºŒé˜¶NODEsï¼Œåœ¨æé«˜å¯è§£é‡Šæ€§å’Œæ€§èƒ½çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æä¾›äº†ç†è®ºåˆ†æï¼Œè¯æ˜MultiKANå—çš„é€¼è¿‘èƒ½åŠ›ä¸è¾“å…¥ç»´åº¦æ— å…³ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å¤šç§äºŒç»´å’Œå•ä¸ªä¸‰ç»´æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹å§‹ç»ˆä¼˜äºç°æœ‰åˆ†å‰²ç½‘ç»œã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03141v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬å·¥ä½œæå‡ºäº†Implicit U-KAN 2.0ï¼Œä¸€ç§æ–°å‹U-Netå˜ä½“ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µç¼–ç å™¨-è§£ç å™¨ç»“æ„ã€‚å¼•å…¥SONOå—å’ŒSONO-MultiKANé˜¶æ®µï¼Œæé«˜æ¨¡å‹æ•ˆç‡ã€è¡¨è¾¾åŠ›ã€å¯è§£é‡Šæ€§å’Œç†è®ºæ”¯æ’‘ã€‚åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰ç½‘ç»œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Implicit U-KAN 2.0æ˜¯ä¸€ç§æ–°å‹çš„U-Netå˜ä½“ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µç¼–ç å™¨-è§£ç å™¨ç»“æ„ã€‚</li>
<li>è¯¥æ¨¡å‹å¼•å…¥SONOå—å’ŒSONO-MultiKANé˜¶æ®µï¼Œä»¥æé«˜æ¨¡å‹æ•ˆç‡å’Œè¡¨è¾¾åŠ›ã€‚</li>
<li>SONOå—é‡‡ç”¨äºŒé˜¶ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆNODEsï¼‰ï¼Œæé«˜æ¨¡å‹çš„ç†è®ºæ”¯æ’‘ã€‚</li>
<li>MultiKANå—å…·æœ‰ç‹¬ç«‹è¾“å…¥ç»´åº¦çš„è¿‘ä¼¼èƒ½åŠ›ï¼Œå¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>åœ¨å¤šç§äºŒç»´å’Œå•ä¸€ä¸‰ç»´æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½ä¼˜äºç°æœ‰åˆ†å‰²ç½‘ç»œã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰è¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d2caf536d01e2d978d6f1b53d70acac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff31b8c293a9847af10bba02cef2b473.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Linking-quantum-mechanical-features-to-structural-phase-transformation-in-inorganic-solids"><a href="#Linking-quantum-mechanical-features-to-structural-phase-transformation-in-inorganic-solids" class="headerlink" title="Linking quantum mechanical features to structural phase-transformation   in inorganic solids"></a>Linking quantum mechanical features to structural phase-transformation   in inorganic solids</h2><p><strong>Authors:Prashant Singh, Anis Biswas, Alexander Thayer, Yaroslav Mudryk</strong></p>
<p>We present a new descriptor, i.e., local lattice distortion, to predict structural phase transformation in inorganic compounds containing lanthanides and transition metals. The descriptor utilizes local lattice and angular distortions obtained from structural optimization of experimentally known crystalline phases within state-of-the-art density-functional theory method. The predictive power of the descriptor was tested on lanthanide based RE2In (RE&#x3D;rare-earth) compounds known for a variety of phase transformations. We show that the inclusion of quantum-mechanical effects through local-charge, bonding, symmetry, and electronic-structure enhances the robustness of the descriptor in predicting structural phase transformation. To gain further insights, we analyzed phononic and electronic behavior of Y2In, and show that experimentally observed phase transformation can only be predicted when atomic strains are included. The descriptor was used to predict structural phase change in couple of new compounds, i.e., (Yb1-xErx)2In and Gd2(In1-xAlx), which was validated by X-ray powder diffraction measurements. Finally, we demonstrated the generality of the proposed descriptor by predicting phase transformation behavior in different classes of compounds indicating the usefulness of our approach in mapping desired phase changes in novel functional materials. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æè¿°ç¬¦ï¼Œå³å±€éƒ¨æ™¶æ ¼ç•¸å˜ï¼Œç”¨äºé¢„æµ‹å«ç¨€åœŸå…ƒç´ å’Œè¿‡æ¸¡é‡‘å±çš„æ— æœºåŒ–åˆç‰©çš„ç»“æ„ç›¸å˜ã€‚è¯¥æè¿°ç¬¦åˆ©ç”¨å±€éƒ¨æ™¶æ ¼å’Œè§’åº¦ç•¸å˜ï¼Œè¿™äº›ç•¸å˜æ˜¯é€šè¿‡æœ€æ–°å‘å±•çš„å¯†åº¦æ³›å‡½ç†è®ºæ–¹æ³•å¯¹å·²çŸ¥æ™¶ä½“ç»“æ„è¿›è¡Œä¼˜åŒ–è€Œè·å¾—çš„ã€‚è¯¥æè¿°ç¬¦çš„é¢„æµ‹èƒ½åŠ›åœ¨åŸºäºç¨€åœŸå…ƒç´ çš„RE2Inï¼ˆRE&#x3D;ç¨€åœŸå…ƒç´ ï¼‰åŒ–åˆç‰©ä¸Šå¾—åˆ°äº†æµ‹è¯•ï¼Œè¿™äº›åŒ–åˆç‰©å…·æœ‰å¤šç§ç›¸å˜ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡å±€éƒ¨ç”µè·ã€é”®åˆã€å¯¹ç§°æ€§å’Œç”µå­ç»“æ„èå…¥é‡å­åŠ›å­¦æ•ˆåº”ï¼Œå¢å¼ºäº†è¯¥æè¿°ç¬¦åœ¨é¢„æµ‹ç»“æ„ç›¸å˜æ–¹é¢çš„ç¨³å¥æ€§ã€‚ä¸ºäº†æ·±å…¥äº†è§£ï¼Œæˆ‘ä»¬åˆ†æäº†Y2Inçš„å£°å­¦å’Œç”µå­è¡Œä¸ºï¼Œå¹¶è¡¨æ˜åªæœ‰å½“åŒ…å«åŸå­åº”å˜æ—¶ï¼Œæ‰èƒ½é¢„æµ‹å®éªŒè§‚å¯Ÿåˆ°çš„ç›¸å˜ã€‚è¯¥æè¿°ç¬¦è¢«ç”¨äºé¢„æµ‹å‡ ç§æ–°åŒ–åˆç‰©çš„ç»“æ„ç›¸å˜ï¼Œå¦‚ï¼ˆYb1-xErxï¼‰2Inå’ŒGd2ï¼ˆIn1-xAlxï¼‰ï¼Œå¹¶é€šè¿‡Xå°„çº¿ç²‰æœ«è¡å°„æµ‹é‡éªŒè¯äº†å…¶é¢„æµ‹ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡é¢„æµ‹ä¸åŒç±»åˆ«åŒ–åˆç‰©çš„ç›¸å˜è¡Œä¸ºæ¥å±•ç¤ºæ‰€æå‡ºæè¿°ç¬¦çš„æ™®éæ€§ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨æ–°å‹åŠŸèƒ½ææ–™ä¸­æ˜ å°„æ‰€éœ€ç›¸å˜çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03120v1">PDF</a> 25 page, 8 figures, 78 references</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹æè¿°ç¬¦â€”â€”å±€éƒ¨æ™¶æ ¼ç•¸å˜è¢«æå‡ºæ¥é¢„æµ‹å«ç¨€åœŸå…ƒç´ å’Œè¿‡æ¸¡é‡‘å±çš„æ— æœºåŒ–åˆç‰©çš„ç»“æ„ç›¸å˜ã€‚è¯¥æè¿°ç¬¦åˆ©ç”¨å±€éƒ¨æ™¶æ ¼å’Œè§’åº¦ç•¸å˜ï¼Œé€šè¿‡å…ˆè¿›çš„å¯†åº¦æ³›å‡½ç†è®ºæ–¹æ³•å¯¹å·²çŸ¥æ™¶ä½“ç»“æ„è¿›è¡Œä¼˜åŒ–å¾—åˆ°ã€‚é€šè¿‡æµ‹è¯•åœ¨ç¨€åœŸå…ƒç´ REâ‚‚Inï¼ˆRE&#x3D;ç¨€åœŸå…ƒç´ ï¼‰åŒ–åˆç‰©ä¸­çš„é¢„æµ‹èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºè¯¥æè¿°ç¬¦åœ¨é¢„æµ‹ç»“æ„ç›¸å˜æ–¹é¢å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§ï¼Œå¹¶ä¸”é‡å­æœºæ¢°æ•ˆåº”åŒ…æ‹¬å±€éƒ¨ç”µè·ã€é”®åˆã€å¯¹ç§°å’Œç”µå­ç»“æ„çš„åŠ å…¥å¢å¼ºäº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å¯¹æ–°åŒ–åˆç‰©ï¼ˆYbÂ¹â‚“Erï¼‰â‚‚Inå’ŒGdâ‚‚ï¼ˆInÂ¹â‚“Alï¼‰è¿›è¡Œäº†é¢„æµ‹ï¼Œå¹¶é€šè¿‡Xå°„çº¿ç²‰æœ«è¡å°„æµ‹é‡éªŒè¯äº†é¢„æµ‹çš„ç›¸å˜è¡Œä¸ºã€‚æœ€ç»ˆè¯æ˜äº†è¯¥æè¿°ç¬¦çš„é€šç”¨æ€§ï¼Œåœ¨é¢„æµ‹ä¸åŒåŒ–åˆç‰©çš„ç›¸å˜è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹æè¿°ç¬¦â€”â€”å±€éƒ¨æ™¶æ ¼ç•¸å˜ï¼Œç”¨äºé¢„æµ‹æ— æœºåŒ–åˆç‰©çš„ç»“æ„ç›¸å˜ã€‚</li>
<li>æè¿°ç¬¦ç»“åˆäº†å…ˆè¿›çš„å¯†åº¦æ³›å‡½ç†è®ºæ–¹æ³•æ¥ä¼˜åŒ–å·²çŸ¥æ™¶ä½“ç»“æ„ï¼Œå¹¶åˆ©ç”¨å±€éƒ¨æ™¶æ ¼å’Œè§’åº¦ç•¸å˜è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>æè¿°ç¬¦åœ¨ç¨€åœŸå…ƒç´ REâ‚‚InåŒ–åˆç‰©ä¸­çš„é¢„æµ‹èƒ½åŠ›å¾—åˆ°äº†æµ‹è¯•éªŒè¯ã€‚</li>
<li>æè¿°ç¬¦åœ¨è€ƒè™‘é‡å­æœºæ¢°æ•ˆåº”ï¼ˆå¦‚å±€éƒ¨ç”µè·ã€é”®åˆã€å¯¹ç§°å’Œç”µå­ç»“æ„ï¼‰çš„æƒ…å†µä¸‹ï¼Œæ›´èƒ½å‡†ç¡®é¢„æµ‹ç»“æ„ç›¸å˜ã€‚</li>
<li>å¯¹æ–°åŒ–åˆç‰©ï¼ˆYbÂ¹â‚“Erï¼‰â‚‚Inå’ŒGdâ‚‚ï¼ˆInÂ¹â‚“Alï¼‰çš„ç›¸å˜è¡Œä¸ºè¿›è¡Œäº†é¢„æµ‹ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶å‡†ç¡®æ€§ã€‚</li>
<li>æè¿°ç¬¦å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿé¢„æµ‹ä¸åŒåŒ–åˆç‰©çš„ç›¸å˜è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03120">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23d640fabcc6049f158b6f08b6a49fea.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-Diffusion-Models-Provide-Rigorous-Uncertainty-Quantification-for-Bayesian-Inverse-Problems"><a href="#Can-Diffusion-Models-Provide-Rigorous-Uncertainty-Quantification-for-Bayesian-Inverse-Problems" class="headerlink" title="Can Diffusion Models Provide Rigorous Uncertainty Quantification for   Bayesian Inverse Problems?"></a>Can Diffusion Models Provide Rigorous Uncertainty Quantification for   Bayesian Inverse Problems?</h2><p><strong>Authors:Evan Scope Crafts, Umberto Villa</strong></p>
<p>In recent years, the ascendance of diffusion modeling as a state-of-the-art generative modeling approach has spurred significant interest in their use as priors in Bayesian inverse problems. However, it is unclear how to optimally integrate a diffusion model trained on the prior distribution with a given likelihood function to obtain posterior samples. While algorithms that have been developed for this purpose can produce high-quality, diverse point estimates of the unknown parameters of interest, they are often tested on problems where the prior distribution is analytically unknown, making it difficult to assess their performance in providing rigorous uncertainty quantification. In this work, we introduce a new framework, Bayesian Inverse Problem Solvers through Diffusion Annealing (BIPSDA), for diffusion model based posterior sampling. The framework unifies several recently proposed diffusion model based posterior sampling algorithms and contains novel algorithms that can be realized through flexible combinations of design choices. Algorithms within our framework were tested on model problems with a Gaussian mixture prior and likelihood functions inspired by problems in image inpainting, x-ray tomography, and phase retrieval. In this setting, approximate ground-truth posterior samples can be obtained, enabling principled evaluation of the performance of the algorithms. The results demonstrate that BIPSDA algorithms can provide strong performance on the image inpainting and x-ray tomography based problems, while the challenging phase retrieval problem, which is difficult to sample from even when the posterior density is known, remains outside the reach of the diffusion model based samplers. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§æœ€å…ˆè¿›çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•é€æ¸å…´èµ·ï¼Œè¿™æ¿€å‘äº†å¯¹å…¶åœ¨è´å¶æ–¯åé—®é¢˜ä¸­ä½œä¸ºå…ˆéªŒä½¿ç”¨çš„æå¤§å…´è¶£ã€‚ç„¶è€Œï¼Œå¯¹äºå¦‚ä½•å°†è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸ç»™å®šçš„ä¼¼ç„¶å‡½æ•°ç›¸ç»“åˆä»¥è·å¾—åéªŒæ ·æœ¬çš„æœ€ä¼˜æ–¹æ³•å°šä¸æ¸…æ¥šã€‚å°½ç®¡ä¸ºæ­¤ç›®çš„è€Œå¼€å‘çš„ç®—æ³•å¯ä»¥äº§ç”Ÿé«˜è´¨é‡çš„æœªçŸ¥å‚æ•°ä¼°è®¡å€¼ï¼Œä½†è¿™äº›ä¼°è®¡å€¼å¾€å¾€æ˜¯å¤šæ ·åŒ–çš„ã€‚ä½†è¿™äº›ç®—æ³•ç»å¸¸åœ¨å…ˆéªŒåˆ†å¸ƒåˆ†ææœªçŸ¥çš„é—®é¢˜ä¸Šæ¥å—æµ‹è¯•ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨æä¾›ä¸¥æ ¼çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹é¢çš„æ€§èƒ½éš¾ä»¥è¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ‰©æ•£é€€ç«çš„è´å¶æ–¯åé—®é¢˜æ±‚è§£å™¨ï¼ˆBIPSDAï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºåŸºäºæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·ã€‚è¯¥æ¡†æ¶ç»Ÿä¸€äº†æœ€è¿‘æå‡ºçš„å‡ ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·ç®—æ³•ï¼Œå¹¶åŒ…å«äº†å¯ä»¥é€šè¿‡çµæ´»ç»„åˆè®¾è®¡é€‰æ‹©æ¥å®ç°çš„æ–°ç®—æ³•ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶å†…çš„ç®—æ³•æ˜¯åœ¨å…·æœ‰é«˜æ–¯æ··åˆå…ˆéªŒå’Œå—å›¾åƒä¿®å¤ã€Xå°„çº¿æ–­å±‚æ‰«æå’Œç›¸ä½æ£€ç´¢é—®é¢˜å¯å‘çš„ä¼¼ç„¶å‡½æ•°æ¨¡å‹é—®é¢˜ä¸Šè¿›è¡Œçš„æµ‹è¯•ã€‚åœ¨æ­¤è®¾ç½®ä¸­ï¼Œå¯ä»¥è·å¾—è¿‘ä¼¼çœŸå®çš„åéªŒæ ·æœ¬ï¼Œè¿™èƒ½å¤ŸæŒ‰ç…§åŸåˆ™è¯„ä¼°ç®—æ³•çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒBIPSDAç®—æ³•åœ¨å›¾åƒä¿®å¤å’ŒåŸºäºXå°„çº¿æ–­å±‚æ‰«æçš„é—®é¢˜ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè€Œå¯¹äºå³ä½¿å·²çŸ¥åéªŒå¯†åº¦ä¹Ÿå¾ˆéš¾é‡‡æ ·çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„ç›¸ä½æ£€ç´¢é—®é¢˜ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·å™¨ä»æ— æ³•è§£å†³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03007v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†é€šè¿‡æ‰©æ•£é€€ç«è§£å†³è´å¶æ–¯åé—®é¢˜çš„æ–°æ¡†æ¶BIPSDAï¼Œè¯¥æ¡†æ¶æ•´åˆäº†å¤šç§æ‰©æ•£æ¨¡å‹åé‡‡æ ·ç®—æ³•ï¼ŒåŒ…æ‹¬æ–°é¢–ç®—æ³•ï¼Œå¯é€šè¿‡çµæ´»çš„è®¾è®¡é€‰æ‹©å®ç°ã€‚åœ¨å…·æœ‰é«˜æ–¯æ··åˆå…ˆéªŒå’Œå—å›¾åƒè¡¥å…¨ã€Xå°„çº¿æ–­å±‚æ‰«æå’Œç›¸ä½æ£€ç´¢é—®é¢˜å¯å‘çš„ä¼¼ç„¶å‡½æ•°çš„æ¨¡å‹é—®é¢˜ä¸Šæµ‹è¯•äº†è¯¥æ¡†æ¶çš„ç®—æ³•ï¼Œå¯ä»¥è·å¾—è¿‘ä¼¼çœŸå®çš„åéªŒæ ·æœ¬ï¼Œä»è€Œå¯ä»¥åŸåˆ™æ€§åœ°è¯„ä¼°ç®—æ³•æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒBIPSDAç®—æ³•åœ¨å›¾åƒè¡¥å…¨å’ŒåŸºäºXå°„çº¿æ–­å±‚æ‰«æçš„é—®é¢˜ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œè€Œåœ¨é‡‡æ ·å›°éš¾çš„ç›¸ä½æ£€ç´¢é—®é¢˜ä¸Šåˆ™æœ‰å¾…è¿›ä¸€æ­¥çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä½œä¸ºæœ€å…ˆè¿›çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ï¼Œåœ¨è´å¶æ–¯åé—®é¢˜ä¸­è¢«å¹¿æ³›ç”¨ä½œå…ˆéªŒã€‚</li>
<li>ç°æœ‰ç®—æ³•åœ¨å°†æ‰©æ•£æ¨¡å‹ä¸ç»™å®šçš„ä¼¼ç„¶å‡½æ•°ç»“åˆä»¥è·å–åéªŒæ ·æœ¬æ–¹é¢å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°æ¡†æ¶BIPSDAï¼Œç”¨äºåŸºäºæ‰©æ•£æ¨¡å‹çš„åéªŒé‡‡æ ·ã€‚</li>
<li>BIPSDAæ¡†æ¶æ•´åˆäº†å¤šç§æ‰©æ•£æ¨¡å‹åé‡‡æ ·ç®—æ³•ï¼ŒåŒ…æ‹¬æ–°é¢–ç®—æ³•ã€‚</li>
<li>åœ¨å…·æœ‰é«˜æ–¯æ··åˆå…ˆéªŒå’Œç‰¹å®šä¼¼ç„¶å‡½æ•°çš„æ¨¡å‹é—®é¢˜ä¸Šæµ‹è¯•äº†BIPSDAæ¡†æ¶çš„ç®—æ³•ã€‚</li>
<li>BIPSDAç®—æ³•åœ¨å›¾åƒè¡¥å…¨å’ŒåŸºäºXå°„çº¿æ–­å±‚æ‰«æçš„é—®é¢˜ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5abfbbcaa66c0c0eb7e0b994e32a29c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Personalizing-the-meshed-SPL-NAC-Brain-Atlas-for-patient-specific-scientific-computing-using-SynthMorph"><a href="#Personalizing-the-meshed-SPL-NAC-Brain-Atlas-for-patient-specific-scientific-computing-using-SynthMorph" class="headerlink" title="Personalizing the meshed SPL&#x2F;NAC Brain Atlas for patient-specific   scientific computing using SynthMorph"></a>Personalizing the meshed SPL&#x2F;NAC Brain Atlas for patient-specific   scientific computing using SynthMorph</h2><p><strong>Authors:Andy Huynh, Benjamin Zwick, Michael Halle, Adam Wittek, Karol Miller</strong></p>
<p>Developing personalized computational models of the human brain remains a challenge for patient-specific clinical applications and neuroscience research. Efficient and accurate biophysical simulations rely on high-quality personalized computational meshes derived from patientâ€™s segmented anatomical MRI scans. However, both automatic and manual segmentation are particularly challenging for tissues with limited visibility or low contrast. In this work, we present a new method to create personalized computational meshes of the brain, streamlining the development of computational brain models for clinical applications and neuroscience research. Our method uses SynthMorph, a state-of-the-art anatomy-aware, learning-based medical image registration approach, to morph a comprehensive hexahedral mesh of the open-source SPL&#x2F;NAC Brain Atlas to patient-specific MRI scans. Each patient-specific mesh includes over 300 labeled anatomical structures, more than any existing manual or automatic methods. Our registration-based method takes approximately 20 minutes, significantly faster than current state-of-the-art mesh generation pipelines, which can take up to two hours. We evaluated several state-of-the-art medical image registration methods, including SynthMorph, to determine the most optimal registration method to morph our meshed anatomical brain atlas to patient MRI scans. Our results demonstrate that SynthMorph achieved high DICE similarity coefficients and low Hausdorff Distance metrics between anatomical structures, while maintaining high mesh element quality. These findings demonstrate that our registration-based method efficiently and accurately produces high-quality, comprehensive personalized brain meshes, representing an important step toward clinical translation. </p>
<blockquote>
<p>æ„å»ºä¸ªæ€§åŒ–çš„è®¡ç®—åŒ–äººç±»å¤§è„‘æ¨¡å‹ä»ç„¶æ˜¯é’ˆå¯¹æ‚£è€…ç‰¹å®šä¸´åºŠåº”ç”¨å’Œç¥ç»ç§‘å­¦ç ”ç©¶çš„æŒ‘æˆ˜ã€‚é«˜æ•ˆä¸”å‡†ç¡®çš„ç”Ÿç‰©ç‰©ç†æ¨¡æ‹Ÿä¾èµ–äºç”±æ‚£è€…åˆ†å‰²çš„MRIæ‰«æå›¾åƒç”Ÿæˆçš„é«˜è´¨é‡ä¸ªæ€§åŒ–è®¡ç®—ç½‘æ ¼ã€‚ç„¶è€Œï¼Œå¯¹äºå¯è§åº¦æœ‰é™æˆ–å¯¹æ¯”åº¦è¾ƒä½çš„ç»„ç»‡ï¼Œè‡ªåŠ¨å’Œæ‰‹åŠ¨åˆ†å‰²éƒ½å…·æœ‰ç‰¹åˆ«çš„æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›å»ºä¸ªæ€§åŒ–è®¡ç®—å¤§è„‘ç½‘æ ¼çš„æ–°æ–¹æ³•ï¼Œä»¥ç®€åŒ–è®¡ç®—å¤§è„‘æ¨¡å‹çš„ä¸´åºŠåº”ç”¨å’Œç¥ç»ç§‘å­¦ç ”ç©¶å¼€å‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨SynthMorphï¼ˆä¸€ç§åŸºäºå­¦ä¹ çš„ã€å…ˆè¿›çš„è§£å‰–å­¦æ„è¯†åŒ»å­¦å›¾åƒé…å‡†æ–¹æ³•ï¼‰ï¼Œå°†å¼€æºSPL&#x2F;NACè„‘å›¾è°±çš„å…¨é¢å…­é¢ä½“ç½‘æ ¼å˜å½¢ä¸ºé’ˆå¯¹æ‚£è€…çš„MRIæ‰«æå›¾åƒã€‚æ¯ä¸ªæ‚£è€…ç‰¹å®šçš„ç½‘æ ¼åŒ…æ‹¬è¶…è¿‡300ä¸ªæ ‡è®°çš„è§£å‰–ç»“æ„ï¼Œæ¯”ç°æœ‰çš„ä»»ä½•æ‰‹åŠ¨æˆ–è‡ªåŠ¨æ–¹æ³•éƒ½è¦å¤šã€‚æˆ‘ä»¬çš„åŸºäºé…å‡†çš„æ–¹æ³•å¤§çº¦éœ€è¦20åˆ†é’Ÿçš„æ—¶é—´ï¼Œè¿™å¤§å¤§å¿«äºå½“å‰çš„å…ˆè¿›ç½‘æ ¼ç”Ÿæˆç®¡é“ï¼ˆå¯èƒ½éœ€è¦é•¿è¾¾ä¸¤ä¸ªå°æ—¶çš„æ—¶é—´ï¼‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§å…ˆè¿›çš„åŒ»å­¦å›¾åƒé…å‡†æ–¹æ³•ï¼ŒåŒ…æ‹¬SynthMorphï¼Œä»¥ç¡®å®šå°†æˆ‘ä»¬ç½‘æ ¼åŒ–çš„è§£å‰–è„‘å›¾è°±å˜å½¢ä¸ºæ‚£è€…MRIæ‰«æå›¾åƒçš„æœ€ä¼˜é…å‡†æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒSynthMorphåœ¨è§£å‰–ç»“æ„ä¹‹é—´å®ç°äº†é«˜çš„DICEç›¸ä¼¼æ€§ç³»æ•°å’Œä½çš„Hausdorffè·ç¦»æŒ‡æ ‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜çš„ç½‘æ ¼å…ƒç´ è´¨é‡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºé…å‡†çš„æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆä¸”å‡†ç¡®åœ°äº§ç”Ÿé«˜è´¨é‡ã€å…¨é¢çš„ä¸ªæ€§åŒ–å¤§è„‘ç½‘æ ¼ï¼Œè¿™æ˜¯å‘ä¸´åºŠåº”ç”¨ç¿»è¯‘çš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00931v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ³¨å†Œçš„æ–°æ–¹æ³•ï¼Œç”¨äºåˆ›å»ºä¸ªæ€§åŒ–çš„è„‘è®¡ç®—ç½‘æ ¼æ¨¡å‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æœ€æ–°çš„è§£å‰–å­¦æ„è¯†å›¾åƒæ³¨å†ŒæŠ€æœ¯ï¼Œå°†å¼€æºSPL&#x2F;NAC Brain Atlasçš„ç»¼åˆå…­é¢ä½“ç½‘æ ¼å½¢æ€åŒ–è‡³æ‚£è€…ç‰¹å®šçš„MRIæ‰«æã€‚æ–°æ–¹æ³•èƒ½é«˜æ•ˆã€å‡†ç¡®åœ°ç”Ÿæˆä¸ªæ€§åŒ–çš„è„‘ç½‘æ ¼æ¨¡å‹ï¼Œæœ‰æœ›ä¿ƒè¿›å…¶åœ¨ä¸´åºŠåº”ç”¨å’Œç¥ç»ç§‘å­¦ç ”ç©¶ä¸­çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨SynthMorphæ³¨å†ŒæŠ€æœ¯åˆ›å»ºä¸ªæ€§åŒ–è„‘è®¡ç®—ç½‘æ ¼æ¨¡å‹ã€‚</li>
<li>å°†å¼€æºSPL&#x2F;NAC Brain Atlasçš„ç½‘æ ¼å½¢æ€åŒ–è‡³æ‚£è€…ç‰¹å®šMRIæ‰«æã€‚</li>
<li>æ¯ä¸ªä¸ªæ€§åŒ–ç½‘æ ¼åŒ…å«è¶…è¿‡300ä¸ªæ ‡è®°çš„è§£å‰–ç»“æ„ï¼Œè¶…è¶Šç°æœ‰æ‰‹åŠ¨æˆ–è‡ªåŠ¨æ–¹æ³•ã€‚</li>
<li>æ³¨å†Œæ–¹æ³•è€—æ—¶çº¦20åˆ†é’Ÿï¼Œæ˜¾è‘—å¿«äºå½“å‰æœ€å…ˆè¿›çš„ç½‘æ ¼ç”Ÿæˆæµç¨‹ã€‚</li>
<li>å¯¹æ¯”è¯„ä¼°äº†å¤šç§å…ˆè¿›çš„åŒ»å­¦å›¾åƒæ³¨å†Œæ–¹æ³•ï¼Œç¡®å®šSynthMorphä¸ºæœ€ä½³é€‰æ‹©ã€‚</li>
<li>SynthMorphåœ¨è§£å‰–ç»“æ„é—´å®ç°äº†é«˜DICEç›¸ä¼¼ç³»æ•°å’Œä½Hausdorffè·ç¦»æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be4a0591fceba9b2009c28be1bae8452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a56ec83cdc322e3abf91627df4c67583.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-524d61db3f2046edd0da1ba7ecf408ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93f700137f58b61a47aa18af928bb1e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5894bfde4d44944a58038fa84f27a529.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8409496dfed810eb5a71d927213af23d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multimodal-Distillation-Driven-Ensemble-Learning-for-Long-Tailed-Histopathology-Whole-Slide-Images-Analysis"><a href="#Multimodal-Distillation-Driven-Ensemble-Learning-for-Long-Tailed-Histopathology-Whole-Slide-Images-Analysis" class="headerlink" title="Multimodal Distillation-Driven Ensemble Learning for Long-Tailed   Histopathology Whole Slide Images Analysis"></a>Multimodal Distillation-Driven Ensemble Learning for Long-Tailed   Histopathology Whole Slide Images Analysis</h2><p><strong>Authors:Xitong Ling, Yifeng Ping, Jiawen Li, Jing Peng, Yuxuan Chen, Minxi Ouyang, Yizhi Wang, Yonghong He, Tian Guan, Xiaoping Liu, Lianghui Zhu</strong></p>
<p>Multiple Instance Learning (MIL) plays a significant role in computational pathology, enabling weakly supervised analysis of Whole Slide Image (WSI) datasets. The field of WSI analysis is confronted with a severe long-tailed distribution problem, which significantly impacts the performance of classifiers. Long-tailed distributions lead to class imbalance, where some classes have sparse samples while others are abundant, making it difficult for classifiers to accurately identify minority class samples. To address this issue, we propose an ensemble learning method based on MIL, which employs expert decoders with shared aggregators and consistency constraints to learn diverse distributions and reduce the impact of class imbalance on classifier performance. Moreover, we introduce a multimodal distillation framework that leverages text encoders pre-trained on pathology-text pairs to distill knowledge and guide the MIL aggregator in capturing stronger semantic features relevant to class information. To ensure flexibility, we use learnable prompts to guide the distillation process of the pre-trained text encoder, avoiding limitations imposed by specific prompts. Our method, MDE-MIL, integrates multiple expert branches focusing on specific data distributions to address long-tailed issues. Consistency control ensures generalization across classes. Multimodal distillation enhances feature extraction. Experiments on Camelyon+-LT and PANDA-LT datasets show it outperforms state-of-the-art methods. </p>
<blockquote>
<p>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨è®¡ç®—ç—…ç†å­¦ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå®ƒèƒ½å¤Ÿå¯¹å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰æ•°æ®é›†è¿›è¡Œå¼±ç›‘ç£åˆ†æã€‚WSIåˆ†æé¢†åŸŸé¢ä¸´ç€ä¸€ä¸ªä¸¥é‡çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œè¿™ä¸¥é‡å½±å“äº†åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚é•¿å°¾åˆ†å¸ƒå¯¼è‡´ç±»åˆ«ä¸å¹³è¡¡ï¼Œå…¶ä¸­ä¸€äº›ç±»åˆ«çš„æ ·æœ¬ç¨€å°‘ï¼Œè€Œå…¶ä»–ç±»åˆ«åˆ™å¾ˆä¸°å¯Œï¼Œè¿™ä½¿å¾—åˆ†ç±»å™¨éš¾ä»¥å‡†ç¡®è¯†åˆ«å‡ºå°‘æ•°ç±»åˆ«çš„æ ·æœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMILçš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¸¦æœ‰å…±äº«èšåˆå™¨å’Œä¸€è‡´æ€§çº¦æŸçš„ä¸“å®¶è§£ç å™¨æ¥å­¦ä¹ å„ç§åˆ†å¸ƒï¼Œå¹¶å‡å°‘ç±»åˆ«ä¸å¹³è¡¡å¯¹åˆ†ç±»å™¨æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šæ¨¡æ€è’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åœ¨ç—…ç†å­¦æ–‡æœ¬å¯¹ä¸Šé¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå¹¶æŒ‡å¯¼MILèšåˆå™¨æ•è·ä¸ç±»åˆ«ä¿¡æ¯ç›¸å…³çš„æ›´å¼ºè¯­ä¹‰ç‰¹å¾ã€‚ä¸ºäº†ç¡®ä¿çµæ´»æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨å¯å­¦ä¹ çš„æç¤ºæ¥å¼•å¯¼é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨çš„è’¸é¦è¿‡ç¨‹ï¼Œé¿å…ç‰¹å®šæç¤ºæ‰€å¸¦æ¥çš„é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•MDE-MILé›†æˆäº†å¤šä¸ªä¸“æ³¨äºç‰¹å®šæ•°æ®åˆ†å¸ƒçš„ä¸“å®¶åˆ†æ”¯ï¼Œä»¥è§£å†³é•¿å°¾é—®é¢˜ã€‚ä¸€è‡´æ€§æ§åˆ¶ç¡®ä¿è·¨ç±»åˆ«çš„æ³›åŒ–ã€‚å¤šæ¨¡æ€è’¸é¦å¢å¼ºäº†ç‰¹å¾æå–ã€‚åœ¨Camelyon+-LTå’ŒPANDA-LTæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00915v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è®¡ç®—ç—…ç†å­¦é¢†åŸŸä¸­ï¼Œå¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨å¤„ç†å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰æ•°æ®é›†æ—¶çš„åº”ç”¨ã€‚é’ˆå¯¹WSIåˆ†æé¢ä¸´çš„ä¸¥é‡é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºMILçš„é›†æˆå­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å…·æœ‰å…±äº«èšåˆå™¨å’Œä¸€è‡´æ€§çº¦æŸçš„ä¸“å®¶è§£ç å™¨ï¼Œä»¥å­¦ä¹ å„ç§åˆ†å¸ƒå¹¶å‡å°‘ç±»åˆ«ä¸å¹³è¡¡å¯¹åˆ†ç±»å™¨æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€è’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨åœ¨ç—…ç†å­¦æ–‡æœ¬å¯¹ä¸Šé¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå¼•å¯¼MILèšåˆå™¨æ•è·ä¸ç±»åˆ«ä¿¡æ¯ç›¸å…³çš„æ›´å¼ºè¯­ä¹‰ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Camelyon+-LTå’ŒPANDA-LTæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨è®¡ç®—ç—…ç†å­¦ä¸­çš„é‡è¦ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰æ•°æ®é›†ä¸Šçš„å¼±ç›‘ç£åˆ†æã€‚</li>
<li>WSIåˆ†æé¢ä¸´çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜åŠå…¶å¯¹åˆ†ç±»å™¨æ€§èƒ½çš„å½±å“ã€‚</li>
<li>åŸºäºMILçš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é‡‡ç”¨ä¸“å®¶è§£ç å™¨å’Œå…±äº«èšåˆå™¨æ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>å¼•å…¥å¤šæ¨¡æ€è’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œæé«˜MILçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨ä¸€è‡´æ€§æ§åˆ¶ç¡®ä¿è·¨ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MDE-MILæ–¹æ³•é€šè¿‡é›†æˆå¤šä¸ªä¸“æ³¨äºç‰¹å®šæ•°æ®åˆ†å¸ƒçš„ä¸“å®¶åˆ†æ”¯æ¥è§£å†³é•¿å°¾é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d60d767db296844d98ea64b6c9530403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60531e5f223daac98a010491288030f5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MR-EIT-Multi-Resolution-Reconstruction-for-Electrical-Impedance-Tomography-via-Data-Driven-and-Unsupervised-Dual-Mode-Neural-Networks"><a href="#MR-EIT-Multi-Resolution-Reconstruction-for-Electrical-Impedance-Tomography-via-Data-Driven-and-Unsupervised-Dual-Mode-Neural-Networks" class="headerlink" title="MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance   Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks"></a>MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance   Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks</h2><p><strong>Authors:Fangming Shi, Jinzhen Liu, Xiangqian Meng, Yapeng Zhou, Hui Xiong</strong></p>
<p>This paper presents a multi-resolution reconstruction method for Electrical Impedance Tomography (EIT), referred to as MR-EIT, which is capable of operating in both supervised and unsupervised learning modes. MR-EIT integrates an ordered feature extraction module and an unordered coordinate feature expression module. The former achieves the mapping from voltage to two-dimensional conductivity features through pre-training, while the latter realizes multi-resolution reconstruction independent of the order and size of the input sequence by utilizing symmetric functions and local feature extraction mechanisms. In the data-driven mode, MR-EIT reconstructs high-resolution images from low-resolution data of finite element meshes through two stages of pre-training and joint training, and demonstrates excellent performance in simulation experiments. In the unsupervised learning mode, MR-EIT does not require pre-training data and performs iterative optimization solely based on measured voltages to rapidly achieve image reconstruction from low to high resolution. It shows robustness to noise and efficient super-resolution reconstruction capabilities in both simulation and real water tank experiments. Experimental results indicate that MR-EIT outperforms the comparison methods in terms of Structural Similarity (SSIM) and Relative Image Error (RIE), especially in the unsupervised learning mode, where it can significantly reduce the number of iterations and improve image reconstruction quality. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç”µé˜»æŠ—æ–­å±‚æ‰«æï¼ˆEITï¼‰çš„å¤šåˆ†è¾¨ç‡é‡å»ºæ–¹æ³•ï¼Œç®€ç§°ä¸ºMR-EITã€‚è¯¥æ–¹æ³•æ—¢å¯åœ¨æœ‰ç›‘ç£å­¦ä¹ æ¨¡å¼ï¼Œä¹Ÿå¯åœ¨æ— ç›‘ç£å­¦ä¹ æ¨¡å¼ä¸‹è¿è¡Œã€‚MR-EITç»“åˆäº†æœ‰åºç‰¹å¾æå–æ¨¡å—å’Œæ— åºåæ ‡ç‰¹å¾è¡¨è¾¾å¼æ¨¡å—ã€‚å‰è€…é€šè¿‡é¢„è®­ç»ƒå®ç°ç”µå‹åˆ°äºŒç»´å¯¼ç”µç‡ç‰¹å¾çš„æ˜ å°„ï¼Œåè€…åˆ™åˆ©ç”¨å¯¹ç§°å‡½æ•°å’Œå±€éƒ¨ç‰¹å¾æå–æœºåˆ¶ï¼Œå®ç°äº†ç‹¬ç«‹äºè¾“å…¥åºåˆ—é¡ºåºå’Œå¤§å°çš„å¤šåˆ†è¾¨ç‡é‡å»ºã€‚åœ¨æ•°æ®é©±åŠ¨æ¨¡å¼ä¸‹ï¼ŒMR-EITé€šè¿‡ä¸¤ä¸ªé˜¶æ®µï¼ˆé¢„è®­ç»ƒå’Œè”åˆè®­ç»ƒï¼‰ä»æœ‰é™å…ƒç½‘æ ¼çš„ä½åˆ†è¾¨ç‡æ•°æ®ä¸­é‡å»ºé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶åœ¨æ¨¡æ‹Ÿå®éªŒä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚åœ¨æ— ç›‘ç£å­¦ä¹ æ¨¡å¼ä¸‹ï¼ŒMR-EITæ— éœ€é¢„è®­ç»ƒæ•°æ®ï¼Œä»…åŸºäºæµ‹é‡çš„ç”µå‹è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»è€Œè¿…é€Ÿå®ç°ä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡çš„å›¾åƒé‡å»ºã€‚è¯¥æ–¹æ³•å¯¹å™ªå£°å…·æœ‰é²æ£’æ€§ï¼Œå¹¶ä¸”åœ¨æ¨¡æ‹Ÿå’Œå®é™…æ°´ç®±å®éªŒä¸­å‡è¡¨ç°å‡ºé«˜æ•ˆçš„è¶…åˆ†è¾¨ç‡é‡å»ºèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMR-EITåœ¨ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œç›¸å¯¹å›¾åƒè¯¯å·®ï¼ˆRIEï¼‰æ–¹é¢ä¼˜äºæ¯”è¾ƒæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— ç›‘ç£å­¦ä¹ æ¨¡å¼ä¸‹ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘è¿­ä»£æ¬¡æ•°å¹¶æé«˜å›¾åƒé‡å»ºè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00762v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šåˆ†è¾¨ç‡é‡å»ºæ–¹æ³•ï¼Œç”¨äºç”µæ°”é˜»æŠ—å±‚ææˆåƒï¼ˆEITï¼‰ï¼Œç§°ä¸ºMR-EITã€‚è¯¥æ–¹æ³•å¯åœ¨æœ‰ç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ æ¨¡å¼ä¸‹è¿è¡Œï¼Œé›†æˆäº†æœ‰åºç‰¹å¾æå–æ¨¡å—å’Œæ— åºåæ ‡ç‰¹å¾è¡¨è¾¾æ¨¡å—ã€‚é€šè¿‡é¢„è®­ç»ƒå®ç°ç”µå‹åˆ°äºŒç»´å¯¼ç”µç‰¹å¾çš„æ˜ å°„ï¼Œåˆ©ç”¨å¯¹ç§°å‡½æ•°å’Œå±€éƒ¨ç‰¹å¾æå–æœºåˆ¶å®ç°ç‹¬ç«‹äºè¾“å…¥åºåˆ—é¡ºåºå’Œå¤§å°çš„å¤šåˆ†è¾¨ç‡é‡å»ºã€‚åœ¨æ•°æ®é©±åŠ¨æ¨¡å¼ä¸‹ï¼ŒMR-EITé€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œé«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºï¼Œæ¨¡æ‹Ÿå®éªŒè¡¨ç°ä¼˜å¼‚ã€‚æ— ç›‘ç£æ¨¡å¼ä¸‹ï¼ŒMR-EITæ— éœ€é¢„è®­ç»ƒæ•°æ®ï¼ŒåŸºäºæµ‹é‡ç”µå‹è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œå®ç°å¿«é€Ÿä½åˆ†è¾¨ç‡è‡³é«˜åˆ†è¾¨ç‡çš„å›¾åƒé‡å»ºï¼Œå¯¹å™ªå£°å…·æœ‰é²æ£’æ€§ï¼Œä»¿çœŸå’ŒçœŸå®æ°´ç®±å®éªŒä¸­çš„è¶…åˆ†è¾¨ç‡é‡å»ºèƒ½åŠ›å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MR-EITæ˜¯ä¸€ç§å¤šåˆ†è¾¨ç‡é‡å»ºæ–¹æ³•ï¼Œç”¨äºç”µæ°”é˜»æŠ—å±‚ææˆåƒï¼ˆEITï¼‰ã€‚</li>
<li>MR-EITå¯åœ¨æœ‰ç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ æ¨¡å¼ä¸‹è¿è¡Œã€‚</li>
<li>MR-EITé›†æˆäº†æœ‰åºç‰¹å¾æå–æ¨¡å—ä¸æ— åºåæ ‡ç‰¹å¾è¡¨è¾¾æ¨¡å—ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒå®ç°ç”µå‹åˆ°äºŒç»´å¯¼ç”µç‰¹å¾çš„æ˜ å°„ã€‚</li>
<li>MR-EITåˆ©ç”¨å¯¹ç§°å‡½æ•°å’Œå±€éƒ¨ç‰¹å¾æå–æœºåˆ¶è¿›è¡Œå¤šåˆ†è¾¨ç‡é‡å»ºï¼Œè¯¥æœºåˆ¶ç‹¬ç«‹äºè¾“å…¥åºåˆ—çš„é¡ºåºå’Œå¤§å°ã€‚</li>
<li>åœ¨æ•°æ®é©±åŠ¨æ¨¡å¼ä¸‹ï¼ŒMR-EITé€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œé«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0928ebdcf6c80a1ef54a3a63858fea4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa553d81777faf2e5939374754334994.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deef2b1af68fc5a6e508a399966d8e42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-649abe2380ab6f4aa442c89754e246ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d48e596daf4ccb2d51368814418799c3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Geodesic-Diffusion-Models-for-Medical-Image-to-Image-Generation"><a href="#Geodesic-Diffusion-Models-for-Medical-Image-to-Image-Generation" class="headerlink" title="Geodesic Diffusion Models for Medical Image-to-Image Generation"></a>Geodesic Diffusion Models for Medical Image-to-Image Generation</h2><p><strong>Authors:Teng Zhang, Hongxu Jiang, Kuang Gong, Wei Shao</strong></p>
<p>Diffusion models transform an unknown data distribution into a Gaussian prior by progressively adding noise until the data become indistinguishable from pure noise. This stochastic process traces a path in probability space, evolving from the original data distribution (considered as a Gaussian with near-zero variance) to an isotropic Gaussian. The denoiser then learns to reverse this process, generating high-quality samples from random Gaussian noise. However, standard diffusion models, such as the Denoising Diffusion Probabilistic Model (DDPM), do not ensure a geodesic (i.e., shortest) path in probability space. This inefficiency necessitates the use of many intermediate time steps, leading to high computational costs in training and sampling. To address this limitation, we propose the Geodesic Diffusion Model (GDM), which defines a geodesic path under the Fisher-Rao metric with a variance-exploding noise scheduler. This formulation transforms the data distribution into a Gaussian prior with minimal energy, significantly improving the efficiency of diffusion models. We trained GDM by continuously sampling time steps from 0 to 1 and using as few as 15 evenly spaced time steps for model sampling. We evaluated GDM on two medical image-to-image generation tasks: CT image denoising and MRI image super-resolution. Experimental results show that GDM achieved state-of-the-art performance while reducing training time by a 50-fold compared to DDPM and 10-fold compared to Fast-DDPM, with 66 times faster sampling than DDPM and a similar sampling speed to Fast-DDPM. These efficiency gains enable rapid model exploration and real-time clinical applications. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/mirthAI/GDM-VE">https://github.com/mirthAI/GDM-VE</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥æ·»åŠ å™ªå£°å°†æœªçŸ¥æ•°æ®åˆ†å¸ƒè½¬å˜ä¸ºé«˜æ–¯å…ˆéªŒï¼Œç›´åˆ°æ•°æ®æ— æ³•ä¸çº¯å™ªå£°åŒºåˆ†å¼€ã€‚è¿™ä¸€éšæœºè¿‡ç¨‹åœ¨æ¦‚ç‡ç©ºé—´ä¸­è¿½è¸ªä¸€æ¡è·¯å¾„ï¼Œä»åŸå§‹æ•°æ®åˆ†å¸ƒï¼ˆè¢«è§†ä¸ºæ–¹å·®æ¥è¿‘é›¶çš„é«˜æ–¯åˆ†å¸ƒï¼‰æ¼”å˜åˆ°å„å‘åŒæ€§é«˜æ–¯ã€‚ç„¶åï¼Œå»å™ªå™¨å­¦ä¹ åè½¬è¿™ä¸€è¿‡ç¨‹ï¼Œä»éšæœºé«˜æ–¯å™ªå£°ä¸­ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚ç„¶è€Œï¼Œæ ‡å‡†æ‰©æ•£æ¨¡å‹ï¼Œå¦‚é™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œå¹¶ä¸èƒ½ç¡®ä¿æ¦‚ç‡ç©ºé—´ä¸­çš„æµ‹åœ°çº¿ï¼ˆå³æœ€çŸ­ï¼‰è·¯å¾„ã€‚è¿™ç§ä½æ•ˆæ€§éœ€è¦ä½¿ç”¨è®¸å¤šä¸­é—´æ—¶é—´æ­¥ï¼Œå¯¼è‡´è®­ç»ƒå’Œé‡‡æ ·ä¸­çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æµ‹åœ°æ‰©æ•£æ¨¡å‹ï¼ˆGDMï¼‰ï¼Œå®ƒåœ¨Fisher-Raoåº¦é‡ä¸‹å®šä¹‰äº†ä¸€ä¸ªæµ‹åœ°çº¿è·¯å¾„ï¼Œå¹¶é…å¤‡äº†ä¸€ä¸ªæ–¹å·®çˆ†ç‚¸å™ªå£°è°ƒåº¦å™¨ã€‚è¿™ç§è¡¨è¿°å°†æ•°æ®åˆ†å¸ƒè½¬å˜ä¸ºé«˜æ–¯å…ˆéªŒï¼Œä»¥æœ€å°çš„èƒ½é‡æ¶ˆè€—ï¼Œæ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚æˆ‘ä»¬é€šè¿‡ä»0åˆ°1æŒç»­é‡‡æ ·æ—¶é—´æ­¥ï¼Œå¹¶ä½¿ç”¨æœ€å¤š15ä¸ªå‡åŒ€é—´éš”çš„æ—¶é—´æ­¥æ¥è®­ç»ƒGDMæ¨¡å‹è¿›è¡Œé‡‡æ ·ã€‚æˆ‘ä»¬åœ¨ä¸¤é¡¹åŒ»å­¦å›¾åƒåˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¯„ä¼°äº†GDMï¼šCTå›¾åƒå»å™ªå’ŒMRIå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸DDPMç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†50å€ï¼Œä¸Fast-DDPMç›¸æ¯”å‡å°‘äº†10å€ã€‚ä¸DDPMç›¸æ¯”ï¼ŒGDMçš„é‡‡æ ·é€Ÿåº¦å¿«66å€ï¼Œä¸Fast-DDPMçš„é‡‡æ ·é€Ÿåº¦ç›¸ä¼¼ã€‚è¿™äº›æ•ˆç‡æå‡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿæ¢ç´¢å¹¶åº”ç”¨äºå®æ—¶ä¸´åºŠã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/mirthAI/GDM-VE">https://github.com/mirthAI/GDM-VE</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00745v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥æ·»åŠ å™ªå£°å°†æœªçŸ¥æ•°æ®åˆ†å¸ƒè½¬åŒ–ä¸ºé«˜æ–¯å…ˆéªŒï¼Œç›´åˆ°æ•°æ®å˜å¾—ä¸çº¯å™ªå£°æ— æ³•åŒºåˆ†ã€‚åœ¨æ¦‚ç‡ç©ºé—´ä¸­ï¼Œå®ƒä»åŸå§‹æ•°æ®åˆ†å¸ƒï¼ˆè¢«è§†ä¸ºå…·æœ‰è¿‘é›¶æ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒï¼‰æ¼”åŒ–åˆ°ä¸€ä¸ªç­‰è·é«˜æ–¯ã€‚å»å™ªå™¨å­¦ä¹ é€†è½¬è¿™ä¸€è¿‡ç¨‹ï¼Œä»éšæœºé«˜æ–¯å™ªå£°ä¸­ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚ç„¶è€Œï¼Œæ ‡å‡†æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹DDPMï¼‰å¹¶ä¸ç¡®ä¿æ¦‚ç‡ç©ºé—´ä¸­çš„æœ€çŸ­è·¯å¾„ï¼ˆå³æµ‹åœ°çº¿ï¼‰ã€‚è¿™ç§ä½æ•ˆéœ€è¦å¤§é‡ä¸­é—´æ—¶é—´æ­¥éª¤ï¼Œå¯¼è‡´è®­ç»ƒå’Œé‡‡æ ·ä¸­çš„é«˜è®¡ç®—æˆæœ¬ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Geodesic Diffusion Modelï¼ˆGDMï¼‰ï¼Œåœ¨Fisher-Raoåº¦é‡ä¸‹å®šä¹‰äº†æµ‹åœ°çº¿è·¯å¾„ï¼Œå¹¶é‡‡ç”¨æ–¹å·®çˆ†ç‚¸å™ªå£°è°ƒåº¦å™¨ã€‚è¿™ç§è¡¨è¿°å°†æ•°æ®åˆ†å¸ƒè½¬æ¢ä¸ºé«˜æ–¯å…ˆéªŒï¼Œä»¥æœ€å°çš„èƒ½é‡ï¼Œæ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŒ»å­¦å›¾åƒåˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¯„ä¼°äº†GDMï¼šCTå›¾åƒå»å™ªå’ŒMRIå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGDMåœ¨å‡å°‘è®­ç»ƒæ—¶é—´çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºDDPMå‡å°‘50å€ï¼Œç›¸è¾ƒäºFast-DDPMå‡å°‘10å€ï¼›ç›¸è¾ƒäºDDPMé‡‡æ ·é€Ÿåº¦æé«˜66å€ï¼Œä¸Fast-DDPMé‡‡æ ·é€Ÿåº¦ç›¸ä¼¼ã€‚è¿™äº›æ•ˆç‡æå‡ä½¿æ¨¡å‹å¿«é€Ÿæ¢ç´¢åŠå®æ—¶ä¸´åºŠåº”ç”¨æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/mirthAI/GDM-VE">https://github.com/mirthAI/GDM-VE</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥æ·»åŠ å™ªå£°å°†æ•°æ®åˆ†å¸ƒè½¬åŒ–ä¸ºé«˜æ–¯å…ˆéªŒã€‚</li>
<li>æ ‡å‡†æ‰©æ•£æ¨¡å‹åœ¨æ¦‚ç‡ç©ºé—´ä¸­å¹¶ä¸æ€»æ˜¯é‡‡å–æœ€çŸ­çš„æµ‹åœ°çº¿è·¯å¾„ï¼Œå¯¼è‡´è®­ç»ƒå’Œé‡‡æ ·çš„é«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>æå‡ºçš„Geodesic Diffusion Modelï¼ˆGDMï¼‰åœ¨Fisher-Raoåº¦é‡ä¸‹å®šä¹‰æµ‹åœ°çº¿è·¯å¾„ï¼Œæé«˜æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>GDMåœ¨åŒ»å­¦å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œå¦‚CTå›¾åƒå»å™ªå’ŒMRIå›¾åƒè¶…åˆ†è¾¨ç‡ã€‚</li>
<li>GDMç›¸è¾ƒäºå…¶ä»–æ¨¡å‹å¤§å¹…å‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œé‡‡æ ·æ—¶é—´ã€‚</li>
<li>GDMçš„å®ç°ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f24cbc4dacac9887ac1e485629e24d70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13180e16fc67eaaf4a8de963818f223e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Flow-Matching-for-Medical-Image-Synthesis-Bridging-the-Gap-Between-Speed-and-Quality"><a href="#Flow-Matching-for-Medical-Image-Synthesis-Bridging-the-Gap-Between-Speed-and-Quality" class="headerlink" title="Flow Matching for Medical Image Synthesis: Bridging the Gap Between   Speed and Quality"></a>Flow Matching for Medical Image Synthesis: Bridging the Gap Between   Speed and Quality</h2><p><strong>Authors:Milad Yazdani, Yasamin Medghalchi, Pooria Ashrafian, Ilker Hacihaliloglu, Dena Shahriari</strong></p>
<p>Deep learning models have emerged as a powerful tool for various medical applications. However, their success depends on large, high-quality datasets that are challenging to obtain due to privacy concerns and costly annotation. Generative models, such as diffusion models, offer a potential solution by synthesizing medical images, but their practical adoption is hindered by long inference times. In this paper, we propose the use of an optimal transport flow matching approach to accelerate image generation. By introducing a straighter mapping between the source and target distribution, our method significantly reduces inference time while preserving and further enhancing the quality of the outputs. Furthermore, this approach is highly adaptable, supporting various medical imaging modalities, conditioning mechanisms (such as class labels and masks), and different spatial dimensions, including 2D and 3D. Beyond image generation, it can also be applied to related tasks such as image enhancement. Our results demonstrate the efficiency and versatility of this framework, making it a promising advancement for medical imaging applications. Code with checkpoints and a synthetic dataset (beneficial for classification and segmentation) is now available on: <a target="_blank" rel="noopener" href="https://github.com/milad1378yz/MOTFM">https://github.com/milad1378yz/MOTFM</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å„ç§åŒ»ç–—åº”ç”¨ä¸­å·²å´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æˆåŠŸä¾èµ–äºéš¾ä»¥è·å¾—çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºéšç§æ‹…å¿§å’Œæ˜‚è´µçš„æ ‡æ³¨æˆæœ¬æ‰€è‡´ã€‚ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡åˆæˆåŒ»ç–—å›¾åƒæä¾›äº†ä¸€ä¸ªæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬ç”±äºæ¼«é•¿çš„æ¨ç†æ—¶é—´è€Œé˜»ç¢äº†å®é™…åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æœ€ä¼˜ä¼ è¾“æµåŒ¹é…æ–¹æ³•æ¥åŠ é€Ÿå›¾åƒç”Ÿæˆçš„æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥æºå’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„æ›´ç›´æ¥æ˜ å°„ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒå’Œè¿›ä¸€æ­¥æé«˜è¾“å‡ºè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•å…·æœ‰é«˜åº¦é€‚åº”æ€§ï¼Œæ”¯æŒå„ç§åŒ»å­¦æˆåƒæ¨¡å¼ã€è°ƒèŠ‚æœºåˆ¶ï¼ˆå¦‚ç±»åˆ«æ ‡ç­¾å’Œæ©ç ï¼‰ï¼Œä»¥åŠä¸åŒçš„ç©ºé—´ç»´åº¦ï¼ŒåŒ…æ‹¬äºŒç»´å’Œä¸‰ç»´ã€‚é™¤äº†å›¾åƒç”Ÿæˆï¼Œå®ƒè¿˜å¯ä»¥åº”ç”¨äºå›¾åƒå¢å¼ºç­‰ç›¸å…³ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶çš„é«˜æ•ˆç‡å’Œå¤šåŠŸèƒ½æ€§ï¼Œä½¿å…¶æˆä¸ºåŒ»ç–—æˆåƒåº”ç”¨ä¸­çš„ä¸€é¡¹æœ‰å‰é€”çš„è¿›æ­¥ã€‚å…³äºæ£€æŸ¥ç‚¹å’Œåˆæˆæ•°æ®é›†çš„ä»£ç ç°åœ¨å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/milad1378yz/MOTFM%EF%BC%88%E8%BF%99%E4%B8%AA%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86%E5%AF%B9%E5%88%86%E7%B1%BB%E5%92%8C%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E6%9C%89%E7%9B%8A%EF%BC%89%E3%80%82">https://github.com/milad1378yz/MOTFMï¼ˆè¿™ä¸ªåˆæˆæ•°æ®é›†å¯¹åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡æœ‰ç›Šï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00266v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œæå‡ºä¸€ç§åŸºäºæœ€ä¼˜ä¼ è¾“æµåŒ¹é…ï¼ˆOTFMï¼‰çš„åŠ é€Ÿå›¾åƒç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æºä¸ç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„æ›´ç›´æ¥æ˜ å°„ï¼Œæ˜¾è‘—å‡å°‘æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¿è¯è¾“å‡ºè´¨é‡ã€‚æ–¹æ³•é«˜åº¦çµæ´»ï¼Œé€‚ç”¨äºå¤šç§åŒ»å­¦æˆåƒæ¨¡å¼ã€æ¡ä»¶æœºåˆ¶å’Œç©ºé—´ç»´åº¦ã€‚ä»£ç åŠæ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€ä¼˜ä¼ è¾“æµåŒ¹é…ï¼ˆOTFMï¼‰æ–¹æ³•è¢«åº”ç”¨äºåŒ»å­¦å›¾åƒç”Ÿæˆï¼Œæ—¨åœ¨åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>OTFMæ–¹æ³•é€šè¿‡åˆ›å»ºæºå’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„ç›´æ¥æ˜ å°„ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆçš„æ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¿è¯å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚</li>
<li>OTFMæ–¹æ³•é«˜åº¦çµæ´»ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„åŒ»å­¦æˆåƒæ¨¡å¼ã€æ¡ä»¶æœºåˆ¶å’Œç©ºé—´ç»´åº¦ï¼ˆå¦‚2Då’Œ3Dï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…é€‚ç”¨äºå›¾åƒç”Ÿæˆï¼Œè¿˜å¯ä»¥åº”ç”¨äºç›¸å…³ä»»åŠ¡ï¼Œå¦‚å›¾åƒå¢å¼ºã€‚</li>
<li>å…¬å¼€çš„ä»£ç å’Œåˆæˆæ•°æ®é›†å¯ç”¨äºåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-078a674888586c3da5787c01a0e64810.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d53910d522c54d2babf4e4bc2418d99f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a589c25241dd817cb0ca279ab605cd2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PRISM-High-Resolution-Precise-Counterfactual-Medical-Image-Generation-using-Language-guided-Stable-Diffusion"><a href="#PRISM-High-Resolution-Precise-Counterfactual-Medical-Image-Generation-using-Language-guided-Stable-Diffusion" class="headerlink" title="PRISM: High-Resolution &amp; Precise Counterfactual Medical Image Generation   using Language-guided Stable Diffusion"></a>PRISM: High-Resolution &amp; Precise Counterfactual Medical Image Generation   using Language-guided Stable Diffusion</h2><p><strong>Authors:Amar Kumar, Anita Kriz, Mohammad Havaei, Tal Arbel</strong></p>
<p>Developing reliable and generalizable deep learning systems for medical imaging faces significant obstacles due to spurious correlations, data imbalances, and limited text annotations in datasets. Addressing these challenges requires architectures robust to the unique complexities posed by medical imaging data. The rapid advancements in vision-language foundation models within the natural image domain prompt the question of how they can be adapted for medical imaging tasks. In this work, we present PRISM, a framework that leverages foundation models to generate high-resolution, language-guided medical image counterfactuals using Stable Diffusion. Our approach demonstrates unprecedented precision in selectively modifying spurious correlations (the medical devices) and disease features, enabling the removal and addition of specific attributes while preserving other image characteristics. Through extensive evaluation, we show how PRISM advances counterfactual generation and enables the development of more robust downstream classifiers for clinically deployable solutions. To facilitate broader adoption and research, we make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/Amarkr1/PRISM">https://github.com/Amarkr1/PRISM</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸï¼Œå¼€å‘å¯é ä¸”å¯æ¨å¹¿çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿé¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ•°æ®ä¸­çš„è™šå‡å…³è”ã€æ•°æ®ä¸å¹³è¡¡ä»¥åŠæ–‡æœ¬æ³¨é‡Šæœ‰é™ã€‚è¦è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œéœ€è¦æ„å»ºèƒ½å¤Ÿåº”å¯¹åŒ»å­¦æˆåƒæ•°æ®ç‹¬ç‰¹å¤æ‚æ€§çš„æ¶æ„ã€‚è‡ªç„¶å›¾åƒé¢†åŸŸçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼Œå³å¦‚ä½•å°†è¿™äº›æ¨¡å‹é€‚åº”äºåŒ»å­¦æˆåƒä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PRISMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨Stable Diffusionç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ã€å—è¯­è¨€å¼•å¯¼çš„åŒ»ç–—å›¾åƒåäº‹å®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥å‰æ‰€æœªæœ‰çš„ç²¾åº¦é€‰æ‹©æ€§åœ°ä¿®æ”¹è™šå‡å…³è”ï¼ˆåŒ»ç–—å™¨æï¼‰å’Œç–¾ç—…ç‰¹å¾ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™å…¶ä»–å›¾åƒç‰¹å¾çš„åŒæ—¶åˆ é™¤å’Œæ·»åŠ ç‰¹å®šå±æ€§ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†PRISMåœ¨åäº‹å®ç”Ÿæˆæ–¹é¢çš„è¿›å±•ï¼Œå¹¶è¯æ˜å…¶èƒ½å¤Ÿå¼€å‘æ›´ç¨³å¥çš„ä¸‹æ¸¸åˆ†ç±»å™¨ï¼Œç”¨äºä¸´åºŠéƒ¨ç½²è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†ä¿ƒè¿›æ›´å¹¿æ³›çš„é‡‡ç”¨å’Œç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Amarkr1/PRISM">https://github.com/Amarkr1/PRISM</a>ä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00196v1">PDF</a> Under Review for MIDL 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦æˆåƒé¢†åŸŸçš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è™šå‡å…³è”ã€æ•°æ®ä¸å¹³è¡¡å’Œæ ‡æ³¨æ–‡æœ¬æœ‰é™ç­‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œéœ€è¦é€‚åº”åŒ»å­¦æˆåƒæ•°æ®ç‹¬ç‰¹å¤æ‚æ€§çš„ç¨³å¥æ¶æ„ã€‚æœ¬ç ”ç©¶æå‡ºPRISMæ¡†æ¶ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€è¯­è¨€å¼•å¯¼çš„åŒ»ç–—å›¾åƒåäº‹å®ï¼Œåˆ©ç”¨Stable Diffusionè¿›è¡Œé€‰æ‹©æ€§åœ°ä¿®æ”¹è™šå‡å…³è”å’Œç–¾ç—…ç‰¹å¾ï¼Œå®ç°ç‰¹å®šå±æ€§çš„æ·»åŠ å’Œç§»é™¤ï¼ŒåŒæ—¶ä¿ç•™å…¶ä»–å›¾åƒç‰¹å¾ã€‚PRISMæ¨åŠ¨åäº‹å®ç”ŸæˆæŠ€æœ¯çš„å‘å±•ï¼Œå¹¶æœ‰åŠ©äºå¼€å‘æ›´ç¨³å¥çš„ä¸‹æ¸¸åˆ†ç±»å™¨ï¼Œä¸ºä¸´åºŠéƒ¨ç½²è§£å†³æ–¹æ¡ˆæä¾›å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒé¢†åŸŸçš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è™šå‡å…³è”ã€æ•°æ®ä¸å¹³è¡¡å’Œæ ‡æ³¨æ–‡æœ¬æœ‰é™ç­‰ã€‚</li>
<li>PRISMæ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆè¯­è¨€å¼•å¯¼çš„é«˜åˆ†è¾¨ç‡åŒ»ç–—å›¾åƒåäº‹å®ã€‚</li>
<li>PRISMèƒ½å¤Ÿé€‰æ‹©æ€§åœ°ä¿®æ”¹åŒ»å­¦å›¾åƒä¸­çš„è™šå‡å…³è”å’Œç–¾ç—…ç‰¹å¾ã€‚</li>
<li>PRISMé€šè¿‡æ·»åŠ å’Œç§»é™¤ç‰¹å®šå±æ€§ï¼ŒåŒæ—¶ä¿ç•™å…¶ä»–å›¾åƒç‰¹å¾ï¼Œå®ç°ç²¾å‡†ä¿®æ”¹ã€‚</li>
<li>PRISMæ¡†æ¶æ¨åŠ¨äº†åäº‹å®ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚</li>
<li>PRISMæœ‰åŠ©äºå¼€å‘æ›´ç¨³å¥çš„ä¸‹æ¸¸åˆ†ç±»å™¨ï¼Œä¸ºä¸´åºŠéƒ¨ç½²è§£å†³æ–¹æ¡ˆæä¾›æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04dae0498b203c0b9df097bfaf49e45b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f91729dbb831971a8f62fd3f1ab6c0da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a44f50a53eda689dd0f401684ffa8bdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e39f1904c80cc6822d2def7e029ceaf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7f6d825d0701082ffabc72abfc6b636.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Tool-or-Tutor-Experimental-evidence-from-AI-deployment-in-cancer-diagnosis"><a href="#Tool-or-Tutor-Experimental-evidence-from-AI-deployment-in-cancer-diagnosis" class="headerlink" title="Tool or Tutor? Experimental evidence from AI deployment in cancer   diagnosis"></a>Tool or Tutor? Experimental evidence from AI deployment in cancer   diagnosis</h2><p><strong>Authors:Vivianna Fang He, Sihan Li, Phanish Puranam</strong></p>
<p>Professionals increasingly use Artificial Intelligence (AI) to enhance their capabilities and assist with task execution. While prior research has examined these uses separately, their potential interaction remains underexplored. We propose that AI-driven training (â€œtutorâ€ effect) and AI-assisted task completion (â€œtoolâ€ effect) can be complementary and test this hypothesis in the context of lung cancer diagnosis. In a field experiment with 336 medical students, we manipulated AI deployment in training, in practice, and in both. Our findings reveal that while AI-integrated training and AI assistance independently improved diagnostic performance, their combination yielded the highest accuracy. These results underscore AIâ€™s dual role in enhancing human performance through both learning and real-time support, offering insights into AI deployment in professional settings where human expertise remains essential. </p>
<blockquote>
<p>ä¸“ä¸šäººå‘˜è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¥å¢å¼ºè‡ªèº«èƒ½åŠ›å¹¶è¾…åŠ©æ‰§è¡Œä»»åŠ¡ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»åˆ†åˆ«ç ”ç©¶äº†è¿™äº›ç”¨é€”ï¼Œä½†å®ƒä»¬ä¹‹é—´çš„æ½œåœ¨ç›¸äº’ä½œç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºAIé©±åŠ¨çš„åŸ¹è®­ï¼ˆâ€œå¯¼å¸ˆâ€æ•ˆåº”ï¼‰å’ŒAIè¾…åŠ©ä»»åŠ¡å®Œæˆï¼ˆâ€œå·¥å…·â€æ•ˆåº”ï¼‰å¯ä»¥äº’è¡¥ï¼Œå¹¶åœ¨è‚ºç™Œè¯Šæ–­çš„æƒ…å¢ƒä¸­æ£€éªŒè¿™ä¸€å‡è®¾ã€‚åœ¨ä¸€é¡¹æœ‰336ååŒ»å­¦å­¦ç”Ÿå‚ä¸çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬æ“ä½œäº†åŸ¹è®­ã€å®è·µå’Œä¸¤è€…éƒ½æ¶‰åŠçš„AIéƒ¨ç½²æƒ…å†µã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶AIé›†æˆåŸ¹è®­å’ŒAIè¾…åŠ©ç‹¬ç«‹æ”¹å–„äº†è¯Šæ–­æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„ç»“åˆå´è·å¾—äº†æœ€é«˜çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†äººå·¥æ™ºèƒ½é€šè¿‡å­¦ä¹ å’Œå®æ—¶æ”¯æŒå¢å¼ºäººç±»æ€§èƒ½çš„åŒé¢è§’è‰²ï¼Œä¸ºåœ¨ä¸“ä¸šç¯å¢ƒä¸­éƒ¨ç½²äººå·¥æ™ºèƒ½æä¾›äº†è§è§£ï¼Œåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼Œäººç±»ä¸“ä¸šçŸ¥è¯†ä»ç„¶è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16411v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŒ»å­¦å›¾åƒè¯Šæ–­é¢†åŸŸæ­£è¢«è¶Šæ¥è¶Šå¤šçš„ä¸“ä¸šäººå£«ç”¨æ¥æå‡èƒ½åŠ›å’Œè¾…åŠ©ä»»åŠ¡æ‰§è¡Œã€‚æœ¬ç ”ç©¶æ¢è®¨äº†AIåœ¨è®­ç»ƒï¼ˆâ€œå¯¼å¸ˆæ•ˆåº”â€ï¼‰å’Œå®ŒæˆåŒ»å­¦ä»»åŠ¡ï¼ˆâ€œå·¥å…·æ•ˆåº”â€ï¼‰æ–¹é¢çš„æ½œåŠ›äº’è¡¥æ€§ï¼Œå¹¶åœ¨è‚ºç™Œè¯Šæ–­çš„æƒ…å¢ƒä¸‹è¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒå‘ç°ï¼Œç»“åˆAIè®­ç»ƒå’ŒAIè¾…åŠ©çš„ç‹¬ç«‹æé«˜è¯Šæ–­æ€§èƒ½çš„åŒæ—¶ï¼Œä¸¤è€…çš„ç»“åˆèƒ½äº§ç”Ÿæœ€é«˜çš„å‡†ç¡®åº¦ã€‚è¿™è¡¨æ˜AIé€šè¿‡å­¦ä¹ å’Œå®æ—¶æ”¯æŒä¸¤æ–¹é¢å¢å¼ºäººç±»è¡¨ç°ï¼Œå¯¹äºåœ¨ä¸“ä¸šäººå£«ä»éœ€å‘æŒ¥é‡è¦ä½œç”¨çš„ä¸“ä¸šç¯å¢ƒä¸­éƒ¨ç½²AIå…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨åŒ»å­¦å›¾åƒè¯Šæ–­é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œèƒ½å¤Ÿæå‡ä¸“ä¸šäººå£«çš„èƒ½åŠ›å’Œè¾…åŠ©ä»»åŠ¡æ‰§è¡Œã€‚</li>
<li>AIåœ¨è®­ç»ƒå’Œä»»åŠ¡å®Œæˆæ–¹é¢çš„æ½œåŠ›å¯ä»¥äº’è¡¥ã€‚</li>
<li>åœ¨è‚ºç™Œè¯Šæ–­çš„æƒ…å¢ƒä¸‹ï¼ŒAIè®­ç»ƒå’ŒAIè¾…åŠ©çš„ç»“åˆäº§ç”Ÿäº†æœ€é«˜çš„è¯Šæ–­å‡†ç¡®åº¦ã€‚</li>
<li>AIé€šè¿‡å­¦ä¹ å’Œå®æ—¶æ”¯æŒä¸¤æ–¹é¢å¢å¼ºäººç±»è¡¨ç°ã€‚</li>
<li>AIéƒ¨ç½²å¯¹äºåœ¨ä¸“ä¸šäººå£«ä»éœ€å‘æŒ¥é‡è¦ä½œç”¨çš„ä¸“ä¸šç¯å¢ƒä¸­å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒAIçš„â€œå¯¼å¸ˆæ•ˆåº”â€å’Œâ€œå·¥å…·æ•ˆåº”â€å…±åŒä½œç”¨å¯æœ€å¤§åŒ–æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7f96b606dd3bd2c28496904c562fbd61.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis"><a href="#MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis" class="headerlink" title="MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis"></a>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis</h2><p><strong>Authors:Wei Dai, Jun Liu</strong></p>
<p>Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the &#96;&#96;Mambaâ€™â€™ model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mambaâ€™s potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models. </p>
<blockquote>
<p>é«˜æ•ˆè¯„ä¼°ä¸‰ç»´ï¼ˆ3Dï¼‰åŒ»å­¦å›¾åƒå¯¹åŒ»ç–—è¯Šæ–­å’Œæ²»ç–—å®è·µè‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰åœ¨åŒ»å­¦å›¾åƒåˆ†æå’Œè§£é‡Šæ–¹é¢çš„åº”ç”¨æœ‰æ‰€å¢é•¿ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ï¼Œé¢ä¸´é‡å¤§çš„è®¡ç®—æŒ‘æˆ˜ï¼Œä¿ƒä½¿æ¶æ„å‘å±•çš„éœ€æ±‚ã€‚æœ€è¿‘çš„åŠªåŠ›å¯¼è‡´äº†æ–°å‹æ¶æ„çš„å‡ºç°ï¼Œå¦‚ä½œä¸ºæ›¿ä»£ä¼ ç»ŸCNNæˆ–ViTçš„è§£å†³æ–¹æ¡ˆçš„â€œMambaâ€æ¨¡å‹ã€‚Mambaæ¨¡å‹åœ¨å¤„ç†ä¸€ç»´æ•°æ®çš„çº¿æ€§å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ä¸”è®¡ç®—éœ€æ±‚è¾ƒä½ã€‚ç„¶è€Œï¼ŒMambaåœ¨3DåŒ»å­¦å›¾åƒåˆ†ææ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œéšç€ç»´åº¦çš„å¢åŠ å¯èƒ½ä¼šé¢ä¸´é‡å¤§çš„è®¡ç®—æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†MobileViMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆåˆ†å‰²3DåŒ»å­¦å›¾åƒçš„æµçº¿åŒ–æ¶æ„ã€‚åœ¨MobileViMç½‘ç»œä¸­ï¼Œæˆ‘ä»¬å‘æ˜äº†ä¸€ç§æ–°çš„ç»´åº¦ç‹¬ç«‹æœºåˆ¶å’Œä¸€ç§åŒå‘éå†æ–¹æ³•ä¸åŸºäºè§†è§‰Mambaçš„æ¡†æ¶ç›¸ç»“åˆã€‚MobileViMè¿˜é‡‡ç”¨è·¨å°ºåº¦æ¡¥æ¥æŠ€æœ¯ï¼Œä»¥æé«˜ä¸åŒåŒ»å­¦æˆåƒæ¨¡å¼çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›å¢å¼ºåŠŸèƒ½ï¼ŒMobileViMåœ¨å•ä¸ªå›¾å½¢å¤„ç†å•å…ƒï¼ˆå³NVIDIA RTX 4090ï¼‰ä¸Šå®ç°äº†è¶…è¿‡æ¯ç§’90å¸§ï¼ˆFPSï¼‰çš„åˆ†å‰²é€Ÿåº¦ã€‚æ­¤æ€§èƒ½æ¯”ä½¿ç”¨ç›¸åŒè®¡ç®—èµ„æºçš„æœ€æ–°æ·±åº¦å­¦ä¹ æ¨¡å‹å¤„ç†3Då›¾åƒçš„é€Ÿåº¦å¿«24 FPSä»¥ä¸Šã€‚æ­¤å¤–ï¼Œå®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMobileViMçš„æ€§èƒ½å“è¶Šï¼Œåœ¨PENGWINã€BraTS2024ã€ATLASå’ŒToothfairy2æ•°æ®é›†ä¸Šçš„Diceç›¸ä¼¼åº¦å¾—åˆ†åˆ†åˆ«è¾¾åˆ°92.72%ã€86.69%ã€80.46%å’Œ77.43%ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13524v3">PDF</a> </p>
<p><strong>Summary</strong><br>    æå‡ºä¸€ç§é«˜æ•ˆçš„ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„MobileViMï¼Œé‡‡ç”¨ç»´åº¦ç‹¬ç«‹æœºåˆ¶ã€åŒå‘éå†æ–¹æ³•å’Œè·¨å°ºåº¦æ¡¥æ¥æŠ€æœ¯ï¼Œå®ç°å¿«é€Ÿè€Œç²¾ç¡®çš„åŒ»å­¦å›¾åƒåˆ†æï¼Œé€Ÿåº¦è¶…è¿‡ç°æœ‰æ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MobileViMæ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„æµçº¿å‹æ¶æ„ã€‚</li>
<li>è¯¥æ¶æ„é‡‡ç”¨æ–°çš„ç»´åº¦ç‹¬ç«‹æœºåˆ¶å’ŒåŒå‘éå†æ–¹æ³•ï¼Œç»“åˆè§†è§‰MambaåŸºç¡€æ¡†æ¶ã€‚</li>
<li>MobileViMçš„è·¨å°ºåº¦æ¡¥æ¥æŠ€æœ¯æé«˜äº†åœ¨å„ç§åŒ»å­¦æˆåƒæ¨¡å¼ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>MobileViMå®ç°äº†åœ¨å•ä¸ªå›¾å½¢å¤„ç†å•å…ƒä¸Šçš„æ¯ç§’è¶…è¿‡90å¸§çš„åˆ†å‰²é€Ÿåº¦ï¼Œæ¯”ç°æœ‰æ¨¡å‹å¿«24å¸§ä»¥ä¸Šã€‚</li>
<li>MobileViMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒDiceç›¸ä¼¼åº¦å¾—åˆ†åˆ†åˆ«ä¸º92.72%ã€86.69%ã€80.46%å’Œ77.43%ã€‚</li>
<li>è¯¥æ¶æ„è§£å†³äº†ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œå’Œè§†è§‰å˜å‹å™¨åœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†æä¸­çš„è®¡ç®—æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a19da823a39016902a1c548a5c88342b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b8d590b505813b80d6e6b5e2bad344e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5082a48f077c477e48276a57b6fe17b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d40734e73903ff1d635e972ebebfdebd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Vision-based-Geo-Localization-of-Future-Mars-Rotorcraft-in-Challenging-Illumination-Conditions"><a href="#Vision-based-Geo-Localization-of-Future-Mars-Rotorcraft-in-Challenging-Illumination-Conditions" class="headerlink" title="Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging   Illumination Conditions"></a>Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging   Illumination Conditions</h2><p><strong>Authors:Dario Pisanti, Robert Hewitt, Roland Brockers, Georgios Georgakis</strong></p>
<p>Planetary exploration using aerial assets has the potential for unprecedented scientific discoveries on Mars. While NASAâ€™s Mars helicopter Ingenuity proved flight in Martian atmosphere is possible, future Mars rotocrafts will require advanced navigation capabilities for long-range flights. One such critical capability is Map-based Localization (MbL) which registers an onboard image to a reference map during flight in order to mitigate cumulative drift from visual odometry. However, significant illumination differences between rotocraft observations and a reference map prove challenging for traditional MbL systems, restricting the operational window of the vehicle. In this work, we investigate a new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model for image registration that is more robust under large illumination differences than prior models. The system is supported by a custom simulation framework that uses real orbital maps to produce large amounts of realistic images of the Martian terrain. Comprehensive evaluations show that our proposed system outperforms prior MbL efforts in terms of localization accuracy under significant lighting and scale variations. Furthermore, we demonstrate the validity of our approach across a simulated Martian day. </p>
<blockquote>
<p>åˆ©ç”¨ç©ºä¸­èµ„æºè¿›è¡Œè¡Œæ˜Ÿæ¢ç´¢å…·æœ‰åœ¨ç«æ˜Ÿä¸Šè·å¾—å‰æ‰€æœªæœ‰çš„ç§‘å­¦å‘ç°çš„æ½œåŠ›ã€‚è™½ç„¶ç¾å›½å®‡èˆªå±€çš„ç«æ˜Ÿç›´å‡æœºâ€œæœºæ™ºå·â€è¯æ˜äº†åœ¨ç«æ˜Ÿå¤§æ°”ä¸­é£è¡Œæ˜¯å¯èƒ½çš„ï¼Œä½†æœªæ¥çš„ç«æ˜Ÿæ—‹ç¿¼æœºå°†éœ€è¦å…ˆè¿›çš„å¯¼èˆªèƒ½åŠ›ä»¥è¿›è¡Œè¿œç¨‹é£è¡Œã€‚å…¶ä¸­ä¸€é¡¹å…³é”®èƒ½åŠ›æ˜¯åœ°å›¾å®šä½ï¼ˆMbLï¼‰ï¼Œå®ƒåœ¨é£è¡Œè¿‡ç¨‹ä¸­å°†æœºè½½å›¾åƒæ³¨å†Œåˆ°å‚è€ƒåœ°å›¾ä¸Šï¼Œä»¥å‡å°‘è§†è§‰é‡Œç¨‹è®¡çš„ç´¯ç§¯æ¼‚ç§»ã€‚ç„¶è€Œï¼Œæ—‹ç¿¼æœºè§‚æµ‹ä¸å‚è€ƒåœ°å›¾ä¹‹é—´çš„æ˜¾è‘—ç…§æ˜å·®å¼‚ç»™ä¼ ç»ŸMbLç³»ç»Ÿå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œé™åˆ¶äº†è½¦è¾†çš„ä½œä¸šçª—å£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†ä¸€ç§æ–°çš„MbLç³»ç»Ÿï¼Œå¹¶æå‡ºGeo-LoFTRï¼Œè¿™æ˜¯ä¸€ç§è¾…åŠ©å‡ ä½•çš„æ·±åº¦å­¦ä¹ å›¾åƒæ³¨å†Œæ¨¡å‹ï¼Œä¸å…ˆå‰æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨è¾ƒå¤§çš„ç…§æ˜å·®å¼‚ä¸‹æ›´ä¸ºç¨³å¥ã€‚è¯¥ç³»ç»Ÿç”±ä¸€ä¸ªè‡ªå®šä¹‰ä»¿çœŸæ¡†æ¶æ”¯æŒï¼Œè¯¥æ¡†æ¶ä½¿ç”¨çœŸå®çš„è½¨é“åœ°å›¾ç”Ÿæˆå¤§é‡çœŸå®çš„ç«æ˜Ÿåœ°å½¢å›¾åƒã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿåœ¨ç…§æ˜å’Œæ¯”ä¾‹å°ºå˜åŒ–è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œåœ¨å®šä½ç²¾åº¦æ–¹é¢ä¼˜äºå…ˆå‰çš„MbLå·¥ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨¡æ‹Ÿçš„ç«æ˜Ÿæ—¥å‘¨æœŸä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09795v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨ç«æ˜Ÿä¸Šï¼Œåˆ©ç”¨ç©ºä¸­èµ„æºè¿›è¡Œè¡Œæ˜Ÿæ¢ç´¢å…·æœ‰å·¨å¤§çš„ç§‘å­¦å‘ç°æ½œåŠ›ã€‚NASAçš„ç«æ˜Ÿç›´å‡æœºâ€œæœºæ™ºå·â€è¯æ˜äº†åœ¨ç«æ˜Ÿå¤§æ°”ä¸­é£è¡Œçš„å¯è¡Œæ€§ï¼Œä½†æœªæ¥çš„ç«æ˜Ÿæ—‹ç¿¼é£è¡Œå™¨éœ€è¦å…ˆè¿›çš„å¯¼èˆªèƒ½åŠ›ä»¥å®ç°è¿œç¨‹é£è¡Œã€‚å…¶ä¸­ä¸€é¡¹å…³é”®èƒ½åŠ›æ˜¯åŸºäºåœ°å›¾çš„å®šä½ï¼ˆMbLï¼‰ï¼Œå®ƒåœ¨é£è¡Œè¿‡ç¨‹ä¸­å°†æœºä¸Šå›¾åƒæ³¨å†Œåˆ°å‚è€ƒåœ°å›¾ä¸Šï¼Œä»¥å‡è½»ç”±è§†è§‰é‡Œç¨‹è®¡å¼•èµ·çš„ç´¯ç§¯æ¼‚ç§»é—®é¢˜ã€‚ç„¶è€Œï¼Œæ—‹ç¿¼é£è¡Œå™¨è§‚æµ‹å’Œå‚è€ƒåœ°å›¾ä¹‹é—´çš„æ˜¾è‘—ç…§æ˜å·®å¼‚å¯¹ä¼ ç»Ÿçš„MbLç³»ç»Ÿæ„æˆæŒ‘æˆ˜ï¼Œé™åˆ¶äº†è½¦è¾†çš„æ“ä½œçª—å£ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ä¸€ç§æ–°å‹çš„MbLç³»ç»Ÿï¼Œå¹¶æå‡ºGeo-LoFTRï¼Œè¿™æ˜¯ä¸€ç§è¾…åŠ©å‡ ä½•çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºå›¾åƒæ³¨å†Œï¼Œèƒ½å¤Ÿåœ¨è¾ƒå¤§çš„ç…§æ˜å·®å¼‚ä¸‹æ¯”å…ˆå‰æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ã€‚è¯¥ç³»ç»Ÿå¾—åˆ°äº†ä¸€ä¸ªè‡ªå®šä¹‰ä»¿çœŸæ¡†æ¶çš„æ”¯æŒï¼Œè¯¥æ¡†æ¶ä½¿ç”¨çœŸå®çš„è½¨é“åœ°å›¾ç”Ÿæˆå¤§é‡é€¼çœŸçš„ç«æ˜Ÿåœ°å½¢å›¾åƒã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ç…§æ˜å’Œæ¯”ä¾‹å˜åŒ–è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œå®šä½ç²¾åº¦ä¼˜äºå…ˆå‰çš„MbLåŠªåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨¡æ‹Ÿçš„ç«æ˜Ÿæ—¥å‘¨æœŸå†…éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡Œæ˜Ÿæ¢ç´¢åˆ©ç”¨ç©ºä¸­èµ„æºå…·æœ‰å·¨å¤§çš„ç§‘å­¦å‘ç°æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç«æ˜Ÿä¸Šã€‚</li>
<li>NASAçš„ç«æ˜Ÿç›´å‡æœºâ€œæœºæ™ºå·â€è¯æ˜äº†åœ¨ç«æ˜Ÿå¤§æ°”ä¸­é£è¡Œçš„å¯è¡Œæ€§ã€‚</li>
<li>æœªæ¥çš„ç«æ˜Ÿæ—‹ç¿¼é£è¡Œå™¨éœ€è¦å…ˆè¿›çš„å¯¼èˆªèƒ½åŠ›ï¼Œå…¶ä¸­å…³é”®ä¹‹ä¸€æ˜¯åŸºäºåœ°å›¾çš„å®šä½ï¼ˆMbLï¼‰ã€‚</li>
<li>æ—‹ç¿¼é£è¡Œå™¨è§‚æµ‹å’Œå‚è€ƒåœ°å›¾ä¹‹é—´çš„ç…§æ˜å·®å¼‚å¯¹ä¼ ç»Ÿçš„MbLç³»ç»Ÿæ„æˆæŒ‘æˆ˜ã€‚</li>
<li>Geo-LoFTRæ˜¯ä¸€ç§æ–°å‹çš„MbLç³»ç»Ÿï¼Œé€šè¿‡è¾…åŠ©å‡ ä½•çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå›¾åƒæ³¨å†Œï¼Œè¡¨ç°æ›´å¼ºçš„ç¨³å¥æ€§ã€‚</li>
<li>è‡ªå®šä¹‰ä»¿çœŸæ¡†æ¶æ”¯æŒGeo-LoFTRç³»ç»Ÿï¼Œä½¿ç”¨çœŸå®çš„è½¨é“åœ°å›¾ç”Ÿæˆå¤§é‡é€¼çœŸçš„ç«æ˜Ÿåœ°å½¢å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a9b3653f319014b25fbc5baf04ea8212.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc42af83f315c318560f2893e6708db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0360e432cb5a466240dc46b94a446dc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5a62e710921343120fbafc1d981bfaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ccb5b3aa052a1b9d722071b87f439dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e757a27bbb268b509dec655634c3b03.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Rethinking-High-speed-Image-Reconstruction-Framework-with-Spike-Camera"><a href="#Rethinking-High-speed-Image-Reconstruction-Framework-with-Spike-Camera" class="headerlink" title="Rethinking High-speed Image Reconstruction Framework with Spike Camera"></a>Rethinking High-speed Image Reconstruction Framework with Spike Camera</h2><p><strong>Authors:Kang Chen, Yajing Zheng, Tiejun Huang, Zhaofei Yu</strong></p>
<p>Spike cameras, as innovative neuromorphic devices, generate continuous spike streams to capture high-speed scenes with lower bandwidth and higher dynamic range than traditional RGB cameras. However, reconstructing high-quality images from the spike input under low-light conditions remains challenging. Conventional learning-based methods often rely on the synthetic dataset as the supervision for training. Still, these approaches falter when dealing with noisy spikes fired under the low-light environment, leading to further performance degradation in the real-world dataset. This phenomenon is primarily due to inadequate noise modelling and the domain gap between synthetic and real datasets, resulting in recovered images with unclear textures, excessive noise, and diminished brightness. To address these challenges, we introduce a novel spike-to-image reconstruction framework SpikeCLIP that goes beyond traditional training paradigms. Leveraging the CLIP modelâ€™s powerful capability to align text and images, we incorporate the textual description of the captured scene and unpaired high-quality datasets as the supervision. Our experiments on real-world low-light datasets U-CALTECH and U-CIFAR demonstrate that SpikeCLIP significantly enhances texture details and the luminance balance of recovered images. Furthermore, the reconstructed images are well-aligned with the broader visual features needed for downstream tasks, ensuring more robust and versatile performance in challenging environments. </p>
<blockquote>
<p>è„‰å†²ç›¸æœºä½œä¸ºä¸€ç§åˆ›æ–°çš„ç¥ç»å½¢æ€è®¾å¤‡ï¼Œèƒ½å¤Ÿäº§ç”Ÿè¿ç»­çš„è„‰å†²æµï¼Œä»¥ä½äºä¼ ç»ŸRGBç›¸æœºçš„å¸¦å®½å’Œæ›´é«˜çš„åŠ¨æ€èŒƒå›´æ¥æ•æ‰é«˜é€Ÿåœºæ™¯ã€‚ç„¶è€Œï¼Œåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹ä»è„‰å†²è¾“å…¥é‡å»ºé«˜è´¨é‡å›¾åƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿçš„åŸºäºå­¦ä¹ çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºåˆæˆæ•°æ®é›†ä½œä¸ºè®­ç»ƒè¿‡ç¨‹çš„ç›‘ç£ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†ä½å…‰ç¯å¢ƒä¸‹äº§ç”Ÿçš„å™ªå£°è„‰å†²æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´åœ¨å®é™…æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¿›ä¸€æ­¥ä¸‹é™ã€‚è¿™ç§ç°è±¡çš„ä¸»è¦åŸå› æ˜¯å™ªå£°å»ºæ¨¡ä¸è¶³ä»¥åŠåˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¹‹é—´çš„åŸŸå·®è·ï¼Œå¯¼è‡´æ¢å¤åçš„å›¾åƒçº¹ç†ä¸æ¸…æ™°ã€å™ªå£°è¿‡å¤šä»¥åŠäº®åº¦é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è„‰å†²åˆ°å›¾åƒé‡å»ºæ¡†æ¶SpikeCLIPï¼Œå®ƒè¶…è¶Šäº†ä¼ ç»Ÿçš„è®­ç»ƒæ¨¡å¼ã€‚æˆ‘ä»¬å€ŸåŠ©CLIPæ¨¡å‹çš„å¼ºå¤§æ–‡æœ¬å’Œå›¾åƒå¯¹é½èƒ½åŠ›ï¼Œå°†æ•è·åœºæ™¯çš„æ–‡æœ¬æè¿°å’Œæœªé…å¯¹çš„é«˜è´¨é‡æ•°æ®é›†ä½œä¸ºç›‘ç£ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨å®é™…ä½å…‰æ•°æ®é›†U-CALTECHå’ŒU-CIFARä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSpikeCLIPæ˜¾è‘—æé«˜äº†æ¢å¤å›¾åƒçš„çº¹ç†ç»†èŠ‚å’Œäº®åº¦å¹³è¡¡ã€‚æ­¤å¤–ï¼Œé‡å»ºçš„å›¾åƒä¸ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„æ›´å¹¿æ³›çš„è§†è§‰ç‰¹å¾å¯¹é½ï¼Œç¡®ä¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­è¡¨ç°å‡ºæ›´ç¨³å¥å’Œå¤šåŠŸèƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04477v2">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºè„‰å†²ç›¸æœºåœ¨ä½å…‰ç…§ç¯å¢ƒä¸‹å›¾åƒé‡å»ºçš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†SpikeCLIPæ¡†æ¶ï¼Œåˆ©ç”¨CLIPæ¨¡å‹çš„æ–‡æœ¬ä¸å›¾åƒå¯¹é½èƒ½åŠ›ï¼Œç»“åˆåœºæ™¯æ–‡æœ¬æè¿°å’Œé«˜è´¨é‡æ•°æ®é›†è¿›è¡Œç›‘ç£ï¼Œè§£å†³äº†ä¼ ç»Ÿå­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹å™ªå£°è„‰å†²æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒSpikeCLIPèƒ½å¤Ÿæ˜¾è‘—æé«˜ä½å…‰ç…§ç¯å¢ƒä¸‹é‡å»ºå›¾åƒçš„çº¹ç†ç»†èŠ‚å’Œäº®åº¦å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‰å†²ç›¸æœºé€šè¿‡è¿ç»­è„‰å†²æµæ•æ‰é«˜é€Ÿåœºæ™¯ï¼Œç›¸æ¯”ä¼ ç»ŸRGBç›¸æœºå…·æœ‰æ›´ä½çš„å¸¦å®½å’Œæ›´é«˜çš„åŠ¨æ€èŒƒå›´ã€‚</li>
<li>ä½å…‰ç…§ç¯å¢ƒä¸‹ä»è„‰å†²è¾“å…¥é‡å»ºé«˜è´¨é‡å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å¸¸è§„çš„å­¦ä¹ å‹æ–¹æ³•ä¾èµ–äºåˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨é¢å¯¹ä½å…‰ç…§ç¯å¢ƒä¸‹å™ªå£°è„‰å†²æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>SpikeCLIPæ¡†æ¶è¶…è¶Šäº†ä¼ ç»Ÿè®­ç»ƒæ¨¡å¼ï¼Œåˆ©ç”¨CLIPæ¨¡å‹çš„æ–‡æœ¬ä¸å›¾åƒå¯¹é½èƒ½åŠ›ã€‚</li>
<li>SpikeCLIPç»“åˆåœºæ™¯æ–‡æœ¬æè¿°å’Œæœªé…å¯¹çš„é«˜è´¨é‡æ•°æ®é›†è¿›è¡Œç›‘ç£ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œçš„ä½å…‰ç…§æ•°æ®é›†U-CALTECHå’ŒU-CIFARä¸Šçš„å®éªŒè¯æ˜ï¼ŒSpikeCLIPèƒ½æ˜¾è‘—æé«˜é‡å»ºå›¾åƒçš„çº¹ç†ç»†èŠ‚å’Œäº®åº¦å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04477">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fce9329c288cddf767380a28522750c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff285dcbe3c326a4b4963101d2742db8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a10e4159d1b7fa4bbfea0d90e79d465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83d8f95fe9a364f186a41fc9477dff9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee32d7c171f4e38e519776b99054cd60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed5bb3a3df28409141efaa8c545f9b76.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder"><a href="#XLSTM-HVED-Cross-Modal-Brain-Tumor-Segmentation-and-MRI-Reconstruction-Method-Using-Vision-XLSTM-and-Heteromodal-Variational-Encoder-Decoder" class="headerlink" title="XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction   Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder"></a>XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction   Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder</h2><p><strong>Authors:Shenghao Zhu, Yifei Chen, Shuo Jiang, Weihong Chen, Chang Liu, Yuanhan Wang, Xu Chen, Yifan Ke, Feiwei Qin, Changmiao Wang, Zhu Zhu</strong></p>
<p>Neurogliomas are among the most aggressive forms of cancer, presenting considerable challenges in both treatment and monitoring due to their unpredictable biological behavior. Magnetic resonance imaging (MRI) is currently the preferred method for diagnosing and monitoring gliomas. However, the lack of specific imaging techniques often compromises the accuracy of tumor segmentation during the imaging process. To address this issue, we introduce the XLSTM-HVED model. This model integrates a hetero-modal encoder-decoder framework with the Vision XLSTM module to reconstruct missing MRI modalities. By deeply fusing spatial and temporal features, it enhances tumor segmentation performance. The key innovation of our approach is the Self-Attention Variational Encoder (SAVE) module, which improves the integration of modal features. Additionally, it optimizes the interaction of features between segmentation and reconstruction tasks through the Squeeze-Fusion-Excitation Cross Awareness (SFECA) module. Our experiments using the BraTS 2024 dataset demonstrate that our model significantly outperforms existing advanced methods in handling cases where modalities are missing. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED">https://github.com/Quanato607/XLSTM-HVED</a>. </p>
<blockquote>
<p>ç¥ç»èƒ¶è´¨ç˜¤æ˜¯æœ€å…·ä¾µè¢­æ€§çš„ç™Œç—‡ä¹‹ä¸€ï¼Œç”±äºå…¶ä¸å¯é¢„æµ‹çš„ç”Ÿç‰©è¡Œä¸ºï¼Œä¸ºæ²»ç–—å’Œç›‘æµ‹å¸¦æ¥äº†ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ç›®å‰ï¼Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯è¯Šæ–­å’Œæ²»ç–—èƒ¶è´¨ç˜¤çš„é¦–é€‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç¼ºä¹ç‰¹å®šçš„æˆåƒæŠ€æœ¯å¾€å¾€ä¼šå½±å“æˆåƒè¿‡ç¨‹ä¸­è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†XLSTM-HVEDæ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¼‚æ¨¡å¼ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å’ŒVision XLSTMæ¨¡å—ï¼Œä»¥é‡å»ºç¼ºå¤±çš„MRIæ¨¡å¼ã€‚é€šè¿‡æ·±åº¦èåˆç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼Œæé«˜äº†è‚¿ç˜¤åˆ†å‰²çš„æ€§èƒ½ã€‚æˆ‘ä»¬æ–¹æ³•çš„å…³é”®åˆ›æ–°ç‚¹æ˜¯è‡ªæ³¨æ„åŠ›å˜åˆ†ç¼–ç å™¨ï¼ˆSAVEï¼‰æ¨¡å—ï¼Œå®ƒæ”¹è¿›äº†æ¨¡å¼ç‰¹å¾çš„èåˆã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡Squeeze-Fusion-Excitation Cross Awarenessï¼ˆSFECAï¼‰æ¨¡å—ä¼˜åŒ–äº†åˆ†å‰²å’Œé‡å»ºä»»åŠ¡ä¹‹é—´çš„ç‰¹å¾äº¤äº’ã€‚æˆ‘ä»¬ä½¿ç”¨BraTS 2024æ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤„ç†ç¼ºå¤±æ¨¡å¼çš„æƒ…å†µæ—¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„é«˜çº§æ–¹æ³•ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/Quanato607/XLSTM-HVED%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/Quanato607/XLSTM-HVEDæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07804v3">PDF</a> 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç¥ç»èƒ¶è´¨ç˜¤çš„æ²»ç–—å’Œç›‘æµ‹æŒ‘æˆ˜ï¼Œä»¥åŠç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨è¯Šæ–­ä¸ç›‘æµ‹ä¸­çš„ä½œç”¨ã€‚ä¸ºè§£å†³MRIæˆåƒè¿‡ç¨‹ä¸­å› ç¼ºä¹ç‰¹å®šæˆåƒæŠ€æœ¯è€Œå½±å“è‚¿ç˜¤åˆ†å‰²å‡†ç¡®æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†XLSTM-HVEDæ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¼‚æ¨¡æ€ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ä¸Vision XLSTMæ¨¡å—ï¼Œå¯é‡å»ºç¼ºå¤±çš„MRIæ¨¡æ€ï¼Œé€šè¿‡æ·±åº¦èåˆæ—¶ç©ºç‰¹å¾æé«˜è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºè‡ªæ³¨æ„åŠ›å˜åˆ†ç¼–ç å™¨ï¼ˆSAVEï¼‰æ¨¡å—ï¼Œå¯æ”¹è¿›æ¨¡æ€ç‰¹å¾çš„æ•´åˆã€‚åŒæ—¶ï¼Œé€šè¿‡Squeeze-Fusion-Excitation Cross Awarenessï¼ˆSFECAï¼‰æ¨¡å—ä¼˜åŒ–åˆ†å‰²ä¸é‡å»ºä»»åŠ¡ä¹‹é—´çš„ç‰¹å¾äº¤äº’ã€‚ä½¿ç”¨BraTS 2024æ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µæ—¶æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»èƒ¶è´¨ç˜¤æ²»ç–—ä¸ç›‘æµ‹å­˜åœ¨æŒ‘æˆ˜ï¼ŒMRIæ˜¯ç›®å‰é¦–é€‰çš„è¯Šæ–­ä¸ç›‘æµ‹æ–¹æ³•ã€‚</li>
<li>ç¼ºä¹ç‰¹å®šæˆåƒæŠ€æœ¯ä¼šå½±å“MRIæˆåƒè¿‡ç¨‹ä¸­çš„è‚¿ç˜¤åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>XLSTM-HVEDæ¨¡å‹é€šè¿‡ç»“åˆå¼‚æ¨¡æ€ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ä¸Vision XLSTMæ¨¡å—ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹å¯é‡å»ºç¼ºå¤±çš„MRIæ¨¡æ€ï¼Œå¹¶é€šè¿‡æ·±åº¦èåˆæ—¶ç©ºç‰¹å¾æé«˜è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›å˜åˆ†ç¼–ç å™¨ï¼ˆSAVEï¼‰æ¨¡å—æ˜¯æ¨¡å‹çš„å…³é”®åˆ›æ–°ï¼Œå¯æ”¹è¿›æ¨¡æ€ç‰¹å¾çš„æ•´åˆã€‚</li>
<li>SFECAæ¨¡å—ä¼˜åŒ–åˆ†å‰²ä¸é‡å»ºä»»åŠ¡ä¹‹é—´çš„ç‰¹å¾äº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07bf78a820d9382ff8bae4344379d0fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da60db8443b675a68c3c47ba18158641.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-970e1b1c8819dbfa82c0b6a5bd066932.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ac994a06936a3646fed8efd39998301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-078694a99b64803abd171ca0ae8cce30.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-51b6b444117a3bd724c529fa9d923b9d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  Good practices for evaluation of synthesized speech
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  DualDiff+ Dual-Branch Diffusion for High-Fidelity Video Generation with   Reward Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
