<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  Process-based Self-Rewarding Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1c0dd782a7ff87c98922cb57986c014d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-07-æ›´æ–°"><a href="#2025-03-07-æ›´æ–°" class="headerlink" title="2025-03-07 æ›´æ–°"></a>2025-03-07 æ›´æ–°</h1><h2 id="Process-based-Self-Rewarding-Language-Models"><a href="#Process-based-Self-Rewarding-Language-Models" class="headerlink" title="Process-based Self-Rewarding Language Models"></a>Process-based Self-Rewarding Language Models</h2><p><strong>Authors:Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, Yeyun Gong</strong></p>
<p>Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMsâ€™ performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œé‡‡ç”¨äººå·¥æ ‡æ³¨çš„åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†ä»å—é™äºäººç±»æ€§èƒ½çš„ä¸Šé™ã€‚å› æ­¤ï¼Œæå‡ºäº†è‡ªæˆ‘å¥–åŠ±æ–¹æ³•ï¼Œå³å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¥–åŠ±è‡ªèº«è¾“å‡ºç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªæˆ‘å¥–åŠ±èŒƒå¼åœ¨æ•°å­¦æ¨ç†åœºæ™¯ä¸­å¹¶ä¸æœ‰æ•ˆï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±ç®¡é“è¯­è¨€æ¨¡å‹ï¼Œå¼•å…¥äº†é•¿æœŸæ¨ç†ã€é€æ­¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜å’ŒåŸºäºè‡ªæˆ‘å¥–åŠ±èŒƒå¼å†…çš„é€æ­¥åå¥½ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼é€šè¿‡åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±è¿­ä»£æˆåŠŸæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼Œè¯æ˜äº†è‡ªæˆ‘å¥–åŠ±åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ–¹é¢å…·æœ‰è¶…è¶Šäººç±»èƒ½åŠ›çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03746v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåœºæ™¯ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½å¹¶å—åˆ°äººç±»è¡¨ç°ä¸Šé™çš„é™åˆ¶ï¼Œé‡‡ç”¨äº†äººç±»æ ‡æ³¨åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªæˆ‘å¥–åŠ±æœºåˆ¶åœ¨æ•°å­¦æ¨ç†åœºæ™¯ä¸­å¹¶ä¸æœ‰æ•ˆï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºæµç¨‹çš„è‡ªæˆ‘å¥–åŠ±ç®¡é“ï¼Œå¼•å…¥é•¿æœŸæ¨ç†ã€é€æ­¥çš„è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜å’Œé€æ­¥åå¥½ä¼˜åŒ–ç­‰æœºåˆ¶ã€‚é€šè¿‡åŸºäºæµç¨‹çš„è¿­ä»£è‡ªæˆ‘å¥–åŠ±ï¼ŒæˆåŠŸæé«˜äº†è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºè‡ªæˆ‘å¥–åŠ±å®ç°è¶…è¶Šäººç±»èƒ½åŠ›çš„è¯­è¨€æ¨ç†çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºä¸åŒåœºæ™¯ã€‚</li>
<li>äººç±»æ ‡æ³¨åå¥½æ•°æ®ç”¨äºè®­ç»ƒï¼Œä»¥æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†ä»å—äººç±»è¡¨ç°ä¸Šé™çš„é™åˆ¶ã€‚</li>
<li>ç°æœ‰è‡ªæˆ‘å¥–åŠ±æœºåˆ¶åœ¨æ•°å­¦æ¨ç†åœºæ™¯ä¸­æ•ˆæœä¸ä½³ï¼Œç”šè‡³å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†åŸºäºæµç¨‹çš„è‡ªæˆ‘å¥–åŠ±ç®¡é“ï¼ŒåŒ…æ‹¬é•¿æœŸæ¨ç†ã€é€æ­¥çš„è¯­è¨€æ¨¡å‹è¯„ä¼°ä»¥åŠåå¥½ä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡è¿­ä»£åŸºäºæµç¨‹çš„è‡ªæˆ‘å¥–åŠ±ï¼ŒæˆåŠŸæé«˜äº†è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºè‡ªæˆ‘å¥–åŠ±çš„è¯­è¨€æ¨¡å‹æœ‰æ½œåŠ›å®ç°è¶…è¶Šäººç±»èƒ½åŠ›çš„è¯­è¨€æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-35bf432485c37194abdcfec20151d8df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f140123d2474bcf923dbc6e1d8f6681.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a09cfcf28df299b0de886a4e62b47dd3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-LLM-Safety-Alignment-with-Dual-Objective-Optimization"><a href="#Improving-LLM-Safety-Alignment-with-Dual-Objective-Optimization" class="headerlink" title="Improving LLM Safety Alignment with Dual-Objective Optimization"></a>Improving LLM Safety Alignment with Dual-Objective Optimization</h2><p><strong>Authors:Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song</strong></p>
<p>Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at <a target="_blank" rel="noopener" href="https://github.com/wicai24/DOOR-Alignment">https://github.com/wicai24/DOOR-Alignment</a> </p>
<blockquote>
<p>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæ—¶å®‰å…¨å¯¹é½æŠ€æœ¯ä»ç„¶å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»çš„å½±å“ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä¸€ç§å¹¿æ³›éƒ¨ç½²çš„å¯¹é½æ–¹æ³•ï¼Œå…¶åœ¨å®éªŒå’Œç†è®ºèƒŒæ™¯ä¸‹å‡å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå…¶æŸå¤±å‡½æ•°å¯¹äºæ‹’ç»å­¦ä¹ è€Œè¨€å¹¶ä¸ç†æƒ³ã€‚é€šè¿‡åŸºäºæ¢¯åº¦çš„åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºè¿™äº›ä¸è¶³ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å®‰å…¨å¯¹é½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†DPOç›®æ ‡åˆ†è§£ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰é²æ£’æ‹’ç»è®­ç»ƒï¼Œå³ä½¿éƒ¨åˆ†ä¸å®‰å…¨ç”Ÿæˆï¼Œä¹Ÿé¼“åŠ±æ‹’ç»ï¼›ï¼ˆ2ï¼‰æœ‰é’ˆå¯¹æ€§åœ°å¿˜è®°æœ‰å®³çŸ¥è¯†ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†LLMå¯¹å„ç§è¶Šç‹±æ”»å‡»çš„é²æ£’æ€§ï¼ŒåŒ…æ‹¬è·¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„é¢„å¡«å……ã€åç¼€å’Œå¤šè½®æ”»å‡»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§åŸºäºå¥–åŠ±çš„ä»¤ç‰Œçº§åŠ æƒæœºåˆ¶æ¥å¼ºè°ƒæ‹’ç»å­¦ä¹ çš„å…³é”®æ‹’ç»ä»¤ç‰Œçš„æ–¹æ³•ï¼Œè¿™è¿›ä¸€æ­¥æé«˜äº†å¯¹å¯¹æŠ—æ€§åˆ©ç”¨çš„é²æ£’æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå¯¹è¶Šç‹±æ”»å‡»çš„é²æ£’æ€§ä¸è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä»¤ç‰Œåˆ†å¸ƒè½¬å˜ä»¥åŠæ‹’ç»å’Œæœ‰å®³ä»¤ç‰Œçš„å†…éƒ¨è¡¨ç¤ºæœ‰å…³ï¼Œä¸ºæœªæ¥LLMå®‰å…¨å¯¹é½çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æ–¹å‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wicai24/DOOR-Alignment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wicai24/DOOR-Alignmentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03710v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒæ—¶çš„å®‰å…¨å¯¹é½æŠ€æœ¯ä»æ˜“å—åˆ°jailbreakæ”»å‡»çš„å½±å“ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨çš„å¯¹é½æ–¹æ³•ï¼Œä½†åœ¨å®éªŒå’Œç†è®ºèƒŒæ™¯ä¸‹éƒ½å­˜åœ¨å±€é™æ€§ï¼Œå…¶æŸå¤±å‡½æ•°å¯¹äºæ‹’ç»å­¦ä¹ å¹¶ä¸ç†æƒ³ã€‚é€šè¿‡æ¢¯åº¦åˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†è¿™äº›ä¸è¶³ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å®‰å…¨å¯¹é½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†DPOç›®æ ‡åˆ†ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰é²æ£’çš„æ‹’ç»è®­ç»ƒï¼Œå³ä½¿åœ¨éƒ¨åˆ†ä¸å®‰å…¨ç”Ÿæˆçš„æƒ…å†µä¸‹ä¹Ÿé¼“åŠ±æ‹’ç»ï¼›ï¼ˆ2ï¼‰æœ‰é’ˆå¯¹æ€§çš„æ¶ˆé™¤æœ‰å®³çŸ¥è¯†ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†LLMå¯¹å„ç§jailbreakæ”»å‡»çš„é²æ£’æ€§ï¼ŒåŒ…æ‹¬é¢„å¡«å……ã€åç¼€å’Œå¤šè½®æ”»å‡»ï¼Œæ¶µç›–äº†åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–åœºæ™¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥åŸºäºå¥–åŠ±çš„ä»¤ç‰Œçº§åŠ æƒæœºåˆ¶æ¥å¼ºè°ƒå…³é”®çš„æ‹’ç»ä»¤ç‰Œï¼Œè¿›ä¸€æ­¥æé«˜å¯¹æŠ—æ¶æ„æ”»å‡»çš„ç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶è¿˜è¡¨æ˜ï¼Œå¯¹jailbreakæ”»å‡»çš„ç¨³å¥æ€§ä¸è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä»¤ç‰Œåˆ†å¸ƒè½¬å˜ä»¥åŠæ‹’ç»å’Œæœ‰å®³ä»¤ç‰Œçš„å†…éƒ¨è¡¨ç¤ºæœ‰å…³ï¼Œä¸ºæœªæ¥LLMå®‰å…¨å¯¹é½çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMç°æœ‰çš„è®­ç»ƒæ—¶é—´å®‰å…¨å¯¹é½æŠ€æœ¯å®¹æ˜“å—åˆ°jailbreakæ”»å‡»çš„å½±å“ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•åœ¨æ‹’ç»å­¦ä¹ æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ”¹è¿›çš„å®‰å…¨å¯¹é½æ–¹æ³•ï¼ŒåŒ…æ‹¬é²æ£’æ‹’ç»è®­ç»ƒå’Œæœ‰å®³çŸ¥è¯†çš„æœ‰é’ˆå¯¹æ€§çš„æ¶ˆé™¤ã€‚</li>
<li>æ–°æ–¹æ³•æ˜¾è‘—æé«˜LLMå¯¹å„ç§jailbreakæ”»å‡»çš„é²æ£’æ€§ï¼Œæ¶µç›–åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–åœºæ™¯ã€‚</li>
<li>å¼•å…¥åŸºäºå¥–åŠ±çš„ä»¤ç‰Œçº§åŠ æƒæœºåˆ¶æ¥å¼ºè°ƒå…³é”®çš„æ‹’ç»ä»¤ç‰Œã€‚</li>
<li>å¯¹jailbreakæ”»å‡»çš„ç¨³å¥æ€§ä¸è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä»¤ç‰Œåˆ†å¸ƒè½¬å˜æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a315615f74d6f21302d942ab01fad146.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0525f5f9412251de311303dfab8f26cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-606b348b91950dc35d5c9e6d61f3635b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07200417e4d883120e50f869825a7dd1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Developing-and-Utilizing-a-Large-Scale-Cantonese-Dataset-for-Multi-Tasking-in-Large-Language-Models"><a href="#Developing-and-Utilizing-a-Large-Scale-Cantonese-Dataset-for-Multi-Tasking-in-Large-Language-Models" class="headerlink" title="Developing and Utilizing a Large-Scale Cantonese Dataset for   Multi-Tasking in Large Language Models"></a>Developing and Utilizing a Large-Scale Cantonese Dataset for   Multi-Tasking in Large Language Models</h2><p><strong>Authors:Jiyue Jiang, Alfred Kar Yin Truong, Yanyu Chen, Qinghang Bao, Sheng Wang, Pengan Chen, Jiuming Wang, Lingpeng Kong, Yu Li, Chuan Wu</strong></p>
<p>High-quality data resources play a crucial role in learning large language models (LLMs), particularly for low-resource languages like Cantonese. Despite having more than 85 million native speakers, Cantonese is still considered a low-resource language in the field of natural language processing (NLP) due to factors such as the dominance of Mandarin, lack of cohesion within the Cantonese-speaking community, diversity in character encoding and input methods, and the tendency of overseas Cantonese speakers to prefer using English. In addition, rich colloquial vocabulary of Cantonese, English loanwords, and code-switching characteristics add to the complexity of corpus collection and processing. To address these challenges, we collect Cantonese texts from a variety of sources, including open source corpora, Hong Kong-specific forums, Wikipedia, and Common Crawl data. We conduct rigorous data processing through language filtering, quality filtering, content filtering, and de-duplication steps, successfully constructing a high-quality Cantonese corpus of over 2 billion tokens for training large language models. We further refined the model through supervised fine-tuning (SFT) on curated Cantonese tasks, enhancing its ability to handle specific applications. Upon completion of the training, the model achieves state-of-the-art (SOTA) performance on four Cantonese benchmarks. After training on our dataset, the model also exhibits improved performance on other mainstream language tasks. </p>
<blockquote>
<p>é«˜è´¨é‡çš„æ•°æ®èµ„æºåœ¨å­¦ä¹ å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¤è¯­è¿™ç§ä½èµ„æºè¯­è¨€é¢†åŸŸã€‚å°½ç®¡ç²¤è¯­æœ‰è¶…è¿‡8500ä¸‡æ¯è¯­è€…ï¼Œä½†ç”±äºæ™®é€šè¯çš„ä¸»å¯¼åœ°ä½ã€ç²¤è¯­ç¤¾ç¾¤å†…éƒ¨çš„ç¼ºä¹å‡èšåŠ›ã€å­—ç¬¦ç¼–ç å’Œè¾“å…¥æ–¹æ³•çš„å¤šæ ·æ€§ä»¥åŠæµ·å¤–ç²¤è¯­ä½¿ç”¨è€…æ›´å€¾å‘äºä½¿ç”¨è‹±è¯­ç­‰å› ç´ ï¼Œç²¤è¯­åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸä»è¢«è§†ä¸ºä¸€ç§ä½èµ„æºè¯­è¨€ã€‚æ­¤å¤–ï¼Œç²¤è¯­çš„ä¸°å¯Œå£è¯­è¯æ±‡ã€è‹±è¯­å€Ÿè¯å’Œä»£ç è½¬æ¢ç‰¹æ€§å¢åŠ äº†è¯­æ–™åº“æ”¶é›†å’Œå¤„ç†å·¥ä½œçš„å¤æ‚æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»å„ç§æ¥æºæ”¶é›†ç²¤è¯­æ–‡æœ¬ï¼ŒåŒ…æ‹¬å¼€æ”¾æºä»£ç è¯­æ–™åº“ã€é¦™æ¸¯ç‰¹å®šè®ºå›ã€Wikipediaå’ŒCommon Crawlæ•°æ®ã€‚æˆ‘ä»¬é€šè¿‡è¯­è¨€è¿‡æ»¤ã€è´¨é‡è¿‡æ»¤ã€å†…å®¹è¿‡æ»¤å’Œå»é‡ç­‰ä¸¥æ ¼çš„æ•°æ®å¤„ç†æ­¥éª¤ï¼ŒæˆåŠŸæ„å»ºäº†è¶…è¿‡2äº¿ä»¤ç‰Œçš„é«˜è´¨é‡ç²¤è¯­è¯­æ–™åº“ï¼Œç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ç²¾é€‰çš„ç²¤è¯­ä»»åŠ¡è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹ï¼Œæé«˜å…¶å¤„ç†ç‰¹å®šåº”ç”¨çš„èƒ½åŠ›ã€‚è®­ç»ƒå®Œæˆåï¼Œè¯¥æ¨¡å‹åœ¨å››ä¸ªç²¤è¯­åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œè¯¥æ¨¡å‹åœ¨å…¶ä»–ä¸»æµè¯­è¨€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¹Ÿæœ‰æ‰€æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç²¤è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€é¢†åŸŸï¼Œå¦‚ä½•æ”¶é›†é«˜è´¨é‡æ•°æ®èµ„æºç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šè¿‡ä»å¤šç§æ¥æºæ”¶é›†ç²¤è¯­æ–‡æœ¬ï¼Œè¿›è¡Œä¸¥è°¨çš„æ•°æ®å¤„ç†ï¼ŒæˆåŠŸæ„å»ºäº†ä¸€ä¸ªè¶…è¿‡ä¸¤äº¿æ ‡è®°çš„é«˜è´¨é‡ç²¤è¯­è¯­æ–™åº“ã€‚ç»è¿‡ç‰¹å®šä»»åŠ¡çš„ç›‘ç£å¾®è°ƒåï¼Œè¯¥æ¨¡å‹åœ¨å››é¡¹ç²¤è¯­åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†ä¸šç•Œæœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸»æµè¯­è¨€ä»»åŠ¡ä¸Šä¹Ÿæœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç²¤è¯­è™½ç„¶æœ‰è¶…è¿‡85ç™¾ä¸‡çš„æ¯è¯­ä½¿ç”¨è€…ï¼Œä½†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä»è¢«è§†ä¸ºä½èµ„æºè¯­è¨€ã€‚ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æ™®é€šè¯çš„ä¸»å¯¼åœ°ä½ã€ç²¤è¯­ç¤¾ç¾¤çš„ä¸å›¢ç»“ã€å­—ç¬¦ç¼–ç å’Œè¾“å…¥æ–¹æ³•çš„å¤šæ ·æ€§ä»¥åŠæµ·å¤–ç²¤è¯­ä½¿ç”¨è€…æ›´å€¾å‘äºä½¿ç”¨è‹±è¯­ç­‰é—®é¢˜ã€‚</li>
<li>æ”¶é›†ç²¤è¯­æ–‡æœ¬æ¥æºå¤šæ ·åŒ–ï¼ŒåŒ…æ‹¬å¼€æºè¯­æ–™åº“ã€é¦™æ¸¯ç‰¹å®šè®ºå›ã€Wikipediaå’ŒCommon Crawlæ•°æ®ç­‰ã€‚</li>
<li>å¯¹æ”¶é›†çš„æ•°æ®è¿›è¡Œäº†ä¸¥æ ¼çš„å¤„ç†ï¼ŒåŒ…æ‹¬è¯­è¨€è¿‡æ»¤ã€è´¨é‡è¿‡æ»¤ã€å†…å®¹è¿‡æ»¤å’Œå»é‡ç­‰æ­¥éª¤ï¼ŒæˆåŠŸæ„å»ºäº†é«˜è´¨é‡ç²¤è¯­è¯­æ–™åº“ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å®šåˆ¶ç²¤è¯­ä»»åŠ¡ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ï¼Œæå‡å…¶å¤„ç†ç‰¹å®šåº”ç”¨çš„èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨å››é¡¹ç²¤è¯­åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚</li>
<li>åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šè®­ç»ƒåï¼Œæ¨¡å‹åœ¨ä¸»æµè¯­è¨€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¹Ÿæœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c53352adf97e0debe6f3578c5d005f3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b475970ce5e20ec9979bee8bc42a850d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2296bd368871b4417ecaf11095d92b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b611fa2946ab27ffbd8a61b28f4f395f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6220e4b8076b5fc786a3be5d428ae9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b82fc9faf22307c437bfd8f789d8b9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2149ec418ba58ca6218856a322ebe6f2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Addressing-Overprescribing-Challenges-Fine-Tuning-Large-Language-Models-for-Medication-Recommendation-Tasks"><a href="#Addressing-Overprescribing-Challenges-Fine-Tuning-Large-Language-Models-for-Medication-Recommendation-Tasks" class="headerlink" title="Addressing Overprescribing Challenges: Fine-Tuning Large Language Models   for Medication Recommendation Tasks"></a>Addressing Overprescribing Challenges: Fine-Tuning Large Language Models   for Medication Recommendation Tasks</h2><p><strong>Authors:Zihao Zhao, Chenxiao Fan, Chongming Gao, Fuli Feng, Xiangnan He</strong></p>
<p>Medication recommendation systems have garnered attention within healthcare for their potential to deliver personalized and efficacious drug combinations based on patientâ€™s clinical data. However, existing methodologies encounter challenges in adapting to diverse Electronic Health Records (EHR) systems and effectively utilizing unstructured data, resulting in limited generalization capabilities and suboptimal performance. Recently, interest is growing in harnessing Large Language Models (LLMs) in the medical domain to support healthcare professionals and enhance patient care. Despite the emergence of medical LLMs and their promising results in tasks like medical question answering, their practical applicability in clinical settings, particularly in medication recommendation, often remains underexplored.   In this study, we evaluate both general-purpose and medical-specific LLMs for medication recommendation tasks. Our findings reveal that LLMs frequently encounter the challenge of overprescribing, leading to heightened clinical risks and diminished medication recommendation accuracy. To address this issue, we propose Language-Assisted Medication Recommendation (LAMO), which employs a parameter-efficient fine-tuning approach to tailor open-source LLMs for optimal performance in medication recommendation scenarios. LAMO leverages the wealth of clinical information within clinical notes, a resource often underutilized in traditional methodologies. As a result of our approach, LAMO outperforms previous state-of-the-art methods by over 10% in internal validation accuracy. Furthermore, temporal and external validations demonstrate LAMOâ€™s robust generalization capabilities across various temporal and hospital contexts. Additionally, an out-of-distribution medication recommendation experiment demonstrates LAMOâ€™s remarkable accuracy even with medications outside the training data. </p>
<blockquote>
<p>åŒ»ç–—æ¨èç³»ç»Ÿå› å…¶æ ¹æ®æ‚£è€…ä¸´åºŠæ•°æ®æä¾›ä¸ªæ€§åŒ–ä¸”æœ‰æ•ˆçš„è¯ç‰©ç»„åˆæ½œåŠ›è€Œå—åˆ°åŒ»ç–—ä¿å¥é¢†åŸŸçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é€‚åº”å¤šæ ·åŒ–çš„ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ç³»ç»Ÿå’Œæœ‰æ•ˆåˆ©ç”¨éç»“æ„åŒ–æ•°æ®æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›æœ‰é™å’Œæ€§èƒ½ä¸ä½³ã€‚æœ€è¿‘ï¼Œè¶Šæ¥è¶Šå¤šçš„å…´è¶£åœ¨äºåˆ©ç”¨åŒ»ç–—é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ”¯æŒåŒ»ç–—ä¸“ä¸šäººå£«å¹¶æ”¹å–„æ‚£è€…æŠ¤ç†ã€‚å°½ç®¡åŒ»ç–—LLMçš„å‡ºç°åŠå…¶åœ¨åŒ»ç–—é—®ç­”ç­‰ä»»åŠ¡ä¸Šçš„å‰æ™¯ä»¤äººé¼“èˆï¼Œä½†å®ƒä»¬åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯ç‰©æ¨èæ–¹é¢ï¼Œå¾€å¾€è¢«æ¢ç´¢å¾—ä¸å¤Ÿã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†é€šç”¨å’ŒåŒ»ç–—ä¸“ç”¨çš„LLMåœ¨è¯ç‰©æ¨èä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒLLMç»å¸¸é¢ä¸´è¿‡åº¦å¼€è¯çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´ä¸´åºŠé£é™©å¢åŠ å’Œè¯ç‰©æ¨èå‡†ç¡®æ€§é™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­è¨€è¾…åŠ©è¯ç‰©æ¨èï¼ˆLAMOï¼‰ï¼Œå®ƒé‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œé‡èº«å®šåˆ¶å¼€æºLLMï¼Œä»¥åœ¨è¯ç‰©æ¨èåœºæ™¯ä¸­å®ç°æœ€ä½³æ€§èƒ½ã€‚LAMOåˆ©ç”¨ç—…å†ç¬”è®°ä¸­çš„ä¸°å¯Œä¸´åºŠä¿¡æ¯ï¼Œè¿™æ˜¯ä¼ ç»Ÿæ–¹æ³•ä¸­ç»å¸¸è¢«å¿½è§†çš„èµ„æºã€‚ç”±äºæˆ‘ä»¬çš„æ–¹æ³•ï¼ŒLAMOåœ¨å†…éƒ¨éªŒè¯å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†å…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•è¶…è¿‡10%ã€‚æ­¤å¤–ï¼Œæ—¶åºå’Œå¤–éƒ¨éªŒè¯è¡¨æ˜LAMOåœ¨ä¸åŒæ—¶é—´å’ŒåŒ»é™¢èƒŒæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›å¼ºå¤§ã€‚å¦å¤–ï¼Œä¸€ä¸ªè¶…å‡ºåˆ†å¸ƒçš„è¯ç‰©æ¨èå®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®ä¹‹å¤–çš„è¯ç‰©ä¸­ï¼ŒLAMOçš„å‡†ç¡®ç‡ä¹Ÿä»¤äººå°è±¡æ·±åˆ»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03687v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»ç–—æ¨èç³»ç»Ÿå› å…¶æ ¹æ®æ‚£è€…ä¸´åºŠæ•°æ®æä¾›ä¸ªæ€§åŒ–ã€æœ‰æ•ˆè¯ç‰©ç»„åˆçš„èƒ½åŠ›è€Œå—åˆ°åŒ»ç–—ä¿å¥é¢†åŸŸçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é€‚åº”å¤šæ ·åŒ–çš„ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰ç³»ç»Ÿå’Œæœ‰æ•ˆåˆ©ç”¨éç»“æ„åŒ–æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´é€šç”¨åŒ–èƒ½åŠ›æœ‰é™å’Œæ€§èƒ½ä¸ä½³ã€‚è¿‘å¹´æ¥ï¼Œäººä»¬è¶Šæ¥è¶Šæœ‰å…´è¶£åˆ©ç”¨åŒ»ç–—é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ”¯æŒåŒ»ç–—ä¸“ä¸šäººå‘˜å¹¶æ”¹å–„æ‚£è€…æŠ¤ç†ã€‚å°½ç®¡åŒ»ç–—LLMçš„å‡ºç°åŠå…¶åœ¨åŒ»ç–—é—®ç­”ç­‰ä»»åŠ¡ä¸­çš„ä»¤äººé¼“èˆçš„ç»“æœï¼Œå…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯ç‰©æ¨èæ–¹é¢ï¼Œä»ç„¶é²œæœ‰ç ”ç©¶ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†é€šç”¨å’ŒåŒ»ç–—ç‰¹å®šLLMåœ¨è¯ç‰©æ¨èä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°LLMç»å¸¸é¢ä¸´è¿‡åº¦å¼€è¯çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´ä¸´åºŠé£é™©å¢åŠ å’Œè¯ç‰©æ¨èå‡†ç¡®æ€§é™ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­è¨€è¾…åŠ©è¯ç‰©æ¨èï¼ˆLAMOï¼‰ï¼Œå®ƒé‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œä¸ºå¼€æºLLMé‡èº«å®šåˆ¶ï¼Œä»¥åœ¨è¯ç‰©æ¨èåœºæ™¯ä¸­å®ç°æœ€ä½³æ€§èƒ½ã€‚LAMOåˆ©ç”¨ä¸´åºŠç¬”è®°ä¸­çš„å¤§é‡ä¸´åºŠä¿¡æ¯ï¼Œè¿™æ˜¯ä¼ ç»Ÿæ–¹æ³•ä¸­ç»å¸¸è¢«å¿½è§†çš„èµ„æºã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿LAMOåœ¨å†…éƒ¨éªŒè¯ç²¾åº¦ä¸Šè¶…è¿‡äº†ä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•è¶…è¿‡10ï¼…ã€‚æ­¤å¤–ï¼Œæ—¶åºå’Œå¤–éƒ¨éªŒè¯è¯æ˜äº†LAMOåœ¨ä¸åŒæ—¶é—´å’ŒåŒ»é™¢èƒŒæ™¯ä¸‹çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚å¦å¤–ï¼Œè¶…å‡ºè®­ç»ƒæ•°æ®èŒƒå›´çš„è¯å“æ¨èå®éªŒè¯æ˜äº†LAMOå³ä½¿åœ¨è®­ç»ƒæ•°æ®å¤–çš„è¯å“æ¨èä¸­ä¹Ÿèƒ½ä¿æŒå‡ºè‰²çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»ç–—æ¨èç³»ç»Ÿä¾èµ–LLMä¸ºæ‚£è€…æä¾›ä¸ªæ€§åŒ–è¯ç‰©ç»„åˆå»ºè®®ã€‚</li>
<li>å½“å‰LLMåœ¨è¯ç‰©æ¨èä¸­é¢ä¸´è¿‡åº¦å¼€è¯ã€æ¨èå‡†ç¡®åº¦ä¸é«˜çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•LAMOï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ¥ä¼˜åŒ–LLMåœ¨è¯ç‰©æ¨èä¸­çš„æ€§èƒ½ã€‚</li>
<li>LAMOåˆ©ç”¨ä¸´åºŠç¬”è®°ä¸­çš„ä¸°å¯Œä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åœ¨ä¼ ç»Ÿæ–¹æ³•ä¸­å¸¸è¢«å¿½è§†ã€‚</li>
<li>LAMOåœ¨å†…éƒ¨éªŒè¯ç²¾åº¦ä¸Šè¾ƒä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æé«˜ï¼Œè¶…è¿‡10%ã€‚</li>
<li>LAMOåœ¨ä¸åŒæ—¶é—´å’ŒåŒ»é™¢èƒŒæ™¯ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b48f613f445789cbf52140cb3cdb33b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0d04b17e19574affd418e7fb2e461ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b3ebe5a2db40c2df0d879fbcae6ec55.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MAS-GPT-Training-LLMs-to-Build-LLM-based-Multi-Agent-Systems"><a href="#MAS-GPT-Training-LLMs-to-Build-LLM-based-Multi-Agent-Systems" class="headerlink" title="MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems"></a>MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems</h2><p><strong>Authors:Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, Jing Shao</strong></p>
<p>LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs. In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS. To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference. The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses. Extensive experiments on 9 benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms 10+ baseline MAS methods on diverse settings, indicating MAS-GPTâ€™s high effectiveness, efficiency and strong generalization ability. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/rui-ye/MAS-GPT">https://github.com/rui-ye/MAS-GPT</a>. </p>
<blockquote>
<p>åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰åœ¨å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¸ºäº†è®¾è®¡æœ‰æ•ˆçš„MASï¼Œç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨é…ç½®æˆ–å¤šæ¬¡è°ƒç”¨é«˜çº§LLMï¼Œå¯¼è‡´ä¸é€‚åº”å’Œé«˜æ¨ç†æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†æ„å»ºMASé‡æ–°æ„å»ºä¸ºç”Ÿæˆè¯­è¨€ä»»åŠ¡æ¥ç®€åŒ–è¯¥è¿‡ç¨‹ï¼Œå…¶ä¸­è¾“å…¥æ˜¯ç”¨æˆ·æŸ¥è¯¢ï¼Œè¾“å‡ºæ˜¯ç›¸åº”çš„MASã€‚ä¸ºäº†è§£å†³è¿™é¡¹æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†MASçš„è¡¨ç¤ºå½¢å¼ç»Ÿä¸€ä¸ºå¯æ‰§è¡Œä»£ç ï¼Œå¹¶æå‡ºäº†ä¸€ç§é¢å‘ä¸€è‡´æ€§çš„æ•°æ®æ„å»ºç®¡é“ï¼Œä»¥åˆ›å»ºåŒ…å«è¿è´¯å’Œä¸€è‡´æŸ¥è¯¢-MASå¯¹çš„é«˜è´¨é‡æ•°æ®é›†ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†MAS-GPTï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ä¸­å‹LLMï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªLLMæ¨ç†å†…ç”ŸæˆæŸ¥è¯¢é€‚åº”æ€§MASã€‚ç”Ÿæˆçš„MASå¯ä»¥æ— ç¼åº”ç”¨äºå¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶æä¾›é«˜è´¨é‡å“åº”ã€‚åœ¨9ä¸ªåŸºå‡†æµ‹è¯•å’Œ5ä¸ªLLMä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MAS-GPTåœ¨å¤šç§è®¾ç½®ä¸‹å§‹ç»ˆä¼˜äº10å¤šä¸ªåŸºçº¿MASæ–¹æ³•ï¼Œè¯æ˜äº†MAS-GPTçš„é«˜æ•ˆç‡ã€é«˜æ•ˆèƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/rui-ye/MAS-GPT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/rui-ye/MAS-GPTä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03686v1">PDF</a> 26 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰åœ¨å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡æ—¶æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¸ºäº†è®¾è®¡æœ‰æ•ˆçš„MASï¼Œç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨é…ç½®æˆ–å¤šæ¬¡è°ƒç”¨é«˜çº§LLMï¼Œå¯¼è‡´ä¸é€‚åº”å’Œé«˜æ¨ç†æˆæœ¬ã€‚æœ¬æ–‡ç®€åŒ–äº†æ„å»ºMASçš„è¿‡ç¨‹ï¼Œå°†å…¶é‡æ–°æ„å»ºä¸ºç”Ÿæˆå¼è¯­è¨€ä»»åŠ¡ï¼Œå…¶ä¸­è¾“å…¥æ˜¯ç”¨æˆ·æŸ¥è¯¢ï¼Œè¾“å‡ºæ˜¯ç›¸åº”çš„MASã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†MASçš„è¡¨ç¤ºå½¢å¼ç»Ÿä¸€ä¸ºå¯æ‰§è¡Œä»£ç ï¼Œå¹¶æå‡ºä¸€ç§é¢å‘ä¸€è‡´æ€§çš„æ•°æ®æ„å»ºç®¡é“ï¼Œä»¥åˆ›å»ºåŒ…å«è¿è´¯å’Œä¸€è‡´æŸ¥è¯¢-MASå¯¹çš„é«˜è´¨é‡æ•°æ®é›†ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†MAS-GPTï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ä¸­å‹LLMï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªLLMæ¨ç†å†…ç”ŸæˆæŸ¥è¯¢é€‚åº”æ€§MASã€‚ç”Ÿæˆçš„MASå¯ä»¥æ— ç¼åº”ç”¨äºå¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶æä¾›é«˜è´¨é‡å“åº”ã€‚åœ¨9ä¸ªåŸºå‡†æµ‹è¯•å’Œ5ä¸ªLLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MAS-GPTåœ¨å¤šç§è®¾ç½®ä¸‹å§‹ç»ˆä¼˜äº10å¤šç§åŸºçº¿MASæ–¹æ³•ï¼Œè¯æ˜äº†MAS-GPTçš„é«˜æ•ˆæ€§ã€é«˜æ•ˆèƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based MASåœ¨å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡æ—¶å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>å½“å‰è®¾è®¡MASçš„æ–¹æ³•è¿‡äºå¤æ‚ï¼Œæ¶‰åŠæ‰‹åŠ¨é…ç½®å’Œå¤šæ¬¡LLMè°ƒç”¨ï¼Œå¯¼è‡´ä¸é€‚åº”å’Œé«˜æˆæœ¬ã€‚</li>
<li>æœ¬æ–‡å°†æ„å»ºMASç®€åŒ–ä¸ºç”Ÿæˆå¼è¯­è¨€ä»»åŠ¡ï¼Œä½¿è¾“å…¥ä¸ºç”¨æˆ·æŸ¥è¯¢ï¼Œè¾“å‡ºä¸ºç›¸åº”çš„MASã€‚</li>
<li>MASçš„ç»Ÿä¸€è¡¨ç¤ºå½¢å¼ä¸ºå¯æ‰§è¡Œä»£ç ï¼Œå¹¶å¼•å…¥ä¸€è‡´æ€§æ•°æ®æ„å»ºç®¡é“åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†ã€‚</li>
<li>è®­ç»ƒäº†MAS-GPTï¼Œä¸€ä¸ªèƒ½å¤Ÿåœ¨å•ä¸ªLLMæ¨ç†å†…ç”ŸæˆæŸ¥è¯¢é€‚åº”æ€§MASçš„å¼€æºä¸­å‹LLMã€‚</li>
<li>MAS-GPTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå¤šç§åŸºçº¿MASæ–¹æ³•ã€‚</li>
<li>MAS-GPTå…·å¤‡é«˜æ•ˆæ€§ã€é«˜æ•ˆèƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03c7febe135aa80f8dd9eccce3b3c7e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67882454ecdba6ad6c30ff5ab1a3ffeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbb7a743126ff26d16a0e3f24215fea2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ad0b951fd187b846168927271f5311d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76f4f42b509e0b4ec3470570bb9d224c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-Neutral-Point-of-View-Text-Generation-through-Parameter-Efficient-Reinforcement-Learning-and-a-Small-Scale-High-Quality-Dataset"><a href="#Improving-Neutral-Point-of-View-Text-Generation-through-Parameter-Efficient-Reinforcement-Learning-and-a-Small-Scale-High-Quality-Dataset" class="headerlink" title="Improving Neutral Point of View Text Generation through   Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality   Dataset"></a>Improving Neutral Point of View Text Generation through   Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality   Dataset</h2><p><strong>Authors:Jessica Hoffmann, Christiane Ahlheim, Zac Yu, Aria Walfrand, Jarvis Jin, Marie Tano, Ahmad Beirami, Erin van Liemt, Nithum Thain, Hakim Sidahmed, Lucas Dixon</strong></p>
<p>This paper describes the construction of a dataset and the evaluation of training methods to improve generative large language modelsâ€™ (LLMs) ability to answer queries on sensitive topics with a Neutral Point of View (NPOV), i.e., to provide significantly more informative, diverse and impartial answers. The dataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written quadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set of links to source texts elaborating the various points of view. The first key contribution of this paper is a new methodology to create such datasets through iterative rounds of human peer-critique and annotator training, which we release alongside the dataset. The second key contribution is the identification of a highly effective training regime for parameter-efficient reinforcement learning (PE-RL) to improve NPOV generation. We compare and extensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a strong baseline), SFT and RLHF.   PE-RL not only improves on overall NPOV quality compared to the strongest baseline ($97.06%\rightarrow 99.08%$), but also scores much higher on features linguists identify as key to separating good answers from the best answers ($60.25%\rightarrow 85.21%$ for presence of supportive details, $68.74%\rightarrow 91.43%$ for absence of oversimplification). A qualitative analysis corroborates this. Finally, our evaluation finds no statistical differences between results on topics that appear in the training dataset and those on separated evaluation topics, which provides strong evidence that our approach to training PE-RL exhibits very effective out of topic generalization. </p>
<blockquote>
<p>æœ¬æ–‡æè¿°äº†ä¸€ä¸ªæ•°æ®é›†çš„æ„å»ºè¿‡ç¨‹ä»¥åŠè®­ç»ƒæ–¹æ³•çš„è¯„ä¼°ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•æ„Ÿè¯é¢˜ä¸Šä»¥ä¸­ç«‹è§‚ç‚¹ï¼ˆNPOVï¼‰å›ç­”é—®é¢˜çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œå³ä¸ºæä¾›æ›´å¯Œæœ‰ä¿¡æ¯ã€å¤šæ ·åŒ–å’Œå…¬æ­£çš„ç­”æ¡ˆã€‚æ•°æ®é›†SHQ-NPOVæ•°æ®é›†åŒ…å«300ä¸ªé«˜è´¨é‡çš„äººä¸ºç¼–å†™çš„å››å…ƒç»„ï¼šå…³äºæ•æ„Ÿè¯é¢˜çš„æŸ¥è¯¢ã€ç­”æ¡ˆã€ä¸­ç«‹è§‚ç‚¹è¯„åˆ†å’Œä¸€ç³»åˆ—é“¾æ¥æ¥æºæ–‡æœ¬ï¼Œé˜è¿°äº†ä¸åŒçš„è§‚ç‚¹ã€‚æœ¬æ–‡çš„ç¬¬ä¸€ä¸ªå…³é”®è´¡çŒ®æ˜¯æå‡ºä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸€ç³»åˆ—çš„äººåŒè¡Œæ‰¹è¯„å’Œæ ‡æ³¨è®­ç»ƒæ¥åˆ›å»ºè¿™æ ·çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬éšæ•°æ®é›†ä¸€èµ·å‘å¸ƒè¿™ç§æ–¹æ³•ã€‚ç¬¬äºŒä¸ªå…³é”®è´¡çŒ®æ˜¯ç¡®å®šäº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒåˆ¶åº¦æ¥æé«˜å‚æ•°æ•ˆç‡å¼ºåŒ–å­¦ä¹ ï¼ˆPE-RLï¼‰çš„ä¸­ç«‹è§‚ç‚¹ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬å°†PE-RLä¸å¤šä¸ªåŸºçº¿æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒå’Œå…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬LoRAå¾®è°ƒï¼ˆä¸€ä¸ªå¼ºæœ‰åŠ›çš„åŸºçº¿æ–¹æ³•ï¼‰ã€SFTå’ŒRLHFã€‚PE-RLä¸ä»…åœ¨æ€»ä½“ä¸­ç«‹è§‚ç‚¹è´¨é‡æ–¹é¢ä¸æœ€å¼ºåŸºçº¿ç›¸æ¯”æœ‰æ‰€æå‡ï¼ˆä»$97.06%$æé«˜åˆ°$99.08%$ï¼‰ï¼Œè€Œä¸”åœ¨è¯­è¨€å­¦å®¶è®¤ä¸ºåŒºåˆ†å¥½ç­”æ¡ˆä¸æœ€ä½³ç­”æ¡ˆçš„å…³é”®ç‰¹å¾æ–¹é¢å¾—åˆ†æ›´é«˜ï¼ˆæ”¯æŒç»†èŠ‚çš„å­˜åœ¨æ€§ä»$60.25%$æé«˜åˆ°$85.21%$ï¼Œæ— ç®€åŒ–ç°è±¡ä»$68.74%$æé«˜åˆ°$91.43%$ï¼‰ã€‚å®šæ€§åˆ†æè¯å®äº†è¿™ä¸€ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬çš„è¯„ä¼°å‘ç°åœ¨è®­ç»ƒæ•°æ®é›†ä¸­å‡ºç°çš„ä¸»é¢˜ä¸å•ç‹¬è¯„ä¼°çš„ä¸»é¢˜ä¹‹é—´ä¸å­˜åœ¨ç»Ÿè®¡å·®å¼‚ï¼Œè¿™ä¸ºæˆ‘ä»¬çš„PE-RLè®­ç»ƒæ–¹æ³•å±•ç°å‡ºäº†æœ‰æ•ˆçš„è·¨è¯é¢˜æ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰åŠ›è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03654v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†è®­ç»ƒæ–¹æ³•æ¥æå‡å¤§å‹ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•æ„Ÿè¯é¢˜ä¸Šçš„ä¸­ç«‹è§‚ç‚¹å›ç­”èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†é€šè¿‡è¿­ä»£çš„äººç±»åŒè¡Œè¯„å®¡å’Œæ³¨é‡Šè€…åŸ¹è®­æ–¹æ³•åˆ›å»ºï¼Œå¹¶å‘å¸ƒäº†æ•°æ®é›†ã€‚ç ”ç©¶å‘ç°äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒæœºåˆ¶â€”â€”å‚æ•°é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ï¼ˆPE-RLï¼‰ï¼Œç”¨äºæé«˜ä¸­ç«‹è§‚ç‚¹çš„ç”Ÿæˆè´¨é‡ã€‚ä¸æœ€å¼ºåŸºçº¿ç›¸æ¯”ï¼ŒPE-RLä¸ä»…æé«˜äº†æ•´ä½“ä¸­ç«‹è§‚ç‚¹çš„è´¨é‡ï¼Œè€Œä¸”åœ¨è¯­è¨€å­¦å®¶è®¤ä¸ºå…³é”®çš„ç‰¹å¾ä¸Šä¹Ÿæœ‰æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¯¥è®­ç»ƒæœºåˆ¶åœ¨è®­ç»ƒé›†ä¸­çš„è¯é¢˜ä¸ç‹¬ç«‹è¯„ä¼°çš„è¯é¢˜ä¹‹é—´æ²¡æœ‰ç»Ÿè®¡å·®å¼‚ï¼Œæ˜¾ç¤ºå‡ºå¾ˆå¼ºçš„è·¨è¯é¢˜æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªåä¸ºSHQ-NPOVçš„æ–°æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•æ„Ÿè¯é¢˜ä¸Šæä¾›ä¸­ç«‹è§‚ç‚¹çš„æŸ¥è¯¢å›ç­”ã€‚</li>
<li>æ•°æ®é›†åŒ…å«é«˜è´¨é‡çš„å››å…ƒç»„ï¼Œæ¯ä¸ªå››å…ƒç»„åŒ…æ‹¬æ•æ„Ÿè¯é¢˜çš„æŸ¥è¯¢ã€ç­”æ¡ˆã€ä¸­ç«‹è§‚ç‚¹è¯„åˆ†å’Œæ¥æºæ–‡æœ¬çš„é“¾æ¥ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è¿­ä»£çš„äººç±»åŒè¡Œè¯„å®¡å’Œæ³¨é‡Šè€…åŸ¹è®­æ¥åˆ›å»ºæ•°æ®é›†çš„æ–°æ–¹æ³•ã€‚</li>
<li>è®ºæ–‡è¯†åˆ«äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒæœºåˆ¶â€”â€”å‚æ•°é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ï¼ˆPE-RLï¼‰ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­ç«‹è§‚ç‚¹ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>PE-RLä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†ä¸­ç«‹è§‚ç‚¹ç­”æ¡ˆçš„æ•´ä½“è´¨é‡å’Œå…³é”®è¯­è¨€ç‰¹å¾çš„è¡¨ç°ã€‚</li>
<li>è¯„ä¼°å‘ç°ï¼Œè®­ç»ƒæœºåˆ¶åœ¨è®­ç»ƒé›†ä¸­çš„è¯é¢˜å’Œç‹¬ç«‹è¯„ä¼°çš„è¯é¢˜ä¹‹é—´æ²¡æœ‰ç»Ÿè®¡å·®å¼‚ï¼Œè¡¨æ˜è¯¥æœºåˆ¶å…·æœ‰å¼ºå¤§çš„è·¨è¯é¢˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc1612fea791676a60734a6dd5e96dfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0425d48325ebd84a1d8690e802d6eed8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fd9bb260e2daccabd25b8b56d52a1ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89263554016badcb49131b57a334d86.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Small-but-Mighty-Enhancing-Time-Series-Forecasting-with-Lightweight-LLMs"><a href="#Small-but-Mighty-Enhancing-Time-Series-Forecasting-with-Lightweight-LLMs" class="headerlink" title="Small but Mighty: Enhancing Time Series Forecasting with Lightweight   LLMs"></a>Small but Mighty: Enhancing Time Series Forecasting with Lightweight   LLMs</h2><p><strong>Authors:Haoran Fan, Bin Li, Yixuan Weng, Shoujun Zhou</strong></p>
<p>While LLMs have demonstrated remarkable potential in time series forecasting, their practical deployment remains constrained by excessive computational demands and memory footprints. Existing LLM-based approaches typically suffer from three critical limitations: Inefficient parameter utilization in handling numerical time series patterns; Modality misalignment between continuous temporal signals and discrete text embeddings; and Inflexibility for real-time expert knowledge integration. We present SMETimes, the first systematic investigation of sub-3B parameter SLMs for efficient and accurate time series forecasting. Our approach centers on three key innovations: A statistically-enhanced prompting mechanism that bridges numerical time series with textual semantics through descriptive statistical features; A adaptive fusion embedding architecture that aligns temporal patterns with language model token spaces through learnable parameters; And a dynamic mixture-of-experts framework enabled by SLMsâ€™ computational efficiency, adaptively combining base predictions with domain-specific models. Extensive evaluations across seven benchmark datasets demonstrate that our 3B-parameter SLM achieves state-of-the-art performance on five primary datasets while maintaining 3.8x faster training and 5.2x lower memory consumption compared to 7B-parameter LLM baselines. Notably, the proposed model exhibits better learning capabilities, achieving 12.3% lower MSE than conventional LLM. Ablation studies validate that our statistical prompting and cross-modal fusion modules respectively contribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks. By redefining the efficiency-accuracy trade-off landscape, this work establishes SLMs as viable alternatives to resource-intensive LLMs for practical time series forecasting. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/xiyan1234567/SMETimes">https://github.com/xiyan1234567/SMETimes</a>. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢å±•ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨å®è·µéƒ¨ç½²ä¸­ä»å—åˆ°è®¡ç®—éœ€æ±‚è¿‡å¤§å’Œå†…å­˜å ç”¨è¿‡é«˜çš„é™åˆ¶ã€‚å½“å‰åŸºäºLLMçš„æ–¹æ³•é€šå¸¸å­˜åœ¨ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼šå¤„ç†æ•°å€¼æ—¶é—´åºåˆ—æ¨¡å¼æ—¶çš„å‚æ•°åˆ©ç”¨æ•ˆç‡ä½ä¸‹ï¼›è¿ç»­æ—¶é—´ä¿¡å·ä¸ç¦»æ•£æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„æ¨¡æ€ä¸åŒ¹é…ï¼›ä»¥åŠå®æ—¶ä¸“å®¶çŸ¥è¯†æ•´åˆçš„ä¸çµæ´»ã€‚æˆ‘ä»¬æ¨å‡ºäº†SMETimesï¼Œè¿™æ˜¯é¦–æ¬¡å¯¹å‚æ•°å°‘äº3Bçš„å­é›†è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰è¿›è¡Œç³»ç»Ÿç ”ç©¶ï¼Œä»¥å®ç°é«˜æ•ˆå’Œå‡†ç¡®çš„æ—¶é—´åºåˆ—é¢„æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥ä¸‰ä¸ªå…³é”®åˆ›æ–°ä¸ºä¸­å¿ƒï¼šä¸€ç§ç»Ÿè®¡å¢å¼ºçš„æç¤ºæœºåˆ¶ï¼Œé€šè¿‡æè¿°æ€§ç»Ÿè®¡ç‰¹å¾å°†æ•°å€¼æ—¶é—´åºåˆ—ä¸æ–‡æœ¬è¯­ä¹‰è”ç³»èµ·æ¥ï¼›ä¸€ç§è‡ªé€‚åº”èåˆåµŒå…¥æ¶æ„ï¼Œé€šè¿‡å¯å­¦ä¹ å‚æ•°å°†æ—¶é—´æ¨¡å¼ä¸è¯­è¨€æ¨¡å‹æ ‡è®°ç©ºé—´å¯¹é½ï¼›ä»¥åŠç”±SLMsçš„è®¡ç®—æ•ˆç‡å¯åŠ¨çš„åŠ¨æ€æ··åˆä¸“å®¶æ¡†æ¶ï¼Œè‡ªé€‚åº”åœ°å°†åŸºæœ¬é¢„æµ‹ä¸é¢†åŸŸç‰¹å®šæ¨¡å‹ç›¸ç»“åˆã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„3Bå‚æ•°SLMåœ¨äº”ä¸ªä¸»è¦æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸7Bå‚æ•°LLMåŸºå‡†ç›¸æ¯”ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†3.8å€ï¼Œå†…å­˜æ¶ˆè€—é™ä½äº†5.2å€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æå‡ºçš„æ¨¡å‹å±•ç°å‡ºæ›´å¥½çš„å­¦ä¹ èƒ½åŠ›ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ¯”ä¼ ç»ŸLLMé™ä½äº†12.3%ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„ç»Ÿè®¡æç¤ºå’Œè·¨æ¨¡æ€èåˆæ¨¡å—åˆ†åˆ«åœ¨é•¿æœŸé¢„æµ‹ä»»åŠ¡ä¸­åˆ†åˆ«è´¡çŒ®äº†15.7%å’Œ18.2%çš„é”™è¯¯ç‡é™ä½ã€‚é€šè¿‡é‡æ–°å®šä¹‰æ•ˆç‡-å‡†ç¡®æ€§æƒè¡¡æ™¯è§‚ï¼Œè¿™é¡¹å·¥ä½œç¡®ç«‹äº†SLMsä½œä¸ºç”¨äºå®é™…æ—¶é—´åºåˆ—é¢„æµ‹çš„æ›¿ä»£èµ„æºå¯†é›†å‹LLMsçš„å¯è¡Œé€‰æ‹©ã€‚ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/xiyan1234567/SMETimes%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xiyan1234567/SMETimesè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03594v1">PDF</a> Work in progress</p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä½†å…¶å®é™…åº”ç”¨å—åˆ°è®¡ç®—éœ€æ±‚å’Œå†…å­˜å ç”¨è¿‡å¤§çš„é™åˆ¶ã€‚ç°æœ‰LLMæ–¹æ³•å­˜åœ¨ä¸‰ä¸ªå…³é”®å±€é™ï¼šæ•°å€¼æ—¶é—´åºåˆ—æ¨¡å¼å¤„ç†ä¸­çš„å‚æ•°åˆ©ç”¨ä½æ•ˆã€è¿ç»­æ—¶é—´ä¿¡å·ä¸ç¦»æ•£æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„æ¨¡æ€ä¸åŒ¹é…ä»¥åŠå®æ—¶ä¸“å®¶çŸ¥è¯†æ•´åˆçš„çµæ´»æ€§ä¸è¶³ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨å°äº3Bå‚æ•°çš„SLMï¼Œä»¥å®ç°é«˜æ•ˆå‡†ç¡®çš„æ—¶é—´åºåˆ—é¢„æµ‹ã€‚é€šè¿‡ä¸‰é¡¹å…³é”®åˆ›æ–°è§£å†³ä¸Šè¿°é—®é¢˜ï¼šé€šè¿‡æè¿°æ€§ç»Ÿè®¡ç‰¹å¾æ¡¥æ¥æ•°å€¼æ—¶é—´åºåˆ—ä¸æ–‡æœ¬è¯­ä¹‰çš„ç»Ÿè®¡å¢å¼ºæç¤ºæœºåˆ¶ï¼›é€šè¿‡å¯å­¦ä¹ å‚æ•°å¯¹é½æ—¶é—´æ¨¡å¼ä¸è¯­è¨€æ¨¡å‹ä»¤ç‰Œç©ºé—´çš„è‡ªé€‚åº”èåˆåµŒå…¥æ¶æ„ï¼›ä»¥åŠç”±SLMè®¡ç®—æ•ˆç‡æ”¯æŒçš„åŠ¨æ€æ··åˆä¸“å®¶æ¡†æ¶ï¼Œè‡ªé€‚åº”ç»“åˆåŸºæœ¬é¢„æµ‹ä¸é¢†åŸŸç‰¹å®šæ¨¡å‹ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„3Bå‚æ•°SLMåœ¨äº”ä¸ªä¸»è¦æ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œä¸7Bå‚æ•°LLMåŸºå‡†ç›¸æ¯”ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜3.8å€ï¼Œå†…å­˜å ç”¨å‡å°‘5.2å€ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ‰€æå‡ºæ¨¡å‹å±•ç°å‡ºæ›´å¥½çš„å­¦ä¹ èƒ½åŠ›ï¼Œå‡æ–¹è¯¯å·®é™ä½äº†12.3%ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„ç»Ÿè®¡æç¤ºå’Œè·¨æ¨¡æ€èåˆæ¨¡å—åˆ†åˆ«åœ¨é•¿æœŸé¢„æµ‹ä»»åŠ¡ä¸­åˆ†åˆ«å®ç°äº†15.7%å’Œ18.2%çš„è¯¯å·®é™ä½ã€‚é€šè¿‡é‡æ–°å®šä¹‰æ•ˆç‡-å‡†ç¡®æ€§æƒè¡¡æ™¯è§‚ï¼Œæœ¬ç ”ç©¶ç¡®ç«‹äº†SLMä½œä¸ºå®ç”¨æ—¶é—´åºåˆ—é¢„æµ‹çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆï¼Œä»¥æ›¿ä»£èµ„æºå¯†é›†å‹çš„LLMã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiyan1234567/SMETimes%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xiyan1234567/SMETimesè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„å®é™…åº”ç”¨å—åˆ°é™åˆ¶ï¼Œä¸»è¦å› ä¸ºè®¡ç®—éœ€æ±‚å¤§ã€å†…å­˜å ç”¨é«˜åŠä¸‰ä¸ªå…³é”®å±€é™ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ•°å€¼æ—¶é—´åºåˆ—æ¨¡å¼ã€æ¨¡æ€å¯¹é½åŠå®æ—¶ä¸“å®¶çŸ¥è¯†æ•´åˆæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>SMETimesé¦–æ¬¡æ¢è®¨å°äº3Bå‚æ•°çš„SLMï¼Œå®ç°é«˜æ•ˆå‡†ç¡®çš„æ—¶é—´åºåˆ—é¢„æµ‹ã€‚</li>
<li>ä¸‰é¡¹å…³é”®åˆ›æ–°åŒ…æ‹¬ç»Ÿè®¡å¢å¼ºæç¤ºæœºåˆ¶ã€è‡ªé€‚åº”èåˆåµŒå…¥æ¶æ„å’ŒåŠ¨æ€æ··åˆä¸“å®¶æ¡†æ¶ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„SLMåœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äº7Bå‚æ•°çš„LLMã€‚</li>
<li>æ‰€æå‡ºæ¨¡å‹å±•ç°å‡ºæ›´å¥½çš„å­¦ä¹ èƒ½åŠ›ï¼Œä¸”æ¶ˆèç ”ç©¶éªŒè¯äº†å…¶å…³é”®ç»„æˆéƒ¨åˆ†çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5df94aa0894eed2495c41de5b864eb75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-445831f5871ede172f16d60fec14e660.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60e631961b6a1030b466e5a8b724ee1e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FairSense-AI-Responsible-AI-Meets-Sustainability"><a href="#FairSense-AI-Responsible-AI-Meets-Sustainability" class="headerlink" title="FairSense-AI: Responsible AI Meets Sustainability"></a>FairSense-AI: Responsible AI Meets Sustainability</h2><p><strong>Authors:Shaina Raza, Mukund Sayeeganesh Chettiar, Matin Yousefabadi, Tahniat Khan, Marcelo Lotif</strong></p>
<p>In this paper, we introduce FairSense-AI: a multimodal framework designed to detect and mitigate bias in both text and images. By leveraging Large Language Models (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle forms of prejudice or stereotyping that can appear in content, providing users with bias scores, explanatory highlights, and automated recommendations for fairness enhancements. In addition, FairSense-AI integrates an AI risk assessment component that aligns with frameworks like the MIT AI Risk Repository and NIST AI Risk Management Framework, enabling structured identification of ethical and safety concerns. The platform is optimized for energy efficiency via techniques such as model pruning and mixed-precision computation, thereby reducing its environmental footprint. Through a series of case studies and applications, we demonstrate how FairSense-AI promotes responsible AI use by addressing both the social dimension of fairness and the pressing need for sustainability in large-scale AI deployments. <a target="_blank" rel="noopener" href="https://vectorinstitute.github.io/FairSense-AI">https://vectorinstitute.github.io/FairSense-AI</a>, <a target="_blank" rel="noopener" href="https://pypi.org/project/fair-sense-ai/">https://pypi.org/project/fair-sense-ai/</a> (Sustainability , Responsible AI , Large Language Models , Vision Language Models , Ethical AI , Green AI) </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†FairSense-AIï¼šä¸€ä¸ªæ—¨åœ¨æ£€æµ‹å’Œç¼“è§£æ–‡æœ¬å’Œå›¾åƒä¸­åè§çš„å¤šæ¨¡å¼æ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼ŒFairSense-AIæ­ç¤ºäº†å†…å®¹ä¸­å¯èƒ½å‡ºç°çš„å¾®å¦™å½¢å¼çš„åè§æˆ–åˆ»æ¿å°è±¡ï¼Œä¸ºç”¨æˆ·æä¾›åè§åˆ†æ•°ã€è§£é‡Šäº®ç‚¹å’Œå…¬å¹³æ€§å¢å¼ºçš„è‡ªåŠ¨åŒ–å»ºè®®ã€‚æ­¤å¤–ï¼ŒFairSense-AIè¿˜é›†æˆäº†ä¸€ä¸ªä¸MITäººå·¥æ™ºèƒ½é£é™©ä»“åº“å’ŒNISTäººå·¥æ™ºèƒ½é£é™©ç®¡ç†æ¡†æ¶ç­‰æ¡†æ¶ç›¸å»åˆçš„äººå·¥æ™ºèƒ½é£é™©è¯„ä¼°ç»„ä»¶ï¼Œèƒ½å¤Ÿå®ç°é“å¾·å’Œå®‰å…¨é—®é¢˜çš„ç»“æ„åŒ–è¯†åˆ«ã€‚è¯¥å¹³å°é€šè¿‡æ¨¡å‹ä¿®å‰ªå’Œæ··åˆç²¾åº¦è®¡ç®—ç­‰æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæé«˜èƒ½æ•ˆï¼Œå‡å°‘ç¯å¢ƒè¶³è¿¹ã€‚é€šè¿‡ä¸€ç³»åˆ—æ¡ˆä¾‹ç ”ç©¶ä¸åº”ç”¨ï¼Œæˆ‘ä»¬å±•ç¤ºäº†FairSense-AIå¦‚ä½•é€šè¿‡è§£å†³å…¬å¹³çš„ç¤¾ä¼šç»´åº¦å’Œå¤§è§„æ¨¡äººå·¥æ™ºèƒ½éƒ¨ç½²ä¸­å¯¹å¯æŒç»­æ€§çš„è¿«åˆ‡éœ€æ±‚ï¼Œä¿ƒè¿›äººå·¥æ™ºèƒ½çš„è´Ÿè´£ä»»ä½¿ç”¨ã€‚<a target="_blank" rel="noopener" href="https://vectorinstitute.github.io/FairSense-AI%EF%BC%8Chttps://pypi.org/project/fair-sense-ai/%EF%BC%88%E5%8F%AF%E6%8C%81%E7%BB%AD%E6%80%A7%E3%80%81%E8%B4%9F%E8%B4%A3%E4%BB%BB%E7%9A%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%81%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E3%80%81%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E3%80%81%E9%81%93%E5%BE%B7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%81%E7%BB%BF%E8%89%B2%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%89%E3%80%82">https://vectorinstitute.github.io/FairSense-AIï¼Œhttps://pypi.org/project/fair-sense-ai/ï¼ˆå¯æŒç»­æ€§ã€è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ã€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰è¯­è¨€æ¨¡å‹ã€é“å¾·äººå·¥æ™ºèƒ½ã€ç»¿è‰²äººå·¥æ™ºèƒ½ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02865v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å¼•å…¥FairSense-AIï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨æ£€æµ‹å’Œç¼“è§£æ–‡æœ¬å’Œå›¾åƒä¸­çš„åè§ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒFairSense-AIå¯ä»¥æ­ç¤ºå†…å®¹ä¸­å¯èƒ½å‡ºç°çš„å¾®å¦™åè§æˆ–åˆ»æ¿å°è±¡ï¼Œä¸ºç”¨æˆ·æä¾›åè§åˆ†æ•°ã€è§£é‡Šäº®ç‚¹å’Œå…¬å¹³å¢å¼ºå»ºè®®ã€‚æ­¤å¤–ï¼Œå®ƒç»“åˆäº†äººå·¥æ™ºèƒ½é£é™©è¯„ä¼°ç»„ä»¶ï¼Œç¬¦åˆéº»çœç†å·¥å­¦é™¢çš„äººå·¥æ™ºèƒ½é£é™©ä»“åº“å’Œç¾å›½å›½å®¶æ ‡å‡†æŠ€æœ¯ç ”ç©¶é™¢çš„äººå·¥æ™ºèƒ½é£é™©ç®¡ç†æ¡†æ¶çš„è¦æ±‚ï¼Œèƒ½ç³»ç»Ÿåœ°è¯†åˆ«é“å¾·å’Œå®‰å…¨æ–¹é¢çš„é¡¾è™‘ã€‚å¹³å°ç»è¿‡ä¼˜åŒ–ï¼Œé€šè¿‡æ¨¡å‹ä¿®å‰ªå’Œæ··åˆç²¾åº¦è®¡ç®—ç­‰æŠ€æœ¯æå‡èƒ½æ•ˆï¼Œé™ä½å¯¹ç¯å¢ƒçš„å½±å“ã€‚é€šè¿‡ä¸€ç³»åˆ—æ¡ˆä¾‹ç ”ç©¶å’Œå®è·µåº”ç”¨è¯æ˜FairSense-AIå¦‚ä½•é€šè¿‡è§£å†³å…¬å¹³æ€§çš„ç¤¾ä¼šç»´åº¦ä»¥åŠåœ¨å¤§è§„æ¨¡éƒ¨ç½²ä¸­å¯¹å¯æŒç»­æ€§çš„ç´§è¿«éœ€æ±‚æ¥æ¨åŠ¨è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FairSense-AIæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹å’Œç¼“è§£æ–‡æœ¬å’Œå›¾åƒä¸­çš„åè§ã€‚</li>
<li>å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹æ¥æ­ç¤ºå†…å®¹ä¸­çš„åè§å’Œåˆ»æ¿å°è±¡ã€‚</li>
<li>FairSense-AIæä¾›åè§åˆ†æ•°ã€è§£é‡Šäº®ç‚¹å’Œå…¬å¹³å¢å¼ºå»ºè®®ã€‚</li>
<li>è¯¥å¹³å°ç»“åˆäººå·¥æ™ºèƒ½é£é™©è¯„ä¼°ç»„ä»¶ä»¥ç¬¦åˆå¤šç§æ ‡å‡†æ¡†æ¶çš„è¦æ±‚ã€‚</li>
<li>FairSense-AIèƒ½å¤Ÿç³»ç»Ÿåœ°è¯†åˆ«é“å¾·å’Œå®‰å…¨æ–¹é¢çš„é¡¾è™‘ã€‚</li>
<li>å¹³å°ç»è¿‡ä¼˜åŒ–ä»¥æé«˜èƒ½æ•ˆå¹¶é™ä½å¯¹ç¯å¢ƒçš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c0dd782a7ff87c98922cb57986c014d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4fde7d38e3d56562ea96eebd0aedeef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d77cb7541762d9c606bcaa498006c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e5a910f1201eafdd58c65cdaebd2c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-083ad7d5739c287985ec93b537854d50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40fede344f9e79de122916e0372dceee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62761ba41b2bb2fee29a1671e909df53.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PaCA-Partial-Connection-Adaptation-for-Efficient-Fine-Tuning"><a href="#PaCA-Partial-Connection-Adaptation-for-Efficient-Fine-Tuning" class="headerlink" title="PaCA: Partial Connection Adaptation for Efficient Fine-Tuning"></a>PaCA: Partial Connection Adaptation for Efficient Fine-Tuning</h2><p><strong>Authors:Sunghyeon Woo, Sol Namkung, Sunwoo Lee, Inho Jeong, Beomseok Kim, Dongsuk Jeon</strong></p>
<p>Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage and computational costs of fine-tuning large neural network models by training only a few additional adapter parameters, rather than the entire model. However, the reduction in computational costs due to PEFT does not necessarily translate to a reduction in training time; although the computational costs of the adapter layers are much smaller than the pretrained layers, it is well known that those two types of layers are processed sequentially on GPUs, resulting in significant latency overhead. LoRA and its variants merge low-rank adapter matrices with pretrained weights during inference to avoid latency overhead, but during training, the pretrained weights remain frozen while the adapter matrices are continuously updated, preventing such merging. To mitigate this issue, we propose Partial Connection Adaptation (PaCA), which fine-tunes randomly selected partial connections within the pretrained weights instead of introducing adapter layers in the model. PaCA not only enhances training speed by eliminating the time overhead due to the sequential processing of the adapter and pretrained layers but also reduces activation memory since only partial activations, rather than full activations, need to be stored for gradient computation. Compared to LoRA, PaCA reduces training time by 22% and total memory usage by 16%, while maintaining comparable accuracy across various fine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction tuning on the Oasst1 dataset. PaCA can also be combined with quantization, enabling the fine-tuning of large models such as LLaMA3.1-70B. In addition, PaCA enables training with 23% longer sequence and improves throughput by 16% on both NVIDIA A100 GPU and INTEL Gaudi2 HPU compared to LoRA. The code is available at <a target="_blank" rel="noopener" href="https://github.com/WooSunghyeon/paca">https://github.com/WooSunghyeon/paca</a>. </p>
<blockquote>
<p>å…ˆå‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ç®—æ³•é€šè¿‡ä»…è®­ç»ƒå°‘é‡é¢å¤–çš„é€‚é…å™¨å‚æ•°ï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå‡å°‘äº†å¾®è°ƒå¤§å‹ç¥ç»ç½‘ç»œæ¨¡å‹çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç”±äºPEFTå¯¼è‡´çš„è®¡ç®—æˆæœ¬é™ä½å¹¶ä¸ä¸€å®šè½¬åŒ–ä¸ºè®­ç»ƒæ—¶é—´çš„å‡å°‘ï¼›è™½ç„¶é€‚é…å™¨å±‚çš„è®¡ç®—æˆæœ¬è¿œè¿œå°äºé¢„è®­ç»ƒå±‚ï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œè¿™ä¸¤ç§ç±»å‹çš„å±‚åœ¨GPUä¸Šæ˜¯é¡ºåºå¤„ç†çš„ï¼Œå¯¼è‡´æ˜¾è‘—çš„å»¶è¿Ÿå¼€é”€ã€‚LoRAåŠå…¶å˜ä½“åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†ä½ç§©é€‚é…å™¨çŸ©é˜µä¸é¢„è®­ç»ƒæƒé‡åˆå¹¶ï¼Œä»¥é¿å…å»¶è¿Ÿå¼€é”€ï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé¢„è®­ç»ƒæƒé‡ä¿æŒå†»ç»“çŠ¶æ€ï¼Œè€Œé€‚é…å™¨çŸ©é˜µä¸æ–­æ›´æ–°ï¼Œä»è€Œé˜»æ­¢è¿™ç§åˆå¹¶ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†éƒ¨åˆ†è¿æ¥é€‚é…ï¼ˆPaCAï¼‰ï¼Œå®ƒåœ¨é¢„è®­ç»ƒæƒé‡å†…å¾®è°ƒéšæœºé€‰æ‹©çš„éƒ¨åˆ†è¿æ¥ï¼Œè€Œä¸æ˜¯åœ¨æ¨¡å‹ä¸­å¼•å…¥é€‚é…å™¨å±‚ã€‚PaCAä¸ä»…é€šè¿‡æ¶ˆé™¤ç”±äºé€‚é…å™¨å±‚å’Œé¢„è®­ç»ƒå±‚çš„é¡ºåºå¤„ç†è€Œäº§ç”Ÿçš„æ—¶é—´å¼€é”€æ¥æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œè€Œä¸”å‡å°‘äº†æ¿€æ´»å†…å­˜ï¼Œå› ä¸ºåªéœ€è¦å­˜å‚¨éƒ¨åˆ†æ¿€æ´»å€¼ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„æ¿€æ´»å€¼æ¥è¿›è¡Œæ¢¯åº¦è®¡ç®—ã€‚ä¸LoRAç›¸æ¯”ï¼ŒPaCAå°†è®­ç»ƒæ—¶é—´å‡å°‘äº†22%ï¼Œæ€»å†…å­˜ä½¿ç”¨é‡å‡å°‘äº†16%ï¼ŒåŒæ—¶åœ¨å„ç§å¾®è°ƒåœºæ™¯ï¼ˆå¦‚ä½¿ç”¨MMLUæ•°æ®é›†è¿›è¡Œå¾®è°ƒå’Œä½¿ç”¨Oasst1æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼‰ä¸­ä¿æŒç›¸å½“çš„å‡†ç¡®æ€§ã€‚PaCAè¿˜å¯ä»¥ä¸é‡åŒ–ç›¸ç»“åˆï¼Œå®ç°å¯¹å¤§å‹æ¨¡å‹ï¼ˆå¦‚LLaMA 3.1-70Bï¼‰çš„å¾®è°ƒã€‚æ­¤å¤–ï¼ŒPaCAä½¿ç”¨NVIDIA A100 GPUå’ŒINTEL Gaudi2 HPUæ—¶ï¼Œèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„åºåˆ—ï¼ˆæé«˜23%ï¼‰ï¼Œå¹¶ä¸”ååé‡æ¯”LoRAæé«˜16%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WooSunghyeon/paca%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WooSunghyeon/pacaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01905v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡ç¥ç»ç½‘ç»œæ¨¡å‹çš„å¾®è°ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œå†…å­˜ã€‚ä¸ºäº†é™ä½è¿™ç§æˆæœ¬ï¼Œç ”ç©¶è€…æå‡ºäº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ç®—æ³•ï¼Œåªè®­ç»ƒå°‘é‡é€‚é…å™¨å‚æ•°è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚ç„¶è€Œï¼ŒPEFTå¹¶ä¸ä¸€å®šèƒ½å‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå› ä¸ºé€‚é…å™¨å±‚å’Œé¢„è®­ç»ƒå±‚åœ¨GPUä¸Šæ˜¯é¡ºåºå¤„ç†çš„ï¼Œå¯¼è‡´å»¶è¿Ÿå¼€é”€è¾ƒå¤§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†éƒ¨åˆ†è¿æ¥é€‚é…ï¼ˆPaCAï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæƒé‡ä¸­çš„éƒ¨åˆ†è¿æ¥è€Œä¸æ˜¯å¼•å…¥é€‚é…å™¨å±‚æ¥æé«˜è®­ç»ƒé€Ÿåº¦å’Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚ç›¸è¾ƒäºLoRAæ–¹æ³•ï¼ŒPaCAå¯ä»¥å‡å°‘è®­ç»ƒæ—¶é—´å’Œæ€»å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ç»´æŒåœ¨ä¸åŒå¾®è°ƒåœºæ™¯ä¸‹çš„å‡†ç¡®åº¦ã€‚æ­¤å¤–ï¼ŒPaCAè¿˜å¯ä»¥ä¸é‡åŒ–ç»“åˆï¼Œä½¿å¤§å‹æ¨¡å‹çš„å¾®è°ƒæˆä¸ºå¯èƒ½ï¼Œå¦‚LLaMA3.1-70Bã€‚PaCAè¿˜æé«˜äº†åºåˆ—é•¿åº¦å’Œååé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ç®—æ³•å¯ä»¥å‡å°‘å¤§è§„æ¨¡ç¥ç»ç½‘ç»œæ¨¡å‹çš„è®¡ç®—èµ„æºå’Œå†…å­˜ä½¿ç”¨ã€‚</li>
<li>PEFTå¹¶ä¸èƒ½ç›´æ¥å‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå› ä¸ºé€‚é…å™¨å±‚å’Œé¢„è®­ç»ƒå±‚çš„å¤„ç†å­˜åœ¨å»¶è¿Ÿå¼€é”€ã€‚</li>
<li>éƒ¨åˆ†è¿æ¥é€‚é…ï¼ˆPaCAï¼‰æ–¹æ³•é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæƒé‡ä¸­çš„éƒ¨åˆ†è¿æ¥æ¥æé«˜è®­ç»ƒé€Ÿåº¦å’Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚</li>
<li>PaCAç›¸è¾ƒäºLoRAæ–¹æ³•å¯ä»¥å‡å°‘è®­ç»ƒæ—¶é—´å’Œæ€»å†…å­˜ä½¿ç”¨ã€‚</li>
<li>PaCAå¯ä»¥ç»´æŒåœ¨ä¸åŒå¾®è°ƒåœºæ™¯ä¸‹çš„å‡†ç¡®åº¦ï¼Œå¦‚MMLUæ•°æ®é›†ä¸Šçš„å¾®è°ƒä»¥åŠOasst1æ•°æ®é›†ä¸Šçš„æŒ‡ä»¤å¾®è°ƒã€‚</li>
<li>PaCAå¯ä»¥ä¸é‡åŒ–ç»“åˆï¼Œä½¿å¤§å‹æ¨¡å‹çš„å¾®è°ƒæˆä¸ºå¯èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fea4602eb3dd8b54e82c46d5f68a874b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ca70077b7eea935ee161e2b872ea259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c31f5d31b8b8c07a3cf173277a1fe769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-578ca7e29e65ec372bdbe8b9980966ce.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Language-Assisted-Feature-Transformation-for-Anomaly-Detection"><a href="#Language-Assisted-Feature-Transformation-for-Anomaly-Detection" class="headerlink" title="Language-Assisted Feature Transformation for Anomaly Detection"></a>Language-Assisted Feature Transformation for Anomaly Detection</h2><p><strong>Authors:EungGu Yun, Heonjin Ha, Yeongwoo Nam, Bryan Dongik Lee</strong></p>
<p>This paper introduces LAFT, a novel feature transformation method designed to incorporate user knowledge and preferences into anomaly detection using natural language. Accurately modeling the boundary of normality is crucial for distinguishing abnormal data, but this is often challenging due to limited data or the presence of nuisance attributes. While unsupervised methods that rely solely on data without user guidance are common, they may fail to detect anomalies of specific interest. To address this limitation, we propose Language-Assisted Feature Transformation (LAFT), which leverages the shared image-text embedding space of vision-language models to transform visual features according to user-defined requirements. Combined with anomaly detection methods, LAFT effectively aligns visual features with user preferences, allowing anomalies of interest to be detected. Extensive experiments on both toy and real-world datasets validate the effectiveness of our method. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†LAFTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç‰¹å¾è½¬æ¢æ–¹æ³•ï¼Œæ—¨åœ¨å°†ç”¨æˆ·çŸ¥è¯†å’Œåå¥½èå…¥è‡ªç„¶è¯­è¨€å¼‚å¸¸æ£€æµ‹ä¸­ã€‚å‡†ç¡®å»ºæ¨¡æ­£å¸¸è¾¹ç•Œå¯¹äºåŒºåˆ†å¼‚å¸¸æ•°æ®è‡³å…³é‡è¦ï¼Œä½†ç”±äºæ•°æ®æœ‰é™æˆ–å­˜åœ¨å¹²æ‰°å±æ€§ï¼Œè¿™é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶ä»…ä¾èµ–æ•°æ®è€Œæ— éœ€ç”¨æˆ·æŒ‡å¯¼çš„æ— ç›‘ç£æ–¹æ³•å¾ˆå¸¸è§ï¼Œä½†å®ƒä»¬å¯èƒ½æ— æ³•æ£€æµ‹åˆ°ç‰¹å®šæ„Ÿå…´è¶£çš„å¼‚å¸¸å€¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­è¨€è¾…åŠ©ç‰¹å¾è½¬æ¢ï¼ˆLAFTï¼‰ï¼Œå®ƒåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å…±äº«å›¾åƒæ–‡æœ¬åµŒå…¥ç©ºé—´ï¼Œæ ¹æ®ç”¨æˆ·å®šä¹‰çš„è¦æ±‚è½¬æ¢è§†è§‰ç‰¹å¾ã€‚ç»“åˆå¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼ŒLAFTæœ‰æ•ˆåœ°å°†è§†è§‰ç‰¹å¾ä¸ç”¨æˆ·åå¥½å¯¹é½ï¼Œä»è€Œå¯ä»¥æ£€æµ‹åˆ°æ„Ÿå…´è¶£çš„å¼‚å¸¸å€¼ã€‚åœ¨ç©å…·å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01184v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>LAFTæ˜¯ä¸€ç§ç»“åˆç”¨æˆ·çŸ¥è¯†å’Œåå¥½è¿›è¡Œè‡ªç„¶è¯­è¨€å¼‚å¸¸æ£€æµ‹çš„æ–°é¢–ç‰¹å¾è½¬æ¢æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å›¾åƒæ–‡æœ¬åµŒå…¥ç©ºé—´ï¼Œæ ¹æ®ç”¨æˆ·å®šä¹‰çš„éœ€æ±‚è½¬æ¢è§†è§‰ç‰¹å¾ï¼Œæœ‰æ•ˆåœ°å°†è§†è§‰ç‰¹å¾ä¸ç”¨æˆ·éœ€æ±‚å¯¹é½ï¼Œä»è€Œæ£€æµ‹æœ‰è¶£çš„å¼‚å¸¸å€¼ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç©å…·å’ŒçœŸå®æ•°æ®é›†ä¸Šå‡æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LAFTæ˜¯ä¸€ç§æ–°çš„ç‰¹å¾è½¬æ¢æ–¹æ³•ï¼Œæ—¨åœ¨ç»“åˆç”¨æˆ·çŸ¥è¯†å’Œåå¥½è¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å›¾åƒæ–‡æœ¬åµŒå…¥ç©ºé—´è¿›è¡Œè§†è§‰ç‰¹å¾çš„è½¬æ¢ã€‚</li>
<li>LAFTå¯ä»¥æœ‰æ•ˆåœ°å°†è§†è§‰ç‰¹å¾ä¸ç”¨æˆ·éœ€æ±‚å¯¹é½ï¼Œä»è€Œæ£€æµ‹æœ‰è¶£çš„å¼‚å¸¸å€¼ã€‚</li>
<li>æœ‰é™çš„æ•°æ®æˆ–å¹²æ‰°å±æ€§ç»™å‡†ç¡®å»ºæ¨¡æ­£å¸¸èŒƒå›´çš„è¾¹ç•Œå¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿçš„æ— ç›‘ç£æ–¹æ³•å¯èƒ½æ— æ³•æ£€æµ‹åˆ°ç‰¹å®šçš„å¼‚å¸¸å€¼ã€‚</li>
<li>LAFTé€šè¿‡ä¸å¼‚å¸¸æ£€æµ‹æ–¹æ³•ç›¸ç»“åˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e7f7378acf8d55dc3d9f1f5377fb4b1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ea9c0eb8d8fe52385abdae9a667dd21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c951ab25b94efbb64aaec845fad0797e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a1a018327994db35d581fd0b8b88a13.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Unify-and-Anchor-A-Context-Aware-Transformer-for-Cross-Domain-Time-Series-Forecasting"><a href="#Unify-and-Anchor-A-Context-Aware-Transformer-for-Cross-Domain-Time-Series-Forecasting" class="headerlink" title="Unify and Anchor: A Context-Aware Transformer for Cross-Domain Time   Series Forecasting"></a>Unify and Anchor: A Context-Aware Transformer for Cross-Domain Time   Series Forecasting</h2><p><strong>Authors:Xiaobin Hong, Jiawen Zhang, Wenzhong Li, Sanglu Lu, Jia Li</strong></p>
<p>The rise of foundation models has revolutionized natural language processing and computer vision, yet their best practices to time series forecasting remains underexplored. Existing time series foundation models often adopt methodologies from these fields without addressing the unique characteristics of time series data. In this paper, we identify two key challenges in cross-domain time series forecasting: the complexity of temporal patterns and semantic misalignment. To tackle these issues, we propose the &#96;&#96;Unify and Anchorâ€ transfer paradigm, which disentangles frequency components for a unified perspective and incorporates external context as domain anchors for guided adaptation. Based on this framework, we introduce ContexTST, a Transformer-based model that employs a time series coordinator for structured representation and the Transformer blocks with a context-informed mixture-of-experts mechanism for effective cross-domain generalization. Extensive experiments demonstrate that ContexTST advances state-of-the-art forecasting performance while achieving strong zero-shot transferability across diverse domains. </p>
<blockquote>
<p>éšç€åŸºç¡€æ¨¡å‹çš„å…´èµ·ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå‘ç”Ÿäº†é©å‘½æ€§çš„å˜åŒ–ï¼Œç„¶è€Œå®ƒä»¬åœ¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢çš„æœ€ä½³å®è·µä»ç„¶è¢«æ¢ç´¢ä¸è¶³ã€‚ç°æœ‰çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹å¾€å¾€é‡‡ç”¨è¿™äº›é¢†åŸŸçš„æ–¹æ³•ï¼Œè€Œæ²¡æœ‰è§£å†³æ—¶é—´åºåˆ—æ•°æ®çš„ç‹¬ç‰¹ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†è·¨åŸŸæ—¶é—´åºåˆ—é¢„æµ‹çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ—¶é—´æ¨¡å¼çš„å¤æ‚æ€§å’Œè¯­ä¹‰ä¸å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œç»Ÿä¸€ä¸é”šç‚¹â€è¿ç§»èŒƒå¼ï¼Œè¯¥èŒƒå¼ä»ç»Ÿä¸€çš„è§’åº¦è§£å†³é¢‘ç‡æˆåˆ†ï¼Œå¹¶å°†å¤–éƒ¨ä¸Šä¸‹æ–‡ä½œä¸ºåŸŸé”šç‚¹è¿›è¡Œå¼•å¯¼é€‚åº”ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ContexTSTæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„æ¨¡å‹ï¼Œé‡‡ç”¨æ—¶é—´åºåˆ—åè°ƒå™¨è¿›è¡Œç»“æ„åŒ–è¡¨ç¤ºï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ··åˆä¸“å®¶æœºåˆ¶çš„Transformerå—ï¼Œå®ç°æœ‰æ•ˆçš„è·¨åŸŸæ³›åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒContexTSTæ¨¡å‹åœ¨é¢„æµ‹æ€§èƒ½ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„è¿›å±•ï¼ŒåŒæ—¶åœ¨ä¸åŒçš„é¢†åŸŸå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01157v1">PDF</a> 20 pages, 12 figures, 8 tables, conference under review</p>
<p><strong>Summary</strong><br>æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸä¸­ï¼Œè·¨åŸŸæ¨¡å‹çš„åº”ç”¨å­˜åœ¨ä¸¤å¤§æŒ‘æˆ˜ï¼šæ—¶é—´æ¨¡å¼çš„å¤æ‚æ€§å’Œè¯­ä¹‰ä¸åŒ¹é…é—®é¢˜ã€‚æœ¬æ–‡æå‡ºâ€œç»Ÿä¸€ä¸é”šå®šâ€è½¬ç§»èŒƒå¼æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä»‹ç»äº†åŸºäºTransformerçš„ContexTSTæ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ—¶é—´åºåˆ—åè°ƒå™¨è¿›è¡Œç»“æ„åŒ–è¡¨ç¤ºï¼Œå¹¶å¼•å…¥å¸¦æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ··åˆä¸“å®¶æœºåˆ¶æ¥å®ç°è·¨åŸŸæ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒContexTSTåœ¨å…ˆè¿›çš„æ—¶é—´åºåˆ—é¢„æµ‹æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼ŒåŒæ—¶åœ¨ä¸åŒçš„é¢†åŸŸä¹‹é—´å®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸä¸­è·¨åŸŸæ¨¡å‹é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ—¶é—´æ¨¡å¼çš„å¤æ‚æ€§å’Œè¯­ä¹‰ä¸åŒ¹é…ã€‚</li>
<li>â€œç»Ÿä¸€ä¸é”šå®šâ€è½¬ç§»èŒƒå¼è¢«æå‡ºä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥èŒƒå¼æ—¨åœ¨ä»ç»Ÿä¸€çš„è§’åº¦è§£æé¢‘ç‡æˆåˆ†ï¼Œå¹¶é€šè¿‡å¼•å…¥é¢†åŸŸé”šç‚¹å®ç°å¼•å¯¼é€‚åº”ã€‚</li>
<li>ContexTSTæ˜¯ä¸€ä¸ªåŸºäºTransformerçš„æ¨¡å‹ï¼Œé‡‡ç”¨æ—¶é—´åºåˆ—åè°ƒå™¨è¿›è¡Œç»“æ„åŒ–è¡¨ç¤ºã€‚</li>
<li>ContexTSTæ¨¡å‹ä½¿ç”¨å¸¦æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ··åˆä¸“å®¶æœºåˆ¶ï¼Œä»¥æé«˜è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºContexTSTåœ¨å…ˆè¿›çš„æ—¶é—´åºåˆ—é¢„æµ‹æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ContexTSTæ¨¡å‹åœ¨ä¸åŒçš„é¢†åŸŸä¹‹é—´å®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4bb9a9c32848f763d30c8f3684fc288d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a075aaa03d259e8a2126e58e5cc8648d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28800b2adb99501c81d3796f15cfea05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ade2db812bdb51bc43e11bc8db4290be.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Personalize-Your-LLM-Fake-it-then-Align-it"><a href="#Personalize-Your-LLM-Fake-it-then-Align-it" class="headerlink" title="Personalize Your LLM: Fake it then Align it"></a>Personalize Your LLM: Fake it then Align it</h2><p><strong>Authors:Yijing Zhang, Dyah Adila, Changho Shin, Frederic Sala</strong></p>
<p>Personalizing large language models (LLMs) is essential for delivering tailored interactions that improve user experience. Many existing personalization methods require fine-tuning LLMs for each user, rendering them prohibitively expensive for widespread adoption. Although retrieval-based approaches offer a more compute-efficient alternative, they still depend on large, high-quality datasets that are not consistently available for all users. To address this challenge, we propose CHAMELEON, a scalable and efficient personalization approach that uses (1) self-generated personal preference data and (2) representation editing to enable quick and cost-effective personalization. Our experiments on various tasks, including those from the LaMP personalization benchmark, show that CHAMELEON efficiently adapts models to personal preferences, improving instruction-tuned models and outperforms two personalization baselines by an average of 40% across two model architectures. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äºæä¾›å®šåˆ¶åŒ–çš„äº¤äº’ä½“éªŒè‡³å…³é‡è¦ï¼Œè¿™å¯ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒã€‚è®¸å¤šç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•éœ€è¦å¯¹æ¯ä¸ªç”¨æˆ·è¿›è¡Œå¾®è°ƒLLMï¼Œè¿™ä½¿å¾—å®ƒä»¬å¹¿æ³›åº”ç”¨çš„æˆæœ¬è¿‡é«˜ã€‚è™½ç„¶åŸºäºæ£€ç´¢çš„æ–¹æ³•æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„è®¡ç®—æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å®ƒä»¬ä»ç„¶ä¾èµ–äºå¹¶éæ‰€æœ‰ç”¨æˆ·éƒ½å¯ç”¨çš„é«˜è´¨é‡å¤§å‹æ•°æ®é›†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CHAMELEONï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œå®ƒé‡‡ç”¨ï¼ˆ1ï¼‰è‡ªæˆ‘ç”Ÿæˆçš„ä¸ªæ€§åŒ–åå¥½æ•°æ®ä»¥åŠï¼ˆ2ï¼‰è¡¨ç¤ºç¼–è¾‘æ¥å®ç°å¿«é€Ÿä¸”ç»æµçš„ä¸ªæ€§åŒ–ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬æ¥è‡ªLaMPä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•çš„å„ç§ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCHAMELEONå¯ä»¥æœ‰æ•ˆåœ°é€‚åº”ä¸ªäººåå¥½ï¼Œæ”¹è¿›æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œå¹¶åœ¨ä¸¤ç§æ¨¡å‹æ¶æ„ä¸Šå¹³å‡ä¼˜äºä¸¤ä¸ªä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•40%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01048v3">PDF</a> NAACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>LLMä¸ªæ€§åŒ–å¯¹äºæä¾›æ”¹å–„ç”¨æˆ·ä½“éªŒçš„å®šåˆ¶äº¤äº’è‡³å…³é‡è¦ã€‚ç°æœ‰ä¸ªäººåŒ–æ–¹æ³•éœ€è¦å¤§é‡é’ˆå¯¹æ¯ä¸ªç”¨æˆ·çš„å¾®è°ƒï¼Œå¯¼è‡´æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥å¹¿æ³›åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºCHAMELEONæ–¹æ³•ï¼Œé€šè¿‡ï¼ˆ1ï¼‰è‡ªæˆ‘ç”Ÿæˆçš„ä¸ªäººåå¥½æ•°æ®å’Œï¼ˆ2ï¼‰è¡¨ç¤ºç¼–è¾‘ï¼Œå®ç°å¿«é€Ÿã€ç»æµçš„ä¸ªæ€§åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒCHAMELEONåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬LaMPä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤Ÿé«˜æ•ˆé€‚åº”ä¸ªäººåå¥½ï¼Œæ”¹è¿›æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œå¹¶åœ¨ä¸¤ç§æ¨¡å‹æ¶æ„ä¸Šå¹³å‡ä¼˜äºä¸¤ä¸ªä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•40%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä¸ªæ€§åŒ–å¯¹äºæå‡ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ä¸ªäººåŒ–æ–¹æ³•å› éœ€è¦å¤§é‡é’ˆå¯¹æ¯ä¸ªç”¨æˆ·çš„å¾®è°ƒè€Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>CHAMELEONæ–¹æ³•é€šè¿‡ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„ä¸ªäººåå¥½æ•°æ®å’Œè¡¨ç¤ºç¼–è¾‘å®ç°å¿«é€Ÿã€ç»æµçš„ä¸ªæ€§åŒ–ã€‚</li>
<li>CHAMELEONåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬LaMPä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•ã€‚</li>
<li>CHAMELEONèƒ½å¤Ÿé«˜æ•ˆé€‚åº”ä¸ªäººåå¥½ï¼Œæ”¹è¿›æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€‚</li>
<li>CHAMELEONåœ¨ä¸¤ç§æ¨¡å‹æ¶æ„ä¸Šçš„æ€§èƒ½å¹³å‡ä¼˜äºä¸¤ä¸ªä¸ªæ€§åŒ–åŸºå‡†æµ‹è¯•40%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e7528df0739dd6284f28b9f49e225955.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c4280d8af0e7ce713926a954a20205a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da58698d5d8d79dbbf8b09c85202db5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b586a3093133a8c90cb29001e4c8207.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Re-Imagining-Multimodal-Instruction-Tuning-A-Representation-View"><a href="#Re-Imagining-Multimodal-Instruction-Tuning-A-Representation-View" class="headerlink" title="Re-Imagining Multimodal Instruction Tuning: A Representation View"></a>Re-Imagining Multimodal Instruction Tuning: A Representation View</h2><p><strong>Authors:Yiyang Liu, James Chenhao Liang, Ruixiang Tang, Yugyung Lee, Majid Rabbani, Sohail Dianat, Raghuveer Rao, Lifu Huang, Dongfang Liu, Qifan Wang, Cheng Han</strong></p>
<p>Multimodal instruction tuning has proven to be an effective strategy for achieving zero-shot generalization by fine-tuning pre-trained Large Multimodal Models (LMMs) with instruction-following data. However, as the scale of LMMs continues to grow, fully fine-tuning these models has become highly parameter-intensive. Although Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced to reduce the number of tunable parameters, a significant performance gap remains compared to full fine-tuning. Furthermore, existing PEFT approaches are often highly parameterized, making them difficult to interpret and control. In light of this, we introduce Multimodal Representation Tuning (MRT), a novel approach that focuses on directly editing semantically rich multimodal representations to achieve strong performance and provide intuitive control over LMMs. Empirical results show that our method surpasses current state-of-the-art baselines with significant performance gains (e.g., 1580.40 MME score) while requiring substantially fewer tunable parameters (e.g., 0.03% parameters). Additionally, we conduct experiments on editing instrumental tokens within multimodal representations, demonstrating that direct manipulation of these representations enables simple yet effective control over network behavior. </p>
<blockquote>
<p>å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´å·²è¢«è¯æ˜æ˜¯ä¸€ç§é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸éµå¾ªæŒ‡ä»¤çš„æ•°æ®ï¼Œå®ç°é›¶å°„å‡»æ³›åŒ–çš„æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œéšç€LMMè§„æ¨¡çš„ä¸æ–­å¢é•¿ï¼Œå®Œå…¨å¾®è°ƒè¿™äº›æ¨¡å‹å·²ç»å˜å¾—éå¸¸å‚æ•°å¯†é›†ã€‚è™½ç„¶å·²å¼•å…¥å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•æ¥å‡å°‘å¯è°ƒå‚æ•°çš„æ•°é‡ï¼Œä½†ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼Œä»å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„PEFTæ–¹æ³•é€šå¸¸é«˜åº¦å‚æ•°åŒ–ï¼Œä½¿å¾—å®ƒä»¬éš¾ä»¥è§£é‡Šå’Œæ§åˆ¶ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€è¡¨ç¤ºè°ƒæ•´ï¼ˆMRTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œä¸“æ³¨äºç›´æ¥ç¼–è¾‘è¯­ä¹‰ä¸°å¯Œçš„å¤šæ¨¡æ€è¡¨ç¤ºï¼Œä»¥å®ç°å¼ºå¤§çš„æ€§èƒ½å’Œç›´è§‚æ§åˆ¶LMMsã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¾¾åˆ°æœ€æ–°åŸºçº¿æ°´å¹³çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ˆä¾‹å¦‚ï¼Œ1580.40 MMEå¾—åˆ†ï¼‰ï¼ŒåŒæ—¶éœ€è¦çš„å¯è°ƒå‚æ•°å¤§å¤§å‡å°‘ï¼ˆä¾‹å¦‚ï¼Œä»…ä½¿ç”¨ç™¾åˆ†ä¹‹é›¶ç‚¹é›¶ä¸‰å‚æ•°ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨ç¼–è¾‘å¤šæ¨¡æ€è¡¨ç¤ºä¸­çš„ä»ªå™¨ç¬¦å·æ–¹é¢è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ç›´æ¥æ“ä½œè¿™äº›è¡¨ç¤ºå¯ä»¥ç®€å•æœ‰æ•ˆåœ°æ§åˆ¶ç½‘ç»œè¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00723v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ˜¯ä¸€ç§é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä»¥å®ç°é›¶æ ·æœ¬æ³›åŒ–çš„æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œéšç€LMMsè§„æ¨¡çš„æ‰©å¤§ï¼Œå®Œå…¨å¾®è°ƒè¿™äº›æ¨¡å‹å˜å¾—æä¸ºå‚æ•°å¯†é›†ã€‚å°½ç®¡å·²ç»å¼•å…¥äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•æ¥å‡å°‘å¯è°ƒå‚æ•°çš„æ•°é‡ï¼Œä½†ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ä»å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€è¡¨ç¤ºè°ƒæ•´ï¼ˆMRTï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥ç¼–è¾‘è¯­ä¹‰ä¸°å¯Œçš„å¤šæ¨¡æ€è¡¨ç¤ºï¼Œä»¥å®ç°å¼ºå¤§çš„æ€§èƒ½å’Œç›´è§‚æ§åˆ¶LMMsçš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ˆä¾‹å¦‚ï¼ŒMMEåˆ†æ•°ä¸º1580.40ï¼‰çš„åŒæ—¶ï¼Œæ‰€éœ€çš„å¯è°ƒå‚æ•°å¤§å¹…å‡å°‘ï¼ˆä¾‹å¦‚ï¼Œä»…0.03%çš„å‚æ•°ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ç¼–è¾‘ä»ªå™¨ç¬¦å·çš„ä»¤ç‰Œå†…å¤šæ¨¡æ€è¡¨ç¤ºçš„æµ‹è¯•ï¼Œè¯æ˜äº†ç›´æ¥æ“ä½œè¿™äº›è¡¨ç¤ºèƒ½ç®€å•æœ‰æ•ˆåœ°æ§åˆ¶ç½‘ç»œè¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ˜¯ä¸€ç§æœ‰æ•ˆçš„é›¶æ ·æœ¬æ³›åŒ–ç­–ç•¥ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚</li>
<li>éšç€LMMsè§„æ¨¡çš„å¢é•¿ï¼Œå®Œå…¨å¾®è°ƒæ¨¡å‹å˜å¾—æä¸ºå‚æ•°å¯†é›†ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•è™½ç„¶å¯ä»¥å‡å°‘å¯è°ƒå‚æ•°æ•°é‡ï¼Œä½†ä»å­˜åœ¨ä¸å…¨å¾®è°ƒæ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚</li>
<li>å¤šæ¨¡æ€è¡¨ç¤ºè°ƒæ•´ï¼ˆMRTï¼‰æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œç›´æ¥ç¼–è¾‘è¯­ä¹‰ä¸°å¯Œçš„å¤šæ¨¡æ€è¡¨ç¤ºï¼Œä»¥å®ç°å¼ºå¤§çš„æ€§èƒ½å’Œç›´è§‚æ§åˆ¶LMMsã€‚</li>
<li>MRTæ–¹æ³•æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒMMEåˆ†æ•°ä¸º1580.40ï¼‰å¹¶å¤§å¹…å‡å°‘äº†å¯è°ƒå‚æ•°çš„éœ€æ±‚ï¼ˆä¾‹å¦‚ï¼Œä»…0.03%çš„å‚æ•°ï¼‰ã€‚</li>
<li>ç›´æ¥ç¼–è¾‘å¤šæ¨¡æ€è¡¨ç¤ºä¸­çš„ä»ªå™¨ç¬¦å·ä»¤ç‰Œå¯ä»¥ç®€å•æœ‰æ•ˆåœ°æ§åˆ¶ç½‘ç»œè¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd945f663df508cf4d08b1fc98de8c50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9331da710b49f23cf7564c1bfbf1c1b0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information"><a href="#Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information" class="headerlink" title="Transformer Meets Twicing: Harnessing Unattended Residual Information"></a>Transformer Meets Twicing: Harnessing Unattended Residual Information</h2><p><strong>Authors:Laziz Abdullaev, Tan Nguyen</strong></p>
<p>Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses kernel twicing procedure in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees and enhanced adversarial robustness. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved robustness and accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å·²åœ¨ä¼—å¤šè¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è™½ç„¶Transformerçš„æ ¸å¿ƒç»„ä»¶è‡ªæ³¨æ„åŠ›æœºåˆ¶å·²è¯æ˜èƒ½å¤Ÿå¤„ç†å¤æ‚çš„æ•°æ®æ¨¡å¼ï¼Œä½†äººä»¬è§‚å¯Ÿåˆ°ï¼Œéšç€Transformerå±‚çš„å¢åŠ ï¼Œæ³¨æ„åŠ›çŸ©é˜µçš„è¡¨ç¤ºèƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ï¼Œä»è€ŒæŸå®³å…¶æ€»ä½“æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸ä½é€šéå±€éƒ¨å‡å€¼ï¼ˆNLMï¼‰å¹³æ»‘æ»¤æ³¢å™¨ä¹‹é—´çš„è”ç³»ï¼Œæå‡ºTwicing Attentionè¿™ä¸€æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ã€‚å®ƒé‡‡ç”¨éå‚æ•°å›å½’ä¸­çš„æ ¸Twicingç¨‹åºï¼Œä»¥ç¼“è§£ä¸NLMå¹³æ»‘ç›¸å…³çš„ä½é€šè¡Œä¸ºï¼Œå…·æœ‰å¼•äººæ³¨ç›®çš„ç†è®ºä¿è¯å’Œå¢å¼ºçš„å¯¹æŠ—é²æ£’æ€§ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæå–å’Œå†åˆ©ç”¨æ¯å±‚ä¸å®Œç¾çš„å¹³æ»‘æ“ä½œåæ®‹ç•™ä¸­çš„æœ‰æ„ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºæ ‡å‡†çš„è‡ªæ³¨æ„åŠ›å…·æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜ç‚¹ï¼š1ï¼‰è¡¨ç¤ºèƒ½åŠ›çš„è¡°å‡é€Ÿåº¦è¾ƒæ…¢ï¼›2ï¼‰åœ¨å„ç§æ•°æ®æ¨¡æ€å’Œä»»åŠ¡ä¸­æé«˜äº†é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸Šå®è¯äº†æˆ‘ä»¬çš„æ¨¡å‹ç›¸å¯¹äºåŸºçº¿Transformerçš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€è¯­è¨€å»ºæ¨¡ä»¥åŠåœ¨å¹²å‡€å’ŒæŸåæ•°æ®ä¸Šçš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00687v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„æ·±åº¦æ¨¡å‹å·²åœ¨å¤šä¸ªè¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸­å–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æœ¬æ–‡å‘ç°Transformerä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†å¤æ‚æ•°æ®æ¨¡å¼æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†æ³¨æ„åŠ›çŸ©é˜µåœ¨Transformerå±‚é—´çš„è¡¨ç¤ºèƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼Œå½±å“äº†æ•´ä½“æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡åˆ©ç”¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸ä½é€šéå±€éƒ¨å‡å€¼å¹³æ»‘æ»¤æ³¢ä¹‹é—´çš„è”ç³»ï¼Œæå‡ºäº†Twicing Attentionæ–°å‹æ³¨æ„åŠ›æœºåˆ¶ã€‚å®ƒé‡‡ç”¨éå‚æ•°å›å½’ä¸­çš„æ ¸æ‰­æ›²è¿‡ç¨‹æ¥ç¼“è§£ä¸NLMå¹³æ»‘ç›¸å…³çš„ä½é€šè¡Œä¸ºï¼Œå…·æœ‰ä»¤äººä¿¡æœçš„ç†è®ºä¿è¯å’Œå¢å¼ºçš„å¯¹æŠ—é²æ£’æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæå–å’Œå†åˆ©ç”¨æ¯ä¸€å±‚ä¸å®Œç¾å¹³æ»‘æ“ä½œåçš„æ®‹å·®ä¸­ä¿ç•™çš„æœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚ä¸æ ‡å‡†è‡ªæ³¨æ„åŠ›ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•æä¾›äº†ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š1ï¼‰è¯æ˜äº†çš„è¡¨ç¤ºèƒ½åŠ›è¡°å‡è¾ƒæ…¢ï¼›2ï¼‰æé«˜äº†åœ¨å„ç§æ•°æ®æ¨¡æ€å’Œä»»åŠ¡ä¸Šçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸Šï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œè¯­è¨€å»ºæ¨¡ï¼Œä»¥åŠåœ¨å¹²å‡€å’ŒæŸåçš„æ•°æ®ä¸Šï¼Œæœ¬æ–‡æ¨¡å‹å‡å®ç°äº†æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†è‡ªæ³¨æ„åŠ›æœºåˆ¶å­˜åœ¨è¡¨ç¤ºèƒ½åŠ›é€€åŒ–é—®é¢˜ã€‚</li>
<li>é€€åŒ–é—®é¢˜å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ•°æ®æ¨¡å¼ä¸‹ã€‚</li>
<li>æå‡ºäº†Twicing Attentionæœºåˆ¶ï¼Œç»“åˆè‡ªæ³¨æ„åŠ›ä¸éå±€éƒ¨å‡å€¼å¹³æ»‘æ»¤æ³¢ï¼Œä»¥ç¼“è§£è¡¨ç¤ºèƒ½åŠ›é€€åŒ–é—®é¢˜ã€‚</li>
<li>Twicing Attentionå…·æœ‰ç†è®ºä¿è¯å’Œå¢å¼ºçš„å¯¹æŠ—é²æ£’æ€§ã€‚</li>
<li>è¯¥æœºåˆ¶èƒ½å¤Ÿæå–å¹¶å†åˆ©ç”¨å±‚é—´å¹³æ»‘æ“ä½œåçš„æ®‹å·®ä¿¡æ¯ã€‚</li>
<li>ä¸æ ‡å‡†è‡ªæ³¨æ„åŠ›ç›¸æ¯”ï¼ŒTwicing Attentionæä¾›è¾ƒæ…¢çš„è¡¨ç¤ºèƒ½åŠ›è¡°å‡å’Œæ›´é«˜çš„é²æ£’æ€§ã€å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdb16c3fb2b896f0691dbe7ee7e66f8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33dbab91cb18b20834a539c9c39cbb6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da29fd57acbcffe975cd8e5f69f5a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75b545dbd23498a3ef47b47130e8b6f2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning"><a href="#Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning" class="headerlink" title="Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning"></a>Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning</h2><p><strong>Authors:Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, Jianke Zhu</strong></p>
<p>Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene, but also compromises training and inference efficiency. To address these challenges, we propose a unified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously. To obtain the fine-grained instance-level visual tokens, we first introduce a novel Multi-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D semantics into their corresponding 3D geometric features. For scene-level relation-aware tokens, we further present a 3D Instance Spatial Relation (3D-ISR) module to capture the intricate pairwise spatial relationships among objects. Additionally, we perform end-to-end multi-task instruction tuning simultaneously without the subsequent task-specific fine-tuning. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods across 3D scene understanding, reasoning and grounding tasks. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a> </p>
<blockquote>
<p>å°½ç®¡åœ¨3Dåœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ï¼Œä½†å¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚3Dç¯å¢ƒä¸­è¿›è¡Œç†è§£å’Œæ¨ç†çš„æœ‰æ•ˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¤§å¤šæ•°ä¹‹å‰çš„æ–¹æ³•é€šå¸¸åˆ†åˆ«ç¼–ç 3Dç‚¹å’Œ2Då›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†2Dè¯­ä¹‰å’Œ3Då¯¹è±¡å±æ€§ä¹‹é—´çš„äº¤äº’ï¼Œä»¥åŠ3Dç¯å¢ƒå†…çš„ç©ºé—´å…³ç³»ã€‚è¿™ç§å±€é™æ€§ä¸ä»…é˜»ç¢äº†å¯¹3Dåœºæ™¯çš„å…¨é¢è¡¨ç¤ºï¼Œè¿˜å½±å“äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00513v1">PDF</a> CVPR2025, Code Link: <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºInst3D-LMMçš„ç»Ÿä¸€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œç”¨äºåŒæ—¶å¤„ç†å¤šä¸ª3Dåœºæ™¯ç†è§£ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥å¤šè§†å›¾è·¨æ¨¡æ€èåˆï¼ˆMCMFï¼‰æ¨¡å—ï¼Œå°†å¤šè§†å›¾2Dè¯­ä¹‰æ³¨å…¥åˆ°ç›¸åº”çš„3Då‡ ä½•ç‰¹å¾ä¸­ï¼Œè·å–ç²¾ç»†çš„å®ä¾‹çº§è§†è§‰ä»¤ç‰Œã€‚åŒæ—¶ï¼Œé€šè¿‡3Då®ä¾‹ç©ºé—´å…³ç³»ï¼ˆ3D-ISRï¼‰æ¨¡å—ï¼Œæ•æ‰å¯¹è±¡ä¹‹é—´çš„å¤æ‚æˆå¯¹ç©ºé—´å…³ç³»ï¼Œå®ç°åœºæ™¯çº§å…³ç³»æ„ŸçŸ¥ä»¤ç‰Œçš„è·å–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3Dåœºæ™¯ç†è§£ã€æ¨ç†å’Œå®šä½ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§åä¸ºInst3D-LMMçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œç”¨äº3Dåœºæ™¯ç†è§£ã€‚</li>
<li>å¼•å…¥å¤šè§†å›¾è·¨æ¨¡æ€èåˆï¼ˆMCMFï¼‰æ¨¡å—ï¼Œå°†2Dè¯­ä¹‰ä¸3Då‡ ä½•ç‰¹å¾ç›¸ç»“åˆã€‚</li>
<li>é€šè¿‡3Då®ä¾‹ç©ºé—´å…³ç³»ï¼ˆ3D-ISRï¼‰æ¨¡å—ï¼Œæ•æ‰å¯¹è±¡ä¹‹é—´çš„å¤æ‚ç©ºé—´å…³ç³»ã€‚</li>
<li>æ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ª3Dåœºæ™¯ç†è§£ä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½åœ¨å¤šä¸ª3Dåœºæ™¯ç†è§£ã€æ¨ç†å’Œå®šä½ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æä¾›äº†æ¨¡å‹çš„æºä»£ç ï¼Œå¯ä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-435a00e774f1b78b5e4b1a35151bc07b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-013e502dbdde407d4c199e6edd2e484b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08b1d0d8ad78774e002c26662a40445e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-020ecc80c8721387c2ad327205b752ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f824df1622d27712f8489f755359a28f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4001a43fec328517b4489305b6e11fd3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Assessing-Correctness-in-LLM-Based-Code-Generation-via-Uncertainty-Estimation"><a href="#Assessing-Correctness-in-LLM-Based-Code-Generation-via-Uncertainty-Estimation" class="headerlink" title="Assessing Correctness in LLM-Based Code Generation via Uncertainty   Estimation"></a>Assessing Correctness in LLM-Based Code Generation via Uncertainty   Estimation</h2><p><strong>Authors:Arindam Sharma, Cristina David</strong></p>
<p>In this work, we explore uncertainty estimation as a proxy for correctness in LLM-generated code. To this end, we adapt two state-of-the-art techniques from natural language generation â€“ one based on entropy and another on mutual information â€“ to the domain of code generation. Given the distinct semantic properties of code, we introduce modifications, including a semantic equivalence check based on symbolic execution. Our findings indicate a strong correlation between the uncertainty computed through these techniques and correctness, highlighting the potential of uncertainty estimation for quality assessment. Additionally, we propose a simplified version of the entropy-based method that assumes a uniform distribution over the LLMâ€™s responses, demonstrating comparable effectiveness. Using these techniques, we develop an abstention policy that prevents the model from making predictions when uncertainty is high, reducing incorrect outputs to near zero. Our evaluation on the LiveCodeBench shows that our approach significantly outperforms a baseline relying solely on LLM-reported log-probabilities. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢å°†ä¸ç¡®å®šæ€§ä¼°è®¡ä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä»£ç æ­£ç¡®æ€§çš„ä»£ç†æŒ‡æ ‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆé¢†åŸŸé‡‡ç”¨äº†ä¸¤ç§å…ˆè¿›æŠ€æœ¯â€”â€”ä¸€ç§åŸºäºç†µï¼Œå¦ä¸€ç§åŸºäºäº’ä¿¡æ¯â€”â€”å¹¶å°†å…¶åº”ç”¨äºä»£ç ç”Ÿæˆé¢†åŸŸã€‚è€ƒè™‘åˆ°ä»£ç çš„ç‹¬ç‰¹è¯­ä¹‰å±æ€§ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ï¼ŒåŒ…æ‹¬åŸºäºç¬¦å·æ‰§è¡Œçš„è¯­ä¹‰ç­‰ä»·æ€§æ£€æŸ¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è¿™äº›æŠ€æœ¯è®¡ç®—çš„ä¸ç¡®å®šæ€§ä¸æ­£ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼Œçªå‡ºäº†ä¸ç¡®å®šæ€§ä¼°è®¡åœ¨è´¨é‡è¯„ä¼°ä¸­çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºç†µçš„æ–¹æ³•çš„ç®€åŒ–ç‰ˆï¼Œè¯¥æ–¹æ³•å‡è®¾å¤§å‹è¯­è¨€æ¨¡å‹çš„å“åº”æœä»å‡åŒ€åˆ†å¸ƒï¼Œå¹¶å±•ç¤ºäº†å…¶ç›¸å½“çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨è¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€é¡¹æ”¾å¼ƒæ”¿ç­–ï¼Œå³å½“ä¸ç¡®å®šæ€§å¾ˆé«˜æ—¶ï¼Œé˜²æ­¢æ¨¡å‹åšå‡ºé¢„æµ‹ï¼Œä»è€Œå°†é”™è¯¯è¾“å‡ºé™ä½åˆ°å‡ ä¹ä¸ºé›¶ã€‚æˆ‘ä»¬åœ¨LiveCodeBenchä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä»…ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹æŠ¥å‘Šçš„æ—¥å¿—æ¦‚ç‡çš„åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11620v2">PDF</a> 18 pages and 3 References Pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ä¸ç¡®å®šæ€§ä¼°è®¡ä½œä¸ºè¯„ä¼°LLMç”Ÿæˆä»£ç æ­£ç¡®æ€§çš„ä»£ç†æŒ‡æ ‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è‡ªç„¶è¯­è¨€ç”Ÿæˆé¢†åŸŸçš„ä¸¤ç§æœ€æ–°æŠ€æœ¯â€”â€”åŸºäºç†µçš„æ–¹æ³•å’ŒåŸºäºäº’ä¿¡æ¯çš„æ–¹æ³•ï¼Œå¹¶å°†å…¶é€‚åº”äºä»£ç ç”Ÿæˆé¢†åŸŸã€‚è€ƒè™‘åˆ°ä»£ç çš„ç‹¬ç‰¹è¯­ä¹‰å±æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€äº›ä¿®æ”¹ï¼ŒåŒ…æ‹¬åŸºäºç¬¦å·æ‰§è¡Œçš„è¯­ä¹‰ç­‰ä»·æ€§æ£€æŸ¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è¿™äº›æŠ€æœ¯è®¡ç®—çš„ä¸ç¡®å®šæ€§ä¸æ­£ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼Œçªæ˜¾äº†ä¸ç¡®å®šæ€§ä¼°è®¡åœ¨è´¨é‡è¯„ä¼°ä¸­çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€åŒ–ç‰ˆçš„åŸºäºç†µçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å‡è®¾LLMçš„å“åº”å‘ˆå‡åŒ€åˆ†å¸ƒï¼Œå¹¶å±•ç¤ºäº†å…¶ç›¸å½“çš„æœ‰æ•ˆæ€§ã€‚åˆ©ç”¨è¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€ç§é¿å…ç­–ç•¥ï¼Œå³å½“ä¸ç¡®å®šæ€§è¾ƒé«˜æ—¶ï¼Œé˜²æ­¢æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œä»è€Œå°†é”™è¯¯è¾“å‡ºé™ä½åˆ°å‡ ä¹ä¸ºé›¶ã€‚æˆ‘ä»¬åœ¨LiveCodeBenchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä»…ä¾èµ–LLMæŠ¥å‘Šçš„å¯¹æ•°æ¦‚ç‡çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡ç« æ¢ç´¢äº†ä¸ç¡®å®šæ€§ä¼°è®¡ä½œä¸ºè¯„ä¼°LLMç”Ÿæˆä»£ç æ­£ç¡®æ€§çš„æ–¹æ³•ã€‚</li>
<li>æ–‡ç« é‡‡ç”¨äº†è‡ªç„¶è¯­è¨€ç”Ÿæˆé¢†åŸŸçš„ä¸¤ç§æŠ€æœ¯ï¼Œå¹¶é€‚åº”äºä»£ç ç”Ÿæˆé¢†åŸŸã€‚</li>
<li>ç ”ç©¶å‘ç°ä¸ç¡®å®šæ€§ä¼°è®¡ä¸ä»£ç æ­£ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§ç®€åŒ–ç‰ˆçš„åŸºäºç†µçš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•ï¼Œå¹¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>æ–‡ç« åˆ¶å®šäº†ä¸€ç§é¿å…ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸ç¡®å®šæ€§è¾ƒé«˜æ—¶é˜²æ­¢æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œæ˜¾è‘—å‡å°‘é”™è¯¯è¾“å‡ºã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºä»…ä¾èµ–LLMæŠ¥å‘Šçš„å¯¹æ•°æ¦‚ç‡çš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºLLMç”Ÿæˆçš„ä»£ç è´¨é‡è¯„ä¼°æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e2863ca31b6a6d311590fac49d1be54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a87af8794d55ab302c70eb96a1576a72.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f5ce8c8a9c2603d8212970bfe805358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37c83c727c6209861532f83de48f976c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CDS-Data-Synthesis-Method-Guided-by-Cognitive-Diagnosis-Theory"><a href="#CDS-Data-Synthesis-Method-Guided-by-Cognitive-Diagnosis-Theory" class="headerlink" title="CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory"></a>CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory</h2><p><strong>Authors:Haokun Zhao, Jinyi Han, Jiaqing Liang, Yanghua Xiao</strong></p>
<p>Large Language Models (LLMs) have achieved significant advancements, but the increasing complexity of tasks and higher performance demands highlight the need for continuous improvement. Some approaches utilize synthetic data generated by advanced LLMs based on evaluation results to train models. However, conventional evaluation methods fail to provide detailed, fine-grained profiles of LLMs, limiting their guidance for data synthesis. In this paper, we introduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a diagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine evaluation results and characterize model profiles at the knowledge component level. Based on these diagnostics, we propose two diagnosis-synthesis strategies for weakness-targeted data synthesis. Additionally, we present an enhanced data augmentation and selection pipeline to improve the quality and diversity of synthesized data. Our experiments with several open-source models show significant improvements across multiple benchmarks, achieving up to 6.00% improvement in code generation, 13.10% in mathematical reasoning, and 5.43% in academic exams. Code and data are available on GitHub. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¯¹æ€§èƒ½è¦æ±‚çš„æé«˜çªæ˜¾äº†æŒç»­æ”¹è¿›çš„å¿…è¦æ€§ã€‚ä¸€äº›æ–¹æ³•åˆ©ç”¨åŸºäºè¯„ä¼°ç»“æœç”±å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•æ— æ³•æä¾›å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯¦ç»†ã€ç²¾ç»†çš„æ¦‚å†µï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨æ•°æ®åˆæˆæ–¹é¢çš„æŒ‡å¯¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†è®¤çŸ¥è¯Šæ–­åˆæˆï¼ˆCDSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å—è®¤çŸ¥è¯Šæ–­ç†è®ºï¼ˆCDTï¼‰å¯å‘çš„è¯Šæ–­è¿‡ç¨‹ï¼Œä»¥ä¼˜åŒ–è¯„ä¼°ç»“æœå¹¶åœ¨çŸ¥è¯†ç»„ä»¶çº§åˆ«åˆ»ç”»æ¨¡å‹æ¦‚å†µã€‚åŸºäºè¿™äº›è¯Šæ–­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§é’ˆå¯¹å¼±ç‚¹è¿›è¡Œçš„æ•°æ®åˆæˆçš„è¯Šæ–­åˆæˆç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸€ä¸ªå¢å¼ºçš„æ•°æ®å¢å¼ºå’Œé€‰æ‹©æµç¨‹ï¼Œä»¥æé«˜åˆæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¼€æºæ¨¡å‹ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨ä»£ç ç”Ÿæˆæ–¹é¢æé«˜äº†6.00%ï¼Œåœ¨æ•°å­¦æ¨ç†æ–¹é¢æé«˜äº†13.10%ï¼Œåœ¨å­¦æœ¯è€ƒè¯•æ–¹é¢æé«˜äº†5.43%ã€‚ä»£ç å’Œæ•°æ®å·²åœ¨GitHubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07674v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´æŒç»­æé«˜æ€§èƒ½çš„éœ€æ±‚ï¼Œéƒ¨åˆ†æ–¹æ³•é€šè¿‡åˆæˆæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿè¯„ä¼°æ–¹æ³•æ— æ³•æä¾›è¯¦ç»†çš„æ¨¡å‹æ€§èƒ½åˆ†æï¼Œé™åˆ¶äº†æ•°æ®åˆæˆçš„æŒ‡å¯¼ã€‚æœ¬æ–‡æå‡ºäº†ç»“åˆè®¤çŸ¥è¯Šæ–­ç†è®ºï¼ˆCDTï¼‰çš„Cognitive Diagnostic Synthesisï¼ˆCDSï¼‰æ–¹æ³•ï¼Œä»¥ç²¾ç»†åŒ–è¯„ä¼°ç»“æœå¹¶åˆ»ç”»æ¨¡å‹åœ¨çŸ¥è¯†ç»„ä»¶å±‚é¢çš„æ€§èƒ½ç‰¹å¾ã€‚åŸºäºè¿™äº›è¯Šæ–­ç»“æœï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§é’ˆå¯¹å¼±ç‚¹åˆæˆæ•°æ®çš„ç­–ç•¥ã€‚åŒæ—¶ï¼Œæ”¹è¿›äº†æ•°æ®å¢å¼ºå’Œé€‰æ‹©æµç¨‹ï¼Œæé«˜äº†åˆæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¦‚ä»£ç ç”Ÿæˆæé«˜äº†6.0%ï¼Œæ•°å­¦æ¨ç†æé«˜äº†13.1%ï¼Œå­¦æœ¯è€ƒè¯•æé«˜äº†5.4%ã€‚GitHubä¸Šæœ‰ç›¸å…³ä»£ç å’Œæ•°æ®å¯ä¾›ä¸‹è½½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMé¢ä¸´æ€§èƒ½æŒç»­æå‡çš„éœ€æ±‚å’ŒæŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•æ— æ³•æä¾›è¯¦ç»†çš„æ¨¡å‹æ€§èƒ½åˆ†æã€‚</li>
<li>Cognitive Diagnostic Synthesis (CDS)æ–¹æ³•ç»“åˆè®¤çŸ¥è¯Šæ–­ç†è®ºï¼ˆCDTï¼‰æ¥ç²¾ç»†åŒ–è¯„ä¼°ç»“æœå¹¶åˆ»ç”»æ¨¡å‹æ€§èƒ½ç‰¹å¾ã€‚</li>
<li>åŸºäºè¯Šæ–­ç»“æœï¼Œæå‡ºäº†ä¸¤ç§é’ˆå¯¹å¼±ç‚¹åˆæˆæ•°æ®çš„ç­–ç•¥ã€‚</li>
<li>æ”¹è¿›äº†æ•°æ®å¢å¼ºå’Œé€‰æ‹©æµç¨‹ï¼Œæé«˜åˆæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-25149d28bde78b83150caf530b91e937.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c4c5903ca3e8b1a6730b78113396162.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d20a1b1a1b08aae324c0266a0e91df44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f60472643003fb490622efddba82d0be.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models"><a href="#Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models" class="headerlink" title="Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models"></a>Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models</h2><p><strong>Authors:Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu</strong></p>
<p>Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in <a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚é—®ç­”å’Œæœºå™¨ç¿»è¯‘ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡æ³¨æ•°æ®å’Œç”ŸåŒ–å±æ€§æ‰‹åŠ¨æ³¨é‡Šçš„å›°éš¾ï¼Œåˆ†å­ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠå¤šå±æ€§çº¦æŸçš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„PEITï¼ˆå±æ€§å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼‰æ¡†æ¶ï¼Œä»¥æé«˜LLMåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç¬¬ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æè¿°ã€SMILESå’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä½œä¸ºå¤šæ¨¡å¼è¾“å…¥ï¼Œé€šè¿‡å¯¹é½å¤šæ¨¡å¼è¡¨ç¤ºæ¥åˆæˆæŒ‡ä»¤æ•°æ®ï¼Œä»è€Œé¢„è®­ç»ƒä¸€ä¸ªåä¸ºPEIT-GENçš„æ¨¡å‹ã€‚åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆæ•°æ®å¯¹ç°æœ‰çš„å¼€æºLLMè¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°çš„PEIT-LLMå¯ä»¥å¤„ç†åˆ†å­æè¿°ã€åŸºäºæ–‡æœ¬çš„åˆ†å­ç”Ÿæˆã€åˆ†å­å±æ€§é¢„æµ‹ä»¥åŠæˆ‘ä»¬æ–°æå‡ºçš„å¤šçº¦æŸåˆ†å­ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒPEIT-GENåœ¨åˆ†å­æè¿°æ–¹é¢è¶…è¶Šäº†MolT5å’ŒBioT5ã€‚è¿™è¯æ˜äº†æ–‡æœ¬æè¿°ã€ç»“æ„å’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä¹‹é—´çš„æ¨¡æ€å¯¹é½è‰¯å¥½ã€‚æ­¤å¤–ï¼ŒPEIT-LLMåœ¨å¤šä»»åŠ¡åˆ†å­ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ”¹è¿›ï¼Œè¯æ˜äº†PEITæ¡†æ¶å¯¹å„ç§åˆ†å­ä»»åŠ¡çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>ä¸Šå‘å¸ƒäº†ä»£ç ã€æ„å»ºå¥½çš„æŒ‡ä»¤æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18084v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šLLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†é’ˆå¯¹åˆ†å­ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ä»å—é™äºç¼ºä¹æ ‡ç­¾æ•°æ®å’Œæ‰‹åŠ¨æ³¨é‡Šçš„éš¾åº¦ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªä¸¤é˜¶æ®µçš„PEITæ¡†æ¶ï¼Œé€šè¿‡é¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œæé«˜LLMåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒçš„PEIT-GENåœ¨åˆ†å­æè¿°ä¸Šä¼˜äºMolT5å’ŒBioT5æ¨¡å‹ï¼Œå¹¶ä¸”PEITæ¡†æ¶å¯¹äºå¤šç§åˆ†å­ä»»åŠ¡è¡¨ç°å‡ºå¯æ‰©å±•æ€§ã€‚PEITå·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨å¤šç§NLPä»»åŠ¡ä¸­åº”ç”¨å¹¿æ³›ï¼Œä½†åœ¨åˆ†å­ç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½å—åˆ°é™åˆ¶ã€‚</li>
<li>PEITæ¡†æ¶åŒ…å«é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨æé«˜LLMåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>PEITä½¿ç”¨æ–‡æœ¬æè¿°ã€SMILESå’Œç”Ÿç‰©åŒ–å­¦å±æ€§ç­‰å¤šæ¨¡å¼è¾“å…¥è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>PEIT-GENçš„åˆæˆæ•°æ®ç”¨äºå¾®è°ƒç°æœ‰å¼€æºLLMã€‚</li>
<li>PEIT-LLMèƒ½å¤Ÿå¤„ç†åˆ†å­æè¿°ã€æ–‡æœ¬åŸºç¡€çš„åˆ†å­ç”Ÿæˆã€åˆ†å­å±æ€§é¢„æµ‹ä»¥åŠæ–°æå‡ºçš„å¤šçº¦æŸåˆ†å­ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPEIT-GENåœ¨åˆ†å­æè¿°ä¸Šä¼˜äºMolT5å’ŒBioT5æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e350f9c443a225542daab311015a59e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-431301f75dfab204d4322d959489b384.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c6e299ad16040444d5c2bb0c92c3a51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb2d9703cb52ff46f3f1b18e5846c929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70ee1e58c3186455090ca950fb68190f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PyGen-A-Collaborative-Human-AI-Approach-to-Python-Package-Creation"><a href="#PyGen-A-Collaborative-Human-AI-Approach-to-Python-Package-Creation" class="headerlink" title="PyGen: A Collaborative Human-AI Approach to Python Package Creation"></a>PyGen: A Collaborative Human-AI Approach to Python Package Creation</h2><p><strong>Authors:Saikat Barua, Mostafizur Rahman, Md Jafor Sadek, Rafiul Islam, Shehnaz Khaled, Md. Shohrab Hossain</strong></p>
<p>The principles of automation and innovation serve as foundational elements for advancement in contemporary science and technology. Here, we introduce Pygen, an automation platform designed to empower researchers, technologists, and hobbyists to bring abstract ideas to life as core, usable software tools written in Python. Pygen leverages the immense power of autoregressive large language models to augment human creativity during the ideation, iteration, and innovation process. By combining state-of-the-art language models with open-source code generation technologies, Pygen has significantly reduced the manual overhead of tool development. From a user prompt, Pygen automatically generates Python packages for a complete workflow from concept to package generation and documentation. The findings of our work show that Pygen considerably enhances the researcherâ€™s productivity by enabling the creation of resilient, modular, and well-documented packages for various specialized purposes. We employ a prompt enhancement approach to distill the userâ€™s package description into increasingly specific and actionable. While being inherently an open-ended task, we have evaluated the generated packages and the documentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with detailed results in the results section. Furthermore, we documented our results, analyzed the limitations, and suggested strategies to alleviate them. Pygen is our vision of ethical automation, a framework that promotes inclusivity, accessibility, and collaborative development. This project marks the beginning of a large-scale effort towards creating tools where intelligent agents collaborate with humans to improve scientific and technological development substantially.   Our code and generated examples are open-sourced at [<a target="_blank" rel="noopener" href="https://github.com/GitsSaikat/Pygen]">https://github.com/GitsSaikat/Pygen]</a> </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–å’Œåˆ›æ–°çš„åŸåˆ™æ˜¯å½“ä»£ç§‘å­¦æŠ€æœ¯è¿›æ­¥çš„åŸºç¡€è¦ç´ ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»Pygenâ€”â€”ä¸€ä¸ªè‡ªåŠ¨åŒ–å¹³å°ï¼Œæ—¨åœ¨èµ‹èƒ½ç ”ç©¶è€…ã€æŠ€æœ¯ä¸“å®¶å’Œçˆ±å¥½è€…ï¼Œå°†æŠ½è±¡çš„æƒ³æ³•è½¬åŒ–ä¸ºå®ç”¨çš„Pythonè½¯ä»¶å·¥å…·ã€‚Pygenåˆ©ç”¨è‡ªåŠ¨å›å½’å¤§å‹è¯­è¨€æ¨¡å‹çš„å·¨å¤§åŠ›é‡ï¼Œåœ¨åˆ›æ„ã€è¿­ä»£å’Œåˆ›æ–°è¿‡ç¨‹ä¸­å¢å¼ºäººç±»çš„åˆ›é€ åŠ›ã€‚é€šè¿‡å°†æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ä¸å¼€æºä»£ç ç”ŸæˆæŠ€æœ¯ç›¸ç»“åˆï¼ŒPygenå¤§å¤§é™ä½äº†å·¥å…·å¼€å‘çš„æ‰‹åŠ¨å·¥ä½œé‡ã€‚ä»ç”¨æˆ·æç¤ºå¼€å§‹ï¼ŒPygenè‡ªåŠ¨ä¸ºä»æ¦‚å¿µåˆ°è½¯ä»¶åŒ…ç”Ÿæˆå’Œæ–‡æ¡£çš„æ•´ä¸ªå·¥ä½œæµç¨‹ç”ŸæˆPythonåŒ…ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒPygené€šè¿‡ä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿåˆ›å»ºé’ˆå¯¹å„ç§ç‰¹å®šç”¨é€”çš„å¥å£®ã€æ¨¡å—åŒ–ä¸”æ–‡æ¡£é½å…¨çš„åŒ…ï¼Œä»è€Œæå¤§åœ°æé«˜äº†ç ”ç©¶äººå‘˜çš„å·¥ä½œæ•ˆç‡ã€‚æˆ‘ä»¬é‡‡ç”¨æç¤ºå¢å¼ºæ³•å¯¹ç”¨æˆ·æè¿°çš„åŒ…è¿›è¡Œæç‚¼ï¼Œä½¿å…¶æ›´åŠ å…·ä½“å¯è¡Œã€‚è™½ç„¶æœ¬è´¨ä¸Šè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬é€šè¿‡äººå·¥è¯„ä¼°ã€åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°å’ŒCodeBLEUå¯¹ç”Ÿæˆçš„è½¯ä»¶åŒ…å’Œæ–‡æ¡£è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¦ç»†ç»“æœè§ç»“æœéƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®°å½•äº†æˆ‘ä»¬çš„ç»“æœï¼Œåˆ†æäº†å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ç¼“è§£ç­–ç•¥ã€‚Pygenæ˜¯æˆ‘ä»¬å¯¹é“å¾·è‡ªåŠ¨åŒ–çš„æ„¿æ™¯ï¼Œæ˜¯ä¸€ä¸ªä¿ƒè¿›åŒ…å®¹æ€§ã€å¯è®¿é—®æ€§å’Œåä½œå‘å±•çš„æ¡†æ¶ã€‚è¿™ä¸ªé¡¹ç›®æ ‡å¿—ç€æœç€åˆ›å»ºæ™ºèƒ½ä¸»ä½“ä¸äººç±»åˆä½œæ”¹å–„ç§‘å­¦å’Œç§‘æŠ€å‘å±•çš„å¤§è§„æ¨¡åŠªåŠ›è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚æˆ‘ä»¬çš„ä»£ç å’Œç”Ÿæˆçš„ç¤ºä¾‹åœ¨<a target="_blank" rel="noopener" href="https://github.com/GitsSaikat/Pygen%E4%B8%8A%E5%BC%BA%E6%BA%90%E3%80%82">https://github.com/GitsSaikat/Pygenä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08932v2">PDF</a> 33 pages, 13 figures</p>
<p><strong>Summary</strong><br>Pygenæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å¹³å°ï¼Œåˆ©ç”¨å…ˆè¿›çš„è‡ªç„¶è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼ŒåŠ©åŠ›ç ”ç©¶äººå‘˜å’ŒæŠ€æœ¯çˆ±å¥½è€…å°†æŠ½è±¡æƒ³æ³•è½¬åŒ–ä¸ºå®ç”¨çš„Pythonè½¯ä»¶å·¥å…·ã€‚è¯¥å¹³å°é€šè¿‡å‡å°‘å·¥å…·å¼€å‘çš„æ‰‹åŠ¨å·¥ä½œé‡ï¼Œæ˜¾è‘—æé«˜äº†ç ”ç©¶äººå‘˜çš„ç”Ÿäº§åŠ›ã€‚Pygenèƒ½è‡ªåŠ¨ä»ç”¨æˆ·æç¤ºç”ŸæˆPythonåŒ…ï¼Œå¹¶æä¾›ä»æ¦‚å¿µåˆ°åŒ…ç”Ÿæˆå’Œæ–‡æ¡£å®Œæ•´çš„è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ã€‚å…¶æ³¨é‡å¼€æ”¾æ€§ã€åŒ…å®¹æ€§å’Œåä½œæ€§ï¼Œè‡´åŠ›äºä¿ƒè¿›ç§‘å­¦å’ŒæŠ€æœ¯å‘å±•ã€‚ç›®å‰å·²åœ¨GitHubä¸Šå¼€æºéƒ¨åˆ†ä»£ç å’Œç”Ÿæˆçš„å®ä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Pygenæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–å¹³å°ï¼Œç»“åˆäº†è‡ªç„¶è¯­è¨€æ¨¡å‹ä¸å¼€æºä»£ç ç”ŸæˆæŠ€æœ¯ã€‚</li>
<li>Pygenå¯å°†æŠ½è±¡æƒ³æ³•è½¬åŒ–ä¸ºPythonè½¯ä»¶å·¥å…·ï¼Œé™ä½å·¥å…·å¼€å‘çš„æ‰‹åŠ¨å·¥ä½œé‡ã€‚</li>
<li>å¹³å°èƒ½è‡ªåŠ¨ä»ç”¨æˆ·æç¤ºç”ŸæˆPythonåŒ…ï¼Œå¹¶æä¾›å®Œæ•´è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ã€‚</li>
<li>Pygenå¼ºè°ƒå¼€æ”¾æ€§ã€åŒ…å®¹æ€§å’Œåä½œæ€§ï¼Œæ¨åŠ¨ç§‘å­¦å’ŒæŠ€æœ¯å‘å±•ã€‚</li>
<li>é€šè¿‡äººç±»è¯„ä¼°ã€LLMè¯„ä¼°å’ŒCodeBLEUè¯„ä¼°éªŒè¯äº†Pygençš„æ•ˆæœã€‚</li>
<li>Pygenéƒ¨åˆ†ä»£ç å’Œç”Ÿæˆçš„å®ä¾‹å·²åœ¨GitHubä¸Šå¼€æºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ba9402ab60774a0640145229784c365.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b65a61cc3b7b2801e9416a2bc412077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10ac885127120172d8350314e3da30c8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6ad0b951fd187b846168927271f5311d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  MAS-GPT Training LLMs to Build LLM-based Multi-Agent Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-81423a77a41ec09154418bba6ec1ef7c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  PacketCLIP Multi-Modal Embedding of Network Traffic and Language for   Cybersecurity Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">20064.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
