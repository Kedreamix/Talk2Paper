<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-03-07  Process-based Self-Rewarding Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1c0dd782a7ff87c98922cb57986c014d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    78 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-07-更新"><a href="#2025-03-07-更新" class="headerlink" title="2025-03-07 更新"></a>2025-03-07 更新</h1><h2 id="Process-based-Self-Rewarding-Language-Models"><a href="#Process-based-Self-Rewarding-Language-Models" class="headerlink" title="Process-based Self-Rewarding Language Models"></a>Process-based Self-Rewarding Language Models</h2><p><strong>Authors:Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, Yeyun Gong</strong></p>
<p>Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs’ performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities. </p>
<blockquote>
<p>大型语言模型已在各种下游任务中展现出卓越性能，并在多个场景中得到广泛应用。为了进一步提升大型语言模型的性能，采用人工标注的偏好数据进行训练，但仍受限于人类性能的上限。因此，提出了自我奖励方法，即大型语言模型通过奖励自身输出生成训练数据。然而，现有的自我奖励范式在数学推理场景中并不有效，甚至可能导致性能下降。本研究提出了基于过程的自我奖励管道语言模型，引入了长期推理、逐步的大型语言模型作为法官和基于自我奖励范式内的逐步偏好优化。我们的新范式通过基于过程的自我奖励迭代成功提高了大型语言模型在多个数学推理基准测试中的性能，证明了自我奖励在大型语言模型推理方面具有超越人类能力的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03746v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模语言模型在不同下游任务上表现出卓越性能，并在多个场景中得到了广泛应用。为进一步提高其性能并受到人类表现上限的限制，采用了人类标注偏好数据进行训练。然而，现有的自我奖励机制在数学推理场景中并不有效，甚至可能导致性能下降。本研究提出了基于流程的自我奖励管道，引入长期推理、逐步的语言模型作为法官和逐步偏好优化等机制。通过基于流程的迭代自我奖励，成功提高了语言模型在数学推理方面的性能，显示出自我奖励实现超越人类能力的语言推理的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模语言模型在多种任务上表现出卓越性能，并广泛应用于不同场景。</li>
<li>人类标注偏好数据用于训练，以提高语言模型的性能，但仍受人类表现上限的限制。</li>
<li>现有自我奖励机制在数学推理场景中效果不佳，甚至导致性能下降。</li>
<li>本研究提出了基于流程的自我奖励管道，包括长期推理、逐步的语言模型评估以及偏好优化。</li>
<li>通过迭代基于流程的自我奖励，成功提高了语言模型在数学推理方面的性能。</li>
<li>基于自我奖励的语言模型有潜力实现超越人类能力的语言推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-35bf432485c37194abdcfec20151d8df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f140123d2474bcf923dbc6e1d8f6681.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a09cfcf28df299b0de886a4e62b47dd3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-LLM-Safety-Alignment-with-Dual-Objective-Optimization"><a href="#Improving-LLM-Safety-Alignment-with-Dual-Objective-Optimization" class="headerlink" title="Improving LLM Safety Alignment with Dual-Objective Optimization"></a>Improving LLM Safety Alignment with Dual-Objective Optimization</h2><p><strong>Authors:Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song</strong></p>
<p>Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at <a target="_blank" rel="noopener" href="https://github.com/wicai24/DOOR-Alignment">https://github.com/wicai24/DOOR-Alignment</a> </p>
<blockquote>
<p>现有大型语言模型（LLM）的训练时安全对齐技术仍然容易受到越狱攻击的影响。直接偏好优化（DPO）是一种广泛部署的对齐方法，其在实验和理论背景下均存在局限性，因为其损失函数对于拒绝学习而言并不理想。通过基于梯度的分析，我们识别出这些不足，并提出了一种改进的安全对齐方法，该方法将DPO目标分解为两个组成部分：（1）鲁棒拒绝训练，即使部分不安全生成，也鼓励拒绝；（2）有针对性地忘记有害知识。该方法显著提高了LLM对各种越狱攻击的鲁棒性，包括跨分布内和分布外的预填充、后缀和多轮攻击。此外，我们通过引入一种基于奖励的令牌级加权机制来强调拒绝学习的关键拒绝令牌的方法，这进一步提高了对对抗性利用的鲁棒性。我们的研究还表明，对越狱攻击的鲁棒性与训练过程中的令牌分布转变以及拒绝和有害令牌的内部表示有关，为未来LLM安全对齐的研究提供了有价值的方向。代码可在<a target="_blank" rel="noopener" href="https://github.com/wicai24/DOOR-Alignment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wicai24/DOOR-Alignment找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03710v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在训练时的安全对齐技术仍易受到jailbreak攻击的影响。直接偏好优化（DPO）是一种广泛应用的对齐方法，但在实验和理论背景下都存在局限性，其损失函数对于拒绝学习并不理想。通过梯度分析，我们确定了这些不足，并提出了一种改进的安全对齐方法，该方法将DPO目标分为两个组成部分：（1）鲁棒的拒绝训练，即使在部分不安全生成的情况下也鼓励拒绝；（2）有针对性的消除有害知识。该方法显著提高了LLM对各种jailbreak攻击的鲁棒性，包括预填充、后缀和多轮攻击，涵盖了分布内和分布外场景。此外，我们引入了一种方法，通过引入基于奖励的令牌级加权机制来强调关键的拒绝令牌，进一步提高对抗恶意攻击的稳健性。本研究还表明，对jailbreak攻击的稳健性与训练过程中的令牌分布转变以及拒绝和有害令牌的内部表示有关，为未来LLM安全对齐的研究提供了有价值的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM现有的训练时间安全对齐技术容易受到jailbreak攻击的影响。</li>
<li>直接偏好优化（DPO）方法在拒绝学习方面存在局限性。</li>
<li>提出一种改进的安全对齐方法，包括鲁棒拒绝训练和有害知识的有针对性的消除。</li>
<li>新方法显著提高LLM对各种jailbreak攻击的鲁棒性，涵盖分布内和分布外场景。</li>
<li>引入基于奖励的令牌级加权机制来强调关键的拒绝令牌。</li>
<li>对jailbreak攻击的稳健性与训练过程中的令牌分布转变有关。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a315615f74d6f21302d942ab01fad146.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0525f5f9412251de311303dfab8f26cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-606b348b91950dc35d5c9e6d61f3635b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07200417e4d883120e50f869825a7dd1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Developing-and-Utilizing-a-Large-Scale-Cantonese-Dataset-for-Multi-Tasking-in-Large-Language-Models"><a href="#Developing-and-Utilizing-a-Large-Scale-Cantonese-Dataset-for-Multi-Tasking-in-Large-Language-Models" class="headerlink" title="Developing and Utilizing a Large-Scale Cantonese Dataset for   Multi-Tasking in Large Language Models"></a>Developing and Utilizing a Large-Scale Cantonese Dataset for   Multi-Tasking in Large Language Models</h2><p><strong>Authors:Jiyue Jiang, Alfred Kar Yin Truong, Yanyu Chen, Qinghang Bao, Sheng Wang, Pengan Chen, Jiuming Wang, Lingpeng Kong, Yu Li, Chuan Wu</strong></p>
<p>High-quality data resources play a crucial role in learning large language models (LLMs), particularly for low-resource languages like Cantonese. Despite having more than 85 million native speakers, Cantonese is still considered a low-resource language in the field of natural language processing (NLP) due to factors such as the dominance of Mandarin, lack of cohesion within the Cantonese-speaking community, diversity in character encoding and input methods, and the tendency of overseas Cantonese speakers to prefer using English. In addition, rich colloquial vocabulary of Cantonese, English loanwords, and code-switching characteristics add to the complexity of corpus collection and processing. To address these challenges, we collect Cantonese texts from a variety of sources, including open source corpora, Hong Kong-specific forums, Wikipedia, and Common Crawl data. We conduct rigorous data processing through language filtering, quality filtering, content filtering, and de-duplication steps, successfully constructing a high-quality Cantonese corpus of over 2 billion tokens for training large language models. We further refined the model through supervised fine-tuning (SFT) on curated Cantonese tasks, enhancing its ability to handle specific applications. Upon completion of the training, the model achieves state-of-the-art (SOTA) performance on four Cantonese benchmarks. After training on our dataset, the model also exhibits improved performance on other mainstream language tasks. </p>
<blockquote>
<p>高质量的数据资源在学习大型语言模型（LLM）中发挥着至关重要的作用，特别是在粤语这种低资源语言领域。尽管粤语有超过8500万母语者，但由于普通话的主导地位、粤语社群内部的缺乏凝聚力、字符编码和输入方法的多样性以及海外粤语使用者更倾向于使用英语等因素，粤语在自然语言处理（NLP）领域仍被视为一种低资源语言。此外，粤语的丰富口语词汇、英语借词和代码转换特性增加了语料库收集和处理工作的复杂性。为了应对这些挑战，我们从各种来源收集粤语文本，包括开放源代码语料库、香港特定论坛、Wikipedia和Common Crawl数据。我们通过语言过滤、质量过滤、内容过滤和去重等严格的数据处理步骤，成功构建了超过2亿令牌的高质量粤语语料库，用于训练大型语言模型。我们还通过精选的粤语任务进行有监督微调（SFT），进一步改进模型，提高其处理特定应用的能力。训练完成后，该模型在四个粤语基准测试中达到了最新技术水平。在我们的数据集上进行训练后，该模型在其他主流语言任务上的性能也有所提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对粤语这一低资源语言领域，如何收集高质量数据资源用于训练大型语言模型（LLM）。通过从多种来源收集粤语文本，进行严谨的数据处理，成功构建了一个超过两亿标记的高质量粤语语料库。经过特定任务的监督微调后，该模型在四项粤语基准测试中达到了业界最佳性能，并且在主流语言任务上也有所提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>粤语虽然有超过85百万的母语使用者，但在自然语言处理领域仍被视为低资源语言。主要挑战包括普通话的主导地位、粤语社群的不团结、字符编码和输入方法的多样性以及海外粤语使用者更倾向于使用英语等问题。</li>
<li>收集粤语文本来源多样化，包括开源语料库、香港特定论坛、Wikipedia和Common Crawl数据等。</li>
<li>对收集的数据进行了严格的处理，包括语言过滤、质量过滤、内容过滤和去重等步骤，成功构建了高质量粤语语料库。</li>
<li>通过监督微调（SFT）在定制粤语任务上进一步优化模型，提升其处理特定应用的能力。</li>
<li>模型在四项粤语基准测试中达到了业界最佳性能。</li>
<li>在训练数据集上训练后，模型在主流语言任务上的性能也有所提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c53352adf97e0debe6f3578c5d005f3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b475970ce5e20ec9979bee8bc42a850d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2296bd368871b4417ecaf11095d92b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b611fa2946ab27ffbd8a61b28f4f395f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6220e4b8076b5fc786a3be5d428ae9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b82fc9faf22307c437bfd8f789d8b9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2149ec418ba58ca6218856a322ebe6f2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Addressing-Overprescribing-Challenges-Fine-Tuning-Large-Language-Models-for-Medication-Recommendation-Tasks"><a href="#Addressing-Overprescribing-Challenges-Fine-Tuning-Large-Language-Models-for-Medication-Recommendation-Tasks" class="headerlink" title="Addressing Overprescribing Challenges: Fine-Tuning Large Language Models   for Medication Recommendation Tasks"></a>Addressing Overprescribing Challenges: Fine-Tuning Large Language Models   for Medication Recommendation Tasks</h2><p><strong>Authors:Zihao Zhao, Chenxiao Fan, Chongming Gao, Fuli Feng, Xiangnan He</strong></p>
<p>Medication recommendation systems have garnered attention within healthcare for their potential to deliver personalized and efficacious drug combinations based on patient’s clinical data. However, existing methodologies encounter challenges in adapting to diverse Electronic Health Records (EHR) systems and effectively utilizing unstructured data, resulting in limited generalization capabilities and suboptimal performance. Recently, interest is growing in harnessing Large Language Models (LLMs) in the medical domain to support healthcare professionals and enhance patient care. Despite the emergence of medical LLMs and their promising results in tasks like medical question answering, their practical applicability in clinical settings, particularly in medication recommendation, often remains underexplored.   In this study, we evaluate both general-purpose and medical-specific LLMs for medication recommendation tasks. Our findings reveal that LLMs frequently encounter the challenge of overprescribing, leading to heightened clinical risks and diminished medication recommendation accuracy. To address this issue, we propose Language-Assisted Medication Recommendation (LAMO), which employs a parameter-efficient fine-tuning approach to tailor open-source LLMs for optimal performance in medication recommendation scenarios. LAMO leverages the wealth of clinical information within clinical notes, a resource often underutilized in traditional methodologies. As a result of our approach, LAMO outperforms previous state-of-the-art methods by over 10% in internal validation accuracy. Furthermore, temporal and external validations demonstrate LAMO’s robust generalization capabilities across various temporal and hospital contexts. Additionally, an out-of-distribution medication recommendation experiment demonstrates LAMO’s remarkable accuracy even with medications outside the training data. </p>
<blockquote>
<p>医疗推荐系统因其根据患者临床数据提供个性化且有效的药物组合潜力而受到医疗保健领域的关注。然而，现有方法在适应多样化的电子健康记录（EHR）系统和有效利用非结构化数据方面面临挑战，导致泛化能力有限和性能不佳。最近，越来越多的兴趣在于利用医疗领域的大型语言模型（LLM）来支持医疗专业人士并改善患者护理。尽管医疗LLM的出现及其在医疗问答等任务上的前景令人鼓舞，但它们在临床环境中的实际应用，特别是在药物推荐方面，往往被探索得不够。在这项研究中，我们评估了通用和医疗专用的LLM在药物推荐任务中的表现。我们的研究发现，LLM经常面临过度开药的挑战，导致临床风险增加和药物推荐准确性降低。为了解决这一问题，我们提出了语言辅助药物推荐（LAMO），它采用参数高效的微调方法，量身定制开源LLM，以在药物推荐场景中实现最佳性能。LAMO利用病历笔记中的丰富临床信息，这是传统方法中经常被忽视的资源。由于我们的方法，LAMO在内部验证准确性上超越了先前最先进的方法超过10%。此外，时序和外部验证表明LAMO在不同时间和医院背景下的泛化能力强大。另外，一个超出分布的药物推荐实验表明，即使在训练数据之外的药物中，LAMO的准确率也令人印象深刻。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03687v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>医疗推荐系统因其根据患者临床数据提供个性化、有效药物组合的能力而受到医疗保健领域的关注。然而，现有方法在适应多样化的电子健康记录（EHR）系统和有效利用非结构化数据时面临挑战，导致通用化能力有限和性能不佳。近年来，人们越来越有兴趣利用医疗领域的大型语言模型（LLM）来支持医疗专业人员并改善患者护理。尽管医疗LLM的出现及其在医疗问答等任务中的令人鼓舞的结果，其在临床环境中的实际应用，特别是在药物推荐方面，仍然鲜有研究。本研究评估了通用和医疗特定LLM在药物推荐任务中的表现。我们发现LLM经常面临过度开药的挑战，导致临床风险增加和药物推荐准确性降低。为了解决这个问题，我们提出了语言辅助药物推荐（LAMO），它采用参数高效的微调方法，为开源LLM量身定制，以在药物推荐场景中实现最佳性能。LAMO利用临床笔记中的大量临床信息，这是传统方法中经常被忽视的资源。我们的方法使LAMO在内部验证精度上超过了以前的最先进方法超过10％。此外，时序和外部验证证明了LAMO在不同时间和医院背景下的稳健泛化能力。另外，超出训练数据范围的药品推荐实验证明了LAMO即使在训练数据外的药品推荐中也能保持出色的准确性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>医疗推荐系统依赖LLM为患者提供个性化药物组合建议。</li>
<li>当前LLM在药物推荐中面临过度开药、推荐准确度不高的问题。</li>
<li>提出了一种新的方法LAMO，通过参数高效的微调来优化LLM在药物推荐中的性能。</li>
<li>LAMO利用临床笔记中的丰富信息，这些信息在传统方法中常被忽视。</li>
<li>LAMO在内部验证精度上较之前的方法有显著提高，超过10%。</li>
<li>LAMO在不同时间和医院背景下表现出强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6b48f613f445789cbf52140cb3cdb33b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0d04b17e19574affd418e7fb2e461ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b3ebe5a2db40c2df0d879fbcae6ec55.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MAS-GPT-Training-LLMs-to-Build-LLM-based-Multi-Agent-Systems"><a href="#MAS-GPT-Training-LLMs-to-Build-LLM-based-Multi-Agent-Systems" class="headerlink" title="MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems"></a>MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems</h2><p><strong>Authors:Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, Jing Shao</strong></p>
<p>LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs. In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS. To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference. The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses. Extensive experiments on 9 benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms 10+ baseline MAS methods on diverse settings, indicating MAS-GPT’s high effectiveness, efficiency and strong generalization ability. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/rui-ye/MAS-GPT">https://github.com/rui-ye/MAS-GPT</a>. </p>
<blockquote>
<p>基于LLM的多智能体系统（MAS）在处理多样化任务方面显示出巨大潜力。然而，为了设计有效的MAS，现有方法严重依赖于手动配置或多次调用高级LLM，导致不适应和高推理成本。在本文中，我们通过将构建MAS重新构建为生成语言任务来简化该过程，其中输入是用户查询，输出是相应的MAS。为了解决这项新任务，我们将MAS的表示形式统一为可执行代码，并提出了一种面向一致性的数据构建管道，以创建包含连贯和一致查询-MAS对的高质量数据集。使用该数据集，我们训练了MAS-GPT，这是一个开源的中型LLM，能够在单个LLM推理内生成查询适应性MAS。生成的MAS可以无缝应用于处理用户查询并提供高质量响应。在9个基准测试和5个LLM上的大量实验表明，所提出的MAS-GPT在多种设置下始终优于10多个基线MAS方法，证明了MAS-GPT的高效率、高效能和强大的泛化能力。代码将在<a target="_blank" rel="noopener" href="https://github.com/rui-ye/MAS-GPT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/rui-ye/MAS-GPT上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03686v1">PDF</a> 26 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>基于LLM的多智能体系统（MAS）在处理多样化任务时显示出巨大潜力。然而，为了设计有效的MAS，现有方法严重依赖于手动配置或多次调用高级LLM，导致不适应和高推理成本。本文简化了构建MAS的过程，将其重新构建为生成式语言任务，其中输入是用户查询，输出是相应的MAS。为了解决这一新任务，我们将MAS的表示形式统一为可执行代码，并提出一种面向一致性的数据构建管道，以创建包含连贯和一致查询-MAS对的高质量数据集。使用该数据集，我们训练了MAS-GPT，这是一个开源的中型LLM，能够在单个LLM推理内生成查询适应性MAS。生成的MAS可以无缝应用于处理用户查询并提供高质量响应。在9个基准测试和5个LLM上的广泛实验表明，所提出的MAS-GPT在多种设置下始终优于10多种基线MAS方法，证明了MAS-GPT的高效性、高效能和强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based MAS在处理多样化任务时具有显著潜力。</li>
<li>当前设计MAS的方法过于复杂，涉及手动配置和多次LLM调用，导致不适应和高成本。</li>
<li>本文将构建MAS简化为生成式语言任务，使输入为用户查询，输出为相应的MAS。</li>
<li>MAS的统一表示形式为可执行代码，并引入一致性数据构建管道创建高质量数据集。</li>
<li>训练了MAS-GPT，一个能够在单个LLM推理内生成查询适应性MAS的开源中型LLM。</li>
<li>MAS-GPT在多个基准测试上表现优异，优于多种基线MAS方法。</li>
<li>MAS-GPT具备高效性、高效能和强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03686">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-03c7febe135aa80f8dd9eccce3b3c7e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67882454ecdba6ad6c30ff5ab1a3ffeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbb7a743126ff26d16a0e3f24215fea2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ad0b951fd187b846168927271f5311d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76f4f42b509e0b4ec3470570bb9d224c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-Neutral-Point-of-View-Text-Generation-through-Parameter-Efficient-Reinforcement-Learning-and-a-Small-Scale-High-Quality-Dataset"><a href="#Improving-Neutral-Point-of-View-Text-Generation-through-Parameter-Efficient-Reinforcement-Learning-and-a-Small-Scale-High-Quality-Dataset" class="headerlink" title="Improving Neutral Point of View Text Generation through   Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality   Dataset"></a>Improving Neutral Point of View Text Generation through   Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality   Dataset</h2><p><strong>Authors:Jessica Hoffmann, Christiane Ahlheim, Zac Yu, Aria Walfrand, Jarvis Jin, Marie Tano, Ahmad Beirami, Erin van Liemt, Nithum Thain, Hakim Sidahmed, Lucas Dixon</strong></p>
<p>This paper describes the construction of a dataset and the evaluation of training methods to improve generative large language models’ (LLMs) ability to answer queries on sensitive topics with a Neutral Point of View (NPOV), i.e., to provide significantly more informative, diverse and impartial answers. The dataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written quadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set of links to source texts elaborating the various points of view. The first key contribution of this paper is a new methodology to create such datasets through iterative rounds of human peer-critique and annotator training, which we release alongside the dataset. The second key contribution is the identification of a highly effective training regime for parameter-efficient reinforcement learning (PE-RL) to improve NPOV generation. We compare and extensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a strong baseline), SFT and RLHF.   PE-RL not only improves on overall NPOV quality compared to the strongest baseline ($97.06%\rightarrow 99.08%$), but also scores much higher on features linguists identify as key to separating good answers from the best answers ($60.25%\rightarrow 85.21%$ for presence of supportive details, $68.74%\rightarrow 91.43%$ for absence of oversimplification). A qualitative analysis corroborates this. Finally, our evaluation finds no statistical differences between results on topics that appear in the training dataset and those on separated evaluation topics, which provides strong evidence that our approach to training PE-RL exhibits very effective out of topic generalization. </p>
<blockquote>
<p>本文描述了一个数据集的构建过程以及训练方法的评估，旨在提高生成式大型语言模型（LLM）在敏感话题上以中立观点（NPOV）回答问题的能力。具体来说，即为提供更富有信息、多样化和公正的答案。数据集SHQ-NPOV数据集包含300个高质量的人为编写的四元组：关于敏感话题的查询、答案、中立观点评分和一系列链接来源文本，阐述了不同的观点。本文的第一个关键贡献是提出一种新的方法，通过一系列的人同行批评和标注训练来创建这样的数据集，我们随数据集一起发布这种方法。第二个关键贡献是确定了一种高效的训练制度来提高参数效率强化学习（PE-RL）的中立观点生成能力。我们将PE-RL与多个基线方法进行了比较和全面评估，包括LoRA微调（一个强有力的基线方法）、SFT和RLHF。PE-RL不仅在总体中立观点质量方面与最强基线相比有所提升（从$97.06%$提高到$99.08%$），而且在语言学家认为区分好答案与最佳答案的关键特征方面得分更高（支持细节的存在性从$60.25%$提高到$85.21%$，无简化现象从$68.74%$提高到$91.43%$）。定性分析证实了这一点。最后，我们的评估发现在训练数据集中出现的主题与单独评估的主题之间不存在统计差异，这为我们的PE-RL训练方法展现出了有效的跨话题泛化能力提供了有力证据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03654v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文构建了一个数据集，并评估了训练方法来提升大型生成式语言模型（LLM）在敏感话题上的中立观点回答能力。该数据集通过迭代的人类同行评审和注释者培训方法创建，并发布了数据集。研究发现了一种高效的训练机制——参数高效强化学习（PE-RL），用于提高中立观点的生成质量。与最强基线相比，PE-RL不仅提高了整体中立观点的质量，而且在语言学家认为关键的特征上也有显著提高。此外，研究结果显示，该训练机制在训练集中的话题与独立评估的话题之间没有统计差异，显示出很强的跨话题泛化能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>论文构建了一个名为SHQ-NPOV的新数据集，用于训练大型语言模型在敏感话题上提供中立观点的查询回答。</li>
<li>数据集包含高质量的四元组，每个四元组包括敏感话题的查询、答案、中立观点评分和来源文本的链接。</li>
<li>论文提出了一种通过迭代的人类同行评审和注释者培训来创建数据集的新方法。</li>
<li>论文识别了一种高效的训练机制——参数高效强化学习（PE-RL），用于提高大型语言模型在中立观点生成方面的性能。</li>
<li>PE-RL与基线方法相比，显著提高了中立观点答案的整体质量和关键语言特征的表现。</li>
<li>评估发现，训练机制在训练集中的话题和独立评估的话题之间没有统计差异，表明该机制具有强大的跨话题泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dc1612fea791676a60734a6dd5e96dfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0425d48325ebd84a1d8690e802d6eed8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fd9bb260e2daccabd25b8b56d52a1ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89263554016badcb49131b57a334d86.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Small-but-Mighty-Enhancing-Time-Series-Forecasting-with-Lightweight-LLMs"><a href="#Small-but-Mighty-Enhancing-Time-Series-Forecasting-with-Lightweight-LLMs" class="headerlink" title="Small but Mighty: Enhancing Time Series Forecasting with Lightweight   LLMs"></a>Small but Mighty: Enhancing Time Series Forecasting with Lightweight   LLMs</h2><p><strong>Authors:Haoran Fan, Bin Li, Yixuan Weng, Shoujun Zhou</strong></p>
<p>While LLMs have demonstrated remarkable potential in time series forecasting, their practical deployment remains constrained by excessive computational demands and memory footprints. Existing LLM-based approaches typically suffer from three critical limitations: Inefficient parameter utilization in handling numerical time series patterns; Modality misalignment between continuous temporal signals and discrete text embeddings; and Inflexibility for real-time expert knowledge integration. We present SMETimes, the first systematic investigation of sub-3B parameter SLMs for efficient and accurate time series forecasting. Our approach centers on three key innovations: A statistically-enhanced prompting mechanism that bridges numerical time series with textual semantics through descriptive statistical features; A adaptive fusion embedding architecture that aligns temporal patterns with language model token spaces through learnable parameters; And a dynamic mixture-of-experts framework enabled by SLMs’ computational efficiency, adaptively combining base predictions with domain-specific models. Extensive evaluations across seven benchmark datasets demonstrate that our 3B-parameter SLM achieves state-of-the-art performance on five primary datasets while maintaining 3.8x faster training and 5.2x lower memory consumption compared to 7B-parameter LLM baselines. Notably, the proposed model exhibits better learning capabilities, achieving 12.3% lower MSE than conventional LLM. Ablation studies validate that our statistical prompting and cross-modal fusion modules respectively contribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks. By redefining the efficiency-accuracy trade-off landscape, this work establishes SLMs as viable alternatives to resource-intensive LLMs for practical time series forecasting. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/xiyan1234567/SMETimes">https://github.com/xiyan1234567/SMETimes</a>. </p>
<blockquote>
<p>尽管大型语言模型（LLMs）在时间序列预测方面展现出了显著潜力，但它们在实践部署中仍受到计算需求过大和内存占用过高的限制。当前基于LLM的方法通常存在三个关键局限性：处理数值时间序列模式时的参数利用效率低下；连续时间信号与离散文本嵌入之间的模态不匹配；以及实时专家知识整合的不灵活。我们推出了SMETimes，这是首次对参数少于3B的子集语言模型（SLMs）进行系统研究，以实现高效和准确的时间序列预测。我们的方法以三个关键创新为中心：一种统计增强的提示机制，通过描述性统计特征将数值时间序列与文本语义联系起来；一种自适应融合嵌入架构，通过可学习参数将时间模式与语言模型标记空间对齐；以及由SLMs的计算效率启动的动态混合专家框架，自适应地将基本预测与领域特定模型相结合。在七个基准数据集上的广泛评估表明，我们的3B参数SLM在五个主要数据集上达到了最先进的性能，同时与7B参数LLM基准相比，训练速度提高了3.8倍，内存消耗降低了5.2倍。值得注意的是，所提出的模型展现出更好的学习能力，均方误差（MSE）比传统LLM降低了12.3%。消融研究验证了我们的统计提示和跨模态融合模块分别在长期预测任务中分别贡献了15.7%和18.2%的错误率降低。通过重新定义效率-准确性权衡景观，这项工作确立了SLMs作为用于实际时间序列预测的替代资源密集型LLMs的可行选择。代码和模型可通过<a target="_blank" rel="noopener" href="https://github.com/xiyan1234567/SMETimes%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xiyan1234567/SMETimes获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03594v1">PDF</a> Work in progress</p>
<p><strong>摘要</strong></p>
<p>LLM在时间序列预测中展现出显著潜力，但其实际应用受到计算需求和内存占用过大的限制。现有LLM方法存在三个关键局限：数值时间序列模式处理中的参数利用低效、连续时间信号与离散文本嵌入之间的模态不匹配以及实时专家知识整合的灵活性不足。本研究首次探讨小于3B参数的SLM，以实现高效准确的时间序列预测。通过三项关键创新解决上述问题：通过描述性统计特征桥接数值时间序列与文本语义的统计增强提示机制；通过可学习参数对齐时间模式与语言模型令牌空间的自适应融合嵌入架构；以及由SLM计算效率支持的动态混合专家框架，自适应结合基本预测与领域特定模型。在七个基准数据集上的广泛评估表明，我们的3B参数SLM在五个主要数据集上实现最佳性能，与7B参数LLM基准相比，训练速度提高3.8倍，内存占用减少5.2倍。特别是，所提出模型展现出更好的学习能力，均方误差降低了12.3%。消融研究验证了我们的统计提示和跨模态融合模块分别在长期预测任务中分别实现了15.7%和18.2%的误差降低。通过重新定义效率-准确性权衡景观，本研究确立了SLM作为实用时间序列预测的可行替代方案，以替代资源密集型的LLM。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/xiyan1234567/SMETimes%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xiyan1234567/SMETimes获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM在时间序列预测中的实际应用受到限制，主要因为计算需求大、内存占用高及三个关键局限。</li>
<li>现有方法在处理数值时间序列模式、模态对齐及实时专家知识整合方面存在不足。</li>
<li>SMETimes首次探讨小于3B参数的SLM，实现高效准确的时间序列预测。</li>
<li>三项关键创新包括统计增强提示机制、自适应融合嵌入架构和动态混合专家框架。</li>
<li>在多个数据集上的评估显示，所提出的SLM在性能和效率方面均优于7B参数的LLM。</li>
<li>所提出模型展现出更好的学习能力，且消融研究验证了其关键组成部分的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03594">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5df94aa0894eed2495c41de5b864eb75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-445831f5871ede172f16d60fec14e660.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60e631961b6a1030b466e5a8b724ee1e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FairSense-AI-Responsible-AI-Meets-Sustainability"><a href="#FairSense-AI-Responsible-AI-Meets-Sustainability" class="headerlink" title="FairSense-AI: Responsible AI Meets Sustainability"></a>FairSense-AI: Responsible AI Meets Sustainability</h2><p><strong>Authors:Shaina Raza, Mukund Sayeeganesh Chettiar, Matin Yousefabadi, Tahniat Khan, Marcelo Lotif</strong></p>
<p>In this paper, we introduce FairSense-AI: a multimodal framework designed to detect and mitigate bias in both text and images. By leveraging Large Language Models (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle forms of prejudice or stereotyping that can appear in content, providing users with bias scores, explanatory highlights, and automated recommendations for fairness enhancements. In addition, FairSense-AI integrates an AI risk assessment component that aligns with frameworks like the MIT AI Risk Repository and NIST AI Risk Management Framework, enabling structured identification of ethical and safety concerns. The platform is optimized for energy efficiency via techniques such as model pruning and mixed-precision computation, thereby reducing its environmental footprint. Through a series of case studies and applications, we demonstrate how FairSense-AI promotes responsible AI use by addressing both the social dimension of fairness and the pressing need for sustainability in large-scale AI deployments. <a target="_blank" rel="noopener" href="https://vectorinstitute.github.io/FairSense-AI">https://vectorinstitute.github.io/FairSense-AI</a>, <a target="_blank" rel="noopener" href="https://pypi.org/project/fair-sense-ai/">https://pypi.org/project/fair-sense-ai/</a> (Sustainability , Responsible AI , Large Language Models , Vision Language Models , Ethical AI , Green AI) </p>
<blockquote>
<p>本文介绍了FairSense-AI：一个旨在检测和缓解文本和图像中偏见的多模式框架。通过利用大型语言模型（LLM）和视觉语言模型（VLM），FairSense-AI揭示了内容中可能出现的微妙形式的偏见或刻板印象，为用户提供偏见分数、解释亮点和公平性增强的自动化建议。此外，FairSense-AI还集成了一个与MIT人工智能风险仓库和NIST人工智能风险管理框架等框架相吻合的人工智能风险评估组件，能够实现道德和安全问题的结构化识别。该平台通过模型修剪和混合精度计算等技术进行优化，从而提高能效，减少环境足迹。通过一系列案例研究与应用，我们展示了FairSense-AI如何通过解决公平的社会维度和大规模人工智能部署中对可持续性的迫切需求，促进人工智能的负责任使用。<a target="_blank" rel="noopener" href="https://vectorinstitute.github.io/FairSense-AI%EF%BC%8Chttps://pypi.org/project/fair-sense-ai/%EF%BC%88%E5%8F%AF%E6%8C%81%E7%BB%AD%E6%80%A7%E3%80%81%E8%B4%9F%E8%B4%A3%E4%BB%BB%E7%9A%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%81%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E3%80%81%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E3%80%81%E9%81%93%E5%BE%B7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%81%E7%BB%BF%E8%89%B2%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%89%E3%80%82">https://vectorinstitute.github.io/FairSense-AI，https://pypi.org/project/fair-sense-ai/（可持续性、负责任的人工智能、大型语言模型、视觉语言模型、道德人工智能、绿色人工智能）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02865v2">PDF</a> </p>
<p><strong>Summary</strong><br>     引入FairSense-AI，这是一个多模态框架，旨在检测和缓解文本和图像中的偏见。利用大型语言模型和视觉语言模型，FairSense-AI可以揭示内容中可能出现的微妙偏见或刻板印象，为用户提供偏见分数、解释亮点和公平增强建议。此外，它结合了人工智能风险评估组件，符合麻省理工学院的人工智能风险仓库和美国国家标准技术研究院的人工智能风险管理框架的要求，能系统地识别道德和安全方面的顾虑。平台经过优化，通过模型修剪和混合精度计算等技术提升能效，降低对环境的影响。通过一系列案例研究和实践应用证明FairSense-AI如何通过解决公平性的社会维度以及在大规模部署中对可持续性的紧迫需求来推动负责任的人工智能使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FairSense-AI是一个多模态框架，用于检测和缓解文本和图像中的偏见。</li>
<li>它利用大型语言模型和视觉语言模型来揭示内容中的偏见和刻板印象。</li>
<li>FairSense-AI提供偏见分数、解释亮点和公平增强建议。</li>
<li>该平台结合人工智能风险评估组件以符合多种标准框架的要求。</li>
<li>FairSense-AI能够系统地识别道德和安全方面的顾虑。</li>
<li>平台经过优化以提高能效并降低对环境的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02865">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1c0dd782a7ff87c98922cb57986c014d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4fde7d38e3d56562ea96eebd0aedeef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d77cb7541762d9c606bcaa498006c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e5a910f1201eafdd58c65cdaebd2c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-083ad7d5739c287985ec93b537854d50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40fede344f9e79de122916e0372dceee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62761ba41b2bb2fee29a1671e909df53.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PaCA-Partial-Connection-Adaptation-for-Efficient-Fine-Tuning"><a href="#PaCA-Partial-Connection-Adaptation-for-Efficient-Fine-Tuning" class="headerlink" title="PaCA: Partial Connection Adaptation for Efficient Fine-Tuning"></a>PaCA: Partial Connection Adaptation for Efficient Fine-Tuning</h2><p><strong>Authors:Sunghyeon Woo, Sol Namkung, Sunwoo Lee, Inho Jeong, Beomseok Kim, Dongsuk Jeon</strong></p>
<p>Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage and computational costs of fine-tuning large neural network models by training only a few additional adapter parameters, rather than the entire model. However, the reduction in computational costs due to PEFT does not necessarily translate to a reduction in training time; although the computational costs of the adapter layers are much smaller than the pretrained layers, it is well known that those two types of layers are processed sequentially on GPUs, resulting in significant latency overhead. LoRA and its variants merge low-rank adapter matrices with pretrained weights during inference to avoid latency overhead, but during training, the pretrained weights remain frozen while the adapter matrices are continuously updated, preventing such merging. To mitigate this issue, we propose Partial Connection Adaptation (PaCA), which fine-tunes randomly selected partial connections within the pretrained weights instead of introducing adapter layers in the model. PaCA not only enhances training speed by eliminating the time overhead due to the sequential processing of the adapter and pretrained layers but also reduces activation memory since only partial activations, rather than full activations, need to be stored for gradient computation. Compared to LoRA, PaCA reduces training time by 22% and total memory usage by 16%, while maintaining comparable accuracy across various fine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction tuning on the Oasst1 dataset. PaCA can also be combined with quantization, enabling the fine-tuning of large models such as LLaMA3.1-70B. In addition, PaCA enables training with 23% longer sequence and improves throughput by 16% on both NVIDIA A100 GPU and INTEL Gaudi2 HPU compared to LoRA. The code is available at <a target="_blank" rel="noopener" href="https://github.com/WooSunghyeon/paca">https://github.com/WooSunghyeon/paca</a>. </p>
<blockquote>
<p>先前的参数高效微调（PEFT）算法通过仅训练少量额外的适配器参数，而不是对整个模型进行训练，从而减少了微调大型神经网络模型的内存使用和计算成本。然而，由于PEFT导致的计算成本降低并不一定转化为训练时间的减少；虽然适配器层的计算成本远远小于预训练层，众所周知，这两种类型的层在GPU上是顺序处理的，导致显著的延迟开销。LoRA及其变体在推理过程中将低秩适配器矩阵与预训练权重合并，以避免延迟开销，但在训练过程中，预训练权重保持冻结状态，而适配器矩阵不断更新，从而阻止这种合并。为了缓解这个问题，我们提出了部分连接适配（PaCA），它在预训练权重内微调随机选择的部分连接，而不是在模型中引入适配器层。PaCA不仅通过消除由于适配器层和预训练层的顺序处理而产生的时间开销来提高训练速度，而且减少了激活内存，因为只需要存储部分激活值，而不是完整的激活值来进行梯度计算。与LoRA相比，PaCA将训练时间减少了22%，总内存使用量减少了16%，同时在各种微调场景（如使用MMLU数据集进行微调和使用Oasst1数据集进行指令微调）中保持相当的准确性。PaCA还可以与量化相结合，实现对大型模型（如LLaMA 3.1-70B）的微调。此外，PaCA使用NVIDIA A100 GPU和INTEL Gaudi2 HPU时，能够处理更长的序列（提高23%），并且吞吐量比LoRA提高16%。代码可在<a target="_blank" rel="noopener" href="https://github.com/WooSunghyeon/paca%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WooSunghyeon/paca找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01905v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模神经网络模型的微调需要大量计算资源和内存。为了降低这种成本，研究者提出了参数高效微调（PEFT）算法，只训练少量适配器参数而不是整个模型。然而，PEFT并不一定能减少训练时间，因为适配器层和预训练层在GPU上是顺序处理的，导致延迟开销较大。为了解决这个问题，研究者提出了部分连接适配（PaCA）方法，它通过微调预训练权重中的部分连接而不是引入适配器层来提高训练速度和减少内存使用。相较于LoRA方法，PaCA可以减少训练时间和总内存使用，同时维持在不同微调场景下的准确度。此外，PaCA还可以与量化结合，使大型模型的微调成为可能，如LLaMA3.1-70B。PaCA还提高了序列长度和吞吐量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>参数高效微调（PEFT）算法可以减少大规模神经网络模型的计算资源和内存使用。</li>
<li>PEFT并不能直接减少训练时间，因为适配器层和预训练层的处理存在延迟开销。</li>
<li>部分连接适配（PaCA）方法通过微调预训练权重中的部分连接来提高训练速度和减少内存使用。</li>
<li>PaCA相较于LoRA方法可以减少训练时间和总内存使用。</li>
<li>PaCA可以维持在不同微调场景下的准确度，如MMLU数据集上的微调以及Oasst1数据集上的指令微调。</li>
<li>PaCA可以与量化结合，使大型模型的微调成为可能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01905">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fea4602eb3dd8b54e82c46d5f68a874b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ca70077b7eea935ee161e2b872ea259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c31f5d31b8b8c07a3cf173277a1fe769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-578ca7e29e65ec372bdbe8b9980966ce.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Language-Assisted-Feature-Transformation-for-Anomaly-Detection"><a href="#Language-Assisted-Feature-Transformation-for-Anomaly-Detection" class="headerlink" title="Language-Assisted Feature Transformation for Anomaly Detection"></a>Language-Assisted Feature Transformation for Anomaly Detection</h2><p><strong>Authors:EungGu Yun, Heonjin Ha, Yeongwoo Nam, Bryan Dongik Lee</strong></p>
<p>This paper introduces LAFT, a novel feature transformation method designed to incorporate user knowledge and preferences into anomaly detection using natural language. Accurately modeling the boundary of normality is crucial for distinguishing abnormal data, but this is often challenging due to limited data or the presence of nuisance attributes. While unsupervised methods that rely solely on data without user guidance are common, they may fail to detect anomalies of specific interest. To address this limitation, we propose Language-Assisted Feature Transformation (LAFT), which leverages the shared image-text embedding space of vision-language models to transform visual features according to user-defined requirements. Combined with anomaly detection methods, LAFT effectively aligns visual features with user preferences, allowing anomalies of interest to be detected. Extensive experiments on both toy and real-world datasets validate the effectiveness of our method. </p>
<blockquote>
<p>本文介绍了LAFT，这是一种新型的特征转换方法，旨在将用户知识和偏好融入自然语言异常检测中。准确建模正常边界对于区分异常数据至关重要，但由于数据有限或存在干扰属性，这通常具有挑战性。虽然仅依赖数据而无需用户指导的无监督方法很常见，但它们可能无法检测到特定感兴趣的异常值。为了解决这一局限性，我们提出了语言辅助特征转换（LAFT），它利用视觉语言模型的共享图像文本嵌入空间，根据用户定义的要求转换视觉特征。结合异常检测方法，LAFT有效地将视觉特征与用户偏好对齐，从而可以检测到感兴趣的异常值。在玩具和真实世界数据集上的大量实验验证了我们的方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01184v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>LAFT是一种结合用户知识和偏好进行自然语言异常检测的新颖特征转换方法。该方法利用视觉语言模型的图像文本嵌入空间，根据用户定义的需求转换视觉特征，有效地将视觉特征与用户需求对齐，从而检测有趣的异常值。实验证明，该方法在玩具和真实数据集上均有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LAFT是一种新的特征转换方法，旨在结合用户知识和偏好进行异常检测。</li>
<li>该方法利用视觉语言模型的图像文本嵌入空间进行视觉特征的转换。</li>
<li>LAFT可以有效地将视觉特征与用户需求对齐，从而检测有趣的异常值。</li>
<li>有限的数据或干扰属性给准确建模正常范围的边界带来了挑战。</li>
<li>传统的无监督方法可能无法检测到特定的异常值。</li>
<li>LAFT通过与异常检测方法相结合来解决这一问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e7f7378acf8d55dc3d9f1f5377fb4b1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ea9c0eb8d8fe52385abdae9a667dd21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c951ab25b94efbb64aaec845fad0797e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a1a018327994db35d581fd0b8b88a13.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Unify-and-Anchor-A-Context-Aware-Transformer-for-Cross-Domain-Time-Series-Forecasting"><a href="#Unify-and-Anchor-A-Context-Aware-Transformer-for-Cross-Domain-Time-Series-Forecasting" class="headerlink" title="Unify and Anchor: A Context-Aware Transformer for Cross-Domain Time   Series Forecasting"></a>Unify and Anchor: A Context-Aware Transformer for Cross-Domain Time   Series Forecasting</h2><p><strong>Authors:Xiaobin Hong, Jiawen Zhang, Wenzhong Li, Sanglu Lu, Jia Li</strong></p>
<p>The rise of foundation models has revolutionized natural language processing and computer vision, yet their best practices to time series forecasting remains underexplored. Existing time series foundation models often adopt methodologies from these fields without addressing the unique characteristics of time series data. In this paper, we identify two key challenges in cross-domain time series forecasting: the complexity of temporal patterns and semantic misalignment. To tackle these issues, we propose the &#96;&#96;Unify and Anchor” transfer paradigm, which disentangles frequency components for a unified perspective and incorporates external context as domain anchors for guided adaptation. Based on this framework, we introduce ContexTST, a Transformer-based model that employs a time series coordinator for structured representation and the Transformer blocks with a context-informed mixture-of-experts mechanism for effective cross-domain generalization. Extensive experiments demonstrate that ContexTST advances state-of-the-art forecasting performance while achieving strong zero-shot transferability across diverse domains. </p>
<blockquote>
<p>随着基础模型的兴起，自然语言处理和计算机视觉领域发生了革命性的变化，然而它们在时间序列预测方面的最佳实践仍然被探索不足。现有的时间序列基础模型往往采用这些领域的方法，而没有解决时间序列数据的独特特征。在本文中，我们确定了跨域时间序列预测的两个关键挑战：时间模式的复杂性和语义不对齐。为了解决这些问题，我们提出了“统一与锚点”迁移范式，该范式从统一的角度解决频率成分，并将外部上下文作为域锚点进行引导适应。基于此框架，我们引入了ContexTST模型，这是一个基于Transformer的模型，采用时间序列协调器进行结构化表示，并结合上下文感知的混合专家机制的Transformer块，实现有效的跨域泛化。大量实验表明，ContexTST模型在预测性能上取得了最先进的进展，同时在不同的领域实现了强大的零样本迁移能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01157v1">PDF</a> 20 pages, 12 figures, 8 tables, conference under review</p>
<p><strong>Summary</strong><br>时间序列预测领域中，跨域模型的应用存在两大挑战：时间模式的复杂性和语义不匹配问题。本文提出“统一与锚定”转移范式来解决这些问题，介绍了基于Transformer的ContexTST模型。该模型利用时间序列协调器进行结构化表示，并引入带有上下文信息的混合专家机制来实现跨域泛化。实验表明，ContexTST在先进的时间序列预测性能上取得了显著进展，同时在不同的领域之间实现了强大的零样本迁移能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间序列预测领域中跨域模型面临两大挑战：时间模式的复杂性和语义不匹配。</li>
<li>“统一与锚定”转移范式被提出以解决这些问题，该范式旨在从统一的角度解析频率成分，并通过引入领域锚点实现引导适应。</li>
<li>ContexTST是一个基于Transformer的模型，采用时间序列协调器进行结构化表示。</li>
<li>ContexTST模型使用带有上下文信息的混合专家机制，以提高跨域泛化能力。</li>
<li>实验结果显示ContexTST在先进的时间序列预测性能上取得了显著进展。</li>
<li>ContexTST模型在不同的领域之间实现了强大的零样本迁移能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4bb9a9c32848f763d30c8f3684fc288d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a075aaa03d259e8a2126e58e5cc8648d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28800b2adb99501c81d3796f15cfea05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ade2db812bdb51bc43e11bc8db4290be.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Personalize-Your-LLM-Fake-it-then-Align-it"><a href="#Personalize-Your-LLM-Fake-it-then-Align-it" class="headerlink" title="Personalize Your LLM: Fake it then Align it"></a>Personalize Your LLM: Fake it then Align it</h2><p><strong>Authors:Yijing Zhang, Dyah Adila, Changho Shin, Frederic Sala</strong></p>
<p>Personalizing large language models (LLMs) is essential for delivering tailored interactions that improve user experience. Many existing personalization methods require fine-tuning LLMs for each user, rendering them prohibitively expensive for widespread adoption. Although retrieval-based approaches offer a more compute-efficient alternative, they still depend on large, high-quality datasets that are not consistently available for all users. To address this challenge, we propose CHAMELEON, a scalable and efficient personalization approach that uses (1) self-generated personal preference data and (2) representation editing to enable quick and cost-effective personalization. Our experiments on various tasks, including those from the LaMP personalization benchmark, show that CHAMELEON efficiently adapts models to personal preferences, improving instruction-tuned models and outperforms two personalization baselines by an average of 40% across two model architectures. </p>
<blockquote>
<p>个性化大型语言模型（LLM）对于提供定制化的交互体验至关重要，这可以改善用户体验。许多现有的个性化方法需要对每个用户进行微调LLM，这使得它们广泛应用的成本过高。虽然基于检索的方法提供了一种更高效的计算替代方案，但它们仍然依赖于并非所有用户都可用的高质量大型数据集。为了应对这一挑战，我们提出了CHAMELEON，这是一种可扩展且高效的个性化方法，它采用（1）自我生成的个性化偏好数据以及（2）表示编辑来实现快速且经济的个性化。我们在包括来自LaMP个性化基准测试的各种任务上的实验表明，CHAMELEON可以有效地适应个人偏好，改进指令调整模型，并在两种模型架构上平均优于两个个性化基准测试40%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01048v3">PDF</a> NAACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>LLM个性化对于提供改善用户体验的定制交互至关重要。现有个人化方法需要大量针对每个用户的微调，导致成本高昂，难以广泛应用。我们提出CHAMELEON方法，通过（1）自我生成的个人偏好数据和（2）表示编辑，实现快速、经济的个性化。实验表明，CHAMELEON在多个任务上表现优异，包括LaMP个性化基准测试，能够高效适应个人偏好，改进指令调整模型，并在两种模型架构上平均优于两个个性化基准测试40%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM个性化对于提升用户体验至关重要。</li>
<li>现有个人化方法因需要大量针对每个用户的微调而成本高昂。</li>
<li>CHAMELEON方法通过使用自我生成的个人偏好数据和表示编辑实现快速、经济的个性化。</li>
<li>CHAMELEON在多个任务上表现优异，包括LaMP个性化基准测试。</li>
<li>CHAMELEON能够高效适应个人偏好，改进指令调整模型。</li>
<li>CHAMELEON在两种模型架构上的性能平均优于两个个性化基准测试40%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01048">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e7528df0739dd6284f28b9f49e225955.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c4280d8af0e7ce713926a954a20205a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da58698d5d8d79dbbf8b09c85202db5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b586a3093133a8c90cb29001e4c8207.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Re-Imagining-Multimodal-Instruction-Tuning-A-Representation-View"><a href="#Re-Imagining-Multimodal-Instruction-Tuning-A-Representation-View" class="headerlink" title="Re-Imagining Multimodal Instruction Tuning: A Representation View"></a>Re-Imagining Multimodal Instruction Tuning: A Representation View</h2><p><strong>Authors:Yiyang Liu, James Chenhao Liang, Ruixiang Tang, Yugyung Lee, Majid Rabbani, Sohail Dianat, Raghuveer Rao, Lifu Huang, Dongfang Liu, Qifan Wang, Cheng Han</strong></p>
<p>Multimodal instruction tuning has proven to be an effective strategy for achieving zero-shot generalization by fine-tuning pre-trained Large Multimodal Models (LMMs) with instruction-following data. However, as the scale of LMMs continues to grow, fully fine-tuning these models has become highly parameter-intensive. Although Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced to reduce the number of tunable parameters, a significant performance gap remains compared to full fine-tuning. Furthermore, existing PEFT approaches are often highly parameterized, making them difficult to interpret and control. In light of this, we introduce Multimodal Representation Tuning (MRT), a novel approach that focuses on directly editing semantically rich multimodal representations to achieve strong performance and provide intuitive control over LMMs. Empirical results show that our method surpasses current state-of-the-art baselines with significant performance gains (e.g., 1580.40 MME score) while requiring substantially fewer tunable parameters (e.g., 0.03% parameters). Additionally, we conduct experiments on editing instrumental tokens within multimodal representations, demonstrating that direct manipulation of these representations enables simple yet effective control over network behavior. </p>
<blockquote>
<p>多模态指令调整已被证明是一种通过微调预训练的大型多模态模型（LMMs）与遵循指令的数据，实现零射击泛化的有效策略。然而，随着LMM规模的不断增长，完全微调这些模型已经变得非常参数密集。虽然已引入参数高效微调（PEFT）方法来减少可调参数的数量，但与完全微调相比，仍存在显著的性能差距。此外，现有的PEFT方法通常高度参数化，使得它们难以解释和控制。鉴于此，我们引入了多模态表示调整（MRT），这是一种新型方法，专注于直接编辑语义丰富的多模态表示，以实现强大的性能和直观控制LMMs。经验结果表明，我们的方法在达到最新基线水平的同时实现了显著的性能提升（例如，1580.40 MME得分），同时需要的可调参数大大减少（例如，仅使用百分之零点零三参数）。此外，我们还在编辑多模态表示中的仪器符号方面进行了实验，结果表明直接操作这些表示可以简单有效地控制网络行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00723v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多模态指令调整是一种通过微调预训练的大型多模态模型（LMMs）以实现零样本泛化的有效策略。然而，随着LMMs规模的扩大，完全微调这些模型变得极为参数密集。尽管已经引入了参数高效微调（PEFT）方法来减少可调参数的数量，但与完全微调相比仍存在显著的性能差距。本文介绍了一种新型的多模态表示调整（MRT）方法，该方法直接编辑语义丰富的多模态表示，以实现强大的性能和直观控制LMMs的能力。实验结果显示，该方法在显著的性能提升（例如，MME分数为1580.40）的同时，所需的可调参数大幅减少（例如，仅0.03%的参数）。此外，我们还进行了编辑仪器符号的令牌内多模态表示的测试，证明了直接操作这些表示能简单有效地控制网络行为。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态指令调整是一种有效的零样本泛化策略，通过微调预训练的大型多模态模型（LMMs）。</li>
<li>随着LMMs规模的增长，完全微调模型变得极为参数密集。</li>
<li>参数高效微调（PEFT）方法虽然可以减少可调参数数量，但仍存在与全微调显著的性能差距。</li>
<li>多模态表示调整（MRT）是一种新型方法，直接编辑语义丰富的多模态表示，以实现强大的性能和直观控制LMMs。</li>
<li>MRT方法显著提高了性能（例如，MME分数为1580.40）并大幅减少了可调参数的需求（例如，仅0.03%的参数）。</li>
<li>直接编辑多模态表示中的仪器符号令牌可以简单有效地控制网络行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00723">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bd945f663df508cf4d08b1fc98de8c50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9331da710b49f23cf7564c1bfbf1c1b0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information"><a href="#Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information" class="headerlink" title="Transformer Meets Twicing: Harnessing Unattended Residual Information"></a>Transformer Meets Twicing: Harnessing Unattended Residual Information</h2><p><strong>Authors:Laziz Abdullaev, Tan Nguyen</strong></p>
<p>Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses kernel twicing procedure in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees and enhanced adversarial robustness. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved robustness and accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data. </p>
<blockquote>
<p>基于Transformer的深度学习模型已在众多语言和视觉任务中实现了最先进的性能。虽然Transformer的核心组件自注意力机制已证明能够处理复杂的数据模式，但人们观察到，随着Transformer层的增加，注意力矩阵的表示能力会显著下降，从而损害其总体性能。在这项工作中，我们利用自注意力计算与低通非局部均值（NLM）平滑滤波器之间的联系，提出Twicing Attention这一新型注意力机制。它采用非参数回归中的核Twicing程序，以缓解与NLM平滑相关的低通行为，具有引人注目的理论保证和增强的对抗鲁棒性。这种方法能够提取和再利用每层不完美的平滑操作后残留中的有意义信息。我们的方法相对于标准的自注意力具有两个主要优点：1）表示能力的衰减速度较慢；2）在各种数据模态和任务中提高了鲁棒性和准确性。我们在多个任务和基准测试上实证了我们的模型相对于基线Transformer的性能提升，包括图像分类、语言建模以及在干净和损坏数据上的表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00687v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Transformer的深度模型已在多个语言和视觉任务中取得最先进的性能。本文发现Transformer中的自注意力机制在处理复杂数据模式时表现出色，但注意力矩阵在Transformer层间的表示能力显著下降，影响了整体性能。为此，本文利用自注意力计算与低通非局部均值平滑滤波之间的联系，提出了Twicing Attention新型注意力机制。它采用非参数回归中的核扭曲过程来缓解与NLM平滑相关的低通行为，具有令人信服的理论保证和增强的对抗鲁棒性。该方法能够提取和再利用每一层不完美平滑操作后的残差中保留的有意义的信息。与标准自注意力相比，本文方法提供了两个主要优势：1）证明了的表示能力衰减较慢；2）提高了在各种数据模态和任务上的鲁棒性和准确性。在多个任务和基准测试上，包括图像分类和语言建模，以及在干净和损坏的数据上，本文模型均实现了性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer模型在语言和视觉任务上表现卓越，但自注意力机制存在表示能力退化问题。</li>
<li>退化问题导致模型性能下降，特别是在复杂数据模式下。</li>
<li>提出了Twicing Attention机制，结合自注意力与非局部均值平滑滤波，以缓解表示能力退化问题。</li>
<li>Twicing Attention具有理论保证和增强的对抗鲁棒性。</li>
<li>该机制能够提取并再利用层间平滑操作后的残差信息。</li>
<li>与标准自注意力相比，Twicing Attention提供较慢的表示能力衰减和更高的鲁棒性、准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bdb16c3fb2b896f0691dbe7ee7e66f8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33dbab91cb18b20834a539c9c39cbb6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da29fd57acbcffe975cd8e5f69f5a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75b545dbd23498a3ef47b47130e8b6f2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning"><a href="#Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning" class="headerlink" title="Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning"></a>Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning</h2><p><strong>Authors:Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, Jianke Zhu</strong></p>
<p>Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene, but also compromises training and inference efficiency. To address these challenges, we propose a unified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously. To obtain the fine-grained instance-level visual tokens, we first introduce a novel Multi-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D semantics into their corresponding 3D geometric features. For scene-level relation-aware tokens, we further present a 3D Instance Spatial Relation (3D-ISR) module to capture the intricate pairwise spatial relationships among objects. Additionally, we perform end-to-end multi-task instruction tuning simultaneously without the subsequent task-specific fine-tuning. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods across 3D scene understanding, reasoning and grounding tasks. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a> </p>
<blockquote>
<p>尽管在3D场景理解方面取得了令人鼓舞的进展，但开发能够在复杂3D环境中进行理解和推理的有效大型多模态模型（LMM）仍然是一个挑战。大多数之前的方法通常分别编码3D点和2D图像特征，忽略了2D语义和3D对象属性之间的交互，以及3D环境内的空间关系。这种局限性不仅阻碍了对3D场景的全面表示，还影响了训练和推理的效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00513v1">PDF</a> CVPR2025, Code Link: <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Inst3D-LMM的统一大型多模态模型，用于同时处理多个3D场景理解任务。该模型通过引入多视图跨模态融合（MCMF）模块，将多视图2D语义注入到相应的3D几何特征中，获取精细的实例级视觉令牌。同时，通过3D实例空间关系（3D-ISR）模块，捕捉对象之间的复杂成对空间关系，实现场景级关系感知令牌的获取。实验表明，该方法在3D场景理解、推理和定位任务上的性能均优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种名为Inst3D-LMM的大型多模态模型，用于3D场景理解。</li>
<li>引入多视图跨模态融合（MCMF）模块，将2D语义与3D几何特征相结合。</li>
<li>通过3D实例空间关系（3D-ISR）模块，捕捉对象之间的复杂空间关系。</li>
<li>模型能够同时处理多个3D场景理解任务。</li>
<li>模型性能在多个3D场景理解、推理和定位任务上优于现有方法。</li>
<li>提供了模型的源代码，可供公众访问和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00513">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-435a00e774f1b78b5e4b1a35151bc07b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-013e502dbdde407d4c199e6edd2e484b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08b1d0d8ad78774e002c26662a40445e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-020ecc80c8721387c2ad327205b752ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f824df1622d27712f8489f755359a28f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4001a43fec328517b4489305b6e11fd3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Assessing-Correctness-in-LLM-Based-Code-Generation-via-Uncertainty-Estimation"><a href="#Assessing-Correctness-in-LLM-Based-Code-Generation-via-Uncertainty-Estimation" class="headerlink" title="Assessing Correctness in LLM-Based Code Generation via Uncertainty   Estimation"></a>Assessing Correctness in LLM-Based Code Generation via Uncertainty   Estimation</h2><p><strong>Authors:Arindam Sharma, Cristina David</strong></p>
<p>In this work, we explore uncertainty estimation as a proxy for correctness in LLM-generated code. To this end, we adapt two state-of-the-art techniques from natural language generation – one based on entropy and another on mutual information – to the domain of code generation. Given the distinct semantic properties of code, we introduce modifications, including a semantic equivalence check based on symbolic execution. Our findings indicate a strong correlation between the uncertainty computed through these techniques and correctness, highlighting the potential of uncertainty estimation for quality assessment. Additionally, we propose a simplified version of the entropy-based method that assumes a uniform distribution over the LLM’s responses, demonstrating comparable effectiveness. Using these techniques, we develop an abstention policy that prevents the model from making predictions when uncertainty is high, reducing incorrect outputs to near zero. Our evaluation on the LiveCodeBench shows that our approach significantly outperforms a baseline relying solely on LLM-reported log-probabilities. </p>
<blockquote>
<p>在这项工作中，我们探索将不确定性估计作为评估大型语言模型（LLM）生成代码正确性的代理指标。为此，我们从自然语言生成领域采用了两种先进技术——一种基于熵，另一种基于互信息——并将其应用于代码生成领域。考虑到代码的独特语义属性，我们进行了一些修改，包括基于符号执行的语义等价性检查。我们的研究发现，通过这些技术计算的不确定性与正确性之间存在强烈的相关性，突出了不确定性估计在质量评估中的潜力。此外，我们还提出了基于熵的方法的简化版，该方法假设大型语言模型的响应服从均匀分布，并展示了其相当的有效性。使用这些技术，我们制定了一项放弃政策，即当不确定性很高时，防止模型做出预测，从而将错误输出降低到几乎为零。我们在LiveCodeBench上的评估表明，我们的方法显著优于仅依赖大型语言模型报告的日志概率的基线方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11620v2">PDF</a> 18 pages and 3 References Pages</p>
<p><strong>摘要</strong></p>
<p>本文探索了不确定性估计作为评估LLM生成代码正确性的代理指标。为此，我们采用了自然语言生成领域的两种最新技术——基于熵的方法和基于互信息的方法，并将其适应于代码生成领域。考虑到代码的独特语义属性，我们引入了一些修改，包括基于符号执行的语义等价性检查。我们的研究发现，通过这些技术计算的不确定性与正确性之间存在强烈的相关性，突显了不确定性估计在质量评估中的潜力。此外，我们还提出了一种简化版的基于熵的方法，该方法假设LLM的响应呈均匀分布，并展示了其相当的有效性。利用这些技术，我们制定了一种避免策略，即当不确定性较高时，防止模型进行预测，从而将错误输出降低到几乎为零。我们在LiveCodeBench上的评估显示，我们的方法显著优于仅依赖LLM报告的对数概率的基线方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文章探索了不确定性估计作为评估LLM生成代码正确性的方法。</li>
<li>文章采用了自然语言生成领域的两种技术，并适应于代码生成领域。</li>
<li>研究发现不确定性估计与代码正确性之间存在强烈的相关性。</li>
<li>文章提出了一种简化版的基于熵的不确定性估计方法，并验证了其有效性。</li>
<li>文章制定了一种避免策略，能够在不确定性较高时防止模型进行预测，显著减少错误输出。</li>
<li>评估显示，该方法显著优于仅依赖LLM报告的对数概率的基线方法。</li>
<li>该研究为LLM生成的代码质量评估提供了新的视角和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11620">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e2863ca31b6a6d311590fac49d1be54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a87af8794d55ab302c70eb96a1576a72.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f5ce8c8a9c2603d8212970bfe805358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37c83c727c6209861532f83de48f976c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CDS-Data-Synthesis-Method-Guided-by-Cognitive-Diagnosis-Theory"><a href="#CDS-Data-Synthesis-Method-Guided-by-Cognitive-Diagnosis-Theory" class="headerlink" title="CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory"></a>CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory</h2><p><strong>Authors:Haokun Zhao, Jinyi Han, Jiaqing Liang, Yanghua Xiao</strong></p>
<p>Large Language Models (LLMs) have achieved significant advancements, but the increasing complexity of tasks and higher performance demands highlight the need for continuous improvement. Some approaches utilize synthetic data generated by advanced LLMs based on evaluation results to train models. However, conventional evaluation methods fail to provide detailed, fine-grained profiles of LLMs, limiting their guidance for data synthesis. In this paper, we introduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a diagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine evaluation results and characterize model profiles at the knowledge component level. Based on these diagnostics, we propose two diagnosis-synthesis strategies for weakness-targeted data synthesis. Additionally, we present an enhanced data augmentation and selection pipeline to improve the quality and diversity of synthesized data. Our experiments with several open-source models show significant improvements across multiple benchmarks, achieving up to 6.00% improvement in code generation, 13.10% in mathematical reasoning, and 5.43% in academic exams. Code and data are available on GitHub. </p>
<blockquote>
<p>大型语言模型（LLM）已经取得了显著的进步，但任务的复杂性和对性能要求的提高突显了持续改进的必要性。一些方法利用基于评估结果由先进的大型语言模型生成合成数据来训练模型。然而，传统的评估方法无法提供大型语言模型的详细、精细的概况，从而限制了它们在数据合成方面的指导。在本文中，我们介绍了认知诊断合成（CDS）方法，该方法结合了受认知诊断理论（CDT）启发的诊断过程，以优化评估结果并在知识组件级别刻画模型概况。基于这些诊断，我们提出了两种针对弱点进行的数据合成的诊断合成策略。此外，我们还展示了一个增强的数据增强和选择流程，以提高合成数据的质量和多样性。我们在多个开源模型上的实验显示，在各种基准测试中实现了显著的改进，在代码生成方面提高了6.00%，在数学推理方面提高了13.10%，在学术考试方面提高了5.43%。代码和数据已在GitHub上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07674v2">PDF</a> </p>
<p><strong>Summary</strong><br>大语言模型（LLM）面临持续提高性能的需求，部分方法通过合成数据来训练模型。然而，传统评估方法无法提供详细的模型性能分析，限制了数据合成的指导。本文提出了结合认知诊断理论（CDT）的Cognitive Diagnostic Synthesis（CDS）方法，以精细化评估结果并刻画模型在知识组件层面的性能特征。基于这些诊断结果，本文提出了两种针对弱点合成数据的策略。同时，改进了数据增强和选择流程，提高了合成数据的质量和多样性。实验结果显示，该方法在多个基准测试中取得了显著改进，如代码生成提高了6.0%，数学推理提高了13.1%，学术考试提高了5.4%。GitHub上有相关代码和数据可供下载。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM面临性能持续提升的需求和挑战。</li>
<li>传统评估方法无法提供详细的模型性能分析。</li>
<li>Cognitive Diagnostic Synthesis (CDS)方法结合认知诊断理论（CDT）来精细化评估结果并刻画模型性能特征。</li>
<li>基于诊断结果，提出了两种针对弱点合成数据的策略。</li>
<li>改进了数据增强和选择流程，提高合成数据的质量和多样性。</li>
<li>实验结果显示该方法在多个基准测试中表现优异。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07674">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-25149d28bde78b83150caf530b91e937.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c4c5903ca3e8b1a6730b78113396162.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d20a1b1a1b08aae324c0266a0e91df44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f60472643003fb490622efddba82d0be.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models"><a href="#Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models" class="headerlink" title="Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models"></a>Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models</h2><p><strong>Authors:Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu</strong></p>
<p>Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in <a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>. </p>
<blockquote>
<p>大型语言模型（LLM）已广泛应用于各种自然语言处理任务，如问答和机器翻译。然而，由于缺乏标注数据和生化属性手动注释的困难，分子生成任务的性能仍然有限，特别是涉及多属性约束的任务。在这项工作中，我们提出了一个两阶段的PEIT（属性增强指令调整）框架，以提高LLM在分子相关任务上的性能。第一步中，我们使用文本描述、SMILES和生物化学属性作为多模式输入，通过对齐多模式表示来合成指令数据，从而预训练一个名为PEIT-GEN的模型。在第二步中，我们使用合成数据对现有的开源LLM进行微调，得到的PEIT-LLM可以处理分子描述、基于文本的分子生成、分子属性预测以及我们新提出的多约束分子生成任务。实验结果表明，我们的预训练PEIT-GEN在分子描述方面超越了MolT5和BioT5。这证明了文本描述、结构和生物化学属性之间的模态对齐良好。此外，PEIT-LLM在多任务分子生成方面显示出有希望的改进，证明了PEIT框架对各种分子任务的可扩展性。我们在<a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>上发布了代码、构建好的指令数据和模型检查点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18084v2">PDF</a> </p>
<p><strong>Summary</strong>：LLM在自然语言处理任务中得到广泛应用，但针对分子生成任务的性能仍受限于缺乏标签数据和手动注释的难度。本文提出一个两阶段的PEIT框架，通过预训练和微调，提高LLM在分子相关任务上的性能。实验结果表明，预训练的PEIT-GEN在分子描述上优于MolT5和BioT5模型，并且PEIT框架对于多种分子任务表现出可扩展性。PEIT已在GitHub上发布。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLM在多种NLP任务中应用广泛，但在分子生成任务上的性能受到限制。</li>
<li>PEIT框架包含预训练和微调两个阶段，旨在提高LLM在分子相关任务上的性能。</li>
<li>PEIT使用文本描述、SMILES和生物化学属性等多模式输入进行预训练。</li>
<li>PEIT-GEN的合成数据用于微调现有开源LLM。</li>
<li>PEIT-LLM能够处理分子描述、文本基础的分子生成、分子属性预测以及新提出的多约束分子生成任务。</li>
<li>实验结果表明，PEIT-GEN在分子描述上优于MolT5和BioT5模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18084">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e350f9c443a225542daab311015a59e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-431301f75dfab204d4322d959489b384.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c6e299ad16040444d5c2bb0c92c3a51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb2d9703cb52ff46f3f1b18e5846c929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70ee1e58c3186455090ca950fb68190f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PyGen-A-Collaborative-Human-AI-Approach-to-Python-Package-Creation"><a href="#PyGen-A-Collaborative-Human-AI-Approach-to-Python-Package-Creation" class="headerlink" title="PyGen: A Collaborative Human-AI Approach to Python Package Creation"></a>PyGen: A Collaborative Human-AI Approach to Python Package Creation</h2><p><strong>Authors:Saikat Barua, Mostafizur Rahman, Md Jafor Sadek, Rafiul Islam, Shehnaz Khaled, Md. Shohrab Hossain</strong></p>
<p>The principles of automation and innovation serve as foundational elements for advancement in contemporary science and technology. Here, we introduce Pygen, an automation platform designed to empower researchers, technologists, and hobbyists to bring abstract ideas to life as core, usable software tools written in Python. Pygen leverages the immense power of autoregressive large language models to augment human creativity during the ideation, iteration, and innovation process. By combining state-of-the-art language models with open-source code generation technologies, Pygen has significantly reduced the manual overhead of tool development. From a user prompt, Pygen automatically generates Python packages for a complete workflow from concept to package generation and documentation. The findings of our work show that Pygen considerably enhances the researcher’s productivity by enabling the creation of resilient, modular, and well-documented packages for various specialized purposes. We employ a prompt enhancement approach to distill the user’s package description into increasingly specific and actionable. While being inherently an open-ended task, we have evaluated the generated packages and the documentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with detailed results in the results section. Furthermore, we documented our results, analyzed the limitations, and suggested strategies to alleviate them. Pygen is our vision of ethical automation, a framework that promotes inclusivity, accessibility, and collaborative development. This project marks the beginning of a large-scale effort towards creating tools where intelligent agents collaborate with humans to improve scientific and technological development substantially.   Our code and generated examples are open-sourced at [<a target="_blank" rel="noopener" href="https://github.com/GitsSaikat/Pygen]">https://github.com/GitsSaikat/Pygen]</a> </p>
<blockquote>
<p>自动化和创新的原则是当代科学技术进步的基础要素。在这里，我们介绍Pygen——一个自动化平台，旨在赋能研究者、技术专家和爱好者，将抽象的想法转化为实用的Python软件工具。Pygen利用自动回归大型语言模型的巨大力量，在创意、迭代和创新过程中增强人类的创造力。通过将最先进的语言模型与开源代码生成技术相结合，Pygen大大降低了工具开发的手动工作量。从用户提示开始，Pygen自动为从概念到软件包生成和文档的整个工作流程生成Python包。我们的研究结果表明，Pygen通过使研究人员能够创建针对各种特定用途的健壮、模块化且文档齐全的包，从而极大地提高了研究人员的工作效率。我们采用提示增强法对用户描述的包进行提炼，使其更加具体可行。虽然本质上这是一个开放的任务，但我们通过人工评估、基于大型语言模型的评估和CodeBLEU对生成的软件包和文档进行了评估，详细结果见结果部分。此外，我们还记录了我们的结果，分析了局限性，并提出了缓解策略。Pygen是我们对道德自动化的愿景，是一个促进包容性、可访问性和协作发展的框架。这个项目标志着朝着创建智能主体与人类合作改善科学和科技发展的大规模努力迈出了重要的一步。我们的代码和生成的示例在<a target="_blank" rel="noopener" href="https://github.com/GitsSaikat/Pygen%E4%B8%8A%E5%BC%BA%E6%BA%90%E3%80%82">https://github.com/GitsSaikat/Pygen上开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08932v2">PDF</a> 33 pages, 13 figures</p>
<p><strong>Summary</strong><br>Pygen是一个自动化平台，利用先进的自然语言模型技术，助力研究人员和技术爱好者将抽象想法转化为实用的Python软件工具。该平台通过减少工具开发的手动工作量，显著提高了研究人员的生产力。Pygen能自动从用户提示生成Python包，并提供从概念到包生成和文档完整的自动化工作流程。其注重开放性、包容性和协作性，致力于促进科学和技术发展。目前已在GitHub上开源部分代码和生成的实例。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Pygen是一个自动化平台，结合了自然语言模型与开源代码生成技术。</li>
<li>Pygen可将抽象想法转化为Python软件工具，降低工具开发的手动工作量。</li>
<li>平台能自动从用户提示生成Python包，并提供完整自动化工作流程。</li>
<li>Pygen强调开放性、包容性和协作性，推动科学和技术发展。</li>
<li>通过人类评估、LLM评估和CodeBLEU评估验证了Pygen的效果。</li>
<li>Pygen部分代码和生成的实例已在GitHub上开源。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08932">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4ba9402ab60774a0640145229784c365.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b65a61cc3b7b2801e9416a2bc412077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10ac885127120172d8350314e3da30c8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6ad0b951fd187b846168927271f5311d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-07  MAS-GPT Training LLMs to Build LLM-based Multi-Agent Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-81423a77a41ec09154418bba6ec1ef7c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-07  PacketCLIP Multi-Modal Embedding of Network Traffic and Language for   Cybersecurity Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">20064.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
