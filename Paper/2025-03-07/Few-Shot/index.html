<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-07  MIAdapt Source-free Few-shot Domain Adaptive Object Detection for   Microscopic Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-58276572fd7ef9418bb2a247dae84426.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-07-更新"><a href="#2025-03-07-更新" class="headerlink" title="2025-03-07 更新"></a>2025-03-07 更新</h1><h2 id="MIAdapt-Source-free-Few-shot-Domain-Adaptive-Object-Detection-for-Microscopic-Images"><a href="#MIAdapt-Source-free-Few-shot-Domain-Adaptive-Object-Detection-for-Microscopic-Images" class="headerlink" title="MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for   Microscopic Images"></a>MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for   Microscopic Images</h2><p><strong>Authors:Nimra Dilawar, Sara Nadeem, Javed Iqbal, Waqas Sultani, Mohsen Ali</strong></p>
<p>Existing generic unsupervised domain adaptation approaches require access to both a large labeled source dataset and a sufficient unlabeled target dataset during adaptation. However, collecting a large dataset, even if unlabeled, is a challenging and expensive endeavor, especially in medical imaging. In addition, constraints such as privacy issues can result in cases where source data is unavailable. Taking in consideration these challenges, we propose MIAdapt, an adaptive approach for Microscopic Imagery Adaptation as a solution for Source-free Few-shot Domain Adaptive Object detection (SF-FSDA). We also define two competitive baselines (1) Faster-FreeShot and (2) MT-FreeShot. Extensive experiments on the challenging M5-Malaria and Raabin-WBC datasets validate the effectiveness of MIAdapt. Without using any image from the source domain MIAdapt surpasses state-of-the-art source-free UDA (SF-UDA) methods by +21.3% mAP and few-shot domain adaptation (FSDA) approaches by +4.7% mAP on Raabin-WBC. Our code and models will be publicly available. </p>
<blockquote>
<p>现有的通用无监督域自适应方法要求在适应过程中访问大量标记的源数据集和足够的未标记的目标数据集。然而，收集大数据集，即使是未标记的，也是一项具有挑战性和昂贵的工作，特别是在医学影像领域。此外，隐私等问题可能导致源数据无法使用。考虑到这些挑战，我们提出了MIAdapt，这是一种用于显微图像自适应的适应方法，作为源自由少样本域自适应目标检测（SF-FSDA）的解决方案。我们还定义了两个竞争基线，（1）Faster-FreeShot和（2）MT-FreeShot。在具有挑战性的M5-Malaria和Raabin-WBC数据集上的大量实验验证了MIAdapt的有效性。MIAdapt在不使用源域任何图像的情况下，超越了最新的源自由UDA（SF-UDA）方法，在Raabin-WBC上的mAP提高了+21.3%，并且相对于少样本域自适应（FSDA）方法的mAP提高了+4.7%。我们的代码和模型将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03370v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了MIAdapt方法，这是一种针对显微镜图像自适应的源免费少镜头域自适应目标检测（SF-FSDA）的适应性方法。考虑到收集大量数据集（即使是未标记的）的挑战性和昂贵性，特别是在医学影像中，作者提出了MIAdapt以及两个竞争基线方法Faster-FreeShot和MT-FreeShot。在具有挑战性的M5-Malaria和Raabin-WBC数据集上的广泛实验验证了MIAdapt的有效性。在不使用源域图像的情况下，MIAdapt在Raabin-WBC数据集上的mAP比最先进的源自由UDA（SF-UDA）方法和少镜头域自适应（FSDA）方法分别高出+21.3%和+4.7%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIAdapt是一种针对显微镜图像自适应的源免费少镜头域自适应目标检测方法。</li>
<li>现有通用无监督域自适应方法需要大量标记源数据集和足够的未标记目标数据集，但数据收集具有挑战性和昂贵性。</li>
<li>MIAdapt在M5-Malaria和Raabin-WBC数据集上的实验验证了其有效性。</li>
<li>MIAdapt在未使用源域图像的情况下表现优异，相较于其他方法有明显的性能提升。</li>
<li>作者提出了两个竞争基线方法Faster-FreeShot和MT-FreeShot。</li>
<li>MIAdapt将公开可用的代码和模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03370">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5a350a73e924dfb5455dff3634a43c45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15bae6af356674b32e943cf399db7933.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36c11912e3ba825a78e7f84a4e48e2c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58276572fd7ef9418bb2a247dae84426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28e881fce8314b446a3cb04432c63a0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b477433883d23a14325a3ee3fd471e9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f5f5852f7e12fab744a57b670e0630d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HoT-Highlighted-Chain-of-Thought-for-Referencing-Supporting-Facts-from-Inputs"><a href="#HoT-Highlighted-Chain-of-Thought-for-Referencing-Supporting-Facts-from-Inputs" class="headerlink" title="HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs"></a>HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs</h2><p><strong>Authors:Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen</strong></p>
<p>An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct. </p>
<blockquote>
<p>大型语言模型（LLM）的一个弱点是它们倾向于产生非事实陈述。由事实和并非事实组成的信息所组成的回答给人类带来了验证以及准确做出决定方面的挑战。为了解决这一问题，我们提出了“突出思考链提示”（HoT）技术，这是一种提示LLM生成带有XML标签的响应的技术，这些标签将事实依据与查询中提供的事实相结合。也就是说，给定一个输入问题，LLM会首先重新格式化问题，添加突出关键事实的XML标签，然后生成一个带有输入引用的亮点的响应。有趣的是，在少量样本的情况下，HoT在算术、阅读理解到逻辑推理等广泛的17项任务上的表现优于普通思考链提示（CoT）。当要求人类验证LLM的响应时，亮点有助于时间有限的参与者更准确、高效地识别LLM是否正确。然而，令人惊讶的是，当LLM错误时，HoT往往使用户认为答案是正确的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02003v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的一个弱点是它们倾向于产生非事实性的陈述，这使得它们的回应难以验证并准确作为决策依据。为解决这一问题，我们提出了高亮化思维链提示（HoT）技术，该技术通过XML标签为LLM的回应提供事实依据。在少量样本的情况下，HoT在算术、阅读理解到逻辑推理的17项任务上的表现优于基础思维链提示（CoT）。对人类参与者的验证显示，高亮有助于他们在有限时间内更准确地识别LLM的正确性，但有趣的是，当LLM错误时，HoT往往会使用户误以为答案是正确的。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）存在生成非事实性陈述的问题。</li>
<li>Highlighted Chain-of-Thought Prompting（HoT）技术通过XML标签为LLM的回应提供事实依据。</li>
<li>在少量样本的情况下，HoT在多种任务上的表现优于基础思维链提示（CoT）。</li>
<li>高亮有助于人类参与者更准确地识别LLM的正确性。</li>
<li>当LLM错误时，HoT技术存在误导用户的可能。</li>
<li>HoT有助于提升人类验证LLM回应时的效率和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02003">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-25005df8ffe12bfbb16bac1cb2a6b9d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cfcafada0c2a49fc221f1b52a1c2ff2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dba5a47ecc8d2d91ac10a67354f1552.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c33430d017f31e144895205bd76d8dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2abd555d82805b0a8a39aed90a1a1284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-596c5aa2e0a70fece1ad798fa9b7c3b2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RIDE-Enhancing-Large-Language-Model-Alignment-through-Restyled-In-Context-Learning-Demonstration-Exemplars"><a href="#RIDE-Enhancing-Large-Language-Model-Alignment-through-Restyled-In-Context-Learning-Demonstration-Exemplars" class="headerlink" title="RIDE: Enhancing Large Language Model Alignment through Restyled   In-Context Learning Demonstration Exemplars"></a>RIDE: Enhancing Large Language Model Alignment through Restyled   In-Context Learning Demonstration Exemplars</h2><p><strong>Authors:Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari</strong></p>
<p>Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment–factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at <a target="_blank" rel="noopener" href="https://github.com/AnonymousCode-ComputerScience/RIDE">https://github.com/AnonymousCode-ComputerScience/RIDE</a>. </p>
<blockquote>
<p>对齐调整对于确保大型语言模型（LLM）以伦理和有帮助的方式行为至关重要。当前的对齐方法需要高质量标注和大量训练资源。本文提出了一种低成本、无需调整的方法，使用上下文学习（ICL）增强LLM的对齐。通过对高质量ICL演示的分析，我们发现风格是影响LLM对齐能力的关键因素，并基于此风格框架明确地重新设计了ICL范例。此外，我们将重新设计的演示结合起来，在LLM对齐的两个相互矛盾的方面——真实性和安全性之间取得了平衡。我们将重新设计的示例打包为提示，以触发少量学习，提高LLM的对齐能力。与最佳基线方法相比（以平均得分5.00为最高），我们的方法在Alpaca任务上最高提高了0.10（从4.50提高到4.60），在Just-eval基准测试上提高了0.22（从4.34提高到4.56），在MT-Bench数据集上最高提高了0.32（从3.53提高到3.85）。我们在<a target="_blank" rel="noopener" href="https://github.com/AnonymousCode-ComputerScience/RIDE">https://github.com/AnonymousCode-ComputerScience/RIDE</a>发布了代码和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11681v4">PDF</a> 38 pages, 2 figures, 20 tables; The paper is under review in ARR</p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种低成本的调参自由方法，利用上下文学习（ICL）增强大型语言模型（LLM）的对齐性。研究通过高质量ICL演示分析，确认风格是影响LLM对齐能力的关键因素，并据此重新设计ICL范例。结合重新设计的演示样本，在LLM对齐的两个方面——真实性和安全性之间取得平衡。将重新设计的示例作为提示触发少样本学习，提高LLM的对齐性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种低成本的、无需调参的方法，通过上下文学习（ICL）增强大型语言模型（LLM）的伦理和有用性对齐。</li>
<li>风格是影响LLM对齐能力的关键因素，研究通过高质量ICL演示分析确认了这一点。</li>
<li>重新设计的ICL范例结合了真实性和安全性，实现了LLM对齐的两个方面之间的平衡。</li>
<li>通过将重新设计的示例作为提示触发少样本学习，提高了LLM的对齐性能。</li>
<li>与最佳基线方法相比，该方法在Alpaca任务、Just-eval基准测试和MT-Bench数据集上均实现了性能提升。</li>
<li>研究成果已发布在<a target="_blank" rel="noopener" href="https://github.com/AnonymousCode-ComputerScience/RIDE%E4%B8%8A%EF%BC%8C%E5%8C%85%E6%8B%AC%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E%E6%8D%AE%E3%80%82">https://github.com/AnonymousCode-ComputerScience/RIDE上，包括代码和数据。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0dad3f6e5f61d133cfed60779a969169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d58d7928170397366cbf8638b88f47a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f156d089c7759aa95d148939c38fa8ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab40690d2dffabbf7fb8b5eb38abf2c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-800b7715e2257f5db9d7e3af024cf51a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CLIP-RT-Learning-Language-Conditioned-Robotic-Policies-from-Natural-Language-Supervision"><a href="#CLIP-RT-Learning-Language-Conditioned-Robotic-Policies-from-Natural-Language-Supervision" class="headerlink" title="CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural   Language Supervision"></a>CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural   Language Supervision</h2><p><strong>Authors:Gi-Cheon Kang, Junghyun Kim, Kyuhwan Shim, Jun Ki Lee, Byoung-Tak Zhang</strong></p>
<p>Teaching robots desired skills in real-world environments remains challenging, especially for non-experts. A key bottleneck is that collecting robotic data often requires expertise or specialized hardware, limiting accessibility and scalability. We posit that natural language offers an intuitive and accessible interface for robot learning. To this end, we study two aspects: (1) enabling non-experts to collect robotic data through natural language supervision (e.g., “move the arm to the right”) and (2) learning robotic policies directly from this supervision. Specifically, we introduce a data collection framework that collects robot demonstrations based on natural language supervision and further augments these demonstrations. We then present CLIP-RT, a vision-language-action (VLA) model that learns language-conditioned visuomotor policies from this supervision. CLIP-RT adapts the pretrained CLIP models and learns to predict language-based motion primitives via contrastive imitation learning. We train CLIP-RT on the Open X-Embodiment dataset and finetune it on in-domain data collected by our framework to learn diverse skills. CLIP-RT demonstrates strong capabilities in learning novel manipulation skills, outperforming the state-of-the-art model, OpenVLA (7B parameters), by 24% in average success rates, while using 7x fewer parameters (1B). We further observe that CLIP-RT shows significant improvements in few-shot generalization. Finally, through collaboration with humans or large pretrained models, we demonstrate that CLIP-RT can further improve its generalization on challenging robotic tasks. </p>
<blockquote>
<p>教授机器人在真实世界环境中所需的技能仍然是一个挑战，尤其对于非专业人士而言。一个关键的瓶颈是，收集机器人数据通常需要专业知识或专用硬件，这限制了可访问性和可扩展性。我们认为自然语言为机器人学习提供了一个直观和可访问的界面。为此，我们研究了两个方面：（1）通过自然语言监督（例如，“将手臂向右移动”）使非专家能够收集机器人数据；（2）直接从这种监督中学习机器人策略。具体来说，我们引入了一个数据收集框架，该框架基于自然语言监督来收集机器人演示，并进一步扩充了这些演示。然后，我们提出了CLIP-RT，这是一种视觉语言动作（VLA）模型，可以从这种监督中学习语言调节的视听觉运动策略。CLIP-RT适应了预训练的CLIP模型，并通过对比模仿学习学会基于语言的运动原始预测。我们在Open X-Embodiment数据集上训练CLIP-RT，并通过我们的框架收集的领域内数据进行微调，以学习各种技能。CLIP-RT在学习新颖操作技能方面表现出强大的能力，在平均成功率方面超越了最先进的模型OpenVLA（7B参数）24%，同时使用参数更少（仅1B）。我们还观察到CLIP-RT在少量镜头概括方面显示出显着改进。最后，通过与人类或大型预训练模型的合作，我们证明CLIP-RT可以在具有挑战性的机器人任务上进一步改善其概括能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00508v3">PDF</a> 27 pages</p>
<p><strong>Summary</strong></p>
<p>本文探讨了在真实环境中教授机器人技能所面临的挑战，特别是非专家用户的挑战。文章提出了一种基于自然语言监督的数据收集框架，以及一个直接从这种监督中学习机器人策略的方法。为此引入了CLIP-RT模型，该模型可以在少量数据上实现高性能的技能学习，并且在少样本泛化方面表现出显著的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人教学在非专家用户中面临挑战，主要瓶颈在于收集机器人数据需要专业知识或特殊硬件。</li>
<li>自然语言为机器人学习提供了一个直观和可访问的接口。</li>
<li>引入了一个基于自然语言监督的机器人数据收集框架。</li>
<li>介绍了CLIP-RT模型，该模型能够从自然语言监督中学习语言条件视觉运动策略。</li>
<li>CLIP-RT模型在Open X-Embodiment数据集上训练，并在框架收集的领域数据上进行微调，以学习各种技能。</li>
<li>CLIP-RT在学习新操作技能方面表现出强大的能力，相较于最新模型OpenVLA，平均成功率提高了24%，同时使用参数更少（仅使用1B参数）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-10bfa61ef6e8ffca5a73ca86151ab44d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a21adca9d61c01f4380fe3757fe22aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6c4a01a9a3229773768c855c2ff585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9046c8b32401ec73af7cfc6e844db0a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d4b8dd515753f1c74cc637149f1c4a4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Adapting-Pre-Trained-Vision-Models-for-Novel-Instance-Detection-and-Segmentation"><a href="#Adapting-Pre-Trained-Vision-Models-for-Novel-Instance-Detection-and-Segmentation" class="headerlink" title="Adapting Pre-Trained Vision Models for Novel Instance Detection and   Segmentation"></a>Adapting Pre-Trained Vision Models for Novel Instance Detection and   Segmentation</h2><p><strong>Authors:Yangxiao Lu, Jishnu Jaykumar P, Yunhui Guo, Nicholas Ruozzi, Yu Xiang</strong></p>
<p>Novel Instance Detection and Segmentation (NIDS) aims at detecting and segmenting novel object instances given a few examples of each instance. We propose a unified, simple, yet effective framework (NIDS-Net) comprising object proposal generation, embedding creation for both instance templates and proposal regions, and embedding matching for instance label assignment. Leveraging recent advancements in large vision methods, we utilize Grounding DINO and Segment Anything Model (SAM) to obtain object proposals with accurate bounding boxes and masks. Central to our approach is the generation of high-quality instance embeddings. We utilized foreground feature averages of patch embeddings from the DINOv2 ViT backbone, followed by refinement through a weight adapter mechanism that we introduce.   We show experimentally that our weight adapter can adjust the embeddings locally within their feature space and effectively limit overfitting in the few-shot setting. Furthermore, the weight adapter optimizes weights to enhance the distinctiveness of instance embeddings during similarity computation. This methodology enables a straightforward matching strategy that results in significant performance gains. Our framework surpasses current state-of-the-art methods, demonstrating notable improvements in four detection datasets. In the segmentation tasks on seven core datasets of the BOP challenge, our method outperforms the leading published RGB methods and remains competitive with the best RGB-D method. We have also verified our method using real-world images from a Fetch robot and a RealSense camera. Project Page: <a target="_blank" rel="noopener" href="https://irvlutd.github.io/NIDSNet/">https://irvlutd.github.io/NIDSNet/</a> </p>
<blockquote>
<p>新型实例检测与分割（NIDS）旨在针对每个实例的少数几个样本进行新型对象实例的检测和分割。我们提出了一个统一、简单、有效的框架（NIDS-Net），它包括对象提案生成、为实例模板和提案区域创建嵌入，以及用于实例标签分配的嵌入匹配。我们利用最新的大规模视觉方法进展，使用接地DINO和分割任何模型（SAM）获得具有准确边界框和蒙版的对象提案。我们的方法的核心是生成高质量实例嵌入。我们使用DINOv2 ViT骨干网中补丁嵌入的前景特征平均值，然后通过我们引入的重量适配器机制进行改进。我们通过实验证明，我们的重量适配器可以在其特征空间内局部调整嵌入，并在小样本设置中有效限制过拟合。此外，重量适配器优化权重，以提高实例嵌入在相似度计算期间的区分性。这种方法使匹配策略更加简单明了，从而实现了显著的性能提升。我们的框架超越了当前最先进的方法，在四个检测数据集上取得了显著的改进。在BOP挑战的七个核心数据集上的分割任务中，我们的方法优于领先的已发布RGB方法，并与最佳的RGB-D方法保持竞争力。我们还使用Fetch机器人和RealSense相机拍摄的真实世界图像验证了我们的方法。项目页面：<a target="_blank" rel="noopener" href="https://irvlutd.github.io/NIDSnet/">https://irvlutd.github.io/NIDSnet/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17859v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://irvlutd.github.io/NIDSNet/">https://irvlutd.github.io/NIDSNet/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对Novel Instance Detection and Segmentation（NIDS）任务的新框架NIDS-Net。该框架通过生成对象提案、创建实例模板和提案区域嵌入，以及嵌入匹配进行实例标签分配。利用最近的视觉方法进展，如Grounding DINO和Segment Anything Model (SAM)，获得带有精确边界框和遮罩的对象提案。核心是高质量的实例嵌入生成，通过使用DINOv2 ViT后端的补丁嵌入的前景色特征平均值，并通过新引入的重量适配器机制进行改进。实验表明，重量适配器可以在特征空间内局部调整嵌入，有效限制小样本设置中的过拟合。此外，重量适配器优化权重，以提高实例嵌入在相似性计算中的独特性。此方法简化了匹配策略，带来了显著的性能提升。NIDS-Net在四个检测数据集上的性能超过了当前的最先进方法，在BOP挑战的七个核心数据集上的分割任务中也表现出色。此外，该方法还通过Fetch机器人和RealSense相机的真实世界图像进行了验证。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NIDS-Net是一个针对Novel Instance Detection and Segmentation任务的统一、简单而有效的框架。</li>
<li>框架包括对象提案生成、实例模板和提案区域嵌入创建，以及嵌入匹配进行实例标签分配。</li>
<li>利用了最近的视觉方法进展，如Grounding DINO和Segment Anything Model (SAM)。</li>
<li>高质量的实例嵌入生成是核心，通过DINOv2 ViT后端的补丁嵌入的前景色特征平均值生成，通过重量适配器机制改进。</li>
<li>重量适配器能够在特征空间内局部调整嵌入，有效限制小样本过拟合，优化权重提高实例嵌入的独特性。</li>
<li>此方法简化了匹配策略，显著提高了性能，在多个数据集上的表现超过了当前最先进的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17859">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e8f640f79ca7c673c7fbd01b83864b37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11ab40ff290ddfdb7613566cc75cb705.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64542b83a405e3bbc9c0b198e8620c37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60b77b2894ec5ce9a3f3dcfbc631075e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a9f655b657ccede111bbcf8a16f04e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1c24e6f8780c8d5aea8c30b63671db2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa1e7c36fd641d3e9a3ef41dade59b6d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MVP-Shot-Multi-Velocity-Progressive-Alignment-Framework-for-Few-Shot-Action-Recognition"><a href="#MVP-Shot-Multi-Velocity-Progressive-Alignment-Framework-for-Few-Shot-Action-Recognition" class="headerlink" title="MVP-Shot: Multi-Velocity Progressive-Alignment Framework for Few-Shot   Action Recognition"></a>MVP-Shot: Multi-Velocity Progressive-Alignment Framework for Few-Shot   Action Recognition</h2><p><strong>Authors:Hongyu Qu, Rui Yan, Xiangbo Shu, Hailiang Gao, Peng Huang, Guo-Sen Xie</strong></p>
<p>Recent few-shot action recognition (FSAR) methods typically perform semantic matching on learned discriminative features to achieve promising performance. However, most FSAR methods focus on single-scale (e.g., frame-level, segment-level, etc) feature alignment, which ignores that human actions with the same semantic may appear at different velocities. To this end, we develop a novel Multi-Velocity Progressive-alignment (MVP-Shot) framework to progressively learn and align semantic-related action features at multi-velocity levels. Concretely, a Multi-Velocity Feature Alignment (MVFA) module is designed to measure the similarity between features from support and query videos with different velocity scales and then merge all similarity scores in a residual fashion. To avoid the multiple velocity features deviating from the underlying motion semantic, our proposed Progressive Semantic-Tailored Interaction (PSTI) module injects velocity-tailored text information into the video feature via feature interaction on channel and temporal domains at different velocities. The above two modules compensate for each other to make more accurate query sample predictions under the few-shot settings. Experimental results show our method outperforms current state-of-the-art methods on multiple standard few-shot benchmarks (i.e., HMDB51, UCF101, Kinetics, and SSv2-small). </p>
<blockquote>
<p>近期的few-shot动作识别（FSAR）方法通常通过对学习到的判别特征进行语义匹配，以取得有前景的性能。然而，大多数FSAR方法侧重于单一尺度的特征对齐（例如帧级别、片段级别等），忽略了相同语义的人类动作可能以不同速度出现。为此，我们开发了一种新的多速度渐进对齐（MVP-Shot）框架，以渐进的方式学习和对齐多速度级别下的语义相关动作特征。具体地，设计了一个多速度特征对齐（MVFA）模块，该模块旨在度量来自不同速度尺度的支持视频和查询视频特征之间的相似性，然后以残差的方式合并所有相似性得分。为了避免多个速度特征偏离底层运动语义，我们提出了一种新的渐进语义定制交互（PSTI）模块，该模块通过在不同速度的通道和时间域上执行特征交互，将速度定制文本信息注入视频特征。上述两个模块相互补充，在少样本设置下实现更准确的查询样本预测。实验结果表明，我们的方法在多个标准少样本基准测试（即HMDB51、UCF101、Kinetics和SSv2-small）上的性能优于当前最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.02077v4">PDF</a> Accepted to TMM 2025</p>
<p><strong>Summary</strong></p>
<p>近期针对少样本动作识别（FSAR）的方法主要通过对学习到的判别特征进行语义匹配来实现较好的性能。然而，大多数FSAR方法仅关注单一尺度的特征对齐，忽略了相同语义的人类动作可能以不同速度出现。为此，我们提出了一个多速度渐进对齐（MVP-Shot）框架，以在多速度级别上渐进学习和对齐与语义相关的动作特征。具体来说，设计了一个多速度特征对齐（MVFA）模块，该模块可以测量来自支持视频和查询视频的不同速度尺度特征之间的相似性，并以残差方式合并所有相似性得分。为避免多速度特征偏离底层运动语义，我们提出了渐进语义定制交互（PSTI）模块，通过在不同速度下的通道和时间域上的特征交互，将定制的速度文本信息注入视频特征。以上两个模块相互补偿，在少样本设置下实现了更准确的查询样本预测。实验结果表明，我们的方法在多个标准少样本基准测试（如HMDB51、UCF101、Kinetics和SSv2-small）上的表现均优于当前最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSAR方法通过语义匹配在判别特征上实现良好性能。</li>
<li>现有FSAR方法主要关注单一尺度的特征对齐，忽略了动作速度多样性。</li>
<li>提出MVP-Shot框架，包含MVFA模块进行多速度特征对齐。</li>
<li>MVFA模块测量不同速度尺度下的特征相似性，并以残差方式合并相似性得分。</li>
<li>为避免多速度特征偏离底层语义，引入PSTI模块，将速度文本信息注入视频特征。</li>
<li>MVP-Shot框架在多个少样本动作识别基准测试上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.02077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8317c38c3f59f694632a0409c551d965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54053b3f46ce04901eb66bfbbe1a9538.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-083636178f392c2659fc183b4e72be8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dfb14c30ea67b11f53a60fe44873771.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a4d3eef0a4b6c03660e08d0eee2187c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-49f1d842eac04cb406c067daa85a7902.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-07  Active 6D Pose Estimation for Textureless Objects using Multi-View RGB   Frames
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-431301f75dfab204d4322d959489b384.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-03-07  Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">22963.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
