<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-07  DualDiff+ Dual-Branch Diffusion for High-Fidelity Video Generation with   Reward Guidance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    42 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-07-更新"><a href="#2025-03-07-更新" class="headerlink" title="2025-03-07 更新"></a>2025-03-07 更新</h1><h2 id="DualDiff-Dual-Branch-Diffusion-for-High-Fidelity-Video-Generation-with-Reward-Guidance"><a href="#DualDiff-Dual-Branch-Diffusion-for-High-Fidelity-Video-Generation-with-Reward-Guidance" class="headerlink" title="DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with   Reward Guidance"></a>DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with   Reward Guidance</h2><p><strong>Authors:Zhao Yang, Zezhong Qian, Xiaofan Li, Weixiang Xu, Gongpeng Zhao, Ruohong Yu, Lingsi Zhu, Longjun Liu</strong></p>
<p>Accurate and high-fidelity driving scene reconstruction demands the effective utilization of comprehensive scene information as conditional inputs. Existing methods predominantly rely on 3D bounding boxes and BEV road maps for foreground and background control, which fail to capture the full complexity of driving scenes and adequately integrate multimodal information. In this work, we present DualDiff, a dual-branch conditional diffusion model designed to enhance driving scene generation across multiple views and video sequences. Specifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional input, offering rich foreground and background semantics alongside 3D spatial geometry to precisely control the generation of both elements. To improve the synthesis of fine-grained foreground objects, particularly complex and distant ones, we propose a Foreground-Aware Mask (FGM) denoising loss function. Additionally, we develop the Semantic Fusion Attention (SFA) mechanism to dynamically prioritize relevant information and suppress noise, enabling more effective multimodal fusion. Finally, to ensure high-quality image-to-video generation, we introduce the Reward-Guided Diffusion (RGD) framework, which maintains global consistency and semantic coherence in generated videos. Extensive experiments demonstrate that DualDiff achieves state-of-the-art (SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff reduces the FID score by 4.09% compared to the best baseline. In downstream tasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and road mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP increases by 1.46%. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/yangzhaojason/DualDiff">https://github.com/yangzhaojason/DualDiff</a>. </p>
<blockquote>
<p>准确且高保真度的驾驶场景重建需要有效利用全面的场景信息作为条件输入。现有方法主要依赖3D边界框和BEV路线图进行前景和背景控制，这无法捕捉驾驶场景的全貌并充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强多视角和视频序列的驾驶场景生成。具体来说，我们引入了Occupancy Ray-shape Sampling（ORS）作为条件输入，它提供了丰富的前景和背景语义，以及3D空间几何，以精确控制这两个元素的生成。为了改进对精细前景对象的合成，特别是复杂和遥远的对象，我们提出了Foreground-Aware Mask（FGM）去噪损失函数。此外，我们开发了Semantic Fusion Attention（SFA）机制，以动态优先处理相关信息并抑制噪声，从而实现更有效的多模态融合。最后，为了确保高质量的图片到视频的生成，我们引入了Reward-Guided Diffusion（RGD）框架，该框架在生成的视频中保持全局一致性和语义连贯性。大量实验表明，DualDiff在多个数据集上实现了最新（SOTA）性能。在NuScenes数据集上，DualDiff将FID得分降低了4.09%，相较于最佳基线模型有所提升。在下游任务中，例如在BEV分割方面，我们的方法提高了车辆mIoU值4.5%，道路mIoU值提高了1.7%，而在BEV 3D目标检测方面，前景mAP提高了1.46%。代码将在<a target="_blank" rel="noopener" href="https://github.com/yangzhaojason/DualDiff%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/yangzhaojason/DualDiff上公开提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03689v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为DualDiff的双分支条件扩散模型，用于增强驾驶场景的生成。该模型引入Occupancy Ray-shape Sampling（ORS）作为条件输入，提供丰富的前景和背景语义以及3D空间几何信息，以精确控制两者的生成。同时，通过Foreground-Aware Mask（FGM）降噪损失函数提升细节前景物体的合成质量，并引入Semantic Fusion Attention（SFA）机制实现多模态信息的有效融合。最后，通过Reward-Guided Diffusion（RGD）框架确保高质量的视频生成。实验证明，DualDiff在多个数据集上取得领先性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DualDiff模型采用双分支条件扩散结构，提升驾驶场景生成能力。</li>
<li>引入Occupancy Ray-shape Sampling（ORS）作为条件输入，提供丰富的场景语义和几何信息。</li>
<li>Foreground-Aware Mask（FGM）降噪损失函数用于增强细节前景物体的合成质量。</li>
<li>Semantic Fusion Attention（SFA）机制实现多模态信息的动态优先级和噪声抑制。</li>
<li>Reward-Guided Diffusion（RGD）框架保证视频生成的高质量及全局一致性。</li>
<li>DualDiff在多个数据集上实现领先性能，如NuScenes数据集的FID分数降低4.09%。</li>
<li>下游任务如BEV分割和BEV 3D目标检测中，DualDiff方法提升车辆和道路的mIoU，以及前景的mAP。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03689">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0f174a3f16b4869966f62d3f94a12129.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab6a0d14f19200d5ca7aeb49b354680f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f08cb9b084b4276f0828b0af15b5932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b03966ecd1e75fae01c116e12f6db90e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-View-Depth-Consistent-Image-Generation-Using-Generative-AI-Models-Application-on-Architectural-Design-of-University-Buildings"><a href="#Multi-View-Depth-Consistent-Image-Generation-Using-Generative-AI-Models-Application-on-Architectural-Design-of-University-Buildings" class="headerlink" title="Multi-View Depth Consistent Image Generation Using Generative AI Models:   Application on Architectural Design of University Buildings"></a>Multi-View Depth Consistent Image Generation Using Generative AI Models:   Application on Architectural Design of University Buildings</h2><p><strong>Authors:Xusheng Du, Ruihan Gui, Zhengyang Wang, Ye Zhang, Haoran Xie</strong></p>
<p>In the early stages of architectural design, shoebox models are typically used as a simplified representation of building structures but require extensive operations to transform them into detailed designs. Generative artificial intelligence (AI) provides a promising solution to automate this transformation, but ensuring multi-view consistency remains a significant challenge. To solve this issue, we propose a novel three-stage consistent image generation framework using generative AI models to generate architectural designs from shoebox model representations. The proposed method enhances state-of-the-art image generation diffusion models to generate multi-view consistent architectural images. We employ ControlNet as the backbone and optimize it to accommodate multi-view inputs of architectural shoebox models captured from predefined perspectives. To ensure stylistic and structural consistency across multi-view images, we propose an image space loss module that incorporates style loss, structural loss and angle alignment loss. We then use depth estimation method to extract depth maps from the generated multi-view images. Finally, we use the paired data of the architectural images and depth maps as inputs to improve the multi-view consistency via the depth-aware 3D attention module. Experimental results demonstrate that the proposed framework can generate multi-view architectural images with consistent style and structural coherence from shoebox model inputs. </p>
<blockquote>
<p>在早期建筑设计阶段，鞋盒模型通常被用作建筑结构的简化表示，但需要大量的操作才能将它们转化为详细的设计。生成式人工智能（AI）为自动化这一转化过程提供了有前景的解决方案，但确保多视角一致性仍然是一个巨大的挑战。为了解决这一问题，我们提出了一种新的三阶段一致性图像生成框架，利用生成式AI模型从鞋盒模型表示生成建筑设计。所提出的方法改进了最先进的图像生成扩散模型，以生成多视角一致的建筑图像。我们采用ControlNet作为骨干网，并对其进行了优化，以适应从预定视角捕获的建筑鞋盒模型的多视角输入。为了确保多视角图像的风格和结构一致性，我们提出了一个图像空间损失模块，它结合了风格损失、结构损失和角度对齐损失。然后，我们使用深度估计方法从生成的多视角图像中提取深度图。最后，我们将建筑图像和深度图的配对数据作为输入，通过深度感知3D注意力模块提高多视角一致性。实验结果表明，该框架能够从鞋盒模型输入生成风格和结构一致的多视角建筑图像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03068v1">PDF</a> 10 pages, 7 figures, in Proceedings of CAADRIA2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于生成式人工智能模型的三阶段一致图像生成框架，用于从鞋盒模型表示生成建筑设计。该方法通过优化ControlNet来适应建筑鞋盒模型的多视角输入，通过图像空间损失模块确保跨多视角图像的风格和结构一致性。此外，还利用深度估计方法从生成的多视角图像中提取深度图，最后通过深度感知的3D注意力模块提高多视角一致性。实验结果表明，该框架能够从鞋盒模型输入生成具有一致风格和结构连贯性的多视角建筑图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了生成式人工智能在建筑设计领域的应用背景及其所面临的挑战。</li>
<li>提出了一种三阶段的图像生成框架，用于从鞋盒模型生成建筑设计图像。</li>
<li>优化了ControlNet以适应建筑鞋盒模型的多视角输入。</li>
<li>通过图像空间损失模块确保不同视角的图像风格和结构一致性。</li>
<li>利用深度估计方法从生成的多视角图像中提取深度图。</li>
<li>使用深度感知的3D注意力模块提高了多视角一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8244f49112f79f77573da0b6f5e1e0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-644ddae304c0df13ee981f6357475e54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c675b8ef23f44106ea700820d564126.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d26621cadbeb12779b3af945e86a8cc5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Can-Diffusion-Models-Provide-Rigorous-Uncertainty-Quantification-for-Bayesian-Inverse-Problems"><a href="#Can-Diffusion-Models-Provide-Rigorous-Uncertainty-Quantification-for-Bayesian-Inverse-Problems" class="headerlink" title="Can Diffusion Models Provide Rigorous Uncertainty Quantification for   Bayesian Inverse Problems?"></a>Can Diffusion Models Provide Rigorous Uncertainty Quantification for   Bayesian Inverse Problems?</h2><p><strong>Authors:Evan Scope Crafts, Umberto Villa</strong></p>
<p>In recent years, the ascendance of diffusion modeling as a state-of-the-art generative modeling approach has spurred significant interest in their use as priors in Bayesian inverse problems. However, it is unclear how to optimally integrate a diffusion model trained on the prior distribution with a given likelihood function to obtain posterior samples. While algorithms that have been developed for this purpose can produce high-quality, diverse point estimates of the unknown parameters of interest, they are often tested on problems where the prior distribution is analytically unknown, making it difficult to assess their performance in providing rigorous uncertainty quantification. In this work, we introduce a new framework, Bayesian Inverse Problem Solvers through Diffusion Annealing (BIPSDA), for diffusion model based posterior sampling. The framework unifies several recently proposed diffusion model based posterior sampling algorithms and contains novel algorithms that can be realized through flexible combinations of design choices. Algorithms within our framework were tested on model problems with a Gaussian mixture prior and likelihood functions inspired by problems in image inpainting, x-ray tomography, and phase retrieval. In this setting, approximate ground-truth posterior samples can be obtained, enabling principled evaluation of the performance of the algorithms. The results demonstrate that BIPSDA algorithms can provide strong performance on the image inpainting and x-ray tomography based problems, while the challenging phase retrieval problem, which is difficult to sample from even when the posterior density is known, remains outside the reach of the diffusion model based samplers. </p>
<blockquote>
<p>近年来，扩散模型作为最先进的生成建模方法，其在贝叶斯反问题中的先验使用引起了极大的兴趣。然而，尚不清楚如何将训练于先验分布的扩散模型与给定的似然函数相结合，以获得后验样本。尽管为此目的而开发的算法可以产生高质量且多样化的未知参数估计，但它们通常是在先验分布分析未知的问题上进行的测试，这使得难以评估它们在提供严格的不确定性量化方面的性能。在这项工作中，我们介绍了一种新的基于扩散退火的贝叶斯反问题求解器（BIPSDA）框架，用于基于扩散模型的采样。该框架统一了最近提出的几种基于扩散模型的采样算法，并包含了通过灵活的设计选择组合可实现的新颖算法。我们的框架内的算法在具有高斯混合先验和受图像修复、X射线断层扫描和相位检索问题启发的似然函数模型问题上进行了测试。在这种设置中，可以获得近似真实值后样本，从而能够按照原则评估算法的性能。结果表明，BIPSDA算法在图像修复和基于X射线断层扫描的问题上表现出强大的性能，而对于采样困难的相位检索问题（即使在已知后验密度的情况下也很难采样），仍超出了基于扩散模型的采样器的范围。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03007v1">PDF</a> </p>
<p><strong>Summary</strong><br>     近年来，扩散模型作为最先进的生成建模方法备受关注，其在贝叶斯反问题中作为先验的使用也备受瞩目。本文引入了一个新的框架——通过扩散退火解决贝叶斯反问题（BIPSDA），用于基于扩散模型的后续采样。该框架统一了最近提出的几种扩散模型后续采样算法，并包含了可以通过灵活组合设计选择实现的新算法。在具有高斯混合先验和受图像修复、X射线断层扫描和相位检索问题启发的似然函数的模型问题上测试了框架内的算法。结果证明，BIPSDA算法在图像修复和基于X射线断层扫描的问题上表现出强大的性能，而对于即使已知后验密度也难以采样的具有挑战性的相位检索问题，仍处于扩散模型采样器的范围之外。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型作为生成建模方法受到广泛关注，并在贝叶斯反问题中作为先验的使用日益重要。</li>
<li>引入新的框架BIPSDA，用于基于扩散模型的后续采样，统一并扩展了现有的扩散模型采样算法。</li>
<li>BIPSDA框架内的算法在具有高斯混合先验和特定似然函数的模型问题上进行了测试。</li>
<li>测试涵盖了图像修复、X射线断层扫描和相位检索等领域的问题。</li>
<li>BIPSDA算法在图像修复和X射线断层扫描问题上表现出良好的性能。</li>
<li>相位检索问题对扩散模型采样器来说仍然具有挑战性。</li>
<li>通过近似地面真实后验样本，可以原则性地评估算法性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5abfbbcaa66c0c0eb7e0b994e32a29c5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Straight-Line-Diffusion-Model-for-Efficient-3D-Molecular-Generation"><a href="#Straight-Line-Diffusion-Model-for-Efficient-3D-Molecular-Generation" class="headerlink" title="Straight-Line Diffusion Model for Efficient 3D Molecular Generation"></a>Straight-Line Diffusion Model for Efficient 3D Molecular Generation</h2><p><strong>Authors:Yuyan Ni, Shikun Feng, Haohan Chi, Bowen Zheng, Huan-ang Gao, Wei-Ying Ma, Zhi-Ming Ma, Yanyan Lan</strong></p>
<p>Diffusion-based models have shown great promise in molecular generation but often require a large number of sampling steps to generate valid samples. In this paper, we introduce a novel Straight-Line Diffusion Model (SLDM) to tackle this problem, by formulating the diffusion process to follow a linear trajectory. The proposed process aligns well with the noise sensitivity characteristic of molecular structures and uniformly distributes reconstruction effort across the generative process, thus enhancing learning efficiency and efficacy. Consequently, SLDM achieves state-of-the-art performance on 3D molecule generation benchmarks, delivering a 100-fold improvement in sampling efficiency. Furthermore, experiments on toy data and image generation tasks validate the generality and robustness of SLDM, showcasing its potential across diverse generative modeling domains. </p>
<blockquote>
<p>基于扩散的模型在分子生成方面显示出巨大的潜力，但通常需要大量的采样步骤来生成有效的样本。在本文中，我们引入了一种新型的直线扩散模型（SLDM）来解决这个问题，通过制定遵循线性轨迹的扩散过程。所提出的过程与分子结构的噪声敏感性特征相吻合，在生成过程中均匀分配重建工作，从而提高了学习效率和学习效果。因此，SLDM在3D分子生成基准测试上实现了最先进的性能，采样效率提高了100倍。此外，对玩具数据和图像生成任务的实验验证了SLDM的通用性和稳健性，展示了其在不同生成建模领域的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02918v1">PDF</a> </p>
<p><strong>Summary</strong>：本文引入了一种新型的直线扩散模型（SLDM），通过制定线性轨迹的扩散过程，解决了分子生成中需要大量采样步骤的问题。该模型对齐噪声敏感性特征，均匀分布重建努力，提高学习效率与效果，实现了在三维分子生成基准测试上的卓越性能，采样效率提高了100倍。同时，在玩具数据和图像生成任务上的实验验证了其通用性和稳健性，展示了其在不同生成建模域中的潜力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>新型直线扩散模型（SLDM）被提出以解决分子生成中的采样问题。</li>
<li>SLDM通过制定线性轨迹的扩散过程，提高了采样效率。</li>
<li>SLDM对齐噪声敏感性特征，使得模型更加适应分子结构的特点。</li>
<li>SLDM在三维分子生成基准测试上实现了卓越性能。</li>
<li>与传统方法相比，SLDM的采样效率提高了100倍。</li>
<li>实验结果显示，SLDM在玩具数据和图像生成任务上具有通用性和稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02918">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6244166971982348b013bf51316e92db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cdf2f94d9e4e686b353839c57909164.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7abd81e80d65a9fce2cc0c6b578781a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6e52a2b225dd2a9fc9f378fda411d3d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="StdGEN-Semantic-Decomposed-3D-Character-Generation-from-Single-Images"><a href="#StdGEN-Semantic-Decomposed-3D-Character-Generation-from-Single-Images" class="headerlink" title="StdGEN: Semantic-Decomposed 3D Character Generation from Single Images"></a>StdGEN: Semantic-Decomposed 3D Character Generation from Single Images</h2><p><strong>Authors:Yuze He, Yanning Zhou, Wang Zhao, Zhongkai Wu, Kaiwen Xiao, Wei Yang, Yong-Jin Liu, Xiao Han</strong></p>
<p>We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: <a target="_blank" rel="noopener" href="https://stdgen.github.io/">https://stdgen.github.io</a> </p>
<blockquote>
<p>我们推出了StdGEN，这是一个从单张图像生成语义分解的高质量3D角色的创新管道，广泛应用于虚拟现实、游戏制作、电影制作等领域。与以往方法在分解性、质量、优化时间等方面的局限不同，StdGEN具有分解性、有效性和高效性；也就是说，它能够在三分钟内生成具有分离语义组件的精细详细的3D角色，如身体、衣物和头发。StdGEN的核心是我们提出的大型语义感知重建模型（S-LRM），这是一个基于变压器的可推广模型，以前馈方式从多视角图像重建几何、颜色和语义。我们引入了可微多层语义表面提取方案，从由S-LRM重建的混合隐式字段中获取网格。此外，一个专门的高效多视角扩散模型和迭代多层表面细化模块被集成到管道中，以促进高质量、可分解的3D角色生成。大量实验表明，我们在3D动画角色生成方面达到了最新技术水平，在几何、纹理和分解性方面大大超过了现有基准。StdGEN提供即插即用的语义分解的3D角色，并为广泛的应用提供灵活的定制。项目页面：<a target="_blank" rel="noopener" href="https://stdgen.github.io/">https://stdgen.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05738v2">PDF</a> CVPR 2025. 13 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了StdGEN，这是一个从单张图像生成语义分解的高质量3D角色的创新管道。StdGEN具有分解性、有效性和效率，能够生成具有分离语义组件的精细3D角色，如身体、衣物和头发。其核心是语义感知大型重建模型（S-LRM），该模型从多视角图像以前馈方式联合重建几何、颜色和语义。实验表明，StdGEN在3D动漫角色生成方面表现出卓越的性能，在几何、纹理和分解性方面大大超越了现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StdGEN是一个生成语义分解的高质量3D角色的创新管道。</li>
<li>它能够从单张图像生成具有精细细节的3D角色，并分离出语义组件（如身体、衣物和头发）。</li>
<li>StdGEN的核心是语义感知大型重建模型（S-LRM），该模型能够从前馈方式联合重建几何、颜色和语义。</li>
<li>通过引入可微多层语义表面提取方案，从由S-LRM重建的混合隐式字段中获取网格。</li>
<li>StdGEN集成了高效的多视角扩散模型和迭代多层表面细化模块，以生成高质量、可分解的3D角色。</li>
<li>实验表明，StdGEN在3D动漫角色生成方面表现出卓越的性能，在几何、纹理和分解性方面超越现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05738">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-09334c41684c8d46c798420aded9a297.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-470d7fc459674e99c138abea41c04642.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0330782444ab3c9e8ad27a78dedc2c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6368e5d4c3006fd8872de5e9561d681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1b02df159a97a9771e6c1fdd1bdd3e7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior"><a href="#LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior" class="headerlink" title="LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior"></a>LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior</h2><p><strong>Authors:Xingjian Tang, Jingwei Guan, Linge Li, Ran Shi, Youmei Zhang, Mengye Lyu, Li Yan</strong></p>
<p>Diffusion models, as powerful generative models, have found a wide range of applications and shown great potential in solving image reconstruction problems. Some works attempted to solve MRI reconstruction with diffusion models, but these methods operate directly in pixel space, leading to higher computational costs for optimization and inference. Latent diffusion models, pre-trained on natural images with rich visual priors, are expected to solve the high computational cost problem in MRI reconstruction by operating in a lower-dimensional latent space. However, direct application to MRI reconstruction faces three key challenges: (1) absence of explicit control mechanisms for medical fidelity, (2) domain gap between natural images and MR physics, and (3) undefined data consistency in latent space. To address these challenges, a novel Latent Diffusion Prior-based undersampled MRI reconstruction (LDPM) method is proposed. Our LDPM framework addresses these challenges by: (1) a sketch-guided pipeline with a two-step reconstruction strategy, which balances perceptual quality and anatomical fidelity, (2) an MRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92 dB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE \cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM sampler, which enforces high-fidelity reconstruction in the latent space. Experiments on the fastMRI dataset\cite{fastmri} demonstrate the state-of-the-art performance of the proposed method and its robustness across various scenarios. The effectiveness of each module is also verified through ablation experiments. </p>
<blockquote>
<p>扩散模型作为强大的生成模型，在图像重建问题解决方案中得到了广泛的应用并表现出了巨大的潜力。一些研究尝试使用扩散模型解决MRI重建问题，但这些方法在像素空间直接操作，导致优化和推理的计算成本较高。预训练在自然图像上的潜在扩散模型具有丰富的视觉先验信息，有望通过降低维度的潜在空间解决MRI重建中的高计算成本问题。然而，直接应用于MRI重建面临三个主要挑战：(1)缺乏医疗准确性的明确控制机制；(2)自然图像与MR物理之间的领域差距；(3)潜在空间中的数据一致性未定义。为了解决这些挑战，提出了一种基于潜在扩散先验的欠采样MRI重建（LDPM）新方法。我们的LDPM框架通过以下方式应对这些挑战：(1)一个草图引导的流程，采用两步重建策略，平衡感知质量和解剖精度；(2)一个针对MRI优化的VAE（MR-VAE），与SD-VAE相比，它在欠采样MRI重建的PSNR上提高了约3.92分贝【sd】；(3)双阶段采样器，这是间隔DDPM采样器的改进版，它在潜在空间中强制进行高保真重建。在fastMRI数据集上的实验证明了该方法的先进性能及其在不同场景中的稳健性【fastmri】。每个模块的有效性也通过消融实验得到了验证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02951v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型作为强大的生成模型，在图像重建问题中展现出了巨大的潜力，并得到了广泛的应用。尽管已有一些工作尝试用扩散模型解决MRI重建问题，但这些方法在像素空间直接操作导致了较高的优化和推理计算成本。预期通过潜在扩散模型（在具有丰富视觉先验的自然图像上预训练）来解决这一问题。然而，将其直接应用于MRI重建面临三个关键挑战。为了解决这些挑战，提出了一种基于潜在扩散先验的欠采样MRI重建（LDPM）方法。我们的LDPM框架通过以下方式应对挑战：1）一个带有两步重建策略的草图引导管道，平衡感知质量和解剖保真度；2）一个针对MRI优化的VAE（MR-VAE），在欠采样MRI重建的PSNR上提高了约3.92 dB；3）双阶段采样器，一个改进的空间DDPM采样器版本，在潜在空间中强制高保真重建。在fastMRI数据集上的实验证明了所提方法的先进性能和在各种场景中的稳健性。每个模块的有效性也通过消融实验得到了验证。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在解决图像重建问题中展现出了巨大潜力，包括MRI重建。</li>
<li>直接在像素空间操作导致较高的计算成本，而潜在扩散模型有望在较低维度的潜在空间中解决此问题。</li>
<li>将潜在扩散模型直接应用于MRI重建面临三个主要挑战：医学保真度的缺乏明确控制机制、自然图像与MR物理之间的领域差距，以及潜在空间中的数据一致性未定义。</li>
<li>提出的LDPM方法通过草图引导管道和两步重建策略平衡感知质量与解剖保真度。</li>
<li>MR-VAE在欠采样MRI重建的PSNR上提高了约3.92 dB。</li>
<li>双阶段采样器是一种改进的空间DDPM采样器版本，它在潜在空间中强制执行高保真重建。</li>
<li>在fastMRI数据集上的实验证明了所提方法具有先进性能和在各种场景中的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02951">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e22c7fa8d8fb4f5ff7f28d3dda1da45e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5789dfc0f7479c776ca018ae0a47c6e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cdb7f71b9ee820a4008700bf2bd88c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ac9a4d23757bfcf5918478f08c6c34a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0251eac9ec15658da969639324cfbb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7d307b2c4d7b4c57391a62c837dee76.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="An-Undetectable-Watermark-for-Generative-Image-Models"><a href="#An-Undetectable-Watermark-for-Generative-Image-Models" class="headerlink" title="An Undetectable Watermark for Generative Image Models"></a>An Undetectable Watermark for Generative Image Models</h2><p><strong>Authors:Sam Gunn, Xuandong Zhao, Dawn Song</strong></p>
<p>We present the first undetectable watermarking scheme for generative image models. Undetectability ensures that no efficient adversary can distinguish between watermarked and un-watermarked images, even after making many adaptive queries. In particular, an undetectable watermark does not degrade image quality under any efficiently computable metric. Our scheme works by selecting the initial latents of a diffusion model using a pseudorandom error-correcting code (Christ and Gunn, 2024), a strategy which guarantees undetectability and robustness. We experimentally demonstrate that our watermarks are quality-preserving and robust using Stable Diffusion 2.1. Our experiments verify that, in contrast to every prior scheme we tested, our watermark does not degrade image quality. Our experiments also demonstrate robustness: existing watermark removal attacks fail to remove our watermark from images without significantly degrading the quality of the images. Finally, we find that we can robustly encode 512 bits in our watermark, and up to 2500 bits when the images are not subjected to watermark removal attacks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/PRC-Watermark">https://github.com/XuandongZhao/PRC-Watermark</a>. </p>
<blockquote>
<p>我们为生成图像模型提出了第一个不可检测的水印方案。不可检测性确保高效的对手即使在执行多次自适应查询后，也无法区分带水印和不带水印的图像。特别是，不可检测的水印在任何可高效计算的指标下都不会降低图像质量。我们的方案通过选择一个扩散模型的初始潜在变量来实现，该选择采用伪随机纠错码（Christ and Gunn, 2024）的策略，该策略保证了不可检测性和稳健性。我们通过实验证明了水印在保持质量和稳健性方面的优势，使用的是Stable Diffusion 2.1。我们的实验验证了我们测试过的所有先前方案相比，我们的水印不会降低图像质量。我们的实验还证明了稳健性：现有的水印移除攻击无法在我们的水印从图像中移除的同时而不显著降低图像质量。最后，我们发现我们可以稳健地在我们的水印中编码512位，当图像未受到水印移除攻击时，甚至可以编码高达2500位。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/PRC-Watermark%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XuandongZhao/PRC-Watermark找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07369v3">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     本文提出首个针对生成图像模型的不可检测水印方案。该方案利用伪随机纠错码选择扩散模型的初始潜变量，确保水印的不可检测性和稳健性。实验证明，该水印不会降低图像质量，对现有的水印移除攻击具有强大的抵御能力。该方案可将水印嵌入到图像中而不会明显影响图像质量，并具有高达512位的稳健编码能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出首个针对生成图像模型的不可检测水印方案。</li>
<li>利用伪随机纠错码选择扩散模型的初始潜变量以确保水印的不可检测性和稳健性。</li>
<li>实验证明水印不会降低图像质量。</li>
<li>对现有的水印移除攻击具有强大的抵御能力。</li>
<li>水印可以嵌入到图像中而不会明显影响图像质量。</li>
<li>该方案具有高达512位的稳健编码能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07369">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee4e523f52cef8c3e74318fa56b5f88c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diffusion-State-Guided-Projected-Gradient-for-Inverse-Problems"><a href="#Diffusion-State-Guided-Projected-Gradient-for-Inverse-Problems" class="headerlink" title="Diffusion State-Guided Projected Gradient for Inverse Problems"></a>Diffusion State-Guided Projected Gradient for Inverse Problems</h2><p><strong>Authors:Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar</strong></p>
<p>Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Anima-Lab/DiffStateGrad">https://github.com/Anima-Lab/DiffStateGrad</a>. </p>
<blockquote>
<p>最近扩散模型的发展在解决反问题方面学习数据先验方面取得了显著成效。他们利用扩散采样步骤来诱导数据先验，同时在每一步使用测量指导梯度来施加数据一致性。对于一般反问题，当使用无条件训练的扩散模型时，由于测量似然性难以处理，导致后验采样不准确。换句话说，由于这些近似方法，它们无法保留由扩散先验定义的数据流形上的生成过程，从而导致图像恢复等应用程序中出现伪影。为了提高扩散模型解决反问题的性能和稳健性，我们提出了扩散状态引导投影梯度（DiffStateGrad），它将测量梯度投影到扩散过程中间状态的低秩近似子空间上。作为一个模块，DiffStateGrad可以添加到广泛的基于扩散的反向求解器中，以提高扩散过程在先验流形上的保留能力，并过滤掉产生伪影的组件。我们强调，DiffStateGrad提高了扩散模型在选择测量指导步长和噪声方面的稳健性，同时提高了最坏情况下的性能。最后，我们证明了DiffStateGrad在解决线性和非线性图像恢复反问题上优于现有技术。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Anima-Lab/DiffStateGrad%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Anima-Lab/DiffStateGrad上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03463v4">PDF</a> Published as a conference paper at ICLR 2025. RZ and BT have equal   contributions</p>
<p><strong>Summary</strong><br>     近期扩散模型在解决反问题方面的进展有效学习了数据先验。它们利用扩散采样步骤来诱导数据先验，同时使用测量指导梯度来施加数据一致性。对于一般反问题，当使用无条件训练的扩散模型时，由于测量似然不可行，需要近似处理，导致后验采样不准确。为解决这一问题，提出了扩散状态引导投影梯度（DiffStateGrad），它将测量梯度投影到扩散过程中间状态的低秩近似子空间上。DiffStateGrad可作为模块添加到各种基于扩散的反问题求解器中，提高了扩散过程在先验流形上的保持能力，并过滤掉了产生伪影的组件。它提高了扩散模型在选择测量指导步长和噪声方面的稳健性，并在线性和非线性图像恢复反问题上取得了先进的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在解决反问题上取得新进展，通过扩散采样步骤学习数据先验并利用测量指导梯度施加数据一致性。</li>
<li>对于一般反问题，使用无条件训练的扩散模型时需要进行近似处理，导致后验采样不准确和生成过程中的失真。</li>
<li>提出DiffStateGrad方法，通过将测量梯度投影到扩散过程的低秩近似子空间上，提高扩散模型的性能。</li>
<li>DiffStateGrad可作为模块添加到各种基于扩散的反问题求解器中，改善扩散过程在先验流形上的保持能力并过滤伪影成分。</li>
<li>DiffStateGrad提高了扩散模型在选择测量指导步长和噪声方面的稳健性。</li>
<li>DiffStateGrad在图像恢复等应用领域的表现优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03463">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e376ce6d1653471be6f7bf6d02d1e548.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ae0b5590baf824e5ae43aac9c99a51a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80d50ea2301487db0de96f84525934bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d73a08f0b1ddc496f7fb4e752bfdf443.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d56557d6bd577bafba3c251e3160f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5ff6bc79d9e77536d65330d42581135.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Counting-Guidance-for-High-Fidelity-Text-to-Image-Synthesis"><a href="#Counting-Guidance-for-High-Fidelity-Text-to-Image-Synthesis" class="headerlink" title="Counting Guidance for High Fidelity Text-to-Image Synthesis"></a>Counting Guidance for High Fidelity Text-to-Image Synthesis</h2><p><strong>Authors:Wonjun Kang, Kevin Galim, Hyung Il Koo, Nam Ik Cho</strong></p>
<p>Recently, there have been significant improvements in the quality and performance of text-to-image generation, largely due to the impressive results attained by diffusion models. However, text-to-image diffusion models sometimes struggle to create high-fidelity content for the given input prompt. One specific issue is their difficulty in generating the precise number of objects specified in the text prompt. For example, when provided with the prompt “five apples and ten lemons on a table,” images generated by diffusion models often contain an incorrect number of objects. In this paper, we present a method to improve diffusion models so that they accurately produce the correct object count based on the input prompt. We adopt a counting network that performs reference-less class-agnostic counting for any given image. We calculate the gradients of the counting network and refine the predicted noise for each step. To address the presence of multiple types of objects in the prompt, we utilize novel attention map guidance to obtain high-quality masks for each object. Finally, we guide the denoising process using the calculated gradients for each object. Through extensive experiments and evaluation, we demonstrate that the proposed method significantly enhances the fidelity of diffusion models with respect to object count. Code is available at <a target="_blank" rel="noopener" href="https://github.com/furiosa-ai/counting-guidance">https://github.com/furiosa-ai/counting-guidance</a>. </p>
<blockquote>
<p>最近，文本到图像生成的质量和性能得到了显著改善，这很大程度上是由于扩散模型取得的令人印象深刻的结果。然而，文本到图像的扩散模型有时在生成给定输入提示的高保真内容时遇到困难。一个具体的问题是它们在生成文本提示中指定的精确数量的对象时遇到困难。例如，当提供“桌子上放着五个苹果和十个柠檬”的提示时，由扩散模型生成的图像通常包含不正确数量的对象。在本文中，我们提出了一种改进扩散模型的方法，使它们能够基于输入提示准确产生正确的对象计数。我们采用了一个计数网络，该网络可以对任何给定图像执行无参考的类无关计数。我们计算计数网络的梯度，并细化每一步预测的噪声。为了解决提示中存在多种类型对象的问题，我们利用新型注意力图引导来获得每个对象的高质量掩模。最后，我们使用为每个对象计算的梯度来引导去噪过程。通过广泛的实验和评估，我们证明所提出的方法显著提高了扩散模型在对象计数方面的保真度。代码可在<a target="_blank" rel="noopener" href="https://github.com/furiosa-ai/counting-guidance%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/furiosa-ai/counting-guidance找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.17567v3">PDF</a> Accepted at WACV 2025 (Oral). Code is available at   <a target="_blank" rel="noopener" href="https://github.com/furiosa-ai/counting-guidance">https://github.com/furiosa-ai/counting-guidance</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了如何通过改进扩散模型，使其能够根据输入提示准确生成对象数量。采用计数网络进行无参考的类无知计数，通过计算计数网络的梯度并优化每一步的预测噪声来实现。为解决提示中存在多种类型对象的问题，利用新型注意力图引导获得每个对象的高质量蒙版，并在每个对象的梯度引导下完成去噪过程。实验证明，该方法显著提高扩散模型在对象计数方面的保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本转图像生成领域取得显著进步，但在生成精确数量的内容方面仍有挑战。</li>
<li>本文提出一种改进方法，使扩散模型能基于输入提示准确生成对象数量。</li>
<li>采用计数网络进行无参考的类无知计数，适用于任何给定图像。</li>
<li>通过计算计数网络的梯度并优化每一步的预测噪声来提高模型性能。</li>
<li>为处理提示中的多种对象类型，使用新型注意力图引导技术，为每个对象生成高质量蒙版。</li>
<li>结合每个对象的梯度引导完成去噪过程。</li>
<li>实验证明，该方法在增强扩散模型对象计数方面的保真度方面表现显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.17567">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6a7abdcc58f3669a243136fdc5b4f6dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e090fce05e778630928f7dd24980630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31b7a770a6b6e9d9b37511c84da54c75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36e4a15beca320441b2c5d1dd0f7c1c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DP-LDMs-Differentially-Private-Latent-Diffusion-Models"><a href="#DP-LDMs-Differentially-Private-Latent-Diffusion-Models" class="headerlink" title="DP-LDMs: Differentially Private Latent Diffusion Models"></a>DP-LDMs: Differentially Private Latent Diffusion Models</h2><p><strong>Authors:Michael F. Liu, Saiyue Lyu, Margarita Vinaroz, Mijung Park</strong></p>
<p>Diffusion models (DMs) are one of the most widely used generative models for producing high quality images. However, a flurry of recent papers points out that DMs are least private forms of image generators, by extracting a significant number of near-identical replicas of training images from DMs. Existing privacy-enhancing techniques for DMs, unfortunately, do not provide a good privacy-utility tradeoff. In this paper, we aim to improve the current state of DMs with differential privacy (DP) by adopting the $\textit{Latent}$ Diffusion Models (LDMs). LDMs are equipped with powerful pre-trained autoencoders that map the high-dimensional pixels into lower-dimensional latent representations, in which DMs are trained, yielding a more efficient and fast training of DMs. Rather than fine-tuning the entire LDMs, we fine-tune only the $\textit{attention}$ modules of LDMs with DP-SGD, reducing the number of trainable parameters by roughly $90%$ and achieving a better privacy-accuracy trade-off. Our approach allows us to generate realistic, high-dimensional images (256x256) conditioned on text prompts with DP guarantees, which, to the best of our knowledge, has not been attempted before. Our approach provides a promising direction for training more powerful, yet training-efficient differentially private DMs, producing high-quality DP images. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DP-LDM-4525">https://anonymous.4open.science/r/DP-LDM-4525</a>. </p>
<blockquote>
<p>扩散模型（DMs）是生成高质量图像的最广泛使用的生成模型之一。然而，最近的一系列论文指出，扩散模型是图像生成器中隐私保护最差的形式之一，能够从扩散模型中提取大量近乎相同的训练图像复制品。不幸的是，现有的用于增强扩散模型隐私保护的技术并没有提供良好的隐私效用权衡。在本文中，我们旨在采用潜在扩散模型（LDMs）改进当前扩散模型与差分隐私（DP）的状态。LDMs配备了强大的预训练自动编码器，将高维像素映射到低维潜在表示中，在其中训练扩散模型，从而实现了更高效、更快的扩散模型训练。我们不需要微调整个LDMs，而只是使用DP-SGD微调LDMs的注意力模块，将可训练参数的数量减少了大约90%，并实现了更好的隐私准确性权衡。我们的方法能够利用文本提示生成具有DP保证的现实主义、高维图像（256x256），据我们所知，之前尚未有人尝试过。我们的方法为训练更强大、更高效且具有差分隐私保护能力的扩散模型提供了有前景的方向，能够生成高质量DP图像。我们的代码可在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DP-LDM-4525%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/DP-LDM-4525上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.15759v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型（DMs）是生成高质量图像最常用的生成模型之一。然而，最近的论文指出DMs是最不私密的图像生成形式。现有增强隐私保护的技术并不能在隐私和效用之间取得良好的平衡。本文旨在采用具有差分隐私（DP）的潜在扩散模型（LDMs）来改善DMs的现状。LDMs配备强大的预训练自动编码器，将高维像素映射到低维潜在表示中训练DMs，实现了更高效、更快的DMs训练。我们只对LDMs的注意力模块进行微调，而不是对整个模型进行微调，并使用DP-SGD实现差分隐私保护。通过减少约90%的可训练参数，我们实现了更好的隐私与准确性之间的权衡。我们的方法能够生成具有文本提示条件的真实、高维图像（256x256），并具有DP保证，这在我们的知识范围内尚未有人尝试过。这为训练更强大、更高效且具有差分隐私的DMs提供了有前景的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型（DMs）是生成高质量图像的主要工具，但存在隐私泄露的问题。</li>
<li>现有隐私增强技术在DMs上的隐私与效用平衡不佳。</li>
<li>采用潜在扩散模型（LDMs）结合差分隐私（DP）技术来改善DMs的隐私问题。</li>
<li>LDMs使用预训练自动编码器将高维像素转化为低维潜在表示，提高了DMs的训练效率和速度。</li>
<li>只对LDMs的注意力模块进行微调，显著减少了可训练参数，实现了更好的隐私与准确性之间的平衡。</li>
<li>该方法能够生成具有文本提示条件的真实、高维图像（256x256），并具有差分隐私保证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.15759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-da9430105efe8e7ed881279948a6a5a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdd6ed66eeac1d4b060fd20f111fcf42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6121a42c460b362f366e359520a65c1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ff31b8c293a9847af10bba02cef2b473.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-07  A self-supervised cyclic neural-analytic approach for novel view   synthesis and 3D reconstruction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6fdc9058a4ab67eeb27058a76a4d3c90.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-03-07  NTR-Gaussian Nighttime Dynamic Thermal Reconstruction with 4D Gaussian   Splatting Based on Thermodynamics
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17447.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
