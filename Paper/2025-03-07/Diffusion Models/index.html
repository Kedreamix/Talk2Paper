<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  DualDiff+ Dual-Branch Diffusion for High-Fidelity Video Generation with   Reward Guidance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    42 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-07-æ›´æ–°"><a href="#2025-03-07-æ›´æ–°" class="headerlink" title="2025-03-07 æ›´æ–°"></a>2025-03-07 æ›´æ–°</h1><h2 id="DualDiff-Dual-Branch-Diffusion-for-High-Fidelity-Video-Generation-with-Reward-Guidance"><a href="#DualDiff-Dual-Branch-Diffusion-for-High-Fidelity-Video-Generation-with-Reward-Guidance" class="headerlink" title="DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with   Reward Guidance"></a>DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with   Reward Guidance</h2><p><strong>Authors:Zhao Yang, Zezhong Qian, Xiaofan Li, Weixiang Xu, Gongpeng Zhao, Ruohong Yu, Lingsi Zhu, Longjun Liu</strong></p>
<p>Accurate and high-fidelity driving scene reconstruction demands the effective utilization of comprehensive scene information as conditional inputs. Existing methods predominantly rely on 3D bounding boxes and BEV road maps for foreground and background control, which fail to capture the full complexity of driving scenes and adequately integrate multimodal information. In this work, we present DualDiff, a dual-branch conditional diffusion model designed to enhance driving scene generation across multiple views and video sequences. Specifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional input, offering rich foreground and background semantics alongside 3D spatial geometry to precisely control the generation of both elements. To improve the synthesis of fine-grained foreground objects, particularly complex and distant ones, we propose a Foreground-Aware Mask (FGM) denoising loss function. Additionally, we develop the Semantic Fusion Attention (SFA) mechanism to dynamically prioritize relevant information and suppress noise, enabling more effective multimodal fusion. Finally, to ensure high-quality image-to-video generation, we introduce the Reward-Guided Diffusion (RGD) framework, which maintains global consistency and semantic coherence in generated videos. Extensive experiments demonstrate that DualDiff achieves state-of-the-art (SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff reduces the FID score by 4.09% compared to the best baseline. In downstream tasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and road mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP increases by 1.46%. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/yangzhaojason/DualDiff">https://github.com/yangzhaojason/DualDiff</a>. </p>
<blockquote>
<p>å‡†ç¡®ä¸”é«˜ä¿çœŸåº¦çš„é©¾é©¶åœºæ™¯é‡å»ºéœ€è¦æœ‰æ•ˆåˆ©ç”¨å…¨é¢çš„åœºæ™¯ä¿¡æ¯ä½œä¸ºæ¡ä»¶è¾“å…¥ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–3Dè¾¹ç•Œæ¡†å’ŒBEVè·¯çº¿å›¾è¿›è¡Œå‰æ™¯å’ŒèƒŒæ™¯æ§åˆ¶ï¼Œè¿™æ— æ³•æ•æ‰é©¾é©¶åœºæ™¯çš„å…¨è²Œå¹¶å……åˆ†æ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DualDiffï¼Œè¿™æ˜¯ä¸€ç§åŒåˆ†æ”¯æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºå¤šè§†è§’å’Œè§†é¢‘åºåˆ—çš„é©¾é©¶åœºæ™¯ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†Occupancy Ray-shape Samplingï¼ˆORSï¼‰ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå®ƒæä¾›äº†ä¸°å¯Œçš„å‰æ™¯å’ŒèƒŒæ™¯è¯­ä¹‰ï¼Œä»¥åŠ3Dç©ºé—´å‡ ä½•ï¼Œä»¥ç²¾ç¡®æ§åˆ¶è¿™ä¸¤ä¸ªå…ƒç´ çš„ç”Ÿæˆã€‚ä¸ºäº†æ”¹è¿›å¯¹ç²¾ç»†å‰æ™¯å¯¹è±¡çš„åˆæˆï¼Œç‰¹åˆ«æ˜¯å¤æ‚å’Œé¥è¿œçš„å¯¹è±¡ï¼Œæˆ‘ä»¬æå‡ºäº†Foreground-Aware Maskï¼ˆFGMï¼‰å»å™ªæŸå¤±å‡½æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†Semantic Fusion Attentionï¼ˆSFAï¼‰æœºåˆ¶ï¼Œä»¥åŠ¨æ€ä¼˜å…ˆå¤„ç†ç›¸å…³ä¿¡æ¯å¹¶æŠ‘åˆ¶å™ªå£°ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆã€‚æœ€åï¼Œä¸ºäº†ç¡®ä¿é«˜è´¨é‡çš„å›¾ç‰‡åˆ°è§†é¢‘çš„ç”Ÿæˆï¼Œæˆ‘ä»¬å¼•å…¥äº†Reward-Guided Diffusionï¼ˆRGDï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆçš„è§†é¢‘ä¸­ä¿æŒå…¨å±€ä¸€è‡´æ€§å’Œè¯­ä¹‰è¿è´¯æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDualDiffåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ï¼ˆSOTAï¼‰æ€§èƒ½ã€‚åœ¨NuScenesæ•°æ®é›†ä¸Šï¼ŒDualDiffå°†FIDå¾—åˆ†é™ä½äº†4.09%ï¼Œç›¸è¾ƒäºæœ€ä½³åŸºçº¿æ¨¡å‹æœ‰æ‰€æå‡ã€‚åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œä¾‹å¦‚åœ¨BEVåˆ†å‰²æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†è½¦è¾†mIoUå€¼4.5%ï¼Œé“è·¯mIoUå€¼æé«˜äº†1.7%ï¼Œè€Œåœ¨BEV 3Dç›®æ ‡æ£€æµ‹æ–¹é¢ï¼Œå‰æ™¯mAPæé«˜äº†1.46%ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/yangzhaojason/DualDiff%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/yangzhaojason/DualDiffä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03689v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDualDiffçš„åŒåˆ†æ”¯æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå¢å¼ºé©¾é©¶åœºæ™¯çš„ç”Ÿæˆã€‚è¯¥æ¨¡å‹å¼•å…¥Occupancy Ray-shape Samplingï¼ˆORSï¼‰ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œæä¾›ä¸°å¯Œçš„å‰æ™¯å’ŒèƒŒæ™¯è¯­ä¹‰ä»¥åŠ3Dç©ºé—´å‡ ä½•ä¿¡æ¯ï¼Œä»¥ç²¾ç¡®æ§åˆ¶ä¸¤è€…çš„ç”Ÿæˆã€‚åŒæ—¶ï¼Œé€šè¿‡Foreground-Aware Maskï¼ˆFGMï¼‰é™å™ªæŸå¤±å‡½æ•°æå‡ç»†èŠ‚å‰æ™¯ç‰©ä½“çš„åˆæˆè´¨é‡ï¼Œå¹¶å¼•å…¥Semantic Fusion Attentionï¼ˆSFAï¼‰æœºåˆ¶å®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚æœ€åï¼Œé€šè¿‡Reward-Guided Diffusionï¼ˆRGDï¼‰æ¡†æ¶ç¡®ä¿é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼ŒDualDiffåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—é¢†å…ˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DualDiffæ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯æ¡ä»¶æ‰©æ•£ç»“æ„ï¼Œæå‡é©¾é©¶åœºæ™¯ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å¼•å…¥Occupancy Ray-shape Samplingï¼ˆORSï¼‰ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œæä¾›ä¸°å¯Œçš„åœºæ™¯è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ã€‚</li>
<li>Foreground-Aware Maskï¼ˆFGMï¼‰é™å™ªæŸå¤±å‡½æ•°ç”¨äºå¢å¼ºç»†èŠ‚å‰æ™¯ç‰©ä½“çš„åˆæˆè´¨é‡ã€‚</li>
<li>Semantic Fusion Attentionï¼ˆSFAï¼‰æœºåˆ¶å®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„åŠ¨æ€ä¼˜å…ˆçº§å’Œå™ªå£°æŠ‘åˆ¶ã€‚</li>
<li>Reward-Guided Diffusionï¼ˆRGDï¼‰æ¡†æ¶ä¿è¯è§†é¢‘ç”Ÿæˆçš„é«˜è´¨é‡åŠå…¨å±€ä¸€è‡´æ€§ã€‚</li>
<li>DualDiffåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°é¢†å…ˆæ€§èƒ½ï¼Œå¦‚NuScenesæ•°æ®é›†çš„FIDåˆ†æ•°é™ä½4.09%ã€‚</li>
<li>ä¸‹æ¸¸ä»»åŠ¡å¦‚BEVåˆ†å‰²å’ŒBEV 3Dç›®æ ‡æ£€æµ‹ä¸­ï¼ŒDualDiffæ–¹æ³•æå‡è½¦è¾†å’Œé“è·¯çš„mIoUï¼Œä»¥åŠå‰æ™¯çš„mAPã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f174a3f16b4869966f62d3f94a12129.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab6a0d14f19200d5ca7aeb49b354680f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f08cb9b084b4276f0828b0af15b5932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b03966ecd1e75fae01c116e12f6db90e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-View-Depth-Consistent-Image-Generation-Using-Generative-AI-Models-Application-on-Architectural-Design-of-University-Buildings"><a href="#Multi-View-Depth-Consistent-Image-Generation-Using-Generative-AI-Models-Application-on-Architectural-Design-of-University-Buildings" class="headerlink" title="Multi-View Depth Consistent Image Generation Using Generative AI Models:   Application on Architectural Design of University Buildings"></a>Multi-View Depth Consistent Image Generation Using Generative AI Models:   Application on Architectural Design of University Buildings</h2><p><strong>Authors:Xusheng Du, Ruihan Gui, Zhengyang Wang, Ye Zhang, Haoran Xie</strong></p>
<p>In the early stages of architectural design, shoebox models are typically used as a simplified representation of building structures but require extensive operations to transform them into detailed designs. Generative artificial intelligence (AI) provides a promising solution to automate this transformation, but ensuring multi-view consistency remains a significant challenge. To solve this issue, we propose a novel three-stage consistent image generation framework using generative AI models to generate architectural designs from shoebox model representations. The proposed method enhances state-of-the-art image generation diffusion models to generate multi-view consistent architectural images. We employ ControlNet as the backbone and optimize it to accommodate multi-view inputs of architectural shoebox models captured from predefined perspectives. To ensure stylistic and structural consistency across multi-view images, we propose an image space loss module that incorporates style loss, structural loss and angle alignment loss. We then use depth estimation method to extract depth maps from the generated multi-view images. Finally, we use the paired data of the architectural images and depth maps as inputs to improve the multi-view consistency via the depth-aware 3D attention module. Experimental results demonstrate that the proposed framework can generate multi-view architectural images with consistent style and structural coherence from shoebox model inputs. </p>
<blockquote>
<p>åœ¨æ—©æœŸå»ºç­‘è®¾è®¡é˜¶æ®µï¼Œé‹ç›’æ¨¡å‹é€šå¸¸è¢«ç”¨ä½œå»ºç­‘ç»“æ„çš„ç®€åŒ–è¡¨ç¤ºï¼Œä½†éœ€è¦å¤§é‡çš„æ“ä½œæ‰èƒ½å°†å®ƒä»¬è½¬åŒ–ä¸ºè¯¦ç»†çš„è®¾è®¡ã€‚ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸ºè‡ªåŠ¨åŒ–è¿™ä¸€è½¬åŒ–è¿‡ç¨‹æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç¡®ä¿å¤šè§†è§’ä¸€è‡´æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸‰é˜¶æ®µä¸€è‡´æ€§å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆå¼AIæ¨¡å‹ä»é‹ç›’æ¨¡å‹è¡¨ç¤ºç”Ÿæˆå»ºç­‘è®¾è®¡ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ”¹è¿›äº†æœ€å…ˆè¿›çš„å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆå¤šè§†è§’ä¸€è‡´çš„å»ºç­‘å›¾åƒã€‚æˆ‘ä»¬é‡‡ç”¨ControlNetä½œä¸ºéª¨å¹²ç½‘ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥é€‚åº”ä»é¢„å®šè§†è§’æ•è·çš„å»ºç­‘é‹ç›’æ¨¡å‹çš„å¤šè§†è§’è¾“å…¥ã€‚ä¸ºäº†ç¡®ä¿å¤šè§†è§’å›¾åƒçš„é£æ ¼å’Œç»“æ„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå›¾åƒç©ºé—´æŸå¤±æ¨¡å—ï¼Œå®ƒç»“åˆäº†é£æ ¼æŸå¤±ã€ç»“æ„æŸå¤±å’Œè§’åº¦å¯¹é½æŸå¤±ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ·±åº¦ä¼°è®¡æ–¹æ³•ä»ç”Ÿæˆçš„å¤šè§†è§’å›¾åƒä¸­æå–æ·±åº¦å›¾ã€‚æœ€åï¼Œæˆ‘ä»¬å°†å»ºç­‘å›¾åƒå’Œæ·±åº¦å›¾çš„é…å¯¹æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡æ·±åº¦æ„ŸçŸ¥3Dæ³¨æ„åŠ›æ¨¡å—æé«˜å¤šè§†è§’ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»é‹ç›’æ¨¡å‹è¾“å…¥ç”Ÿæˆé£æ ¼å’Œç»“æ„ä¸€è‡´çš„å¤šè§†è§’å»ºç­‘å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03068v1">PDF</a> 10 pages, 7 figures, in Proceedings of CAADRIA2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„ä¸‰é˜¶æ®µä¸€è‡´å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºä»é‹ç›’æ¨¡å‹è¡¨ç¤ºç”Ÿæˆå»ºç­‘è®¾è®¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–ControlNetæ¥é€‚åº”å»ºç­‘é‹ç›’æ¨¡å‹çš„å¤šè§†è§’è¾“å…¥ï¼Œé€šè¿‡å›¾åƒç©ºé—´æŸå¤±æ¨¡å—ç¡®ä¿è·¨å¤šè§†è§’å›¾åƒçš„é£æ ¼å’Œç»“æ„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨æ·±åº¦ä¼°è®¡æ–¹æ³•ä»ç”Ÿæˆçš„å¤šè§†è§’å›¾åƒä¸­æå–æ·±åº¦å›¾ï¼Œæœ€åé€šè¿‡æ·±åº¦æ„ŸçŸ¥çš„3Dæ³¨æ„åŠ›æ¨¡å—æé«˜å¤šè§†è§’ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»é‹ç›’æ¨¡å‹è¾“å…¥ç”Ÿæˆå…·æœ‰ä¸€è‡´é£æ ¼å’Œç»“æ„è¿è´¯æ€§çš„å¤šè§†è§’å»ºç­‘å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å»ºç­‘è®¾è®¡é¢†åŸŸçš„åº”ç”¨èƒŒæ™¯åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µçš„å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºä»é‹ç›’æ¨¡å‹ç”Ÿæˆå»ºç­‘è®¾è®¡å›¾åƒã€‚</li>
<li>ä¼˜åŒ–äº†ControlNetä»¥é€‚åº”å»ºç­‘é‹ç›’æ¨¡å‹çš„å¤šè§†è§’è¾“å…¥ã€‚</li>
<li>é€šè¿‡å›¾åƒç©ºé—´æŸå¤±æ¨¡å—ç¡®ä¿ä¸åŒè§†è§’çš„å›¾åƒé£æ ¼å’Œç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨æ·±åº¦ä¼°è®¡æ–¹æ³•ä»ç”Ÿæˆçš„å¤šè§†è§’å›¾åƒä¸­æå–æ·±åº¦å›¾ã€‚</li>
<li>ä½¿ç”¨æ·±åº¦æ„ŸçŸ¥çš„3Dæ³¨æ„åŠ›æ¨¡å—æé«˜äº†å¤šè§†è§’ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8244f49112f79f77573da0b6f5e1e0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-644ddae304c0df13ee981f6357475e54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c675b8ef23f44106ea700820d564126.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d26621cadbeb12779b3af945e86a8cc5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Can-Diffusion-Models-Provide-Rigorous-Uncertainty-Quantification-for-Bayesian-Inverse-Problems"><a href="#Can-Diffusion-Models-Provide-Rigorous-Uncertainty-Quantification-for-Bayesian-Inverse-Problems" class="headerlink" title="Can Diffusion Models Provide Rigorous Uncertainty Quantification for   Bayesian Inverse Problems?"></a>Can Diffusion Models Provide Rigorous Uncertainty Quantification for   Bayesian Inverse Problems?</h2><p><strong>Authors:Evan Scope Crafts, Umberto Villa</strong></p>
<p>In recent years, the ascendance of diffusion modeling as a state-of-the-art generative modeling approach has spurred significant interest in their use as priors in Bayesian inverse problems. However, it is unclear how to optimally integrate a diffusion model trained on the prior distribution with a given likelihood function to obtain posterior samples. While algorithms that have been developed for this purpose can produce high-quality, diverse point estimates of the unknown parameters of interest, they are often tested on problems where the prior distribution is analytically unknown, making it difficult to assess their performance in providing rigorous uncertainty quantification. In this work, we introduce a new framework, Bayesian Inverse Problem Solvers through Diffusion Annealing (BIPSDA), for diffusion model based posterior sampling. The framework unifies several recently proposed diffusion model based posterior sampling algorithms and contains novel algorithms that can be realized through flexible combinations of design choices. Algorithms within our framework were tested on model problems with a Gaussian mixture prior and likelihood functions inspired by problems in image inpainting, x-ray tomography, and phase retrieval. In this setting, approximate ground-truth posterior samples can be obtained, enabling principled evaluation of the performance of the algorithms. The results demonstrate that BIPSDA algorithms can provide strong performance on the image inpainting and x-ray tomography based problems, while the challenging phase retrieval problem, which is difficult to sample from even when the posterior density is known, remains outside the reach of the diffusion model based samplers. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºæœ€å…ˆè¿›çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ï¼Œå…¶åœ¨è´å¶æ–¯åé—®é¢˜ä¸­çš„å…ˆéªŒä½¿ç”¨å¼•èµ·äº†æå¤§çš„å…´è¶£ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šå¦‚ä½•å°†è®­ç»ƒäºå…ˆéªŒåˆ†å¸ƒçš„æ‰©æ•£æ¨¡å‹ä¸ç»™å®šçš„ä¼¼ç„¶å‡½æ•°ç›¸ç»“åˆï¼Œä»¥è·å¾—åéªŒæ ·æœ¬ã€‚å°½ç®¡ä¸ºæ­¤ç›®çš„è€Œå¼€å‘çš„ç®—æ³•å¯ä»¥äº§ç”Ÿé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„æœªçŸ¥å‚æ•°ä¼°è®¡ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯åœ¨å…ˆéªŒåˆ†å¸ƒåˆ†ææœªçŸ¥çš„é—®é¢˜ä¸Šè¿›è¡Œçš„æµ‹è¯•ï¼Œè¿™ä½¿å¾—éš¾ä»¥è¯„ä¼°å®ƒä»¬åœ¨æä¾›ä¸¥æ ¼çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹é¢çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£é€€ç«çš„è´å¶æ–¯åé—®é¢˜æ±‚è§£å™¨ï¼ˆBIPSDAï¼‰æ¡†æ¶ï¼Œç”¨äºåŸºäºæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·ã€‚è¯¥æ¡†æ¶ç»Ÿä¸€äº†æœ€è¿‘æå‡ºçš„å‡ ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·ç®—æ³•ï¼Œå¹¶åŒ…å«äº†é€šè¿‡çµæ´»çš„è®¾è®¡é€‰æ‹©ç»„åˆå¯å®ç°çš„æ–°é¢–ç®—æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶å†…çš„ç®—æ³•åœ¨å…·æœ‰é«˜æ–¯æ··åˆå…ˆéªŒå’Œå—å›¾åƒä¿®å¤ã€Xå°„çº¿æ–­å±‚æ‰«æå’Œç›¸ä½æ£€ç´¢é—®é¢˜å¯å‘çš„ä¼¼ç„¶å‡½æ•°æ¨¡å‹é—®é¢˜ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚åœ¨è¿™ç§è®¾ç½®ä¸­ï¼Œå¯ä»¥è·å¾—è¿‘ä¼¼çœŸå®å€¼åæ ·æœ¬ï¼Œä»è€Œèƒ½å¤ŸæŒ‰ç…§åŸåˆ™è¯„ä¼°ç®—æ³•çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒBIPSDAç®—æ³•åœ¨å›¾åƒä¿®å¤å’ŒåŸºäºXå°„çº¿æ–­å±‚æ‰«æçš„é—®é¢˜ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè€Œå¯¹äºé‡‡æ ·å›°éš¾çš„ç›¸ä½æ£€ç´¢é—®é¢˜ï¼ˆå³ä½¿åœ¨å·²çŸ¥åéªŒå¯†åº¦çš„æƒ…å†µä¸‹ä¹Ÿå¾ˆéš¾é‡‡æ ·ï¼‰ï¼Œä»è¶…å‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·å™¨çš„èŒƒå›´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03007v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºæœ€å…ˆè¿›çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•å¤‡å—å…³æ³¨ï¼Œå…¶åœ¨è´å¶æ–¯åé—®é¢˜ä¸­ä½œä¸ºå…ˆéªŒçš„ä½¿ç”¨ä¹Ÿå¤‡å—ç©ç›®ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶â€”â€”é€šè¿‡æ‰©æ•£é€€ç«è§£å†³è´å¶æ–¯åé—®é¢˜ï¼ˆBIPSDAï¼‰ï¼Œç”¨äºåŸºäºæ‰©æ•£æ¨¡å‹çš„åç»­é‡‡æ ·ã€‚è¯¥æ¡†æ¶ç»Ÿä¸€äº†æœ€è¿‘æå‡ºçš„å‡ ç§æ‰©æ•£æ¨¡å‹åç»­é‡‡æ ·ç®—æ³•ï¼Œå¹¶åŒ…å«äº†å¯ä»¥é€šè¿‡çµæ´»ç»„åˆè®¾è®¡é€‰æ‹©å®ç°çš„æ–°ç®—æ³•ã€‚åœ¨å…·æœ‰é«˜æ–¯æ··åˆå…ˆéªŒå’Œå—å›¾åƒä¿®å¤ã€Xå°„çº¿æ–­å±‚æ‰«æå’Œç›¸ä½æ£€ç´¢é—®é¢˜å¯å‘çš„ä¼¼ç„¶å‡½æ•°çš„æ¨¡å‹é—®é¢˜ä¸Šæµ‹è¯•äº†æ¡†æ¶å†…çš„ç®—æ³•ã€‚ç»“æœè¯æ˜ï¼ŒBIPSDAç®—æ³•åœ¨å›¾åƒä¿®å¤å’ŒåŸºäºXå°„çº¿æ–­å±‚æ‰«æçš„é—®é¢˜ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè€Œå¯¹äºå³ä½¿å·²çŸ¥åéªŒå¯†åº¦ä¹Ÿéš¾ä»¥é‡‡æ ·çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„ç›¸ä½æ£€ç´¢é—®é¢˜ï¼Œä»å¤„äºæ‰©æ•£æ¨¡å‹é‡‡æ ·å™¨çš„èŒƒå›´ä¹‹å¤–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆå»ºæ¨¡æ–¹æ³•å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå¹¶åœ¨è´å¶æ–¯åé—®é¢˜ä¸­ä½œä¸ºå…ˆéªŒçš„ä½¿ç”¨æ—¥ç›Šé‡è¦ã€‚</li>
<li>å¼•å…¥æ–°çš„æ¡†æ¶BIPSDAï¼Œç”¨äºåŸºäºæ‰©æ•£æ¨¡å‹çš„åç»­é‡‡æ ·ï¼Œç»Ÿä¸€å¹¶æ‰©å±•äº†ç°æœ‰çš„æ‰©æ•£æ¨¡å‹é‡‡æ ·ç®—æ³•ã€‚</li>
<li>BIPSDAæ¡†æ¶å†…çš„ç®—æ³•åœ¨å…·æœ‰é«˜æ–¯æ··åˆå…ˆéªŒå’Œç‰¹å®šä¼¼ç„¶å‡½æ•°çš„æ¨¡å‹é—®é¢˜ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>æµ‹è¯•æ¶µç›–äº†å›¾åƒä¿®å¤ã€Xå°„çº¿æ–­å±‚æ‰«æå’Œç›¸ä½æ£€ç´¢ç­‰é¢†åŸŸçš„é—®é¢˜ã€‚</li>
<li>BIPSDAç®—æ³•åœ¨å›¾åƒä¿®å¤å’ŒXå°„çº¿æ–­å±‚æ‰«æé—®é¢˜ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>ç›¸ä½æ£€ç´¢é—®é¢˜å¯¹æ‰©æ•£æ¨¡å‹é‡‡æ ·å™¨æ¥è¯´ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>é€šè¿‡è¿‘ä¼¼åœ°é¢çœŸå®åéªŒæ ·æœ¬ï¼Œå¯ä»¥åŸåˆ™æ€§åœ°è¯„ä¼°ç®—æ³•æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5abfbbcaa66c0c0eb7e0b994e32a29c5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Straight-Line-Diffusion-Model-for-Efficient-3D-Molecular-Generation"><a href="#Straight-Line-Diffusion-Model-for-Efficient-3D-Molecular-Generation" class="headerlink" title="Straight-Line Diffusion Model for Efficient 3D Molecular Generation"></a>Straight-Line Diffusion Model for Efficient 3D Molecular Generation</h2><p><strong>Authors:Yuyan Ni, Shikun Feng, Haohan Chi, Bowen Zheng, Huan-ang Gao, Wei-Ying Ma, Zhi-Ming Ma, Yanyan Lan</strong></p>
<p>Diffusion-based models have shown great promise in molecular generation but often require a large number of sampling steps to generate valid samples. In this paper, we introduce a novel Straight-Line Diffusion Model (SLDM) to tackle this problem, by formulating the diffusion process to follow a linear trajectory. The proposed process aligns well with the noise sensitivity characteristic of molecular structures and uniformly distributes reconstruction effort across the generative process, thus enhancing learning efficiency and efficacy. Consequently, SLDM achieves state-of-the-art performance on 3D molecule generation benchmarks, delivering a 100-fold improvement in sampling efficiency. Furthermore, experiments on toy data and image generation tasks validate the generality and robustness of SLDM, showcasing its potential across diverse generative modeling domains. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„æ¨¡å‹åœ¨åˆ†å­ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤æ¥ç”Ÿæˆæœ‰æ•ˆçš„æ ·æœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç›´çº¿æ‰©æ•£æ¨¡å‹ï¼ˆSLDMï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡åˆ¶å®šéµå¾ªçº¿æ€§è½¨è¿¹çš„æ‰©æ•£è¿‡ç¨‹ã€‚æ‰€æå‡ºçš„è¿‡ç¨‹ä¸åˆ†å­ç»“æ„çš„å™ªå£°æ•æ„Ÿæ€§ç‰¹å¾ç›¸å»åˆï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡åŒ€åˆ†é…é‡å»ºå·¥ä½œï¼Œä»è€Œæé«˜äº†å­¦ä¹ æ•ˆç‡å’Œå­¦ä¹ æ•ˆæœã€‚å› æ­¤ï¼ŒSLDMåœ¨3Dåˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé‡‡æ ·æ•ˆç‡æé«˜äº†100å€ã€‚æ­¤å¤–ï¼Œå¯¹ç©å…·æ•°æ®å’Œå›¾åƒç”Ÿæˆä»»åŠ¡çš„å®éªŒéªŒè¯äº†SLDMçš„é€šç”¨æ€§å’Œç¨³å¥æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸åŒç”Ÿæˆå»ºæ¨¡é¢†åŸŸçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02918v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç›´çº¿æ‰©æ•£æ¨¡å‹ï¼ˆSLDMï¼‰ï¼Œé€šè¿‡åˆ¶å®šçº¿æ€§è½¨è¿¹çš„æ‰©æ•£è¿‡ç¨‹ï¼Œè§£å†³äº†åˆ†å­ç”Ÿæˆä¸­éœ€è¦å¤§é‡é‡‡æ ·æ­¥éª¤çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹å¯¹é½å™ªå£°æ•æ„Ÿæ€§ç‰¹å¾ï¼Œå‡åŒ€åˆ†å¸ƒé‡å»ºåŠªåŠ›ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ä¸æ•ˆæœï¼Œå®ç°äº†åœ¨ä¸‰ç»´åˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„å“è¶Šæ€§èƒ½ï¼Œé‡‡æ ·æ•ˆç‡æé«˜äº†100å€ã€‚åŒæ—¶ï¼Œåœ¨ç©å…·æ•°æ®å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒéªŒè¯äº†å…¶é€šç”¨æ€§å’Œç¨³å¥æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸åŒç”Ÿæˆå»ºæ¨¡åŸŸä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–°å‹ç›´çº¿æ‰©æ•£æ¨¡å‹ï¼ˆSLDMï¼‰è¢«æå‡ºä»¥è§£å†³åˆ†å­ç”Ÿæˆä¸­çš„é‡‡æ ·é—®é¢˜ã€‚</li>
<li>SLDMé€šè¿‡åˆ¶å®šçº¿æ€§è½¨è¿¹çš„æ‰©æ•£è¿‡ç¨‹ï¼Œæé«˜äº†é‡‡æ ·æ•ˆç‡ã€‚</li>
<li>SLDMå¯¹é½å™ªå£°æ•æ„Ÿæ€§ç‰¹å¾ï¼Œä½¿å¾—æ¨¡å‹æ›´åŠ é€‚åº”åˆ†å­ç»“æ„çš„ç‰¹ç‚¹ã€‚</li>
<li>SLDMåœ¨ä¸‰ç»´åˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSLDMçš„é‡‡æ ·æ•ˆç‡æé«˜äº†100å€ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSLDMåœ¨ç©å…·æ•°æ®å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå…·æœ‰é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6244166971982348b013bf51316e92db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cdf2f94d9e4e686b353839c57909164.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7abd81e80d65a9fce2cc0c6b578781a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6e52a2b225dd2a9fc9f378fda411d3d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="StdGEN-Semantic-Decomposed-3D-Character-Generation-from-Single-Images"><a href="#StdGEN-Semantic-Decomposed-3D-Character-Generation-from-Single-Images" class="headerlink" title="StdGEN: Semantic-Decomposed 3D Character Generation from Single Images"></a>StdGEN: Semantic-Decomposed 3D Character Generation from Single Images</h2><p><strong>Authors:Yuze He, Yanning Zhou, Wang Zhao, Zhongkai Wu, Kaiwen Xiao, Wei Yang, Yong-Jin Liu, Xiao Han</strong></p>
<p>We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: <a target="_blank" rel="noopener" href="https://stdgen.github.io/">https://stdgen.github.io</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†StdGENï¼Œè¿™æ˜¯ä¸€ä¸ªä»å•å¼ å›¾åƒç”Ÿæˆè¯­ä¹‰åˆ†è§£çš„é«˜è´¨é‡3Dè§’è‰²çš„åˆ›æ–°ç®¡é“ï¼Œå¹¿æ³›åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆåˆ¶ä½œã€ç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚ä¸ä»¥å¾€æ–¹æ³•åœ¨åˆ†è§£æ€§ã€è´¨é‡ã€ä¼˜åŒ–æ—¶é—´ç­‰æ–¹é¢çš„å±€é™ä¸åŒï¼ŒStdGENå…·æœ‰åˆ†è§£æ€§ã€æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆå…·æœ‰åˆ†ç¦»è¯­ä¹‰ç»„ä»¶çš„ç²¾ç»†è¯¦ç»†çš„3Dè§’è‰²ï¼Œå¦‚èº«ä½“ã€è¡£ç‰©å’Œå¤´å‘ã€‚StdGENçš„æ ¸å¿ƒæ˜¯æˆ‘ä»¬æå‡ºçš„å¤§å‹è¯­ä¹‰æ„ŸçŸ¥é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„å¯æ¨å¹¿æ¨¡å‹ï¼Œä»¥å‰é¦ˆæ–¹å¼ä»å¤šè§†è§’å›¾åƒé‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚æˆ‘ä»¬å¼•å…¥äº†å¯å¾®å¤šå±‚è¯­ä¹‰è¡¨é¢æå–æ–¹æ¡ˆï¼Œä»ç”±S-LRMé‡å»ºçš„æ··åˆéšå¼å­—æ®µä¸­è·å–ç½‘æ ¼ã€‚æ­¤å¤–ï¼Œä¸€ä¸ªä¸“é—¨çš„é«˜æ•ˆå¤šè§†è§’æ‰©æ•£æ¨¡å‹å’Œè¿­ä»£å¤šå±‚è¡¨é¢ç»†åŒ–æ¨¡å—è¢«é›†æˆåˆ°ç®¡é“ä¸­ï¼Œä»¥ä¿ƒè¿›é«˜è´¨é‡ã€å¯åˆ†è§£çš„3Dè§’è‰²ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨3DåŠ¨ç”»è§’è‰²ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨å‡ ä½•ã€çº¹ç†å’Œåˆ†è§£æ€§æ–¹é¢å¤§å¤§è¶…è¿‡äº†ç°æœ‰åŸºå‡†ã€‚StdGENæä¾›å³æ’å³ç”¨çš„è¯­ä¹‰åˆ†è§£çš„3Dè§’è‰²ï¼Œå¹¶ä¸ºå¹¿æ³›çš„åº”ç”¨æä¾›çµæ´»çš„å®šåˆ¶ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://stdgen.github.io/">https://stdgen.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05738v2">PDF</a> CVPR 2025. 13 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†StdGENï¼Œè¿™æ˜¯ä¸€ä¸ªä»å•å¼ å›¾åƒç”Ÿæˆè¯­ä¹‰åˆ†è§£çš„é«˜è´¨é‡3Dè§’è‰²çš„åˆ›æ–°ç®¡é“ã€‚StdGENå…·æœ‰åˆ†è§£æ€§ã€æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰åˆ†ç¦»è¯­ä¹‰ç»„ä»¶çš„ç²¾ç»†3Dè§’è‰²ï¼Œå¦‚èº«ä½“ã€è¡£ç‰©å’Œå¤´å‘ã€‚å…¶æ ¸å¿ƒæ˜¯è¯­ä¹‰æ„ŸçŸ¥å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œè¯¥æ¨¡å‹ä»å¤šè§†è§’å›¾åƒä»¥å‰é¦ˆæ–¹å¼è”åˆé‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚å®éªŒè¡¨æ˜ï¼ŒStdGENåœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å‡ ä½•ã€çº¹ç†å’Œåˆ†è§£æ€§æ–¹é¢å¤§å¤§è¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StdGENæ˜¯ä¸€ä¸ªç”Ÿæˆè¯­ä¹‰åˆ†è§£çš„é«˜è´¨é‡3Dè§’è‰²çš„åˆ›æ–°ç®¡é“ã€‚</li>
<li>å®ƒèƒ½å¤Ÿä»å•å¼ å›¾åƒç”Ÿæˆå…·æœ‰ç²¾ç»†ç»†èŠ‚çš„3Dè§’è‰²ï¼Œå¹¶åˆ†ç¦»å‡ºè¯­ä¹‰ç»„ä»¶ï¼ˆå¦‚èº«ä½“ã€è¡£ç‰©å’Œå¤´å‘ï¼‰ã€‚</li>
<li>StdGENçš„æ ¸å¿ƒæ˜¯è¯­ä¹‰æ„ŸçŸ¥å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»å‰é¦ˆæ–¹å¼è”åˆé‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¯å¾®å¤šå±‚è¯­ä¹‰è¡¨é¢æå–æ–¹æ¡ˆï¼Œä»ç”±S-LRMé‡å»ºçš„æ··åˆéšå¼å­—æ®µä¸­è·å–ç½‘æ ¼ã€‚</li>
<li>StdGENé›†æˆäº†é«˜æ•ˆçš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹å’Œè¿­ä»£å¤šå±‚è¡¨é¢ç»†åŒ–æ¨¡å—ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡ã€å¯åˆ†è§£çš„3Dè§’è‰²ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒStdGENåœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å‡ ä½•ã€çº¹ç†å’Œåˆ†è§£æ€§æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-09334c41684c8d46c798420aded9a297.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-470d7fc459674e99c138abea41c04642.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0330782444ab3c9e8ad27a78dedc2c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6368e5d4c3006fd8872de5e9561d681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1b02df159a97a9771e6c1fdd1bdd3e7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior"><a href="#LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior" class="headerlink" title="LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior"></a>LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior</h2><p><strong>Authors:Xingjian Tang, Jingwei Guan, Linge Li, Ran Shi, Youmei Zhang, Mengye Lyu, Li Yan</strong></p>
<p>Diffusion models, as powerful generative models, have found a wide range of applications and shown great potential in solving image reconstruction problems. Some works attempted to solve MRI reconstruction with diffusion models, but these methods operate directly in pixel space, leading to higher computational costs for optimization and inference. Latent diffusion models, pre-trained on natural images with rich visual priors, are expected to solve the high computational cost problem in MRI reconstruction by operating in a lower-dimensional latent space. However, direct application to MRI reconstruction faces three key challenges: (1) absence of explicit control mechanisms for medical fidelity, (2) domain gap between natural images and MR physics, and (3) undefined data consistency in latent space. To address these challenges, a novel Latent Diffusion Prior-based undersampled MRI reconstruction (LDPM) method is proposed. Our LDPM framework addresses these challenges by: (1) a sketch-guided pipeline with a two-step reconstruction strategy, which balances perceptual quality and anatomical fidelity, (2) an MRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92 dB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE \cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM sampler, which enforces high-fidelity reconstruction in the latent space. Experiments on the fastMRI dataset\cite{fastmri} demonstrate the state-of-the-art performance of the proposed method and its robustness across various scenarios. The effectiveness of each module is also verified through ablation experiments. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å›¾åƒé‡å»ºé—®é¢˜è§£å†³æ–¹æ¡ˆä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨å¹¶è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ä¸€äº›ç ”ç©¶å°è¯•ä½¿ç”¨æ‰©æ•£æ¨¡å‹è§£å†³MRIé‡å»ºé—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨åƒç´ ç©ºé—´ç›´æ¥æ“ä½œï¼Œå¯¼è‡´ä¼˜åŒ–å’Œæ¨ç†çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚é¢„è®­ç»ƒåœ¨è‡ªç„¶å›¾åƒä¸Šçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å…·æœ‰ä¸°å¯Œçš„è§†è§‰å…ˆéªŒä¿¡æ¯ï¼Œæœ‰æœ›é€šè¿‡é™ä½ç»´åº¦çš„æ½œåœ¨ç©ºé—´è§£å†³MRIé‡å»ºä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š(1)ç¼ºä¹åŒ»ç–—å‡†ç¡®æ€§çš„æ˜ç¡®æ§åˆ¶æœºåˆ¶ï¼›(2)è‡ªç„¶å›¾åƒä¸MRç‰©ç†ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼›(3)æ½œåœ¨ç©ºé—´ä¸­çš„æ•°æ®ä¸€è‡´æ€§æœªå®šä¹‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£å…ˆéªŒçš„æ¬ é‡‡æ ·MRIé‡å»ºï¼ˆLDPMï¼‰æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„LDPMæ¡†æ¶é€šè¿‡ä»¥ä¸‹æ–¹å¼åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼š(1)ä¸€ä¸ªè‰å›¾å¼•å¯¼çš„æµç¨‹ï¼Œé‡‡ç”¨ä¸¤æ­¥é‡å»ºç­–ç•¥ï¼Œå¹³è¡¡æ„ŸçŸ¥è´¨é‡å’Œè§£å‰–ç²¾åº¦ï¼›(2)ä¸€ä¸ªé’ˆå¯¹MRIä¼˜åŒ–çš„VAEï¼ˆMR-VAEï¼‰ï¼Œä¸SD-VAEç›¸æ¯”ï¼Œå®ƒåœ¨æ¬ é‡‡æ ·MRIé‡å»ºçš„PSNRä¸Šæé«˜äº†çº¦3.92åˆ†è´ã€sdã€‘ï¼›(3)åŒé˜¶æ®µé‡‡æ ·å™¨ï¼Œè¿™æ˜¯é—´éš”DDPMé‡‡æ ·å™¨çš„æ”¹è¿›ç‰ˆï¼Œå®ƒåœ¨æ½œåœ¨ç©ºé—´ä¸­å¼ºåˆ¶è¿›è¡Œé«˜ä¿çœŸé‡å»ºã€‚åœ¨fastMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„å…ˆè¿›æ€§èƒ½åŠå…¶åœ¨ä¸åŒåœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€fastmriã€‘ã€‚æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ä¹Ÿé€šè¿‡æ¶ˆèå®éªŒå¾—åˆ°äº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02951v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å›¾åƒé‡å»ºé—®é¢˜ä¸­å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚å°½ç®¡å·²æœ‰ä¸€äº›å·¥ä½œå°è¯•ç”¨æ‰©æ•£æ¨¡å‹è§£å†³MRIé‡å»ºé—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨åƒç´ ç©ºé—´ç›´æ¥æ“ä½œå¯¼è‡´äº†è¾ƒé«˜çš„ä¼˜åŒ–å’Œæ¨ç†è®¡ç®—æˆæœ¬ã€‚é¢„æœŸé€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆåœ¨å…·æœ‰ä¸°å¯Œè§†è§‰å…ˆéªŒçš„è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œå°†å…¶ç›´æ¥åº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£å…ˆéªŒçš„æ¬ é‡‡æ ·MRIé‡å»ºï¼ˆLDPMï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„LDPMæ¡†æ¶é€šè¿‡ä»¥ä¸‹æ–¹å¼åº”å¯¹æŒ‘æˆ˜ï¼š1ï¼‰ä¸€ä¸ªå¸¦æœ‰ä¸¤æ­¥é‡å»ºç­–ç•¥çš„è‰å›¾å¼•å¯¼ç®¡é“ï¼Œå¹³è¡¡æ„ŸçŸ¥è´¨é‡å’Œè§£å‰–ä¿çœŸåº¦ï¼›2ï¼‰ä¸€ä¸ªé’ˆå¯¹MRIä¼˜åŒ–çš„VAEï¼ˆMR-VAEï¼‰ï¼Œåœ¨æ¬ é‡‡æ ·MRIé‡å»ºçš„PSNRä¸Šæé«˜äº†çº¦3.92 dBï¼›3ï¼‰åŒé˜¶æ®µé‡‡æ ·å™¨ï¼Œä¸€ä¸ªæ”¹è¿›çš„ç©ºé—´DDPMé‡‡æ ·å™¨ç‰ˆæœ¬ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­å¼ºåˆ¶é«˜ä¿çœŸé‡å»ºã€‚åœ¨fastMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æ‰€ææ–¹æ³•çš„å…ˆè¿›æ€§èƒ½å’Œåœ¨å„ç§åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ä¹Ÿé€šè¿‡æ¶ˆèå®éªŒå¾—åˆ°äº†éªŒè¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³å›¾åƒé‡å»ºé—®é¢˜ä¸­å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ï¼ŒåŒ…æ‹¬MRIé‡å»ºã€‚</li>
<li>ç›´æ¥åœ¨åƒç´ ç©ºé—´æ“ä½œå¯¼è‡´è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼Œè€Œæ½œåœ¨æ‰©æ•£æ¨¡å‹æœ‰æœ›åœ¨è¾ƒä½ç»´åº¦çš„æ½œåœ¨ç©ºé—´ä¸­è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>å°†æ½œåœ¨æ‰©æ•£æ¨¡å‹ç›´æ¥åº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåŒ»å­¦ä¿çœŸåº¦çš„ç¼ºä¹æ˜ç¡®æ§åˆ¶æœºåˆ¶ã€è‡ªç„¶å›¾åƒä¸MRç‰©ç†ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä»¥åŠæ½œåœ¨ç©ºé—´ä¸­çš„æ•°æ®ä¸€è‡´æ€§æœªå®šä¹‰ã€‚</li>
<li>æå‡ºçš„LDPMæ–¹æ³•é€šè¿‡è‰å›¾å¼•å¯¼ç®¡é“å’Œä¸¤æ­¥é‡å»ºç­–ç•¥å¹³è¡¡æ„ŸçŸ¥è´¨é‡ä¸è§£å‰–ä¿çœŸåº¦ã€‚</li>
<li>MR-VAEåœ¨æ¬ é‡‡æ ·MRIé‡å»ºçš„PSNRä¸Šæé«˜äº†çº¦3.92 dBã€‚</li>
<li>åŒé˜¶æ®µé‡‡æ ·å™¨æ˜¯ä¸€ç§æ”¹è¿›çš„ç©ºé—´DDPMé‡‡æ ·å™¨ç‰ˆæœ¬ï¼Œå®ƒåœ¨æ½œåœ¨ç©ºé—´ä¸­å¼ºåˆ¶æ‰§è¡Œé«˜ä¿çœŸé‡å»ºã€‚</li>
<li>åœ¨fastMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æ‰€ææ–¹æ³•å…·æœ‰å…ˆè¿›æ€§èƒ½å’Œåœ¨å„ç§åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e22c7fa8d8fb4f5ff7f28d3dda1da45e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5789dfc0f7479c776ca018ae0a47c6e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cdb7f71b9ee820a4008700bf2bd88c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ac9a4d23757bfcf5918478f08c6c34a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0251eac9ec15658da969639324cfbb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7d307b2c4d7b4c57391a62c837dee76.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="An-Undetectable-Watermark-for-Generative-Image-Models"><a href="#An-Undetectable-Watermark-for-Generative-Image-Models" class="headerlink" title="An Undetectable Watermark for Generative Image Models"></a>An Undetectable Watermark for Generative Image Models</h2><p><strong>Authors:Sam Gunn, Xuandong Zhao, Dawn Song</strong></p>
<p>We present the first undetectable watermarking scheme for generative image models. Undetectability ensures that no efficient adversary can distinguish between watermarked and un-watermarked images, even after making many adaptive queries. In particular, an undetectable watermark does not degrade image quality under any efficiently computable metric. Our scheme works by selecting the initial latents of a diffusion model using a pseudorandom error-correcting code (Christ and Gunn, 2024), a strategy which guarantees undetectability and robustness. We experimentally demonstrate that our watermarks are quality-preserving and robust using Stable Diffusion 2.1. Our experiments verify that, in contrast to every prior scheme we tested, our watermark does not degrade image quality. Our experiments also demonstrate robustness: existing watermark removal attacks fail to remove our watermark from images without significantly degrading the quality of the images. Finally, we find that we can robustly encode 512 bits in our watermark, and up to 2500 bits when the images are not subjected to watermark removal attacks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/PRC-Watermark">https://github.com/XuandongZhao/PRC-Watermark</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºç”Ÿæˆå›¾åƒæ¨¡å‹æå‡ºäº†ç¬¬ä¸€ä¸ªä¸å¯æ£€æµ‹çš„æ°´å°æ–¹æ¡ˆã€‚ä¸å¯æ£€æµ‹æ€§ç¡®ä¿é«˜æ•ˆçš„å¯¹æ‰‹å³ä½¿åœ¨æ‰§è¡Œå¤šæ¬¡è‡ªé€‚åº”æŸ¥è¯¢åï¼Œä¹Ÿæ— æ³•åŒºåˆ†å¸¦æ°´å°å’Œä¸å¸¦æ°´å°çš„å›¾åƒã€‚ç‰¹åˆ«æ˜¯ï¼Œä¸å¯æ£€æµ‹çš„æ°´å°åœ¨ä»»ä½•å¯é«˜æ•ˆè®¡ç®—çš„æŒ‡æ ‡ä¸‹éƒ½ä¸ä¼šé™ä½å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆé€šè¿‡é€‰æ‹©ä¸€ä¸ªæ‰©æ•£æ¨¡å‹çš„åˆå§‹æ½œåœ¨å˜é‡æ¥å®ç°ï¼Œè¯¥é€‰æ‹©é‡‡ç”¨ä¼ªéšæœºçº é”™ç ï¼ˆChrist and Gunn, 2024ï¼‰çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¿è¯äº†ä¸å¯æ£€æµ‹æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜äº†æ°´å°åœ¨ä¿æŒè´¨é‡å’Œç¨³å¥æ€§æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä½¿ç”¨çš„æ˜¯Stable Diffusion 2.1ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬æµ‹è¯•è¿‡çš„æ‰€æœ‰å…ˆå‰æ–¹æ¡ˆç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ°´å°ä¸ä¼šé™ä½å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜è¯æ˜äº†ç¨³å¥æ€§ï¼šç°æœ‰çš„æ°´å°ç§»é™¤æ”»å‡»æ— æ³•åœ¨æˆ‘ä»¬çš„æ°´å°ä»å›¾åƒä¸­ç§»é™¤çš„åŒæ—¶è€Œä¸æ˜¾è‘—é™ä½å›¾åƒè´¨é‡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬å¯ä»¥ç¨³å¥åœ°åœ¨æˆ‘ä»¬çš„æ°´å°ä¸­ç¼–ç 512ä½ï¼Œå½“å›¾åƒæœªå—åˆ°æ°´å°ç§»é™¤æ”»å‡»æ—¶ï¼Œç”šè‡³å¯ä»¥ç¼–ç é«˜è¾¾2500ä½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/PRC-Watermark%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XuandongZhao/PRC-Watermarkæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07369v3">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºé¦–ä¸ªé’ˆå¯¹ç”Ÿæˆå›¾åƒæ¨¡å‹çš„ä¸å¯æ£€æµ‹æ°´å°æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨ä¼ªéšæœºçº é”™ç é€‰æ‹©æ‰©æ•£æ¨¡å‹çš„åˆå§‹æ½œå˜é‡ï¼Œç¡®ä¿æ°´å°çš„ä¸å¯æ£€æµ‹æ€§å’Œç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ°´å°ä¸ä¼šé™ä½å›¾åƒè´¨é‡ï¼Œå¯¹ç°æœ‰çš„æ°´å°ç§»é™¤æ”»å‡»å…·æœ‰å¼ºå¤§çš„æŠµå¾¡èƒ½åŠ›ã€‚è¯¥æ–¹æ¡ˆå¯å°†æ°´å°åµŒå…¥åˆ°å›¾åƒä¸­è€Œä¸ä¼šæ˜æ˜¾å½±å“å›¾åƒè´¨é‡ï¼Œå¹¶å…·æœ‰é«˜è¾¾512ä½çš„ç¨³å¥ç¼–ç èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºé¦–ä¸ªé’ˆå¯¹ç”Ÿæˆå›¾åƒæ¨¡å‹çš„ä¸å¯æ£€æµ‹æ°´å°æ–¹æ¡ˆã€‚</li>
<li>åˆ©ç”¨ä¼ªéšæœºçº é”™ç é€‰æ‹©æ‰©æ•£æ¨¡å‹çš„åˆå§‹æ½œå˜é‡ä»¥ç¡®ä¿æ°´å°çš„ä¸å¯æ£€æµ‹æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>å®éªŒè¯æ˜æ°´å°ä¸ä¼šé™ä½å›¾åƒè´¨é‡ã€‚</li>
<li>å¯¹ç°æœ‰çš„æ°´å°ç§»é™¤æ”»å‡»å…·æœ‰å¼ºå¤§çš„æŠµå¾¡èƒ½åŠ›ã€‚</li>
<li>æ°´å°å¯ä»¥åµŒå…¥åˆ°å›¾åƒä¸­è€Œä¸ä¼šæ˜æ˜¾å½±å“å›¾åƒè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ¡ˆå…·æœ‰é«˜è¾¾512ä½çš„ç¨³å¥ç¼–ç èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee4e523f52cef8c3e74318fa56b5f88c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diffusion-State-Guided-Projected-Gradient-for-Inverse-Problems"><a href="#Diffusion-State-Guided-Projected-Gradient-for-Inverse-Problems" class="headerlink" title="Diffusion State-Guided Projected Gradient for Inverse Problems"></a>Diffusion State-Guided Projected Gradient for Inverse Problems</h2><p><strong>Authors:Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar</strong></p>
<p>Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Anima-Lab/DiffStateGrad">https://github.com/Anima-Lab/DiffStateGrad</a>. </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹çš„å‘å±•åœ¨è§£å†³åé—®é¢˜æ–¹é¢å­¦ä¹ æ•°æ®å…ˆéªŒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚ä»–ä»¬åˆ©ç”¨æ‰©æ•£é‡‡æ ·æ­¥éª¤æ¥è¯±å¯¼æ•°æ®å…ˆéªŒï¼ŒåŒæ—¶åœ¨æ¯ä¸€æ­¥ä½¿ç”¨æµ‹é‡æŒ‡å¯¼æ¢¯åº¦æ¥æ–½åŠ æ•°æ®ä¸€è‡´æ€§ã€‚å¯¹äºä¸€èˆ¬åé—®é¢˜ï¼Œå½“ä½¿ç”¨æ— æ¡ä»¶è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ—¶ï¼Œç”±äºæµ‹é‡ä¼¼ç„¶æ€§éš¾ä»¥å¤„ç†ï¼Œå¯¼è‡´åéªŒé‡‡æ ·ä¸å‡†ç¡®ã€‚æ¢å¥è¯è¯´ï¼Œç”±äºè¿™äº›è¿‘ä¼¼æ–¹æ³•ï¼Œå®ƒä»¬æ— æ³•ä¿ç•™ç”±æ‰©æ•£å…ˆéªŒå®šä¹‰çš„æ•°æ®æµå½¢ä¸Šçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œå¯¼è‡´å›¾åƒæ¢å¤ç­‰åº”ç”¨ç¨‹åºä¸­å‡ºç°ä¼ªå½±ã€‚ä¸ºäº†æé«˜æ‰©æ•£æ¨¡å‹è§£å†³åé—®é¢˜çš„æ€§èƒ½å’Œç¨³å¥æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£çŠ¶æ€å¼•å¯¼æŠ•å½±æ¢¯åº¦ï¼ˆDiffStateGradï¼‰ï¼Œå®ƒå°†æµ‹é‡æ¢¯åº¦æŠ•å½±åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­é—´çŠ¶æ€çš„ä½ç§©è¿‘ä¼¼å­ç©ºé—´ä¸Šã€‚ä½œä¸ºä¸€ä¸ªæ¨¡å—ï¼ŒDiffStateGradå¯ä»¥æ·»åŠ åˆ°å¹¿æ³›çš„åŸºäºæ‰©æ•£çš„åå‘æ±‚è§£å™¨ä¸­ï¼Œä»¥æé«˜æ‰©æ•£è¿‡ç¨‹åœ¨å…ˆéªŒæµå½¢ä¸Šçš„ä¿ç•™èƒ½åŠ›ï¼Œå¹¶è¿‡æ»¤æ‰äº§ç”Ÿä¼ªå½±çš„ç»„ä»¶ã€‚æˆ‘ä»¬å¼ºè°ƒï¼ŒDiffStateGradæé«˜äº†æ‰©æ•£æ¨¡å‹åœ¨é€‰æ‹©æµ‹é‡æŒ‡å¯¼æ­¥é•¿å’Œå™ªå£°æ–¹é¢çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶æé«˜äº†æœ€åæƒ…å†µä¸‹çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†DiffStateGradåœ¨è§£å†³çº¿æ€§å’Œéçº¿æ€§å›¾åƒæ¢å¤åé—®é¢˜ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Anima-Lab/DiffStateGrad%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Anima-Lab/DiffStateGradä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03463v4">PDF</a> Published as a conference paper at ICLR 2025. RZ and BT have equal   contributions</p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜æ–¹é¢çš„è¿›å±•æœ‰æ•ˆå­¦ä¹ äº†æ•°æ®å…ˆéªŒã€‚å®ƒä»¬åˆ©ç”¨æ‰©æ•£é‡‡æ ·æ­¥éª¤æ¥è¯±å¯¼æ•°æ®å…ˆéªŒï¼ŒåŒæ—¶ä½¿ç”¨æµ‹é‡æŒ‡å¯¼æ¢¯åº¦æ¥æ–½åŠ æ•°æ®ä¸€è‡´æ€§ã€‚å¯¹äºä¸€èˆ¬åé—®é¢˜ï¼Œå½“ä½¿ç”¨æ— æ¡ä»¶è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ—¶ï¼Œç”±äºæµ‹é‡ä¼¼ç„¶ä¸å¯è¡Œï¼Œéœ€è¦è¿‘ä¼¼å¤„ç†ï¼Œå¯¼è‡´åéªŒé‡‡æ ·ä¸å‡†ç¡®ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†æ‰©æ•£çŠ¶æ€å¼•å¯¼æŠ•å½±æ¢¯åº¦ï¼ˆDiffStateGradï¼‰ï¼Œå®ƒå°†æµ‹é‡æ¢¯åº¦æŠ•å½±åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­é—´çŠ¶æ€çš„ä½ç§©è¿‘ä¼¼å­ç©ºé—´ä¸Šã€‚DiffStateGradå¯ä½œä¸ºæ¨¡å—æ·»åŠ åˆ°å„ç§åŸºäºæ‰©æ•£çš„åé—®é¢˜æ±‚è§£å™¨ä¸­ï¼Œæé«˜äº†æ‰©æ•£è¿‡ç¨‹åœ¨å…ˆéªŒæµå½¢ä¸Šçš„ä¿æŒèƒ½åŠ›ï¼Œå¹¶è¿‡æ»¤æ‰äº†äº§ç”Ÿä¼ªå½±çš„ç»„ä»¶ã€‚å®ƒæé«˜äº†æ‰©æ•£æ¨¡å‹åœ¨é€‰æ‹©æµ‹é‡æŒ‡å¯¼æ­¥é•¿å’Œå™ªå£°æ–¹é¢çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨çº¿æ€§å’Œéçº¿æ€§å›¾åƒæ¢å¤åé—®é¢˜ä¸Šå–å¾—äº†å…ˆè¿›çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³åé—®é¢˜ä¸Šå–å¾—æ–°è¿›å±•ï¼Œé€šè¿‡æ‰©æ•£é‡‡æ ·æ­¥éª¤å­¦ä¹ æ•°æ®å…ˆéªŒå¹¶åˆ©ç”¨æµ‹é‡æŒ‡å¯¼æ¢¯åº¦æ–½åŠ æ•°æ®ä¸€è‡´æ€§ã€‚</li>
<li>å¯¹äºä¸€èˆ¬åé—®é¢˜ï¼Œä½¿ç”¨æ— æ¡ä»¶è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ—¶éœ€è¦è¿›è¡Œè¿‘ä¼¼å¤„ç†ï¼Œå¯¼è‡´åéªŒé‡‡æ ·ä¸å‡†ç¡®å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¤±çœŸã€‚</li>
<li>æå‡ºDiffStateGradæ–¹æ³•ï¼Œé€šè¿‡å°†æµ‹é‡æ¢¯åº¦æŠ•å½±åˆ°æ‰©æ•£è¿‡ç¨‹çš„ä½ç§©è¿‘ä¼¼å­ç©ºé—´ä¸Šï¼Œæé«˜æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>DiffStateGradå¯ä½œä¸ºæ¨¡å—æ·»åŠ åˆ°å„ç§åŸºäºæ‰©æ•£çš„åé—®é¢˜æ±‚è§£å™¨ä¸­ï¼Œæ”¹å–„æ‰©æ•£è¿‡ç¨‹åœ¨å…ˆéªŒæµå½¢ä¸Šçš„ä¿æŒèƒ½åŠ›å¹¶è¿‡æ»¤ä¼ªå½±æˆåˆ†ã€‚</li>
<li>DiffStateGradæé«˜äº†æ‰©æ•£æ¨¡å‹åœ¨é€‰æ‹©æµ‹é‡æŒ‡å¯¼æ­¥é•¿å’Œå™ªå£°æ–¹é¢çš„ç¨³å¥æ€§ã€‚</li>
<li>DiffStateGradåœ¨å›¾åƒæ¢å¤ç­‰åº”ç”¨é¢†åŸŸçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e376ce6d1653471be6f7bf6d02d1e548.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ae0b5590baf824e5ae43aac9c99a51a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80d50ea2301487db0de96f84525934bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d73a08f0b1ddc496f7fb4e752bfdf443.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d56557d6bd577bafba3c251e3160f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5ff6bc79d9e77536d65330d42581135.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Counting-Guidance-for-High-Fidelity-Text-to-Image-Synthesis"><a href="#Counting-Guidance-for-High-Fidelity-Text-to-Image-Synthesis" class="headerlink" title="Counting Guidance for High Fidelity Text-to-Image Synthesis"></a>Counting Guidance for High Fidelity Text-to-Image Synthesis</h2><p><strong>Authors:Wonjun Kang, Kevin Galim, Hyung Il Koo, Nam Ik Cho</strong></p>
<p>Recently, there have been significant improvements in the quality and performance of text-to-image generation, largely due to the impressive results attained by diffusion models. However, text-to-image diffusion models sometimes struggle to create high-fidelity content for the given input prompt. One specific issue is their difficulty in generating the precise number of objects specified in the text prompt. For example, when provided with the prompt â€œfive apples and ten lemons on a table,â€ images generated by diffusion models often contain an incorrect number of objects. In this paper, we present a method to improve diffusion models so that they accurately produce the correct object count based on the input prompt. We adopt a counting network that performs reference-less class-agnostic counting for any given image. We calculate the gradients of the counting network and refine the predicted noise for each step. To address the presence of multiple types of objects in the prompt, we utilize novel attention map guidance to obtain high-quality masks for each object. Finally, we guide the denoising process using the calculated gradients for each object. Through extensive experiments and evaluation, we demonstrate that the proposed method significantly enhances the fidelity of diffusion models with respect to object count. Code is available at <a target="_blank" rel="noopener" href="https://github.com/furiosa-ai/counting-guidance">https://github.com/furiosa-ai/counting-guidance</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºæ‰©æ•£æ¨¡å‹å–å¾—çš„ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹æœ‰æ—¶åœ¨ç”Ÿæˆç»™å®šè¾“å…¥æç¤ºçš„é«˜ä¿çœŸå†…å®¹æ—¶é‡åˆ°å›°éš¾ã€‚ä¸€ä¸ªå…·ä½“çš„é—®é¢˜æ˜¯å®ƒä»¬åœ¨ç”Ÿæˆæ–‡æœ¬æç¤ºä¸­æŒ‡å®šçš„ç²¾ç¡®æ•°é‡çš„å¯¹è±¡æ—¶é‡åˆ°å›°éš¾ã€‚ä¾‹å¦‚ï¼Œå½“æä¾›â€œæ¡Œå­ä¸Šæ”¾ç€äº”ä¸ªè‹¹æœå’Œåä¸ªæŸ æª¬â€çš„æç¤ºæ—¶ï¼Œç”±æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒé€šå¸¸åŒ…å«ä¸æ­£ç¡®æ•°é‡çš„å¯¹è±¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿å®ƒä»¬èƒ½å¤ŸåŸºäºè¾“å…¥æç¤ºå‡†ç¡®äº§ç”Ÿæ­£ç¡®çš„å¯¹è±¡è®¡æ•°ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªè®¡æ•°ç½‘ç»œï¼Œè¯¥ç½‘ç»œå¯ä»¥å¯¹ä»»ä½•ç»™å®šå›¾åƒæ‰§è¡Œæ— å‚è€ƒçš„ç±»æ— å…³è®¡æ•°ã€‚æˆ‘ä»¬è®¡ç®—è®¡æ•°ç½‘ç»œçš„æ¢¯åº¦ï¼Œå¹¶ç»†åŒ–æ¯ä¸€æ­¥é¢„æµ‹çš„å™ªå£°ã€‚ä¸ºäº†è§£å†³æç¤ºä¸­å­˜åœ¨å¤šç§ç±»å‹å¯¹è±¡çš„é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨æ–°å‹æ³¨æ„åŠ›å›¾å¼•å¯¼æ¥è·å¾—æ¯ä¸ªå¯¹è±¡çš„é«˜è´¨é‡æ©æ¨¡ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ºæ¯ä¸ªå¯¹è±¡è®¡ç®—çš„æ¢¯åº¦æ¥å¼•å¯¼å»å™ªè¿‡ç¨‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜æ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹åœ¨å¯¹è±¡è®¡æ•°æ–¹é¢çš„ä¿çœŸåº¦ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/furiosa-ai/counting-guidance%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/furiosa-ai/counting-guidanceæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.17567v3">PDF</a> Accepted at WACV 2025 (Oral). Code is available at   <a target="_blank" rel="noopener" href="https://github.com/furiosa-ai/counting-guidance">https://github.com/furiosa-ai/counting-guidance</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•é€šè¿‡æ”¹è¿›æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®è¾“å…¥æç¤ºå‡†ç¡®ç”Ÿæˆå¯¹è±¡æ•°é‡ã€‚é‡‡ç”¨è®¡æ•°ç½‘ç»œè¿›è¡Œæ— å‚è€ƒçš„ç±»æ— çŸ¥è®¡æ•°ï¼Œé€šè¿‡è®¡ç®—è®¡æ•°ç½‘ç»œçš„æ¢¯åº¦å¹¶ä¼˜åŒ–æ¯ä¸€æ­¥çš„é¢„æµ‹å™ªå£°æ¥å®ç°ã€‚ä¸ºè§£å†³æç¤ºä¸­å­˜åœ¨å¤šç§ç±»å‹å¯¹è±¡çš„é—®é¢˜ï¼Œåˆ©ç”¨æ–°å‹æ³¨æ„åŠ›å›¾å¼•å¯¼è·å¾—æ¯ä¸ªå¯¹è±¡çš„é«˜è´¨é‡è’™ç‰ˆï¼Œå¹¶åœ¨æ¯ä¸ªå¯¹è±¡çš„æ¢¯åº¦å¼•å¯¼ä¸‹å®Œæˆå»å™ªè¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜æ‰©æ•£æ¨¡å‹åœ¨å¯¹è±¡è®¡æ•°æ–¹é¢çš„ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨ç”Ÿæˆç²¾ç¡®æ•°é‡çš„å†…å®¹æ–¹é¢ä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ”¹è¿›æ–¹æ³•ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½åŸºäºè¾“å…¥æç¤ºå‡†ç¡®ç”Ÿæˆå¯¹è±¡æ•°é‡ã€‚</li>
<li>é‡‡ç”¨è®¡æ•°ç½‘ç»œè¿›è¡Œæ— å‚è€ƒçš„ç±»æ— çŸ¥è®¡æ•°ï¼Œé€‚ç”¨äºä»»ä½•ç»™å®šå›¾åƒã€‚</li>
<li>é€šè¿‡è®¡ç®—è®¡æ•°ç½‘ç»œçš„æ¢¯åº¦å¹¶ä¼˜åŒ–æ¯ä¸€æ­¥çš„é¢„æµ‹å™ªå£°æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¸ºå¤„ç†æç¤ºä¸­çš„å¤šç§å¯¹è±¡ç±»å‹ï¼Œä½¿ç”¨æ–°å‹æ³¨æ„åŠ›å›¾å¼•å¯¼æŠ€æœ¯ï¼Œä¸ºæ¯ä¸ªå¯¹è±¡ç”Ÿæˆé«˜è´¨é‡è’™ç‰ˆã€‚</li>
<li>ç»“åˆæ¯ä¸ªå¯¹è±¡çš„æ¢¯åº¦å¼•å¯¼å®Œæˆå»å™ªè¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¢å¼ºæ‰©æ•£æ¨¡å‹å¯¹è±¡è®¡æ•°æ–¹é¢çš„ä¿çœŸåº¦æ–¹é¢è¡¨ç°æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.17567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a7abdcc58f3669a243136fdc5b4f6dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e090fce05e778630928f7dd24980630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31b7a770a6b6e9d9b37511c84da54c75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36e4a15beca320441b2c5d1dd0f7c1c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DP-LDMs-Differentially-Private-Latent-Diffusion-Models"><a href="#DP-LDMs-Differentially-Private-Latent-Diffusion-Models" class="headerlink" title="DP-LDMs: Differentially Private Latent Diffusion Models"></a>DP-LDMs: Differentially Private Latent Diffusion Models</h2><p><strong>Authors:Michael F. Liu, Saiyue Lyu, Margarita Vinaroz, Mijung Park</strong></p>
<p>Diffusion models (DMs) are one of the most widely used generative models for producing high quality images. However, a flurry of recent papers points out that DMs are least private forms of image generators, by extracting a significant number of near-identical replicas of training images from DMs. Existing privacy-enhancing techniques for DMs, unfortunately, do not provide a good privacy-utility tradeoff. In this paper, we aim to improve the current state of DMs with differential privacy (DP) by adopting the $\textit{Latent}$ Diffusion Models (LDMs). LDMs are equipped with powerful pre-trained autoencoders that map the high-dimensional pixels into lower-dimensional latent representations, in which DMs are trained, yielding a more efficient and fast training of DMs. Rather than fine-tuning the entire LDMs, we fine-tune only the $\textit{attention}$ modules of LDMs with DP-SGD, reducing the number of trainable parameters by roughly $90%$ and achieving a better privacy-accuracy trade-off. Our approach allows us to generate realistic, high-dimensional images (256x256) conditioned on text prompts with DP guarantees, which, to the best of our knowledge, has not been attempted before. Our approach provides a promising direction for training more powerful, yet training-efficient differentially private DMs, producing high-quality DP images. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DP-LDM-4525">https://anonymous.4open.science/r/DP-LDM-4525</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æ˜¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æœ€å¹¿æ³›ä½¿ç”¨çš„ç”Ÿæˆæ¨¡å‹ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ä¸€ç³»åˆ—è®ºæ–‡æŒ‡å‡ºï¼Œæ‰©æ•£æ¨¡å‹æ˜¯å›¾åƒç”Ÿæˆå™¨ä¸­éšç§ä¿æŠ¤æœ€å·®çš„å½¢å¼ä¹‹ä¸€ï¼Œèƒ½å¤Ÿä»æ‰©æ•£æ¨¡å‹ä¸­æå–å¤§é‡è¿‘ä¹ç›¸åŒçš„è®­ç»ƒå›¾åƒå¤åˆ¶å“ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç°æœ‰çš„ç”¨äºå¢å¼ºæ‰©æ•£æ¨¡å‹éšç§ä¿æŠ¤çš„æŠ€æœ¯å¹¶æ²¡æœ‰æä¾›è‰¯å¥½çš„éšç§æ•ˆç”¨æƒè¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰æ”¹è¿›å½“å‰æ‰©æ•£æ¨¡å‹ä¸å·®åˆ†éšç§ï¼ˆDPï¼‰çš„çŠ¶æ€ã€‚LDMsé…å¤‡äº†å¼ºå¤§çš„é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ï¼Œå°†é«˜ç»´åƒç´ æ˜ å°„åˆ°ä½ç»´æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œåœ¨å…¶ä¸­è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œå®ç°äº†æ›´é«˜æ•ˆã€æ›´å¿«çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬ä¸éœ€è¦å¾®è°ƒæ•´ä¸ªLDMsï¼Œè€Œåªæ˜¯ä½¿ç”¨DP-SGDå¾®è°ƒLDMsçš„æ³¨æ„åŠ›æ¨¡å—ï¼Œå°†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å‡å°‘äº†å¤§çº¦90%ï¼Œå¹¶å®ç°äº†æ›´å¥½çš„éšç§å‡†ç¡®æ€§æƒè¡¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨æ–‡æœ¬æç¤ºç”Ÿæˆå…·æœ‰DPä¿è¯çš„ç°å®ä¸»ä¹‰ã€é«˜ç»´å›¾åƒï¼ˆ256x256ï¼‰ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œä¹‹å‰å°šæœªæœ‰äººå°è¯•è¿‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºè®­ç»ƒæ›´å¼ºå¤§ã€æ›´é«˜æ•ˆä¸”å…·æœ‰å·®åˆ†éšç§ä¿æŠ¤èƒ½åŠ›çš„æ‰©æ•£æ¨¡å‹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡DPå›¾åƒã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DP-LDM-4525%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/DP-LDM-4525ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.15759v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æ˜¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒæœ€å¸¸ç”¨çš„ç”Ÿæˆæ¨¡å‹ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„è®ºæ–‡æŒ‡å‡ºDMsæ˜¯æœ€ä¸ç§å¯†çš„å›¾åƒç”Ÿæˆå½¢å¼ã€‚ç°æœ‰å¢å¼ºéšç§ä¿æŠ¤çš„æŠ€æœ¯å¹¶ä¸èƒ½åœ¨éšç§å’Œæ•ˆç”¨ä¹‹é—´å–å¾—è‰¯å¥½çš„å¹³è¡¡ã€‚æœ¬æ–‡æ—¨åœ¨é‡‡ç”¨å…·æœ‰å·®åˆ†éšç§ï¼ˆDPï¼‰çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰æ¥æ”¹å–„DMsçš„ç°çŠ¶ã€‚LDMsé…å¤‡å¼ºå¤§çš„é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ï¼Œå°†é«˜ç»´åƒç´ æ˜ å°„åˆ°ä½ç»´æ½œåœ¨è¡¨ç¤ºä¸­è®­ç»ƒDMsï¼Œå®ç°äº†æ›´é«˜æ•ˆã€æ›´å¿«çš„DMsè®­ç»ƒã€‚æˆ‘ä»¬åªå¯¹LDMsçš„æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œå¾®è°ƒï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨DP-SGDå®ç°å·®åˆ†éšç§ä¿æŠ¤ã€‚é€šè¿‡å‡å°‘çº¦90%çš„å¯è®­ç»ƒå‚æ•°ï¼Œæˆ‘ä»¬å®ç°äº†æ›´å¥½çš„éšç§ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ–‡æœ¬æç¤ºæ¡ä»¶çš„çœŸå®ã€é«˜ç»´å›¾åƒï¼ˆ256x256ï¼‰ï¼Œå¹¶å…·æœ‰DPä¿è¯ï¼Œè¿™åœ¨æˆ‘ä»¬çš„çŸ¥è¯†èŒƒå›´å†…å°šæœªæœ‰äººå°è¯•è¿‡ã€‚è¿™ä¸ºè®­ç»ƒæ›´å¼ºå¤§ã€æ›´é«˜æ•ˆä¸”å…·æœ‰å·®åˆ†éšç§çš„DMsæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æ˜¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„ä¸»è¦å·¥å…·ï¼Œä½†å­˜åœ¨éšç§æ³„éœ²çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰éšç§å¢å¼ºæŠ€æœ¯åœ¨DMsä¸Šçš„éšç§ä¸æ•ˆç”¨å¹³è¡¡ä¸ä½³ã€‚</li>
<li>é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ç»“åˆå·®åˆ†éšç§ï¼ˆDPï¼‰æŠ€æœ¯æ¥æ”¹å–„DMsçš„éšç§é—®é¢˜ã€‚</li>
<li>LDMsä½¿ç”¨é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨å°†é«˜ç»´åƒç´ è½¬åŒ–ä¸ºä½ç»´æ½œåœ¨è¡¨ç¤ºï¼Œæé«˜äº†DMsçš„è®­ç»ƒæ•ˆç‡å’Œé€Ÿåº¦ã€‚</li>
<li>åªå¯¹LDMsçš„æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—å‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œå®ç°äº†æ›´å¥½çš„éšç§ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ–‡æœ¬æç¤ºæ¡ä»¶çš„çœŸå®ã€é«˜ç»´å›¾åƒï¼ˆ256x256ï¼‰ï¼Œå¹¶å…·æœ‰å·®åˆ†éšç§ä¿è¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.15759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-da9430105efe8e7ed881279948a6a5a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdd6ed66eeac1d4b060fd20f111fcf42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6121a42c460b362f366e359520a65c1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ff31b8c293a9847af10bba02cef2b473.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  A self-supervised cyclic neural-analytic approach for novel view   synthesis and 3D reconstruction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6fdc9058a4ab67eeb27058a76a4d3c90.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  NTR-Gaussian Nighttime Dynamic Thermal Reconstruction with 4D Gaussian   Splatting Based on Thermodynamics
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17447.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
