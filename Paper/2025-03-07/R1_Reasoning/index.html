<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  PacketCLIP Multi-Modal Embedding of Network Traffic and Language for   Cybersecurity Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-81423a77a41ec09154418bba6ec1ef7c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-07-æ›´æ–°"><a href="#2025-03-07-æ›´æ–°" class="headerlink" title="2025-03-07 æ›´æ–°"></a>2025-03-07 æ›´æ–°</h1><h2 id="PacketCLIP-Multi-Modal-Embedding-of-Network-Traffic-and-Language-for-Cybersecurity-Reasoning"><a href="#PacketCLIP-Multi-Modal-Embedding-of-Network-Traffic-and-Language-for-Cybersecurity-Reasoning" class="headerlink" title="PacketCLIP: Multi-Modal Embedding of Network Traffic and Language for   Cybersecurity Reasoning"></a>PacketCLIP: Multi-Modal Embedding of Network Traffic and Language for   Cybersecurity Reasoning</h2><p><strong>Authors:Ryozo Masukawa, Sanggeon Yun, Sungheon Jeong, Wenjun Huang, Yang Ni, Ian Bryant, Nathaniel D. Bastian, Mohsen Imani</strong></p>
<p>Traffic classification is vital for cybersecurity, yet encrypted traffic poses significant challenges. We present PacketCLIP, a multi-modal framework combining packet data with natural language semantics through contrastive pretraining and hierarchical Graph Neural Network (GNN) reasoning. PacketCLIP integrates semantic reasoning with efficient classification, enabling robust detection of anomalies in encrypted network flows. By aligning textual descriptions with packet behaviors, it offers enhanced interpretability, scalability, and practical applicability across diverse security scenarios. PacketCLIP achieves a 95% mean AUC, outperforms baselines by 11.6%, and reduces model size by 92%, making it ideal for real-time anomaly detection. By bridging advanced machine learning techniques and practical cybersecurity needs, PacketCLIP provides a foundation for scalable, efficient, and interpretable solutions to tackle encrypted traffic classification and network intrusion detection challenges in resource-constrained environments. </p>
<blockquote>
<p>ç½‘ç»œæµé‡åˆ†ç±»å¯¹ç½‘ç»œå®‰å…¨è‡³å…³é‡è¦ï¼Œç„¶è€ŒåŠ å¯†æµé‡å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†PacketCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒå’Œåˆ†å±‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¨ç†ï¼Œå°†æ•°æ®åŒ…æ•°æ®ä¸è‡ªç„¶è¯­è¨€è¯­ä¹‰ç›¸ç»“åˆã€‚PacketCLIPå°†è¯­ä¹‰æ¨ç†ä¸é«˜æ•ˆåˆ†ç±»ç›¸ç»“åˆï¼Œå®ç°å¯¹åŠ å¯†ç½‘ç»œæµé‡å¼‚å¸¸çš„ç¨³å¥æ£€æµ‹ã€‚é€šè¿‡æ–‡æœ¬æè¿°ä¸æ•°æ®åŒ…è¡Œä¸ºçš„å¯¹é½ï¼Œå®ƒæä¾›äº†å¢å¼ºçš„å¯è§£é‡Šæ€§ã€å¯æ‰©å±•æ€§å’Œåœ¨å„ç§å®‰å…¨åœºæ™¯ä¸­çš„å®é™…é€‚ç”¨æ€§ã€‚PacketCLIPçš„å¹³å‡AUCè¾¾åˆ°95%ï¼Œæ¯”åŸºçº¿é«˜å‡º11.6%ï¼Œå¹¶ä¸”æ¨¡å‹å¤§å°å‡å°‘äº†92%ï¼Œéå¸¸é€‚åˆå®æ—¶å¼‚å¸¸æ£€æµ‹ã€‚é€šè¿‡å¼¥åˆå…ˆè¿›çš„æœºå™¨å­¦ä¹ ä¸å®é™…çš„ç½‘ç»œå®‰å…¨éœ€æ±‚ä¹‹é—´çš„é¸¿æ²Ÿï¼ŒPacketCLIPä¸ºè§£å†³èµ„æºå—é™ç¯å¢ƒä¸­çš„åŠ å¯†æµé‡åˆ†ç±»å’Œç½‘ç»œå…¥ä¾µæ£€æµ‹æŒ‘æˆ˜æä¾›äº†å¯æ‰©å±•ã€é«˜æ•ˆå’Œå¯è§£é‡Šè§£å†³æ–¹æ¡ˆçš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03747v1">PDF</a> 7 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>åŠ å¯†æµé‡åˆ†ç±»å¯¹ç½‘ç»œå®‰å…¨æ€§è‡³å…³é‡è¦ï¼Œä½†åŠ å¯†æµé‡å¸¦æ¥çš„æŒ‘æˆ˜ä¹Ÿä¸å®¹å¿½è§†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºPacketCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒå’Œåˆ†å±‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¨ç†ï¼Œå°†æ•°æ®åŒ…æ•°æ®ä¸è‡ªç„¶è¯­è¨€è¯­ä¹‰ç›¸ç»“åˆã€‚PacketCLIPå°†è¯­ä¹‰æ¨ç†ä¸é«˜æ•ˆåˆ†ç±»ç›¸ç»“åˆï¼Œå¯å®ç°åŠ å¯†ç½‘ç»œæµé‡ä¸­å¼‚å¸¸çš„ç¨³å¥æ£€æµ‹ã€‚é€šè¿‡å°†æ–‡æœ¬æè¿°ä¸æ•°æ®åŒ…è¡Œä¸ºå¯¹é½ï¼Œå®ƒæä¾›äº†å¢å¼ºçš„å¯è§£é‡Šæ€§ã€å¯æ‰©å±•æ€§å’Œä¸åŒå®‰å…¨åœºæ™¯ä¸‹çš„å®ç”¨æ€§ã€‚PacketCLIPçš„å¹³å‡AUCè¾¾åˆ°95%ï¼Œè¶…å‡ºåŸºçº¿11.6%ï¼Œå¹¶å‡å°‘æ¨¡å‹å¤§å°92%ï¼Œé€‚åˆå®æ—¶å¼‚å¸¸æ£€æµ‹ã€‚å®ƒä¸ºè§£å†³èµ„æºå—é™ç¯å¢ƒä¸­çš„åŠ å¯†æµé‡åˆ†ç±»å’Œç½‘ç»œå…¥ä¾µæ£€æµ‹æŒ‘æˆ˜æä¾›äº†å¯æ‰©å±•ã€é«˜æ•ˆå’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆåŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PacketCLIPæ˜¯ä¸€ä¸ªç”¨äºåŠ å¯†æµé‡åˆ†ç±»çš„å¤šæ¨¡æ€æ¡†æ¶ã€‚</li>
<li>å®ƒç»“åˆäº†æ•°æ®åŒ…æ•°æ®å’Œè‡ªç„¶è¯­è¨€è¯­ä¹‰ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒå’Œåˆ†å±‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¨ç†å®ç°é›†æˆã€‚</li>
<li>PacketCLIPå®ç°äº†é«˜æ•ˆçš„å¼‚å¸¸æ£€æµ‹ï¼Œå°¤å…¶é’ˆå¯¹åŠ å¯†ç½‘ç»œæµé‡ã€‚</li>
<li>è¯¥æ¡†æ¶å°†æ–‡æœ¬æè¿°ä¸æ•°æ®åŒ…è¡Œä¸ºå¯¹é½ï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>PacketCLIPçš„å¹³å‡AUCè¾¾åˆ°95%ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œé€‚åˆå®æ—¶åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-274d0f3c8f77d51b2c56c7a16c6ab536.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-305b9190f62a180f8d203cc71c56d96d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-305662942425dd0b1e8e0f0ad935f647.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e92398c11182ab60f3e4969f72eb78f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-955e94d235515250389e03ffebbcaf86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-678b686e25422e487e77e4383a7ce906.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c9447d1361ebddb789900a984847677.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e752bb3e015563f05afc4e612bd9371e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Process-based-Self-Rewarding-Language-Models"><a href="#Process-based-Self-Rewarding-Language-Models" class="headerlink" title="Process-based Self-Rewarding Language Models"></a>Process-based Self-Rewarding Language Models</h2><p><strong>Authors:Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, Yeyun Gong</strong></p>
<p>Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMsâ€™ performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œé‡‡ç”¨äººå·¥æ ‡æ³¨çš„åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†ä»å—åˆ°äººç±»æ€§èƒ½ä¸Šé™çš„åˆ¶çº¦ã€‚å› æ­¤ï¼Œæå‡ºäº†è‡ªæˆ‘å¥–åŠ±æ–¹æ³•ï¼Œå³å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¥–åŠ±è‡ªå·±çš„è¾“å‡ºç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªæˆ‘å¥–åŠ±èŒƒå¼åœ¨æ•°å­¦æ¨ç†åœºæ™¯ä¸­å¹¶ä¸æœ‰æ•ˆï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºè¯­è¨€æ¨¡å‹æå‡ºäº†åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±æµæ°´çº¿ï¼Œè¯¥æµç¨‹å¼•å…¥äº†é•¿æœŸæ€è€ƒæ¨ç†ã€é€æ­¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜å’ŒåŸºäºè‡ªæˆ‘å¥–åŠ±èŒƒå¼çš„é€æ­¥åå¥½ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼é€šè¿‡åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±è¿­ä»£æˆåŠŸæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†è‡ªæˆ‘å¥–åŠ±åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ–¹é¢å®ç°è¶…è¶Šäººç±»èƒ½åŠ›çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03746v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ï¼Œé‡‡ç”¨äººç±»æ³¨é‡Šçš„åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†ä»å—é™äºäººç±»æ€§èƒ½çš„ä¸Šé™ã€‚å› æ­¤ï¼Œæå‡ºäº†è‡ªæˆ‘å¥–åŠ±æ–¹æ³•ï¼Œè®©è¯­è¨€æ¨¡å‹é€šè¿‡å¥–åŠ±è‡ªå·±çš„è¾“å‡ºç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªæˆ‘å¥–åŠ±æ¨¡å¼åœ¨æ•°å­¦æ¨ç†åœºæ™¯ä¸­å¹¶ä¸æœ‰æ•ˆï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±ç®¡é“ï¼Œå¼•å…¥é•¿æœŸæ€è€ƒæ¨ç†ã€é€æ­¥çš„LLMä½œä¸ºæ³•å®˜å’Œé€æ­¥åå¥½ä¼˜åŒ–ç­‰ç­–ç•¥ã€‚æ–°çš„æ¨¡å¼é€šè¿‡è¿­ä»£è¿‡ç¨‹ä¸ºåŸºç¡€çš„è‡ªæˆ‘å¥–åŠ±ï¼ŒæˆåŠŸæé«˜äº†è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå±•ç°å‡ºè‡ªæˆ‘å¥–åŠ±å®ç°è¶…è¶Šäººç±»èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹æ¨ç†çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>äººç±»æ³¨é‡Šçš„åå¥½æ•°æ®ç”¨äºè¿›ä¸€æ­¥æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†ä»å—é™äºäººç±»æ€§èƒ½çš„ä¸Šé™ã€‚</li>
<li>è‡ªæˆ‘å¥–åŠ±æ–¹æ³•è¢«æå‡ºï¼Œè®©è¯­è¨€æ¨¡å‹é€šè¿‡å¥–åŠ±è‡ªå·±çš„è¾“å‡ºç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚</li>
<li>ç°æœ‰çš„è‡ªæˆ‘å¥–åŠ±æ¨¡å¼åœ¨æ•°å­¦æ¨ç†åœºæ™¯ä¸­æ•ˆæœæœ‰é™ï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>åŸºäºè¿‡ç¨‹çš„è‡ªæˆ‘å¥–åŠ±ç®¡é“è¢«æå‡ºï¼ŒåŒ…æ‹¬é•¿æœŸæ€è€ƒæ¨ç†ã€é€æ­¥çš„LLMä½œä¸ºæ³•å®˜å’Œé€æ­¥åå¥½ä¼˜åŒ–ç­‰ç­–ç•¥ã€‚</li>
<li>æ–°çš„æ¨¡å¼æˆåŠŸæé«˜äº†è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35bf432485c37194abdcfec20151d8df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f140123d2474bcf923dbc6e1d8f6681.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a09cfcf28df299b0de886a4e62b47dd3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Neutral-Point-of-View-Text-Generation-through-Parameter-Efficient-Reinforcement-Learning-and-a-Small-Scale-High-Quality-Dataset"><a href="#Improving-Neutral-Point-of-View-Text-Generation-through-Parameter-Efficient-Reinforcement-Learning-and-a-Small-Scale-High-Quality-Dataset" class="headerlink" title="Improving Neutral Point of View Text Generation through   Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality   Dataset"></a>Improving Neutral Point of View Text Generation through   Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality   Dataset</h2><p><strong>Authors:Jessica Hoffmann, Christiane Ahlheim, Zac Yu, Aria Walfrand, Jarvis Jin, Marie Tano, Ahmad Beirami, Erin van Liemt, Nithum Thain, Hakim Sidahmed, Lucas Dixon</strong></p>
<p>This paper describes the construction of a dataset and the evaluation of training methods to improve generative large language modelsâ€™ (LLMs) ability to answer queries on sensitive topics with a Neutral Point of View (NPOV), i.e., to provide significantly more informative, diverse and impartial answers. The dataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written quadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set of links to source texts elaborating the various points of view. The first key contribution of this paper is a new methodology to create such datasets through iterative rounds of human peer-critique and annotator training, which we release alongside the dataset. The second key contribution is the identification of a highly effective training regime for parameter-efficient reinforcement learning (PE-RL) to improve NPOV generation. We compare and extensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a strong baseline), SFT and RLHF.   PE-RL not only improves on overall NPOV quality compared to the strongest baseline ($97.06%\rightarrow 99.08%$), but also scores much higher on features linguists identify as key to separating good answers from the best answers ($60.25%\rightarrow 85.21%$ for presence of supportive details, $68.74%\rightarrow 91.43%$ for absence of oversimplification). A qualitative analysis corroborates this. Finally, our evaluation finds no statistical differences between results on topics that appear in the training dataset and those on separated evaluation topics, which provides strong evidence that our approach to training PE-RL exhibits very effective out of topic generalization. </p>
<blockquote>
<p>æœ¬æ–‡æè¿°äº†ä¸€ä¸ªæ•°æ®é›†çš„æ„å»ºä»¥åŠè®­ç»ƒæ–¹æ³•çš„è¯„ä¼°ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•æ„Ÿè¯é¢˜ä¸Šçš„æŸ¥è¯¢å›ç­”èƒ½åŠ›ï¼Œä»¥ä¸­ç«‹è§‚ç‚¹ï¼ˆNPOVï¼‰ä¸ºå‡†åˆ™ï¼Œå³æä¾›æ›´ä¸°å¯Œã€å¤šæ ·åŒ–å’Œå®¢è§‚å…¬æ­£çš„ç­”æ¡ˆã€‚æ•°æ®é›†åä¸ºSHQ-NPOVæ•°æ®é›†ï¼ŒåŒ…å«äº†äººç±»æ’°å†™çš„300ä¸ªé«˜è´¨é‡çš„å››å…ƒç»„ï¼šå…³äºæ•æ„Ÿè¯é¢˜çš„æŸ¥è¯¢ã€ç­”æ¡ˆã€ä¸­ç«‹è§‚ç‚¹è¯„åˆ†ä»¥åŠé“¾æ¥æºæ–‡æœ¬çš„ä¸€ç»„é˜è¿°ä¸åŒè§‚ç‚¹çš„é“¾æ¥ã€‚æœ¬æ–‡çš„ç¬¬ä¸€ä¸ªå…³é”®è´¡çŒ®æ˜¯é€šè¿‡äººç±»åŒè¡Œè¯„ä»·å’Œæ³¨é‡Šè€…åŸ¹è®­çš„è¿­ä»£è½®æ¬¡åˆ›å»ºæ­¤ç±»æ•°æ®é›†çš„æ–¹æ³•è®ºï¼Œæˆ‘ä»¬éšæ•°æ®é›†ä¸€åŒå‘å¸ƒã€‚ç¬¬äºŒä¸ªå…³é”®è´¡çŒ®æ˜¯ç¡®å®šäº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒåˆ¶åº¦ï¼Œç”¨äºå‚æ•°æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆPE-RLï¼‰ï¼Œä»¥æé«˜ä¸­ç«‹è§‚ç‚¹ç”Ÿæˆçš„è´¨é‡ã€‚æˆ‘ä»¬å°†PE-RLä¸å¤šä¸ªåŸºçº¿æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒå’Œå¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬LoRAå¾®è°ƒï¼ˆä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿æ–¹æ³•ï¼‰ã€SFTå’ŒRLHFã€‚PE-RLä¸ä»…åœ¨ä¸­ç«‹è§‚ç‚¹çš„æ•´ä½“è´¨é‡ä¸Šè¾ƒæœ€å¼ºåŸºçº¿æœ‰æ‰€æå‡ï¼ˆä»97.06%æé«˜åˆ°99.08%ï¼‰ï¼Œè€Œä¸”åœ¨è¯­è¨€å­¦å®¶è®¤ä¸ºåŒºåˆ†å¥½ç­”æ¡ˆä¸æœ€ä½³ç­”æ¡ˆçš„å…³é”®ç‰¹å¾ä¸Šä¹Ÿæœ‰æ›´é«˜çš„å¾—åˆ†ï¼ˆæ”¯æŒç»†èŠ‚çš„å­˜åœ¨ä»60.25%æé«˜åˆ°85.21%ï¼Œæ²¡æœ‰ç®€åŒ–çš„å­˜åœ¨ä»68.74%æé«˜åˆ°91.43%ï¼‰ã€‚å®šæ€§åˆ†æè¯å®äº†è¿™ä¸€ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬çš„è¯„ä¼°å‘ç°åœ¨è®­ç»ƒæ•°æ®é›†ä¸­å‡ºç°çš„ä¸»é¢˜ä¸å•ç‹¬è¯„ä¼°çš„ä¸»é¢˜ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„ç»Ÿè®¡å·®å¼‚ï¼Œè¿™ä¸ºæˆ‘ä»¬çš„PE-RLè®­ç»ƒæ–¹æ³•å±•ç°å‡ºå‡ºè‰²çš„è·¨è¯é¢˜æ³›åŒ–èƒ½åŠ›æä¾›äº†æœ‰åŠ›è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03654v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†è®­ç»ƒæ–¹æ³•æ¥æå‡ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹å›ç­”æ•æ„Ÿè¯é¢˜æ—¶çš„ä¸­ç«‹è§‚ç‚¹èƒ½åŠ›ã€‚ä»‹ç»äº†SHQ-NPOVæ•°æ®é›†çš„æ„é€ æ–¹æ³•ï¼ŒåŒ…æ‹¬é€šè¿‡äººç±»åŒè¡Œè¯„ä»·å’Œæ ‡æ³¨è€…åŸ¹è®­è¿­ä»£åˆ›å»ºæ•°æ®é›†ã€‚ç ”ç©¶è¿˜å‘ç°äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒåˆ¶åº¦ï¼Œç”¨äºå‚æ•°é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆPE-RLï¼‰ï¼Œä»¥æé«˜ä¸­ç«‹è§‚ç‚¹çš„ç”Ÿæˆè´¨é‡ã€‚æ¯”è¾ƒå’Œè¯„ä¼°äº†PE-RLå’Œå¤šä¸ªåŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬LoRAå¾®è°ƒï¼ˆå¼ºåŸºçº¿ï¼‰ã€SFTå’ŒRLHFã€‚PE-RLåœ¨ä¸­ç«‹è§‚ç‚¹è´¨é‡ä¸Šè¶…è¿‡äº†æœ€å¼ºåŸºçº¿ï¼Œå¹¶ä¸”åœ¨è¯­è¨€å­¦å®¶è®¤ä¸ºåŒºåˆ†å¥½ç­”æ¡ˆä¸æœ€ä½³ç­”æ¡ˆçš„å…³é”®ç‰¹å¾ä¸Šå¾—åˆ†æ›´é«˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¯¹è®­ç»ƒè¯é¢˜å¤–çš„æ¨å¹¿èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°æ— ç»Ÿè®¡å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ„å»ºäº†SHQ-NPOVæ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡çš„äººå†™å››å…ƒç»„ï¼Œç”¨äºè®­ç»ƒè¯­è¨€æ¨¡å‹å›ç­”æ•æ„Ÿè¯é¢˜çš„ä¸­ç«‹è§‚ç‚¹ã€‚</li>
<li>æå‡ºäº†é€šè¿‡äººç±»åŒè¡Œè¯„ä»·å’Œæ ‡æ³¨è€…åŸ¹è®­çš„è¿­ä»£æ–¹æ³•åˆ›å»ºæ•°æ®é›†ã€‚</li>
<li>å‘ç°äº†å‚æ•°é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆPE-RLï¼‰çš„æœ‰æ•ˆè®­ç»ƒåˆ¶åº¦ï¼Œèƒ½æé«˜è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸­ç«‹è§‚ç‚¹çš„èƒ½åŠ›ã€‚</li>
<li>PE-RLç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•åœ¨ä¸­ç«‹è§‚ç‚¹è´¨é‡ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>PE-RLåœ¨è¯­è¨€å­¦å®¶è®¤ä¸ºåŒºåˆ†å¥½ç­”æ¡ˆä¸æœ€ä½³ç­”æ¡ˆçš„å…³é”®ç‰¹å¾ä¸Šè¡¨ç°æ›´å¥½ã€‚</li>
<li>è¯„ä¼°å‘ç°ï¼ŒPE-RLåœ¨è®­ç»ƒè¯é¢˜å¤–çš„æ¨å¹¿èƒ½åŠ›æ— ç»Ÿè®¡å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc1612fea791676a60734a6dd5e96dfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0425d48325ebd84a1d8690e802d6eed8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fd9bb260e2daccabd25b8b56d52a1ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d89263554016badcb49131b57a334d86.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Visual-Discrimination-and-Reasoning-of-Real-World-Physical-Dynamics-Physics-Grounded-Anomaly-Detection"><a href="#Towards-Visual-Discrimination-and-Reasoning-of-Real-World-Physical-Dynamics-Physics-Grounded-Anomaly-Detection" class="headerlink" title="Towards Visual Discrimination and Reasoning of Real-World Physical   Dynamics: Physics-Grounded Anomaly Detection"></a>Towards Visual Discrimination and Reasoning of Real-World Physical   Dynamics: Physics-Grounded Anomaly Detection</h2><p><strong>Authors:Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu</strong></p>
<p>Humans detect real-world object anomalies by perceiving, interacting, and reasoning based on object-conditioned physical knowledge. The long-term goal of Industrial Anomaly Detection (IAD) is to enable machines to autonomously replicate this skill. However, current IAD algorithms are largely developed and tested on static, semantically simple datasets, which diverge from real-world scenarios where physical understanding and reasoning are essential.To bridge this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the first large-scale, real-world, physics-grounded video dataset for industrial anomaly detection. Collected using a real robot arm and motor, Phys-AD provides a diverse set of dynamic, semantically rich scenarios. The dataset includes more than 6400 videos across 22 real-world object categories, interacting with robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in Phys-AD requires visual reasoning, combining both physical knowledge and video content to determine object abnormality.We benchmark state-of-the-art anomaly detection methods under three settings: unsupervised AD, weakly-supervised AD, and video-understanding AD, highlighting their limitations in handling physics-grounded anomalies. Additionally, we introduce the Physics Anomaly Explanation (PAEval) metric, designed to assess the ability of visual-language foundation models to not only detect anomalies but also provide accurate explanations for their underlying physical causes. Our dataset and benchmark will be publicly available. </p>
<blockquote>
<p>äººç±»é€šè¿‡æ„ŸçŸ¥ã€äº¤äº’å’ŒåŸºäºå¯¹è±¡æ¡ä»¶çš„ç‰©ç†çŸ¥è¯†æ¨ç†æ¥æ£€æµ‹ç°å®ä¸–ç•Œä¸­çš„å¯¹è±¡å¼‚å¸¸ã€‚å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰çš„é•¿æœŸç›®æ ‡æ˜¯ä¸ºäº†ä½¿æœºå™¨èƒ½å¤Ÿè‡ªä¸»åœ°å¤åˆ¶è¿™é¡¹æŠ€èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„IADç®—æ³•ä¸»è¦åœ¨é™æ€ã€è¯­ä¹‰ç®€å•çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¼€å‘å’Œæµ‹è¯•ï¼Œè¿™äº›åœºæ™¯ä¸çœŸå®ä¸–ç•Œä¸­éœ€è¦ç‰©ç†ç†è§£å’Œæ¨ç†çš„æƒ…å†µç›¸å»ç”šè¿œã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Physics Anomaly Detectionï¼ˆPhys-ADï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºå·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„å¤§è§„æ¨¡ã€ç°å®ä¸–ç•Œã€åŸºäºç‰©ç†çš„è§†é¢‘æ•°æ®é›†ã€‚é€šè¿‡ä½¿ç”¨çœŸå®çš„æœºæ¢°è‡‚å’Œé©¬è¾¾è¿›è¡Œæ”¶é›†ï¼ŒPhys-ADæä¾›äº†ä¸°å¯Œå¤šæ ·çš„åŠ¨æ€ã€è¯­ä¹‰ä¸°å¯Œçš„åœºæ™¯ã€‚æ•°æ®é›†åŒ…å«è¶…è¿‡6400ä¸ªè§†é¢‘ï¼Œæ¶µç›–22ä¸ªç°å®å¯¹è±¡ç±»åˆ«ï¼Œä¸æœºæ¢°è‡‚å’Œé©¬è¾¾è¿›è¡Œäº¤äº’ï¼Œå¹¶å±•ç¤ºäº†47ç§å¼‚å¸¸ç±»å‹ã€‚Phys-ADä¸­çš„å¼‚å¸¸æ£€æµ‹éœ€è¦è§†è§‰æ¨ç†ï¼Œç»“åˆç‰©ç†çŸ¥è¯†å’Œè§†é¢‘å†…å®¹æ¥ç¡®å®šå¯¹è±¡çš„å¼‚å¸¸æƒ…å†µã€‚æˆ‘ä»¬åœ¨ä¸‰ç§è®¾ç½®ä¸‹å¯¹æœ€å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼šæ— ç›‘ç£ADã€å¼±ç›‘ç£ADå’Œè§†é¢‘ç†è§£ADï¼Œçªå‡ºäº†å®ƒä»¬åœ¨å¤„ç†åŸºäºç‰©ç†çš„å¼‚å¸¸æ–¹é¢çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†Physics Anomaly Explanationï¼ˆPAEvalï¼‰æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡æ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ä¸ä»…æ£€æµ‹å¼‚å¸¸ï¼Œè€Œä¸”ä¸ºå®ƒä»¬æä¾›å‡†ç¡®çš„æ½œåœ¨ç‰©ç†åŸå› è§£é‡Šçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03562v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong>ï¼š<br>äººç±»é€šè¿‡æ„ŸçŸ¥ã€äº¤äº’å’ŒåŸºäºå¯¹è±¡æ¡ä»¶çš„ç‰©ç†çŸ¥è¯†æ¥æ£€æµ‹ç°å®ä¸–ç•Œä¸­çš„å¯¹è±¡å¼‚å¸¸ã€‚å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰çš„é•¿æœŸç›®æ ‡æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿè‡ªä¸»å¤åˆ¶è¿™ä¸€æŠ€èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„IADç®—æ³•ä¸»è¦åœ¨é™æ€ã€è¯­ä¹‰ç®€å•çš„æ•°æ®é›†ä¸Šå¼€å‘å’Œæµ‹è¯•ï¼Œè¿™ä¸éœ€è¦ç‰©ç†ç†è§£å’Œæ¨ç†çš„ç°å®ä¸–ç•Œåœºæ™¯å­˜åœ¨å·®è·ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Physics Anomaly Detectionï¼ˆPhys-ADï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„å¤§è§„æ¨¡ã€ç°å®ä¸–ç•Œã€åŸºäºç‰©ç†çš„è§†é¢‘æ•°æ®é›†ã€‚é€šè¿‡çœŸå®çš„æœºæ¢°è‡‚å’Œé©¬è¾¾æ”¶é›†ï¼ŒPhys-ADæä¾›äº†ä¸°å¯Œçš„åŠ¨æ€ã€è¯­ä¹‰ä¸°å¯Œçš„åœºæ™¯ã€‚æ•°æ®é›†åŒ…å«è¶…è¿‡6400ä¸ªè§†é¢‘ï¼Œæ¶µç›–22ä¸ªç°å®å¯¹è±¡ç±»åˆ«ï¼Œä¸æœºæ¢°è‡‚å’Œé©¬è¾¾è¿›è¡Œäº¤äº’ï¼Œå¹¶å±•ç¤ºäº†47ç§å¼‚å¸¸ã€‚Phys-ADä¸­çš„å¼‚å¸¸æ£€æµ‹éœ€è¦è§†è§‰æ¨ç†ï¼Œç»“åˆç‰©ç†çŸ¥è¯†å’Œè§†é¢‘å†…å®¹æ¥ç¡®å®šå¯¹è±¡å¼‚å¸¸ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨ä¸‰ç§è®¾ç½®ä¸‹çš„æ€§èƒ½ï¼šæ— ç›‘ç£ADã€å¼±ç›‘ç£ADå’Œè§†é¢‘ç†è§£ADï¼Œçªå‡ºäº†å®ƒä»¬åœ¨å¤„ç†åŸºäºç‰©ç†çš„å¼‚å¸¸æ–¹é¢çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†Physics Anomaly Explanationï¼ˆPAEvalï¼‰æŒ‡æ ‡ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ä¸ä»…æ£€æµ‹å¼‚å¸¸ï¼Œè€Œä¸”ä¸ºå®ƒä»¬æä¾›å‡†ç¡®çš„åŸºæœ¬ç‰©ç†åŸå› è§£é‡Šçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å°†å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººç±»é€šè¿‡æ„ŸçŸ¥ã€äº¤äº’å’ŒåŸºäºå¯¹è±¡æ¡ä»¶çš„ç‰©ç†çŸ¥è¯†æ£€æµ‹ç°å®ä¸–ç•Œä¸­çš„å¯¹è±¡å¼‚å¸¸ã€‚</li>
<li>å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰çš„ç›®æ ‡æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿè‡ªä¸»å¤åˆ¶äººç±»çš„å¼‚å¸¸æ£€æµ‹æŠ€èƒ½ã€‚</li>
<li>å½“å‰IADç®—æ³•ä¸»è¦åœ¨é™æ€ã€è¯­ä¹‰ç®€å•çš„æ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œä¸ç°å®ä¸–ç•Œåœºæ™¯å­˜åœ¨å·®è·ã€‚</li>
<li>å¼•å…¥Physics Anomaly Detectionï¼ˆPhys-ADï¼‰æ•°æ®é›†ï¼Œä¸ºå·¥ä¸šå¼‚å¸¸æ£€æµ‹æä¾›å¤§è§„æ¨¡ã€ç°å®ä¸–ç•Œçš„è§†é¢‘æ•°æ®é›†ã€‚</li>
<li>Phys-ADæ•°æ®é›†åŒ…å«å¤šç§åŠ¨æ€ã€è¯­ä¹‰ä¸°å¯Œçš„åœºæ™¯ï¼Œæ¶µç›–å¤šä¸ªå¯¹è±¡ç±»åˆ«å’Œå¼‚å¸¸ç±»å‹ã€‚</li>
<li>å¼‚å¸¸æ£€æµ‹éœ€è¦è§†è§‰æ¨ç†ï¼Œç»“åˆç‰©ç†çŸ¥è¯†å’Œè§†é¢‘å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e62eeed359764b9ba307214b33a63919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e732126a431cda6a5b1f46ac76d2224e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b29e8637fe9cf085377764b8e6baaa0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85217105cf4434add07910a1443fa578.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-744ff0d73d230e20b788c44f0f4af80d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3620090b29258d7bd76c27fa5b833940.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e80ddfb76d90aa4359b5926eea3e235.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Afford-X-Generalizable-and-Slim-Affordance-Reasoning-for-Task-oriented-Manipulation"><a href="#Afford-X-Generalizable-and-Slim-Affordance-Reasoning-for-Task-oriented-Manipulation" class="headerlink" title="Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented   Manipulation"></a>Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented   Manipulation</h2><p><strong>Authors:Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, Hao Zhao</strong></p>
<p>Object affordance reasoning, the ability to infer object functionalities based on physical properties, is fundamental for task-oriented planning and activities in both humans and Artificial Intelligence (AI). This capability, required for planning and executing daily activities in a task-oriented manner, relies on commonsense knowledge of object physics and functionalities, extending beyond simple object recognition. Current computational models for affordance reasoning from perception lack generalizability, limiting their applicability in novel scenarios. Meanwhile, comprehensive Large Language Models (LLMs) with emerging reasoning capabilities are challenging to deploy on local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a large-scale dataset comprising 1,496 tasks and 119k images, designed to enhance the generalizability of affordance reasoning from perception. Utilizing this dataset, we develop Afford-X, an end-to-end trainable affordance reasoning model that incorporates Verb Attention and Bi-Fusion modules to improve multi-modal understanding. This model achieves up to a 12.1% performance improvement over the best-reported results from non-LLM methods, while also demonstrating a 1.2% enhancement compared to our previous conference paper. Additionally, it maintains a compact 187M parameter size and infers nearly 50 times faster than the GPT-4V API. Our work demonstrates the potential for efficient, generalizable affordance reasoning models that can be deployed on local devices for task-oriented manipulations. We showcase Afford-Xâ€™s effectiveness in enabling task-oriented manipulations for robots across various tasks and environments, underscoring its efficiency and broad implications for advancing robotics and AI systems in real-world applications. </p>
<blockquote>
<p>å¯¹è±¡åŠŸèƒ½æ¨ç†ï¼ˆObject Affordance Reasoningï¼‰æ˜¯ä¾æ®ç‰©ç†å±æ€§æ¨æ–­å¯¹è±¡åŠŸèƒ½çš„èƒ½åŠ›ï¼Œå¯¹äºäººç±»å’Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„ä»»åŠ¡å¯¼å‘è§„åˆ’å’Œæ´»åŠ¨éƒ½æ˜¯åŸºç¡€æ€§çš„ã€‚è¿™ç§èƒ½åŠ›å¯¹äºä»¥ä»»åŠ¡ä¸ºå¯¼å‘çš„æ–¹å¼è§„åˆ’å’Œæ‰§è¡Œæ—¥å¸¸æ´»åŠ¨è‡³å…³é‡è¦ï¼Œå®ƒä¾èµ–äºå¯¹å¯¹è±¡ç‰©ç†å’ŒåŠŸèƒ½çš„å¸¸è¯†æ€§çŸ¥è¯†ï¼Œè¶…è¶Šäº†ç®€å•çš„å¯¹è±¡è¯†åˆ«ã€‚å½“å‰ä»æ„ŸçŸ¥ä¸­è¿›è¡ŒåŠŸèƒ½æ¨ç†çš„è®¡ç®—æ¨¡å‹ç¼ºä¹é€šç”¨æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ–°å‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¸¦æœ‰æ–°å…´æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœ¬åœ°è®¾å¤‡ä¸Šéƒ¨ç½²ä»¥è¿›è¡Œä»»åŠ¡å¯¼å‘çš„æ“ä½œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼•å…¥äº†LVIS-Affå¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«1496é¡¹ä»»åŠ¡å’Œ11.9ä¸‡å¼ å›¾åƒï¼Œæ—¨åœ¨æé«˜ä»æ„ŸçŸ¥ä¸­è¿›è¡ŒåŠŸèƒ½æ¨ç†çš„é€šç”¨æ€§ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†Afford-Xç«¯åˆ°ç«¯å¯è®­ç»ƒçš„åŠŸèƒ½æ¨ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†åŠ¨è¯æ³¨æ„åŠ›å’ŒåŒå‘èåˆæ¨¡å—ï¼Œä»¥æé«˜å¤šæ¨¡å¼ç†è§£ã€‚è¯¥æ¨¡å‹çš„æ€§èƒ½æ¯”éLLMæ–¹æ³•çš„æœ€ä½³æŠ¥å‘Šç»“æœæé«˜äº†é«˜è¾¾12.1%ï¼Œä¸æˆ‘ä»¬ä¹‹å‰çš„ä¼šè®®è®ºæ–‡ç›¸æ¯”ä¹Ÿæé«˜äº†1.2%ã€‚æ­¤å¤–ï¼Œå®ƒä¿æŒç´§å‡‘çš„1.87äº¿ä¸ªå‚æ•°å¤§å°ï¼Œå¹¶ä¸”æ¯”GPT-4V APIå¿«è¿‘50å€è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†é«˜æ•ˆã€å¯é€šç”¨çš„åŠŸèƒ½æ¨ç†æ¨¡å‹çš„æ½œåŠ›ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥éƒ¨ç½²åœ¨æœ¬åœ°è®¾å¤‡è¿›è¡Œä»»åŠ¡å¯¼å‘çš„æ“ä½œã€‚æˆ‘ä»¬å±•ç¤ºäº†Afford-Xåœ¨æœºå™¨äººæ‰§è¡Œå„ç§ä»»åŠ¡å’Œç¯å¢ƒä¸­çš„ä»»åŠ¡å¯¼å‘æ“ä½œçš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†å…¶åœ¨æ¨åŠ¨æœºå™¨äººæŠ€æœ¯å’ŒAIç³»ç»Ÿåœ¨ç°å®åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå¹¿æ³›åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03556v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¯¹è±¡åŠŸèƒ½æ¨ç†æ˜¯æ„ŸçŸ¥åŸºäºç‰©ç†å±æ€§çš„å¯¹è±¡åŠŸèƒ½æ¨æ–­çš„èƒ½åŠ›ï¼Œå¯¹äººå’Œäººå·¥æ™ºèƒ½çš„ä»»åŠ¡å¯¼å‘è§„åˆ’åŠæ´»åŠ¨è‡³å…³é‡è¦ã€‚è¿™ä¸€èƒ½åŠ›å¯¹ä»¥ä»»åŠ¡ä¸ºå¯¼å‘çš„è§„åˆ’æ—¥å¸¸æ´»åŠ¨è‡³å…³é‡è¦ï¼Œå®ƒä¾èµ–äºè¶…è¶Šç®€å•å¯¹è±¡è¯†åˆ«çš„å¸¸è¯†æ€§çŸ¥è¯†å…³äºå¯¹è±¡çš„ç‰©ç†å’ŒåŠŸèƒ½æ€§ã€‚å½“å‰çš„è®¡ç®—æ¨¡å‹ç¼ºä¹æ™®éæ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ–°å‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚åŒæ—¶ï¼Œå…·æœ‰æ–°å…´æ¨ç†èƒ½åŠ›çš„å…¨é¢å¤§å‹è¯­è¨€æ¨¡å‹éš¾ä»¥éƒ¨ç½²åœ¨æœ¬åœ°è®¾å¤‡ä¸Šç”¨äºä»»åŠ¡å¯¼å‘æ“ä½œã€‚æœ¬æ–‡ä»‹ç»äº†LVIS-Affå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«1496é¡¹ä»»åŠ¡å’Œ11.9ä¸‡å¼ å›¾åƒï¼Œæ—¨åœ¨æé«˜æ„ŸçŸ¥åŠŸèƒ½çš„æ™®éæ€§ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†Afford-Xç«¯åˆ°ç«¯å¯è®­ç»ƒçš„è´Ÿæ‹…æ¨ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†åŠ¨è¯æ³¨æ„åŠ›å’ŒåŒå‘èåˆæ¨¡å—ï¼Œä»¥æé«˜å¤šæ¨¡æ€ç†è§£ã€‚è¯¥æ¨¡å‹åœ¨éè¯­è¨€æ¨¡å‹æ–¹æ³•ä¸ŠæŠ¥çš„æœ€ä½³ç»“æœä¸Šå–å¾—äº†é«˜è¾¾12.1%çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¸æˆ‘ä»¬çš„å‰æœŸä¼šè®®è®ºæ–‡ç›¸æ¯”ä¹Ÿå®ç°äº†1.2%çš„æå‡ã€‚æ­¤å¤–ï¼Œå®ƒä¿æŒäº†ç´§å‡‘çš„1.87äº¿å‚æ•°è§„æ¨¡ï¼Œæ¯”GPT-4V APIå¿«è¿‘50å€è¿›è¡Œæ¨æ–­ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†å…·æœ‰æ½œåŠ›çš„é«˜æ•ˆã€é€šç”¨è´Ÿæ‹…æ¨ç†æ¨¡å‹ï¼Œå¯éƒ¨ç½²åœ¨æœ¬åœ°è®¾å¤‡ä¸Šç”¨äºä»»åŠ¡å¯¼å‘æ“ä½œã€‚å±•ç¤ºäº†Afford-Xåœ¨æœºå™¨äººå„ç§ä»»åŠ¡å’Œç¯å¢ƒä¸­çš„ä»»åŠ¡å¯¼å‘æ“ä½œçš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾å…¶åœ¨æ¨åŠ¨æœºå™¨äººæŠ€æœ¯å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿå®é™…åº”ç”¨ä¸­çš„æ•ˆç‡å’Œå¹¿æ³›å½±å“ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹è±¡åŠŸèƒ½æ¨ç†å¯¹äºäººç±»å’Œäººå·¥æ™ºèƒ½çš„ä»»åŠ¡å¯¼å‘è§„åˆ’å’Œæ´»åŠ¨è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è®¡ç®—æ¨¡å‹åœ¨æ„ŸçŸ¥è´Ÿæ‹…æ¨ç†æ–¹é¢ç¼ºä¹æ™®éæ€§å’Œå¤šåœºæ™¯åº”ç”¨æ½œåŠ›ã€‚</li>
<li>LVIS-Affå¤§è§„æ¨¡æ•°æ®é›†çš„å¼•å…¥æœ‰åŠ©äºæé«˜æ„ŸçŸ¥è´Ÿæ‹…æ¨ç†çš„æ™®éæ€§ã€‚</li>
<li>Afford-Xæ¨¡å‹ç»“åˆåŠ¨è¯æ³¨æ„åŠ›å’ŒåŒå‘èåˆæ¨¡å—ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€ç†è§£ã€‚</li>
<li>Afford-Xç›¸è¾ƒäºéLLMæ–¹æ³•å±•ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>Afford-Xæ¨¡å‹å…·æœ‰é«˜æ•ˆæ€§ã€ç´§å‡‘çš„å‚æ•°è§„æ¨¡åŠå¿«é€Ÿæ¨æ–­èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea4f577bad13d5289cf52df1bbdf9f7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5efb0f1fde9004184d59718b8bd1bb84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f23352ed6cde189f9358774c71cb0215.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba85081a42f3a62287243e8d1e21590e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-797f07dee9722e0c6ec3c39d4e0d94ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-882d13ffb9ec2b9d4f9f295c6b398981.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Generative-Artificial-Intelligence-in-Robotic-Manipulation-A-Survey"><a href="#Generative-Artificial-Intelligence-in-Robotic-Manipulation-A-Survey" class="headerlink" title="Generative Artificial Intelligence in Robotic Manipulation: A Survey"></a>Generative Artificial Intelligence in Robotic Manipulation: A Survey</h2><p><strong>Authors:Kun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao Zhao, Tao Feng, Michael Yu Wang, Qifeng Chen, Jia Pan, Bo Yang, Hua Chen</strong></p>
<p>This survey provides a comprehensive review on recent advancements of generative learning models in robotic manipulation, addressing key challenges in the field. Robotic manipulation faces critical bottlenecks, including significant challenges in insufficient data and inefficient data acquisition, long-horizon and complex task planning, and the multi-modality reasoning ability for robust policy learning performance across diverse environments. To tackle these challenges, this survey introduces several generative model paradigms, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), diffusion models, probabilistic flow models, and autoregressive models, highlighting their strengths and limitations. The applications of these models are categorized into three hierarchical layers: the Foundation Layer, focusing on data generation and reward generation; the Intermediate Layer, covering language, code, visual, and state generation; and the Policy Layer, emphasizing grasp generation and trajectory generation. Each layer is explored in detail, along with notable works that have advanced the state of the art. Finally, the survey outlines future research directions and challenges, emphasizing the need for improved efficiency in data utilization, better handling of long-horizon tasks, and enhanced generalization across diverse robotic scenarios. All the related resources, including research papers, open-source data, and projects, are collected for the community in <a target="_blank" rel="noopener" href="https://github.com/GAI4Manipulation/AwesomeGAIManipulation">https://github.com/GAI4Manipulation/AwesomeGAIManipulation</a> </p>
<blockquote>
<p>æœ¬æ¬¡è°ƒæŸ¥å…¨é¢å›é¡¾äº†æœºå™¨äººæ“ä½œä¸­çš„ç”Ÿæˆå­¦ä¹ æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œå¹¶è§£å†³äº†è¯¥é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ã€‚æœºå™¨äººæ“ä½œé¢ä¸´å…³é”®çš„ç“¶é¢ˆé—®é¢˜ï¼ŒåŒ…æ‹¬æ•°æ®ä¸è¶³å’Œæ•°æ®è·å–æ•ˆç‡ä½ä¸‹ã€é•¿æœŸä»»åŠ¡å’Œå¤æ‚ä»»åŠ¡è§„åˆ’ä»¥åŠè·¨ä¸åŒç¯å¢ƒçš„ç¨³å¥ç­–ç•¥å­¦ä¹ æ€§èƒ½çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ç­‰å¤šæ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ¬¡è°ƒæŸ¥ä»‹ç»äº†å¤šç§ç”Ÿæˆæ¨¡å‹èŒƒå¼ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ã€æ‰©æ•£æ¨¡å‹ã€æ¦‚ç‡æµæ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ç­‰ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†å®ƒä»¬çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚è¿™äº›æ¨¡å‹çš„åº”ç”¨è¢«åˆ†ä¸ºä¸‰å±‚ï¼šåŸºç¡€å±‚ï¼Œä¸“æ³¨äºæ•°æ®ç”Ÿæˆå’Œå¥–åŠ±ç”Ÿæˆï¼›ä¸­é—´å±‚ï¼Œæ¶µç›–è¯­è¨€ã€ä»£ç ã€è§†è§‰å’ŒçŠ¶æ€ç”Ÿæˆï¼›ä»¥åŠç­–ç•¥å±‚ï¼Œä¾§é‡äºæŠ“å–ç”Ÿæˆå’Œè½¨è¿¹ç”Ÿæˆã€‚æ¯ä¸€å±‚éƒ½è¿›è¡Œäº†è¯¦ç»†æ¢ç´¢ï¼Œå¹¶ä»‹ç»äº†æ¨åŠ¨æŠ€æœ¯è¿›æ­¥çš„æ˜¾è‘—æˆæœã€‚æœ€åï¼Œæœ¬æ¬¡è°ƒæŸ¥æ¦‚è¿°äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘å’ŒæŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒæé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ã€æ›´å¥½åœ°å¤„ç†é•¿æœŸä»»åŠ¡å’Œå¢å¼ºè·¨ä¸åŒæœºå™¨äººåœºæ™¯çš„æ³›åŒ–èƒ½åŠ›çš„éœ€æ±‚ã€‚æ‰€æœ‰ç›¸å…³èµ„æºï¼ŒåŒ…æ‹¬ç ”ç©¶è®ºæ–‡ã€å¼€æºæ•°æ®å’Œé¡¹ç›®ï¼Œéƒ½æ”¶é›†åœ¨<a target="_blank" rel="noopener" href="https://github.com/GAI4Manipulation/AwesomeGAIManipulation%EF%BC%8C%E4%BB%A5%E4%BE%9B%E8%AF%A5%E9%A2%86%E5%9F%9F%E7%9A%84%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/GAI4Manipulation/AwesomeGAIManipulationï¼Œä»¥ä¾›è¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03464v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç»¼è¿°å…¨é¢æ¢è®¨äº†æœºå™¨äººæ“ä½œä¸­çš„ç”Ÿæˆå­¦ä¹ æ¨¡å‹æœ€æ–°è¿›å±•ï¼Œå¹¶è§£å†³äº†è¯¥é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ã€‚é¢ä¸´çš„å…³é”®ç“¶é¢ˆåŒ…æ‹¬æ•°æ®ä¸è¶³ã€æ•°æ®è·å–æ•ˆç‡ä½ä¸‹ã€é•¿æœŸå’Œå¤æ‚çš„ä»»åŠ¡è§„åˆ’ä»¥åŠè·¨ä¸åŒç¯å¢ƒçš„å¤šå…ƒæ¨¡å¼æ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥ç»¼è¿°ä»‹ç»äº†å¤šç§ç”Ÿæˆæ¨¡å‹èŒƒå¼ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ã€æ‰©æ•£æ¨¡å‹ã€æ¦‚ç‡æµæ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ç­‰ï¼Œå¹¶çªå‡ºäº†å®ƒä»¬çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚è¿™äº›æ¨¡å‹çš„åº”ç”¨è¢«åˆ†ç±»ä¸ºä¸‰ä¸ªå±‚æ¬¡ï¼šåŸºç¡€å±‚ã€ä¸­é—´å±‚å’Œæ”¿ç­–å±‚ï¼Œæ¯ä¸ªå±‚æ¬¡éƒ½è¿›è¡Œäº†è¯¦ç»†æ¢ç´¢ï¼Œå¹¶ä»‹ç»äº†æ¨åŠ¨è‰ºæœ¯è¿›æ­¥çš„é‡è¦ä½œå“ã€‚æœ€åï¼Œè¯¥ç»¼è¿°è¿˜æ¦‚è¿°äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘å’ŒæŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†æé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ã€æ›´å¥½åœ°å¤„ç†é•¿æœŸä»»åŠ¡å’Œå¢å¼ºè·¨æœºå™¨äººåœºæ™¯çš„æ³›åŒ–èƒ½åŠ›çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç»¼è¿°å…¨é¢å›é¡¾äº†æœºå™¨äººæ“ä½œä¸­çš„ç”Ÿæˆå­¦ä¹ æ¨¡å‹çš„æœ€æ–°è¿›å±•ã€‚</li>
<li>æœºå™¨äººæ“ä½œé¢ä¸´æ•°æ®ä¸è¶³ã€æ•°æ®è·å–æ•ˆç‡ä½ä¸‹ã€é•¿æœŸå’Œå¤æ‚çš„ä»»åŠ¡è§„åˆ’ä»¥åŠå¤šå…ƒæ¨¡å¼æ¨ç†èƒ½åŠ›ç­‰å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç»¼è¿°ä»‹ç»äº†å¤šç§ç”Ÿæˆæ¨¡å‹èŒƒå¼ï¼ŒåŒ…æ‹¬GANsã€VAEsã€æ‰©æ•£æ¨¡å‹ç­‰ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹çš„åº”ç”¨è¢«åˆ†ä¸ºåŸºç¡€å±‚ã€ä¸­é—´å±‚å’Œæ”¿ç­–å±‚ä¸‰ä¸ªå±‚æ¬¡ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†æ¯ä¸ªå±‚æ¬¡çš„å…³é”®ç‚¹å’Œé‡è¦ä½œå“ã€‚</li>
<li>ç»¼è¿°å¼ºè°ƒäº†æé«˜æ•°æ®åˆ©ç”¨æ•ˆç‡ã€å¤„ç†é•¿æœŸä»»åŠ¡ä»¥åŠå¢å¼ºè·¨æœºå™¨äººåœºæ™¯çš„æ³›åŒ–èƒ½åŠ›çš„éœ€æ±‚ã€‚</li>
<li>è¯¥ç»¼è¿°æä¾›äº†ä¸€ä¸ªå…³äºæœºå™¨äººæ“ä½œé¢†åŸŸçš„èµ„æºé“¾æ¥ï¼ŒåŒ…æ‹¬ç ”ç©¶è®ºæ–‡ã€å¼€æºæ•°æ®å’Œé¡¹ç›®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2f431046e2944d2639dcb07124ccab25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec3aae4527689d6d036a8dafd6b3c019.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34a1ee723b07c7f97a7990a73ea4bd2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aa7858470891ab7c15c4a83b104b929.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-Box-is-in-the-Pen-Evaluating-Commonsense-Reasoning-in-Neural-Machine-Translation"><a href="#The-Box-is-in-the-Pen-Evaluating-Commonsense-Reasoning-in-Neural-Machine-Translation" class="headerlink" title="The Box is in the Pen: Evaluating Commonsense Reasoning in Neural   Machine Translation"></a>The Box is in the Pen: Evaluating Commonsense Reasoning in Neural   Machine Translation</h2><p><strong>Authors:Jie He, Tao Wang, Deyi Xiong, Qun Liu</strong></p>
<p>Does neural machine translation yield translations that are congenial with common sense? In this paper, we present a test suite to evaluate the commonsense reasoning capability of neural machine translation. The test suite consists of three test sets, covering lexical and contextless&#x2F;contextual syntactic ambiguity that requires commonsense knowledge to resolve. We manually create 1,200 triples, each of which contain a source sentence and two contrastive translations, involving 7 different common sense types. Language models pretrained on large-scale corpora, such as BERT, GPT-2, achieve a commonsense reasoning accuracy of lower than 72% on target translations of this test suite. We conduct extensive experiments on the test suite to evaluate commonsense reasoning in neural machine translation and investigate factors that have impact on this capability. Our experiments and analyses demonstrate that neural machine translation performs poorly on commonsense reasoning of the three ambiguity types in terms of both reasoning accuracy (60.1%) and reasoning consistency (31%). The built commonsense test suite is available at <a target="_blank" rel="noopener" href="https://github.com/tjunlp-lab/CommonMT">https://github.com/tjunlp-lab/CommonMT</a>. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œæœºå™¨ç¿»è¯‘äº§ç”Ÿçš„ç¿»è¯‘ç»“æœæ˜¯å¦ç¬¦åˆå¸¸è¯†ï¼Ÿåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€å¥—æµ‹è¯•é›†æ¥è¯„ä¼°ç¥ç»ç½‘ç»œæœºå™¨ç¿»è¯‘çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ã€‚æµ‹è¯•é›†ç”±ä¸‰ä¸ªæµ‹è¯•é›†ç»„æˆï¼Œæ¶µç›–è¯æ±‡å’Œä¸Šä¸‹æ–‡æ— å…³çš„&#x2F;ä¸Šä¸‹æ–‡çš„å¥æ³•æ­§ä¹‰ï¼Œéœ€è¦å¸¸è¯†çŸ¥è¯†æ¥è§£å†³ã€‚æˆ‘ä»¬æ‰‹åŠ¨åˆ›å»ºäº†1200ä¸ªä¸‰å…ƒç»„ï¼Œæ¯ä¸ªä¸‰å…ƒç»„éƒ½åŒ…å«ä¸€å¥æºå¥å’Œä¸¤ä¸ªå¯¹æ¯”ç¿»è¯‘ï¼Œæ¶‰åŠ7ç§ä¸åŒçš„å¸¸è¯†ç±»å‹ã€‚åœ¨å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„è¯¸å¦‚BERTã€GPT-2ç­‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨è¿™ä¸ªæµ‹è¯•é›†çš„ç›®æ ‡ç¿»è¯‘ä¸Šï¼Œå¸¸è¯†æ¨ç†çš„å‡†ç¡®æ€§ä½äº72%ã€‚æˆ‘ä»¬å¯¹æµ‹è¯•é›†è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œä»¥è¯„ä¼°ç¥ç»ç½‘ç»œæœºå™¨ç¿»è¯‘ä¸­çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç ”ç©¶å½±å“è¿™ä¸€èƒ½åŠ›çš„å› ç´ ã€‚æˆ‘ä»¬çš„å®éªŒå’Œåˆ†æè¡¨æ˜ï¼Œç¥ç»ç½‘ç»œæœºå™¨ç¿»è¯‘åœ¨ä¸‰ç§æ¨¡ç³Šç±»å‹çš„å¸¸è¯†æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæ— è®ºæ˜¯æ¨ç†å‡†ç¡®æ€§ï¼ˆ60.1%ï¼‰è¿˜æ˜¯æ¨ç†ä¸€è‡´æ€§ï¼ˆ31%ï¼‰ã€‚æ„å»ºçš„å¸¸è¯†æµ‹è¯•é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tjunlp-lab/CommonMT%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tjunlp-lab/CommonMTä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03308v1">PDF</a> EMNLP findings 2020</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€å¥—æµ‹è¯•å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°ç¥ç»æœºå™¨ç¿»è¯‘åœ¨å¸¸è¯†æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æµ‹è¯•å¥—ä»¶åŒ…å«ä¸‰ä¸ªæµ‹è¯•é›†ï¼Œæ¶µç›–è¯æ±‡å’Œè¯­å¢ƒæ— å…³&#x2F;ç›¸å…³çš„å¥æ³•æ­§ä¹‰ï¼Œéœ€è¦å¸¸è¯†çŸ¥è¯†æ¥è§£å†³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢„è®­ç»ƒåœ¨å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šçš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚BERTã€GPT-2ç­‰ï¼Œåœ¨è¯¥æµ‹è¯•å¥—ä»¶ä¸Šçš„å¸¸è¯†æ¨ç†å‡†ç¡®ç‡ä½äº72%ã€‚ç¥ç»æœºå™¨ç¿»è¯‘åœ¨è§£å†³ä¸‰ç§æ­§ä¹‰ç±»å‹çš„å¸¸è¯†æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæ¨ç†å‡†ç¡®ç‡ä»…ä¸º60.1%ï¼Œæ¨ç†ä¸€è‡´æ€§ä¸º31%ã€‚æ‰€æ„å»ºçš„å¸¸è¯†æµ‹è¯•å¥—ä»¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tjunlp-lab/CommonMT%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tjunlp-lab/CommonMTä¸Šè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€å¥—ç”¨äºè¯„ä¼°ç¥ç»æœºå™¨ç¿»è¯‘å¸¸è¯†æ¨ç†èƒ½åŠ›çš„æµ‹è¯•å¥—ä»¶ã€‚</li>
<li>æµ‹è¯•å¥—ä»¶åŒ…å«ä¸‰ä¸ªæµ‹è¯•é›†ï¼Œæ¶‰åŠè¯æ±‡å’Œè¯­å¢ƒç›¸å…³çš„å¥æ³•æ­§ä¹‰ã€‚</li>
<li>éœ€è¦å¸¸è¯†çŸ¥è¯†æ¥è§£å†³è¿™äº›æ­§ä¹‰ã€‚</li>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTã€GPT-2ï¼‰åœ¨æµ‹è¯•å¥—ä»¶ä¸Šçš„å¸¸è¯†æ¨ç†å‡†ç¡®ç‡ä½äº72%ã€‚</li>
<li>ç¥ç»æœºå™¨ç¿»è¯‘åœ¨è§£å†³ä¸‰ç§æ­§ä¹‰ç±»å‹çš„å¸¸è¯†æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæ¨ç†å‡†ç¡®ç‡ä»…ä¸º60.1%ã€‚</li>
<li>ç¥ç»æœºå™¨ç¿»è¯‘çš„æ¨ç†ä¸€è‡´æ€§ä»…ä¸º31%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6e84d9549348af658c24322f096efce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-137fda1b3e9235b66c6f6c94b0c21700.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-118ae87c8502854b276aff603306fba6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FANS-â€“-Formal-Answer-Selection-for-Natural-Language-Math-Reasoning-Using-Lean4"><a href="#FANS-â€“-Formal-Answer-Selection-for-Natural-Language-Math-Reasoning-Using-Lean4" class="headerlink" title="FANS â€“ Formal Answer Selection for Natural Language Math Reasoning   Using Lean4"></a>FANS â€“ Formal Answer Selection for Natural Language Math Reasoning   Using Lean4</h2><p><strong>Authors:Jiarui Yao, Ruida Wang, Tong Zhang</strong></p>
<p>Large Language Models (LLMs) have displayed astonishing abilities in various tasks, especially in text generation, classification, question answering, etc. However, the reasoning ability of LLMs still faces many debates. The inherent ambiguity of Natural Language (NL) limits LLMsâ€™ ability to perform verifiable reasoning, making its answers lack coherence and trustworthy support. To tackle the above problems, we propose a novel framework named FANS: Formal ANswer Selection for Natural Language Math Reasoning Using Lean4. To the best of our knowledge, it is the first framework that utilizes Lean4 to enhance LLMsâ€™ NL math reasoning ability. In particular, given an NL math question and LLM-generated answers, FANS first translates it into Lean4 theorem statements. Then it tries to prove it using a Lean4 prover and verify it by Lean4. Finally, it uses the FL result to assist in answer selection. It enhances LLMsâ€™ NL math ability in providing a computer-verifiable solution for its correct answer and proposes an alternative method for answer selection beyond the reward model. Extensive experiments indicate the effectiveness of our framework. It can improve the accuracy rate of reward model enhanced LLMs in the MATH-500 dataset by at most 1.91% and AMC-23 by at most 8.33% on strong reward-model baselines. In some particular fields like number theory that Lean4 experts in, we can even select all correct solutions. The qualitative analysis also shows our framework can make NL results formally backed by Lean4 proofs. As a pioneering work in the corresponding field, we will open-source all our models and datasets to further boost the development of the field. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æƒŠäººçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€é—®ç­”ç­‰ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼ŒLLMçš„æ¨ç†èƒ½åŠ›ä»ç„¶é¢ä¸´è®¸å¤šäº‰è®®ã€‚è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰å›ºæœ‰çš„æ¨¡ç³Šæ€§é™åˆ¶äº†LLMè¿›è¡ŒéªŒè¯æ¨ç†çš„èƒ½åŠ›ï¼Œä½¿å…¶ç­”æ¡ˆç¼ºä¹è¿è´¯æ€§å’Œå¯ä¿¡çš„æ”¯æŒã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFANSçš„æ–°å‹æ¡†æ¶ï¼šåˆ©ç”¨Lean4å¯¹è‡ªç„¶è¯­è¨€æ•°å­¦æ¨ç†è¿›è¡Œå½¢å¼åŒ–ç­”æ¡ˆé€‰æ‹©ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨Lean4å¢å¼ºLLMçš„NLæ•°å­¦æ¨ç†èƒ½åŠ›çš„æ¡†æ¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œç»™å®šä¸€ä¸ªNLæ•°å­¦é—®é¢˜ä»¥åŠLLMç”Ÿæˆçš„ç­”æ¡ˆï¼ŒFANSé¦–å…ˆå°†å…¶ç¿»è¯‘ä¸ºLean4å®šç†é™ˆè¿°ã€‚ç„¶åï¼Œå®ƒå°è¯•ä½¿ç”¨Lean4è¯æ˜å™¨è¿›è¡Œè¯æ˜å¹¶è¿›è¡ŒéªŒè¯ã€‚æœ€åï¼Œå®ƒä½¿ç”¨FLç»“æœæ¥è¾…åŠ©ç­”æ¡ˆé€‰æ‹©ã€‚å®ƒæé«˜äº†LLMæä¾›è®¡ç®—æœºå¯éªŒè¯çš„æ­£ç¡®ç­”æ¡ˆçš„èƒ½åŠ›ï¼Œå¹¶ä¸ºç­”æ¡ˆé€‰æ‹©æå‡ºäº†é™¤å¥–åŠ±æ¨¡å‹ä¹‹å¤–çš„æ›¿ä»£æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶éå¸¸æœ‰æ•ˆã€‚å®ƒå¯ä»¥åœ¨MATH-500æ•°æ®é›†ä¸Šæé«˜å¥–åŠ±æ¨¡å‹å¢å¼ºLLMçš„å‡†ç¡®ç‡é«˜è¾¾1.91%ï¼Œåœ¨AMC-23ä¸Šæé«˜é«˜è¾¾8.33%ï¼ŒåŸºäºå¼ºå¤§çš„å¥–åŠ±æ¨¡å‹åŸºçº¿ã€‚åœ¨ä¸€äº›ç‰¹å®šé¢†åŸŸï¼Œå¦‚æ•°è®ºç­‰Lean4æ“…é•¿çš„é¢†åŸŸï¼Œæˆ‘ä»¬ç”šè‡³å¯ä»¥é€‰å‡ºæ‰€æœ‰æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚å®šæ€§åˆ†æè¿˜è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥ä½¿NLç»“æœå¾—åˆ°Lean4è¯æ˜çš„æ­£å¼æ”¯æŒã€‚ä½œä¸ºè¯¥é¢†åŸŸçš„å¼€åˆ›æ€§å·¥ä½œï¼Œæˆ‘ä»¬å°†å¼€æºæ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03238v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€é—®ç­”ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºæƒŠäººèƒ½åŠ›ï¼Œä½†å…¶æ¨ç†èƒ½åŠ›ä»å—è´¨ç–‘ã€‚è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰çš„å›ºæœ‰æ¨¡ç³Šæ€§é™åˆ¶äº†LLMsè¿›è¡Œå¯éªŒè¯æ¨ç†çš„èƒ½åŠ›ï¼Œä½¿å…¶ç­”æ¡ˆç¼ºä¹è¿è´¯æ€§å’Œå¯ä¿¡çš„æ”¯æŒã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºFANSçš„æ–°å‹æ¡†æ¶ï¼šåˆ©ç”¨Lean4å¯¹è‡ªç„¶è¯­è¨€æ•°å­¦æ¨ç†è¿›è¡Œå½¢å¼åŒ–ç­”æ¡ˆé€‰æ‹©ã€‚FANSé¦–å…ˆå°†NLæ•°å­¦é—®é¢˜åŠLLMç”Ÿæˆçš„ç­”æ¡ˆç¿»è¯‘ä¸ºLean4å®šç†é™ˆè¿°ï¼Œç„¶åä½¿ç”¨Lean4è¯æ˜å™¨è¿›è¡Œè¯æ˜å’ŒéªŒè¯ï¼Œå†åˆ©ç”¨FLç»“æœè¾…åŠ©ç­”æ¡ˆé€‰æ‹©ã€‚æ­¤æ¡†æ¶æé«˜äº†LLMsåœ¨æä¾›è®¡ç®—æœºå¯éªŒè¯çš„æ­£ç¡®ç­”æ¡ˆæ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶ä¸ºç­”æ¡ˆé€‰æ‹©æä¾›äº†é™¤å¥–åŠ±æ¨¡å‹ä¹‹å¤–çš„æ›¿ä»£æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å¯æé«˜MATH-500æ•°æ®é›†å¥–åŠ±æ¨¡å‹å¢å¼ºLLMsçš„å‡†ç¡®ç‡è‡³å¤š1.91%ï¼ŒAMC-23çš„å‡†ç¡®ç‡è‡³å¤šæé«˜8.33%ã€‚åœ¨æŸäº›ç‰¹å®šé¢†åŸŸå¦‚æ•°è®ºä¸­ï¼Œæˆ‘ä»¬ç”šè‡³å¯ä»¥é€‰æ‹©å‡ºæ‰€æœ‰çš„æ­£ç¡®è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶å¯ä½¿NLç»“æœå¾—åˆ°Lean4è¯æ˜çš„æ­£å¼æ”¯æŒã€‚ä½œä¸ºè¯¥é¢†åŸŸçš„å¼€åˆ›æ€§å·¥ä½œï¼Œæˆ‘ä»¬å°†å¼€æºæ‰€æœ‰æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥è¿›ä¸€æ­¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰çš„æ¨¡ç³Šæ€§é™åˆ¶äº†LLMsè¿›è¡Œå¯éªŒè¯æ¨ç†ã€‚</li>
<li>æå‡ºçš„FANSæ¡†æ¶åˆ©ç”¨Lean4å¢å¼ºLLMsçš„è‡ªç„¶è¯­è¨€æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>FANSæ¡†æ¶é€šè¿‡ç¿»è¯‘ã€è¯æ˜ã€éªŒè¯å’Œç­”æ¡ˆé€‰æ‹©æµç¨‹æ¥æé«˜LLMsçš„å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨ç‰¹å®šé¢†åŸŸå¦‚æ•°è®ºä¸­ï¼ŒFANSèƒ½å¤Ÿé€‰æ‹©å‡ºæ‰€æœ‰æ­£ç¡®è§£å†³æ–¹æ¡ˆã€‚</li>
<li>FANSæ¡†æ¶ä½¿NLç»“æœå¾—åˆ°Lean4è¯æ˜çš„æ­£å¼æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03238">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bea95f306c720af41e6fa06dcf1f0f1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc336c24321243525d6cfd7298af7a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa2bb8cea0c4592216426f5c99849d60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff8683eedcb417d3f9c334bd16cb4313.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MA-LoT-Multi-Agent-Lean-based-Long-Chain-of-Thought-Reasoning-enhances-Formal-Theorem-Proving"><a href="#MA-LoT-Multi-Agent-Lean-based-Long-Chain-of-Thought-Reasoning-enhances-Formal-Theorem-Proving" class="headerlink" title="MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances   Formal Theorem Proving"></a>MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances   Formal Theorem Proving</h2><p><strong>Authors:Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang</strong></p>
<p>Solving mathematical problems using computer-verifiable languages like Lean has significantly impacted mathematical and computer science communities. State-of-the-art methods utilize single Large Language Models (LLMs) as agents or provers to either generate complete proof or perform tree searches. However, single-agent methods inherently lack a structured way to combine high-level reasoning in Natural Language (NL) with Formal Language (FL) verification feedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought framework, (to the best of our knowledge), the first multi-agent framework for Lean4 theorem proving that balance high-level NL reasoning and FL verification in Long CoT. Using this structured interaction, our approach enables deeper insights and long-term coherence in proof generation, with which past methods struggle. We do this by leveraging emergent formal reasoning ability in Long CoT using our novel LoT-Transfer Learning training-inference pipeline. Extensive experiments show that our framework achieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset, largely outperforming GPT-4 (22.95%), single-agent tree search (InternLM-Step-Prover, 50.70%), and whole-proof generation (DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight the potential of combining Long CoT with formal verification for a more insightful generation in a broader perspective. </p>
<blockquote>
<p>ä½¿ç”¨åƒLeanè¿™æ ·çš„å¯éªŒè¯è®¡ç®—æœºè¯­è¨€è§£å†³æ•°å­¦é—®é¢˜å¯¹æ•°å­¦å’Œè®¡ç®—æœºç§‘å­¦ç¤¾åŒºäº§ç”Ÿäº†é‡å¤§å½±å“ã€‚æœ€å…ˆè¿›çš„æ–¹æ³•ä½¿ç”¨å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä»£ç†æˆ–è¯æ˜è€…æ¥ç”Ÿæˆå®Œæ•´çš„è¯æ˜æˆ–æ‰§è¡Œæ ‘æœç´¢ã€‚ç„¶è€Œï¼Œå•ä»£ç†æ–¹æ³•æœ¬è´¨ä¸Šç¼ºä¹å°†è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰ä¸­çš„é«˜çº§æ¨ç†ä¸å½¢å¼è¯­è¨€ï¼ˆFLï¼‰éªŒè¯åé¦ˆç›¸ç»“åˆçš„ç»“æ„åŒ–æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MA-LoTï¼šåŸºäºLeançš„å¤šä»£ç†é•¿æ€è€ƒé“¾æ¡†æ¶ï¼ˆæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨Lean4å®šç†è¯æ˜ä¸­å¹³è¡¡é«˜çº§NLæ¨ç†å’ŒFLéªŒè¯çš„å¤šä»£ç†æ¡†æ¶ï¼‰ã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–çš„äº¤äº’ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨è¯æ˜ç”Ÿæˆè¿‡ç¨‹ä¸­æä¾›æ›´æ·±å…¥çš„è§è§£å’Œé•¿æœŸçš„ä¸€è‡´æ€§ï¼Œè¿™æ˜¯è¿‡å»çš„æ–¹æ³•æ‰€éš¾ä»¥åšåˆ°çš„ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨æˆ‘ä»¬çš„æ–°å‹LoT-è¿ç§»å­¦ä¹ è®­ç»ƒæ¨ç†ç®¡é“ä¸­çš„æ–°å…´å½¢å¼æ¨ç†èƒ½åŠ›æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨Lean4ç‰ˆæœ¬çš„MiniF2F-Testæ•°æ®é›†ä¸Šè¾¾åˆ°äº†54.51%çš„å‡†ç¡®ç‡ï¼Œå¤§å¤§è¶…è¿‡äº†GPT-4ï¼ˆ22.95%ï¼‰ã€å•ä»£ç†æ ‘æœç´¢ï¼ˆInternLM-Step-Proverï¼Œ50.70%ï¼‰å’Œæ•´ä¸ªè¯æ˜ç”Ÿæˆï¼ˆDeepSeek-Prover-v1.5ï¼Œ48.36%ï¼‰çš„åŸºå‡†çº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†é•¿æ€è€ƒé“¾ä¸å½¢å¼éªŒè¯ç›¸ç»“åˆï¼Œåœ¨æ›´å¹¿æ³›çš„è§†è§’ä¸­äº§ç”Ÿæ›´æœ‰æ´å¯ŸåŠ›çš„ç”Ÿæˆå…·æœ‰æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03205v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åˆ©ç”¨ Lean ç­‰è®¡ç®—æœºå¯éªŒè¯è¯­è¨€è§£å†³æ•°å­¦é—®é¢˜å¯¹æ•°å­¦å’Œè®¡ç®—æœºç§‘å­¦ç¤¾åŒºäº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚å½“å‰å…ˆè¿›çš„æ–¹æ³•ä½¿ç”¨å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä»£ç†æˆ–è¯æ˜è€…æ¥ç”Ÿæˆå®Œæ•´çš„è¯æ˜æˆ–æ‰§è¡Œæ ‘æœç´¢ã€‚ç„¶è€Œï¼Œå•ä»£ç†æ–¹æ³•ç¼ºä¹å°†è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰ä¸­çš„é«˜çº§æ¨ç†ä¸å½¢å¼è¯­è¨€ï¼ˆFLï¼‰éªŒè¯åé¦ˆç›¸ç»“åˆçš„ç»“æ„åŒ–æ–¹å¼ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† MA-LoTï¼šåŸºäº Lean çš„å¤šä»£ç†é•¿æ€ç»´é“¾æ¡†æ¶ï¼ˆæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨ Lean4 å®šç†è¯æ˜ä¸­å¹³è¡¡é«˜çº§ NL æ¨ç†å’Œ FL éªŒè¯çš„å¤šä»£ç†æ¡†æ¶ï¼‰ã€‚é€šè¿‡ç»“æ„åŒ–äº¤äº’ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨è¯æ˜ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°æ›´æ·±å…¥ã€æ›´é•¿æœŸçš„è¿è´¯æ€§ï¼Œè¿™æ˜¯è¿‡å»çš„æ–¹æ³•æ‰€éš¾ä»¥åšåˆ°çš„ã€‚æˆ‘ä»¬åˆ©ç”¨æ–°å…´çš„å½¢å¼åŒ–æ¨ç†èƒ½åŠ›åœ¨é•¿æ€ç»´é“¾ä¸­ä½¿ç”¨æˆ‘ä»¬æ–°é¢–çš„å¤šä»»åŠ¡å­¦ä¹ è®­ç»ƒæ¨ç†ç®¡é“æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ Lean4 ç‰ˆæœ¬çš„ MiniF2F æµ‹è¯•æ•°æ®é›†ä¸Šè¾¾åˆ°äº† 54.51% çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…è¶…è¶Šäº† GPT-4ï¼ˆ22.95%ï¼‰ã€å•ä»£ç†æ ‘æœç´¢ï¼ˆInternLM-Step-Proverï¼Œ50.70%ï¼‰å’Œå…¨è¯æ˜ç”Ÿæˆï¼ˆDeepSeek-Prover-v1.5ï¼Œ48.36%ï¼‰çš„åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å‘ç°çªæ˜¾äº†å°†é•¿æ€ç»´é“¾ä¸å½¢å¼éªŒè¯ç›¸ç»“åˆåœ¨æ›´å¹¿æ³›è§†è§’ä¸­äº§ç”Ÿæ›´å…·æ´å¯ŸåŠ›çš„ç”Ÿæˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨è®¡ç®—æœºå¯éªŒè¯è¯­è¨€å¦‚ Lean è§£å†³æ•°å­¦é—®é¢˜å¯¹æ•°å­¦å’Œè®¡ç®—æœºç§‘å­¦ç¤¾åŒºäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä½¿ç”¨å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®šç†è¯æ˜ä¸­ç”Ÿæˆå®Œæ•´è¯æ˜æˆ–æ‰§è¡Œæ ‘æœç´¢ã€‚</li>
<li>å•ä»£ç†æ–¹æ³•ç¼ºä¹ç»“åˆè‡ªç„¶è¯­è¨€é«˜çº§æ¨ç†ä¸å½¢å¼è¯­è¨€éªŒè¯çš„ç»“æ„åŒ–æ–¹å¼ã€‚</li>
<li>æå‡ºçš„ MA-LoT æ¡†æ¶æ˜¯é¦–ä¸ªåœ¨ Lean4 å®šç†è¯æ˜ä¸­å¹³è¡¡ NL æ¨ç†å’Œ FL éªŒè¯çš„å¤šä»£ç†æ¡†æ¶ã€‚</li>
<li>MA-LoT é€šè¿‡ç»“æ„åŒ–äº¤äº’å®ç°æ›´æ·±å…¥ã€é•¿æœŸçš„è¯æ˜ç”Ÿæˆè¿è´¯æ€§ã€‚</li>
<li>åˆ©ç”¨æ–°å…´çš„å½¢å¼åŒ–æ¨ç†èƒ½åŠ›åœ¨é•¿æ€ç»´é“¾ä¸­ï¼Œé€šè¿‡å¤šä»»åŠ¡å­¦ä¹ è®­ç»ƒæ¨ç†ç®¡é“å®ç°é«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-310c398853c706321d89b7c64c31fc26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e92f648da858e68121f21a10083f4e71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8c2848199ac38ebcee737f9456233b6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="KodCode-A-Diverse-Challenging-and-Verifiable-Synthetic-Dataset-for-Coding"><a href="#KodCode-A-Diverse-Challenging-and-Verifiable-Synthetic-Dataset-for-Coding" class="headerlink" title="KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for   Coding"></a>KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for   Coding</h2><p><strong>Authors:Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, Radha Poovendran</strong></p>
<p>We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºKodCodeï¼Œè¿™æ˜¯ä¸€æ¬¾åˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³åœ¨ç¼–ç æ–¹é¢è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´çš„ä¸€ä¸ªé•¿æœŸæŒ‘æˆ˜ï¼Œå³è·å–é«˜è´¨é‡ã€å¯éªŒè¯çš„è·¨å„ç§éš¾åº¦å’Œé¢†åŸŸçš„æ•°æ®é›†ã€‚ç°æœ‰çš„ä»¥ä»£ç ä¸ºä¸­å¿ƒçš„èµ„æºé€šå¸¸ä¸èƒ½ä¿è¯æ¶µç›–èŒƒå›´ï¼ˆä¾‹å¦‚æ¶µç›–ç®€å•çš„ç¼–ç ä»»åŠ¡åˆ°é«˜çº§ç®—æ³•é—®é¢˜ï¼‰æˆ–å¯éªŒè¯çš„æ­£ç¡®æ€§ï¼ˆä¾‹å¦‚å•å…ƒæµ‹è¯•ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒKodCodeåŒ…å«é—®é¢˜-è§£å†³æ–¹æ¡ˆ-æµ‹è¯•ä¸‰å…ƒç»„ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘éªŒè¯ç¨‹åºè¿›è¡Œç³»ç»Ÿçš„éªŒè¯ã€‚æˆ‘ä»¬çš„ç®¡é“é¦–å…ˆåˆæˆå¹¿æ³›çš„ç¼–ç é—®é¢˜ï¼Œç„¶åç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜è¿›è¡Œé¢å¤–å°è¯•ã€‚æœ€åï¼ŒåŸºäºæ¨ç†æ¨¡å‹ï¼ˆDeepSeek R1ï¼‰é€šè¿‡åŸºäºæµ‹è¯•çš„æ‹’ç»æŠ½æ ·ç¨‹åºç”Ÿæˆå›åº”ï¼Œæ¥å®Œæˆå¯¹è®­ç»ƒåçš„æ•°æ®åˆæˆæ”¹å†™é—®é¢˜æˆå¤šç§æ ¼å¼ã€‚è¿™ä½¿å¾—KadCodeæˆä¸ºå¤§è§„æ¨¡ã€ç¨³å¥ä¸”å¤šæ ·çš„ç¼–ç æ•°æ®é›†ã€‚KodCodeé€‚åˆç”¨äºç›‘ç£å¾®è°ƒï¼Œå¹¶ä¸”å…¶é…å¥—å•å…ƒæµ‹è¯•ä¹Ÿå…·å¤‡å·¨å¤§æ½œåŠ›ç”¨äºå¼ºåŒ–å­¦ä¹ å¾®è°ƒã€‚åœ¨ç¼–ç åŸºå‡†æµ‹è¯•ï¼ˆHumanEvalï¼ˆ+ï¼‰ã€MBPPï¼ˆ+ï¼‰ã€BigCodeBenchå’ŒLiveCodeBenchï¼‰ä¸Šè¿›è¡Œå¾®è°ƒå®éªŒè¡¨æ˜ï¼Œä½¿ç”¨KodCodeè®­ç»ƒçš„æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè¶…è¶Šäº†å¦‚Qwen2.5-Coder-32B-Instructå’ŒDeepSeek-R1-Distill-Llama-70Bç­‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02951v1">PDF</a> Codes and Data: <a target="_blank" rel="noopener" href="https://kodcode-ai.github.io/">https://kodcode-ai.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æŸ¯å¾·ç§‘å¾·ï¼ˆKodCodeï¼‰æ˜¯ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œè§£å†³äº†è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ç¼–ç æ—¶è·å–é«˜è´¨é‡ã€å¯éªŒè¯è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚ç°æœ‰ä»£ç èµ„æºé€šå¸¸æ— æ³•ä¿è¯è¦†ç›–èŒƒå›´å’Œæ­£ç¡®æ€§ã€‚æŸ¯å¾·ç§‘å¾·åŒ…å«é—®é¢˜-è§£å†³æ–¹æ¡ˆ-æµ‹è¯•ä¸‰å…ƒç»„ï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯ç¨‹åºè¿›è¡Œç³»ç»ŸéªŒè¯ã€‚æ­¤æ•°æ®é›†é€‚åˆç›‘ç£å¾®è°ƒï¼Œå…¶é…å¥—çš„å•å…ƒæµ‹è¯•ä¹Ÿé€‚åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚åœ¨å¤šä¸ªç¼–ç åŸºå‡†æµ‹è¯•ä¸Šçš„å¾®è°ƒå®éªŒè¡¨æ˜ï¼ŒæŸ¯å¾·ç§‘å¾·è°ƒå‚çš„æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¯å¾·ç§‘å¾·ï¼ˆKodCodeï¼‰æ˜¯ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰ç¼–ç èµ„æºå¾€å¾€è¦†ç›–é¢ä¸è¶³ä¸”æ­£ç¡®æ€§æ— æ³•å¾—åˆ°ä¿éšœã€‚</li>
<li>æŸ¯å¾·ç§‘å¾·åŒ…å«é—®é¢˜ã€è§£å†³æ–¹æ¡ˆå’Œæµ‹è¯•ä¸‰å…ƒç»„ï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯ç¨‹åºè¿›è¡ŒéªŒè¯ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡åˆæˆå¤šç§ç¼–ç é—®é¢˜ã€ç”Ÿæˆè§£å†³æ–¹æ¡ˆå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶å°è¯•è§£å†³éš¾é¢˜æ¥æ„å»ºã€‚</li>
<li>æŸ¯å¾·ç§‘å¾·é‡‡ç”¨æµ‹è¯•ä¸ºåŸºç¡€çš„æ•°æ®æ‹’ç»é‡‡æ ·ç¨‹åºç”Ÿæˆå“åº”ï¼Œç¡®ä¿æ•°æ®é›†çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚</li>
<li>æŸ¯å¾·ç§‘å¾·é€‚ç”¨äºç›‘ç£å¾®è°ƒï¼Œå¹¶ä¸”å…¶é…å¥—å•å…ƒæµ‹è¯•å…·æœ‰å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f8e4874ab902a0d7a1c116e20967335.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b22cb4a58b3df42f88bb488b04c92102.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f12777bf252465e2e373a60f40a3efc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88d02fd922b82e46f00efdbb98a0a2c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b00648fef2c80ef78ef09775f0c7de8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7608040760b34da04a655aae5e207ffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d15bcac5674cd57db786bc002115735.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Shakespearean-Sparks-The-Dance-of-Hallucination-and-Creativity-in-LLMsâ€™-Decoding-Layers"><a href="#Shakespearean-Sparks-The-Dance-of-Hallucination-and-Creativity-in-LLMsâ€™-Decoding-Layers" class="headerlink" title="Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMsâ€™   Decoding Layers"></a>Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMsâ€™   Decoding Layers</h2><p><strong>Authors:Zicong He, Boxuan Zhang, Lu Cheng</strong></p>
<p>Large language models (LLMs) are known to hallucinate, a phenomenon often linked to creativity. While previous research has primarily explored this connection through theoretical or qualitative lenses, our work takes a quantitative approach to systematically examine the relationship between hallucination and creativity in LLMs. Given the complex nature of creativity, we propose a narrow definition tailored to LLMs and introduce an evaluation framework, HCL, which quantifies Hallucination and Creativity across different Layers of LLMs during decoding. Our empirical analysis reveals a tradeoff between hallucination and creativity that is consistent across layer depth, model type, and model size. Notably, across different model architectures, we identify a specific layer at each model size that optimally balances this tradeoff. Additionally, the optimal layer tends to appear in the early layers of larger models, and the confidence of the model is also significantly higher at this layer. These findings provide a quantitative perspective that offers new insights into the interplay between LLM creativity and hallucination. The code and data for our experiments are available at <a target="_blank" rel="noopener" href="https://github.com/ZicongHe2002/HCL-Spark">https://github.com/ZicongHe2002/HCL-Spark</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šå‡ºç°å¹»è§‰ï¼Œè¿™ä¸€ç°è±¡é€šå¸¸ä¸åˆ›é€ åŠ›æœ‰å…³ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é€šè¿‡ç†è®ºæˆ–å®šæ€§è§†è§’æ¢ç´¢è¿™ä¸€è”ç³»ï¼Œä½†æˆ‘ä»¬çš„å·¥ä½œé‡‡ç”¨å®šé‡æ–¹æ³•æ¥ç³»ç»Ÿåœ°ç ”ç©¶LLMä¸­å¹»è§‰ä¸åˆ›é€ åŠ›çš„å…³ç³»ã€‚è€ƒè™‘åˆ°åˆ›é€ åŠ›çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬é’ˆå¯¹LLMæå‡ºäº†ä¸€ä¸ªç‹­ä¹‰çš„å®šä¹‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶HCLï¼Œè¯¥æ¡†æ¶åœ¨è§£ç è¿‡ç¨‹ä¸­é‡åŒ–LLMä¸åŒå±‚çº§çš„å¹»è§‰å’Œåˆ›é€ åŠ›ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ­ç¤ºäº†å¹»è§‰å’Œåˆ›é€ åŠ›ä¹‹é—´çš„æƒè¡¡ï¼Œè¿™ç§æƒè¡¡åœ¨ä¸åŒå±‚æ·±ã€æ¨¡å‹ç±»å‹å’Œæ¨¡å‹å¤§å°ä¸­æ˜¯ä¸€è‡´çš„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†æ¯ä¸ªæ¨¡å‹å¤§å°ä¸­ç‰¹å®šå±‚çº§èƒ½å¤Ÿæœ€ä½³åœ°å¹³è¡¡è¿™ç§æƒè¡¡ã€‚æ­¤å¤–ï¼Œæœ€ä½³å±‚çº§å¾€å¾€å‡ºç°åœ¨è¾ƒå¤§æ¨¡å‹çš„æ—©æœŸå±‚çº§ä¸­ï¼Œå¹¶ä¸”æ¨¡å‹åœ¨è¿™ä¸€å±‚çº§çš„ç½®ä¿¡åº¦ä¹Ÿæ˜æ˜¾æ›´é«˜ã€‚è¿™äº›å‘ç°æä¾›äº†ä¸€ä¸ªå®šé‡è§’åº¦ï¼Œä¸ºæ·±å…¥äº†è§£LLMåˆ›é€ åŠ›å’Œå¹»è§‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›äº†æ–°çš„è§è§£ã€‚æˆ‘ä»¬å®éªŒçš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZicongHe2002/HCL-Spark%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZicongHe2002/HCL-Sparkæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02851v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨å¹»è§‰ç°è±¡ï¼Œè¿™ä¸åˆ›é€ åŠ›æœ‰å…³ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦ä»ç†è®ºæˆ–å®šæ€§è§’åº¦æ¢ç´¢è¿™ç§è”ç³»ï¼Œè€Œæˆ‘ä»¬çš„å·¥ä½œåˆ™é‡‡ç”¨å®šé‡æ–¹æ³•æ¥ç³»ç»Ÿåœ°ç ”ç©¶LLMä¸­å¹»è§‰ä¸åˆ›é€ åŠ›çš„å…³ç³»ã€‚é’ˆå¯¹åˆ›é€ åŠ›çš„å¤æ‚æ€§è´¨ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹LLMçš„ç‹­çª„å®šä¹‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶HCLï¼Œè¯¥æ¡†æ¶åœ¨LLMè§£ç è¿‡ç¨‹ä¸­é‡åŒ–ä¸åŒå±‚çº§çš„å¹»è§‰å’Œåˆ›é€ åŠ›ã€‚å®è¯åˆ†ææ˜¾ç¤ºï¼Œå¹»è§‰ä¸åˆ›é€ åŠ›çš„æƒè¡¡åœ¨ä¸åŒå±‚çº§ã€æ¨¡å‹ç±»å‹å’Œæ¨¡å‹å¤§å°ä¸­éƒ½å­˜åœ¨ä¸€è‡´æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ä¸­ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªæ¨¡å‹å¤§å°ä¸­éƒ½è¯†åˆ«å‡ºä¸€ä¸ªç‰¹å®šå±‚çº§ï¼Œè¯¥å±‚çº§æœ€ä¼˜åœ°å¹³è¡¡äº†è¿™ç§æƒè¡¡ã€‚æ­¤å¤–ï¼Œåœ¨è¾ƒå¤§çš„æ¨¡å‹ä¸­ï¼Œæœ€ä½³å±‚çº§å¾€å¾€å‡ºç°åœ¨æ—©æœŸå±‚çº§ï¼Œä¸”è¯¥å±‚çº§çš„æ¨¡å‹ç½®ä¿¡åº¦ä¹Ÿæ˜¾è‘—æé«˜ã€‚è¿™äº›å‘ç°æä¾›äº†ä¸€ä¸ªå®šé‡è§†è§’ï¼Œä¸ºç†è§£LLMåˆ›é€ åŠ›å’Œå¹»è§‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äº§ç”Ÿå¹»è§‰ç°è±¡çš„åŒæ—¶å±•ç°å‡ºåˆ›é€ åŠ›ã€‚</li>
<li>é¦–æ¬¡é‡‡ç”¨å®šé‡æ–¹æ³•ç³»ç»Ÿç ”ç©¶LLMsä¸­å¹»è§‰ä¸åˆ›é€ åŠ›çš„å…³ç³»ã€‚</li>
<li>æå‡ºé’ˆå¯¹LLMsçš„ç‹­çª„å®šä¹‰åŠè¯„ä¼°æ¡†æ¶HCLï¼Œä»¥é‡åŒ–ä¸åŒå±‚çº§çš„å¹»è§‰å’Œåˆ›é€ åŠ›ã€‚</li>
<li>å‘ç°å¹»è§‰ä¸åˆ›é€ åŠ›çš„æƒè¡¡åœ¨ä¸åŒå±‚çº§ã€æ¨¡å‹ç±»å‹å’Œå¤§å°ä¸­ä¸€è‡´æ€§å­˜åœ¨ã€‚</li>
<li>è¯†åˆ«å‡ºåœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸­æ¯ä¸ªæ¨¡å‹å¤§å°çš„æœ€ä½³å±‚çº§ï¼Œè¯¥å±‚çº§åœ¨å¹³è¡¡å¹»è§‰ä¸åˆ›é€ åŠ›æ–¹é¢è¡¨ç°æœ€ä¼˜ã€‚</li>
<li>æœ€ä½³å±‚çº§å¾€å¾€å‡ºç°åœ¨è¾ƒå¤§æ¨¡å‹çš„æ—©æœŸå±‚çº§ï¼Œä¸”è¯¥å±‚çº§çš„æ¨¡å‹ç½®ä¿¡åº¦è¾ƒé«˜ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†å®šé‡è§†è§’ï¼Œä¸ºç†è§£LLMåˆ›é€ åŠ›å’Œå¹»è§‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›æ–°è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a77acdc050e6effddd093c84ab7b2c2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c110c1fe1092f93a69956b7907633622.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a6a1ca3305a74bc760753ce6deb34dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8f8815a9f205a52193c7e205215342f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AlignDistil-Token-Level-Language-Model-Alignment-as-Adaptive-Policy-Distillation"><a href="#AlignDistil-Token-Level-Language-Model-Alignment-as-Adaptive-Policy-Distillation" class="headerlink" title="AlignDistil: Token-Level Language Model Alignment as Adaptive Policy   Distillation"></a>AlignDistil: Token-Level Language Model Alignment as Adaptive Policy   Distillation</h2><p><strong>Authors:Songming Zhang, Xue Zhang, Tong Zhang, Bojie Hu, Yufeng Chen, Jinan Xu</strong></p>
<p>In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization. </p>
<blockquote>
<p>åœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼ŒLLMå¯¹é½è‡³å…³é‡è¦ï¼Œé€šå¸¸é€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰æ–¹æ³•å®ç°ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°ç°æœ‰çš„LLMå¯¹é½æ–¹æ³•ä¸­ï¼Œå“åº”ä¸­çš„æ‰€æœ‰æ ‡è®°éƒ½æ˜¯ä½¿ç”¨ç¨€ç–çš„å“åº”çº§å¥–åŠ±æˆ–åå¥½æ³¨é‡Šè¿›è¡Œä¼˜åŒ–ã€‚å¿½ç•¥æ ‡è®°çº§å¥–åŠ±å¯èƒ½ä¼šé”™è¯¯åœ°æƒ©ç½šé«˜è´¨é‡æ ‡è®°æˆ–é¼“åŠ±ä½è´¨é‡æ ‡è®°ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³å’Œæ”¶æ•›é€Ÿåº¦æ…¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AlignDistilï¼Œè¿™æ˜¯ä¸€ç§ä¸RLHFç›¸å½“çš„æ ‡è®°çº§å¥–åŠ±ä¼˜åŒ–è’¸é¦æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†DPOå­¦åˆ°çš„å¥–åŠ±å¼•å…¥RLHFç›®æ ‡ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†è¯¥ç›®æ ‡ä¸æ ‡è®°çº§è’¸é¦è¿‡ç¨‹çš„ç­‰ä»·æ€§ï¼Œå…¶ä¸­æ•™å¸ˆåˆ†å¸ƒå°†DPOæ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„é€»è¾‘çº¿æ€§ç»„åˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡æ„å»ºæ­£å¸¸çš„DPOå¥–åŠ±å’Œåå‘DPOå¥–åŠ±çš„å¯¹æ¯”ï¼Œè¿›ä¸€æ­¥ç¼©å°äº†DPOæ¨¡å‹å¥–åŠ±å’Œçº¯å¥–åŠ±æ¨¡å‹ä¹‹é—´çš„ç²¾åº¦å·®è·ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…ä¸åŒæ ‡è®°ä¸Šçš„æ¬ ä¼˜åŒ–å’Œè¿‡åº¦ä¼˜åŒ–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ ‡è®°è‡ªé€‚åº”é€»è¾‘æ‰©å±•æœºåˆ¶ï¼Œä¸ºæ¯ä¸ªæ ‡è®°æ„å»ºé€‚å½“çš„æ•™å¸ˆåˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AlignDistilä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ç”±äºå…¶æ ‡è®°çº§åˆ†å¸ƒå¥–åŠ±ä¼˜åŒ–è€Œå…·æœ‰çš„å¿«é€Ÿæ”¶æ•›æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02832v1">PDF</a> 15 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¯¹é½ï¼ˆalignmentï¼‰çš„é‡è¦æ€§åŠå…¶ç°æœ‰æ–¹æ³•ä¸­çš„ä¸è¶³ã€‚ä¸ºæé«˜LLMçš„æ€§èƒ½å’Œæ”¶æ•›é€Ÿåº¦ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAlignDistilçš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„å¥–åŠ±åˆ°å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰çš„ç›®æ ‡ä¸­ï¼Œå®ç°tokençº§åˆ«çš„å¥–åŠ±ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆDPOæ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„logitsæ„å»ºæ•™å¸ˆåˆ†å¸ƒï¼Œç¼©å°äº†DPOæ¨¡å‹å¥–åŠ±ä¸çº¯å¥–åŠ±æ¨¡å‹ä¹‹é—´çš„ç²¾åº¦å·®è·ã€‚åŒæ—¶ï¼Œä¸ºé¿å…ä¸åŒtokenä¸Šçš„æ¬ ä¼˜åŒ–å’Œè¿‡åº¦ä¼˜åŒ–ï¼Œè®¾è®¡äº†ä¸€ç§tokenè‡ªé€‚åº”logitå¤–æ¨æœºåˆ¶ï¼Œä¸ºæ¯ä¸ªtokenæ„å»ºé€‚å½“çš„æ•™å¸ˆåˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlignDistilä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å› å…¶tokençº§åˆ«çš„åˆ†å¸ƒå¥–åŠ±ä¼˜åŒ–è€Œå…·æœ‰å¿«é€Ÿæ”¶æ•›æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¯¹é½åœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è‡³å…³é‡è¦ï¼Œé€šå¸¸é€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å®ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥tokençº§åˆ«çš„å¥–åŠ±ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³å’Œæ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>AlignDistilæ–¹æ³•æ˜¯ä¸€ç§é’ˆå¯¹tokençº§åˆ«å¥–åŠ±ä¼˜åŒ–çš„RLHFç­‰æ•ˆè’¸é¦æ–¹æ³•ã€‚</li>
<li>AlignDistilç»“åˆäº†DPOæ¨¡å‹çš„å¥–åŠ±å’Œå‚è€ƒæ¨¡å‹çš„logitsï¼Œæ„å»ºäº†ä¸€ä¸ªæ•™å¸ˆåˆ†å¸ƒã€‚</li>
<li>é€šè¿‡å¯¹æ¯”æ­£å¸¸å’Œåå‘DPOæ¨¡å‹ï¼Œç¼©å°äº†å¥–åŠ±ç²¾åº¦å·®è·ã€‚</li>
<li>è®¾è®¡äº†tokenè‡ªé€‚åº”logitå¤–æ¨æœºåˆ¶ï¼Œä¸ºæ¯ä¸ªtokenæ„å»ºé€‚å½“çš„æ•™å¸ˆåˆ†å¸ƒï¼Œé¿å…è¿‡åº¦æˆ–ä¸è¶³çš„ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6844bf536df2202eca73d9cdc0be960.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-34663ef04c39e656a2ac47c6e89514ea.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Multidimensional-Consistency-Improves-Reasoning-in-Language-Models"><a href="#Multidimensional-Consistency-Improves-Reasoning-in-Language-Models" class="headerlink" title="Multidimensional Consistency Improves Reasoning in Language Models"></a>Multidimensional Consistency Improves Reasoning in Language Models</h2><p><strong>Authors:Huiyuan Lai, Xiao Zhang, Malvina Nissim</strong></p>
<p>While Large language models (LLMs) have proved able to address some complex reasoning tasks, we also know that they are highly sensitive to input variation, which can lead to different solution paths and final answers. Answer consistency across input variations can thus be taken as a sign of stronger confidence. Leveraging this insight, we introduce a framework, {\em Multidimensional Reasoning Consistency} where, focusing on math problems, models are systematically pushed to diversify solution paths towards a final answer, thereby testing them for answer consistency across multiple input variations. We induce variations in (i) order of shots in prompt, (ii) problem phrasing, and (iii) languages used. Extensive experiments on a large range of open-source state-of-the-art LLMs of various sizes show that reasoning consistency differs by variation dimension, and that by aggregating consistency across dimensions, our framework consistently enhances mathematical reasoning performance on both monolingual dataset GSM8K and multilingual dataset MGSM, especially for smaller models. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¯æ˜èƒ½å¤Ÿè§£å†³ä¸€äº›å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬ä¹ŸçŸ¥é“å®ƒä»¬å¯¹è¾“å…¥å˜åŒ–éå¸¸æ•æ„Ÿï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸åŒçš„è§£å†³æ–¹æ¡ˆè·¯å¾„å’Œæœ€ç»ˆç­”æ¡ˆã€‚å› æ­¤ï¼Œç­”æ¡ˆåœ¨ä¸åŒè¾“å…¥å˜åŒ–ä¸­çš„ä¸€è‡´æ€§å¯ä»¥ä½œä¸ºæ›´å¼ºä¿¡å¿ƒçš„æ ‡å¿—ã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå³â€œå¤šç»´åº¦æ¨ç†ä¸€è‡´æ€§â€ï¼Œåœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ•°å­¦é—®é¢˜ï¼Œç³»ç»Ÿåœ°æ¨åŠ¨æ¨¡å‹é€šè¿‡å¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆè·¯å¾„æ¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä»è€Œæµ‹è¯•å®ƒä»¬åœ¨å¤šç§è¾“å…¥å˜åŒ–ä¸­çš„ç­”æ¡ˆä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨ï¼ˆiï¼‰æç¤ºä¸­çš„å°„å‡»é¡ºåºã€ï¼ˆiiï¼‰é—®é¢˜è¡¨è¿°ã€ï¼ˆiiiï¼‰æ‰€ç”¨è¯­è¨€ç­‰æ–¹é¢å¼•å…¥å˜åŒ–ã€‚åœ¨å¤šç§å¼€æºçš„ã€æœ€æ–°æŠ€æœ¯çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸åŒç»´åº¦çš„æ¨ç†ä¸€è‡´æ€§å­˜åœ¨å·®å¼‚ï¼Œè€Œé€šè¿‡èšåˆå„ç»´åº¦çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å•è¯­æ•°æ®é›†GSM8Kå’Œå¤šè¯­è¨€æ•°æ®é›†MGSMä¸Šå§‹ç»ˆæé«˜äº†æ•°å­¦æ¨ç†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¯¹äºè¾ƒå°çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02670v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºä¸€å®šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¯¹è¾“å…¥å˜åŒ–éå¸¸æ•æ„Ÿï¼Œå¯èƒ½å¯¼è‡´ä¸åŒçš„è§£å†³æ–¹æ¡ˆè·¯å¾„å’Œæœ€ç»ˆç­”æ¡ˆã€‚ä¸ºæé«˜ç­”æ¡ˆçš„ä¸€è‡´æ€§ï¼Œç ”ç©¶å¼•å…¥äº†â€œå¤šç»´åº¦æ¨ç†ä¸€è‡´æ€§â€æ¡†æ¶ï¼Œé’ˆå¯¹æ•°å­¦é—®é¢˜ï¼Œé€šè¿‡æ¨åŠ¨æ¨¡å‹å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè·¯å¾„æ¥æµ‹è¯•å…¶å¯¹å¤šç§è¾“å…¥å˜åŒ–çš„ç­”æ¡ˆä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŒç»´åº¦çš„æ¨ç†ä¸€è‡´æ€§å­˜åœ¨å·®å¼‚ï¼Œé€šè¿‡è·¨ç»´åº¦èšåˆä¸€è‡´æ€§ï¼Œè¯¥æ¡†æ¶åœ¨å•è¯­æ•°æ®é›†GSM8Kå’Œå¤šè¯­ç§æ•°æ®é›†MGSMä¸Šå‡èƒ½æé«˜æ•°å­¦æ¨ç†æ€§èƒ½ï¼Œå¯¹å°å‹æ¨¡å‹å°¤å…¶æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶è™½æœ‰èƒ½åŠ›ï¼Œä½†å¯¹è¾“å…¥å˜åŒ–æ•æ„Ÿï¼Œå¯¼è‡´ç­”æ¡ˆä¸ä¸€è‡´ã€‚</li>
<li>â€œå¤šç»´åº¦æ¨ç†ä¸€è‡´æ€§â€æ¡†æ¶è¢«å¼•å…¥ï¼Œä»¥æµ‹è¯•æ¨¡å‹å¯¹å¤šç§è¾“å…¥å˜åŒ–çš„ç­”æ¡ˆä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ¨åŠ¨æ¨¡å‹å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè·¯å¾„æ¥ç³»ç»Ÿæµ‹è¯•æ•°å­¦é—®é¢˜çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥çš„æ¡†æ¶è€ƒè™‘äº†è¾“å…¥å˜åŒ–çš„å¤šä¸ªç»´åº¦ï¼Œå¦‚æç¤ºä¸­çš„å°„å‡»é¡ºåºã€é—®é¢˜æªè¾å’Œæ‰€ç”¨è¯­è¨€ã€‚</li>
<li>è·¨ç»´åº¦èšåˆä¸€è‡´æ€§æœ‰åŠ©äºæé«˜æ•°å­¦æ¨ç†æ€§èƒ½ã€‚</li>
<li>åœ¨å•è¯­å’Œå¤šè¯­ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯¹å°å‹æ¨¡å‹çš„æ€§èƒ½æå‡å°¤ä¸ºæ˜¾è‘—ã€‚</li>
<li>æ­¤æ¡†æ¶ä¸ºè¯„ä¼°å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d4ec07960e76b545dc6529f503fc719c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5def37d4393a17f8ee6d995e2af3426e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e509d42f79085ad11e2d2dcdf2c229e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81423a77a41ec09154418bba6ec1ef7c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-based-Threat-Assessment"><a href="#Reinforcement-Learning-based-Threat-Assessment" class="headerlink" title="Reinforcement Learning-based Threat Assessment"></a>Reinforcement Learning-based Threat Assessment</h2><p><strong>Authors:Wuzhou Sun, Siyi Li, Qingxiang Zou, Zixing Liao</strong></p>
<p>In some game scenarios, due to the uncertainty of the number of enemy units and the priority of various attributes, the evaluation of the threat level of enemy units as well as the screening has been a challenging research topic, and the core difficulty lies in how to reasonably set the priority of different attributes in order to achieve quantitative evaluation of the threat. In this paper, we innovatively transform the problem of threat assessment into a reinforcement learning problem, and through systematic reinforcement learning training, we successfully construct an efficient neural network evaluator. The evaluator can not only comprehensively integrate the multidimensional attribute features of the enemy, but also effectively combine our state information, thus realizing a more accurate and scientific threat assessment. </p>
<blockquote>
<p>åœ¨æŸäº›æ¸¸æˆåœºæ™¯ä¸­ï¼Œç”±äºæ•Œæ–¹å•ä½æ•°é‡å’Œå„å±æ€§çš„ä¼˜å…ˆçº§ä¸ç¡®å®šï¼Œæ•Œæ–¹å•ä½çš„å¨èƒç­‰çº§è¯„ä¼°åŠç­›é€‰ä¸€ç›´æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶è¯¾é¢˜ï¼Œå…¶æ ¸å¿ƒéš¾ç‚¹åœ¨äºå¦‚ä½•åˆç†è®¾ç½®ä¸åŒå±æ€§çš„ä¼˜å…ˆçº§ï¼Œä»¥å®ç°å¨èƒçš„å®šé‡è¯„ä¼°ã€‚æœ¬æ–‡åˆ›æ–°åœ°å°†å¨èƒè¯„ä¼°é—®é¢˜è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œé€šè¿‡ç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒæˆåŠŸæ„å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ç¥ç»ç½‘ç»œè¯„ä¼°å™¨ã€‚è¯¥è¯„ä¼°å™¨ä¸ä»…èƒ½ç»¼åˆæ•Œäººå¤šç»´å±æ€§çš„ç‰¹å¾ï¼Œè¿˜èƒ½æœ‰æ•ˆç»“åˆæˆ‘ä»¬çš„çŠ¶æ€ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®ã€æ›´ç§‘å­¦çš„å¨èƒè¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02612v1">PDF</a> 10 pages,9 figures</p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯è§£å†³æ¸¸æˆä¸­å¯¹æ•Œæ–¹å•ä½çš„å¨èƒè¯„ä¼°é—®é¢˜ï¼Œé€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œè¯„ä»·å™¨ï¼Œç»¼åˆè€ƒè™‘æ•Œæ–¹å¤šç»´å±æ€§ç‰¹å¾ï¼Œç»“åˆçŠ¶æ€ä¿¡æ¯ï¼Œå®ç°æ›´å‡†ç¡®ç§‘å­¦çš„å¨èƒè¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¸¸æˆä¸­æ•Œæ–¹å•ä½å¨èƒè¯„ä¼°æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶ä¸»é¢˜ã€‚</li>
<li>æ ¸å¿ƒéš¾ç‚¹åœ¨äºå¦‚ä½•åˆç†è®¾ç½®ä¸åŒå±æ€§çš„ä¼˜å…ˆçº§ï¼Œä»¥å®ç°å¨èƒçš„å®šé‡è¯„ä¼°ã€‚</li>
<li>æœ¬æ–‡å°†å¨èƒè¯„ä¼°é—®é¢˜åˆ›æ–°åœ°è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒæˆåŠŸæ„å»ºäº†é«˜æ•ˆçš„ç¥ç»ç½‘ç»œè¯„ä»·å™¨ã€‚</li>
<li>è¯„ä»·å™¨èƒ½ç»¼åˆæ•Œæ–¹å¤šç»´å±æ€§ç‰¹å¾ã€‚</li>
<li>è¯„ä»·å™¨èƒ½æœ‰æ•ˆç»“åˆçŠ¶æ€ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87b480f4343d484e55f29269eb9617a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a16527f9a03f27966c2df1faa5cead6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6410b3d8dab318cfff0b82306f7c51da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66dbd9bd7a431e3b77d081bbf0090e1a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs"><a href="#Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs" class="headerlink" title="Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs"></a>Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs</h2><p><strong>Authors:Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have demonstrated significant progress in perceiving and reasoning over multimodal inquiries, ushering in a new research era for foundation models. However, vision-language misalignment in MLLMs has emerged as a critical challenge, where the textual responses generated by these models are not factually aligned with the given text-image inputs. Existing efforts to address vision-language misalignment have focused on developing specialized vision-language connectors or leveraging visual instruction tuning from diverse domains. In this paper, we tackle this issue from a fundamental yet unexplored perspective by revisiting the core architecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs consisting of a causal attention mechanism, which limits the ability of earlier modalities (e.g., images) to incorporate information from later modalities (e.g., text). To address this problem, we propose AKI, a novel MLLM that unlocks causal attention into modality-mutual attention (MMA) to enable image tokens to attend to text tokens. This simple yet effective design allows AKI to achieve superior performance in 12 multimodal understanding benchmarks (+7.2% on average) without introducing additional parameters and increasing training time. Our MMA design is intended to be generic, allowing for application across various modalities, and scalable to accommodate diverse multimodal scenarios. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/sony/aki">https://github.com/sony/aki</a>, and we will release our AKI-4B model to encourage further advancements in MLLMs across various directions. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥å’Œå¤„ç†å¤šæ¨¡æ€æŸ¥è¯¢æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºåŸºç¡€æ¨¡å‹å¼€å¯äº†æ–°çš„ç ”ç©¶æ—¶ä»£ã€‚ç„¶è€Œï¼ŒMLLMsä¸­çš„è§†è§‰è¯­è¨€ä¸å¯¹é½é—®é¢˜å·²æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå…¶ä¸­è¿™äº›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å›å¤ä¸ç»™å®šçš„æ–‡æœ¬å›¾åƒè¾“å…¥åœ¨äº‹å®ä¸Šå¹¶ä¸å¯¹é½ã€‚ä¸ºè§£å†³è§†è§‰è¯­è¨€ä¸å¯¹é½é—®é¢˜ï¼Œç°æœ‰åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨å¼€å‘ä¸“é—¨çš„è§†è§‰è¯­è¨€è¿æ¥å™¨æˆ–ä»å„ç§é¢†åŸŸåˆ©ç”¨è§†è§‰æŒ‡ä»¤è°ƒæ•´ã€‚æœ¬æ–‡ä»ä¸€ä¸ªåŸºæœ¬ä¸”æœªè¢«æ¢ç´¢çš„è§†è§’æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡é‡æ–°æ£€æŸ¥MLLMsçš„æ ¸å¿ƒæ¶æ„ã€‚å¤§å¤šæ•°MLLMsé€šå¸¸å»ºç«‹åœ¨ä»…è§£ç çš„LLMsä¸Šï¼Œç”±å› æœæ³¨æ„æœºåˆ¶ç»„æˆï¼Œè¿™é™åˆ¶äº†æ—©æœŸæ¨¡æ€ï¼ˆä¾‹å¦‚å›¾åƒï¼‰èå…¥åæœŸæ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬ï¼‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AKIï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹MLLMï¼Œå®ƒå°†å› æœæ³¨æ„è§£é”ä¸ºæ¨¡æ€ç›¸äº’æ³¨æ„ï¼ˆMMAï¼‰ï¼Œä½¿å›¾åƒæ ‡è®°èƒ½å¤Ÿå…³æ³¨æ–‡æœ¬æ ‡è®°ã€‚è¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ä½¿AKIèƒ½å¤Ÿåœ¨12ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°å“è¶Šæ€§èƒ½ï¼ˆå¹³å‡æé«˜7.2%ï¼‰ï¼ŒåŒæ—¶ä¸å¼•å…¥é¢å¤–å‚æ•°å¹¶å¢åŠ è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬çš„MMAè®¾è®¡æ˜¯é€šç”¨çš„ï¼Œå¯åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œå¹¶ä¸”å¯æ‰©å±•åˆ°é€‚åº”å„ç§å¤šæ¨¡æ€åœºæ™¯ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/sony/aki">https://github.com/sony/aki</a>ï¼Œæˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„AKI-4Bæ¨¡å‹ï¼Œä»¥é¼“åŠ±åœ¨MLLMsçš„å„ä¸ªé¢†åŸŸè¿›ä¸€æ­¥å–å¾—è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02597v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºAKIçš„æ–°å‹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è§£é”å› æœæ³¨æ„åŠ›ä¸ºæ¨¡æ€é—´ç›¸äº’æ³¨æ„åŠ›ï¼ˆMMAï¼‰ï¼Œä½¿å›¾åƒæ ‡è®°èƒ½å¤Ÿå…³æ³¨æ–‡æœ¬æ ‡è®°ï¼Œä»è€Œè§£å†³ç°æœ‰æ¨¡å‹ä¸­çš„è§†è§‰è¯­è¨€ä¸åŒ¹é…é—®é¢˜ã€‚è¯¥è®¾è®¡åœ¨12ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡+7.2%çš„ä¼˜è¶Šæ€§èƒ½æå‡ï¼ŒåŒæ—¶æœªå¼•å…¥é¢å¤–çš„å‚æ•°å’Œå¢åŠ è®­ç»ƒæ—¶é—´ã€‚AKIè®¾è®¡æ—¨åœ¨å…·æœ‰é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯åº”ç”¨äºå„ç§æ¨¡æ€å’Œå¤šæ¨¡æ€åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥å’Œæ¨ç†å¤šæ¨¡æ€æŸ¥è¯¢æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è§†è§‰è¯­è¨€ä¸åŒ¹é…æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¸“æ³¨äºå¼€å‘ä¸“é—¨çš„è§†è§‰è¯­è¨€è¿æ¥å™¨æˆ–ä»å¤šä¸ªé¢†åŸŸåˆ©ç”¨è§†è§‰æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹MLLMâ€”â€”AKIï¼Œé€šè¿‡è§£é”å› æœæ³¨æ„åŠ›ä¸ºæ¨¡æ€é—´ç›¸äº’æ³¨æ„åŠ›ï¼ˆMMAï¼‰ï¼Œä½¿å›¾åƒæ ‡è®°èƒ½å¤Ÿå…³æ³¨æ–‡æœ¬æ ‡è®°ã€‚</li>
<li>AKIåœ¨å¤šä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°ä¼˜è¶Šæ€§èƒ½æå‡ã€‚</li>
<li>è®¾è®¡ç®€æ´æœ‰æ•ˆï¼Œæœªå¼•å…¥é¢å¤–çš„å‚æ•°å’Œå¢åŠ è®­ç»ƒæ—¶é—´ã€‚</li>
<li>AKIè®¾è®¡å…·æœ‰é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯åº”ç”¨äºå„ç§æ¨¡æ€å’Œå¤šæ¨¡æ€åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f9631bb198214146b9ebc74bab028622.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba860edccebed781a143e8644642aa31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-913470c87169155f8b7ee470a3116194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c646ce9fb294b1e9730b899e36de328.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c84f01c51ad0762dc24abedfc986e6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AutoEval-A-Practical-Framework-for-Autonomous-Evaluation-of-Mobile-Agents"><a href="#AutoEval-A-Practical-Framework-for-Autonomous-Evaluation-of-Mobile-Agents" class="headerlink" title="AutoEval: A Practical Framework for Autonomous Evaluation of Mobile   Agents"></a>AutoEval: A Practical Framework for Autonomous Evaluation of Mobile   Agents</h2><p><strong>Authors:Jiahui Sun, Zhichao Hua, Yubin Xia</strong></p>
<p>Accurate and systematic evaluation of mobile agents can significantly advance their development and real-world applicability. However, existing benchmarks for mobile agents lack practicality and scalability due to the extensive manual effort required to define task reward signals and implement corresponding evaluation codes. To this end, we propose AutoEval, an autonomous agent evaluation framework that tests a mobile agent without any manual effort. First, we design a Structured Substate Representation to describe the UI state changes while agent execution, such that task reward signals can be automatically generated. Second, we utilize a Judge System that can autonomously evaluate agentsâ€™ performance given the automatically generated task reward signals. By providing only a task description, our framework evaluates agents with fine-grained performance feedback to that task without any extra manual effort. We implement a prototype of our framework and validate the automatically generated task reward signals, finding over 93% coverage to human-annotated reward signals. Moreover, to prove the effectiveness of our autonomous Judge System, we manually verify its judge results and demonstrate that it achieves 94% accuracy. Finally, we evaluate the state-of-the-art mobile agents using our framework, providing detailed insights into their performance characteristics and limitations. </p>
<blockquote>
<p>å¯¹ç§»åŠ¨ä»£ç†è¿›è¡Œå‡†ç¡®ã€ç³»ç»Ÿçš„è¯„ä¼°å¯ä»¥æ˜¾è‘—ä¿ƒè¿›å…¶å‘å±•å’Œåœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ã€‚ç„¶è€Œï¼Œç”±äºå®šä¹‰ä»»åŠ¡å¥–åŠ±ä¿¡å·å’Œå®ç°ç›¸åº”è¯„ä¼°ä»£ç éœ€è¦å¤§é‡çš„äººå·¥åŠªåŠ›ï¼Œç°æœ‰çš„ç§»åŠ¨ä»£ç†åŸºå‡†æµ‹è¯•ç¼ºä¹å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AutoEvalï¼Œä¸€ä¸ªæ— éœ€äººå·¥åŠªåŠ›çš„è‡ªä¸»ä»£ç†è¯„ä¼°æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç»“æ„åŒ–å­çŠ¶æ€è¡¨ç¤ºæ¥æè¿°ä»£ç†æ‰§è¡Œè¿‡ç¨‹ä¸­çš„UIçŠ¶æ€å˜åŒ–ï¼Œä»è€Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆä»»åŠ¡å¥–åŠ±ä¿¡å·ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆ¤æ–­ç³»ç»Ÿï¼Œæ ¹æ®è‡ªåŠ¨ç”Ÿæˆçš„ä»»åŠ¡å¥–åŠ±ä¿¡å·è‡ªä¸»è¯„ä¼°ä»£ç†çš„æ€§èƒ½ã€‚åªè¦æä¾›ä»»åŠ¡æè¿°ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å°±èƒ½å¯¹ä»£ç†è¿›è¡Œç²¾ç»†çš„åé¦ˆè¯„ä¼°ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„äººå·¥åŠªåŠ›ã€‚æˆ‘ä»¬å®ç°äº†æ¡†æ¶çš„åŸå‹ï¼Œå¹¶éªŒè¯äº†è‡ªåŠ¨ç”Ÿæˆçš„ä»»åŠ¡å¥–åŠ±ä¿¡å·ï¼Œå‘ç°å…¶è¦†ç›–ç‡è¶…è¿‡93%ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯æ˜æˆ‘ä»¬è‡ªä¸»çš„åˆ¤æ–­ç³»ç»Ÿæœ‰æ•ˆï¼Œæˆ‘ä»¬æ‰‹åŠ¨éªŒè¯äº†å…¶åˆ¤æ–­ç»“æœï¼Œå¹¶è¯æ˜å…¶å‡†ç¡®ç‡è¾¾åˆ°äº†94%ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶å¯¹æœ€å…ˆè¿›çš„ç§»åŠ¨ä»£ç†è¿›è¡Œäº†è¯„ä¼°ï¼Œæä¾›äº†å…³äºå…¶æ€§èƒ½ç‰¹å¾å’Œå±€é™æ€§çš„è¯¦ç»†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§åä¸ºAutoEvalçš„è‡ªä¸»ä»£ç†è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¯¹ç§»åŠ¨ä»£ç†çš„å‡†ç¡®å’Œç³»ç»ŸåŒ–è¯„ä¼°ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–å­çŠ¶æ€è¡¨ç¤ºæ¥æè¿°ä»£ç†æ‰§è¡Œæ—¶çš„UIçŠ¶æ€å˜åŒ–ï¼Œè‡ªåŠ¨ç”Ÿæˆä»»åŠ¡å¥–åŠ±ä¿¡å·ï¼Œå¹¶åˆ©ç”¨è¯„åˆ¤ç³»ç»Ÿè‡ªä¸»è¯„ä¼°ä»£ç†æ€§èƒ½ã€‚è¯¥æ¡†æ¶åªéœ€æä¾›ä»»åŠ¡æè¿°ï¼Œå³å¯å¯¹ä»£ç†è¿›è¡Œç²¾ç»†çš„åé¦ˆè¯„ä¼°ï¼Œæ— éœ€é¢å¤–çš„äººå·¥åŠªåŠ›ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶çš„ä»»åŠ¡å¥–åŠ±ä¿¡å·è¦†ç›–ç‡é«˜ï¼Œè¯„åˆ¤ç³»ç»Ÿå‡†ç¡®ç‡é«˜ï¼Œå¹¶èƒ½å¯¹ç°æœ‰çš„ç§»åŠ¨ä»£ç†è¿›è¡Œè¯¦å°½çš„æ€§èƒ½è¯„ä»·å’Œå±€é™æ€§åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AutoEvalæ˜¯ä¸€ä¸ªæ— éœ€äººå·¥åŠªåŠ›çš„è‡ªä¸»ä»£ç†è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ¨è¿›ç§»åŠ¨ä»£ç†çš„å‘å±•å’Œå®é™…åº”ç”¨ã€‚</li>
<li>é€šè¿‡ç»“æ„åŒ–å­çŠ¶æ€è¡¨ç¤ºæè¿°UIçŠ¶æ€å˜åŒ–ï¼Œè‡ªåŠ¨ç”Ÿæˆä»»åŠ¡å¥–åŠ±ä¿¡å·ã€‚</li>
<li>åˆ©ç”¨è¯„åˆ¤ç³»ç»Ÿè‡ªä¸»è¯„ä¼°ä»£ç†æ€§èƒ½ï¼Œæä¾›ç²¾ç»†çš„åé¦ˆè¯„ä¼°ã€‚</li>
<li>æ¡†æ¶çš„ä»»åŠ¡å¥–åŠ±ä¿¡å·è¦†ç›–ç‡é«˜ï¼Œä¸äººå·¥æ ‡æ³¨çš„ä¿¡å·ç›¸æ¯”è¶…è¿‡93%ã€‚</li>
<li>è¯„åˆ¤ç³»ç»Ÿçš„å‡†ç¡®ç‡é«˜è¾¾94%ï¼Œç»è¿‡æ‰‹åŠ¨éªŒè¯ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½è¯„ä»·ç°æœ‰ç§»åŠ¨ä»£ç†çš„æ€§èƒ½ç‰¹æ€§ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ´å¯Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ecc28377a445ae2373116bfd49515a1e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5c6da1308d914bc85c92e172bf6e50a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-392524d63539c6796b5eaf9005790689.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b3e1e854f80819bcb36d4dc3bc2cac5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c76442fc7855ea05810cfa77602b02c5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ReSo-A-Reward-driven-Self-organizing-LLM-based-Multi-Agent-System-for-Reasoning-Tasks"><a href="#ReSo-A-Reward-driven-Self-organizing-LLM-based-Multi-Agent-System-for-Reasoning-Tasks" class="headerlink" title="ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for   Reasoning Tasks"></a>ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for   Reasoning Tasks</h2><p><strong>Authors:Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei Yin, Lei Bai</strong></p>
<p>Multi-agent systems have emerged as a promising approach for enhancing the reasoning capabilities of large language models in complex problem-solving. However, current MAS frameworks are limited by poor flexibility and scalability, with underdeveloped optimization strategies. To address these challenges, we propose ReSo, which integrates task graph generation with a reward-driven two-stage agent selection process. The core of ReSo is the proposed Collaborative Reward Model, which can provide fine-grained reward signals for MAS cooperation for optimization. We also introduce an automated data synthesis framework for generating MAS benchmarks, without human annotations. Experimentally, ReSo matches or outperforms existing methods. ReSo achieves \textbf{33.7%} and \textbf{32.3%} accuracy on Math-MAS and SciBench-MAS SciBench, while other methods completely fail. Code is available at: \href{<a target="_blank" rel="noopener" href="https://github.com/hengzzzhou/ReSo%7D%7BReSo%7D">https://github.com/hengzzzhou/ReSo}{ReSo}</a> </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œåœ¨å¤æ‚é—®é¢˜è§£å†³çš„é¢†åŸŸä¸­ï¼Œæœ‰æœ›å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶å—é™äºçµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä¸è¶³ï¼Œä»¥åŠä¼˜åŒ–ç­–ç•¥ä¸å¤Ÿæˆç†Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReSoæ–¹æ³•ï¼Œå®ƒå°†ä»»åŠ¡å›¾ç”Ÿæˆä¸å¥–åŠ±é©±åŠ¨çš„ä¸¤é˜¶æ®µæ™ºèƒ½ä½“é€‰æ‹©è¿‡ç¨‹ç›¸ç»“åˆã€‚ReSoçš„æ ¸å¿ƒæ˜¯æå‡ºçš„ååŒå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åˆä½œæä¾›ç²¾ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·ä»¥å®ç°ä¼˜åŒ–ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•é›†ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚å®éªŒè¯æ˜ï¼ŒReSoä¸ç°æœ‰æ–¹æ³•ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä¼˜ã€‚ReSoåœ¨Math-MASå’ŒSciBench-MAS SciBenchä¸Šåˆ†åˆ«å®ç°äº†**33.7%<strong>å’Œ</strong>32.3%**çš„å‡†ç¡®ç‡ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™å®Œå…¨å¤±è´¥ã€‚ç›¸å…³ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/hengzzzhou/ReSo%E8%BF%9B%E8%A1%8C%E7%9C%8B%E6%9F%A5%E3%80%82">https://github.com/hengzzzhou/ReSoè¿›è¡ŒæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02390v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæœ‰æœ›æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é—®é¢˜è§£å†³ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶å—é™äºçµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä¸è¶³ï¼Œä¼˜åŒ–ç­–ç•¥ä¹Ÿå°šæœªå®Œå–„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReSoï¼Œå®ƒç»“åˆäº†ä»»åŠ¡å›¾ç”Ÿæˆå’Œå¥–åŠ±é©±åŠ¨çš„ä¸¤é˜¶æ®µæ™ºèƒ½ä½“é€‰æ‹©è¿‡ç¨‹ã€‚ReSoçš„æ ¸å¿ƒæ˜¯æå‡ºçš„ååŒå¥–åŠ±æ¨¡å‹ï¼Œå¯ä»¥ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åˆä½œæä¾›ç²¾ç»†çš„å¥–åŠ±ä¿¡å·ä»¥å®ç°ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤šæ™ºèƒ½ä½“ç³»ç»ŸåŸºå‡†æµ‹è¯•é›†ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚å®éªŒè¡¨æ˜ï¼ŒReSoçš„æ€§èƒ½ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”è¾¾åˆ°æˆ–è¶…è¿‡ï¼Œåœ¨Math-MASå’ŒSciBench-MASä¸Šåˆ†åˆ«å®ç°äº†33.7%å’Œ32.3%çš„å‡†ç¡®ç‡ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™å®Œå…¨å¤±è´¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰MASæ¡†æ¶å­˜åœ¨çµæ´»æ€§å’Œå¯æ‰©å±•æ€§é—®é¢˜ï¼Œä»¥åŠä¼˜åŒ–ç­–ç•¥çš„ä¸è¶³ã€‚</li>
<li>ReSoé€šè¿‡ç»“åˆä»»åŠ¡å›¾ç”Ÿæˆå’Œå¥–åŠ±é©±åŠ¨çš„ä¸¤é˜¶æ®µæ™ºèƒ½ä½“é€‰æ‹©æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>ReSoçš„æ ¸å¿ƒæ˜¯ååŒå¥–åŠ±æ¨¡å‹ï¼Œæä¾›ç²¾ç»†å¥–åŠ±ä¿¡å·ä¼˜åŒ–MASåˆä½œã€‚</li>
<li>å¼•å…¥è‡ªåŠ¨åŒ–æ•°æ®åˆæˆæ¡†æ¶ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ç”ŸæˆMASåŸºå‡†æµ‹è¯•é›†ã€‚</li>
<li>ReSoåœ¨Math-MASå’ŒSciBench-MASä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†33.7%å’Œ32.3%ã€‚</li>
<li>å…¶ä»–æ–¹æ³•åœ¨Math-MASå’ŒSciBench-MASä¸Šçš„è¡¨ç°ä¸ä½³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3ca3be22e09e23623ec1cfa0589f0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b7ce059c252e0e00369fd17aab3a5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2b5cecbabc593af58eec2d8439d7cde.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="An-Efficient-and-Precise-Training-Data-Construction-Framework-for-Process-supervised-Reward-Model-in-Mathematical-Reasoning"><a href="#An-Efficient-and-Precise-Training-Data-Construction-Framework-for-Process-supervised-Reward-Model-in-Mathematical-Reasoning" class="headerlink" title="An Efficient and Precise Training Data Construction Framework for   Process-supervised Reward Model in Mathematical Reasoning"></a>An Efficient and Precise Training Data Construction Framework for   Process-supervised Reward Model in Mathematical Reasoning</h2><p><strong>Authors:Wei Sun, Qianlong Du, Fuwei Cui, Jiajun Zhang</strong></p>
<p>Enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) is of great scientific and practical significance. Researchers typically employ process-supervised reward models (PRMs) to guide the reasoning process, effectively improving the modelsâ€™ reasoning abilities. However, existing methods for constructing process supervision training data, such as manual annotation and per-step Monte Carlo estimation, are often costly or suffer from poor quality. To address these challenges, this paper introduces a framework called EpicPRM, which annotates each intermediate reasoning step based on its quantified contribution and uses an adaptive binary search algorithm to enhance both annotation precision and efficiency. Using this approach, we efficiently construct a high-quality process supervision training dataset named Epic50k, consisting of 50k annotated intermediate steps. Compared to other publicly available datasets, the PRM trained on Epic50k demonstrates significantly superior performance. Getting Epic50k at <a target="_blank" rel="noopener" href="https://github.com/xiaolizh1/EpicPRM">https://github.com/xiaolizh1/EpicPRM</a>. </p>
<blockquote>
<p>å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›åœ¨ç§‘å­¦ä¸å®é™…åº”ç”¨ä¸­å…·æœ‰é‡å¤§æ„ä¹‰ã€‚ç ”ç©¶è€…é€šå¸¸é‡‡ç”¨æµç¨‹ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ¥å¼•å¯¼æ¨ç†è¿‡ç¨‹ï¼Œæœ‰æ•ˆæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ„å»ºæµç¨‹ç›‘ç£è®­ç»ƒæ•°æ®çš„æ–¹æ³•ï¼Œå¦‚æ‰‹åŠ¨æ ‡æ³¨å’Œåˆ†æ­¥è’™ç‰¹å¡æ´›ä¼°è®¡ï¼Œå¾€å¾€æˆæœ¬é«˜æ˜‚æˆ–è´¨é‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåä¸ºEpicPRMçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºä¸­é—´æ¨ç†æ­¥éª¤çš„é‡åŒ–è´¡çŒ®è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ä½¿ç”¨è‡ªé€‚åº”äºŒåˆ†æœç´¢ç®—æ³•æé«˜æ ‡æ³¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„è¿‡ç¨‹ç›‘ç£è®­ç»ƒæ•°æ®é›†Epic50kï¼ŒåŒ…å«5ä¸‡ä¸ªæ ‡æ³¨çš„ä¸­é—´æ­¥éª¤ã€‚ä¸å…¶ä»–å…¬å¼€æ•°æ®é›†ç›¸æ¯”ï¼Œåœ¨Epic50kä¸Šè®­ç»ƒçš„PRMè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/xiaolizh1/EpicPRM%E8%8E%B7%E5%8F%96Epic50k%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/xiaolizh1/EpicPRMè·å–Epic50kæ•°æ®é›†ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02382v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›å¢å¼ºå…·æœ‰é‡è¦çš„ç§‘å­¦å’Œå®é™…æ„ä¹‰ã€‚ç ”ç©¶è€…é€šå¸¸ä½¿ç”¨è¿‡ç¨‹ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ¥å¼•å¯¼æ¨ç†è¿‡ç¨‹ï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ„å»ºè¿‡ç¨‹ç›‘ç£è®­ç»ƒæ•°æ®çš„æ–¹æ³•ï¼Œå¦‚æ‰‹åŠ¨æ ‡æ³¨å’Œåˆ†æ­¥è’™ç‰¹å¡æ´›ä¼°è®¡ï¼Œæˆæœ¬è¾ƒé«˜æˆ–è´¨é‡ä¸ä½³ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåä¸ºEpicPRMçš„æ¡†æ¶ï¼Œå®ƒæ ¹æ®æ¯ä¸ªä¸­é—´æ¨ç†æ­¥éª¤çš„é‡åŒ–è´¡çŒ®è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ä½¿ç”¨è‡ªé€‚åº”äºŒè¿›åˆ¶æœç´¢ç®—æ³•æé«˜æ ‡æ³¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚é€šè¿‡ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°æ„å»ºäº†é«˜è´¨é‡çš„è¿‡ç¨‹ç›‘ç£è®­ç»ƒæ•°æ®é›†Epic50kï¼ŒåŒ…å«5ä¸‡ä¸ªæ ‡æ³¨çš„ä¸­é—´æ­¥éª¤ã€‚ä¸å…¶ä»–å…¬å¼€æ•°æ®é›†ç›¸æ¯”ï¼Œåœ¨Epic50kä¸Šè®­ç»ƒçš„PRMè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ•°å­¦æ¨ç†èƒ½åŠ›å¢å¼ºå¾ˆé‡è¦ã€‚</li>
<li>è¿‡ç¨‹ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ç”¨äºæé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ„å»ºè¿‡ç¨‹ç›‘ç£è®­ç»ƒæ•°æ®çš„æ–¹æ³•æˆæœ¬è¾ƒé«˜æˆ–è´¨é‡ä¸ä½³ã€‚</li>
<li>EpicPRMæ¡†æ¶é€šè¿‡é‡åŒ–ä¸­é—´æ¨ç†æ­¥éª¤çš„è´¡çŒ®è¿›è¡Œæ ‡æ³¨ã€‚</li>
<li>EpicPRMä½¿ç”¨è‡ªé€‚åº”äºŒè¿›åˆ¶æœç´¢ç®—æ³•æé«˜æ ‡æ³¨çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„è¿‡ç¨‹ç›‘ç£è®­ç»ƒæ•°æ®é›†Epic50kï¼ŒåŒ…å«5ä¸‡ä¸ªæ ‡æ³¨çš„ä¸­é—´æ­¥éª¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02382">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0965bd0cc648b522ae99ea8b522191ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b73e3435afde0b2ff7c24d75f49946f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea17b162612e9967b4a4ec50eacd19ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d776930cb2885c21e8b244d74bd77ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-683a2c8c590630e6becc9efb3f7afb03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a702c56e1693f4d94bce58a67fcde0c3.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Are-Large-Vision-Language-Models-Good-Game-Players"><a href="#Are-Large-Vision-Language-Models-Good-Game-Players" class="headerlink" title="Are Large Vision Language Models Good Game Players?"></a>Are Large Vision Language Models Good Game Players?</h2><p><strong>Authors:Xinyu Wang, Bohan Zhuang, Qi Wu</strong></p>
<p>Large Vision Language Models (LVLMs) have demonstrated remarkable abilities in understanding and reasoning about both visual and textual information. However, existing evaluation methods for LVLMs, primarily based on benchmarks like Visual Question Answering and image captioning, often fail to capture the full scope of LVLMsâ€™ capabilities. These benchmarks are limited by issues such as inadequate assessment of detailed visual perception, data contamination, and a lack of focus on multi-turn reasoning. To address these challenges, we propose \method{}, a game-based evaluation framework designed to provide a comprehensive assessment of LVLMsâ€™ cognitive and reasoning skills in structured environments. \method{} uses a set of games to evaluate LVLMs on four core tasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing, with each target task designed to assess specific abilities, including visual perception, reasoning, decision-making, etc. Based on this framework, we conduct extensive experiments that explore the limitations of current LVLMs, such as handling long structured outputs and perceiving detailed and dense elements. Code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/xinke-wang/LVLM-Playground">https://github.com/xinke-wang/LVLM-Playground</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç†è§£å’Œæ¨ç†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LVLMsè¯„ä¼°æ–¹æ³•ï¼Œä¸»è¦åŸºäºè§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ç­‰åŸºå‡†æµ‹è¯•ï¼Œå¾€å¾€æ— æ³•å…¨é¢æ•æ‰LVLMsçš„èƒ½åŠ›èŒƒå›´ã€‚è¿™äº›åŸºå‡†æµ‹è¯•å—é™äºè¯¦ç»†è§†è§‰æ„ŸçŸ¥è¯„ä¼°ä¸è¶³ã€æ•°æ®æ±¡æŸ“ä»¥åŠç¼ºä¹å¤šè½®æ¨ç†çš„ç„¦ç‚¹ç­‰é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†\method{}ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¸¸æˆçš„è®¾è®¡è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›å¯¹LVLMsåœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­çš„è®¤çŸ¥å’Œæ¨ç†æŠ€èƒ½çš„å…¨é¢è¯„ä¼°ã€‚\method{}ä½¿ç”¨ä¸€ç³»åˆ—æ¸¸æˆæ¥è¯„ä¼°LVLMsåœ¨å››ä¸ªæ ¸å¿ƒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šæ„ŸçŸ¥ã€é—®ç­”ã€éµå¾ªè§„åˆ™å’Œç«¯åˆ°ç«¯æ¸¸æˆã€‚æ¯ä¸ªç›®æ ‡ä»»åŠ¡éƒ½æ˜¯è®¾è®¡ç”¨æ¥è¯„ä¼°ç‰¹å®šçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§†è§‰æ„ŸçŸ¥ã€æ¨ç†ã€å†³ç­–ç­‰ã€‚åŸºäºè¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œæ¢è®¨äº†å½“å‰LVLMsçš„å±€é™æ€§ï¼Œå¦‚å¤„ç†é•¿ç»“æ„åŒ–è¾“å‡ºå’Œæ„ŸçŸ¥è¯¦ç»†å¯†é›†å…ƒç´ ç­‰ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xinke-wang/LVLM-Playground%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xinke-wang/LVLM-Playgroundä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02358v1">PDF</a> ICLR2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç†è§£å’Œæ¨ç†è§†è§‰ä¸æ–‡æœ¬ä¿¡æ¯æ–¹é¢å±•ç°å‡ºæ˜¾è‘—èƒ½åŠ›ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ï¼Œå¦‚è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ç­‰åŸºå‡†æµ‹è¯•ï¼Œå¾€å¾€æ— æ³•å…¨é¢æ•æ‰LVLMsçš„å®Œæ•´èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åŸºäºæ¸¸æˆè¯„ä¼°æ¡†æ¶çš„æ–¹æ³•ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°LVLMsåœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­çš„è®¤çŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ç³»åˆ—æ¸¸æˆï¼Œå¯¹LVLMsåœ¨æ„ŸçŸ¥ã€é—®ç­”ã€éµå¾ªè§„åˆ™å’Œç«¯åˆ°ç«¯ç©è€ç­‰å››ä¸ªæ ¸å¿ƒä»»åŠ¡ä¸Šçš„èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMså±•ç°å‡ºå¼ºå¤§çš„ç†è§£å’Œæ¨ç†è§†è§‰ä¸æ–‡æœ¬ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•å…¨é¢æ•æ‰LVLMsçš„èƒ½åŠ›ï¼Œå­˜åœ¨è¯¦ç»†è§†è§‰æ„ŸçŸ¥è¯„ä¼°ä¸è¶³ã€æ•°æ®æ±¡æŸ“ä»¥åŠç¼ºä¹å¤šè½®æ¨ç†çš„ç„¦ç‚¹ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ¸¸æˆè¯„ä¼°æ¡†æ¶çš„æ–¹æ³•ï¼Œä¸ºLVLMsæä¾›å…¨é¢çš„è®¤çŸ¥å’Œæ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ¸¸æˆè¯„ä¼°LVLMsåœ¨æ„ŸçŸ¥ã€é—®ç­”ã€éµå¾ªè§„åˆ™å’Œç«¯åˆ°ç«¯ç©è€ç­‰å››ä¸ªæ ¸å¿ƒä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æ¯ä¸ªç›®æ ‡ä»»åŠ¡æ—¨åœ¨è¯„ä¼°ç‰¹å®šçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§†è§‰æ„ŸçŸ¥ã€æ¨ç†ã€å†³ç­–åˆ¶å®šç­‰ã€‚</li>
<li>åŸºäºè¯¥æ¡†æ¶è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå‘ç°LVLMsåœ¨å¤„ç†é•¿ç»“æ„è¾“å‡ºå’Œæ„ŸçŸ¥è¯¦ç»†å¯†é›†å…ƒç´ ç­‰æ–¹é¢çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0001003a29b1a8006a0c4a6165c35af8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1baabf8b0f47e5c17d7e58263c3732fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cd6bb664613c0958af42d5168d3489b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5149b7590ba50ccd37ae13b57bb93762.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PromptCoT-Synthesizing-Olympiad-level-Problems-for-Mathematical-Reasoning-in-Large-Language-Models"><a href="#PromptCoT-Synthesizing-Olympiad-level-Problems-for-Mathematical-Reasoning-in-Large-Language-Models" class="headerlink" title="PromptCoT: Synthesizing Olympiad-level Problems for Mathematical   Reasoning in Large Language Models"></a>PromptCoT: Synthesizing Olympiad-level Problems for Mathematical   Reasoning in Large Language Models</h2><p><strong>Authors:Xueliang Zhao, Wei Wu, Jian Guan, Lingpeng Kong</strong></p>
<p>The ability of large language models to solve complex mathematical problems has progressed significantly, particularly for tasks requiring advanced reasoning. However, the scarcity of sufficiently challenging problems, particularly at the Olympiad level, hinders further advancements. In this work, we introduce PromptCoT, a novel approach for automatically generating high-quality Olympiad-level math problems. The proposed method synthesizes complex problems based on mathematical concepts and the rationale behind problem construction, emulating the thought processes of experienced problem designers. We provide a theoretical analysis demonstrating that an optimal rationale should maximize both the likelihood of rationale generation given the associated concepts and the likelihood of problem generation conditioned on both the rationale and the concepts. Our method is evaluated on standard benchmarks including GSM8K, MATH-500, and AIME2024, where it consistently outperforms existing problem generation methods. Furthermore, we demonstrate that PromptCoT exhibits superior data scalability, consistently maintaining high performance as the dataset size increases, outperforming the baselines. The implementation is available at <a target="_blank" rel="noopener" href="https://github.com/zhaoxlpku/PromptCoT">https://github.com/zhaoxlpku/PromptCoT</a>. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹è§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éœ€è¦é«˜çº§æ¨ç†çš„ä»»åŠ¡ï¼Œå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç¼ºä¹è¶³å¤Ÿå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¥¥æ—åŒ¹å…‹çº§åˆ«çš„é—®é¢˜ï¼Œé˜»ç¢äº†è¿›ä¸€æ­¥çš„è¿›æ­¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PromptCoTï¼Œä¸€ç§è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡å¥¥æ—åŒ¹å…‹çº§åˆ«æ•°å­¦é—®é¢˜çš„æ–°å‹æ–¹æ³•ã€‚æ‰€æå‡ºçš„æ–¹æ³•åŸºäºæ•°å­¦æ¦‚å¿µå’Œé—®é¢˜æ„å»ºèƒŒåçš„åŸç†ï¼Œç»¼åˆç”Ÿæˆå¤æ‚é—®é¢˜ï¼Œæ¨¡æ‹Ÿç»éªŒä¸°å¯Œçš„é—®é¢˜è®¾è®¡äººå‘˜çš„æ€ç»´è¿‡ç¨‹ã€‚æˆ‘ä»¬æä¾›ç†è®ºåˆ†æï¼Œè¯æ˜æœ€ä¼˜åŸç†åº”æœ€å¤§åŒ–ç»™å®šç›¸å…³æ¦‚å¿µæ—¶åŸç†ç”Ÿæˆçš„å¯èƒ½æ€§ï¼Œä»¥åŠä»¥åŸç†å’Œç›¸å…³æ¦‚å¿µä¸ºæ¡ä»¶çš„é—®é¢˜ç”Ÿæˆçš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨GSM8Kã€MATH-500å’ŒAIME2024ç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„é—®é¢˜ç”Ÿæˆæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜PromptCoTåœ¨æ•°æ®è§„æ¨¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ï¼Œéšç€æ•°æ®é›†è§„æ¨¡çš„å¢åŠ ï¼Œæ€§èƒ½æŒç»­ä¿æŒé«˜æ°´å¹³ï¼Œè¶…è¿‡äº†åŸºçº¿ã€‚ç›¸å…³å®ç°å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/zhaoxlpku/PromptCoT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zhaoxlpku/PromptCoTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02324v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³å¤æ‚æ•°å­¦é—®é¢˜èƒ½åŠ›æ˜¾è‘—è¿›æ­¥ï¼Œå°¤å…¶åœ¨éœ€è¦é«˜çº§æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚ä½†ç¼ºä¹è¶³å¤ŸæŒ‘æˆ˜æ€§çš„é¢˜ç›®ï¼Œå¦‚å¥¥æ—åŒ¹å…‹çº§åˆ«é¢˜ç›®ï¼Œé˜»ç¢äº†è¿›ä¸€æ­¥çš„å‘å±•ã€‚æœ¬ç ”ç©¶æå‡ºPromptCoTæ–¹æ³•ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡å¥¥æ—åŒ¹å…‹çº§åˆ«æ•°å­¦é—®é¢˜ã€‚æ­¤æ–¹æ³•åŸºäºæ•°å­¦æ¦‚å¿µåŠé—®é¢˜æ„å»ºçš„é€»è¾‘ï¼Œåˆæˆå¤æ‚é—®é¢˜ï¼Œæ¨¡æ‹Ÿç»éªŒä¸°å¯Œçš„é¢˜ç›®è®¾è®¡è€…çš„æ€è€ƒè¿‡ç¨‹ã€‚ç»ç†è®ºåˆ†æï¼Œæœ€ä½³é€»è¾‘åº”æœ€å¤§åŒ–ç›¸å…³æ¦‚å¿µä¸‹é€»è¾‘ç”Ÿæˆçš„å¯èƒ½æ€§ï¼Œä»¥åŠåŸºäºé€»è¾‘å’Œæ¦‚å¿µçš„é—®é¢˜ç”Ÿæˆçš„å¯èƒ½æ€§ã€‚è¯¥æ–¹æ³•åœ¨GSM8Kã€MATH-500å’ŒAIME2024ç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œè¶…è¶Šç°æœ‰é—®é¢˜ç”Ÿæˆæ–¹æ³•ã€‚PromptCoTå±•ç°å‡ºå“è¶Šçš„æ•°æ®å¯æ‰©å±•æ€§ï¼Œéšç€æ•°æ®é›†å¢åŠ ï¼Œæ€§èƒ½å§‹ç»ˆä¿æŒåœ¨é«˜æ°´å¹³ã€‚å®æ–½ç»†èŠ‚å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhaoxlpku/PromptCoT%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/zhaoxlpku/PromptCoTæŸ¥çœ‹ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ç¼ºä¹æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¥¥æ—åŒ¹å…‹çº§åˆ«çš„é—®é¢˜ï¼Œé™åˆ¶äº†è¿›ä¸€æ­¥çš„å‘å±•ã€‚</li>
<li>PromptCoTæ˜¯ä¸€ç§è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡å¥¥æ—åŒ¹å…‹çº§åˆ«æ•°å­¦é—®é¢˜çš„æ–°æ–¹æ³•ã€‚</li>
<li>PromptCoTåŸºäºæ•°å­¦æ¦‚å¿µå’Œé—®é¢˜æ„å»ºçš„é€»è¾‘æ¥åˆæˆé—®é¢˜ï¼Œæ¨¡æ‹Ÿç»éªŒä¸°å¯Œçš„è®¾è®¡è€…çš„æ€è€ƒè¿‡ç¨‹ã€‚</li>
<li>ç†è®ºåˆ†ææ˜¾ç¤ºï¼Œæœ€ä½³é€»è¾‘åº”åŒæ—¶è€ƒè™‘ç›¸å…³æ¦‚å¿µä¸‹é€»è¾‘ç”Ÿæˆçš„å¯èƒ½æ€§åŠé—®é¢˜ç”Ÿæˆçš„å¯èƒ½æ€§ã€‚</li>
<li>PromptCoTåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰çš„é—®é¢˜ç”Ÿæˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02324">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a009b43ad468931398ddfcc4faf33b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-898e0aae9514c756df4d3574f7b18033.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03c72c21bfeff5d0fbd02293305fa5d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebf9d36d473a697b517e76d87b99f33e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-147b119d5850479a8bff7856c23f73a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dda39493f1b1d4f20c75757bb8bffd1f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-07/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1c0dd782a7ff87c98922cb57986c014d.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  Process-based Self-Rewarding Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-68e3d63ce9b756986ef25d2ded589ea3.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  KeyFace Expressive Audio-Driven Facial Animation for Long Sequences via   KeyFrame Interpolation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
