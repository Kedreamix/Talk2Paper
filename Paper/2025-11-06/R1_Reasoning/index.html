<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Agent-Omni Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2f776ed60e692f0ffd1e478593546123')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-06-æ›´æ–°"><a href="#2025-11-06-æ›´æ–°" class="headerlink" title="2025-11-06 æ›´æ–°"></a>2025-11-06 æ›´æ–°</h1><h2 id="Agent-Omni-Test-Time-Multimodal-Reasoning-via-Model-Coordination-for-Understanding-Anything"><a href="#Agent-Omni-Test-Time-Multimodal-Reasoning-via-Model-Coordination-for-Understanding-Anything" class="headerlink" title="Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything"></a>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything</h2><p><strong>Authors:Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh</strong></p>
<p>Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available. %We release an open-source implementation to support continued research on scalable and reliable omni-modal reasoning. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»æ˜¾ç¤ºå‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»ç„¶å±€é™äºå›ºå®šçš„æ¨¡æ€å¯¹ï¼Œéœ€è¦æ˜‚è´µçš„ç²¾ç»†è°ƒæ•´å’Œå¤§è§„æ¨¡å¯¹é½æ•°æ®é›†ã€‚æ„å»ºèƒ½å¤Ÿæ•´åˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘çš„å®Œå…¨é€šç”¨æ¨¡å‹ä»ç„¶ä¸åˆ‡å®é™…ï¼Œç¼ºä¹ç¨³å¥çš„æ¨ç†æ”¯æŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Agent-Omniæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿåè°ƒç°æœ‰çš„åŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†çµæ´»çš„å¤šæ¨¡æ€æ¨ç†ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¸»ä»£ç†è§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå°†å­ä»»åŠ¡å§”æ´¾ç»™ç‰¹å®šæ¨¡æ€çš„ä»£ç†ï¼Œå¹¶å°†ä»–ä»¬çš„è¾“å‡ºæ•´åˆæˆè¿è´¯çš„å“åº”ã€‚åœ¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œå…¨æ¨¡æ€åŸºå‡†æµ‹è¯•æ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAgent-OmniæŒç»­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚å…¶åŸºäºä»£ç†çš„è®¾è®¡å¯å®ç°ä¸“ä¸šåŸºç¡€æ¨¡å‹çš„æ— ç¼é›†æˆï¼Œç¡®ä¿é€‚åº”å„ç§è¾“å…¥ï¼ŒåŒæ—¶ä¿æŒé€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¯æ¨¡å—åŒ–çš„ï¼Œæ˜“äºæ‰©å±•ï¼Œéšç€æ›´å¼ºå¤§çš„æ¨¡å‹çš„å‡ºç°ï¼Œå…è®¸æœªæ¥è¿›è¡Œæ”¹è¿›ã€‚%æˆ‘ä»¬å‘å¸ƒå¼€æºå®ç°ï¼Œä»¥æ”¯æŒåœ¨å¯æ‰©å±•å’Œå¯é çš„å…¨æ¨¡æ€æ¨ç†æ–¹é¢çš„æŒç»­ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02834v1">PDF</a> 16 pages, 7 figures, 14 tables. Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgent-Omniçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿåè°ƒç°æœ‰çš„åŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†çµæ´»çš„å¤šæ¨¡æ€æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå°†å­ä»»åŠ¡å§”æ´¾ç»™ç‰¹å®šæ¨¡æ€çš„ä»£ç†ï¼Œå¹¶å°†å®ƒä»¬çš„è¾“å‡ºæ•´åˆä¸ºè¿è´¯çš„å“åº”ã€‚åœ¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œå…¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgent-Omniè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚å…¶åŸºäºä»£ç†çš„è®¾è®¡ç¡®ä¿äº†å¯¹å„ç§è¾“å…¥çš„é€‚åº”æ€§ã€é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¯æ¨¡å—åŒ–çš„å¹¶ä¸”æ˜“äºæ‰©å±•ï¼Œéšç€æ›´å¼ºå¤§çš„æ¨¡å‹çš„å‡ºç°ï¼Œå…è®¸æœªæ¥çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agent-Omniæ¡†æ¶åè°ƒç°æœ‰åŸºç¡€æ¨¡å‹ï¼Œå®ç°çµæ´»å¤šæ¨¡æ€æ¨ç†ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>æ¡†æ¶é€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿè§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå°†ä»»åŠ¡å§”æ´¾ç»™ç‰¹å®šæ¨¡æ€çš„ä»£ç†ã€‚</li>
<li>Agent-Omniåœ¨å¤šç§æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚</li>
<li>åŸºäºä»£ç†çš„è®¾è®¡ç¡®ä¿äº†å¯¹å„ç§è¾“å…¥çš„é€‚åº”æ€§ã€é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>Agent-Omniæ¡†æ¶æ˜¯æ¨¡å—åŒ–çš„ï¼Œæ˜“äºæ‰©å±•ï¼Œå…è®¸æœªæ¥éšç€æ›´å¼ºæ¨¡å‹çš„å‡ºç°è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨è·¨æ¨¡æ€æ¨ç†æ–¹é¢å…·æœ‰åˆ›æ–°æ€§ï¼Œèƒ½å¤Ÿæ•´åˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18270fdfdd1011cf1d1c4150b952fe13" align="middle">
<img src="https://picx.zhimg.com/v2-dce1ac02af7fca26ab12ff5f7408c985" align="middle">
<img src="https://picx.zhimg.com/v2-c3a01543767599167d24ba00c9a0bea9" align="middle">
<img src="https://picx.zhimg.com/v2-91a10ea385811e0f083f0a09ed7c88e0" align="middle">
<img src="https://picx.zhimg.com/v2-703127fa741a33a4a6ff8843b40b6669" align="middle">
<img src="https://picx.zhimg.com/v2-a7e39165e1bc5d8d30214d90cf847b80" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MemSearcher-Training-LLMs-to-Reason-Search-and-Manage-Memory-via-End-to-End-Reinforcement-Learning"><a href="#MemSearcher-Training-LLMs-to-Reason-Search-and-Manage-Memory-via-End-to-End-Reinforcement-Learning" class="headerlink" title="MemSearcher: Training LLMs to Reason, Search and Manage Memory via   End-to-End Reinforcement Learning"></a>MemSearcher: Training LLMs to Reason, Search and Manage Memory via   End-to-End Reinforcement Learning</h2><p><strong>Authors:Qianhao Yuan, Jie Lou, Zichao Li, Jiawei Chen, Yaojie Lu, Hongyu Lin, Le Sun, Debing Zhang, Xianpei Han</strong></p>
<p>Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemSearcher, an agent workflow that iteratively maintains a compact memory and combines the current turn with it. At each turn, MemSearcher fuses the userâ€™s question with the memory to generate reasoning traces, perform search actions, and update memory to retain only information essential for solving the task. This design stabilizes context length across multi-turn interactions, improving efficiency without sacrificing accuracy. To optimize this workflow, we introduce multi-context GRPO, an end-to-end RL framework that jointly optimize reasoning, search strategies, and memory management of MemSearcher Agents. Specifically, multi-context GRPO samples groups of trajectories under different contexts and propagates trajectory-level advantages across all conversations within them. Trained on the same dataset as Search-R1, MemSearcher achieves significant improvements over strong baselines on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher even outperforms 7B-based baselines, demonstrating that striking a balance between information integrity and efficiency yields both higher accuracy and lower computational overhead. The code and models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/icip-cas/MemSearcher">https://github.com/icip-cas/MemSearcher</a> </p>
<blockquote>
<p>å…¸å‹çš„æœç´¢ä»£ç†ä¼šå°†æ•´ä¸ªäº¤äº’å†å²è¿æ¥åˆ°å¤§æ¨¡å‹è¯­å¢ƒä¸­ï¼Œè™½ç„¶èƒ½å¤Ÿä¿æŒä¿¡æ¯å®Œæ•´æ€§ï¼Œä½†ä¼šäº§ç”Ÿå†—é•¿ä¸”å˜ˆæ‚çš„è¯­å¢ƒï¼Œå¯¼è‡´è®¡ç®—å’Œå†…å­˜æˆæœ¬é«˜æ˜‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»…ä½¿ç”¨å½“å‰å›åˆå¯ä»¥é¿å…è¿™ç§å¼€é”€ï¼Œä½†ä¼šä¸¢å¼ƒé‡è¦ä¿¡æ¯ã€‚è¿™ç§æƒè¡¡é™åˆ¶äº†æœç´¢ä»£ç†çš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MemSearcherï¼Œè¿™æ˜¯ä¸€ç§ä»£ç†å·¥ä½œæµç¨‹ï¼Œå¯ä»¥è¿­ä»£åœ°ç»´æŠ¤ä¸€ä¸ªç´§å‡‘çš„å†…å­˜ï¼Œå¹¶å°†å½“å‰å›åˆä¸ä¹‹ç»“åˆã€‚åœ¨æ¯ä¸ªå›åˆä¸­ï¼ŒMemSearcherå°†ç”¨æˆ·çš„é—®é¢˜ä¸å†…å­˜èåˆï¼Œä»¥ç”Ÿæˆæ¨ç†è½¨è¿¹ã€æ‰§è¡Œæœç´¢æ“ä½œå¹¶æ›´æ–°å†…å­˜ï¼Œä»…ä¿ç•™å®Œæˆä»»åŠ¡æ‰€éœ€çš„å…³é”®ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡åœ¨å¤šå›åˆäº¤äº’ä¸­ç¨³å®šä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæé«˜äº†æ•ˆç‡ï¼Œè€Œä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚ä¸ºäº†ä¼˜åŒ–è¿™ä¸€å·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šä¸Šä¸‹æ–‡GRPOï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥è”åˆä¼˜åŒ–MemSearcherä»£ç†çš„æ¨ç†ã€æœç´¢ç­–ç•¥å’Œå†…å­˜ç®¡ç†ã€‚å…·ä½“è€Œè¨€ï¼Œå¤šä¸Šä¸‹æ–‡GRPOåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­é‡‡æ ·è½¨è¿¹ç»„ï¼Œå¹¶åœ¨æ‰€æœ‰å¯¹è¯ä¸­ä¼ æ’­è½¨è¿¹çº§ä¼˜åŠ¿ã€‚MemSearcheråœ¨Search-R1çš„åŒä¸€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨ä¸ƒä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼šç›¸å¯¹äºQwen2.5-3B-Instructæœ‰+11%çš„å¹³å‡å¢ç›Šæå‡å’Œç›¸å¯¹äºQwen2.5-7B-Instructæœ‰+12%çš„å¹³å‡å¢ç›Šæå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäº3Bçš„MemSearcherç”šè‡³è¶…è¶Šäº†åŸºäº7Bçš„åŸºçº¿æ¨¡å‹ï¼Œè¡¨æ˜åœ¨ä¿¡æ¯å®Œæ•´æ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡å¯ä»¥å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„è®¡ç®—å¼€é”€ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/icip-cas/MemSearcher%E5%B9%B6%E5%BC%BA%E5%BC%95%E7%94%A8%E5%85%AC%E4%BC%97%E3%80%82">https://github.com/icip-cas/MemSearcherå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02805v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/icip-cas/MemSearcher">https://github.com/icip-cas/MemSearcher</a></p>
<p><strong>æ‘˜è¦</strong><br>     å…¸å‹æœç´¢ä»£ç†ä¼šä¸²è”æ•´ä¸ªäº’åŠ¨å†å²åˆ°å¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…å¢ƒä¸­ï¼Œå°½ç®¡è¿™ç§æ–¹æ³•èƒ½å¤Ÿä¿ç•™ä¿¡æ¯å®Œæ•´æ€§ï¼Œä½†å´ä¼šäº§ç”Ÿæ¼«é•¿è€Œç¹æ‚çš„æƒ…å¢ƒï¼Œä»è€Œå¢åŠ è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚ç›¸åï¼Œä»…ä½¿ç”¨å½“å‰è½®æ¬¡é¿å…äº†è¿™äº›å¼€é”€ä½†åˆä¸¢å¼ƒäº†é‡è¦ä¿¡æ¯ã€‚è¿™ç§æƒè¡¡é™åˆ¶äº†æœç´¢ä»£ç†çš„å¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MemSearcherä»£ç†å·¥ä½œæµç¨‹ï¼Œå®ƒè¿­ä»£åœ°ç»´æŠ¤ä¸€ä¸ªç´§å‡‘çš„å†…å­˜å¹¶ç»“åˆå½“å‰å›åˆæƒ…å†µã€‚åœ¨æ¯ä¸ªå›åˆä¸­ï¼ŒMemSearcherå°†ç”¨æˆ·çš„é—®é¢˜ä¸å†…å­˜ç›¸èåˆï¼Œç”Ÿæˆæ¨ç†è½¨è¿¹ï¼Œæ‰§è¡Œæœç´¢æ“ä½œå¹¶æ›´æ–°å†…å­˜ï¼Œä»…ä¿ç•™å¯¹å®Œæˆä»»åŠ¡è‡³å…³é‡è¦çš„ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡åœ¨å¤šå›åˆäº’åŠ¨ä¸­ç¨³å®šäº†ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæé«˜äº†æ•ˆç‡ä¸”ä¸ä¼šç‰ºç‰²å‡†ç¡®æ€§ã€‚ä¸ºä¼˜åŒ–è¿™ä¸€å·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šä¸Šä¸‹æ–‡GRPOï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè”åˆä¼˜åŒ–MemSearcherä»£ç†çš„æ¨ç†ã€æœç´¢ç­–ç•¥å’Œå†…å­˜ç®¡ç†ã€‚å…·ä½“è€Œè¨€ï¼Œå¤šä¸Šä¸‹æ–‡GRPOåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­é‡‡æ ·è½¨è¿¹ç»„å¹¶åœ¨å®ƒä»¬ä¹‹é—´ä¼ æ’­è½¨è¿¹çº§åˆ«çš„ä¼˜åŠ¿ã€‚åœ¨Search-R1æ•°æ®é›†ä¸Šè®­ç»ƒçš„MemSearcheråœ¨ä¸ƒä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼šç›¸å¯¹äºQwen2.5-3B-Instructçš„+11%å’Œç›¸å¯¹äºQwen2.5-7B-Instructçš„+12%çš„ç›¸å¯¹å¹³å‡å¢ç›Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäº3Bçš„MemSearcherç”šè‡³è¶…è¶Šäº†åŸºäº7Bçš„åŸºçº¿æ¨¡å‹ï¼Œè¡¨æ˜åœ¨å¹³è¡¡ä¿¡æ¯å®Œæ•´æ€§å’Œæ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„è®¡ç®—å¼€é”€ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/icip-cas/MemSearcher%E3%80%82">https://github.com/icip-cas/MemSearcherã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MemSearcherä»£ç†è§£å†³äº†ä¼ ç»Ÿæœç´¢ä»£ç†åœ¨å¤„ç†äº’åŠ¨å†å²æ—¶é¢ä¸´çš„è®¡ç®—ä¸å†…å­˜æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>MemSearcheré€šè¿‡ç»“åˆå½“å‰å›åˆä¸ç´§å‡‘å†…å­˜è¿­ä»£ç»´æŠ¤çš„æ–¹å¼ä¼˜åŒ–äº†ä¸Šä¸‹æ–‡å¤„ç†ã€‚</li>
<li>MemSearcherç”Ÿæˆæ¨ç†è½¨è¿¹ã€æ‰§è¡Œæœç´¢æ“ä½œå¹¶æ›´æ–°å†…å­˜ï¼Œç¡®ä¿ä»…ä¿ç•™å®Œæˆä»»åŠ¡æ‰€éœ€çš„å…³é”®ä¿¡æ¯ã€‚</li>
<li>å¤šä¸Šä¸‹æ–‡GRPOæ¡†æ¶è¢«å¼•å…¥ä»¥è”åˆä¼˜åŒ–MemSearcherçš„æ¨ç†ã€æœç´¢ç­–ç•¥å’Œå†…å­˜ç®¡ç†ã€‚</li>
<li>MemSearcherèƒ½å¤Ÿåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­é‡‡æ ·è½¨è¿¹ç»„å¹¶ä¼ æ’­è½¨è¿¹çº§åˆ«çš„ä¼˜åŠ¿ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚</li>
<li>MemSearcheråœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œç›¸å¯¹äºå…¶ä»–æ¨¡å‹æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffcdbb527b964040e940673cf313b465" align="middle">
<img src="https://picx.zhimg.com/v2-dd0ba4e82a1e4659385cfcd38486fcfc" align="middle">
<img src="https://picx.zhimg.com/v2-7857437170b0687acc62f95d3615ae0d" align="middle">
<img src="https://picx.zhimg.com/v2-998e6417ead2c86f44e056f9b95e0af2" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="When-One-Modality-Sabotages-the-Others-A-Diagnostic-Lens-on-Multimodal-Reasoning"><a href="#When-One-Modality-Sabotages-the-Others-A-Diagnostic-Lens-on-Multimodal-Reasoning" class="headerlink" title="When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal   Reasoning"></a>When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal   Reasoning</h2><p><strong>Authors:Chenyu Zhang, Minsol Kim, Shohreh Ghorbani, Jingyao Wu, Rosalind Picard, Patricia Maes, Paul Pu Liang</strong></p>
<p>Despite rapid growth in multimodal large language models (MLLMs), their reasoning traces remain opaque: it is often unclear which modality drives a prediction, how conflicts are resolved, or when one stream dominates. In this paper, we introduce modality sabotage, a diagnostic failure mode in which a high-confidence unimodal error overrides other evidence and misleads the fused result. To analyze such dynamics, we propose a lightweight, model-agnostic evaluation layer that treats each modality as an agent, producing candidate labels and a brief self-assessment used for auditing. A simple fusion mechanism aggregates these outputs, exposing contributors (modalities supporting correct outcomes) and saboteurs (modalities that mislead). Applying our diagnostic layer in a case study on multimodal emotion recognition benchmarks with foundation models revealed systematic reliability profiles, providing insight into whether failures may arise from dataset artifacts or model limitations. More broadly, our framework offers a diagnostic scaffold for multimodal reasoning, supporting principled auditing of fusion dynamics and informing possible interventions. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘å±•è¿…é€Ÿï¼Œä½†å…¶æ¨ç†è½¨è¿¹ä»ç„¶ä¸æ˜ç¡®ï¼šé€šå¸¸ä¸æ¸…æ¥šæ˜¯å“ªç§æ¨¡æ€é©±åŠ¨é¢„æµ‹ï¼Œå¦‚ä½•è§£å†³å†²çªï¼Œæˆ–è€…ä½•æ—¶ä¸€ä¸ªæµå ä¸»å¯¼åœ°ä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ¨¡æ€ç ´åï¼ˆmodality sabotageï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¯Šæ–­å¤±è´¥æ¨¡å¼ï¼Œé«˜ç½®ä¿¡åº¦çš„å•æ¨¡æ€é”™è¯¯ä¼šè¦†ç›–å…¶ä»–è¯æ®å¹¶è¯¯å¯¼èåˆç»“æœã€‚ä¸ºäº†åˆ†æè¿™ç§åŠ¨æ€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ¨¡å‹ä¸å¯çŸ¥çš„è¯„ä¼°å±‚ï¼Œå®ƒå°†æ¯ä¸ªæ¨¡æ€è§†ä¸ºä¸€ä¸ªä»£ç†ï¼Œç”Ÿæˆå€™é€‰æ ‡ç­¾å’Œç®€çŸ­çš„è‡ªæˆ‘è¯„ä¼°ï¼Œç”¨äºå®¡è®¡ã€‚ä¸€ä¸ªç®€å•çš„èåˆæœºåˆ¶å¯ä»¥èšåˆè¿™äº›è¾“å‡ºï¼Œæš´éœ²è´¡çŒ®è€…ï¼ˆæ”¯æŒæ­£ç¡®ç»“æœçš„æ¨¡æ€ï¼‰å’Œç ´åè€…ï¼ˆè¯¯å¯¼çš„æ¨¡æ€ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„è¯Šæ–­å±‚åœ¨åŸºäºåŸºå‡†æ¨¡å‹çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æµ‹è¯•æ¡ˆä¾‹ç ”ç©¶ä¸­çš„åº”ç”¨æ­ç¤ºäº†ç³»ç»Ÿçš„å¯é æ€§é…ç½®æ–‡ä»¶ï¼Œè¿™æœ‰åŠ©äºäº†è§£å¤±è´¥æ˜¯å¦æ˜¯ç”±äºæ•°æ®é›†çš„äººå·¥åˆ¶å“æˆ–æ¨¡å‹æœ¬èº«çš„å±€é™æ€§æ‰€å¯¼è‡´çš„ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†è¯Šæ–­æ”¯æ¶ï¼Œæ”¯æŒæœ‰åŸåˆ™åœ°å®¡è®¡èåˆåŠ¨æ€å¹¶å‘ŠçŸ¥å¯èƒ½çš„å¹²é¢„æªæ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02794v1">PDF</a> Accepted at the Multimodal Algorithmic Reasoning (MAR) Workshop,   NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†é€æ˜åº¦é—®é¢˜ï¼Œæå‡ºä¸€ç§è¯Šæ–­å¤±æ•ˆæ¨¡å¼â€”â€”æ¨¡æ€ç ´åï¼Œå³é«˜ç½®ä¿¡åº¦çš„å•æ¨¡æ€é”™è¯¯ä¼šè¦†ç›–å…¶ä»–è¯æ®å¹¶è¯¯å¯¼èåˆç»“æœã€‚ä¸ºåˆ†æè¿™ç§ç°è±¡ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ¨¡å‹ä¸å¯çŸ¥çš„è¯„ä¼°å±‚ï¼Œè¯¥è¯„ä¼°å±‚å°†å„æ¨¡æ€è§†ä¸ºä»£ç†ï¼Œäº§ç”Ÿå€™é€‰æ ‡ç­¾å’Œç®€çŸ­çš„è‡ªæˆ‘è¯„ä¼°ç”¨äºå®¡è®¡ã€‚ä¸€ä¸ªç®€å•çš„èåˆæœºåˆ¶èƒ½å¤Ÿèšåˆè¿™äº›è¾“å‡ºï¼Œæ­ç¤ºè´¡çŒ®è€…ï¼ˆæ”¯æŒæ­£ç¡®ç»“æœçš„æ¨¡æ€ï¼‰å’Œç ´åè€…ï¼ˆè¯¯å¯¼çš„æ¨¡æ€ï¼‰ã€‚åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æµ‹è¯•é›†çš„æ¡ˆä¾‹ç ”ç©¶ä¸­åº”ç”¨è¯¥è¯Šæ–­å±‚æ­ç¤ºäº†ç³»ç»Ÿçš„å¯é æ€§ç‰¹å¾ï¼Œæœ‰åŠ©äºäº†è§£å¤±è´¥æ˜¯æºäºæ•°æ®é›†çš„äººå·¥åˆ¶å“è¿˜æ˜¯æ¨¡å‹æœ¬èº«çš„å±€é™æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶ä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†è¯Šæ–­æ¡†æ¶ï¼Œæ”¯æŒèåˆåŠ¨åŠ›å­¦çš„åŸåˆ™æ€§å®¡è®¡ï¼Œå¹¶ä¸ºå¯èƒ½çš„å¹²é¢„æä¾›äº†ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†è¿‡ç¨‹å­˜åœ¨é€æ˜åº¦é—®é¢˜ï¼Œéš¾ä»¥ç¡®å®šé¢„æµ‹èƒŒåçš„é©±åŠ¨å› ç´ ä»¥åŠå†²çªå¦‚ä½•è§£å†³æˆ–å“ªä¸ªæ•°æ®æµå æ®ä¸»å¯¼åœ°ä½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯Šæ–­å¤±æ•ˆæ¨¡å¼â€”â€”æ¨¡æ€ç ´åï¼Œå…¶ä¸­é«˜ç½®ä¿¡åº¦çš„å•æ¨¡æ€é”™è¯¯ä¼šè¦†ç›–å…¶ä»–è¯æ®å¹¶è¯¯å¯¼èåˆç»“æœã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è½»é‡çº§ã€æ¨¡å‹ä¸å¯çŸ¥çš„è¯„ä¼°å±‚ï¼Œè¯¥è¯„ä¼°å±‚å¯ä»¥åˆ†æå„æ¨¡æ€çš„è´¡çŒ®å’Œå¯èƒ½çš„è¯¯å¯¼ã€‚</li>
<li>é€šè¿‡æ¡ˆä¾‹ç ”ç©¶æ­ç¤ºäº†ç³»ç»Ÿçš„å¯é æ€§ç‰¹å¾ï¼Œæœ‰åŠ©äºåŒºåˆ†å¤±è´¥æ˜¯æºäºæ•°æ®é›†çš„é—®é¢˜è¿˜æ˜¯æ¨¡å‹æœ¬èº«çš„å±€é™æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†è¯Šæ–­å·¥å…·ï¼Œæ”¯æŒå¯¹èåˆè¿‡ç¨‹çš„å®¡è®¡ï¼Œå¹¶æœ‰åŠ©äºè¯†åˆ«å¯èƒ½çš„æ”¹è¿›æªæ–½ã€‚</li>
<li>æå‡ºçš„è¯Šæ–­å±‚å¯ä»¥åº”ç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ç­‰åœºæ™¯ï¼Œæœ‰åŠ©äºæ·±å…¥äº†è§£æ¨¡å‹çš„æ€§èƒ½ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-757f8ebf480f7f1617533ca3ad13c3f4" align="middle">
<img src="https://picx.zhimg.com/v2-47097db86b335708c718308cfee39f93" align="middle">
<img src="https://picx.zhimg.com/v2-4168e0e524f1d3e5e5997517ad1a1b20" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Controlling-Performance-and-Budget-of-a-Centralized-Multi-agent-LLM-System-with-Reinforcement-Learning"><a href="#Controlling-Performance-and-Budget-of-a-Centralized-Multi-agent-LLM-System-with-Reinforcement-Learning" class="headerlink" title="Controlling Performance and Budget of a Centralized Multi-agent LLM   System with Reinforcement Learning"></a>Controlling Performance and Budget of a Centralized Multi-agent LLM   System with Reinforcement Learning</h2><p><strong>Authors:Bowen Jin, TJ Collins, Donghan Yu, Mert Cemri, Shenao Zhang, Mengyu Li, Jay Tang, Tian Qin, Zhiyang Xu, Jiarui Lu, Guoli Yin, Jiawei Han, Zirui Wang</strong></p>
<p>Large language models (LLMs) exhibit complementary strengths across domains and come with varying inference costs, motivating the design of multi-agent LLM systems where specialized models collaborate efficiently. Existing approaches predominantly rely on decentralized frameworks, which invoke multiple LLMs for every input and thus lead to substantial and uncontrolled inference costs. In this work, we introduce a centralized multi-LLM framework, where a controller LLM selectively coordinates a pool of expert models in a cost-efficient and cost-controllable manner. We formulate this coordination problem as reinforcement learning with dual objectives: maximizing task performance while minimizing the overall inference cost. In addition, we expect the multi-agent system to have adapted behavior with different budget conditions during inference. To this end, we propose CoRL, a reinforcement learning framework that optimizes the performance cost trade-off in a controllable multi-budget setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a single system to surpass the best expert LLM under high-budget settings, while maintaining strong performance in more economical low-budget modes, highlighting the effectiveness of centralized coordination for scalable and cost-efficient multi-agent LLM systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒé¢†åŸŸå±•ç°å‡ºäº’è¡¥çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”å…·æœ‰ä¸åŒçš„æ¨ç†æˆæœ¬ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬è®¾è®¡å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼Œå…¶ä¸­ä¸“ä¸šæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåä½œã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆ†å¸ƒå¼æ¡†æ¶ï¼Œå®ƒä¸ºæ¯ä¸ªè¾“å…¥è°ƒç”¨å¤šä¸ªLLMï¼Œä»è€Œå¯¼è‡´æ¨ç†æˆæœ¬å·¨å¤§ä¸”ä¸å¯æ§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é›†ä¸­å¼çš„å¤šLLMæ¡†æ¶ï¼Œå…¶ä¸­æ§åˆ¶å™¨LLMä»¥æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯æ§çš„æ–¹å¼é€‰æ‹©æ€§åœ°åè°ƒä¸“å®¶æ¨¡å‹æ± ã€‚æˆ‘ä»¬å°†è¿™ç§åè°ƒé—®é¢˜åˆ¶å®šä¸ºå…·æœ‰åŒé‡ç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼šåœ¨æœ€å¤§åŒ–ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶æœ€å°åŒ–æ€»ä½“æ¨ç†æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¸Œæœ›å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åœ¨ä¸åŒé¢„ç®—æ¡ä»¶ä¸‹èƒ½å¤Ÿé€‚åº”æ€§è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CoRLï¼Œè¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–å¯æ§å¤šé¢„ç®—è®¾ç½®ä¸­æ€§èƒ½æˆæœ¬æƒè¡¡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚åœ¨å››ä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoRLä½¿å•ä¸€ç³»ç»Ÿåœ¨é«˜æ€§èƒ½é¢„ç®—è®¾ç½®ä¸‹è¶…è¶Šæœ€ä½³ä¸“å®¶LLMï¼ŒåŒæ—¶åœ¨æ›´ç»æµçš„ä½é¢„ç®—æ¨¡å¼ä¸‹ä¿æŒå¼ºåŠ²æ€§èƒ½ï¼Œçªæ˜¾é›†ä¸­å¼åè°ƒåœ¨å¯æ‰©å±•å’Œæˆæœ¬æ•ˆç›Šé«˜çš„å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02755v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒé¢†åŸŸå…·æœ‰äº’è¡¥ä¼˜åŠ¿ï¼ŒåŒæ—¶æ¨ç†æˆæœ¬å„å¼‚ï¼Œå› æ­¤è®¾è®¡å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼Œè®©ä¸“ä¸šæ¨¡å‹é«˜æ•ˆåä½œè‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å»ä¸­å¿ƒåŒ–æ¡†æ¶ï¼Œä¸ºæ¯æ¬¡è¾“å…¥è°ƒç”¨å¤šä¸ªLLMï¼Œå¯¼è‡´æ¨ç†æˆæœ¬å·¨å¤§ä¸”ä¸å¯æ§ã€‚æœ¬æ–‡æå‡ºä¸€ç§é›†ä¸­å¼å¤šLLMæ¡†æ¶ï¼Œæ§åˆ¶å™¨LLMæœ‰é€‰æ‹©æ€§åœ°åè°ƒä¸“å®¶æ¨¡å‹æ± ï¼Œä»¥é«˜æ•ˆä¸”å¯æ§çš„æ–¹å¼é™ä½æˆæœ¬ã€‚æˆ‘ä»¬å°†åè°ƒé—®é¢˜å…¬å¼åŒ–ä¸ºå…·æœ‰åŒé‡ç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼šæœ€å¤§åŒ–ä»»åŠ¡æ€§èƒ½ï¼ŒåŒæ—¶æœ€å°åŒ–æ€»ä½“æ¨ç†æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å››ç§ä¸åŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCoRLï¼ˆä¸€ç§ä¼˜åŒ–æ€§èƒ½æˆæœ¬æƒè¡¡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼‰ä½¿å•ä¸€ç³»ç»Ÿåœ¨é¢„ç®—å……è¶³æ—¶è¡¨ç°è¶…è¶Šæœ€ä½³ä¸“å®¶LLMï¼ŒåŒæ—¶åœ¨ç»æµèŠ‚çº¦å‹æ¨¡å¼ä¸‹ä¹Ÿä¿æŒå¼ºåŠ²æ€§èƒ½ï¼Œå‡¸æ˜¾é›†ä¸­å¼åè°ƒå¯¹äºå¯æ‰©å±•å’Œé«˜æ•ˆçš„å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿçš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒé¢†åŸŸå…·æœ‰ä¼˜åŠ¿ï¼Œä¸”æ¨ç†æˆæœ¬å„å¼‚ï¼Œéœ€è¦è®¾è®¡å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿå®ç°é«˜æ•ˆåä½œã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å»ä¸­å¿ƒåŒ–æ¡†æ¶ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬å·¨å¤§ä¸”ä¸å¯æ§ã€‚</li>
<li>é›†ä¸­å¼å¤šLLMæ¡†æ¶æå‡ºï¼Œé€šè¿‡æ§åˆ¶å™¨LLMé€‰æ‹©æ€§åè°ƒä¸“å®¶æ¨¡å‹æ± ä»¥é™ä½æ¨ç†æˆæœ¬ã€‚</li>
<li>åè°ƒé—®é¢˜è¢«å…¬å¼åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶æœ€å°åŒ–æ€»ä½“æ¨ç†æˆæœ¬ã€‚</li>
<li>æå‡ºçš„CoRLæ¡†æ¶èƒ½åœ¨ä¸åŒé¢„ç®—æ¡ä»¶ä¸‹ä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬çš„æƒè¡¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCoRLåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½åœ¨é«˜é¢„ç®—ä¸‹è¶…è¶Šæœ€ä½³ä¸“å®¶LLMï¼ŒåŒæ—¶åœ¨ä½é¢„ç®—æ¨¡å¼ä¸‹ä¹Ÿä¿æŒå¼ºåŠ²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6dbe80b767ffda912e0031e4107bd798" align="middle">
<img src="https://picx.zhimg.com/v2-ee38e3923c722c5a481994b3533f35b3" align="middle">
<img src="https://picx.zhimg.com/v2-04d067143403ef954d084a320e98e602" align="middle">
<img src="https://picx.zhimg.com/v2-1b8650d0dfecf30c36e67df11ed218da" align="middle">
<img src="https://picx.zhimg.com/v2-35241e82e15b10592963de640fc170aa" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Curriculum-Design-for-Trajectory-Constrained-Agent-Compressing-Chain-of-Thought-Tokens-in-LLMs"><a href="#Curriculum-Design-for-Trajectory-Constrained-Agent-Compressing-Chain-of-Thought-Tokens-in-LLMs" class="headerlink" title="Curriculum Design for Trajectory-Constrained Agent: Compressing   Chain-of-Thought Tokens in LLMs"></a>Curriculum Design for Trajectory-Constrained Agent: Compressing   Chain-of-Thought Tokens in LLMs</h2><p><strong>Authors:Georgios Tzannetos, Parameswaran Kamalaruban, Adish Singla</strong></p>
<p>Training agents to operate under strict constraints during deployment, such as limited resource budgets or stringent safety requirements, presents significant challenges, especially when these constraints render the task complex. In this work, we propose a curriculum learning strategy that gradually tightens constraints during training, enabling the agent to incrementally master the deployment requirements. Inspired by self-paced learning techniques in unconstrained reinforcement learning (RL), our approach facilitates a smoother transition to challenging environments by initially training on simplified versions of the constraints and progressively introducing the full deployment conditions. We provide a theoretical analysis using an RL agent in a binary-tree Markov Decision Process (MDP) to demonstrate that our curriculum strategy can accelerate training relative to a baseline approach that imposes the trajectory constraints from the outset. Moreover, we empirically validate the effectiveness and generality of our method across both RL and large language model (LLM) agents in diverse settings, including a binary-tree MDP, a multi-task navigation domain, and a math reasoning task with two benchmarks. These results highlight the potential of curriculum design in enhancing the efficiency and performance of agents operating under complex trajectory constraints during deployment. Moreover, when applied to LLMs, our strategy enables compression of output chain-of-thought tokens, achieving a substantial inference speedup on consumer hardware, demonstrating its effectiveness for resource-constrained deployment. </p>
<blockquote>
<p>åœ¨éƒ¨ç½²æœŸé—´ï¼Œè®­ç»ƒæ™ºèƒ½ä½“åœ¨ä¸¥æ ¼çš„çº¦æŸæ¡ä»¶ä¸‹è¿›è¡Œæ“ä½œï¼Œå¦‚èµ„æºé¢„ç®—æœ‰é™æˆ–å®‰å…¨è¦æ±‚ä¸¥æ ¼ï¼Œå­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“è¿™äº›çº¦æŸä½¿ä»»åŠ¡å¤æ‚åŒ–æ—¶ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥åŠ å¼ºçº¦æŸï¼Œä½¿æ™ºèƒ½ä½“èƒ½é€æ­¥æŒæ¡éƒ¨ç½²è¦æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°æ— çº¦æŸå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„è‡ªæˆ‘è¿›åº¦å­¦ä¹ æŠ€æœ¯çš„å¯å‘ï¼Œé€šè¿‡æœ€åˆåœ¨çº¦æŸçš„ç®€åŒ–ç‰ˆæœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€æ­¥å¼•å…¥å…¨é¢çš„éƒ¨ç½²æ¡ä»¶ï¼Œå®ç°å‘å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒçš„å¹³ç¨³è¿‡æ¸¡ã€‚æˆ‘ä»¬ä½¿ç”¨äºŒè¿›åˆ¶æ ‘é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­çš„RLä»£ç†è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„è¯¾ç¨‹ç­–ç•¥å¯ä»¥åŠ é€Ÿè®­ç»ƒï¼Œç›¸å¯¹äºä¸€ç§ä»ä¸€å¼€å§‹å°±å¼ºåˆ¶è½¨è¿¹çº¦æŸçš„åŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“åœ¨å¤šç§ç¯å¢ƒä¸­çš„å®è¯ç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ™®éæ€§ï¼ŒåŒ…æ‹¬äºŒè¿›åˆ¶æ ‘MDPã€å¤šä»»åŠ¡å¯¼èˆªé¢†åŸŸå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ã€‚è¿™äº›ç»“æœçªå‡ºäº†è¯¾ç¨‹è®¾è®¡åœ¨å¢å¼ºæ™ºèƒ½ä½“åœ¨éƒ¨ç½²æœŸé—´åœ¨å¤æ‚è½¨è¿¹çº¦æŸä¸‹æ“ä½œçš„æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œå½“åº”ç”¨äºLLMæ—¶ï¼Œæˆ‘ä»¬çš„ç­–ç•¥èƒ½å¤Ÿå®ç°è¾“å‡ºæ€ç»´é“¾ä»£å¸çš„å‹ç¼©ï¼Œåœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šå®ç°äº†å¯è§‚çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œè¯æ˜äº†å…¶åœ¨èµ„æºå—é™éƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02690v1">PDF</a> NeurIPSâ€™25 paper</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æå‡ºä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥åŠ å¼ºçº¦æŸæ¡ä»¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€æ­¥æŒæ¡éƒ¨ç½²è¦æ±‚ã€‚è¯¥æ–¹æ³•å—åˆ°æ— çº¦æŸå¼ºåŒ–å­¦ä¹ ä¸­çš„è‡ªæˆ‘è¿›åº¦å­¦ä¹ æŠ€æœ¯çš„å¯å‘ï¼Œé€šè¿‡æœ€åˆåœ¨ç®€åŒ–ç‰ˆæœ¬çš„çº¦æŸä¸‹è¿›è¡Œè®­ç»ƒï¼Œç„¶åé€æ­¥å¼•å…¥å®Œæ•´çš„éƒ¨ç½²æ¡ä»¶ï¼Œå®ç°æ›´å¹³ç¨³åœ°è¿‡æ¸¡åˆ°å¤æ‚ç¯å¢ƒã€‚ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶å‡è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ä¸­å‡æœ‰æ•ˆï¼Œå¹¶ä¸”åœ¨å¤„ç†å¤æ‚è½¨è¿¹çº¦æŸæ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®­ç»ƒä»£ç†åœ¨éƒ¨ç½²æ—¶é¢ä¸´ä¸¥æ ¼çº¦æŸï¼Œå¦‚èµ„æºé¢„ç®—æœ‰é™æˆ–å®‰å…¨è¦æ±‚ä¸¥æ ¼ï¼Œä¼šå¸¦æ¥æ˜¾è‘—æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥é€šè¿‡é€æ­¥åŠ å¼ºçº¦æŸæ¡ä»¶ï¼Œä½¿ä»£ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥é€‚åº”éƒ¨ç½²è¦æ±‚ã€‚</li>
<li>è¯¥ç­–ç•¥å—åˆ°æ— çº¦æŸå¼ºåŒ–å­¦ä¹ ä¸­çš„è‡ªæˆ‘è¿›åº¦å­¦ä¹ æŠ€æœ¯çš„å¯å‘ï¼Œé€šè¿‡ç®€åŒ–çº¦æŸçš„åˆå§‹è®­ç»ƒï¼Œé€æ­¥å¼•å…¥å®Œå…¨éƒ¨ç½²æ¡ä»¶ï¼Œå®ç°å¹³æ»‘è¿‡æ¸¡ã€‚</li>
<li>ç†è®ºåˆ†æè¯æ˜ï¼Œè¯¥ç­–ç•¥åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„äºŒè¿›åˆ¶æ ‘é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸Šç›¸å¯¹åŸºçº¿æ–¹æ³•èƒ½åŠ é€Ÿè®­ç»ƒã€‚</li>
<li>å®è¯ç ”ç©¶åœ¨å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ä¸­å‡éªŒè¯äº†è¯¥ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬åœ¨äºŒè¿›åˆ¶æ ‘MDPã€å¤šä»»åŠ¡å¯¼èˆªåŸŸå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥ç­–ç•¥èƒ½å¤Ÿæé«˜ä»£ç†åœ¨éƒ¨ç½²æ—¶å¤„ç†å¤æ‚è½¨è¿¹çº¦æŸçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ac9d64c09fb73ee132d5702669f97f9" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Scalable-Evaluation-and-Neural-Models-for-Compositional-Generalization"><a href="#Scalable-Evaluation-and-Neural-Models-for-Compositional-Generalization" class="headerlink" title="Scalable Evaluation and Neural Models for Compositional Generalization"></a>Scalable Evaluation and Neural Models for Compositional Generalization</h2><p><strong>Authors:Giacomo Camposampiero, Pietro Barbiero, Michael Hersche, Roger Wattenhofer, Abbas Rahimi</strong></p>
<p>Compositional generalization-a key open challenge in modern machine learning-requires models to predict unknown combinations of known concepts. However, assessing compositional generalization remains a fundamental challenge due to the lack of standardized evaluation protocols and the limitations of current benchmarks, which often favor efficiency over rigor. At the same time, general-purpose vision architectures lack the necessary inductive biases, and existing approaches to endow them compromise scalability. As a remedy, this paper introduces: 1) a rigorous evaluation framework that unifies and extends previous approaches while reducing computational requirements from combinatorial to constant; 2) an extensive and modern evaluation on the status of compositional generalization in supervised vision backbones, training more than 5000 models; 3) Attribute Invariant Networks, a class of models establishing a new Pareto frontier in compositional generalization, achieving a 23.43% accuracy improvement over baselines while reducing parameter overhead from 600% to 16% compared to fully disentangled counterparts. </p>
<blockquote>
<p>ç»„åˆæ³›åŒ–æ˜¯ç°ä»£æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå…³é”®çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å·²çŸ¥æ¦‚å¿µç»„åˆä¸­çš„æœªçŸ¥å†…å®¹ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’Œå½“å‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼ˆè¿™äº›æµ‹è¯•é€šå¸¸æ›´æ³¨é‡æ•ˆç‡è€Œéä¸¥è°¨æ€§ï¼‰ï¼Œå› æ­¤è¯„ä¼°ç»„åˆæ³›åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œé€šç”¨è§†è§‰æ¶æ„ç¼ºä¹å¿…è¦çš„å½’çº³åè§ï¼Œç°æœ‰çš„èµ‹äºˆå®ƒä»¬çš„æ–¹æ³•åˆä¼šå½±å“å…¶å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†ï¼š1ï¼‰ä¸€ä¸ªä¸¥è°¨çš„è¯„ä»·æ¡†æ¶ï¼Œå®ƒç»Ÿä¸€å¹¶æ‰©å±•äº†ä»¥å‰çš„æ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†ä»ç»„åˆåˆ°å¸¸æ•°çš„è®¡ç®—è¦æ±‚ï¼›2ï¼‰å¯¹ç›‘ç£è§†è§‰ä¸»å¹²ä¸­ç»„åˆæ³›åŒ–ç°çŠ¶çš„å¹¿æ³›å’Œç°ä»£è¯„ä¼°ï¼Œè®­ç»ƒäº†è¶…è¿‡5000ä¸ªæ¨¡å‹ï¼›3ï¼‰å±æ€§ä¸å˜ç½‘ç»œæ˜¯ä¸€ç±»æ¨¡å‹ï¼Œåœ¨ç»„åˆæ³›åŒ–æ–¹é¢å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜è¾¹ç•Œï¼Œåœ¨åŸºçº¿åŸºç¡€ä¸Šå®ç°äº†23.43%çš„å‡†ç¡®ç‡æå‡ï¼ŒåŒæ—¶ä¸å®Œå…¨è§£è€¦çš„å¯¹åº”æ¨¡å‹ç›¸æ¯”ï¼Œå‚æ•°å¼€é”€ä»600%å‡å°‘åˆ°16%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02667v1">PDF</a> Accepted at the Thirty-ninth Annual Conference on Neural Information   Processing Systems (NeurIPS), 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†ç°ä»£æœºå™¨å­¦ä¹ ä¸­çš„å…³é”®æŒ‘æˆ˜â€”â€”ç»„åˆæ³›åŒ–ï¼Œå®ƒè¦æ±‚æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å·²çŸ¥æ¦‚å¿µç»„åˆçš„æœªçŸ¥æƒ…å†µã€‚æ–‡ä¸­æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ä¸¥æ ¼è¯„ä»·æ¡†æ¶æ¥è¯„ä¼°æ¨¡å‹åœ¨è§†è§‰æ–¹é¢çš„ç»„åˆæ³›åŒ–èƒ½åŠ›ï¼Œå»ºç«‹äº†ä¸€ç§æ–°å‹æ¨¡å‹è¯„ä¼°åè®®æ¥è§£å†³ç¼ºä¹æ ‡å‡†åŒ–è¯„ä»·åè®®å’Œç°æœ‰åŸºå‡†æµ‹è¯•æ³¨é‡æ•ˆç‡è€Œéä¸¥è°¨æ€§çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ¨¡å‹â€”â€”å±æ€§ä¸å˜ç½‘ç»œï¼ˆAttribute Invariant Networksï¼‰ï¼Œå®ƒåœ¨ç»„æˆæ³›åŒ–æ–¹é¢å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜è¾¹ç•Œï¼Œä¸åŸºçº¿ç›¸æ¯”æé«˜äº†å‡†ç¡®ç‡ã€‚ç›¸è¾ƒäºå®Œå…¨åˆ†ç¦»å¯¹ç­‰æ¨¡å‹ï¼Œå®ƒé™ä½äº†å‚æ•°å¼€é”€ï¼Œåœ¨ä¿è¯äº†è‰¯å¥½æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶å…¼å…·è¾ƒä½çš„å¤æ‚åº¦ã€‚æ­¤å¤–è¿˜æå‡ºäº†ä¸€ç§è¾ƒä¸ºç®€å•çš„åº¦é‡æ¡†æ¶ä»¥å‡å°‘æµ‹è¯•æ‰€éœ€çš„è®¡ç®—èµ„æºè¦æ±‚ï¼Œé™ä½äº†ä¹‹å‰æ‰€è¾¾åˆ°çš„å¤§è§„æ¨¡æ•°æ®é›†åˆè®­ç»ƒçš„æµ‹è¯•æ¨¡å‹è¯„ä¼°æ¡†æ¶ä¸‹çš„è¿ç®—è´Ÿè·æ°´å¹³è¦æ±‚è¾ƒé«˜çš„æˆæœ¬ã€‚é€šè¿‡è®­ç»ƒè¶…è¿‡äº”åƒä¸ªæ¨¡å‹å¯¹ç›‘ç£è§†è§‰ä¸»å¹²è¿›è¡Œäº†å…¨é¢çš„ç°ä»£è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>ç»„åˆæ³›åŒ–æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å·²çŸ¥æ¦‚å¿µç»„åˆå½¢æˆçš„æœªçŸ¥äº‹ç‰©ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’Œç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ä½¿å¾—è¯„ä¼°ç»„åˆæ³›åŒ–æˆä¸ºä¸€é¡¹æŒ‘æˆ˜ã€‚è¿™äº›æµ‹è¯•å¾€å¾€è¿‡äºæ³¨é‡æ•ˆç‡è€Œéä¸¥è°¨æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3fee894a59d2ad4d4eecbc227e5363a7" align="middle">
<img src="https://picx.zhimg.com/v2-2fb3cc7a632cd7ec41838b33bb0b5134" align="middle">
<img src="https://picx.zhimg.com/v2-66481f21f56e8e87522f9169129fc02d" align="middle">
<img src="https://picx.zhimg.com/v2-c316e275370e304e39111b45aed8d316" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CGES-Confidence-Guided-Early-Stopping-for-Efficient-and-Accurate-Self-Consistency"><a href="#CGES-Confidence-Guided-Early-Stopping-for-Efficient-and-Accurate-Self-Consistency" class="headerlink" title="CGES: Confidence-Guided Early Stopping for Efficient and Accurate   Self-Consistency"></a>CGES: Confidence-Guided Early Stopping for Efficient and Accurate   Self-Consistency</h2><p><strong>Authors:Ehsan Aghazadeh, Ahmad Ghasemi, Hedyeh Beyhaghi, Hossein Pishro-Nik</strong></p>
<p>Large language models (LLMs) are often queried multiple times at test time, with predictions aggregated by majority vote. While effective, this self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls and can fail when the correct answer is rare. We introduce Confidence-Guided Early Stopping (CGES), a Bayesian framework that forms posteriors over candidate answers using scalar confidence signals derived from token probabilities or reward models. CGES adaptively halts sampling once the posterior mass of a candidate exceeds a threshold. We provide theoretical guarantees for both perfectly calibrated confidences and realistic noisy confidence signals. Across five reasoning benchmarks, CGES reduces the average number of model calls by about 69 percent (for example, from 16.0 to 4.9) while matching the accuracy of self-consistency within 0.06 percentage points. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹è¯•æ—¶ç»å¸¸è¢«å¤šæ¬¡æŸ¥è¯¢ï¼Œé€šè¿‡å¤šæ•°æŠ•ç¥¨çš„æ–¹å¼å¯¹é¢„æµ‹ç»“æœè¿›è¡Œæ±‡æ€»ã€‚è™½ç„¶è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†è¿™ç§è‡ªæ´½ç­–ç•¥ï¼ˆarXiv:2203.11171ï¼‰éœ€è¦å›ºå®šæ¬¡æ•°çš„æŸ¥è¯¢ï¼Œå½“æ­£ç¡®ç­”æ¡ˆç¨€å°‘æ—¶å¯èƒ½ä¼šå¤±æ•ˆã€‚æˆ‘ä»¬å¼•å…¥äº†ç½®ä¿¡å¼•å¯¼æå‰ç»ˆæ­¢ï¼ˆCGESï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§è´å¶æ–¯æ¡†æ¶ï¼Œåˆ©ç”¨æ¥è‡ªä»¤ç‰Œæ¦‚ç‡æˆ–å¥–åŠ±æ¨¡å‹çš„æ ‡é‡ç½®ä¿¡ä¿¡å·å¯¹å€™é€‰ç­”æ¡ˆå½¢æˆåéªŒåˆ†å¸ƒã€‚ä¸€æ—¦å€™é€‰ç­”æ¡ˆçš„åéªŒæ¦‚ç‡è¶…è¿‡é˜ˆå€¼ï¼ŒCGESå°±ä¼šè‡ªé€‚åº”åœ°åœæ­¢é‡‡æ ·ã€‚æˆ‘ä»¬å¯¹å®Œå…¨æ ¡å‡†çš„ç½®ä¿¡åº¦å’Œç°å®çš„å˜ˆæ‚ç½®ä¿¡ä¿¡å·éƒ½æä¾›äº†ç†è®ºä¿è¯ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCGESé€šè¿‡å°†æ¨¡å‹è°ƒç”¨æ¬¡æ•°å¹³å‡å‡å°‘çº¦69%ï¼ˆä¾‹å¦‚ï¼Œä»16.0å‡å°‘åˆ°4.9ï¼‰ï¼ŒåŒæ—¶åœ¨0.06ä¸ªç™¾åˆ†ç‚¹å†…è¾¾åˆ°è‡ªæ´½ç­–ç•¥çš„ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02603v1">PDF</a> Efficient Reasoning @ NeurIPS2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶å¸¸å¸¸éœ€è¦è¿›è¡Œå¤šæ¬¡æŸ¥è¯¢å¹¶é€šè¿‡å¯¹å¤šæ•°ç­”æ¡ˆçš„èšåˆæ¥è¿›è¡Œé¢„æµ‹ã€‚ç„¶è€Œï¼Œè¿™ç§åšæ³•éœ€è¦åœ¨å›ºå®šæ¬¡æ•°çš„æŸ¥è¯¢è°ƒç”¨åå¾—å‡ºç»“æœï¼Œå› æ­¤å¯èƒ½ä¼šåœ¨æ­£ç¡®ç­”æ¡ˆç½•è§çš„æƒ…å†µä¸‹å¯¼è‡´å¤±è´¥ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç½®ä¿¡åº¦å¼•å¯¼æ—©æœŸåœæ­¢ç­–ç•¥ï¼ˆCGESï¼‰ã€‚è¿™æ˜¯ä¸€ç§è´å¶æ–¯æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ¥è‡ªä»¤ç‰Œæ¦‚ç‡æˆ–å¥–åŠ±æ¨¡å‹çš„æ ‡é‡ç½®ä¿¡ä¿¡å·æ¥æ„å»ºå€™é€‰ç­”æ¡ˆçš„åéªŒåˆ†å¸ƒã€‚ä¸€æ—¦æŸä¸ªå€™é€‰ç­”æ¡ˆçš„åéªŒæ¦‚ç‡è¶…è¿‡é˜ˆå€¼ï¼ŒCGESå°±ä¼šè‡ªé€‚åº”åœ°åœæ­¢é‡‡æ ·ã€‚æˆ‘ä»¬å¯¹å®Œå…¨æ ¡å‡†çš„ç½®ä¿¡åº¦å’Œç°å®çš„å™ªå£°ç½®ä¿¡ä¿¡å·æä¾›äº†ç†è®ºä¿è¯ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCGESé€šè¿‡å°†æ¨¡å‹è°ƒç”¨æ¬¡æ•°å¹³å‡å‡å°‘çº¦69%ï¼ˆä¾‹å¦‚ä»16.0é™è‡³4.9ï¼‰çš„åŒæ—¶ï¼Œå‡†ç¡®ç‡ä¸ä¸€è‡´æ€§é¢„æµ‹ç›¸è¿‘ï¼Œå·®å¼‚åœ¨0.06ä¸ªç™¾åˆ†ç‚¹ä»¥å†…ã€‚è¿™ä¸€æ–°æ–¹æ³•åœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶é‡‡ç”¨å¤šæ¬¡æŸ¥è¯¢å¹¶èšåˆçš„ç­–ç•¥æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å­˜åœ¨å›ºå®šè°ƒç”¨æ¬¡æ•°å’Œæ­£ç¡®ç­”æ¡ˆç½•è§æƒ…å†µä¸‹çš„å¤±è´¥é£é™©ã€‚</li>
<li>ç½®ä¿¡åº¦å¼•å¯¼æ—©æœŸåœæ­¢ç­–ç•¥ï¼ˆCGESï¼‰æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯æ¡†æ¶çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æ ‡é‡ç½®ä¿¡ä¿¡å·æ„å»ºå€™é€‰ç­”æ¡ˆçš„åéªŒåˆ†å¸ƒã€‚</li>
<li>CGESèƒ½è‡ªé€‚åº”åœ°åœæ­¢é‡‡æ ·ï¼Œå½“æŸä¸ªå€™é€‰ç­”æ¡ˆçš„åéªŒæ¦‚ç‡è¶…è¿‡é˜ˆå€¼æ—¶ã€‚</li>
<li>åœ¨ç†è®ºå±‚é¢ä¸Šï¼ŒCGESå¯¹å®Œå…¨æ ¡å‡†çš„ç½®ä¿¡åº¦å’Œç°å®å™ªå£°æƒ…å¢ƒä¸‹çš„ç½®ä¿¡ä¿¡å·æä¾›ä¿éšœã€‚</li>
<li>åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCGESåœ¨å‡å°‘æ¨¡å‹è°ƒç”¨æ¬¡æ•°çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸ä¸€è‡´æ€§é¢„æµ‹ç›¸è¿‘çš„å‡†ç¡®ç‡ã€‚</li>
<li>CGESèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ï¼Œå¹³å‡å‡å°‘æ¨¡å‹è°ƒç”¨æ¬¡æ•°çº¦69%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6121d7c7ee00eca8aa8310f2df38edad" align="middle">
<img src="https://picx.zhimg.com/v2-2d038d63926ac617e47732cd60766d10" align="middle">
<img src="https://picx.zhimg.com/v2-fa8eb9cb7abf24dd5f84a2d38b1a0c54" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CoCoVa-Chain-of-Continuous-Vision-Language-Thought-for-Latent-Space-Reasoning"><a href="#CoCoVa-Chain-of-Continuous-Vision-Language-Thought-for-Latent-Space-Reasoning" class="headerlink" title="CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space   Reasoning"></a>CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space   Reasoning</h2><p><strong>Authors:Jizheng Ma, Xiaofei Zhou, Yanlong Song, Han Yan</strong></p>
<p>In human cognition, there exist numerous thought processes that are tacit and beyond verbal expression, enabling us to understand and interact with the world in multiple ways. However, contemporary Vision-Language Models (VLMs) remain constrained to reasoning within the discrete and rigid space of linguistic tokens, thereby bottlenecking the rich, high-dimensional nature of visual perception. To bridge this gap, we propose CoCoVa (Chain of Continuous Vision-Language Thought), a novel framework for vision-language model that leverages continuous cross-modal reasoning for diverse vision-language tasks. The core of CoCoVa is an iterative reasoning cycle, where a novel Latent Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a chain of latent thought vectors through cross-modal fusion. To focus this process, a token selection mechanism dynamically identifies salient visual regions, mimicking attentional focus. To ensure these latent thoughts remain grounded, we train the model with a multi-task objective that combines contrastive learning and diffusion-based reconstruction, enforcing alignment between latent representations and both visual and textual modalities. Evaluations show CoCoVa improves accuracy and token efficiency over strong baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B models on almost all benchmarks. When scaled to 7B LLM backbones, it remains competitive with state-of-the-art models. Qualitative analysis validates that learned latent space captures interpretable and structured reasoning patterns, highlighting the potential of CoCoVa to bridge the representational gap between discrete language processing and the continuous nature of visual understanding. </p>
<blockquote>
<p>åœ¨äººç±»è®¤çŸ¥ä¸­ï¼Œå­˜åœ¨è®¸å¤šæ— è¨€çš„ã€è¶…è¶Šè¯­è¨€è¡¨è¾¾çš„æ€ç»´è¿‡ç¨‹ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥å¤šç§æ–¹å¼ç†è§£å’Œä¸ä¸–ç•Œäº’åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»ç„¶å—é™äºè¯­è¨€ç¬¦å·çš„ç¦»æ•£å’ŒåƒµåŒ–ç©ºé—´å†…çš„æ¨ç†ï¼Œä»è€Œé™åˆ¶äº†è§†è§‰æ„ŸçŸ¥çš„ä¸°å¯Œã€é«˜ç»´ç‰¹æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†CoCoVaï¼ˆè¿ç»­è§†è§‰è¯­è¨€æ€ç»´é“¾ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è¿ç»­è·¨æ¨¡æ€æ¨ç†æ¥å®Œæˆå„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚CoCoVaçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªè¿­ä»£æ¨ç†å¾ªç¯ï¼Œå…¶ä¸­æ–°å‹æ½œæ€Q-Formersï¼ˆLQ-Formersï¼‰ä½œä¸ºåŠ¨æ€æ¨ç†å¼•æ“ï¼Œé€šè¿‡è·¨æ¨¡æ€èåˆè¿­ä»£åœ°å®Œå–„ä¸€ç³»åˆ—æ½œåœ¨æ€ç»´å‘é‡ã€‚ä¸ºäº†é›†ä¸­è¿™ä¸€è¿‡ç¨‹ï¼Œæ ‡è®°é€‰æ‹©æœºåˆ¶ä¼šåŠ¨æ€è¯†åˆ«é‡è¦çš„è§†è§‰åŒºåŸŸï¼Œæ¨¡ä»¿æ³¨æ„åŠ›çš„é›†ä¸­ã€‚ä¸ºäº†ç¡®ä¿è¿™äº›æ½œåœ¨æ€ç»´ä»ç„¶ä¿æŒæ ¹åŸºï¼Œæˆ‘ä»¬ç”¨å¤šä»»åŠ¡ç›®æ ‡æ¥è®­ç»ƒæ¨¡å‹ï¼Œå°†å¯¹æ¯”å­¦ä¹ å’ŒåŸºäºæ‰©æ•£çš„é‡å»ºç»“åˆèµ·æ¥ï¼Œå¼ºåŒ–æ½œåœ¨è¡¨å¾ä¸è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å¯¹é½ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒCoCoVaåœ¨å¼ºå¤§çš„åŸºçº¿åŸºç¡€ä¸Šæé«˜äº†å‡†ç¡®æ€§å’Œæ ‡è®°æ•ˆç‡ã€‚ä½¿ç”¨è§„æ¨¡ä¸º1.5Bçš„åç«¯éª¨å¹²ç½‘ï¼Œå®ƒåœ¨å‡ ä¹æ‰€æœ‰çš„åŸºå‡†æµ‹è¯•ä¸­éƒ½èƒ½ä¸æ›´å¤§çš„7B-9Bæ¨¡å‹ç«äº‰ç”šè‡³è¶…è¶Šå®ƒä»¬ã€‚å½“æ‰©å±•åˆ°è§„æ¨¡ä¸º7Bçš„å¤§å‹è¯­è¨€æ¨¡å‹åç«¯éª¨å¹²ç½‘æ—¶ï¼Œå®ƒä»ç„¶ä¸æœ€æ–°æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚å®šæ€§åˆ†æéªŒè¯äº†å­¦ä¹ åˆ°çš„æ½œåœ¨ç©ºé—´æ•è·äº†è§£é‡Šæ€§å’Œç»“æ„åŒ–æ¨ç†æ¨¡å¼ï¼Œè¿™çªæ˜¾äº†CoCoVaåœ¨ç¦»æ•£è¯­è¨€å¤„ç†å’Œè§†è§‰ç†è§£çš„è¿ç»­æ€§ä¹‹é—´å¼¥åˆä»£è¡¨æ€§å·®è·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02360v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„è·¨æ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹â€”â€”CoCoVaï¼ˆè¿ç»­è§†è§‰è¯­è¨€æ€ç»´é“¾ï¼‰ï¼Œå®ƒçªç ´äº†ä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç¦»æ•£è¯­è¨€ç¬¦å·ä¸Šçš„å±€é™ï¼Œåˆ©ç”¨è¿ç»­è·¨æ¨¡æ€æ¨ç†å®Œæˆå¤šæ ·åŒ–çš„è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚æ ¸å¿ƒåœ¨äºä¸€ä¸ªè¿­ä»£æ¨ç†å¾ªç¯ï¼Œå…¶ä¸­æ–°é¢–çš„æ½œæ€Q-Formerï¼ˆLQ-Formerï¼‰ä½œä¸ºåŠ¨æ€æ¨ç†å¼•æ“ï¼Œé€šè¿‡è·¨æ¨¡æ€èåˆï¼Œä¸æ–­ç²¾ç‚¼æ½œæ€æ€ç»´å‘é‡é“¾ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ‰©æ•£é‡å»ºçš„å¤šä»»åŠ¡ç›®æ ‡ï¼Œç¡®ä¿æ½œæ€æ€ç»´ä¸è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€å¯¹é½ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒCoCoVaåœ¨å¼ºåŸºçº¿åŸºç¡€ä¸Šæé«˜äº†å‡†ç¡®æ€§å’Œç¬¦å·æ•ˆç‡ï¼Œå…·æœ‰æ½œåŠ›ç¼©å°ç¦»æ•£è¯­è¨€å¤„ç†å’Œè¿ç»­è§†è§‰ç†è§£ä¹‹é—´çš„ä»£è¡¨æ€§å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoCoVaæ˜¯ä¸€ç§æ–°å‹çš„è·¨æ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨çªç ´ä¼ ç»Ÿæ¨¡å‹çš„å±€é™ã€‚</li>
<li>å®ƒåˆ©ç”¨è¿ç»­è·¨æ¨¡æ€æ¨ç†å®Œæˆå¤šæ ·åŒ–çš„è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚</li>
<li>CoCoVaçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªè¿­ä»£æ¨ç†å¾ªç¯ï¼Œå…¶ä¸­LQ-Formerä½œä¸ºåŠ¨æ€æ¨ç†å¼•æ“å‘æŒ¥å…³é”®ä½œç”¨ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ‰©æ•£é‡å»ºçš„å¤šä»»åŠ¡ç›®æ ‡ï¼Œç¡®ä¿æ½œæ€æ€ç»´ä¸è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€å¯¹é½ã€‚</li>
<li>CoCoVaæé«˜äº†å‡†ç¡®æ€§å’Œç¬¦å·æ•ˆç‡ï¼Œè¡¨ç°ä¼˜äºå¼ºåŸºçº¿ã€‚</li>
<li>å½“æ‰©å±•åˆ°å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼ŒCoCoVaä¿æŒä¸æœ€æ–°æ¨¡å‹çš„ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-981f0a8c6800f34fd17a373627633b3c" align="middle">
<img src="https://picx.zhimg.com/v2-0b8b4d823464adde19141202b6c00dba" align="middle">
<img src="https://picx.zhimg.com/v2-41a792da7ca101b426adb8d5107fc1de" align="middle">
<img src="https://picx.zhimg.com/v2-c14474025fe7134f1b5ab5ba2a5f2106" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute"><a href="#The-Sequential-Edge-Inverse-Entropy-Voting-Beats-Parallel-Self-Consistency-at-Matched-Compute" class="headerlink" title="The Sequential Edge: Inverse-Entropy Voting Beats Parallel   Self-Consistency at Matched Compute"></a>The Sequential Edge: Inverse-Entropy Voting Beats Parallel   Self-Consistency at Matched Compute</h2><p><strong>Authors:Aman Sharma, Paras Chopra</strong></p>
<p>We revisit test-time scaling for language model reasoning and ask a fundamental question: at equal token budget and compute, is it better to run multiple independent chains in parallel, or to run fewer chains that iteratively refine through sequential steps? Through comprehensive evaluation across 5 state-of-the-art open source models and 3 challenging reasoning benchmarks, we find that sequential scaling where chains explicitly build upon previous attempts consistently outperforms the dominant parallel self-consistency paradigm in 95.6% of configurations with gains in accuracy upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel training-free method to further boost the accuracy of sequential scaling. By weighing answers in proportion to the inverse entropy of their reasoning chains, we increase our success rate over parallel majority and establish it as the optimal test-time scaling strategy. Our findings fundamentally challenge the parallel reasoning orthodoxy that has dominated test-time scaling since Wang et al.â€™s self-consistency decoding (Wang et al., 2022), positioning sequential refinement as the robust default for modern LLM reasoning and necessitating a paradigm shift in how we approach inference-time optimization. </p>
<blockquote>
<p>æˆ‘ä»¬é‡æ–°æ¢è®¨äº†è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šåœ¨ç›¸åŒçš„ä»¤ç‰Œé¢„ç®—å’Œè®¡ç®—æ¡ä»¶ä¸‹ï¼Œæ˜¯å¹¶è¡Œè¿è¡Œå¤šä¸ªç‹¬ç«‹é“¾æ›´å¥½ï¼Œè¿˜æ˜¯è¿è¡Œæ›´å°‘çš„é“¾ï¼Œé€šè¿‡ä¸€ç³»åˆ—æ­¥éª¤è¿›è¡Œè¿­ä»£ç»†åŒ–ï¼Ÿé€šè¿‡å¯¹5ä¸ªæœ€å…ˆè¿›å¼€æºæ¨¡å‹å’Œ3ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§æ¨ç†åŸºå‡†çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°åŸºäºæ˜ç¡®å»ºç«‹äºå…ˆå‰å°è¯•ä¹‹ä¸Šçš„åºåˆ—ç¼©æ”¾æ–¹æ³•å§‹ç»ˆä¼˜äºä¸»æµå¹¶è¡Œä¸€è‡´æ€§èŒƒå¼ï¼Œåœ¨95.6%çš„é…ç½®ä¸­å‡†ç¡®æ€§æé«˜äº†é«˜è¾¾46.7%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ— é€†ç†µåŠ æƒæŠ•ç¥¨æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°æ–¹æ³•ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜åºåˆ—ç¼©æ”¾çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å°†ç­”æ¡ˆçš„æ¯”ä¾‹æŒ‰ç…§å…¶æ¨ç†é“¾çš„é€†ç†µè¿›è¡ŒåŠ æƒï¼Œæˆ‘ä»¬çš„æˆåŠŸç‡è¶…è¿‡äº†å¹¶è¡Œå¤šæ•°æŠ•ç¥¨ï¼Œå¹¶å°†å®ƒç¡®ç«‹ä¸ºæœ€ä½³çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ç­–ç•¥ã€‚æˆ‘ä»¬çš„å‘ç°ä»æ ¹æœ¬ä¸ŠæŒ‘æˆ˜äº†è‡ªç‹ç­‰äººæå‡ºä¸€è‡´æ€§è§£ç ä»¥æ¥çš„å¹¶è¡Œæ¨ç†æ­£ç»Ÿè§‚å¿µï¼ˆç‹ç­‰äººï¼Œ2022å¹´ï¼‰ï¼Œå°†åºåˆ—ç»†åŒ–å®šä½ä¸ºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„ç¨³å¥é»˜è®¤æ–¹æ³•ï¼Œå¹¶æ”¹å˜äº†æˆ‘ä»¬å¯¹æ¨ç†æ—¶é—´ä¼˜åŒ–çš„çœ‹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02309v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å…³äºæµ‹è¯•æ—¶è¯­è¨€æ¨¡å‹æ¨ç†çš„è§„æ¨¡æ‰©å±•ï¼Œç ”ç©¶é‡æ–°æ¢è®¨äº†å¹¶è¡Œé“¾ä¸é¡ºåºç²¾ç»†åŒ–ä¸¤ç§ç­–ç•¥ã€‚è¯„ä¼°å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•åï¼Œå‘ç°é¡ºåºç²¾ç»†åŒ–æ‰©å±•æ³•ï¼Œå³åœ¨å°è¯•ä¹‹é—´æ„å»ºæ˜ç¡®çš„ä¾èµ–æ€§å¹¶æŒç»­ä¼˜åŒ–ï¼Œåœ¨æ‰€æœ‰é…ç½®ä¸­ä¸€è‡´åœ°ä¼˜äºå¹¶è¡Œä¸€è‡´æ€§èŒƒå¼ï¼Œå‡†ç¡®ç‡æå‡é«˜è¾¾46.7%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†é€†ç†µåŠ æƒæŠ•ç¥¨æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯è¿›ä¸€æ­¥æå‡é¡ºåºç²¾ç»†åŒ–çš„å‡†ç¡®æ€§ã€‚é€šè¿‡æ ¹æ®æ¨ç†é“¾çš„é€†ç†µæ¯”ä¾‹åŠ æƒç­”æ¡ˆï¼Œè¯¥æ–¹æ³•æé«˜äº†æˆåŠŸç‡ï¼Œå¹¶ç¡®ç«‹é¡ºåºç²¾ç»†åŒ–ä¸ºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„ç¨³å¥é»˜è®¤é€‰æ‹©ã€‚è¿™æŒ‘æˆ˜äº†è‡ªWangç­‰äººä»¥æ¥çš„å¹¶è¡Œæ¨ç†æ­£ç»Ÿè§‚å¿µï¼Œå¹¶æ”¹å˜äº†æˆ‘ä»¬å¯¹æ¨ç†æ—¶é—´ä¼˜åŒ–çš„çœ‹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶è¯­è¨€æ¨¡å‹æ¨ç†çš„è§„æ¨¡æ‰©å±•å­˜åœ¨ä¸¤ç§ä¸»è¦ç­–ç•¥ï¼šå¹¶è¡Œé“¾å’Œé¡ºåºç²¾ç»†åŒ–ã€‚</li>
<li>åœ¨å¤§å¤šæ•°é…ç½®ä¸­ï¼Œé¡ºåºç²¾ç»†åŒ–æ‰©å±•æ³•ä¼˜äºå¹¶è¡Œä¸€è‡´æ€§èŒƒå¼ã€‚æ­¤æ–¹æ³•åœ¨å°è¯•é—´æ„å»ºä¾èµ–å…³ç³»å¹¶æŒç»­ä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡ç»¼åˆè¯„ä¼°äº”ä¸ªæœ€æ–°å¼€æºæ¨¡å‹å’Œä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†æµ‹è¯•å¾—å‡ºä¸Šè¿°ç»“è®ºã€‚é¡ºåºç²¾ç»†åŒ–æ–¹æ³•å‡†ç¡®ç‡æå‡é«˜è¾¾46.7%ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„ä¼˜åŒ–æ–¹æ³•ï¼šé€†ç†µåŠ æƒæŠ•ç¥¨æ³•ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜é¡ºåºç²¾ç»†åŒ–çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ ¹æ®æ¨ç†é“¾çš„é€†ç†µæ¯”ä¾‹åŠ æƒç­”æ¡ˆæ¥æé«˜æˆåŠŸç‡ã€‚è¿™æ˜¯æµ‹è¯•æ—¶ä¸€ç§æœ‰æ•ˆä¸”æœ€ä¼˜çš„ç­–ç•¥é€‰æ‹©ã€‚å¯¹æ¯”å…ˆå‰çš„ä¸»è¦æ–¹å¼æœ‰æ˜æ˜¾ä¼˜åŠ¿æå‡ã€‚è¯¥ç ”ç©¶ç»“æœæ‰“ç ´äº†ç›®å‰å¹¶è¡Œæ¨ç†çš„æ­£ç»Ÿè§‚å¿µã€‚</li>
<li>ç ”ç©¶ç»“æœç¡®ç«‹äº†é¡ºåºç²¾ç»†åŒ–ä¸ºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„ç¨³å¥é»˜è®¤é€‰æ‹©ã€‚è¿™å¯¹äºæœªæ¥çš„è¯­è¨€æ¨¡å‹è®¾è®¡å’Œåº”ç”¨å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02309">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a64d91162000c1ffb681c89132aab60" align="middle">
<img src="https://picx.zhimg.com/v2-50004749f7550d6f51b47916b24f0695" align="middle">
<img src="https://picx.zhimg.com/v2-30ab7c41936230aede29b3b27c103ad6" align="middle">
<img src="https://picx.zhimg.com/v2-a9186a05fe1a9db3e87b3e3ceb13a8fb" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VFocus-Better-Verilog-Generation-from-Large-Language-Model-via-Focused-Reasoning"><a href="#VFocus-Better-Verilog-Generation-from-Large-Language-Model-via-Focused-Reasoning" class="headerlink" title="VFocus: Better Verilog Generation from Large Language Model via Focused   Reasoning"></a>VFocus: Better Verilog Generation from Large Language Model via Focused   Reasoning</h2><p><strong>Authors:Zhuorui Zhao, Bing Li, Grace Li Zhang, Ulf Schlichtmann</strong></p>
<p>Large Language Models (LLMs) have shown impressive potential in generating Verilog codes, but ensuring functional correctness remains a challenge. Existing approaches often rely on self-consistency or simulation feedback to select the best candidate, but they miss opportunities to focus LLM reasoning on the most informative parts of the design. We propose VFocus, a three-stage framework that enhances Verilog generation by sharpening the focus of LLM reasoning onto critical decision points in the code generation process. In the \textbf{pre-ranking stage}, VFocus generates multiple code candidates through LLM prompting, retries for syntactically valid outputs, and introduces a \textit{Density-guided Filtering} to retain candidates that fall within the â€œreasoning sweet spotâ€ for functional correctness. In the \textbf{ranking stage}, we simulate each code candidate using an automatically generated testbench and apply self-consistency-based clustering to identify the most consistent outputs. Finally, in the \textbf{post-ranking refinement stage}, VFocus performs inconsistency mining on top-ranked candidates and invokes reasoning-augmented LLM prompts for candidate refinement. Experiments on the VerilogEval-Human benchmark show that VFocus significantly improves the pass@1 correctness across multiple reasoning LLMs, demonstrating its effectiveness in enhancing Verilog generation for complex hardware design tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”ŸæˆVerilogä»£ç æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ½œåŠ›ï¼Œä½†ç¡®ä¿åŠŸèƒ½æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºè‡ªæˆ‘ä¸€è‡´æ€§æˆ–æ¨¡æ‹Ÿåé¦ˆæ¥é€‰æ‹©æœ€ä½³å€™é€‰è€…ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å°†LLMæ¨ç†é›†ä¸­åœ¨è®¾è®¡ä¸­æœ€å…·ä¿¡æ¯æ€§çš„éƒ¨åˆ†çš„æœºä¼šã€‚æˆ‘ä»¬æå‡ºäº†VFocusï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡å¢å¼ºLLMæ¨ç†åœ¨ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…³é”®å†³ç­–ç‚¹çš„å…³æ³¨åº¦ï¼Œæ¥æå‡Verilogçš„ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨<strong>é¢„æ’åºé˜¶æ®µ</strong>ï¼ŒVFocusé€šè¿‡LLMæç¤ºç”Ÿæˆå¤šä¸ªä»£ç å€™é€‰è€…ï¼Œå¯¹è¯­æ³•æœ‰æ•ˆçš„è¾“å‡ºè¿›è¡Œé‡è¯•ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§<strong>å¯†åº¦å¼•å¯¼è¿‡æ»¤</strong>ï¼Œä»¥ä¿ç•™é‚£äº›å¤„äºåŠŸèƒ½æ­£ç¡®æ€§â€œæ¨ç†ç”œç‚¹â€å†…çš„å€™é€‰è€…ã€‚åœ¨<strong>æ’åºé˜¶æ®µ</strong>ï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•å¹³å°å¯¹æ¯ä¸ªä»£ç å€™é€‰è€…è¿›è¡Œæ¨¡æ‹Ÿï¼Œå¹¶åº”ç”¨åŸºäºè‡ªæˆ‘ä¸€è‡´æ€§èšç±»çš„æ–¹æ³•ï¼Œä»¥è¯†åˆ«æœ€ä¸€è‡´çš„è¾“å‡ºã€‚æœ€åï¼Œåœ¨<strong>åæ’åºä¼˜åŒ–é˜¶æ®µ</strong>ï¼ŒVFocuså¯¹æ’åé å‰çš„å€™é€‰è€…è¿›è¡Œä¸ä¸€è‡´æ€§æŒ–æ˜ï¼Œå¹¶è°ƒç”¨å¢å¼ºæ¨ç†çš„LLMæç¤ºè¿›è¡Œå€™é€‰è€…ä¼˜åŒ–ã€‚åœ¨VerilogEval-HumanåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVFocusæ˜¾è‘—æé«˜äº†å¤šä¸ªæ¨ç†LLMçš„pass@1æ­£ç¡®æ€§ï¼Œè¯æ˜äº†å…¶åœ¨å¢å¼ºå¤æ‚ç¡¬ä»¶è®¾è®¡ä»»åŠ¡çš„Verilogç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02285v1">PDF</a> accepted by SOCC 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”ŸæˆVerilogä»£ç æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ä¿è¯åŠŸèƒ½æ­£ç¡®æ€§ä»æ˜¯æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–äºè‡ªæˆ‘ä¸€è‡´æ€§æˆ–æ¨¡æ‹Ÿåé¦ˆæ¥é€‰æ‹©æœ€ä½³å€™é€‰ï¼Œä½†å¿½ç•¥äº†å°†LLMæ¨ç†èšç„¦äºè®¾è®¡ä¸­æœ€å…·ä¿¡æ¯ä»·å€¼çš„éƒ¨åˆ†çš„æœºä¼šã€‚æœ¬æ–‡æå‡ºVFocusï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µçš„æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–LLMæ¨ç†çš„ç„¦ç‚¹æ¥æå‡Verilogä»£ç ç”Ÿæˆçš„è´¨é‡ã€‚é¢„æ’åºé˜¶æ®µé€šè¿‡LLMæç¤ºç”Ÿæˆå¤šä¸ªä»£ç å€™é€‰ï¼Œå¯¹è¯­æ³•æœ‰æ•ˆçš„è¾“å‡ºè¿›è¡Œé‡è¯•ï¼Œå¹¶é€šè¿‡å¯†åº¦å¼•å¯¼è¿‡æ»¤æ¥ä¿ç•™å¤„äºâ€œæ¨ç†ç”œç‚¹â€å†…çš„å€™é€‰ï¼Œä»¥ç¡®ä¿åŠŸèƒ½æ­£ç¡®æ€§ã€‚æ’åºé˜¶æ®µä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•å¹³å°å¯¹ä»£ç å€™é€‰è¿›è¡Œæ¨¡æ‹Ÿï¼Œå¹¶é€šè¿‡åŸºäºè‡ªæˆ‘ä¸€è‡´æ€§çš„èšç±»æ¥è¯†åˆ«æœ€ä¸€è‡´çš„è¾“å‡ºã€‚æœ€ååœ¨æ’ååçš„æ”¹è¿›é˜¶æ®µï¼ŒVFocuså¯¹æ’åé å‰çš„å€™é€‰è¿›è¡Œä¸ä¸€è‡´æ€§æŒ–æ˜ï¼Œå¹¶è°ƒç”¨å¢å¼ºæ¨ç†çš„LLMæç¤ºè¿›è¡Œå€™é€‰ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒVFocusåœ¨VerilogEval-HumanåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æé«˜äº†ä¸€è‡´æ€§å‡†ç¡®ç‡ï¼Œè¯æ˜å…¶åœ¨å¤æ‚ç¡¬ä»¶è®¾è®¡ä»»åŠ¡ä¸­å¢å¼ºVerilogä»£ç ç”Ÿæˆçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨Verilogä»£ç ç”Ÿæˆæ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åŠŸèƒ½æ­£ç¡®æ€§æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è‡ªæˆ‘ä¸€è‡´æ€§æˆ–æ¨¡æ‹Ÿåé¦ˆï¼Œä½†å¯èƒ½å¿½ç•¥å…³é”®å†³ç­–ç‚¹ã€‚</li>
<li>VFocusæ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–LLMæ¨ç†æ¥æå‡Verilogä»£ç ç”Ÿæˆè´¨é‡ã€‚</li>
<li>åœ¨é¢„æ’åºé˜¶æ®µï¼ŒVFocusé€šè¿‡LLMæç¤ºç”Ÿæˆå¤šä¸ªå€™é€‰ï¼Œè¿›è¡Œè¯­æ³•éªŒè¯å’Œå¯†åº¦å¼•å¯¼è¿‡æ»¤ã€‚</li>
<li>æ’åºé˜¶æ®µä½¿ç”¨æµ‹è¯•å¹³å°æ¨¡æ‹Ÿä»£ç å€™é€‰ï¼Œé€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§èšç±»è¯†åˆ«æœ€ä¸€è‡´è¾“å‡ºã€‚</li>
<li>åœ¨æ’ååçš„æ”¹è¿›é˜¶æ®µï¼ŒVFocusæŒ–æ˜å€™é€‰ä¸ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡å¢å¼ºæ¨ç†çš„LLMæç¤ºè¿›è¡Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02285">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa6414f2a34a77c0472c9346917d7ba9" align="middle">
<img src="https://picx.zhimg.com/v2-d727b598e747fc1d4d9aa973a8803cb1" align="middle">
<img src="https://picx.zhimg.com/v2-33522eab469b9d1bf618e4b2c4fd97b1" align="middle">
<img src="https://picx.zhimg.com/v2-3be0654e3be72de796a05ca123f57a9e" align="middle">
<img src="https://picx.zhimg.com/v2-72b814ad285b9909b93073016bf49efe" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning"><a href="#SAIL-RL-Guiding-MLLMs-in-When-and-How-to-Think-via-Dual-Reward-RL-Tuning" class="headerlink" title="SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL   Tuning"></a>SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL   Tuning</h2><p><strong>Authors:Fangxun Shu, Yongjie Ye, Yue Liao, Zijian Kang, Weijie Yin, Jiacong Wang, Xiao Liang, Shuicheng Yan, Chao Feng</strong></p>
<p>We introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/BytedanceDouyinContent/SAIL-RL">https://github.com/BytedanceDouyinContent/SAIL-RL</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SAIL-RLï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ•™æˆå®ƒä»¬ä½•æ—¶ä»¥åŠå¦‚ä½•æ€è€ƒï¼Œå¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•å—åˆ°ä»…ç»“æœç›‘ç£çš„é™åˆ¶ï¼Œè¿™ç§ç›‘ç£åªå¥–åŠ±æ­£ç¡®ç­”æ¡ˆï¼Œè€Œä¸ç¡®ä¿åˆç†çš„æ¨ç†è¿‡ç¨‹ï¼›åŒæ—¶ä¹Ÿå—åˆ°ç»Ÿä¸€æ€è€ƒç­–ç•¥çš„é™åˆ¶ï¼Œè¿™å¾€å¾€å¯¼è‡´åœ¨ç®€å•ä»»åŠ¡ä¸Šè¿‡åº¦æ€è€ƒä»¥åŠåœ¨å¤æ‚ä»»åŠ¡ä¸Šæ€è€ƒä¸è¶³ã€‚SAIL-RLé€šè¿‡åŒé‡å¥–åŠ±ç³»ç»Ÿè§£å†³è¿™äº›æŒ‘æˆ˜ï¼šæ€è€ƒå¥–åŠ±ï¼Œå®ƒé€šè¿‡åŸºäºäº‹å®çš„ä¾æ®ã€é€»è¾‘è¿è´¯æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§æ¥è¯„ä¼°æ¨ç†è´¨é‡ï¼›åˆ¤æ–­å¥–åŠ±ï¼Œå®ƒè‡ªé€‚åº”åœ°ç¡®å®šæ·±åº¦æ¨ç†æˆ–ç›´æ¥å›ç­”æ˜¯å¦åˆé€‚ã€‚åœ¨å…ˆè¿›SAIL-VL2ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒSAIL-RLåœ¨4Bå’Œ8Bè§„æ¨¡ä¸Šæé«˜äº†æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œä¸GPT-4oç­‰å•†ä¸šé—­æºæ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶å¤§å¤§å‡å°‘å¹»è§‰ç°è±¡ï¼Œæˆä¸ºæ„å»ºæ›´å¯é å’Œé€‚åº”æ€§æ›´å¼ºçš„MLLMsçš„åŸåˆ™æ€§æ¡†æ¶ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/BytedanceDouyinContent/SAIL-RL%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/BytedanceDouyinContent/SAIL-RLä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02280v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åå¤„ç†è®­ç»ƒæ¡†æ¶SAIL-RLï¼Œé€šè¿‡æ•™æˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½•æ—¶ä»¥åŠå¦‚ä½•æ€è€ƒï¼Œå¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•ä»…é€šè¿‡ç»“æœç›‘ç£å¸¦æ¥çš„é—®é¢˜ï¼Œå¦‚æ— æ³•ç¡®ä¿æ¨ç†è´¨é‡åŠé€‚åº”æ€§åœ°åˆ¤æ–­æ·±æ¨ç†æˆ–ç›´æ¥å›ç­”çš„éœ€æ±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒSAIL-RLæé«˜äº†æœ€å…ˆè¿›çš„SAIL-VL2æ¨¡å‹çš„æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œä¸GPT-4oç­‰å•†ä¸šé—­æºæ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†å¹»è§‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAIL-RLæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ åå¤„ç†è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ä»…é€šè¿‡ç»“æœç›‘ç£çš„é—®é¢˜ï¼Œæ— æ³•ç¡®ä¿æ¨ç†è´¨é‡ã€‚</li>
<li>SAIL-RLé€šè¿‡åŒå¥–åŠ±ç³»ç»Ÿè§£å†³è¿™äº›é—®é¢˜ï¼šæ€è€ƒå¥–åŠ±ç”¨äºè¯„ä¼°æ¨ç†è´¨é‡ï¼Œåˆ¤æ–­å¥–åŠ±ç”¨äºç¡®å®šé€‚å½“çš„æ¨ç†æ·±åº¦ã€‚</li>
<li>SAIL-RLæé«˜äº†SAIL-VL2æ¨¡å‹çš„æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•æ€§èƒ½ã€‚</li>
<li>ä¸å•†ä¸šé—­æºæ¨¡å‹å¦‚GPT-4oç›¸æ¯”ï¼ŒSAIL-RLå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>SAIL-RLæ˜¾è‘—å‡å°‘äº†æ¨¡å‹äº§ç”Ÿçš„å¹»è§‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-163c2f787046981a62df59bea4b93897" align="middle">
<img src="https://picx.zhimg.com/v2-02501d68cfa704838c67cf440e2f1c73" align="middle">
<img src="https://picx.zhimg.com/v2-0ec3ca0ef9626e98a440fccedca0a2fc" align="middle">
<img src="https://picx.zhimg.com/v2-ee3ddb6bea10fd0be99068ab20d1da81" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data"><a href="#TabDSR-Decompose-Sanitize-and-Reason-for-Complex-Numerical-Reasoning-in-Tabular-Data" class="headerlink" title="TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning   in Tabular Data"></a>TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning   in Tabular Data</h2><p><strong>Authors:Changjiang Jiang, Fengchang Yu, Haihua Chen, Wei Lu, Jin Zeng</strong></p>
<p>Complex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose \method, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that \method consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œçš„æ•°æ®åˆ†æä¸­ï¼Œå¯¹è¡¨æ ¼æ•°æ®è¿›è¡Œå¤æ‚æ¨ç†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¾€å¾€å› å¤æ‚çš„æŸ¥è¯¢ã€å˜ˆæ‚çš„æ•°æ®å’Œæœ‰é™çš„æ•°å€¼èƒ½åŠ›è€Œè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†\methodæ¡†æ¶ï¼Œå®ƒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æŸ¥è¯¢åˆ†è§£å™¨ï¼Œç”¨äºåˆ†è§£å¤æ‚é—®é¢˜ï¼›ï¼ˆ2ï¼‰è¡¨æ ¼å‡€åŒ–å™¨ï¼Œç”¨äºæ¸…ç†å’Œè¿‡æ»¤å˜ˆæ‚çš„è¡¨æ ¼ï¼›ï¼ˆ3ï¼‰åŸºäºç¨‹åºæ€ç»´ï¼ˆPoTï¼‰çš„æ¨ç†å™¨ï¼Œç”Ÿæˆå¯æ‰§è¡Œä»£ç ä»å‡€åŒ–åçš„è¡¨æ ¼ä¸­æ¨å¯¼å‡ºæœ€ç»ˆç­”æ¡ˆã€‚ä¸ºäº†ç¡®ä¿å…¬æ­£çš„è¯„ä»·å¹¶å‡è½»æ•°æ®æ³„éœ²é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°æ•°æ®é›†CalTab151ï¼Œè¯¥æ•°æ®é›†ä¸“ä¸ºè¡¨æ ¼ä¸Šçš„å¤æ‚æ•°å€¼æ¨ç†è®¾è®¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ\methodæ¡†æ¶å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨TAT-QAã€TableBenchå’Œ\methodä¸Šåˆ†åˆ«å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡æé«˜äº†8.79%ã€6.08%å’Œ19.87%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ä¸»æµLLMsä¸­ï¼Œä¸ºå¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨å¢å¼ºLLMè¿›è¡Œå¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®å’Œä»£ç å¯åœ¨è¯·æ±‚æ—¶æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02219v1">PDF</a> Accepted to EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤æ‚è¡¨æ ¼æ•°æ®æ¨ç†åœ¨çœŸå®æ•°æ®åˆ†æä¸­å…·æœ‰é‡è¦åœ°ä½ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å› å¤æ‚æŸ¥è¯¢ã€å™ªå£°æ•°æ®å’Œæœ‰é™æ•°å€¼å¤„ç†èƒ½åŠ›è€Œè¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåä¸ºâ€œæ–¹æ³•â€çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬æŸ¥è¯¢åˆ†è§£å™¨ã€è¡¨æ ¼å‡€åŒ–å™¨å’ŒåŸºäºç¨‹åºæ€ç»´ï¼ˆPoTï¼‰çš„æ¨ç†å™¨ã€‚ä¸ºç¡®ä¿å…¬æ­£è¯„ä¼°å¹¶é¿å…æ•°æ®æ³„éœ²ï¼Œæˆ‘ä»¬å¼•å…¥ä¸“ä¸ºå¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†è®¾è®¡çš„æ–°æ•°æ®é›†CalTab151ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œâ€œæ–¹æ³•â€æ¡†æ¶åœ¨TAT-QAã€TableBenchå’Œè‡ªèº«æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†8.79%ã€6.08%å’Œ19.87%ï¼Œå¹¶å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼Œâ€œæ–¹æ³•â€æ¡†æ¶ä¸ä¸»æµLLMæ— ç¼é›†æˆï¼Œä¸ºå¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›å‘ç°çªæ˜¾äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨æå‡LLMå¤„ç†å¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®å’Œä»£ç å¯ä¾›è¯·æ±‚è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚è¡¨æ ¼æ•°æ®æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å¤„ç†å¤æ‚æŸ¥è¯¢ã€å™ªå£°æ•°æ®å’Œæœ‰é™çš„æ•°å€¼å¤„ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºåä¸ºâ€œæ–¹æ³•â€çš„æ¡†æ¶ï¼ŒåŒ…å«æŸ¥è¯¢åˆ†è§£å™¨ã€è¡¨æ ¼å‡€åŒ–å™¨å’ŒåŸºäºç¨‹åºæ€ç»´çš„æ¨ç†å™¨ï¼Œä»¥æ”¹è¿›LLMsçš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ–°æ•°æ®é›†CalTab151ï¼Œä¸“ä¸ºå¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†è®¾è®¡ï¼Œä»¥ç¡®ä¿å…¬æ­£è¯„ä¼°å¹¶é¿å…æ•°æ®æ³„éœ²ã€‚</li>
<li>â€œæ–¹æ³•â€æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>â€œæ–¹æ³•â€æ¡†æ¶ä¸ä¸»æµLLMæ— ç¼é›†æˆï¼Œä¸ºå¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ¡†æ¶çš„æœ‰æ•ˆæ€§åœ¨äºæé«˜LLMå¤„ç†å¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bc1327d808e30b2576551c447e6ff02" align="middle">
<img src="https://picx.zhimg.com/v2-44166456826338049ec8b53758f0ceb0" align="middle">
<img src="https://picx.zhimg.com/v2-1698f4edd342681a39ed6832cd7459e4" align="middle">
<img src="https://picx.zhimg.com/v2-3654fb58861752fa42a3b55ed0a0dbf3" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Optimal-Agent-Selection-State-Aware-Routing-Framework-for-Efficient-Multi-Agent-Collaboration"><a href="#Optimal-Agent-Selection-State-Aware-Routing-Framework-for-Efficient-Multi-Agent-Collaboration" class="headerlink" title="Optimal-Agent-Selection: State-Aware Routing Framework for Efficient   Multi-Agent Collaboration"></a>Optimal-Agent-Selection: State-Aware Routing Framework for Efficient   Multi-Agent Collaboration</h2><p><strong>Authors:Jingbo Wang, Sendong Zhao, Haochun Wang, Yuzheng Fan, Lizhe Zhang, Yan Liu, Ting Liu</strong></p>
<p>The emergence of multi-agent systems powered by large language models (LLMs) has unlocked new frontiers in complex task-solving, enabling diverse agents to integrate unique expertise, collaborate flexibly, and address challenges unattainable for individual models. However, the full potential of such systems is hindered by rigid agent scheduling and inefficient coordination strategies that fail to adapt to evolving task requirements. In this paper, we propose STRMAC, a state-aware routing framework designed for efficient collaboration in multi-agent systems. Our method separately encodes interaction history and agent knowledge to power the router, which adaptively selects the most suitable single agent at each step for efficient and effective collaboration. Furthermore, we introduce a self-evolving data generation approach that accelerates the collection of high-quality execution paths for efficient system training. Experiments on challenging collaborative reasoning benchmarks demonstrate that our method achieves state-of-the-art performance, achieving up to 23.8% improvement over baselines and reducing data collection overhead by up to 90.1% compared to exhaustive search. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ­£é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠ›é‡å¼€æ‹“å¤æ‚ä»»åŠ¡è§£å†³çš„æ–°ç–†ç•Œï¼Œå®ƒå…è®¸å„ç±»æ™ºèƒ½ä½“èå…¥ç‹¬ç‰¹ä¸“é•¿ï¼Œçµæ´»åä½œï¼Œå¹¶è§£å†³å•ä¸ªæ¨¡å‹æ— æ³•åº”å¯¹çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œæ­¤ç±»ç³»ç»Ÿçš„å…¨éƒ¨æ½œåŠ›å—é™äºåƒµåŒ–çš„æ™ºèƒ½ä½“è°ƒåº¦å’Œæ— æ³•é€‚åº”ä¸æ–­å˜åŒ–çš„ä»»åŠ¡è¦æ±‚çš„ä½æ•ˆåè°ƒç­–ç•¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé«˜æ•ˆåä½œçš„çŠ¶æ€æ„ŸçŸ¥è·¯ç”±æ¡†æ¶STRMACã€‚æˆ‘ä»¬çš„æ–¹æ³•å•ç‹¬ç¼–ç äº¤äº’å†å²å’Œæ™ºèƒ½ä½“çŸ¥è¯†æ¥ä¸ºè·¯ç”±å™¨æä¾›åŠ¨åŠ›ï¼Œè·¯ç”±å™¨è‡ªé€‚åº”åœ°é€‰æ‹©æ¯ä¸€æ­¥ä¸­æœ€åˆé€‚çš„å•ä¸€æ™ºèƒ½ä½“ä»¥å®ç°é«˜æ•ˆä¸”æœ‰æ•ˆçš„åä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œå¯åŠ é€Ÿé«˜è´¨é‡æ‰§è¡Œè·¯å¾„çš„æ”¶é›†ä»¥ç”¨äºé«˜æ•ˆçš„ç³»ç»Ÿè®­ç»ƒã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åä½œæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•å®ç°äº†é«˜è¾¾23.8%çš„æå‡ï¼Œç›¸è¾ƒäºç©·ä¸¾æœç´¢å‡å°‘äº†é«˜è¾¾90.1%çš„æ•°æ®æ”¶é›†å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02200v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å‡ºç°ä¸ºå¤æ‚ä»»åŠ¡è§£å†³å¼€å¯äº†æ–°çºªå…ƒï¼Œä½¿ä¸åŒæ™ºèƒ½ä½“èƒ½æ•´åˆç‹¬ç‰¹ä¸“ä¸šçŸ¥è¯†ã€çµæ´»åä½œï¼Œå¹¶åº”å¯¹ä¸ªä½“æ¨¡å‹æ— æ³•è§£å†³çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¯¥ç³»ç»Ÿå®Œå…¨æ½œåŠ›å—é™åƒµåŒ–è°ƒåº¦ä¸ä½æ•ˆåè°ƒç­–ç•¥ï¼Œæœªèƒ½é€‚åº”ä»»åŠ¡å˜åŒ–çš„å¤šæ ·æ€§ã€‚æœ¬ç ”ç©¶æå‡ºSTRMACè¿™ä¸€æ€åŠ¿æ„ŸçŸ¥è·¯ç”±æ¡†æ¶ä»¥æ¨åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æœ‰æ•ˆåä½œã€‚æ–¹æ³•åŒ…æ‹¬äº’åŠ¨å†å²åŠä»£ç†çŸ¥è¯†ç¼–ç é©±åŠ¨è·¯ç”±å™¨é€‰æ‹©æœºåˆ¶ï¼Œä»¥ä¾¿æ ¹æ®è‡ªé€‚åº”é€‰å®šå„é˜¶æ®µçš„æœ€ä½³å•ä¸€æ™ºèƒ½ä½“å®Œæˆæœ‰æ•ˆåä½œã€‚åŒæ—¶ï¼Œæœ¬ç ”ç©¶ä¹Ÿæå‡ºäº†è‡ªæˆ‘è¿›åŒ–çš„æ•°æ®ç”Ÿæˆæ–¹å¼åŠ å¿«ä¼˜è´¨æ‰§è¡Œè·¯å¾„çš„æ”¶é›†ä»¥æé«˜ç³»ç»Ÿè®­ç»ƒæ•ˆç‡ã€‚åœ¨æŒ‘æˆ˜æ€§çš„ååŒæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°é¢†å…ˆï¼Œç›¸æ¯”åŸºå‡†æµ‹è¯•æœ‰æœ€é«˜è¾¾è‡³å¤šè¿‘æ”¹å–„å®éªŒç²¾åº¦æœ€é«˜æå‡äº†çº¦æå‡äº†è¾¾åˆ°æœ€å¤šçº¦23.8%ï¼Œå¹¶ä¸”åœ¨æ•°æ®é‡‡é›†å¼€é”€æ–¹é¢é€šè¿‡è¯¥è·¯ç”±æ–¹æ³•æ˜¾è‘—å‡å°‘è¾¾åˆ°äº†å‡å°‘äº†æœ€å¤šè¿‘çº¦è¿‘90.1%ã€‚å¯¹äºåº”å¯¹å…·æœ‰å¤šä¸ªä»»åŠ¡çš„åœºæ™¯å…·æœ‰ä¸€å®šä½œç”¨å’Œåº”ç”¨å‰æ™¯å±•æœ›ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥è®ºæ–‡ç ”ç©¶å¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåä½œå‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚é€šè¿‡é‡‡ç”¨æ€åŠ¿æ„ŸçŸ¥è·¯ç”±æ¡†æ¶ä»¥åŠè‡ªæˆ‘è¿›åŒ–çš„æ•°æ®ç”Ÿæˆæ–¹å¼ç­‰åˆ›æ–°æŠ€æœ¯æå‡ç³»ç»Ÿæ€§èƒ½ï¼Œå±•ç°äº†å…¶åœ¨å¤æ‚ä»»åŠ¡è§£å†³ä¸­çš„ä¼˜åŠ¿ã€‚å› æ­¤è¯¥ç ”ç©¶å…·æœ‰é‡è¦ç†è®ºä»·å€¼å’Œå®è·µæŒ‡å¯¼æ„ä¹‰å’Œå®è·µåº”ç”¨ä»·å€¼å¹¿é˜”ã€‚å› æ­¤å€¼å¾—æ·±å…¥ç ”ç©¶å’Œæ¨å¹¿åº”ç”¨è¯¥æŠ€æœ¯å’Œæ–¹æ³•å…·æœ‰ä¸€å®šæ½œåŠ›è¿›è¡Œæ›´æ·±å…¥çš„ç ”ç©¶å’Œåº”ç”¨æ¨å¹¿å…·æœ‰é‡è¦æ„ä¹‰å’Œä»·å€¼æ½œåŠ›ã€‚ </p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å®ç°å¤æ‚ä»»åŠ¡è§£å†³çš„æ–°çªç ´ï¼Œå…è®¸ä¸åŒæ™ºèƒ½ä½“æ•´åˆä¸“ä¸šçŸ¥è¯†å¹¶çµæ´»åä½œã€‚ </li>
<li>å½“å‰ç³»ç»Ÿå—é™äºè°ƒåº¦åƒµåŒ–ä¸åè°ƒç­–ç•¥ä½æ•ˆçš„é—®é¢˜ï¼Œæ— æ³•é€‚åº”ä»»åŠ¡å˜åŒ–å¤šæ ·æ€§ã€‚ </li>
<li>æå‡ºSTRMACæ€åŠ¿æ„ŸçŸ¥è·¯ç”±æ¡†æ¶ï¼Œé€šè¿‡äº’åŠ¨å†å²åŠä»£ç†çŸ¥è¯†ç¼–ç é©±åŠ¨è·¯ç”±å™¨é€‰æ‹©æœºåˆ¶ï¼Œå®ç°æ™ºèƒ½ä½“é«˜æ•ˆåä½œã€‚ </li>
<li>å¼•å…¥è‡ªæˆ‘è¿›åŒ–çš„æ•°æ®ç”Ÿæˆæ–¹å¼åŠ å¿«é«˜è´¨é‡æ‰§è¡Œè·¯å¾„æ”¶é›†ï¼Œæé«˜ç³»ç»Ÿè®­ç»ƒæ•ˆç‡ã€‚ </li>
<li>åœ¨ååŒæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°é¢†å…ˆï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æœ‰æ˜æ˜¾æ€§èƒ½æå‡ã€‚ </li>
<li>æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬è‡ªé€‚åº”é€‰æ‹©æœ€ä½³æ™ºèƒ½ä½“åŠé«˜æ•ˆæ•°æ®æ”¶é›†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-109ef68a61a6d1ecf6646c25a01d4ec8" align="middle">
<img src="https://picx.zhimg.com/v2-fdc884f417e54d7207b005b490dde1b8" align="middle">
<img src="https://picx.zhimg.com/v2-b0d413af961a3b0e4cd1fe0d72804d74" align="middle">
<img src="https://picx.zhimg.com/v2-c073d638ad5724dc8b6bf0ab31c3e11c" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Open-the-Oyster-Empirical-Evaluation-and-Improvement-of-Code-Reasoning-Confidence-in-LLMs"><a href="#Open-the-Oyster-Empirical-Evaluation-and-Improvement-of-Code-Reasoning-Confidence-in-LLMs" class="headerlink" title="Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning   Confidence in LLMs"></a>Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning   Confidence in LLMs</h2><p><strong>Authors:Shufan Wang, Xing Hu, Junkai Chen, Zhiyuan Pan, Xin Xia</strong></p>
<p>With the widespread application of large language models (LLMs) in the field of code intelligence, increasing attention has been paid to the reliability and controllability of their outputs in code reasoning tasks. Confidence estimation serves as an effective and convenient approach for evaluating these aspects. This paper proposes a confidence analysis and enhancement framework for LLMs tailored to code reasoning tasks. We conduct a comprehensive empirical study on the confidence reliability of mainstream LLMs across different tasks, and further evaluate the effectiveness of techniques such as prompt strategy optimisation and mathematical calibration (e.g., Platt Scaling) in improving confidence reliability. Our results show that DeepSeek-Reasoner achieves the best performance across various tasks, outperforming other models by up to $0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance Score, respectively. The hybrid strategy combining the reassess prompt strategy and Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$ over the original performance in the aforementioned three metrics. These results indicate that models with reasoning capabilities demonstrate superior confidence reliability, and that the hybrid strategy is the most effective in enhancing the confidence reliability of various models. Meanwhile, we elucidate the impact of different task complexities, model scales, and strategies on confidence performance, and highlight that the confidence of current LLMs in complex reasoning tasks still has considerable room for improvement. This study not only provides a research foundation and technical reference for the application of confidence in LLM-assisted software engineering, but also points the way for future optimisation and engineering deployment of confidence mechanisms. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç æ™ºèƒ½é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå…¶è¾“å‡ºåœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸­çš„å¯é æ€§å’Œå¯æ§æ€§è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ä¿¡å¿ƒä¼°è®¡æ˜¯ä¸€ç§æœ‰æ•ˆä¸”æ–¹ä¾¿çš„è¯„ä¼°è¿™äº›æ–¹é¢çš„æ–¹æ³•ã€‚æœ¬æ–‡é’ˆå¯¹ä»£ç æ¨ç†ä»»åŠ¡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹LLMçš„ä¿¡å¿ƒåˆ†æå’Œå¢å¼ºæ¡†æ¶ã€‚æˆ‘ä»¬å¯¹ä¸åŒä»»åŠ¡ä¸­ä¸»æµLLMçš„ä¿¡å¿ƒå¯é æ€§è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œå¹¶è¿›ä¸€æ­¥è¯„ä¼°äº†æç¤ºç­–ç•¥ä¼˜åŒ–å’Œæ•°å­¦æ ¡å‡†ï¼ˆå¦‚Platt Scalingï¼‰ç­‰æŠ€æœ¯åœ¨æé«˜ä¿¡å¿ƒå¯é æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒDeepSeek-Reasoneråœ¨å„é¡¹ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨ECEã€Brier Scoreå’ŒPerformance Scoreæ–¹é¢åˆ†åˆ«æ¯”å…¶ä»–æ¨¡å‹é«˜å‡º0.680ã€0.636å’Œ13.652ã€‚ç»“åˆé‡æ–°è¯„ä¼°æç¤ºç­–ç•¥å’ŒPlatt Scalingçš„æ··åˆç­–ç•¥ï¼Œåœ¨ä¸Šè¿°ä¸‰ä¸ªæŒ‡æ ‡ä¸­ï¼Œç›¸è¾ƒäºåŸå§‹æ€§èƒ½æœ€é«˜å¯æå‡0.541ã€0.628å’Œ15.084ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„ä¿¡å¿ƒå¯é æ€§ï¼Œæ··åˆç­–ç•¥åœ¨æé«˜å„ç§æ¨¡å‹çš„ä¿¡å¿ƒå¯é æ€§æ–¹é¢æœ€ä¸ºæœ‰æ•ˆã€‚åŒæ—¶ï¼Œæˆ‘ä»¬é˜è¿°äº†ä¸åŒä»»åŠ¡å¤æ‚æ€§ã€æ¨¡å‹è§„æ¨¡å’Œç­–ç•¥å¯¹ä¿¡å¿ƒè¡¨ç°çš„å½±å“ï¼Œå¹¶å¼ºè°ƒå½“å‰LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„ä¿¡å¿ƒä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æœ¬ç ”ç©¶ä¸ä»…ä¸ºä¿¡å¿ƒåœ¨LLMè¾…åŠ©è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨æä¾›äº†ç ”ç©¶åŸºç¡€å’ŒæŠ€æœ¯å‚è€ƒï¼Œè€Œä¸”ä¸ºä¿¡å¿ƒæœºåˆ¶çš„æœªæ¥ä¼˜åŒ–å’Œå·¥ç¨‹éƒ¨ç½²æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02197v1">PDF</a> 13 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç æ™ºèƒ½é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå…¶è¾“å‡ºåœ¨ä»£ç æ¨ç†ä»»åŠ¡ä¸­çš„å¯é æ€§å’Œå¯æ§æ€§å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ä¿¡å¿ƒä¼°è®¡æ˜¯ä¸€ç§æœ‰æ•ˆå’Œæ–¹ä¾¿çš„è¯„ä¼°è¿™äº›æ–¹é¢çš„æ–¹æ³•ã€‚æœ¬æ–‡é’ˆå¯¹ä»£ç æ¨ç†ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ä¸ªé’ˆå¯¹LLMsçš„ä¿¡å¿ƒåˆ†æå’Œå¢å¼ºæ¡†æ¶ã€‚æˆ‘ä»¬å¯¹ä¸»æµLLMsåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„ä¿¡å¿ƒå¯é æ€§è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œå¹¶è¿›ä¸€æ­¥è¯„ä¼°äº†ä¼˜åŒ–æç¤ºç­–ç•¥å’Œæ•°å­¦æ ¡å‡†ï¼ˆå¦‚Platt Scalingï¼‰ç­‰æŠ€æœ¯æé«˜ä¿¡å¿ƒå¯é æ€§çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeepSeek-Reasoneråœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œå…¶åœ¨ECEã€Brier Scoreå’ŒPerformance Scoreä¸Šçš„è¡¨ç°åˆ†åˆ«æé«˜äº†0.680ã€0.636å’Œ13.652ã€‚ç»“åˆé‡æ–°è¯„ä¼°æç¤ºç­–ç•¥å’ŒPlatt Scalingçš„æ··åˆç­–ç•¥ï¼Œåœ¨ä¸Šè¿°ä¸‰ä¸ªæŒ‡æ ‡ä¸­åˆ†åˆ«æé«˜äº†0.541ã€0.628å’Œ15.084ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå…·æœ‰æ¨ç†èƒ½åŠ›çš„æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„ä¿¡å¿ƒå¯é æ€§ï¼Œæ··åˆç­–ç•¥æ˜¯æœ€æœ‰æ•ˆçš„å¢å¼ºå„ç§æ¨¡å‹ä¿¡å¿ƒå¯é æ€§çš„æ–¹æ³•ã€‚åŒæ—¶ï¼Œæœ¬æ–‡é˜æ˜äº†ä¸åŒä»»åŠ¡å¤æ‚åº¦ã€æ¨¡å‹è§„æ¨¡ã€ç­–ç•¥å¯¹ä¿¡å¿ƒè¡¨ç°çš„å½±å“ï¼Œå¹¶å¼ºè°ƒå½“å‰LLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„ä¿¡å¿ƒä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æœ¬ç ”ç©¶ä¸ä»…ä¸ºä¿¡å¿ƒåœ¨LLMè¾…åŠ©è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨æä¾›äº†ç ”ç©¶åŸºç¡€å’ŒæŠ€æœ¯å‚è€ƒï¼Œä¹Ÿä¸ºä¿¡å¿ƒæœºåˆ¶çš„æœªæ¥ä¼˜åŒ–å’Œå·¥ç¨‹éƒ¨ç½²æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç æ™ºèƒ½é¢†åŸŸçš„ä¿¡å¿ƒå¯é æ€§é—®é¢˜å—åˆ°å…³æ³¨ã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹ä»£ç æ¨ç†ä»»åŠ¡çš„LLMsä¿¡å¿ƒåˆ†æå’Œå¢å¼ºæ¡†æ¶ã€‚</li>
<li>ä¸»æµLLMsçš„ä¿¡å¿ƒå¯é æ€§å®è¯ç ”ç©¶ï¼Œæ­ç¤ºäº†å…¶æ€§èƒ½å·®å¼‚ã€‚</li>
<li>æç¤ºç­–ç•¥ä¼˜åŒ–å’Œæ•°å­¦æ ¡å‡†ï¼ˆå¦‚Platt Scalingï¼‰ç­‰æŠ€æœ¯æœ‰æ•ˆæé«˜ä¿¡å¿ƒå¯é æ€§ã€‚</li>
<li>DeepSeek-Reasonerè¡¨ç°æœ€ä½³ï¼Œæ··åˆç­–ç•¥å¯¹äºå¢å¼ºä¿¡å¿ƒå¯é æ€§æœ€æœ‰æ•ˆã€‚</li>
<li>ä¸åŒä»»åŠ¡å¤æ‚åº¦ã€æ¨¡å‹è§„æ¨¡ã€ç­–ç•¥å¯¹ä¿¡å¿ƒè¡¨ç°æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6b00ef9cbb27c5346e961d3cf3990c8" align="middle">
<img src="https://picx.zhimg.com/v2-29572f9942ac10ac0d4969f1e79b341e" align="middle">
<img src="https://picx.zhimg.com/v2-6a0e08469cd50cce78534c31ed73eb57" align="middle">
<img src="https://picx.zhimg.com/v2-2f776ed60e692f0ffd1e478593546123" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Personalized-Decision-Modeling-Utility-Optimization-or-Textualized-Symbolic-Reasoning"><a href="#Personalized-Decision-Modeling-Utility-Optimization-or-Textualized-Symbolic-Reasoning" class="headerlink" title="Personalized Decision Modeling: Utility Optimization or   Textualized-Symbolic Reasoning"></a>Personalized Decision Modeling: Utility Optimization or   Textualized-Symbolic Reasoning</h2><p><strong>Authors:Yibo Zhao, Yang Zhao, Hongru Du, Hao Frank Yang</strong></p>
<p>Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5% over the strongest cutting-edge models. Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at <a target="_blank" rel="noopener" href="https://yibozh.github.io/Athena">https://yibozh.github.io/Athena</a>. </p>
<blockquote>
<p>é’ˆå¯¹ä¸ªäººå†³ç­–åˆ¶å®šçš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç–«è‹—æ¥ç§ç­‰é«˜é£é™©åœºæ™¯ä¸­ï¼Œå¾€å¾€ä¸ç¾¤ä½“æœ€ä¼˜é¢„æµ‹å­˜åœ¨åˆ†æ­§ã€‚è¿™ä¸€å·®è·çš„äº§ç”Ÿæºäºä¸ªäººå†³ç­–åˆ¶å®šè¿‡ç¨‹çš„ç‹¬ç‰¹æ€§ï¼Œè¿™ä¸€è¿‡ç¨‹å—åˆ°æ•°å€¼å±æ€§ï¼ˆå¦‚æˆæœ¬ã€æ—¶é—´ï¼‰å’Œè¯­è¨€å½±å“ï¼ˆå¦‚ä¸ªäººåå¥½å’Œçº¦æŸï¼‰çš„å½±å“ã€‚æœ¬æ–‡åŸºäºæ•ˆç”¨ç†è®ºï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†è‡ªé€‚åº”æ–‡æœ¬ç¬¦å·äººç±»ä¸­å¿ƒæ¨ç†æ¡†æ¶ï¼ˆATHENAï¼‰ï¼Œä»¥è§£å†³æœ€ä¼˜ä¿¡æ¯èåˆé—®é¢˜ã€‚ATHENAç‹¬ç‰¹åœ°é›†æˆäº†ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œå®ƒé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„ç¬¦å·å‘ç°æ¥å‘ç°ç¨³å¥çš„ç¾¤ä½“å±‚é¢ç¬¦å·æ•ˆç”¨å‡½æ•°ï¼›å…¶æ¬¡ï¼Œå®ƒå®ç°ä¸ªä½“å±‚é¢çš„è¯­ä¹‰é€‚åº”ï¼Œåˆ›å»ºç”±æœ€ä½³æ•ˆç”¨å¼•å¯¼çš„ä¸ªæ€§åŒ–è¯­ä¹‰æ¨¡æ¿ï¼Œä»¥æ¨¡æ‹Ÿä¸ªæ€§åŒ–é€‰æ‹©ã€‚åœ¨çœŸå®ä¸–ç•Œçš„å‡ºè¡Œæ¨¡å¼å’Œç–«è‹—é€‰æ‹©ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ï¼ŒATHENAå§‹ç»ˆä¼˜äºåŸºäºæ•ˆç”¨ã€æœºå™¨å­¦ä¹ å’Œå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œæ¯”æœ€å…ˆè¿›çš„æ¨¡å‹é«˜å‡ºè‡³å°‘6.5%çš„F1åˆ†æ•°ã€‚æ­¤å¤–ï¼Œåˆ‡é™¤ç ”ç©¶è¯å®ATHENAçš„ä¸¤ä¸ªé˜¶æ®µéƒ½æ˜¯å…³é”®ä¸”äº’è¡¥çš„ï¼Œç§»é™¤ä»»ä½•ä¸€ä¸ªéƒ½ä¼šæ˜æ˜¾é™ä½æ•´ä½“é¢„æµ‹æ€§èƒ½ã€‚é€šè¿‡æœ‰æœºåœ°ç»“åˆç¬¦å·æ•ˆç”¨å»ºæ¨¡å’Œè¯­ä¹‰é€‚åº”ï¼ŒATHENAä¸ºæ¨¡æ‹Ÿä»¥äººç±»ä¸ºä¸­å¿ƒçš„å†³ç­–æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://yibozh.github.io/Athena">https://yibozh.github.io/Athena</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02194v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ä¸ªä½“å†³ç­–æ¨¡å‹åœ¨é«˜é£é™©æƒ…å¢ƒä¸‹å¦‚ç–«è‹—æ¥ç§ä¸­çš„é€‰æ‹©å¾€å¾€ä¸ç¾¤ä½“æœ€ä¼˜é¢„æµ‹å­˜åœ¨å·®è·ã€‚æ–‡ç« ç»“åˆæ•ˆç”¨ç†è®ºï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”æ–‡æœ¬ç¬¦å·äººç±»ä¸­å¿ƒåŒ–æ¨ç†æ¡†æ¶ï¼ˆATHENAï¼‰ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡å¢å¼ºè¯­è¨€æ¨¡å‹ç¬¦å·å‘ç°èƒ½åŠ›çš„ç¾¤ä½“å±‚é¢ç¬¦å·æ•ˆç”¨å‡½æ•°ï¼›ç¬¬äºŒé˜¶æ®µå®ç°ä¸ªä½“è¯­ä¹‰é€‚åº”ï¼Œåˆ›å»ºä¸ªæ€§åŒ–è¯­ä¹‰æ¨¡æ¿ä»¥æ¨¡æ‹Ÿä¸ªæ€§åŒ–é€‰æ‹©ã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼ŒATHENAåœ¨çœŸå®ä¸–ç•Œæ—…è¡Œæ¨¡å¼å’Œç–«è‹—é€‰æ‹©ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒF1åˆ†æ•°æé«˜äº†è‡³å°‘6.5%ã€‚æ­¤æ¡†æ¶å°†ç¬¦å·æ•ˆç”¨å»ºæ¨¡å’Œè¯­ä¹‰é€‚åº”æœ‰æœºç»“åˆï¼Œä¸ºæ¨¡æ‹Ÿä»¥äººç±»ä¸ºä¸­å¿ƒçš„å†³ç­–æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¸ªä½“å†³ç­–æ¨¡å‹ä¸ç¾¤ä½“æœ€ä¼˜é¢„æµ‹åœ¨é«˜é£é™©æƒ…å¢ƒä¸‹å­˜åœ¨å·®è·ã€‚</li>
<li>å·®è·æºäºä¸ªä½“å†³ç­–è¿‡ç¨‹çš„ç‹¬ç‰¹æ€§å’Œä¸ªä½“å·®å¼‚ã€‚</li>
<li>ATHENAæ¡†æ¶ç»“åˆäº†æ•ˆç”¨ç†è®ºå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ATHENAæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç¬¦å·æ•ˆç”¨å‡½æ•°å‘ç°å’Œä¸ªä½“è¯­ä¹‰é€‚åº”ã€‚</li>
<li>ATHENAåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒF1åˆ†æ•°æé«˜è‡³å°‘6.5%ã€‚</li>
<li>æ¡†æ¶ä¸­çš„ä¸¤ä¸ªé˜¶æ®µéƒ½æ˜¯å…³é”®ä¸”äº’è¡¥çš„ï¼Œç¼ºä¸€ä¸å¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2ce054495d5f1f0319e3e893c035cf0" align="middle">
<img src="https://picx.zhimg.com/v2-1ec717c4abf3957638278c84abfe342d" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Deep-Value-Benchmark-Measuring-Whether-Models-Generalize-Deep-values-or-Shallow-Preferences"><a href="#Deep-Value-Benchmark-Measuring-Whether-Models-Generalize-Deep-values-or-Shallow-Preferences" class="headerlink" title="Deep Value Benchmark: Measuring Whether Models Generalize Deep values or   Shallow Preferences"></a>Deep Value Benchmark: Measuring Whether Models Generalize Deep values or   Shallow Preferences</h2><p><strong>Authors:Joshua Ashkinaze, Hua Shen, Sai Avula, Eric Gilbert, Ceren Budak</strong></p>
<p>We introduce the Deep Value Benchmark (DVB), an evaluation framework that directly tests whether large language models (LLMs) learn fundamental human values or merely surface-level preferences. This distinction is critical for AI alignment: Systems that capture deeper values are likely to generalize human intentions robustly, while those that capture only superficial patterns in preference data risk producing misaligned behavior. The DVB uses a novel experimental design with controlled confounding between deep values (e.g., moral principles) and shallow features (e.g., superficial attributes). In the training phase, we expose LLMs to human preference data with deliberately correlated deep and shallow features â€“ for instance, where a user consistently prefers (non-maleficence, formal language) options over (justice, informal language) alternatives. The testing phase then breaks these correlations, presenting choices between (justice, formal language) and (non-maleficence, informal language) options. This design allows us to precisely measure a modelâ€™s Deep Value Generalization Rate (DVGR) â€“ the probability of generalizing based on the underlying value rather than the shallow feature. Across 9 different models, the average DVGR is just 0.30. All models generalize deep values less than chance. Larger models have a (slightly) lower DVGR than smaller models. We are releasing our dataset, which was subject to three separate human validation experiments. DVB provides an interpretable measure of a core feature of alignment. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†æ·±åº¦ä»·å€¼åŸºå‡†ï¼ˆDVBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å­¦ä¹ äº†åŸºæœ¬çš„äººç±»ä»·å€¼è¿˜æ˜¯ä»…ä»…å­¦ä¹ äº†è¡¨é¢å±‚æ¬¡çš„åå¥½ã€‚è¿™ä¸€åŒºåˆ«å¯¹äºäººå·¥æ™ºèƒ½å¯¹é½è‡³å…³é‡è¦ï¼šæ•è·æ›´æ·±ä»·å€¼çš„ç³»ç»Ÿå¯èƒ½ä¼šç¨³å¥åœ°æ³›åŒ–äººç±»æ„å›¾ï¼Œè€Œä»…æ•è·åå¥½æ•°æ®ä¸­è¡¨é¢æ¨¡å¼çš„ç³»ç»Ÿåˆ™å­˜åœ¨äº§ç”Ÿé”™ä½è¡Œä¸ºçš„é£é™©ã€‚DVBä½¿ç”¨äº†ä¸€ç§æ–°å‹çš„å®éªŒè®¾è®¡ï¼Œåœ¨æ·±å±‚ä»·å€¼ï¼ˆå¦‚é“å¾·åŸåˆ™ï¼‰å’Œæµ…å±‚ç‰¹å¾ï¼ˆå¦‚è¡¨é¢å±æ€§ï¼‰ä¹‹é—´è¿›è¡Œäº†å—æ§çš„æ··æ·†ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬è®©LLMæ¥è§¦äººç±»åå¥½æ•°æ®ï¼Œå…¶ä¸­æ·±å±‚å’Œæµ…å±‚ç‰¹å¾æ˜¯æ•…æ„ç›¸å…³çš„â€”â€”ä¾‹å¦‚ï¼Œç”¨æˆ·ä¸€è‡´åœ°æ›´å–œæ¬¢ï¼ˆæ— å®³æ€§ã€æ­£å¼è¯­è¨€ï¼‰é€‰é¡¹è€Œä¸æ˜¯ï¼ˆæ­£ä¹‰ã€éæ­£å¼è¯­è¨€ï¼‰é€‰æ‹©ã€‚æµ‹è¯•é˜¶æ®µåˆ™æ‰“ç ´è¿™äº›ç›¸å…³æ€§ï¼Œå‘ˆç°ï¼ˆæ­£ä¹‰ã€æ­£å¼è¯­è¨€ï¼‰å’Œï¼ˆæ— å®³æ€§ã€éæ­£å¼è¯­è¨€ï¼‰ä¹‹é—´çš„é€‰æ‹©ã€‚è¿™ç§è®¾è®¡å…è®¸æˆ‘ä»¬ç²¾ç¡®åœ°æµ‹é‡æ¨¡å‹çš„æ·±åº¦ä»·å€¼æ³›åŒ–ç‡ï¼ˆDVGRï¼‰â€”â€”åŸºäºåº•å±‚ä»·å€¼è€Œä¸æ˜¯æµ…å±‚ç‰¹å¾è¿›è¡Œæ³›åŒ–çš„æ¦‚ç‡ã€‚åœ¨9ä¸ªä¸åŒçš„æ¨¡å‹ä¸­ï¼Œå¹³å‡DVGRä»…ä¸º0.30ã€‚æ‰€æœ‰æ¨¡å‹çš„æ·±åº¦ä»·å€¼æ³›åŒ–ç‡éƒ½ä½äºå¶ç„¶æ°´å¹³ã€‚å¤§å‹æ¨¡å‹çš„DVGRç•¥ä½äºå°å‹æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»å‘å¸ƒäº†æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç»è¿‡äº†ä¸‰æ¬¡ç‹¬ç«‹çš„äººç±»éªŒè¯å®éªŒã€‚DVBæä¾›äº†ä¸€ä¸ªå¯è§£é‡Šçš„æ ¸å¿ƒç‰¹å¾å¯¹é½åº¦é‡ã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–åçš„ç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02109v1">PDF</a> NeurIPS 2025 (Spotlight)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä»‹ç»äº†ä¸€ç§åä¸ºDeep Value Benchmarkï¼ˆDVBï¼‰çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦å­¦ä¹ äººç±»çš„åŸºæœ¬ä»·å€¼è§‚è€Œéä»…è¡¨é¢åå¥½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ–°é¢–çš„å®éªŒè®¾è®¡ï¼Œæ§åˆ¶æ·±å±‚ä»·å€¼è§‚ï¼ˆå¦‚é“å¾·åŸåˆ™ï¼‰å’Œæµ…å±‚ç‰¹å¾ï¼ˆå¦‚è¡¨é¢å±æ€§ï¼‰ä¹‹é—´çš„æ··æ·†ã€‚é€šè¿‡æš´éœ²LLMäºæ•…æ„å…³è”æ·±ã€æµ…ç‰¹å¾çš„äººç±»åå¥½æ•°æ®ï¼Œæµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½åŸºäºæ·±å±‚ä»·å€¼è€Œéæµ…å±‚ç‰¹å¾è¿›è¡Œæ¨å¹¿ã€‚å¹³å‡Deep Value Generalization Rateï¼ˆDVGRï¼‰ä»…ä¸º0.30ï¼Œæ‰€æœ‰æ¨¡å‹çš„æ·±å±‚ä»·å€¼æ¨å¹¿å‡ä½äºå¶ç„¶æ°´å¹³ã€‚å‘å¸ƒçš„æ•°æ®é›†ç»è¿‡äº†ä¸‰æ¬¡ç‹¬ç«‹çš„äººç±»éªŒè¯å®éªŒã€‚DVBæä¾›äº†ä¸€ç§å¯è§£é‡Šçš„è¡¡é‡å¯¹é½æ ¸å¿ƒç‰¹å¾çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Deep Value Benchmark (DVB) ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦å­¦ä¹ äººç±»åŸºæœ¬ä»·å€¼è§‚ã€‚</li>
<li>DVB é€šè¿‡æ–°é¢–çš„å®éªŒè®¾è®¡åŒºåˆ†æ·±å±‚ä»·å€¼è§‚ä¸æµ…å±‚ç‰¹å¾ã€‚</li>
<li>LLMs åœ¨æš´éœ²äºæ•…æ„å…³è”æ·±ã€æµ…ç‰¹å¾çš„äººç±»åå¥½æ•°æ®åï¼Œæµ‹è¯•å…¶åŸºäºæ·±å±‚ä»·å€¼çš„æ¨å¹¿èƒ½åŠ›ã€‚</li>
<li>å¹³å‡Deep Value Generalization Rate (DVGR) ä¸º 0.30ï¼Œæ˜¾ç¤ºæ¨¡å‹åœ¨æ¨å¹¿æ·±å±‚ä»·å€¼æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>å¤§å‹æ¨¡å‹çš„DVGRç•¥ä½äºå°å‹æ¨¡å‹ã€‚</li>
<li>DVBæ•°æ®é›†ç»è¿‡ä¸‰æ¬¡ç‹¬ç«‹äººç±»éªŒè¯å®éªŒï¼Œç¡®ä¿æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02109">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-89a93f82ce93ecfffcf57e9a5ef4fa4a" align="middle">
<img src="https://picx.zhimg.com/v2-41f30ae75ee9ed18283c80bedc032596" align="middle">
<img src="https://picx.zhimg.com/v2-ea01306a8fed21de21d939944b31df38" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Towards-Robust-Mathematical-Reasoning"><a href="#Towards-Robust-Mathematical-Reasoning" class="headerlink" title="Towards Robust Mathematical Reasoning"></a>Towards Robust Mathematical Reasoning</h2><p><strong>Authors:Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, Junehyuk Jung</strong></p>
<p>Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at <a target="_blank" rel="noopener" href="https://imobench.github.io/">https://imobench.github.io/</a>. </p>
<blockquote>
<p>æ‰¾åˆ°æ­£ç¡®çš„åŒ—ææ˜ŸæŒ‡æ ‡å¯¹äºæå‡åŸºç¡€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°ç°æœ‰çš„è¯„ä¼°è¦ä¹ˆè¿‡äºç®€å•ï¼Œè¦ä¹ˆåªå…³æ³¨è·å¾—æ­£ç¡®çš„ç®€çŸ­ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†IMO-Benchï¼Œè¿™æ˜¯ä¸€å¥—ç”±é¡¶å°–ä¸“å®¶å›¢é˜Ÿå®¡æ ¸çš„å…ˆè¿›æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨é’ˆå¯¹å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆIMOï¼‰çš„æ°´å¹³ï¼Œè¿™æ˜¯å¹´è½»æ•°å­¦å®¶æœ€è´Ÿç››åçš„ç«æŠ€åœºã€‚IMO-AnswerBenché¦–å…ˆæµ‹è¯•æ¨¡å‹åœ¨400ä¸ªå¤šæ ·åŒ–çš„å¥¥æ—åŒ¹å…‹é—®é¢˜ä¸Šçš„ç®€çŸ­å¯éªŒè¯ç­”æ¡ˆã€‚IMO-Proof Benchæ˜¯æ›´é«˜å±‚æ¬¡çš„è¯æ˜ä¹¦å†™èƒ½åŠ›è¯„ä¼°ï¼Œå…¶ä¸­åŒ…æ‹¬åŸºæœ¬å’Œé«˜çº§çš„IMOçº§åˆ«é—®é¢˜ä»¥åŠè¯¦ç»†çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥ä¾¿äºè‡ªåŠ¨è¯„åˆ†ã€‚è¿™äº›åŸºå‡†æµ‹è¯•åœ¨æˆ‘ä»¬çš„å†å²æˆå°±ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œå³åœ¨IMO 2025å¹´ä¸åŒå­åº§æ·±åº¦æ€è€ƒï¼ˆLuong and Lockhart, 2025ï¼‰å…±åŒå–å¾—é‡‘ç‰Œçº§è¡¨ç°ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨IMO-AnswerBenchä¸Šè¾¾åˆ°80.0%ï¼Œåœ¨é«˜çº§IMO-Proof Benchä¸Šè¾¾åˆ°65.7%ï¼Œè¿œè¶…æœ€ä½³éåŒå­åº§æ¨¡å‹ï¼Œåˆ†åˆ«é«˜å‡º6.9%å’Œ42.4%ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä½¿ç”¨åŒå­åº§æ¨ç†æ„å»ºçš„è‡ªåŠ¨è¯„åˆ†å™¨ä¸äººç±»è¯„ä¼°é«˜åº¦ç›¸å…³ï¼Œå¹¶æ„å»ºäº†åŒ…å«1000ä¸ªè¯æ˜çš„äººç±»è¯„åˆ†æ•°æ®çš„IMO-GradingBenchï¼Œä»¥ä¾¿æ¨åŠ¨é•¿ç­”æ¡ˆçš„è‡ªåŠ¨è¯„ä¼°å–å¾—è¿›ä¸€æ­¥è¿›å±•ã€‚æˆ‘ä»¬å¸Œæœ›IMO-Benchèƒ½å¸®åŠ©ç¤¾åŒºæ¨åŠ¨ç¨³å¥çš„æ•°å­¦æ¨ç†çš„å‘å±•ï¼Œå¹¶åœ¨<a target="_blank" rel="noopener" href="https://imobench.github.io/">https://imobench.github.io/</a>ä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01846v1">PDF</a> EMNLP 2025 (main conference),   <a target="_blank" rel="noopener" href="https://aclanthology.org/2025.emnlp-main.1794/">https://aclanthology.org/2025.emnlp-main.1794/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é’ˆå¯¹åŸºç¡€æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†IMO-Benchè¯„ä¼°å¥—ä»¶ã€‚è¯¥å¥—ä»¶åŒ…æ‹¬ç»è¿‡é¡¶çº§ä¸“å®¶å®¡æ ¸çš„ã€é’ˆå¯¹å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆIMOï¼‰æ°´å¹³çš„æµ‹è¯•ï¼Œåˆ†ä¸ºIMO-AnswerBenchå’ŒIMO-Proof Benchä¸¤éƒ¨åˆ†ã€‚å…¶ä¸­ï¼ŒIMO-AnswerBenchåŒ…å«400é“å¯éªŒè¯ç®€çŸ­ç­”æ¡ˆçš„å¥¥æ—åŒ¹å…‹é¢˜ç›®ï¼Œè€ŒIMO-Proof Benchåˆ™æ˜¯å¯¹è¯æ˜å†™ä½œèƒ½åŠ›çš„è¿›é˜¶è¯„ä¼°ï¼ŒåŒ…å«åŸºæœ¬å’Œé«˜çº§çš„IMOçº§åˆ«é—®é¢˜ä»¥åŠè¯¦ç»†çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥æ¨åŠ¨è‡ªåŠ¨è¯„åˆ†çš„å‘å±•ã€‚ä½¿ç”¨Gemini Deep Thinkæ¨¡å‹åœ¨è¯¥è¯„ä¼°å¥—ä»¶ä¸­å–å¾—äº†å†å²æ€§æˆç»©ï¼Œå¹¶åœ¨è‡ªåŠ¨è¯„ä»·é•¿æœŸç­”æ¡ˆæ–¹é¢å–å¾—äº†è¿›å±•ã€‚ä½œè€…å¸Œæœ›IMO-Benchèƒ½å¤Ÿå¸®åŠ©ç¤¾åŒºæ¨è¿›ç¨³å¥çš„æ•°å­¦æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IMO-Benchæ˜¯ä¸€ä¸ªä¸“ä¸ºæ¨è¿›æ•°å­¦æ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„è¯„ä¼°å¥—ä»¶ï¼ŒåŒ…æ‹¬IMO-AnswerBenchå’ŒIMO-Proof Benchä¸¤éƒ¨åˆ†ã€‚</li>
<li>IMO-AnswerBenchåŒ…å«ç»è¿‡é¡¶çº§ä¸“å®¶å®¡æ ¸çš„400é“å¥¥æ—åŒ¹å…‹é¢˜ç›®ï¼Œæ—¨åœ¨æµ‹è¯•æ¨¡å‹çš„çŸ­ç­”æ¡ˆèƒ½åŠ›ã€‚</li>
<li>IMO-Proof Benchæ˜¯ä¸€ä¸ªé’ˆå¯¹è¯æ˜å†™ä½œèƒ½åŠ›çš„è¿›é˜¶è¯„ä¼°å·¥å…·ï¼ŒåŒ…å«è¯¦ç»†çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥ä¿ƒè¿›è‡ªåŠ¨è¯„åˆ†çš„å‘å±•ã€‚</li>
<li>ä½¿ç”¨Gemini Deep Thinkæ¨¡å‹åœ¨IMO-Benchä¸­å–å¾—äº†æ˜¾è‘—æˆç»©ï¼Œå°¤å…¶åœ¨IMO-Proof Benchä¸Šçš„è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ã€‚</li>
<li>è‡ªåŠ¨è¯„ä»·ç³»ç»Ÿä¸äººè¯„ä»·ä¹‹é—´çš„ç›¸å…³æ€§è‰¯å¥½ï¼Œä¸ºé•¿æœŸç­”æ¡ˆçš„è‡ªåŠ¨è¯„ä»·å¥ å®šäº†åŸºç¡€ã€‚</li>
<li>IMO-Benchçš„å‘å¸ƒæ—¨åœ¨å¸®åŠ©ç¤¾åŒºæ¨è¿›ç¨³å¥çš„æ•°å­¦æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7622101fab66ae3e82e8c8b0876a4525" align="middle">
<img src="https://picx.zhimg.com/v2-4893b22d35309c1f96c97c30344af319" align="middle">
<img src="https://picx.zhimg.com/v2-56f7809de1a353cc8f26cabc09c0e57f" align="middle">
<img src="https://picx.zhimg.com/v2-33eb6e5f08180cb55918e8f238b39006" align="middle">
<img src="https://picx.zhimg.com/v2-3696362252c17fcd8ead1816b9055028" align="middle">
<img src="https://picx.zhimg.com/v2-ce58dabc665a7337454b7490e57da45f" align="middle">
<img src="https://picx.zhimg.com/v2-49bf11143fa643bee978ed938d03a213" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="RLAC-Reinforcement-Learning-with-Adversarial-Critic-for-Free-Form-Generation-Tasks"><a href="#RLAC-Reinforcement-Learning-with-Adversarial-Critic-for-Free-Form-Generation-Tasks" class="headerlink" title="RLAC: Reinforcement Learning with Adversarial Critic for Free-Form   Generation Tasks"></a>RLAC: Reinforcement Learning with Adversarial Critic for Free-Form   Generation Tasks</h2><p><strong>Authors:Mian Wu, Gavin Zhang, Sewon Min, Sergey Levine, Aviral Kumar</strong></p>
<p>Open-ended generation tasks require outputs to satisfy diverse and often implicit task-specific evaluation rubrics. The sheer number of relevant rubrics leads to prohibitively high verification costs and incomplete assessments of a response, making reinforcement learning (RL) post-training with rubric-based rewards difficult to scale. This problem is exacerbated by the fact that often the best way to combine these rubrics into one single reward is also highly prompt-specific. We propose Reinforcement Learning with Adversarial Critic (RLAC), a post-training approach that addresses these challenges via dynamic rubric verification. Our approach employs a large language model (LLM) as a critic that dynamically identifies only the most likely failure modes (e.g., a factual error or unhandled edge case), which are then verified by an external validator to optimize both generator and critic jointly. By training both the generator and the critic, this game enhances the criticâ€™s error detection and the generatorâ€™s output quality while reducing required verifications. Our experiments demonstrate that RLAC improves factual accuracy in text generation and correctness in code generation, while also outperforming exhaustive verification and reward model methods. We show that dynamic critics are more effective than fixed critics, showcasing the potential of RLAC for scaling RL post-training to free-form generation tasks. </p>
<blockquote>
<p>å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡éœ€è¦è¾“å‡ºæ»¡è¶³å¤šæ ·åŒ–å’Œé€šå¸¸éšå¼çš„ç‰¹å®šä»»åŠ¡è¯„ä¼°æ ‡å‡†ã€‚ç›¸å…³æ ‡å‡†çš„æ•°é‡ä¼—å¤šå¯¼è‡´éªŒè¯æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”å“åº”è¯„ä¼°ä¸å®Œæ•´ï¼Œä½¿å¾—ä½¿ç”¨åŸºäºæ ‡å‡†çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒéš¾ä»¥æ‰©å±•ã€‚é—®é¢˜çš„ä¸¥é‡æ€§è¿˜åœ¨äºï¼Œé€šå¸¸å°†è¿™äº›æ ‡å‡†ç»„åˆæˆå•ä¸€å¥–åŠ±çš„æœ€ä½³æ–¹å¼ä¹Ÿæ˜¯é«˜åº¦æç¤ºç‰¹å®šçš„ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨å¯¹æŠ—æ€§è¯„è®ºå®¶ï¼ˆRLACï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ç§åè®­ç»ƒæ³•ï¼Œé€šè¿‡åŠ¨æ€æ ‡å‡†éªŒè¯æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„è®ºå®¶ï¼ŒåŠ¨æ€è¯†åˆ«æœ€å¯èƒ½çš„å¤±è´¥æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼Œäº‹å®é”™è¯¯æˆ–æœªå¤„ç†çš„è¾¹ç¼˜æƒ…å†µï¼‰ï¼Œç„¶åé€šè¿‡è¿™äº›æ¨¡å¼ç”±å¤–éƒ¨éªŒè¯å™¨éªŒè¯ï¼Œä»¥è”åˆä¼˜åŒ–ç”Ÿæˆå™¨å’Œè¯„è®ºå®¶ã€‚é€šè¿‡å¯¹ç”Ÿæˆå™¨å’Œè¯„è®ºå®¶çš„å…±åŒè®­ç»ƒï¼Œè¿™æ¬¾æ¸¸æˆå¢å¼ºäº†è¯„è®ºå®¶çš„é”™è¯¯æ£€æµ‹èƒ½åŠ›å’Œç”Ÿæˆå™¨çš„è¾“å‡ºè´¨é‡ï¼ŒåŒæ—¶å‡å°‘äº†æ‰€éœ€çš„éªŒè¯æ¬¡æ•°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRLACæé«˜äº†æ–‡æœ¬ç”Ÿæˆçš„äº‹å®å‡†ç¡®æ€§å’Œä»£ç ç”Ÿæˆçš„æ­£ç¡®æ€§ï¼ŒåŒæ—¶ä¼˜äºè¯¦å°½éªŒè¯å’Œå¥–åŠ±æ¨¡å‹æ–¹æ³•ã€‚æˆ‘ä»¬è¯æ˜äº†åŠ¨æ€è¯„è®ºå®¶æ¯”å›ºå®šè¯„è®ºå®¶æ›´æœ‰æ•ˆï¼Œå±•ç¤ºäº†RLACåœ¨æ‰©å±•åˆ°è‡ªç”±å½¢å¼ç”Ÿæˆä»»åŠ¡æ—¶å…·æœ‰çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01758v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://mianwu01.github.io/RLAC_website/">https://mianwu01.github.io/RLAC_website/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§åŸºäºå¯¹æŠ—æ€§æ‰¹åˆ¤çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLACï¼‰æ–¹æ³•ï¼Œç”¨äºè§£å†³å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ä¸­çš„å¤šæ ·åŒ–è¯„ä»·æ ‡å‡†çš„æŒ‘æˆ˜ã€‚RLACé€šè¿‡åŠ¨æ€è¯„ä¼°æ ‡å‡†æ¥å‡å°‘éªŒè¯æˆæœ¬å’Œæé«˜è¯„ä¼°çš„å®Œæ•´æ€§ï¼ŒåŒæ—¶è§£å†³å¥–åŠ±ç»„åˆä¸­é¢ä¸´çš„éš¾é¢˜ã€‚å®ƒé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ‰¹è¯„å®¶ï¼ŒåŠ¨æ€è¯†åˆ«æœ€å¯èƒ½çš„å¤±è´¥æ¨¡å¼ï¼Œå¹¶é€šè¿‡å¤–éƒ¨éªŒè¯å™¨è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒRLACå¯ä»¥æé«˜æ–‡æœ¬ç”Ÿæˆå’Œä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œå¹¶ä¼˜äºç©·å°½éªŒè¯å’Œå¥–åŠ±æ¨¡å‹æ–¹æ³•ã€‚åŠ¨æ€æ‰¹è¯„å®¶æ¯”å›ºå®šæ‰¹è¯„å®¶æ›´æœ‰æ•ˆï¼Œå±•ç¤ºäº†RLACåœ¨è‡ªç”±å½¢å¼ç”Ÿæˆä»»åŠ¡ä¸­æ‰©å±•å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æœªæ¥æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼€æ”¾å¼çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡æ¶‰åŠå¤šä¸ªéš¾ä»¥ç¡®å®šä¼˜å…ˆçº§ä¸”å¯èƒ½éš¾ä»¥å‡†ç¡®å®ç°çš„è¯„ä»·æ ‡å‡†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c65042f21d67625ad5225f302c5447d9" align="middle">
<img src="https://picx.zhimg.com/v2-989dab4e35cc27138e7cc16785879154" align="middle">
<img src="https://picx.zhimg.com/v2-575bddb98d90d2ef28c5612c7625eeb6" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Cross-Treatment-Effect-Estimation-for-Multi-Category-Multi-Valued-Causal-Inference-via-Dynamic-Neural-Masking"><a href="#Cross-Treatment-Effect-Estimation-for-Multi-Category-Multi-Valued-Causal-Inference-via-Dynamic-Neural-Masking" class="headerlink" title="Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued   Causal Inference via Dynamic Neural Masking"></a>Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued   Causal Inference via Dynamic Neural Masking</h2><p><strong>Authors:Xiaopeng Ke, Yihan Yu, Ruyue Zhang, Zhishuo Zhou, Fangzhou Shi, Chang Men, Zhengdan Zhu</strong></p>
<p>Counterfactual causal inference faces significant challenges when extended to multi-category, multi-valued treatments, where complex cross-effects between heterogeneous interventions are difficult to model. Existing methodologies remain constrained to binary or single-type treatments and suffer from restrictive assumptions, limited scalability, and inadequate evaluation frameworks for complex intervention scenarios.   We present XTNet, a novel network architecture for multi-category, multi-valued treatment effect estimation. Our approach introduces a cross-effect estimation module with dynamic masking mechanisms to capture treatment interactions without restrictive structural assumptions. The architecture employs a decomposition strategy separating basic effects from cross-treatment interactions, enabling efficient modeling of combinatorial treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that accounts for treatment costs and interaction effects. Extensive experiments on synthetic and real-world datasets demonstrate that XTNet consistently outperforms state-of-the-art baselines in both ranking accuracy and effect estimation quality. The results of the real-world A&#x2F;B test further confirm its effectiveness. </p>
<blockquote>
<p>åœ¨æ‰©å±•åˆ°å¤šç±»åˆ«ã€å¤šå€¼å¤„ç†æ—¶ï¼Œåäº‹å®å› æœæ¨æ–­é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¼‚è´¨å¹²é¢„ä¹‹é—´çš„å¤æ‚äº¤å‰æ•ˆåº”éš¾ä»¥å»ºæ¨¡ã€‚ç°æœ‰æ–¹æ³•ä»ç„¶å±€é™äºäºŒå…ƒæˆ–å•ä¸€ç±»å‹çš„å¤„ç†ï¼Œå¹¶å—åˆ°ä¸¥æ ¼å‡è®¾ã€æœ‰é™çš„å¯æ‰©å±•æ€§å’Œå¤æ‚çš„å¹²é¢„åœºæ™¯è¯„ä¼°æ¡†æ¶ä¸è¶³çš„å›°æ‰°ã€‚æˆ‘ä»¬æå‡ºäº†XTNetï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šç±»åˆ«ã€å¤šå€¼å¤„ç†æ•ˆæœä¼°è®¡çš„æ–°å‹ç½‘ç»œæ¶æ„ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªå¸¦æœ‰åŠ¨æ€å±è”½æœºåˆ¶çš„äº¤å‰æ•ˆåº”ä¼°è®¡æ¨¡å—ï¼Œå¯ä»¥åœ¨æ²¡æœ‰ä¸¥æ ¼çš„ç»“æ„å‡è®¾çš„æƒ…å†µä¸‹æ•è·å¤„ç†äº¤äº’ã€‚è¯¥æ¶æ„é‡‡ç”¨åˆ†è§£ç­–ç•¥ï¼Œå°†åŸºæœ¬æ•ˆåº”ä¸è·¨å¤„ç†äº¤äº’åˆ†å¼€ï¼Œå®ç°å¯¹ç»„åˆå¤„ç†ç©ºé—´çš„æœ‰æ•ˆå»ºæ¨¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MCMV-AUCCï¼Œè¿™æ˜¯ä¸€ä¸ªé€‚å½“çš„è¯„ä¼°æŒ‡æ ‡ï¼Œè€ƒè™‘åˆ°å¤„ç†æˆæœ¬å’Œäº¤äº’æ•ˆæœã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒXTNetåœ¨æ’åå‡†ç¡®æ€§å’Œæ•ˆæœä¼°è®¡è´¨é‡æ–¹é¢éƒ½å§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ã€‚ç°å®ä¸–ç•Œä¸­çš„A&#x2F;Bæµ‹è¯•ç»“æœè¿›ä¸€æ­¥è¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01641v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤šç±»åˆ«ã€å¤šå€¼å¤„ç†æƒ…å†µä¸‹ï¼Œç°æœ‰çš„åäº‹å®å› æœæ¨æ–­æ–¹æ³•é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå»ºæ¨¡å¤æ‚çš„äº¤å‰æ•ˆåº”å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æå‡ºXTNetç½‘ç»œæ¶æ„ï¼Œç”¨äºå¤šç±»åˆ«ã€å¤šå€¼å¤„ç†æ•ˆåº”ä¼°è®¡ã€‚è¯¥æ¶æ„å¼•å…¥äº¤å‰æ•ˆåº”ä¼°è®¡æ¨¡å—å’ŒåŠ¨æ€æ©ç æœºåˆ¶ï¼Œæ•æ‰å¤„ç†äº¤äº’ä½œç”¨ï¼Œæ— éœ€ä¸¥æ ¼çš„å‡è®¾ã€‚é€šè¿‡åˆ†è§£ç­–ç•¥åˆ†ç¦»åŸºæœ¬æ•ˆåº”å’Œäº¤å‰å¤„ç†äº¤äº’ä½œç”¨ï¼Œæœ‰æ•ˆå»ºæ¨¡ç»„åˆå¤„ç†ç©ºé—´ã€‚åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒXTNetåœ¨æ’åå‡†ç¡®æ€§å’Œæ•ˆæœä¼°è®¡è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚çœŸå®ä¸–ç•Œçš„A&#x2F;Bæµ‹è¯•ç»“æœè¿›ä¸€æ­¥è¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç±»åˆ«ã€å¤šå€¼å¤„ç†çš„åäº‹å®å› æœæ¨æ–­é¢ä¸´æŒ‘æˆ˜ï¼Œå»ºæ¨¡å¤æ‚äº¤å‰æ•ˆåº”å›°éš¾ã€‚</li>
<li>XTNetç½‘ç»œæ¶æ„ç”¨äºå¤šç±»åˆ«ã€å¤šå€¼å¤„ç†æ•ˆåº”ä¼°è®¡ã€‚</li>
<li>XTNetå¼•å…¥äº¤å‰æ•ˆåº”ä¼°è®¡æ¨¡å—å’ŒåŠ¨æ€æ©ç æœºåˆ¶ï¼Œæ— éœ€ä¸¥æ ¼å‡è®¾ã€‚</li>
<li>XTNeté€šè¿‡åˆ†è§£ç­–ç•¥æœ‰æ•ˆå»ºæ¨¡ç»„åˆå¤„ç†ç©ºé—´ã€‚</li>
<li>XTNetåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>XTNetåœ¨æ’åå‡†ç¡®æ€§å’Œæ•ˆæœä¼°è®¡è´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bbbf3d6149a9b44b8d41fc4149abd0a" align="middle">
<img src="https://picx.zhimg.com/v2-5ba8b42dadb9742206595e860016048c" align="middle">
<img src="https://picx.zhimg.com/v2-aa51347cb9c07e00f13b241cb223140b" align="middle">
<img src="https://picx.zhimg.com/v2-db5f27646ea49aca24456248f34aeb2c" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Prompt-Injection-as-an-Emerging-Threat-Evaluating-the-Resilience-of-Large-Language-Models"><a href="#Prompt-Injection-as-an-Emerging-Threat-Evaluating-the-Resilience-of-Large-Language-Models" class="headerlink" title="Prompt Injection as an Emerging Threat: Evaluating the Resilience of   Large Language Models"></a>Prompt Injection as an Emerging Threat: Evaluating the Resilience of   Large Language Models</h2><p><strong>Authors:Daniyal Ganiuly, Assel Smaiyl</strong></p>
<p>Large Language Models (LLMs) are increasingly used in intelligent systems that perform reasoning, summarization, and code generation. Their ability to follow natural-language instructions, while powerful, also makes them vulnerable to a new class of attacks known as prompt injection. In these attacks, hidden or malicious instructions are inserted into user inputs or external content, causing the model to ignore its intended task or produce unsafe responses. This study proposes a unified framework for evaluating how resistant Large Language Models (LLMs) are to prompt injection attacks. The framework defines three complementary metrics such as the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM) to jointly measure robustness, safety, and semantic stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3 8B Instruct, and Flan-T5-Large) on five common language tasks: question answering, summarization, translation, reasoning, and code generation. Results show that GPT-4 performs best overall, while open-weight models remain more vulnerable. The findings highlight that strong alignment and safety tuning are more important for resilience than model size alone. Results show that all models remain partially vulnerable, especially to indirect and direct-override attacks. GPT-4 achieved the best overall resilience (RDR &#x3D; 9.8 %, SCR &#x3D; 96.4 %), while open-source models exhibited higher performance degradation and lower safety scores. The findings demonstrate that alignment strength and safety tuning play a greater role in resilience than model size alone. The proposed framework offers a structured, reproducible approach for assessing model robustness and provides practical insights for improving LLM safety and reliability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­è¶Šæ¥è¶Šå¹¿æ³›åœ°åº”ç”¨äºæ¨ç†ã€æ‘˜è¦å’Œä»£ç ç”Ÿæˆã€‚å®ƒä»¬éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›è™½ç„¶å¼ºå¤§ï¼Œä½†ä¹Ÿä½¿å®ƒä»¬å®¹æ˜“å—åˆ°ä¸€ç§åä¸ºæç¤ºæ³¨å…¥çš„æ–°æ”»å‡»çš„å½±å“ã€‚åœ¨è¿™äº›æ”»å‡»ä¸­ï¼Œéšè—æˆ–æ¶æ„çš„æŒ‡ä»¤è¢«æ’å…¥ç”¨æˆ·è¾“å…¥æˆ–å¤–éƒ¨å†…å®¹ä¸­ï¼Œå¯¼è‡´æ¨¡å‹å¿½ç•¥å…¶é¢„å®šä»»åŠ¡æˆ–äº§ç”Ÿä¸å®‰å…¨çš„å“åº”ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„æŠµæŠ—åŠ›ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†ä¸‰ä¸ªäº’è¡¥çš„åº¦é‡æ ‡å‡†ï¼Œå³éŸ§æ€§é™çº§æŒ‡æ•°ï¼ˆRDIï¼‰ã€å®‰å…¨åˆè§„ç³»æ•°ï¼ˆSCCï¼‰å’ŒæŒ‡ä»¤å®Œæ•´æ€§åº¦é‡ï¼ˆIIMï¼‰ï¼Œä»¥å…±åŒè¡¡é‡ç¨³å¥æ€§ã€å®‰å…¨æ€§å’Œè¯­ä¹‰ç¨³å®šæ€§ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå¸¸è§çš„è¯­è¨€ä»»åŠ¡ä¸Šè¯„ä¼°äº†å››ä¸ªæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼ˆGPT-4ã€GPT-4oã€LLaMA-3 8B Instructå’ŒFlan-T5-Largeï¼‰ï¼šé—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ã€æ¨ç†å’Œä»£ç ç”Ÿæˆã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-4æ€»ä½“ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€Œå¼€æ”¾æƒé‡æ¨¡å‹ä»ç„¶æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç»“æœè¡¨æ˜ï¼Œå¼ºå¯¹é½å’Œå®‰å…¨è°ƒæ•´å¯¹äºéŸ§æ€§æ¯”å•çº¯æ¨¡å‹å¤§å°æ›´é‡è¦ã€‚ç»“æœè¿˜è¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹ä»ç„¶éƒ¨åˆ†è„†å¼±ï¼Œå°¤å…¶æ˜¯é—´æ¥å’Œç›´æ¥è¦†ç›–æ”»å‡»ã€‚GPT-4æ€»ä½“éŸ§æ€§æœ€ä½³ï¼ˆRDR&#x3D;9.8%ï¼ŒSCR&#x3D;96.4%ï¼‰ï¼Œè€Œå¼€æºæ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ä¸‹é™å’Œè¾ƒä½çš„å®‰å…¨åˆ†æ•°ã€‚ç»“æœè¡¨æ˜ï¼Œå¯¹é½å¼ºåº¦å’Œå®‰å…¨è°ƒæ•´åœ¨éŸ§æ€§æ–¹é¢æ¯”å•çº¯çš„æ¨¡å‹å¤§å°æ›´é‡è¦ã€‚æ‰€æå‡ºçš„æ¡†æ¶æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–ã€å¯é‡å¤çš„æ–¹æ³•æ¥è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå¹¶ä¸ºæé«˜LLMçš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›äº†å®é™…è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01634v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå¦‚æ¨ç†ã€æ‘˜è¦å’Œä»£ç ç”Ÿæˆç­‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›ä¹Ÿä½¿å…¶é¢ä¸´ä¸€ç§æ–°çš„æ”»å‡»æ–¹å¼â€”â€”æç¤ºæ³¨å…¥ã€‚æ”»å‡»è€…ä¼šåœ¨ç”¨æˆ·è¾“å…¥æˆ–å¤–éƒ¨å†…å®¹ä¸­éšè—æˆ–æ’å…¥æ¶æ„æŒ‡ä»¤ï¼Œå¯¼è‡´æ¨¡å‹å¿½ç•¥å…¶ç›®æ ‡ä»»åŠ¡æˆ–äº§ç”Ÿä¸å®‰å…¨å“åº”ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„æŠµæŠ—åŠ›ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼Œå³æ¢å¤åŠ›é™ä½æŒ‡æ•°ï¼ˆRDIï¼‰ã€å®‰å…¨åˆè§„ç³»æ•°ï¼ˆSCCï¼‰å’ŒæŒ‡ä»¤å®Œæ•´æ€§æŒ‡æ ‡ï¼ˆIIMï¼‰ï¼Œä»¥è”åˆæµ‹é‡ç¨³å¥æ€§ã€å®‰å…¨æ€§å’Œè¯­ä¹‰ç¨³å®šæ€§ã€‚åœ¨äº”ç§å¸¸è§çš„è¯­è¨€ä»»åŠ¡ä¸Šè¯„ä¼°äº†å››ä¸ªæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºGPT-4æ€»ä½“è¡¨ç°æœ€ä½³ï¼Œè€Œå¼€æ”¾æƒé‡æ¨¡å‹ä»ç„¶æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºæ¢å¤åŠ›è€Œè¨€ï¼Œå¼ºå¯¹é½å’Œå®‰å…¨è°ƒæ•´æ¯”å•çº¯æ¨¡å‹å¤§å°æ›´é‡è¦ã€‚æ‰€æœ‰æ¨¡å‹ä»ç„¶éƒ¨åˆ†æ˜“å—æ”»å‡»ï¼Œå°¤å…¶æ˜¯é—´æ¥å’Œç›´æ¥è¦†ç›–æ”»å‡»ã€‚GPT-4åœ¨æ¢å¤åŠ›å’Œå®‰å…¨æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚ç ”ç©¶æå‡ºçš„æ¡†æ¶ä¸ºè¯„ä¼°æ¨¡å‹ç¨³å¥æ€§æä¾›äº†ç»“æ„åŒ–å’Œå¯é‡å¤çš„æ–¹æ³•ï¼Œå¹¶ä¸ºæé«˜LLMçš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›äº†å®é™…è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†é¢ä¸´æç¤ºæ³¨å…¥æ”»å‡»çš„é£é™©ã€‚</li>
<li>æç¤ºæ³¨å…¥æ”»å‡»é€šè¿‡æ’å…¥éšè—æˆ–æ¶æ„æŒ‡ä»¤ï¼Œä½¿æ¨¡å‹å¿½ç•¥ç›®æ ‡ä»»åŠ¡æˆ–äº§ç”Ÿä¸å®‰å…¨å“åº”ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥è¯„ä¼°LLMså¯¹æç¤ºæ³¨å…¥çš„æŠµæŠ—åŠ›ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼šRDIã€SCCã€IIMã€‚</li>
<li>åœ¨äº”ç§è¯­è¨€ä»»åŠ¡ä¸Šè¯„ä¼°äº†å››ä¸ªæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼ŒGPT-4è¡¨ç°æœ€ä½³ã€‚</li>
<li>å¼€æ”¾æƒé‡æ¨¡å‹ä»ç„¶æ›´å®¹æ˜“å—åˆ°æ”»å‡»ï¼Œå¼ºå¯¹é½å’Œå®‰å…¨è°ƒæ•´æ¯”æ¨¡å‹å¤§å°æ›´é‡è¦ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹å¯¹é—´æ¥å’Œç›´æ¥è¦†ç›–æ”»å‡»ä»ç„¶éƒ¨åˆ†æ˜“å—æ”»å‡»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a968ab4b6d044d46bf99c9eef6444e0f" align="middle">
<img src="https://picx.zhimg.com/v2-8d4c77eac425d4f2ea54502d58409f85" align="middle">
<img src="https://picx.zhimg.com/v2-eb13ca704e75b703480d936647383739" align="middle">
<img src="https://picx.zhimg.com/v2-0ae48e5cd5c26335733084cc8289b9e9" align="middle">
<img src="https://picx.zhimg.com/v2-753485ab11072f8f58282313f8e83288" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7b2fece82c8b0a799c4af337bf5051aa" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Agent-Omni Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7e50e979b90b47a71545c2fde3f71161" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  See the Speaker Crafting High-Resolution Talking Faces from Speech with   Prior Guidance and Region Refinement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
