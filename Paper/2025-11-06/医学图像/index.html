<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Resource-efficient Automatic Refinement of Segmentations via Weak   Supervision from Light Feedback">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a89cda7d3982b44c11b295c82c2a1d2e')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-06-æ›´æ–°"><a href="#2025-11-06-æ›´æ–°" class="headerlink" title="2025-11-06 æ›´æ–°"></a>2025-11-06 æ›´æ–°</h1><h2 id="Resource-efficient-Automatic-Refinement-of-Segmentations-via-Weak-Supervision-from-Light-Feedback"><a href="#Resource-efficient-Automatic-Refinement-of-Segmentations-via-Weak-Supervision-from-Light-Feedback" class="headerlink" title="Resource-efficient Automatic Refinement of Segmentations via Weak   Supervision from Light Feedback"></a>Resource-efficient Automatic Refinement of Segmentations via Weak   Supervision from Light Feedback</h2><p><strong>Authors:Alix de Langlais, Benjamin Billot, ThÃ©o Aguilar Vidal, Marc-Olivier Gauci, HervÃ© Delingette</strong></p>
<p>Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over&#x2F;under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: <a target="_blank" rel="noopener" href="https://gitlab.inria.fr/adelangl/SCORE">https://gitlab.inria.fr/adelangl/SCORE</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œåˆ’å®šè§£å‰–åŒºåŸŸæ˜¯ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚æ‰‹åŠ¨åˆ†å‰²å¯ä»¥å®ç°è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä½†åŠ³åŠ¨å¼ºåº¦å¤§ä¸”æ˜“å‡ºç°å·®å¼‚ï¼Œä»è€Œæ¨åŠ¨äº†è‡ªåŠ¨åŒ–æ–¹æ³•çš„å‘å±•ã€‚æœ€è¿‘ï¼Œä¸€ç³»åˆ—åŸºç¡€æ¨¡å‹å·²ç»å®ç°äº†è·¨ä¸åŒè§£å‰–ç»“æ„å’Œæˆåƒæ–¹å¼çš„è‡ªåŠ¨åˆ†å‰²ï¼Œä½†è¿™äº›å¯èƒ½å¹¶ä¸æ€»èƒ½è¾¾åˆ°ä¸´åºŠç²¾åº¦æ ‡å‡†ã€‚è™½ç„¶åˆ†å‰²ç»†åŒ–ç­–ç•¥å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†å½“å‰çš„æ–¹æ³•ä¾èµ–äºå¤§é‡çš„ç”¨æˆ·äº¤äº’æˆ–ä¸ºè®­ç»ƒéœ€è¦å¤§é‡å®Œå…¨ç›‘ç£çš„åˆ†å‰²ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†SCOREï¼ˆåŸºäºåŒºåŸŸè¯„ä¼°çš„åˆ†å‰²æ ¡æ­£ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼±ç›‘ç£æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨è®­ç»ƒæœŸé—´ä»…ä½¿ç”¨è½»é‡çº§åé¦ˆæ¥å­¦ä¹ æ”¹è¿›è’™ç‰ˆé¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼ŒSCOREä¸æ˜¯ä¾èµ–äºå¯†é›†çš„è®­ç»ƒå›¾åƒæ³¨é‡Šï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§æ–°å‹æŸå¤±å‡½æ•°ï¼Œè¯¥æŸå¤±å‡½æ•°åˆ©ç”¨åŒºåŸŸè´¨é‡åˆ†æ•°å’Œè¿‡åº¦æˆ–ä¸è¶³çš„åˆ†å‰²è¯¯å·®æ ‡ç­¾ã€‚æˆ‘ä»¬åœ¨è‚±éª¨CTæ‰«æä¸­å±•ç¤ºäº†SCOREçš„åº”ç”¨ï¼Œå®ƒèƒ½æ˜¾è‘—æ”¹è¿›TotalSegmentatorçš„åˆæ­¥é¢„æµ‹ç»“æœï¼Œå®ç°ä¸ç°æœ‰ç»†åŒ–æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§é™ä½å…¶ç›‘ç£è¦æ±‚å’Œæ³¨é‡Šæ—¶é—´ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gitlab.inria.fr/adelangl/SCORE%E3%80%82">https://gitlab.inria.fr/adelangl/SCOREã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02576v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSCOREçš„å¼±ç›‘ç£æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç²¾åº¦ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨è½»é‡çº§åé¦ˆå¯¹åˆ†å‰²æ©è†œè¿›è¡Œä¿®æ­£ï¼Œé€šè¿‡å¼•å…¥æ–°çš„æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨åŒºåŸŸè´¨é‡åˆ†æ•°å’Œè¿‡åº¦æˆ–ä¸è¶³åˆ†å‰²è¯¯å·®æ ‡ç­¾ï¼Œå®ç°å¯¹å¤šç§è§£å‰–éƒ¨ä½å’Œæˆåƒæ¨¡æ€çš„è‡ªåŠ¨åŒ–åˆ†å‰²ã€‚åœ¨éª¨éª¼CTæ‰«æä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSCOREèƒ½å¤Ÿæ˜¾è‘—æé«˜åˆå§‹é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ç°æœ‰ä¿®æ­£æ–¹æ³•è¾¾åˆ°ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§é™ä½ç›‘ç£è¦æ±‚å’Œæ ‡æ³¨æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SCOREæ¡†æ¶ä½¿ç”¨å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•å¯¹åŒ»å­¦å›¾åƒåˆ†å‰²è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚</li>
<li>è¯¥æ¡†æ¶å¼•å…¥äº†æ–°çš„æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨åŒºåŸŸè´¨é‡åˆ†æ•°å’Œåˆ†å‰²è¯¯å·®æ ‡ç­¾æ¥æå‡åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>SCOREæ¡†æ¶åœ¨éª¨éª¼CTæ‰«æä¸Šçš„å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æé«˜åˆå§‹é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸ç°æœ‰ä¿®æ­£æ–¹æ³•ç›¸æ¯”ï¼ŒSCOREæ¡†æ¶é™ä½äº†ç›‘ç£è¦æ±‚å’Œæ ‡æ³¨æ—¶é—´ã€‚</li>
<li>SCOREæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯åº”ç”¨äºå¤šç§è§£å‰–éƒ¨ä½å’Œæˆåƒæ¨¡æ€çš„è‡ªåŠ¨åŒ–åˆ†å‰²ã€‚</li>
<li>è¯¥æ¡†æ¶çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
<li>æ­¤æ–¹æ³•è§£å†³äº†åŒ»å­¦å›¾åƒåˆ†æä¸­æ‰‹åŠ¨åˆ†å‰²åŠ³åŠ¨å¼ºåº¦å¤§ã€æ˜“å‡ºé”™çš„é—®é¢˜ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c61ca5516d95bbf9d7265c946dba007" align="middle">
<img src="https://picx.zhimg.com/v2-65ff0a724f1855329e96292c75b96974" align="middle">
<img src="https://picx.zhimg.com/v2-b645c84ac8d59f8fb8424c6c791d538b" align="middle">
<img src="https://picx.zhimg.com/v2-a89cda7d3982b44c11b295c82c2a1d2e" align="middle">
<img src="https://picx.zhimg.com/v2-9cc650cb2d5a38682606acf0770e80e0" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Forecasting-Future-Anatomies-Longitudianl-Brain-Mri-to-Mri-Prediction"><a href="#Forecasting-Future-Anatomies-Longitudianl-Brain-Mri-to-Mri-Prediction" class="headerlink" title="Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction"></a>Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction</h2><p><strong>Authors:Ali Farki, Elaheh Moradi, Deepika Koundal, Jussi Tohka</strong></p>
<p>Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimerâ€™s disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participantâ€™s entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis. </p>
<blockquote>
<p>ä»åŸºçº¿ç£å…±æŒ¯å›¾åƒï¼ˆMRIï¼‰é¢„æµ‹æœªæ¥çš„è„‘çŠ¶æ€æ˜¯ç¥ç»å½±åƒå­¦ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¯¹äºç ”ç©¶é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ç­‰ç¥ç»é€€è¡Œæ€§ç–¾ç—…å…·æœ‰é‡è¦æ„ä¹‰ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é¢„æµ‹æœªæ¥çš„è®¤çŸ¥å¾—åˆ†æˆ–ä¸´åºŠç»“æœï¼Œä¾‹å¦‚ä»è½»åº¦è®¤çŸ¥éšœç¢è½¬å˜ä¸ºç—´å‘†ã€‚ä¸æ­¤ç›¸åï¼Œæˆ‘ä»¬åœ¨ç ”ç©¶ä¸­æ¢ç´¢äº†çºµå‘MRIå›¾åƒé¢„æµ‹ï¼Œè¿™æ˜¯ä¸€ç§å¯ä»¥é¢„æµ‹å‚ä¸è€…åœ¨æœªæ¥å‡ å¹´å†…æ•´ä¸ªå¤§è„‘çš„MRIå›¾åƒçš„æ–¹æ³•ï¼Œå†…åœ¨åœ°æ¨¡æ‹Ÿå¤æ‚çš„ç©ºé—´åˆ†å¸ƒç¥ç»é€€è¡Œæ¨¡å¼ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªçºµå‘é˜Ÿåˆ—ç ”ç©¶ï¼ˆADNIå’ŒAIBLï¼‰ä¸Šå®ç°äº†äº”ç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆUNetã€U2-Netã€UNETRã€æ—¶é—´åµŒå…¥UNetå’ŒODE-UNetï¼‰ï¼Œå¹¶å°†é¢„æµ‹çš„è·Ÿæ£€MRIä¸å®é™…è·Ÿè¸ªæ‰«æç»“æœç›´æ¥è¿›è¡Œæ¯”è¾ƒï¼Œé‡‡ç”¨è¡¡é‡æŒ‡æ ‡æ•è·å…¨å±€ç›¸ä¼¼æ€§å’Œå±€éƒ¨å·®å¼‚ã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹å¯å®ç°é«˜ä¿çœŸé¢„æµ‹ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨ç‹¬ç«‹å¤–éƒ¨æ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œæ˜¾ç¤ºå‡ºç¨³å¥çš„è·¨é˜Ÿåˆ—æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ å¯ä»¥å¯é åœ°é¢„æµ‹ç‰¹å®šå‚ä¸è€…çš„ä¸ªæ€§åŒ–å¤§è„‘MRIå›¾åƒï¼Œè¿™ä¸ºä¸ªä½“åŒ–é¢„åæä¾›äº†æ–°çš„æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02558v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•é¢„æµ‹æœªæ¥å¤§è„‘çŠ¶æ€MRIå›¾åƒï¼Œé€šè¿‡å¯¹é•¿æœŸçºµå‘MRIå›¾åƒè¿›è¡Œé¢„æµ‹ï¼Œæ¨¡æ‹Ÿå¤æ‚çš„ç¥ç»é€€åŒ–æ¨¡å¼ï¼Œç›´æ¥å¯¹æ¯”é¢„æµ‹ç»“æœä¸å®é™…æ‰«æç»“æœï¼Œå¹¶åœ¨ç‹¬ç«‹å¤–éƒ¨æ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚æ­¤ç ”ç©¶ä¸ºä¸ªæ€§åŒ–é¢„æµ‹æä¾›äº†æ–°çš„æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æŒ‘æˆ˜äº†ä»åŸºçº¿ç£å…±æŒ¯å›¾åƒï¼ˆMRIï¼‰é¢„æµ‹æœªæ¥å¤§è„‘çŠ¶æ€çš„ä¸­å¿ƒé—®é¢˜ï¼Œè¿™åœ¨ç¥ç»å½±åƒå­¦ä¸­å¯¹ç ”ç©¶ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼ˆå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼‰å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œé•¿æœŸçºµå‘MRIå›¾åƒé¢„æµ‹ï¼Œå†…åœ¨æ¨¡æ‹Ÿå¤æ‚çš„ç¥ç»é€€åŒ–æ¨¡å¼ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†äº”ç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆUNetã€U2-Netã€UNETRã€Time-Embedding UNetå’ŒODE-UNetï¼‰åœ¨ä¸¤ä¸ªçºµå‘é˜Ÿåˆ—ï¼ˆADNIå’ŒAIBLï¼‰ä¸­çš„è¡¨ç°ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„æ¨¡å‹èƒ½å¤Ÿå®ç°é«˜ä¿çœŸåº¦çš„é¢„æµ‹ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹éƒ½èƒ½åœ¨ç‹¬ç«‹å¤–éƒ¨æ•°æ®é›†ä¸Šè‰¯å¥½åœ°æ³›åŒ–ï¼Œè¡¨ç°å‡ºè·¨é˜Ÿåˆ—çš„ç¨³å®šæ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯æ˜æ·±åº¦å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿå¯é åœ°é¢„æµ‹ç‰¹å®šå‚ä¸è€…çš„MRIå›¾åƒï¼Œè¾¾åˆ°ä½“ç´ çº§åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49ef59308bee1176af845f95d886c5ff" align="middle">
<img src="https://picx.zhimg.com/v2-98116a806d13a4ae366d5447be6cedd2" align="middle">
<img src="https://picx.zhimg.com/v2-8ce3b4b08cd224cda109984065898d8e" align="middle">
<img src="https://picx.zhimg.com/v2-5c7543a85a02c8ecd12d041ee0cd11c2" align="middle">
<img src="https://picx.zhimg.com/v2-5a60abdcca5ba929d4aceba37686ef25" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Adapting-General-Purpose-Foundation-Models-for-X-ray-Ptychography-in-Low-Data-Regimes"><a href="#Adapting-General-Purpose-Foundation-Models-for-X-ray-Ptychography-in-Low-Data-Regimes" class="headerlink" title="Adapting General-Purpose Foundation Models for X-ray Ptychography in   Low-Data Regimes"></a>Adapting General-Purpose Foundation Models for X-ray Ptychography in   Low-Data Regimes</h2><p><strong>Authors:Robinson Umeike, Neil Getty, Yin Xiangyu, Yi Jiang</strong></p>
<p>The automation of workflows in advanced microscopy is a key goal where foundation models like Language Models (LLMs) and Vision-Language Models (VLMs) show great potential. However, adapting these general-purpose models for specialized scientific tasks is critical, and the optimal domain adaptation strategy is often unclear. To address this, we introduce PtychoBench, a new multi-modal, multi-task benchmark for ptychographic analysis. Using this benchmark, we systematically compare two specialization strategies: Supervised Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies on a visual artifact detection task with VLMs and a textual parameter recommendation task with LLMs in a data-scarce regime. Our findings reveal that the optimal specialization pathway is task-dependent. For the visual task, SFT and ICL are highly complementary, with a fine-tuned model guided by context-aware examples achieving the highest mean performance (Micro-F1 of 0.728). Conversely, for the textual task, ICL on a large base model is the superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a powerful â€œsuper-expertâ€ SFT model (0-shot Micro-F1 of 0.839). We also confirm the superiority of context-aware prompting and identify a consistent contextual interference phenomenon in fine-tuned models. These results, benchmarked against strong baselines including GPT-4o and a DINOv3-based classifier, offer key observations for AI in science: the optimal specialization path in our benchmark is dependent on the task modality, offering a clear framework for developing more effective science-based agentic systems. </p>
<blockquote>
<p>åœ¨é«˜çº§æ˜¾å¾®é•œä¸­ï¼Œå·¥ä½œæµç¨‹è‡ªåŠ¨åŒ–æ˜¯ä¸€ä¸ªå…³é”®ç›®æ ‡ï¼Œè€ŒåŸºç¡€æ¨¡å‹å¦‚è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›é€šç”¨æ¨¡å‹é€‚åº”äºç‰¹å®šç§‘å­¦ä»»åŠ¡è‡³å…³é‡è¦ï¼Œè€Œæœ€ä½³é¢†åŸŸé€‚åº”ç­–ç•¥å¾€å¾€ä¸æ˜ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PtychoBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡å¼å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹ptychographicè¿›è¡Œåˆ†æã€‚åˆ©ç”¨è¿™ä¸€åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†ä¸¤ç§ä¸“ä¸šåŒ–ç­–ç•¥ï¼šæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚æˆ‘ä»¬åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå¯¹VLMçš„è§†è§‰ä¼ªå½±æ£€æµ‹ä»»åŠ¡å’ŒLLMçš„æ–‡æœ¬å‚æ•°æ¨èä»»åŠ¡ä¸Šè¯„ä¼°äº†è¿™äº›ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæœ€ä½³çš„ä¸“ä¸šåŒ–é€”å¾„å–å†³äºä»»åŠ¡ã€‚å¯¹äºè§†è§‰ä»»åŠ¡ï¼ŒSFTå’ŒICLæ˜¯äº’è¡¥çš„ï¼Œç»è¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¾‹å­å¼•å¯¼çš„å¾®è°ƒæ¨¡å‹å–å¾—äº†æœ€é«˜å¹³å‡æ€§èƒ½ï¼ˆMicro-F1ä¸º0.728ï¼‰ã€‚ç›¸åï¼Œå¯¹äºæ–‡æœ¬ä»»åŠ¡ï¼Œåœ¨å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ä¸Šè¿›è¡ŒICLæ˜¯æ›´ä¼˜è¶Šçš„ç­–ç•¥ï¼Œè¾¾åˆ°äº†å³°å€¼Micro-F1ä¸º0.847ï¼Œå¹¶è¶…è¶Šäº†å¼ºå¤§çš„â€œè¶…çº§ä¸“å®¶â€SFTæ¨¡å‹ï¼ˆé›¶æ ·æœ¬Micro-F1ä¸º0.839ï¼‰ã€‚æˆ‘ä»¬è¿˜è¯å®äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥æç¤ºçš„ä¼˜åŠ¿ï¼Œå¹¶å‘ç°å¾®è°ƒæ¨¡å‹ä¸­æ™®éå­˜åœ¨ä¸€è‡´çš„ä¸Šä¸‹æ–‡å¹²æ‰°ç°è±¡ã€‚è¿™äº›ç»“æœä¸åŒ…æ‹¬GPT-4oå’ŒåŸºäºDINOv3çš„åˆ†ç±»å™¨åœ¨å†…çš„å¼ºå¤§åŸºçº¿ç›¸æ¯”ï¼Œä¸ºäººå·¥æ™ºèƒ½åœ¨ç§‘å­¦é¢†åŸŸæä¾›äº†å…³é”®è§‚å¯Ÿï¼šåœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ€ä½³çš„ä¸“ä¸šåŒ–è·¯å¾„å–å†³äºä»»åŠ¡æ¨¡å¼ï¼Œä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„ç§‘å­¦åŸºç¡€æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ¸…æ™°çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02503v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºPtychoBenchçš„å¤šæ¨¡æ€å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°æ—¨åœ¨è§£å†³å…ˆè¿›æ˜¾å¾®é•œå·¥ä½œæµç¨‹è‡ªåŠ¨åŒ–ä¸­é¢ä¸´çš„æ¨¡å‹åŸŸé€‚åº”æ€§é—®é¢˜ã€‚é€šè¿‡è¯¥å¹³å°ï¼Œæœ¬æ–‡å¯¹æ¯”äº†ä¸¤ç§ä¸“ä¸šåŒ–ç­–ç•¥ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚åœ¨è§†è§‰ä¼ªç‰©æ£€æµ‹ä»»åŠ¡å’Œæ–‡æœ¬å‚æ•°æ¨èä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€ä½³çš„ä¸“ä¸šåŒ–è·¯å¾„ä¾èµ–äºä»»åŠ¡ç‰¹æ€§ã€‚è§†è§‰ä»»åŠ¡ä¸­ï¼ŒSFTå’ŒICLç›¸äº’è¡¥å……ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç¤ºä¾‹å¼•å¯¼å¾®è°ƒæ¨¡å‹å–å¾—æœ€é«˜å¹³å‡æ€§èƒ½ï¼ˆMicro-F1ä¸º0.728ï¼‰ã€‚è€Œæ–‡æœ¬ä»»åŠ¡ä¸­ï¼Œåœ¨å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ä¸Šè¿›è¡ŒICLæ˜¯æ›´ä¼˜ç­–ç•¥ï¼Œè¾¾åˆ°å³°å€¼Micro-F1ä¸º0.847ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„â€œè¶…çº§ä¸“å®¶â€SFTæ¨¡å‹ï¼ˆé›¶æ ·æœ¬Micro-F1ä¸º0.839ï¼‰ã€‚ç ”ç©¶è¿˜ç¡®è®¤äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥æç¤ºçš„ä¼˜åŠ¿ï¼Œå¹¶å‘ç°äº†å¾®è°ƒæ¨¡å‹ä¸­ä¸€è‡´æ€§çš„ä¸Šä¸‹æ–‡å¹²æ‰°ç°è±¡ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹æ¯”äº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒåŸºäºDINOv3çš„åˆ†ç±»å™¨ï¼Œä¸ºç§‘å­¦äººå·¥æ™ºèƒ½æä¾›äº†å…³é”®è§‚å¯Ÿï¼šåœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ€ä½³çš„ä¸“ä¸šåŒ–è·¯å¾„ä¾èµ–äºä»»åŠ¡æ¨¡å¼æ€ï¼Œä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„ç§‘å­¦åŸºç¡€ä»£ç†ç³»ç»Ÿæä¾›äº†æ¸…æ™°æ¡†æ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–å…ˆè¿›æ˜¾å¾®é•œçš„å·¥ä½œæµç¨‹æ˜¯ä¸€ä¸ªå…³é”®ç›®æ ‡ï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ç­‰åŸºç¡€æ¨¡å‹å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>é€‚åº”è¿™äº›é€šç”¨æ¨¡å‹äºç‰¹å®šç§‘å­¦ä»»åŠ¡æ˜¯è‡³å…³é‡è¦çš„ï¼Œä½†æœ€ä½³åŸŸé€‚åº”ç­–ç•¥å°šä¸æ¸…æ¥šã€‚</li>
<li>å¼•å…¥PtychoBenchä½œä¸ºå¤šæ¨¡æ€å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•å¹³å°ç”¨äºè§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>å¯¹æ¯”äº†ä¸¤ç§ä¸“ä¸šåŒ–ç­–ç•¥ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚</li>
<li>è§†è§‰ä¼ªç‰©æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒSFTå’ŒICLç»“åˆè¡¨ç°æœ€ä½³ï¼ˆMicro-F1ä¸º0.728ï¼‰ã€‚</li>
<li>æ–‡æœ¬å‚æ•°æ¨èä»»åŠ¡ä¸­ï¼Œå¤§æ¨¡å‹çš„ICLç­–ç•¥æ›´ä¼˜è¶Šï¼ˆMicro-F1å³°å€¼è¾¾åˆ°0.847ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00684ad5dba0c9b42d91cb489357e2ba" align="middle">
<img src="https://picx.zhimg.com/v2-df51bdc16d1c14eebb307a22c942e0dc" align="middle">
<img src="https://picx.zhimg.com/v2-b5df44c4c66ab2cc984975678e7db9a2" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Wavelet-Optimized-Motion-Artifact-Correction-in-3D-MRI-Using-Pre-trained-2D-Score-Priors"><a href="#Wavelet-Optimized-Motion-Artifact-Correction-in-3D-MRI-Using-Pre-trained-2D-Score-Priors" class="headerlink" title="Wavelet-Optimized Motion Artifact Correction in 3D MRI Using Pre-trained   2D Score Priors"></a>Wavelet-Optimized Motion Artifact Correction in 3D MRI Using Pre-trained   2D Score Priors</h2><p><strong>Authors:Genyuan Zhang, Xuyang Duan, Songtao Zhu, Ao Wang, Fenglin Liu</strong></p>
<p>Motion artifacts in magnetic resonance imaging (MRI) remain a major challenge, as they degrade image quality and compromise diagnostic reliability. Score-based generative models (SGMs) have recently shown promise for artifact removal. However, existing 3D SGM-based approaches are limited in two key aspects: (1) their strong dependence on known forward operators makes them ineffective for correcting MRI motion artifacts, and (2) their slow inference speed hinders clinical translation. To overcome these challenges, we propose a wavelet-optimized end-to-end framework for 3D MRI motion correct using pre-trained 2D score priors (3D-WMoCo). Specifically, two orthogonal 2D score priors are leveraged to guide the 3D distribution prior, while a mean-reverting stochastic differential equation (SDE) is employed to model the restoration process of motion-corrupted 3D volumes to motion-free 3D distribution. Furthermore, wavelet diffusion is introduced to accelerate inference, and wavelet convolution is applied to enhance feature extraction. We validate the effectiveness of our approach through both simulated motion artifact experiments and real-world clinical motion artifact correction tests. The proposed method achieves robust performance improvements over existing techniques. Implementation details and source code are available at: <a target="_blank" rel="noopener" href="https://github.com/ZG-yuan/3D-WMoCo">https://github.com/ZG-yuan/3D-WMoCo</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„è¿åŠ¨ä¼ªå½±ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ä¼šé™ä½å›¾åƒè´¨é‡å¹¶å½±å“è¯Šæ–­çš„å¯é æ€§ã€‚åŸºäºè¯„åˆ†çš„ç”Ÿæˆæ¨¡å‹ï¼ˆSGMsï¼‰æœ€è¿‘åœ¨å»é™¤ä¼ªå½±æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„3D SGMæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰å®ƒä»¬å¯¹å·²çŸ¥å‰å‘ç®—å­çš„å¼ºçƒˆä¾èµ–ä½¿å®ƒä»¬æ— æ³•æœ‰æ•ˆåœ°æ ¡æ­£MRIè¿åŠ¨ä¼ªå½±ï¼›ï¼ˆ2ï¼‰å…¶ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦é˜»ç¢äº†ä¸´åºŠç¿»è¯‘ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„2Dè¯„åˆ†å…ˆéªŒçŸ¥è¯†ï¼ˆ3D-WMoCoï¼‰è¿›è¡Œ3D MRIè¿åŠ¨æ ¡æ­£çš„å°æ³¢ä¼˜åŒ–ç«¯åˆ°ç«¯æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œåˆ©ç”¨ä¸¤ä¸ªæ­£äº¤2Dè¯„åˆ†å…ˆéªŒæ¥å¼•å¯¼3Dåˆ†å¸ƒå…ˆéªŒï¼Œå¹¶é‡‡ç”¨å‡å€¼å›å½’éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰å¯¹è¿åŠ¨æŸåçš„3Dä½“ç§¯çš„æ¢å¤è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ï¼Œä»¥å¾—åˆ°æ— è¿åŠ¨çš„3Dåˆ†å¸ƒã€‚æ­¤å¤–ï¼Œå¼•å…¥å°æ³¢æ‰©æ•£ä»¥åŠ é€Ÿæ¨ç†ï¼Œåº”ç”¨å°æ³¢å·ç§¯ä»¥å¢å¼ºç‰¹å¾æå–ã€‚æˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿè¿åŠ¨ä¼ªå½±å®éªŒå’Œç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠè¿åŠ¨ä¼ªå½±æ ¡æ­£æµ‹è¯•éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•åœ¨ç°æœ‰æŠ€æœ¯ä¸Šå®ç°äº†ç¨³å¥çš„æ€§èƒ½æå‡ã€‚å®æ–½ç»†èŠ‚å’Œæºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ZG-yuan/3D-WMoCo%E3%80%82">https://github.com/ZG-yuan/3D-WMoCoã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02256v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå°æ³¢ä¼˜åŒ–çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼ˆ3D-WMoCoï¼‰ï¼Œç”¨äºåˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´åˆ†æ•°å…ˆéªŒå€¼æ ¡æ­£ä¸‰ç»´MRIè¿åŠ¨ä¼ªå½±ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªæ­£äº¤äºŒç»´åˆ†æ•°å…ˆéªŒå€¼æ¥å¼•å¯¼ä¸‰ç»´åˆ†å¸ƒå…ˆéªŒå€¼ï¼Œå¹¶é‡‡ç”¨å‡å€¼å›å½’éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰å¯¹è¿åŠ¨å¹²æ‰°çš„ä¸‰ç»´ä½“ç§¯è¿›è¡Œæ¢å¤å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œå¼•å…¥å°æ³¢æ‰©æ•£ä»¥åŠ é€Ÿæ¨ç†ï¼Œå¹¶é‡‡ç”¨å°æ³¢å·ç§¯å¢å¼ºç‰¹å¾æå–ã€‚å®éªŒéªŒè¯è¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸´åºŠè¿åŠ¨ä¼ªå½±æ ¡æ­£æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼ˆ3D-WMoCoï¼‰ç”¨äºæ ¡æ­£ä¸‰ç»´MRIä¸­çš„è¿åŠ¨ä¼ªå½±ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´åˆ†æ•°å…ˆéªŒå€¼æ¥å¼•å¯¼ä¸‰ç»´åˆ†å¸ƒå…ˆéªŒå€¼çš„å»ºæ¨¡ã€‚</li>
<li>é‡‡ç”¨å‡å€¼å›å½’éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰å¯¹è¿åŠ¨å¹²æ‰°çš„ä¸‰ç»´ä½“ç§¯è¿›è¡Œæ¢å¤å»ºæ¨¡ã€‚</li>
<li>å°æ³¢æ‰©æ•£è¢«å¼•å…¥ä»¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œæé«˜ä¸´åºŠåº”ç”¨çš„å®æ—¶æ€§ã€‚</li>
<li>å°æ³¢å·ç§¯å¢å¼ºäº†ç‰¹å¾æå–èƒ½åŠ›ï¼Œæé«˜äº†è¿åŠ¨ä¼ªå½±æ ¡æ­£çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ¨¡æ‹Ÿå’ŒçœŸå®ä¸´åºŠæµ‹è¯•éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>æ–‡ç« æä¾›äº†è¯¦ç»†çš„å®ç°ç»†èŠ‚å’Œæºä»£ç ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e89d7bc1dc618a431d2a0ee2f11d1e91" align="middle">
<img src="https://picx.zhimg.com/v2-05e0aee5b51306a96bbe2578b51a33bd" align="middle">
<img src="https://picx.zhimg.com/v2-b14ffda30b8e80e06b4dff6a02c841bf" align="middle">
<img src="https://picx.zhimg.com/v2-8ad63262047f2720efb1e48fa3f3d2b9" align="middle">
<img src="https://picx.zhimg.com/v2-3c2513b3fb4160498f426d6fb62516fb" align="middle">
<img src="https://picx.zhimg.com/v2-d05f1b920bd399df99c52fab59c7995f" align="middle">
<img src="https://picx.zhimg.com/v2-950fa4752c4dff3d62f8f8b206439954" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Language-Enhanced-Generative-Modeling-for-PET-Synthesis-from-MRI-and-Blood-Biomarkers"><a href="#Language-Enhanced-Generative-Modeling-for-PET-Synthesis-from-MRI-and-Blood-Biomarkers" class="headerlink" title="Language-Enhanced Generative Modeling for PET Synthesis from MRI and   Blood Biomarkers"></a>Language-Enhanced Generative Modeling for PET Synthesis from MRI and   Blood Biomarkers</h2><p><strong>Authors:Zhengjie Zhang, Xiaoxie Mao, Qihao Guo, Shaoting Zhang, Qi Huang, Mu Zhou, Fang Xie, Mianxin Liu</strong></p>
<p>Background: Alzheimerâ€™s disease (AD) diagnosis heavily relies on amyloid-beta positron emission tomography (Abeta-PET), which is limited by high cost and limited accessibility. This study explores whether Abeta-PET spatial patterns can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566 participants. A language-enhanced generative model, driven by a large language model (LLM) and multimodal information fusion, was developed to synthesize PET images. Synthesized images were evaluated for image quality, diagnostic consistency, and clinical applicability within a fully automated diagnostic pipeline. Findings: The synthetic PET images closely resemble real PET scans in both structural details (SSIM &#x3D; 0.920 +&#x2F;- 0.003) and regional patterns (Pearsonâ€™s r &#x3D; 0.955 +&#x2F;- 0.007). Diagnostic outcomes using synthetic PET show high agreement with real PET-based diagnoses (accuracy &#x3D; 0.80). Using synthetic PET, we developed a fully automatic AD diagnostic pipeline integrating PET synthesis and classification. The synthetic PET-based model (AUC &#x3D; 0.78) outperforms T1-based (AUC &#x3D; 0.68) and BBM-based (AUC &#x3D; 0.73) models, while combining synthetic PET and BBMs further improved performance (AUC &#x3D; 0.79). Ablation analysis supports the advantages of LLM integration and prompt engineering. Interpretation: Our language-enhanced generative model synthesizes realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial pattern assessment and improving the diagnostic workflow for Alzheimerâ€™s disease. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„è¯Šæ–­ä¸¥é‡ä¾èµ–äºæ·€ç²‰æ ·è›‹ç™½Î²æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆAbeta-PETï¼‰ï¼Œä½†ç”±äºæˆæœ¬é«˜æ˜‚å’Œå¯åŠæ€§æœ‰é™è€Œå—åˆ°é™åˆ¶ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶Abeta-PETç©ºé—´æ¨¡å¼æ˜¯å¦èƒ½é€šè¿‡è¡€æ¶²ç”Ÿç‰©æ ‡å¿—ç‰©ï¼ˆBBMsï¼‰å’Œæ ¸ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰é¢„æµ‹ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ä»566åå‚ä¸è€…ä¸­æ”¶é›†äº†Abeta-PETå›¾åƒã€T1åŠ æƒMRIæ‰«æå’ŒBBMsæ•°æ®ã€‚å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€ä¿¡æ¯èåˆæŠ€æœ¯ï¼Œå¼€å‘äº†ä¸€ç§è¯­è¨€å¢å¼ºç”Ÿæˆæ¨¡å‹ï¼Œä»¥åˆæˆPETå›¾åƒã€‚è¯„ä¼°åˆæˆå›¾åƒåœ¨å›¾åƒè´¨é‡ã€è¯Šæ–­ä¸€è‡´æ€§å’Œä¸´åºŠé€‚ç”¨æ€§æ–¹é¢çš„è¡¨ç°ï¼Œå¹¶å°†å…¶çº³å…¥å…¨è‡ªåŠ¨è¯Šæ–­æµç¨‹ä¸­ã€‚ç»“æœï¼šåˆæˆPETå›¾åƒåœ¨ç»“æ„ç»†èŠ‚ï¼ˆSSIM &#x3D; 0.920 Â± 0.003ï¼‰å’ŒåŒºåŸŸæ¨¡å¼ï¼ˆPearson r &#x3D; 0.955 Â± 0.007ï¼‰ä¸Šç´§å¯†æ¨¡æ‹ŸçœŸå®PETæ‰«æã€‚ä½¿ç”¨åˆæˆPETçš„è¯Šæ–­ç»“æœä¸åŸºäºçœŸå®PETçš„è¯Šæ–­ç»“æœé«˜åº¦ä¸€è‡´ï¼ˆå‡†ç¡®åº¦&#x3D; 0.80ï¼‰ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨è‡ªåŠ¨ADè¯Šæ–­æµç¨‹ï¼Œè¯¥æµç¨‹é›†æˆäº†PETåˆæˆå’Œåˆ†ç±»ã€‚åŸºäºåˆæˆPETçš„æ¨¡å‹ï¼ˆAUC &#x3D; 0.78ï¼‰çš„è¡¨ç°ä¼˜äºåŸºäºT1ï¼ˆAUC &#x3D; 0.68ï¼‰å’ŒåŸºäºBBMsï¼ˆAUC &#x3D; 0.73ï¼‰çš„æ¨¡å‹ï¼ŒåŒæ—¶ç»“åˆåˆæˆPETå’ŒBBMså¯è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼ˆAUC &#x3D; 0.79ï¼‰ã€‚æ¶ˆèåˆ†ææ”¯æŒäº†LLMé›†æˆå’Œå³æ—¶å·¥ç¨‹åŒ–çš„ä¼˜åŠ¿ã€‚è§£è¯»ï¼šæˆ‘ä»¬çš„è¯­è¨€å¢å¼ºç”Ÿæˆæ¨¡å‹èƒ½åˆæˆé€¼çœŸçš„PETå›¾åƒï¼Œæé«˜äº†MRIå’ŒBBMsåœ¨è¯„ä¼°Abetaç©ºé—´æ¨¡å¼æ–¹é¢çš„å®ç”¨æ€§ï¼Œå¹¶æ”¹å–„äº†é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¯Šæ–­æµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02206v1">PDF</a> 31 pages, 8 figures</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢ç´¢äº†åŸºäºè¡€æ¶²ç”Ÿç‰©æ ‡å¿—ç‰©ï¼ˆBBMsï¼‰å’ŒMRIæ‰«æé¢„æµ‹æ·€ç²‰æ ·è›‹ç™½Î²æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆAbeta-PETï¼‰ç©ºé—´æ¨¡å¼çš„å¯èƒ½æ€§ã€‚å¼€å‘äº†ä¸€ç§ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€ä¿¡æ¯èåˆé©±åŠ¨çš„ç”Ÿæˆæ¨¡å‹ï¼ŒåˆæˆPETå›¾åƒï¼Œå¹¶è¿›è¡Œäº†å›¾åƒè´¨é‡ã€è¯Šæ–­ä¸€è‡´æ€§å’Œä¸´åºŠé€‚ç”¨æ€§çš„è¯„ä¼°ã€‚åˆæˆPETå›¾åƒä¸çœŸå®PETæ‰«æåœ¨ç»“æ„ç»†èŠ‚å’ŒåŒºåŸŸæ¨¡å¼ä¸Šé«˜åº¦ç›¸ä¼¼ï¼Œä½¿ç”¨åˆæˆPETçš„è¯Šæ–­ç»“æœä¸çœŸå®PETçš„è¯Šæ–­ç»“æœé«˜åº¦ä¸€è‡´ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†ä¸€ä¸ªå…¨è‡ªåŠ¨çš„åŸºäºåˆæˆPETçš„é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­æµç¨‹ï¼Œå…¶æ€§èƒ½ä¼˜äºåŸºäºT1å’ŒBBMsçš„æ¨¡å‹ï¼Œä¸”ç»“åˆåˆæˆPETå’ŒBBMså¯è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨è§£å†³é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­ä¸­Abeta-PETçš„é«˜æˆæœ¬å’Œæœ‰é™çš„å¯è®¿é—®æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€ä¿¡æ¯èåˆå¼€å‘äº†ä¸€ç§ç”Ÿæˆæ¨¡å‹æ¥åˆæˆPETå›¾åƒã€‚</li>
<li>åˆæˆPETå›¾åƒåœ¨å›¾åƒè´¨é‡ã€è¯Šæ–­ä¸€è‡´æ€§å’Œä¸´åºŠé€‚ç”¨æ€§æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>åˆæˆPETå›¾åƒä¸çœŸå®PETæ‰«æåœ¨ç»“æ„å’ŒåŒºåŸŸæ¨¡å¼ä¸Šå…·æœ‰é«˜åº¦çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>ä½¿ç”¨åˆæˆPETçš„è¯Šæ–­ç»“æœä¸çœŸå®PETçš„è¯Šæ–­ç»“æœé«˜åº¦ä¸€è‡´ã€‚</li>
<li>åŸºäºåˆæˆPETçš„è‡ªåŠ¨è¯Šæ–­æµç¨‹åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…è¯Šæ–­ä¸­å…·æœ‰ä¼˜è¶Šæ€§ï¼Œä¼˜äºåŸºäºT1å’ŒBBMsçš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73b7567c008259bb419d27b04e2c0acc" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Spin-Coating-Homogeneous-All-Inorganic-Perovskite-Films-via-High-Pressure-Recrystallization"><a href="#Beyond-Spin-Coating-Homogeneous-All-Inorganic-Perovskite-Films-via-High-Pressure-Recrystallization" class="headerlink" title="Beyond Spin Coating: Homogeneous All-Inorganic Perovskite Films via   High-Pressure Recrystallization"></a>Beyond Spin Coating: Homogeneous All-Inorganic Perovskite Films via   High-Pressure Recrystallization</h2><p><strong>Authors:Trong Tam Nguyen, JosÃ© Penuelas, Aziz Benamrouche, CÃ©line Chevalier, Thi Kim Anh Hoang, GaÃ«lle TrippÃ©-Allard, Elsa Cassette, Brice Devif, Emmanuel Drouard, Emmanuelle Deleporte, Hong Hanh Mai, Abdelaziz Bouazizi, Christian Seassal, Hai Son Nguyen</strong></p>
<p>Metal halide perovskites are promising materials for optoelectronic applications owing to their outstanding optical and electronic properties. Among them, all-inorganic perovskites such as CsPbBr$_3$ offer superior thermal and chemical stability. However, obtaining high-quality CsPbBr$_3$ thin films via solution processing remains challenging due to the precursorâ€™s low solubility, and current additive or solvent engineering strategies are often complex and poorly reproducible. High-pressure recrystallization has recently emerged as a promising route to improve film quality, yet its impact on film properties remains insufficiently explored. Here, we systematically investigate the morphological, structural, and optical properties of CsPbBr$_3$ thin films prepared by high-pressure recrystallization, in comparison with standard non-recrystallized films. Optimized recrystallization at 300 bar produces smooth, pinhole-free, single-phase 3D perovskite layers with sub-nanometer roughness, while the film thickness is precisely tunable via precursor concentration. The process enhances both grain and crystallite sizes, leading to amplified spontaneous emission with a reduced excitation threshold and improved photostability. Temperature-dependent X-ray diffraction further reveals the orthorhombicâ€“tetragonalâ€“cubic phase transition, consistent with single-crystal behavior. This study provides fundamental insights into pressure-driven recrystallization and establishes a reproducible, scalable approach for fabricating high-quality CsPbBr$_3$ films for optoelectronic devices. </p>
<blockquote>
<p>å¤åŒ–ç‰©é’™é’›çŸ¿ææ–™å› å…¶å“è¶Šçš„å…‰å­¦å’Œç”µå­ç‰¹æ€§è€Œæˆä¸ºå…‰ç”µå­åº”ç”¨ä¸­çš„æœ‰å‰é€”çš„ææ–™ã€‚å…¶ä¸­ï¼Œå…¨æ— æœºé’™é’›çŸ¿å¦‚CsPbBr3å…·æœ‰å‡ºè‰²çš„çƒ­å’ŒåŒ–å­¦ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œé€šè¿‡æº¶æ¶²å¤„ç†è·å¾—é«˜è´¨é‡çš„CsPbBr3è–„è†œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™æ˜¯ç”±äºå‰é©±ä½“çš„æº¶è§£åº¦ä½ï¼Œå¹¶ä¸”å½“å‰çš„æ·»åŠ å‰‚æˆ–æº¶å‰‚å·¥ç¨‹ç­–ç•¥é€šå¸¸å¤æ‚ä¸”é‡ç°æ€§å·®ã€‚é«˜å‹å†ç»“æ™¶æœ€è¿‘è¢«è®¤ä¸ºæ˜¯æé«˜è–„è†œè´¨é‡çš„æœ‰å‰é€”çš„é€”å¾„ï¼Œç„¶è€Œå®ƒå¯¹è–„è†œæ€§èƒ½çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†é€šè¿‡é«˜å‹å†ç»“æ™¶åˆ¶å¤‡çš„CsPbBr3è–„è†œçš„å½¢æ€å­¦ã€ç»“æ„å’Œå…‰å­¦æ€§èƒ½ï¼Œå¹¶ä¸æ ‡å‡†çš„éå†ç»“æ™¶è–„è†œè¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨300å·´ä¸‹ä¼˜åŒ–çš„å†ç»“æ™¶äº§ç”Ÿäº†å…‰æ»‘ã€æ— é’ˆå­”ã€å•ç›¸çš„3Dé’™é’›çŸ¿å±‚ï¼Œç²—ç³™åº¦åœ¨äºšçº³ç±³èŒƒå›´å†…ï¼ŒåŒæ—¶å¯ä»¥é€šè¿‡å‰é©±ä½“æµ“åº¦ç²¾ç¡®è°ƒèŠ‚è–„è†œåšåº¦ã€‚è¯¥è¿‡ç¨‹å¢åŠ äº†æ™¶ç²’å’Œå¾®æ™¶çš„å°ºå¯¸ï¼Œä»è€Œå¢å¼ºäº†è‡ªå‘å‘å°„ï¼Œé™ä½äº†æ¿€å‘é˜ˆå€¼å¹¶æ”¹å–„äº†å…‰ç¨³å®šæ€§ã€‚æ¸©åº¦ä¾èµ–çš„Xå°„çº¿è¡å°„è¿›ä¸€æ­¥æ­ç¤ºäº†æ­£äº¤-å››æ£±-ç«‹æ–¹ç›¸å˜ï¼Œè¿™ä¸å•æ™¶è¡Œä¸ºä¸€è‡´ã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†å…³äºå‹åŠ›é©±åŠ¨å†ç»“æ™¶çš„åŸºæœ¬è§è§£ï¼Œå¹¶å»ºç«‹äº†ä¸€ç§å¯é‡å¤ã€å¯æ‰©å±•çš„æ–¹æ³•æ¥åˆ¶é€ ç”¨äºå…‰ç”µå­å™¨ä»¶çš„é«˜è´¨é‡CsPbBr3è–„è†œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02177v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é‡‘å±å¤åŒ–ç‰©é’™é’›çŸ¿å› å…¶ä¼˜å¼‚çš„å…‰å­¦å’Œç”µå­æ€§èƒ½è€Œæˆä¸ºå…‰ç”µå­åº”ç”¨çš„æœ‰å‰é€”çš„ææ–™ã€‚å…¶ä¸­ï¼Œå…¨æ— æœºé’™é’›çŸ¿å¦‚CsPbBr3å…·æœ‰ä¼˜è¶Šçš„çƒ­å’ŒåŒ–å­¦ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œé€šè¿‡æº¶æ¶²å¤„ç†è·å¾—é«˜è´¨é‡çš„CsPbBr3è–„è†œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå‰é©±ä½“çš„æº¶è§£åº¦ä½ï¼Œç›®å‰çš„æ·»åŠ å‰‚æˆ–æº¶å‰‚å·¥ç¨‹ç­–ç•¥é€šå¸¸å¤æ‚ä¸”é‡ç°æ€§å·®ã€‚é«˜å‹å†ç»“æ™¶æœ€è¿‘è¢«è¯æ˜æ˜¯æé«˜è–„è†œè´¨é‡çš„æœ‰å‰é€”çš„é€”å¾„ï¼Œä½†å…¶å¯¹è–„è†œæ€§èƒ½çš„å½±å“ä»æ¢ç´¢ä¸è¶³ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†é€šè¿‡é«˜å‹å†ç»“æ™¶åˆ¶å¤‡çš„CsPbBr3è–„è†œçš„å½¢æ€å­¦ã€ç»“æ„å’Œå…‰å­¦æ€§èƒ½ï¼Œå¹¶ä¸æ ‡å‡†çš„éå†ç»“æ™¶è–„è†œè¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨300barä¸‹ä¼˜åŒ–çš„å†ç»“æ™¶äº§ç”Ÿäº†å…‰æ»‘ã€æ— é’ˆå­”ã€å•ç›¸çš„3Dé’™é’›çŸ¿å±‚ï¼Œç²—ç³™åº¦è¾¾äºšçº³ç±³çº§ï¼Œè€Œè–„è†œåšåº¦å¯é€šè¿‡å‰é©±ä½“æµ“åº¦ç²¾ç¡®è°ƒèŠ‚ã€‚è¯¥è¿‡ç¨‹æé«˜äº†æ™¶ç²’å’Œå¾®æ™¶çš„å°ºå¯¸ï¼Œå¯¼è‡´è‡ªå‘å‘å°„å¢å¼ºï¼Œæ¿€å‘é˜ˆå€¼é™ä½ï¼Œå…‰ç¨³å®šæ€§æé«˜ã€‚æ¸©åº¦ä¾èµ–çš„Xå°„çº¿è¡å°„è¿›ä¸€æ­¥æ­ç¤ºäº†æ­£äº¤-å››æ£±-ç«‹æ–¹ç›¸å˜ï¼Œä¸å•æ™¶è¡Œä¸ºä¸€è‡´ã€‚æœ¬ç ”ç©¶ä¸ºå‹åŠ›é©±åŠ¨å†ç»“æ™¶æä¾›äº†åŸºç¡€è§è§£ï¼Œå¹¶å»ºç«‹äº†ä¸€ç§å¯é‡å¤ã€å¯è§„æ¨¡åŒ–çš„æ–¹æ³•æ¥åˆ¶é€ ç”¨äºå…‰ç”µå­å™¨ä»¶çš„é«˜è´¨é‡CsPbBr3è–„è†œã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é‡‘å±å¤åŒ–ç‰©é’™é’›çŸ¿ï¼Œå¦‚CsPbBr3ï¼Œåœ¨å…‰ç”µå­åº”ç”¨æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å…¶æº¶æ¶²å¤„ç†è¿‡ç¨‹ä¸­é¢ä¸´å‰é©±ä½“æº¶è§£åº¦ä½å’Œåˆ¶å¤‡é«˜è´¨é‡è–„è†œçš„æŒ‘æˆ˜ã€‚</li>
<li>é«˜å‹å†ç»“æ™¶è¢«è¯æ˜æ˜¯æé«˜CsPbBr3è–„è†œè´¨é‡çš„æœ‰æ•ˆé€”å¾„ã€‚</li>
<li>åœ¨300barä¸‹ä¼˜åŒ–çš„å†ç»“æ™¶äº§ç”Ÿçš„è–„è†œè¡¨ç°å‡ºå¹³æ»‘ã€æ— é’ˆå­”ã€å•ç›¸çš„3Dé’™é’›çŸ¿å±‚ç‰¹æ€§ï¼Œä¸”è–„è†œåšåº¦å¯é€šè¿‡å‰é©±ä½“æµ“åº¦è°ƒèŠ‚ã€‚</li>
<li>é«˜å‹å†ç»“æ™¶å¢å¼ºäº†æ™¶ç²’å’Œå¾®æ™¶çš„å°ºå¯¸ï¼Œæ”¹å–„äº†è–„è†œçš„å…‰å­¦æ€§èƒ½ï¼ŒåŒ…æ‹¬è‡ªå‘å‘å°„ã€æ¿€å‘é˜ˆå€¼å’Œå…‰ç¨³å®šæ€§ã€‚</li>
<li>æ¸©åº¦ä¾èµ–çš„Xå°„çº¿è¡å°„ç ”ç©¶æ­ç¤ºäº†CsPbBr3è–„è†œåœ¨é«˜å‹å†ç»“æ™¶è¿‡ç¨‹ä¸­çš„ç›¸å˜è¡Œä¸ºï¼Œä¸å•æ™¶æ€§è´¨ç›¸ç¬¦ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºå‹åŠ›é©±åŠ¨å†ç»“æ™¶æä¾›äº†æ·±å…¥ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-746afda71dfd4041536444167295ef78" align="middle">
<img src="https://picx.zhimg.com/v2-ccef4c01a1fe3d652d9bbcdd5efa6013" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MicroAUNet-Boundary-Enhanced-Multi-scale-Fusion-with-Knowledge-Distillation-for-Colonoscopy-Polyp-Image-Segmentation"><a href="#MicroAUNet-Boundary-Enhanced-Multi-scale-Fusion-with-Knowledge-Distillation-for-Colonoscopy-Polyp-Image-Segmentation" class="headerlink" title="MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge   Distillation for Colonoscopy Polyp Image Segmentation"></a>MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge   Distillation for Colonoscopy Polyp Image Segmentation</h2><p><strong>Authors:Ziyi Wang, Yuanmei Zhang, Dorna Esrafilzadeh, Ali R. Jalili, Suncheng Xiang</strong></p>
<p>Early and accurate segmentation of colorectal polyps is critical for reducing colorectal cancer mortality, which has been extensively explored by academia and industry. However, current deep learning-based polyp segmentation models either compromise clinical decision-making by providing ambiguous polyp margins in segmentation outputs or rely on heavy architectures with high computational complexity, resulting in insufficient inference speeds for real-time colorectal endoscopic applications. To address this problem, we propose MicroAUNet, a light-weighted attention-based segmentation network that combines depthwise-separable dilated convolutions with a single-path, parameter-shared channel-spatial attention block to strengthen multi-scale boundary features. On the basis of it, a progressive two-stage knowledge-distillation scheme is introduced to transfer semantic and boundary cues from a high-capacity teacher. Extensive experiments on benchmarks also demonstrate the state-of-the-art accuracy under extremely low model complexity, indicating that MicroAUNet is suitable for real-time clinical polyp segmentation. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/JeremyXSC/MicroAUNet">https://github.com/JeremyXSC/MicroAUNet</a>. </p>
<blockquote>
<p>æ—©æœŸä¸”å‡†ç¡®çš„ç»“è‚ æ¯è‚‰åˆ†å‰²å¯¹äºé™ä½ç»“è‚ ç™Œæ­»äº¡ç‡è‡³å…³é‡è¦ï¼Œå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¯¹æ­¤è¿›è¡Œäº†å¹¿æ³›æ¢ç´¢ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ¯è‚‰åˆ†å‰²æ¨¡å‹è¦ä¹ˆåœ¨åˆ†å‰²ç»“æœä¸­æä¾›æ¨¡ç³Šçš„æ¯è‚‰è¾¹ç•Œï¼Œä»è€Œå½±å“ä¸´åºŠå†³ç­–ï¼Œè¦ä¹ˆä¾èµ–äºè®¡ç®—å¤æ‚åº¦é«˜çš„å¤æ‚æ¶æ„ï¼Œå¯¼è‡´å®æ—¶ç»“è‚ å†…çª¥é•œåº”ç”¨çš„æ¨ç†é€Ÿåº¦ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MicroAUNetï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„åŸºäºæ³¨æ„åŠ›çš„åˆ†å‰²ç½‘ç»œï¼Œå®ƒç»“åˆäº†æ·±åº¦å¯åˆ†ç¦»è†¨èƒ€å·ç§¯å’Œå•è·¯å¾„ã€å‚æ•°å…±äº«é€šé“ç©ºé—´æ³¨æ„åŠ›å—ï¼Œä»¥å¼ºåŒ–å¤šå°ºåº¦è¾¹ç•Œç‰¹å¾ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†ä¸€ç§æ¸è¿›çš„ä¸¤é˜¶æ®µçŸ¥è¯†è’¸é¦æ–¹æ¡ˆï¼Œä»¥ä»é«˜æ€§èƒ½æ•™å¸ˆæ¨¡å‹ä¸­è½¬ç§»è¯­ä¹‰å’Œè¾¹ç•Œçº¿ç´¢ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒä¹Ÿè¯æ˜äº†åœ¨æ¨¡å‹å¤æ‚åº¦æä½çš„æƒ…å†µä¸‹ï¼Œå…¶å‡†ç¡®æ€§è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè¡¨æ˜MicroAUNeté€‚åˆç”¨äºå®æ—¶ä¸´åºŠæ¯è‚‰åˆ†å‰²ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/JeremyXSC/MicroAUNet%E3%80%82">https://github.com/JeremyXSC/MicroAUNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01143v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å®æ—¶ä¸´åºŠç»“è‚ æ¯è‚‰åˆ†å‰²çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§è½»é‡çº§æ³¨æ„åŠ›åŸºç¡€çš„åˆ†å‰²ç½‘ç»œMicroAUNetã€‚å®ƒé€šè¿‡ç»“åˆæ·±åº¦å¯åˆ†ç¦»è†¨èƒ€å·ç§¯å’Œå•è·¯å¾„å‚æ•°å…±äº«é€šé“ç©ºé—´æ³¨æ„åŠ›å—ï¼Œå¼ºåŒ–äº†å¤šå°ºåº¦è¾¹ç•Œç‰¹å¾ã€‚åŒæ—¶å¼•å…¥äº†ä¸€ç§æ¸è¿›çš„ä¸¤é˜¶æ®µçŸ¥è¯†è’¸é¦æ–¹æ¡ˆï¼Œä»é«˜æ€§èƒ½çš„æ•™å¸ˆæ¨¡å‹ä¸­è½¬ç§»è¯­ä¹‰å’Œè¾¹ç•Œçº¿ç´¢ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMicroAUNetåœ¨æ¨¡å‹å¤æ‚åº¦æä½çš„æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œé€‚åˆç”¨äºå®æ—¶ä¸´åºŠæ¯è‚‰åˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MicroAUNetè§£å†³äº†å®æ—¶ä¸´åºŠç»“è‚ æ¯è‚‰åˆ†å‰²ä¸­çš„å…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬æ¨¡ç³Šçš„å¤šè¾¹å½¢è¾¹ç•Œå’Œè®¡ç®—å¤æ‚æ€§é«˜çš„é—®é¢˜ã€‚</li>
<li>MicroAUNetç»“åˆäº†æ·±åº¦å¯åˆ†ç¦»è†¨èƒ€å·ç§¯å’Œå•è·¯å¾„å‚æ•°å…±äº«é€šé“ç©ºé—´æ³¨æ„åŠ›å—æŠ€æœ¯ï¼Œå¼ºåŒ–äº†å¤šå°ºåº¦è¾¹ç•Œç‰¹å¾ã€‚</li>
<li>è¯¥ç½‘ç»œé€šè¿‡æ¸è¿›çš„ä¸¤é˜¶æ®µçŸ¥è¯†è’¸é¦æ–¹æ¡ˆï¼Œä»é«˜æ€§èƒ½çš„æ•™å¸ˆæ¨¡å‹ä¸­è½¬ç§»è¯­ä¹‰å’Œè¾¹ç•Œçº¿ç´¢ã€‚</li>
<li>MicroAUNetåœ¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”æ¨¡å‹å¤æ‚åº¦æä½ã€‚</li>
<li>MicroAUNeté€‚ç”¨äºå®æ—¶ä¸´åºŠæ¯è‚‰åˆ†å‰²ï¼Œæœ‰åŠ©äºå‡å°‘ç»“è‚ ç™Œæ­»äº¡ç‡ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²å…¬å¼€å¯ç”¨ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8992fa30442e9649c31f56e9c618071d" align="middle">
<img src="https://picx.zhimg.com/v2-b79251611bf0f18b1e8d86e49b7a666d" align="middle">
<img src="https://picx.zhimg.com/v2-effccef7964822c183acee364d964518" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Knowledge-Elicitation-with-Large-Language-Models-for-Interpretable-Cancer-Stage-Identification-from-Pathology-Reports"><a href="#Knowledge-Elicitation-with-Large-Language-Models-for-Interpretable-Cancer-Stage-Identification-from-Pathology-Reports" class="headerlink" title="Knowledge Elicitation with Large Language Models for Interpretable   Cancer Stage Identification from Pathology Reports"></a>Knowledge Elicitation with Large Language Models for Interpretable   Cancer Stage Identification from Pathology Reports</h2><p><strong>Authors:Yeawon Lee, Christopher C. Yang, Chia-Hsuan Chang, Grace Lu-Yao</strong></p>
<p>Cancer staging is critical for patient prognosis and treatment planning, yet extracting pathologic TNM staging from unstructured pathology reports poses a persistent challenge. Existing natural language processing (NLP) and machine learning (ML) strategies often depend on large annotated datasets, limiting their scalability and adaptability. In this study, we introduce two Knowledge Elicitation methods designed to overcome these limitations by enabling large language models (LLMs) to induce and apply domain-specific rules for cancer staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses an iterative prompting strategy to derive staging rules directly from unannotated pathology reports, without requiring ground-truth labels. The second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG), employs a variation of RAG where rules are pre-extracted from relevant guidelines in a single step and then applied, enhancing interpretability and avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply broad knowledge learned during pre-training to new tasks. Using breast cancer pathology reports from the TCGA dataset, we evaluate their performance in identifying T and N stages, comparing them against various baseline approaches on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG achieves better performance when ZSCOT inference is less effective. Both methods offer transparent, interpretable interfaces by making the induced rules explicit. These findings highlight the promise of our Knowledge Elicitation methods as scalable, high-performing solutions for automated cancer staging with enhanced interpretability, particularly in clinical settings with limited annotated data. </p>
<blockquote>
<p>ç™Œç—‡åˆ†æœŸå¯¹æ‚£è€…çš„é¢„åå’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†ä»éç»“æ„åŒ–ç—…ç†æŠ¥å‘Šä¸­æå–ç—…ç†TNMåˆ†æœŸæ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç­–ç•¥é€šå¸¸ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§çŸ¥è¯†æå–æ–¹æ³•ï¼Œæ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿè¯±å¯¼å’Œåº”ç”¨ç™Œç—‡åˆ†æœŸçš„é¢†åŸŸç‰¹å®šè§„åˆ™ã€‚ç¬¬ä¸€ç§æ˜¯â€œåŸºäºé•¿æœŸè®°å¿†çš„çŸ¥è¯†æå–â€ï¼ˆKEwLTMï¼‰ï¼Œå®ƒä½¿ç”¨è¿­ä»£æç¤ºç­–ç•¥ç›´æ¥ä»æœªæ ‡æ³¨çš„ç—…ç†æŠ¥å‘Šä¸­æ¨å¯¼åˆ†æœŸè§„åˆ™ï¼Œæ— éœ€çœŸå®æ ‡ç­¾ã€‚ç¬¬äºŒç§æ˜¯â€œåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„çŸ¥è¯†æå–â€ï¼ˆKEwRAGï¼‰ï¼Œå®ƒé‡‡ç”¨RAGçš„å˜ä½“ï¼Œä¸€æ¬¡æ€§ä»ç›¸å…³æŒ‡å—ä¸­æå–è§„åˆ™ï¼Œç„¶ååº”ç”¨è¿™äº›è§„åˆ™ï¼Œæé«˜å¯è§£é‡Šæ€§ï¼Œé¿å…é‡å¤æ£€ç´¢å¼€é”€ã€‚æˆ‘ä»¬åˆ©ç”¨LLMå°†é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ åˆ°çš„å¹¿æ³›çŸ¥è¯†åº”ç”¨äºæ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨TCGAæ•°æ®é›†ä¸­çš„ä¹³è…ºç™Œç—…ç†æŠ¥å‘Šï¼Œè¯„ä¼°å®ƒä»¬åœ¨è¯†åˆ«Tå’ŒNé˜¶æ®µçš„èƒ½åŠ›ï¼Œä¸ä¸¤ç§å¼€æºLLMçš„å„ç§åŸºçº¿æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“é›¶å°„å‡»æ€ç»´é“¾ï¼ˆZSCOTï¼‰æ¨ç†æœ‰æ•ˆæ—¶ï¼ŒKEwLTMçš„è¡¨ç°ä¼˜äºKEwRAGï¼Œè€Œå½“ZSCOTæ¨ç†æ•ˆæœè¾ƒå·®æ—¶ï¼ŒKEwRAGçš„è¡¨ç°æ›´å¥½ã€‚ä¸¤ç§æ–¹æ³•éƒ½é€šè¿‡ä½¿è¯±å¯¼çš„è§„åˆ™æ˜ç¡®æ¥æä¾›é€æ˜ã€å¯è§£é‡Šçš„ç•Œé¢ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æˆ‘ä»¬çŸ¥è¯†æå–æ–¹æ³•ä½œä¸ºå¯ä¼¸ç¼©ã€é«˜æ€§èƒ½çš„è‡ªåŠ¨ç™Œç—‡åˆ†æœŸè§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ ‡æ³¨æ•°æ®çš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œå®ƒä»¬å…·æœ‰å¢å¼ºçš„å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01052v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸¤ç§çŸ¥è¯†æå–æ–¹æ³•â€”â€”é€šè¿‡é•¿æœŸè®°å¿†çš„çŸ¥è¯†æå–ï¼ˆKEwLTMï¼‰å’Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆçš„çŸ¥è¯†æå–ï¼ˆKEwRAGï¼‰ï¼Œç”¨äºä»éç»“æ„åŒ–ç—…ç†æŠ¥å‘Šä¸­æå–ç™Œç—‡TNMåˆ†æœŸä¿¡æ¯ã€‚è¿™ä¸¤ç§æ–¹æ³•å…‹æœäº†ç°æœ‰è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ ç­–ç•¥çš„å±€é™æ€§ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¯±å¯¼å’Œåº”ç”¨ç™Œç—‡åˆ†æœŸçš„ç‰¹å®šè§„åˆ™ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨é›¶å°„å‡»é“¾æ€ç»´ï¼ˆZSCOTï¼‰æ¨ç†æœ‰æ•ˆæ—¶ï¼ŒKEwLTMè¡¨ç°ä¼˜äºKEwRAGï¼Œè€Œåœ¨ZSCOTæ¨ç†ä¸é‚£ä¹ˆæœ‰æ•ˆæ—¶ï¼ŒKEwRAGè¡¨ç°æ›´å¥½ã€‚ä¸¤ç§æ–¹æ³•éƒ½æä¾›äº†é€æ˜çš„ã€å¯è§£é‡Šçš„ç•Œé¢ï¼Œä½¿è¯±å¯¼çš„è§„åˆ™å˜å¾—æ˜ç¡®ã€‚è¿™ä¸ºè‡ªåŠ¨åŒ–ç™Œç—‡åˆ†æœŸæä¾›äº†å¯æ‰©å±•ã€é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹æ³¨é‡Šæ•°æ®çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™Œç—‡åˆ†æœŸå¯¹äºæ‚£è€…é¢„åå’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>ä»éç»“æ„åŒ–ç—…ç†æŠ¥å‘Šä¸­æå–TNMåˆ†æœŸä¿¡æ¯æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>çŸ¥è¯†æå–æ–¹æ³•å¦‚KEwLTMå’ŒKEwRAGèƒ½å¤Ÿå…‹æœç°æœ‰NLPå’ŒMLç­–ç•¥çš„å±€é™æ€§ã€‚</li>
<li>LLMsèƒ½å¤Ÿé€šè¿‡è¯±å¯¼å’Œåº”ç”¨ç‰¹å®šè§„åˆ™è¿›è¡Œç™Œç—‡åˆ†æœŸã€‚</li>
<li>KEwLTMä½¿ç”¨è¿­ä»£æç¤ºç­–ç•¥ç›´æ¥ä»æœªæ³¨é‡Šçš„ç—…ç†æŠ¥å‘Šä¸­æ¨å¯¼åˆ†æœŸè§„åˆ™ï¼Œæ— éœ€çœŸå®æ ‡ç­¾ã€‚</li>
<li>KEwRAGé‡‡ç”¨ä»ç›¸å…³æŒ‡å—ä¸­æå–è§„åˆ™çš„æ–¹æ³•ï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§å¹¶é¿å…äº†é‡å¤æ£€ç´¢å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e28f3f09ff81d08b4313c51d2448554" align="middle">
<img src="https://picx.zhimg.com/v2-9e2e78c85ca81edb6763e1472695d61b" align="middle">
<img src="https://picx.zhimg.com/v2-013cbb7817e6e56d5961d44a849a5f36" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="OmniBrainBench-A-Comprehensive-Multimodal-Benchmark-for-Brain-Imaging-Analysis-Across-Multi-stage-Clinical-Tasks"><a href="#OmniBrainBench-A-Comprehensive-Multimodal-Benchmark-for-Brain-Imaging-Analysis-Across-Multi-stage-Clinical-Tasks" class="headerlink" title="OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging   Analysis Across Multi-stage Clinical Tasks"></a>OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging   Analysis Across Multi-stage Clinical Tasks</h2><p><strong>Authors:Zhihao Peng, Cheng Wang, Shengyuan Liu, Zhiying Liang, Yixuan Yuan</strong></p>
<p>Brain imaging analysis is vital for diagnosing and treating brain disorders, and multimodal large language models (MLLMs) are increasingly assisting in that analysis. However, current brain-oriented visual question-answering (VQA) benchmarks either cover a few imaging modalities or are limited to coarse-grained pathological descriptions, hindering a comprehensive assessment of MLLMs throughout the full clinical continuum. To address these, we introduce OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically designed to assess the multimodal comprehension capabilities of MLLMs in brain imaging analysis.OmniBrainBench consists of 15 distinct brain imaging modalities collected from 30 verified medical sources, yielding 9,527 validated VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15 multi-stage clinical tasks rigorously validated by a professional radiologist. Evaluation of 24 state-of-the-art models, including open-source, medical, and proprietary MLLMs, highlights the substantial challenges posed by OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5) beat open-source and medical models but lag physicians; (2) medical MLLMs vary widely in performance; (3) open-source MLLMs trail overall but excel in specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks, revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new standard for evaluating and advancing MLLMs in brain imaging analysis, highlighting gaps compared to expert clinical reasoning. We release it at benchmark &amp; code. </p>
<blockquote>
<p>è„‘æˆåƒåˆ†æåœ¨è¯Šæ–­ä¸æ²»ç–—è„‘éƒ¨ç–¾ç—…ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œè€Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£æ—¥ç›Šä¸ºè¿™ä¸€åˆ†ææä¾›è¾…åŠ©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢å‘å¤§è„‘çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•è¦ä¹ˆæ¶µç›–å°‘æ•°æˆåƒæ¨¡æ€ï¼Œè¦ä¹ˆä»…é™äºç²—ç²’åº¦çš„ç—…ç†æè¿°ï¼Œé˜»ç¢äº†å¯¹MLLMsåœ¨æ•´ä¸ªä¸´åºŠè¿‡ç¨‹ä¸­çš„å…¨é¢è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniBrainBenchï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºè¯„ä¼°MLLMsåœ¨å¤šæ¨¡æ€è„‘æˆåƒåˆ†æä¸­çš„ç†è§£èƒ½åŠ›è€Œè®¾è®¡çš„é¦–ä¸ªå…¨é¢å¤šæ¨¡æ€VQAåŸºå‡†æµ‹è¯•ã€‚OmniBrainBenchåŒ…å«äº†ä»30ä¸ªç»è¿‡éªŒè¯çš„åŒ»ç–—æ¥æºæ”¶é›†çš„15ç§ä¸åŒçš„è„‘æˆåƒæ¨¡æ€ï¼Œäº§ç”Ÿäº†9527ä¸ªç»è¿‡éªŒè¯çš„VQAå¯¹å’Œ31706å¼ å›¾åƒã€‚å®ƒæ¨¡æ‹Ÿäº†ä¸´åºŠå·¥ä½œæµç¨‹ï¼ŒåŒ…å«äº†ç”±ä¸“ä¸šæ”¾å°„ç§‘åŒ»ç”Ÿä¸¥æ ¼éªŒè¯çš„15ä¸ªå¤šé˜¶æ®µä¸´åºŠä»»åŠ¡ã€‚å¯¹24ä¸ªæœ€æ–°æ¨¡å‹çš„è¯„ä»·ï¼ŒåŒ…æ‹¬å¼€æºã€åŒ»ç–—å’Œä¸“æœ‰MLLMsï¼Œçªæ˜¾äº†OmniBrainBenchå¸¦æ¥çš„å·¨å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼šï¼ˆ1ï¼‰ä¸“æœ‰MLLMsï¼ˆå¦‚GPT-5ï¼‰å‡»è´¥äº†å¼€æºå’ŒåŒ»ç–—æ¨¡å‹ï¼Œä½†ä»è½åäºåŒ»ç”Ÿï¼›ï¼ˆ2ï¼‰åŒ»ç–—MLLMsçš„æ€§èƒ½å·®å¼‚å¾ˆå¤§ï¼›ï¼ˆ3ï¼‰å¼€æºMLLMsæ€»ä½“ä¸Šè¡¨ç°ä¸ä½³ï¼Œä½†åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼›ï¼ˆ4ï¼‰MLLMsåœ¨å¤æ‚çš„æœ¯å‰ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºè§†è§‰åˆ°ä¸´åºŠæ¨ç†çš„å·®è·ã€‚OmniBrainBenchä¸ºè¯„ä¼°å’Œæ¨è¿›è„‘æˆåƒåˆ†æä¸­çš„MLLMsè®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œå¹¶çªæ˜¾äº†ä¸ä¸“å®¶ä¸´åºŠæ¨ç†ç›¸æ¯”çš„å·®è·ã€‚æˆ‘ä»¬å·²åœ¨åŸºå‡†æµ‹è¯•å’Œä»£ç å¹³å°ä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00846v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤šæ¨¡æ€å¤§è„‘å½±åƒåˆ†æçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹å…¨é¢è¯„ä¼°æ ‡å‡†ã€‚OmniBrainBenchåº”è¿è€Œç”Ÿï¼Œå®ƒæ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°MLLMåœ¨å¤šæ¨¡æ€å¤§è„‘å½±åƒåˆ†æä¸­çš„ç†è§£èƒ½åŠ›çš„ç»¼åˆVQAåŸºå‡†æµ‹è¯•ã€‚OmniBrainBenchæ¶µç›–15ç§ç‹¬ç‰¹çš„è„‘éƒ¨æˆåƒæ–¹å¼ï¼Œæ¨¡æ‹ŸçœŸå®ä¸´åºŠæµç¨‹ï¼Œå¹¶è¿›è¡Œäº†ä¸¥æ ¼çš„éªŒè¯ã€‚å¯¹æ¯”è¯„ä¼°å¤šç§æœ€æ–°æ¨¡å‹åå‘ç°ï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤æ‚æœ¯å‰ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ï¼Œä¸ä¸“å®¶ä¸´åºŠæ¨ç†ç›¸æ¯”å­˜åœ¨å·®è·ã€‚æ­¤åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’Œæ”¹è¿›å¤§è„‘å½±åƒåˆ†æä¸­çš„MLLMsæä¾›äº†æ–°çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è„‘å½±åƒåˆ†æåœ¨è¯Šæ–­ä¸æ²»ç–—è„‘ç–¾ç—…ä¸­è‡³å…³é‡è¦ï¼Œè€Œç°æœ‰çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•åœ¨å¤šæ¨¡æ€å¤§è„‘å½±åƒåˆ†ææ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>OmniBrainBenchæ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€å¤§è„‘å½±åƒåˆ†æä¸­çš„ç»¼åˆVQAåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†å¤šç§æˆåƒæ–¹å¼å’Œä¸´åºŠä»»åŠ¡ã€‚</li>
<li>OmniBrainBenchæ¨¡æ‹ŸçœŸå®ä¸´åºŠæµç¨‹ï¼Œå¹¶ç»è¿‡ä¸“ä¸šæ”¾å°„ç§‘åŒ»ç”Ÿä¸¥æ ¼éªŒè¯ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-5ï¼‰èƒœè¿‡å¼€æºå’ŒåŒ»ç–—æ¨¡å‹ï¼Œä½†ä¸åŒ»ç”Ÿç›¸æ¯”ä»æœ‰å·®è·ã€‚</li>
<li>åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œè€Œå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹æ€»ä½“è¡¨ç°å¹³å¹³ï¼Œä½†åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æœ¯å‰ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ï¼Œå­˜åœ¨è§†è§‰åˆ°ä¸´åºŠæ¨ç†çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c59edc085361f862a8a610f59abadf6" align="middle">
<img src="https://picx.zhimg.com/v2-889ee547c41400bc02a92f4df90d013a" align="middle">
<img src="https://picx.zhimg.com/v2-98de24d02a3d46bb51b9c1396278630f" align="middle">
<img src="https://picx.zhimg.com/v2-71c877be0463f404e87bad25ae75d085" align="middle">
<img src="https://picx.zhimg.com/v2-f2a093043b762b6cd7d78d70a49613c9" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Med-Banana-50K-A-Cross-modality-Large-Scale-Dataset-for-Text-guided-Medical-Image-Editing"><a href="#Med-Banana-50K-A-Cross-modality-Large-Scale-Dataset-for-Text-guided-Medical-Image-Editing" class="headerlink" title="Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing"></a>Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing</h2><p><strong>Authors:Zhihui Chen, Mengling Feng</strong></p>
<p>Recent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research communityâ€™s progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric (instruction compliance, structural plausibility, realism, and fidelity preservation) and history-aware iterative refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K includes 37K failed attempts with full conversation logs for preference learning and alignment research. By providing this large-scale, medically validated, and fully documented resource, Med-Banana-50K establishes a foundation for training and evaluating the next generation of medical image editing models.Our dataset and code are publicly available at [<a target="_blank" rel="noopener" href="https://github.com/richardChenzhihui/med-banana-50k]">https://github.com/richardChenzhihui/med-banana-50k]</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä¸ºåŒ»å­¦å›¾åƒç¼–è¾‘æä¾›äº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç ”ç©¶é¢†åŸŸçš„è¿›å±•ä»å—åˆ°ç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å…¬å¼€å¯è®¿é—®çš„åŒ»å­¦å›¾åƒç¼–è¾‘æ•°æ®é›†çš„åˆ¶çº¦ï¼Œè¿™äº›æ•°æ®é›†éœ€è¦ä¸¥æ ¼çš„è§£å‰–å’Œä¸´åºŠçº¦æŸã€‚æˆ‘ä»¬ä»‹ç»äº†Med-Banana-50Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„åŒ»å­¦å›¾åƒç¼–è¾‘çš„ç»¼åˆæ€§5ä¸‡å¼ å›¾åƒæ•°æ®é›†ï¼Œæ¶µç›–ä¸‰ç§æ¨¡æ€ï¼ˆèƒ¸éƒ¨Xå°„çº¿ã€è„‘éƒ¨MRIã€çœ¼åº•æ‘„å½±ï¼‰å’Œ23ç§ç–¾ç—…ç±»å‹ã€‚æˆ‘ä»¬çš„æ•°æ®é›†é€šè¿‡åˆ©ç”¨Gemini-2.5-Flash-Imageç”ŸæˆçœŸå®åŒ»å­¦å›¾åƒçš„åŒå‘ç¼–è¾‘ï¼ˆç—…ç¶å¢åŠ å’Œç§»é™¤ï¼‰æ¥æ„å»ºã€‚Med-Banana-50Kä¸ä¸€èˆ¬é¢†åŸŸç¼–è¾‘æ•°æ®é›†çš„åŒºåˆ«åœ¨äºæˆ‘ä»¬çš„åŒ»ç–—è´¨é‡æ§åˆ¶ç³»ç»Ÿæ–¹æ³•ï¼šæˆ‘ä»¬é‡‡ç”¨LLM-as-Judgeï¼Œä½¿ç”¨åŸºäºåŒ»å­¦çš„è¯„åˆ†æ ‡å‡†ï¼ˆæŒ‡ä»¤åˆè§„æ€§ã€ç»“æ„å¯è¡Œæ€§ã€çœŸå®æ€§å’Œä¿çœŸåº¦ä¿ç•™ï¼‰ï¼Œå¹¶è¿›è¡Œæœ€å¤šäº”è½®çš„å†å²æ„ŸçŸ¥è¿­ä»£æ”¹è¿›ã€‚é™¤äº†å•å›åˆç¼–è¾‘ï¼ŒMed-Banana-50Kè¿˜åŒ…æ‹¬3.7ä¸‡æ¬¡å¤±è´¥å°è¯•åŠå®Œæ•´å¯¹è¯è®°å½•ï¼Œå¯ç”¨äºåå¥½å­¦ä¹ å’Œå¯¹é½ç ”ç©¶ã€‚é€šè¿‡æä¾›å¤§è§„æ¨¡ã€ç»è¿‡åŒ»å­¦éªŒè¯å’Œå®Œæ•´è®°å½•çš„è¿™ä¸€èµ„æºï¼ŒMed-Banana-50Kä¸ºåŸ¹è®­å’Œè¯„ä¼°ä¸‹ä¸€ä»£åŒ»å­¦å›¾åƒç¼–è¾‘æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/richardChenzhihui/med-banana-50k]%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/richardChenzhihui/med-banana-50k]å…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00801v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMed-Banana-50Kçš„åŒ»å­¦å›¾åƒç¼–è¾‘æ•°æ®é›†ï¼ŒåŒ…å«5ä¸‡å¼ å›¾åƒï¼Œæ¶‰åŠä¸‰ç§æ¨¡æ€å’Œ23ç§ç–¾ç—…ç±»å‹ã€‚è¯¥æ•°æ®é›†é€šè¿‡ç”ŸæˆåŒå‘ç¼–è¾‘ï¼ˆç—…å˜å¢åŠ å’Œç§»é™¤ï¼‰ä»çœŸå®åŒ»å­¦å›¾åƒä¸­æ„å»ºï¼Œé‡‡ç”¨LLM-as-Judgeè¿›è¡ŒåŒ»å­¦è´¨é‡æ§åˆ¶ï¼Œå¹¶åŒ…å«å¤±è´¥å°è¯•å’Œå®Œæ•´å¯¹è¯æ—¥å¿—ï¼Œä¸ºä¸‹ä¸€ä»£åŒ»å­¦å›¾åƒç¼–è¾‘æ¨¡å‹æä¾›è®­ç»ƒå’Œè¯„ä¼°åŸºç¡€ã€‚æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-Banana-50Kæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„åŒ»å­¦å›¾åƒç¼–è¾‘æ•°æ®é›†ï¼ŒåŒ…å«50Kå¼ å›¾åƒï¼Œè¦†ç›–ä¸‰ç§æ¨¡æ€å’Œ23ç§ç–¾ç—…ç±»å‹ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡çœŸå®åŒ»å­¦å›¾åƒç”ŸæˆåŒå‘ç¼–è¾‘æ„å»ºã€‚</li>
<li>é‡‡ç”¨LLM-as-Judgeè¿›è¡ŒåŒ»å­¦è´¨é‡æ§åˆ¶ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>åŒ…å«äº†å…¨é¢çš„å¤±è´¥å°è¯•å’Œå¯¹è¯æ—¥å¿—ï¼Œæœ‰åŠ©äºåå¥½å­¦ä¹ å’Œå¯¹é½ç ”ç©¶ã€‚</li>
<li>Med-Banana-50Kä¸ºè®­ç»ƒå’Œè¯„ä¼°åŒ»å­¦å›¾åƒç¼–è¾‘æ¨¡å‹æä¾›äº†åŸºç¡€ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºç ”ç©¶å’Œåˆ©ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5cd2da2408f9e4ef9486dc4f3596e37" align="middle">
<img src="https://picx.zhimg.com/v2-402a21eb739948e412bf1e39be5a9b2d" align="middle">
<img src="https://picx.zhimg.com/v2-e9a1fa0899be83216ffa88d661660a64" align="middle">
<img src="https://picx.zhimg.com/v2-9a8f62ea18bb7d0f2ab4b261dbda832a" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Applying-Medical-Imaging-Tractography-Techniques-to-Painterly-Rendering-of-Images"><a href="#Applying-Medical-Imaging-Tractography-Techniques-to-Painterly-Rendering-of-Images" class="headerlink" title="Applying Medical Imaging Tractography Techniques to Painterly Rendering   of Images"></a>Applying Medical Imaging Tractography Techniques to Painterly Rendering   of Images</h2><p><strong>Authors:Alberto Di Biase</strong></p>
<p>Doctors and researchers routinely use diffusion tensor imaging (DTI) and tractography to visualize the fibrous structure of tissues in the human body. This paper explores the connection of these techniques to the painterly rendering of images. Using a tractography algorithm the presented method can place brush strokes that mimic the painting process of human artists, analogously to how fibres are tracked in DTI. The analogue to the diffusion tensor for image orientation is the structural tensor, which can provide better local orientation information than the gradient alone. I demonstrate this technique in portraits and general images, and discuss the parallels between fibre tracking and brush stroke placement, and frame it in the language of tractography. This work presents an exploratory investigation into the cross-domain application of diffusion tensor imaging techniques to painterly rendering of images. All the code is available at <a target="_blank" rel="noopener" href="https://github.com/tito21/st-python">https://github.com/tito21/st-python</a> </p>
<blockquote>
<p>åŒ»ç”Ÿå’Œç ”ç©¶è€…é€šå¸¸ä½¿ç”¨æ‰©æ•£å¼ é‡æˆåƒï¼ˆDTIï¼‰å’Œçº¤ç»´è¿½è¸ªæŠ€æœ¯æ¥å¯è§†åŒ–äººä½“å†…çš„çº¤ç»´ç»“æ„ã€‚æœ¬æ–‡æ¢è®¨äº†è¿™äº›æŠ€æœ¯ä¸å›¾åƒç»˜ç”»æ¸²æŸ“ä¹‹é—´çš„è”ç³»ã€‚é€šè¿‡ä½¿ç”¨çº¤ç»´è¿½è¸ªç®—æ³•ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æ”¾ç½®æ¨¡ä»¿äººç±»è‰ºæœ¯å®¶ç»˜ç”»è¿‡ç¨‹çš„ç¬”è§¦ï¼Œè¿™ä¸DTIä¸­å¦‚ä½•è¿½è¸ªçº¤ç»´ç±»ä¼¼ã€‚ç”¨äºå›¾åƒæ–¹å‘çš„æ‰©æ•£å¼ é‡çš„ç±»ä¼¼ç‰©æ˜¯ç»“æ„å¼ é‡ï¼Œå®ƒå¯ä»¥æä¾›æ¯”å•çº¯æ¢¯åº¦æ›´å¥½çš„å±€éƒ¨æ–¹å‘ä¿¡æ¯ã€‚æˆ‘åœ¨è‚–åƒå’Œé€šç”¨å›¾åƒä¸­å±•ç¤ºäº†è¿™é¡¹æŠ€æœ¯ï¼Œå¹¶è®¨è®ºäº†çº¤ç»´è¿½è¸ªä¸ç¬”è§¦æ”¾ç½®ä¹‹é—´çš„ç›¸ä¼¼ä¹‹å¤„ï¼Œå¹¶ç”¨çº¤ç»´è¿½è¸ªçš„è¯­è¨€æ¥è¡¨è¿°ã€‚è¿™é¡¹å·¥ä½œæ˜¯å¯¹æ‰©æ•£å¼ é‡æˆåƒæŠ€æœ¯è·¨é¢†åŸŸåº”ç”¨äºå›¾åƒç»˜ç”»æ¸²æŸ“çš„æ¢ç´¢æ€§ç ”ç©¶ã€‚æ‰€æœ‰ä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tito21/st-python%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tito21/st-pythonæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00702v1">PDF</a> Exploratory investigation applying medical imaging tractography   techniques to painterly image rendering. Code available at   <a target="_blank" rel="noopener" href="https://github.com/tito21/st-python">https://github.com/tito21/st-python</a></p>
<p><strong>æ‘˜è¦</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸä¸­ï¼ŒåŒ»ç”Ÿå’Œç ”ç©¶è€…ç»å¸¸ä½¿ç”¨æ‰©æ•£å¼ é‡æˆåƒï¼ˆDTIï¼‰å’Œçº¤ç»´è·Ÿè¸ªæŠ€æœ¯æ¥å¯è§†åŒ–äººä½“ä¸­çš„çº¤ç»´ç»“æ„ã€‚æœ¬æ–‡æ¢è®¨äº†è¿™äº›æŠ€æœ¯ä¸å›¾åƒç»˜ç”»æ¸²æŸ“ä¹‹é—´çš„è”ç³»ã€‚é€šè¿‡ä½¿ç”¨çº¤ç»´è·Ÿè¸ªç®—æ³•ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æ”¾ç½®æ¨¡ä»¿äººç±»è‰ºæœ¯å®¶ç»˜ç”»è¿‡ç¨‹çš„ç¬”è§¦ï¼Œç±»ä¼¼äºDTIä¸­çº¤ç»´çš„è¿½è¸ªæ–¹å¼ã€‚å›¾åƒæ–¹å‘çš„æ‰©æ•£å¼ é‡çš„ç±»ä¼¼ç‰©æ˜¯ç»“æ„å¼ é‡ï¼Œå®ƒå¯ä»¥æä¾›æ¯”æ¢¯åº¦æ›´å¥½çš„å±€éƒ¨æ–¹å‘ä¿¡æ¯ã€‚ä½œè€…åœ¨è‚–åƒå’Œä¸€èˆ¬å›¾åƒä¸­å±•ç¤ºäº†è¿™é¡¹æŠ€æœ¯ï¼Œå¹¶è®¨è®ºäº†çº¤ç»´è·Ÿè¸ªä¸ç¬”è§¦æ”¾ç½®ä¹‹é—´çš„å¹³è¡Œæ€§ï¼Œå¹¶ç”¨çº¤ç»´è·Ÿè¸ªçš„è¯­è¨€è¿›è¡Œæè¿°ã€‚è¿™é¡¹å·¥ä½œæ˜¯å¯¹æ‰©æ•£å¼ é‡æˆåƒæŠ€æœ¯åœ¨å›¾åƒç»˜ç”»æ¸²æŸ“ä¸­çš„è·¨åŸŸåº”ç”¨è¿›è¡Œçš„æ¢ç´¢æ€§ç ”ç©¶ã€‚æ‰€æœ‰ä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tito21/st-python%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tito21/st-pythonä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>åŒ»ç”Ÿå’Œç ”ç©¶è€…ä½¿ç”¨æ‰©æ•£å¼ é‡æˆåƒï¼ˆDTIï¼‰å’Œçº¤ç»´è·Ÿè¸ªæŠ€æœ¯æ¥å¯è§†åŒ–äººä½“çº¤ç»´ç»“æ„ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†åŒ»å­¦æˆåƒæŠ€æœ¯ä¸å›¾åƒç»˜ç”»æ¸²æŸ“ä¹‹é—´çš„è¿æ¥ã€‚</li>
<li>é€šè¿‡çº¤ç»´è·Ÿè¸ªç®—æ³•ï¼Œæ¨¡æ‹Ÿäººç±»è‰ºæœ¯å®¶çš„ç»˜ç”»è¿‡ç¨‹ï¼Œå°†çº¤ç»´è¿½è¸ªä¸å›¾åƒç¬”è§¦æ”¾ç½®è¿›è¡Œç±»æ¯”ã€‚</li>
<li>ç»“æ„å¼ é‡ä½œä¸ºæ‰©æ•£å¼ é‡çš„ç±»ä¼¼ç‰©ï¼Œèƒ½æä¾›æ¯”å•çº¯æ¢¯åº¦æ›´ä¸°å¯Œçš„å±€éƒ¨æ–¹å‘ä¿¡æ¯ã€‚</li>
<li>ä½œè€…åœ¨è‚–åƒå’Œä¸€èˆ¬å›¾åƒä¸­å±•ç¤ºäº†èåˆåŒ»å­¦æˆåƒæŠ€æœ¯ä¸ç»˜ç”»æŠ€æœ¯çš„æ•ˆæœã€‚</li>
<li>æ¢è®¨äº†çº¤ç»´è·Ÿè¸ªä¸å›¾åƒç¬”è§¦ä¹‹é—´çš„å¹³è¡Œæ€§ï¼Œå¹¶è¿ç”¨çº¤ç»´è·Ÿè¸ªè¯­è¨€è¿›è¡Œæè¿°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-856d611b55282be80a275d2502b613b1" align="middle">
<img src="https://picx.zhimg.com/v2-d483e71f1de26e6b78fa27f8f59b357f" align="middle">
<img src="https://picx.zhimg.com/v2-3c023483f1cfec7db62c3cc0ee0b497c" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Towards-Reliable-Pediatric-Brain-Tumor-Segmentation-Task-Specific-nnU-Net-Enhancements"><a href="#Towards-Reliable-Pediatric-Brain-Tumor-Segmentation-Task-Specific-nnU-Net-Enhancements" class="headerlink" title="Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific   nnU-Net Enhancements"></a>Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific   nnU-Net Enhancements</h2><p><strong>Authors:Xiaolong Li, Zhi-Qin John Xu, Yan Ren, Tianming Qiu, Xiaowen Wang</strong></p>
<p>Accurate segmentation of pediatric brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and monitoring, yet faces unique challenges due to limited data, high anatomical variability, and heterogeneous imaging across institutions. In this work, we present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the largest public dataset of pre-treatment pediatric high-grade gliomas. Our contributions include: (1) a widened residual encoder with squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions; (3) a specificity-driven regularization term; and (4) small-scale Gaussian weight initialization. We further refine predictions with two postprocessing steps. Our models achieved first place on the Task-6 validation leaderboard, attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910 (NET), 0.928 (TC) and 0.928 (WT). </p>
<blockquote>
<p>åœ¨å¤šå‚æ•°ç£å…±æŒ¯æˆåƒï¼ˆmpMRIï¼‰ä¸­å¯¹å„¿ç«¥è„‘è‚¿ç˜¤è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®æœ‰é™ã€è§£å‰–ç»“æ„é«˜åº¦å¯å˜ä»¥åŠæœºæ„é—´æˆåƒçš„å¼‚è´¨æ€§ï¼Œè¿™é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹BraTS 2025 Task-6ï¼ˆPEDï¼‰ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§å…ˆè¿›çš„nnU-Netæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¯å„¿ç«¥é«˜çº§åˆ«èƒ¶è´¨ç˜¤é¢„å¤„ç†çš„æœ€å¤§å…¬å…±æ•°æ®é›†ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å¸¦æœ‰æŒ¤å‹å’Œæ¿€åŠ±ï¼ˆSEï¼‰æ³¨æ„åŠ›çš„å®½æ®‹å·®ç¼–ç å™¨ï¼›ï¼ˆ2ï¼‰3Dæ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼›ï¼ˆ3ï¼‰ç‰¹å¼‚æ€§é©±åŠ¨çš„æ­£åˆ™åŒ–é¡¹ï¼›ï¼ˆ4ï¼‰å°è§„æ¨¡é«˜æ–¯æƒé‡åˆå§‹åŒ–ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ä¸¤ä¸ªåå¤„ç†æ­¥éª¤å®Œå–„é¢„æµ‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨Task-6éªŒè¯æ’è¡Œæ¦œä¸Šååˆ—å‰èŒ…ï¼Œè·å¾—äº†ç—…å˜çº§åˆ«çš„Diceåˆ†æ•°ï¼š0.759ï¼ˆCCï¼‰ã€0.967ï¼ˆEDï¼‰ã€0.826ï¼ˆETï¼‰ã€0.910ï¼ˆNETï¼‰ã€0.928ï¼ˆTCï¼‰å’Œ0.928ï¼ˆWTï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00449v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šå‚æ•°ç£å…±æŒ¯æˆåƒï¼ˆmpMRIï¼‰å¯¹å°å„¿è„‘è‚¿ç˜¤è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç›‘æµ‹è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é‡‡ç”¨å…ˆè¿›çš„nnU-Netæ¡†æ¶ï¼Œé’ˆå¯¹BraTS 2025 Task-6ï¼ˆPEDï¼‰è¿›è¡Œå®šåˆ¶åŒ–è®¾è®¡ï¼Œè¯¥æ–¹æ³•åŒ…å«ä¸€ç³»åˆ—åˆ›æ–°è´¡çŒ®ï¼Œå¦‚å®½æ®‹å·®ç¼–ç å™¨ä¸æŒ¤å‹æ¿€å‘ï¼ˆSEï¼‰æ³¨æ„åŠ›æœºåˆ¶ã€3Dæ·±åº¦å¯åˆ†ç¦»å·ç§¯ã€ç‰¹å¼‚æ€§é©±åŠ¨çš„æ­£åˆ™åŒ–æœ¯è¯­ä»¥åŠå°å°ºåº¦é«˜æ–¯æƒé‡åˆå§‹åŒ–ç­‰ã€‚é€šè¿‡ä¸¤ä¸ªåå¤„ç†æ­¥éª¤è¿›ä¸€æ­¥ä¼˜åŒ–é¢„æµ‹ç»“æœï¼Œæ¨¡å‹åœ¨Task-6éªŒè¯æ’è¡Œæ¦œä¸Šè·å¾—ç¬¬ä¸€åï¼Œå„ç—…ç¶Diceå¾—åˆ†è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨å¤šå‚æ•°ç£å…±æŒ¯æˆåƒï¼ˆmpMRIï¼‰åœ¨å°å„¿è„‘è‚¿ç˜¤åˆ†å‰²ä¸­çš„åº”ç”¨ï¼Œè¿™æ˜¯è¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç›‘æµ‹çš„å…³é”®ç¯èŠ‚ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å…ˆè¿›çš„nnU-Netæ¡†æ¶ï¼Œé’ˆå¯¹BraTS 2025 Task-6ï¼ˆPEDï¼‰è¿›è¡Œå®šåˆ¶åŒ–è®¾è®¡ï¼Œåº”å¯¹æœ‰é™æ•°æ®ã€é«˜è§£å‰–å˜å¼‚å’Œæœºæ„é—´æˆåƒå·®å¼‚ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹çš„è´¡çŒ®åŒ…æ‹¬å®½æ®‹å·®ç¼–ç å™¨ä¸æŒ¤å‹æ¿€å‘ï¼ˆSEï¼‰æ³¨æ„åŠ›æœºåˆ¶ã€3Dæ·±åº¦å¯åˆ†ç¦»å·ç§¯ç­‰åˆ›æ–°æŠ€æœ¯ã€‚</li>
<li>æ¨¡å‹å¼•å…¥ç‰¹å¼‚æ€§é©±åŠ¨çš„æ­£åˆ™åŒ–æœ¯è¯­å’Œå°å°ºåº¦é«˜æ–¯æƒé‡åˆå§‹åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªåå¤„ç†æ­¥éª¤è¿›ä¸€æ­¥ä¼˜åŒ–é¢„æµ‹ç»“æœã€‚</li>
<li>æ¨¡å‹åœ¨Task-6éªŒè¯æ’è¡Œæ¦œä¸Šè·å¾—ç¬¬ä¸€åï¼Œè¡¨æ˜å…¶ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ddc0498dbb674d4afaaceb323a5f1c9" align="middle">
<img src="https://picx.zhimg.com/v2-8b138b6633b61bfa6b0fadfe0513ed26" align="middle">
<img src="https://picx.zhimg.com/v2-d71416e657c6d4febd1bf632806384bc" align="middle">
<img src="https://picx.zhimg.com/v2-b775dafee82906a27991bfba99795163" align="middle">
<img src="https://picx.zhimg.com/v2-7661284db865f914286b023bff0a772f" align="middle">
<img src="https://picx.zhimg.com/v2-3a056e09a663ac1faa9ac5e9caa52269" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VisionCAD-An-Integration-Free-Radiology-Copilot-Framework"><a href="#VisionCAD-An-Integration-Free-Radiology-Copilot-Framework" class="headerlink" title="VisionCAD: An Integration-Free Radiology Copilot Framework"></a>VisionCAD: An Integration-Free Radiology Copilot Framework</h2><p><strong>Authors:Jiaming Li, Junlei Wu, Sheng Wang, Honglin Xiong, Jiangdong Cai, Zihao Zhao, Yitao Zhu, Yuan Yin, Dinggang Shen, Qian Wang</strong></p>
<p>Widespread clinical deployment of computer-aided diagnosis (CAD) systems is hindered by the challenge of integrating with existing hospital IT infrastructure. Here, we introduce VisionCAD, a vision-based radiological assistance framework that circumvents this barrier by capturing medical images directly from displays using a camera system. The framework operates through an automated pipeline that detects, restores, and analyzes on-screen medical images, transforming camera-captured visual data into diagnostic-quality images suitable for automated analysis and report generation. We validated VisionCAD across diverse medical imaging datasets, demonstrating that our modular architecture can flexibly utilize state-of-the-art diagnostic models for specific tasks. The system achieves diagnostic performance comparable to conventional CAD systems operating on original digital images, with an F1-score degradation typically less than 2% across classification tasks, while natural language generation metrics for automated reports remain within 1% of those derived from original images. By requiring only a camera device and standard computing resources, VisionCAD offers an accessible approach for AI-assisted diagnosis, enabling the deployment of diagnostic capabilities in diverse clinical settings without modifications to existing infrastructure. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿçš„å¹¿æ³›ä¸´åºŠåº”ç”¨å—åˆ°äº†ä¸ç°æœ‰åŒ»é™¢ITåŸºç¡€è®¾æ–½é›†æˆæŒ‘æˆ˜çš„é™åˆ¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†VisionCADï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§†è§‰çš„æ”¾å°„è¾…åŠ©æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç›¸æœºç³»ç»Ÿç›´æ¥ä»æ˜¾ç¤ºå±æ•è·åŒ»ç–—å›¾åƒï¼Œä»è€Œç»•è¿‡äº†è¿™ä¸€éšœç¢ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“è¿è¡Œï¼Œè¯¥ç®¡é“èƒ½å¤Ÿæ£€æµ‹ã€æ¢å¤å’Œåˆ†æå±å¹•ä¸Šçš„åŒ»ç–—å›¾åƒï¼Œå°†æ‘„åƒå¤´æ•è·çš„è§†è§‰æ•°æ®è½¬æ¢ä¸ºé€‚åˆè‡ªåŠ¨åŒ–åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆçš„è¯Šæ–­çº§å›¾åƒã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸ŠéªŒè¯äº†VisionCADï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å—åŒ–æ¶æ„å¯ä»¥çµæ´»åœ°åˆ©ç”¨æœ€å…ˆè¿›çš„è¯Šæ–­æ¨¡å‹æ¥å®Œæˆç‰¹å®šä»»åŠ¡ã€‚ç³»ç»Ÿçš„è¯Šæ–­æ€§èƒ½ä¸åœ¨åŸå§‹æ•°å­—å›¾åƒä¸Šè¿è¡Œçš„å¸¸è§„CADç³»ç»Ÿç›¸å½“ï¼Œåˆ†ç±»ä»»åŠ¡çš„F1åˆ†æ•°ä¸‹é™é€šå¸¸ä¸åˆ°2%ï¼Œè€Œè‡ªåŠ¨æŠ¥å‘Šçš„è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡ä¸ä»åŸå§‹å›¾åƒä¸­å¾—å‡ºçš„æŒ‡æ ‡ç›¸æ¯”ä»ä¿æŒåœ¨1%ä»¥å†…ã€‚VisionCADä»…éœ€æ‘„åƒå¤´è®¾å¤‡å’Œæ ‡å‡†è®¡ç®—èµ„æºï¼Œä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­æä¾›äº†å¯è®¿é—®çš„é€”å¾„ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹ç°æœ‰åŸºç¡€è®¾æ–½çš„æƒ…å†µä¸‹ï¼Œåœ¨ä¸åŒçš„ä¸´åºŠç¯å¢ƒä¸­éƒ¨ç½²è¯Šæ–­èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00381v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VisionCADæ¡†æ¶è§£å†³äº†CADç³»ç»Ÿåœ¨ä¸´åºŠä¸­çš„éƒ¨ç½²é—®é¢˜ã€‚å®ƒé€šè¿‡ç›¸æœºç³»ç»Ÿç›´æ¥æ•è·å±å¹•ä¸Šçš„åŒ»å­¦å›¾åƒï¼Œå°†å…¶è½¬åŒ–ä¸ºé€‚åˆè‡ªåŠ¨åŒ–åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆçš„è¯Šæ–­çº§å›¾åƒã€‚è¯¥æ¡†æ¶å…·æœ‰æ¨¡å—åŒ–æ¶æ„ï¼Œå¯çµæ´»åˆ©ç”¨æœ€æ–°è¯Šæ–­æ¨¡å‹è¿›è¡Œç‰¹å®šä»»åŠ¡ã€‚å…¶è¯Šæ–­æ€§èƒ½ä¸åœ¨åŸå§‹æ•°å­—å›¾åƒä¸Šè¿è¡Œçš„å¸¸è§„CADç³»ç»Ÿç›¸å½“ï¼Œåˆ†ç±»ä»»åŠ¡çš„F1å¾—åˆ†ä¸‹é™é€šå¸¸ä¸åˆ°2%ï¼Œè‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šçš„è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡ä¸åŸå§‹å›¾åƒä¹‹é—´çš„å·®å¼‚åœ¨1%ä»¥å†…ã€‚æ­¤å¤–ï¼Œå®ƒåªéœ€ç›¸æœºè®¾å¤‡å’Œæ ‡å‡†è®¡ç®—èµ„æºï¼Œä¸ºåœ¨å¤šæ ·åŒ–ä¸´åºŠç¯å¢ƒä¸­éƒ¨ç½²AIè¾…åŠ©è¯Šæ–­æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚æ— éœ€å¯¹ç°æœ‰åŸºç¡€è®¾æ–½è¿›è¡Œä»»ä½•æ”¹åŠ¨å³å¯ä½¿ç”¨æ­¤æŠ€æœ¯ï¼Œè¿™ä½¿å¾—VisionCADå¾—ä»¥å¹¿æ³›åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisionCADæ˜¯ä¸€ä¸ªåŸºäºè§†è§‰çš„æ”¾å°„è¾…åŠ©è¯Šæ–­æ¡†æ¶ï¼Œé€šè¿‡ç›¸æœºç³»ç»Ÿç›´æ¥æ•è·å±å¹•ä¸Šçš„åŒ»å­¦å›¾åƒã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“å¤„ç†æ•è·çš„å›¾åƒï¼Œå°†å…¶è½¬åŒ–ä¸ºé€‚åˆè‡ªåŠ¨åŒ–åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆçš„è¯Šæ–­çº§å›¾åƒã€‚</li>
<li>VisionCADå…·æœ‰æ¨¡å—åŒ–æ¶æ„ï¼Œèƒ½å¤Ÿçµæ´»åˆ©ç”¨æœ€æ–°çš„è¯Šæ–­æ¨¡å‹è¿›è¡Œç‰¹å®šä»»åŠ¡çš„å¤„ç†ã€‚</li>
<li>VisionCADçš„è¯Šæ–­æ€§èƒ½ä¸å¸¸è§„CADç³»ç»Ÿç›¸å½“ï¼ŒF1å¾—åˆ†ä¸‹é™é€šå¸¸ä¸åˆ°2%ã€‚</li>
<li>è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆä¸­çš„è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡ä¸åŸå§‹å›¾åƒå·®å¼‚åœ¨1%ä»¥å†…ã€‚</li>
<li>VisionCADå¯¹ç¡¬ä»¶è¦æ±‚ä½ï¼Œåªéœ€ç›¸æœºè®¾å¤‡å’Œæ ‡å‡†è®¡ç®—èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e57618fe727a13d73ea77ffb5d1b445" align="middle">
<img src="https://picx.zhimg.com/v2-3429034d7c265226522483ea1efe0cbc" align="middle">
<img src="https://picx.zhimg.com/v2-e3c3a118538044025bf02fc317ccccac" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Target-Guided-Bayesian-Flow-Networks-for-Quantitatively-Constrained-CAD-Generation"><a href="#Target-Guided-Bayesian-Flow-Networks-for-Quantitatively-Constrained-CAD-Generation" class="headerlink" title="Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD   Generation"></a>Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD   Generation</h2><p><strong>Authors:Wenhao Zheng, Chenwei Sun, Wenbo Zhang, Jiancheng Lv, Xianggen Liu</strong></p>
<p>Deep generative models, such as diffusion models, have shown promising progress in image generation and audio generation via simplified continuity assumptions. However, the development of generative modeling techniques for generating multi-modal data, such as parametric CAD sequences, still lags behind due to the challenges in addressing long-range constraints and parameter sensitivity. In this work, we propose a novel framework for quantitatively constrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN). For the first time, TGBFN handles the multi-modality of CAD sequences (i.e., discrete commands and continuous parameters) in a unified continuous and differentiable parameter space rather than in the discrete data space. In addition, TGBFN penetrates the parameter update kernel and introduces a guided Bayesian flow to control the CAD properties. To evaluate TGBFN, we construct a new dataset for quantitatively constrained CAD generation. Extensive comparisons across single-condition and multi-condition constrained generation tasks demonstrate that TGBFN achieves state-of-the-art performance in generating high-fidelity, condition-aware CAD sequences. The code is available at <a target="_blank" rel="noopener" href="https://github.com/scu-zwh/TGBFN">https://github.com/scu-zwh/TGBFN</a>. </p>
<blockquote>
<p>æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç®€åŒ–çš„è¿ç»­æ€§å‡è®¾åœ¨å›¾åƒç”Ÿæˆå’ŒéŸ³é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æœ‰å‰æ™¯çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºè§£å†³é•¿æœŸçº¦æŸå’Œå‚æ•°æ•æ„Ÿæ€§çš„æŒ‘æˆ˜ï¼Œç”Ÿæˆå¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚å‚æ•°åŒ–CADåºåˆ—ï¼‰çš„ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯å‘å±•ä»æ»åã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºå®šé‡çº¦æŸCADç”Ÿæˆçš„æ–°å‹æ¡†æ¶ï¼Œç§°ä¸ºç›®æ ‡å¯¼å‘è´å¶æ–¯æµç½‘ç»œï¼ˆTGBFNï¼‰ã€‚TGBFNé¦–æ¬¡åœ¨ç»Ÿä¸€ã€è¿ç»­å’Œå¯å¾®çš„å‚æ•°ç©ºé—´ä¸­å¤„ç†CADåºåˆ—çš„å¤šæ¨¡æ€æ€§ï¼ˆå³ç¦»æ•£å‘½ä»¤å’Œè¿ç»­å‚æ•°ï¼‰ï¼Œè€Œä¸æ˜¯åœ¨ç¦»æ•£æ•°æ®ç©ºé—´ä¸­ã€‚æ­¤å¤–ï¼ŒTGBFNæ¸—é€å‚æ•°æ›´æ–°å†…æ ¸å¹¶å¼•å…¥æœ‰å¯¼å‘çš„è´å¶æ–¯æµæ¥æ§åˆ¶CADå±æ€§ã€‚ä¸ºäº†è¯„ä¼°TGBFNï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ç”¨äºå®šé‡çº¦æŸCADç”Ÿæˆã€‚åœ¨å•æ¡ä»¶å’Œå¤šæ¡ä»¶çº¦æŸç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹¿æ³›æ¯”è¾ƒè¡¨æ˜ï¼ŒTGBFNåœ¨ç”Ÿæˆé«˜ä¿çœŸã€æ¡ä»¶æ„ŸçŸ¥çš„CADåºåˆ—æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/scu-zwh/TGBFN%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/scu-zwh/TGBFNè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25163v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹ç­‰æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå’ŒéŸ³é¢‘ç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œé’ˆå¯¹CADåºåˆ—ç­‰å¤šæ¨¡æ€æ•°æ®ç”ŸæˆæŠ€æœ¯ä»é¢ä¸´é•¿è¿œçº¦æŸå’Œå‚æ•°æ•æ„Ÿæ€§çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºç›®æ ‡å¯¼å‘è´å¶æ–¯æµç½‘ç»œï¼ˆTGBFNï¼‰çš„å®šé‡çº¦æŸCADç”Ÿæˆæ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–æ¬¡åœ¨ç»Ÿä¸€ã€è¿ç»­ã€å¯å¾®åˆ†çš„å‚æ•°ç©ºé—´å†…å¤„ç†CADåºåˆ—çš„å¤šæ¨¡æ€æ€§ï¼ˆå³ç¦»æ•£å‘½ä»¤å’Œè¿ç»­å‚æ•°ï¼‰ï¼Œè€Œéç¦»æ•£æ•°æ®ç©ºé—´ã€‚æ­¤å¤–ï¼ŒTGBFNæ¸—é€å‚æ•°æ›´æ–°å†…æ ¸å¹¶å¼•å…¥å¯¼å‘è´å¶æ–¯æµä»¥æ§åˆ¶CADå±æ€§ã€‚é€šè¿‡æ„å»ºæ–°çš„å®šé‡çº¦æŸCADç”Ÿæˆæ•°æ®é›†å¯¹TGBFNè¿›è¡Œè¯„ä¼°ï¼Œåœ¨å•æ¡ä»¶å’Œå¤šæ¡ä»¶çº¦æŸç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹¿æ³›å¯¹æ¯”è¡¨æ˜ï¼ŒTGBFNåœ¨ç”Ÿæˆé«˜ä¿çœŸã€æ¡ä»¶æ„ŸçŸ¥çš„CADåºåˆ—æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç”Ÿæˆæ¨¡å‹å¦‚æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’ŒéŸ³é¢‘ç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>é’ˆå¯¹CADåºåˆ—ç­‰å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆï¼Œä»å­˜åœ¨é•¿è¿œçº¦æŸå’Œå‚æ•°æ•æ„Ÿæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ¡†æ¶TGBFNï¼Œç”¨äºå®šé‡çº¦æŸçš„CADç”Ÿæˆã€‚</li>
<li>TGBFNé¦–æ¬¡åœ¨ç»Ÿä¸€ã€è¿ç»­ã€å¯å¾®åˆ†çš„å‚æ•°ç©ºé—´å¤„ç†CADåºåˆ—çš„å¤šæ¨¡æ€æ€§ã€‚</li>
<li>TGBFNå¼•å…¥å‚æ•°æ›´æ–°å†…æ ¸å’Œå¯¼å‘è´å¶æ–¯æµä»¥æ§åˆ¶CADå±æ€§ã€‚</li>
<li>ä¸ºè¯„ä¼°TGBFNï¼Œæ„å»ºäº†æ–°çš„å®šé‡çº¦æŸCADç”Ÿæˆæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ea89acc12b511eeb6644a1d567d0e49" align="middle">
<img src="https://picx.zhimg.com/v2-6d68e04df26645256769ad0a77772143" align="middle">
<img src="https://picx.zhimg.com/v2-4ca08e96d0ff1696439eeb143ea503ce" align="middle">
<img src="https://picx.zhimg.com/v2-555c6f015bc976986c6a1262cce4910a" align="middle">
<img src="https://picx.zhimg.com/v2-4bf6ddb8bcb110f94b6d637d5c9bc5ee" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Breast-Cancer-VLMs-Clinically-Practical-Vision-Language-Train-Inference-Models"><a href="#Breast-Cancer-VLMs-Clinically-Practical-Vision-Language-Train-Inference-Models" class="headerlink" title="Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference   Models"></a>Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference   Models</h2><p><strong>Authors:Shunjie-Fabian Zheng, Hyeonjun Lee, Thijs Kooi, Ali Diba</strong></p>
<p>Breast cancer remains the most commonly diagnosed malignancy among women in the developed world. Early detection through mammography screening plays a pivotal role in reducing mortality rates. While computer-aided diagnosis (CAD) systems have shown promise in assisting radiologists, existing approaches face critical limitations in clinical deployment - particularly in handling the nuanced interpretation of multi-modal data and feasibility due to the requirement of prior clinical history. This study introduces a novel framework that synergistically combines visual features from 2D mammograms with structured textual descriptors derived from easily accessible clinical metadata and synthesized radiological reports through innovative tokenization modules. Our proposed methods in this study demonstrate that strategic integration of convolutional neural networks (ConvNets) with language representations achieves superior performance to vision transformer-based models while handling high-resolution images and enabling practical deployment across diverse populations. By evaluating it on multi-national cohort screening mammograms, our multi-modal approach achieves superior performance in cancer detection and calcification identification compared to unimodal baselines, with particular improvements. The proposed method establishes a new paradigm for developing clinically viable VLM-based CAD systems that effectively leverage imaging data and contextual patient information through effective fusion mechanisms. </p>
<blockquote>
<p>ä¹³è…ºç™Œä»æ˜¯å‘è¾¾å›½å®¶å¥³æ€§ä¸­æœ€å¸¸è§çš„æ¶æ€§è‚¿ç˜¤ã€‚é€šè¿‡ä¹³è…ºXå…‰ç­›æŸ¥è¿›è¡Œæ—©æœŸæ£€æµ‹åœ¨é™ä½æ­»äº¡ç‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿåœ¨è¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿæ–¹é¢å·²æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨æ–¹é¢é¢ä¸´é‡å¤§å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šæ¨¡å¼æ•°æ®çš„ç»†å¾®è§£è¯»å’Œå¯è¡Œæ€§ï¼ˆå› éœ€è¦å…ˆå‰ä¸´åºŠç—…å²ï¼‰æ–¹é¢ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒç»“åˆäº†2Dä¹³è…ºXå…‰å›¾åƒä¸­çš„è§†è§‰ç‰¹å¾ä¸ä»æ˜“äºè·å–çš„ä¸´åºŠå…ƒæ•°æ®å’Œåˆæˆæ”¾å°„å­¦æŠ¥å‘Šå¾—å‡ºçš„ç»“æ„åŒ–æ–‡æœ¬æè¿°ç¬¦ï¼Œå¹¶é€šè¿‡åˆ›æ–°çš„ä»¤ç‰ŒåŒ–æ¨¡å—è¿›è¡Œæå–ã€‚æœ¬ç ”ç©¶ä¸­æå‡ºçš„æ–¹æ³•è¡¨æ˜ï¼Œé€šè¿‡ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetsï¼‰å’Œè¯­è¨€è¡¨ç¤ºï¼Œåœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒå¹¶å®ç°è·¨ä¸åŒäººç¾¤çš„å®ç”¨éƒ¨ç½²æ–¹é¢ï¼Œç›¸è¾ƒäºåŸºäºè§†è§‰å˜å‹å™¨çš„æ¨¡å‹ï¼Œèƒ½å–å¾—å“è¶Šæ€§èƒ½ã€‚é€šè¿‡å¯¹å¤šå›½é˜Ÿåˆ—ç­›æŸ¥ä¹³è…ºXå…‰å›¾åƒè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡å¼æ–¹æ³•åœ¨ç™Œç—‡æ£€æµ‹å’Œé’™åŒ–è¯†åˆ«æ–¹é¢å®ç°äº†ä¼˜äºå•æ¨¡å¼åŸºå‡†çº¿çš„å“è¶Šæ€§èƒ½ï¼Œå¹¶æœ‰ç‰¹æ®Šæ”¹è¿›ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¸ºå¼€å‘ä¸´åºŠä¸Šå¯è¡Œçš„åŸºäºVLMçš„CADç³»ç»Ÿå»ºç«‹äº†æ–°èŒƒå¼ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æœ‰æ•ˆçš„èåˆæœºåˆ¶æœ‰æ•ˆåœ°åˆ©ç”¨æˆåƒæ•°æ®å’Œä¸Šä¸‹æ–‡æ‚£è€…ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25051v1">PDF</a> Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)   Workshop at ICCV 2025</p>
<p><strong>Summary</strong><br>     ä¹³è…ºç™Œåœ¨å‘è¾¾å›½å®¶å¥³æ€§ä¸­æœ€å¸¸è§çš„æ¶æ€§è‚¿ç˜¤ã€‚æ—©æœŸé€šè¿‡ä¹³è…ºXå…‰æ‘„å½±ç­›æŸ¥å¯¹é™ä½æ­»äº¡ç‡èµ·å…³é”®ä½œç”¨ã€‚è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿåœ¨è¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿæ–¹é¢æ˜¾ç¤ºå‡ºå‰æ™¯ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨ä¸´åºŠéƒ¨ç½²ä¸­å­˜åœ¨å¤„ç†å¤šæ¨¡å¼æ•°æ®çš„ç»†å¾®è§£è¯»å’Œå¯è¡Œæ€§æ–¹é¢çš„å±€é™æ€§ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒç»“åˆäº†äºŒç»´ä¹³è…ºé’¼é¶çš„è§†è§‰ç‰¹å¾ä¸ä»æ˜“äºè·å–çš„ä¸´åºŠå…ƒæ•°æ®å’Œåˆæˆæ”¾å°„å­¦æŠ¥å‘Šä¸­æ´¾ç”Ÿçš„ç»“æ„åŒ–æ–‡æœ¬æè¿°ç¬¦ã€‚æœ¬ç ”ç©¶ä¸­æå‡ºçš„æ–¹æ³•è¯æ˜ï¼Œä¸åŸºäºè§†è§‰è½¬æ¢å™¨æ¨¡å‹çš„èåˆç›¸æ¯”ï¼Œå°†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetsï¼‰ä¸è¯­è¨€è¡¨ç¤ºç›¸ç»“åˆçš„æˆ˜ç•¥é›†æˆåœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒäººç¾¤ä¸­å®ç°å®é™…éƒ¨ç½²ã€‚åœ¨å¤šå›½é˜Ÿåˆ—ç­›æŸ¥ä¹³è…ºé’¼é¶æ‘„å½±æœ¯ä¸­è¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡å¼æ–¹æ³•ç›¸è¾ƒäºå•æ¨¡å¼åŸºçº¿åœ¨ç™Œç—‡æ£€æµ‹å’Œé’™åŒ–è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·æœ‰ç‰¹åˆ«çš„æ”¹è¿›ã€‚è¯¥æ–¹æ³•ä¸ºå¼€å‘ä¸´åºŠå¯è¡Œçš„åŸºäºVLMçš„CADç³»ç»Ÿå»ºç«‹äº†æ–°çš„èŒƒä¾‹ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æœ‰æ•ˆçš„èåˆæœºåˆ¶æœ‰æ•ˆåœ°åˆ©ç”¨æˆåƒæ•°æ®å’Œä¸Šä¸‹æ–‡æ‚£è€…ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œåœ¨å‘è¾¾å›½å®¶å¥³æ€§ä¸­ä»ç„¶æ˜¯æœ€å¸¸è§çš„æ¶æ€§è‚¿ç˜¤ã€‚</li>
<li>æ—©æœŸé€šè¿‡ä¹³è…ºXå…‰æ‘„å½±ç­›æŸ¥å¯¹é™ä½æ­»äº¡ç‡è‡³å…³é‡è¦ã€‚</li>
<li>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿåœ¨ä¸´åºŠéƒ¨ç½²ä¸­å­˜åœ¨å¤„ç†å¤šæ¨¡å¼æ•°æ®å’Œå¯è¡Œæ€§æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶ç»“åˆäºŒç»´ä¹³è…ºé’¼é¶çš„è§†è§‰ç‰¹å¾ä¸ä»ä¸´åºŠå…ƒæ•°æ®å’Œæ”¾å°„å­¦æŠ¥å‘Šä¸­æ´¾ç”Ÿçš„ç»“æ„åŒ–æ–‡æœ¬æè¿°ç¬¦ã€‚</li>
<li>é›†æˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetsï¼‰ä¸è¯­è¨€è¡¨ç¤ºçš„æ–¹æ³•åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥å¤šæ¨¡å¼æ–¹æ³•ç›¸è¾ƒäºå•æ¨¡å¼åŸºçº¿åœ¨ç™Œç—‡æ£€æµ‹å’Œé’™åŒ–è¯†åˆ«æ–¹é¢å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-546b623c7b7d3dd547b35bd81c4028ca" align="middle">
<img src="https://picx.zhimg.com/v2-6c74614882c7913010374615c4863407" align="middle">
<img src="https://picx.zhimg.com/v2-67aa4155990d25edb6e0634ce101f85f" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="When-are-radiology-reports-useful-for-training-medical-image-classifiers"><a href="#When-are-radiology-reports-useful-for-training-medical-image-classifiers" class="headerlink" title="When are radiology reports useful for training medical image   classifiers?"></a>When are radiology reports useful for training medical image   classifiers?</h2><p><strong>Authors:Herman BergstrÃ¶m, Zhongqi Yue, Fredrik D. Johansson</strong></p>
<p>Medical images used to train machine learning models are often accompanied by radiology reports containing rich expert annotations. However, relying on these reports as inputs for clinical prediction requires the timely manual work of a trained radiologist. This raises a natural question: when can radiology reports be leveraged during training to improve image-only classification? Prior works are limited to evaluating pre-trained image representations by fine-tuning them to predict diagnostic labels, often extracted from reports, ignoring tasks with labels that are weakly associated with the text. To address this gap, we conduct a systematic study of how radiology reports can be used during both pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g., 12-month readmission), and under varying training set sizes. Our findings reveal that: (1) Leveraging reports during pre-training is beneficial for downstream classification tasks where the label is well-represented in the text; however, pre-training through explicit image-text alignment can be detrimental in settings where itâ€™s not; (2) Fine-tuning with reports can lead to significant improvements and even have a larger impact than the pre-training method in certain settings. These results provide actionable insights into when and how to leverage privileged text data to train medical image classifiers while highlighting gaps in current research. </p>
<blockquote>
<p>ç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„åŒ»å­¦å›¾åƒé€šå¸¸ä¼´éšæœ‰åŒ…å«ä¸°å¯Œä¸“å®¶æ³¨é‡Šçš„æ”¾å°„å­¦æŠ¥å‘Šã€‚ç„¶è€Œï¼Œä¾èµ–è¿™äº›æŠ¥å‘Šä½œä¸ºä¸´åºŠé¢„æµ‹è¾“å…¥éœ€è¦è®­ç»ƒæœ‰ç´ çš„æ”¾å°„ç§‘åŒ»ç”ŸåŠæ—¶è¿›è¡Œæ‰‹åŠ¨å·¥ä½œã€‚è¿™å°±æå‡ºäº†ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•åˆ©ç”¨æ”¾å°„å­¦æŠ¥å‘Šæ¥æ”¹å–„ä»…ä½¿ç”¨å›¾åƒçš„åˆ†ç±»ï¼Ÿå…ˆå‰çš„å·¥ä½œä»…é™äºè¯„ä¼°é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å›¾åƒè¡¨ç¤ºæ¥é¢„æµ‹ä»æŠ¥å‘Šä¸­æå–çš„è¯Šæ–­æ ‡ç­¾ï¼Œå¿½ç•¥äº†ä¸æ–‡æœ¬å…³è”è¾ƒå¼±çš„æ ‡ç­¾ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹å¦‚ä½•åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æ”¾å°„å­¦æŠ¥å‘Šè¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œæ¶µç›–äº†è¯Šæ–­å’Œé¢„åä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œ12ä¸ªæœˆå†å…¥é™¢ï¼‰ï¼Œä»¥åŠä¸åŒå¤§å°çš„è®­ç»ƒé›†ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼šï¼ˆ1ï¼‰åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨æŠ¥å‘Šå¯¹äºä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡æ˜¯æœ‰ç›Šçš„ï¼Œè¿™äº›ä»»åŠ¡çš„æ ‡ç­¾åœ¨æ–‡æœ¬ä¸­å¾—åˆ°äº†å¾ˆå¥½çš„è¡¨ç¤ºï¼›ç„¶è€Œï¼Œé€šè¿‡æ˜ç¡®çš„å›¾åƒæ–‡æœ¬å¯¹é½è¿›è¡Œé¢„è®­ç»ƒå¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹æ˜¯æœ‰å®³çš„ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨æŠ¥å‘Šè¿›è¡Œå¾®è°ƒå¯ä»¥å¯¼è‡´æ˜¾ç€æ”¹è¿›ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹å…¶å½±å“å¤§äºé¢„è®­ç»ƒçš„æ–¹æ³•ã€‚è¿™äº›ç»“æœæä¾›äº†å¦‚ä½•åˆ©ç”¨ç‰¹æƒæ–‡æœ¬æ•°æ®æ¥è®­ç»ƒåŒ»å­¦å›¾åƒåˆ†ç±»å™¨çš„å¯æ“ä½œè§è§£ï¼ŒåŒæ—¶çªå‡ºäº†å½“å‰ç ”ç©¶çš„ç©ºç™½ä¹‹å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24385v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œå¸¸ä¼´éšæœ‰åŒ…å«ä¸°å¯Œä¸“å®¶æ³¨é‡Šçš„æ”¾å°„å­¦æŠ¥å‘Šã€‚ç„¶è€Œï¼Œä¾èµ–è¿™äº›æŠ¥å‘Šä½œä¸ºä¸´åºŠé¢„æµ‹è¾“å…¥éœ€è¦è®­ç»ƒæœ‰ç´ çš„æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡ŒåŠæ—¶çš„æ‰‹åŠ¨å·¥ä½œã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•åˆ©ç”¨æ”¾å°„å­¦æŠ¥å‘Šæ”¹è¿›ä»…åŸºäºå›¾åƒçš„åˆ†ç±»é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µåˆ©ç”¨æŠ¥å‘Šå¯¹ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡æœ‰ç›Šï¼Œä½†åœ¨æ ‡ç­¾åœ¨æ–‡æœ¬ä¸­ä»£è¡¨æ€§ä¸è¶³çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ˜ç¡®çš„å›¾åƒæ–‡æœ¬å¯¹é½è¿›è¡Œé¢„è®­ç»ƒå¯èƒ½æ˜¯æœ‰å®³çš„ã€‚æ­¤å¤–ï¼Œç”¨æŠ¥å‘Šè¿›è¡Œå¾®è°ƒå¯èƒ½å¯¼è‡´æ˜¾è‘—æ”¹è¿›ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå…¶å½±å“å¤§äºé¢„è®­ç»ƒçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæ”¾å°„å­¦æŠ¥å‘Šå¯ä½œä¸ºé‡è¦èµ„æºã€‚</li>
<li>æŠ¥å‘Šä¸­çš„ä¿¡æ¯å¯ä»¥åœ¨é¢„è®­ç»ƒé˜¶æ®µç”¨äºæ”¹è¿›å›¾åƒåˆ†ç±»ã€‚</li>
<li>å½“æ ‡ç­¾åœ¨æŠ¥å‘Šä¸­çš„ä»£è¡¨æ€§è¶³å¤Ÿæ—¶ï¼Œåˆ©ç”¨æŠ¥å‘Šè¿›è¡Œé¢„è®­ç»ƒå¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç›Šã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé€šè¿‡å›¾åƒæ–‡æœ¬æ˜ç¡®å¯¹é½è¿›è¡Œé¢„è®­ç»ƒå¯èƒ½æœ‰é£é™©ã€‚</li>
<li>ä½¿ç”¨æŠ¥å‘Šè¿›è¡Œå¾®è°ƒå¯ä»¥å¸¦æ¥æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>åœ¨æŸäº›è®¾ç½®ä¸‹ï¼Œå¾®è°ƒçš„å½±å“å¯èƒ½å¤§äºé¢„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d21377bcf2cc6f5136a5c85c29a6e40e" align="middle">
<img src="https://picx.zhimg.com/v2-ffb09cf312e70a1df3558a0ac49c7b62" align="middle">
<img src="https://picx.zhimg.com/v2-737dbcafb9326d8d4962634bcad18cd6" align="middle">
<img src="https://picx.zhimg.com/v2-722db3f53daa57a9ac40e3062da5bb29" align="middle">
<img src="https://picx.zhimg.com/v2-a88b8da3dbac7b29e76b1123cef0cd99" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MiCADangelo-Fine-Grained-Reconstruction-of-Constrained-CAD-Models-from-3D-Scans"><a href="#MiCADangelo-Fine-Grained-Reconstruction-of-Constrained-CAD-Models-from-3D-Scans" class="headerlink" title="MiCADangelo: Fine-Grained Reconstruction of Constrained CAD Models from   3D Scans"></a>MiCADangelo: Fine-Grained Reconstruction of Constrained CAD Models from   3D Scans</h2><p><strong>Authors:Ahmet Serdar Karadeniz, Dimitrios Mallis, Danila Rukhovich, Kseniya Cherenkova, Anis Kacem, Djamila Aouada</strong></p>
<p>Computer-Aided Design (CAD) plays a foundational role in modern manufacturing and product development, often requiring designers to modify or build upon existing models. Converting 3D scans into parametric CAD representationsâ€“a process known as CAD reverse engineeringâ€“remains a significant challenge due to the high precision and structural complexity of CAD models. Existing deep learning-based approaches typically fall into two categories: bottom-up, geometry-driven methods, which often fail to produce fully parametric outputs, and top-down strategies, which tend to overlook fine-grained geometric details. Moreover, current methods neglect an essential aspect of CAD modeling: sketch-level constraints. In this work, we introduce a novel approach to CAD reverse engineering inspired by how human designers manually perform the task. Our method leverages multi-plane cross-sections to extract 2D patterns and capture fine parametric details more effectively. It enables the reconstruction of detailed and editable CAD models, outperforming state-of-the-art methods and, for the first time, incorporating sketch constraints directly into the reconstruction process. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨ç°ä»£åˆ¶é€ å’Œäº§å“å¼€å‘ä¸­å‘æŒ¥åŸºç¡€ä½œç”¨ï¼Œé€šå¸¸éœ€è¦è®¾è®¡å¸ˆä¿®æ”¹æˆ–åŸºäºç°æœ‰æ¨¡å‹è¿›è¡Œåˆ›å»ºã€‚å°†3Dæ‰«æè½¬åŒ–ä¸ºå‚æ•°åŒ–CADè¡¨ç¤ºçš„è¿‡ç¨‹ï¼Œå³æ‰€è°“çš„CADé€†å‘å·¥ç¨‹ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºCADæ¨¡å‹å…·æœ‰é«˜ç²¾åº¦å’Œç»“æ„å¤æ‚æ€§ã€‚ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šè‡ªä¸‹è€Œä¸Šçš„å‡ ä½•é©±åŠ¨æ–¹æ³•ï¼Œå¾€å¾€æ— æ³•äº§ç”Ÿå®Œå…¨å‚æ•°åŒ–çš„è¾“å‡ºï¼›è‡ªä¸Šè€Œä¸‹çš„ç­–ç•¥ï¼Œå¾€å¾€å¿½ç•¥ç»†ç²’åº¦å‡ ä½•ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•å¿½ç•¥äº†CADå»ºæ¨¡çš„ä¸€ä¸ªåŸºæœ¬æ–¹é¢ï¼šè‰å›¾çº§åˆ«çš„çº¦æŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å—åˆ°äººç±»è®¾è®¡å¸ˆæ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡çš„å¯å‘ï¼Œä»‹ç»äº†ä¸€ç§æ–°çš„CADé€†å‘å·¥ç¨‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤šå¹³é¢æˆªé¢æå–äºŒç»´å›¾æ¡ˆï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰ç²¾ç»†å‚æ•°ç»†èŠ‚ã€‚å®ƒèƒ½å¤Ÿå®ç°è¯¦ç»†çš„å¯ç¼–è¾‘CADæ¨¡å‹çš„é‡å»ºï¼Œä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”é¦–æ¬¡å°†è‰å›¾çº¦æŸç›´æ¥çº³å…¥é‡å»ºè¿‡ç¨‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23429v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨ç°ä»£åˆ¶é€ ä¸šå’Œäº§å“å¼€å‘ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠå°†3Dæ‰«æè½¬æ¢ä¸ºå‚æ•°åŒ–CADè¡¨ç¤ºï¼ˆå³CADé€†å‘å·¥ç¨‹ï¼‰çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•å®Œå…¨æ»¡è¶³éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å—äººç±»è®¾è®¡å¸ˆæ‰‹åŠ¨æ“ä½œå¯å‘çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å¤šå¹³é¢æˆªé¢æå–2Dæ¨¡å¼ï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰ç²¾ç»†å‚æ•°ç»†èŠ‚ï¼Œå¹¶é¦–æ¬¡å°†è‰å›¾çº¦æŸç›´æ¥çº³å…¥é‡å»ºè¿‡ç¨‹ï¼Œèƒ½å¤Ÿé‡å»ºè¯¦ç»†ä¸”å¯ç¼–è¾‘çš„CADæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨ç°ä»£åˆ¶é€ ä¸šå’Œäº§å“å¼€å‘ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œ3Dæ‰«æè½¬å‚æ•°åŒ–CADè¡¨ç¤ºï¼ˆCADé€†å‘å·¥ç¨‹ï¼‰æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•åˆ†ä¸ºè‡ªä¸‹è€Œä¸Šçš„å‡ ä½•é©±åŠ¨æ–¹æ³•å’Œè‡ªä¸Šè€Œä¸‹çš„ç­–ç•¥ï¼Œå‰è€…éš¾ä»¥äº§ç”Ÿå®Œå…¨å‚æ•°åŒ–è¾“å‡ºï¼Œåè€…å¿½ç•¥ç»†èŠ‚ã€‚</li>
<li>å½“å‰æ–¹æ³•å¿½è§†äº†CADå»ºæ¨¡çš„é‡è¦æ–¹é¢ï¼šè‰å›¾çº§çº¦æŸã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„CADé€†å‘å·¥ç¨‹æ–¹æ³•ï¼Œå—äººç±»è®¾è®¡å¸ˆæ“ä½œå¯å‘ï¼Œåˆ©ç”¨å¤šå¹³é¢æˆªé¢æå–2Dæ¨¡å¼ã€‚</li>
<li>è¯¥æ–¹æ³•æ›´æœ‰æ•ˆåœ°æ•æ‰ç²¾ç»†å‚æ•°ç»†èŠ‚ï¼Œå¹¶é¦–æ¬¡å°†è‰å›¾çº¦æŸçº³å…¥é‡å»ºè¿‡ç¨‹ã€‚</li>
<li>æ‰€ææ–¹æ³•èƒ½å¤Ÿé‡å»ºè¯¦ç»†ä¸”å¯ç¼–è¾‘çš„CADæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66cadc001aaa2d0692000f50bf123ed6" align="middle">
<img src="https://picx.zhimg.com/v2-50c5acd51d6a3ff8adcfab5e094202ae" align="middle">
<img src="https://picx.zhimg.com/v2-822b919f0e7254b2b8d711d1f3016e8b" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Progressive-Growing-of-Patch-Size-Curriculum-Learning-for-Accelerated-and-Improved-Medical-Image-Segmentation"><a href="#Progressive-Growing-of-Patch-Size-Curriculum-Learning-for-Accelerated-and-Improved-Medical-Image-Segmentation" class="headerlink" title="Progressive Growing of Patch Size: Curriculum Learning for Accelerated   and Improved Medical Image Segmentation"></a>Progressive Growing of Patch Size: Curriculum Learning for Accelerated   and Improved Medical Image Segmentation</h2><p><strong>Authors:Stefan M. Fischer, Johannes Kiechle, Laura Daza, Lina Felsner, Richard Osuala, Daniel M. Lang, Karim Lekadir, Jan C. Peeken, Julia A. Schnabel</strong></p>
<p>In this work, we introduce Progressive Growing of Patch Size, an automatic curriculum learning approach for 3D medical image segmentation. Our approach progressively increases the patch size during model training, resulting in an improved class balance for smaller patch sizes and accelerated convergence of the training process. We evaluate our curriculum approach in two settings: a resource-efficient mode and a performance mode, both regarding Dice score performance and computational costs across 15 diverse and popular 3D medical image segmentation tasks. The resource-efficient mode matches the Dice score performance of the conventional constant patch size sampling baseline with a notable reduction in training time to only 44%. The performance mode improves upon constant patch size segmentation results, achieving a statistically significant relative mean performance gain of 1.28% in Dice Score. Remarkably, across all 15 tasks, our proposed performance mode manages to surpass the constant patch size baseline in Dice Score performance, while simultaneously reducing training time to only 89%. The benefits are particularly pronounced for highly imbalanced tasks such as lesion segmentation tasks. Rigorous experiments demonstrate that our performance mode not only improves mean segmentation performance but also reduces performance variance, yielding more trustworthy model comparison. Furthermore, our findings reveal that the proposed curriculum sampling is not tied to a specific architecture but represents a broadly applicable strategy that consistently boosts performance across diverse segmentation models, including UNet, UNETR, and SwinUNETR. In summary, we show that this simple yet elegant transformation on input data substantially improves both Dice Score performance and training runtime, while being compatible across diverse segmentation backbones. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¸è¿›å¼æ–‘å—å¤§å°å¢é•¿ï¼ˆProgressive Growing of Patch Sizeï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ æ–‘å—å¤§å°ï¼Œä»è€Œæé«˜è¾ƒå°æ–‘å—å¤§å°çš„ç±»åˆ«å¹³è¡¡å¹¶åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹çš„æ”¶æ•›ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§è®¾ç½®ä¸‹è¯„ä¼°äº†æˆ‘ä»¬çš„è¯¾ç¨‹æ–¹æ³•ï¼šèµ„æºé«˜æ•ˆæ¨¡å¼å’Œæ€§èƒ½æ¨¡å¼ï¼Œè¿™ä¸¤ç§æ¨¡å¼éƒ½æ¶‰åŠDiceå¾—åˆ†æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ï¼Œè·¨è¶Šäº†å¤šæ ·ä¸”æµè¡Œçš„åäº”ç§ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚èµ„æºé«˜æ•ˆæ¨¡å¼ä¸å¸¸è§„æ’å®šæ–‘å—å¤§å°é‡‡æ ·çš„åŸºå‡†æ°´å¹³åŒ¹é…Diceå¾—åˆ†æ€§èƒ½ï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­åˆ°åªæœ‰ç™¾åˆ†ä¹‹å››åå››ã€‚æ€§èƒ½æ¨¡å¼åœ¨æ’å®šæ–‘å—å¤§å°åˆ†å‰²ç»“æœçš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œåœ¨Diceå¾—åˆ†ä¸Šå®ç°äº†ç»Ÿè®¡æ„ä¹‰ä¸Šç›¸å¯¹å¹³å‡æ€§èƒ½æå‡ç™¾åˆ†ä¹‹ä¸€ç‚¹äºŒå…«ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ‰€æœ‰åäº”é¡¹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„æ€§èƒ½æ¨¡å¼æˆåŠŸè¶…è¶Šäº†æ’å®šæ–‘å—å¤§å°çš„åŸºçº¿æ°´å¹³åœ¨Diceå¾—åˆ†æ€§èƒ½æ–¹é¢çš„è¡¨ç°ï¼ŒåŒæ—¶è¿˜å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­åˆ°åªæœ‰ç™¾åˆ†ä¹‹å…«åä¹ã€‚å¯¹äºé«˜åº¦ä¸å¹³è¡¡çš„ä»»åŠ¡ï¼ˆå¦‚ç—…å˜åˆ†å‰²ä»»åŠ¡ï¼‰çš„å¥½å¤„å°¤ä¸ºçªå‡ºã€‚ä¸¥æ ¼çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ€§èƒ½æ¨¡å¼ä¸ä»…æé«˜äº†å¹³å‡åˆ†å‰²æ€§èƒ½ï¼Œè¿˜é™ä½äº†æ€§èƒ½æ–¹å·®ï¼Œä»è€Œäº§ç”Ÿäº†æ›´å¯é çš„æ¨¡å‹æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è¯¾ç¨‹é‡‡æ ·å¹¶ä¸å±€é™äºç‰¹å®šçš„æ¶æ„ï¼Œè€Œæ˜¯ä¸€ç§å¹¿æ³›é€‚ç”¨çš„ç­–ç•¥ï¼Œåœ¨å„ç§åˆ†å‰²æ¨¡å‹ä¸­éƒ½èƒ½æŒç»­æå‡æ€§èƒ½ï¼ŒåŒ…æ‹¬UNetã€UNETRå’ŒSwinUNETRã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§ç®€å•è€Œä¼˜é›…çš„è¾“å…¥æ•°æ®è½¬æ¢åœ¨å¤§å¹…æé«˜Diceå¾—åˆ†æ€§èƒ½å’Œè®­ç»ƒè¿è¡Œæ—¶é—´çš„åŒæ—¶ï¼Œè¿˜å…¼å®¹å¤šç§åˆ†å‰²ä¸»å¹²æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23241v2">PDF</a> Journal Extension of â€œProgressive Growing of Patch Size:   Resource-Efficient Curriculum Learning for Dense Prediction Tasksâ€   (MICCAI2024) submitted to MedIA</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºæ¸è¿›å¼å¢é•¿è¡¥ä¸å¤§å°çš„è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œç”¨äº3DåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥æ–¹æ³•åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ è¡¥ä¸å¤§å°ï¼Œæé«˜äº†è¾ƒå°è¡¥ä¸å¤§å°çš„ç±»åˆ«å¹³è¡¡ï¼Œå¹¶åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹çš„æ”¶æ•›ã€‚é€šè¿‡èµ„æºé«˜æ•ˆæ¨¡å¼å’Œæ€§èƒ½æ¨¡å¼ä¸¤ç§è®¾ç½®éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚èµ„æºé«˜æ•ˆæ¨¡å¼åœ¨ä¿æŒä¸å¸¸è§„æ’å®šè¡¥ä¸å¤§å°é‡‡æ ·åŸºå‡†ç›¸åŒçš„Diceå¾—åˆ†æ€§èƒ½çš„åŒæ—¶ï¼Œå°†è®­ç»ƒæ—¶é—´ç¼©çŸ­è‡³44%ã€‚æ€§èƒ½æ¨¡å¼åˆ™å®ç°äº†å¯¹æ’å®šè¡¥ä¸å¤§å°åˆ†å‰²ç»“æœçš„æ”¹è¿›ï¼Œç›¸å¯¹å¹³å‡æ€§èƒ½æå‡1.28%ã€‚åœ¨æ‰€æœ‰15é¡¹ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½æ¨¡å¼åœ¨Diceå¾—åˆ†æ€§èƒ½ä¸Šè¶…è¶Šäº†æ’å®šè¡¥ä¸å¤§å°åŸºå‡†çº¿ï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´ç¼©çŸ­è‡³89%ã€‚è¯¥æ–¹æ³•å¯¹é«˜åº¦ä¸å¹³è¡¡çš„ä»»åŠ¡ï¼Œå¦‚ç—…å˜åˆ†å‰²ä»»åŠ¡ï¼Œå…·æœ‰ç‰¹åˆ«æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘ç°ï¼Œæ‰€æå‡ºçš„è¯¾ç¨‹é‡‡æ ·ç­–ç•¥å¹¶éå±€é™äºç‰¹å®šçš„æ¶æ„ï¼Œè€Œæ˜¯ä¸€ç§å¹¿æ³›é€‚ç”¨çš„ç­–ç•¥ï¼Œå¯åœ¨å„ç§åˆ†å‰²æ¨¡å‹ï¼ˆåŒ…æ‹¬UNetã€UNETRå’ŒSwinUNETRï¼‰ä¸­ä¸æ–­æé«˜æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶è¯æ˜è¿™ç§ç®€å•è€Œä¼˜é›…çš„è¾“å…¥æ•°æ®è½¬æ¢å¯æ˜¾è‘—æé«˜Diceå¾—åˆ†æ€§èƒ½å’Œè®­ç»ƒæ—¶é—´æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºæ¸è¿›å¼å¢é•¿è¡¥ä¸å¤§å°çš„è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ”¹å–„3DåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>é€šè¿‡ä¸¤ç§æ¨¡å¼ï¼ˆèµ„æºé«˜æ•ˆæ¨¡å¼å’Œæ€§èƒ½æ¨¡å¼ï¼‰éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>èµ„æºé«˜æ•ˆæ¨¡å¼åœ¨ä¿æŒDiceå¾—åˆ†æ€§èƒ½çš„åŒæ—¶å¤§å¹…ç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚</li>
<li>æ€§èƒ½æ¨¡å¼åœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¶…è¶Šäº†å¸¸è§„æ’å®šè¡¥ä¸å¤§å°åˆ†å‰²æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºé«˜åº¦ä¸å¹³è¡¡çš„ä»»åŠ¡å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>æ‰€æå‡ºçš„è¯¾ç¨‹é‡‡æ ·ç­–ç•¥é€‚ç”¨äºå¤šç§åˆ†å‰²æ¨¡å‹ï¼ŒåŒ…æ‹¬UNetã€UNETRå’ŒSwinUNETRã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-efddf700795f91cb9bcf5130dc699348" align="middle">
<img src="https://picx.zhimg.com/v2-25f1a7d2636c44248393ef9c9515f63b" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Scaling-Tumor-Segmentation-Best-Lessons-from-Real-and-Synthetic-Data"><a href="#Scaling-Tumor-Segmentation-Best-Lessons-from-Real-and-Synthetic-Data" class="headerlink" title="Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data"></a>Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data</h2><p><strong>Authors:Qi Chen, Xinze Zhou, Chen Liu, Hao Chen, Wenxuan Li, Zekun Jiang, Ziyan Huang, Yuxuan Zhao, Dexin Yu, Junjun He, Yefeng Zheng, Ling Shao, Alan Yuille, Zongwei Zhou</strong></p>
<p>AI for tumor segmentation is limited by the lack of large, voxel-wise annotated datasets, which are hard to create and require medical experts. In our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found that AI performance stopped improving after 1,500 scans. With synthetic data, we reached the same performance using only 500 real scans. This finding suggests that synthetic data can steepen data scaling laws, enabling more efficient model training than real data alone. Motivated by these lessons, we created AbdomenAtlas 2.0â€“a dataset of 10,135 CT scans with a total of 15,130 tumor instances per-voxel manually annotated in six organs (pancreas, liver, kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23 expert radiologists, it is several orders of magnitude larger than existing public tumor datasets. While we continue expanding the dataset, the current version of AbdomenAtlas 2.0 already provides a strong foundationâ€“based on lessons from the JHH datasetâ€“for training AI to segment tumors in six organs. It achieves notable improvements over public datasets, with a +7% DSC gain on in-distribution tests and +16% on out-of-distribution tests. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½åœ¨è‚¿ç˜¤åˆ†å‰²æ–¹é¢çš„åº”ç”¨å—é™äºå¤§è§„æ¨¡ã€é€ä½“ç´ æ ‡æ³¨çš„æ•°æ®é›†çš„ç¼ºä¹ï¼Œè¿™ç±»æ•°æ®é›†çš„åˆ›å»ºå›°éš¾ï¼Œéœ€è¦åŒ»å­¦ä¸“å®¶å‚ä¸ã€‚åœ¨æˆ‘ä»¬æ‹¥æœ‰çš„3000ä¸ªæ ‡æ³¨èƒ°è…ºè‚¿ç˜¤æ‰«æçš„JHHæ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬å‘ç°äººå·¥æ™ºèƒ½çš„æ€§èƒ½åœ¨1500æ¬¡æ‰«æåå°±ä¸å†æé«˜ã€‚ä½¿ç”¨åˆæˆæ•°æ®ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨500æ¬¡çœŸå®æ‰«æå°±è¾¾åˆ°äº†åŒæ ·çš„æ€§èƒ½ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œåˆæˆæ•°æ®å¯ä»¥åŠ å¿«æ•°æ®æ‰©å±•è§„å¾‹ï¼Œä½¿å¾—æ¨¡å‹è®­ç»ƒæ¯”å•ç‹¬ä½¿ç”¨çœŸå®æ•°æ®æ›´åŠ é«˜æ•ˆã€‚åŸºäºè¿™äº›ç»éªŒï¼Œæˆ‘ä»¬åˆ›å»ºäº†AbdomenAtlas 2.0æ•°æ®é›†ï¼ŒåŒ…å«10,135ä¸ªCTæ‰«æï¼Œå…±æ‰‹åŠ¨é€ä½“ç´ æ ‡æ³¨äº†15,130ä¸ªè‚¿ç˜¤å®ä¾‹ï¼Œæ¶‰åŠå…­ä¸ªå™¨å®˜ï¼ˆèƒ°è…ºã€è‚è„ã€è‚¾è„ã€ç»“è‚ ã€é£Ÿé“å’Œå­å®«ï¼‰ï¼Œè¿˜æœ‰5,893ä¸ªå¯¹ç…§æ‰«æã€‚ç”±23ä½ä¸“ä¸šæ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œæ ‡æ³¨ï¼Œå…¶è§„æ¨¡è¿œå¤§äºç°æœ‰çš„å…¬å…±è‚¿ç˜¤æ•°æ®é›†ã€‚è™½ç„¶æˆ‘ä»¬æ­£åœ¨ç»§ç»­æ‰©å¤§æ•°æ®é›†ï¼Œä½†AbdomenAtlas 2.0çš„å½“å‰ç‰ˆæœ¬å·²ç»ä¸ºåœ¨å…­ä¸ªå™¨å®˜ä¸­è®­ç»ƒç”¨äºåˆ†å‰²è‚¿ç˜¤çš„AIæä¾›äº†åšå®çš„åŸºç¡€ï¼ŒåŸºäºJHHæ•°æ®é›†çš„ç»éªŒï¼Œå®ƒåœ¨å…¬å…±æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨å†…éƒ¨æµ‹è¯•ä¸­è·å¾—äº†+7%çš„DSCå¢ç›Šï¼Œåœ¨å¤–éƒ¨æµ‹è¯•ä¸Šè·å¾—äº†+16%çš„å¢ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14831v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong><br>     è‚¿ç˜¤åˆ†å‰²äººå·¥æ™ºèƒ½å—é™äºå¤§è§„æ¨¡ã€é€åƒç´ æ³¨é‡Šæ•°æ®é›†çš„ç¼ºä¹ï¼Œåˆ›å»ºæ­¤ç±»æ•°æ®é›†å›°éš¾ä¸”éœ€è¦åŒ»å­¦ä¸“å®¶ã€‚åœ¨è‡ªæœ‰JHHèƒ°è…ºè‚¿ç˜¤æ‰«ææ•°æ®é›†ï¼ˆå«3000ä¸ªæ³¨é‡Šï¼‰ä¸­å‘ç°ï¼ŒAIæ€§èƒ½åœ¨1500ä¸ªæ‰«æåä¸å†æå‡ã€‚åˆæˆæ•°æ®çš„ä½¿ç”¨ä»…ä½¿ç”¨500ä¸ªçœŸå®æ‰«æå³è¾¾åˆ°ç›¸åŒæ€§èƒ½ï¼Œæ˜¾ç¤ºåˆæˆæ•°æ®å¯ä¼˜åŒ–æ•°æ®ç¼©æ”¾æ³•åˆ™ï¼Œæ¯”å•ç‹¬ä½¿ç”¨çœŸå®æ•°æ®æ›´é«˜æ•ˆè®­ç»ƒæ¨¡å‹ã€‚åŸºäºè¿™äº›ç»éªŒï¼Œåˆ›å»ºäº†AbdomenAtlas 2.0æ•°æ®é›†ï¼ŒåŒ…å«æ‰‹åŠ¨é€åƒç´ æ³¨é‡Šçš„å…­ä¸ªå™¨å®˜ï¼ˆèƒ°è…ºã€è‚è„ã€è‚¾è„ã€ç»“è‚ ã€é£Ÿé“å’Œå­å®«ï¼‰çš„è‚¿ç˜¤å®ä¾‹å…±15,130ä¸ªï¼Œæ§åˆ¶æ‰«ææ•°é‡è¾¾æ•°åƒå¼ ã€‚ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿæ ‡æ³¨çš„æ•°æ®é›†æ¯”ç°æœ‰å…¬å…±è‚¿ç˜¤æ•°æ®é›†å¤§å‡ ä¸ªæ•°é‡çº§ã€‚ç›®å‰ç‰ˆæœ¬å·²åŸºäºJHHæ•°æ®é›†çš„ç»éªŒä¸ºè®­ç»ƒAIåˆ†å‰²å…­ä¸ªå™¨å®˜çš„è‚¿ç˜¤æä¾›äº†åšå®åŸºç¡€ï¼Œç›¸è¾ƒäºå…¬å…±æ•°æ®é›†æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨å†…éƒ¨æµ‹è¯•ä¸­å¯æé«˜DSCå¾—åˆ†7%ï¼Œå¤–éƒ¨æµ‹è¯•ä¸­æé«˜DSCå¾—åˆ†è¾¾16%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. AIåœ¨è‚¿ç˜¤åˆ†å‰²é¢†åŸŸå—é™äºç¼ºä¹å¤§è§„æ¨¡é€åƒç´ æ³¨é‡Šçš„æ•°æ®é›†ã€‚
 2. åˆ›å»ºè¿™ç§æ•°æ®é›†æ—¢å›°éš¾åˆéœ€è¦åŒ»å­¦ä¸“å®¶å‚ä¸ã€‚
 3. åœ¨è‡ªæœ‰JHHæ•°æ®é›†ä¸­å‘ç°AIæ€§èƒ½åœ¨è¾¾åˆ°ä¸€å®šæ‰«ææ•°é‡åä¸å†æå‡ã€‚
 4. åˆæˆæ•°æ®èƒ½å¤Ÿä¼˜åŒ–æ•°æ®ç¼©æ”¾æ³•åˆ™ï¼Œå¹¶å¯å®ç°ä»…ä½¿ç”¨è¾ƒå°‘çœŸå®æ•°æ®å°±è¾¾åˆ°é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒæ•ˆæœã€‚
 5. AbdomenAtlas 2.0æ•°æ®é›†ç”±å¤šä¸ªå™¨å®˜çš„è‚¿ç˜¤å®ä¾‹ç»„æˆï¼ŒåŒ…æ‹¬èƒ°è…ºã€è‚è„ç­‰å…­ä¸ªå™¨å®˜ï¼Œä¸”æ•°æ®é›†è§„æ¨¡è¿œè¶…ç°æœ‰å…¬å…±è‚¿ç˜¤æ•°æ®é›†ã€‚
 6. AbdomenAtlas 2.0æ•°æ®é›†çš„åˆ›å»ºå¾—ç›Šäºä»JHHæ•°æ®é›†ä¸­æ±²å–çš„ç»éªŒæ•™è®­ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7262569cdf5d3cc6f24bb905adb3f67c" align="middle">
<img src="https://picx.zhimg.com/v2-7aa5491dde1e7aca402cb5e170b1637b" align="middle">
<img src="https://picx.zhimg.com/v2-5aa866291d49c915bd3e8e404209f8f5" align="middle">
<img src="https://picx.zhimg.com/v2-97065db79880b1b44a9274ff3682f790" align="middle">
<img src="https://picx.zhimg.com/v2-f97a49e509daf051fea0260a59943e3b" align="middle">
<img src="https://picx.zhimg.com/v2-cf831d3242d2f593e52fa9e3a2665e64" align="middle">
<img src="https://picx.zhimg.com/v2-4103ba0e4089ce0cbe4aac2beae6ac2c" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="WeCKD-Weakly-supervised-Chained-Distillation-Network-for-Efficient-Multimodal-Medical-Imaging"><a href="#WeCKD-Weakly-supervised-Chained-Distillation-Network-for-Efficient-Multimodal-Medical-Imaging" class="headerlink" title="WeCKD: Weakly-supervised Chained Distillation Network for Efficient   Multimodal Medical Imaging"></a>WeCKD: Weakly-supervised Chained Distillation Network for Efficient   Multimodal Medical Imaging</h2><p><strong>Authors:Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach</strong></p>
<p>Knowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning and addresses the limitations of one-step KD. Each model in the chain is trained on only a fraction of the dataset and shows that effective learning can be achieved with minimal supervision. Extensive evaluation on six imaging datasets across otoscopic, microscopic, and magnetic resonance imaging modalities shows that it generalizes and outperforms existing methods. Furthermore, the proposed distillation chain resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption. </p>
<blockquote>
<p>çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ä¼ ç»Ÿä¸Šä¾èµ–äºé™æ€çš„æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œå…¶ä¸­å¤§å‹ã€è®­ç»ƒè‰¯å¥½çš„æ•™å¸ˆæ¨¡å‹å°†çŸ¥è¯†è½¬ç§»ç»™å•ä¸€çš„å­¦ç”Ÿæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸é­å—çŸ¥è¯†é€€åŒ–ã€ç›‘ç£æ•ˆç‡ä½ä¸‹ä»¥åŠä¾èµ–éå¸¸å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹æˆ–å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†å¼±ç›‘ç£é“¾å¼KDç½‘ç»œï¼ˆWeCKDï¼‰ï¼Œå®ƒé€šè¿‡ä¸€ç³»åˆ—ç›¸äº’è¿æ¥çš„ç»“æ„åŒ–æ¨¡å‹é‡æ–°å®šä¹‰çŸ¥è¯†è½¬ç§»ã€‚ä¸åŒäºä¼ ç»Ÿçš„KDï¼Œå®ƒå½¢æˆäº†ä¸€ä¸ªæ¸è¿›çš„è’¸é¦é“¾ï¼Œå…¶ä¸­æ¯ä¸ªæ¨¡å‹ä¸ä»…ä»å®ƒçš„å‰èº«ä¸­å­¦ä¹ ï¼Œè¿˜ç²¾ç‚¼çŸ¥è¯†åå†å‘å‰ä¼ é€’ã€‚è¿™ç§ç»“æ„åŒ–çš„çŸ¥è¯†è½¬ç§»è¿›ä¸€æ­¥å¢å¼ºäº†ç‰¹å¾å­¦ä¹ ï¼Œå¹¶è§£å†³äº†ä¸€æ­¥KDçš„å±€é™æ€§ã€‚é“¾ä¸­çš„æ¯ä¸ªæ¨¡å‹åªåœ¨æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶è¯æ˜åœ¨æœ€å°ç›‘ç£ä¸‹å¯ä»¥å®ç°æœ‰æ•ˆçš„å­¦ä¹ ã€‚åœ¨è€³é•œã€æ˜¾å¾®é•œå’Œç£å…±æŒ¯æˆåƒç­‰å…­ç§æˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œå®ƒå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å¹¶åœ¨ç°æœ‰æ–¹æ³•ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œä¸åœ¨ç›¸åŒæœ‰é™æ•°æ®ä¸Šè®­ç»ƒçš„å•ä¸ªä¸»å¹²ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„çŸ¥è¯†è’¸é¦é“¾çš„ç´¯ç§¯ç²¾åº¦æé«˜äº†é«˜è¾¾+23%ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14668v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£é“¾å¼çŸ¥è¯†è’¸é¦ç½‘ç»œï¼ˆWeCKDï¼‰ï¼Œè¯¥ç½‘ç»œé€šè¿‡ä¸€ç³»åˆ—äº’è”æ¨¡å‹è¿›è¡Œç»“æ„åŒ–çŸ¥è¯†è½¬ç§»ï¼Œè§£å†³äº†ä¼ ç»ŸçŸ¥è¯†è’¸é¦ä¸­çš„çŸ¥è¯†é€€åŒ–ã€ç›‘ç£æ•ˆç‡ä½ä¸‹ä»¥åŠå¯¹å¼ºæ•™å¸ˆæ¨¡å‹æˆ–å¤§é‡æ ‡è®°æ•°æ®é›†çš„ä¾èµ–é—®é¢˜ã€‚WeCKDå½¢æˆäº†ä¸€ç§æ¸è¿›å¼çš„è’¸é¦é“¾ï¼Œæ¯ä¸ªæ¨¡å‹ä¸ä»…ä»å‰ä¸€ä¸ªæ¨¡å‹å­¦ä¹ ï¼Œè¿˜ç²¾ç‚¼çŸ¥è¯†åä¼ é€’ç»™ä¸‹ä¸€ä¸ªæ¨¡å‹ã€‚è¿™ç§ç»“æ„åŒ–çš„çŸ¥è¯†è½¬ç§»æé«˜äº†ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶å…‹æœäº†ä¸€æ­¥çŸ¥è¯†è’¸é¦çš„å±€é™æ€§ã€‚åœ¨å…­ä¸ªæˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è€³é•œã€æ˜¾å¾®é•œå’Œç£å…±æŒ¯æˆåƒç­‰å¤šç§æ¨¡æ€ä¸‹å…·æœ‰é€šç”¨æ€§å¹¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä¸åœ¨ç›¸åŒæœ‰é™æ•°æ®ä¸Šè®­ç»ƒçš„å•ä¸€ä¸»å¹²ç›¸æ¯”ï¼Œè’¸é¦é“¾çš„ç´¯ç§¯å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾+23%ï¼Œçªæ˜¾äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸçŸ¥è¯†è’¸é¦ä¾èµ–äºé™æ€çš„æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œå­˜åœ¨çŸ¥è¯†é€€åŒ–ã€ç›‘ç£æ•ˆç‡ä½ä¸‹å’Œå¯¹å¼ºæ•™å¸ˆæ¨¡å‹çš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>WeCKDç½‘ç»œæ˜¯ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£é“¾å¼çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡ä¸€ç³»åˆ—äº’è”æ¨¡å‹è¿›è¡Œç»“æ„åŒ–çŸ¥è¯†è½¬ç§»ã€‚</li>
<li>WeCKDå½¢æˆæ¸è¿›å¼çš„è’¸é¦é“¾ï¼Œæ¯ä¸ªæ¨¡å‹ä¸ä»…å­¦ä¹ å‰åºæ¨¡å‹çš„çŸ¥è¯†ï¼Œè¿˜ç²¾ç‚¼åä¼ é€’ã€‚</li>
<li>ç»“æ„åŒ–çš„çŸ¥è¯†è½¬ç§»æé«˜äº†ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶è§£å†³äº†å•æ­¥çŸ¥è¯†è’¸é¦çš„å±€é™æ€§ã€‚</li>
<li>WeCKDåœ¨å¤šç§æˆåƒæ¨¡æ€ä¸‹å…·æœ‰é€šç”¨æ€§ï¼Œå¹¶åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ä¸å•ä¸€ä¸»å¹²ç›¸æ¯”ï¼Œè’¸é¦é“¾çš„ç´¯ç§¯å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>WeCKDçš„æ½œåŠ›åœ¨äºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ•°æ®çš„åœºæ™¯ä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53b0e7f272dd5a896e21fc4d208dc8a1" align="middle">
<img src="https://picx.zhimg.com/v2-a8265f3882a2e33a5375c0493ed9d46d" align="middle">
<img src="https://picx.zhimg.com/v2-374467f39200a04abb9ad2c053800fd7" align="middle">
<img src="https://picx.zhimg.com/v2-8f1b4b9a30f9904ce815e8753b936fae" align="middle">
<img src="https://picx.zhimg.com/v2-a6200db74a64e4d8a12af6d2f4b84d39" align="middle">
<img src="https://picx.zhimg.com/v2-99310e9ddc006c831a17c1097ebcdade" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-556d576aae11f28777e455235f7357a4" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Augmenting Open-Vocabulary Dysarthric Speech Assessment with Human   Perceptual Supervision
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-40850e19634e7e0e853ad21db3af08c1" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Diffusion Models are Robust Pretrainers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
