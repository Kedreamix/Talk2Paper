<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Agent-Omni Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7b2fece82c8b0a799c4af337bf5051aa')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-06-æ›´æ–°"><a href="#2025-11-06-æ›´æ–°" class="headerlink" title="2025-11-06 æ›´æ–°"></a>2025-11-06 æ›´æ–°</h1><h2 id="Agent-Omni-Test-Time-Multimodal-Reasoning-via-Model-Coordination-for-Understanding-Anything"><a href="#Agent-Omni-Test-Time-Multimodal-Reasoning-via-Model-Coordination-for-Understanding-Anything" class="headerlink" title="Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything"></a>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything</h2><p><strong>Authors:Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh</strong></p>
<p>Multimodal large language models (MLLMs) have shown strong capabilities but remain limited to fixed modality pairs and require costly fine-tuning with large aligned datasets. Building fully omni-capable models that can integrate text, images, audio, and video remains impractical and lacks robust reasoning support. In this paper, we propose an Agent-Omni framework that coordinates existing foundation models through a master-agent system, enabling flexible multimodal reasoning without retraining. The master agent interprets user intent, delegates subtasks to modality-specific agents, and integrates their outputs into coherent responses. Extensive experiments across text, image, audio, video, and omni benchmarks show that Agent-Omni consistently achieves state-of-the-art performance, particularly on tasks requiring complex cross-modal reasoning. Its agent-based design enables seamless integration of specialized foundation models, ensuring adaptability to diverse inputs while maintaining transparency and interpretability. In addition, the framework is modular and easily extensible, allowing future improvements as stronger models become available. %We release an open-source implementation to support continued research on scalable and reliable omni-modal reasoning. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»æ˜¾ç¤ºå‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»…é™äºå›ºå®šçš„æ¨¡æ€å¯¹ï¼Œå¹¶ä¸”éœ€è¦æ˜‚è´µçš„ç²¾ç»†è°ƒæ•´ä¸å¤§å‹å¯¹é½æ•°æ®é›†ã€‚æ„å»ºèƒ½å¤Ÿæ•´åˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘çš„å®Œå…¨é€šç”¨æ¨¡å‹ä»ç„¶ä¸åˆ‡å®é™…ï¼Œç¼ºä¹ç¨³å¥çš„æ¨ç†æ”¯æŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªAgent-Omniæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿåè°ƒç°æœ‰çš„åŸºç¡€æ¨¡å‹ï¼Œå®ç°çµæ´»çš„å¤šæ¨¡æ€æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¸»ä»£ç†è§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå°†å­ä»»åŠ¡å§”æ´¾ç»™ç‰¹å®šæ¨¡æ€çš„ä»£ç†ï¼Œå¹¶å°†ä»–ä»¬çš„è¾“å‡ºæ•´åˆä¸ºè¿è´¯çš„å“åº”ã€‚åœ¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œå…¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAgent-OmniæŒç»­å®ç°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚å…¶åŸºäºä»£ç†çš„è®¾è®¡å¯å®ç°ä¸“ä¸šåŸºç¡€æ¨¡å‹çš„æ— ç¼é›†æˆï¼Œç¡®ä¿é€‚åº”å„ç§è¾“å…¥çš„åŒæ—¶ä¿æŒé€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¯æ¨¡å—åŒ–çš„ä¸”æ˜“äºæ‰©å±•ï¼Œå…è®¸éšç€æ›´å¼ºå¤§çš„æ¨¡å‹çš„å‡ºç°è€Œè¿›è¡Œæœªæ¥æ”¹è¿›ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªå¼€æºå®ç°ï¼Œä»¥æ”¯æŒåœ¨å¯æ‰©å±•å’Œå¯é çš„å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„æŒç»­ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02834v1">PDF</a> 16 pages, 7 figures, 14 tables. Under Review</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§Agent-Omniæ¡†æ¶ï¼Œé€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿåè°ƒç°æœ‰çš„åŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†çµæ´»çš„å¤šæ¨¡æ€æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå°†å­ä»»åŠ¡å§”æ´¾ç»™ç‰¹å®šæ¨¡æ€çš„ä»£ç†ï¼Œå¹¶å°†å®ƒä»¬çš„è¾“å‡ºæ•´åˆä¸ºè¿è´¯çš„å“åº”ã€‚Agent-Omniåœ¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œå…¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚å…¶åŸºäºä»£ç†çš„è®¾è®¡å¯æ— ç¼é›†æˆä¸“ä¸šåŸºç¡€æ¨¡å‹ï¼Œç¡®ä¿é€‚åº”å„ç§è¾“å…¥çš„åŒæ—¶ä¿æŒé€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agent-Omniæ¡†æ¶é€šè¿‡ä¸»ä»£ç†ç³»ç»Ÿåè°ƒç°æœ‰åŸºç¡€æ¨¡å‹ï¼Œå®ç°çµæ´»å¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿè§£é‡Šç”¨æˆ·æ„å›¾ï¼Œå¹¶å§”æ´¾å­ä»»åŠ¡ç»™ç‰¹å®šæ¨¡æ€çš„ä»£ç†ã€‚</li>
<li>Agent-Omniåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨éœ€è¦å¤æ‚è·¨æ¨¡æ€æ¨ç†çš„ä»»åŠ¡ä¸Šã€‚</li>
<li>æ¡†æ¶åŸºäºä»£ç†çš„è®¾è®¡å¯æ— ç¼é›†æˆä¸“ä¸šåŸºç¡€æ¨¡å‹ï¼Œé€‚åº”å¤šç§è¾“å…¥ã€‚</li>
<li>Agent-Omniä¿æŒé€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡æ˜“äºæ‰©å±•ï¼Œå¯éšç€æ›´å¼ºå¤§çš„æ¨¡å‹çš„å‡ºç°è€Œè¿›è¡Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18270fdfdd1011cf1d1c4150b952fe13" align="middle">
<img src="https://picx.zhimg.com/v2-dce1ac02af7fca26ab12ff5f7408c985" align="middle">
<img src="https://picx.zhimg.com/v2-c3a01543767599167d24ba00c9a0bea9" align="middle">
<img src="https://picx.zhimg.com/v2-91a10ea385811e0f083f0a09ed7c88e0" align="middle">
<img src="https://picx.zhimg.com/v2-703127fa741a33a4a6ff8843b40b6669" align="middle">
<img src="https://picx.zhimg.com/v2-a7e39165e1bc5d8d30214d90cf847b80" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MemSearcher-Training-LLMs-to-Reason-Search-and-Manage-Memory-via-End-to-End-Reinforcement-Learning"><a href="#MemSearcher-Training-LLMs-to-Reason-Search-and-Manage-Memory-via-End-to-End-Reinforcement-Learning" class="headerlink" title="MemSearcher: Training LLMs to Reason, Search and Manage Memory via   End-to-End Reinforcement Learning"></a>MemSearcher: Training LLMs to Reason, Search and Manage Memory via   End-to-End Reinforcement Learning</h2><p><strong>Authors:Qianhao Yuan, Jie Lou, Zichao Li, Jiawei Chen, Yaojie Lu, Hongyu Lin, Le Sun, Debing Zhang, Xianpei Han</strong></p>
<p>Typical search agents concatenate the entire interaction history into the LLM context, preserving information integrity but producing long, noisy contexts, resulting in high computation and memory costs. In contrast, using only the current turn avoids this overhead but discards essential information. This trade-off limits the scalability of search agents. To address this challenge, we propose MemSearcher, an agent workflow that iteratively maintains a compact memory and combines the current turn with it. At each turn, MemSearcher fuses the userâ€™s question with the memory to generate reasoning traces, perform search actions, and update memory to retain only information essential for solving the task. This design stabilizes context length across multi-turn interactions, improving efficiency without sacrificing accuracy. To optimize this workflow, we introduce multi-context GRPO, an end-to-end RL framework that jointly optimize reasoning, search strategies, and memory management of MemSearcher Agents. Specifically, multi-context GRPO samples groups of trajectories under different contexts and propagates trajectory-level advantages across all conversations within them. Trained on the same dataset as Search-R1, MemSearcher achieves significant improvements over strong baselines on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher even outperforms 7B-based baselines, demonstrating that striking a balance between information integrity and efficiency yields both higher accuracy and lower computational overhead. The code and models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/icip-cas/MemSearcher">https://github.com/icip-cas/MemSearcher</a> </p>
<blockquote>
<p>å…¸å‹çš„æœç´¢ä»£ç†å°†æ•´ä¸ªäº¤äº’å†å²ä¸²è”åˆ°LLMçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™ä¿è¯äº†ä¿¡æ¯çš„å®Œæ•´æ€§ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†å†—é•¿å’Œå™ªéŸ³å¤šçš„ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´äº†è®¡ç®—é‡å’Œå†…å­˜æˆæœ¬çš„å¢åŠ ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåªä½¿ç”¨å½“å‰å¯¹è¯å¯ä»¥é¿å…è¿™ç§å¼€é”€ï¼Œä½†åŒæ—¶ä¹Ÿèˆå¼ƒäº†é‡è¦ä¿¡æ¯ã€‚è¿™ç§æƒè¡¡é™åˆ¶äº†æœç´¢ä»£ç†çš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MemSearcherä»£ç†å·¥ä½œæµç¨‹ï¼Œå®ƒé€šè¿‡è¿­ä»£ç»´æŠ¤ç´§å‡‘çš„å†…å­˜å¹¶ç»“åˆå½“å‰å¯¹è¯æ¥å®ç°ã€‚åœ¨æ¯ä¸€è½®å¯¹è¯ä¸­ï¼ŒMemSearcherå°†ç”¨æˆ·çš„é—®é¢˜ä¸å†…å­˜èåˆï¼Œç”Ÿæˆæ¨ç†è½¨è¿¹ï¼Œæ‰§è¡Œæœç´¢æ“ä½œï¼Œå¹¶æ›´æ–°å†…å­˜ä»¥ä»…ä¿ç•™å¯¹å®Œæˆä»»åŠ¡è‡³å…³é‡è¦çš„ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡ç¨³å®šäº†å¤šè½®äº¤äº’ä¸­çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæé«˜äº†æ•ˆç‡è€Œä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚ä¸ºäº†ä¼˜åŒ–è¿™ä¸€å·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šä¸Šä¸‹æ–‡GRPOï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„RLæ¡†æ¶ï¼Œå¯ä»¥è”åˆä¼˜åŒ–MemSearcherä»£ç†çš„æ¨ç†ã€æœç´¢ç­–ç•¥å’Œå†…å­˜ç®¡ç†ã€‚å…·ä½“æ¥è¯´ï¼Œå¤šä¸Šä¸‹æ–‡GRPOåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­é‡‡æ ·è½¨è¿¹ç»„ï¼Œå¹¶ä¼ æ’­è½¨è¿¹çº§åˆ«çš„ä¼˜åŠ¿åœ¨å®ƒä»¬ä¸­çš„æ‰€æœ‰å¯¹è¯ä¸­ã€‚MemSearcheråœ¨ä¸ƒä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç›¸å¯¹äºSearch-R1çš„ç›¸åŒæ•°æ®é›†è®­ç»ƒçš„åŸºçº¿ï¼Œåœ¨Qwen2. 5-3B-Instructä¸Šç›¸å¯¹å¹³å‡å¢å¹…ä¸º+11%ï¼Œåœ¨Qwen2. 5-7B-Instructä¸Šä¸º+12%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäº3Bçš„MemSearcherç”šè‡³è¶…è¶Šäº†åŸºäº7Bçš„åŸºçº¿ï¼Œè¡¨æ˜åœ¨ä¿¡æ¯å®Œæ•´æ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡å¯ä»¥å¸¦æ¥æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„è®¡ç®—å¼€é”€ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/icip-cas/MemSearcher%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/icip-cas/MemSearcherå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02805v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/icip-cas/MemSearcher">https://github.com/icip-cas/MemSearcher</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>MemSearcheré€šè¿‡è¿­ä»£ç»´æŠ¤ç´§å‡‘å†…å­˜å¹¶ç»“åˆå½“å‰å›åˆçš„å†…å®¹æ¥è§£å†³æœç´¢å¼•æ“åœ¨é¢ä¸´ç”¨æˆ·è¿ç»­æé—®æ—¶äº§ç”Ÿçš„ä¿¡æ¯æ•´åˆé—®é¢˜ã€‚å®ƒåœ¨æ¯æ¬¡å¯¹è¯å›åˆä¸­å°†ç”¨æˆ·é—®é¢˜ä¸å†…å­˜ç›¸èåˆï¼Œç”Ÿæˆæ¨ç†ç—•è¿¹ï¼Œæ‰§è¡Œæœç´¢æ“ä½œå¹¶æ›´æ–°ä»…ä¿ç•™è§£å†³é—®é¢˜æ‰€éœ€çš„å†…å­˜ä¿¡æ¯ã€‚è¯¥ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç§°ä¸ºå¤šä¸Šä¸‹æ–‡GRPOçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè”åˆä¼˜åŒ–MemSearcheræ¨ç†ã€æœç´¢ç­–ç•¥å’Œå†…å­˜ç®¡ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMemSearcheråœ¨ä¸ƒä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç›¸è¾ƒäºåŸºå‡†æµ‹è¯•æœ‰ç€å¹³å‡+11%çš„ç›¸å¯¹å¢ç›Šã€‚ç ”ç©¶è¯æ˜åœ¨ç»´æŠ¤ä¿¡æ¯å®Œæ•´æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´æ‰¾åˆ°å¹³è¡¡èƒ½åŒæ—¶æé«˜å‡†ç¡®æ€§å’Œé™ä½è®¡ç®—å¼€é”€ã€‚å…·ä½“ç»†èŠ‚å¯è®¿é—®ç›¸å…³ä»£ç å’Œæ¨¡å‹å…¬å¼€é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/icip-cas/MemSearcher%E3%80%82">https://github.com/icip-cas/MemSearcherã€‚</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>MemSearcherè§£å†³äº†æœç´¢å¼•æ“é¢ä¸´ç”¨æˆ·è¿ç»­æé—®æ—¶çš„ä¿¡æ¯æ•´åˆé—®é¢˜ï¼Œé€šè¿‡ç»´æŠ¤ç´§å‡‘å†…å­˜å¹¶ç»“åˆå½“å‰å›åˆå†…å®¹ç”Ÿæˆæ¨ç†ç—•è¿¹ã€‚</li>
<li>MemSearcherèƒ½å¤Ÿåœ¨æ¯æ¬¡å¯¹è¯å›åˆä¸­æ‰§è¡Œæœç´¢æ“ä½œå¹¶æ›´æ–°å†…å­˜ï¼Œä»…ä¿ç•™è§£å†³é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¤šä¸Šä¸‹æ–‡GRPOçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶è”åˆä¼˜åŒ–æ¨ç†ã€æœç´¢ç­–ç•¥å’Œå†…å­˜ç®¡ç†ã€‚</li>
<li>MemSearcheråœ¨ä¸ƒä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œç›¸å¯¹å¹³å‡å¢ç›Šè¾¾åˆ°+11%ã€‚</li>
<li>MemSearcheråœ¨ä¿æŒä¿¡æ¯å®Œæ•´æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´æ‰¾åˆ°äº†å¹³è¡¡ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„è®¡ç®—å¼€é”€ã€‚</li>
<li>ç ”ç©¶æˆæœåŒ…æ‹¬æ¨¡å‹å’Œä»£ç å…¬å¼€ï¼Œæ–¹ä¾¿ç ”ç©¶è€…å’Œå¼€å‘è€…è¿›è¡Œè¿›ä¸€æ­¥çš„æ¢ç´¢å’Œç ”ç©¶ã€‚å…·ä½“å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/icip-cas/MemSearcher%E4%BA%86%E8%A7%A3%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF%E3%80%82">https://github.com/icip-cas/MemSearcheräº†è§£è¯¦ç»†ä¿¡æ¯ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffcdbb527b964040e940673cf313b465" align="middle">
<img src="https://picx.zhimg.com/v2-dd0ba4e82a1e4659385cfcd38486fcfc" align="middle">
<img src="https://picx.zhimg.com/v2-7857437170b0687acc62f95d3615ae0d" align="middle">
<img src="https://picx.zhimg.com/v2-998e6417ead2c86f44e056f9b95e0af2" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="When-One-Modality-Sabotages-the-Others-A-Diagnostic-Lens-on-Multimodal-Reasoning"><a href="#When-One-Modality-Sabotages-the-Others-A-Diagnostic-Lens-on-Multimodal-Reasoning" class="headerlink" title="When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal   Reasoning"></a>When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal   Reasoning</h2><p><strong>Authors:Chenyu Zhang, Minsol Kim, Shohreh Ghorbani, Jingyao Wu, Rosalind Picard, Patricia Maes, Paul Pu Liang</strong></p>
<p>Despite rapid growth in multimodal large language models (MLLMs), their reasoning traces remain opaque: it is often unclear which modality drives a prediction, how conflicts are resolved, or when one stream dominates. In this paper, we introduce modality sabotage, a diagnostic failure mode in which a high-confidence unimodal error overrides other evidence and misleads the fused result. To analyze such dynamics, we propose a lightweight, model-agnostic evaluation layer that treats each modality as an agent, producing candidate labels and a brief self-assessment used for auditing. A simple fusion mechanism aggregates these outputs, exposing contributors (modalities supporting correct outcomes) and saboteurs (modalities that mislead). Applying our diagnostic layer in a case study on multimodal emotion recognition benchmarks with foundation models revealed systematic reliability profiles, providing insight into whether failures may arise from dataset artifacts or model limitations. More broadly, our framework offers a diagnostic scaffold for multimodal reasoning, supporting principled auditing of fusion dynamics and informing possible interventions. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿…é€Ÿå¢é•¿ï¼Œä½†å…¶æ¨ç†è½¨è¿¹ä»ç„¶ä¸æ˜ç¡®ï¼šé€šå¸¸ä¸æ¸…æ¥šæ˜¯å“ªä¸ªæ¨¡æ€é©±åŠ¨é¢„æµ‹ï¼Œå¦‚ä½•è§£å†³å†²çªï¼Œæˆ–è€…ä½•æ—¶ä¸€ä¸ªæµå ä¸»å¯¼åœ°ä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ¨¡æ€ç ´åï¼Œè¿™æ˜¯ä¸€ç§è¯Šæ–­å¤±è´¥æ¨¡å¼ï¼Œé«˜ç½®ä¿¡åº¦çš„å•æ¨¡æ€é”™è¯¯ä¼šè¦†ç›–å…¶ä»–è¯æ®å¹¶è¯¯å¯¼èåˆç»“æœã€‚ä¸ºäº†åˆ†æè¿™ç§åŠ¨æ€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„è¯„ä»·å±‚ï¼Œå®ƒå°†æ¯ä¸ªæ¨¡æ€è§†ä¸ºä»£ç†ï¼Œç”Ÿæˆå€™é€‰æ ‡ç­¾å’Œç®€çŸ­çš„è‡ªæˆ‘è¯„ä¼°ï¼Œç”¨äºå®¡è®¡ã€‚ç®€å•çš„èåˆæœºåˆ¶èšåˆè¿™äº›è¾“å‡ºï¼Œæš´éœ²è´¡çŒ®è€…ï¼ˆæ”¯æŒæ­£ç¡®ç»“æœçš„æ¨¡æ€ï¼‰å’Œç ´åè€…ï¼ˆè¯¯å¯¼çš„æ¨¡æ€ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„è¯Šæ–­å±‚åœ¨åŸºäºåŸºç¡€æ¨¡å‹çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æµ‹è¯•æ¡ˆä¾‹ç ”ç©¶ä¸­çš„åº”ç”¨æ­ç¤ºäº†ç³»ç»Ÿçš„å¯é æ€§é…ç½®æ–‡ä»¶ï¼Œæä¾›äº†æœ‰å…³å¤±è´¥æ˜¯å¦å¯èƒ½æºäºæ•°æ®é›†çš„äººå·¥åˆ¶å“æˆ–æ¨¡å‹å±€é™æ€§çš„è§è§£ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†è¯Šæ–­æ”¯æ¶ï¼Œæ”¯æŒæœ‰åŸåˆ™çš„å®¡è®¡èåˆåŠ¨æ€å¹¶ä¸ºå¯èƒ½çš„å¹²é¢„æªæ–½æä¾›ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02794v1">PDF</a> Accepted at the Multimodal Algorithmic Reasoning (MAR) Workshop,   NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†è¿‡ç¨‹é€æ˜åº¦é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§è¯Šæ–­å¤±æ•ˆæ¨¡å¼ï¼Œå³æ¨¡æ€ç ´åï¼ŒæŒ‡çš„æ˜¯é«˜ç½®ä¿¡åº¦çš„å•æ¨¡æ€é”™è¯¯ä¼šè¦†ç›–å…¶ä»–è¯æ®å¹¶è¯¯å¯¼èåˆç»“æœã€‚ä¸ºåˆ†æè¿™ä¸€ç°è±¡ï¼Œæå‡ºäº†è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„è¯„ä»·å±‚ï¼Œå°†æ¯ä¸ªæ¨¡æ€è§†ä¸ºä»£ç†ï¼Œç”Ÿæˆå€™é€‰æ ‡ç­¾å¹¶è¿›è¡Œç®€çŸ­çš„è‡ªæˆ‘è¯„ä¼°ï¼Œç”¨äºå®¡è®¡ã€‚ä¸€ä¸ªç®€å•çš„èåˆæœºåˆ¶èšåˆè¿™äº›è¾“å‡ºï¼Œæš´éœ²è´¡çŒ®è€…ï¼ˆæ”¯æŒæ­£ç¡®ç»“æœçš„æ¨¡æ€ï¼‰å’Œç ´åè€…ï¼ˆè¯¯å¯¼çš„æ¨¡æ€ï¼‰ã€‚åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œæ¡ˆä¾‹ç ”ç©¶æ­ç¤ºäº†ç³»ç»Ÿçš„å¯é æ€§é…ç½®æ–‡ä»¶ï¼Œè¿™æœ‰åŠ©äºäº†è§£å¤±è´¥æ˜¯ç”±äºæ•°æ®é›†çš„äººå·¥åˆ¶å“è¿˜æ˜¯æ¨¡å‹æœ¬èº«çš„å±€é™æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡æ¡†æ¶ä¸ºè¯Šæ–­å¤šæ¨¡æ€æ¨ç†æä¾›äº†è„šæ‰‹æ¶ï¼Œæ”¯æŒå¯¹èåˆåŠ¨åŠ›å­¦çš„åŸåˆ™æ€§å®¡è®¡å¹¶å‘ŠçŸ¥å¯èƒ½çš„å¹²é¢„æªæ–½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­˜åœ¨æ¨ç†é€æ˜åº¦é—®é¢˜ã€‚</li>
<li>æ¨¡æ€ç ´åæ˜¯ä¸€ç§è¯Šæ–­å¤±æ•ˆæ¨¡å¼ï¼Œå…¶ä¸­é«˜ç½®ä¿¡åº¦çš„å•æ¨¡æ€é”™è¯¯ä¼šå½±å“èåˆç»“æœã€‚</li>
<li>æå‡ºäº†ä¸€ç§è½»é‡çº§ã€æ¨¡å‹æ— å…³çš„è¯„ä»·å±‚æ¥å®¡è®¡å’Œåˆ†æå¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>è¯„ä»·å±‚å°†æ¯ä¸ªæ¨¡æ€è§†ä¸ºä»£ç†ï¼Œç”Ÿæˆå€™é€‰æ ‡ç­¾å’Œç®€çŸ­çš„è‡ªæˆ‘è¯„ä¼°ã€‚</li>
<li>é€šè¿‡ç®€å•çš„èåˆæœºåˆ¶èšåˆè¾“å‡ºï¼ŒåŒºåˆ†è´¡çŒ®è€…å’Œç ´åè€…ã€‚</li>
<li>åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„æ¡ˆä¾‹ç ”ç©¶æ­ç¤ºäº†ç³»ç»Ÿçš„å¯é æ€§é…ç½®æ–‡ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-757f8ebf480f7f1617533ca3ad13c3f4" align="middle">
<img src="https://picx.zhimg.com/v2-47097db86b335708c718308cfee39f93" align="middle">
<img src="https://picx.zhimg.com/v2-4168e0e524f1d3e5e5997517ad1a1b20" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Controlling-Performance-and-Budget-of-a-Centralized-Multi-agent-LLM-System-with-Reinforcement-Learning"><a href="#Controlling-Performance-and-Budget-of-a-Centralized-Multi-agent-LLM-System-with-Reinforcement-Learning" class="headerlink" title="Controlling Performance and Budget of a Centralized Multi-agent LLM   System with Reinforcement Learning"></a>Controlling Performance and Budget of a Centralized Multi-agent LLM   System with Reinforcement Learning</h2><p><strong>Authors:Bowen Jin, TJ Collins, Donghan Yu, Mert Cemri, Shenao Zhang, Mengyu Li, Jay Tang, Tian Qin, Zhiyang Xu, Jiarui Lu, Guoli Yin, Jiawei Han, Zirui Wang</strong></p>
<p>Large language models (LLMs) exhibit complementary strengths across domains and come with varying inference costs, motivating the design of multi-agent LLM systems where specialized models collaborate efficiently. Existing approaches predominantly rely on decentralized frameworks, which invoke multiple LLMs for every input and thus lead to substantial and uncontrolled inference costs. In this work, we introduce a centralized multi-LLM framework, where a controller LLM selectively coordinates a pool of expert models in a cost-efficient and cost-controllable manner. We formulate this coordination problem as reinforcement learning with dual objectives: maximizing task performance while minimizing the overall inference cost. In addition, we expect the multi-agent system to have adapted behavior with different budget conditions during inference. To this end, we propose CoRL, a reinforcement learning framework that optimizes the performance cost trade-off in a controllable multi-budget setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a single system to surpass the best expert LLM under high-budget settings, while maintaining strong performance in more economical low-budget modes, highlighting the effectiveness of centralized coordination for scalable and cost-efficient multi-agent LLM systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸå±•ç°å‡ºäº’è¡¥çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”å…·æœ‰ä¸åŒçš„æ¨ç†æˆæœ¬ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬è®¾è®¡å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼Œå…¶ä¸­ä¸“ä¸šæ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆåä½œã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆ†å¸ƒå¼æ¡†æ¶ï¼Œä¸ºæ¯ä¸€ä¸ªè¾“å…¥è°ƒç”¨å¤šä¸ªLLMï¼Œä»è€Œå¯¼è‡´æ¨ç†æˆæœ¬å·¨å¤§ä¸”ä¸å¯æ§åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé›†ä¸­å¼çš„å¤šLLMæ¡†æ¶ï¼Œå…¶ä¸­æ§åˆ¶å™¨LLMä»¥èŠ‚çº¦æˆæœ¬ä¸”å¯æ§çš„æ–¹å¼é€‰æ‹©æ€§åœ°åè°ƒä¸“å®¶æ¨¡å‹æ± ã€‚æˆ‘ä»¬å°†è¿™ç§åè°ƒé—®é¢˜åˆ¶å®šä¸ºå…·æœ‰åŒé‡ç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ ï¼šæœ€å¤§åŒ–ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶æœ€å°åŒ–æ€»ä½“æ¨ç†æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¸Œæœ›å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€‚åº”ä¸åŒçš„é¢„ç®—æ¡ä»¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CoRLï¼Œè¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–æ€§èƒ½æˆæœ¬æƒè¡¡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåœ¨ä¸€ä¸ªå¯æ§çš„å¤šé¢„ç®—ç¯å¢ƒä¸­å®ç°ä¼˜åŒ–ã€‚åœ¨å››ä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoRLèƒ½å¤Ÿåœ¨é«˜é¢„ç®—è®¾ç½®ä¸‹ä½¿å•ä¸€ç³»ç»Ÿè¶…è¶Šæœ€ä½³ä¸“å®¶LLMï¼ŒåŒæ—¶åœ¨æ›´ç»æµçš„ä½é¢„ç®—æ¨¡å¼ä¸‹ä¿æŒå¼ºåŠ²æ€§èƒ½ï¼Œå‡¸æ˜¾é›†ä¸­å¼åè°ƒåœ¨å¯æ‰©å±•å’Œæˆæœ¬æ•ˆç›Šé«˜çš„å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02755v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>å¤šé¢†åŸŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å„å…·ä¼˜åŠ¿ï¼Œä¸”æ¨ç†æˆæœ¬å„å¼‚ï¼Œä¿ƒä½¿è®¾è®¡å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿï¼Œè®©ä¸“ä¸šæ¨¡å‹é«˜æ•ˆåä½œã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å»ä¸­å¿ƒåŒ–æ¡†æ¶ï¼Œæ¯æ¬¡è¾“å…¥å‡éœ€è°ƒç”¨å¤šä¸ªLLMï¼Œå¯¼è‡´æ¨ç†æˆæœ¬å·¨å¤§ä¸”ä¸å¯æ§ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§é›†ä¸­å¼å¤šLLMæ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶å™¨LLMé€‰æ‹©æ€§åè°ƒä¸“å®¶æ¨¡å‹æ± ï¼Œä»¥ä½æˆæœ¬ã€å¯æ§çš„æ–¹å¼è¿è¡Œã€‚æœ¬ç ”ç©¶å°†åè°ƒé—®é¢˜åˆ¶å®šä¸ºå…·æœ‰åŒé‡ç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼šæœ€å¤§åŒ–ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶æœ€å°åŒ–æ€»ä½“æ¨ç†æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šç§é¢„ç®—æ¡ä»¶ä¸‹ï¼ŒCoRLæ¡†æ¶èƒ½åœ¨é«˜é¢„ç®—è®¾ç½®ä¸‹è¶…è¶Šæœ€ä½³ä¸“å®¶LLMç³»ç»Ÿï¼ŒåŒæ—¶åœ¨ç»æµå‹ä½é¢„ç®—æ¨¡å¼ä¸‹ä¿æŒå¼ºåŠ²æ€§èƒ½ï¼Œå‡¸æ˜¾é›†ä¸­å¼åè°ƒåœ¨å¯æ‰©å±•å’Œæˆæœ¬æ•ˆç›Šé«˜çš„å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒé¢†åŸŸæœ‰å„è‡ªçš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”å­˜åœ¨æ¨ç†æˆæœ¬å·®å¼‚ã€‚</li>
<li>ç°æœ‰LLMç³»ç»Ÿåä½œä¸»è¦é‡‡ç”¨å»ä¸­å¿ƒåŒ–æ¡†æ¶ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬é«˜æ˜‚ä¸”ä¸å¯æ§ã€‚</li>
<li>æå‡ºä¸€ç§é›†ä¸­å¼å¤šLLMæ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶å™¨LLMé€‰æ‹©æ€§åè°ƒä¸“å®¶æ¨¡å‹ï¼Œå®ç°ä½æˆæœ¬ä¸”å¯æ§çš„æ¨ç†ã€‚</li>
<li>åè°ƒé—®é¢˜è¢«åˆ¶å®šä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä»»åŠ¡æ€§èƒ½ä¸æœ€å°åŒ–æ¨ç†æˆæœ¬ã€‚</li>
<li>æå‡ºçš„CoRLæ¡†æ¶èƒ½åœ¨ä¸åŒé¢„ç®—æ¡ä»¶ä¸‹è‡ªé€‚åº”è°ƒæ•´è¡Œä¸ºã€‚</li>
<li>å®éªŒè¯æ˜CoRLæ¡†æ¶åœ¨é«˜é¢„ç®—å’Œä½é¢„ç®—è®¾ç½®ä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºæœ€ä½³ä¸“å®¶LLMç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6dbe80b767ffda912e0031e4107bd798" align="middle">
<img src="https://picx.zhimg.com/v2-ee38e3923c722c5a481994b3533f35b3" align="middle">
<img src="https://picx.zhimg.com/v2-04d067143403ef954d084a320e98e602" align="middle">
<img src="https://picx.zhimg.com/v2-1b8650d0dfecf30c36e67df11ed218da" align="middle">
<img src="https://picx.zhimg.com/v2-35241e82e15b10592963de640fc170aa" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AI-Diffusion-in-Low-Resource-Language-Countries"><a href="#AI-Diffusion-in-Low-Resource-Language-Countries" class="headerlink" title="AI Diffusion in Low Resource Language Countries"></a>AI Diffusion in Low Resource Language Countries</h2><p><strong>Authors:Amit Misra, Syed Waqas Zamir, Wassim Hamidouche, Inbal Becker-Reshef, Juan Lavista Ferres</strong></p>
<p>Artificial intelligence (AI) is diffusing globally at unprecedented speed, but adoption remains uneven. Frontier Large Language Models (LLMs) are known to perform poorly on low-resource languages due to data scarcity. We hypothesize that this performance deficit reduces the utility of AI, thereby slowing adoption in Low-Resource Language Countries (LRLCs). To test this, we use a weighted regression model to isolate the language effect from socioeconomic and demographic factors, finding that LRLCs have a share of AI users that is approximately 20% lower relative to their baseline. These results indicate that linguistic accessibility is a significant, independent barrier to equitable AI diffusion. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£åœ¨å…¨çƒèŒƒå›´å†…ä»¥å‰æ‰€æœªæœ‰çš„é€Ÿåº¦æ‰©æ•£ï¼Œä½†å…¶åº”ç”¨ä»ç„¶ä¸å‡è¡¡ã€‚å·²çŸ¥å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èµ„æºè´«ä¹çš„è¯­è¨€ä¸Šè¡¨ç°è¾ƒå·®ï¼Œè¿™æ˜¯ç”±äºæ•°æ®ç¨€ç¼ºé€ æˆçš„ã€‚æˆ‘ä»¬å‡è®¾è¿™ç§æ€§èƒ½ç¼ºé™·é™ä½äº†äººå·¥æ™ºèƒ½çš„å®ç”¨æ€§ï¼Œä»è€Œå‡ç¼“äº†åœ¨èµ„æºè´«ä¹è¯­è¨€å›½å®¶ï¼ˆLRLCï¼‰çš„é‡‡ç”¨é€Ÿåº¦ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨åŠ æƒå›å½’æ¨¡å‹æ¥å°†è¯­è¨€æ•ˆåº”ä»ç¤¾ä¼šç»æµå’Œäººå£å› ç´ ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå‘ç°èµ„æºè´«ä¹è¯­è¨€å›½å®¶çš„AIç”¨æˆ·æ¯”ä¾‹æ¯”åŸºçº¿ä½çº¦20%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯­è¨€å¯è®¿é—®æ€§æ˜¯å®ç°äººå·¥æ™ºèƒ½å…¬å¹³ä¼ æ’­çš„é‡è¦ç‹¬ç«‹éšœç¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02752v1">PDF</a> 9 pages, 4 tables. Also available at   <a target="_blank" rel="noopener" href="https://aka.ms/AI_Diffusion_Low_Resource_Language_Countries">https://aka.ms/AI_Diffusion_Low_Resource_Language_Countries</a></p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½åœ¨å…¨çƒèŒƒå›´å†…ä»¥å‰æ‰€æœªæœ‰çš„é€Ÿåº¦æ‰©æ•£ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€å›½å®¶ï¼ˆLRLCsï¼‰çš„é‡‡çº³ç¨‹åº¦ä»ç„¶ä¸å‡ã€‚å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› æ•°æ®ç¨€ç¼ºè€Œå¯¹ä½èµ„æºè¯­è¨€è¡¨ç°ä¸ä½³ï¼Œè¿™é™ä½äº†äººå·¥æ™ºèƒ½çš„å®ç”¨æ€§ï¼Œä»è€Œå‡ç¼“äº†åœ¨è¿™äº›å›½å®¶çš„é‡‡ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œç›¸è¾ƒäºåŸºçº¿ï¼ŒLRLCsçš„AIç”¨æˆ·å æ¯”é™ä½äº†çº¦20%ï¼Œè¡¨æ˜è¯­è¨€å¯è¾¾æ€§æ˜¯å…¬å¹³çš„äººå·¥æ™ºèƒ½æ‰©æ•£çš„ç‹¬ç«‹éšœç¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>äººå·¥æ™ºèƒ½åœ¨å…¨çƒèŒƒå›´å†…å¿«é€Ÿæ‰©æ•£ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€å›½å®¶ï¼ˆLRLCsï¼‰çš„é‡‡çº³ç¨‹åº¦å­˜åœ¨å·®å¼‚ã€‚</li>
<li>å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä½èµ„æºè¯­è¨€çš„æ€§èƒ½è¡¨ç°ä¸ä½³ã€‚</li>
<li>è¯­è¨€å¯è¾¾æ€§æ˜¯é˜»ç¢äººå·¥æ™ºèƒ½å…¬å¹³æ‰©æ•£çš„é‡è¦ç‹¬ç«‹å› ç´ ã€‚</li>
<li>LRLCsçš„AIç”¨æˆ·å æ¯”ç›¸è¾ƒäºåŸºçº¿é™ä½äº†çº¦20%ã€‚</li>
<li>æ•°æ®ç¨€ç¼ºæ˜¯å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€è¡¨ç°ä¸ä½³çš„ä¸»è¦åŸå› ã€‚</li>
<li>éœ€è¦è€ƒè™‘è¯­è¨€å› ç´ åœ¨äººå·¥æ™ºèƒ½å…¨çƒæ‰©æ•£ä¸­çš„å½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bca4213f7816ff6583f4bb293138765" align="middle">
<img src="https://picx.zhimg.com/v2-187de935c72cd39fea36e7a541282af4" align="middle">
<img src="https://picx.zhimg.com/v2-ecb6fde8a620dce1eb37f46163ae86ef" align="middle">
<img src="https://picx.zhimg.com/v2-698a6289c66e951aec4e4f8a23ef140b" align="middle">
<img src="https://picx.zhimg.com/v2-ed68ffa1b27efd132e55039771788761" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Curriculum-Design-for-Trajectory-Constrained-Agent-Compressing-Chain-of-Thought-Tokens-in-LLMs"><a href="#Curriculum-Design-for-Trajectory-Constrained-Agent-Compressing-Chain-of-Thought-Tokens-in-LLMs" class="headerlink" title="Curriculum Design for Trajectory-Constrained Agent: Compressing   Chain-of-Thought Tokens in LLMs"></a>Curriculum Design for Trajectory-Constrained Agent: Compressing   Chain-of-Thought Tokens in LLMs</h2><p><strong>Authors:Georgios Tzannetos, Parameswaran Kamalaruban, Adish Singla</strong></p>
<p>Training agents to operate under strict constraints during deployment, such as limited resource budgets or stringent safety requirements, presents significant challenges, especially when these constraints render the task complex. In this work, we propose a curriculum learning strategy that gradually tightens constraints during training, enabling the agent to incrementally master the deployment requirements. Inspired by self-paced learning techniques in unconstrained reinforcement learning (RL), our approach facilitates a smoother transition to challenging environments by initially training on simplified versions of the constraints and progressively introducing the full deployment conditions. We provide a theoretical analysis using an RL agent in a binary-tree Markov Decision Process (MDP) to demonstrate that our curriculum strategy can accelerate training relative to a baseline approach that imposes the trajectory constraints from the outset. Moreover, we empirically validate the effectiveness and generality of our method across both RL and large language model (LLM) agents in diverse settings, including a binary-tree MDP, a multi-task navigation domain, and a math reasoning task with two benchmarks. These results highlight the potential of curriculum design in enhancing the efficiency and performance of agents operating under complex trajectory constraints during deployment. Moreover, when applied to LLMs, our strategy enables compression of output chain-of-thought tokens, achieving a substantial inference speedup on consumer hardware, demonstrating its effectiveness for resource-constrained deployment. </p>
<blockquote>
<p>åœ¨éƒ¨ç½²æœŸé—´ï¼Œè®­ç»ƒæ™ºèƒ½ä½“åœ¨ä¸¥æ ¼çº¦æŸä¸‹è¿è¡Œï¼Œå¦‚èµ„æºé¢„ç®—æœ‰é™æˆ–å®‰å…¨è¦æ±‚ä¸¥æ ¼ï¼Œä¼šé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å½“è¿™äº›çº¦æŸä½¿ä»»åŠ¡å˜å¾—å¤æ‚æ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸åŠ å¼ºçº¦æŸï¼Œä½¿æ™ºèƒ½ä½“èƒ½é€æ­¥æŒæ¡éƒ¨ç½²è¦æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°æ— çº¦æŸå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„è‡ªæˆ‘èŠ‚å¥å­¦ä¹ æŠ€æœ¯çš„å¯å‘ï¼Œé€šè¿‡æœ€åˆåœ¨çº¦æŸçš„ç®€åŒ–ç‰ˆæœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€æ­¥å¼•å…¥å…¨é¢çš„éƒ¨ç½²æ¡ä»¶ï¼Œä»è€Œå®ç°å‘å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒçš„å¹³ç¨³è¿‡æ¸¡ã€‚æˆ‘ä»¬ä½¿ç”¨åœ¨äºŒå‰æ ‘é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­çš„RLæ™ºèƒ½ä½“è¿›è¡Œç†è®ºåˆ†æï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„è¯¾ç¨‹ç­–ç•¥å¯ä»¥åŠ é€Ÿè®­ç»ƒï¼Œç›¸å¯¹äºä¸€ç§ä»ä¸€å¼€å§‹å°±å¼ºåˆ¶è½¨è¿¹çº¦æŸçš„åŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡åœ¨å¤šç§ç¯å¢ƒä¸­çš„RLå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“è¿›è¡Œå®è¯ç ”ç©¶ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚è¿™äº›ç¯å¢ƒåŒ…æ‹¬äºŒå‰æ ‘MDPã€å¤šä»»åŠ¡å¯¼èˆªåŸŸä»¥åŠä¸¤ä¸ªåŸºå‡†çš„æ•°å­¦æ¨ç†ä»»åŠ¡ã€‚è¿™äº›ç»“æœçªå‡ºäº†è¯¾ç¨‹è®¾è®¡åœ¨å¢å¼ºæ™ºèƒ½ä½“åœ¨éƒ¨ç½²æœŸé—´åœ¨å¤æ‚è½¨è¿¹çº¦æŸä¸‹è¿è¡Œæ—¶çš„æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œå°†æˆ‘ä»¬çš„ç­–ç•¥åº”ç”¨äºLLMæ—¶ï¼Œèƒ½å¤Ÿå®ç°è¾“å‡ºæ€ç»´é“¾ä»£å¸çš„å‹ç¼©ï¼Œåœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šå®ç°äº†æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œè¯æ˜äº†å…¶åœ¨èµ„æºå—é™çš„éƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02690v1">PDF</a> NeurIPSâ€™25 paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨è®­ç»ƒæœŸé—´é€æ­¥åŠ å¼ºçº¦æŸæ¡ä»¶çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€æ­¥æŒæ¡éƒ¨ç½²è¦æ±‚ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†æ— çº¦æŸå¼ºåŒ–å­¦ä¹ ä¸­çš„è‡ªæˆ‘è¿›åº¦å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡æœ€åˆåœ¨ç®€åŒ–ç‰ˆæœ¬çš„çº¦æŸä¸‹è¿›è¡Œè®­ç»ƒï¼Œç„¶åé€æ­¥å¼•å…¥å®Œæ•´çš„éƒ¨ç½²æ¡ä»¶ï¼Œå®ç°æ›´å¹³ç¨³åœ°è¿‡æ¸¡åˆ°å¤æ‚ç¯å¢ƒã€‚ç†è®ºåˆ†æå’Œå®è¯éªŒè¯å‡è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ä¸­å‡æœ‰æ•ˆï¼Œå¹¶ä¸”åœ¨å…·æœ‰å¤æ‚è½¨è¿¹çº¦æŸçš„éƒ¨ç½²ç¯å¢ƒä¸­è¡¨ç°å‡ºæ½œåœ¨çš„ä¼˜åŠ¿ã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ï¼Œæ­¤ç­–ç•¥è¿˜èƒ½å‹ç¼©è¾“å‡ºæ€ç»´é“¾ä»¤ç‰Œï¼Œå®ç°æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šçš„æ¨ç†é€Ÿåº¦å¤§å¹…æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨è®­ç»ƒæœŸé—´é€æ­¥åŠ å¼ºçº¦æŸæ¡ä»¶ï¼Œä»¥åº”å¯¹éƒ¨ç½²æ—¶çš„ä¸¥æ ¼çº¦æŸæŒ‘æˆ˜ã€‚</li>
<li>å€Ÿé‰´äº†æ— çº¦æŸå¼ºåŒ–å­¦ä¹ ä¸­çš„è‡ªæˆ‘è¿›åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€æ­¥é€‚åº”å¹¶å¤„ç†å¤æ‚çš„éƒ¨ç½²ç¯å¢ƒã€‚</li>
<li>é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯éªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨å¤šç§è®¾ç½®ä¸‹ï¼ŒåŒ…æ‹¬äºŒå‰æ ‘é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€å¤šä»»åŠ¡å¯¼èˆªåŸŸå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ç­‰ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•ç›¸å¯¹äºåŸºçº¿æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</li>
<li>å½“åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿå®ç°è¾“å‡ºæ€ç»´é“¾ä»¤ç‰Œçš„å‹ç¼©ï¼Œä»è€Œæé«˜æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šçš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¢å¼ºä»£ç†åœ¨éƒ¨ç½²æ—¶å¤„ç†å¤æ‚è½¨è¿¹çº¦æŸçš„æ•ˆç‡ä¸è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ac9d64c09fb73ee132d5702669f97f9" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Tool-to-Agent-Retrieval-Bridging-Tools-and-Agents-for-Scalable-LLM-Multi-Agent-Systems"><a href="#Tool-to-Agent-Retrieval-Bridging-Tools-and-Agents-for-Scalable-LLM-Multi-Agent-Systems" class="headerlink" title="Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM   Multi-Agent Systems"></a>Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM   Multi-Agent Systems</h2><p><strong>Authors:Elias Lumer, Faheem Nizar, Anmol Gulati, Pradeep Honaganahalli Basavaraju, Vamse Kumar Subbiah</strong></p>
<p>Recent advances in LLM Multi-Agent Systems enable scalable orchestration of sub-agents, each coordinating hundreds or thousands of tools or Model Context Protocol (MCP) servers. However, existing retrieval methods typically match queries against coarse agent-level descriptions before routing, which obscures fine-grained tool functionality and often results in suboptimal agent selection. We introduce Tool-to-Agent Retrieval, a unified framework that embeds both tools and their parent agents in a shared vector space and connects them through metadata relationships. By explicitly representing tool capabilities and traversing metadata to the agent level, Tool-to-Agent Retrieval enables granular tool-level or agent-level retrieval, ensuring that agents and their underlying tools or MCP servers are equally represented without the context dilution that arises from chunking many tools together. Evaluating Tool-to-Agent Retrieval across eight embedding models, our approach achieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over previous state-of-the-art agent retrievers on the LiveMCPBench benchmark. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒLLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå–å¾—äº†è¿›å±•ï¼Œèƒ½å¤Ÿå®ç°å­æ™ºèƒ½ä½“çš„å¯æ‰©å±•ååŒï¼Œæ¯ä¸ªæ™ºèƒ½ä½“å¯åè°ƒæ•°ç™¾æˆ–æ•°åƒä¸ªå·¥å…·æˆ–æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æœåŠ¡å™¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€ç´¢æ–¹æ³•é€šå¸¸åœ¨è·¯ç”±ä¹‹å‰é’ˆå¯¹ç²—ç•¥çš„æ™ºèƒ½ä½“çº§åˆ«æè¿°è¿›è¡ŒåŒ¹é…æŸ¥è¯¢ï¼Œè¿™æ©ç›–äº†ç»†ç²’åº¦çš„å·¥å…·åŠŸèƒ½ï¼Œå¹¶ä¸”å¾€å¾€å¯¼è‡´æ¬¡ä¼˜çš„æ™ºèƒ½ä½“é€‰æ‹©ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œå·¥å…·åˆ°æ™ºèƒ½ä½“æ£€ç´¢â€ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†å·¥å…·å’Œå®ƒä»¬çš„çˆ¶æ™ºèƒ½ä½“åµŒå…¥å…±äº«å‘é‡ç©ºé—´ä¸­ï¼Œå¹¶é€šè¿‡å…ƒæ•°æ®å…³ç³»å°†å®ƒä»¬è¿æ¥èµ·æ¥ã€‚é€šè¿‡æ˜¾å¼è¡¨ç¤ºå·¥å…·èƒ½åŠ›å¹¶éå†å…ƒæ•°æ®åˆ°æ™ºèƒ½ä½“çº§åˆ«ï¼Œå·¥å…·åˆ°æ™ºèƒ½ä½“æ£€ç´¢èƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„å·¥å…·çº§æˆ–æ™ºèƒ½ä½“çº§æ£€ç´¢ï¼Œç¡®ä¿æ™ºèƒ½ä½“åŠå…¶åº•å±‚å·¥å…·æˆ–MCPæœåŠ¡å™¨åœ¨æ— éœ€åˆå¹¶å¤šä¸ªå·¥å…·è€Œäº§ç”Ÿçš„ä¸Šä¸‹æ–‡ç¨€é‡Šçš„æƒ…å†µä¸‹å¾—åˆ°åŒç­‰è¡¨ç¤ºã€‚åœ¨LiveMCPBenchåŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…«ä¸ªåµŒå…¥æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨Recall@5å’ŒnDCG@5æ–¹é¢åˆ†åˆ«æ¯”æœ€æ–°çš„æ™ºèƒ½ä½“æ£€ç´¢æ–¹æ³•æé«˜äº†19.4%å’Œ17.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01854v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸLLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ–°è¿›å±•å®ç°äº†å¯¹æ•°ç™¾æˆ–æ•°åƒä¸ªå·¥å…·æˆ–æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æœåŠ¡å™¨è¿›è¡Œå­æ™ºèƒ½ä½“çš„å¯æ‰©å±•ååŒå·¥ä½œã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€ç´¢æ–¹æ³•é€šå¸¸åªåœ¨ç²—ç²’åº¦çš„æ™ºèƒ½ä½“çº§åˆ«æè¿°ä¸­è¿›è¡ŒåŒ¹é…å’Œè·¯ç”±æŸ¥è¯¢ï¼Œè¿™ä¼šæ©ç›–å·¥å…·çš„ç²¾ç»†åŠŸèƒ½å¹¶ç»å¸¸å¯¼è‡´æ™ºèƒ½ä½“é€‰æ‹©çš„æ¬¡ä¼˜æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†å·¥å…·åˆ°æ™ºèƒ½ä½“æ£€ç´¢è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å·¥å…·å’Œå…¶çˆ¶æ™ºèƒ½ä½“åµŒå…¥å…±äº«å‘é‡ç©ºé—´ä¸­ï¼Œå¹¶é€šè¿‡å…ƒæ•°æ®å…³ç³»å°†å®ƒä»¬è¿æ¥èµ·æ¥ã€‚é€šè¿‡æ˜¾å¼è¡¨ç¤ºå·¥å…·èƒ½åŠ›å¹¶éå†å…ƒæ•°æ®åˆ°æ™ºèƒ½ä½“çº§åˆ«ï¼Œå·¥å…·åˆ°æ™ºèƒ½ä½“æ£€ç´¢èƒ½å¤Ÿå®ç°å·¥å…·çº§åˆ«æˆ–æ™ºèƒ½ä½“çº§åˆ«çš„ç²¾ç»†æ£€ç´¢ï¼Œç¡®ä¿æ™ºèƒ½ä½“åŠå…¶åº•å±‚å·¥å…·æˆ–MCPæœåŠ¡å™¨åœ¨æ— éœ€åˆå¹¶å¤šä¸ªå·¥å…·çš„æƒ…å†µä¸‹å¾—åˆ°åŒç­‰è¡¨ç°ã€‚åœ¨LiveMCPBenchåŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºä¹‹å‰æœ€å…ˆè¿›çš„æ™ºèƒ½ä½“æ£€ç´¢æ–¹æ³•ï¼ŒRecall@5æé«˜äº†19.4%ï¼ŒnDCG@5æé«˜äº†17.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¯ä»¥å®ç°å­æ™ºèƒ½ä½“çš„å¯æ‰©å±•ååŒå·¥ä½œï¼Œç®¡ç†å¤§é‡å·¥å…·æˆ–MCPæœåŠ¡å™¨ã€‚</li>
<li>ç°æœ‰æ£€ç´¢æ–¹æ³•é€šå¸¸åœ¨ç²—ç²’åº¦çš„æ™ºèƒ½ä½“çº§åˆ«è¿›è¡ŒåŒ¹é…å’Œè·¯ç”±ï¼Œè¿™é™åˆ¶äº†å·¥å…·åŠŸèƒ½çš„ç²¾ç»†è¡¨è¾¾ï¼Œå¹¶å¯èƒ½å¯¼è‡´æ™ºèƒ½ä½“é€‰æ‹©çš„ä¸ç†æƒ³ã€‚</li>
<li>å·¥å…·åˆ°æ™ºèƒ½ä½“æ£€ç´¢æ¡†æ¶å°†å·¥å…·å’Œæ™ºèƒ½ä½“åµŒå…¥å…±äº«å‘é‡ç©ºé—´ï¼Œé€šè¿‡å…ƒæ•°æ®å…³ç³»è¿æ¥ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿæ”¯æŒå·¥å…·çº§åˆ«å’Œæ™ºèƒ½ä½“çº§åˆ«çš„ç²¾ç»†æ£€ç´¢ã€‚</li>
<li>å·¥å…·åˆ°æ™ºèƒ½ä½“æ£€ç´¢ç¡®ä¿æ™ºèƒ½ä½“åŠå…¶å·¥å…·æˆ–MCPæœåŠ¡å™¨åœ¨æ— éœ€åˆå¹¶ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å¾—åˆ°å¹³ç­‰è¡¨è¾¾ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨LiveMCPBenchåŸºå‡†æµ‹è¯•ä¸Šç›¸è¾ƒäºå…¶ä»–æ£€ç´¢æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼ŒRecall@5å’ŒnDCG@5æŒ‡æ ‡æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c763263fea1cf1d2955f42270aca8446" align="middle">
<img src="https://picx.zhimg.com/v2-1468cdd55c6118dd842053a3c55e85a7" align="middle">
<img src="https://picx.zhimg.com/v2-8ead969a9a847a4339fe4ddd4feb315e" align="middle">
<img src="https://picx.zhimg.com/v2-b0e149a1ecec799fb66706cab73d22e8" align="middle">
<img src="https://picx.zhimg.com/v2-5061db21bedecc046f06df7770a5cf43" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="KV-Cache-Transform-Coding-for-Compact-Storage-in-LLM-Inference"><a href="#KV-Cache-Transform-Coding-for-Compact-Storage-in-LLM-Inference" class="headerlink" title="KV Cache Transform Coding for Compact Storage in LLM Inference"></a>KV Cache Transform Coding for Compact Storage in LLM Inference</h2><p><strong>Authors:Konrad Staniszewski, Adrian ÅaÅ„cucki</strong></p>
<p>Serving large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\times$ compression while maintaining reasoning and long-context accuracy, and 40$\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡æä¾›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡æ—¶ï¼Œéœ€è¦é«˜æ•ˆçš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ç®¡ç†ã€‚åœ¨è¿­ä»£ä»£ç ç¼–è¾‘å’ŒèŠå¤©ä¸­å¸¸è§çš„å…±äº«å‰ç¼€æç¤ºå¯ä»¥é€šè¿‡è·¨å¯¹è¯å›åˆé‡å¤ä½¿ç”¨KVç¼“å­˜ã€‚ç„¶è€Œï¼Œè¿‡æ—¶çš„ç¼“å­˜ä¼šæ¶ˆè€—ç¨€ç¼ºçš„GPUå†…å­˜ï¼Œéœ€è¦è¿›è¡Œå¸è½½æˆ–å¼ºåˆ¶é‡æ–°è®¡ç®—ã€‚æˆ‘ä»¬æå‡ºäº†KVTCï¼Œå®ƒæ˜¯ä¸€ç§è½»é‡çº§çš„è½¬æ¢ç¼–ç å™¨ï¼Œå¯ä»¥å‹ç¼©KVç¼“å­˜ï¼Œä»¥å®ç°ç´§å‡‘çš„GPUå†…å¤–å­˜å‚¨ã€‚KVTCå€Ÿé‰´äº†ç»å…¸çš„åª’ä½“å‹ç¼©æŠ€æœ¯ï¼Œç»“åˆäº†åŸºäºPCAçš„ç‰¹å¾å»ç›¸å…³ã€è‡ªé€‚åº”é‡åŒ–å’Œç†µç¼–ç ã€‚å®ƒåªéœ€è¦çŸ­æš‚çš„åˆå§‹æ ¡å‡†ï¼Œä¸”ä¸ä¼šæ”¹å˜æ¨¡å‹å‚æ•°ã€‚é€šè¿‡åˆ©ç”¨KVç¼“å­˜ä¸­çš„å†—ä½™ä¿¡æ¯ï¼ŒKVTCå¯å®ç°é«˜è¾¾20å€çš„å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒæ¨ç†å’Œé•¿æœŸä¸Šä¸‹æ–‡å‡†ç¡®æ€§ï¼Œé’ˆå¯¹ç‰¹å®šç”¨ä¾‹ç”šè‡³å¯è¾¾40å€æˆ–æ›´é«˜ã€‚æˆ‘ä»¬åœ¨AIME25ã€LiveCodeBenchã€GSM8Kã€MMLUã€Qasperã€RULERå’ŒMATH-500ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹Llama 3ã€Mistral NeMoå’ŒR1-Qwen 2.5æ¨¡å‹è¿›è¡Œäº†KVTCæµ‹è¯•ã€‚å®ƒå§‹ç»ˆä¼˜äºæ¨ç†æ—¶é—´åŸºçº¿ï¼Œå¦‚ä»¤ç‰Œé©±é€ã€é‡åŒ–å’ŒåŸºäºSVDçš„æ–¹æ³•ï¼ŒåŒæ—¶å®ç°æ›´é«˜çš„å‹ç¼©ç‡ã€‚è¿™äº›ç»“æœæ”¯æŒKVTCä½œä¸ºå†…å­˜é«˜æ•ˆLLMæœåŠ¡ä¸­å¯é‡ç”¨KVç¼“å­˜çš„å®é™…æ„å»ºå—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01815v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨å¤§è§„æ¨¡æä¾›è¯­è¨€æ¨¡å‹æœåŠ¡æ—¶ï¼Œéœ€è¦é«˜æ•ˆçš„é”®å€¼ç¼“å­˜ç®¡ç†ã€‚é€šè¿‡å…±äº«å‰ç¼€æç¤ºåœ¨å¯¹è¯å›åˆä¹‹é—´é‡å¤ä½¿ç”¨é”®å€¼ç¼“å­˜ï¼Œè¿™åœ¨è¿­ä»£ä»£ç ç¼–è¾‘å’ŒèŠå¤©ä¸­å¾ˆå¸¸è§ã€‚ç„¶è€Œï¼Œè¿‡æ—¶çš„ç¼“å­˜æ¶ˆè€—æœ‰é™çš„GPUå†…å­˜ï¼Œéœ€è¦å¸è½½æˆ–å¼ºåˆ¶é‡æ–°è®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†KVTCï¼Œä¸€ç§ç”¨äºå‹ç¼©é”®å€¼ç¼“å­˜çš„è½»é‡çº§è½¬æ¢ç¼–ç å™¨ï¼Œä»¥å®ç°ç´§å‡‘çš„GPUå’ŒGPUå¤–å­˜å‚¨ã€‚å®ƒç»“åˆäº†åŸºäºPCAçš„ç‰¹å¾å»ç›¸å…³ã€è‡ªé€‚åº”é‡åŒ–å’Œç†µç¼–ç ï¼Œå€Ÿé‰´äº†ç»å…¸çš„åª’ä½“å‹ç¼©æŠ€æœ¯ã€‚å®ƒåªéœ€è¦çŸ­æš‚çš„åˆå§‹æ ¡å‡†ï¼Œä¸ä¼šæ”¹å˜æ¨¡å‹å‚æ•°ã€‚é€šè¿‡åˆ©ç”¨é”®å€¼ç¼“å­˜ä¸­çš„å†—ä½™ä¿¡æ¯ï¼ŒKVTCå®ç°äº†é«˜è¾¾20å€çš„å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†å’Œé•¿æ–‡æœ¬å‡†ç¡®æ€§ï¼Œç‰¹å®šç”¨ä¾‹ç”šè‡³è¾¾åˆ°äº†40å€æˆ–æ›´é«˜çš„å‹ç¼©ç‡ã€‚åœ¨AIME25ã€LiveCodeBenchã€GSM8Kã€MMLUã€Qasperã€RULERå’ŒMATH-500ç­‰åŸºå‡†æµ‹è¯•ä¸Šï¼ŒKVTCä¸Llama 3ã€Mistral NeMoå’ŒR1-Qwen 2.5æ¨¡å‹è¿›è¡Œçš„æµ‹è¯•è¡¨æ˜ï¼Œå®ƒå§‹ç»ˆä¼˜äºè¯¸å¦‚ä»¤ç‰Œé€å‡ºã€é‡åŒ–å’ŒSVDæ–¹æ³•ç­‰æ¨ç†æ—¶é—´åŸºçº¿ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„å‹ç¼©ç‡ã€‚è¿™äº›ç»“æœæ”¯æŒKVTCä½œä¸ºå…·æœ‰å¯é‡ç”¨é”®å€¼ç¼“å­˜çš„å†…å­˜é«˜æ•ˆè¯­è¨€æ¨¡å‹æœåŠ¡å®ç”¨æ„å»ºå—ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§è§„æ¨¡æä¾›è¯­è¨€æ¨¡å‹æœåŠ¡éœ€è¦é«˜æ•ˆçš„é”®å€¼ç¼“å­˜ç®¡ç†ã€‚</li>
<li>è¿‡æ—¶çš„ç¼“å­˜æ¶ˆè€—æœ‰é™çš„GPUå†…å­˜èµ„æºã€‚</li>
<li>KVTCæ˜¯ä¸€ç§è½»é‡çº§è½¬æ¢ç¼–ç å™¨ï¼Œç”¨äºå‹ç¼©é”®å€¼ç¼“å­˜ä»¥å®ç°å†…å­˜ä¼˜åŒ–ã€‚</li>
<li>KVTCç»“åˆPCAç‰¹å¾å»ç›¸å…³ã€è‡ªé€‚åº”é‡åŒ–å’Œç†µç¼–ç å®ç°é«˜æ•ˆå‹ç¼©ã€‚</li>
<li>KVTCä»…éœ€åˆå§‹æ ¡å‡†ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°ã€‚</li>
<li>KVTCå®ç°äº†é«˜è¾¾20å€çš„å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒæ¨ç†å’Œé•¿æ–‡æœ¬å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-646c988b9a798ef8a4b350a11ba31561" align="middle">
<img src="https://picx.zhimg.com/v2-06630bdded0dfbbd77e303c3c26b85c8" align="middle">
<img src="https://picx.zhimg.com/v2-e26e5a31c81b55780b940a0597589778" align="middle">
<img src="https://picx.zhimg.com/v2-9ceb051db8f331724e98a07a7b542d95" align="middle">
<img src="https://picx.zhimg.com/v2-a92ef4f466b0a0819fcf44c8aad2dace" align="middle">
<img src="https://picx.zhimg.com/v2-1ec8458d2f82120c6b8aaa66328a8c26" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-LLM-Powered-Task-Aware-Retrieval-of-Scientific-Workflows-for-Galaxy"><a href="#Towards-LLM-Powered-Task-Aware-Retrieval-of-Scientific-Workflows-for-Galaxy" class="headerlink" title="Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for   Galaxy"></a>Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for   Galaxy</h2><p><strong>Authors:Shamse Tasnim Cynthia, Banani Roy</strong></p>
<p>Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become essential infrastructure in bioinformatics, supporting the design, execution, and sharing of complex multi-step analyses. Despite hosting hundreds of reusable workflows across domains, Galaxyâ€™s current keyword-based retrieval system offers limited support for semantic query interpretation and often fails to surface relevant workflows when exact term matches are absent. To address this gap, we propose a task-aware, two-stage retrieval framework that integrates dense vector search with large language model (LLM)-based reranking. Our system first retrieves candidate workflows using state-of-the-art embedding models and then reranks them using instruction-tuned generative LLMs (GPT-4o, Mistral-7B) based on semantic task alignment. To support robust evaluation, we construct a benchmark dataset of Galaxy workflows annotated with semantic topics via BERTopic and synthesize realistic task-oriented queries using LLMs. We conduct a comprehensive comparison of lexical, dense, and reranking models using standard IR metrics, presenting the first systematic evaluation of retrieval performance in the Galaxy ecosystem. Results show that our approach significantly improves top-k accuracy and relevance, particularly for long or under-specified queries. We further integrate our system as a prototype tool within Galaxy, providing a proof-of-concept for LLM-enhanced workflow search. This work advances the usability and accessibility of scientific workflows, especially for novice users and interdisciplinary researchers. </p>
<blockquote>
<p>ç§‘å­¦å·¥ä½œæµç¨‹ç®¡ç†ç³»ç»Ÿï¼ˆå¦‚Galaxyï¼‰å·²æˆä¸ºç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„åŸºæœ¬è®¾æ–½ï¼Œæ”¯æŒå¤æ‚çš„å¤šæ­¥åˆ†æçš„è®¾è®¡ã€æ‰§è¡Œå’Œå…±äº«ã€‚å°½ç®¡Galaxyå½“å‰çš„å…³é”®å­—æ£€ç´¢ç³»ç»Ÿå·²æ¶µç›–äº†å¤šä¸ªé¢†åŸŸçš„å¤§é‡å¯é‡ç”¨å·¥ä½œæµç¨‹ï¼Œä½†åœ¨ç¼ºä¹ç²¾ç¡®æœ¯è¯­åŒ¹é…çš„æƒ…å†µä¸‹ï¼Œå®ƒæä¾›çš„è¯­ä¹‰æŸ¥è¯¢è§£é‡Šæ”¯æŒæœ‰é™ï¼Œä¸”å¾€å¾€æ— æ³•æ‰¾åˆ°ç›¸å…³çš„å·¥ä½œæµç¨‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„ä¸¤é˜¶æ®µæ£€ç´¢æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¯†é›†å‘é‡æœç´¢å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡æ’åºã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé¦–å…ˆä½¿ç”¨å…ˆè¿›çš„åµŒå…¥æ¨¡å‹æ£€ç´¢å€™é€‰å·¥ä½œæµç¨‹ï¼Œç„¶åä½¿ç”¨é’ˆå¯¹æŒ‡ä»¤è°ƒæ•´çš„ç”Ÿæˆå¼LLMï¼ˆGPT-4oã€Mistral-7Bï¼‰æ ¹æ®è¯­ä¹‰ä»»åŠ¡å¯¹é½è¿›è¡Œé‡æ’åºã€‚ä¸ºäº†æ”¯æŒç¨³å¥çš„è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨BERTopicå¯¹Galaxyå·¥ä½œæµç¨‹è¿›è¡Œè¯­ä¹‰ä¸»é¢˜æ ‡æ³¨ï¼Œæ„å»ºäº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨LLMåˆæˆé¢å‘ä»»åŠ¡çš„ç°å®æŸ¥è¯¢ã€‚æˆ‘ä»¬é‡‡ç”¨å…¨é¢çš„è¯æ±‡æ¨¡å‹ã€å¯†é›†æ¨¡å‹å’Œé‡æ’åºæ¨¡å‹çš„æ¯”è¾ƒï¼Œä½¿ç”¨æ ‡å‡†çš„IRæŒ‡æ ‡å¯¹Galaxyç”Ÿæ€ç³»ç»Ÿä¸­çš„æ£€ç´¢æ€§èƒ½è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†å‰kä¸ªç»“æœçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿æˆ–æœªæ˜ç¡®æŒ‡å®šçš„æŸ¥è¯¢ã€‚æˆ‘ä»¬è¿˜å°†æˆ‘ä»¬çš„ç³»ç»Ÿæ•´åˆä¸ºGalaxyä¸­çš„åŸå‹å·¥å…·ï¼Œä¸ºLLMå¢å¼ºçš„æµç¨‹æœç´¢æä¾›äº†æ¦‚å¿µéªŒè¯ã€‚è¿™é¡¹å·¥ä½œæé«˜äº†ç§‘å­¦å·¥ä½œæµç¨‹çš„å¯ç”¨æ€§å’Œå¯åŠæ€§ï¼Œå°¤å…¶æ˜¯å¯¹äºæ–°æ‰‹ç”¨æˆ·å’Œè·¨å­¦ç§‘ç ”ç©¶è€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01757v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨ç§‘å­¦å·¥ä½œæµç®¡ç†ç³»ç»Ÿï¼ˆSWfMSsï¼‰ä¸­ï¼Œå¦‚Galaxyç­‰ç³»ç»Ÿå·²æˆä¸ºç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„å…³é”®åŸºç¡€è®¾æ–½ï¼Œæ”¯æŒå¤æ‚å¤šæ­¥åˆ†æçš„è®¾è®¡ã€æ‰§è¡Œå’Œå…±äº«ã€‚Galaxyçš„å½“å‰å…³é”®è¯æ£€ç´¢ç³»ç»Ÿæä¾›æœ‰é™çš„è¯­ä¹‰æŸ¥è¯¢è§£é‡Šæ”¯æŒï¼Œå¹¶åœ¨ç¼ºå°‘ç²¾ç¡®åŒ¹é…æœ¯è¯­æ—¶å¾€å¾€æ— æ³•å±•ç°ç›¸å…³çš„å·¥ä½œæµã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„ä¸¤é˜¶æ®µæ£€ç´¢æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¯†é›†å‘é‡æœç´¢å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡æ–°æ’åºã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé¦–å…ˆä½¿ç”¨å…ˆè¿›çš„åµŒå…¥æ¨¡å‹æ£€ç´¢å€™é€‰å·¥ä½œæµï¼Œç„¶åä½¿ç”¨é’ˆå¯¹æŒ‡ä»¤ä¼˜åŒ–çš„ç”Ÿæˆå¼LLMï¼ˆGPT-4oï¼ŒMistral-7Bï¼‰æ ¹æ®è¯­ä¹‰ä»»åŠ¡å¯¹é½è¿›è¡Œé‡æ–°æ’åºã€‚ä¸ºäº†æ”¯æŒç¨³å¥è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨BERTopicå¯¹Galaxyå·¥ä½œæµæ„å»ºäº†å¸¦æœ‰è¯­ä¹‰ä¸»é¢˜æ ‡æ³¨çš„åŸºå‡†æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨LLMåˆæˆé¢å‘ä»»åŠ¡çš„å®é™…æŸ¥è¯¢ã€‚æˆ‘ä»¬å…¨é¢æ¯”è¾ƒäº†è¯æ±‡ã€å¯†é›†å’Œé‡æ–°æ’åºæ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†IRæŒ‡æ ‡å‘ˆç°äº†Galaxyç”Ÿæ€ç³»ç»Ÿä¸­æ£€ç´¢æ€§èƒ½çš„ç³»ç»Ÿæ€§è¯„ä»·ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†å‰kä¸ªç»“æœçš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿æˆ–æœªæŒ‡å®šçš„æŸ¥è¯¢ã€‚æˆ‘ä»¬è¿˜å°†æˆ‘ä»¬çš„ç³»ç»Ÿä½œä¸ºGalaxyä¸­çš„åŸå‹å·¥å…·è¿›è¡Œé›†æˆï¼Œä¸ºå¢å¼ºå·¥ä½œæµæœç´¢çš„LLMæä¾›äº†æ¦‚å¿µè¯æ˜ã€‚æœ¬å·¥ä½œæé«˜äº†ç§‘å­¦å·¥ä½œæµçš„å¯ç”¨æ€§å’Œå¯è®¿é—®æ€§ï¼Œå°¤å…¶æ˜¯å¯¹äºæ–°æ‰‹ç”¨æˆ·å’Œè·¨å­¦ç§‘ç ”ç©¶è€…ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Galaxyç­‰ç§‘å­¦å·¥ä½œæµç®¡ç†ç³»ç»Ÿåœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œæ”¯æŒå¤æ‚åˆ†æçš„è®¾è®¡ã€æ‰§è¡Œå’Œå…±äº«ã€‚</li>
<li>å½“å‰Galaxyçš„å…³é”®è¯æ£€ç´¢ç³»ç»Ÿåœ¨è¯­ä¹‰æŸ¥è¯¢å¤„ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯¹äºéç²¾ç¡®åŒ¹é…æŸ¥è¯¢çš„å“åº”ä¸ä½³ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„ä¸¤é˜¶æ®µæ£€ç´¢æ¡†æ¶ï¼Œç»“åˆå¯†é›†å‘é‡æœç´¢å’ŒLLMé‡æ–°æ’åºæ¥æå‡æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨å…ˆè¿›çš„åµŒå…¥æ¨¡å‹è¿›è¡Œå€™é€‰å·¥ä½œæµæ£€ç´¢ï¼Œå¹¶é‡‡ç”¨é¢å‘æŒ‡ä»¤ä¼˜åŒ–çš„ç”Ÿæˆå¼LLMè¿›è¡Œè¯­ä¹‰ä»»åŠ¡å¯¹é½çš„é‡æ–°æ’åºã€‚</li>
<li>æ„å»ºäº†å¸¦æœ‰è¯­ä¹‰ä¸»é¢˜æ ‡æ³¨çš„Galaxyå·¥ä½œæµåŸºå‡†æ•°æ®é›†ï¼Œä»¥æ”¯æŒç¨³å¥è¯„ä¼°ã€‚</li>
<li>å¯¹æ¯”äº†ä¸åŒçš„æ£€ç´¢æ¨¡å‹ï¼ŒåŒ…æ‹¬è¯æ±‡ã€å¯†é›†å’Œé‡æ–°æ’åºæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†IRæŒ‡æ ‡è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82938bbf0cf7d713195c3879d31e5566" align="middle">
<img src="https://picx.zhimg.com/v2-5a2dcef3758cdad5fb422bcfdd32c000" align="middle">
<img src="https://picx.zhimg.com/v2-a3ef8633fc27138525134f1f7ba8ba5b" align="middle">
<img src="https://picx.zhimg.com/v2-f19da9b6322299973504f7da13b1e074" align="middle">
<img src="https://picx.zhimg.com/v2-9ed43d7856a9d55fff20efc9d4029695" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement"><a href="#Multi-Step-Knowledge-Interaction-Analysis-via-Rank-2-Subspace-Disentanglement" class="headerlink" title="Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace   Disentanglement"></a>Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace   Disentanglement</h2><p><strong>Authors:Sekh Mainul Islam, Pepa Atanasova, Isabelle Augenstein</strong></p>
<p>Natural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions, drawing on both external Context Knowledge (CK) and Parametric Knowledge (PK) stored in model weights. Understanding their interaction is key to assessing the grounding of NLEs, yet it remains underexplored. Prior work has largely examined only single-step generation, typically the final answer, and has modelled PK and CK interaction only as a binary choice in a rank-1 subspace. This overlooks richer forms of interaction, such as complementary or supportive knowledge. We propose a novel rank-2 projection subspace that disentangles PK and CK contributions more accurately and use it for the first multi-step analysis of knowledge interactions across longer NLE sequences. Experiments on four QA datasets and three open-weight instruction-tuned LLMs show that diverse knowledge interactions are poorly represented in a rank-1 subspace but are effectively captured in our rank-2 formulation. Our multi-step analysis reveals that hallucinated NLEs align strongly with the PK direction, context-faithful ones balance PK and CK, and Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing PK reliance. This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through a richer rank-2 subspace disentanglement. Code and data: <a target="_blank" rel="noopener" href="https://github.com/copenlu/pk-ck-knowledge-disentanglement">https://github.com/copenlu/pk-ck-knowledge-disentanglement</a>. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€è§£é‡Šï¼ˆNLEsï¼‰æè¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦‚ä½•åŸºäºå¤–éƒ¨ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼ˆCKï¼‰å’Œæ¨¡å‹æƒé‡ä¸­å­˜å‚¨çš„å‚æ•°çŸ¥è¯†ï¼ˆPKï¼‰æ¥åšå‡ºå†³ç­–çš„ã€‚äº†è§£ä¸¤è€…çš„ç›¸äº’ä½œç”¨æ˜¯è¯„ä¼°NLEå®šä½çš„å…³é”®ï¼Œä½†è¿™ä»ç„¶æ˜¯ä¸€ä¸ªå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚å…ˆå‰çš„ç ”ç©¶å¤§å¤šåªå…³æ³¨å•ä¸€æ­¥éª¤çš„ç”Ÿæˆï¼Œé€šå¸¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼Œå¹¶ä¸”ä»…å°†PKå’ŒCKçš„äº¤äº’å»ºæ¨¡ä¸ºrank-1å­ç©ºé—´ä¸­çš„äºŒå…ƒé€‰æ‹©ã€‚è¿™å¿½ç•¥äº†æ›´ä¸°å¯Œçš„äº¤äº’å½¢å¼ï¼Œå¦‚äº’è¡¥æˆ–æ”¯æŒæ€§çŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„rank-2æŠ•å½±å­ç©ºé—´ï¼Œå®ƒèƒ½æ›´å‡†ç¡®åœ°åˆ†ç¦»PKå’ŒCKçš„è´¡çŒ®ï¼Œå¹¶é¦–æ¬¡ç”¨äºå¯¹æ›´é•¿NLEåºåˆ—ä¸­çš„çŸ¥è¯†äº¤äº’è¿›è¡Œå¤šæ­¥åˆ†æã€‚åœ¨å››ä¸ªé—®ç­”æ•°æ®é›†å’Œä¸‰ä¸ªå…¬å¼€æƒé‡æŒ‡ä»¤è°ƒæ•´å‹LLMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨rank-1å­ç©ºé—´ä¸­ï¼Œå¤šæ ·çš„çŸ¥è¯†äº¤äº’è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨æˆ‘ä»¬çš„rank-2å…¬å¼ä¸­å¾—åˆ°äº†æœ‰æ•ˆæ•è·ã€‚æˆ‘ä»¬çš„å¤šæ­¥åˆ†æè¡¨æ˜ï¼Œè™šæ„çš„NLEsä¸PKæ–¹å‘é«˜åº¦ä¸€è‡´ï¼Œä¸Šä¸‹æ–‡å¿ å®çš„NLEså¹³è¡¡PKå’ŒCKï¼Œè€ŒNLEsçš„é“¾å¼æ€ç»´æç¤ºé€šè¿‡å‡å°‘PKä¾èµ–è€Œä½¿ç”Ÿæˆçš„NLEsè½¬å‘CKã€‚è¿™é¡¹å·¥ä½œé€šè¿‡æ›´ä¸°å¯Œçš„rank-2å­ç©ºé—´åˆ†ç¦»æ³•ï¼Œä¸ºLLMä¸­å¤šæ­¥çŸ¥è¯†äº¤äº’çš„ç³»ç»Ÿæ€§ç ”ç©¶æä¾›äº†é¦–ä¸ªæ¡†æ¶ã€‚ä»£ç å’Œæ•°æ®ï¼š<a target="_blank" rel="noopener" href="https://github.com/copenlu/pk-ck-knowledge-disentanglement%E3%80%82">https://github.com/copenlu/pk-ck-knowledge-disentanglementã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01706v1">PDF</a> Under review</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨å¤–éƒ¨ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼ˆCKï¼‰å’Œå‚æ•°çŸ¥è¯†ï¼ˆPKï¼‰è¿›è¡Œå†³ç­–çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼ˆNLEsï¼‰ã€‚ç†è§£ä¸¤è€…äº¤äº’æ˜¯è¯„ä¼°NLEsçš„åŸºç¡€ï¼Œä½†è¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å…ˆå‰çš„ç ”ç©¶å¤§å¤šåªå…³æ³¨å•ä¸€æ­¥éª¤çš„ç”Ÿæˆï¼Œå¦‚æœ€ç»ˆç­”æ¡ˆï¼Œå¹¶ä¸”åªå°†PKå’ŒCKçš„äº¤äº’å»ºæ¨¡ä¸ºrank-1å­ç©ºé—´ä¸­çš„äºŒå…ƒé€‰æ‹©ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„rank-2æŠ•å½±å­ç©ºé—´ï¼Œèƒ½æ›´å‡†ç¡®åœ°åˆ†è§£PKå’ŒCKçš„è´¡çŒ®ï¼Œå¹¶é¦–æ¬¡è¿›è¡Œå¤šæ­¥éª¤åˆ†æï¼Œç ”ç©¶çŸ¥è¯†äº¤äº’åœ¨è¾ƒé•¿NLEåºåˆ—ä¸­çš„è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨rank-1å­ç©ºé—´ä¸­ï¼Œå¤šæ ·çš„çŸ¥è¯†äº¤äº’è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨æˆ‘ä»¬çš„rank-2å…¬å¼ä¸­å¾—åˆ°äº†æœ‰æ•ˆæ•æ‰ã€‚æˆ‘ä»¬çš„å¤šæ­¥åˆ†æå‘ç°ï¼Œè™šæ„çš„NLEsä¸PKæ–¹å‘ç´§å¯†å¯¹é½ï¼Œä¸Šä¸‹æ–‡å¿ å®çš„NLEså¹³è¡¡PKå’ŒCKï¼Œè€Œé€šè¿‡Chain-of-Thoughtæç¤ºç”Ÿæˆçš„NLEsåˆ™é€šè¿‡å‡å°‘PKä¾èµ–è½¬å‘CKã€‚æœ¬æ–‡æä¾›äº†é€šè¿‡æ›´ä¸°å¯Œçš„rank-2å­ç©ºé—´åˆ†è§£è¿›è¡ŒLLMä¸­å¤šæ­¥çŸ¥è¯†äº¤äº’çš„ç³»ç»Ÿæ€§ç ”ç©¶æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€è§£é‡Šï¼ˆNLEsï¼‰æè¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•åˆ©ç”¨å¤–éƒ¨ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼ˆCKï¼‰å’Œå‚æ•°çŸ¥è¯†ï¼ˆPKï¼‰è¿›è¡Œå†³ç­–ã€‚</li>
<li>ç†è§£CKå’ŒPKçš„äº¤äº’æ˜¯è¯„ä¼°NLEsçš„å…³é”®ï¼Œä½†è¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>å…ˆå‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨å•ä¸€æ­¥éª¤ç”Ÿæˆï¼ˆå¦‚æœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œå¿½è§†äº†æ›´ä¸°å¯Œçš„çŸ¥è¯†äº¤äº’å½¢å¼ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„rank-2æŠ•å½±å­ç©ºé—´æ¨¡å‹ï¼Œä»¥æ›´å‡†ç¡®åœ°åˆ†è§£CKå’ŒPKçš„è´¡çŒ®ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå¤šæ ·çš„çŸ¥è¯†äº¤äº’åœ¨rank-1å­ç©ºé—´ä¸­è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨rank-2æ¨¡å‹ä¸­å¾—åˆ°äº†æœ‰æ•ˆæ•æ‰ã€‚</li>
<li>å¤šæ­¥åˆ†æå‘ç°ï¼Œè™šæ„çš„NLEsä¸PKç´§å¯†ç›¸å…³ï¼Œè€Œä¸Šä¸‹æ–‡å¿ å®çš„NLEsåˆ™å¹³è¡¡PKå’ŒCKã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9923adaaf733dc4b1e6c9463c054b4ba" align="middle">
<img src="https://picx.zhimg.com/v2-f7e234f3a4c43ff2bd8709eab81f9818" align="middle">
<img src="https://picx.zhimg.com/v2-fc176c8fd87db9b77767ad4d692d736d" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Prompt-Injection-as-an-Emerging-Threat-Evaluating-the-Resilience-of-Large-Language-Models"><a href="#Prompt-Injection-as-an-Emerging-Threat-Evaluating-the-Resilience-of-Large-Language-Models" class="headerlink" title="Prompt Injection as an Emerging Threat: Evaluating the Resilience of   Large Language Models"></a>Prompt Injection as an Emerging Threat: Evaluating the Resilience of   Large Language Models</h2><p><strong>Authors:Daniyal Ganiuly, Assel Smaiyl</strong></p>
<p>Large Language Models (LLMs) are increasingly used in intelligent systems that perform reasoning, summarization, and code generation. Their ability to follow natural-language instructions, while powerful, also makes them vulnerable to a new class of attacks known as prompt injection. In these attacks, hidden or malicious instructions are inserted into user inputs or external content, causing the model to ignore its intended task or produce unsafe responses. This study proposes a unified framework for evaluating how resistant Large Language Models (LLMs) are to prompt injection attacks. The framework defines three complementary metrics such as the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM) to jointly measure robustness, safety, and semantic stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3 8B Instruct, and Flan-T5-Large) on five common language tasks: question answering, summarization, translation, reasoning, and code generation. Results show that GPT-4 performs best overall, while open-weight models remain more vulnerable. The findings highlight that strong alignment and safety tuning are more important for resilience than model size alone. Results show that all models remain partially vulnerable, especially to indirect and direct-override attacks. GPT-4 achieved the best overall resilience (RDR &#x3D; 9.8 %, SCR &#x3D; 96.4 %), while open-source models exhibited higher performance degradation and lower safety scores. The findings demonstrate that alignment strength and safety tuning play a greater role in resilience than model size alone. The proposed framework offers a structured, reproducible approach for assessing model robustness and provides practical insights for improving LLM safety and reliability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­è¶Šæ¥è¶Šå¸¸ç”¨äºæ¨ç†ã€æ‘˜è¦å’Œä»£ç ç”Ÿæˆã€‚å®ƒä»¬éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›è™½ç„¶å¼ºå¤§ï¼Œä½†ä¹Ÿä½¿å®ƒä»¬å®¹æ˜“å—åˆ°ä¸€ç§åä¸ºæç¤ºæ³¨å…¥çš„æ–°æ”»å‡»çš„å½±å“ã€‚åœ¨è¿™äº›æ”»å‡»ä¸­ï¼Œéšè—æˆ–æ¶æ„çš„æŒ‡ä»¤è¢«æ’å…¥åˆ°ç”¨æˆ·è¾“å…¥æˆ–å¤–éƒ¨å†…å®¹ä¸­ï¼Œå¯¼è‡´æ¨¡å‹å¿½ç•¥å…¶é¢„æœŸçš„ä»»åŠ¡æˆ–äº§ç”Ÿä¸å®‰å…¨çš„å“åº”ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„æŠµæŠ—åŠ›çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†ä¸‰ä¸ªäº’è¡¥çš„æŒ‡æ ‡ï¼Œå³æ¢å¤åŠ›é™è§£æŒ‡æ•°ï¼ˆRDIï¼‰ã€å®‰å…¨åˆè§„ç³»æ•°ï¼ˆSCCï¼‰å’ŒæŒ‡ä»¤å®Œæ•´æ€§æŒ‡æ ‡ï¼ˆIIMï¼‰ï¼Œä»¥å…±åŒè¡¡é‡ç¨³å¥æ€§ã€å®‰å…¨æ€§å’Œè¯­ä¹‰ç¨³å®šæ€§ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå¸¸è§çš„è¯­è¨€ä»»åŠ¡ä¸Šè¯„ä¼°äº†å››ä¸ªæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼ˆGPT-4ã€GPT-4oã€LLaMA-3 8B Instructå’ŒFlan-T5-Largeï¼‰ï¼šé—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ã€æ¨ç†å’Œä»£ç ç”Ÿæˆã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-4æ€»ä½“ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€Œå¼€æ”¾æƒé‡æ¨¡å‹ä»ç„¶æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç»“æœå¼ºè°ƒï¼Œå¯¹äºæ¢å¤åŠ›è€Œè¨€ï¼Œå¼ºå¯¹é½å’Œå®‰å…¨è°ƒæ•´æ¯”å•çº¯çš„æ¨¡å‹å¤§å°æ›´é‡è¦ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹ä»ç„¶å­˜åœ¨éƒ¨åˆ†è„†å¼±æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é—´æ¥å’Œç›´æ¥è¦†ç›–æ”»å‡»æ–¹é¢ã€‚GPT-4è·å¾—äº†æœ€ä½³çš„æ€»ä½“æ¢å¤åŠ›ï¼ˆRDR &#x3D; 9.8%ï¼ŒSCR &#x3D; 96.4%ï¼‰ï¼Œè€Œå¼€æºæ¨¡å‹çš„æ€§èƒ½ä¸‹é™æ›´é«˜ï¼Œå®‰å…¨åˆ†æ•°æ›´ä½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹é½å¼ºåº¦å’Œå®‰å…¨è°ƒæ•´åœ¨æé«˜æ¢å¤åŠ›æ–¹é¢æ¯”å•çº¯çš„æ¨¡å‹å¤§å°æ›´é‡è¦ã€‚æ‰€æå‡ºçš„æ¡†æ¶æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–ã€å¯å¤åˆ¶çš„è¯„ä¼°æ¨¡å‹ç¨³å¥æ€§çš„æ–¹æ³•ï¼Œå¹¶ä¸ºæé«˜LLMçš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›äº†å®é™…è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01634v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­è¢«å¹¿æ³›åº”ç”¨äºæ¨ç†ã€æ‘˜è¦å’Œä»£ç ç”Ÿæˆã€‚ç„¶è€Œï¼Œå®ƒä»¬éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›åŒæ—¶ä¹Ÿä½¿å…¶å®¹æ˜“å—åˆ°ä¸€ç§åä¸ºæç¤ºæ³¨å…¥çš„æ–°æ”»å‡»çš„å½±å“ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥è¯„ä¼°LLMå¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„æŠµæŠ—åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼ˆå³æ¢å¤åŠ›é™ä½æŒ‡æ•°ã€å®‰å…¨åˆè§„ç³»æ•°å’ŒæŒ‡ä»¤å®Œæ•´æ€§æŒ‡æ ‡ï¼‰æ¥è”åˆæµ‹é‡ç¨³å¥æ€§ã€å®‰å…¨æ€§å’Œè¯­ä¹‰ç¨³å®šæ€§ã€‚è¯„ä¼°äº†å››ç§æŒ‡ä»¤è°ƒæ•´æ¨¡å‹åœ¨äº”ç§å¸¸è§è¯­è¨€ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜GPT-4æ€»ä½“è¡¨ç°æœ€ä½³ï¼Œè€Œå¼€æ”¾å¼æƒé‡æ¨¡å‹ä»ç„¶æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚å‘ç°å¼ºå¯¹é½å’Œå®‰å…¨è°ƒæ•´å¯¹äºæé«˜æ¨¡å‹éŸ§æ€§æ¯”å•çº¯æ‰©å¤§æ¨¡å‹è§„æ¨¡æ›´é‡è¦ã€‚æ‰€æœ‰æ¨¡å‹ä»å­˜åœ¨éƒ¨åˆ†è„†å¼±æ€§ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹é—´æ¥å’Œç›´æ¥è¦†ç›–æ”»å‡»ã€‚GPT-4åœ¨æ¢å¤åŠ›å’Œå®‰å…¨æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œå¼€æºæ¨¡å‹æ€§èƒ½ä¸‹é™è¾ƒä¸ºä¸¥é‡ï¼Œå®‰å…¨å¾—åˆ†è¾ƒä½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹é½å¼ºåº¦å’Œå®‰å…¨è°ƒæ•´å¯¹æ¨¡å‹éŸ§æ€§å½±å“å¤§äºæ¨¡å‹è§„æ¨¡ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ºè¯„ä¼°æ¨¡å‹ç¨³å¥æ€§æä¾›äº†ç»“æ„åŒ–ã€å¯é‡å¤çš„æ–¹æ³•ï¼Œå¹¶ä¸ºæé«˜LLMçš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›äº†å®é™…è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­å¹¿æ³›åº”ç”¨äºå¤šç§ä»»åŠ¡ï¼Œä½†ä¹Ÿé¢ä¸´æ–°çš„å®‰å…¨å¨èƒâ€”â€”æç¤ºæ³¨å…¥æ”»å‡»ã€‚</li>
<li>æç¤ºæ³¨å…¥æ”»å‡»é€šè¿‡æ’å…¥éšè—æˆ–æ¶æ„æŒ‡ä»¤å½±å“LLMçš„è¡¨ç°ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹åç¦»åŸå®šä»»åŠ¡æˆ–äº§ç”Ÿä¸å®‰å…¨å›åº”ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥è¯„ä¼°LLMå¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„æŠµæŠ—åŠ›ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®æŒ‡æ ‡ï¼šæ¢å¤åŠ›é™ä½æŒ‡æ•°ã€å®‰å…¨åˆè§„ç³»æ•°å’ŒæŒ‡ä»¤å®Œæ•´æ€§æŒ‡æ ‡ã€‚</li>
<li>åœ¨äº”ç§å¸¸è§è¯­è¨€ä»»åŠ¡ä¸Šè¯„ä¼°äº†å››ç§æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œå‘ç°GPT-4æ€»ä½“è¡¨ç°æœ€ä½³ï¼Œè€Œå¼€æ”¾å¼æƒé‡æ¨¡å‹æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚</li>
<li>å¼ºçƒˆå¯¹é½å’Œå®‰å…¨è°ƒæ•´æ˜¯æé«˜æ¨¡å‹éŸ§æ€§çš„å…³é”®ï¼Œå…¶é‡è¦æ€§è¶…è¿‡å•çº¯çš„æ¨¡å‹è§„æ¨¡æ‰©å¤§ã€‚</li>
<li>æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹ä»å­˜åœ¨éƒ¨åˆ†è„†å¼±æ€§ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹é—´æ¥å’Œç›´æ¥è¦†ç›–æ”»å‡»ã€‚GPT-4åœ¨æ¢å¤åŠ›å’Œå®‰å…¨æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a968ab4b6d044d46bf99c9eef6444e0f" align="middle">
<img src="https://picx.zhimg.com/v2-8d4c77eac425d4f2ea54502d58409f85" align="middle">
<img src="https://picx.zhimg.com/v2-eb13ca704e75b703480d936647383739" align="middle">
<img src="https://picx.zhimg.com/v2-0ae48e5cd5c26335733084cc8289b9e9" align="middle">
<img src="https://picx.zhimg.com/v2-753485ab11072f8f58282313f8e83288" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FirstAidQA-A-Synthetic-Dataset-for-First-Aid-and-Emergency-Response-in-Low-Connectivity-Settings"><a href="#FirstAidQA-A-Synthetic-Dataset-for-First-Aid-and-Emergency-Response-in-Low-Connectivity-Settings" class="headerlink" title="FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in   Low-Connectivity Settings"></a>FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in   Low-Connectivity Settings</h2><p><strong>Authors:Saiyma Sittul Muna, Rezwan Islam Salvi, Mushfiqur Rahman Mushfique, Ajwad Abrar</strong></p>
<p>In emergency situations, every second counts. The deployment of Large Language Models (LLMs) in time-sensitive, low or zero-connectivity environments remains limited. Current models are computationally intensive and unsuitable for low-tier devices often used by first responders or civilians. A major barrier to developing lightweight, domain-specific solutions is the lack of high-quality datasets tailored to first aid and emergency response. To address this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500 high-quality question answer pairs that encompass a wide range of first aid and emergency response scenarios. The dataset was generated using a Large Language Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from the Vital First Aid Book (2019). We applied preprocessing steps such as text cleaning, contextual chunking, and filtering, followed by human validation to ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is designed to support instruction-tuning and fine-tuning of LLMs and Small Language Models (SLMs), enabling faster, more reliable, and offline-capable systems for emergency settings. We publicly release the dataset to advance research on safety-critical and resource-constrained AI applications in first aid and emergency response. The dataset is available on Hugging Face at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA">https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA</a>. </p>
<blockquote>
<p>åœ¨ç´§æ€¥æƒ…å†µä¸‹ï¼Œæ¯ä¸€ç§’éƒ½è‡³å…³é‡è¦ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€è¦å³æ—¶å“åº”ã€ä½è¿é€šæ€§æˆ–æ— è¿é€šæ€§çš„ç¯å¢ƒä¸­çš„éƒ¨ç½²ä»ç„¶å—é™ã€‚å½“å‰æ¨¡å‹è®¡ç®—å¯†é›†ï¼Œä¸é€‚åˆä¸€çº¿æ•‘æ´äººå‘˜æˆ–å¹³æ°‘å¸¸ç”¨çš„ä½ç«¯è®¾å¤‡ã€‚å¼€å‘è½»é‡çº§ã€ç‰¹å®šé¢†åŸŸçš„è§£å†³æ–¹æ¡ˆçš„ä¸»è¦éšœç¢æ˜¯ç¼ºä¹é’ˆå¯¹æ€¥æ•‘å’Œç´§æ€¥å“åº”çš„é«˜è´¨é‡æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FirstAidQAï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼ŒåŒ…å«5500ç»„é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–å¹¿æ³›çš„æ€¥æ•‘å’Œç´§æ€¥å“åº”åœºæ™¯ã€‚è¯¥æ•°æ®é›†æ˜¯ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ChatGPT-4o-miniç”Ÿæˆçš„ï¼Œé‡‡ç”¨åŸºäºæç¤ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ æ³•ï¼Œä½¿ç”¨ã€Šæ€¥æ•‘æ‰‹å†Œã€‹ï¼ˆ2019å¹´ï¼‰ä¸­çš„æ–‡æœ¬ã€‚æˆ‘ä»¬åº”ç”¨äº†æ–‡æœ¬æ¸…æ´—ã€ä¸Šä¸‹æ–‡åˆ†å—å’Œè¿‡æ»¤ç­‰é¢„å¤„ç†æ­¥éª¤ï¼Œç„¶åè¿›è¡Œäººå·¥éªŒè¯ï¼Œä»¥ç¡®ä¿é—®ç­”å¯¹çš„å‡†ç¡®æ€§ã€å®‰å…¨æ€§å’Œå®ç”¨æ€§ã€‚FirstAidQAæ—¨åœ¨æ”¯æŒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æŒ‡ä»¤è°ƒæ•´å’Œå¾®è°ƒï¼Œä¸ºç´§æ€¥æƒ…å†µä¸‹çš„ç³»ç»Ÿæä¾›æ›´å¿«ã€æ›´å¯é ã€æ”¯æŒç¦»çº¿ä½¿ç”¨çš„ç³»ç»Ÿã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæ­¤æ•°æ®é›†ï¼Œä»¥æ¨åŠ¨æ€¥æ•‘å’Œç´§æ€¥å“åº”é¢†åŸŸä¸­å®‰å…¨å…³é”®å’Œèµ„æºå—é™çš„äººå·¥æ™ºèƒ½åº”ç”¨çš„ç ”ç©¶ã€‚è¯¥æ•°æ®é›†å·²åœ¨Hugging Faceä¸Šå‘å¸ƒï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA%E3%80%82">https://huggingface.co/datasets/i-am-mushfiq/FirstAidQAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01289v1">PDF</a> Accepted at the 5th Muslims in Machine Learning (MusIML) Workshop,   co-located with NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong><br>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç´§æ€¥æƒ…å†µä¸‹å‘æŒ¥ä½œç”¨çš„åœºæ™¯ä¸­ï¼Œæ¯ä¸€ç§’éƒ½è‡³å…³é‡è¦ã€‚ç›®å‰LLMåœ¨å³æ—¶å“åº”ã€ä½æˆ–é›¶è¿æ¥ç¯å¢ƒä¸­éƒ¨ç½²æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ä½å±‚çº§è®¾å¤‡çš„åœºæ™¯ä¸­ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºç¼ºä¹ä¸“ä¸ºæ€¥æ•‘å’Œç´§æ€¥å“åº”é‡èº«å®šåˆ¶çš„é«˜è´¨é‡æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™ä¸€ç¼ºå£ï¼Œæˆ‘ä»¬æ¨å‡ºFirstAidQAåˆæˆæ•°æ®é›†ï¼ŒåŒ…å«é’ˆå¯¹å¹¿æ³›çš„ç¬¬ä¸€æ€¥æ•‘å’Œç´§æ€¥å“åº”åœºæ™¯çš„æ•°åƒé«˜è´¨é‡é—®ç­”å¯¹ã€‚æ•°æ®é›†é€šè¿‡ChatGPT-4o-miniå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆï¼Œé‡‡ç”¨åŸºäºæç¤ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ æ³•ï¼Œä½¿ç”¨ã€Šæ€¥æ•‘æ‰‹å†Œã€‹ï¼ˆ2019å¹´ï¼‰ä¸­çš„æ–‡æœ¬ã€‚æˆ‘ä»¬é‡‡ç”¨æ–‡æœ¬æ¸…æ´—ã€ä¸Šä¸‹æ–‡åˆ†å—å’Œè¿‡æ»¤ç­‰é¢„å¤„ç†æ­¥éª¤ï¼Œç»äººå·¥éªŒè¯ç¡®ä¿é—®ç­”å¯¹çš„å‡†ç¡®æ€§ã€å®‰å…¨æ€§å’Œå®ç”¨æ€§ã€‚FirstAidQAæ—¨åœ¨æ”¯æŒå¤§å‹è¯­è¨€æ¨¡å‹å’Œå°å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´å’Œç²¾ç»†è°ƒæ•´ï¼Œé€‚ç”¨äºæ€¥æ•‘åœºæ™¯ä¸­çš„æ›´å¿«ã€æ›´å¯é å’Œç¦»çº¿å¯ç”¨çš„ç³»ç»Ÿã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæ•°æ®é›†ï¼Œä»¥æ¨åŠ¨åœ¨æ€¥æ•‘å’Œç´§æ€¥å“åº”é¢†åŸŸçš„å®‰å…¨å…³é”®å’Œèµ„æºå—é™çš„äººå·¥æ™ºèƒ½åº”ç”¨çš„ç ”ç©¶ã€‚æ•°æ®é›†å¯åœ¨Hugging Faceç½‘ç«™è·å–ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA">https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA</a> ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>åœ¨ç´§æ€¥æƒ…å†µä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æˆ–é›¶è¿æ¥ç¯å¢ƒä¸­ã€‚</li>
<li>å½“å‰æ¨¡å‹è®¡ç®—å¯†é›†ï¼Œä¸é€‚åˆç”¨äºä½å±‚æ¬¡è®¾å¤‡ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹æ€¥æ•‘å’Œç´§æ€¥å“åº”é¢†åŸŸçš„ç‰¹å®šæ•°æ®é›†æ˜¯å¼€å‘è§£å†³æ–¹æ¡ˆçš„ä¸»è¦éšœç¢ä¹‹ä¸€ã€‚</li>
<li>FirstAidQAæ•°æ®é›†é‡‡ç”¨åˆæˆæ–¹æ³•åˆ›å»ºï¼ŒåŒ…å«å¤šç§ç´§æ€¥å“åº”åœºæ™¯ä¸‹çš„é«˜è´¨é‡é—®ç­”å¯¹ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¹¶ä½¿ç”¨ç‰¹å®šæŠ€æœ¯è¿›è¡ŒéªŒè¯ï¼Œä»¥ç¡®ä¿å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45439a861d53385fa18120b109289eae" align="middle">
<img src="https://picx.zhimg.com/v2-85eaf4c248a5b72a332f2929884ac41f" align="middle">
<img src="https://picx.zhimg.com/v2-f22aad7a10a82917df903ced897f3689" align="middle">
<img src="https://picx.zhimg.com/v2-d751f302ef08d6d0384f4e19b1b5e8f7" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Do-Math-Reasoning-LLMs-Help-Predict-the-Impact-of-Public-Transit-Events"><a href="#Do-Math-Reasoning-LLMs-Help-Predict-the-Impact-of-Public-Transit-Events" class="headerlink" title="Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?"></a>Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?</h2><p><strong>Authors:Bowen Fang, Ruijian Zha, Xuan Di</strong></p>
<p>Predicting public transit incident duration from unstructured text alerts is a critical but challenging task. Addressing the domain sparsity of transit operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task involves noisy, continuous labels and lacks reliable expert demonstrations for reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels at tasks with binary correctness, like mathematics, its applicability to noisy, continuous forecasting is an open question. This work, to our knowledge, is the first to bridge the gap between RLVR LLM training with the critical, real-world forecasting challenges in public transit operations. We adapt RLVR to this task by introducing a tolerance-based, shaped reward function that grants partial credit within a continuous error margin, rather than demanding a single correct answer. We systematically evaluate this framework on a curated dataset of NYC MTA service alerts. Our findings show that general-purpose, instruction-tuned LLMs significantly outperform specialized math-reasoning models, which struggle with the ambiguous, real-world text. We empirically demonstrate that the binary reward is unstable and degrades performance, whereas our shaped reward design is critical and allows our model to dominate on the most challenging metrics. While classical regressors are superior at minimizing overall MAE or MSE, our RLVR approach achieved a 35% relative improvement in 5-minute accuracy (Acc@5) over the strongest baseline. This demonstrates that RLVR can be successfully adapted to real-world, noisy forecasting, but requires a verifier design that reflects the continuous nature of the problem. </p>
<blockquote>
<p>é¢„æµ‹å…¬å…±äº¤é€šäº‹ä»¶æŒç»­æ—¶é—´ä»éç»“æ„åŒ–æ–‡æœ¬è­¦æŠ¥æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä½¿ç”¨æ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è§£å†³å…¬å…±äº¤é€šè¿è¥é¢†åŸŸçš„ç¨€ç–æ€§æ˜¯å¾ˆå›°éš¾çš„ï¼Œå› ä¸ºè¿™é¡¹ä»»åŠ¡æ¶‰åŠå¸¦æœ‰å™ªå£°çš„è¿ç»­æ ‡ç­¾ï¼Œå¹¶ä¸”ç¼ºä¹å¯é çš„ä¸“å®¶æ¼”ç¤ºæ¥è¿›è¡Œæ¨ç†ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦ç­‰å…·æœ‰äºŒå…ƒæ­£ç¡®æ€§çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨å™ªå£°å¤§ã€è¿ç»­æ€§é¢„æµ‹æ–¹é¢çš„é€‚ç”¨æ€§å°šå¾…è§£ç­”ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œé¦–æ¬¡æ¶èµ·äº†RLVRå¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒå’Œå…¬å…±äº¤é€šè¿è¥ä¸­å…³é”®ç°å®é¢„æµ‹æŒ‘æˆ˜ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥åŸºäºå®¹å¿åº¦çš„å¥–åŠ±å‡½æ•°æ¥é€‚åº”RLVRè¿›è¡Œè¿™é¡¹ä»»åŠ¡ï¼Œåœ¨è¿ç»­çš„è¯¯å·®èŒƒå›´å†…æˆäºˆéƒ¨åˆ†ä¿¡ç”¨ï¼Œè€Œä¸æ˜¯è¦æ±‚å•ä¸€çš„æ­£ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬åœ¨çº½çº¦å¸‚MTAæœåŠ¡è­¦æŠ¥ç¼–è¾‘çš„æ•°æ®é›†ä¸Šç³»ç»Ÿåœ°è¯„ä¼°äº†è¿™ä¸€æ¡†æ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šç”¨æŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹æ˜¾è‘—ä¼˜äºä¸“ä¸šæ•°å­¦æ¨ç†æ¨¡å‹ï¼Œåè€…åœ¨å¤„ç†æ¨¡ç³Šç°å®æ–‡æœ¬æ—¶é‡åˆ°éº»çƒ¦ã€‚æˆ‘ä»¬é€šè¿‡å®è¯è¯æ˜äº†äºŒå…ƒå¥–åŠ±æ˜¯ä¸ç¨³å®šçš„å¹¶ä¸”ä¼šé™ä½æ€§èƒ½ï¼Œè€Œæˆ‘ä»¬çš„å½¢çŠ¶å¥–åŠ±è®¾è®¡æ˜¯å…³é”®çš„ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æŒ‡æ ‡ä¸Šå æ®ä¸»å¯¼åœ°ä½ã€‚è™½ç„¶ç»å…¸å›å½’å™¨åœ¨æœ€å°åŒ–æ€»ä½“MAEæˆ–MSEæ–¹é¢æ›´ä¼˜è¶Šï¼Œä½†æˆ‘ä»¬çš„RLVRæ–¹æ³•åœ¨æœ€å‡†ç¡®çš„äº”åˆ†é’ŸæŒ‡æ ‡ï¼ˆAcc@5ï¼‰ä¸Šç›¸å¯¹äºæœ€å¼ºçš„åŸºçº¿å®ç°äº†35%çš„ç›¸å¯¹æ”¹è¿›ã€‚è¿™è¡¨æ˜RLVRå¯ä»¥æˆåŠŸé€‚åº”ç°å®ä¸–ç•Œçš„å™ªå£°é¢„æµ‹ï¼Œä½†éœ€è¦åæ˜ é—®é¢˜è¿ç»­æ€§çš„éªŒè¯å™¨è®¾è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00808v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¬å…±äº¤é€šè¿è¾“é¢†åŸŸä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨é¢„æµ‹å…¬äº¤äº‹ä»¶æŒç»­æ—¶é—´æ–¹é¢çš„æŒ‘æˆ˜ä¸åˆ›æ–°ã€‚ç”±äºå…¬å…±äº¤é€šæ“ä½œçš„é¢†åŸŸç‰¹æ®Šæ€§ï¼Œæ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é¢ä¸´å›°éš¾ã€‚æœ¬æ–‡é¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸å…¬äº¤è¿è¥ä¸­çš„é¢„æµ‹æŒ‘æˆ˜ç›¸ç»“åˆã€‚é€šè¿‡å¼•å…¥åŸºäºå®¹å¿åº¦çš„å¥–åŠ±å‡½æ•°ï¼Œæ¨¡å‹èƒ½åœ¨è¿ç»­è¯¯å·®èŒƒå›´å†…ç»™äºˆéƒ¨åˆ†ä¿¡ç”¨ï¼Œè€Œéå•ä¸€æ­£ç¡®ç­”æ¡ˆã€‚ç ”ç©¶åœ¨çº½çº¦å¸‚äº¤é€šå±€æœåŠ¡è­¦æŠ¥çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°é€šç”¨ã€æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜äºä¸“ä¸šçš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå¹¶æŒ‡å‡ºäº†å¯¹çœŸå®ä¸–ç•Œæ¨¡ç³Šæ–‡æœ¬çš„è‰¯å¥½é€‚åº”æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æŒ‡æ ‡ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚è™½ç„¶ä¼ ç»Ÿçš„å›å½’å™¨åœ¨æœ€å°åŒ–æ•´ä½“MAEæˆ–MSEæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨äº”åˆ†é’Ÿçš„å‡†ç¡®æ€§ä¸Šï¼Œæˆ‘ä»¬çš„RLVRæ–¹æ³•ç›¸å¯¹äºæœ€å¼ºçš„åŸºçº¿å®ç°äº†ç›¸å¯¹æ”¹å–„ç‡è¾¾åˆ°äº†35%ã€‚è¿™è¡¨æ˜RLVRå¯ä»¥æˆåŠŸé€‚åº”ç°å®ä¸–ç•Œçš„å™ªå£°é¢„æµ‹ï¼Œä½†éœ€è¦åæ˜ é—®é¢˜çš„è¿ç»­æ€§çš„éªŒè¯å™¨è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„æµ‹å…¬äº¤äº‹ä»¶æŒç»­æ—¶é—´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œæ¶‰åŠå¤„ç†å…·æœ‰é¢†åŸŸç‰¹å®šæ€§å’Œè¿ç»­æ ‡ç­¾çš„å™ªå£°æ•°æ®ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é¦–æ¬¡è¢«å¼•å…¥è§£å†³æ­¤ç±»é—®é¢˜ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¼•å…¥åŸºäºå®¹å¿åº¦çš„å¥–åŠ±å‡½æ•°ã€‚</li>
<li>LLMçš„è¡¨ç°ä¼˜äºä¸“é—¨çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå¤„ç†æ¨¡ç³Šç°å®æ–‡æœ¬çš„ä¼˜åŠ¿ã€‚</li>
<li>åœ¨äº”åˆ†é’Ÿå‡†ç¡®åº¦æ–¹é¢ï¼ŒRLVRæ–¹æ³•ç›¸å¯¹äºä¼ ç»Ÿå›å½’æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ã€‚è¿™ä½“ç°äº†å¯¹ç°å®ä¸–ç•Œå™ªå£°é¢„æµ‹çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61612212749932281c92d35ae82e46ed" align="middle">
<img src="https://picx.zhimg.com/v2-95ab20af924071b629b628dab23ed51e" align="middle">
<img src="https://picx.zhimg.com/v2-ad94bcee26e65429567c6903784bce99" align="middle">
<img src="https://picx.zhimg.com/v2-4e88b73ac91f4b15c6d97266ab6117ec" align="middle">
<img src="https://picx.zhimg.com/v2-18f5cb3055428b45d2c63b0ad263553e" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Count-Based-Approaches-Remain-Strong-A-Benchmark-Against-Transformer-and-LLM-Pipelines-on-Structured-EHR"><a href="#Count-Based-Approaches-Remain-Strong-A-Benchmark-Against-Transformer-and-LLM-Pipelines-on-Structured-EHR" class="headerlink" title="Count-Based Approaches Remain Strong: A Benchmark Against Transformer   and LLM Pipelines on Structured EHR"></a>Count-Based Approaches Remain Strong: A Benchmark Against Transformer   and LLM Pipelines on Structured EHR</h2><p><strong>Authors:Jifan Gao, Michael Rosenthal, Brian Wolpin, Simona Cristea</strong></p>
<p>Structured electronic health records (EHR) are essential for clinical prediction. While count-based learners continue to perform strongly on such data, no benchmarking has directly compared them against more recent mixture-of-agents LLM pipelines, which have been reported to outperform single LLMs in various NLP tasks. In this study, we evaluated three categories of methodologies for EHR prediction using the EHRSHOT dataset: count-based models built from ontology roll-ups with two time bins, based on LightGBM and the tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR); and a mixture-of-agents pipeline that converts tabular histories to natural-language summaries followed by a text classifier. We assessed eight outcomes using the EHRSHOT dataset. Across the eight evaluation tasks, head-to-head wins were largely split between the count-based and the mixture-of-agents methods. Given their simplicity and interpretability, count-based models remain a strong candidate for structured EHR benchmarking. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/cristea-lab/Structured_EHR_Benchmark">https://github.com/cristea-lab/Structured_EHR_Benchmark</a>. </p>
<blockquote>
<p>ç»“æ„åŒ–ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰å¯¹ä¸´åºŠé¢„æµ‹è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºè®¡æ•°çš„å­¦ä¹ è€…åœ¨æ­¤ç±»æ•°æ®ä¸Šçš„è¡¨ç°ä»ç„¶å¼ºåŠ²ï¼Œä½†å°šæ— åŸºå‡†æµ‹è¯•ç›´æ¥å°†å…¶ä¸æœ€è¿‘çš„æ··åˆä»£ç†LLMç®¡é“è¿›è¡Œæ¯”è¾ƒã€‚æ®æŠ¥é“ï¼Œè¿™äº›ç®¡é“åœ¨å„ç§NLPä»»åŠ¡ä¸­çš„æ€§èƒ½è¶…è¿‡äº†å•ä¸€LLMã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨EHRSHOTæ•°æ®é›†è¯„ä¼°äº†ä¸‰ç±»EHRé¢„æµ‹æ–¹æ³•ï¼šåŸºäºæœ¬ä½“æ»šåŠ¨å’Œä¸¤æ—¶é—´ä»“çš„åŸºäºè®¡æ•°çš„æ¨¡å‹ï¼ŒåŸºäºLightGBMå’Œè¡¨æ ¼åŸºç¡€æ¨¡å‹TabPFNï¼›é¢„è®­ç»ƒçš„é¡ºåºå˜å‹å™¨ï¼ˆCLMBRï¼‰ï¼›ä»¥åŠå°†è¡¨æ ¼å†å²è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æ‘˜è¦ç„¶åé€šè¿‡æ–‡æœ¬åˆ†ç±»å™¨å¤„ç†çš„æ··åˆä»£ç†ç®¡é“ã€‚æˆ‘ä»¬ä½¿ç”¨EHRSHOTæ•°æ®é›†è¯„ä¼°äº†å…«ç§ç»“æœã€‚åœ¨å…«ä¸ªè¯„ä¼°ä»»åŠ¡ä¸­ï¼ŒåŸºäºè®¡æ•°çš„æ–¹æ³•å’Œæ··åˆä»£ç†æ–¹æ³•ä¹‹é—´çš„å¤´å¯¹å¤´è·èƒœè€…å¤§å¤šåˆ†åº­æŠ—ç¤¼ã€‚é‰´äºå…¶ç®€å•æ€§å’Œå¯è§£é‡Šæ€§ï¼ŒåŸºäºè®¡æ•°çš„æ¨¡å‹ä»ç„¶æ˜¯ç»“æ„åŒ–EHRåŸºå‡†æµ‹è¯•çš„å¼ºåŠ²å€™é€‰è€…ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/cristea-lab/Structured_EHR_Benchmark%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/cristea-lab/Structured_EHR_Benchmarkè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰é¢„æµ‹ä¸­ä½¿ç”¨ä¸åŒæ–¹æ³•çš„æ•ˆæœã€‚æ–‡ç« å¯¹æ¯”äº†åŸºäºè®¡æ•°çš„æ¨¡å‹ã€åŸºäºLightGBMçš„æ¨¡å‹ã€é¢„è®­ç»ƒåºåˆ—è½¬æ¢å™¨CLMBRä»¥åŠæ··åˆä»£ç†ç®¡é“ç­‰æ–¹æ³•ã€‚ä½¿ç”¨EHRSHOTæ•°æ®é›†è¯„ä¼°äº†å…«ç§ç»“æœã€‚å¤´å¯¹å¤´æ¯”è¾ƒæ˜¾ç¤ºï¼ŒåŸºäºè®¡æ•°çš„æ–¹æ³•å’Œæ··åˆä»£ç†æ–¹æ³•å„æœ‰ä¼˜åŠ¿ã€‚è€ƒè™‘åˆ°ç®€å•æ€§å’Œå¯è§£é‡Šæ€§ï¼ŒåŸºäºè®¡æ•°çš„æ¨¡å‹ä»æ˜¯ç»“æ„åŒ–EHRåŸºå‡†æµ‹è¯•çš„æœ‰åŠ›å€™é€‰è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰åœ¨ä¸´åºŠé¢„æµ‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå¤šç§æ–¹æ³•è¢«ç”¨äºå¤„ç†å’Œåˆ†æè¿™äº›æ•°æ®ã€‚</li>
<li>å°šæ— åŸºå‡†æµ‹è¯•ç›´æ¥å¯¹æ¯”åŸºäºè®¡æ•°çš„æ¨¡å‹ä¸æ›´è¿‘çš„æ··åˆä»£ç†LLMç®¡é“åœ¨EHRé¢„æµ‹ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨EHRSHOTæ•°æ®é›†è¯„ä¼°äº†ä¸‰ç§æ–¹æ³•çš„æ€§èƒ½ï¼šåŸºäºè®¡æ•°çš„æ¨¡å‹ã€åŸºäºLightGBMçš„æ¨¡å‹å’Œæ··åˆä»£ç†ç®¡é“ã€‚</li>
<li>åœ¨å…«é¡¹è¯„ä¼°ä»»åŠ¡ä¸­ï¼ŒåŸºäºè®¡æ•°çš„æ–¹æ³•å’Œæ··åˆä»£ç†æ–¹æ³•å„æœ‰ä¼˜åŠ¿ï¼Œæ²¡æœ‰ä¸€ç§æ–¹æ³•åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°æœ€å¥½ã€‚</li>
<li>åŸºäºè®¡æ•°çš„æ¨¡å‹å› å…¶ç®€å•æ€§å’Œå¯è§£é‡Šæ€§ï¼Œåœ¨ç»“æ„åŒ–EHRåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>æ–‡ç« æä¾›çš„æºä»£ç å¯ç”¨äºè¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d10de0726c617ac3205961ead6c2840" align="middle">
<img src="https://picx.zhimg.com/v2-bd57837cec1c999665512ebb383a0a9b" align="middle">
<img src="https://picx.zhimg.com/v2-0525a9750794d63ddf1ee8fd9e5d05fe" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Exploring-and-Mitigating-Gender-Bias-in-Encoder-Based-Transformer-Models"><a href="#Exploring-and-Mitigating-Gender-Bias-in-Encoder-Based-Transformer-Models" class="headerlink" title="Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models"></a>Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models</h2><p><strong>Authors:Ariyan Hossain, Khondokar Mohammad Ahanaf Hannan, Rakinul Haque, Nowreen Tarannum Rafa, Humayra Musarrat, Shoaib Ahmed Dipu, Farig Yousuf Sadeque</strong></p>
<p>Gender bias in language models has gained increasing attention in the field of natural language processing. Encoder-based transformer models, which have achieved state-of-the-art performance in various language tasks, have been shown to exhibit strong gender biases inherited from their training data. This paper investigates gender bias in contextualized word embeddings, a crucial component of transformer-based models. We focus on prominent architectures such as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to gender bias. To quantify the degree of bias, we introduce a novel metric, MALoR, which assesses bias based on model probabilities for filling masked tokens. We further propose a mitigation approach involving continued pre-training on a gender-balanced dataset generated via Counterfactual Data Augmentation. Our experiments reveal significant reductions in gender bias scores across different pronoun pairs. For instance, in BERT-base, bias scores for â€œhe-sheâ€ dropped from 1.27 to 0.08, and â€œhis-herâ€ from 2.51 to 0.36 following our mitigation approach. We also observed similar improvements across other models, with â€œmale-femaleâ€ bias decreasing from 1.82 to 0.10 in BERT-large. Our approach effectively reduces gender bias without compromising model performance on downstream tasks. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­ï¼Œè¯­è¨€æ¨¡å‹ä¸­çš„æ€§åˆ«åè§å·²å¼•èµ·è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚åŸºäºç¼–ç å™¨è½¬æ¢å™¨æ¨¡å‹åœ¨å„ç§è¯­è¨€ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å·²è¢«è¯æ˜è¡¨ç°å‡ºå¼ºçƒˆçš„æ€§åˆ«åè§ï¼Œè¿™äº›åè§æ˜¯ä»å…¶è®­ç»ƒæ•°æ®ä¸­ç»§æ‰¿è€Œæ¥çš„ã€‚æœ¬æ–‡ç ”ç©¶äº†åŸºäºè½¬æ¢æ¨¡å‹çš„è¯­å¢ƒåŒ–è¯åµŒå…¥ä¸­çš„æ€§åˆ«åè§é—®é¢˜ã€‚æˆ‘ä»¬å…³æ³¨BERTã€ALBERTã€RoBERTaå’ŒDistilBERTç­‰çªå‡ºæ¶æ„ï¼Œä»¥æ£€æŸ¥å®ƒä»¬å¯¹æ€§åˆ«åè§çš„è„†å¼±æ€§ã€‚ä¸ºäº†è¡¡é‡åè§çš„ç¨‹åº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹åº¦é‡æ ‡å‡†MALoRï¼Œå®ƒæ ¹æ®æ¨¡å‹å¡«å……é®ç½©ç¬¦å·çš„æ¦‚ç‡æ¥è¯„ä¼°åè§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ç¼“è§£æ–¹æ³•ï¼Œæ¶‰åŠåœ¨é€šè¿‡åäº‹å®æ•°æ®å¢å¼ºç”Ÿæˆçš„æ€§åˆ«å¹³è¡¡æ•°æ®é›†ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒã€‚æˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºä¸åŒä»£è¯å¯¹çš„æ€§åˆ«åè§åˆ†æ•°æ˜¾è‘—ä¸‹é™ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„ç¼“è§£æ–¹æ³•å®æ–½åï¼ŒBERT-baseä¸­â€œhe-sheâ€çš„åè§åˆ†æ•°ä»1.27é™è‡³0.08ï¼Œâ€œhis-herâ€ä»2.51é™è‡³0.36ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°å…¶ä»–æ¨¡å‹çš„ç±»ä¼¼æ”¹è¿›ï¼Œâ€œmale-femaleâ€åè§åœ¨BERT-largeä¸­ä»1.82é™è‡³0.10ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸æŸå®³ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹æ€§èƒ½çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°å‡å°‘äº†æ€§åˆ«åè§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00519v1">PDF</a> 25 pages, 20 figures</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ç ”ç©¶äº†åŸºäºç¼–ç å™¨çš„è½¬æ¢æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ€§åˆ«åè§é—®é¢˜ã€‚æ–‡ç« èšç„¦äºBERTã€ALBERTã€RoBERTaå’ŒDistilBERTç­‰ä¸»æµæ¶æ„ï¼Œé€šè¿‡å¼•å…¥æ–°çš„åº¦é‡æ ‡å‡†MALoRæ¥è¯„ä¼°æ¨¡å‹å¯¹æ€§åˆ«åè§çš„æ˜“æ„Ÿæ€§ã€‚åŒæ—¶æå‡ºäº†ä¸€ç§é€šè¿‡å¹³è¡¡æ€§åˆ«æ•°æ®é›†æŒç»­é¢„è®­ç»ƒæ¥å‡è½»åè§çš„æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ­¤æ–¹æ³•æ˜¾è‘—é™ä½äº†æ€§åˆ«åè§å¾—åˆ†ï¼Œå¹¶ä¸”ä¸ä¼šå¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½äº§ç”Ÿå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®ºæ–‡èšç„¦äºåŸºäºç¼–ç å™¨è½¬æ¢æ¨¡å‹çš„è¯­å¢ƒåŒ–è¯åµŒå…¥ä¸­çš„æ€§åˆ«åè§é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹BERTç­‰ä¸»æµæ¶æ„è¿›è¡Œæ€§åˆ«åè§è¯„ä¼°ã€‚</li>
<li>å¼•å…¥æ–°çš„åº¦é‡æ ‡å‡†MALoRæ¥è¯„ä¼°æ¨¡å‹å¯¹æ€§åˆ«åè§çš„ç¨‹åº¦ã€‚</li>
<li>æå‡ºé€šè¿‡å¹³è¡¡æ€§åˆ«æ•°æ®é›†æŒç»­é¢„è®­ç»ƒæ¥å‡è½»æ€§åˆ«åè§çš„æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†ä¸åŒä»£è¯å¯¹çš„æ€§åˆ«åè§å¾—åˆ†ã€‚</li>
<li>æ–¹æ³•åœ¨é™ä½æ€§åˆ«åè§çš„åŒæ—¶ï¼Œä¸ä¼šæŸå®³æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-647f90670c71fa31596bb4e48f3cabd5" align="middle">
<img src="https://picx.zhimg.com/v2-d5522c714dd827cbf80eef2be18459cf" align="middle">
<img src="https://picx.zhimg.com/v2-0f07b6ccb825fbc1bdfcc236a4192a6c" align="middle">
<img src="https://picx.zhimg.com/v2-7339df2d5c9a12e4dc7dff875a2f8c26" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Analysis-of-LLM-Adaptation-SFT-LoRA-and-ICL-in-Data-Scarce-Scenarios"><a href="#A-Comparative-Analysis-of-LLM-Adaptation-SFT-LoRA-and-ICL-in-Data-Scarce-Scenarios" class="headerlink" title="A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in   Data-Scarce Scenarios"></a>A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in   Data-Scarce Scenarios</h2><p><strong>Authors:Bernd Bohnet, Rumen Dangovski, Kevin Swersky, Sherry Moore, Arslan Chaudhry, Kathleen Kenealy, Noah Fiedel</strong></p>
<p>The remarkable capabilities of Large Language Models (LLMs) often need to be tailored for specific applications, requiring the integration of new knowledge or the acquisition of new skills. While full fine-tuning is a powerful adaptation method, it is computationally expensive and can lead to a degradation of general reasoning abilities, a phenomenon known as catastrophic forgetting. A range of alternative techniques exists, each with its own trade-offs. In-Context Learning (ICL) is fast but limited by context length, while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) offer a middle ground by minimizing parameter changes. However, the challenge of catastrophic forgetting persists, raising questions about the best adaptation strategy for a given task. This paper presents a comparative analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce scenarios. We find that LoRA provides the most effective balance, successfully instilling new skills with minimal impact on the base modelâ€™s general knowledge. In contrast, while SFT excels at skill acquisition, it is highly susceptible to catastrophic forgetting. ICL is effective for incorporating factual knowledge but struggles with complex skills. Our findings offer a practical framework for selecting an LLM adaptation strategy. We highlight the critical distinction between skill acquisition and knowledge integration, clarify the trade-offs between task-specific performance and the preservation of general capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ˜¾è‘—èƒ½åŠ›é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šåº”ç”¨è¿›è¡Œå®šåˆ¶ï¼Œè¿™éœ€è¦æ•´åˆæ–°çŸ¥è¯†æˆ–è·å–æ–°æŠ€èƒ½ã€‚è™½ç„¶å®Œå…¨å¾®è°ƒæ˜¯ä¸€ç§å¼ºå¤§çš„é€‚åº”æ–¹æ³•ï¼Œä½†å®ƒçš„è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¹¶å¯èƒ½å¯¼è‡´ä¸€èˆ¬æ¨ç†èƒ½åŠ›çš„ä¸‹é™ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºç¾éš¾æ€§é—å¿˜ã€‚å­˜åœ¨ä¸€ç³»åˆ—æ›¿ä»£æŠ€æœ¯ï¼Œæ¯ç§æŠ€æœ¯éƒ½æœ‰å…¶è‡ªèº«çš„æƒè¡¡ã€‚ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰è™½ç„¶å¿«é€Ÿï¼Œä½†å—é™äºä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè€Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åˆ™é€šè¿‡æœ€å°åŒ–å‚æ•°å˜åŒ–æä¾›äº†ä¸­é—´åœ°å¸¦ã€‚ç„¶è€Œï¼Œç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼Œè¿™å¼•å‘äº†å…³äºç»™å®šä»»åŠ¡çš„æœ€ä½³é€‚åº”ç­–ç•¥çš„é—®é¢˜ã€‚æœ¬æ–‡å¯¹æ•°æ®ç¨€ç¼ºåœºæ™¯ä¸‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€LoRAå’ŒICLè¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚æˆ‘ä»¬å‘ç°LoRAæä¾›äº†æœ€æœ‰æ•ˆçš„å¹³è¡¡ï¼Œåœ¨æˆåŠŸçŒè¾“æ–°æŠ€èƒ½çš„åŒæ—¶ï¼Œå¯¹åŸºç¡€æ¨¡å‹çš„ä¸€èˆ¬çŸ¥è¯†å½±å“æœ€å°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè™½ç„¶SFTåœ¨æŠ€èƒ½è·å–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒå¾ˆå®¹æ˜“å—åˆ°ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚ICLåœ¨èå…¥äº‹å®çŸ¥è¯†æ–¹é¢æœ‰æ•ˆï¼Œä½†åœ¨å¤„ç†å¤æ‚æŠ€èƒ½æ—¶å´é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬çš„å‘ç°æä¾›äº†ä¸€ä¸ªé€‰æ‹©LLMé€‚åº”ç­–ç•¥çš„å®é™…æ¡†æ¶ã€‚æˆ‘ä»¬å¼ºè°ƒäº†æŠ€èƒ½è·å–å’ŒçŸ¥è¯†æ•´åˆä¹‹é—´çš„å…³é”®åŒºåˆ«ï¼Œæ¾„æ¸…äº†ç‰¹å®šä»»åŠ¡æ€§èƒ½å’Œä¿ç•™ä¸€èˆ¬èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00130v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šåº”ç”¨ä¸­çš„å‡ºè‰²è¡¨ç°éœ€è¦å¯¹å…¶è¿›è¡Œé€‚åº”æ€§è°ƒæ•´ï¼Œè¿™åŒ…æ‹¬é›†æˆæ–°çŸ¥è¯†æˆ–è·å–æ–°æŠ€èƒ½ã€‚è™½ç„¶å…¨é¢å¾®è°ƒæ˜¯ä¸€ç§å¼ºå¤§çš„é€‚åº”æ–¹æ³•ï¼Œä½†å…¶è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å¯èƒ½å¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼ˆç¾éš¾æ€§é—å¿˜ï¼‰ã€‚å­˜åœ¨ä¸€ç³»åˆ—æ›¿ä»£æŠ€æœ¯ï¼Œå„æœ‰åˆ©å¼Šã€‚ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å¿«é€Ÿä½†å—é™äºä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè€Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åˆ™é€šè¿‡æœ€å°åŒ–å‚æ•°å˜åŒ–æ‰¾åˆ°äº†ä¸€ç§å¹³è¡¡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ä¾ç„¶å­˜åœ¨ï¼Œå¯¹äºç»™å®šä»»åŠ¡çš„æœ€ä½³é€‚åº”ç­–ç•¥ä»æœ‰é—®é¢˜å¾…è§£ç­”ã€‚æœ¬ç ”ç©¶å¯¹ç¨€ç¼ºæ•°æ®åœºæ™¯ä¸‹Supervised Finetuningï¼ˆSFTï¼‰ã€LoRAå’ŒICLè¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚å‘ç°LoRAåœ¨ä¼ æˆæ–°æŠ€èƒ½æ—¶å½±å“æœ€å°ï¼Œå¹³è¡¡æ€§æœ€ä½³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSFTåœ¨æŠ€èƒ½è·å–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®¹æ˜“é­å—ç¾éš¾æ€§é—å¿˜ã€‚ICLåœ¨èå…¥äº‹å®çŸ¥è¯†æ–¹é¢æœ‰æ•ˆï¼Œä½†åœ¨å¤„ç†å¤æ‚æŠ€èƒ½æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºé€‰æ‹©LLMé€‚åº”ç­–ç•¥æä¾›äº†å®ç”¨æ¡†æ¶ï¼Œå¹¶å¼ºè°ƒäº†æŠ€èƒ½è·å–ä¸çŸ¥è¯†æ•´åˆä¹‹é—´çš„å…³é”®åŒºåˆ«ï¼Œæ˜ç¡®äº†ä»»åŠ¡ç‰¹å®šæ€§èƒ½ä¸ä¿ç•™ä¸€èˆ¬èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç‰¹å®šåº”ç”¨ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé€‚åº”æ€§è°ƒæ•´ã€‚</li>
<li>å…¨é¢å¾®è°ƒæ˜¯ä¸€ç§å¼ºå¤§çš„é€‚åº”æ–¹æ³•ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ä¸”å¯èƒ½å¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼ˆç¾éš¾æ€§é—å¿˜ï¼‰ã€‚</li>
<li>å­˜åœ¨å¤šç§é€‚åº”ç­–ç•¥ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å¦‚ä½ç§©é€‚åº”ç­‰ã€‚</li>
<li>LoRAåœ¨ä¼ æˆæ–°æŠ€èƒ½æ—¶å½±å“æœ€å°ï¼Œå¹³è¡¡æ€§æœ€ä½³ï¼›Supervised Finetuningåœ¨æŠ€èƒ½è·å–æ–¹é¢è¡¨ç°å‡ºè‰²ä½†æ˜“é—å¿˜ï¼›ä¸Šä¸‹æ–‡å­¦ä¹ åœ¨èå…¥äº‹å®çŸ¥è¯†æ–¹é¢æœ‰æ•ˆä½†å¤„ç†å¤æ‚æŠ€èƒ½æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>é€‰æ‹©LLMé€‚åº”ç­–ç•¥æ—¶éœ€è¦è€ƒè™‘ä»»åŠ¡ç‰¹æ€§ã€æ•°æ®ç¨€ç¼ºæ€§å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>æŠ€èƒ½è·å–å’ŒçŸ¥è¯†æ•´åˆæ˜¯ä¸¤ç§ä¸åŒçš„è¿‡ç¨‹ï¼Œéœ€è¦åŒºåˆ†å¯¹å¾…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1fce12541c25da90d5e19a70dda55d3a" align="middle">
<img src="https://picx.zhimg.com/v2-331e4e0b9b9f413031bdea8540665d74" align="middle">
<img src="https://picx.zhimg.com/v2-1dbd21cb8dd7e00dc4fa4cdcdd1865bb" align="middle">
<img src="https://picx.zhimg.com/v2-16a0047e57c939c54e0e63ab84d63e31" align="middle">
<img src="https://picx.zhimg.com/v2-f020322d57bd76506e6472317755804e" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AWARE-Beyond-Sentence-Boundaries-A-Contextual-Transformer-Framework-for-Identifying-Cultural-Capital-in-STEM-Narratives"><a href="#AWARE-Beyond-Sentence-Boundaries-A-Contextual-Transformer-Framework-for-Identifying-Cultural-Capital-in-STEM-Narratives" class="headerlink" title="AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework   for Identifying Cultural Capital in STEM Narratives"></a>AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework   for Identifying Cultural Capital in STEM Narratives</h2><p><strong>Authors:Khalid Mehtab Khan, Anagha Kulkarni</strong></p>
<p>Identifying cultural capital (CC) themes in student reflections can offer valuable insights that help foster equitable learning environments in classrooms. However, themes such as aspirational goals or family support are often woven into narratives, rather than appearing as direct keywords. This makes them difficult to detect for standard NLP models that process sentences in isolation. The core challenge stems from a lack of awareness, as standard models are pre-trained on general corpora, leaving them blind to the domain-specific language and narrative context inherent to the data. To address this, we introduce AWARE, a framework that systematically attempts to improve a transformer modelâ€™s awareness for this nuanced task. AWARE has three core components: 1) Domain Awareness, adapting the modelâ€™s vocabulary to the linguistic style of student reflections; 2) Context Awareness, generating sentence embeddings that are aware of the full essay context; and 3) Class Overlap Awareness, employing a multi-label strategy to recognize the coexistence of themes in a single sentence. Our results show that by making the model explicitly aware of the properties of the input, AWARE outperforms a strong baseline by 2.1 percentage points in Macro-F1 and shows considerable improvements across all themes. This work provides a robust and generalizable methodology for any text classification task in which meaning depends on the context of the narrative. </p>
<blockquote>
<p>åœ¨å­¦ç”Ÿåæ€ä¸­è¯†åˆ«æ–‡åŒ–èµ„æœ¬ï¼ˆCCï¼‰ä¸»é¢˜ï¼Œå¯ä»¥æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œæœ‰åŠ©äºåŸ¹å…»è¯¾å ‚å…¬å¹³å­¦ä¹ ç¯å¢ƒã€‚ç„¶è€Œï¼Œè¯¸å¦‚å¿—å‘ç›®æ ‡æˆ–å®¶åº­æ”¯æŒç­‰ä¸»é¢˜é€šå¸¸è¢«ç¼–ç»‡æˆå™äº‹ï¼Œè€Œéä½œä¸ºç›´æ¥å…³é”®è¯å‡ºç°ã€‚è¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥è¢«å­¤ç«‹çš„å¥å­å¤„ç†çš„æ ‡å‡†NLPæ¨¡å‹æ£€æµ‹å‡ºæ¥ã€‚æ ¸å¿ƒæŒ‘æˆ˜æºäºç¼ºä¹æ„è¯†ï¼Œå› ä¸ºæ ‡å‡†æ¨¡å‹æ˜¯åœ¨ä¸€èˆ¬è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œè¿™ä½¿å¾—å®ƒä»¬å¯¹æ•°æ®å›ºæœ‰çš„ç‰¹å®šé¢†åŸŸè¯­è¨€å’Œå™äº‹ä¸Šä¸‹æ–‡è§†è€Œä¸è§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AWAREæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°å°è¯•æé«˜è½¬æ¢å™¨æ¨¡å‹å¯¹æ­¤ç±»ç»†å¾®ä»»åŠ¡çš„æ„è¯†ã€‚AWAREæœ‰ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1ï¼‰é¢†åŸŸæ„è¯†ï¼Œä½¿æ¨¡å‹çš„è¯æ±‡é€‚åº”å­¦ç”Ÿåæ€çš„è¯­è¨€é£æ ¼ï¼›2ï¼‰ä¸Šä¸‹æ–‡æ„è¯†ï¼Œç”Ÿæˆå¥å­åµŒå…¥ï¼Œå¯¹å…¨æ–‡ä¸Šä¸‹æ–‡æœ‰æ‰€äº†è§£ï¼›3ï¼‰ç±»åˆ«é‡å æ„è¯†ï¼Œé‡‡ç”¨å¤šæ ‡ç­¾ç­–ç•¥æ¥è¯†åˆ«å•ä¸ªå¥å­ä¸­å…±å­˜çš„ä¸»é¢˜ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ä½¿æ¨¡å‹æ˜ç¡®æ„è¯†åˆ°è¾“å…¥çš„ç‰¹æ€§ï¼ŒAWAREåœ¨å®è§‚F1åˆ†æ•°ä¸Šè¶…è¶Šäº†å¼ºå¤§åŸºçº¿2.1ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶åœ¨æ‰€æœ‰ä¸»é¢˜ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚è¿™é¡¹å·¥ä½œä¸ºä»»ä½•ä¾èµ–äºå™äº‹ä¸Šä¸‹æ–‡çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.04983v3">PDF</a> The authors are withdrawing this version to correct issues identified   in the experimental design and analysis. A revised and validated version will   be submitted after further review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å­¦ç”Ÿçš„åæ€ä¸­è¯†åˆ«æ–‡åŒ–èµ„æœ¬ä¸»é¢˜çš„é‡è¦æ€§ï¼Œè¿™æœ‰åŠ©äºä¿ƒè¿›å…¬å¹³çš„æ•™å®¤å­¦ä¹ ç¯å¢ƒã€‚ç„¶è€Œï¼Œä¸»é¢˜å¦‚å¿—å‘ç›®æ ‡æˆ–å®¶åº­æ”¯æŒç»å¸¸èå…¥å™äº‹ä¹‹ä¸­ï¼Œè€Œéä»¥ç›´æ¥å…³é”®è¯çš„å½¢å¼å‡ºç°ï¼Œä½¿å¾—æ ‡å‡†çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹éš¾ä»¥æ£€æµ‹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†AWAREæ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æé«˜æ¨¡å‹å¯¹æ­¤ç±»ä»»åŠ¡çš„æ„ŸçŸ¥èƒ½åŠ›ï¼šé¢†åŸŸæ„ŸçŸ¥ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œç±»åˆ«é‡å æ„ŸçŸ¥ã€‚ç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡ä½¿æ¨¡å‹æ˜ç¡®æ„ŸçŸ¥è¾“å…¥çš„ç‰¹æ€§ï¼ŒAWAREåœ¨å®è§‚F1å¾—åˆ†ä¸Šè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨æ‰€æœ‰ä¸»é¢˜ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚è¿™ä¸ºä»»ä½•ä¾èµ–äºå™äº‹ä¸Šä¸‹æ–‡çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æ–¹æ³•è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯†åˆ«å­¦ç”Ÿåæ€ä¸­çš„æ–‡åŒ–èµ„æœ¬ä¸»é¢˜æœ‰åŠ©äºä¿ƒè¿›å…¬å¹³çš„å­¦ä¹ ç¯å¢ƒã€‚</li>
<li>ä¸»é¢˜å¸¸å¸¸èå…¥å™äº‹ä¸­ï¼Œè€Œéä»¥ç›´æ¥å…³é”®è¯å½¢å¼å‡ºç°ï¼Œä½¿å¾—æ ‡å‡†NLPæ¨¡å‹éš¾ä»¥æ£€æµ‹ã€‚</li>
<li>AWAREæ¡†æ¶é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶æé«˜æ¨¡å‹å¯¹ä»»åŠ¡çš„æ„ŸçŸ¥èƒ½åŠ›ï¼šé¢†åŸŸæ„ŸçŸ¥ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œç±»åˆ«é‡å æ„ŸçŸ¥ã€‚</li>
<li>AWAREæ¡†æ¶é€šè¿‡ä½¿æ¨¡å‹æ˜ç¡®æ„ŸçŸ¥è¾“å…¥çš„ç‰¹æ€§ï¼Œåœ¨å®è§‚F1å¾—åˆ†ä¸Šè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚</li>
<li>AWAREæ¡†æ¶åœ¨æ‰€æœ‰ä¸»é¢˜ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>æ­¤æ–¹æ³•æä¾›äº†ä¸€ç§è§£å†³ä¾èµ–äºå™äº‹ä¸Šä¸‹æ–‡çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.04983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-729eebafb616561f0762e3cfe2f6b91a" align="middle">
<img src="https://picx.zhimg.com/v2-7858f8afd2033d970909f41d3f0d2ca1" align="middle">
<img src="https://picx.zhimg.com/v2-302d3f041026412a9fcb80960ce42fec" align="middle">
<img src="https://picx.zhimg.com/v2-4d4e126271fb53402545a3603211f988" align="middle">
<img src="https://picx.zhimg.com/v2-659bb768de122e4210eb2d43acbcf2c1" align="middle">
<img src="https://picx.zhimg.com/v2-9d075576ac5224fac7e934e32b7299a7" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FraudTransformer-Time-Aware-GPT-for-Transaction-Fraud-Detection"><a href="#FraudTransformer-Time-Aware-GPT-for-Transaction-Fraud-Detection" class="headerlink" title="FraudTransformer: Time-Aware GPT for Transaction Fraud Detection"></a>FraudTransformer: Time-Aware GPT for Transaction Fraud Detection</h2><p><strong>Authors:Gholamali Aminian, Andrew Elliott, Tiger Li, Timothy Cheuk Hin Wong, Victor Claude Dehon, Lukasz Szpruch, Carsten Maple, Christopher Read, Martin Brown, Gesine Reinert, Mo Mamouei</strong></p>
<p>Detecting payment fraud in real-world banking streams requires models that can exploit both the order of events and the irregular time gaps between them. We introduce FraudTransformer, a sequence model that augments a vanilla GPT-style architecture with (i) a dedicated time encoder that embeds either absolute timestamps or inter-event values, and (ii) a learned positional encoder that preserves relative order. Experiments on a large industrial dataset â€“ tens of millions of transactions and auxiliary events â€“ show that FraudTransformer surpasses four strong classical baselines (Logistic Regression, XGBoost and LightGBM) as well as transformer ablations that omit either the time or positional component. On the held-out test set it delivers the highest AUROC and PRAUC. </p>
<blockquote>
<p>åœ¨çœŸå®ä¸–ç•Œçš„é“¶è¡Œæµæ°´äº¤æ˜“ä¸­æ£€æµ‹æ”¯ä»˜æ¬ºè¯ˆï¼Œéœ€è¦èƒ½å¤Ÿåˆ©ç”¨äº‹ä»¶é¡ºåºå’Œäº‹ä»¶ä¹‹é—´ä¸è§„åˆ™æ—¶é—´é—´éš”çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†FraudTransformerï¼Œè¿™æ˜¯ä¸€ä¸ªåºåˆ—æ¨¡å‹ï¼Œå®ƒå¢åŠ äº†åŸºæœ¬çš„GPTé£æ ¼æ¶æ„ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼ˆiï¼‰ä¸“ç”¨æ—¶é—´ç¼–ç å™¨ï¼Œå¯ä»¥åµŒå…¥ç»å¯¹æ—¶é—´æˆ³æˆ–äº‹ä»¶é—´å€¼ï¼Œï¼ˆiiï¼‰å­¦ä¹ ä½ç½®ç¼–ç å™¨ï¼Œå¯ä»¥ä¿ç•™ç›¸å¯¹é¡ºåºã€‚åœ¨å¤§å‹å·¥ä¸šæ•°æ®é›†ä¸Šçš„å®éªŒâ€”â€”æ•°åƒä¸‡ç¬”äº¤æ˜“å’Œè¾…åŠ©äº‹ä»¶â€”â€”è¡¨æ˜ï¼ŒFraudTransformerè¶…è¶Šäº†å››ä¸ªå¼ºå¤§çš„ç»å…¸åŸºçº¿ï¼ˆLogisticå›å½’ã€XGBoostå’ŒLightGBMï¼‰ï¼Œä»¥åŠçœç•¥äº†æ—¶é—´æˆ–ä½ç½®ç»„ä»¶çš„å˜å‹å™¨æ¶ˆèæ¨¡å‹ã€‚åœ¨ä¿ç•™çš„æµ‹è¯•é›†ä¸Šï¼Œå®ƒè¾¾åˆ°äº†æœ€é«˜çš„AUROCå’ŒPRAUCã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23712v2">PDF</a> Accepted in AI-FIND ICAIFâ€™25   (<a target="_blank" rel="noopener" href="https://sites.google.com/view/icaif-fraud-detection-workshop/home">https://sites.google.com/view/icaif-fraud-detection-workshop/home</a>)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æå‡ºäº†ä¸€ç§åä¸ºFraudTransformerçš„åºåˆ—æ¨¡å‹ï¼Œç”¨äºçœŸå®é“¶è¡Œæµä¸­æ”¯ä»˜æ¬ºè¯ˆçš„æ£€æµ‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤ä¸ªç»„ä»¶å¢å¼ºäº†åŸºæœ¬çš„GPTé£æ ¼æ¶æ„ï¼šä¸€æ˜¯ä¸“é—¨çš„æ—¶é—´ç¼–ç å™¨ï¼Œç”¨äºåµŒå…¥ç»å¯¹æ—¶é—´æˆ³æˆ–äº‹ä»¶é—´å€¼ï¼›äºŒæ˜¯å­¦ä¹ åˆ°çš„ä½ç½®ç¼–ç å™¨ï¼Œç”¨äºä¿ç•™ç›¸å¯¹é¡ºåºã€‚åœ¨å¤§å‹å·¥ä¸šæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFraudTransformerè¶…è¶Šäº†å››ç§å¼ºå¤§çš„ç»å…¸åŸºçº¿ä»¥åŠçœç•¥äº†æ—¶é—´æˆ–ä½ç½®ç»„ä»¶çš„è½¬æ¢å™¨ã€‚åœ¨ä¿ç•™çš„æµ‹è¯•é›†ä¸Šï¼Œå®ƒæä¾›äº†æœ€é«˜çš„AUROCå’ŒPRAUCã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>FraudTransformeræ˜¯ä¸€ä¸ªç”¨äºæ£€æµ‹çœŸå®é“¶è¡Œæµä¸­æ”¯ä»˜æ¬ºè¯ˆçš„åºåˆ—æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡åµŒå…¥æ—¶é—´ä¿¡æ¯å’Œäº‹ä»¶ç›¸å¯¹é¡ºåºæ¥å¢å¼ºæ€§èƒ½ã€‚</li>
<li>æ—¶é—´ç¼–ç å™¨å¯ä»¥å¤„ç†ç»å¯¹æ—¶é—´æˆ³æˆ–äº‹ä»¶é—´å€¼ã€‚</li>
<li>ä½ç½®ç¼–ç å™¨èƒ½å¤Ÿä¿ç•™äº‹ä»¶çš„ç›¸å¯¹é¡ºåºã€‚</li>
<li>åœ¨å¤§å‹å·¥ä¸šæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFraudTransformeræ€§èƒ½è¶…è¶Šäº†å¤šç§ç»å…¸æ¨¡å‹å’Œçœç•¥äº†æ—¶é—´æˆ–ä½ç½®ç»„ä»¶çš„è½¬æ¢å™¨ã€‚</li>
<li>FraudTransformeråœ¨æµ‹è¯•é›†ä¸Šæä¾›äº†æœ€é«˜çš„AUROCå’ŒPRAUCã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0387b5961bc12ac605a5564fee750e83" align="middle">
<img src="https://picx.zhimg.com/v2-f04072eddbe3c1e6e95f11d7c9614e60" align="middle">
<img src="https://picx.zhimg.com/v2-c8854de78d10a730291fb81d263590ea" align="middle">
<img src="https://picx.zhimg.com/v2-c3ff37e408e9154a7cbbf75c16a18e6a" align="middle">
<img src="https://picx.zhimg.com/v2-19e9bedfddd620ad27f3f915e2919e93" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Teaching-According-to-Talents-Instruction-Tuning-LLMs-with-Competence-Aware-Curriculum-Learning"><a href="#Teaching-According-to-Talents-Instruction-Tuning-LLMs-with-Competence-Aware-Curriculum-Learning" class="headerlink" title="Teaching According to Talents! Instruction Tuning LLMs with   Competence-Aware Curriculum Learning"></a>Teaching According to Talents! Instruction Tuning LLMs with   Competence-Aware Curriculum Learning</h2><p><strong>Authors:Yangning Li, Tingwei Lu, Yinghui Li, Yankai Chen, Wei-Chieh Huang, Wenhao Jiang, Hui Wang, Hai-Tao Zheng, Philip S. Yu</strong></p>
<p>Efficient instruction tuning aims to enhance the ultimate performance of large language models (LLMs) trained on a given instruction dataset. Curriculum learning as a typical data organization strategy has shown preliminary effectiveness in instruction tuning. However, current curriculum tuning methods suffer from the curriculum rigidity, since they rely solely on static heuristic difficulty metrics. These methods fail to adapt to the evolving capabilities of models during training, resulting in a fixed and potentially sub-optimal learning trajectory. To address the issue, Competence-Aware Multi-Perspective cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS offers several advantages: (1) Dynamic selection for sub-curriculum. (2) Competency-aware adjustment to the curriculum schedule. (3) Multiple difficulty-based scheduling. Extensive experiments prove the superior performance of CAMPUS, compared to other state-of-the-art baselines for efficient instruction tuning. </p>
<blockquote>
<p>é«˜æ•ˆæŒ‡ä»¤è°ƒæ•´æ—¨åœ¨æé«˜åœ¨ç»™å®šæŒ‡ä»¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€ç»ˆæ€§èƒ½ã€‚ä½œä¸ºå…¸å‹çš„æ•°æ®ç»„ç»‡ç­–ç•¥ï¼Œè¯¾ç¨‹å­¦ä¹ åœ¨æŒ‡ä»¤è°ƒæ•´ä¸­å·²æ˜¾ç¤ºå‡ºåˆæ­¥çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯¾ç¨‹è°ƒæ•´æ–¹æ³•å—åˆ°è¯¾ç¨‹åˆšæ€§çš„å›°æ‰°ï¼Œå› ä¸ºå®ƒä»¬å®Œå…¨ä¾èµ–äºé™æ€çš„å¯å‘å¼éš¾åº¦æŒ‡æ ‡ã€‚è¿™äº›æ–¹æ³•æœªèƒ½é€‚åº”æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„èƒ½åŠ›æ¼”å˜ï¼Œå¯¼è‡´å›ºå®šä¸”å¯èƒ½æ¬¡ä¼˜çš„å­¦ä¹ è½¨è¿¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åä¸ºCAMPUSçš„èƒ½åŠ›æ„ŸçŸ¥å¤šè§†è§’è¯¾ç¨‹æŒ‡ä»¤è°ƒæ•´æ¡†æ¶ã€‚CAMPUSæä¾›å‡ ä¸ªä¼˜ç‚¹ï¼šï¼ˆ1ï¼‰å­è¯¾ç¨‹çš„åŠ¨æ€é€‰æ‹©ã€‚ï¼ˆ2ï¼‰å¯¹è¯¾ç¨‹è¡¨çš„èƒ½åŠ›æ„ŸçŸ¥è°ƒæ•´ã€‚ï¼ˆ3ï¼‰åŸºäºéš¾åº¦çš„å¤šç§è°ƒåº¦ã€‚å¤§é‡å®éªŒè¯æ˜äº†CAMPUSåœ¨é«˜æ•ˆæŒ‡ä»¤è°ƒæ•´æ–¹é¢ä¼˜äºå…¶ä»–æœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13790v2">PDF</a> EMNLP 2025 Findings</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œæ•ˆç‡å¯¼å‘çš„æŒ‡ä»¤è°ƒä¼˜æ—¨åœ¨æå‡æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½è¡¨ç°ã€‚è¯¾ç¨‹å­¦ä¹ ä½œä¸ºä¸€ç§å…¸å‹çš„æ•°æ®ç»„ç»‡ç­–ç•¥ï¼Œåœ¨æŒ‡ä»¤è°ƒä¼˜ä¸­åˆæ­¥æ˜¾ç°å‡ºå…¶æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå½“å‰è¯¾ç¨‹å¼è°ƒä¼˜æ–¹æ³•å­˜åœ¨è¯¾ç¨‹åˆšæ€§ï¼Œå³ä¾èµ–äºé™æ€å¯å‘å¼éš¾åº¦åº¦é‡æŒ‡æ ‡çš„é—®é¢˜ã€‚è¿™äº›æ–¹æ³•æ— æ³•é€‚åº”æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„èƒ½åŠ›å˜åŒ–ï¼Œå¯¼è‡´å›ºå®šçš„å­¦ä¹ è½¨è¿¹å¯èƒ½ä¸å°½äººæ„ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŸºäºèƒ½åŠ›çš„å¤šè§’åº¦è¯¾ç¨‹æŒ‡ä»¤è°ƒä¼˜æ¡†æ¶â€”â€”CAMPUSã€‚CAMPUSå…·æœ‰å‡ å¤§ä¼˜åŠ¿ï¼šåŠ¨æ€é€‰æ‹©å­è¯¾ç¨‹ï¼›åŸºäºèƒ½åŠ›çš„è¯¾ç¨‹å®‰æ’è°ƒæ•´ï¼›å¤šç§éš¾åº¦å¯¼å‘çš„è°ƒåº¦ç­–ç•¥ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼ŒCAMPUSåœ¨é«˜æ•ˆæŒ‡ä»¤è°ƒä¼˜æ–¹é¢è¡¨ç°å“è¶Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CAMPUSåŠ¨æ€é€‰æ‹©å­è¯¾ç¨‹çš„èƒ½åŠ›æ˜¯å…¶ä¸€å¤§ä¼˜åŠ¿ï¼Œèƒ½æ ¹æ®ä¸åŒçš„è®­ç»ƒé˜¶æ®µé€‰æ‹©åˆé€‚çš„å­é›†è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>CAMPUSå…·å¤‡åŸºäºæ¨¡å‹èƒ½åŠ›çš„è¯¾ç¨‹å®‰æ’è°ƒæ•´åŠŸèƒ½ï¼Œä½¿å¾—è¯¾ç¨‹å­¦ä¹ æ›´åŠ çµæ´»å’Œé«˜æ•ˆã€‚</li>
<li>CAMPUSé‡‡ç”¨å¤šç§éš¾åº¦å¯¼å‘çš„è°ƒåº¦ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒéš¾åº¦ä»»åŠ¡ä¸Šçš„å‡è¡¡å­¦ä¹ ã€‚</li>
<li>CAMPUSè§£å†³äº†ç°æœ‰è¯¾ç¨‹å¼è°ƒä¼˜æ–¹æ³•çš„è¯¾ç¨‹åˆšæ€§é—®é¢˜ï¼Œæå‡äº†æ¨¡å‹è®­ç»ƒçš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>CAMPUSæ¡†æ¶åœ¨å¹¿æ³›å®éªŒä¸­è¢«è¯æ˜èƒ½æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒCAMPUSåœ¨é«˜æ•ˆæŒ‡ä»¤è°ƒä¼˜æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13790">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed0825a9133befdbd5c62e7436366340" align="middle">
<img src="https://picx.zhimg.com/v2-d94cf671741410628f57ac50c7afaa71" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TinyTim-A-Family-of-Language-Models-for-Divergent-Generation"><a href="#TinyTim-A-Family-of-Language-Models-for-Divergent-Generation" class="headerlink" title="TinyTim: A Family of Language Models for Divergent Generation"></a>TinyTim: A Family of Language Models for Divergent Generation</h2><p><strong>Authors:Christopher J. Agostino</strong></p>
<p>In the search for artificial general intelligence, model development and training has focused primarily on vast datasets of known problems and their accepted solutions. This process necessarily produces convergent systems which are fundamentally incapable of the conceptual reframing that is required for genuine creative breakthroughs. Inspired by the divergent cognitive processes that allow humans to make such creative leaps, our work introduces a family of language models, TinyTim, to serve as sources of divergent generation within broader systems. These models have been created by fine-tuning on the anti-parsimonious text of James Joyceâ€™s &#96;Finnegans Wakeâ€™. Quantitative analysis of both an unsupervised fine-tuned model (TinyTim-V1) and a new instruction-tuned variant (TinyTim-V2) demonstrates a profound capacity for lexical invention; the foundational V1 model exhibits a Yuleâ€™s K score for lexical richness over twenty times greater than that of convergent baselines. This trait is a stable property of the family, as the instruction-tuned V2 maintains a statistically distinct profile and resists factual convergence, sacrificing benchmark performance to preserve its core generative style. This work establishes a methodology for engineering specialized divergent models that, when paired with convergent systems, can reframe problems and force breakthroughs beyond the reach of statistical optimization alone. </p>
<blockquote>
<p>åœ¨å¯»æ‰¾äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½çš„è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¼€å‘å’Œè®­ç»ƒä¸»è¦é›†ä¸­åœ¨å·²çŸ¥é—®é¢˜çš„å¤§é‡æ•°æ®é›†åŠå…¶æ¥å—çš„è§£å†³æ–¹æ¡ˆä¸Šã€‚è¿™ä¸€è¿‡ç¨‹å¿…ç„¶ä¼šäº§ç”Ÿæ”¶æ•›ç³»ç»Ÿï¼Œè€Œè¿™äº›ç³»ç»Ÿä»æ ¹æœ¬ä¸Šæ— æ³•å®Œæˆæ¦‚å¿µé‡æ„ï¼Œæ— æ³•è¿›è¡ŒçœŸæ­£çš„åˆ›é€ æ€§çªç ´ã€‚æˆ‘ä»¬çš„å·¥ä½œå—åˆ°äººç±»èƒ½å¤Ÿå®ç°å¦‚æ­¤è·³è·ƒçš„å‘æ•£è®¤çŸ¥è¿‡ç¨‹çš„å¯å‘ï¼Œå¼•å…¥äº†ä¸€ç³»åˆ—è¯­è¨€æ¨¡å‹TinyTimï¼Œä½œä¸ºæ›´å¹¿æ³›ç³»ç»Ÿä¸­å‘æ•£ç”Ÿæˆçš„æ¥æºã€‚è¿™äº›æ¨¡å‹é€šè¿‡å¾®è°ƒè©¹å§†æ–¯Â·ä¹”ä¼Šæ–¯ï¼ˆJames Joyceï¼‰çš„ã€ŠèŠ¬å°¼æ ¹å®ˆçµå¤œã€‹ï¼ˆFinnegans Wakeï¼‰ä¸­çš„åç®€çº¦æ–‡æœ¬è€Œåˆ›å»ºã€‚å¯¹æ— ç›‘ç£å¾®è°ƒæ¨¡å‹ï¼ˆTinyTim-V1ï¼‰å’Œæ–°æŒ‡ä»¤è°ƒæ•´å˜ä½“ï¼ˆTinyTim-V2ï¼‰çš„å®šé‡åˆ†æè¡¨æ˜ï¼Œå®ƒä»¬åœ¨è¯æ±‡å‘æ˜æ–¹é¢å…·æœ‰æ·±åˆ»çš„èƒ½åŠ›ï¼›åŸºç¡€V1æ¨¡å‹çš„è¯æ±‡ä¸°å¯Œåº¦çš„å°¤å°”Kå€¼ï¼ˆYuleâ€™s K scoreï¼‰æ˜¯æ”¶æ•›åŸºå‡†å€¼çš„äºŒåå€ä»¥ä¸Šã€‚è¿™ä¸€ç‰¹è´¨æ˜¯è¿™ä¸ªå®¶æ—çš„ä¸€ä¸ªç¨³å®šç‰¹å¾ï¼Œå› ä¸ºæŒ‡ä»¤è°ƒæ•´åçš„V2ä¿æŒäº†ä¸€ä¸ªç»Ÿè®¡ä¸Šç‹¬ç‰¹çš„ç‰¹å¾ï¼ŒæŠµåˆ¶äº‹å®æ”¶æ•›ï¼Œç‰ºç‰²äº†åŸºå‡†æ€§èƒ½ä»¥ä¿ç•™å…¶æ ¸å¿ƒç”Ÿæˆé£æ ¼ã€‚è¿™é¡¹å·¥ä½œå»ºç«‹äº†ä¸€ç§å·¥ç¨‹åŒ–ä¸“ä¸šå‘æ•£æ¨¡å‹çš„æ–¹æ³•è®ºï¼Œå½“ä¸æ”¶æ•›ç³»ç»Ÿé…å¯¹æ—¶ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥é‡æ„é—®é¢˜å¹¶è¿«ä½¿çªç ´ç»Ÿè®¡ä¼˜åŒ–èƒ½åŠ›ä»¥å¤–çš„è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11607v2">PDF</a> 7 pages, 3 figures, accepted to NeurIPS Creative AI track, models   available at <a target="_blank" rel="noopener" href="https://hf.co/npc-worldwide/">https://hf.co/npc-worldwide/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¯»æ‰¾äººå·¥é€šç”¨æ™ºèƒ½çš„è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¼€å‘å’Œè®­ç»ƒä¸»è¦ä¾èµ–äºå¤§é‡å·²çŸ¥é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆçš„æ•°æ®é›†ï¼Œè¿™å¯¼è‡´äº†ç³»ç»Ÿæ”¶æ•›æ€§çš„å±€é™æ€§ï¼Œæ— æ³•å®ç°äººç±»çœŸæ­£çš„åˆ›é€ æ€§çªç ´ã€‚ä¸ºäº†çªç ´è¿™ä¸€å±€é™æ€§ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åä¸ºTinyTimçš„è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œå®ƒä»¬é€šè¿‡åœ¨è©¹å§†æ–¯Â·ä¹”ä¼Šæ–¯çš„å°è¯´ã€ŠèŠ¬å°¼æ ¹å®ˆå¤œäººã€‹çš„åå¸¸ç†æ–‡æœ¬ä¸Šè¿›è¡Œå¾®è°ƒè®­ç»ƒæ¥ç”Ÿæˆå‘æ•£æ€§æ€ç»´ã€‚å¯¹TinyTim-V1å’Œæ–°çš„æŒ‡ä»¤è°ƒæ•´å‹å˜ç§TinyTim-V2çš„å®šé‡åˆ†æè¡¨æ˜ï¼Œå®ƒä»¬å…·æœ‰æ˜¾è‘—çš„è¯æ±‡å‘æ˜èƒ½åŠ›ï¼›åŸºç¡€æ¨¡å‹V1çš„å°¤å°”Kå€¼å¾—åˆ†åœ¨è¯æ±‡ä¸°å¯Œåº¦æ–¹é¢æ¯”æ”¶æ•›åŸºçº¿é«˜å‡ºäºŒåå€ä»¥ä¸Šã€‚è¿™ç§ç‰¹è´¨æ˜¯è¿™ä¸€ç³»åˆ—æ¨¡å‹çš„ç¨³å®šå±æ€§ï¼ŒæŒ‡ä»¤è°ƒæ•´å‹V2ä¿æŒäº†ç‹¬ç‰¹çš„ç»Ÿè®¡ç‰¹å¾ï¼Œé¿å…äº‹å®æ”¶æ•›ï¼Œå°½ç®¡ç‰ºç‰²äº†ä¸€äº›åŸºå‡†æ€§èƒ½ä½†ä¿ç•™äº†å…¶æ ¸å¿ƒç”Ÿæˆé£æ ¼ã€‚æœ¬ç ”ç©¶ä¸ºå·¥ç¨‹åŒ–ä¸“ä¸šåŒ–çš„å‘æ•£æ¨¡å‹å»ºç«‹äº†æ–¹æ³•ï¼Œä¸æ”¶æ•›ç³»ç»Ÿé…å¯¹æ—¶ï¼Œèƒ½å¤Ÿé‡æ–°å®šä½é—®é¢˜å¹¶å®ç°å•ä¸€ç»Ÿè®¡ä¼˜åŒ–æ— æ³•è¾¾åˆ°çš„çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹å‘å±•å’Œè®­ç»ƒä¸»è¦ä¾èµ–äºå¤§é‡å·²çŸ¥é—®é¢˜æ•°æ®é›†ï¼Œå¯¼è‡´ç³»ç»Ÿæ”¶æ•›æ€§å±€é™ã€‚</li>
<li>TinyTimè¯­è¨€æ¨¡å‹å®¶æ—é€šè¿‡å‘æ•£æ€§æ€ç»´ç”Ÿæˆï¼Œæ—¨åœ¨çªç ´è¿™ä¸€å±€é™æ€§ã€‚</li>
<li>TinyTim-V1å’ŒTinyTim-V2å…·æœ‰æ˜¾è‘—çš„è¯æ±‡å‘æ˜èƒ½åŠ›ï¼Œå°¤å°”Kå€¼å¾—åˆ†è¿œé«˜äºæ”¶æ•›åŸºçº¿ã€‚</li>
<li>TinyTimç³»åˆ—æ¨¡å‹åœ¨ä¿æŒæ ¸å¿ƒç”Ÿæˆé£æ ¼çš„åŒæ—¶ï¼Œèƒ½å¤ŸæŠµæŠ—äº‹å®æ”¶æ•›ã€‚</li>
<li>è¿™äº›æ¨¡å‹é€šè¿‡ç‰ºç‰²éƒ¨åˆ†åŸºå‡†æ€§èƒ½æ¥å®ç°å‘æ•£ç‰¹æ€§ã€‚</li>
<li>ç»“åˆæ”¶æ•›ç³»ç»Ÿå’Œå‘æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿé‡æ–°å®šä½é—®é¢˜å¹¶å®ç°è¶…è¶Šå•ä¸€ç»Ÿè®¡ä¼˜åŒ–çš„çªç ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11607">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9524f19efee2f25b4910b1fc55cebe8a" align="middle">
<img src="https://picx.zhimg.com/v2-6e87c34666594338600a960714e3fdf4" align="middle">
<img src="https://picx.zhimg.com/v2-7b2fece82c8b0a799c4af337bf5051aa" align="middle">
<img src="https://picx.zhimg.com/v2-d4b3b68224c96125454e0d6a87163dec" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-194c4f64f1884c14982d69753cb2f1ba" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  1 PoCo Agentic Proof-of-Concept Exploit Generation for Smart Contracts
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2f776ed60e692f0ffd1e478593546123" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Agent-Omni Test-Time Multimodal Reasoning via Model Coordination for   Understanding Anything
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
