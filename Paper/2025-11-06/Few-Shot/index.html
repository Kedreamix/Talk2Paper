<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Assessing the value of Geo-Foundational Models for Flood Inundation   Mapping Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for   end-users">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c8d906f4c4843b4192a114eb0ab7e4f7')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    73 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-06-æ›´æ–°"><a href="#2025-11-06-æ›´æ–°" class="headerlink" title="2025-11-06 æ›´æ–°"></a>2025-11-06 æ›´æ–°</h1><h2 id="Assessing-the-value-of-Geo-Foundational-Models-for-Flood-Inundation-Mapping-Benchmarking-models-for-Sentinel-1-Sentinel-2-and-Planetscope-for-end-users"><a href="#Assessing-the-value-of-Geo-Foundational-Models-for-Flood-Inundation-Mapping-Benchmarking-models-for-Sentinel-1-Sentinel-2-and-Planetscope-for-end-users" class="headerlink" title="Assessing the value of Geo-Foundational Models for Flood Inundation   Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for   end-users"></a>Assessing the value of Geo-Foundational Models for Flood Inundation   Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for   end-users</h2><p><strong>Authors:Saurabh Kaushik, Lalit Maurya, Elizabeth Tellman, ZhiJie Zhang</strong></p>
<p>Geo-Foundational Models (GFMs) enable fast and reliable extraction of spatiotemporal information from satellite imagery, improving flood inundation mapping by leveraging location and time embeddings. Despite their potential, it remains unclear whether GFMs outperform traditional models like U-Net. A systematic comparison across sensors and data availability scenarios is still lacking, which is an essential step to guide end-users in model selection. To address this, we evaluate three GFMs, Prithvi 2.0, Clay V1.5, DOFA, and UViT (a Prithvi variant), against TransNorm, U-Net, and Attention U-Net using PlanetScope, Sentinel-1, and Sentinel-2. We observe competitive performance among all GFMs, with only 2-5% variation between the best and worst models across sensors. Clay outperforms others on PlanetScope (0.79 mIoU) and Sentinel-2 (0.70), while Prithvi leads on Sentinel-1 (0.57). In leave-one-region-out cross-validation across five regions, Clay shows slightly better performance across all sensors (mIoU: 0.72(0.04), 0.66(0.07), 0.51(0.08)) compared to Prithvi (0.70(0.05), 0.64(0.09), 0.49(0.13)) and DOFA (0.67(0.07), 0.64(0.04), 0.49(0.09)) for PlanetScope, Sentinel-2, and Sentinel-1, respectively. Across all 19 sites, leave-one-region-out cross-validation reveals a 4% improvement by Clay compared to U-Net. Visual inspection highlights Clayâ€™s superior ability to retain fine details. Few-shot experiments show Clay achieves 0.64 mIoU on PlanetScope with just five training images, outperforming Prithvi (0.24) and DOFA (0.35). In terms of computational time, Clay is a better choice due to its smaller model size (26M parameters), making it ~3x faster than Prithvi (650M) and 2x faster than DOFA (410M). Contrary to previous findings, our results suggest GFMs offer small to moderate improvements in flood mapping accuracy at lower computational cost and labeling effort compared to traditional U-Net. </p>
<blockquote>
<p>åœ°ç†åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰èƒ½å¤Ÿä»å«æ˜Ÿå›¾åƒä¸­å¿«é€Ÿå¯é åœ°æå–æ—¶ç©ºä¿¡æ¯ï¼Œåˆ©ç”¨ä½ç½®å’Œæ—¶é—´åµŒå…¥æŠ€æœ¯æ”¹è¿›æ´ªæ°´æ·¹æ²¡åœ°å›¾ç»˜åˆ¶ã€‚å°½ç®¡å…¶æ½œåŠ›å·¨å¤§ï¼Œä½†å°šä¸æ¸…æ¥šGFMsæ˜¯å¦ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼ˆå¦‚U-Netï¼‰ã€‚è·¨ä¼ æ„Ÿå™¨å’Œæ•°æ®å¯ç”¨æ€§çš„ç³»ç»Ÿæ¯”è¾ƒä»æ˜¯ä¸€ä¸ªç¼ºå¤±çš„ç¯èŠ‚ï¼Œè¿™æ˜¯å¼•å¯¼æœ€ç»ˆç”¨æˆ·è¿›è¡Œæ¨¡å‹é€‰æ‹©çš„å…³é”®æ­¥éª¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§GFMsï¼ˆPrithvi 2.0ã€Clay V1.5ã€DOFAå’ŒUViTï¼ˆPrithviçš„ä¸€ä¸ªå˜ä½“ï¼‰ï¼‰ä¸TransNormã€U-Netå’ŒAttention U-Netåœ¨PlanetScopeã€Sentinel-1å’ŒSentinel-2ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°æ‰€æœ‰GFMséƒ½è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä¼ æ„Ÿå™¨ä¹‹é—´æœ€ä½³å’Œæœ€å·®æ¨¡å‹ä¹‹é—´çš„å·®å¼‚åªæœ‰2-5%ã€‚Clayåœ¨PlanetScopeï¼ˆ0.79 mIoUï¼‰å’ŒSentinel-2ï¼ˆ0.70ï¼‰ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè€ŒPrithviåœ¨Sentinel-1ä¸Šè¡¨ç°æœ€ä½³ï¼ˆ0.57ï¼‰ã€‚åœ¨äº”åœ°åŒºçš„ç•™ä¸€åœ°åŒºå¤–å‡ºäº¤å‰éªŒè¯ä¸­ï¼ŒClayåœ¨æ‰€æœ‰ä¼ æ„Ÿå™¨ä¸Šçš„è¡¨ç°ç•¥å¥½äºPrithviå’ŒDOFAï¼ˆå¯¹äºPlanetScopeã€Sentinel-2å’ŒSentinel-1çš„mIoUåˆ†åˆ«ä¸ºï¼š0.72ï¼ˆ0.04ï¼‰ã€0.66ï¼ˆ0.07ï¼‰ã€0.51ï¼ˆ0.08ï¼‰ã€ç›¸è¾ƒäºPrithviçš„0.70ï¼ˆ0.05ï¼‰ã€0.64ï¼ˆ0.09ï¼‰ã€ä»¥åŠDOFAçš„0.67ï¼ˆ0.07ï¼‰ã€åœ¨æ‰€æœ‰19ä¸ªåœ°ç‚¹ä¸­è¿›è¡Œçš„äº¤å‰éªŒè¯è¡¨æ˜ï¼Œä¸U-Netç›¸æ¯”ï¼ŒClayæ”¹è¿›äº†çº¦ç™¾åˆ†ä¹‹å››çš„è§†è§‰æ£€æŸ¥ã€‚çªæ˜¾Clayä¿ç•™ç»†èŠ‚çš„èƒ½åŠ›æ›´èƒœä¸€ç­¹ã€‚å°‘æ•°æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œä»…åœ¨äº”ä¸ªè®­ç»ƒå›¾åƒçš„æƒ…å†µä¸‹ï¼ŒClayåœ¨PlanetScopeä¸Šå®ç°è¾¾åˆ°6IoUï¼‰ä¸ºåœ¨å«æ˜Ÿå½±åƒä¸­å®ç°æ´ªæ°´ç²¾ç»†åŒ–æå–ç­‰å¤æ‚åœºæ™¯ä¸‹çš„æœºå™¨å­¦ä¹ æä¾›äº†æ›´å¤šçš„å¯èƒ½æ€§å’Œå¯è¡Œæ€§åˆ†ææ–¹å‘ã€‚ä¸ä¼ ç»Ÿçš„U-Netç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜GFMsèƒ½å¤Ÿä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬å’Œæ ‡æ³¨å·¥ä½œé‡æä¾›å¾®å°è‡³ä¸­ç­‰çš„æ´ªæ°´æ˜ å°„ç²¾åº¦æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01990v1">PDF</a> </p>
<p><strong>Summary</strong>:</p>
<p>åŸºäºåœ°ç†åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰èƒ½å¤Ÿåˆ©ç”¨å«æ˜Ÿå›¾åƒçš„æ—¶é—´å’Œåœ°ç‚¹åµŒå…¥ä¿¡æ¯ï¼Œå®ç°æ´ªæ°´æ·¹æ²¡åœ°å›¾çš„å¿«é€Ÿå¯é æå–ã€‚é€šè¿‡ä¸ä¼ ç»Ÿæ¨¡å‹å¦‚U-Netçš„æ¯”è¾ƒï¼ŒGFMsåœ¨æ´ªæ°´æ˜ å°„å‡†ç¡®æ€§ä¸Šæä¾›å°å¹…è‡³ä¸­ç­‰çš„æ”¹è¿›ï¼Œå¹¶é™ä½äº†è®¡ç®—æˆæœ¬å’Œæ ‡æ³¨å·¥ä½œé‡ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>GFMsåœ¨æ´ªæ°´æ·¹æ²¡åœ°å›¾åˆ¶ä½œä¸­å…·æœ‰å¿«é€Ÿå¯é æå–æ—¶ç©ºä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>ç›®å‰å°šç¼ºä¹å…³äºGFMsä¸ä¼ ç»Ÿæ¨¡å‹å¦‚U-Netåœ¨å¤šç§ä¼ æ„Ÿå™¨å’Œæ•°æ®å¯ç”¨æƒ…å†µä¸‹çš„ç³»ç»Ÿæ¯”è¾ƒã€‚</li>
<li>åœ¨ä¸åŒä¼ æ„Ÿå™¨å’ŒåŒºåŸŸè¿›è¡Œçš„æµ‹è¯•ä¸­ï¼ŒClayåœ¨æ´ªæ°´æ˜ å°„æ–¹é¢è¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½ã€‚</li>
<li>Few-shotå®éªŒè¡¨æ˜ï¼ŒClayåœ¨ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒå›¾åƒçš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒè¾ƒé«˜çš„æ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–GFMså’ŒU-Netç›¸æ¯”ï¼ŒClayå…·æœ‰è¾ƒå°çš„æ¨¡å‹å°ºå¯¸å’Œæ›´å¿«çš„è®¡ç®—é€Ÿåº¦ã€‚</li>
<li>GFMsåœ¨æ´ªæ°´æ˜ å°„å‡†ç¡®æ€§ä¸Šæä¾›å°å¹…è‡³ä¸­ç­‰çš„æ”¹è¿›ï¼Œå¹¶é™ä½äº†è®¡ç®—æˆæœ¬å’Œæ ‡æ³¨å·¥ä½œé‡ã€‚</li>
<li>ç»“æœè¡¨æ˜ï¼ŒGFMsåœ¨æ´ªæ°´åœ°å›¾åˆ¶ä½œä¸­å…·æœ‰æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9422b7c9d8a85ab05d198f60e57208f7" align="middle">
<img src="https://picx.zhimg.com/v2-96a4ea3c22ab7db39408bdb9cc693639" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Bayesian-Natural-Gradient-Fine-Tuning-of-CLIP-Models-via-Kalman-Filtering"><a href="#Bayesian-Natural-Gradient-Fine-Tuning-of-CLIP-Models-via-Kalman-Filtering" class="headerlink" title="Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman   Filtering"></a>Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman   Filtering</h2><p><strong>Authors:Hossein Abdi, Mingfei Sun, Wei Pan</strong></p>
<p>Vision-language pre-trained models, such as CLIP, have established new benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a major challenge to achieve optimal performance on both in-distribution (ID) and out-of-distribution (OOD) datasets, especially when labeled data is scarce. Most existing fine-tuning approaches rely on first-order gradient-based optimizers, which typically suffer from slow convergence, sensitivity to step-size hyperparameters, and poor generalization in OOD settings. In contrast, second-order methods utilize local curvature information of the loss landscape to adjust the update step size. This is particularly beneficial for CLIP models, whose non-convex loss functions often contain sharp critical points. In such cases, natural gradient direction can offer more substantial and efficient per-iteration updates when fine-tuning with limited data. Natural Gradient Descent (NGD) is obtained by preconditioning the standard gradient with the inverse Fisher Information Matrix (FIM), which is computationally expensive for large models. To address this, we propose a Bayesian approximation of NGD using a Kalman filter for CLIP models. Our method combines the benefits of second-order optimization with Bayesian inference, which enhances generalization while providing uncertainty quantification. Extensive experiments conducted on diverse image classification datasets demonstrate that our algorithm consistently achieves superiorâ€“or comparableâ€“ID performance and improved OOD robustness compared to state-of-the-art baselines. To the best of our knowledge, this work represents the first successful application of Kalman filtering to fine-tuning CLIP-based models, which enables more robust and efficient learning in vision-language tasks. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚CLIPï¼Œå·²ç»åœ¨å¤šæ¨¡æ€æ•°æ®æŒ–æ˜ä¸­å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚åœ¨è¿™ç§æ¨¡å‹ä¸­ï¼Œå°æ ·æœ¬å¾®è°ƒæ˜¯åœ¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½çš„é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ ‡ç­¾æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚å¤§å¤šæ•°ç°æœ‰çš„å¾®è°ƒæ–¹æ³•ä¾èµ–äºä¸€é˜¶åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å™¨ï¼Œé€šå¸¸å­˜åœ¨æ”¶æ•›é€Ÿåº¦æ…¢ã€å¯¹æ­¥é•¿è¶…å‚æ•°æ•æ„Ÿä»¥åŠåœ¨OODè®¾ç½®ä¸­çš„æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒäºŒé˜¶æ–¹æ³•åˆ©ç”¨æŸå¤±æ™¯è§‚çš„å±€éƒ¨æ›²ç‡ä¿¡æ¯æ¥è°ƒæ•´æ›´æ–°æ­¥é•¿ã€‚è¿™å¯¹äºCLIPæ¨¡å‹ç‰¹åˆ«æœ‰ç›Šï¼Œå…¶éå‡¸æŸå¤±å‡½æ•°é€šå¸¸åŒ…å«å°–é”çš„ä¸´ç•Œç‚¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“ä½¿ç”¨æœ‰é™æ•°æ®è¿›è¡Œå¾®è°ƒæ—¶ï¼Œè‡ªç„¶æ¢¯åº¦æ–¹å‘å¯ä»¥æä¾›æ›´å®è´¨å’Œæ›´æœ‰æ•ˆçš„æ¯æ¬¡è¿­ä»£æ›´æ–°ã€‚è‡ªç„¶æ¢¯åº¦ä¸‹é™ï¼ˆNGDï¼‰æ˜¯é€šè¿‡ç”¨é€†è´¹é›ªä¿¡æ¯çŸ©é˜µï¼ˆFIMï¼‰å¯¹æ ‡å‡†æ¢¯åº¦è¿›è¡Œé¢„å¤„ç†è€Œå¾—åˆ°çš„ï¼Œè¿™å¯¹äºå¤§å‹æ¨¡å‹æ¥è¯´è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¡å°”æ›¼æ»¤æ³¢çš„NGDçš„è´å¶æ–¯è¿‘ä¼¼æ–¹æ³•ï¼Œç”¨äºCLIPæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†äºŒé˜¶ä¼˜åŒ–å’Œè´å¶æ–¯æ¨æ–­çš„ä¼˜ç‚¹ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æä¾›äº†ä¸ç¡®å®šæ€§é‡åŒ–ã€‚åœ¨å¤šç§å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç®—æ³•å§‹ç»ˆå®ç°äº†æ›´ä¼˜è¶Šæˆ–ç›¸å½“çš„è¡¨ç°ï¼Œåœ¨IDæ€§èƒ½ä¸Šæœ‰æ‰€æé«˜ï¼Œå¹¶ä¸”åœ¨OODè®¾ç½®ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œæ˜¯å°†å¡å°”æ›¼æ»¤æ³¢é¦–æ¬¡æˆåŠŸåº”ç”¨äºåŸºäºCLIPçš„æ¨¡å‹çš„å¾®è°ƒï¼Œä¸ºè§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†æ›´ç¨³å¥å’Œé«˜æ•ˆçš„å­¦ä¹ èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01694v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPçš„è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°æ®æŒ–æ˜ä¸­æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚é’ˆå¯¹è¯¥æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬å¾®è°ƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹å¦‚ä½•åœ¨ä¿æŒå†…éƒ¨åˆ†å¸ƒæ€§èƒ½çš„åŒæ—¶ä¼˜åŒ–åœ¨å¤–éƒ¨åˆ†å¸ƒçš„æ€§èƒ½ï¼Œç°æœ‰çš„å¤§å¤šæ•°å¾®è°ƒæ–¹æ³•ä¾èµ–ä¸€é˜¶æ¢¯åº¦ä¼˜åŒ–å™¨ï¼Œè¿™å¾€å¾€å¯¼è‡´æ”¶æ•›ç¼“æ…¢ã€å¯¹æ­¥é•¿è¶…å‚æ•°æ•æ„Ÿä»¥åŠåœ¨è¶…å‡ºè®¾è®¡åœºæ™¯çš„é€šç”¨åŒ–ä¸è‰¯çš„é—®é¢˜ã€‚ç›¸å¯¹è€Œè¨€ï¼ŒäºŒé˜¶æ–¹æ³•é€šè¿‡ä½¿ç”¨æŸå¤±åœ°è²Œçš„å±€éƒ¨æ›²ç‡ä¿¡æ¯è°ƒæ•´æ›´æ–°æ­¥é•¿ç‰¹åˆ«æœ‰åˆ©äºCLIPæ¨¡å‹ã€‚æœ¬ç ”ç©¶å°†è‡ªç„¶æ¢¯åº¦æ–¹å‘èå…¥CLIPæ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå°¤å…¶æ˜¯åˆ©ç”¨é€†Fisherä¿¡æ¯çŸ©é˜µä½œä¸ºè‡ªç„¶æ¢¯åº¦ä¸‹é™çš„å‰ç½®æ¡ä»¶ä»¥æ”¹å–„æ¨¡å‹è®­ç»ƒï¼Œè®¡ç®—æ•ˆç‡å¤§å¤§æé«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆäºŒé˜¶ä¼˜åŒ–ä¸è´å¶æ–¯æ¨æ–­çš„Kalmanæ»¤æ³¢è¿‘ä¼¼è‡ªç„¶æ¢¯åº¦ä¸‹é™æ–¹æ³•ç”¨äºCLIPæ¨¡å‹çš„æ–¹æ³•ã€‚å¹¿æ³›çš„å®éªŒæ•°æ®è¡¨æ˜ï¼Œç›¸æ¯”äºæœ€æ–°çš„ç®—æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æ¡ˆä¾‹ä¸­ä¿æŒäº†æ›´é«˜çš„ç¨³å®šæ€§æˆ–æ›´å¥½çš„è¡¨ç°èƒ½åŠ›ã€‚æœ€é‡è¦çš„æ˜¯æˆ‘ä»¬çš„ç ”ç©¶æˆæœå¼€åˆ›æ€§åœ°å°†å¡å°”æ›¼æ»¤æ³¢åº”ç”¨äºCLIPæ¨¡å‹çš„å¾®è°ƒï¼Œä¸ºè§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†æ›´ä¸ºç¨³å¥å’Œé«˜æ•ˆçš„å­¦ä¹ é€”å¾„ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶åœ¨æå‡CLIPæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPå’Œå…¶ä»–è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°æ®æŒ–æ˜ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å°‘é‡æ ·æœ¬å¾®è°ƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¸€é˜¶æ¢¯åº¦ä¼˜åŒ–å™¨åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å­˜åœ¨æ”¶æ•›ç¼“æ…¢ã€å¯¹è¶…å‚æ•°æ•æ„Ÿä»¥åŠåœ¨æ–°åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚</li>
<li>äºŒé˜¶æ–¹æ³•åˆ©ç”¨æŸå¤±åœ°è²Œçš„å±€éƒ¨æ›²ç‡ä¿¡æ¯è°ƒæ•´æ›´æ–°æ­¥é•¿ï¼Œå¯¹CLIPæ¨¡å‹ç‰¹åˆ«æœ‰åˆ©ã€‚</li>
<li>è‡ªç„¶æ¢¯åº¦ä¸‹é™é€šè¿‡é€†Fisherä¿¡æ¯çŸ©é˜µå‰ç½®æ¡ä»¶æ”¹å–„æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>æå‡ºç»“åˆäºŒé˜¶ä¼˜åŒ–ä¸è´å¶æ–¯æ¨æ–­çš„Kalmanæ»¤æ³¢è¿‘ä¼¼è‡ªç„¶æ¢¯åº¦ä¸‹é™æ–°æ–¹æ³•åº”ç”¨äºCLIPæ¨¡å‹å¾®è°ƒï¼Œå¤§å¹…æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚ç›¸è¾ƒäºå½“å‰é¡¶å°–æŠ€æœ¯æœ‰ç€æ›´ä½³çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ–¹æ³•åˆ›æ–°åœ°å°†å¡å°”æ›¼æ»¤æ³¢å¼•å…¥CLIPæ¨¡å‹çš„å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä¸ºåç»­è§†è§‰è¯­è¨€ä»»åŠ¡çš„ç ”ç©¶æä¾›äº†æ–°è§†è§’å’Œæ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07817303aa30ab3ce9a133f30ae8408b" align="middle">
<img src="https://picx.zhimg.com/v2-539c2becf339a823d36f371341856a1a" align="middle">
<img src="https://picx.zhimg.com/v2-6508e2e4234ee4e3e36bf04b377079a6" align="middle">
<img src="https://picx.zhimg.com/v2-e7c48e4b788b85148882ff6f7751d9a2" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Real-time-Continual-Learning-on-Intel-Loihi-2"><a href="#Real-time-Continual-Learning-on-Intel-Loihi-2" class="headerlink" title="Real-time Continual Learning on Intel Loihi 2"></a>Real-time Continual Learning on Intel Loihi 2</h2><p><strong>Authors:Elvin Hajizada, Danielle Rager, Timothy Shea, Leobardo Campos-Macias, Andreas Wild, Eyke HÃ¼llermeier, Yulia Sandamirskaya, Mike Davies</strong></p>
<p>AI systems on edge devices face a critical challenge in open-world environments: adapting when data distributions shift and novel classes emerge. While offline training dominates current paradigms, online continual learning (OCL)â€“where models learn incrementally from non-stationary streams without catastrophic forgettingâ€“remains challenging in power-constrained settings. We present a neuromorphic solution called CLP-SNN: a spiking neural network architecture for Continually Learning Prototypes and its implementation on Intelâ€™s Loihi 2 chip. Our approach introduces three innovations: (1) event-driven and spatiotemporally sparse local learning, (2) a self-normalizing three-factor learning rule maintaining weight normalization, and (3) integrated neurogenesis and metaplasticity for capacity expansion and forgetting mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves accuracy competitive with replay methods while being rehearsal-free. CLP-SNN delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms), and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired algorithms and neuromorphic hardware can break traditional accuracy-efficiency trade-offs for future edge AI systems. </p>
<blockquote>
<p>è¾¹ç¼˜è®¾å¤‡ä¸Šçš„AIç³»ç»Ÿåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­é¢ä¸´ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå½“æ•°æ®åˆ†å¸ƒå‘ç”Ÿå˜åŒ–å¹¶ä¸”å‡ºç°æ–°å‹ç±»åˆ«æ—¶ï¼Œå¦‚ä½•é€‚åº”è¿™äº›å˜åŒ–ã€‚è™½ç„¶ç¦»çº¿è®­ç»ƒæ˜¯ç›®å‰çš„ä¸»æµèŒƒå¼ï¼Œä½†åœ¨èƒ½æºå—é™çš„ç¯å¢ƒä¸­ï¼ŒæŒç»­åœ¨çº¿å­¦ä¹ ï¼ˆOCLï¼‰â€”â€”æ¨¡å‹ä»éå¹³ç¨³æ•°æ®æµä¸­é€æ­¥å­¦ä¹ è€Œä¸ä¼šå‘ç”Ÿç¾éš¾æ€§é—å¿˜â€”â€”ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCLP-SNNçš„ç¥ç»å½¢æ€è§£å†³æ–¹æ¡ˆï¼šä¸€ç§ç”¨äºæŒç»­å­¦ä¹ åŸå‹çš„è„‰å†²ç¥ç»ç½‘ç»œæ¶æ„åŠå…¶åœ¨è‹±ç‰¹å°”Loihi 2èŠ¯ç‰‡ä¸Šçš„å®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰é¡¹åˆ›æ–°ï¼šï¼ˆ1ï¼‰äº‹ä»¶é©±åŠ¨å’Œæ—¶ç©ºç¨€ç–å±€éƒ¨å­¦ä¹ ï¼Œï¼ˆ2ï¼‰ç»´æŒæƒé‡æ ‡å‡†åŒ–çš„è‡ªå½’ä¸€åŒ–ä¸‰å› ç´ å­¦ä¹ è§„åˆ™ï¼Œä»¥åŠï¼ˆ3ï¼‰é›†æˆç¥ç»å‘ç”Ÿå’Œçªè§¦å¯å¡‘æ€§ä»¥è¿›è¡Œå®¹é‡æ‰©å±•å’Œç¼“è§£é—å¿˜ã€‚åœ¨OpenLORISçš„å°‘é‡å­¦ä¹ å®éªŒä¸­ï¼ŒCLP-SNNçš„å‡†ç¡®ç‡ä¸å›æ”¾æ–¹æ³•ç›¸å½“ï¼Œä¸”æ— éœ€å›æ”¾ã€‚CLP-SNNæä¾›äº†å˜é©æ€§çš„æ•ˆç‡å¢ç›Šï¼šæ¯”è¾¹ç¼˜GPUä¸Šæœ€ä½³çš„æ›¿ä»£OCLå¿«70å€ï¼ˆ0.33æ¯«ç§’å¯¹23.2æ¯«ç§’ï¼‰ï¼Œå¹¶ä¸”èƒ½æ•ˆé«˜5600å€ï¼ˆ0.05æ¯«ç„¦å¯¹281æ¯«ç„¦ï¼‰ã€‚è¿™è¯æ˜äº†ååŒè®¾è®¡çš„è„‘å¯å‘ç®—æ³•å’Œç¥ç»å½¢æ€ç¡¬ä»¶å¯ä»¥æ‰“ç ´ä¼ ç»Ÿçš„ç²¾åº¦-æ•ˆç‡æƒè¡¡ï¼Œä¸ºæœªæ¥è¾¹ç¼˜AIç³»ç»Ÿçš„å‘å±•å¼€è¾Ÿé“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01553v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¾¹ç¼˜è®¾å¤‡çš„AIç³»ç»Ÿåœ¨å¼€æ”¾ç¯å¢ƒä¸­é¢ä¸´æ•°æ®åˆ†å¸ƒå˜åŒ–å’Œæ–°å…´ç±»åˆ«æŒ‘æˆ˜æ—¶ï¼Œéœ€è¦é€‚åº”æ–°æ•°æ®ã€‚å½“å‰ä¸»æµæ˜¯ç¦»çº¿è®­ç»ƒï¼Œä½†åœ¨çº¿æŒç»­å­¦ä¹ ï¼ˆOCLï¼‰åœ¨æœ‰é™çš„èµ„æºç¯å¢ƒä¸­ä¾ç„¶é¢‡å…·æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCLP-SNNçš„ç¥ç»å½¢æ€è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨æŒç»­å­¦ä¹ åŸå‹å¹¶å®ç°åŸºäºè‹±ç‰¹å°”Loihi 2èŠ¯ç‰‡çš„åŸå‹æ¶æ„å®æ–½ã€‚å¼•å…¥ä¸‰å¤§åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šäº‹ä»¶é©±åŠ¨ã€æ—¶ç©ºç¨€ç–çš„æœ¬åœ°å­¦ä¹ ï¼Œè‡ªæˆ‘æ­£å¸¸åŒ–çš„ä¸‰å› ç´ å­¦ä¹ è§„åˆ™åŠç»´æŒæƒé‡ç­‰åŒ–æœºåˆ¶é›†æˆç¥ç»å…ƒå‘ç”ŸåŠå¯å¡‘æ€§åŠŸèƒ½æ¥å¢å¼ºç³»ç»Ÿæ‰©å±•å’Œå‡å°‘é—å¿˜ã€‚åœ¨OpenLORISå°‘æ ·æœ¬å­¦ä¹ å®éªŒä¸­ï¼ŒCLP-SNNçš„å‡†ç¡®ç‡ä¸å›æ”¾æ–¹æ³•ç›¸å½“ä¸”æ— éœ€å›æ”¾ã€‚CLP-SNNå®ç°äº†æ˜¾è‘—æ•ˆç‡æå‡ï¼Œç›¸è¾ƒäºæœ€ä½³æ›¿ä»£æ–¹æ¡ˆåœ¨è¾¹ç¼˜GPUä¸Šçš„è¿è¡Œé€Ÿåº¦æå‡äº†70å€ï¼ˆ0.33æ¯«ç§’å¯¹æ¯”è‡³23.2æ¯«ç§’ï¼‰ï¼Œèƒ½è€—æå‡è¾¾åˆ°è‡³æ›´å‡ºè‰²çš„è¡¨ç°ã€‚è¯æ˜äº†è”åˆè®¾è®¡çš„è„‘å¯å‘å¼ç®—æ³•å’Œç¥ç»å½¢æ€ç¡¬ä»¶èƒ½ä¸ºæœªæ¥è¾¹ç¼˜AIç³»ç»Ÿæ‰“ç ´ä¼ ç»Ÿçš„å‡†ç¡®åº¦ä¸æ•ˆç‡çš„æƒè¡¡æŒ‘æˆ˜ã€‚æ€»çš„æ¥è¯´ï¼Œç¥ç»å½¢æ€ç®—æ³•çš„ä½¿ç”¨å¯å®ç°æ›´å¥½çš„å®æ—¶ååº”èƒ½åŠ›åŠæ€§èƒ½æ•ˆç‡ä¼˜åŒ–ï¼Œä½¿å…¶æˆä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸä¸­å¾ˆæœ‰æ½œåŠ›çš„åº”ç”¨æ–¹å‘ã€‚æœªæ¥è¯¥æŠ€æœ¯åœ¨æ™ºèƒ½æœºå™¨äººã€æ™ºèƒ½ç©¿æˆ´è®¾å¤‡ç­‰é¢†åŸŸæœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚æ­¤å¤–ï¼Œå…¶åœ¨èŠ¯ç‰‡ä¸Šç›´æ¥å®ç°çš„ç‰¹æ€§ä¹Ÿå°†æå¤§æ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•ä¸åº”ç”¨ã€‚é€šè¿‡æ­¤ç ”ç©¶å¯ä»¥çœ‹å‡ºç¥ç»å½¢æ€è®¡ç®—ä¸ºäººå·¥æ™ºèƒ½çš„å‘å±•å¼€å¯äº†æ–°çš„ç¯‡ç« ã€‚æ€»ä¹‹ï¼Œè¿™ä¸€åˆ›æ–°æ–¹æ¡ˆä¸ºæœªæ¥è¾¹ç¼˜AIç³»ç»Ÿå‘å±•å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚æ€»çš„æ¥è¯´éå¸¸å€¼å¾—æœŸå¾…ã€‚å¸Œæœ›è¿™é¡¹æŠ€æœ¯å¯ä»¥æ›´å¿«åœ°å¾—åˆ°æ¨å¹¿å’Œåº”ç”¨åœ¨å®é™…é¢†åŸŸä¸Šå¤§æ”¾å¼‚å½©ï¼é€šè¿‡åº”ç”¨è¿™é¡¹æŠ€æœ¯èƒ½å¤Ÿåœ¨æ™ºèƒ½ç©¿æˆ´è®¾å¤‡å’Œæœºå™¨äººç­‰é¢†åŸŸäº§ç”Ÿå¹¿æ³›çš„å®é™…åº”ç”¨ä»·å€¼å¹¶å®ç°é©å‘½æ€§çš„è¿›æ­¥çªç ´æå‡å®é™…åº”ç”¨æ€§èƒ½å®ç°æ›´å¤šåº”ç”¨åœºæ™¯çš„å‘å±•æå‡æ™ºèƒ½ç©¿æˆ´è®¾å¤‡æœºå™¨äººç­‰çš„æ™ºèƒ½æ°´å¹³ä¸ºäººä»¬å¸¦æ¥æ›´åŠ ä¾¿æ·çš„ç”Ÿæ´»ä½“éªŒæ”¹å˜æ•´ä¸ªè¡Œä¸šçš„ç«äº‰æ ¼å±€å’Œå¸‚åœºç”Ÿæ€æ ¼å±€æœç€æ›´åŠ æ™ºèƒ½åŒ–è‡ªåŠ¨åŒ–çš„æ–¹å‘å‘å±•å‰è¿›ä¿ƒè¿›æ•´ä¸ªç¤¾ä¼šç»æµçš„æŒç»­å‘å±•å’Œè¿›æ­¥æ”¹å–„äººä»¬çš„ç”Ÿæ´»è´¨é‡å®ç°äººå·¥æ™ºèƒ½æŠ€æœ¯çš„çœŸæ­£ä»·å€¼å®ç°æœªæ¥çš„æ™ºèƒ½åŒ–ç¤¾ä¼šå˜é©ç­‰ç­‰æ›´å¤šç²¾å½©çš„åº”ç”¨å‰æ™¯ã€‚å€¼å¾—æœŸå¾…å’Œå…³æ³¨ï¼æœ¬æ–‡æ€»ç»“å¦‚ä¸‹ï¼šè¯¥æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºç¥ç»å½¢æ€è®¡ç®—çš„AIç³»ç»Ÿè§£å†³æ–¹æ¡ˆâ€”â€”CLP-SNNã€‚é€šè¿‡è¯¥æ–¹æ¡ˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†åœ¨çº¿æŒç»­å­¦ä¹ åŸå‹ï¼Œå®ç°äº†é«˜æ•ˆç‡å’Œé«˜å‡†ç¡®ç‡çš„æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„é€‚åº”æ€§æ”¹è¿›ã€‚è¯¥æ–¹æ¡ˆå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œæ½œåŠ›ï¼Œæœ‰æœ›æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•å’Œåº”ç”¨æ¨å¹¿ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ¡ˆä¹Ÿå±•ç¤ºäº†æœªæ¥è¾¹ç¼˜AIç³»ç»Ÿçš„å¯èƒ½å‘å±•æ–¹å‘å’ŒæŒ‘æˆ˜ã€‚æœªæ¥è¯¥æ–¹æ¡ˆæœ‰æœ›åœ¨æ™ºèƒ½æœºå™¨äººã€æ™ºèƒ½ç©¿æˆ´è®¾å¤‡ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ™®åŠå’Œå‘å±•æ”¹å˜æ•´ä¸ªè¡Œä¸šçš„ç«äº‰æ ¼å±€å’Œå¸‚åœºç”Ÿæ€æ ¼å±€ç­‰ç­‰æ›´å¤šç²¾å½©çš„çªç ´åˆ›æ–°å€¼å¾—å…³æ³¨å’ŒæœŸå¾…ã€‚æ–‡ä¸­ä»‹ç»çš„æ–¹æ¡ˆç»“åˆäº†è„‘å¯å‘å¼ç®—æ³•å’Œç¥ç»å½¢æ€ç¡¬ä»¶çš„è¿ç”¨å¤§å¤§æé«˜äº†æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å’Œåˆ›æ–°ç¨‹åº¦å…·å¤‡å¹¿æ³›çš„ç ”ç©¶ä»·å€¼å’Œå‘å±•å‰æ™¯è®©æˆ‘ä»¬å¯¹æœªæ¥çš„å‘å±•å……æ»¡äº†æœŸå¾…ã€‚æ­¤æ¬¡æŠ€æœ¯çš„åˆ›æ–°ä¸å‘å±•æ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸æ–­å‰è¿›çš„ä¸€å¤§æ¨åŠ¨åŠ›æœªæ¥çš„ç ”ç©¶å¯ä»¥æ›´åŠ æ·±å…¥åœ°æ¢ç´¢è¿™ä¸€é¢†åŸŸå‘æ˜æ›´å¤šçš„æ½œåŠ›ä¸ºäººç±»ç¤¾ä¼šçš„ç§‘æŠ€è¿›æ­¥å’Œå‘å±•è´¡çŒ®åŠ›é‡å±•ç°å‡ºæ›´åŠ å¹¿é˜”çš„å‘å±•å‰æ™¯å’Œå‘å±•ç©ºé—´ä»¤äººæœŸå¾…æœªæ¥çš„å‘å±•è¶‹åŠ¿å’ŒæŠ€æœ¯çªç ´å°†ä¸ºæˆ‘ä»¬çš„ç”Ÿæ´»å¸¦æ¥æ›´å¤šçš„ä¾¿åˆ©å’Œåˆ›æ–°æ€§åº”ç”¨ï¼å€¼å¾—å…³æ³¨ï¼æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç¥ç»å½¢æ€è®¡ç®—çš„AIç³»ç»Ÿæ–¹æ¡ˆç”¨äºè§£å†³è¾¹ç¼˜è®¾å¤‡åœ¨å¼€æ”¾ç¯å¢ƒä¸‹æŒç»­å­¦ä¹ çš„é—®é¢˜å®ç°é«˜æ•ˆå‡†ç¡®çš„æœºå™¨å­¦ä¹ æ¨¡å‹é€‚åº”æ€§æ”¹è¿›ã€‚é€šè¿‡å¼•å…¥äº‹ä»¶é©±åŠ¨æ—¶ç©ºç¨€ç–å­¦ä¹ è‡ªæˆ‘æ­£å¸¸åŒ–çš„ä¸‰å› ç´ å­¦ä¹ è§„åˆ™ç­‰åˆ›æ–°ç‚¹å®ç°æŒç»­å­¦ä¹ åŸå‹å¹¶ä¸”å…·æœ‰è¾ƒé«˜çš„æ€§èƒ½å’Œæ•ˆç‡å±•ç°å‡ºå¹¿æ³›çš„åº”ç”¨å‰æ™¯å’ŒæŒ‘æˆ˜æ„ä¹‰æ¿€å‘äº†æˆ‘ä»¬å¯¹æœªæ¥è¾¹ç¼˜AIç³»ç»Ÿå‘å±•çš„æœŸå¾…å…³æ³¨æ­¤é¡¹æŠ€æœ¯çš„å‘å±•èƒ½å¤Ÿç»™æˆ‘ä»¬ç¤¾ä¼šå¸¦æ¥æ›´å¤šåˆ›æ–°å˜é©çš„ä»·å€¼å¹¶å±•ç°å‡ºå¹¿é˜”çš„åº”ç”¨å‰æ™¯å€¼å¾—æˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶å’Œæ¢ç´¢ä¸ºäººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•è´¡çŒ®åŠ›é‡æ¨åŠ¨ç§‘æŠ€è¿›æ­¥ï¼è¯¥æ–¹æ¡ˆæˆåŠŸè§£å†³äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•é¢ä¸´çš„ä¸€ç³»åˆ—é—®é¢˜ä¸ºäººå·¥æ™ºèƒ½çš„å‘å±•å¸¦æ¥äº†å…¨æ–°çš„æœºé‡å’ŒæŒ‘æˆ˜å±•ç°å‡ºå…¶å¼ºå¤§çš„æ½œåŠ›å’Œåº”ç”¨ä»·å€¼ä¸ºæˆ‘ä»¬æ‰“å¼€äº†å…¨æ–°çš„ç§‘æŠ€å¤§é—¨è®©äººä»¬å¯¹æœªæ¥å……æ»¡ä¿¡å¿ƒå……æ»¡äº†æ— é™æœŸå¾…ï¼æœªæ¥çš„å‘å±•è¶‹åŠ¿ä»¤äººç©ç›®è¿™é¡¹æŠ€æœ¯å°†ç»§ç»­å¼•é¢†ç§‘æŠ€å‰æ²¿çš„å‘å±•æ½®æµä¸æ–­çªç ´æ–°çš„æŠ€æœ¯ç“¶é¢ˆå±•ç°å‡ºæ›´åŠ å¹¿é˜”çš„å‘å±•ç©ºé—´å’Œæ— é™çš„åˆ›æ–°æ½œåŠ›æ¨åŠ¨äººç±»ç¤¾ä¼šä¸æ–­å‘å‰å‘å±•ï¼è®©æˆ‘ä»¬å…±åŒæœŸå¾…è¿™é¡¹æŠ€æœ¯æœªæ¥çš„å‘å±•å§ï¼</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIç³»ç»Ÿåœ¨è¾¹ç¼˜è®¾å¤‡é¢ä¸´å¼€æ”¾ç¯å¢ƒä¸‹çš„æ•°æ®åˆ†å¸ƒå˜åŒ–å’Œæ–°å…´ç±»åˆ«æŒ‘æˆ˜æ—¶å­˜åœ¨é€‚åº”éš¾é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c64b2b5678339174e066e8d291d205a9" align="middle">
<img src="https://picx.zhimg.com/v2-06ae348c2585522403b805ff7e59e716" align="middle">
<img src="https://picx.zhimg.com/v2-5b0a522b2fde1226b7686b308499aa73" align="middle">
<img src="https://picx.zhimg.com/v2-b6c326f940ac5c25c471be91f52bcb42" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-Ordinal-Meta-Learning-with-VLMs-for-Fine-Grained-Fruit-Quality-Prediction"><a href="#Privacy-Preserving-Ordinal-Meta-Learning-with-VLMs-for-Fine-Grained-Fruit-Quality-Prediction" class="headerlink" title="Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained   Fruit Quality Prediction"></a>Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained   Fruit Quality Prediction</h2><p><strong>Authors:Riddhi Jain, Manasi Patwardhan, Aayush Mishra, Parijat Deshpande, Beena Rai</strong></p>
<p>To effectively manage the wastage of perishable fruits, it is crucial to accurately predict their freshness or shelf life using non-invasive methods that rely on visual data. In this regard, deep learning techniques can offer a viable solution. However, obtaining fine-grained fruit freshness labels from experts is costly, leading to a scarcity of data. Closed proprietary Vision Language Models (VLMs), such as Gemini, have demonstrated strong performance in fruit freshness detection task in both zero-shot and few-shot settings. Nonetheless, food retail organizations are unable to utilize these proprietary models due to concerns related to data privacy, while existing open-source VLMs yield sub-optimal performance for the task. Fine-tuning these open-source models with limited data fails to achieve the performance levels of proprietary models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm, designed to train smaller VLMs. This approach utilizes meta-learning to address data sparsity and leverages label ordinality, thereby achieving state-of-the-art performance in the fruit freshness classification task under both zero-shot and few-shot settings. Our method achieves an industry-standard accuracy of 92.71%, averaged across all fruits.   Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning, Ordinal Regression </p>
<blockquote>
<p>é’ˆå¯¹æ˜“è…æ°´æœçš„æµªè´¹é—®é¢˜ï¼Œä½¿ç”¨éä¾µå…¥æ€§æ–¹æ³•ï¼ˆä¾èµ–äºè§†è§‰æ•°æ®ï¼‰å‡†ç¡®é¢„æµ‹å…¶æ–°é²œåº¦æˆ–ä¿è´¨æœŸè‡³å…³é‡è¦ã€‚åœ¨è¿™æ–¹é¢ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å¯ä»¥æä¾›å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»ä¸“å®¶é‚£é‡Œè·å–ç²¾ç»†çš„æ°´æœæ–°é²œåº¦æ ‡ç­¾æˆæœ¬é«˜æ˜‚ï¼Œå¯¼è‡´æ•°æ®ç¨€ç¼ºã€‚å°é—­çš„ä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Geminiï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸­å‡æ˜¾ç¤ºå‡ºåœ¨æ°´æœæ–°é²œåº¦æ£€æµ‹ä»»åŠ¡ä¸­çš„å¼ºå¤§æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºä¸æ•°æ®éšç§ç›¸å…³çš„æ‹…å¿§ï¼Œé£Ÿå“é›¶å”®ç»„ç»‡æ— æ³•åˆ©ç”¨è¿™äº›ä¸“æœ‰æ¨¡å‹ï¼Œè€Œç°æœ‰çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ä½¿ç”¨æœ‰é™æ•°æ®å¯¹è¿™äº›å¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒä¹Ÿæ— æ³•è¾¾åˆ°ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¨¡å‹æ— å…³çš„åºæ•°å…ƒå­¦ä¹ ï¼ˆMAOMLï¼‰ç®—æ³•ï¼Œæ—¨åœ¨è®­ç»ƒè¾ƒå°çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…ƒå­¦ä¹ æ¥è§£å†³æ•°æ®ç¨€ç–é—®é¢˜ï¼Œå¹¶åˆ©ç”¨æ ‡ç­¾åºæ•°ï¼Œä»è€Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹åœ¨æ°´æœæ–°é²œåº¦åˆ†ç±»ä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æ°´æœä¸Šçš„å¹³å‡è¡Œä¸šæ ‡å‡†å‡†ç¡®ç‡ä¸º92.71%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01449v1">PDF</a> 9 pages, 1 figure, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨éä¾µå…¥æ€§è§†è§‰æ•°æ®é¢„æµ‹æ˜“è…æ°´æœæ–°é²œåº¦çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºæ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚ç”±äºä»ä¸“å®¶é‚£é‡Œè·å–ç²¾ç»†çš„æ°´æœæ–°é²œåº¦æ ‡ç­¾æˆæœ¬é«˜æ˜‚ï¼Œå¯¼è‡´æ•°æ®ç¨€ç¼ºã€‚è™½ç„¶å°é—­çš„ä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Geminiï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ°´æœæ–°é²œåº¦æ£€æµ‹æ€§èƒ½ï¼Œä½†ç”±äºæ•°æ®éšç§æ‹…å¿§ï¼Œé£Ÿå“é›¶å”®ç»„ç»‡æ— æ³•åˆ©ç”¨è¿™äº›æ¨¡å‹ã€‚åŒæ—¶ï¼Œç°æœ‰å¼€æºVLMsçš„æ€§èƒ½å¹¶ä¸ç†æƒ³ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„åºæ•°å…ƒå­¦ä¹ ï¼ˆMAOMLï¼‰ç®—æ³•ï¼Œç”¨äºè®­ç»ƒè¾ƒå°çš„VLMsã€‚è¯¥ç®—æ³•åˆ©ç”¨å…ƒå­¦ä¹ å’Œæ ‡ç­¾åºæ•°æ¥è§£å†³æ•°æ®ç¨€ç–é—®é¢˜ï¼Œåœ¨æ°´æœæ–°é²œåº¦åˆ†ç±»ä»»åŠ¡ä¸­å®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„æœ€æ–°æ€§èƒ½ã€‚è¯¥æ–¹æ³•å¹³å‡å‡†ç¡®ç‡ä¸º92.71%ï¼Œè¾¾åˆ°è¡Œä¸šé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éä¾µå…¥æ€§æ–¹æ³•åˆ©ç”¨è§†è§‰æ•°æ®é¢„æµ‹æ°´æœæ–°é²œåº¦è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨æ°´æœæ–°é²œåº¦é¢„æµ‹ä¸­å…·æœ‰å¯è¡Œæ€§ã€‚</li>
<li>è·å–ç²¾ç»†çš„æ°´æœæ–°é²œåº¦æ ‡ç­¾æˆæœ¬é«˜æ˜‚ï¼Œå¯¼è‡´æ•°æ®ç¨€ç¼ºã€‚</li>
<li>å°é—­çš„ä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Geminiï¼‰åœ¨æ°´æœæ–°é²œåº¦æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>é£Ÿå“é›¶å”®ç»„ç»‡å› æ•°æ®éšç§æ‹…å¿§æ— æ³•åˆ©ç”¨ä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>ç°æœ‰å¼€æºVLMsåœ¨æ°´æœæ–°é²œåº¦åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f309248e99911c8fcc34e840bbdf05bc" align="middle">
<img src="https://picx.zhimg.com/v2-7ecfa8ae53676835050b941cc0263ec7" align="middle">
<img src="https://picx.zhimg.com/v2-96398d7ccd66ed1c95b77ba2c65205ff" align="middle">
<img src="https://picx.zhimg.com/v2-09084ca3c9d9ca1599c2d8b017bdcb7a" align="middle">
<img src="https://picx.zhimg.com/v2-9dc002538c863096f45baa4589d3ab0b" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-Intractable-Multimodal-Policies-with-Reparameterization-and-Diversity-Regularization"><a href="#Learning-Intractable-Multimodal-Policies-with-Reparameterization-and-Diversity-Regularization" class="headerlink" title="Learning Intractable Multimodal Policies with Reparameterization and   Diversity Regularization"></a>Learning Intractable Multimodal Policies with Reparameterization and   Diversity Regularization</h2><p><strong>Authors:Ziqi Wang, Jiashun Liu, Ling Pan</strong></p>
<p>Traditional continuous deep reinforcement learning (RL) algorithms employ deterministic or unimodal Gaussian actors, which cannot express complex multimodal decision distributions. This limitation can hinder their performance in diversity-critical scenarios. There have been some attempts to design online multimodal RL algorithms based on diffusion or amortized actors. However, these actors are intractable, making existing methods struggle with balancing performance, decision diversity, and efficiency simultaneously. To overcome this challenge, we first reformulate existing intractable multimodal actors within a unified framework, and prove that they can be directly optimized by policy gradient via reparameterization. Then, we propose a distance-based diversity regularization that does not explicitly require decision probabilities. We identify two diversity-critical domains, namely multi-goal achieving and generative RL, to demonstrate the advantages of multimodal policies and our method, particularly in terms of few-shot robustness. In conventional MuJoCo benchmarks, our algorithm also shows competitive performance. Moreover, our experiments highlight that the amortized actor is a promising policy model class with strong multimodal expressivity and high performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/PneuC/DrAC">https://github.com/PneuC/DrAC</a> </p>
<blockquote>
<p>ä¼ ç»Ÿçš„è¿ç»­æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•é‡‡ç”¨ç¡®å®šæ€§æˆ–å•å³°é«˜æ–¯è¡ŒåŠ¨è€…ï¼Œæ— æ³•è¡¨è¾¾å¤æ‚çš„å¤šå³°å†³ç­–åˆ†å¸ƒã€‚è¿™ä¸€å±€é™æ€§å¯èƒ½ä¼šé˜»ç¢å®ƒä»¬åœ¨å…³é”®å¤šæ ·æ€§åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚è™½ç„¶æœ‰ä¸€äº›åŸºäºæ‰©æ•£æˆ–æ‘Šé”€è¡ŒåŠ¨è€…çš„åœ¨çº¿å¤šå³°RLç®—æ³•çš„è®¾è®¡å°è¯•ï¼Œä½†è¿™äº›è¡ŒåŠ¨è€…éš¾ä»¥å¤„ç†ï¼Œä½¿å¾—ç°æœ‰æ–¹æ³•åœ¨æ€§èƒ½ã€å†³ç­–å¤šæ ·æ€§å’Œæ•ˆç‡ä¹‹é—´å¾ˆéš¾è¾¾åˆ°å¹³è¡¡ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…é‡æ–°è¡¨è¿°ç°æœ‰çš„éš¾ä»¥å¤„ç†çš„å¤šå³°è¡ŒåŠ¨è€…ï¼Œå¹¶è¯æ˜å®ƒä»¬å¯ä»¥é€šè¿‡å†å‚æ•°åŒ–ç›´æ¥ç”±æ”¿ç­–æ¢¯åº¦è¿›è¡Œä¼˜åŒ–ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè·ç¦»çš„å¤šæ ·æ€§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ƒä¸éœ€è¦æ˜ç¡®å†³ç­–æ¦‚ç‡ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸¤ä¸ªå…³é”®å¤šæ ·æ€§çš„é¢†åŸŸï¼Œå³å¤šç›®æ ‡å®ç°å’Œç”ŸæˆRLï¼Œä»¥å±•ç¤ºå¤šå³°æ”¿ç­–å’Œæˆ‘ä»¬çš„æ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯åœ¨å°‘é‡é•œå¤´ä¸‹çš„ç¨³å¥æ€§ï¼‰çš„ä¼˜åŠ¿ã€‚åœ¨å¸¸è§„çš„MuJoCoåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ç®—æ³•ä¹Ÿæ˜¾ç¤ºå‡ºå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¿˜å¼ºè°ƒï¼Œæ‘Šé”€è¡ŒåŠ¨è€…æ˜¯ä¸€ä¸ªå…·æœ‰å¼ºå¤§å¤šå³°è¡¨è¾¾èƒ½åŠ›å’Œé«˜æ€§èƒ½çš„æœ‰å‰é€”çš„æ”¿ç­–æ¨¡å‹ç±»åˆ«ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/PneuC/DrAC">https://github.com/PneuC/DrAC</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01374v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>     ä¼ ç»Ÿè¿ç»­æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•é‡‡ç”¨ç¡®å®šæ€§æˆ–å•å³°é«˜æ–¯è¡Œä¸ºè€…ï¼Œæ— æ³•è¡¨è¾¾å¤æ‚çš„å¤šå…ƒå†³ç­–åˆ†å¸ƒã€‚åœ¨å…³é”®å¤šæ ·åŒ–åœºæ™¯ä¸­ï¼Œè¿™é™åˆ¶äº†å…¶æ€§èƒ½ã€‚å°½ç®¡æœ‰ä¸€äº›åŸºäºæ‰©æ•£æˆ–æ‘Šé”€è¡Œä¸ºè€…çš„åœ¨çº¿å¤šå…ƒRLç®—æ³•å°è¯•ï¼Œä½†è¿™äº›è¡Œä¸ºè€…éš¾ä»¥å¤„ç†ï¼Œä½¿å¾—ç°æœ‰æ–¹æ³•åœ¨æ€§èƒ½ã€å†³ç­–å¤šæ ·æ€§å’Œæ•ˆç‡ä¹‹é—´éš¾ä»¥å¹³è¡¡ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»Ÿä¸€æ¡†æ¶é‡æ–°åˆ¶å®šç°æœ‰éš¾ä»¥å¤„ç†çš„å¤šæ¨¡æ€è¡Œä¸ºè€…ï¼Œå¹¶è¯æ˜å¯é€šè¿‡å‚æ•°åŒ–ç›´æ¥ä¼˜åŒ–æ”¿ç­–æ¢¯åº¦ã€‚æå‡ºä¸€ç§åŸºäºè·ç¦»å¤šæ ·æ€§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ— éœ€æ˜ç¡®å†³ç­–æ¦‚ç‡ã€‚åœ¨å¤šç›®æ ‡è¾¾æˆå’Œç”ŸæˆRLä¸¤ä¸ªå¤šæ ·æ€§å…³é”®é¢†åŸŸä¸­éªŒè¯äº†å¤šæ¨¡æ€ç­–ç•¥åŠæœ¬æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬ç¨³å¥æ€§æ–¹é¢ã€‚åœ¨å¸¸è§„MuJoCoåŸºå‡†æµ‹è¯•ä¸­ï¼Œæœ¬ç®—æ³•è¡¨ç°å‡ºç«äº‰åŠ›ã€‚å®éªŒæ˜¾ç¤ºæ‘Šé”€è¡Œä¸ºè€…æ˜¯ä¸€ä¸ªå…·æœ‰å¼ºå¤§å¤šå…ƒè¡¨è¾¾åŠ›å’Œé«˜æ€§èƒ½çš„æ½œåŠ›æ”¿ç­–æ¨¡å‹ç±»åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿè¿ç»­æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ä½¿ç”¨ç¡®å®šæ€§æˆ–å•å³°é«˜æ–¯è¡Œä¸ºè€…ï¼Œå­˜åœ¨è¡¨è¾¾å¤æ‚å¤šå…ƒå†³ç­–åˆ†å¸ƒçš„å±€é™æ€§ã€‚</li>
<li>åœ¨å¤šæ ·åŒ–åœºæ™¯ä¸­ï¼Œè¯¥å±€é™æ€§ä¼šå½±å“ç®—æ³•æ€§èƒ½ã€‚</li>
<li>æœ‰åœ¨çº¿å¤šå…ƒRLç®—æ³•çš„å°è¯•åŸºäºéš¾ä»¥å¤„ç†çš„è¡Œä¸ºè€…ï¼ˆå¦‚æ‰©æ•£æˆ–æ‘Šé”€è¡Œä¸ºè€…ï¼‰ï¼Œä½¿å¾—å¹³è¡¡æ€§èƒ½ã€å†³ç­–å¤šæ ·æ€§å’Œæ•ˆç‡å˜å¾—å›°éš¾ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡ç»Ÿä¸€æ¡†æ¶é‡æ–°åˆ¶å®šå¤šå…ƒè¡Œä¸ºè€…ï¼Œå¹¶è¯æ˜å¯é€šè¿‡å‚æ•°åŒ–ç›´æ¥ä¼˜åŒ–æ”¿ç­–æ¢¯åº¦ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºè·ç¦»å¤šæ ·æ€§æ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œæ— éœ€æ˜ç¡®å†³ç­–æ¦‚ç‡ï¼Œæé«˜ç®—æ³•æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šç›®æ ‡è¾¾æˆå’Œç”ŸæˆRLé¢†åŸŸçš„å®éªŒéªŒè¯äº†å¤šæ¨¡æ€ç­–ç•¥çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬ç¨³å¥æ€§æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8fcdab6ec311d09b33828ec5cb43846" align="middle">
<img src="https://picx.zhimg.com/v2-fc88036898ff3b4c2b9fde642ee4bfa6" align="middle">
<img src="https://picx.zhimg.com/v2-b7f16b6b83325f8f6a8a8db90938c408" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Speech-DRAME-A-Framework-for-Human-Aligned-Benchmarks-in-Speech-Role-Play"><a href="#Speech-DRAME-A-Framework-for-Human-Aligned-Benchmarks-in-Speech-Role-Play" class="headerlink" title="Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech   Role-Play"></a>Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech   Role-Play</h2><p><strong>Authors:Jiatong Shi, Jionghao Han, Yichen Lu, Santiago Pascual, Pengfei Wu, Chenye Cui, Shinji Watanabe, Chao Weng, Cong Zhou</strong></p>
<p>Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present Speech-DRAME, a unified framework that contributes at three levels: (i) Speech-DRAME-EvalBench, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models (SEMs), (ii) DRAME-Eval, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) Speech-DRAME-RoleBench, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: Archetype Evaluation, a top-down approach measuring adherence to broad role archetypes, and Realism Evaluation, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, DRAME-Eval achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play. </p>
<blockquote>
<p>è§’è‰²æ‰®æ¼”å·²ç»æˆä¸ºç”Ÿæˆæ¨¡å‹çš„å…³é”®æµ‹è¯•å¹³å°ï¼Œä»å•çº¯çš„æ–‡æœ¬å¯¹è¯æ‰©å±•åˆ°å¤šæ¨¡å¼äº¤äº’ã€‚å°†è§’è‰²æ‰®æ¼”æ‰©å±•åˆ°è¯­éŸ³æ•æ‰äº†è¯­è°ƒã€æƒ…æ„Ÿå’Œä¼ è¾¾ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ–°çš„è¯„ä¼°æŒ‘æˆ˜ã€‚å½“å‰ç®¡é“ç»å¸¸ä½¿ç”¨éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰ä½œä¸ºé›¶æ ·æœ¬è¯„ä¼°å™¨ï¼Œè¿™ä¼šé—æ¼å‰¯è¯­è¨€çº¿ç´¢ï¼Œå°†å¤šä¸ªæ–¹é¢åˆå¹¶ä¸ºç²—ç•¥åˆ†æ•°ï¼Œå¹¶ä¸”ä¾èµ–äºæ— æ³•åæ˜ çœŸå®ä¸–ç•Œè§’è‰²çš„åˆæˆè¯­éŸ³å‚è€ƒã€‚æˆ‘ä»¬æå‡ºäº†Speech-DRAMEï¼Œä¸€ä¸ªè´¡çŒ®äº†ä¸‰ä¸ªå±‚æ¬¡çš„ç»Ÿä¸€æ¡†æ¶ï¼šé¦–å…ˆæ˜¯Speech-DRAME-EvalBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«åŒè¯­äººç±»æ³¨é‡Šæ•°æ®å’Œç”¨äºè®­ç»ƒå’Œæµ‹è¯•è¯­éŸ³è¯„ä¼°æ¨¡å‹ï¼ˆSEMï¼‰çš„åè®®ï¼›å…¶æ¬¡æ˜¯DRAME-Evalï¼Œä¸€ä¸ªç»è¿‡ç²¾ç»†è°ƒæ•´çš„è¯„ä¼°æ¨¡å‹ï¼Œå®ƒæ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬çš„ALLMï¼›æœ€åæ˜¯Speech-DRAME-RoleBenchï¼Œä¸€ä¸ªè¯­éŸ³è§’è‰²æ‰®æ¼”åŸºå‡†ï¼Œåˆ©ç”¨DRAME-Evalä½œä¸ºè‡ªåŠ¨è£åˆ¤æ¥æ¯”è¾ƒè¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰ã€‚Speech-DRAMEåŒºåˆ†äº†ä¸¤ç§äº’è¡¥çš„è¯„ä¼°ç­–ç•¥ï¼šåŸå‹è¯„ä¼°ï¼Œè¿™æ˜¯ä¸€ç§è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•ï¼Œæµ‹é‡å¯¹å¹¿æ³›è§’è‰²åŸå‹çš„éµå¾ªç¨‹åº¦ï¼›å’Œç°å®ä¸»ä¹‰è¯„ä¼°ï¼Œè¿™æ˜¯ä¸€ç§è‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•ï¼ŒåŸºäºçœŸå®çš„äººç±»è¯­éŸ³ï¼Œå¼ºè°ƒç»†å¾®çš„è§’è‰²è´¨é‡ã€‚ä¸é›¶æ ·æœ¬ALLMè¯„å§”ç›¸æ¯”ï¼ŒDRAME-Evalä¸äººç±»è¯„åˆ†è¾¾æˆæ›´å¼ºçš„å…±è¯†ï¼ˆåœ¨åŸå‹ä¸­ï¼ŒPearsonç›¸å…³ç³»æ•°ä»0.480æé«˜åˆ°0.629ï¼›åœ¨ç°å®ä¸»ä¹‰ä¸­ï¼Œä»0.390æé«˜åˆ°0.625ï¼‰ã€‚é€šè¿‡æ•´åˆé€æ˜çš„åŸºå‡†èµ„æºã€å»ºæ¨¡æ–¹æ³•å’Œç³»ç»Ÿçº§è¯„ä¼°ï¼ŒSpeech-DRAMEä¸ºè¯„ä¼°å£è¯­è§’è‰²æ‰®æ¼”æä¾›äº†ç¬¬ä¸€ä¸ªå…¨é¢ã€å¯å¤åˆ¶çš„åŸºçŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01261v1">PDF</a> 67 pages</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºç”Ÿæˆæ¨¡å‹çš„è§’è‰²æ‰®æ¼”å·²ç»ä»çº¯æ–‡æœ¬å¯¹è¯æ‰©å±•åˆ°å¤šåª’ä½“äº¤äº’é¢†åŸŸã€‚ç„¶è€Œï¼Œå½“æ‰©å±•åˆ°è¯­éŸ³æ•æ‰æ—¶ï¼Œå¯¹è¯­è°ƒã€æƒ…æ„Ÿå’Œè¡¨è¾¾çš„æ•æ‰ä¼šå¸¦æ¥æ–°çš„æŒ‘æˆ˜ã€‚å½“å‰è¯„ä¼°æ–¹æ³•é€šå¸¸ä½¿ç”¨é›¶æ ·æœ¬çš„éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰ä½œä¸ºè¯„ä¼°å™¨ï¼Œä½†å­˜åœ¨å¿½ç•¥éè¯­è¨€çº¿ç´¢ã€å°†å¤šä¸ªæ–¹é¢åˆå¹¶ä¸ºç²—ç•¥åˆ†æ•°ä»¥åŠä¾èµ–æ— æ³•åæ˜ çœŸå®ä¸–ç•Œè§’è‰²çš„åˆæˆè¯­éŸ³å‚è€ƒç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†Speech-DRAMEæ¡†æ¶ï¼Œåœ¨ä¸‰ä¸ªå±‚é¢ä¸Šåšå‡ºè´¡çŒ®ï¼šï¼ˆä¸€ï¼‰Speech-DRAME-EvalBenchè¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«åŒè¯­äººç±»æ³¨é‡Šæ•°æ®å’Œç”¨äºè®­ç»ƒå’Œæµ‹è¯•è¯­éŸ³è¯„ä¼°æ¨¡å‹ï¼ˆSEMï¼‰çš„åè®®ï¼›ï¼ˆäºŒï¼‰ç»è¿‡ç²¾ç»†è®­ç»ƒçš„DRAME-Evalè¯„ä¼°æ¨¡å‹ï¼Œå¤§å¹…ä¼˜äºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬çš„ALLMï¼›ï¼ˆä¸‰ï¼‰Speech-DRAME-RoleBenchè¯­éŸ³è§’è‰²æ‰®æ¼”åŸºå‡†ï¼Œåˆ©ç”¨DRAME-Evalä½œä¸ºè‡ªåŠ¨è¯„ä¼°å™¨æ¥æ¯”è¾ƒè¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰ã€‚Speech-DRAMEåŒºåˆ†äº†ä¸¤ç§äº’è¡¥çš„è¯„ä¼°ç­–ç•¥ï¼šåŸå‹è¯„ä¼°ï¼ˆè‡ªä¸Šè€Œä¸‹è¡¡é‡å¯¹è§’è‰²åŸå‹çš„éµå¾ªç¨‹åº¦ï¼‰å’Œç°å®æ€§è¯„ä¼°ï¼ˆè‡ªä¸‹è€Œä¸Šä»¥çœŸå®äººç±»è¯­éŸ³ä¸ºåŸºç¡€ï¼Œå¼ºè°ƒè§’è‰²è´¨é‡çš„ç»†å¾®å·®åˆ«ï¼‰ã€‚ç›¸è¾ƒäºé›¶æ ·æœ¬çš„ALLMè¯„ä¼°å™¨ï¼ŒDRAME-Evalä¸äººç±»è¯„åˆ†çš„ç›¸å…³æ€§æ›´é«˜ï¼ˆåŸå‹ç›¸å…³åº¦ä»0.48æå‡åˆ°0.629ï¼Œç°å®æ€§ç›¸å…³åº¦ä»0.39æå‡åˆ°0.625ï¼‰ã€‚é€šè¿‡æ•´åˆé€æ˜çš„åŸºå‡†èµ„æºã€å»ºæ¨¡æ–¹æ³•å’Œç³»ç»Ÿçº§è¯„ä¼°ï¼ŒSpeech-DRAMEä¸ºè¯„ä¼°å£è¯­è§’è‰²æ‰®æ¼”æä¾›äº†é¦–ä¸ªå…¨é¢ã€å¯å¤åˆ¶çš„åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§’è‰²æ‰®æ¼”å·²æˆä¸ºç”Ÿæˆæ¨¡å‹çš„å…³é”®æµ‹è¯•å¹³å°ï¼Œå·²ä»æ–‡æœ¬æ‰©å±•åˆ°å¤šåª’ä½“äº¤äº’ã€‚</li>
<li>è¯­éŸ³è§’è‰²æ‰®æ¼”çš„è¯„ä¼°é¢ä¸´æ–°çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ•æ‰è¯­è°ƒã€æƒ…æ„Ÿå’Œè¡¨è¾¾ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä½¿ç”¨é›¶æ ·æœ¬çš„ALLMï¼Œå­˜åœ¨å¿½ç•¥éè¯­è¨€çº¿ç´¢å’Œåˆæˆè¯­éŸ³å‚è€ƒä¸çœŸå®çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†Speech-DRAMEæ¡†æ¶ï¼ŒåŒ…æ‹¬è¯„ä¼°åŸºå‡†ã€ç²¾ç»†è®­ç»ƒçš„è¯„ä¼°æ¨¡å‹å’Œè¯­éŸ³è§’è‰²æ‰®æ¼”åŸºå‡†ã€‚</li>
<li>Speech-DRAMEåŒºåˆ†äº†ä¸¤ç§è¯„ä¼°ç­–ç•¥ï¼šåŸå‹è¯„ä¼°å’Œç°å®æ€§è¯„ä¼°ã€‚</li>
<li>DRAME-Evalè¯„ä¼°æ¨¡å‹ä¸äººç±»çš„è¯„åˆ†ä¸€è‡´æ€§è¾ƒé«˜ï¼Œç›¸æ¯”é›¶æ ·æœ¬çš„ALLMæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>Speech-DRAMEæ¡†æ¶ä¸ºå£è¯­è§’è‰²æ‰®æ¼”çš„è¯„ä¼°æä¾›äº†å…¨é¢ã€å¯å¤åˆ¶çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ffdaf16c640db91564b53bee58c9906" align="middle">
<img src="https://picx.zhimg.com/v2-757a1f6512d58042f7b6681c9468e50b" align="middle">
<img src="https://picx.zhimg.com/v2-fa03f68ce7a05f67ec76f3361248c915" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FEval-TTC-Fair-Evaluation-Protocol-for-Test-Time-Compute"><a href="#FEval-TTC-Fair-Evaluation-Protocol-for-Test-Time-Compute" class="headerlink" title="FEval-TTC: Fair Evaluation Protocol for Test-Time Compute"></a>FEval-TTC: Fair Evaluation Protocol for Test-Time Compute</h2><p><strong>Authors:Pavel Rumiantsev, Soumyasundar Pal, Yingxue Zhang, Mark Coates</strong></p>
<p>The performance of Large Language Models (LLMs) and the associated dollar costs of API calls can fluctuate over time, potentially invalidating conclusions drawn in prior research. To address this, we propose a Fair Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure consistent assessment of test-time compute (TTC) methods, regardless of such fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize underlying Chains-of-Thought (CoT). It supports evaluations across multiple LLMs on a diverse set of mathematical and commonsense reasoning datasets. The few-shot prompting and answer extraction processes are standardized across datasets, reducing both time and monetary overhead for researchers. Furthermore, we provide a cost modelling procedure that estimates both the token and dollar cost per query, facilitating equitable comparisons of prevalent TTC methods. We open-source FEval-TTC for public use at <a target="_blank" rel="noopener" href="https://github.com/networkslab/feval_ttc">https://github.com/networkslab/feval_ttc</a> . </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ä»¥åŠAPIè°ƒç”¨ç›¸å…³çš„ç¾å…ƒæˆæœ¬éšæ—¶é—´å¯èƒ½äº§ç”Ÿæ³¢åŠ¨ï¼Œè¿™å¯èƒ½ä¼šä½¿ä¹‹å‰ç ”ç©¶ä¸­å¾—å‡ºçš„ç»“è®ºå¤±æ•ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘æµ‹è¯•æ—¶é—´è®¡ç®—ï¼ˆTTCï¼‰çš„å…¬å¹³è¯„ä¼°åè®®ï¼ˆFEval-TTCï¼‰ï¼Œæ—¨åœ¨ç¡®ä¿å¯¹æµ‹è¯•æ—¶é—´è®¡ç®—æ–¹æ³•çš„ä¸€è‡´è¯„ä¼°ï¼Œæ— è®ºæ³¢åŠ¨å¦‚ä½•ã€‚FEval-TTCä¸“æ³¨äºè¯„ä¼°åˆ©ç”¨åº•å±‚æ€ç»´é“¾ï¼ˆCoTï¼‰çš„TTCæ–¹æ³•ã€‚å®ƒæ”¯æŒåœ¨å¤šä¸ªæ•°å­¦å’Œå¸¸è¯†æ¨ç†æ•°æ®é›†ä¸Šå¯¹ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚å„ä¸ªæ•°æ®é›†ä¸­çš„å°æ ·æœ¬æç¤ºå’Œç­”æ¡ˆæå–æµç¨‹å®ç°äº†æ ‡å‡†åŒ–ï¼Œå‡å°‘äº†ç ”ç©¶äººå‘˜çš„æ—¶é—´å’Œé‡‘é’±å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€é¡¹æˆæœ¬å»ºæ¨¡ç¨‹åºï¼Œè¯¥ç¨‹åºå¯ä»¥ä¼°ç®—æ¯ä¸ªæŸ¥è¯¢çš„ä»¤ç‰Œå’Œç¾å…ƒæˆæœ¬ï¼Œä¿ƒè¿›å…¬å¹³çš„TTCæ–¹æ³•æ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/networkslab/feval_ttc%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86FEval-TTC%E4%BE%9B%E5%85%AC%E4%BC%97%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/networkslab/feval_ttcä¸Šå…¬å¼€äº†FEval-TTCä¾›å…¬ä¼—ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01203v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶é—´è®¡ç®—ï¼ˆTTCï¼‰æ–¹æ³•çš„å…¬å¹³è¯„ä¼°åè®®FEval-TTCã€‚FEval-TTCæ—¨åœ¨ç¡®ä¿åœ¨ä¸åŒæ—¶é—´ä¸‹å¯¹TTCæ–¹æ³•çš„ä¸€è‡´æ€§è¯„ä»·ï¼Œå‡å°‘ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½åŠAPIè°ƒç”¨æˆæœ¬æ³¢åŠ¨å¸¦æ¥çš„å½±å“ã€‚å®ƒé€šè¿‡æ ‡å‡†åŒ–å°‘æ ·æœ¬æç¤ºå’Œç­”æ¡ˆæŠ½å–è¿‡ç¨‹ï¼Œæ”¯æŒè·¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šç§æ•°å­¦ä¸å¸¸è¯†æ¨ç†æ•°æ®é›†çš„è¯„ä»·ã€‚æ­¤å¤–ï¼ŒFEval-TTCè¿˜æä¾›æˆæœ¬å»ºæ¨¡ç¨‹åºï¼Œä¼°ç®—æ¯ä¸ªæŸ¥è¯¢çš„ä»¤ç‰Œå’Œç¾å…ƒæˆæœ¬ï¼Œä»¥ä¿ƒè¿›å¯¹å„ç§æµè¡ŒTTCæ–¹æ³•çš„å…¬å¹³æ¯”è¾ƒã€‚FEval-TTCå·²åœ¨ç½‘ç»œä¸Šå¼€æºä¾›å…¬ä¼—ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FEval-TTCæ˜¯ä¸ºäº†è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½æ³¢åŠ¨å’ŒAPIè°ƒç”¨æˆæœ¬å˜åŒ–å¸¦æ¥çš„é—®é¢˜è€Œè®¾è®¡çš„æµ‹è¯•æ—¶é—´è®¡ç®—å…¬å¹³è¯„ä¼°åè®®ã€‚</li>
<li>è¯¥åè®®æ”¯æŒè·¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šç§æ•°å­¦ä¸å¸¸è¯†æ¨ç†æ•°æ®é›†çš„è¯„ä»·ã€‚</li>
<li>FEval-TTCé€šè¿‡æ ‡å‡†åŒ–å°‘æ ·æœ¬æç¤ºå’Œç­”æ¡ˆæŠ½å–è¿‡ç¨‹æ¥ç¡®ä¿è¯„ä»·çš„ä¸€è‡´æ€§ã€‚</li>
<li>å®ƒæä¾›äº†ä¸€ä¸ªæˆæœ¬å»ºæ¨¡ç¨‹åºï¼Œå¯ä»¥ä¼°ç®—æ¯ä¸ªæŸ¥è¯¢çš„ä»¤ç‰Œå’Œç¾å…ƒæˆæœ¬ã€‚</li>
<li>FEval-TTCæœ‰åŠ©äºä¿ƒè¿›å¯¹å„ç§æµè¡Œæµ‹è¯•æ—¶é—´è®¡ç®—æ–¹æ³•çš„å…¬å¹³æ¯”è¾ƒã€‚</li>
<li>è¯¥åè®®æ—¨åœ¨å‡å°‘ç ”ç©¶è€…åœ¨è¯„ä»·å¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„æ—¶é—´å’Œé‡‘é’±æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-774daedbe30d69e7402b8a902638e8a9" align="middle">
<img src="https://picx.zhimg.com/v2-6cb616e3ec7f46d59c4ed7952c3d7383" align="middle">
<img src="https://picx.zhimg.com/v2-86a44308c0ebb66403310ea278a375b3" align="middle">
<img src="https://picx.zhimg.com/v2-3ae24c216e360e9ae40300a8229590c0" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ZoFia-Zero-Shot-Fake-News-Detection-with-Entity-Guided-Retrieval-and-Multi-LLM-Interaction"><a href="#ZoFia-Zero-Shot-Fake-News-Detection-with-Entity-Guided-Retrieval-and-Multi-LLM-Interaction" class="headerlink" title="ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and   Multi-LLM Interaction"></a>ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and   Multi-LLM Interaction</h2><p><strong>Authors:Lvhua Wu, Xuefeng Jiang, Sheng Sun, Tian Wen, Yuwei Wang, Min Liu</strong></p>
<p>The rapid spread of fake news threatens social stability and public trust, rendering its detection an imperative research priority. Although large language models (LLMs) excel at numerous natural language processing tasks with their remarkable contextual understanding and extensive prior knowledge, the time-bounded knowledge coverage and tendency for generating hallucination content reduce their reliability when handling fast-evolving news streams. Furthermore, models trained on existing static datasets also often lack the generalization needed for emerging news topics. To address these challenges, we propose ZoFia, a novel two-stage zero-shot fake news detection framework. First, we introduce Hierarchical Salience to quantify the importance of entities in the news content, and propose the SC-MMR algorithm to effectively select an informative and diverse set of keywords that serve as queries for retrieving up-to-date external evidence. Subsequently, a multi LLM interactive system, in which each agent assumes a distinct role, performs multi-view collaborative analysis and adversarial debate over the news text and its related information, and finally produces an interpretable and robust judgment. Comprehensive experiments on two public datasets demonstrate that ZoFia obviously outperforms existing zero-shot baselines and most of few-shot methods. Our codes will be open-sourced to facilitate related communities. </p>
<blockquote>
<p>è™šå‡æ–°é—»çš„å¿«é€Ÿä¼ æ’­å¨èƒç¤¾ä¼šç¨³å®šå’Œå…¬ä¼—ä¿¡ä»»ï¼Œå› æ­¤å…¶æ£€æµ‹æˆä¸ºè¿«åˆ‡çš„ç ”ç©¶é‡ç‚¹ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡­å€Ÿå‡ºè‰²çš„ä¸Šä¸‹æ–‡ç†è§£å’Œå¹¿æ³›çš„å‰æœŸçŸ¥è¯†ï¼Œåœ¨ä¼—å¤šçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶çŸ¥è¯†è¦†ç›–çš„æ—¶é—´é™åˆ¶å’Œäº§ç”Ÿå¹»è§‰å†…å®¹çš„å€¾å‘ï¼Œåœ¨å¤„ç†å¿«é€Ÿæ¼”å˜çš„æ–°é—»æµæ—¶é™ä½äº†å…¶å¯é æ€§ã€‚æ­¤å¤–ï¼Œåœ¨ç°æœ‰é™æ€æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹é€šå¸¸ç¼ºä¹æ–°å…´æ–°é—»è¯é¢˜æ‰€éœ€çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ZoFiaï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ä¸¤é˜¶æ®µé›¶æ ·æœ¬è™šå‡æ–°é—»æ£€æµ‹æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥åˆ†å±‚æ˜¾è‘—æ€§æ¥é‡åŒ–æ–°é—»å†…å®¹ä¸­å®ä½“çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºSC-MMRç®—æ³•æœ‰æ•ˆåœ°é€‰æ‹©ä¸€ç»„æœ‰ä¿¡æ¯é‡å’Œå¤šæ ·æ€§çš„å…³é”®è¯ï¼Œä½œä¸ºæ£€ç´¢æœ€æ–°å¤–éƒ¨è¯æ®çš„æŸ¥è¯¢ã€‚ç„¶åï¼Œä¸€ä¸ªå¤šLLMäº¤äº’ç³»ç»Ÿï¼Œå…¶ä¸­çš„æ¯ä¸ªä»£ç†æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼Œå¯¹æ–°é—»æ–‡æœ¬åŠå…¶ç›¸å…³ä¿¡æ¯è¿›è¡Œå¤šè§†è§’åä½œåˆ†æå’Œå¯¹æŠ—æ€§è¾©è®ºï¼Œå¹¶æœ€ç»ˆåšå‡ºå¯è§£é‡Šå’Œç¨³å¥çš„åˆ¤æ–­ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒZoFiaæ˜æ˜¾ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬åŸºå‡†çº¿å’Œå¤šæ•°å°‘æ ·æœ¬æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å°†å¼€æºï¼Œä»¥ä¾¿ç›¸å…³ç¤¾åŒºä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01188v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬æ¢è®¨çš„æ˜¯å‡æ–°é—»å¯¹ç¤¾ä¼šç¨³å®šå’Œå…¬ä¼—ä¿¡ä»»é€ æˆçš„å¨èƒåŠå…¶æ£€æµ‹çš„é‡è¦æ€§ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¿«é€Ÿå˜åŒ–çš„æ–°é—»æµæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´å—é™çš„çŸ¥è¯†è¦†ç›–å’Œäº§ç”Ÿå¹»è§‰å†…å®¹çš„å€¾å‘ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºZoFiaçš„ä¸¤é˜¶æ®µé›¶æ ·æœ¬å‡æ–°é—»æ£€æµ‹æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥åˆ†å±‚æ˜¾è‘—æ€§æ¥è¡¡é‡æ–°é—»å†…å®¹ä¸­å®ä½“çš„é‡è¦æ€§ï¼Œå¹¶åˆ©ç”¨SC-MMRç®—æ³•æœ‰æ•ˆåœ°é€‰æ‹©äº†æœ‰ä¿¡æ¯é‡ä¸”å¤šæ ·åŒ–çš„å…³é”®è¯é›†ä½œä¸ºæŸ¥è¯¢æ¥æ£€ç´¢æœ€æ–°çš„å¤–éƒ¨è¯æ®ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ªå¤šå¤§å‹è¯­è¨€æ¨¡å‹äº¤äº’ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå†…çš„æ¯ä¸ªä»£ç†éƒ½æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼Œå¯¹æ–°é—»æ–‡æœ¬å’Œç›¸å…³ä¿¡æ¯è¿›è¡Œå¤šè§†è§’ååŒåˆ†æå’Œå¯¹æŠ—æ€§è¾©è®ºï¼Œæœ€ç»ˆå¾—å‡ºå¯è§£é‡Šæ€§å’Œç¨³å¥æ€§çš„åˆ¤æ–­ã€‚å®éªŒè¡¨æ˜ï¼ŒZoFiaæ˜æ˜¾ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬åŸºçº¿ä»¥åŠå¤§å¤šæ•°å°‘æ ·æœ¬æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡æ–°é—»å¯¹ç¤¾ä¼šç¨³å®šå’Œå…¬ä¼—ä¿¡ä»»æ„æˆå¨èƒï¼Œæ£€æµ‹å‡æ–°é—»æ˜¯ä¸€é¡¹é‡è¦çš„ç ”ç©¶ä»»åŠ¡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¿«é€Ÿå˜åŒ–çš„æ–°é—»æµå¤„ç†ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´å—é™çš„çŸ¥è¯†è¦†ç›–å’Œç”Ÿæˆå¹»è§‰å†…å®¹çš„å€¾å‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºZoFiaçš„ä¸¤é˜¶æ®µé›¶æ ·æœ¬å‡æ–°é—»æ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ZoFiaé€šè¿‡å¼•å…¥åˆ†å±‚æ˜¾è‘—æ€§æ¥é‡åŒ–æ–°é—»å†…å®¹ä¸­å®ä½“çš„é‡è¦æ€§ï¼Œå¹¶åˆ©ç”¨SC-MMRç®—æ³•é€‰æ‹©å…³é”®è¯è¿›è¡Œå¤–éƒ¨è¯æ®æ£€ç´¢ã€‚</li>
<li>å¤šå¤§å‹è¯­è¨€æ¨¡å‹äº¤äº’ç³»ç»Ÿç”¨äºå¯¹æ–°é—»æ–‡æœ¬å’Œç›¸å…³ä¿¡æ¯è¿›è¡Œå¤šè§†è§’ååŒåˆ†æå’Œå¯¹æŠ—æ€§è¾©è®ºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒZoFiaåœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬åŸºçº¿å’Œå¤§å¤šæ•°å°‘æ ·æœ¬æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a8a8571c38b7cc9343a2e27a26ee240" align="middle">
<img src="https://picx.zhimg.com/v2-1838d9bc5828a88f022d44e700813d32" align="middle">
<img src="https://picx.zhimg.com/v2-6fc8b9185489378728e3867abc2b5184" align="middle">
<img src="https://picx.zhimg.com/v2-e8525920374effa98a2c2ff97746b7bb" align="middle">
<img src="https://picx.zhimg.com/v2-cf3802a3fcc1d84e0fef0d31c42376a1" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="None-To-Optima-in-Few-Shots-Bayesian-Optimization-with-MDP-Priors"><a href="#None-To-Optima-in-Few-Shots-Bayesian-Optimization-with-MDP-Priors" class="headerlink" title="None To Optima in Few Shots: Bayesian Optimization with MDP Priors"></a>None To Optima in Few Shots: Bayesian Optimization with MDP Priors</h2><p><strong>Authors:Diantong Li, Kyunghyun Cho, Chong Liu</strong></p>
<p>Bayesian Optimization (BO) is an efficient tool for optimizing black-box functions, but its theoretical guarantees typically hold in the asymptotic regime. In many critical real-world applications such as drug discovery or materials design, where each evaluation can be very costly and time-consuming, BO becomes impractical for many evaluations. In this paper, we introduce the Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization with remarkably few function evaluations. At the heart of our algorithmic design are Markov Decision Process (MDP) priors that model optimization trajectories from related source tasks, thereby capturing procedural knowledge on efficient optimization. We embed these MDP priors into a prior-fitted neural network and employ model-agnostic meta-learning for fast adaptation to new target tasks. Experiments on real-world Covid and Cancer benchmarks and hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms state-of-the-art methods by achieving high-quality solutions with significantly fewer evaluations, making it ready for practical deployment. </p>
<blockquote>
<p>è´å¶æ–¯ä¼˜åŒ–ï¼ˆBOï¼‰æ˜¯ä¼˜åŒ–é»‘ç®±å‡½æ•°çš„æœ‰æ•ˆå·¥å…·ï¼Œä½†å…¶ç†è®ºä¿è¯é€šå¸¸åœ¨æ¸è¿‘æƒ…å†µä¸‹æ‰æˆç«‹ã€‚åœ¨è®¸å¤šå…³é”®çš„å®é™…åº”ç”¨ä¸­ï¼Œå¦‚è¯ç‰©å‘ç°æˆ–ææ–™è®¾è®¡ï¼Œæ¯æ¬¡è¯„ä¼°éƒ½å¯èƒ½éå¸¸æ˜‚è´µå’Œè€—æ—¶ï¼Œå› æ­¤BOå¯¹äºè®¸å¤šè¯„ä¼°æ¥è¯´å˜å¾—ä¸åˆ‡å®é™…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†è¿‡ç¨‹ä¿¡æ¯è´å¶æ–¯ä¼˜åŒ–ï¼ˆProfBOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•è§£å†³äº†é»‘ç®±ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ä¸”å‡½æ•°è¯„ä¼°æ¬¡æ•°å¤§å¤§å‡å°‘ã€‚ç®—æ³•è®¾è®¡çš„æ ¸å¿ƒåœ¨äºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å…ˆéªŒï¼Œå®ƒæ¨¡æ‹Ÿäº†æ¥è‡ªç›¸å…³æºä»»åŠ¡çš„ä¼˜åŒ–è½¨è¿¹ï¼Œä»è€Œæ•è·äº†å…³äºé«˜æ•ˆä¼˜åŒ–çš„è¿‡ç¨‹çŸ¥è¯†ã€‚æˆ‘ä»¬å°†è¿™äº›MDPå…ˆéªŒåµŒå…¥åˆ°å…ˆéªŒæ‹Ÿåˆç¥ç»ç½‘ç»œä¸­ï¼Œå¹¶é‡‡ç”¨æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ æ¥å¿«é€Ÿé€‚åº”æ–°çš„ç›®æ ‡ä»»åŠ¡ã€‚åœ¨ç°å®ä¸–ç•Œçš„æ–°å† ç—…æ¯’å’Œç™Œç—‡åŸºå‡†æµ‹è¯•ä»¥åŠè¶…å‚æ•°è°ƒæ•´ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒProfBOå§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œé€šè¿‡æ˜¾è‘—å‡å°‘è¯„ä¼°æ¬¡æ•°æ¥å®ç°é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºå®é™…éƒ¨ç½²åšå¥½å‡†å¤‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01006v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è´å¶æ–¯ä¼˜åŒ–ï¼ˆBOï¼‰æ˜¯ä¼˜åŒ–é»‘ç®±å‡½æ•°çš„å·¥å…·ï¼Œä½†å…¶ç†è®ºä¿è¯é€šå¸¸åœ¨æ¸è¿‘çŠ¶æ€ä¸‹æˆç«‹ã€‚åœ¨ç°å®ä¸–ç•Œçš„è®¸å¤šå…³é”®åº”ç”¨ä¸­ï¼Œå¦‚è¯ç‰©å‘ç°æˆ–ææ–™è®¾è®¡ï¼Œæ¯æ¬¡è¯„ä¼°éƒ½éå¸¸æ˜‚è´µä¸”è€—æ—¶ï¼ŒBOå¾ˆéš¾è¿›è¡Œå¤šæ¬¡è¯„ä¼°ã€‚æœ¬æ–‡ä»‹ç»äº†Procedure-inFormed BOï¼ˆProfBOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•è§£å†³äº†é»‘ç®±ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å…·æœ‰éå¸¸å°‘çš„å‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚ç®—æ³•è®¾è®¡çš„æ ¸å¿ƒæ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å…ˆéªŒï¼Œå®ƒæ¨¡æ‹Ÿäº†æ¥è‡ªç›¸å…³æºä»»åŠ¡çš„ä¼˜åŒ–è½¨è¿¹ï¼Œä»è€Œæ•è·äº†é«˜æ•ˆçš„ä¼˜åŒ–è¿‡ç¨‹çŸ¥è¯†ã€‚æˆ‘ä»¬å°†è¿™äº›MDPå…ˆéªŒåµŒå…¥åˆ°å…ˆéªŒæ‹Ÿåˆç¥ç»ç½‘ç»œä¸­ï¼Œå¹¶åˆ©ç”¨æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ å¿«é€Ÿé€‚åº”æ–°çš„ç›®æ ‡ä»»åŠ¡ã€‚åœ¨ç°å®ä¸–ç•Œçš„æ–°å† è‚ºç‚ã€ç™Œç—‡åŸºå‡†æµ‹è¯•å’Œè¶…å‚æ•°è°ƒæ•´ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒProfBOå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œé€šè¿‡æ˜¾è‘—è¾ƒå°‘çš„è¯„ä¼°æ¬¡æ•°å®ç°é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºå®é™…åº”ç”¨éƒ¨ç½²åšå¥½å‡†å¤‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è´å¶æ–¯ä¼˜åŒ–ï¼ˆBOï¼‰æ˜¯ä¼˜åŒ–é»‘ç®±å‡½æ•°çš„å·¥å…·ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„è®¸å¤šåº”ç”¨ä¸­ï¼Œç”±äºå…¶ç†è®ºä¿è¯ä»…åœ¨æ¸è¿‘çŠ¶æ€ä¸‹æˆç«‹ï¼Œä¸”è¯„ä¼°æˆæœ¬é«˜æ˜‚ï¼Œå…¶å®ç”¨æ€§å—é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Procedure-inFormed BOï¼ˆProfBOï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡æ¨¡æ‹Ÿç›¸å…³æºä»»åŠ¡çš„ä¼˜åŒ–è½¨è¿¹æ¥æ•è·é«˜æ•ˆçš„ä¼˜åŒ–è¿‡ç¨‹çŸ¥è¯†ï¼Œä»è€Œè§£å†³äº†é»‘ç®±ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>ProfBOç®—æ³•çš„æ ¸å¿ƒæ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å…ˆéªŒï¼Œè¿™äº›å…ˆéªŒè¢«åµŒå…¥åˆ°å…ˆéªŒæ‹Ÿåˆç¥ç»ç½‘ç»œä¸­ã€‚</li>
<li>ProfBOåˆ©ç”¨æ¨¡å‹æ— å…³çš„å…ƒå­¦ä¹ å¿«é€Ÿé€‚åº”æ–°çš„ç›®æ ‡ä»»åŠ¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œåœ¨ç°å®ä¸–ç•Œçš„æ–°å† è‚ºç‚ã€ç™Œç—‡åŸºå‡†æµ‹è¯•å’Œè¶…å‚æ•°è°ƒæ•´ä»»åŠ¡ä¸Šï¼ŒProfBOä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>ProfBOèƒ½å¤Ÿä»¥æ˜¾è‘—æ›´å°‘çš„è¯„ä¼°æ¬¡æ•°å®ç°é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿å…¶é€‚åˆå®é™…åº”ç”¨éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3c35a4aa47ed85976186a24459ce335" align="middle">
<img src="https://picx.zhimg.com/v2-c206c5aeab7d007cebf3f8feb61a971e" align="middle">
<img src="https://picx.zhimg.com/v2-54823ce775b61d880d3dbae510c06928" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-Riddle-of-Reflection-Evaluating-Reasoning-and-Self-Awareness-in-Multilingual-LLMs-using-Indian-Riddles"><a href="#The-Riddle-of-Reflection-Evaluating-Reasoning-and-Self-Awareness-in-Multilingual-LLMs-using-Indian-Riddles" class="headerlink" title="The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in   Multilingual LLMs using Indian Riddles"></a>The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in   Multilingual LLMs using Indian Riddles</h2><p><strong>Authors:Abhinav P M, Ojasva Saxena, Oswald C, Parameswari Krishnamurthy</strong></p>
<p>The extent to which large language models (LLMs) can perform culturally grounded reasoning across non-English languages remains underexplored. This paper examines the reasoning and self-assessment abilities of LLMs across seven major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and Telugu. We introduce a multilingual riddle dataset combining traditional riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5 Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under seven prompting strategies. In the first stage, we assess riddle-solving performance and find that while Gemini 2.5 Pro performs best overall, few-shot methods yield only marginal gains, and accuracy varies notably across languages. In the second stage, we conduct a self-evaluation experiment to measure reasoning consistency. The results reveal a key finding: a modelâ€™s initial accuracy is inversely correlated with its ability to identify its own mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34% True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are substantially more self-aware (42.09% True Negative Rate). These results point to clear gaps in multilingual reasoning and highlight the need for models that not only reason effectively but also recognize their own limitations. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éè‹±è¯­ç¯å¢ƒä¸­è¿›è¡Œæ–‡åŒ–æ¨ç†çš„èƒ½åŠ›å°šæœªå®Œå…¨æ¢ç´¢ã€‚æœ¬æ–‡å¯¹LLMåœ¨å°åº¦ä¸»è¦ä¸ƒç§è¯­è¨€ï¼ˆå­ŸåŠ æ‹‰è¯­ã€å¤å‰æ‹‰ç‰¹è¯­ã€å°åœ°è¯­ã€åçº³æ‹‰è¯­ã€é©¬æ‹‰äºšæ‹‰å§†è¯­ã€æ³°ç±³å°”è¯­å’Œæ³°å¢å›ºè¯­ï¼‰çš„æ¨ç†å’Œè‡ªæˆ‘è¯„ä»·èƒ½åŠ›è¿›è¡Œäº†ç ”ç©¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šè¯­è¨€è°œé¢˜æ•°æ®é›†ï¼Œç»“åˆäº†ä¼ ç»Ÿè°œé¢˜å’Œä¸Šä¸‹æ–‡é‡æ„çš„å˜ä½“ï¼Œå¹¶è¯„ä¼°äº†äº”ç§LLMâ€”â€”Gemini 2.5 Proã€Gemini 2.5 Flashã€Mistral-Sabaã€LLaMA 4 Scoutå’ŒLLaMA 4 Maverickï¼Œé‡‡ç”¨ä¸ƒç§æç¤ºç­–ç•¥ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è¯„ä¼°äº†è§£è°œæ€§èƒ½ï¼Œå‘ç°è™½ç„¶Gemini 2.5 Proæ€»ä½“è¡¨ç°æœ€ä½³ï¼Œä½†å°‘æ ·æœ¬æ–¹æ³•åªå¸¦æ¥å¾®å¼±æå‡ï¼Œä¸”ä¸åŒè¯­è¨€ä¹‹é—´çš„å‡†ç¡®ç‡å·®å¼‚æ˜¾è‘—ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹è‡ªæˆ‘è¯„ä»·å®éªŒï¼Œä»¥è¡¡é‡æ¨ç†ä¸€è‡´æ€§ã€‚ç»“æœæ­ç¤ºäº†ä¸€ä¸ªå…³é”®å‘ç°ï¼šæ¨¡å‹çš„åˆå§‹å‡†ç¡®ç‡ä¸å…¶è¯†åˆ«è‡ªèº«é”™è¯¯çš„èƒ½åŠ›å‘ˆè´Ÿç›¸å…³ã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹å¦‚Gemini 2.5 Proè¿‡äºè‡ªä¿¡ï¼ˆçœŸé˜´æ€§ç‡ä¸º4.34%ï¼‰ï¼Œè€Œè¡¨ç°è¾ƒå·®çš„æ¨¡å‹å¦‚LLaMA 4 Scoutåˆ™æ›´å…·è‡ªæˆ‘æ„è¯†ï¼ˆçœŸé˜´æ€§ç‡ä¸º42.09%ï¼‰ã€‚è¿™äº›ç»“æœæŒ‡å‡ºäº†å¤šè¯­è¨€æ¨ç†çš„æ˜æ˜¾å·®è·ï¼Œå¹¶å¼ºè°ƒéœ€è¦æ¨¡å‹ä¸ä»…è¿›è¡Œæœ‰æ•ˆæ¨ç†ï¼Œè¿˜èƒ½è®¤è¯†åˆ°è‡ªèº«çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00960v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éè‹±è¯­æ–‡åŒ–èƒŒæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ç ”ç©¶å°šå¾…æ·±å…¥ã€‚æœ¬ç ”ç©¶é’ˆå¯¹å°åº¦ä¸ƒç§ä¸»è¦è¯­è¨€çš„LLMsçš„æ¨ç†å’Œè‡ªè¯„èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å­ŸåŠ æ‹‰è¯­ã€å¤å‰æ‹‰ç‰¹è¯­ã€å°åœ°è¯­ç­‰ã€‚é€šè¿‡å¼•å…¥å¤šè¯­è¨€è°œé¢˜æ•°æ®é›†ï¼Œå¯¹äº”æ¬¾LLMsè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬Gemini 2.5 Proç­‰åœ¨å†…çš„ä¸åŒæ¨¡å‹è¡¨ç°ä¸ä¸€ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡Gemini 2.5 Proæ€»ä½“è¡¨ç°æœ€ä½³ï¼Œä½†å°‘æ ·æœ¬æ–¹æ³•å¸¦æ¥çš„æå‡æœ‰é™ï¼Œä¸”å‡†ç¡®ç‡åœ¨ä¸åŒè¯­è¨€é—´å­˜åœ¨è¾ƒå¤§å·®å¼‚ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„åˆå§‹å‡†ç¡®ç‡ä¸å…¶è¯†åˆ«è‡ªèº«é”™è¯¯çš„èƒ½åŠ›å‘ˆè´Ÿç›¸å…³ï¼Œè¡¨æ˜å½“å‰LLMsåœ¨å¤šè¯­ç§æ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œéœ€è¦æ—¢èƒ½å¤Ÿé«˜æ•ˆæ¨ç†åˆèƒ½è¯†åˆ«è‡ªèº«å±€é™æ€§çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éè‹±è¯­æ–‡åŒ–èƒŒæ™¯ä¸‹çš„æ¨ç†èƒ½åŠ›ç ”ç©¶å°šä¸å……åˆ†ã€‚</li>
<li>åœ¨å°åº¦ä¸ƒç§ä¸»è¦è¯­è¨€ç¯å¢ƒä¸‹å¯¹LLMsçš„æ¨ç†å’Œè‡ªè¯„èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>Gemini 2.5 Proåœ¨æ€»ä½“è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†å°‘æ ·æœ¬æ–¹æ³•çš„æå‡æœ‰é™ã€‚</li>
<li>ä¸åŒè¯­è¨€é—´LLMsçš„å‡†ç¡®ç‡å­˜åœ¨è¾ƒå¤§å·®å¼‚ã€‚</li>
<li>æ¨¡å‹çš„åˆå§‹å‡†ç¡®ç‡ä¸å…¶è¯†åˆ«è‡ªèº«é”™è¯¯çš„èƒ½åŠ›å‘ˆè´Ÿç›¸å…³ã€‚</li>
<li>å½“å‰LLMsåœ¨å¤šè¯­ç§æ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0910223d4745c8f3ad0c1e596d3fa27a" align="middle">
<img src="https://picx.zhimg.com/v2-5e598c99dbe574e27beb3344136abe00" align="middle">
<img src="https://picx.zhimg.com/v2-7c7ec9ec730a4fe9a98e4831a7babbd7" align="middle">
<img src="https://picx.zhimg.com/v2-88420b26a7330b0ef513634d2a29bd06" align="middle">
<img src="https://picx.zhimg.com/v2-76ab6151bea85d1a0917ee815b25a4a1" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Real-IAD-Variety-Pushing-Industrial-Anomaly-Detection-Dataset-to-a-Modern-Era"><a href="#Real-IAD-Variety-Pushing-Industrial-Anomaly-Detection-Dataset-to-a-Modern-Era" class="headerlink" title="Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a   Modern Era"></a>Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a   Modern Era</h2><p><strong>Authors:Wenbing Zhu, Chengjie Wang, Bin-Bin Gao, Jiangning Zhang, Guannan Jiang, Jie Hu, Zhenye Gan, Lidong Wang, Ziqing Zhou, Linjie Cheng, Yurui Pan, Bo Peng, Mingmin Chi, Lizhuang Ma</strong></p>
<p>Industrial Anomaly Detection (IAD) is critical for enhancing operational safety, ensuring product quality, and optimizing manufacturing efficiency across global industries. However, the IAD algorithms are severely constrained by the limitations of existing public benchmarks. Current datasets exhibit restricted category diversity and insufficient scale, frequently resulting in metric saturation and limited model transferability to real-world scenarios. To address this gap, we introduce Real-IAD Variety, the largest and most diverse IAD benchmark, comprising 198,960 high-resolution images across 160 distinct object categories. Its diversity is ensured through comprehensive coverage of 28 industries, 24 material types, and 22 color variations. Our comprehensive experimental analysis validates the benchmarkâ€™s substantial challenge: state-of-the-art multi-class unsupervised anomaly detection methods experience significant performance degradation when scaled from 30 to 160 categories. Crucially, we demonstrate that vision-language models exhibit remarkable robustness to category scale-up, with minimal performance variation across different category counts, significantly enhancing generalization capabilities in diverse industrial contexts. The unprecedented scale and complexity of Real-IAD Variety position it as an essential resource for training and evaluating next-generation foundation models for anomaly detection. By providing this comprehensive benchmark with rigorous evaluation protocols across multi-class unsupervised, multi-view, and zero-&#x2F;few-shot settings, we aim to accelerate research beyond domain-specific constraints, enabling the development of scalable, general-purpose anomaly detection systems. Real-IAD Variety will be made publicly available to facilitate innovation in this critical field. </p>
<blockquote>
<p>å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰å¯¹äºæé«˜å…¨çƒå·¥ä¸šçš„è¿è¡Œå®‰å…¨ã€ç¡®ä¿äº§å“è´¨é‡ä»¥åŠä¼˜åŒ–åˆ¶é€ æ•ˆç‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒIADç®—æ³•å—åˆ°ç°æœ‰å…¬å…±åŸºå‡†æµ‹è¯•çš„é™åˆ¶ã€‚å½“å‰æ•°æ®é›†è¡¨ç°å‡ºç±»åˆ«å¤šæ ·æ€§æœ‰é™å’Œè§„æ¨¡ä¸è¶³çš„é—®é¢˜ï¼Œç»å¸¸å¯¼è‡´æŒ‡æ ‡é¥±å’Œä»¥åŠæ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„å¯è¿ç§»æ€§å—é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Real-IAD Varietyï¼Œè¿™æ˜¯æœ€å¤§ä¸”æœ€å…·å¤šæ ·æ€§çš„IADåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«160ä¸ªä¸åŒå¯¹è±¡ç±»åˆ«çš„198,960å¼ é«˜åˆ†è¾¨ç‡å›¾åƒã€‚å…¶å¤šæ ·æ€§é€šè¿‡æ¶µç›–28ä¸ªè¡Œä¸šã€24ç§ææ–™ç±»å‹å’Œ22ç§é¢œè‰²å˜åŒ–å¾—åˆ°ä¿éšœã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒåˆ†æéªŒè¯äº†è¯¥åŸºå‡†æµ‹è¯•å…·æœ‰å¾ˆå¤§æŒ‘æˆ˜æ€§ï¼šå½“ä»30ä¸ªç±»åˆ«æ‰©å±•åˆ°160ä¸ªç±»åˆ«æ—¶ï¼Œæœ€å…ˆè¿›çš„å¤šå…ƒæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜è§†è§‰è¯­è¨€æ¨¡å‹å¯¹ç±»åˆ«æ‰©å±•å…·æœ‰æƒŠäººçš„ç¨³å¥æ€§ï¼Œä¸åŒç±»åˆ«æ•°é‡ä¸‹çš„æ€§èƒ½å˜åŒ–æå°ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†åœ¨ä¸åŒå·¥ä¸šç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚Real-IAD Varietyå‰æ‰€æœªæœ‰çš„è§„æ¨¡å’Œå¤æ‚æ€§ä½¿å…¶æˆä¸ºè®­ç»ƒå’Œè¯„ä¼°ä¸‹ä¸€ä»£å¼‚å¸¸æ£€æµ‹åŸºç¡€æ¨¡å‹çš„é‡è¦èµ„æºã€‚é€šè¿‡æä¾›è¿™ä¸€å…¨é¢çš„åŸºå‡†æµ‹è¯•ä»¥åŠè·¨å¤šç±»æ— ç›‘ç£ã€å¤šè§†è§’å’Œé›¶æ ·æœ¬&#x2F;å°æ ·æœ¬è®¾ç½®çš„ä¸¥æ ¼è¯„ä¼°åè®®ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŠ é€Ÿçªç ´é¢†åŸŸç‰¹å®šçº¦æŸçš„ç ”ç©¶ï¼Œæ¨åŠ¨å¼€å‘å¯æ‰©å±•çš„ã€é€šç”¨çš„å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿã€‚Real-IAD Varietyå°†å…¬å¼€æä¾›ï¼Œä»¥ä¿ƒè¿›è¿™ä¸€å…³é”®é¢†åŸŸçš„åˆ›æ–°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00540v1">PDF</a> 13 pages, 4 figures and 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰å…¬å…±åŸºå‡†æ•°æ®çš„å±€é™æ€§ï¼Œæå‡ºReal-IAD VarietyåŸºå‡†æ•°æ®ï¼ŒåŒ…å«198,960å¼ é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ¶µç›–160ä¸ªä¸åŒå¯¹è±¡ç±»åˆ«ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼Œè¯¥åŸºå‡†æ•°æ®å¯¹å¤šç±»æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•æ„æˆæŒ‘æˆ˜ï¼Œè€Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç±»åˆ«æ‰©å±•ä¸Šè¡¨ç°å‡ºå“è¶Šç¨³å¥æ€§ã€‚Real-IAD Varietyä¸ºå¼‚å¸¸æ£€æµ‹ä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†å…³é”®èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰å¯¹äºæé«˜æ“ä½œå®‰å…¨ã€ç¡®ä¿äº§å“è´¨é‡å’Œä¼˜åŒ–åˆ¶é€ æ•ˆç‡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰IADç®—æ³•å—åˆ°å…¬å…±åŸºå‡†æ•°æ®çš„é™åˆ¶ï¼Œéœ€è¦æ›´å¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚</li>
<li>Real-IAD Varietyæ˜¯æœ€å¤§ä¸”æœ€å¤šæ ·åŒ–çš„IADåŸºå‡†æ•°æ®ï¼ŒåŒ…å«198,960å¼ å›¾åƒï¼Œæ¶µç›–160ä¸ªä¸åŒå¯¹è±¡ç±»åˆ«ï¼Œç¡®ä¿æ¶µç›–28ä¸ªè¡Œä¸šã€24ç§ææ–™å’Œ22ç§é¢œè‰²å˜åŒ–ã€‚</li>
<li>ç°æœ‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•é¢ä¸´ä»30åˆ°160ç±»åˆ«çš„æ‰©å±•æŒ‘æˆ˜ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç±»åˆ«æ‰©å±•ä¸Šè¡¨ç°å‡ºå“è¶Šç¨³å¥æ€§ï¼Œå¢å¼ºåœ¨ä¸åŒå·¥ä¸šèƒŒæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Real-IAD Varietyä¸ºå¼‚å¸¸æ£€æµ‹çš„ä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†å…³é”®èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccc5e7c828088ce72bb2b04615e564d1" align="middle">
<img src="https://picx.zhimg.com/v2-b4fb9bf3b29aa9901d66217e4afebf9c" align="middle">
<img src="https://picx.zhimg.com/v2-5b2e2c461ccc5edc1dbd8488d2ebd675" align="middle">
<img src="https://picx.zhimg.com/v2-fc18d659d88ee5f558617d6266df6afc" align="middle">
<img src="https://picx.zhimg.com/v2-88a2ae8a504ce3f8529ed243ba524517" align="middle">
<img src="https://picx.zhimg.com/v2-9f7fddf70e397841468c4674e2cff25e" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling"><a href="#Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling" class="headerlink" title="Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling"></a>Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling</h2><p><strong>Authors:Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi</strong></p>
<p>Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual&#x2F;textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL">https://github.com/bryanwong17/HiVE-MIL</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ€è¿‘å·²è¢«çº³å…¥å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³å¯¹å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œå°‘é‡ã€å¼±ç›‘ç£åˆ†ç±»çš„æŒ‘æˆ˜ã€‚ä¸€ç§å…³é”®è¶‹åŠ¿æ˜¯è¿ç”¨å¤šå°ºåº¦ä¿¡æ¯æ¥æ›´å¥½åœ°è¡¨ç¤ºå±‚æ¬¡åŒ–çš„ç»„ç»‡ç»“æ„ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰åŒä¸€æ¨¡æ€å†…ä¸åŒå°ºåº¦ï¼ˆä¾‹å¦‚ï¼Œ5å€å’Œ20å€ï¼‰ä¹‹é—´äº¤äº’çš„å»ºæ¨¡ä¸è¶³ï¼›ï¼ˆ2ï¼‰åŒä¸€å°ºåº¦ä¸Šè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´å¯¹é½ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†HiVE-MILï¼Œè¿™æ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œå®ƒæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€å›¾ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰ç²—å°ºåº¦ï¼ˆ5å€ï¼‰å’Œç»†å°ºåº¦ï¼ˆ20å€ï¼‰è§†è§‰&#x2F;æ–‡æœ¬èŠ‚ç‚¹ä¹‹é—´çš„çˆ¶å­é“¾æ¥ï¼Œä»¥æ•æ‰å±‚æ¬¡å…³ç³»ï¼Œä»¥åŠï¼ˆ2ï¼‰åŒä¸€å°ºåº¦ä¸Šè¿æ¥è§†è§‰å’Œæ–‡æœ¬èŠ‚ç‚¹çš„å¼‚æ„å°ºåº¦å†…è¾¹ç¼˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ï¼ŒHiVE-MILé‡‡ç”¨äº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ–‡æœ¬å¼•å¯¼åŠ¨æ€è¿‡æ»¤æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥æ¶ˆé™¤å¼±ç›¸å…³çš„è¡¥ä¸-æ–‡æœ¬å¯¹ï¼Œå¹¶å¼•å…¥å±‚æ¬¡å¯¹æ¯”æŸå¤±ä»¥å¯¹é½ä¸åŒå°ºåº¦çš„æ–‡æœ¬è¯­ä¹‰ã€‚åœ¨TCGAä¹³è…ºç™Œã€è‚ºç™Œå’Œè‚¾ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHiVE-MILå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„MILæ–¹æ³•å’Œæœ€æ–°çš„åŸºäºVLMçš„MILæ–¹æ³•ï¼Œåœ¨16ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå®F1å¾—åˆ†æé«˜äº†é«˜è¾¾4.1%ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†è”åˆå»ºæ¨¡å±‚æ¬¡ç»“æ„å’Œå¤šæ¨¡æ€å¯¹é½å¯¹äºä»æœ‰é™çš„ç—…ç†æ•°æ®ä¸­å®ç°é«˜æ•ˆå’Œå¯æ‰©å±•å­¦ä¹ çš„ä»·å€¼ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bryanwong17/HiVE-MILä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17982v4">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong><br>     è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è§†é‡å±‚çº§åŒ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆHiVE-MILï¼‰ï¼Œç”¨äºè§£å†³å°‘æ•°ç—…ä¾‹çš„å¼±ç›‘ç£åˆ†ç±»é—®é¢˜ã€‚HiVE-MILæ¨¡å‹ç»“åˆäº†å¤šå°ºåº¦ä¿¡æ¯ï¼Œå»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„å›¾è°±ç»“æ„ï¼Œé€šè¿‡æ„å»ºç²—å°ºåº¦ï¼ˆå¦‚5å€æ”¾å¤§ï¼‰å’Œç²¾ç»†å°ºåº¦ï¼ˆå¦‚20å€æ”¾å¤§ï¼‰ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ï¼Œå®ç°äº†è·¨å°ºåº¦çš„äº’åŠ¨å»ºæ¨¡ä»¥åŠè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å¯¹é½ã€‚è¯¥æ¨¡å‹è¿˜é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„æ–‡æœ¬å¼•å¯¼åŠ¨æ€è¿‡æ»¤æœºåˆ¶å’Œå±‚æ¬¡å¯¹æ¯”æŸå¤±ï¼Œä»¥æé«˜è¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨å¤šä¸ªç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHiVE-MILæ¨¡å‹åœ¨å®è§‚F1å¾—åˆ†ä¸Šä¼˜äºä¼ ç»Ÿçš„MILæ¨¡å‹å’Œæœ€æ–°çš„VLM-based MILæ–¹æ³•ï¼Œåœ¨16ä¸ªæ ·æœ¬ç‚¹è®¾ç½®ä¸‹æå‡äº†é«˜è¾¾4.1%ã€‚è¿™ä¸€ç ”ç©¶å±•ç¤ºäº†è”åˆå»ºæ¨¡å±‚æ¬¡ç»“æ„å’Œå¤šæ¨¡æ€å¯¹é½å¯¹äºæœ‰é™ç—…ç†å­¦æ•°æ®çš„æœ‰æ•ˆæ€§å’Œå¯ä¼¸ç¼©æ€§å­¦ä¹ çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HiVE-MILç»“åˆäº†å¤šå°ºåº¦ä¿¡æ¯æ¥è§£å†³å°‘æ•°ç—…ä¾‹çš„å¼±ç›‘ç£åˆ†ç±»é—®é¢˜ã€‚</li>
<li>æ¨¡å‹å»ºç«‹äº†ç»Ÿä¸€çš„å›¾è°±ç»“æ„ï¼ŒåŒ…æ‹¬è·¨å°ºåº¦çš„çˆ¶å­é“¾æ¥å’ŒåŒä¸€å°ºåº¦ä¸Šçš„å¼‚ç§å†…å°ºåº¦è¾¹ç¼˜ï¼Œä»¥æ•æ‰è§†è§‰å’Œæ–‡æœ¬èŠ‚ç‚¹ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µçš„æ–‡æœ¬å¼•å¯¼åŠ¨æ€è¿‡æ»¤æœºåˆ¶å’Œå±‚æ¬¡å¯¹æ¯”æŸå¤±æé«˜è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>æ¨¡å‹è§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­çš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šåŒä¸€å°ºåº¦å†…æ¨¡æ€é—´äº¤äº’å»ºæ¨¡ä¸è¶³ä»¥åŠè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€é—´çš„ä¸å¯¹é½é—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHiVE-MILæ¨¡å‹æ€§èƒ½ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨å®è§‚F1å¾—åˆ†ä¸Šã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜è”åˆå»ºæ¨¡å±‚æ¬¡ç»“æ„å’Œå¤šæ¨¡æ€å¯¹é½å¯¹äºæœ‰æ•ˆå’Œå¯æ‰©å±•çš„å­¦ä¹ è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯å¯¹äºæœ‰é™çš„ç—…ç†å­¦æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-caa3f82f5b89a42ecd165d4f304f3ac8" align="middle">
<img src="https://picx.zhimg.com/v2-c1fedc76669865d94d175ae9b63f7901" align="middle">
<img src="https://picx.zhimg.com/v2-3ae16c1376802116af05fc563edae129" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Words-That-Unite-The-World-A-Unified-Framework-for-Deciphering-Central-Bank-Communications-Globally"><a href="#Words-That-Unite-The-World-A-Unified-Framework-for-Deciphering-Central-Bank-Communications-Globally" class="headerlink" title="Words That Unite The World: A Unified Framework for Deciphering Central   Bank Communications Globally"></a>Words That Unite The World: A Unified Framework for Deciphering Central   Bank Communications Globally</h2><p><strong>Authors:Agam Shah, Siddhant Sukhani, Huzaifa Pardawala, Saketh Budideti, Riya Bhadani, Rudra Gopal, Siddhartha Somani, Rutwik Routu, Michael Galarnyk, Soungmin Lee, Arnav Hiray, Akshar Ravichandran, Eric Kim, Pranav Aluru, Joshua Zhang, Sebastian Jaskowski, Veer Guda, Meghaj Tarte, Liqin Ye, Spencer Gosden, Rachel Yuh, Sloka Chava, Sahasra Chava, Dylan Patrick Kelly, Aiden Chiang, Harsit Mittal, Sudheer Chava</strong></p>
<p>Central banks around the world play a crucial role in maintaining economic stability. Deciphering policy implications in their communications is essential, especially as misinterpretations can disproportionately impact vulnerable populations. To address this, we introduce the World Central Banks (WCB) dataset, the most comprehensive monetary policy corpus to date, comprising over 380k sentences from 25 central banks across diverse geographic regions, spanning 28 years of historical data. After uniformly sampling 1k sentences per bank (25k total) across all available years, we annotate and review each sentence using dual annotators, disagreement resolutions, and secondary expert reviews. We define three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation, with each sentence annotated for all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on these tasks, running 15,075 benchmarking experiments. We find that a model trained on aggregated data across banks significantly surpasses a model trained on an individual bankâ€™s data, confirming the principle â€œthe whole is greater than the sum of its parts.â€ Additionally, rigorous human evaluations, error analyses, and predictive tasks validate our frameworkâ€™s economic utility. Our artifacts are accessible through the HuggingFace and GitHub under the CC-BY-NC-SA 4.0 license. </p>
<blockquote>
<p>ä¸–ç•Œå„åœ°çš„ä¸­å¤®é“¶è¡Œåœ¨ç»´æŒç»æµç¨³å®šæ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è§£è¯»å…¶å…¬å‘Šä¸­çš„æ”¿ç­–å†…æ¶µè‡³å…³é‡è¦ï¼Œå› ä¸ºè¯¯è§£å¯èƒ½ä¼šç»™è„†å¼±äººç¾¤å¸¦æ¥ä¸æˆæ¯”ä¾‹çš„å½±å“ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å…¨çƒä¸­å¤®é“¶è¡Œï¼ˆWCBï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å…¨é¢çš„è´§å¸æ”¿ç­–è¯­æ–™åº“ï¼ŒåŒ…å«æ¥è‡ª25ä¸ªä¸­å¤®é“¶è¡Œä¸åŒåœ°ç†åŒºåŸŸçš„è¶…è¿‡38ä¸‡å¥è¯ï¼Œæ¶µç›–28å¹´çš„å†å²æ•°æ®ã€‚é€šè¿‡å¯¹æ¯ä¸ªé“¶è¡Œå‡åŒ€æŠ½æ ·1000å¥è¯ï¼ˆæ€»å…±25000å¥ï¼‰ï¼Œæ¶µç›–æ‰€æœ‰å¯ç”¨å¹´ä»½ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä½æ³¨é‡Šè€…ã€äº‰è®®è§£å†³æ–¹æ¡ˆå’ŒäºŒæ¬¡ä¸“å®¶è¯„å®¡æ¥å¯¹æ¯ä¸€å¥è¯è¿›è¡Œæ³¨é‡Šå’Œè¯„å®¡ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸‰ä¸ªä»»åŠ¡ï¼šç«‹åœºæ£€æµ‹ã€æ—¶é—´åˆ†ç±»å’Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ¯ä¸ªå¥å­éƒ½ä¼šè¢«æ³¨é‡Šè¿™ä¸‰ä¸ªæ–¹é¢çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¯¹ä¸ƒä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰å’Œä¹ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ˆé›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¸¦æ³¨é‡ŠæŒ‡å—ï¼‰è¿›è¡Œäº†è¿™äº›ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œå…±è¿›è¡Œäº†15075æ¬¡åŸºå‡†æµ‹è¯•å®éªŒã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºè·¨é“¶è¡Œèšåˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºä»…åŸºäºå•ä¸ªé“¶è¡Œæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œè¿™è¯å®äº†â€œæ•´ä½“å¤§äºéƒ¨åˆ†ä¹‹å’Œâ€çš„åŸåˆ™ã€‚æ­¤å¤–ï¼Œä¸¥æ ¼çš„äººç±»è¯„ä¼°ã€è¯¯å·®åˆ†æå’Œé¢„æµ‹ä»»åŠ¡éªŒè¯äº†æˆ‘ä»¬æ¡†æ¶çš„ç»æµå®ç”¨æ€§ã€‚æˆ‘ä»¬çš„æˆæœå¯ä»¥é€šè¿‡HuggingFaceå’ŒGitHubä»¥CC-BY-NC-SA 4.0è®¸å¯è¯è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17048v2">PDF</a> Accepted at NeurIPS 2025 (main conference)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸–ç•Œä¸­å¤®é“¶è¡Œï¼ˆWCBï¼‰æ•°æ®é›†çš„é‡è¦æ€§å’Œä½œç”¨ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªä¸åŒåœ°ç†åŒºåŸŸçš„25å®¶ä¸­å¤®é“¶è¡Œçš„æ•°æ®ï¼Œç”¨äºè§£è¯»è´§å¸æ”¿ç­–ä¸­çš„æ”¿ç­–å«ä¹‰ã€‚è¯¥æ•°æ®é›†æ¶µç›–å¤šå¹´å†å²æ•°æ®ï¼Œå¹¶è¿›è¡Œäº†è¯¦å°½çš„æ ‡æ³¨å’Œå®¡æŸ¥ã€‚åŒæ—¶ä»‹ç»äº†é’ˆå¯¹è¯¥æ•°æ®é›†çš„ä¸‰ä¸ªä»»åŠ¡åŠå¯¹è¯¥æ•°æ®é›†çš„åŸºå‡†æµ‹è¯•æƒ…å†µã€‚æ•´ä½“ä¸Šï¼Œé€šè¿‡â€œæ•´ä½“ä¼˜äºéƒ¨åˆ†ä¹‹å’Œâ€çš„åŸåˆ™è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹æ€§èƒ½æ›´å¥½ã€‚åŒæ—¶æ–‡ç« æä¾›äº†è¯¦ç»†çš„è®¸å¯è¯ä¸‹å¯ä¾›å…¬ä¼—ä½¿ç”¨çš„æ¸ é“ã€‚æ­¤æ‘˜è¦åŒ…å«äº†å…³é”®ä¿¡æ¯ï¼Œç®€æ´æ˜äº†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36fcdfc66df316fd4de8cc16d1057a06" align="middle">
<img src="https://picx.zhimg.com/v2-149ede2be9867087c8ced049aa72a864" align="middle">
<img src="https://picx.zhimg.com/v2-9634bd60790379af6585750982754e76" align="middle">
<img src="https://picx.zhimg.com/v2-30ab766fc85fdcff7951455a33fe6a06" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AutoPDL-Automatic-Prompt-Optimization-for-LLM-Agents"><a href="#AutoPDL-Automatic-Prompt-Optimization-for-LLM-Agents" class="headerlink" title="AutoPDL: Automatic Prompt Optimization for LLM Agents"></a>AutoPDL: Automatic Prompt Optimization for LLM Agents</h2><p><strong>Authors:Claudio Spiess, Mandana Vaziri, Louis Mandel, Martin Hirzel</strong></p>
<p>The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.21\pm15.46$ percentage points), up to 67.5pp, and reveal that selected prompting strategies vary across models and tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å–å†³äºå¦‚ä½•å¯¹å…¶è¿›è¡Œæç¤ºï¼Œæç¤ºçš„é€‰æ‹©èŒƒå›´åŒ…æ‹¬é«˜çº§æç¤ºæ¨¡å¼ï¼ˆä¾‹å¦‚Zero-Shotã€CoTã€ReActã€ReWOOï¼‰å’Œç‰¹å®šæç¤ºå†…å®¹ï¼ˆæŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼‰ã€‚æ‰‹åŠ¨è°ƒæ•´è¿™ç§ç»“åˆæ–¹å¼æ—¢ç¹çåˆå®¹æ˜“å‡ºç°é”™è¯¯ï¼Œè€Œä¸”ä»…é™äºç‰¹å®šçš„è¯­è¨€æ¨¡å‹å’Œä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AutoPDLï¼Œä¸€ç§å‘ç°è‰¯å¥½è¯­è¨€æ¨¡å‹ä»£ç†é…ç½®çš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å…¶æ„å»ºä¸ºä¸€ä¸ªç»“æ„åŒ–è‡ªåŠ¨æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œæ¶‰åŠä»£ç†å’Œéä»£ç†æç¤ºæ¨¡å¼çš„ç»„åˆç©ºé—´ï¼Œå¹¶ä½¿ç”¨è¿ç»­å‡åŠæ³•æœ‰æ•ˆåœ°éå†è¿™ä¸ªç©ºé—´ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä½¿ç”¨PDLæç¤ºç¼–ç¨‹è¯­è¨€çš„åº“æ¥å®ç°å¸¸è§çš„æç¤ºæ¨¡å¼ã€‚AutoPDLè§£å†³æ–¹æ¡ˆæ˜¯å¯è¯»ã€å¯ç¼–è¾‘å’Œå¯æ‰§è¡Œçš„PDLç¨‹åºï¼Œä½¿ç”¨æ­¤åº“ã€‚è¿™ç§æ–¹æ³•è¿˜å®ç°äº†æºåˆ°æºçš„ä¼˜åŒ–ï¼Œå…è®¸äººç±»å‚ä¸å¾ªç¯ä¼˜åŒ–å’Œé‡ç”¨ã€‚åœ¨ä¸‰ä¸ªä»»åŠ¡å’Œä¸ƒä¸ªè¯­è¨€æ¨¡å‹ï¼ˆä»3Båˆ°70Bå‚æ•°ï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå‡†ç¡®ç‡æŒç»­æé«˜ï¼ˆ9.21Â±15.46ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œæœ€é«˜è¾¾67.5ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¸”è¡¨æ˜æ‰€é€‰çš„æç¤ºç­–ç•¥å› æ¨¡å‹å’Œä»»åŠ¡è€Œå¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04365v5">PDF</a> An earlier version of this paper was published in AutoML 2025 Methods   Track. This version adds missing standard deviations in Table 1</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å–å†³äºå¦‚ä½•æç¤ºå®ƒä»¬ï¼ŒåŒ…æ‹¬é«˜çº§æç¤ºæ¨¡å¼ï¼ˆå¦‚é›¶æ ·æœ¬ã€CoTã€ReActã€ReWOOï¼‰å’Œç‰¹å®šæç¤ºå†…å®¹ï¼ˆæŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼‰ã€‚æ‰‹åŠ¨è°ƒæ•´è¿™ç§ç»„åˆå¾ˆç¹çï¼Œä¸”æ˜“å‡ºé”™ï¼Œé’ˆå¯¹ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†AutoPDLï¼Œä¸€ç§ç”¨äºå‘ç°è‰¯å¥½å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†é…ç½®çš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚å°†è¿™ä¸ªé—®é¢˜è¡¨è¿°ä¸ºä¸€ä¸ªç»“æ„åŒ–è‡ªåŠ¨æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œåœ¨ä»£ç†å’Œéä»£ç†æç¤ºæ¨¡å¼å’Œæ¼”ç¤ºçš„ç»„åˆç©ºé—´ä¸­é«˜æ•ˆå¯¼èˆªã€‚å¼•å…¥ä¸€ä¸ªä½¿ç”¨PDLæç¤ºç¼–ç¨‹è¯­è¨€çš„å¸¸è§æç¤ºæ¨¡å¼åº“ã€‚AutoPDLè§£å†³æ–¹æ¡ˆæ˜¯å¯è¯»ã€å¯ç¼–è¾‘å’Œå¯æ‰§è¡Œçš„äººç±»å¯è¯»PDLç¨‹åºï¼Œä½¿ç”¨æ­¤åº“ã€‚è¿™ç§æ–¹æ³•è¿˜å®ç°äº†æºåˆ°æºçš„ä¼˜åŒ–ï¼Œå…è®¸äººç±»å‚ä¸æ”¹è¿›å’Œé‡ç”¨ã€‚åœ¨ä¸‰ä¸ªä»»åŠ¡å’Œä¸ƒä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆä»3Båˆ°70Bå‚æ•°ï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå‡†ç¡®ç‡æœ‰æŒç»­çš„æé«˜ï¼ˆ9.21Â±15.46ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œæœ€é«˜è¾¾67.5ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”é€‰æ‹©çš„æç¤ºç­–ç•¥å› æ¨¡å‹å’Œä»»åŠ¡è€Œå¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å—æç¤ºæ–¹å¼å½±å“ï¼ŒåŒ…æ‹¬é«˜çº§æç¤ºæ¨¡å¼å’Œå…·ä½“æç¤ºå†…å®¹ã€‚</li>
<li>æ‰‹åŠ¨è°ƒæ•´æç¤ºç»„åˆæ—¢ç¹çåˆæ˜“å‡ºé”™ï¼Œéœ€è¦é’ˆå¯¹ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•AutoPDLï¼Œç”¨äºå‘ç°è‰¯å¥½çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†é…ç½®ã€‚</li>
<li>å°†é—®é¢˜è¡¨è¿°ä¸ºç»“æ„åŒ–è‡ªåŠ¨æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œåœ¨ä»£ç†å’Œéä»£ç†æç¤ºæ¨¡å¼å’Œæ¼”ç¤ºçš„ç»„åˆç©ºé—´ä¸­é«˜æ•ˆå¯¼èˆªã€‚</li>
<li>å¼•å…¥äº†ä½¿ç”¨PDLæç¤ºç¼–ç¨‹è¯­è¨€çš„å¸¸è§æç¤ºæ¨¡å¼åº“ï¼Œä½¿è§£å†³æ–¹æ¡ˆæ›´åŠ äººç±»å¯è¯»ã€å¯ç¼–è¾‘å’Œå¯æ‰§è¡Œã€‚</li>
<li>AutoPDLå®ç°äº†æºåˆ°æºçš„ä¼˜åŒ–ï¼Œå…è®¸äººç±»çš„å‚ä¸æ”¹è¿›å’Œé‡ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe59ec4e59c3ee8f4ac31727e862b2fe" align="middle">
<img src="https://picx.zhimg.com/v2-fdb37e2d07509aafb24da166e397e2c4" align="middle">
<img src="https://picx.zhimg.com/v2-1b71f89fd39f141bf09b3fd3839a7e68" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-are-Unreliable-for-Cyber-Threat-Intelligence"><a href="#Large-Language-Models-are-Unreliable-for-Cyber-Threat-Intelligence" class="headerlink" title="Large Language Models are Unreliable for Cyber Threat Intelligence"></a>Large Language Models are Unreliable for Cyber Threat Intelligence</h2><p><strong>Authors:Emanuele Mezzi, Fabio Massacci, Katja Tuma</strong></p>
<p>Several recent works have argued that Large Language Models (LLMs) can be used to tame the data deluge in the cybersecurity field, by improving the automation of Cyber Threat Intelligence (CTI) tasks. This work presents an evaluation methodology that other than allowing to test LLMs on CTI tasks when using zero-shot learning, few-shot learning and fine-tuning, also allows to quantify their consistency and their confidence level. We run experiments with three state-of-the-art LLMs and a dataset of 350 threat intelligence reports and present new evidence of potential security risks in relying on LLMs for CTI. We show how LLMs cannot guarantee sufficient performance on real-size reports while also being inconsistent and overconfident. Few-shot learning and fine-tuning only partially improve the results, thus posing doubts about the possibility of using LLMs for CTI scenarios, where labelled datasets are lacking and where confidence is a fundamental factor. </p>
<blockquote>
<p>è¿‘æœŸæœ‰å‡ é¡¹ç ”ç©¶æŒ‡å‡ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡æå‡ç½‘ç»œå¨èƒæƒ…æŠ¥ï¼ˆCTIï¼‰ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ç¨‹åº¦ï¼Œæ¥åº”å¯¹ç½‘ç»œå®‰å…¨é¢†åŸŸçš„æ•°æ®æ´ªæµã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è¯„ä¼°æ–¹æ³•ï¼Œé™¤äº†æµ‹è¯•CTIä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬å­¦ä¹ ã€å°‘æ ·æœ¬å­¦ä¹ å’Œå¾®è°ƒä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…å†µå¤–ï¼Œè¿˜å…è®¸å¯¹å…¶ä¸€è‡´æ€§å’Œç½®ä¿¡æ°´å¹³è¿›è¡Œé‡åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„ä¸‰ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŒ…å«350ä»½å¨èƒæƒ…æŠ¥æŠ¥å‘Šçš„æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œå¹¶æä¾›äº†ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒCTIå·¥ä½œå¯èƒ½å¸¦æ¥çš„æ½œåœ¨å®‰å…¨é£é™©çš„æ–°è¯æ®ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹æ— æ³•ä¿è¯åœ¨çœŸå®å¤§å°çš„æŠ¥å‘Šä¸Šå®ç°è¶³å¤Ÿæ€§èƒ½çš„åŒæ—¶ï¼Œè¿˜å­˜åœ¨ä¸ä¸€è‡´å’Œè¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ã€‚å°‘æ ·æœ¬å­¦ä¹ å’Œå¾®è°ƒåªèƒ½éƒ¨åˆ†æ”¹å–„ç»“æœï¼Œå› æ­¤å¯¹ç¼ºä¹æ ‡ç­¾æ•°æ®é›†ä¸”ç½®ä¿¡åº¦æ˜¯åŸºæœ¬å› ç´ çš„CTIåœºæ™¯ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯è¡Œæ€§æå‡ºäº†è´¨ç–‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23175v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨å—åˆ°å…³æ³¨ï¼Œå¯ä»¥æ”¹å–„ç½‘ç»œå¨èƒæƒ…æŠ¥ï¼ˆCTIï¼‰ä»»åŠ¡çš„è‡ªåŠ¨åŒ–å¤„ç†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯„ä¼°æ–¹æ³•ï¼Œä¸ä»…å¯ä»¥æµ‹è¯•LLMsåœ¨CTIä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å­¦ä¹ ã€å°‘æ ·æœ¬å­¦ä¹ å’Œå¾®è°ƒï¼Œè¿˜å¯ä»¥é‡åŒ–å…¶ä¸€è‡´æ€§å’Œç½®ä¿¡æ°´å¹³ã€‚é€šè¿‡å¯¹ä¸‰æ¬¾æœ€å…ˆè¿›çš„LLMså’ŒåŒ…å«350ä»½å¨èƒæƒ…æŠ¥æŠ¥å‘Šçš„æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œæœ¬æ–‡æä¾›äº†ä¾èµ–LLMsè¿›è¡ŒCTIå¯èƒ½å­˜åœ¨çš„æ½œåœ¨å®‰å…¨é£é™©çš„æ–°è¯æ®ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMsæ— æ³•ä¿è¯åœ¨çœŸå®æŠ¥å‘Šä¸Šçš„æ€§èƒ½ï¼Œå­˜åœ¨ä¸ä¸€è‡´å’Œè¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ã€‚å°‘æ ·æœ¬å­¦ä¹ å’Œå¾®è°ƒåªèƒ½éƒ¨åˆ†æ”¹å–„ç»“æœï¼Œå› æ­¤å¯¹ç¼ºä¹æ ‡ç­¾æ•°æ®é›†å’Œéœ€è¦é«˜åº¦è‡ªä¿¡çš„CTIåœºæ™¯çš„ä½¿ç”¨æå‡ºäº†è´¨ç–‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œèƒ½å¤Ÿæ”¹å–„ç½‘ç»œå¨èƒæƒ…æŠ¥ï¼ˆCTIï¼‰ä»»åŠ¡çš„è‡ªåŠ¨åŒ–å¤„ç†ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯„ä¼°LLMsåœ¨CTIä»»åŠ¡ä¸­è¡¨ç°çš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å­¦ä¹ ã€å°‘æ ·æœ¬å­¦ä¹ å’Œå¾®è°ƒï¼Œå¹¶èƒ½é‡åŒ–å…¶ä¸€è‡´æ€§å’Œç½®ä¿¡æ°´å¹³ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºLLMsåœ¨å¤„ç†çœŸå®è§„æ¨¡çš„æŠ¥å‘Šæ—¶è¡¨ç°ä¸è¶³ï¼Œå­˜åœ¨æ€§èƒ½ä¸ç¨³å®šå’Œè¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ã€‚</li>
<li>å°‘æ ·æœ¬å­¦ä¹ å’Œå¾®è°ƒæ–¹æ³•åªèƒ½éƒ¨åˆ†æ”¹å–„LLMsåœ¨CTIä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>ä¾èµ–LLMsè¿›è¡ŒCTIå­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚</li>
<li>åœ¨ç¼ºä¹æ ‡ç­¾æ•°æ®é›†å’Œéœ€è¦é«˜åº¦è‡ªä¿¡çš„CTIåœºæ™¯ä¸­ï¼Œä½¿ç”¨LLMsçš„å¯è¡Œæ€§å—åˆ°è´¨ç–‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60e2986eb49e17815e66ec963e78c808" align="middle">
<img src="https://picx.zhimg.com/v2-2f09f2a4805e047f3da6ef0b4083bdd5" align="middle">
<img src="https://picx.zhimg.com/v2-af5d344f0ae4a311395ef1cea544dedd" align="middle">
<img src="https://picx.zhimg.com/v2-1c89fcc068e7c1ffbf639e5f8d2142e3" align="middle">
<img src="https://picx.zhimg.com/v2-c8d906f4c4843b4192a114eb0ab7e4f7" align="middle">
<img src="https://picx.zhimg.com/v2-33793409f9ee94e61e8929294c5abd56" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Multi-Step-Reasoning-with-Large-Language-Models-a-Survey"><a href="#Multi-Step-Reasoning-with-Large-Language-Models-a-Survey" class="headerlink" title="Multi-Step Reasoning with Large Language Models, a Survey"></a>Multi-Step Reasoning with Large Language Models, a Survey</h2><p><strong>Authors:Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back</strong></p>
<p>Large language models (LLMs) with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks. The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This article reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods use reinforcement learning for finetuning, external optimization loops, in-context reinforcement learning, and self-reflection. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰æ•°åäº¿å‚æ•°ï¼Œå±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æœªä¸“é—¨è®­ç»ƒçš„ä»»åŠ¡ä¸Šå®ç°å°‘é‡å­¦ä¹ ã€‚ä¼ ç»Ÿæ¨¡å‹åœ¨è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œä½†åœ¨åŸºæœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å¹¶ä¸å‡ºè‰²ã€‚ç„¶è€Œï¼Œä¸€ç§æ–°çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•â€”â€”æ€ç»´é“¾ï¼ˆChain-of-thoughtï¼‰åœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚å…³äºLLMæ¨ç†èƒ½åŠ›çš„ç ”ç©¶å§‹äºLLMæ˜¯å¦èƒ½è§£å†³å°å­¦æ•°å­¦åº”ç”¨é¢˜çš„é—®é¢˜ï¼Œå¹¶åœ¨è¿‡å»å‡ å¹´ä¸­æ‰©å±•åˆ°äº†å…¶ä»–ä»»åŠ¡ã€‚æœ¬æ–‡ç»¼è¿°äº†ä½¿ç”¨LLMè¿›è¡Œå¤šæ­¥æ¨ç†çš„é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†ç±»æ³•ï¼Œç¡®å®šäº†ç”Ÿæˆã€è¯„ä¼°å’Œæ§åˆ¶å¤šæ­¥æ¨ç†çš„ä¸åŒæ–¹æ³•ã€‚æˆ‘ä»¬å¯¹æ ¸å¿ƒæ–¹æ³•å’Œå¼€æ”¾é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œå¹¶æå‡ºäº†è¿‘æœŸçš„ç ”ç©¶è®®ç¨‹ã€‚æˆ‘ä»¬å‘ç°å¤šæ­¥æ¨ç†æ–¹æ³•å·²ç»è¶…è¶Šäº†æ•°å­¦åº”ç”¨é¢˜ï¼Œç°åœ¨èƒ½å¤ŸæˆåŠŸè§£å†³é€»è¾‘ã€ç»„åˆæ¸¸æˆå’Œæœºå™¨äººæŠ€æœ¯ç­‰æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæœ‰æ—¶æ˜¯å…ˆç”Ÿæˆä»£ç ï¼Œç„¶åå€ŸåŠ©å¤–éƒ¨å·¥å…·æ‰§è¡Œã€‚å¤šæ­¥æ–¹æ³•çš„ç ”ç©¶ç»å¸¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒã€å¤–éƒ¨ä¼˜åŒ–å¾ªç¯ã€ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ å’Œè‡ªæˆ‘åæ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11511v3">PDF</a> ACM Computing Surveys</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½åœ¨æœªç»è¿‡ä¸“é—¨è®­ç»ƒçš„ä»»åŠ¡ä¸­å®ç°å°‘æ ·æœ¬å­¦ä¹ ã€‚ä¼ ç»Ÿæ¨¡å‹åœ¨è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†åœ¨åŸºæœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸€ç§åä¸ºâ€œæ€ç»´é“¾â€çš„æ–°ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å›é¡¾äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šæ­¥æ¨ç†çš„ç ”ç©¶é¢†åŸŸï¼Œæå‡ºäº†ç”Ÿæˆã€è¯„ä¼°å’Œæ§åˆ¶ç³»ç»Ÿå¤šæ­¥æ¨ç†çš„ä¸åŒæ–¹å¼çš„åˆ†ç±»ï¼Œæ·±å…¥æ¢è®¨äº†æ ¸å¿ƒæ–¹æ³•å’Œå¼€æ”¾é—®é¢˜ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶è®®ç¨‹ã€‚å‘ç°å¤šæ­¥æ¨ç†æ–¹æ³•ä¸ä»…é™äºè§£å†³æ•°å­¦æ–‡å­—é¢˜ï¼Œè¿˜èƒ½æˆåŠŸåº”å¯¹é€»è¾‘ã€ç»„åˆæ¸¸æˆå’Œæœºå™¨äººç­‰é¢†åŸŸçš„æŒ‘æˆ˜ï¼Œæœ‰æ—¶é€šè¿‡é¦–å…ˆç”Ÿæˆä»£ç ï¼Œç„¶åå€ŸåŠ©å¤–éƒ¨å·¥å…·æ‰§è¡Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¯åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹å®Œæˆä»»åŠ¡ã€‚</li>
<li>ä¼ ç»Ÿæ¨¡å‹åœ¨åŸºæœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
<li>â€œæ€ç»´é“¾â€æ–¹æ³•å±•ç¤ºäº†å¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤šæ­¥æ¨ç†æ–¹æ³•ä¸ä»…é™äºè§£å†³æ•°å­¦æ–‡å­—é¢˜ï¼Œè¿˜èƒ½åº”ç”¨äºé€»è¾‘ã€ç»„åˆæ¸¸æˆå’Œæœºå™¨äººç­‰é¢†åŸŸã€‚</li>
<li>å¤šæ­¥æ¨ç†æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒã€å¤–éƒ¨ä¼˜åŒ–å¾ªç¯ã€ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ å’Œè‡ªæˆ‘åæ€ç­‰æŠ€æœ¯ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ç”Ÿæˆã€è¯„ä¼°å’Œæ§åˆ¶ç³»ç»Ÿå¤šæ­¥æ¨ç†çš„ä¸åŒæ–¹å¼çš„åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da90ec754461d8502efbf129dd3b6088" align="middle">
<img src="https://picx.zhimg.com/v2-5a3063c434242f531700399fab730876" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9c819698da2c06973cc68e3e0d3c31c6" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  PLUTO-4 Frontier Pathology Foundation Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-194c4f64f1884c14982d69753cb2f1ba" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  1 PoCo Agentic Proof-of-Concept Exploit Generation for Smart Contracts
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
