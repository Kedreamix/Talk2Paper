<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Diffusion Models are Robust Pretrainers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-40850e19634e7e0e853ad21db3af08c1')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    58 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-06-æ›´æ–°"><a href="#2025-11-06-æ›´æ–°" class="headerlink" title="2025-11-06 æ›´æ–°"></a>2025-11-06 æ›´æ–°</h1><h2 id="Diffusion-Models-are-Robust-Pretrainers"><a href="#Diffusion-Models-are-Robust-Pretrainers" class="headerlink" title="Diffusion Models are Robust Pretrainers"></a>Diffusion Models are Robust Pretrainers</h2><p><strong>Authors:Mika Yagoda, Shady Abu-Hussein, Raja Giryes</strong></p>
<p>Diffusion models have gained significant attention for high-fidelity image generation. Our work investigates the potential of exploiting diffusion models for adversarial robustness in image classification and object detection. Adversarial attacks challenge standard models in these tasks by perturbing inputs to force incorrect predictions. To address this issue, many approaches use training schemes for forcing the robustness of the models, which increase training costs. In this work, we study models built on top of off-the-shelf diffusion models and demonstrate their practical significance: they provide a low-cost path to robust representations, allowing lightweight heads to be trained on frozen features without full adversarial training. Our empirical evaluations on ImageNet, CIFAR-10, and PASCAL VOC show that diffusion-based classifiers and detectors achieve meaningful adversarial robustness with minimal compute. While clean and adversarial accuracies remain below state-of-the-art adversarially trained CNNs or ViTs, diffusion pretraining offers a favorable tradeoff between efficiency and robustness. This work opens a promising avenue for integrating diffusion models into resource-constrained robust deployments. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆé¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æˆ‘ä»¬çš„å·¥ä½œç ”ç©¶äº†å°†æ‰©æ•£æ¨¡å‹ç”¨äºå›¾åƒåˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ä¸­çš„å¯¹æŠ—æ€§ç¨³å¥æ€§çš„æ½œåŠ›ã€‚å¯¹æŠ—æ€§æ”»å‡»é€šè¿‡æ‰°åŠ¨è¾“å…¥æ¥æŒ‘æˆ˜è¿™äº›ä»»åŠ¡çš„æ ‡å‡†æ¨¡å‹ï¼Œä»è€Œè¿«ä½¿æ¨¡å‹åšå‡ºé”™è¯¯çš„é¢„æµ‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®¸å¤šæ–¹æ³•ä½¿ç”¨è®­ç»ƒæ–¹æ¡ˆæ¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ï¼Œè¿™å¢åŠ äº†è®­ç»ƒæˆæœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºç°æˆæ‰©æ•£æ¨¡å‹çš„æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬çš„å®é™…æ„ä¹‰ï¼šå®ƒä»¬ä¸ºç¨³å¥è¡¨ç¤ºæä¾›äº†ä½æˆæœ¬é€”å¾„ï¼Œå…è®¸åœ¨å†»ç»“çš„ç‰¹å¾ä¸Šè®­ç»ƒè½»é‡çº§å¤´éƒ¨ï¼Œè€Œæ— éœ€è¿›è¡Œå…¨é¢çš„å¯¹æŠ—æ€§è®­ç»ƒã€‚æˆ‘ä»¬åœ¨ImageNetã€CIFAR-10å’ŒPASCAL VOCä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„åˆ†ç±»å™¨å’Œæ£€æµ‹å™¨å®ç°äº†æœ‰æ„ä¹‰çš„å¯¹æŠ—æ€§ç¨³å¥æ€§ï¼Œè®¡ç®—é‡æå°ã€‚è™½ç„¶æ¸…æ´å’Œå¯¹æŠ—æ€§ç²¾åº¦ä»ä½äºæœ€å…ˆè¿›çš„å¯¹æŠ—æ€§è®­ç»ƒçš„CNNæˆ–ViTsï¼Œä½†æ‰©æ•£é¢„è®­ç»ƒåœ¨æ•ˆç‡å’Œç¨³å¥æ€§ä¹‹é—´æä¾›äº†æœ‰åˆ©çš„æƒè¡¡ã€‚è¿™é¡¹å·¥ä½œä¸ºå°†æ‰©æ•£æ¨¡å‹é›†æˆåˆ°èµ„æºå—é™çš„ç¨³å¥éƒ¨ç½²ä¸­æ‰“å¼€äº†æœ‰å‰é€”çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02793v1">PDF</a> To be published in IEEE Signal Processing Letters</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ä¸­çš„å¯¹æŠ—ç¨³å¥æ€§æ½œåŠ›ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒåŸºäºè´§æ¶ä¸Šçš„æ‰©æ•£æ¨¡å‹æ„å»ºæ¨¡å‹å…·æœ‰å®é™…æ„ä¹‰ï¼Œå®ƒä»¬æä¾›äº†ä¸€ç§å®ç°ç¨³å¥è¡¨ç¤ºçš„ä½æˆæœ¬é€”å¾„ï¼Œå…è®¸åœ¨å†»ç»“çš„ç‰¹å¾ä¸Šè®­ç»ƒè½»é‡çº§å¤´éƒ¨ï¼Œæ— éœ€å…¨é¢çš„å¯¹æŠ—æ€§è®­ç»ƒã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„åˆ†ç±»å™¨å’Œæ£€æµ‹å™¨å®ç°äº†æœ‰æ„ä¹‰çš„å¯¹æŠ—æ€§ç¨³å¥æ€§ï¼Œè®¡ç®—é‡å°ã€‚è™½ç„¶æ¸…æ´å’Œå¯¹æŠ—æ€§å‡†ç¡®ç‡ä»ä½äºç»è¿‡å¯¹æŠ—æ€§è®­ç»ƒçš„CNNæˆ–ViTsçš„å½“å‰æœ€ä½³æ°´å¹³ï¼Œä½†æ‰©æ•£é¢„è®­ç»ƒåœ¨æ•ˆç‡å’Œç¨³å¥æ€§ä¹‹é—´æä¾›äº†æœ‰åˆ©çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å—åˆ°é«˜åº¦å…³æ³¨ã€‚</li>
<li>å¯¹æŠ—æ”»å‡»æŒ‘æˆ˜äº†å›¾åƒåˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹çš„æ ‡å‡†æ¨¡å‹ã€‚</li>
<li>é€šè¿‡åœ¨è´§æ¶ä¸Šçš„æ‰©æ•£æ¨¡å‹æ„å»ºæ¨¡å‹å¯ä»¥æé«˜æ¨¡å‹çš„å¯¹æŠ—ç¨³å¥æ€§ã€‚</li>
<li>å†»ç»“ç‰¹å¾ä¸Šçš„è½»é‡çº§å¤´éƒ¨è®­ç»ƒå¯ä»¥å®ç°ç¨³å¥è¡¨ç¤ºè€Œæ— éœ€å…¨å¯¹æŠ—è®­ç»ƒã€‚</li>
<li>åŸºäºæ‰©æ•£çš„åˆ†ç±»å™¨å’Œæ£€æµ‹å™¨åœ¨ä¿è¯å¯¹æŠ—æ€§ç¨³å¥æ€§çš„åŒæ—¶ï¼Œè®¡ç®—é‡è¾ƒå°ã€‚</li>
<li>è™½ç„¶å‡†ç¡®ç‡å°šä½äºæœ€å…ˆè¿›çš„å¯¹æŠ—è®­ç»ƒæ¨¡å‹ï¼Œä½†æ‰©æ•£é¢„è®­ç»ƒæä¾›äº†æ•ˆç‡å’Œç¨³å¥æ€§çš„æœ‰åˆ©æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5dd680485a0bb6adde50b70fecb3c7e" align="middle">
<img src="https://picx.zhimg.com/v2-c6b910eb9f27027eef20f80d48ffe4cb" align="middle">
<img src="https://picx.zhimg.com/v2-f6fdf7cdcde187d6bd513979951d5c0d" align="middle">
<img src="https://picx.zhimg.com/v2-e8b639eb9d818e324107bb81082e0efb" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Watermarking-Discrete-Diffusion-Language-Models"><a href="#Watermarking-Discrete-Diffusion-Language-Models" class="headerlink" title="Watermarking Discrete Diffusion Language Models"></a>Watermarking Discrete Diffusion Language Models</h2><p><strong>Authors:Avi Bagchi, Akhil Bhimaraju, Moulik Choraria, Daniel Alabi, Lav R. Varshney</strong></p>
<p>Watermarking has emerged as a promising technique to track AI-generated content and differentiate it from authentic human creations. While prior work extensively studies watermarking for autoregressive large language models (LLMs) and image diffusion models, none address discrete diffusion language models, which are becoming popular due to their high inference throughput. In this paper, we introduce the first watermarking method for discrete diffusion models by applying the distribution-preserving Gumbel-max trick at every diffusion step and seeding the randomness with the sequence index to enable reliable detection. We experimentally demonstrate that our scheme is reliably detectable on state-of-the-art diffusion language models and analytically prove that it is distortion-free with an exponentially decaying probability of false detection in the token sequence length. </p>
<blockquote>
<p>æ°´å°æŠ€æœ¯å·²æˆä¸ºè¿½è¸ªäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹å¹¶å°†å…¶ä¸çœŸå®äººç±»åˆ›ä½œåŒºåˆ†å¼€æ¥çš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œå¯¹è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå›¾åƒæ‰©æ•£æ¨¡å‹çš„æ°´å°è¿›è¡Œäº†å¹¿æ³›ç ”ç©¶ï¼Œä½†æ²¡æœ‰é’ˆå¯¹ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ï¼Œç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹ç”±äºå…¶é«˜æ¨ç†ååé‡è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­åº”ç”¨ä¿æŒåˆ†å¸ƒçš„å¤å§†è´å°”-æœ€å¤§æŠ€å·§ï¼Œå¹¶ä»¥åºåˆ—ç´¢å¼•ä¸ºéšæœºæ€§æä¾›ç§å­ï¼Œä»è€Œå®ç°äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„é¦–ä¸ªæ°´å°æ–¹æ³•ï¼Œä»¥å¯é åœ°æ£€æµ‹æ°´å°ã€‚æˆ‘ä»¬å®éªŒè¯æ˜æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨å…ˆè¿›æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸Šèƒ½å¯é æ£€æµ‹æ°´å°ï¼Œå¹¶é€šè¿‡åˆ†æè¯æ˜å®ƒæ˜¯æ— å¤±çœŸçš„ï¼Œåœ¨ä»¤ç‰Œåºåˆ—é•¿åº¦ä¸­è¯¯æ£€çš„æ¦‚ç‡å‘ˆæŒ‡æ•°è¡°å‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02083v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æ°´å°æŠ€æœ¯ã€‚é€šè¿‡åº”ç”¨åˆ†å¸ƒä¿æŒçš„Gumbel-maxæŠ€å·§ï¼Œåœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­æ¤å…¥æ°´å°ï¼Œå¹¶åˆ©ç”¨åºåˆ—ç´¢å¼•ä¸ºéšæœºæ€§æä¾›ç§å­ï¼Œå®ç°äº†åœ¨ç¦»æ•£æ‰©æ•£æ¨¡å‹ä¸­çš„å¯é æ°´å°åµŒå…¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å…ˆè¿›çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸Šå¯æ£€æµ‹ï¼Œä¸”ç»è¿‡åˆ†æè¯æ˜å…¶æ— å¤±çœŸï¼Œåœ¨ä»¤ç‰Œåºåˆ—é•¿åº¦ä¸Šå‘ˆæŒ‡æ•°è¡°å‡çš„è¯¯æ£€æ¦‚ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´å°æŠ€æœ¯ç”¨äºè¿½è¸ªAIç”Ÿæˆå†…å®¹å¹¶åŒºåˆ†å…¶å’ŒçœŸå®äººç±»åˆ›ä½œã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹å’Œå›¾åƒæ‰©æ•£æ¨¡å‹çš„æ°´å°æŠ€æœ¯ä¸Šã€‚</li>
<li>ç¦»æ•£æ‰©æ•£æ¨¡å‹å› é«˜æ¨ç†ååé‡è€Œå—æ¬¢è¿ï¼Œä½†ç¼ºä¹æ°´å°æŠ€æœ¯ç ”ç©¶ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡ä¸ºç¦»æ•£æ‰©æ•£æ¨¡å‹å¼•å…¥æ°´å°æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨åˆ†å¸ƒä¿æŒçš„Gumbel-maxæŠ€å·§åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­åµŒå…¥æ°´å°ã€‚</li>
<li>é€šè¿‡åºåˆ—ç´¢å¼•ä¸ºéšæœºæ€§æä¾›ç§å­ï¼Œå®ç°å¯é æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e154ea6ef0a7bffbcfef5202204704fa" align="middle">
<img src="https://picx.zhimg.com/v2-01207478f127e833cf2b8d676d1c744b" align="middle">
<img src="https://picx.zhimg.com/v2-677d51f7a5b60eb05caf4817e81f8028" align="middle">
<img src="https://picx.zhimg.com/v2-5e3ab211218d49d69223bc003b43b1c7" align="middle">
<img src="https://picx.zhimg.com/v2-3114b45c4e0e28a8f5ee391ee1459244" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Wonder3D-Cross-domain-Diffusion-for-High-fidelity-3D-Generation-from-a-Single-Image"><a href="#Wonder3D-Cross-domain-Diffusion-for-High-fidelity-3D-Generation-from-a-Single-Image" class="headerlink" title="Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from   a Single Image"></a>Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from   a Single Image</h2><p><strong>Authors:Yuxiao Yang, Xiao-Xiao Long, Zhiyang Dou, Cheng Lin, Yuan Liu, Qingsong Yan, Yuexin Ma, Haoqian Wang, Zhiqiang Wu, Wei Yin</strong></p>
<p>In this work, we introduce \textbf{Wonder3D++}, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about $3$ minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works. Code available at <a target="_blank" rel="noopener" href="https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus">https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†**Wonder3D++**ï¼Œè¿™æ˜¯ä¸€ç§ä»å•è§†å›¾å›¾åƒé«˜æ•ˆç”Ÿæˆé«˜ä¿çœŸçº¹ç†ç½‘æ ¼çš„æ–°æ–¹æ³•ã€‚æœ€è¿‘åŸºäºå¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰çš„æ–¹æ³•å·²æ˜¾ç¤ºå‡ºä»äºŒç»´æ‰©æ•£å…ˆéªŒæ¢å¤ä¸‰ç»´å‡ ä½•çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸é¢ä¸´è€—æ—¶ä¸”é’ˆå¯¹æ¯ä¸ªå½¢çŠ¶çš„ä¼˜åŒ–ä»¥åŠå‡ ä½•ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒæŸäº›å·¥ä½œé€šè¿‡å¿«é€Ÿç½‘ç»œæ¨ç†ç›´æ¥äº§ç”Ÿä¸‰ç»´ä¿¡æ¯ï¼Œä½†å…¶ç»“æœå¾€å¾€è´¨é‡è¾ƒä½ä¸”ç¼ºä¹å‡ ä½•ç»†èŠ‚ã€‚ä¸ºäº†å…¨é¢æé«˜å•è§†å›¾é‡å»ºä»»åŠ¡çš„è´¨é‡ã€ä¸€è‡´æ€§å’Œæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨åŸŸæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆå¤šè§†å›¾æ³•çº¿è´´å›¾å’Œç›¸åº”çš„å½©è‰²å›¾åƒã€‚ä¸ºç¡®ä¿ç”Ÿæˆçš„è¿è´¯æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å¤šè§†å›¾è·¨åŸŸæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¿ƒè¿›äº†è·¨è§†å›¾å’Œè·¨æ¨¡æ€çš„ä¿¡æ¯äº¤æ¢ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çº§è”ä¸‰ç»´ç½‘æ ¼æå–ç®—æ³•ï¼Œè¯¥ç®—æ³•ä»¥ç²—ç•¥åˆ°ç²¾ç»†çš„æ–¹å¼ä»å¤šè§†å›¾äºŒç»´è¡¨ç¤ºä¸­é©±åŠ¨é«˜è´¨é‡è¡¨é¢ï¼Œä»…çº¦3åˆ†é’Ÿå³å¯å®Œæˆã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»¥å‰çš„å·¥ä½œç›¸æ¯”ï¼Œå®ç°äº†é«˜è´¨é‡çš„é‡å»ºç»“æœã€ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›å’Œè‰¯å¥½çš„æ•ˆç‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xxlong">https://github.com/xxlong</a> è®¿é—®ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ¨¡å‹å‘½åä¸ºWonder3D+ï¼Œåœ¨xxlongçš„Wonder3Dæ ‘ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01767v1">PDF</a> 21 pages, 19 figures, accepted by TPAMI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Wonder3D++æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»å•è§†å›¾å›¾åƒé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çº¹ç†ç½‘æ ¼ã€‚è¯¥æ–¹æ³•ç»“åˆScore Distillation Samplingï¼ˆSDSï¼‰çš„ä¼˜åŠ¿ï¼Œè§£å†³äº†ç°æœ‰æŠ€æœ¯ä¸­æ—¶é—´æ¶ˆè€—å¤§ã€å½¢çŠ¶ä¼˜åŒ–ä¸ä¸€è‡´çš„é—®é¢˜ã€‚é€šè¿‡è·¨åŸŸæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šè§†è§’æ³•çº¿å›¾åŠå¯¹åº”çš„å½©è‰²å›¾åƒï¼Œç¡®ä¿ç”Ÿæˆçš„è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨çº§è”çš„3Dç½‘æ ¼æå–ç®—æ³•ï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼ä»å¤šè§†è§’çš„äºŒç»´è¡¨ç¤ºä¸­å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è¡¨é¢ã€‚ç›¸æ¯”ç°æœ‰æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºè´¨é‡ã€é€šç”¨æ€§å’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Wonder3D++æ˜¯ä¸€ç§ä»å•è§†å›¾å›¾åƒç”Ÿæˆé«˜è´¨é‡çº¹ç†ç½‘æ ¼çš„æ–°æ–¹æ³•ã€‚</li>
<li>ç»“åˆScore Distillation Samplingï¼ˆSDSï¼‰çš„ä¼˜åŠ¿ï¼Œè§£å†³äº†æ—¶é—´æ¶ˆè€—å¤§ã€å½¢çŠ¶ä¼˜åŒ–ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡è·¨åŸŸæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šè§†è§’æ³•çº¿å›¾åŠå½©è‰²å›¾åƒï¼Œç¡®ä¿ç”Ÿæˆçš„è¿è´¯æ€§ã€‚</li>
<li>å¼•å…¥è·¨åŸŸæ³¨æ„åŠ›æœºåˆ¶ä¿ƒè¿›ä¸åŒè§†è§’å’Œæ¨¡æ€é—´çš„ä¿¡æ¯äº¤æµã€‚</li>
<li>é‡‡ç”¨çº§è”çš„3Dç½‘æ ¼æå–ç®—æ³•ï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡è¡¨é¢ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨é‡å»ºè´¨é‡ã€é€šç”¨æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37877a19c0389336052e281c6c9e5b03" align="middle">
<img src="https://picx.zhimg.com/v2-04bf346657622b3852a6a0932eb652fb" align="middle">
<img src="https://picx.zhimg.com/v2-48721280b800a0aa472098f66a757873" align="middle">
<img src="https://picx.zhimg.com/v2-e491e567d0d1a8f1b40516b6370d818c" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback"><a href="#UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback" class="headerlink" title="UniLumos: Fast and Unified Image and Video Relighting with   Physics-Plausible Feedback"></a>UniLumos: Fast and Unified Image and Video Relighting with   Physics-Plausible Feedback</h2><p><strong>Authors:Ropeway Liu, Hangjie Yuan, Bo Dong, Jiazheng Xing, Jinwang Wang, Rui Zhao, Yan Xing, Weihua Chen, Fan Wang</strong></p>
<p>Relighting is a crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic results, such as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design a structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, a disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering a 20x speedup for both image and video relighting. Code is available at <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/Lumos-Custom">https://github.com/alibaba-damo-academy/Lumos-Custom</a>. </p>
<blockquote>
<p>é‡ç…§æ˜æ˜¯ä¸€é¡¹æ—¢æœ‰å®é™…éœ€æœ¯åˆæœ‰è‰ºæœ¯ä»·å€¼çš„è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œæœ€è¿‘çš„æ‰©æ•£æ¨¡å‹é€šè¿‡å®ç°ä¸°å¯Œä¸”å¯æ§çš„ç…§æ˜æ•ˆæœå±•ç¤ºäº†å¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºå®ƒä»¬é€šå¸¸åœ¨è¯­ä¹‰æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥ç©ºé—´çš„é‚»è¿‘æ€§ä¸èƒ½ä¿è¯è§†è§‰ç©ºé—´çš„ç‰©ç†æ­£ç¡®æ€§ï¼Œå› æ­¤å®ƒä»¬é€šå¸¸ä¼šäº§ç”Ÿä¸çœŸå®çš„ç»“æœï¼Œå¦‚æ›å…‰è¿‡åº¦çš„é«˜å…‰ã€å¯¹ä¸é½çš„å½±å­å’Œé”™è¯¯çš„é®æŒ¡ã€‚æˆ‘ä»¬é€šè¿‡UniLumosæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒUniLumosæ˜¯ä¸€ä¸ªç”¨äºå›¾åƒå’Œè§†é¢‘çš„ç»Ÿä¸€é‡ç…§æ˜æ¡†æ¶ï¼Œå®ƒå°†RGBç©ºé—´å‡ ä½•åé¦ˆå¸¦å…¥æµåŒ¹é…ä¸»å¹²ç½‘ã€‚é€šè¿‡ç”¨ä»è¾“å‡ºä¸­æå–çš„æ·±åº¦å›¾å’Œæ³•çº¿å›¾æ¥ç›‘ç£æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æ˜ç¡®åœ°å°†ç…§æ˜æ•ˆæœä¸åœºæ™¯ç»“æ„å¯¹é½ï¼Œæé«˜ç‰©ç†å¯ä¿¡åº¦ã€‚ç„¶è€Œï¼Œè¿™ç§åé¦ˆéœ€è¦é«˜è´¨é‡è¾“å‡ºæ¥ç›‘ç£è§†è§‰ç©ºé—´ï¼Œä½¿å¾—æ ‡å‡†çš„å¤šæ­¥å»å™ªè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è·¯å¾„ä¸€è‡´æ€§å­¦ä¹ ï¼Œå…è®¸ç›‘ç£åœ¨å°‘æ•°å‡ æ­¥è®­ç»ƒä½“åˆ¶ä¸‹ä»ç„¶æœ‰æ•ˆã€‚ä¸ºäº†å®ç°ç²¾ç»†çš„é‡ç…§æ˜æ§åˆ¶å’Œç›‘ç£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»“æ„åŒ–çš„å…­ç»´æ³¨é‡Šåè®®ï¼Œæ•æ‰æ ¸å¿ƒç…§æ˜å±æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†LumosBenchï¼Œä¸€ä¸ªè§£è€¦çš„å±æ€§å±‚é¢åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¯„ä¼°ç…§æ˜å¯æ§æ€§ï¼Œå®ç°å¯¹é‡ç…§æ˜ç²¾åº¦åœ¨å„ä¸ªç»´åº¦çš„è‡ªåŠ¨å’Œå¯è§£é‡Šè¯„ä¼°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniLumoså®ç°äº†æœ€å…ˆè¿›çš„é‡ç…§æ˜è´¨é‡ï¼Œç‰©ç†ä¸€è‡´æ€§æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶åœ¨å›¾åƒå’Œè§†é¢‘é‡ç…§æ˜ä¸Šå®ç°äº†20å€çš„é€Ÿåº¦æå‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/Lumos-Custom%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alibaba-damo-academy/Lumos-Customä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01678v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹åœ¨é‡ç…§æ˜é¢†åŸŸå±•ç°å‡ºå¼ºå¤§æ½œåŠ›ï¼Œèƒ½å®ç°ä¸°å¯Œä¸”å¯æ§çš„ç…§æ˜æ•ˆæœã€‚ä½†ç”±äºå®ƒä»¬åœ¨è¯­ä¹‰æ½œåœ¨ç©ºé—´è¿›è¡Œä¼˜åŒ–ï¼Œå¿½ç•¥äº†ç‰©ç†æ­£ç¡®æ€§ï¼Œå¸¸äº§ç”Ÿä¸çœŸå®ç»“æœï¼Œå¦‚æ›å…‰è¿‡åº¦ã€é˜´å½±é”™ä½å’Œé®æŒ¡é”™è¯¯ã€‚ä¸ºæ­¤ï¼Œæå‡ºUniLumosæ¡†æ¶ï¼Œç»“åˆå›¾åƒå’Œè§†é¢‘é‡ç…§æ˜ï¼Œé€šè¿‡RGBç©ºé—´å‡ ä½•åé¦ˆåŒ¹é…ä¸»å¹²ç½‘ï¼Œæé«˜ç‰©ç†åˆç†æ€§ã€‚é‡‡ç”¨æ·±åº¦å›¾å’Œæ³•çº¿å›¾è¿›è¡Œç›‘ç£ï¼Œæ˜ç¡®å°†ç…§æ˜æ•ˆæœä¸åœºæ™¯ç»“æ„å¯¹é½ã€‚ä½†åé¦ˆéœ€é«˜è´¨é‡è¾“å‡ºè¿›è¡Œè§†è§‰ç©ºé—´ç›‘ç£ï¼Œä½¿å¤šæ­¥å»å™ªè®¡ç®—æ˜‚è´µã€‚ä¸ºæ­¤ï¼Œé‡‡ç”¨è·¯å¾„ä¸€è‡´æ€§å­¦ä¹ ï¼Œåœ¨å°‘æ­¥è®­ç»ƒä¸‹ä¿æŒæœ‰æ•ˆç›‘ç£ã€‚è®¾è®¡ç»“æ„åŒ–å…­ç»´æ³¨é‡Šåè®®ï¼Œå»ºç«‹LumosBenchåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°é‡ç…§æ˜å¯æ§æ€§ã€‚å®éªŒè¯æ˜UniLumoså®ç°æœ€ä¼˜é‡ç…§æ˜è´¨é‡ï¼Œæ˜¾è‘—æé«˜ç‰©ç†ä¸€è‡´æ€§ï¼Œä¸”å›¾åƒå’Œè§†é¢‘é‡ç…§æ˜é€Ÿåº¦æå‡20å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é‡ç…§æ˜é¢†åŸŸå…·æœ‰å¼ºå¤§æ½œåŠ›ï¼Œèƒ½å®ç°ä¸°å¯Œå¯æ§çš„ç…§æ˜æ•ˆæœã€‚</li>
<li>è¯­ä¹‰æ½œåœ¨ç©ºé—´çš„ä¼˜åŒ–å¯¼è‡´ç‰©ç†æ­£ç¡®æ€§é—®é¢˜ï¼Œäº§ç”Ÿä¸çœŸå®ç»“æœã€‚</li>
<li>UniLumosæ¡†æ¶ç»“åˆå›¾åƒå’Œè§†é¢‘é‡ç…§æ˜ï¼Œé€šè¿‡RGBç©ºé—´å‡ ä½•åé¦ˆæé«˜ç‰©ç†åˆç†æ€§ã€‚</li>
<li>æ·±åº¦å›¾å’Œæ³•çº¿å›¾çš„ç›‘ç£æ˜ç¡®å¯¹é½ç…§æ˜æ•ˆæœä¸åœºæ™¯ç»“æ„ã€‚</li>
<li>è·¯å¾„ä¸€è‡´æ€§å­¦ä¹ æœ‰æ•ˆåœ¨å°‘æ­¥è®­ç»ƒä¸‹ä¿æŒç›‘ç£ã€‚</li>
<li>ç»“æ„åŒ–å…­ç»´æ³¨é‡Šåè®®å’ŒLumosBenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°é‡ç…§æ˜çš„å¯æ§æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3f30753eba69b026217ab6e59ede22e" align="middle">
<img src="https://picx.zhimg.com/v2-ca0eb6c39aa25569fad28f5fa59247d4" align="middle">
<img src="https://picx.zhimg.com/v2-bf00a9441be7cd41092e3b730aa0ca18" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NSYNC-Negative-Synthetic-Image-Generation-for-Contrastive-Training-to-Improve-Stylized-Text-To-Image-Translation"><a href="#NSYNC-Negative-Synthetic-Image-Generation-for-Contrastive-Training-to-Improve-Stylized-Text-To-Image-Translation" class="headerlink" title="NSYNC: Negative Synthetic Image Generation for Contrastive Training to   Improve Stylized Text-To-Image Translation"></a>NSYNC: Negative Synthetic Image Generation for Contrastive Training to   Improve Stylized Text-To-Image Translation</h2><p><strong>Authors:Serkan Ozturk, Samet Hicsonmez, Pinar Duygulu</strong></p>
<p>Current text conditioned image generation methods output realistic looking images, but they fail to capture specific styles. Simply finetuning them on the target style datasets still struggles to grasp the style features. In this work, we present a novel contrastive learning framework to improve the stylization capability of large text-to-image diffusion models. Motivated by the astonishing advance in image generation models that makes synthetic data an intrinsic part of model training in various computer vision tasks, we exploit synthetic image generation in our approach. Usually, the generated synthetic data is dependent on the task, and most of the time it is used to enlarge the available real training dataset. With NSYNC, alternatively, we focus on generating negative synthetic sets to be used in a novel contrastive training scheme along with real positive images. In our proposed training setup, we forward negative data along with positive data and obtain negative and positive gradients, respectively. We then refine the positive gradient by subtracting its projection onto the negative gradient to get the orthogonal component, based on which the parameters are updated. This orthogonal component eliminates the trivial attributes that are present in both positive and negative data and directs the model towards capturing a more unique style. Experiments on various styles of painters and illustrators show that our approach improves the performance over the baseline methods both quantitatively and qualitatively. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/giddyyupp/NSYNC">https://github.com/giddyyupp/NSYNC</a>. </p>
<blockquote>
<p>å½“å‰åŸºäºæ–‡æœ¬æ¡ä»¶çš„å›¾åƒç”Ÿæˆæ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä½†å®ƒä»¬æ— æ³•æ•æ‰ç‰¹å®šçš„é£æ ¼ã€‚ä»…ä»…å¯¹ç›®æ ‡é£æ ¼æ•°æ®é›†è¿›è¡Œå¾®è°ƒä»ç„¶éš¾ä»¥æŠŠæ¡é£æ ¼ç‰¹å¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é£æ ¼åŒ–èƒ½åŠ›ã€‚å—å›¾åƒç”Ÿæˆæ¨¡å‹æƒŠäººè¿›å±•çš„å¯å‘ï¼Œåˆæˆæ•°æ®å·²æˆä¸ºå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­æ¨¡å‹è®­ç»ƒä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†åˆæˆå›¾åƒç”Ÿæˆã€‚é€šå¸¸ï¼Œç”Ÿæˆçš„åˆæˆæ•°æ®å–å†³äºä»»åŠ¡ï¼Œå¹¶ä¸”å¤§éƒ¨åˆ†æ—¶é—´ç”¨äºæ‰©å¤§å¯ç”¨çš„çœŸå®è®­ç»ƒæ•°æ®é›†ã€‚ç„¶è€Œï¼Œé€šè¿‡NSYNCï¼Œæˆ‘ä»¬ä¸“æ³¨äºç”Ÿæˆç”¨äºæ–°å‹å¯¹æ¯”è®­ç»ƒæ–¹æ¡ˆçš„è´Ÿåˆæˆé›†ï¼Œä»¥åŠä¸çœŸå®æ­£å›¾åƒä¸€èµ·ä½¿ç”¨ã€‚åœ¨æˆ‘ä»¬æå‡ºçš„è®­ç»ƒè®¾ç½®ä¸­ï¼Œæˆ‘ä»¬æ­£å‘ä¼ è¾“è´Ÿæ•°æ®ä»¥åŠæ­£æ•°æ®ï¼Œå¹¶åˆ†åˆ«è·å¾—è´Ÿå’Œæ­£æ¢¯åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ä»æ­£æ¢¯åº¦ä¸­å‡å»å…¶åœ¨è´Ÿæ¢¯åº¦ä¸Šçš„æŠ•å½±æ¥ä¼˜åŒ–æ­£æ¢¯åº¦ï¼Œä»è€Œè·å¾—æ­£äº¤åˆ†é‡ï¼ŒåŸºäºè¯¥æ­£äº¤åˆ†é‡æ›´æ–°å‚æ•°ã€‚è¿™ä¸ªæ­£äº¤åˆ†é‡æ¶ˆé™¤äº†æ­£è´Ÿæ•°æ®ä¸­å…±åŒçš„å¸¸è§„å±æ€§ï¼Œå¹¶æŒ‡å¯¼æ¨¡å‹æ•æ‰æ›´ç‹¬ç‰¹çš„é£æ ¼ã€‚å¯¹å„ç§é£æ ¼çš„ç”»å®¶å’Œæ’å›¾å¸ˆçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— è®ºåœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢éƒ½è¶…è¿‡äº†åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/giddyyupp/NSYNC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/giddyyupp/NSYNCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01517v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é£æ ¼åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆè´Ÿåˆæˆé›†ï¼Œå°†å…¶ç”¨äºæ–°é¢–çš„å¯¹æ¯”è®­ç»ƒæ–¹æ¡ˆï¼Œå¹¶ç»“åˆçœŸå®çš„æ­£å›¾åƒã€‚é€šè¿‡æ­£å‘è´Ÿæ•°æ®ä»¥åŠæ­£æ•°æ®ï¼Œè·å¾—è´Ÿå’Œæ­£æ¢¯åº¦ã€‚ç„¶åï¼Œé€šè¿‡å‡å»è´Ÿæ¢¯åº¦ä¸Šçš„æŠ•å½±æ¥ä¼˜åŒ–æ­£æ¢¯åº¦ï¼Œä»è€Œè·å¾—æ­£äº¤åˆ†é‡ï¼ŒåŸºäºè¯¥æ­£äº¤åˆ†é‡æ›´æ–°å‚æ•°ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†æ­£è´Ÿæ•°æ®ä¸­å­˜åœ¨çš„æ™®é€šå±æ€§ï¼Œä½¿æ¨¡å‹æ›´ä¸“æ³¨äºæ•æ‰ç‹¬ç‰¹çš„é£æ ¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”»å®¶å’Œæ’ç”»å®¶çš„å„ç§é£æ ¼ä¸Šå‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå®šé‡å’Œå®šæ€§è¡¨ç°éƒ½æœ‰æ‰€æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆæ–¹æ³•æ— æ³•æ•æ‰ç‰¹å®šé£æ ¼ï¼Œå³ä½¿å¯¹ç›®æ ‡æ ·å¼æ•°æ®é›†è¿›è¡Œå¾®è°ƒä¹Ÿéš¾ä»¥æŠŠæ¡æ ·å¼ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é£æ ¼åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆè´Ÿåˆæˆé›†ï¼Œå¹¶å°†å…¶çº³å…¥å¯¹æ¯”è®­ç»ƒæ–¹æ¡ˆï¼Œç»“åˆçœŸå®æ­£å›¾åƒè¿›è¡Œè®­ç»ƒã€‚</li>
<li>é€šè¿‡æ­£å‘è´Ÿæ•°æ®å’Œæ­£æ•°æ®è·å¾—æ­£è´Ÿæ¢¯åº¦ï¼Œå¹¶ä¼˜åŒ–æ­£æ¢¯åº¦ä»¥è·å¾—æ­£äº¤åˆ†é‡ã€‚</li>
<li>æ­£äº¤åˆ†é‡æœ‰åŠ©äºæ¶ˆé™¤æ­£è´Ÿæ•°ä¸­çš„æ™®é€šå±æ€§ï¼Œä½¿æ¨¡å‹ä¸“æ³¨äºæ•æ‰ç‹¬ç‰¹é£æ ¼ã€‚</li>
<li>åœ¨å„ç§é£æ ¼çš„ç”»å®¶å’Œæ’ç”»å®¶çš„å®éªŒä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01517">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bcffdff615246bc81e2278b6fef300a" align="middle">
<img src="https://picx.zhimg.com/v2-f9e656aa10107ce3ed28badf3bb7cc5e" align="middle">
<img src="https://picx.zhimg.com/v2-75adfb8d1a8de314a91cafc4cd14d52e" align="middle">
<img src="https://picx.zhimg.com/v2-892bf2ccb896828674ba584ff4ad92da" align="middle">
<img src="https://picx.zhimg.com/v2-40850e19634e7e0e853ad21db3af08c1" align="middle">
<img src="https://picx.zhimg.com/v2-c2f27ac64f6d6ea052780fea2df71a54" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Expanding-the-Content-Style-Frontier-a-Balanced-Subspace-Blending-Approach-for-Content-Style-LoRA-Fusion"><a href="#Expanding-the-Content-Style-Frontier-a-Balanced-Subspace-Blending-Approach-for-Content-Style-LoRA-Fusion" class="headerlink" title="Expanding the Content-Style Frontier: a Balanced Subspace Blending   Approach for Content-Style LoRA Fusion"></a>Expanding the Content-Style Frontier: a Balanced Subspace Blending   Approach for Content-Style LoRA Fusion</h2><p><strong>Authors:Linhao Huang</strong></p>
<p>Recent advancements in text-to-image diffusion models have significantly improved the personalization and stylization of generated images. However, previous studies have only assessed content similarity under a single style intensity. In our experiments, we observe that increasing style intensity leads to a significant loss of content features, resulting in a suboptimal content-style frontier. To address this, we propose a novel approach to expand the content-style frontier by leveraging Content-Style Subspace Blending and a Content-Style Balance loss. Our method improves content similarity across varying style intensities, significantly broadening the content-style frontier. Extensive experiments demonstrate that our approach outperforms existing techniques in both qualitative and quantitative evaluations, achieving superior content-style trade-off with significantly lower Inverted Generational Distance (IGD) and Generational Distance (GD) scores compared to current methods. </p>
<blockquote>
<p>æœ€è¿‘æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›å±•åœ¨ä¸ªæ€§åŒ–ç”Ÿæˆå’Œé£æ ¼åŒ–ç”Ÿæˆå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶åªåœ¨å•ä¸€çš„é£æ ¼å¼ºåº¦ä¸‹è¯„ä¼°å†…å®¹ç›¸ä¼¼æ€§ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°å¢åŠ é£æ ¼å¼ºåº¦ä¼šå¯¼è‡´å†…å®¹ç‰¹å¾çš„æ˜¾è‘—æŸå¤±ï¼Œä»è€Œå½¢æˆä¸€ä¸ªæ¬¡ä¼˜çš„å†…å®¹-é£æ ¼è¾¹ç•Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ‰©å±•å†…å®¹-é£æ ¼è¾¹ç•Œï¼Œé€šè¿‡åˆ©ç”¨å†…å®¹-é£æ ¼å­ç©ºé—´æ··åˆå’Œå†…å®¹-é£æ ¼å¹³è¡¡æŸå¤±ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ä¸åŒé£æ ¼å¼ºåº¦ä¸‹çš„å†…å®¹ç›¸ä¼¼æ€§ï¼Œæ˜¾è‘—æ‰©å¤§äº†å†…å®¹-é£æ ¼è¾¹ç•Œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°æ–¹é¢éƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®ç°äº†æ›´å¥½çš„å†…å®¹-é£æ ¼æƒè¡¡ï¼Œä¸å½“å‰æ–¹æ³•ç›¸æ¯”ï¼Œå€’ç½®ç”Ÿæˆè·ç¦»ï¼ˆIGDï¼‰å’Œç”Ÿæˆè·ç¦»ï¼ˆGDï¼‰å¾—åˆ†æ˜¾è‘—é™ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01355v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ‰©æ•£æ¨¡å‹æœ€æ–°è¿›å±•åœ¨ä¸ªæ€§åŒ–ä¸é£æ ¼åŒ–ç”Ÿæˆå›¾åƒä¸Šå–å¾—æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œå…ˆå‰ç ”ç©¶ä»…åœ¨å•ä¸€é£æ ¼å¼ºåº¦ä¸‹è¯„ä¼°å†…å®¹ç›¸ä¼¼æ€§ã€‚æœ¬ç ”ç©¶å‘ç°å¢åŠ é£æ ¼å¼ºåº¦ä¼šå¯¼è‡´å†…å®¹ç‰¹å¾å¤§é‡æŸå¤±ï¼Œå½¢æˆæ¬¡ä¼˜çš„å†…å®¹é£æ ¼è¾¹ç•Œã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å†…å®¹é£æ ¼å­ç©ºé—´èåˆå’Œå†…å®¹é£æ ¼å¹³è¡¡æŸå¤±æ¥æ‹“å±•å†…å®¹é£æ ¼è¾¹ç•Œã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒé£æ ¼å¼ºåº¦ä¸‹æé«˜å†…å®¹ç›¸ä¼¼æ€§ï¼Œæ˜¾è‘—æ‹“å®½å†…å®¹é£æ ¼è¾¹ç•Œã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— è®ºåœ¨å®šæ€§è¿˜æ˜¯å®šé‡è¯„ä¼°ä¸Šéƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸å½“å‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ›´ä¼˜è´¨çš„å†…å®¹é£æ ¼æƒè¡¡ï¼Œé™ä½äº†åå‘ç”Ÿæˆè·ç¦»ï¼ˆIGDï¼‰å’Œç”Ÿæˆè·ç¦»ï¼ˆGDï¼‰å¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°æ–‡æœ¬æ‰©æ•£æ¨¡å‹åœ¨ä¸ªæ€§åŒ–ä¸é£æ ¼åŒ–å›¾åƒç”Ÿæˆä¸Šå–å¾—è¿›å±•ã€‚</li>
<li>å•ä¸€é£æ ¼å¼ºåº¦ä¸‹çš„å†…å®¹ç›¸ä¼¼æ€§è¯„ä¼°å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¢åŠ é£æ ¼å¼ºåº¦ä¼šå¯¼è‡´å†…å®¹ç‰¹å¾æŸå¤±ï¼Œå½¢æˆæ¬¡ä¼˜çš„å†…å®¹é£æ ¼è¾¹ç•Œã€‚</li>
<li>æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å†…å®¹é£æ ¼å­ç©ºé—´èåˆå’Œå†…å®¹é£æ ¼å¹³è¡¡æŸå¤±æ¥æ‹“å±•å†…å®¹é£æ ¼è¾¹ç•Œã€‚</li>
<li>æ–°æ–¹æ³•åœ¨ä¸åŒé£æ ¼å¼ºåº¦ä¸‹æé«˜å†…å®¹ç›¸ä¼¼æ€§ã€‚</li>
<li>å¤§é‡å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸Šéƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83edef7536502870d8bd243611ee5b3c" align="middle">
<img src="https://picx.zhimg.com/v2-8ed0eb5543d26fa744c6c63d301130c7" align="middle">
<img src="https://picx.zhimg.com/v2-04308aedbc954a321df84fed3820b566" align="middle">
<img src="https://picx.zhimg.com/v2-618163d698556f68bf48f91168740e07" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Perturb-a-Model-Not-an-Image-Towards-Robust-Privacy-Protection-via-Anti-Personalized-Diffusion-Models"><a href="#Perturb-a-Model-Not-an-Image-Towards-Robust-Privacy-Protection-via-Anti-Personalized-Diffusion-Models" class="headerlink" title="Perturb a Model, Not an Image: Towards Robust Privacy Protection via   Anti-Personalized Diffusion Models"></a>Perturb a Model, Not an Image: Towards Robust Privacy Protection via   Anti-Personalized Diffusion Models</h2><p><strong>Authors:Tae-Young Lee, Juwon Seo, Jong Hwan Ko, Gyeong-Moon Park</strong></p>
<p>Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized content. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called Anti-Personalized Diffusion Models (APDM). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at <a target="_blank" rel="noopener" href="https://github.com/KU-VGI/APDM">https://github.com/KU-VGI/APDM</a>. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„è¿›æ­¥å·²ç»èƒ½å¤Ÿå®ç°ç‰¹å®šä¸»é¢˜ï¼Œå¦‚èº«ä»½æˆ–å¯¹è±¡çš„é«˜è´¨é‡åˆæˆã€‚è¿™ä¸€èƒ½åŠ›è™½ç„¶åœ¨å†…å®¹åˆ›ä½œä¸Šå¼€å¯äº†æ–°çš„å¯èƒ½æ€§ï¼Œä½†ä¹Ÿå¼•å…¥äº†é‡å¤§çš„éšç§é£é™©ï¼Œå› ä¸ºä¸ªæ€§åŒ–æŠ€æœ¯å¯èƒ½ä¼šè¢«æ¶æ„ç”¨æˆ·æ»¥ç”¨ï¼Œç”Ÿæˆæœªç»æˆæƒçš„å†…å®¹ã€‚å°½ç®¡å·²æœ‰ä¸€äº›ç ”ç©¶è¯•å›¾é€šè¿‡å¯¹æŠ—æ‰°åŠ¨æ ·æœ¬çš„ç”Ÿæˆæ¥å¯¹æŠ—ä¸ªæ€§åŒ–æŠ€æœ¯ï¼Œè¿™äº›æ‰°åŠ¨æ ·æœ¬æ—¨åœ¨å¹²æ‰°ä¸ªæ€§åŒ–è¿‡ç¨‹ï¼Œä½†å®ƒä»¬ä¾èµ–äºä¸åˆ‡å®é™…çš„å‡è®¾ï¼Œç”šè‡³åœ¨åªæœ‰å‡ å¼ å¹²å‡€å›¾åƒæˆ–ç®€å•çš„å›¾åƒå˜æ¢çš„æƒ…å†µä¸‹ä¹Ÿä¼šå˜å¾—æ— æ•ˆã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†ä¿æŠ¤ç›®æ ‡ä»å›¾åƒè½¬å‘æ‰©æ•£æ¨¡å‹æœ¬èº«ï¼Œé€šè¿‡æˆ‘ä»¬ç§°ä¸ºåä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ï¼ˆAPDMï¼‰çš„æ–°å‹æ¡†æ¶æ¥é˜»ç¢ç‰¹å®šä¸»é¢˜çš„ä¸ªäººåŒ–ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œç†è®ºåˆ†æï¼Œè¯æ˜ç°æœ‰æŸå¤±å‡½æ•°å¯¹æ‰©æ•£æ¨¡å‹çš„ç®€å•åº”ç”¨æœ¬è´¨ä¸Šæ— æ³•ä¿è¯å®ç°ç¨³å¥çš„åä¸ªæ€§åŒ–æ‰€éœ€çš„æ”¶æ•›æ€§ã€‚å—è¿™ä¸€å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç›´æ¥ä¿æŠ¤ä¼˜åŒ–ï¼ˆDPOï¼‰è¿™ä¸€æ–°å‹æŸå¤±å‡½æ•°ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸æŸå®³ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°ç ´åç›®æ ‡æ¨¡å‹ä¸­çš„ä¸»é¢˜ä¸ªæ€§åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„åŒè·¯å¾„ä¼˜åŒ–ç­–ç•¥ï¼Œç§°ä¸ºå­¦ä¹ ä¿æŠ¤ï¼ˆL2Pï¼‰ã€‚é€šè¿‡äº¤æ›¿ä½¿ç”¨ä¸ªæ€§åŒ–å’Œä¿æŠ¤è·¯å¾„ï¼ŒL2Pæ¨¡æ‹Ÿæœªæ¥çš„ä¸ªæ€§åŒ–è½¨è¿¹ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥è‡ªé€‚åº”åœ°å¼ºåŒ–ä¿æŠ¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨é˜²æ­¢æœªç»æˆæƒçš„ä¸ªäººåŒ–æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/KU-VGI/APDM%E4%B8%8A%E3%80%82">https://github.com/KU-VGI/APDMä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01307v1">PDF</a> 26 pages, 9 figures, 16 tables, NeurIPS 2025</p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•èƒ½å¤Ÿå®ç°ç‰¹å®šä¸»é¢˜çš„é«˜è´¨é‡åˆæˆï¼Œå¦‚èº«ä»½æˆ–ç‰©ä½“ã€‚ç„¶è€Œï¼Œè¿™ç§èƒ½åŠ›åœ¨è§£é”å†…å®¹åˆ›ä½œæ–°å¯èƒ½æ€§çš„åŒæ—¶ï¼Œä¹Ÿå¸¦æ¥äº†é‡å¤§éšç§é£é™©ã€‚æ¶æ„ç”¨æˆ·å¯èƒ½æ»¥ç”¨ä¸ªæ€§åŒ–æŠ€æœ¯æ¥ç”Ÿæˆæœªç»æˆæƒçš„å†…å®¹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºAnti-Personalized Diffusion Modelsï¼ˆAPDMï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨ä»æ‰©æ•£æ¨¡å‹æœ¬èº«å‡ºå‘ï¼Œé˜»æ­¢ç‰¹å®šä¸»é¢˜çš„ä¸ªäººåŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†Direct Protective Optimizationï¼ˆDPOï¼‰å’ŒLearning to Protectï¼ˆL2Pï¼‰ç­–ç•¥ï¼Œæœ‰æ•ˆé˜»æ–­ç›®æ ‡æ¨¡å‹ä¸­çš„ä¸»é¢˜ä¸ªäººåŒ–ï¼ŒåŒæ—¶ä¸æŸå®³ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°é˜²æ­¢æœªç»æˆæƒä¸ªäººåŒ–çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æœ€æ–°è¿›å±•å¯å®ç°ç‰¹å®šä¸»é¢˜çš„é«˜è´¨é‡åˆæˆï¼Œå¦‚èº«ä»½å’Œç‰©ä½“ã€‚</li>
<li>è™½ç„¶ä¸ªäººåŒ–æŠ€æœ¯å¼€å¯äº†æ–°çš„åˆ›ä½œå¯èƒ½æ€§ï¼Œä½†ä¹Ÿå¸¦æ¥éšç§é£é™©ï¼Œå¯èƒ½è¢«æ¶æ„ç”¨æˆ·åˆ©ç”¨ç”Ÿæˆæœªç»æˆæƒçš„å†…å®¹ã€‚</li>
<li>å½“å‰ç ”ç©¶å°è¯•é€šè¿‡å¯¹æŠ—æ‰°åŠ¨æ ·æœ¬æ¥åº”å¯¹æ­¤é—®é¢˜ï¼Œä½†åœ¨é¢å¯¹æ¸…æ´å›¾åƒæˆ–ç®€å•å›¾åƒè½¬æ¢æ—¶æ•ˆæœä¸ä½³ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºAnti-Personalized Diffusion Modelsï¼ˆAPDMï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œä»æ‰©æ•£æ¨¡å‹æœ¬èº«ä¿æŠ¤ç”¨æˆ·éšç§ã€‚</li>
<li>å¼•å…¥Direct Protective Optimizationï¼ˆDPOï¼‰ä½œä¸ºæ–°çš„æŸå¤±å‡½æ•°ï¼Œèƒ½æœ‰æ•ˆé˜»æ­¢ä¸»é¢˜ä¸ªäººåŒ–è€Œä¸æŸå®³ç”Ÿæˆè´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŒè·¯å¾„ä¼˜åŒ–ç­–ç•¥Learning to Protectï¼ˆL2Pï¼‰ï¼Œé€šè¿‡äº¤æ›¿è¿›è¡Œä¸ªäººåŒ–å’Œä¿æŠ¤è·¯å¾„ï¼Œæ¨¡æ‹Ÿæœªæ¥ä¸ªäººåŒ–è½¨è¿¹å¹¶è‡ªé€‚åº”åŠ å¼ºä¿æŠ¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ddaafea81bb660e860b0fdebb94523c8" align="middle">
<img src="https://picx.zhimg.com/v2-80515dc5aa27b31131821a81aea95104" align="middle">
<img src="https://picx.zhimg.com/v2-ba026c3827784936a82c8c3a3b0b1cf2" align="middle">
<img src="https://picx.zhimg.com/v2-991218504f7244738dd28ca69cc97396" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diffusion-Transformer-meets-Multi-level-Wavelet-Spectrum-for-Single-Image-Super-Resolution"><a href="#Diffusion-Transformer-meets-Multi-level-Wavelet-Spectrum-for-Single-Image-Super-Resolution" class="headerlink" title="Diffusion Transformer meets Multi-level Wavelet Spectrum for Single   Image Super-Resolution"></a>Diffusion Transformer meets Multi-level Wavelet Spectrum for Single   Image Super-Resolution</h2><p><strong>Authors:Peng Du, Hui Li, Han Xu, Paul Barom Jeon, Dongwook Lee, Daehyun Ji, Ran Yang, Feng Zhu</strong></p>
<p>Discrete Wavelet Transform (DWT) has been widely explored to enhance the performance of image superresolution (SR). Despite some DWT-based methods improving SR by capturing fine-grained frequency signals, most existing approaches neglect the interrelations among multiscale frequency sub-bands, resulting in inconsistencies and unnatural artifacts in the reconstructed images. To address this challenge, we propose a Diffusion Transformer model based on image Wavelet spectra for SR (DTWSR). DTWSR incorporates the superiority of diffusion models and transformers to capture the interrelations among multiscale frequency sub-bands, leading to a more consistence and realistic SR image. Specifically, we use a Multi-level Discrete Wavelet Transform to decompose images into wavelet spectra. A pyramid tokenization method is proposed which embeds the spectra into a sequence of tokens for transformer model, facilitating to capture features from both spatial and frequency domain. A dual-decoder is designed elaborately to handle the distinct variances in low-frequency and high-frequency sub-bands, without omitting their alignment in image generation. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method, with high performance on both perception quality and fidelity. </p>
<blockquote>
<p>ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºæé«˜å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰çš„æ€§èƒ½ã€‚å°½ç®¡æœ‰äº›åŸºäºDWTçš„æ–¹æ³•é€šè¿‡æ•æ‰ç²¾ç»†é¢‘ç‡ä¿¡å·æ¥æé«˜SRï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•å¿½è§†äº†å¤šå°ºåº¦é¢‘ç‡å­å¸¦ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´é‡å»ºçš„å›¾åƒå‡ºç°ä¸ä¸€è‡´å’Œä¸è‡ªç„¶çš„ä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå°æ³¢è°±çš„ç”¨äºSRçš„æ‰©æ•£å˜å‹å™¨æ¨¡å‹ï¼ˆDTWSRï¼‰ã€‚DTWSRç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œå˜å‹å™¨çš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿæ•æ‰å¤šå°ºåº¦é¢‘ç‡å­å¸¦ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œäº§ç”Ÿæ›´ä¸€è‡´å’Œæ›´é€¼çœŸçš„SRå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šçº§ç¦»æ•£å°æ³¢å˜æ¢å°†å›¾åƒåˆ†è§£æˆå°æ³¢è°±ã€‚æå‡ºäº†ä¸€ç§é‡‘å­—å¡”æ ‡è®°æ³•ï¼Œå°†è°±åµŒå…¥åˆ°æ ‡è®°åºåˆ—ä¸­ä¾›å˜å‹å™¨æ¨¡å‹ä½¿ç”¨ï¼Œä¾¿äºä»ç©ºé—´å’Œé¢‘ç‡åŸŸæ•æ‰ç‰¹å¾ã€‚ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªåŒè§£ç å™¨ï¼Œä»¥å¤„ç†ä½é¢‘å’Œé«˜é¢‘å­å¸¦çš„ç‹¬ç‰¹å·®å¼‚ï¼ŒåŒæ—¶ä¸ä¼šåœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­å¿½ç•¥å®ƒä»¬çš„å¯¹é½ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ„ŸçŸ¥è´¨é‡å’Œä¿çœŸåº¦æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01175v2">PDF</a> ICCV 2025 Oral Paper</p>
<p><strong>Summary</strong><br>     ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ä»¥æé«˜æ€§èƒ½ã€‚å°½ç®¡ä¸€äº›åŸºäºDWTçš„æ–¹æ³•é€šè¿‡æ•è·ç²¾ç»†é¢‘ç‡ä¿¡å·æ”¹è¿›äº†SRï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•å¿½ç•¥äº†å¤šå°ºåº¦é¢‘ç‡å­å¸¦ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´é‡å»ºå›¾åƒå‡ºç°ä¸ä¸€è‡´å’Œä¸è‡ªç„¶çš„äººå·¥ç—•è¿¹ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå›¾åƒå°æ³¢å…‰è°±çš„æ‰©æ•£å˜æ¢æ¨¡å‹ï¼ˆDTWSRï¼‰ã€‚DTWSRç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œå˜å‹å™¨çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ•æ‰å¤šå°ºåº¦é¢‘ç‡å­å¸¦ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œç”Ÿæˆæ›´ä¸€è‡´å’Œé€¼çœŸçš„SRå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šçº§ç¦»æ•£å°æ³¢å˜æ¢å°†å›¾åƒåˆ†è§£æˆå°æ³¢å…‰è°±ï¼Œå¹¶æå‡ºäº†ä¸€ç§é‡‘å­—å¡”æ ‡è®°æ–¹æ³•ï¼Œå°†å…‰è°±åµŒå…¥åˆ°æ ‡è®°åºåˆ—ä¸­ï¼Œä¾¿äºä»ç©ºé—´å’Œé¢‘ç‡åŸŸæ•è·ç‰¹å¾ã€‚ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªåŒè§£ç å™¨ï¼Œä»¥å¤„ç†ä½é¢‘å’Œé«˜é¢‘å­å¸¦çš„ç‹¬ç‰¹å·®å¼‚ï¼ŒåŒæ—¶ä¸å¿½ç•¥å®ƒä»¬åœ¨å›¾åƒç”Ÿæˆä¸­çš„å¯¹é½ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜æ„ŸçŸ¥è´¨é‡å’Œä¿çœŸåº¦æ–¹é¢éƒ½ååˆ†æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å·²è¢«ç”¨äºæé«˜å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†å¤šå°ºåº¦é¢‘ç‡å­å¸¦é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¯¼è‡´é‡å»ºå›¾åƒå­˜åœ¨ä¸ä¸€è‡´å’Œä¸è‡ªç„¶çš„ç°è±¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå°æ³¢å…‰è°±çš„æ‰©æ•£å˜æ¢æ¨¡å‹ï¼ˆDTWSRï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰å¤šå°ºåº¦é¢‘ç‡å­å¸¦ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>ä½¿ç”¨å¤šçº§ç¦»æ•£å°æ³¢å˜æ¢å°†å›¾åƒåˆ†è§£æˆå°æ³¢å…‰è°±ã€‚</li>
<li>é‡‡ç”¨é‡‘å­—å¡”æ ‡è®°æ³•å°†å…‰è°±åµŒå…¥æ ‡è®°åºåˆ—ï¼Œä¾¿äºä»ç©ºé—´å’Œé¢‘ç‡åŸŸæå–ç‰¹å¾ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªåŒè§£ç å™¨ï¼Œä»¥å¤„ç†ä½é¢‘å’Œé«˜é¢‘å­å¸¦çš„ç‹¬ç‰¹å·®å¼‚ï¼Œå¹¶ä¿æŒè‰¯å¥½çš„å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9416870d7e82f1b6079b9ac98123a33" align="middle">
<img src="https://picx.zhimg.com/v2-ba797779bb96431acb0e3598ed6ad3c9" align="middle">
<img src="https://picx.zhimg.com/v2-dd2b542f77c5c1cc31ecfd92d431575b" align="middle">
<img src="https://picx.zhimg.com/v2-4c508941086e32491af4c8b91d66963f" align="middle">
<img src="https://picx.zhimg.com/v2-2dc869fdfdd70b95fde1b2ca0f7de427" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Deep-Generative-Models-for-Enhanced-Vitreous-OCT-Imaging"><a href="#Deep-Generative-Models-for-Enhanced-Vitreous-OCT-Imaging" class="headerlink" title="Deep Generative Models for Enhanced Vitreous OCT Imaging"></a>Deep Generative Models for Enhanced Vitreous OCT Imaging</h2><p><strong>Authors:Simone Sarrocco, Philippe C. Cattin, Peter M. Maloca, Paul Friedrich, Philippe Valmaggia</strong></p>
<p>Purpose: To evaluate deep learning (DL) models for enhancing vitreous optical coherence tomography (OCT) image quality and reducing acquisition time. Methods: Conditional Denoising Diffusion Probabilistic Models (cDDPMs), Brownian Bridge Diffusion Models (BBDMs), U-Net, Pix2Pix, and Vector-Quantised Generative Adversarial Network (VQ-GAN) were used to generate high-quality spectral-domain (SD) vitreous OCT images. Inputs were SD ART10 images, and outputs were compared to pseudoART100 images obtained by averaging ten ART10 images per eye location. Model performance was assessed using image quality metrics and Visual Turing Tests, where ophthalmologists ranked generated images and evaluated anatomical fidelity. The best modelâ€™s performance was further tested within the manually segmented vitreous on newly acquired data. Results: U-Net achieved the highest Peak Signal-to-Noise Ratio (PSNR: 30.230) and Structural Similarity Index Measure (SSIM: 0.820), followed by cDDPM. For Learned Perceptual Image Patch Similarity (LPIPS), Pix2Pix (0.697) and cDDPM (0.753) performed best. In the first Visual Turing Test, cDDPM ranked highest (3.07); in the second (best model only), cDDPM achieved a 32.9% fool rate and 85.7% anatomical preservation. On newly acquired data, cDDPM generated vitreous regions more similar in PSNR to the ART100 reference than true ART1 or ART10 B-scans and achieved higher PSNR on whole images when conditioned on ART1 than ART10. Conclusions: Results reveal discrepancies between quantitative metrics and clinical evaluation, highlighting the need for combined assessment. cDDPM showed strong potential for generating clinically meaningful vitreous OCT images while reducing acquisition time fourfold. Translational Relevance: cDDPMs show promise for clinical integration, supporting faster, higher-quality vitreous imaging. Dataset and code will be made publicly available. </p>
<blockquote>
<p>ç›®çš„ï¼šæ—¨åœ¨è¯„ä¼°æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹åœ¨æé«˜ç»ç’ƒä½“å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰å›¾åƒè´¨é‡å’Œå‡å°‘é‡‡é›†æ—¶é—´æ–¹é¢çš„è¡¨ç°ã€‚æ–¹æ³•ï¼šä½¿ç”¨æ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆcDDPMsï¼‰ã€å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹ï¼ˆBBDMsï¼‰ã€U-Netã€Pix2Pixå’Œå‘é‡é‡åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆVQ-GANï¼‰ç”Ÿæˆé«˜è´¨é‡å…‰è°±åŸŸï¼ˆSDï¼‰ç»ç’ƒä½“OCTå›¾åƒã€‚è¾“å…¥ä¸ºSD ART10å›¾åƒï¼Œè¾“å‡ºä¸é€šè¿‡å¹³å‡æ¯ä¸ªçœ¼ä½10å¼ ART10å›¾åƒè·å¾—çš„ä¼ªART100å›¾åƒè¿›è¡Œæ¯”è¾ƒã€‚ä½¿ç”¨å›¾åƒè´¨é‡æŒ‡æ ‡å’Œè§†è§‰å›¾çµæµ‹è¯•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå…¶ä¸­çœ¼ç§‘åŒ»ç”Ÿå¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œæ’åå¹¶è¯„ä¼°å…¶è§£å‰–å­¦çš„å¿ å®åº¦ã€‚æœ€ä½³æ¨¡å‹çš„æ€§èƒ½åœ¨å…¨æ–°è·å–çš„æ•°æ®ä¸­çš„æ‰‹åŠ¨åˆ†å‰²ç»ç’ƒä½“ä¸Šè¿›è¡Œè¿›ä¸€æ­¥æµ‹è¯•ã€‚ç»“æœï¼šU-Netåœ¨å³°å€¼ä¿¡å·å™ªå£°æ¯”ï¼ˆPSNRï¼š30.230ï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼š0.820ï¼‰æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯cDDPMã€‚åœ¨æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰æ–¹é¢ï¼ŒPix2Pixï¼ˆ0.697ï¼‰å’ŒcDDPMï¼ˆ0.753ï¼‰è¡¨ç°æœ€å¥½ã€‚åœ¨ç¬¬ä¸€æ¬¡è§†è§‰å›¾çµæµ‹è¯•ä¸­ï¼ŒcDDPMæ’åæœ€é«˜ï¼ˆ3.07ï¼‰ï¼›åœ¨ç¬¬äºŒæ¬¡ï¼ˆä»…æœ€ä½³æ¨¡å‹ï¼‰ä¸­ï¼ŒcDDPMçš„æ¬ºéª—ç‡è¾¾åˆ°32.9%ï¼Œè§£å‰–å­¦ä¿ç•™ç‡ä¸º85.7%ã€‚åœ¨å…¨æ–°è·å–çš„æ•°æ®ä¸Šï¼ŒcDDPMç”Ÿæˆçš„ç»ç’ƒä½“åŒºåŸŸåœ¨PSNRä¸Šä¸ART100å‚è€ƒç›¸æ¯”æ›´ä¸ºç›¸ä¼¼ï¼Œé«˜äºçœŸå®çš„ART1æˆ–ART10 Bæ‰«æå›¾åƒï¼Œå¹¶ä¸”åœ¨ä»¥ART1ä¸ºæ¡ä»¶æ—¶ï¼Œæ•´ä¸ªå›¾åƒçš„PSNRæ›´é«˜ã€‚ç»“è®ºï¼šç»“æœæ­ç¤ºäº†å®šé‡æŒ‡æ ‡ä¸ä¸´åºŠè¯„ä¼°ä¹‹é—´çš„å·®å¼‚ï¼Œå¼ºè°ƒäº†éœ€è¦ç»“åˆè¯„ä¼°çš„å¿…è¦æ€§ã€‚cDDPMåœ¨ç”Ÿæˆå…·æœ‰ä¸´åºŠæ„ä¹‰çš„ç»ç’ƒä½“OCTå›¾åƒåŒæ—¶å‡å°‘é‡‡é›†æ—¶é—´å››å€æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§æ½œåŠ›ã€‚ç¿»è¯‘ç›¸å…³æ€§ï¼šcDDPMsè¡¨ç°å‡ºä¸´åºŠæ•´åˆçš„æ½œåŠ›ï¼Œæ”¯æŒæ›´å¿«ã€æ›´é«˜è´¨é‡çš„ç»ç’ƒä½“æˆåƒã€‚æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00881v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æé«˜ç»ç’ƒä½“å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰å›¾åƒè´¨é‡å’Œå‡å°‘é‡‡é›†æ—¶é—´æ–¹é¢çš„åº”ç”¨ã€‚ç ”ç©¶é‡‡ç”¨æ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆcDDPMsï¼‰ã€å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹ï¼ˆBBDMsï¼‰ã€U-Netã€Pix2Pixå’Œå‘é‡é‡åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆVQ-GANï¼‰ç”Ÿæˆé«˜è´¨é‡å…‰è°±åŸŸï¼ˆSDï¼‰ç»ç’ƒä½“OCTå›¾åƒã€‚æ¨¡å‹æ€§èƒ½é€šè¿‡å›¾åƒè´¨é‡æŒ‡æ ‡å’Œè§†è§‰å›¾çµæµ‹è¯•è¿›è¡Œè¯„ä¼°ï¼Œå…¶ä¸­çœ¼ç§‘åŒ»ç”Ÿå¯¹ç”Ÿæˆå›¾åƒè¿›è¡Œæ’åå¹¶è¯„ä¼°å…¶è§£å‰–ä¿çœŸåº¦ã€‚ç»“æœæ˜¾ç¤ºï¼ŒU-Netåœ¨å³°å€¼ä¿¡å·å™ªå£°æ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€ŒcDDPMåœ¨è§†è§‰å›¾çµæµ‹è¯•ä¸­æ’åç¬¬ä¸€ã€‚cDDPMåœ¨æ–°è·å–çš„æ•°æ®ä¸Šç”Ÿæˆçš„ç»ç’ƒä½“åŒºåŸŸä¸ART100å‚è€ƒç›¸æ¯”ï¼ŒPSNRæ›´é«˜ï¼Œä¸”åœ¨ä»¥ART1ä¸ºæ¡ä»¶æ—¶ï¼Œå…¨å›¾åƒPSNRä¹Ÿè¾ƒé«˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå®šé‡æŒ‡æ ‡ä¸ä¸´åºŠè¯„ä»·ä¹‹é—´å­˜åœ¨å·®å¼‚ï¼Œå¼ºè°ƒéœ€è¦è”åˆè¯„ä¼°ã€‚cDDPMåœ¨ç”Ÿæˆå…·æœ‰ä¸´åºŠæ„ä¹‰çš„ç»ç’ƒä½“OCTå›¾åƒæ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§æ½œåŠ›ï¼ŒåŒæ—¶å¯å°†é‡‡é›†æ—¶é—´ç¼©çŸ­å››å€ã€‚ç¿»è¯‘ç›¸å…³æ€§ï¼šcDDPMsæ˜¾ç¤ºå‡ºä¸´åºŠæ•´åˆçš„æ½œåŠ›ï¼Œæ”¯æŒæ›´å¿«ã€æ›´é«˜è´¨é‡çš„ç»ç’ƒä½“æˆåƒã€‚æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€æä¾›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æé«˜ç»ç’ƒä½“OCTå›¾åƒè´¨é‡å¹¶å‡å°‘é‡‡é›†æ—¶é—´ã€‚</li>
<li>å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆåŒ…æ‹¬cDDPMsã€U-Netã€Pix2Pixç­‰ï¼‰è¢«ç”¨äºç”Ÿæˆé«˜è´¨é‡OCTå›¾åƒã€‚</li>
<li>U-Netåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡PSNRå’ŒSSIMä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒcDDPMåœ¨è§†è§‰å›¾çµæµ‹è¯•ä¸­æ’åç¬¬ä¸€ã€‚</li>
<li>cDDPMåœ¨æ–°æ•°æ®ä¸Šç”Ÿæˆçš„ç»ç’ƒä½“åŒºåŸŸä¸é«˜çº§å‚è€ƒå›¾åƒç›¸ä¼¼ï¼Œè¡¨æ˜å…¶å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å®šé‡æŒ‡æ ‡ä¸ä¸´åºŠè¯„ä»·ä¹‹é—´çš„å·®å¼‚ï¼Œæç¤ºéœ€è¦ç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>cDDPMæ˜¾ç¤ºå‡ºåœ¨ä¸´åºŠå®è·µä¸­åº”ç”¨çš„æ½œåŠ›ï¼Œå¯æ”¯æŒæ›´å¿«ã€æ›´é«˜è´¨é‡çš„ç»ç’ƒä½“æˆåƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e80c4c1ad5db82e546b3d56c9f808cd9" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Evolve-to-Inspire-Novelty-Search-for-Diverse-Image-Generation"><a href="#Evolve-to-Inspire-Novelty-Search-for-Diverse-Image-Generation" class="headerlink" title="Evolve to Inspire: Novelty Search for Diverse Image Generation"></a>Evolve to Inspire: Novelty Search for Diverse Image Generation</h2><p><strong>Authors:Alex Inch, Passawis Chaiyapattanaporn, Yuchen Zhu, Yuan Lu, Ting-Wen Ko, Davide Paglieri</strong></p>
<p>Text-to-image diffusion models, while proficient at generating high-fidelity images, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹è™½ç„¶åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é€šå¸¸å­˜åœ¨è¾“å‡ºå¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨æ¢ç´¢å’Œåˆ›æ„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç°æœ‰çš„æç¤ºä¼˜åŒ–æŠ€æœ¯é€šå¸¸é’ˆå¯¹å®¡ç¾é€‚åº”æ€§ï¼Œæˆ–è€…ä¸é€‚åˆåˆ›æ„è§†è§‰é¢†åŸŸã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬å¼•å…¥äº†WANDERï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ–°é¢–æ€§æœç´¢çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å•ä¸ªè¾“å…¥æç¤ºç”Ÿæˆå¤šç§å›¾åƒé›†ã€‚WANDERç›´æ¥åœ¨è‡ªç„¶è¯­è¨€æç¤ºä¸Šæ“ä½œï¼Œé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå›¾åƒé›†çš„è¯­ä¹‰æ¼”å˜ï¼Œå¹¶ä½¿ç”¨CLIPåµŒå…¥æ¥é‡åŒ–æ–°é¢–æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åº”ç”¨äº†å‘å°„å™¨æ¥å¼•å¯¼æœç´¢è¿›å…¥æç¤ºç©ºé—´çš„ä¸åŒåŒºåŸŸï¼Œå¹¶è¯æ˜å®ƒä»¬å¯ä»¥æé«˜ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ã€‚ä½¿ç”¨FLUX-DEVè¿›è¡Œç”Ÿæˆå’ŒGPT-4o-miniè¿›è¡Œå˜å¼‚çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œåœ¨å¤šæ ·æ€§æŒ‡æ ‡æ–¹é¢ï¼ŒWANDERæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¿›åŒ–æç¤ºä¼˜åŒ–åŸºçº¿ã€‚æ¶ˆèç ”ç©¶è¯å®äº†å‘å°„å™¨çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00686v1">PDF</a> 14 pages, 10 figures, Accepted to Neurips 2025 GenProCC Workshop</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸­çš„æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†è¾“å‡ºçš„å¤šæ ·æ€§å—é™ï¼Œé™åˆ¶äº†å…¶åœ¨æ¢ç´¢å’Œåˆ›æ„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç°æœ‰æç¤ºä¼˜åŒ–æŠ€æœ¯ä¸»è¦é’ˆå¯¹ç¾å­¦é€‚åº”æ€§ï¼Œå¹¶ä¸é€‚åˆåˆ›æ„è§†è§‰é¢†åŸŸã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæ–°é¢–æ€§æœç´¢çš„WANDERæ–¹æ³•ï¼Œä»å•ä¸ªè¾“å…¥æç¤ºç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒé›†ã€‚WANDERç›´æ¥æ“ä½œè‡ªç„¶è¯­è¨€æç¤ºï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå›¾åƒé›†çš„è¯­ä¹‰æ¼”å˜ï¼Œå¹¶ä½¿ç”¨CLIPåµŒå…¥æ¥é‡åŒ–æ–°é¢–æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åº”ç”¨å‘å°„å™¨æ¥å¼•å¯¼æœç´¢è¿›å…¥æç¤ºç©ºé—´çš„ç‰¹å®šåŒºåŸŸï¼Œå¹¶è¯æ˜å…¶èƒ½æé«˜ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ã€‚é€šè¿‡FLUX-DEVçš„ç”Ÿæˆå’ŒGPT-4o-miniçš„å˜å¼‚è¿›è¡Œå®è¯ç ”ç©¶ï¼Œè¡¨æ˜WANDERåœ¨å¤šæ ·æ€§æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¿›åŒ–æç¤ºä¼˜åŒ–åŸºçº¿ã€‚æ¶ˆèç ”ç©¶è¯å®äº†å‘å°„å™¨çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶å­˜åœ¨è¾“å‡ºå¤šæ ·æ€§å—é™çš„é—®é¢˜ï¼Œå½±å“äº†å…¶åœ¨æ¢ç´¢å’Œåˆ›æ„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>ç°æœ‰æç¤ºä¼˜åŒ–æŠ€æœ¯ä¸»è¦å…³æ³¨ç¾å­¦é€‚åº”æ€§ï¼Œä¸é€‚ç”¨äºåˆ›æ„è§†è§‰é¢†åŸŸã€‚</li>
<li>WANDERæ–¹æ³•é€šè¿‡åŸºäºæ–°é¢–æ€§æœç´¢çš„æ–¹å¼ï¼Œä»å•ä¸ªè¾“å…¥æç¤ºç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒé›†ã€‚</li>
<li>WANDERåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå›¾åƒé›†çš„è¯­ä¹‰æ¼”å˜ï¼Œå¹¶ä½¿ç”¨CLIPåµŒå…¥é‡åŒ–æ–°é¢–æ€§ã€‚</li>
<li>å‘å°„å™¨çš„åº”ç”¨èƒ½å¤Ÿå¼•å¯¼æœç´¢è¿›å…¥æç¤ºç©ºé—´çš„ç‰¹å®šåŒºåŸŸï¼Œæé«˜ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ã€‚</li>
<li>å®è¯ç ”ç©¶è¯æ˜ï¼ŒWANDERåœ¨å¤šæ ·æ€§æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„è¿›åŒ–æç¤ºä¼˜åŒ–åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20317f97b00a8967a70beebd0de70088" align="middle">
<img src="https://picx.zhimg.com/v2-a546a01ed0d615dde58a880b80ac663d" align="middle">
<img src="https://picx.zhimg.com/v2-d0d05be9f130c0bda8a86494f336c52c" align="middle">
<img src="https://picx.zhimg.com/v2-acd45f3bc55aa8b2e53bfaebbe8fb136" align="middle">
<img src="https://picx.zhimg.com/v2-11c7820388adf050aa357f55c56e3723" align="middle">
<img src="https://picx.zhimg.com/v2-280fa4418a987d603edf342130f6464f" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FreeArt3D-Training-Free-Articulated-Object-Generation-using-3D-Diffusion"><a href="#FreeArt3D-Training-Free-Articulated-Object-Generation-using-3D-Diffusion" class="headerlink" title="FreeArt3D: Training-Free Articulated Object Generation using 3D   Diffusion"></a>FreeArt3D: Training-Free Articulated Object Generation using 3D   Diffusion</h2><p><strong>Authors:Chuhao Chen, Isabella Liu, Xinyue Wei, Hao Su, Minghua Liu</strong></p>
<p>Articulated 3D objects are central to many applications in robotics, AR&#x2F;VR, and animation. Recent approaches to modeling such objects either rely on optimization-based reconstruction pipelines that require dense-view supervision or on feed-forward generative models that produce coarse geometric approximations and often overlook surface texture. In contrast, open-world 3D generation of static objects has achieved remarkable success, especially with the advent of native 3D diffusion models such as Trellis. However, extending these methods to articulated objects by training native 3D diffusion models poses significant challenges. In this work, we present FreeArt3D, a training-free framework for articulated 3D object generation. Instead of training a new model on limited articulated data, FreeArt3D repurposes a pre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape prior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by treating articulation as an additional generative dimension. Given a few images captured in different articulation states, FreeArt3D jointly optimizes the objectâ€™s geometry, texture, and articulation parameters without requiring task-specific training or access to large-scale articulated datasets. Our method generates high-fidelity geometry and textures, accurately predicts underlying kinematic structures, and generalizes well across diverse object categories. Despite following a per-instance optimization paradigm, FreeArt3D completes in minutes and significantly outperforms prior state-of-the-art approaches in both quality and versatility. Please check our website for more details: <a target="_blank" rel="noopener" href="https://czzzzh.github.io/FreeArt3D">https://czzzzh.github.io/FreeArt3D</a> </p>
<blockquote>
<p>å…³èŠ‚å¼3Dç‰©ä½“åœ¨æœºå™¨äººæŠ€æœ¯ã€å¢å¼ºç°å®&#x2F;è™šæ‹Ÿç°å®å’ŒåŠ¨ç”»ç­‰å¤šä¸ªåº”ç”¨ä¸­å æ®æ ¸å¿ƒåœ°ä½ã€‚æœ€è¿‘çš„å»ºæ¨¡æ–¹æ³•è¦ä¹ˆä¾èµ–äºéœ€è¦å¯†é›†è§†å›¾ç›‘ç£çš„ä¼˜åŒ–é‡å»ºæµç¨‹ï¼Œè¦ä¹ˆä¾èµ–äºå‰é¦ˆç”Ÿæˆæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä¼šäº§ç”Ÿç²—ç³™çš„å‡ ä½•è¿‘ä¼¼å¹¶ç»å¸¸å¿½ç•¥è¡¨é¢çº¹ç†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé™æ€ç‰©ä½“çš„å¼€æ”¾ä¸–ç•Œ3Dç”Ÿæˆå·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå°¤å…¶æ˜¯éšç€æœ¬åœ°3Dæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Trellisï¼‰çš„å‡ºç°ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å…³èŠ‚å¼ç‰©ä½“é€šè¿‡è®­ç»ƒæœ¬åœ°3Dæ‰©æ•£æ¨¡å‹æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FreeArt3Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å…³èŠ‚å¼3Dç‰©ä½“ç”Ÿæˆæ¡†æ¶ã€‚FreeArt3Dä¸éœ€è¦åœ¨æœ‰é™çš„å…³èŠ‚å¼æ•°æ®ä¸Šè®­ç»ƒæ–°æ¨¡å‹ï¼Œè€Œæ˜¯å°†é¢„è®­ç»ƒçš„é™æ€3Dæ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚Trellisï¼‰ä½œä¸ºå¼ºå¤§çš„å½¢çŠ¶å…ˆéªŒã€‚å®ƒé€šè¿‡æ‰©å±•è¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰åˆ°3Dåˆ°4Dé¢†åŸŸï¼Œé€šè¿‡å°†å…³èŠ‚è¿åŠ¨è§†ä¸ºé¢å¤–çš„ç”Ÿæˆç»´åº¦æ¥å®ç°ã€‚ç»™å®šåœ¨ä¸åŒå…³èŠ‚çŠ¶æ€ä¸‹æ•è·çš„å‡ å¼ å›¾åƒï¼ŒFreeArt3Dè”åˆä¼˜åŒ–ç‰©ä½“çš„å‡ ä½•ã€çº¹ç†å’Œå…³èŠ‚å‚æ•°ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæˆ–è®¿é—®å¤§è§„æ¨¡å…³èŠ‚æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†é«˜ä¿çœŸåº¦çš„å‡ ä½•å½¢çŠ¶å’Œçº¹ç†ï¼Œå‡†ç¡®é¢„æµ‹äº†æ½œåœ¨çš„è¿åŠ¨å­¦ç»“æ„ï¼Œå¹¶åœ¨ä¸åŒçš„ç‰©ä½“ç±»åˆ«ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚å°½ç®¡éµå¾ªäº†æŒ‰å®ä¾‹ä¼˜åŒ–çš„æ¨¡å¼ï¼Œä½†FreeArt3Då¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆï¼Œå¹¶ä¸”åœ¨è´¨é‡å’Œé€šç”¨æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ï¼š[<a target="_blank" rel="noopener" href="https://czzzzh.github.io/FreeArt3D/]">https://czzzzh.github.io/FreeArt3D/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25765v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://czzzzh.github.io/FreeArt3D">https://czzzzh.github.io/FreeArt3D</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/CzzzzH/FreeArt3D">https://github.com/CzzzzH/FreeArt3D</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†FreeArt3Dæ¡†æ¶åœ¨è®­ç»ƒæ— å…³çš„æƒ…å†µä¸‹ç”Ÿæˆå¯åŠ¨ä¸‰ç»´ç‰©ä½“çš„èƒ½åŠ›ã€‚å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„ä¸‰ç»´æ‰©æ•£æ¨¡å‹ä½œä¸ºå½¢çŠ¶å…ˆéªŒï¼Œé€šè¿‡æ‰©å±•å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰åˆ°ä¸‰ç»´åˆ°å››ç»´é¢†åŸŸï¼Œå°†å¯åŠ¨æ€§è§†ä¸ºé¢å¤–çš„ç”Ÿæˆç»´åº¦ã€‚ç»™å®šå‡ å¼ åœ¨ä¸åŒå¯åŠ¨çŠ¶æ€ä¸‹çš„å›¾åƒï¼ŒFreeArt3Då¯ä»¥è”åˆä¼˜åŒ–ç‰©ä½“çš„å‡ ä½•ã€çº¹ç†å’Œå¯åŠ¨å‚æ•°ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒæˆ–å¤§è§„æ¨¡çš„å¯åŠ¨æ•°æ®é›†æ”¯æŒã€‚è¯¥æ–¹æ³•ç”Ÿæˆçš„é«˜ä¿çœŸå‡ ä½•å’Œçº¹ç†èƒ½å‡†ç¡®é¢„æµ‹åº•å±‚è¿åŠ¨å­¦ç»“æ„ï¼Œå¹¶åœ¨å¤šç§ç‰©ä½“ç±»åˆ«ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚å°½ç®¡é‡‡ç”¨æŒ‰å®ä¾‹ä¼˜åŒ–çš„æ¨¡å¼ï¼Œä½†FreeArt3Då¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆï¼Œå¹¶åœ¨è´¨é‡å’Œçµæ´»æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚è¯¦æƒ…è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>FreeArt3Dæ˜¯ä¸€ä¸ªè®­ç»ƒæ— å…³çš„ä¸‰ç»´ç‰©ä½“ç”Ÿæˆæ¡†æ¶ï¼Œé€‚ç”¨äºæœºå™¨äººã€AR&#x2F;VRå’ŒåŠ¨ç”»ç­‰å¤šä¸ªåº”ç”¨ã€‚</li>
<li>å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„ä¸‰ç»´æ‰©æ•£æ¨¡å‹ä½œä¸ºå½¢çŠ¶å…ˆéªŒï¼Œå°†å¯åŠ¨æ€§è§†ä¸ºé¢å¤–çš„ç”Ÿæˆç»´åº¦ã€‚</li>
<li>FreeArt3Dé€šè¿‡æ‰©å±•å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰åˆ°ä¸‰ç»´åˆ°å››ç»´é¢†åŸŸï¼Œå®ç°å¯¹ç‰©ä½“å‡ ä½•ã€çº¹ç†å’Œå¯åŠ¨å‚æ•°çš„è”åˆä¼˜åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½åœ¨æ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒæˆ–å¤§è§„æ¨¡å¯åŠ¨æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼ŒåŸºäºå‡ å¼ ä¸åŒçŠ¶æ€ä¸‹çš„å›¾åƒç”Ÿæˆé«˜ä¿çœŸå‡ ä½•å’Œçº¹ç†ã€‚</li>
<li>FreeArt3Dèƒ½å‡†ç¡®é¢„æµ‹åº•å±‚è¿åŠ¨å­¦ç»“æ„ï¼Œå¹¶åœ¨å¤šç§ç‰©ä½“ç±»åˆ«ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚</li>
<li>é‡‡ç”¨æŒ‰å®ä¾‹ä¼˜åŒ–çš„æ¨¡å¼ï¼ŒFreeArt3Dèƒ½åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆæ“ä½œã€‚</li>
<li>FreeArt3Dåœ¨è´¨é‡å’Œçµæ´»æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25765">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-017c324f687adfc267e24ec719c98a7d" align="middle">
<img src="https://picx.zhimg.com/v2-1fe8652a454b830564880bea7f018463" align="middle">
<img src="https://picx.zhimg.com/v2-4c3d257faa9e454e4eb0ed9024fc6be7" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MediQ-GAN-Quantum-Inspired-GAN-for-High-Resolution-Medical-Image-Generation"><a href="#MediQ-GAN-Quantum-Inspired-GAN-for-High-Resolution-Medical-Image-Generation" class="headerlink" title="MediQ-GAN: Quantum-Inspired GAN for High Resolution Medical Image   Generation"></a>MediQ-GAN: Quantum-Inspired GAN for High Resolution Medical Image   Generation</h2><p><strong>Authors:Qingyue Jiao, Yongcan Tang, Jun Zhuang, Jason Cong, Yiyu Shi</strong></p>
<p>Machine learning-assisted diagnosis shows promise, yet medical imaging datasets are often scarce, imbalanced, and constrained by privacy, making data augmentation essential. Classical generative models typically demand extensive computational and sample resources. Quantum computing offers a promising alternative, but existing quantum-based image generation methods remain limited in scale and often face barren plateaus. We present MediQ-GAN, a quantum-inspired GAN with prototype-guided skip connections and a dual-stream generator that fuses classical and quantum-inspired branches. Its variational quantum circuits inherently preserve full-rank mappings, avoid rank collapse, and are theory-guided to balance expressivity with trainability. Beyond generation quality, we provide the first latent-geometry and rank-based analysis of quantum-inspired GANs, offering theoretical insight into their performance. Across three medical imaging datasets, MediQ-GAN outperforms state-of-the-art GANs and diffusion models. While validated on IBM hardware for robustness, our contribution is hardware-agnostic, offering a scalable and data-efficient framework for medical image generation and augmentation. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ è¾…åŠ©è¯Šæ–­å…·æœ‰å¹¿é˜”å‰æ™¯ï¼Œä½†åŒ»å­¦æˆåƒæ•°æ®é›†å¾€å¾€ç¨€ç¼ºã€åˆ†å¸ƒä¸å‡ä¸”å—åˆ°éšç§é™åˆ¶ï¼Œè¿™ä½¿å¾—æ•°æ®å¢å¼ºå˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—å’Œæ ·æœ¬èµ„æºã€‚é‡å­è®¡ç®—æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„åŸºäºé‡å­å›¾åƒçš„ç”Ÿæˆæ–¹æ³•ä»ç„¶è§„æ¨¡æœ‰é™ï¼Œå¹¶ç»å¸¸é¢ä¸´è’èŠœçš„å¹³å°æœŸã€‚æˆ‘ä»¬æå‡ºäº†MediQ-GANï¼Œè¿™æ˜¯ä¸€ä¸ªå—é‡å­å¯å‘çš„GANï¼Œå…·æœ‰åŸå‹å¼•å¯¼çš„è·³è¿‡è¿æ¥å’ŒåŒæµç”Ÿæˆå™¨ï¼Œèåˆäº†ç»å…¸å’Œé‡å­å¯å‘åˆ†æ”¯ã€‚å…¶å˜åˆ†é‡å­ç”µè·¯å›ºæœ‰çš„ä¿ç•™äº†å…¨ç§©æ˜ å°„ï¼Œé¿å…äº†ç§©å´©æºƒï¼Œå¹¶åœ¨ç†è®ºçš„æŒ‡å¯¼ä¸‹å¹³è¡¡äº†è¡¨è¾¾ä¸å¯è®­ç»ƒæ€§ã€‚é™¤äº†ç”Ÿæˆè´¨é‡å¤–ï¼Œæˆ‘ä»¬è¿˜é¦–æ¬¡å¯¹é‡å­å¯å‘çš„GANè¿›è¡Œäº†æ½œå‡ ä½•å’Œç§©åˆ†æï¼Œä¸ºå…¶æ€§èƒ½æä¾›äº†ç†è®ºæ´å¯Ÿã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šï¼ŒMediQ-GANä¼˜äºæœ€å…ˆè¿›çš„GANå’Œæ‰©æ•£æ¨¡å‹ã€‚è™½ç„¶åœ¨IBMç¡¬ä»¶ä¸Šè¿›è¡Œäº†ç¨³å¥æ€§éªŒè¯ï¼Œä½†æˆ‘ä»¬çš„è´¡çŒ®ä¸ç¡¬ä»¶æ— å…³ï¼Œä¸ºåŒ»å­¦å›¾åƒç”Ÿæˆå’Œå¢å¼ºæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•å’Œæ•°æ®é«˜æ•ˆæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21015v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ è¾…åŠ©è¯Šæ–­å…·æœ‰å¹¿é˜”å‰æ™¯ï¼Œä½†ç”±äºåŒ»ç–—å½±åƒæ•°æ®é›†ç¨€ç¼ºã€ä¸å‡è¡¡ä¸”å—éšç§é™åˆ¶ï¼Œæ•°æ®å¢å¼ºè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹éœ€è¦å¤§é‡è®¡ç®—å’Œæ ·æœ¬èµ„æºã€‚é‡å­è®¡ç®—æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰åŸºäºé‡å­å›¾åƒç”Ÿæˆæ–¹æ³•è§„æ¨¡å’Œå®ç”¨æ€§æœ‰é™ã€‚æˆ‘ä»¬æå‡ºMediQ-GANï¼Œä¸€ä¸ªèåˆç»å…¸ä¸é‡å­åˆ†æ”¯çš„åŸå‹å¼•å¯¼è·³è·ƒè¿æ¥å’ŒåŒé‡æµç”Ÿæˆå™¨æ‰€æ„æˆçš„é‡å­å¯å‘GANã€‚å…¶å˜åˆ†é‡å­ç”µè·¯èƒ½å¤Ÿä¿ç•™å…¨ç§©æ˜ å°„ã€é¿å…ç§©å´©æºƒï¼Œå¹¶èƒ½åœ¨ç†è®ºæŒ‡å¯¼ä¸‹å®ç°è¡¨è¾¾åŠ›ä¸è®­ç»ƒèƒ½åŠ›ä¹‹é—´çš„å¹³è¡¡ã€‚é™¤ç”Ÿæˆè´¨é‡å¤–ï¼Œæˆ‘ä»¬è¿˜é¦–æ¬¡å¯¹é‡å­å¯å‘GANè¿›è¡Œæ½œåœ¨å‡ ä½•å’Œç§©åˆ†æï¼Œå¯¹å…¶æ€§èƒ½æä¾›ç†è®ºè§è§£ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šï¼ŒMediQ-GANä¼˜äºæœ€æ–°GANå’Œæ‰©æ•£æ¨¡å‹ã€‚åœ¨IBMç¡¬ä»¶ä¸ŠéªŒè¯å…¶ç¨³å¥æ€§ï¼Œæˆ‘ä»¬çš„è´¡çŒ®å…·æœ‰ç¡¬ä»¶æ— å…³æ€§ï¼Œä¸ºåŒ»å­¦å½±åƒç”Ÿæˆå’Œå¢å¼ºæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ è¾…åŠ©è¯Šæ–­å…·æœ‰å¹¿é˜”å‰æ™¯ï¼Œæ•°æ®å¢å¼ºå¯¹äºè§£å†³åŒ»ç–—å½±åƒæ•°æ®é›†ç¨€ç¼ºã€ä¸å‡è¡¡å’Œéšç§é—®é¢˜è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹éœ€è¦å¤§é‡è®¡ç®—å’Œæ ·æœ¬èµ„æºï¼Œè€Œé‡å­è®¡ç®—æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆç”¨äºè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>MediQ-GANæ˜¯ä¸€ä¸ªèåˆäº†ç»å…¸å’Œé‡å­åˆ†æ”¯çš„æ–°å‹GANæ¨¡å‹ï¼Œå®ƒç»“åˆäº†åŸå‹å¼•å¯¼è·³è·ƒè¿æ¥å’ŒåŒé‡æµç”Ÿæˆå™¨ã€‚</li>
<li>å˜åˆ†é‡å­ç”µè·¯æ˜¯MediQ-GANçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œå®ƒèƒ½ä¿ç•™å…¨ç§©æ˜ å°„å¹¶é¿å…ç§©å´©æºƒï¼ŒåŒæ—¶å¹³è¡¡è¡¨è¾¾åŠ›å’Œè®­ç»ƒèƒ½åŠ›ã€‚</li>
<li>é™¤äº†ç”Ÿæˆè´¨é‡å¤–ï¼Œè¿˜å¯¹é‡å­å¯å‘GANè¿›è¡Œäº†æ½œåœ¨å‡ ä½•å’Œç§©çš„ç†è®ºåˆ†æï¼Œä»¥æ·±å…¥äº†è§£å…¶æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMediQ-GANçš„æ€§èƒ½ä¼˜äºå½“å‰çš„æœ€æ–°GANå’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d851a33ebf2b3fd494091250aa67bf27" align="middle">
<img src="https://picx.zhimg.com/v2-0ed5eebdc9768da734fd0496729bdd36" align="middle">
<img src="https://picx.zhimg.com/v2-ab8fdd2d8d6af91df074f4605e1b8a90" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Remasking-Discrete-Diffusion-Models-with-Inference-Time-Scaling"><a href="#Remasking-Discrete-Diffusion-Models-with-Inference-Time-Scaling" class="headerlink" title="Remasking Discrete Diffusion Models with Inference-Time Scaling"></a>Remasking Discrete Diffusion Models with Inference-Time Scaling</h2><p><strong>Authors:Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, Volodymyr Kuleshov</strong></p>
<p>Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: <a target="_blank" rel="noopener" href="https://remdm.github.io/">https://remdm.github.io</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„éƒ¨åˆ†æˆåŠŸæºäºå…¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–çš„èƒ½åŠ›ï¼Œå³ç”Ÿæˆè¿‡ç¨‹ä¸­åå¤ä¿®æ­£è¾“å‡ºã€‚ç„¶è€Œï¼Œç°ä»£æ©ç ç¦»æ•£æ‰©æ•£ç¼ºä¹è¿™ç§èƒ½åŠ›ï¼šä¸€æ—¦ç”Ÿæˆæ ‡è®°ï¼Œå³ä½¿å‡ºç°é”™è¯¯ï¼Œä¹Ÿæ— æ³•å†æ¬¡æ›´æ–°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥é‡æ–°æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆReMDMï¼‰é‡‡æ ·å™¨æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œè¿™æ˜¯ä¸€ç§å¯ä»¥åº”ç”¨äºé¢„è®­ç»ƒæ©ç æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå®ƒæ¥æºäºå…·æœ‰è‡ªå®šä¹‰åå‘æ©ç è¿‡ç¨‹çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€‚æœ€æœ‰è¶£çš„æ˜¯ï¼ŒReMDMä¸ºç¦»æ•£æ‰©æ•£èµ‹äºˆäº†æ¨ç†æ—¶é—´è®¡ç®—ç¼©æ”¾çš„å½¢å¼ã€‚é€šè¿‡å¢åŠ é‡‡æ ·æ­¥éª¤çš„æ•°é‡ï¼ŒReMDMç”Ÿæˆçš„è‡ªç„¶è¯­è¨€è¾“å‡ºæ¥è¿‘è‡ªå›å½’æ¨¡å‹çš„è´¨é‡ï¼Œè€Œåœ¨è®¡ç®—é¢„ç®—æœ‰é™çš„æƒ…å†µä¸‹ï¼ŒReMDMèƒ½æ›´å¥½åœ°ä¿æŒè´¨é‡ã€‚ReMDMè¿˜æé«˜äº†ç¦»æ•£å›¾åƒæ©ç æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬è´¨é‡ï¼Œåœ¨ç§‘å­¦é¢†åŸŸå¦‚åˆ†å­è®¾è®¡æ–¹é¢ï¼ŒReMDMä¿ƒè¿›äº†æ‰©æ•£æŒ‡å¯¼å¹¶æ¨åŠ¨äº†ä¸ç»å…¸æ©ç å’Œå‡åŒ€å™ªå£°æ‰©æ•£ç›¸æ¯”çš„å¯æ§æ€§å¸•ç´¯æ‰˜å‰æ²¿ã€‚æˆ‘ä»¬å·²åœ¨é¡¹ç›®é¡µé¢ä¸Šæä¾›ä»£ç å’Œåšå®¢æ–‡ç« ï¼š<a target="_blank" rel="noopener" href="https://remdm.github.io/">https://remdm.github.io</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00307v3">PDF</a> NeurIPS 2025. Project page: <a target="_blank" rel="noopener" href="https://remdm.github.io/">https://remdm.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„ä¸€éƒ¨åˆ†æˆåŠŸåœ¨äºå…¶èƒ½å¤Ÿè¿›è¡Œè¿­ä»£ä¼˜åŒ–çš„èƒ½åŠ›ï¼Œå³ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸æ–­ä¿®æ­£è¾“å‡ºã€‚ç„¶è€Œï¼Œç°ä»£æ©ç ç¦»æ•£æ‰©æ•£ç¼ºä¹è¿™ç§èƒ½åŠ›ï¼šä¸€æ—¦ç”Ÿæˆæ ‡è®°ï¼Œå³ä½¿å‡ºç°é”™è¯¯ä¹Ÿæ— æ³•å†æ¬¡æ›´æ–°ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é‡æ–°æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆReMDMï¼‰é‡‡æ ·å™¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯åº”ç”¨äºé¢„è®­ç»ƒçš„æ©ç æ‰©æ•£æ¨¡å‹ï¼Œæºè‡ªç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œå…·æœ‰è‡ªå®šä¹‰çš„é‡æ–°æ©ç åå‘è¿‡ç¨‹ã€‚ReMDMèµ‹äºˆäº†ç¦»æ•£æ‰©æ•£ä¸€ç§å½¢å¼çš„æ¨æ–­æ—¶é—´è®¡ç®—ç¼©æ”¾ã€‚é€šè¿‡å¢åŠ é‡‡æ ·æ­¥éª¤çš„æ•°é‡ï¼ŒReMDMç”Ÿæˆçš„è‡ªç„¶è¯­è¨€è¾“å‡ºæ¥è¿‘è‡ªå›å½’æ¨¡å‹çš„è´¨é‡ï¼Œè€Œå½“è®¡ç®—é¢„ç®—æœ‰é™æ—¶ï¼ŒReMDMèƒ½æ›´å¥½åœ°ä¿æŒè´¨é‡ã€‚æ­¤å¤–ï¼ŒReMDMè¿˜æé«˜äº†æ©ç æ‰©æ•£æ¨¡å‹åœ¨ç¦»æ•£å›¾åƒä»¥åŠç§‘å­¦é¢†åŸŸï¼ˆå¦‚åˆ†å­è®¾è®¡ï¼‰çš„æ ·æœ¬è´¨é‡ï¼Œå¹¶æ¨åŠ¨äº†ç›¸å¯¹äºä¼ ç»Ÿæ©ç å’Œå‡åŒ€å™ªå£°æ‰©æ•£çš„å¯æ§æ€§çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è¿­ä»£ä¼˜åŒ–ç”Ÿæˆæ›´å‡†ç¡®çš„è¾“å‡ºã€‚</li>
<li>ç°ä»£æ©ç ç¦»æ•£æ‰©æ•£æ— æ³•æ›´æ–°å·²ç”Ÿæˆçš„æ ‡è®°ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ReMDMé‡‡æ ·å™¨è§£å†³äº†è¿™ä¸€å±€é™æ€§ï¼Œæé«˜äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ReMDMé€šè¿‡å¢åŠ é‡‡æ ·æ­¥éª¤çš„æ•°é‡ï¼Œå¯ä»¥ç”Ÿæˆæ¥è¿‘è‡ªå›å½’æ¨¡å‹è´¨é‡çš„è‡ªç„¶è¯­è¨€è¾“å‡ºã€‚</li>
<li>ReMDMåœ¨ä¸åŒè®¡ç®—é¢„ç®—ä¸‹éƒ½èƒ½ä¿æŒé«˜è´¨é‡è¾“å‡ºã€‚</li>
<li>ReMDMæé«˜äº†æ©ç æ‰©æ•£æ¨¡å‹åœ¨ç¦»æ•£å›¾åƒé¢†åŸŸçš„æ ·æœ¬è´¨é‡ã€‚</li>
<li>åœ¨ç§‘å­¦é¢†åŸŸå¦‚åˆ†å­è®¾è®¡ï¼ŒReMDMæ¨åŠ¨äº†æ‰©æ•£æ¨¡å‹çš„å¯æ§æ€§å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5888146037f503eb83f0d8b6fb74036f" align="middle">
<img src="https://picx.zhimg.com/v2-9fb34e54c244a25d7dbdfe2dc976784d" align="middle">
<img src="https://picx.zhimg.com/v2-842b3e529dce5bcd1f899ddd10233fd9" align="middle">
<img src="https://picx.zhimg.com/v2-fc23466909c51c22e5178b8afea170ca" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Cross-modal-Diffusion-Modelling-for-Super-resolved-Spatial-Transcriptomics"><a href="#Cross-modal-Diffusion-Modelling-for-Super-resolved-Spatial-Transcriptomics" class="headerlink" title="Cross-modal Diffusion Modelling for Super-resolved Spatial   Transcriptomics"></a>Cross-modal Diffusion Modelling for Super-resolved Spatial   Transcriptomics</h2><p><strong>Authors:Xiaofei Wang, Xingxu Huang, Stephen J. Price, Chao Li</strong></p>
<p>The recent advancement of spatial transcriptomics (ST) allows to characterize spatial gene expression within tissue for discovery research. However, current ST platforms suffer from low resolution, hindering in-depth understanding of spatial gene expression. Super-resolution approaches promise to enhance ST maps by integrating histology images with gene expressions of profiled tissue spots. However, current super-resolution methods are limited by restoration uncertainty and mode collapse. Although diffusion models have shown promise in capturing complex interactions between multi-modal conditions, it remains a challenge to integrate histology images and gene expression for super-resolved ST maps. This paper proposes a cross-modal conditional diffusion model for super-resolving ST maps with the guidance of histology images. Specifically, we design a multi-modal disentangling network with cross-modal adaptive modulation to utilize complementary information from histology images and spatial gene expression. Moreover, we propose a dynamic cross-attention modelling strategy to extract hierarchical cell-to-tissue information from histology images. Lastly, we propose a co-expression-based gene-correlation graph network to model the co-expression relationship of multiple genes. Experiments show that our method outperforms other state-of-the-art methods in ST super-resolution on three public datasets. </p>
<blockquote>
<p>è¿‘æœŸç©ºé—´è½¬å½•å­¦ï¼ˆSTï¼‰çš„è¿›å±•å…è®¸åœ¨å‘ç°ç ”ç©¶ä¸­å¯¹ç»„ç»‡å†…çš„ç©ºé—´åŸºå› è¡¨è¾¾è¿›è¡Œç‰¹å¾æè¿°ã€‚ç„¶è€Œï¼Œå½“å‰STå¹³å°çš„åˆ†è¾¨ç‡è¾ƒä½ï¼Œé˜»ç¢äº†å¯¹ç©ºé—´åŸºå› è¡¨è¾¾çš„æ·±å…¥ç†è§£ã€‚è¶…åˆ†è¾¨ç‡æ–¹æ³•é€šè¿‡å°†ç»„ç»‡æ–‘ç‚¹çš„ç»„ç»‡å›¾åƒä¸åŸºå› è¡¨è¾¾è¿›è¡Œæ•´åˆï¼Œæœ‰æœ›å¢å¼ºSTå›¾çš„åˆ†è¾¨ç‡ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•å—åˆ°æ¢å¤ä¸ç¡®å®šæ€§å’Œæ¨¡å¼å´©æºƒçš„é™åˆ¶ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨æ•æ‰å¤šæ¨¡æ€æ¡ä»¶ä¹‹é—´çš„å¤æ‚äº¤äº’æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å°†ç»„ç»‡å›¾åƒå’ŒåŸºå› è¡¨è¾¾æ•´åˆåˆ°è¶…åˆ†è¾¨ç‡STå›¾ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨ç»„ç»‡å›¾åƒçš„æŒ‡å¯¼æ¥è¿›è¡ŒSTå›¾çš„è¶…åˆ†è¾¨ç‡å¤„ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€åˆ†ç¦»ç½‘ç»œï¼Œå…·æœ‰è·¨æ¨¡æ€è‡ªé€‚åº”è°ƒåˆ¶åŠŸèƒ½ï¼Œä»¥åˆ©ç”¨ç»„ç»‡å›¾åƒå’Œç©ºé—´åŸºå› è¡¨è¾¾çš„äº’è¡¥ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€äº¤å‰æ³¨æ„åŠ›å»ºæ¨¡ç­–ç•¥ï¼Œä»ç»„ç»‡å›¾åƒä¸­æå–åˆ†å±‚ç»†èƒåˆ°ç»„ç»‡çš„ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå…±è¡¨è¾¾çš„åŸºå› å…³è”å›¾ç½‘ç»œï¼Œä»¥æ¨¡æ‹Ÿå¤šä¸ªåŸºå› çš„å…±è¡¨è¾¾å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„STè¶…åˆ†è¾¨ç‡æ€§èƒ½ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12973v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç©ºé—´è½¬å½•ç»„å­¦ï¼ˆSTï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå‘ç°ç ”ç©¶ä¸­ç»„ç»‡å†…ç©ºé—´åŸºå› è¡¨è¾¾çš„ç‰¹å¾æä¾›äº†å¯èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰STå¹³å°åˆ†è¾¨ç‡è¾ƒä½ï¼Œé™åˆ¶äº†ç©ºé—´åŸºå› è¡¨è¾¾çš„æ·±å…¥ç†è§£ã€‚è¶…åˆ†è¾¨ç‡æ–¹æ³•æœ‰æœ›é€šè¿‡æ•´åˆç»„ç»‡å…‰é•œå›¾åƒä¸åŸºå› è¡¨è¾¾æ•°æ®æ¥æå‡STå›¾è°±çš„åˆ†è¾¨ç‡ã€‚ä½†ç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•å—é™äºä¿®å¤ä¸ç¡®å®šæ€§å’Œæ¨¡å¼å´©æºƒé—®é¢˜ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨æ•æ‰å¤šæ¨¡å¼æ¡ä»¶é—´çš„å¤æ‚äº¤äº’æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å°†å…‰é•œå›¾åƒä¸åŸºå› è¡¨è¾¾æ•°æ®æ•´åˆä»¥ç”Ÿæˆè¶…åˆ†è¾¨ç‡STå›¾è°±ä»å…·æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»¥å…‰é•œå›¾åƒä¸ºæŒ‡å¯¼ï¼Œå®ç°STå›¾è°±çš„è¶…åˆ†è¾¨ç‡ã€‚å…·ä½“åœ°ï¼Œè®¾è®¡äº†ä¸€ç§å¤šæ¨¡æ€è§£è€¦ç½‘ç»œï¼Œå…·æœ‰è·¨æ¨¡æ€è‡ªé€‚åº”è°ƒåˆ¶åŠŸèƒ½ï¼Œä»¥åˆ©ç”¨å…‰é•œå›¾åƒå’ŒåŸºå› è¡¨è¾¾çš„äº’è¡¥ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†åŠ¨æ€è·¨æ³¨æ„åŠ›å»ºæ¨¡ç­–ç•¥ï¼Œä»å…‰é•œå›¾åƒä¸­æå–åˆ†å±‚ç»†èƒåˆ°ç»„ç»‡çš„ä¿¡æ¯ã€‚æœ€åï¼ŒåŸºäºå…±è¡¨è¾¾æå‡ºäº†åŸºå› å…³è”å›¾ç½‘ç»œï¼Œä»¥æ¨¡æ‹Ÿå¤šä¸ªåŸºå› çš„å…±è¡¨è¾¾å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„STè¶…åˆ†è¾¨ç‡è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´è½¬å½•ç»„å­¦ï¼ˆSTï¼‰èƒ½å¤Ÿæ­ç¤ºç»„ç»‡å†…åŸºå› è¡¨è¾¾çš„ç©ºé—´ç‰¹å¾ï¼Œä½†å½“å‰æŠ€æœ¯åˆ†è¾¨ç‡è¾ƒä½ã€‚</li>
<li>è¶…åˆ†è¾¨ç‡æ–¹æ³•å¯ä»¥é€šè¿‡ç»“åˆå…‰é•œå›¾åƒå’ŒåŸºå› è¡¨è¾¾æ•°æ®æå‡STå›¾è°±çš„åˆ†è¾¨ç‡ã€‚</li>
<li>å½“å‰è¶…åˆ†è¾¨ç‡æ–¹æ³•é¢ä¸´ä¿®å¤ä¸ç¡®å®šæ€§å’Œæ¨¡å¼å´©æºƒçš„æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ•æ‰å¤šæ¨¡å¼æ¡ä»¶é—´çš„äº¤äº’æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†æ•´åˆå…‰é•œå›¾åƒå’ŒåŸºå› è¡¨è¾¾æ•°æ®ä»¥å®ç°è¶…åˆ†è¾¨ç‡STå›¾è°±ä»å…·æŒ‘æˆ˜ã€‚</li>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå…‰é•œå›¾åƒæŒ‡å¯¼STå›¾è°±çš„è¶…åˆ†è¾¨ç‡ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¤šæ¨¡æ€è§£è€¦ç½‘ç»œã€åŠ¨æ€è·¨æ³¨æ„åŠ›å»ºæ¨¡å’ŒåŸºå› å…³è”å›¾ç½‘ç»œç­‰æŠ€æœ¯å®ç°ä¼˜ç§€æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.12973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55db1960b1f31c112a7966f54967af12" align="middle">
<img src="https://picx.zhimg.com/v2-92e2bc913b543f9d3d77749099699f87" align="middle">
<img src="https://picx.zhimg.com/v2-4b24ea9033807bae2bbc769b487f029d" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a89cda7d3982b44c11b295c82c2a1d2e" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Resource-efficient Automatic Refinement of Segmentations via Weak   Supervision from Light Feedback
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ab8fdd2d8d6af91df074f4605e1b8a90" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction   and Phenotyping
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
