<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Diffusion Models are Robust Pretrainers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ad4db1eb33984e939a0c754ef1116cd3')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-06-æ›´æ–°"><a href="#2025-11-06-æ›´æ–°" class="headerlink" title="2025-11-06 æ›´æ–°"></a>2025-11-06 æ›´æ–°</h1><h2 id="Diffusion-Models-are-Robust-Pretrainers"><a href="#Diffusion-Models-are-Robust-Pretrainers" class="headerlink" title="Diffusion Models are Robust Pretrainers"></a>Diffusion Models are Robust Pretrainers</h2><p><strong>Authors:Mika Yagoda, Shady Abu-Hussein, Raja Giryes</strong></p>
<p>Diffusion models have gained significant attention for high-fidelity image generation. Our work investigates the potential of exploiting diffusion models for adversarial robustness in image classification and object detection. Adversarial attacks challenge standard models in these tasks by perturbing inputs to force incorrect predictions. To address this issue, many approaches use training schemes for forcing the robustness of the models, which increase training costs. In this work, we study models built on top of off-the-shelf diffusion models and demonstrate their practical significance: they provide a low-cost path to robust representations, allowing lightweight heads to be trained on frozen features without full adversarial training. Our empirical evaluations on ImageNet, CIFAR-10, and PASCAL VOC show that diffusion-based classifiers and detectors achieve meaningful adversarial robustness with minimal compute. While clean and adversarial accuracies remain below state-of-the-art adversarially trained CNNs or ViTs, diffusion pretraining offers a favorable tradeoff between efficiency and robustness. This work opens a promising avenue for integrating diffusion models into resource-constrained robust deployments. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆæ–¹é¢å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚æˆ‘ä»¬çš„å·¥ä½œç ”ç©¶äº†å°†æ‰©æ•£æ¨¡å‹ç”¨äºå›¾åƒåˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ä¸­çš„å¯¹æŠ—æ€§ç¨³å¥æ€§çš„æ½œåŠ›ã€‚å¯¹æŠ—æ€§æ”»å‡»é€šè¿‡æ‰°åŠ¨è¾“å…¥æ¥è¿«ä½¿æ¨¡å‹åšå‡ºé”™è¯¯é¢„æµ‹ï¼Œä»è€ŒæŒ‘æˆ˜è¿™äº›ä»»åŠ¡çš„æ ‡å‡†æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®¸å¤šæ–¹æ³•ä½¿ç”¨è®­ç»ƒæ–¹æ¡ˆæ¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ï¼Œè¿™å¢åŠ äº†è®­ç»ƒæˆæœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºç°æˆçš„æ‰©æ•£æ¨¡å‹æ„å»ºçš„æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬çš„å®é™…æ„ä¹‰ï¼šå®ƒä»¬ä¸ºè·å¾—ç¨³å¥è¡¨ç¤ºæä¾›äº†ä½æˆæœ¬é€”å¾„ï¼Œå…è®¸åœ¨å†»ç»“çš„ç‰¹å¾ä¸Šè®­ç»ƒè½»é‡çº§å¤´éƒ¨ï¼Œè€Œæ— éœ€è¿›è¡Œå…¨é¢çš„å¯¹æŠ—æ€§è®­ç»ƒã€‚æˆ‘ä»¬åœ¨ImageNetã€CIFAR-10å’ŒPASCAL VOCä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„åˆ†ç±»å™¨å’Œæ£€æµ‹å™¨å®ç°äº†æœ‰æ„ä¹‰çš„å¯¹æŠ—æ€§ç¨³å¥æ€§ï¼Œè®¡ç®—é‡æœ€å°ã€‚è™½ç„¶å¹²å‡€å’Œå¯¹æŠ—æ€§å‡†ç¡®æ€§ä»ç„¶ä½äºæœ€å…ˆè¿›çš„å¯¹æŠ—æ€§è®­ç»ƒçš„CNNæˆ–ViTsï¼Œä½†æ‰©æ•£é¢„è®­ç»ƒåœ¨æ•ˆç‡å’Œç¨³å¥æ€§ä¹‹é—´æä¾›äº†æœ‰åˆ©çš„æƒè¡¡ã€‚è¿™é¡¹å·¥ä½œä¸ºå°†æ‰©æ•£æ¨¡å‹é›†æˆåˆ°èµ„æºå—é™çš„ç¨³å¥éƒ¨ç½²ä¸­æ‰“å¼€äº†æœ‰å‰é€”çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02793v1">PDF</a> To be published in IEEE Signal Processing Letters</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ä¸­çš„å¯¹æŠ—é²æ£’æ€§æ½œåŠ›ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå¯¹æŠ—æ€§æ”»å‡»é€šè¿‡æ‰°åŠ¨è¾“å…¥æ¥æŒ‘æˆ˜æ ‡å‡†æ¨¡å‹ï¼Œå¯¼è‡´é¢„æµ‹é”™è¯¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®¸å¤šæ–¹æ³•ä½¿ç”¨è®­ç»ƒæ–¹æ¡ˆæ¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ï¼Œè¿™å¢åŠ äº†è®­ç»ƒæˆæœ¬ã€‚è€Œæœ¬æ–‡ç ”ç©¶äº†åŸºäºç°æˆæ‰©æ•£æ¨¡å‹çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å®è·µæ„ä¹‰ï¼šå®ƒæä¾›äº†ä¸€ç§ä½æˆæœ¬é€”å¾„æ¥æ„å»ºç¨³å¥çš„è¡¨ç¤ºï¼Œå…è®¸åœ¨å†»ç»“çš„ç‰¹å¾ä¸Šè®­ç»ƒè½»é‡çº§å¤´éƒ¨ï¼Œæ— éœ€è¿›è¡Œå…¨é¢çš„å¯¹æŠ—æ€§è®­ç»ƒã€‚åœ¨ImageNetã€CIFAR-10å’ŒPASCAL VOCä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„åˆ†ç±»å™¨å’Œæ£€æµ‹å™¨åœ¨æœ‰é™çš„è®¡ç®—ä¸‹å®ç°äº†æœ‰æ„ä¹‰çš„å¯¹æŠ—é²æ£’æ€§ã€‚è™½ç„¶æ¸…æ´å’Œå¯¹æŠ—æ€§ç²¾åº¦ä»ä½äºæœ€å…ˆè¿›çš„å¯¹æŠ—æ€§è®­ç»ƒçš„CNNæˆ–ViTsï¼Œä½†æ‰©æ•£é¢„è®­ç»ƒåœ¨æ•ˆç‡å’Œç¨³å¥æ€§ä¹‹é—´æä¾›äº†æœ‰åˆ©çš„æƒè¡¡ã€‚æœ¬æ–‡å¼€è¾Ÿäº†ä¸€æ¡å°†æ‰©æ•£æ¨¡å‹é›†æˆåˆ°èµ„æºå—é™çš„ç¨³å¥éƒ¨ç½²ä¸­çš„æœ‰å‰é€”çš„é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»å’Œå¯¹è±¡æ£€æµ‹ä¸­çš„å¯¹æŠ—é²æ£’æ€§æ½œåŠ›ã€‚</li>
<li>å¯¹æŠ—æ€§æ”»å‡»é€šè¿‡æ‰°åŠ¨è¾“å…¥æŒ‘æˆ˜æ ‡å‡†æ¨¡å‹ã€‚</li>
<li>è®¸å¤šæ–¹æ³•ä½¿ç”¨æ˜‚è´µçš„è®­ç»ƒæ–¹æ¡ˆå¢å¼ºæ¨¡å‹é²æ£’æ€§ã€‚</li>
<li>åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¨¡å‹å±•ç¤ºå®è·µæ„ä¹‰ï¼šæä¾›ä½æˆæœ¬é€”å¾„æ„å»ºç¨³å¥è¡¨ç¤ºã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼Œæ‰©æ•£æ¨¡å‹åœ¨æœ‰é™çš„è®¡ç®—ä¸‹å®ç°äº†æœ‰æ„ä¹‰çš„å¯¹æŠ—é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5dd680485a0bb6adde50b70fecb3c7e" align="middle">
<img src="https://picx.zhimg.com/v2-c6b910eb9f27027eef20f80d48ffe4cb" align="middle">
<img src="https://picx.zhimg.com/v2-f6fdf7cdcde187d6bd513979951d5c0d" align="middle">
<img src="https://picx.zhimg.com/v2-e8b639eb9d818e324107bb81082e0efb" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PercHead-Perceptual-Head-Model-for-Single-Image-3D-Head-Reconstruction-Editing"><a href="#PercHead-Perceptual-Head-Model-for-Single-Image-3D-Head-Reconstruction-Editing" class="headerlink" title="PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction   &amp; Editing"></a>PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction   &amp; Editing</h2><p><strong>Authors:Antonio Oroz, Matthias NieÃŸner, Tobias Kirschstein</strong></p>
<p>We present PercHead, a method for single-image 3D head reconstruction and semantic 3D editing - two tasks that are inherently challenging due to severe view occlusions, weak perceptual supervision, and the ambiguity of editing in 3D space. We develop a unified base model for reconstructing view-consistent 3D heads from a single input image. The model employs a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. At the heart of our approach is a novel perceptual supervision strategy based on DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric and appearance fidelity. Our model achieves state-of-the-art performance in novel-view synthesis and, furthermore, exhibits exceptional robustness to extreme viewing angles compared to established baselines. Furthermore, this base model can be seamlessly extended for semantic 3D editing by swapping the encoder and finetuning the network. In this variant, we disentangle geometry and style through two distinct input modalities: a segmentation map to control geometry and either a text prompt or a reference image to specify appearance. We highlight the intuitive and powerful 3D editing capabilities of our model through a lightweight, interactive GUI, where users can effortlessly sculpt geometry by drawing segmentation maps and stylize appearance via natural language or image prompts.   Project Page: <a target="_blank" rel="noopener" href="https://antoniooroz.github.io/PercHead">https://antoniooroz.github.io/PercHead</a> Video: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4hFybgTk4kE">https://www.youtube.com/watch?v=4hFybgTk4kE</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†PercHeadæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå•å›¾åƒ3Då¤´éƒ¨é‡å»ºå’Œè¯­ä¹‰3Dç¼–è¾‘çš„æ–¹æ³•â€”â€”è¿™ä¸¤é¡¹ä»»åŠ¡ç”±äºä¸¥é‡çš„è§†å›¾é®æŒ¡ã€å¾®å¼±çš„æ„ŸçŸ¥ç›‘ç£ä»¥åŠ3Dç©ºé—´ç¼–è¾‘çš„æ¨¡ç³Šæ€§è€Œå…·æœ‰å›ºæœ‰çš„æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†æ¨¡å‹ï¼Œç”¨äºä»å•ä¸ªè¾“å…¥å›¾åƒé‡å»ºè§†å›¾ä¸€è‡´çš„3Då¤´éƒ¨ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯ç¼–ç å™¨ï¼Œåè·ŸåŸºäºViTçš„è§£ç å™¨ï¼Œé€šè¿‡è¿­ä»£äº¤å‰æ³¨æ„åŠ›å°†2Dç‰¹å¾æå‡åˆ°3Dç©ºé—´ã€‚æ¸²æŸ“æ˜¯ä½¿ç”¨é«˜æ–¯è´´å›¾æŠ€æœ¯å®Œæˆçš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ç§åŸºäºDINOv2å’ŒSAM2.1çš„æ–°å‹æ„ŸçŸ¥ç›‘ç£ç­–ç•¥ï¼Œå®ƒä¸ºå‡ ä½•å’Œå¤–è§‚ä¿çœŸåº¦æä¾›äº†ä¸°å¯Œçš„ä¸€èˆ¬åŒ–ä¿¡å·ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ–°è§†å›¾åˆæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸æ—¢å®šçš„åŸºå‡†çº¿ç›¸æ¯”ï¼Œå¯¹æç«¯è§†è§’è¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªåŸºå‡†æ¨¡å‹å¯ä»¥é€šè¿‡äº¤æ¢ç¼–ç å™¨å’Œå¾®è°ƒç½‘ç»œæ— ç¼æ‰©å±•åˆ°è¯­ä¹‰3Dç¼–è¾‘ã€‚åœ¨è¯¥å˜ç§ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸¤ç§ä¸åŒçš„è¾“å…¥æ¨¡å¼ï¼šåˆ†å‰²å›¾æ§åˆ¶å‡ ä½•ï¼Œæ–‡æœ¬æç¤ºæˆ–å‚è€ƒå›¾åƒæŒ‡å®šå¤–è§‚ï¼Œæ¥åˆ†ç¦»å‡ ä½•å’Œé£æ ¼ã€‚æˆ‘ä»¬é€šè¿‡è½»ä¾¿çš„äº¤äº’å¼GUIçªå‡ºæ˜¾ç¤ºäº†æˆ‘ä»¬æ¨¡å‹çš„ç›´è§‚å’Œå¼ºå¤§çš„3Dç¼–è¾‘åŠŸèƒ½ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç»˜åˆ¶åˆ†å‰²å›¾è½»æ¾åœ°å¡‘é€ å‡ ä½•å½¢çŠ¶ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€æˆ–å›¾åƒæç¤ºæ¥ç¾åŒ–å¤–è§‚ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://antoniooroz.github.io/PercHead%EF%BC%9B%E8%A7%86%E9%A2%91%EF%BC%9Ahttps://www.youtube.com/watch?v=4hFybgTk4kE%E3%80%82">https://antoniooroz.github.io/PercHeadï¼›è§†é¢‘ï¼šhttps://www.youtube.com/watch?v=4hFybgTk4kEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02777v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://antoniooroz.github.io/PercHead/">https://antoniooroz.github.io/PercHead/</a> Video:   <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4hFybgTk4kE">https://www.youtube.com/watch?v=4hFybgTk4kE</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PercHeadæ–¹æ³•ï¼Œç”¨äºå•å›¾åƒä¸‰ç»´å¤´éƒ¨é‡å»ºå’Œè¯­ä¹‰ä¸‰ç»´ç¼–è¾‘ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»Ÿä¸€åŸºæ¨¡å‹å®ç°ï¼Œé‡‡ç”¨åŒåˆ†æ”¯ç¼–ç å™¨ä¸åŸºäºViTçš„è§£ç å™¨ï¼Œå°†äºŒç»´ç‰¹å¾æå‡åˆ°ä¸‰ç»´ç©ºé—´ï¼Œå¹¶ä½¿ç”¨é«˜æ–¯è´´å›¾è¿›è¡Œæ¸²æŸ“ã€‚æ ¸å¿ƒåœ¨äºåŸºäºDINOv2å’ŒSAM2.1çš„æ„ŸçŸ¥ç›‘ç£ç­–ç•¥ï¼Œä¸ºå‡ ä½•å’Œå¤–è§‚ä¿çœŸåº¦æä¾›ä¸°å¯Œçš„é€šç”¨ä¿¡å·ã€‚è¯¥æ–¹æ³•åœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¯¹æç«¯è§†è§’å…·æœ‰å‡ºè‰²çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œè¯¥åŸºæ¨¡å‹å¯æ— ç¼æ‰©å±•ç”¨äºè¯­ä¹‰ä¸‰ç»´ç¼–è¾‘ï¼Œé€šè¿‡æ›´æ¢ç¼–ç å™¨å’Œå¾®è°ƒç½‘ç»œæ¥å®ç°ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡ç»˜åˆ¶åˆ†å‰²å›¾ã€è‡ªç„¶è¯­è¨€æˆ–å›¾åƒæç¤ºæ¥è½»æ¾å¡‘é€ å‡ ä½•å½¢çŠ¶å’Œé£æ ¼åŒ–å¤–è§‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PercHeadæ˜¯ä¸€ä¸ªç”¨äºå•å›¾åƒä¸‰ç»´å¤´éƒ¨é‡å»ºå’Œè¯­ä¹‰ä¸‰ç»´ç¼–è¾‘çš„æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨ç»Ÿä¸€åŸºæ¨¡å‹å®ç°ï¼Œé€šè¿‡åŒåˆ†æ”¯ç¼–ç å™¨ä¸åŸºäºViTçš„è§£ç å™¨å°†äºŒç»´ç‰¹å¾æå‡åˆ°ä¸‰ç»´ç©ºé—´ã€‚</li>
<li>ä½¿ç”¨é«˜æ–¯è´´å›¾è¿›è¡Œæ¸²æŸ“ï¼Œå¹¶é‡‡ç”¨åŸºäºDINOv2å’ŒSAM2.1çš„æ„ŸçŸ¥ç›‘ç£ç­–ç•¥ã€‚</li>
<li>åœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢å–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¯¹æç«¯è§†è§’å…·æœ‰å‡ºè‰²ç¨³å¥æ€§ã€‚</li>
<li>åŸºæ¨¡å‹å¯æ‰©å±•åˆ°è¯­ä¹‰ä¸‰ç»´ç¼–è¾‘ï¼Œé€šè¿‡æ›´æ¢ç¼–ç å™¨å’Œå¾®è°ƒç½‘ç»œå®ç°ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥é€šè¿‡ç»˜åˆ¶åˆ†å‰²å›¾ã€è‡ªç„¶è¯­è¨€æˆ–å›¾åƒæç¤ºæ¥è½»æ¾å¡‘é€ å‡ ä½•å½¢çŠ¶å’Œé£æ ¼åŒ–å¤–è§‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1eb85ae68a821681c734847a0a778923" align="middle">
<img src="https://picx.zhimg.com/v2-d5e6b31f5e0b8e3017f9821a21662393" align="middle">
<img src="https://picx.zhimg.com/v2-3695d66f77433ea89a4f6268423a005e" align="middle">
<img src="https://picx.zhimg.com/v2-45cf9f873f52513770175215513c7a0b" align="middle">
<img src="https://picx.zhimg.com/v2-74bbd9c49e14e26e8d77bfc3a2f5a9bd" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RxnCaption-Reformulating-Reaction-Diagram-Parsing-as-Visual-Prompt-Guided-Captioning"><a href="#RxnCaption-Reformulating-Reaction-Diagram-Parsing-as-Visual-Prompt-Guided-Captioning" class="headerlink" title="RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt   Guided Captioning"></a>RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt   Guided Captioning</h2><p><strong>Authors:Jiahe Song, Chuang Wang, Bowen Jiang, Yinfan Wang, Hao Zheng, Xingjian Wei, Chengjin Liu, Junyuan Gao, Yubin Wang, Lijun Wu, Jiang Wu, Qian Yu, Conghui He</strong></p>
<p>Large-scale chemical reaction datasets are crucial for AI research in chemistry. However, existing chemical reaction data often exist as images within papers, making them not machine-readable and unusable for training machine learning models. In response to this challenge, we propose the RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP). Our framework reformulates the traditional coordinate prediction driven parsing process into an image captioning problem, which Large Vision-Language Models (LVLMs) handle naturally. We introduce a strategy termed â€œBBox and Index as Visual Promptâ€ (BIVP), which uses our state-of-the-art molecular detector, MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the input image. This turns the downstream parsing into a natural-language description problem. Extensive experiments show that the BIVP strategy significantly improves structural extraction quality while simplifying model design. We further construct the RxnCaption-11k dataset, an order of magnitude larger than prior real-world literature benchmarks, with a balanced test subset across four layout archetypes. Experiments demonstrate that RxnCaption-VL achieves state-of-the-art performance on multiple metrics. We believe our method, dataset, and models will advance structured information extraction from chemical literature and catalyze broader AI applications in chemistry. We will release data, models, and code on GitHub. </p>
<blockquote>
<p>å¤§è§„æ¨¡åŒ–å­¦ååº”æ•°æ®é›†å¯¹åŒ–å­¦äººå·¥æ™ºèƒ½ç ”ç©¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŒ–å­¦ååº”æ•°æ®é€šå¸¸ä»¥è®ºæ–‡ä¸­çš„å›¾ç‰‡å½¢å¼å­˜åœ¨ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ— æ³•è¢«æœºå™¨è¯»å–ï¼Œä¹Ÿæ— æ³•ç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºåŒ–å­¦ååº”å›¾è§£æï¼ˆRxnDPï¼‰ä»»åŠ¡çš„RxnCaptionæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†ä¼ ç»Ÿçš„åæ ‡é¢„æµ‹é©±åŠ¨è§£æè¿‡ç¨‹é‡æ–°æ„å»ºä¸ºå›¾åƒæè¿°é—®é¢˜ï¼Œè¿™æ˜¯å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰è‡ªç„¶å¤„ç†çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºâ€œBBoxå’Œç´¢å¼•ä½œä¸ºè§†è§‰æç¤ºâ€ï¼ˆBIVPï¼‰çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨æˆ‘ä»¬æœ€å…ˆè¿›çš„åˆ†å­æ£€æµ‹å™¨MolYOLOï¼Œç›´æ¥åœ¨è¾“å…¥å›¾åƒä¸Šé¢„å…ˆç»˜åˆ¶åˆ†å­è¾¹ç•Œæ¡†å’Œç´¢å¼•ã€‚è¿™å°†ä¸‹æ¸¸è§£æè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°é—®é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBIVPç­–ç•¥åœ¨ç®€åŒ–æ¨¡å‹è®¾è®¡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ç»“æ„æå–è´¨é‡ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†RxnCaption-11kæ•°æ®é›†ï¼Œå…¶è§„æ¨¡æ¯”å…ˆå‰çš„ç°å®ä¸–ç•Œæ–‡çŒ®åŸºå‡†æµ‹è¯•å¤§ä¸€ä¸ªæ•°é‡çº§ï¼Œå¹¶ä¸”åœ¨å››ç§å¸ƒå±€åŸå‹ä¸­å…·æœ‰å¹³è¡¡çš„æµ‹è¯•å­é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒRxnCaption-VLåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„æ–¹æ³•ã€æ•°æ®é›†å’Œæ¨¡å‹å°†æ¨åŠ¨ä»åŒ–å­¦æ–‡çŒ®ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¹¶åœ¨åŒ–å­¦é¢†åŸŸçš„æ›´å¹¿æ³›äººå·¥æ™ºèƒ½åº”ç”¨ä¸­å‘æŒ¥ä½œç”¨ã€‚æˆ‘ä»¬ä¼šåœ¨GitHubä¸Šå‘å¸ƒæ•°æ®ã€æ¨¡å‹å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02384v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>åŒ–å­¦ååº”æ•°æ®é›†åœ¨åŒ–å­¦ååº”é¢†åŸŸçš„AIç ”ç©¶ä¸­å…·æœ‰å…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰æ•°æ®é›†å¸¸ä»¥å›¾åƒå½¢å¼å­˜åœ¨äºè®ºæ–‡ä¸­ï¼Œä¸å¯ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºååº”å›¾è§£æä»»åŠ¡ï¼ˆRxnDPï¼‰çš„RxnCaptionæ¡†æ¶ï¼Œå°†ä¼ ç»Ÿçš„åæ ‡é¢„æµ‹é©±åŠ¨è§£æè¿‡ç¨‹è½¬åŒ–ä¸ºå›¾åƒæè¿°é—®é¢˜ï¼Œä¾¿äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å¤„ç†ã€‚é€šè¿‡å¼•å…¥â€œBBoxå’Œç´¢å¼•ä½œä¸ºè§†è§‰æç¤ºâ€ï¼ˆBIVPï¼‰ç­–ç•¥åŠå…ˆè¿›çš„åˆ†å­æ£€æµ‹å™¨MolYOLOï¼Œç›´æ¥åœ¨è¾“å…¥å›¾åƒä¸Šç»˜åˆ¶åˆ†å­è¾¹ç•Œæ¡†å’Œç´¢å¼•ï¼Œç®€åŒ–æ¨¡å‹è®¾è®¡å¹¶æé«˜ç»“æ„æå–è´¨é‡ã€‚åŒæ—¶æ„å»ºå¤§å‹RxnCaption-11kæ•°æ®é›†ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚æœ¬æ–‡çš„æ–¹æ³•ã€æ•°æ®é›†å’Œæ¨¡å‹å°†æ¨åŠ¨ä»åŒ–å­¦æ–‡çŒ®ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¹¶åŠ é€ŸAIåœ¨åŒ–å­¦é¢†åŸŸçš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹åŒ–å­¦ååº”æ•°æ®é›†å¯¹åŒ–å­¦AIç ”ç©¶è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰åŒ–å­¦ååº”æ•°æ®å¸¸ä»¥å›¾åƒå½¢å¼å­˜åœ¨ï¼Œä¸å¯ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒã€‚</li>
<li>RxnCaptionæ¡†æ¶å°†åŒ–å­¦ååº”å›¾è§£æè½¬åŒ–ä¸ºå›¾åƒæè¿°é—®é¢˜ï¼Œä¾¿äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†ã€‚</li>
<li>å¼•å…¥çš„BIVPç­–ç•¥ç®€åŒ–äº†æ¨¡å‹è®¾è®¡å¹¶æé«˜äº†ç»“æ„æå–è´¨é‡ã€‚</li>
<li>æ„å»ºçš„å¤§å‹RxnCaption-11kæ•°æ®é›†åŒ…å«å¤šç§å¸ƒå±€ç±»å‹ï¼Œæ€§èƒ½ä¼˜äºå…ˆå‰åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•ã€æ•°æ®é›†å’Œæ¨¡å‹æ¨åŠ¨äº†ä»åŒ–å­¦æ–‡çŒ®ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dd7501466de4fc42e67b1ad29ffcdd7" align="middle">
<img src="https://picx.zhimg.com/v2-51cc7f14d2c7170bd18161cdc061da49" align="middle">
<img src="https://picx.zhimg.com/v2-418251436cf513aba6b9b63bc7d3ef39" align="middle">
<img src="https://picx.zhimg.com/v2-cd09076edf6d10317e0f28afdb862c83" align="middle">
<img src="https://picx.zhimg.com/v2-775de45d1deaf24f11dba81baccb749c" align="middle">
<img src="https://picx.zhimg.com/v2-47617f2ec40628a65574ee5a2bb698cb" align="middle">
<img src="https://picx.zhimg.com/v2-b2137374044e3d8f8ad8018919ae1dd5" align="middle">
<img src="https://picx.zhimg.com/v2-d377cc31922a58f71258fc26b0da7cdf" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dynamic-Multi-level-Weighted-Alignment-Network-for-Zero-shot-Sketch-based-Image-Retrieval"><a href="#Dynamic-Multi-level-Weighted-Alignment-Network-for-Zero-shot-Sketch-based-Image-Retrieval" class="headerlink" title="Dynamic Multi-level Weighted Alignment Network for Zero-shot   Sketch-based Image Retrieval"></a>Dynamic Multi-level Weighted Alignment Network for Zero-shot   Sketch-based Image Retrieval</h2><p><strong>Authors:Hanwen Su, Ge Song, Jiyan Wang, Yuanbo Zhu</strong></p>
<p>The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved increasing attention due to its wide applications, e.g. e-commerce. Despite progress made in this field, previous works suffer from using imbalanced samples of modalities and inconsistent low-quality information during training, resulting in sub-optimal performance. Therefore, in this paper, we introduce an approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It consists of three components: (i) a Uni-modal Feature Extraction Module that includes a CLIP text encoder and a ViT for extracting textual and visual tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an alignment weight list by the local and global aggregation blocks to measure the aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss Module aiming to improve the balance of domains in the triplet loss. Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, show our method delivers superior performances over the state-of-the-art ZS-SBIR methods. </p>
<blockquote>
<p>åŸºäºé›¶æ ·æœ¬çš„è‰å›¾å›¾åƒæ£€ç´¢ï¼ˆZS-SBIRï¼‰é—®é¢˜å› å…¶å¹¿æ³›çš„åº”ç”¨ï¼ˆä¾‹å¦‚ç”µå­å•†åŠ¡ï¼‰è€Œå¤‡å—å…³æ³¨ã€‚å°½ç®¡è¯¥é¢†åŸŸå·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ä¹‹å‰çš„å·¥ä½œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨æ¨¡æ€æ ·æœ¬ä¸å¹³è¡¡å’Œè·å–çš„ä¸ä¸€è‡´ã€ä½è´¨é‡ä¿¡æ¯çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºåŠ¨æ€å¤šçº§åŠ æƒå¯¹é½ç½‘ç»œï¼ˆDynamic Multi-level Weighted Alignment Networkï¼‰çš„ZS-SBIRæ–¹æ³•ã€‚å®ƒç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šï¼ˆiï¼‰å•æ¨¡æ€ç‰¹å¾æå–æ¨¡å—ï¼ŒåŒ…æ‹¬CLIPæ–‡æœ¬ç¼–ç å™¨å’Œç”¨äºæå–æ–‡æœ¬å’Œè§†è§‰æ ‡è®°çš„ViTï¼›ï¼ˆiiï¼‰è·¨æ¨¡æ€å¤šçº§åŠ æƒæ¨¡å—ï¼Œé€šè¿‡å±€éƒ¨å’Œå…¨å±€èšåˆå—ç”Ÿæˆå¯¹é½æƒé‡åˆ—è¡¨ï¼Œä»¥æµ‹é‡è‰å›¾å’Œå›¾åƒæ ·æœ¬çš„å¯¹é½è´¨é‡ï¼›ï¼ˆiiiï¼‰åŠ æƒå››é‡æŸå¤±æ¨¡å—ï¼Œæ—¨åœ¨æ”¹å–„ä¸‰é‡æŸå¤±ä¸­çš„åŸŸå¹³è¡¡ã€‚åœ¨Sketchyã€TU-Berlinå’ŒQuickDrawä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬è‰å›¾å›¾åƒæ£€ç´¢é¢†åŸŸçš„æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00925v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ‘˜è¦ï¼šé’ˆå¯¹é›¶æ ·æœ¬è‰å›¾åŸºäºå›¾åƒæ£€ç´¢ï¼ˆZS-SBIRï¼‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŠ¨æ€å¤šçº§åŠ æƒå¯¹é½ç½‘ç»œæ–¹æ³•ã€‚é€šè¿‡é‡‡ç”¨Uni-modalç‰¹å¾æå–æ¨¡å—ã€è·¨æ¨¡æ€å¤šçº§åŠ æƒæ¨¡å—å’ŒåŠ æƒå››é‡æŸå¤±æ¨¡å—ï¼Œè§£å†³äº†æ ·æœ¬æ¨¡æ€ä¸å‡è¡¡å’Œè®­ç»ƒæ—¶ä¿¡æ¯ä¸ä¸€è‡´ã€ä½è´¨é‡çš„é—®é¢˜ã€‚åœ¨Sketchyã€TU-Berlinå’ŒQuickDrawä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰ZS-SBIRæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZS-SBIRé—®é¢˜å› å…¶å¹¿æ³›åº”ç”¨è€Œå—åˆ°å…³æ³¨ï¼Œå¦‚ç”µå­å•†åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´æ ·æœ¬æ¨¡æ€ä¸å‡è¡¡å’Œè®­ç»ƒæ—¶ä¿¡æ¯ä¸ä¸€è‡´ã€ä½è´¨é‡çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºåŠ¨æ€å¤šçº§åŠ æƒå¯¹é½ç½‘ç»œæ–¹æ³•ï¼ŒåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šUni-modalç‰¹å¾æå–æ¨¡å—ã€è·¨æ¨¡æ€å¤šçº§åŠ æƒæ¨¡å—å’ŒåŠ æƒå››é‡æŸå¤±æ¨¡å—ã€‚</li>
<li>Uni-modalç‰¹å¾æå–æ¨¡å—ä½¿ç”¨CLIPæ–‡æœ¬ç¼–ç å™¨å’ŒViTæå–æ–‡æœ¬å’Œè§†è§‰æ ‡è®°ã€‚</li>
<li>è·¨æ¨¡æ€å¤šçº§åŠ æƒæ¨¡å—é€šè¿‡å±€éƒ¨å’Œå…¨å±€èšåˆå—ç”Ÿæˆå¯¹é½æƒé‡åˆ—è¡¨ï¼Œä»¥æµ‹é‡è‰å›¾ä¸å›¾åƒæ ·æœ¬çš„å¯¹é½è´¨é‡ã€‚</li>
<li>åŠ æƒå››é‡æŸå¤±æ¨¡å—æ—¨åœ¨æ”¹å–„åŸŸå¹³è¡¡çš„ä¸‰é‡æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-830f37ac1a5f755be3a60d887a40af1f" align="middle">
<img src="https://picx.zhimg.com/v2-31bd750e6e690d9e887b61b566eef3d2" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Linear-Differential-Vision-Transformer-Learning-Visual-Contrasts-via-Pairwise-Differentials"><a href="#Linear-Differential-Vision-Transformer-Learning-Visual-Contrasts-via-Pairwise-Differentials" class="headerlink" title="Linear Differential Vision Transformer: Learning Visual Contrasts via   Pairwise Differentials"></a>Linear Differential Vision Transformer: Learning Visual Contrasts via   Pairwise Differentials</h2><p><strong>Authors:Yifan Pu, Jixuan Ying, Qixiu Li, Tianzhu Ye, Dongchen Han, Xiaochen Wang, Ziyi Wang, Xinyu Shao, Gao Huang, Xiu Li</strong></p>
<p>Vision Transformers (ViTs) have become a universal backbone for both image recognition and image generation. Yet their Multi-Head Self-Attention (MHSA) layer still performs a quadratic query-key interaction for every token pair, spending the bulk of computation on visually weak or redundant correlations. We introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that injects an explicit notion of discrimination while reducing the theoretical complexity from O(N N C) to O(N n C) with n &lt;&lt; N. VCA first distils each headâ€™s dense query field into a handful of spatially pooled visual-contrast tokens, then splits them into a learnable positive and negative stream whose differential interaction highlights what truly separates one region from another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone, requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and improves three strong hierarchical ViTs by up to 3.1%, while in class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm that (i) spatial pooling supplies low-variance global cues, (ii) dual positional embeddings are indispensable for contrastive reasoning, and (iii) combining the two in both stages yields the strongest synergy. VCA therefore offers a simple path towards faster and sharper Vision Transformers. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/LeapLabTHU/LinearDiff">https://github.com/LeapLabTHU/LinearDiff</a>. </p>
<blockquote>
<p>Vision Transformersï¼ˆViTsï¼‰å·²ç»æˆä¸ºå›¾åƒè¯†åˆ«å’Œå›¾åƒç”Ÿæˆé¢†åŸŸçš„é€šç”¨ä¸»å¹²ç½‘ç»œã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMHSAï¼‰å±‚ä»ç„¶å¯¹æ¯ä¸ªtokenå¯¹æ‰§è¡ŒäºŒæ¬¡æŸ¥è¯¢-é”®äº¤äº’ï¼Œå°†å¤§éƒ¨åˆ†è®¡ç®—é‡èŠ±è´¹åœ¨è§†è§‰è¾ƒå¼±æˆ–å†—ä½™çš„å…³è”ä¸Šã€‚æˆ‘ä»¬å¼•å…¥äº†è§†è§‰å¯¹æ¯”æ³¨æ„åŠ›ï¼ˆVCAï¼‰ï¼Œå®ƒæ˜¯MHSAçš„å³æ’å³ç”¨æ›¿ä»£å“ï¼Œåœ¨å‡å°‘ç†è®ºå¤æ‚åº¦ä»O(N^2C)åˆ°O(NnC)çš„åŒæ—¶æ³¨å…¥äº†æ˜ç¡®çš„è¾¨åˆ«æ¦‚å¿µï¼Œå…¶ä¸­n&lt;&lt;Nã€‚VCAé¦–å…ˆæç‚¼æ¯ä¸ªå¤´éƒ¨çš„å¯†é›†æŸ¥è¯¢å­—æ®µä¸ºå°‘é‡ç©ºé—´æ± åŒ–çš„è§†è§‰å¯¹æ¯”ä»¤ç‰Œï¼Œç„¶åå°†å…¶åˆ†ä¸ºå¯å­¦ä¹ çš„æ­£è´Ÿææµï¼Œå…¶å·®å¼‚äº¤äº’çªå‡ºäº†çœŸæ­£åŒºåˆ†ä¸€ä¸ªåŒºåŸŸä¸å¦ä¸€ä¸ªåŒºåŸŸçš„å†…å®¹ã€‚è¯¥æ¨¡å—å‘DeiT-Tinyä¸»å¹²æ·»åŠ äº†å°‘äº0.3Mçš„å‚æ•°ï¼Œæ— éœ€é¢å¤–çš„FLOPsï¼Œå¹¶ä¸”å®Œå…¨ä¸å—æ¶æ„é™åˆ¶ã€‚ç»éªŒä¸Šï¼ŒVCAå°†DeiT-Tinyåœ¨ImageNet-1Kä¸Šçš„top-1å‡†ç¡®ç‡ä»72.2%æé«˜åˆ°75.6%ï¼ˆ+3.4%ï¼‰ï¼Œå¹¶æ”¹è¿›äº†ä¸‰ä¸ªå¼ºå¤§çš„åˆ†å±‚ViTsé«˜è¾¾3.1%ï¼Œè€Œåœ¨ç±»æ¡ä»¶ImageNetç”Ÿæˆä¸­ï¼Œå®ƒåœ¨æ‰©æ•£ï¼ˆDiTï¼‰å’Œæµï¼ˆSiTï¼‰æ¨¡å‹ä¸­å°†FID-50Ké™ä½äº†2.1è‡³5.2ç‚¹ã€‚å¹¿æ³›çš„æ¶ˆèå®éªŒè¯å®ï¼šï¼ˆiï¼‰ç©ºé—´æ± åŒ–æä¾›ä½æ–¹å·®å…¨å±€çº¿ç´¢ï¼Œï¼ˆiiï¼‰åŒä½ç½®åµŒå…¥å¯¹äºå¯¹æ¯”æ¨ç†å¿…ä¸å¯å°‘ï¼Œï¼ˆiiiï¼‰åœ¨ä¸¤ä¸ªé˜¶æ®µä¸­ç»“åˆä¸¤è€…ä¼šäº§ç”Ÿæœ€å¼ºçš„ååŒä½œç”¨ã€‚å› æ­¤ï¼ŒVCAä¸ºæ›´å¿«ã€æ›´é”åˆ©çš„Vision Transformersæä¾›äº†ä¸€æ¡ç®€å•çš„é€”å¾„ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LeapLabTHU/LinearDiff%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LeapLabTHU/LinearDiffä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00833v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision Transformersï¼ˆViTsï¼‰çš„æ–°æ”¹è¿›â€”â€”Visual-Contrast Attentionï¼ˆVCAï¼‰ã€‚VCAä½œä¸ºMulti-Head Self-Attentionï¼ˆMHSAï¼‰å±‚çš„æ›¿ä»£å“ï¼Œé€šè¿‡å¼•å…¥è§†è§‰å¯¹æ¯”çš„æ¦‚å¿µï¼Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶ä»ç†è®ºä¸Šæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚VCAé€šè¿‡ç©ºé—´æ± åŒ–ç”Ÿæˆè§†è§‰å¯¹æ¯”ä»¤ç‰Œï¼Œå¹¶é€šè¿‡æ­£è´Ÿæµå·®å¼‚äº¤äº’æ¥åŒºåˆ†ä¸åŒåŒºåŸŸã€‚åœ¨ImageNet-1Kä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVCAåœ¨DeiT-Tinyä¸Šçš„å‡†ç¡®ç‡æé«˜äº†3.4%ï¼Œå¹¶åœ¨å…¶ä»–ä¸‰ä¸ªå¼ºå¤§çš„å±‚æ¬¡å‹ViTsä¸­æé«˜äº†æœ€å¤š3.1%çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨ç±»æ¡ä»¶ImageNetç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒVCAé™ä½äº†æ‰©æ•£å’Œæµæ¨¡å‹çš„FID-50Kåˆ†æ•°ã€‚æºä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) å·²æˆä¸ºå›¾åƒè¯†åˆ«å’Œå›¾åƒç”Ÿæˆé¢†åŸŸçš„é€šç”¨æ¶æ„ã€‚</li>
<li>Multi-Head Self-Attention (MHSA) å±‚åœ¨è®¡ç®—ä¸­æ¶ˆè€—äº†å¤§é‡çš„èµ„æºï¼Œä¸»è¦å¤„ç†è§†è§‰è¾ƒå¼±æˆ–å†—ä½™çš„å…³è”ã€‚</li>
<li>Visual-Contrast Attention (VCA) æ˜¯MHSAçš„æ›¿ä»£å“ï¼Œé€šè¿‡å¼•å…¥è§†è§‰å¯¹æ¯”çš„æ¦‚å¿µæ¥é™ä½è®¡ç®—å¤æ‚åº¦å¹¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>VCAé€šè¿‡ç©ºé—´æ± åŒ–ç”Ÿæˆè§†è§‰å¯¹æ¯”ä»¤ç‰Œï¼Œå¹¶é€šè¿‡æ­£è´Ÿæµçš„å·®å¼‚äº¤äº’æ¥åŒºåˆ†åŒºåŸŸã€‚</li>
<li>VCAåœ¨DeiT-Tinyä¸Šçš„å‡†ç¡®ç‡æé«˜äº†3.4%ï¼Œå¹¶åœ¨å…¶ä»–ViTsä¸­ä¹Ÿæœ‰æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨ç±»æ¡ä»¶ImageNetç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒVCAè¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œé™ä½äº†FID-50Kåˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-743e5517b64d53bb53ef901ac3d77731" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Weakly-Supervised-Pneumonia-Localization-from-Chest-X-Rays-Using-Deep-Neural-Network-and-Grad-CAM-Explanations"><a href="#Weakly-Supervised-Pneumonia-Localization-from-Chest-X-Rays-Using-Deep-Neural-Network-and-Grad-CAM-Explanations" class="headerlink" title="Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep   Neural Network and Grad-CAM Explanations"></a>Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep   Neural Network and Grad-CAM Explanations</h2><p><strong>Authors:Kiran Shahi, Anup Bagale</strong></p>
<p>This study proposes a weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays, utilizing Grad-CAM explanations. Instead of costly pixel-level annotations, our approach utilizes image-level labels to generate clinically meaningful heatmaps that highlight regions affected by pneumonia. We evaluate seven ImageNet-pretrained architectures ResNet-18&#x2F;50, DenseNet-121, EfficientNet-B0, MobileNet-V2&#x2F;V3, and ViT-B16 under identical training conditions with focal loss and patient-wise splits to prevent data leakage. Experimental results on the Kermany CXR dataset demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test accuracy of 98%, ROC-AUC &#x3D; 0.997, and F1 &#x3D; 0.987, while MobileNet-V2 provides an optimal trade-off between accuracy and computational cost. Grad-CAM visualizations confirm that the proposed models focus on clinically relevant lung regions, supporting the use of interpretable AI for radiological diagnostics. This work highlights the potential of weakly supervised explainable models that enhance pneumonia screening transparency, and clinical trust in AI-assisted medical imaging.   <a target="_blank" rel="noopener" href="https://github.com/kiranshahi/pneumonia-analysis">https://github.com/kiranshahi/pneumonia-analysis</a> </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨Grad-CAMè§£é‡Šä»èƒ¸éƒ¨Xå°„çº¿å›¾åƒè¿›è¡Œè‚ºç‚åˆ†ç±»å’Œå®šä½çš„å¼±ç›‘ç£æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦æ˜‚è´µçš„åƒç´ çº§æ³¨é‡Šï¼Œè€Œæ˜¯åˆ©ç”¨å›¾åƒçº§æ ‡ç­¾ç”Ÿæˆå…·æœ‰ä¸´åºŠæ„ä¹‰çš„çƒ­å›¾ï¼Œçªå‡ºæ˜¾ç¤ºå—è‚ºç‚å½±å“çš„åŒºåŸŸã€‚æˆ‘ä»¬åœ¨ç›¸åŒçš„è®­ç»ƒæ¡ä»¶ä¸‹è¯„ä¼°äº†ResNet-18&#x2F;50ã€DenseNet-121ã€EfficientNet-B0ã€MobileNet-V2&#x2F;V3å’ŒViT-B16ä¸ƒç§åœ¨ImageNetä¸Šé¢„è®­ç»ƒçš„æ¶æ„ï¼Œä½¿ç”¨ç„¦ç‚¹æŸå¤±å’Œæ‚£è€…çº§åˆ†å‰²æ¥é˜²æ­¢æ•°æ®æ³„éœ²ã€‚åœ¨Kermany CXRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒResNet-18å’ŒEfficientNet-B0çš„æœ€ä½³æ€»ä½“æµ‹è¯•å‡†ç¡®ç‡è¾¾åˆ°äº†98%ï¼ŒROC-AUCä¸º0.997ï¼ŒF1ä¸º0.987ã€‚è€ŒMobileNet-V2åˆ™åœ¨ç²¾åº¦å’Œè®¡ç®—æˆæœ¬ä¹‹é—´æä¾›äº†æœ€ä½³çš„æƒè¡¡ã€‚Grad-CAMå¯è§†åŒ–è¯å®ï¼Œæ‰€æå‡ºçš„æ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³çš„è‚ºéƒ¨åŒºåŸŸï¼Œæ”¯æŒä½¿ç”¨å¯è§£é‡Šçš„AIè¿›è¡Œæ”¾å°„å­¦è¯Šæ–­ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†å¼±ç›‘ç£è§£é‡Šæ€§æ¨¡å‹çš„æ½œåŠ›ï¼Œå¯ä»¥æé«˜è‚ºç‚ç­›æŸ¥çš„é€æ˜åº¦ä»¥åŠä¸´åºŠå¯¹AIè¾…åŠ©åŒ»å­¦æˆåƒçš„ä¿¡ä»»åº¦ã€‚ç›¸å…³ç ”ç©¶å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://github.com/kiranshahi/pneumonia-analysis">https://github.com/kiranshahi/pneumonia-analysis</a> äº†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00456v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‚ºç‚åˆ†ç±»ä¸å®šä½ï¼šåŸºäºå¼±ç›‘ç£æ·±åº¦å­¦ä¹ å’ŒGrad-CAMè§£é‡Šçš„èƒ¸éƒ¨Xå…‰ç‰‡ç ”ç©¶ã€‚è¯¥ç ”ç©¶åˆ©ç”¨å›¾åƒçº§æ ‡ç­¾ç”Ÿæˆæœ‰æ„ä¹‰çƒ­å›¾ï¼Œçªå‡ºè‚ºç‚å½±å“åŒºåŸŸï¼Œå‡å°‘æˆæœ¬é«˜æ˜‚çš„åƒç´ çº§æ ‡æ³¨éœ€æ±‚ã€‚åœ¨Kermany CXRæ•°æ®é›†ä¸Šæµ‹è¯•ï¼ŒResNet-18å’ŒEfficientNet-B0æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œæµ‹è¯•å‡†ç¡®åº¦è¾¾98%ï¼ŒROC-AUCä¸º0.997ï¼ŒF1åˆ†æ•°ä¸º0.987ã€‚Grad-CAMå¯è§†åŒ–è¯æ˜æ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³è‚ºåŒºï¼Œä¸ºæ”¾å°„è¯Šæ–­æä¾›å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½æ”¯æŒã€‚è¯¥ç ”ç©¶çªæ˜¾å¼±ç›‘ç£å¯è§£é‡Šæ¨¡å‹åœ¨è‚ºç‚ç­›æŸ¥ä¸­çš„æ½œåŠ›ï¼Œå¢å¼ºåŒ»ç–—å½±åƒçš„é€æ˜åº¦å’Œä¸´åºŠå¯¹AIçš„ä¿¡ä»»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºå¼±ç›‘ç£æ·±åº¦å­¦ä¹ çš„è‚ºç‚åˆ†ç±»ä¸å®šä½æ–¹æ³•ï¼Œåˆ©ç”¨èƒ¸éƒ¨Xå…‰ç‰‡è¿›è¡Œè¯Šæ–­ã€‚</li>
<li>ä½¿ç”¨Grad-CAMè§£é‡ŠæŠ€æœ¯ï¼Œé€šè¿‡å›¾åƒçº§æ ‡ç­¾ç”Ÿæˆä¸´åºŠæœ‰æ„ä¹‰çš„çƒ­å›¾ï¼Œçªå‡ºæ˜¾ç¤ºè‚ºç‚å½±å“åŒºåŸŸã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šç§é¢„è®­ç»ƒæ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬ResNet-18&#x2F;50ã€DenseNet-121ã€EfficientNet-B0ã€MobileNet-V2&#x2F;V3å’ŒViT-B16ã€‚</li>
<li>åœ¨Kermany CXRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒResNet-18å’ŒEfficientNet-B0æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œæµ‹è¯•å‡†ç¡®åº¦é«˜è¾¾98%ã€‚</li>
<li>Grad-CAMå¯è§†åŒ–éªŒè¯æ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³çš„è‚ºéƒ¨åŒºåŸŸï¼Œå¢å¼ºäº†äººå·¥æ™ºèƒ½åœ¨æ”¾å°„è¯Šæ–­ä¸­çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œå¼±ç›‘ç£æ¨¡å‹åœ¨è‚ºç‚ç­›æŸ¥ä¸­å…·æœ‰æ½œåŠ›ï¼Œæœ‰åŠ©äºæé«˜è¯Šæ–­é€æ˜åº¦å’Œä¸´åºŠå¯¹AIçš„ä¿¡ä»»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a913bb370e240d97be8a70be13a7409e" align="middle">
<img src="https://picx.zhimg.com/v2-029ffee6881f36486e1c8bbb546963b0" align="middle">
<img src="https://picx.zhimg.com/v2-d60f42ba69153099f406d1769ac152d7" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CompAgent-An-Agentic-Framework-for-Visual-Compliance-Verification"><a href="#CompAgent-An-Agentic-Framework-for-Visual-Compliance-Verification" class="headerlink" title="CompAgent: An Agentic Framework for Visual Compliance Verification"></a>CompAgent: An Agentic Framework for Visual Compliance Verification</h2><p><strong>Authors:Rahul Ghosh, Baishali Chaudhury, Hari Prasanna Das, Meghana Ashok, Ryan Razkenari, Sungmin Hong, Chun-Hao Liu</strong></p>
<p>Visual compliance verification is a critical yet underexplored problem in computer vision, especially in domains such as media, entertainment, and advertising where content must adhere to complex and evolving policy rules. Existing methods often rely on task-specific deep learning models trained on manually labeled datasets, which are costly to build and limited in generalizability. While recent multi-modal large language models (MLLMs) offer broad real-world knowledge and policy understanding, they struggle to reason over fine-grained visual details and apply structured compliance rules effectively on their own. In this paper, we propose CompAgent, the first agentic framework for visual compliance verification. CompAgent augments MLLMs with a suite of visual tools - such as object detectors, face analyzers, NSFW detectors, and captioning models - and introduces a planning agent that dynamically selects appropriate tools based on the compliance policy. A verification agent then integrates image, tool outputs, and policy context to perform multi-modal reasoning. Experiments on public benchmarks show that CompAgent outperforms specialized classifiers, direct MLLM prompting, and curated routing baselines, achieving up to 76% F1 score and a 10% improvement over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate the effectiveness of agentic planning and tool-augmented reasoning for scalable, accurate, and adaptable visual compliance verification. </p>
<blockquote>
<p>è§†è§‰åˆè§„éªŒè¯æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€ä¸ªè‡³å…³é‡è¦ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åª’ä½“ã€å¨±ä¹å’Œå¹¿å‘Šç­‰é¢†åŸŸï¼Œå†…å®¹å¿…é¡»éµå®ˆå¤æ‚ä¸”ä¸æ–­å˜åŒ–çš„æ”¿ç­–è§„åˆ™ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä½¿ç”¨æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†è®­ç»ƒçš„ç‰¹å®šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ„å»ºæˆæœ¬é«˜æ˜‚ï¼Œä¸”é€šç”¨æ€§æœ‰é™ã€‚è™½ç„¶æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æä¾›äº†å¹¿æ³›çš„ç°å®ä¸–ç•ŒçŸ¥è¯†å’Œæ”¿ç­–ç†è§£ï¼Œä½†å®ƒä»¬éš¾ä»¥å¯¹ç»†å¾®çš„è§†è§‰ç»†èŠ‚è¿›è¡Œæ¨ç†ï¼Œå¹¶ä¸”æ— æ³•ç‹¬ç«‹æœ‰æ•ˆåœ°åº”ç”¨ç»“æ„åŒ–åˆè§„è§„åˆ™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè§†è§‰åˆè§„éªŒè¯çš„ç¬¬ä¸€ä¸ªæ™ºèƒ½æ¡†æ¶CompAgentã€‚CompAgentä½¿ç”¨ä¸€ç³»åˆ—è§†è§‰å·¥å…·å¢å¼ºMLLMsï¼Œä¾‹å¦‚å¯¹è±¡æ£€æµ‹å™¨ã€é¢éƒ¨åˆ†æå™¨ã€NSFWæ£€æµ‹å™¨å’Œå­—å¹•æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè§„åˆ’ä»£ç†ï¼Œè¯¥ä»£ç†æ ¹æ®åˆè§„ç­–ç•¥åŠ¨æ€é€‰æ‹©é€‚å½“çš„å·¥å…·ã€‚ç„¶åï¼ŒéªŒè¯ä»£ç†å°†å›¾åƒã€å·¥å…·è¾“å‡ºå’Œæ”¿ç­–ä¸Šä¸‹æ–‡é›†æˆåœ¨ä¸€èµ·ï¼Œè¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCompAgentä¼˜äºä¸“ç”¨åˆ†ç±»å™¨ã€ç›´æ¥MLLMæç¤ºå’Œå®šåˆ¶è·¯ç”±åŸºçº¿ï¼Œåœ¨UnsafeBenchæ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾76%çš„F1åˆ†æ•°ï¼Œæ¯”ç°æœ‰æŠ€æœ¯å…ˆè¿›10%ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†æ™ºèƒ½è§„åˆ’å’Œå·¥å…·å¢å¼ºæ¨ç†åœ¨å¯æ‰©å±•ã€å‡†ç¡®å’Œå¯é€‚åº”çš„è§†è§‰åˆè§„éªŒè¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00171v1">PDF</a> Under review</p>
<p><strong>Summary</strong><br>è§†è§‰åˆè§„éªŒè¯æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€ä¸ªè‡³å…³é‡è¦ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åª’ä½“ã€å¨±ä¹å’Œå¹¿å‘Šç­‰é¢†åŸŸï¼Œå†…å®¹å¿…é¡»éµå®ˆå¤æ‚ä¸”ä¸æ–­å‘å±•çš„æ”¿ç­–è§„åˆ™ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä½¿ç”¨æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†è®­ç»ƒçš„ç‰¹å®šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ„å»ºæˆæœ¬é«˜ä¸”é€šç”¨æ€§æœ‰é™ã€‚æœ¬æ–‡æå‡ºCompAgentï¼Œé¦–ä¸ªç”¨äºè§†è§‰åˆè§„éªŒè¯çš„agenticæ¡†æ¶ã€‚CompAgenté€šè¿‡ä¸€ç³»åˆ—è§†è§‰å·¥å…·ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹å™¨ã€é¢éƒ¨åˆ†æå™¨ã€NSFWæ£€æµ‹å™¨å’Œæè¿°æ¨¡å‹ï¼‰å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªè§„åˆ’agentï¼Œè¯¥agentæ ¹æ®åˆè§„æ”¿ç­–åŠ¨æ€é€‰æ‹©é€‚å½“çš„å·¥å…·ã€‚éªŒè¯agentç„¶åæ•´åˆå›¾åƒã€å·¥å…·è¾“å‡ºå’Œæ”¿ç­–ä¸Šä¸‹æ–‡ï¼Œè¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCompAgentä¼˜äºä¸“ä¸šåˆ†ç±»å™¨ã€ç›´æ¥MLLLæç¤ºå’Œå®šåˆ¶è·¯ç”±åŸºçº¿ï¼Œåœ¨UnsafeBenchæ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾76%çš„F1åˆ†æ•°ï¼Œæ¯”ç°æœ‰æŠ€æœ¯æé«˜äº†10%ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†agenticè§„åˆ’å’Œå·¥å…·å¢å¼ºæ¨ç†åœ¨å¯æ‰©å±•ã€å‡†ç¡®å’Œé€‚åº”æ€§å¼ºçš„è§†è§‰åˆè§„éªŒè¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰åˆè§„éªŒè¯æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å…³é”®é—®é¢˜ï¼Œå°¤å…¶åœ¨åª’ä½“ã€å¨±ä¹å’Œå¹¿å‘Šé¢†åŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†è®­ç»ƒçš„æ·±åº¦å­¦ä¹ ä»»åŠ¡æ¨¡å‹ï¼Œæˆæœ¬é«˜æ˜‚ä¸”é€šç”¨æ€§æœ‰é™ã€‚</li>
<li>CompAgentæ˜¯é¦–ä¸ªç”¨äºè§†è§‰åˆè§„éªŒè¯çš„agenticæ¡†æ¶ï¼Œç»“åˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šç§è§†è§‰å·¥å…·ã€‚</li>
<li>CompAgentåŒ…æ‹¬è§„åˆ’agentå’ŒéªŒè¯agentï¼Œåˆ†åˆ«è´Ÿè´£æ ¹æ®åˆè§„æ”¿ç­–é€‰æ‹©å·¥å…·å’Œæ•´åˆå›¾åƒã€å·¥å…·è¾“å‡ºåŠæ”¿ç­–ä¸Šä¸‹æ–‡è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCompAgentåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†è¾ƒé«˜çš„F1åˆ†æ•°ã€‚</li>
<li>CompAgentçš„æ–¹æ³•å…·æœ‰å¯æ‰©å±•æ€§ã€å‡†ç¡®æ€§å’Œé€‚åº”æ€§å¼ºçš„ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c9ea50d507a377ea3f6ddec57e4a924" align="middle">
<img src="https://picx.zhimg.com/v2-4ecf9f21f28a4f2a08a6d749665ba4b3" align="middle">
<img src="https://picx.zhimg.com/v2-c92c200bac09def0fd55f55d43b5fb34" align="middle">
<img src="https://picx.zhimg.com/v2-8eeb6549171900a7f15f5486a42638e5" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Integrating-ConvNeXt-and-Vision-Transformers-for-Enhancing-Facial-Age-Estimation"><a href="#Integrating-ConvNeXt-and-Vision-Transformers-for-Enhancing-Facial-Age-Estimation" class="headerlink" title="Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age   Estimation"></a>Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age   Estimation</h2><p><strong>Authors:Gaby Maroun, Salah Eddine Bekhouche, Fadi Dornaika</strong></p>
<p>Age estimation from facial images is a complex and multifaceted challenge in computer vision. In this study, we present a novel hybrid architecture that combines ConvNeXt, a state-of-the-art advancement of convolutional neural networks (CNNs), with Vision Transformers (ViT). While each model independently delivers excellent performance on a variety of tasks, their integration leverages the complementary strengths of the CNNs localized feature extraction capabilities and the Transformers global attention mechanisms. Our proposed ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior performance in terms of mean absolute error (MAE). To address computational constraints, we leverage pre-trained models and systematically explore different configurations, using linear layers and advanced regularization techniques to optimize the architecture. Comprehensive ablation studies highlight the critical role of individual components and training strategies, and in particular emphasize the importance of adapted attention mechanisms within the CNN framework to improve the model focus on age-relevant facial features. The results show that the ConvNeXt-ViT hybrid not only outperforms traditional methods, but also provides a robust foundation for future advances in age estimation and related visual tasks. This work underscores the transformative potential of hybrid architectures and represents a promising direction for the seamless integration of CNNs and transformers to address complex computer vision challenges. </p>
<blockquote>
<p>ä»é¢éƒ¨å›¾åƒè¿›è¡Œå¹´é¾„ä¼°è®¡æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€ä¸ªå¤æ‚ä¸”å¤šç»´çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ¶æ„ï¼Œå®ƒç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æœ€æ–°è¿›å±•ConvNeXtä¸Vision Transformerï¼ˆViTï¼‰ã€‚è™½ç„¶æ¯ç§æ¨¡å‹ç‹¬ç«‹åœ°åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„ç»“åˆåˆ™åˆ©ç”¨äº†CNNçš„å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›å’ŒTransformerçš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬æå‡ºçš„ConvNeXt-ViTæ··åˆè§£å†³æ–¹æ¡ˆåœ¨å¹´é¾„ä¼°è®¡åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬MORPH IIã€CACDå’ŒAFADï¼Œå¹¶åœ¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰æ–¹é¢å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è®¡ç®—çº¦æŸï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨çº¿æ€§å±‚å’Œå…ˆè¿›çš„æ­£åˆ™åŒ–æŠ€æœ¯æ¥ä¼˜åŒ–æ¶æ„çš„ä¸åŒé…ç½®ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶çªå‡ºäº†å„ä¸ªç»„ä»¶å’ŒåŸ¹è®­ç­–ç•¥çš„å…³é”®ä½œç”¨ï¼Œå¹¶ç‰¹åˆ«å¼ºè°ƒäº†é€‚åº”CNNæ¡†æ¶ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶çš„é‡è¦æ€§ï¼Œä»¥æé«˜æ¨¡å‹å¯¹å¹´é¾„ç›¸å…³é¢éƒ¨ç‰¹å¾çš„å…³æ³¨ã€‚ç»“æœè¡¨æ˜ï¼ŒConvNeXt-ViTæ··åˆæ¶æ„ä¸ä»…ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè€Œä¸”ä¸ºæœªæ¥çš„å¹´é¾„ä¼°è®¡å’Œç›¸å…³è§†è§‰ä»»åŠ¡æä¾›äº†ç¨³å¥çš„åŸºç¡€ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†æ··åˆæ¶æ„çš„å˜é©æ½œåŠ›ï¼Œå¹¶ä»£è¡¨äº†æ— ç¼é›†æˆCNNå’ŒTransformerä»¥åº”å¯¹å¤æ‚è®¡ç®—æœºè§†è§‰æŒ‘æˆ˜çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00123v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆæ¶æ„ï¼Œç»“åˆäº†ConvNeXtï¼ˆå·ç§¯ç¥ç»ç½‘ç»œCNNçš„æœ€æ–°è¿›å±•ï¼‰å’ŒVision Transformerï¼ˆViTï¼‰ã€‚è¯¥æ¶æ„å……åˆ†åˆ©ç”¨äº†CNNçš„å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›å’ŒTransformerçš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨é¢éƒ¨å›¾åƒå¹´é¾„ä¼°è®¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒåŒ…æ‹¬MORPH IIã€CACDå’ŒAFADï¼Œä»¥å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œè¯¥æ¶æ„å‡å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œæ¢ç´¢ä¸åŒé…ç½®åŠè®­ç»ƒç­–ç•¥ï¼Œå¼ºè°ƒäº†é€‚åº”çš„æ³¨æ„åŠ›æœºåˆ¶åœ¨CNNæ¡†æ¶ä¸­çš„é‡è¦æ€§ï¼Œä»¥æ”¹å–„å¯¹å¹´é¾„ç›¸å…³é¢éƒ¨ç‰¹å¾çš„å…³æ³¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ConvNeXtä¸Vision Transformerï¼ˆViTï¼‰çš„æ··åˆæ¶æ„ï¼Œç”¨äºé¢éƒ¨å›¾åƒå¹´é¾„ä¼°è®¡ã€‚</li>
<li>æ··åˆæ¶æ„ç»“åˆäº†CNNçš„å±€éƒ¨ç‰¹å¾æå–å’ŒTransformerçš„å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬MORPH IIã€CACDå’ŒAFADã€‚</li>
<li>è¯¥æ¶æ„å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œä»¥å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸ºè¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€çº¿æ€§å±‚å’Œå…ˆè¿›æ­£åˆ™åŒ–æŠ€æœ¯æ¥ä¼˜åŒ–æ¶æ„ï¼Œä»¥åº”å¯¹è®¡ç®—çº¦æŸã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿçš„æ¶ˆèç ”ç©¶ï¼Œå¼ºè°ƒäº†é€‚åº”çš„æ³¨æ„åŠ›æœºåˆ¶åœ¨CNNæ¡†æ¶ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb6f8dffb025acd570f8acde83ca53d7" align="middle">
<img src="https://picx.zhimg.com/v2-916bcb4fa4f64969ca9fd976f5a761c9" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Invited-Paper-BitMedViT-Ternary-Quantized-Vision-Transformer-for-Medical-AI-Assistants-on-the-Edge"><a href="#Invited-Paper-BitMedViT-Ternary-Quantized-Vision-Transformer-for-Medical-AI-Assistants-on-the-Edge" class="headerlink" title="Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for   Medical AI Assistants on the Edge"></a>Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for   Medical AI Assistants on the Edge</h2><p><strong>Authors:Mikolaj Walczak, Uttej Kallakuri, Edward Humes, Xiaomin Lin, Tinoosh Mohsenin</strong></p>
<p>Vision Transformers (ViTs) have demonstrated strong capabilities in interpreting complex medical imaging data. However, their significant computational and memory demands pose challenges for deployment in real-time, resource-constrained mobile and wearable devices used in clinical environments. We introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI assistants that perform structured analysis of medical images directly on the edge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical imaging and com- bines a training procedure with multi-query attention, preserving stability under ternary weights with low-precision activations. Furthermore, BiTMedViT employs task-aware distillation from a high-capacity teacher to recover accuracy lost due to extreme quantization. Lastly, we also present a pipeline that maps the ternarized ViTs to a custom CUDA kernel for efficient memory bandwidth utilization and latency reduction on the Jetson Orin Nano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on MedMNIST across 12 datasets, while reducing model size by 43x, memory traffic by 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that of SOTA models at 183.62 GOPs&#x2F;J on the Orin Nano. Our results demonstrate a practical and scientifically grounded route for extreme-precision medical imaging ViTs deployable on the edge, narrowing the gap between algorithmic advances and deployable clinical tools. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTï¼‰åœ¨è§£è¯»å¤æ‚åŒ»å­¦æˆåƒæ•°æ®æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ä¸ºåœ¨èµ„æºå—é™çš„å®æ—¶ç§»åŠ¨å’Œå¯ç©¿æˆ´è®¾å¤‡çš„éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œè¿™äº›è®¾å¤‡åœ¨ä¸´åºŠç¯å¢ƒä¸­è¢«å¹¿æ³›åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†BiTMedViTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è¾¹ç¼˜ViTï¼Œä½œä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œç›´æ¥åœ¨è¾¹ç¼˜å¯¹åŒ»ç–—å›¾åƒè¿›è¡Œç»“æ„åŒ–åˆ†æã€‚BiTMedViTåˆ©ç”¨é’ˆå¯¹åŒ»å­¦æˆåƒå®šåˆ¶çš„ä¸‰å…ƒé‡åŒ–çº¿æ€§å±‚ï¼Œå¹¶ç»“åˆå¤šæŸ¥è¯¢æ³¨æ„åŠ›çš„è®­ç»ƒè¿‡ç¨‹ï¼Œåœ¨ä¸‰å…ƒæƒé‡ä¸‹ä¿æŒä½ç²¾åº¦æ¿€æ´»çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼ŒBiTMedViTè¿˜é‡‡ç”¨ä»»åŠ¡æ„ŸçŸ¥è’¸é¦æ³•ï¼Œä»é«˜æ€§èƒ½æ•™å¸ˆæ¨¡å‹ä¸­æ¢å¤å› æç«¯é‡åŒ–è€ŒæŸå¤±çš„å‡†ç¡®æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå°†ä¸‰å…ƒViTæ˜ å°„åˆ°è‡ªå®šä¹‰CUDAå†…æ ¸çš„ç®¡é“ï¼Œä»¥å®ç°Jetson Orin Nanoä¸Šçš„é«˜æ•ˆå†…å­˜å¸¦å®½åˆ©ç”¨å’Œå»¶è¿Ÿé™ä½ã€‚æœ€ç»ˆï¼ŒBiTMedViTåœ¨MedMNISTçš„12ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†86%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼ˆæœ€é«˜89%ï¼‰ï¼ŒåŒæ—¶å‡å°äº†æ¨¡å‹å¤§å°43å€ï¼Œå‡å°‘äº†å†…å­˜æµé‡39å€ï¼Œå¹¶åœ¨Orin Nanoä¸Šä»¥183.62 GOPs&#x2F;Jçš„èƒ½æ•ˆå®ç°äº†16.8æ¯«ç§’çš„æ¨ç†æ—¶é—´ï¼Œèƒ½æ•ˆé«˜è¾¾ç°æœ‰æŠ€æœ¯æ¨¡å‹çš„41å€ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†åœ¨è¾¹ç¼˜éƒ¨ç½²æç«¯ç²¾åº¦åŒ»ç–—æˆåƒViTçš„å®é™…ä¸”ç§‘å­¦çš„é€”å¾„ï¼Œç¼©å°äº†ç®—æ³•è¿›æ­¥å’Œå¯éƒ¨ç½²ä¸´åºŠå·¥å…·ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13760v2">PDF</a> Accepted at 2025 IEEE&#x2F;ACM International Conf. on Computer-Aided   Design (ICCAD) Oct. 26-30 2025, Munich, DE</p>
<p><strong>Summary</strong></p>
<p>BiTMedViTæ˜¯ä¸€ç§æ–°å‹çš„Edge ViTåŒ»ç–—äººå·¥æ™ºèƒ½è¾…åŠ©å·¥å…·ï¼Œå¯åœ¨è¾¹ç¼˜ç«¯ç›´æ¥å¯¹åŒ»ç–—å›¾åƒè¿›è¡Œç»“æ„åŒ–åˆ†æã€‚å®ƒé€šè¿‡é‡‡ç”¨é’ˆå¯¹åŒ»ç–—æˆåƒçš„ä¸‰å…ƒé‡åŒ–çº¿æ€§å±‚ã€ç»“åˆå¤šæŸ¥è¯¢æ³¨æ„åŠ›çš„è®­ç»ƒç¨‹åºã€ä»»åŠ¡æ„ŸçŸ¥è’¸é¦æŠ€æœ¯ï¼Œå®ç°äº†åœ¨æç«¯é‡åŒ–ä¸‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§æ¢å¤ã€‚æ­¤å¤–ï¼ŒBiTMedViTè¿˜æ˜ å°„äº†ä¸‰å…ƒåŒ–ViTåˆ°è‡ªå®šä¹‰CUDAå†…æ ¸ï¼Œä»¥æé«˜Jetson Orin Nanoä¸Šçš„å†…å­˜å¸¦å®½åˆ©ç”¨ç‡å¹¶å‡å°‘å»¶è¿Ÿã€‚åœ¨MedMNISTçš„12ä¸ªæ•°æ®é›†ä¸Šï¼ŒBiTMedViTå®ç°äº†86%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼ˆè¾¾åˆ°æœ€æ–°æŠ€æœ¯çš„89%ï¼‰ï¼ŒåŒæ—¶ç¼©å°äº†æ¨¡å‹å¤§å°43å€ï¼Œå‡å°‘äº†å†…å­˜æµé‡39å€ï¼Œå¹¶åœ¨Orin Nanoä¸Šä»¥183.62 GOPs&#x2F;Jçš„èƒ½æ•ˆå®ç°äº†16.8æ¯«ç§’çš„æ¨ç†é€Ÿåº¦ã€‚å®ƒä¸ºè¾¹ç¼˜éƒ¨ç½²çš„åŒ»ç–—æˆåƒViTæä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”ç§‘å­¦çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BiTMedViTæ˜¯ä¸€ç§ç”¨äºåŒ»ç–—å›¾åƒç»“æ„åŒ–åˆ†æçš„Edge ViTå·¥å…·ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„ç§»åŠ¨ç«¯å’Œå¯ç©¿æˆ´è®¾å¤‡ã€‚</li>
<li>BiTMedViTä½¿ç”¨ä¸‰å…ƒé‡åŒ–çº¿æ€§å±‚ï¼Œä»¥é’ˆå¯¹åŒ»ç–—å›¾åƒè¿›è¡Œä¸“é—¨è®¾è®¡ã€‚</li>
<li>ç»“åˆå¤šæŸ¥è¯¢æ³¨æ„åŠ›è®­ç»ƒç¨‹åºï¼Œç¡®ä¿åœ¨æç«¯é‡åŒ–æ¡ä»¶ä¸‹ä¿æŒç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨ä»»åŠ¡æ„ŸçŸ¥è’¸é¦æŠ€æœ¯ä»é«˜æ€§èƒ½æ•™å¸ˆæ¨¡å‹ä¸­æ¢å¤å› æç«¯é‡åŒ–è€ŒæŸå¤±çš„å‡†ç¡®æ€§ã€‚</li>
<li>BiTMedViTå°†ä¸‰å…ƒåŒ–ViTæ˜ å°„åˆ°è‡ªå®šä¹‰CUDAå†…æ ¸ï¼Œä»¥æé«˜å†…å­˜å¸¦å®½åˆ©ç”¨ç‡å¹¶å‡å°‘å»¶è¿Ÿã€‚</li>
<li>BiTMedViTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é«˜è¯Šæ–­å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ¨¡å‹å¤§å°ã€å†…å­˜æµé‡å’Œæ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d388cc9c3a7ef6cba8f36f4f19e9d931" align="middle">
<img src="https://picx.zhimg.com/v2-cdff02d32e4248eadcad19b0837bed00" align="middle">
<img src="https://picx.zhimg.com/v2-3ddbfd9bdc18e5ad2ccb0faf91115868" align="middle">
<img src="https://picx.zhimg.com/v2-484103d7514dd85203c0a65b98da0e1b" align="middle">
<img src="https://picx.zhimg.com/v2-fb27479c9920148ab9dc9a62bb745e33" align="middle">
<img src="https://picx.zhimg.com/v2-e1ea55845f56d9a55c77898cd8900b23" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="3DViT-GAT-A-Unified-Atlas-Based-3D-Vision-Transformer-and-Graph-Learning-Framework-for-Major-Depressive-Disorder-Detection-Using-Structural-MRI-Data"><a href="#3DViT-GAT-A-Unified-Atlas-Based-3D-Vision-Transformer-and-Graph-Learning-Framework-for-Major-Depressive-Disorder-Detection-Using-Structural-MRI-Data" class="headerlink" title="3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph   Learning Framework for Major Depressive Disorder Detection Using Structural   MRI Data"></a>3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph   Learning Framework for Major Depressive Disorder Detection Using Structural   MRI Data</h2><p><strong>Authors:Nojod M. Alotaibi, Areej M. Alhothali, Manar S. Ali</strong></p>
<p>Major depressive disorder (MDD) is a prevalent mental health condition that negatively impacts both individual well-being and global public health. Automated detection of MDD using structural magnetic resonance imaging (sMRI) and deep learning (DL) methods holds increasing promise for improving diagnostic accuracy and enabling early intervention. Most existing methods employ either voxel-level features or handcrafted regional representations built from predefined brain atlases, limiting their ability to capture complex brain patterns. This paper develops a unified pipeline that utilizes Vision Transformers (ViTs) for extracting 3D region embeddings from sMRI data and Graph Neural Network (GNN) for classification. We explore two strategies for defining regions: (1) an atlas-based approach using predefined structural and functional brain atlases, and (2) an cube-based method by which ViTs are trained directly to identify regions from uniformly extracted 3D patches. Further, cosine similarity graphs are generated to model interregional relationships, and guide GNN-based classification. Extensive experiments were conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of our model. With stratified 10-fold cross-validation, the best model obtained 81.51% accuracy, 85.94% sensitivity, 76.36% specificity, 80.88% precision, and 83.33% F1-score. Further, atlas-based models consistently outperformed the cube-based approach, highlighting the importance of using domain-specific anatomical priors for MDD detection. </p>
<blockquote>
<p>æŠ‘éƒç—‡æ˜¯ä¸€ç§å¸¸è§çš„å¿ƒç†å¥åº·ç–¾ç—…ï¼Œå¯¹ä¸ªäººç¦ç¥‰å’Œå…¨çƒå…¬å…±å«ç”Ÿéƒ½æœ‰è´Ÿé¢å½±å“ã€‚åˆ©ç”¨ç»“æ„ç£å…±æŒ¯æˆåƒï¼ˆsMRIï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•è¿›è¡ŒæŠ‘éƒç—‡çš„è‡ªåŠ¨åŒ–æ£€æµ‹åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œæ—©æœŸå¹²é¢„æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é‡‡ç”¨åŸºäºä½“ç´ çš„ç‰¹å¾æˆ–æ ¹æ®é¢„å…ˆå®šä¹‰çš„è„‘å›¾è°±æ‰‹å·¥æ„å»ºçš„åŒºåŸŸè¡¨ç¤ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰å¤æ‚è„‘æ¨¡å¼çš„èƒ½åŠ›ã€‚æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„æµç¨‹ï¼Œåˆ©ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ä»sMRIæ•°æ®ä¸­æå–ä¸‰ç»´åŒºåŸŸåµŒå…¥ï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§å®šä¹‰åŒºåŸŸçš„æ–¹æ³•ï¼šï¼ˆ1ï¼‰ä¸€ç§åŸºäºå›¾è°±çš„æ–¹æ³•ï¼Œä½¿ç”¨é¢„å…ˆå®šä¹‰çš„ç»“æ„å’ŒåŠŸèƒ½è„‘å›¾è°±ï¼›ï¼ˆ2ï¼‰ä¸€ç§åŸºäºç«‹æ–¹ä½“å—çš„æ–¹æ³•ï¼Œè®­ç»ƒViTsç›´æ¥è¯†åˆ«ä»å‡åŒ€æå–çš„ä¸‰ç»´æ–‘å—åŒºåŸŸã€‚æ­¤å¤–ï¼Œé€šè¿‡æ„å»ºä½™å¼¦ç›¸ä¼¼åº¦å›¾æ¥æ¨¡æ‹ŸåŒºåŸŸé—´çš„å…³ç³»ï¼Œå¹¶å¼•å¯¼åŸºäºGNNçš„åˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨äº†REST-meta-MDDæ•°æ®é›†è¿›è¡Œå¤§é‡å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆ†å±‚åæŠ˜äº¤å‰éªŒè¯ï¼Œæœ€ä½³æ¨¡å‹è·å¾—äº†81.51%çš„å‡†ç¡®ç‡ã€85.94%çš„çµæ•åº¦ã€76.36%çš„ç‰¹å¼‚åº¦ã€80.88%çš„ç²¾ç¡®åº¦å’Œ83.33%çš„F1åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒåŸºäºå›¾è°±çš„æ¨¡å‹æŒç»­ä¼˜äºåŸºäºç«‹æ–¹ä½“å—çš„æ–¹æ³•ï¼Œçªæ˜¾äº†åœ¨æŠ‘éƒç—‡æ£€æµ‹ä¸­ä½¿ç”¨ç‰¹å®šé¢†åŸŸçš„è§£å‰–å­¦å…ˆéªŒä¿¡æ¯çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12143v3">PDF</a> 17 pages, 3 figure, 9 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨Vision Transformerå’ŒGraph Neural Networkå¯¹æŠ‘éƒç—‡è¿›è¡Œè‡ªåŠ¨åŒ–æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–sMRIæ•°æ®çš„3DåŒºåŸŸåµŒå…¥å’Œæ„å»ºä½™å¼¦ç›¸ä¼¼æ€§å›¾æ¥å»ºæ¨¡è„‘åŒºé—´å…³ç³»ï¼Œå®ç°äº†è¾ƒé«˜çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚ç ”ç©¶é‡‡ç”¨ä¸¤ç§å®šä¹‰åŒºåŸŸçš„æ–¹æ³•ï¼šä¸€ç§æ˜¯åŸºäºé¢„è®¾çš„ç»“æ„å’ŒåŠŸèƒ½è„‘å›¾è°±çš„æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯ç›´æ¥è®­ç»ƒVision Transformerè¯†åˆ«ä»å‡åŒ€æå–çš„3Dæ–‘å—çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå›¾è°±çš„æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œå‡¸æ˜¾äº†åˆ©ç”¨é¢†åŸŸç‰¹å®šè§£å‰–å…ˆéªŒä¿¡æ¯çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformerå’ŒGraph Neural Networkè¢«åº”ç”¨äºæŠ‘éƒç—‡çš„è‡ªåŠ¨åŒ–æ£€æµ‹ï¼Œæé«˜äº†è¯Šæ–­å‡†ç¡®æ€§å’Œæ—©æœŸå¹²é¢„çš„å¯èƒ½æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æå–sMRIæ•°æ®çš„3DåŒºåŸŸåµŒå…¥å’Œæ„å»ºä½™å¼¦ç›¸ä¼¼æ€§å›¾ï¼Œæœ‰æ•ˆå»ºæ¨¡äº†è„‘åŒºé—´å…³ç³»ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†ä¸¤ç§å®šä¹‰è„‘åŒºçš„æ–¹æ³•ï¼šåŸºäºé¢„è®¾è„‘å›¾è°±çš„æ–¹æ³•å’ŒåŸºäºVision Transformerç›´æ¥è¯†åˆ«3Dæ–‘å—çš„æ–¹æ³•ã€‚</li>
<li>åŸºäºé¢„è®¾è„‘å›¾è°±çš„æ–¹æ³•è¡¨ç°è¾ƒå¥½ï¼Œå‡¸æ˜¾äº†è§£å‰–å…ˆéªŒä¿¡æ¯åœ¨æŠ‘éƒç—‡æ£€æµ‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨REST-meta-MDDæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œæœ€ä½³æ¨¡å‹è·å¾—äº†è¾ƒé«˜çš„è¯Šæ–­å‡†ç¡®ç‡ã€æ•æ„Ÿæ€§ã€ç‰¹å¼‚æ€§å’ŒF1åˆ†æ•°ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæŠ‘éƒç—‡çš„è‡ªåŠ¨åŒ–æ£€æµ‹æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰æœ›æ”¹å–„ç°æœ‰çš„è¯Šæ–­æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e6fc5d35af58bfa40d828bd480d821c" align="middle">
<img src="https://picx.zhimg.com/v2-ad4db1eb33984e939a0c754ef1116cd3" align="middle">
<img src="https://picx.zhimg.com/v2-4978fca01f291372fb1a1d8f45cfd68c" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Face-Spoofing-Detection-using-Deep-Learning"><a href="#Face-Spoofing-Detection-using-Deep-Learning" class="headerlink" title="Face Spoofing Detection using Deep Learning"></a>Face Spoofing Detection using Deep Learning</h2><p><strong>Authors: Najeebullah, Maaz Salman, Zar Nawab Khan Swati</strong></p>
<p>Digital image spoofing has emerged as a significant security threat in biometric authentication systems, particularly those relying on facial recognition. This study evaluates the performance of three vision based models, MobileNetV2, ResNET50, and Vision Transformer, ViT, for spoof detection in image classification, utilizing a dataset of 150,986 images divided into training , 140,002, testing, 10,984, and validation ,39,574, sets. Spoof detection is critical for enhancing the security of image recognition systems, and this research compares the models effectiveness through accuracy, precision, recall, and F1 score metrics. Results reveal that MobileNetV2 outperforms other architectures on the test dataset, achieving an accuracy of 91.59%, precision of 91.72%, recall of 91.59%, and F1 score of 91.58%, compared to ViT 86.54%, 88.28%, 86.54%, and 86.39%, respectively. On the validation dataset, MobileNetV2, and ViT excel, with MobileNetV2 slightly ahead at 97.17% accuracy versus ViT 96.36%. MobileNetV2 demonstrates faster convergence during training and superior generalization to unseen data, despite both models showing signs of overfitting. These findings highlight MobileNetV2 balanced performance and robustness, making it the preferred choice for spoof detection applications where reliability on new data is essential. The study underscores the importance of model selection in security sensitive contexts and suggests MobileNetV2 as a practical solution for real world deployment. </p>
<blockquote>
<p>æ•°å­—å›¾åƒæ¬ºéª—å·²æˆä¸ºç”Ÿç‰©ç‰¹å¾è¯†åˆ«ç³»ç»Ÿï¼ˆå°¤å…¶æ˜¯ä¾èµ–é¢éƒ¨è¯†åˆ«çš„ç³»ç»Ÿï¼‰ä¸­çš„é‡å¤§å®‰å…¨å¨èƒã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§åŸºäºè§†è§‰çš„æ¨¡å‹ï¼ˆMobileNetV2ã€ResNET50å’ŒVision Transformerï¼Œç®€ç§°ViTï¼‰åœ¨å›¾åƒåˆ†ç±»ä¸­çš„æ¬ºéª—æ£€æµ‹æ€§èƒ½ï¼Œç ”ç©¶ä½¿ç”¨äº†åŒ…å«150986å¼ å›¾åƒçš„æ•°æ®é›†ï¼Œå…¶ä¸­è®­ç»ƒé›†åŒ…å«14002å¼ å›¾åƒï¼Œæµ‹è¯•é›†åŒ…å«10984å¼ å›¾åƒï¼ŒéªŒè¯é›†åŒ…å«39574å¼ å›¾åƒã€‚æ¬ºéª—æ£€æµ‹å¯¹äºæé«˜å›¾åƒè¯†åˆ«ç³»ç»Ÿçš„å®‰å…¨æ€§è‡³å…³é‡è¦ï¼Œæœ¬ç ”ç©¶é€šè¿‡å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰æŒ‡æ ‡æ¯”è¾ƒäº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æµ‹è¯•æ•°æ®é›†ä¸Šï¼ŒMobileNetV2ä¼˜äºå…¶ä»–æ¶æ„ï¼Œå…¶å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º91.59%ã€91.72%ã€91.59%å’Œ91.58%ï¼Œè€ŒViTåˆ†åˆ«ä¸º86.54%ã€88.28%ã€86.54%å’Œ86.39%ã€‚åœ¨éªŒè¯æ•°æ®é›†ä¸Šï¼ŒMobileNetV2å’ŒViTè¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­MobileNetV2ä»¥97.17%çš„å‡†ç¡®æ€§ç•¥èƒœä¸€ç­¹ï¼ŒViTçš„å‡†ç¡®æ€§ä¸º96.36%ã€‚å°½ç®¡ä¸¤è€…éƒ½æ˜¾ç¤ºå‡ºè¿‡æ‹Ÿåˆçš„è¿¹è±¡ï¼Œä½†MobileNetV2åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œå¯¹æ–°æ•°æ®çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°çªæ˜¾äº†MobileNetV2çš„å‡è¡¡æ€§èƒ½å’Œç¨³å¥æ€§ï¼Œä½¿å…¶æˆä¸ºå¯é æ€§å¯¹æ–°æ•°æ®è‡³å…³é‡è¦çš„æ¬ºéª—æ£€æµ‹åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å®‰å…¨æ•æ„Ÿç¯å¢ƒä¸­æ¨¡å‹é€‰æ‹©çš„é‡è¦æ€§ï¼Œå¹¶å»ºè®®å°†MobileNetV2ä½œä¸ºå®é™…éƒ¨ç½²çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19223v2">PDF</a> The authorâ€™s school has a conflict of interest regarding the   submission of this article prior to his graduation thesis submission</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶å¯¹æ¯”äº†MobileNetV2ã€ResNET50å’ŒVision Transformerï¼ˆViTï¼‰ä¸‰ç§è§†è§‰æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­çš„æŠ—æ¬ºéª—æ£€æµ‹æ€§èƒ½ã€‚ä½¿ç”¨150,986å¼ å›¾åƒæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€æµ‹è¯•å’ŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒMobileNetV2åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€ç²¾åº¦ã€å¬å›ç‡å’ŒF1å¾—åˆ†ï¼Œå°¤å…¶æ˜¯åœ¨æœªè§æ•°æ®ä¸Šçš„æ³›åŒ–æ€§èƒ½ä¼˜å¼‚ã€‚å› æ­¤ï¼Œå¯¹äºéœ€è¦å¯é åº”å¯¹æ–°æ•°æ®çš„æ¬ºéª—æ£€æµ‹åº”ç”¨ï¼ŒMobileNetV2æ˜¯é¦–é€‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç§è§†è§‰æ¨¡å‹ï¼ˆMobileNetV2ã€ResNET50å’ŒVision Transformerï¼‰åœ¨å›¾åƒåˆ†ç±»ä¸­çš„æŠ—æ¬ºéª—æ£€æµ‹æ€§èƒ½è¢«è¯„ä¼°ã€‚</li>
<li>MobileNetV2åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€ç²¾åº¦ã€å¬å›ç‡å’ŒF1å¾—åˆ†ã€‚</li>
<li>MobileNetV2åœ¨æœªè§æ•°æ®ä¸Šçš„æ³›åŒ–æ€§èƒ½è¾ƒå¥½ï¼Œä¸”è®­ç»ƒè¿‡ç¨‹ä¸­æ”¶æ•›è¾ƒå¿«ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜MobileNetV2å…·æœ‰å¹³è¡¡çš„æ€§èƒ½å’Œç¨³å¥æ€§ï¼Œæ˜¯æ¬ºéª—æ£€æµ‹åº”ç”¨ä¸­çš„ç†æƒ³é€‰æ‹©ã€‚</li>
<li>æ¨¡å‹é€‰æ‹©åœ¨å®‰å…¨æ•æ„Ÿç¯å¢ƒä¸­è‡³å…³é‡è¦ï¼Œè€ŒMobileNetV2å¯ä½œä¸ºå®é™…éƒ¨ç½²çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ¬ºéª—æ£€æµ‹å¯¹äºæé«˜å›¾åƒè¯†åˆ«ç³»ç»Ÿçš„å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e35f7addefd3c109fcc75c57fdd3520d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RoMA-Scaling-up-Mamba-based-Foundation-Models-for-Remote-Sensing"><a href="#RoMA-Scaling-up-Mamba-based-Foundation-Models-for-Remote-Sensing" class="headerlink" title="RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing"></a>RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing</h2><p><strong>Authors:Fengxiang Wang, Yulin Wang, Mingshuo Chen, Haiyan Zhao, Yangang Sun, Shuo Wang, Hongzhen Wang, Di Wang, Long Lan, Wenjing Yang, Jing Zhang</strong></p>
<p>Recent advances in self-supervised learning for Vision Transformers (ViTs) have fueled breakthroughs in remote sensing (RS) foundation models. However, the quadratic complexity of self-attention poses a significant barrier to scalability, particularly for large models and high-resolution images. While the linear-complexity Mamba architecture offers a promising alternative, existing RS applications of Mamba remain limited to supervised tasks on small, domain-specific datasets. To address these challenges, we propose RoMA, a framework that enables scalable self-supervised pretraining of Mamba-based RS foundation models using large-scale, diverse, unlabeled data. RoMA enhances scalability for high-resolution images through a tailored auto-regressive learning strategy, incorporating two key innovations: 1) a rotation-aware pretraining mechanism combining adaptive cropping with angular embeddings to handle sparsely distributed objects with arbitrary orientations, and 2) multi-scale token prediction objectives that address the extreme variations in object scales inherent to RS imagery. Systematic empirical studies validate that Mamba adheres to RS data and parameter scaling laws, with performance scaling reliably as model and data size increase. Furthermore, experiments across scene classification, object detection, and semantic segmentation tasks demonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based counterparts in both accuracy and computational efficiency. The source code and pretrained models will be released at <a target="_blank" rel="noopener" href="https://github.com/MiliLab/RoMA">https://github.com/MiliLab/RoMA</a>. </p>
<blockquote>
<p>å…³äºVision Transformersï¼ˆViTsï¼‰çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æœ€æ–°è¿›å±•å·²ç»æ¨åŠ¨äº†é¥æ„Ÿï¼ˆRSï¼‰åŸºç¡€æ¨¡å‹çš„çªç ´ã€‚ç„¶è€Œï¼Œè‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦å¯¹å¯æ‰©å±•æ€§æ„æˆäº†é‡å¤§éšœç¢ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹æ¨¡å‹å’Œé«˜åˆ†è¾¨ç‡å›¾åƒã€‚è™½ç„¶çº¿æ€§å¤æ‚åº¦çš„Mambaæ¶æ„æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„Mambaé¥æ„Ÿåº”ç”¨ä»…é™äºå°å‹ç‰¹å®šåŸŸæ•°æ®é›†ä¸Šçš„ç›‘ç£ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RoMAæ¡†æ¶ï¼Œå®ƒä½¿åŸºäºMambaçš„é¥æ„ŸåŸºç¡€æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å¤§è§„æ¨¡ã€å¤šæ ·åŒ–ã€æ— æ ‡ç­¾æ•°æ®è¿›è¡Œå¯æ‰©å±•çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒã€‚RoMAé€šè¿‡å®šåˆ¶çš„è‡ªå›å½’å­¦ä¹ ç­–ç•¥å¢å¼ºäº†é«˜åˆ†è¾¨ç‡å›¾åƒçš„æ‰©å±•æ€§ï¼Œå¹¶èå…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼š1ï¼‰ä¸€ç§æ—‹è½¬æ„ŸçŸ¥é¢„è®­ç»ƒæœºåˆ¶ï¼Œç»“åˆäº†è‡ªé€‚åº”è£å‰ªå’Œè§’åº¦åµŒå…¥æ¥å¤„ç†æ–¹å‘ä»»æ„ä¸”ç¨€ç–åˆ†å¸ƒçš„å¯¹è±¡ï¼›2ï¼‰å¤šå°ºåº¦ä»¤ç‰Œé¢„æµ‹ç›®æ ‡ï¼Œè§£å†³äº†é¥æ„Ÿå›¾åƒå›ºæœ‰å¯¹è±¡å°ºåº¦æç«¯å˜åŒ–çš„é—®é¢˜ã€‚ç³»ç»Ÿçš„å®è¯ç ”ç©¶éªŒè¯äº†Mambaå¯¹é¥æ„Ÿæ•°æ®å’Œå‚æ•°ç¼©æ”¾è§„å¾‹çš„é€‚åº”æ€§ï¼Œéšç€æ¨¡å‹å’Œæ•°æ®è§„æ¨¡çš„å¢åŠ ï¼Œæ€§èƒ½å¯é åœ°æé«˜ã€‚æ­¤å¤–ï¼Œè·¨åœºæ™¯åˆ†ç±»ã€å¯¹è±¡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼ŒRoMAé¢„è®­ç»ƒçš„Mambaæ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å§‹ç»ˆä¼˜äºåŸºäºViTçš„åŒç±»æ¨¡å‹ã€‚æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiliLab/RoMA%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/MiliLab/RoMAå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10392v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>     è¿‘æœŸè‡ªç›‘ç£å­¦ä¹ åœ¨Vision Transformerï¼ˆViTï¼‰æ–¹é¢çš„è¿›å±•ä¸ºé¥æ„Ÿï¼ˆRSï¼‰åŸºç¡€æ¨¡å‹å¸¦æ¥äº†çªç ´ã€‚ç„¶è€Œï¼Œè‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦å¯¹å¯æ‰©å±•æ€§é€ æˆäº†é‡å¤§éšœç¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹é«˜åˆ†è¾¨ç‡å›¾åƒæ–¹é¢ã€‚è™½ç„¶å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„Mambaæ¶æ„æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰RSåº”ç”¨ä»…é™äºå°è§„æ¨¡ç‰¹å®šåŸŸæ•°æ®é›†ä¸Šçš„ç›‘ç£ä»»åŠ¡ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºRoMAæ¡†æ¶ï¼Œä½¿åŸºäºMambaçš„RSåŸºç¡€æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å¤§è§„æ¨¡ã€å¤šæ ·åŒ–ã€æ— æ ‡ç­¾æ•°æ®è¿›è¡Œè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒï¼Œå®ç°å¯æ‰©å±•æ€§ã€‚RoMAé€šè¿‡å®šåˆ¶çš„è‡ªå›å½’å­¦ä¹ ç­–ç•¥å¢å¼ºé«˜åˆ†è¾¨ç‡å›¾åƒçš„æ‰©å±•æ€§ï¼ŒåŒ…æ‹¬ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯ç»“åˆè‡ªé€‚åº”è£å‰ªå’Œè§’åº¦åµŒå…¥çš„æ—‹è½¬æ„ŸçŸ¥é¢„è®­ç»ƒæœºåˆ¶ï¼Œä»¥å¤„ç†å…·æœ‰ä»»æ„æ–¹å‘çš„ç¨€ç–åˆ†å¸ƒå¯¹è±¡ï¼›äºŒæ˜¯å¤šå°ºåº¦ä»¤ç‰Œé¢„æµ‹ç›®æ ‡ï¼Œè§£å†³é¥æ„Ÿå›¾åƒä¸­å¯¹è±¡å°ºåº¦æç«¯å˜åŒ–çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒMambaç¬¦åˆRSæ•°æ®å’Œå‚æ•°æ‰©å±•å®šå¾‹ï¼Œéšç€æ¨¡å‹å’Œæ•°æ®è§„æ¨¡çš„å¢åŠ ï¼Œæ€§èƒ½å¯é åœ°æ‰©å±•ã€‚æ­¤å¤–ï¼Œåœºæ™¯åˆ†ç±»ã€å¯¹è±¡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„å®éªŒæ˜¾ç¤ºï¼ŒRoMAé¢„è®­ç»ƒçš„Mambaæ¨¡å‹åœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¸€è‡´ä¼˜äºViTåŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè‡ªç›‘ç£å­¦ä¹ åœ¨Vision Transformerï¼ˆViTï¼‰æ–¹é¢çš„è¿›å±•æ¨åŠ¨äº†é¥æ„ŸåŸºç¡€æ¨¡å‹çš„çªç ´ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦é˜»ç¢äº†å¯æ‰©å±•æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹é«˜åˆ†è¾¨å›¾åƒæ–¹é¢ã€‚</li>
<li>Mambaæ¶æ„ä¸ºç›‘ç£ä»»åŠ¡æä¾›äº†çº¿æ€§å¤æ‚åº¦çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨é¥æ„Ÿåº”ç”¨ä¸Šä»æœ‰é™åˆ¶ã€‚</li>
<li>RoMAæ¡†æ¶é€šè¿‡è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒä½¿Mambaæ¨¡å‹å¯æ‰©å±•ï¼Œåˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®ã€‚</li>
<li>RoMAé€šè¿‡è‡ªå›å½’å­¦ä¹ ç­–ç•¥å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼ŒåŒ…æ‹¬æ—‹è½¬æ„ŸçŸ¥é¢„è®­ç»ƒå’Œå¤šå°ºåº¦ä»¤ç‰Œé¢„æµ‹ç›®æ ‡ä¸¤é¡¹å…³é”®åˆ›æ–°ã€‚</li>
<li>Mambaç¬¦åˆé¥æ„Ÿæ•°æ®å’Œå‚æ•°æ‰©å±•å®šå¾‹ï¼Œæ€§èƒ½éšæ¨¡å‹å’Œæ•°æ®çš„å¢åŠ è€Œæé«˜ã€‚</li>
<li>RoMAé¢„è®­ç»ƒçš„Mambaæ¨¡å‹åœ¨åœºæ™¯åˆ†ç±»ã€å¯¹è±¡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šä¼˜äºViTåŸºç¡€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d352c60d7133db87d9c0b6c1eea9e873" align="middle">
<img src="https://picx.zhimg.com/v2-af6cd2b02fce82be8ee32a7122358028" align="middle">
<img src="https://picx.zhimg.com/v2-71d91086bec7b98218952831fb75b95b" align="middle">
<img src="https://picx.zhimg.com/v2-626165befb9f10deb0d9d079dc2079f5" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Prompt-to-Restore-Restore-to-Prompt-Cyclic-Prompting-for-Universal-Adverse-Weather-Removal"><a href="#Prompt-to-Restore-Restore-to-Prompt-Cyclic-Prompting-for-Universal-Adverse-Weather-Removal" class="headerlink" title="Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal   Adverse Weather Removal"></a>Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal   Adverse Weather Removal</h2><p><strong>Authors:Rongxin Liao, Feng Li, Yanyan Wei, Zenglin Shi, Le Zhang, Huihui Bai, Meng Wang</strong></p>
<p>Universal adverse weather removal (UAWR) seeks to address various weather degradations within a unified framework. Recent methods are inspired by prompt learning using pre-trained vision-language models (e.g., CLIP), leveraging degradation-aware prompts to facilitate weather-free image restoration, yielding significant improvements. In this work, we propose CyclicPrompt, an innovative cyclic prompt approach designed to enhance the effectiveness, adaptability, and generalizability of UAWR. CyclicPrompt Comprises two key components: 1) a composite context prompt that integrates weather-related information and context-aware representations into the network to guide restoration. This prompt differs from previous methods by marrying learnable input-conditional vectors with weather-specific knowledge, thereby improving adaptability across various degradations. 2) The erase-and-paste mechanism, after the initial guided restoration, substitutes weather-specific knowledge with constrained restoration priors, inducing high-quality weather-free concepts into the composite prompt to further fine-tune the restoration process. Therefore, we can form a cyclic â€œPrompt-Restore-Promptâ€ pipeline that adeptly harnesses weather-specific knowledge, textual contexts, and reliable textures. Extensive experiments on synthetic and real-world datasets validate the superior performance of CyclicPrompt. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/RongxinL/CyclicPrompt">https://github.com/RongxinL/CyclicPrompt</a>. </p>
<blockquote>
<p>æ™®éæ¶åŠ£å¤©æ°”å»é™¤ï¼ˆUAWRï¼‰æ—¨åœ¨åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…è§£å†³å„ç§å¤©æ°”é€€åŒ–é—®é¢˜ã€‚æœ€è¿‘çš„æ–¹æ³•å—åˆ°é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰çš„æç¤ºå­¦ä¹ çš„å¯å‘ï¼Œåˆ©ç”¨é€€åŒ–æ„ŸçŸ¥æç¤ºæ¥ä¿ƒè¿›æ— å¤©æ°”å›¾åƒæ¢å¤ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CyclicPromptï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„å¾ªç¯æç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜UAWRçš„æœ‰æ•ˆæ€§ã€é€‚åº”æ€§å’Œé€šç”¨æ€§ã€‚CyclicPromptåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š1ï¼‰å¤åˆä¸Šä¸‹æ–‡æç¤ºï¼Œå®ƒå°†å¤©æ°”ç›¸å…³ä¿¡æ¯å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤ºé›†æˆåˆ°ç½‘ç»œä¸­ï¼Œä»¥æŒ‡å¯¼æ¢å¤ã€‚æ­¤æç¤ºä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼Œå®ƒå°†å¯å­¦ä¹ çš„è¾“å…¥æ¡ä»¶å‘é‡ä¸ç‰¹å®šå¤©æ°”çŸ¥è¯†ç›¸ç»“åˆï¼Œä»è€Œæé«˜äº†å¯¹å„ç§é€€åŒ–çš„é€‚åº”æ€§ã€‚2ï¼‰æ“¦é™¤å’Œç²˜è´´æœºåˆ¶ï¼Œåœ¨åˆå§‹å¼•å¯¼æ¢å¤ä¹‹åï¼Œç”¨å—é™åˆ¶çš„æ¢å¤å…ˆéªŒæ›¿æ¢ç‰¹å®šå¤©æ°”çŸ¥è¯†ï¼Œå°†é«˜è´¨é‡çš„æ— å¤©æ°”æ¦‚å¿µå¼•å…¥å¤åˆæç¤ºä¸­ï¼Œä»¥è¿›ä¸€æ­¥å¾®è°ƒæ¢å¤è¿‡ç¨‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å½¢æˆä¸€ä¸ªå¾ªç¯çš„â€œæç¤º-æ¢å¤-æç¤ºâ€ç®¡é“ï¼Œå·§å¦™åœ°åˆ©ç”¨ç‰¹å®šå¤©æ°”çŸ¥è¯†ã€æ–‡æœ¬ä¸Šä¸‹æ–‡å’Œå¯é çº¹ç†ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†CyclicPromptçš„ä¼˜è¶Šæ€§èƒ½ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/RongxinL/CyclicPrompt%E3%80%82%EF%BC%88%E7%BD%91%E7%AB%99%E7%BF%BB%E8%AF%91%E5%8F%AF%E8%83%BD%E4%B8%8D%E7%BB%86%E7%BB%BC%EF%BC%8C%E5%BB%BA%E8%AE%AE%E6%9F%A5%E7%9C%8B%E5%AF%B9%E5%BA%94%E4%B8%AD%E6%96%87%E7%BD%91%E7%AB%99%EF%BC%89">https://github.com/RongxinL/CyclicPromptã€‚ï¼ˆç½‘å€ç¿»è¯‘å¯èƒ½ä¸ç²¾ç¡®ï¼Œå»ºè®®æŸ¥é˜…å¯¹åº”ä¸­æ–‡ç½‘ç«™ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09013v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹æ¶åŠ£å¤©æ°”å»é™¤é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºCyclicPromptçš„å¾ªç¯æç¤ºæ–¹æ³•ï¼Œé›†æˆå¤©æ°”ç›¸å…³ä¿¡æ¯å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤ºï¼Œé€šè¿‡å¼•å¯¼æ¢å¤è¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨æ“¦é™¤å’Œç²˜è´´æœºåˆ¶è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚å½¢æˆå¾ªç¯çš„â€œæç¤º-æ¢å¤-æç¤ºâ€ç®¡é“ï¼Œå……åˆ†åˆ©ç”¨å¤©æ°”ç‰¹å®šçŸ¥è¯†ã€æ–‡æœ¬ä¸Šä¸‹æ–‡å’Œå¯é çº¹ç†ï¼Œåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CyclicPromptæ˜¯ä¸€ç§é’ˆå¯¹æ¶åŠ£å¤©æ°”å»é™¤çš„ç»Ÿä¸€æ¡†æ¶ä¸­çš„æ–°æ–¹æ³•ã€‚</li>
<li>å®ƒé€šè¿‡é›†æˆå¤©æ°”ç›¸å…³ä¿¡æ¯å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤ºï¼Œæé«˜æ¢å¤æ•ˆæœã€é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CyclicPromptåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå¤åˆä¸Šä¸‹æ–‡æç¤ºå’Œæ“¦é™¤ä¸ç²˜è´´æœºåˆ¶ã€‚</li>
<li>å¤åˆä¸Šä¸‹æ–‡æç¤ºé€šè¿‡ç»“åˆå¯å­¦ä¹ çš„è¾“å…¥æ¡ä»¶å‘é‡å’Œå¤©æ°”ç‰¹å®šçŸ¥è¯†ï¼Œæ”¹è¿›äº†ä¸åŒé€€åŒ–çš„é€‚åº”æ€§ã€‚</li>
<li>æ“¦é™¤å’Œç²˜è´´æœºåˆ¶åœ¨åˆå§‹å¼•å¯¼æ¢å¤åï¼Œç”¨å—é™çš„æ¢å¤å…ˆéªŒæ›¿æ¢å¤©æ°”ç‰¹å®šçŸ¥è¯†ï¼Œå°†é«˜è´¨é‡çš„æ— å¤©æ°”æ¦‚å¿µå¼•å…¥å¤åˆæç¤ºä¸­ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¢å¤è¿‡ç¨‹ã€‚</li>
<li>CyclicPromptå½¢æˆä¸€ä¸ªå¾ªç¯çš„â€œæç¤º-æ¢å¤-æç¤ºâ€ç®¡é“ï¼Œå……åˆ†åˆ©ç”¨å„ç§èµ„æºï¼ŒåŒ…æ‹¬å¤©æ°”ç‰¹å®šçŸ¥è¯†ã€æ–‡æœ¬ä¸Šä¸‹æ–‡å’Œå¯é çº¹ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3aea5d14088d12f0bd272e9a7ab26791" align="middle">
<img src="https://picx.zhimg.com/v2-ccb61767de73013f94522a8ce2f08cd1" align="middle">
<img src="https://picx.zhimg.com/v2-d9c1ecedfc4d7e3983f11632223b828c" align="middle">
<img src="https://picx.zhimg.com/v2-0737b94752f1234c870adff1b199ceef" align="middle">
<img src="https://picx.zhimg.com/v2-b2054b89b2d0825694acf03e6f3809b2" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Semantic-Alignment-and-Reinforcement-for-Data-Free-Quantization-of-Vision-Transformers"><a href="#Semantic-Alignment-and-Reinforcement-for-Data-Free-Quantization-of-Vision-Transformers" class="headerlink" title="Semantic Alignment and Reinforcement for Data-Free Quantization of   Vision Transformers"></a>Semantic Alignment and Reinforcement for Data-Free Quantization of   Vision Transformers</h2><p><strong>Authors:Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Wanchen Sui, Shen Li, Yong Li, Fei Chao, Rongrong Ji</strong></p>
<p>Data-free quantization (DFQ) enables model quantization without accessing real data, addressing concerns regarding data security and privacy. With the growing adoption of Vision Transformers (ViTs), DFQ for ViTs has garnered significant attention. However, existing DFQ methods exhibit two limitations: (1) semantic distortion, where the semantics of synthetic images deviate substantially from those of real images, and (2) semantic inadequacy, where synthetic images contain extensive regions with limited content and oversimplified textures, leading to suboptimal quantization performance. To address these limitations, we propose SARDFQ, a novel Semantics Alignment and Reinforcement Data-Free Quantization method for ViTs. To address semantic distortion, SARDFQ incorporates Attention Priors Alignment (APA), which optimizes synthetic images to follow randomly generated structure attention priors. To mitigate semantic inadequacy, SARDFQ introduces Multi-Semantic Reinforcement (MSR), leveraging localized patch optimization to enhance semantic richness across synthetic images. Furthermore, SARDFQ employs Soft-Label Learning (SL), wherein multiple semantic targets are adapted to facilitate the learning of multi-semantic images augmented by MSR. Extensive experiments demonstrate the effectiveness of SARDFQ, significantly surpassing existing methods. For example, SARDFQ improves top-1 accuracy on ImageNet by 15.52% for W4A4 ViT-B. The code is at <a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ">https://github.com/zysxmu/SARDFQ</a>. </p>
<blockquote>
<p>æ— æ•°æ®é‡åŒ–ï¼ˆDFQï¼‰èƒ½å¤Ÿåœ¨ä¸è®¿é—®çœŸå®æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é‡åŒ–ï¼Œè§£å†³äº†å¯¹æ•°æ®å®‰å…¨å’Œéšç§çš„æ‹…å¿§ã€‚éšç€è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼ŒViTsçš„æ— æ•°æ®é‡åŒ–å·²å¼•èµ·äººä»¬çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„DFQæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼šï¼ˆ1ï¼‰è¯­ä¹‰å¤±çœŸï¼Œå³åˆæˆå›¾åƒçš„è¯­ä¹‰ä¸çœŸå®å›¾åƒçš„è¯­ä¹‰å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼›ï¼ˆ2ï¼‰è¯­ä¹‰ä¸è¶³ï¼Œåˆæˆå›¾åƒåŒ…å«å¤§é‡å†…å®¹æœ‰é™ã€çº¹ç†è¿‡äºç®€å•çš„åŒºåŸŸï¼Œå¯¼è‡´é‡åŒ–æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SARDFQï¼Œä¸€ç§ç”¨äºViTsçš„æ–°å‹è¯­ä¹‰å¯¹é½å’Œå¼ºåŒ–æ— æ•°æ®é‡åŒ–æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¯­ä¹‰å¤±çœŸé—®é¢˜ï¼ŒSARDFQå¼•å…¥äº†æ³¨æ„åŠ›ä¼˜å…ˆå¯¹é½ï¼ˆAPAï¼‰ï¼Œä¼˜åŒ–åˆæˆå›¾åƒä»¥éµå¾ªéšæœºç”Ÿæˆçš„ç»“æ„æ³¨æ„åŠ›ä¼˜å…ˆçº§ã€‚ä¸ºäº†ç¼“è§£è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ï¼ŒSARDFQå¼•å…¥äº†å¤šè¯­ä¹‰å¼ºåŒ–ï¼ˆMSRï¼‰ï¼Œåˆ©ç”¨å±€éƒ¨è¡¥ä¸ä¼˜åŒ–æ¥å¢å¼ºåˆæˆå›¾åƒçš„è¯­ä¹‰ä¸°å¯Œæ€§ã€‚æ­¤å¤–ï¼ŒSARDFQé‡‡ç”¨è½¯æ ‡ç­¾å­¦ä¹ ï¼ˆSLï¼‰ï¼Œé€‚åº”å¤šä¸ªè¯­ä¹‰ç›®æ ‡ï¼Œä»¥ä¿ƒè¿›ç”±MSRå¢å¼ºçš„å¤šè¯­ä¹‰å›¾åƒçš„å­¦ä¹ ã€‚å¤§é‡å®éªŒè¯æ˜äº†SARDFQçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒSARDFQåœ¨ImageNetä¸Šæé«˜äº†W4A4 ViT-Bçš„top-1å‡†ç¡®ç‡ï¼Œæé«˜äº†15.52%ã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ%E3%80%82">https://github.com/zysxmu/SARDFQã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16553v5">PDF</a> ICCV2025</p>
<p><strong>Summary</strong><br>æ•°æ®æ— å…³é‡åŒ–ï¼ˆDFQï¼‰åœ¨ä¸æ¥è§¦çœŸå®æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é‡åŒ–ï¼Œæ³¨é‡æ•°æ®å®‰å…¨å’Œéšç§ä¿æŠ¤ã€‚é’ˆå¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„DFQæ–¹æ³•é¢ä¸´è¯­ä¹‰å¤±çœŸå’Œè¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§æ–°å‹çš„SARDFQæ–¹æ³•ï¼Œé€šè¿‡æ³¨æ„åŠ›ä¼˜å…ˆå¯¹é½ï¼ˆAPAï¼‰è§£å†³è¯­ä¹‰å¤±çœŸé—®é¢˜ï¼Œå¹¶é€šè¿‡å¤šè¯­ä¹‰å¢å¼ºï¼ˆMSRï¼‰å‡è½»è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚SARDFQé‡‡ç”¨è½¯æ ‡ç­¾å­¦ä¹ ï¼ˆSLï¼‰ï¼Œé€‚åº”å¤šç§è¯­ä¹‰ç›®æ ‡ï¼Œä¿ƒè¿›ç”±MSRå¢å¼ºçš„å¤šè¯­ä¹‰å›¾åƒçš„å­¦ä¹ ã€‚å®éªŒè¯æ˜SARDFQæ•ˆæœæ˜¾è‘—ï¼Œå¦‚W4A4 ViT-Båœ¨ImageNetä¸Šçš„top-1å‡†ç¡®ç‡æé«˜15.52%ã€‚æ›´å¤šä¿¡æ¯å¯é€šè¿‡è®¿é—®ä»£ç åº“<a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ">https://github.com/zysxmu/SARDFQ</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®æ— å…³é‡åŒ–ï¼ˆDFQï¼‰åœ¨ä¸ä½¿ç”¨çœŸå®æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é‡åŒ–ï¼Œä¿æŠ¤æ•°æ®å®‰å…¨å’Œéšç§ã€‚</li>
<li>é’ˆå¯¹ViTsçš„ç°æœ‰DFQæ–¹æ³•å­˜åœ¨è¯­ä¹‰å¤±çœŸå’Œè¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>SARDFQæ–¹æ³•é€šè¿‡æ³¨æ„åŠ›ä¼˜å…ˆå¯¹é½ï¼ˆAPAï¼‰è§£å†³è¯­ä¹‰å¤±çœŸé—®é¢˜ã€‚</li>
<li>SARDFQå¼•å…¥å¤šè¯­ä¹‰å¢å¼ºï¼ˆMSRï¼‰å‡è½»è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ï¼Œæé«˜é‡åŒ–æ€§èƒ½ã€‚</li>
<li>SARDFQé‡‡ç”¨è½¯æ ‡ç­¾å­¦ä¹ ï¼ˆSLï¼‰ï¼Œä¿ƒè¿›å¤šè¯­ä¹‰å›¾åƒçš„å­¦ä¹ ã€‚</li>
<li>å®éªŒè¯æ˜SARDFQåœ¨ImageNetä¸Šçš„æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b20ee6d9a309abb77bb6676c3ab5391" align="middle">
<img src="https://picx.zhimg.com/v2-fd05a03ab7e79073e1962828e14e023a" align="middle">
<img src="https://picx.zhimg.com/v2-18fa203703979bf85a9848cdeea83d76" align="middle">
<img src="https://picx.zhimg.com/v2-d5d75ce875e3463cdaec6a5e5b87849b" align="middle">
<img src="https://picx.zhimg.com/v2-2c766735de2aff444b0351c2a19c5c39" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Static-for-Dynamic-Towards-a-Deeper-Understanding-of-Dynamic-Facial-Expressions-Using-Static-Expression-Data"><a href="#Static-for-Dynamic-Towards-a-Deeper-Understanding-of-Dynamic-Facial-Expressions-Using-Static-Expression-Data" class="headerlink" title="Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial   Expressions Using Static Expression Data"></a>Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial   Expressions Using Static Expression Data</h2><p><strong>Authors:Yin Chen, Jia Li, Yu Zhang, Zhenzhen Hu, Shiguang Shan, Meng Wang, Richang Hong</strong></p>
<p>Dynamic facial expression recognition (DFER) infers emotions from the temporal evolution of expressions, unlike static facial expression recognition (SFER), which relies solely on a single snapshot. This temporal analysis provides richer information and promises greater recognition capability. However, current DFER methods often exhibit unsatisfied performance largely due to fewer training samples compared to SFER. Given the inherent correlation between static and dynamic expressions, we hypothesize that leveraging the abundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic (S4D), a unified dual-modal learning framework that integrates SFER data as a complementary resource for DFER. Specifically, S4D employs dual-modal self-supervised pre-training on facial images and videos using a shared Vision Transformer (ViT) encoder-decoder architecture, yielding improved spatiotemporal representations. The pre-trained encoder is then fine-tuned on static and dynamic expression datasets in a multi-task learning setup to facilitate emotional information interaction. Unfortunately, vanilla multi-task learning in our study results in negative transfer. To address this, we propose an innovative Mixture of Adapter Experts (MoAE) module that facilitates task-specific knowledge acquisition while effectively extracting shared knowledge from both static and dynamic expression data. Extensive experiments demonstrate that S4D achieves a deeper understanding of DFER, setting new state-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with weighted average recall (WAR) of 53.65%, 58.44%, and 76.68%, respectively. Additionally, a systematic correlation analysis between SFER and DFER tasks is presented, which further elucidates the potential benefits of leveraging SFER. </p>
<blockquote>
<p>åŠ¨æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆDFERï¼‰æ˜¯ä»è¡¨æƒ…çš„æ—¶é—´æ¼”å˜ä¸­æ¨æ–­æƒ…æ„Ÿï¼Œä¸åŒäºé™æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆSFERï¼‰ï¼Œåè€…ä»…ä¾èµ–äºå•ä¸€å¿«ç…§ã€‚è¿™ç§æ—¶é—´åˆ†ææä¾›äº†æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¹¶æ‰¿è¯ºå…·æœ‰æ›´å¤§çš„è¯†åˆ«èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„DFERæ–¹æ³•é€šå¸¸ç”±äºè®­ç»ƒæ ·æœ¬è¾ƒå°‘è€Œè¡¨ç°ä¸ä½³ï¼Œä¸SFERç›¸æ¯”ã€‚é‰´äºé™æ€å’ŒåŠ¨æ€è¡¨æƒ…ä¹‹é—´çš„å†…åœ¨å…³è”ï¼Œæˆ‘ä»¬å‡è®¾åˆ©ç”¨ä¸°å¯Œçš„SFERæ•°æ®å¯ä»¥å¢å¼ºDFERã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Static-for-Dynamicï¼ˆS4Dï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŒæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†SFERæ•°æ®ä½œä¸ºDFERçš„è¡¥å……èµ„æºã€‚å…·ä½“æ¥è¯´ï¼ŒS4Dé‡‡ç”¨é¢éƒ¨å›¾åƒå’Œè§†é¢‘çš„åŒé‡æ¨¡æ€è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä½¿ç”¨å…±äº«çš„Vision Transformerï¼ˆViTï¼‰ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œäº§ç”Ÿæ”¹è¿›çš„æ—¶ç©ºè¡¨ç¤ºã€‚ç„¶åï¼Œåœ¨å¤šä»»åŠ¡å­¦ä¹ è®¾ç½®ä¸­å¯¹é™æ€å’ŒåŠ¨æ€è¡¨æƒ…æ•°æ®é›†å¯¹é¢„è®­ç»ƒçš„ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¿ƒè¿›æƒ…æ„Ÿä¿¡æ¯äº¤äº’ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ç ”ç©¶ä¸­çš„ç®€å•å¤šä»»åŠ¡å­¦ä¹ å¯¼è‡´äº†è´Ÿé¢è¿ç§»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ›æ–°çš„Mixture of Adapter Expertsï¼ˆMoAEï¼‰æ¨¡å—ï¼Œå®ƒæœ‰åŠ©äºè·å–ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ä»é™æ€å’ŒåŠ¨æ€è¡¨æƒ…æ•°æ®ä¸­æå–å…±äº«çŸ¥è¯†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒS4Då¯¹DFERæœ‰äº†æ›´æ·±å…¥çš„ç†è§£ï¼Œåœ¨FERV39Kã€MAFWå’ŒDFEWåŸºå‡†æµ‹è¯•ä¸Šåˆ›ä¸‹äº†æœ€æ–°çºªå½•ï¼ŒåŠ æƒå¹³å‡å¬å›ç‡åˆ†åˆ«ä¸º53.65%ã€58.44%å’Œ76.68%ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†SFERå’ŒDFERä»»åŠ¡ä¹‹é—´çš„ç³»ç»Ÿç›¸å…³æ€§åˆ†æï¼Œè¿›ä¸€æ­¥é˜æ˜äº†åˆ©ç”¨SFERçš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06154v3">PDF</a> The code and model are publicly available here   <a target="_blank" rel="noopener" href="https://github.com/MSA-LMC/S4D">https://github.com/MSA-LMC/S4D</a></p>
<p><strong>Summary</strong><br>     åŠ¨æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆDFERï¼‰é€šè¿‡è¡¨æƒ…çš„æ—¶é—´æ¼”å˜æ¥æ¨æ–­æƒ…ç»ªï¼Œä¸ä»…ä¾èµ–äºå•ä¸€é™æ€å›¾åƒçš„é™æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆSFERï¼‰ä¸åŒã€‚æœ¬æ–‡æå‡ºStatic-for-Dynamicï¼ˆS4Dï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨ä¸°å¯Œçš„SFERæ•°æ®å¢å¼ºDFERæ€§èƒ½ã€‚S4Dé‡‡ç”¨ç»Ÿä¸€åŒæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œé›†æˆé¢éƒ¨å›¾åƒå’Œè§†é¢‘è¿›è¡ŒåŒæ¨¡æ€è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä½¿ç”¨å…±äº«Vision Transformerï¼ˆViTï¼‰ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè·å¾—æ”¹è¿›çš„æ—¶ç©ºè¡¨å¾ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°å•çº¯çš„å¤šä»»åŠ¡å­¦ä¹ ä¼šå¯¼è‡´è´Ÿè¿ç§»ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºåˆ›æ–°çš„Mixture of Adapter Expertsï¼ˆMoAEï¼‰æ¨¡å—ï¼Œä¿ƒè¿›ä»»åŠ¡ç‰¹å®šçŸ¥è¯†çš„è·å–ï¼ŒåŒæ—¶æœ‰æ•ˆæå–é™æ€å’ŒåŠ¨æ€è¡¨æƒ…æ•°æ®çš„å…±äº«çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼ŒS4Dåœ¨FERV39Kã€MAFWå’ŒDFEWåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆDFERï¼‰é€šè¿‡è¡¨æƒ…çš„æ—¶é—´æ¼”å˜æ¨æ–­æƒ…ç»ªï¼Œè¾ƒé™æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆSFERï¼‰æ›´å…·ä¼˜åŠ¿ã€‚</li>
<li>ç°æœ‰çš„DFERæ–¹æ³•å› è®­ç»ƒæ ·æœ¬è¾ƒå°‘è€Œæ€§èƒ½ä¸ä½³ã€‚</li>
<li>æå‡ºçš„Static-for-Dynamicï¼ˆS4Dï¼‰æ–¹æ³•åˆ©ç”¨ä¸°å¯Œçš„SFERæ•°æ®å¢å¼ºDFERæ€§èƒ½ã€‚</li>
<li>S4Dé‡‡ç”¨åŒæ¨¡æ€è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä½¿ç”¨å…±äº«Vision Transformeræ¶æ„è·å¾—æ”¹è¿›çš„æ—¶ç©ºè¡¨å¾ã€‚</li>
<li>ç ”ç©¶å‘ç°å•çº¯çš„å¤šä»»åŠ¡å­¦ä¹ ä¼šå¯¼è‡´è´Ÿè¿ç§»ã€‚</li>
<li>Mixture of Adapter Expertsï¼ˆMoAEï¼‰æ¨¡å—è¢«æå‡ºä»¥è§£å†³è´Ÿè¿ç§»é—®é¢˜ï¼Œä¿ƒè¿›ä»»åŠ¡ç‰¹å®šå’Œå…±äº«çŸ¥è¯†çš„è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1aa2e86ccd66cee68710c3a67fbe32f0" align="middle">
<img src="https://picx.zhimg.com/v2-c3741a3d446d8443d3e6771613fc51ed" align="middle">
<img src="https://picx.zhimg.com/v2-88e4b95a51404934b6b307ff74db6157" align="middle">
<img src="https://picx.zhimg.com/v2-79e07207240c94c50a0ed93dad8b6111" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="REP-Resource-Efficient-Prompting-for-Rehearsal-Free-Continual-Learning"><a href="#REP-Resource-Efficient-Prompting-for-Rehearsal-Free-Continual-Learning" class="headerlink" title="REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning"></a>REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning</h2><p><strong>Authors:Sungho Jeon, Xinyue Ma, Kwang In Kim, Myeongjae Jeon</strong></p>
<p>Recent rehearsal-free continual learning (CL) methods guided by prompts achieve strong performance on vision tasks with non-stationary data but remain resource-intensive, hindering real-world edge deployment. We introduce resource-efficient prompting (REP), which improves the computational and memory efficiency of prompt-based rehearsal-free continual learning methods while minimizing accuracy trade-offs. Our approach employs swift prompt selection to refine input data using a carefully provisioned model and introduces adaptive token merging (AToM) and adaptive layer dropping (ALD) for efficient prompt updates. AToM and ALD selectively skip data and model layers while preserving task-specific features during the learning of new tasks. Extensive experiments on multiple image classification datasets demonstrate REPâ€™s superior resource efficiency over state-of-the-art rehearsal-free CL methods. </p>
<blockquote>
<p>åœ¨æ— éœ€å¤ç°çš„è¿ç»­å­¦ä¹ ï¼ˆCLï¼‰æ–¹æ³•ä¸­ï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•åœ¨éç¨³æ€æ•°æ®ä¸Šå®ç°äº†å¼ºå¤§çš„è§†è§‰ä»»åŠ¡æ€§èƒ½ï¼Œä½†èµ„æºæ¶ˆè€—ä»ç„¶å¾ˆå¤§ï¼Œé˜»ç¢äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¸­çš„å®é™…åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†èµ„æºé«˜æ•ˆæç¤ºï¼ˆREPï¼‰ï¼Œå®ƒåœ¨æé«˜åŸºäºæç¤ºçš„æ— å¤ç°è¿ç»­å­¦ä¹ æ–¹æ³•çš„åŒæ—¶ï¼Œå°½é‡å‡å°‘å‡†ç¡®åº¦çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¿«é€Ÿæç¤ºé€‰æ‹©æ¥ä¼˜åŒ–è¾“å…¥æ•°æ®ï¼Œå¹¶ä½¿ç”¨ç²¾å¿ƒå‡†å¤‡çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ï¼ˆAToMï¼‰å’Œè‡ªé€‚åº”å±‚ä¸¢å¼ƒï¼ˆALDï¼‰æ¥å®ç°é«˜æ•ˆçš„æç¤ºæ›´æ–°ã€‚AToMå’ŒALDåœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶é€‰æ‹©æ€§è·³è¿‡æ•°æ®å’Œæ¨¡å‹å±‚ï¼ŒåŒæ—¶ä¿ç•™ç‰¹å®šä»»åŠ¡çš„ç‰¹æ€§ã€‚åœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒREPåœ¨èµ„æºæ•ˆç‡æ–¹é¢ä¼˜äºæœ€æ–°çš„æ— å¤ç°CLæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04772v5">PDF</a> accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†èµ„æºé«˜æ•ˆæç¤ºï¼ˆREPï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æé«˜äº†åŸºäºæç¤ºçš„æ— å¤ä¹ æŒç»­å­¦ä¹ æ–¹æ³•çš„è®¡ç®—å’Œå†…å­˜æ•ˆç‡ï¼ŒåŒæ—¶æœ€å°åŒ–äº†å‡†ç¡®åº¦çš„æŸå¤±ã€‚REPé€šè¿‡å¿«é€Ÿæç¤ºé€‰æ‹©ä¼˜åŒ–è¾“å…¥æ•°æ®ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ï¼ˆAToMï¼‰å’Œè‡ªé€‚åº”å±‚ä¸¢å¼ƒï¼ˆALDï¼‰è¿›è¡Œé«˜æ•ˆçš„æç¤ºæ›´æ–°ã€‚å®éªŒè¯æ˜ï¼ŒREPåœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„èµ„æºæ•ˆç‡ä¼˜äºæœ€æ–°çš„æ— å¤ä¹ CLæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”èµ„æºé«˜æ•ˆæç¤ºï¼ˆREPï¼‰ï¼Œæ—¨åœ¨æé«˜åŸºäºæç¤ºçš„æ— å¤ä¹ æŒç»­å­¦ä¹ æ–¹æ³•çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>REPé€šè¿‡å¿«é€Ÿæç¤ºé€‰æ‹©ä¼˜åŒ–è¾“å…¥æ•°æ®ï¼Œä½¿ç”¨ç²¾å¿ƒå‡†å¤‡çš„æ¨¡å‹è¿›è¡Œç²¾ç‚¼ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ï¼ˆAToMï¼‰å’Œè‡ªé€‚åº”å±‚ä¸¢å¼ƒï¼ˆALDï¼‰æŠ€æœ¯ï¼Œç”¨äºåœ¨æŒç»­å­¦ä¹ ä¸­é«˜æ•ˆæ›´æ–°æç¤ºã€‚</li>
<li>AToMå’ŒALDèƒ½å¤Ÿé€‰æ‹©æ€§è·³è¿‡æ•°æ®å’Œæ¨¡å‹å±‚ï¼ŒåŒæ—¶ä¿ç•™ä»»åŠ¡ç‰¹å®šç‰¹å¾ï¼Œä»¥æ”¯æŒæ–°ä»»åŠ¡çš„å­¦ä¹ ã€‚</li>
<li>REPåœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæ˜¾ç¤ºå‡ºå…¶å“è¶Šçš„èµ„æºæ•ˆç‡ã€‚</li>
<li>ä¸æœ€æ–°çš„æ— å¤ä¹ æŒç»­å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒREPå…·æœ‰æ›´é«˜çš„èµ„æºæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e478acfd620e5539cd466dc40a8ebe36" align="middle">
<img src="https://picx.zhimg.com/v2-0e69d41073a3644c8f70f9a598fe1881" align="middle">
<img src="https://picx.zhimg.com/v2-c15475f1b2b382262689b04429aef7fe" align="middle">
<img src="https://picx.zhimg.com/v2-927d1305a3c28b7eefcc3eab1dd1841a" align="middle">
<img src="https://picx.zhimg.com/v2-20b74edbb13f2aba6e7dff406809d7ea" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-48d11c5bf4e7dbd2a07c0107e499e9c3" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Object Detection as an Optional Basis A Graph Matching Network for   Cross-View UAV Localization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2a25625348991da4f51bf151e0a4a2b8" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  CueBench Advancing Unified Understanding of Context-Aware Video   Anomalies in Real-World
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
