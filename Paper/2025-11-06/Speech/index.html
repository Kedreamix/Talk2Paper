<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  Improving DF-Conformer Using Hydra For High-Fidelity Generative Speech   Enhancement on Discrete Codec Token">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-de2b7690f30877216a8f4bb44614e676~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376790&auth_key=1762376790-0-0-61218054de1973cbf4635d981adab0a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    50 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-06-æ›´æ–°"><a href="#2025-11-06-æ›´æ–°" class="headerlink" title="2025-11-06 æ›´æ–°"></a>2025-11-06 æ›´æ–°</h1><h2 id="Improving-DF-Conformer-Using-Hydra-For-High-Fidelity-Generative-Speech-Enhancement-on-Discrete-Codec-Token"><a href="#Improving-DF-Conformer-Using-Hydra-For-High-Fidelity-Generative-Speech-Enhancement-on-Discrete-Codec-Token" class="headerlink" title="Improving DF-Conformer Using Hydra For High-Fidelity Generative Speech   Enhancement on Discrete Codec Token"></a>Improving DF-Conformer Using Hydra For High-Fidelity Generative Speech   Enhancement on Discrete Codec Token</h2><p><strong>Authors:Shogo Seki, Shaoxiang Dang, Li Li</strong></p>
<p>The Dilated FAVOR Conformer (DF-Conformer) is an efficient variant of the Conformer architecture designed for speech enhancement (SE). It employs fast attention through positive orthogonal random features (FAVOR+) to mitigate the quadratic complexity associated with self-attention, while utilizing dilated convolution to expand the receptive field. This combination results in impressive performance across various SE models. In this paper, we propose replacing FAVOR+ with bidirectional selective structured state-space sequence models to achieve two main objectives:(1) enhancing global sequential modeling by eliminating the approximations inherent in FAVOR+, and (2) maintaining linear complexity relative to the sequence length. Specifically, we utilize Hydra, a bidirectional extension of Mamba, framed within the structured matrix mixer framework. Experiments conducted using a generative SE model on discrete codec tokens, known as Genhancer, demonstrate that the proposed method surpasses the performance of the DF-Conformer. </p>
<blockquote>
<p>Dilated FAVOR Conformerï¼ˆDF-Conformerï¼‰æ˜¯ä¸“ä¸ºè¯­éŸ³å¢å¼ºï¼ˆSEï¼‰è®¾è®¡çš„Conformeræ¶æ„çš„æœ‰æ•ˆå˜ä½“ã€‚å®ƒé€šè¿‡æ­£å‘æ­£äº¤éšæœºç‰¹å¾ï¼ˆFAVOR+ï¼‰å®ç°å¿«é€Ÿæ³¨æ„åŠ›ï¼Œä»¥å‡è½»è‡ªæ³¨æ„åŠ›ç›¸å…³çš„äºŒæ¬¡å¤æ‚æ€§ï¼ŒåŒæ—¶åˆ©ç”¨è†¨èƒ€å·ç§¯æ¥æ‰©å¤§æ„Ÿå—é‡ã€‚è¿™ç§ç»“åˆåœ¨å„ç§SEæ¨¡å‹ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºç”¨åŒå‘é€‰æ‹©æ€§ç»“æ„åŒ–çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹æ›¿æ¢FAVOR+ï¼Œä»¥å®ç°ä¸¤ä¸ªä¸»è¦ç›®æ ‡ï¼šï¼ˆ1ï¼‰é€šè¿‡æ¶ˆé™¤FAVOR+æ‰€å›ºæœ‰çš„è¿‘ä¼¼å€¼ï¼Œå¢å¼ºå…¨å±€åºåˆ—å»ºæ¨¡ï¼›ï¼ˆ2ï¼‰ä¿æŒç›¸å¯¹äºåºåˆ—é•¿åº¦çš„çº¿æ€§å¤æ‚æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ç»“æ„åŒ–çŸ©é˜µæ··åˆå™¨æ¡†æ¶å†…ä½¿ç”¨Hydraï¼Œè¿™æ˜¯Mambaçš„åŒå‘æ‰©å±•ã€‚ä½¿ç”¨ç§°ä¸ºGenhancerçš„ç¦»æ•£ç¼–è§£ç ç¬¦å·ç”ŸæˆSEæ¨¡å‹è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•è¶…è¿‡äº†DF-Conformerçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02454v1">PDF</a> Submitted to ICASSP 2026. Audio samples available at   <a target="_blank" rel="noopener" href="https://s-seki.github.io/dc_hydra/">https://s-seki.github.io/dc_hydra/</a></p>
<p><strong>Summary</strong><br>     é«˜æ•ˆè¯­éŸ³å¢å¼ºæ¨¡å‹DF-Conformerçš„æ–°å˜ä½“æå‡ºä½¿ç”¨åŒå‘é€‰æ‹©æ€§ç»“æ„åŒ–çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹æ›¿æ¢FAVORe+ï¼Œä»¥æé«˜å…¨å±€åºåˆ—å»ºæ¨¡æ€§èƒ½å¹¶ç»´æŒçº¿æ€§å¤æ‚åº¦ã€‚æ–°æ¨¡å‹ä½¿ç”¨åä¸ºHydraçš„ç»“æ„åŒ–çŸ©é˜µæ··åˆå™¨æ¡†æ¶å®ç°ï¼Œé€šè¿‡å®éªŒè¯æ˜æ€§èƒ½è¶…è¶ŠDF-Conformerã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DF-Conformeræ˜¯ä¸€ç§ç”¨äºè¯­éŸ³å¢å¼ºçš„é«˜æ•ˆå˜ä½“ã€‚å®ƒé€šè¿‡å¼•å…¥è†¨èƒ€å·ç§¯æé«˜äº†æ¨¡å‹æ„Ÿå—é‡ï¼Œå¹¶é‡‡ç”¨FAVORe+æœºåˆ¶å‡å°‘è‡ªæ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºä½¿ç”¨åŒå‘é€‰æ‹©æ€§ç»“æ„åŒ–çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹æ¥æ”¹è¿›DF-Conformerä¸­çš„FAVORe+ï¼Œä»¥å®ç°æ›´é«˜çš„å…¨å±€åºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚æ–°æ¨¡å‹æ—¨åœ¨æ¶ˆé™¤FAVORe+å›ºæœ‰çš„è¿‘ä¼¼é—®é¢˜ã€‚</li>
<li>æ–°æ¨¡å‹åŸºäºç»“æ„åŒ–çŸ©é˜µæ··åˆå™¨æ¡†æ¶å®ç°ï¼Œç§°ä¸ºHydraï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŒå‘æ‰©å±•çš„ç‰¹æ€§ã€‚è¿™ç§åŒå‘æ‰©å±•çš„å®ç°åŸºäºMambaç®—æ³•ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–°æ¨¡å‹åœ¨åŸºäºç¦»æ•£ç¼–ç è§£ç æ ‡è®°ç”Ÿæˆæ¨¡å‹çš„è¯­éŸ³å¢å¼ºä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºDF-Conformerã€‚è¿™æ„å‘³ç€æ–°æ¨¡å‹å¯èƒ½åœ¨å®é™…åº”ç”¨ä¸­æä¾›æ›´é«˜æ•ˆçš„è¯­éŸ³å¢å¼ºæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-94194f29ce7be50316ca809e5df2e58a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376447&auth_key=1762376447-0-0-35140d8db70fe42bfeaa89aedfefba36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4eaa1ad37ace93ccf2342b5dda777b9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376454&auth_key=1762376454-0-0-3c510678c6435f42df9cee2f2c95f878&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-52d1a9a6fb3ce1772ebeb5cf7462c90a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376462&auth_key=1762376462-0-0-b7ea936e3b9738927ece10c62729ca6f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b3ae983d9857174abcde21de6e6565dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376469&auth_key=1762376469-0-0-867ce29f9bb6add6c78ddd5bdfe8018d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Emotion-Detection-in-Speech-Using-Lightweight-and-Transformer-Based-Models-A-Comparative-and-Ablation-Study"><a href="#Emotion-Detection-in-Speech-Using-Lightweight-and-Transformer-Based-Models-A-Comparative-and-Ablation-Study" class="headerlink" title="Emotion Detection in Speech Using Lightweight and Transformer-Based   Models: A Comparative and Ablation Study"></a>Emotion Detection in Speech Using Lightweight and Transformer-Based   Models: A Comparative and Ablation Study</h2><p><strong>Authors:Lucky Onyekwelu-Udoka, Md Shafiqul Islam, Md Shahedul Hasan</strong></p>
<p>Emotion recognition from speech plays a vital role in the development of empathetic human-computer interaction systems. This paper presents a comparative analysis of lightweight transformer-based models, DistilHuBERT and PaSST, by classifying six core emotions from the CREMA-D dataset. We benchmark their performance against a traditional CNN-LSTM baseline model using MFCC features. DistilHuBERT demonstrates superior accuracy (70.64%) and F1 score (70.36%) while maintaining an exceptionally small model size (0.02 MB), outperforming both PaSST and the baseline. Furthermore, we conducted an ablation study on three variants of the PaSST, Linear, MLP, and Attentive Pooling heads, to understand the effect of classification head architecture on model performance. Our results indicate that PaSST with an MLP head yields the best performance among its variants but still falls short of DistilHuBERT. Among the emotion classes, angry is consistently the most accurately detected, while disgust remains the most challenging. These findings suggest that lightweight transformers like DistilHuBERT offer a compelling solution for real-time speech emotion recognition on edge devices. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/luckymaduabuchi/Emotion-detection-">https://github.com/luckymaduabuchi/Emotion-detection-</a>. </p>
<blockquote>
<p>è¯­éŸ³ä¸­çš„æƒ…ç»ªè¯†åˆ«åœ¨å¼€å‘å…·æœ‰åŒæƒ…å¿ƒçš„è®¡ç®—æœºäº¤äº’ç³»ç»Ÿä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ¬æ–‡å¯¹æ¯”åˆ†æäº†åŸºäºè½»é‡çº§è½¬æ¢æ¨¡å‹çš„DistilHuBERTå’ŒPaSSTï¼Œé€šè¿‡å¯¹CREMA-Dæ•°æ®é›†çš„æ ¸å¿ƒå…­ç§æƒ…ç»ªè¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨MFCCç‰¹å¾çš„ä¼ ç»ŸCNN-LSTMåŸºçº¿æ¨¡å‹æ¥è¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ã€‚DistilHuBERTåœ¨ä¿æŒæ¨¡å‹å¤§å°å¼‚å¸¸å°ï¼ˆä»…0.02MBï¼‰çš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’ŒF1åˆ†æ•°ï¼ˆåˆ†åˆ«ä¸º70.64%å’Œ70.36%ï¼‰ï¼Œä¼˜äºPaSSTå’ŒåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹PaSSTçš„ä¸‰ç§å˜ä½“ï¼ˆLinearã€MLPå’ŒAttentive Pooling headsï¼‰è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥äº†è§£åˆ†ç±»å¤´éƒ¨æ¶æ„å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒPaSSTçš„MLPå¤´å˜ä½“åœ¨å…¶å˜ä½“ä¸­è¡¨ç°æœ€ä½³ï¼Œä½†ä»ä¸åŠDistilHuBERTã€‚åœ¨æƒ…ç»ªç±»åˆ«ä¸­ï¼Œæ„¤æ€’çš„æƒ…ç»ªå§‹ç»ˆæ˜¯æœ€å‡†ç¡®æ£€æµ‹çš„ï¼Œè€ŒåŒæ¶åˆ™æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒåƒDistilHuBERTè¿™æ ·çš„è½»é‡çº§è½¬æ¢å™¨ä¸ºè¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶è¯­éŸ³æƒ…ç»ªè¯†åˆ«æä¾›äº†å¼•äººæ³¨ç›®çš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/luckymaduabuchi/Emotion-detection-%E3%80%82">https://github.com/luckymaduabuchi/Emotion-detection-ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00402v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¯¹æ¯”åˆ†æäº†åŸºäºè½»é‡çº§å˜å‹å™¨çš„æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹DistilHuBERTå’ŒPaSSTï¼Œä»¥åŠä¼ ç»Ÿçš„CNN-LSTMåŸºçº¿æ¨¡å‹ã€‚åœ¨CREMA-Dæ•°æ®é›†ä¸Šè¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼ŒDistilHuBERTè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’ŒF1å¾—åˆ†ï¼Œä¸”æ¨¡å‹ä½“ç§¯è¾ƒå°ã€‚PaSSTçš„MLPå¤´å˜ä½“æ€§èƒ½æœ€ä½³ä½†ä»é€ŠäºDistilHuBERTã€‚ç ”ç©¶è¡¨æ˜ï¼Œè½»é‡çº§å˜å‹å™¨ä¸ºè¾¹ç¼˜è®¾å¤‡çš„å®æ—¶è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿè¯†åˆ«åœ¨å¼€å‘å…·æœ‰åŒæƒ…å¿ƒçš„è®¡ç®—æœºäº¤äº’ç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>DistilHuBERTæ¨¡å‹åœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œè¾ƒå°çš„æ¨¡å‹ä½“ç§¯ã€‚</li>
<li>PaSSTæ¨¡å‹çš„MLPå¤´å˜ä½“åœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢æ€§èƒ½æœ€ä½³ã€‚</li>
<li>åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼ˆå¦‚DistilHuBERTï¼‰å¯¹äºå®æ—¶è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>â€œæ„¤æ€’â€æƒ…ç»ªæ˜¯æœ€å®¹æ˜“æ£€æµ‹çš„æƒ…æ„Ÿç±»åˆ«ï¼Œâ€œåŒæ¶â€æƒ…æ„Ÿåˆ™æ˜¯æœ€éš¾æ£€æµ‹çš„ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æ–°çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„æ¯”è¾ƒåŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-42663eab196d35d9694f25232da24c94~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376476&auth_key=1762376476-0-0-fa15471512d1098d702dc7c2e3ed088a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d1e53b34827f8e85ed660297be9528e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376484&auth_key=1762376484-0-0-41c4a5df27cd775521f923a4dee92260&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8af27e58b20f6544a77b31ad16ce58a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376490&auth_key=1762376490-0-0-a401eed48b76bae6a799a2b2c84eea84&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-534e9ca61d0ef3dbb4e6cf5f813eabc3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376496&auth_key=1762376496-0-0-0accb9cf4781f7d219a1de5b41a42841&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0a18293538c4a9428e7833a862a10d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376502&auth_key=1762376502-0-0-885b466c7f94a21bb4bb86409d5fa2ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94a4565bbb9129870a9e9afb3c6d711b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376509&auth_key=1762376509-0-0-fcb0c0a43c0df0ad4e4d298100694a92&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c97fb5cd5d024fbed299ef6983fbb04~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376515&auth_key=1762376515-0-0-4b1e73b87ced155208d68dfe5e3bc4fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-daf88e054cc28107266099f04c7aebc3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376521&auth_key=1762376521-0-0-34c24f147382afa47490eaa6fc06f6de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-949ff2e45561c84ab92629b9d44e93fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376528&auth_key=1762376528-0-0-0db2855c7e89e59fc528111266b03992&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DARAS-Dynamic-Audio-Room-Acoustic-Synthesis-for-Blind-Room-Impulse-Response-Estimation"><a href="#DARAS-Dynamic-Audio-Room-Acoustic-Synthesis-for-Blind-Room-Impulse-Response-Estimation" class="headerlink" title="DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse   Response Estimation"></a>DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse   Response Estimation</h2><p><strong>Authors:Chunxi Wang, Maoshen Jia, Wenyu Jin</strong></p>
<p>Room Impulse Responses (RIRs) accurately characterize acoustic properties of indoor environments and play a crucial role in applications such as speech enhancement, speech recognition, and audio rendering in augmented reality (AR) and virtual reality (VR). Existing blind estimation methods struggle to achieve practical accuracy. To overcome this challenge, we propose the dynamic audio-room acoustic synthesis (DARAS) model, a novel deep learning framework that is explicitly designed for blind RIR estimation from monaural reverberant speech signals. First, a dedicated deep audio encoder effectively extracts relevant nonlinear latent space features. Second, the Mamba-based self-supervised blind room parameter estimation (MASS-BRPE) module, utilizing the efficient Mamba state space model (SSM), accurately estimates key room acoustic parameters and features. Third, the system incorporates a hybrid-path cross-attention feature fusion module, enhancing deep integration between audio and room acoustic features. Finally, our proposed dynamic acoustic tuning (DAT) decoder adaptively segments early reflections and late reverberation to improve the realism of synthesized RIRs. Experimental results, including a MUSHRA-based subjective listening study, demonstrate that DARAS substantially outperforms existing baseline models, providing a robust and effective solution for practical blind RIR estimation in real-world acoustic environments. </p>
<blockquote>
<p>å®¤å†…ç¯å¢ƒçš„å£°å­¦ç‰¹æ€§å¯ä»¥é€šè¿‡æˆ¿é—´è„‰å†²å“åº”ï¼ˆRIRsï¼‰æ¥ç²¾ç¡®æè¿°ï¼Œå…¶åœ¨è¯­éŸ³å¢å¼ºã€è¯­éŸ³è¯†åˆ«ä»¥åŠå¢å¼ºç°å®ï¼ˆARï¼‰å’Œè™šæ‹Ÿç°å®ï¼ˆVRï¼‰ä¸­çš„éŸ³é¢‘æ¸²æŸ“ç­‰åº”ç”¨ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰çš„ç›²ä¼°è®¡æ–¹æ³•éš¾ä»¥å®ç°å®ç”¨å‡†ç¡®æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€éŸ³é¢‘æˆ¿é—´å£°å­¦åˆæˆï¼ˆDARASï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºä»å•å£°é“æ··å“è¯­éŸ³ä¿¡å·è¿›è¡Œç›²RIRä¼°è®¡è€Œè®¾è®¡çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚é¦–å…ˆï¼Œä¸“ç”¨çš„æ·±åº¦éŸ³é¢‘ç¼–ç å™¨æœ‰æ•ˆåœ°æå–äº†ç›¸å…³çš„éçº¿æ€§æ½œåœ¨ç©ºé—´ç‰¹å¾ã€‚å…¶æ¬¡ï¼ŒåŸºäºMambaçš„è‡ªç›‘ç£ç›²æˆ¿é—´å‚æ•°ä¼°è®¡ï¼ˆMASS-BRPEï¼‰æ¨¡å—ï¼Œåˆ©ç”¨é«˜æ•ˆçš„MambaçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œå‡†ç¡®ä¼°è®¡äº†å…³é”®çš„æˆ¿é—´å£°å­¦å‚æ•°å’Œç‰¹å¾ã€‚ç¬¬ä¸‰ï¼Œç³»ç»Ÿç»“åˆäº†ä¸€ä¸ªæ··åˆè·¯å¾„äº¤å‰æ³¨æ„ç‰¹å¾èåˆæ¨¡å—ï¼Œå¢å¼ºäº†éŸ³é¢‘å’Œæˆ¿é—´å£°å­¦ç‰¹å¾ä¹‹é—´çš„æ·±åº¦é›†æˆã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºçš„åŠ¨æ€å£°å­¦è°ƒæ•´ï¼ˆDATï¼‰è§£ç å™¨è‡ªé€‚åº”åœ°åˆ†å‰²æ—©æœŸåå°„å’ŒåæœŸå›å£°ï¼Œæé«˜äº†åˆæˆRIRsçš„çœŸå®æ€§ã€‚å®éªŒç»“æœåŒ…æ‹¬åŸºäºMUSHRAçš„ä¸»è§‚å¬è§‰ç ”ç©¶ï¼Œè¯æ˜DARASåœ¨çœŸå®ä¸–ç•Œå£°å­¦ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºå®é™…ç›²RIRä¼°è®¡æä¾›äº†ç¨³å¥æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08135v2">PDF</a> 14 pages, 9 figures, accepted for publication in IEEE&#x2F;ACM   Transactions on Audio, Speech, and Language Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŠ¨æ€éŸ³é¢‘å®¤å£°åˆæˆï¼ˆDARASï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºä»å•å£°é“æ··å“è¯­éŸ³ä¿¡å·ä¸­ç›²ä¼°è®¡æˆ¿é—´å†²å‡»å“åº”ï¼ˆRIRï¼‰è€Œè®¾è®¡çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚DARASé€šè¿‡æ·±åº¦éŸ³é¢‘ç¼–ç å™¨æå–ç›¸å…³éçº¿æ€§æ½œåœ¨ç©ºé—´ç‰¹å¾ï¼Œåˆ©ç”¨åŸºäºMambaçš„è‡ªæˆ‘ç›‘ç£ç›²å®¤å‚æ•°ä¼°è®¡æ¨¡å—å‡†ç¡®ä¼°è®¡å…³é”®å®¤å£°å‚æ•°å’Œç‰¹å¾ï¼Œå¹¶ç»“åˆæ··åˆè·¯å¾„äº¤å‰æ³¨æ„ç‰¹å¾èåˆæ¨¡å—ï¼Œæé«˜éŸ³é¢‘å’Œå®¤å£°ç‰¹å¾çš„æ·±åº¦èåˆã€‚æœ€åï¼Œé€šè¿‡åŠ¨æ€å£°å­¦è°ƒæ•´è§£ç å™¨è‡ªé€‚åº”åˆ†å‰²æ—©æœŸåå°„å’ŒåæœŸæ··å“ï¼Œæé«˜äº†åˆæˆRIRçš„çœŸå®æ„Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDARASåœ¨çœŸå®ä¸–ç•Œå£°å­¦ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œä¸ºå®é™…ç›²RIRä¼°è®¡æä¾›äº†ç¨³å¥æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€éŸ³é¢‘å®¤å£°åˆæˆï¼ˆDARASï¼‰æ¨¡å‹æ˜¯ä¸€ç§ç”¨äºç›²ä¼°è®¡æˆ¿é—´å†²å‡»å“åº”ï¼ˆRIRï¼‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>DARASèƒ½å¤Ÿä»å•å£°é“æ··å“è¯­éŸ³ä¿¡å·ä¸­æœ‰æ•ˆæå–ç›¸å…³ç‰¹å¾ã€‚</li>
<li>Mamba-basedè‡ªæˆ‘ç›‘ç£ç›²å®¤å‚æ•°ä¼°è®¡æ¨¡å—ç”¨äºå‡†ç¡®ä¼°è®¡å…³é”®å®¤å£°å‚æ•°ã€‚</li>
<li>æ··åˆè·¯å¾„äº¤å‰æ³¨æ„ç‰¹å¾èåˆæ¨¡å—æé«˜äº†éŸ³é¢‘å’Œå®¤å£°ç‰¹å¾çš„æ·±åº¦èåˆã€‚</li>
<li>åŠ¨æ€å£°å­¦è°ƒæ•´è§£ç å™¨è‡ªé€‚åº”åˆ†å‰²æ—©æœŸåå°„å’ŒåæœŸæ··å“ï¼Œå¢å¼ºäº†åˆæˆRIRçš„çœŸå®æ„Ÿã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜DARASåœ¨çœŸå®ä¸–ç•Œå£°å­¦ç¯å¢ƒä¸­æ€§èƒ½ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-da4b9c22a084740008cb03497e4f05a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376536&auth_key=1762376536-0-0-96d1e5630eed11e6c02183d903699290&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21e441330753ef7c5e0d026b0a5c7a68~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376543&auth_key=1762376543-0-0-49ab6543398f3533ab93f1055dee82aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d00637c650262377999b5fa4e0cb9c2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376550&auth_key=1762376550-0-0-b6bcb164933e83734f4743d5307fd1b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-840f62d1c6c79a1551f62f76285257e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376556&auth_key=1762376556-0-0-3326345ca2e3b5f21950aa5ee25be73d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeepOmni-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE"><a href="#DeepOmni-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE" class="headerlink" title="DeepOmni: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE"></a>DeepOmni: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE</h2><p><strong>Authors:Hang Shao, Heting Gao, Yunhang Shen, Jiawei Chen, Zuwei Long, Dong Yang, Ke Li, Xing Sun</strong></p>
<p>Native multimodal large language models (MLLMs) restructure a single large language model (LLM) into a spoken language model (SLM) capable of both speech and text generation. Compared to modular and aligned MLLMs, native MLLMs preserve richer paralinguistic features such as emotion and prosody, and generate speech responses directly within the backbone LLM rather than using a separate speech decoder. This integration also results in lower response latency and smoother interaction. However, native MLLMs suffer from catastrophic forgetting and performance degradation because the available paired speech-text data is insufficient to support the pretraining of MLLMs compared to the vast amount of text data required to pretrain text LLMs. To address this issue, we propose DeepTalk, a framework for adaptive modality expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk first adaptively distinguishes modality experts according to their modality load within the LLM. Each modality expert then undergoes specialized single-modality training, followed by joint multimodal collaborative training. As a result, DeepTalk incurs only a 5.5% performance drop compared to the original LLM, which is significantly lower than the average performance drop of over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within 0.5 seconds, ensuring a seamless and intelligent speech interaction experience. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk">https://github.com/talkking/DeepTalk</a>. </p>
<blockquote>
<p>åŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å°†å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡æ„ä¸ºèƒ½å¤Ÿç”Ÿæˆè¯­éŸ³å’Œæ–‡æœ¬çš„å£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ã€‚ä¸æ¨¡å—åŒ–å’Œå¯¹é½çš„MLLMsç›¸æ¯”ï¼ŒåŸç”ŸMLLMsä¿ç•™äº†æ›´ä¸°å¯Œçš„å‰¯è¯­è¨€ç‰¹å¾ï¼Œå¦‚æƒ…æ„Ÿå’Œè¯­è°ƒï¼Œå¹¶åœ¨ä¸»å¹²LLMå†…ç›´æ¥ç”Ÿæˆè¯­éŸ³å“åº”ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å•ç‹¬çš„è¯­éŸ³è§£ç å™¨ã€‚è¿™ç§é›†æˆè¿˜å¸¦æ¥äº†æ›´ä½çš„å“åº”å»¶è¿Ÿå’Œæ›´æµç•…çš„äº’åŠ¨ã€‚ç„¶è€Œï¼Œç”±äºå¯ç”¨çš„é…å¯¹è¯­éŸ³-æ–‡æœ¬æ•°æ®ä¸è¶³ä»¥æ”¯æŒMLLMsçš„é¢„è®­ç»ƒï¼Œä¸éœ€è¦é¢„è®­ç»ƒæ–‡æœ¬LLMsçš„å¤§é‡æ–‡æœ¬æ•°æ®ç›¸æ¯”ï¼ŒåŸç”ŸMLLMsé­å—ç¾éš¾æ€§é—å¿˜å’Œæ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepTalkï¼Œä¸€ä¸ªåŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„è‡ªé€‚åº”æ¨¡æ€ä¸“å®¶å­¦ä¹ æ¡†æ¶ã€‚DeepTalké¦–å…ˆæ ¹æ®LLMå†…çš„æ¨¡æ€è´Ÿè½½è‡ªé€‚åº”åœ°åŒºåˆ†æ¨¡æ€ä¸“å®¶ã€‚ç„¶åï¼Œæ¯ä¸ªæ¨¡æ€ä¸“å®¶è¿›è¡Œä¸“ä¸šåŒ–çš„å•æ¨¡æ€è®­ç»ƒï¼Œæ¥ç€è¿›è¡Œè”åˆå¤šæ¨¡æ€åä½œè®­ç»ƒã€‚å› æ­¤ï¼ŒDeepTalkçš„æ€§èƒ½ä»…æ¯”åŸå§‹LLMä¸‹é™5.5%ï¼Œè¿œä½äºåŸç”ŸMLLMsé€šå¸¸å‡ºç°çš„è¶…è¿‡20%çš„å¹³å‡æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸æ¨¡å—åŒ–MLLMsç›¸å½“ã€‚åŒæ—¶ï¼Œç«¯åˆ°ç«¯å¯¹è¯å»¶è¿Ÿä¿æŒåœ¨0.5ç§’å†…ï¼Œç¡®ä¿æ— ç¼ä¸”æ™ºèƒ½çš„è¯­éŸ³äº¤äº’ä½“éªŒã€‚ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk%E3%80%82">https://github.com/talkking/DeepTalkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21864v3">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤ŸåŒæ—¶å¤„ç†è¯­éŸ³å’Œæ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›ã€‚ç›¸è¾ƒäºæ¨¡å—åŒ–å’Œå¯¹é½çš„MLLMsï¼ŒåŸç”ŸMLLMsä¿ç•™äº†æ›´ä¸°å¯Œçš„è¯­è¨€ç‰¹å¾å¦‚æƒ…æ„Ÿå’Œè¯­è°ƒã€‚ä½†å…¶åœ¨é¢„è®­ç»ƒæ—¶å¯¹é…å¯¹è¯­éŸ³-æ–‡æœ¬æ•°æ®çš„éœ€æ±‚è¾ƒé«˜ï¼Œä½¿å¾—å…¶æ€§èƒ½åœ¨æ•°æ®ä¸è¶³æ—¶æ˜“å—å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DeepTalkæ¡†æ¶ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„è¿›è¡Œè‡ªé€‚åº”æ¨¡æ€ä¸“å®¶å­¦ä¹ ã€‚DeepTalkèƒ½åœ¨LLMå†…éƒ¨è‡ªé€‚åº”åœ°åŒºåˆ†æ¨¡æ€ä¸“å®¶å¹¶è¿›è¡Œå•ä¸€æ¨¡æ€å’Œè”åˆå¤šæ¨¡æ€ååŒè®­ç»ƒï¼Œä»è€Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æé«˜å“åº”é€Ÿåº¦ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤ŸåŒæ—¶å¤„ç†è¯­éŸ³å’Œæ–‡æœ¬ç”Ÿæˆï¼Œä¿ç•™äº†ä¸°å¯Œçš„è¯­è¨€ç‰¹å¾ã€‚</li>
<li>ä¸æ¨¡å—åŒ–å’Œå¯¹é½çš„MLLMsç›¸æ¯”ï¼ŒåŸç”ŸMLLMså…·æœ‰æ›´ä½çš„å“åº”å»¶è¿Ÿå’Œæ›´æµç•…çš„äº’åŠ¨æ€§èƒ½ã€‚</li>
<li>åŸç”ŸMLLMsé¢ä¸´æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½æ˜“å—å½±å“ã€‚</li>
<li>DeepTalkæ¡†æ¶é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„è¿›è¡Œè‡ªé€‚åº”æ¨¡æ€ä¸“å®¶å­¦ä¹ ï¼Œæ—¨åœ¨è§£å†³åŸç”ŸMLLMsåœ¨æ•°æ®ä¸è¶³æ—¶çš„æ€§èƒ½é—®é¢˜ã€‚</li>
<li>DeepTalkæ¡†æ¶åŒ…æ‹¬è‡ªé€‚åº”åŒºåˆ†æ¨¡æ€ä¸“å®¶ã€å•ä¸€æ¨¡æ€è®­ç»ƒå’Œè”åˆå¤šæ¨¡æ€ååŒè®­ç»ƒä¸‰ä¸ªä¸»è¦æ­¥éª¤ã€‚</li>
<li>DeepTalkç›¸è¾ƒäºåŸç”ŸLLMçš„æ€§èƒ½ä¸‹é™ä»…æœ‰5.5%ï¼Œæ˜¾è‘—ä¼˜äºå¤§å¤šæ•°åŸç”ŸMLLMsï¼ˆå¦‚GLM-4-Voiceï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5f40abedd2ad73f088c4fd0518dddd9b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376564&auth_key=1762376564-0-0-1ab8977cff956144555d2845a68f6a9b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe8595fb480aa96b73e1a1f9e399111b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376571&auth_key=1762376571-0-0-5817bc33f3080567d049943e14f841ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6fecc5d706d591e4f84b7786fed7dcc3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376577&auth_key=1762376577-0-0-ed6786de5280cccb460f723d5e4eb4a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f18fe3941dc0f81d211bb977c620b94f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376584&auth_key=1762376584-0-0-29e643bc4d4f8b841c652283e6570165&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ARECHO-Autoregressive-Evaluation-via-Chain-Based-Hypothesis-Optimization-for-Speech-Multi-Metric-Estimation"><a href="#ARECHO-Autoregressive-Evaluation-via-Chain-Based-Hypothesis-Optimization-for-Speech-Multi-Metric-Estimation" class="headerlink" title="ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis   Optimization for Speech Multi-Metric Estimation"></a>ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis   Optimization for Speech Multi-Metric Estimation</h2><p><strong>Authors:Jiatong Shi, Yifan Cheng, Bo-Hao Su, Hye-jin Shim, Jinchuan Tian, Samuele Cornell, Yiwen Zhao, Siddhant Arora, Shinji Watanabe</strong></p>
<p>Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding. </p>
<blockquote>
<p>è¯­éŸ³ä¿¡å·å¤„ç†é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³è´¨é‡è¯„ä¼°å’Œè¯­éŸ³ç‰¹å¾æè¿°ç­‰ä»»åŠ¡ä¸­ã€‚è¿™äº›ä»»åŠ¡çš„ç›®æ ‡æ˜¯é¢„æµ‹å¤šç§æ„ŸçŸ¥å’Œå®¢è§‚æŒ‡æ ‡ã€‚ä¾‹å¦‚ï¼ŒPESQï¼ˆè¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä¼°ï¼‰ã€STOIï¼ˆçŸ­æœŸå®¢è§‚å¯æ‡‚åº¦ï¼‰å’ŒMOSï¼ˆå¹³å‡æ„è§å¾—åˆ†ï¼‰ç­‰æŒ‡æ ‡å„è‡ªæ•æ‰è¯­éŸ³è´¨é‡çš„ä¸åŒæ–¹é¢ã€‚ç„¶è€Œï¼Œè¿™äº›æŒ‡æ ‡é€šå¸¸æœ‰ä¸åŒçš„å°ºåº¦ã€å‡è®¾å’Œä¾èµ–å…³ç³»ï¼Œä½¿å¾—è”åˆä¼°è®¡å˜å¾—éå¹³å‡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARECHOï¼ˆåŸºäºé“¾å‡è®¾ä¼˜åŒ–çš„è‡ªå›å½’è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªå›å½’ä¾èµ–å»ºæ¨¡çš„é€šç”¨è¯­éŸ³è¯„ä¼°ç³»ç»Ÿã€‚ARECHOä»¥ä¸‰ç§å…³é”®åˆ›æ–°ä¸ºç‰¹è‰²ï¼šï¼ˆ1ï¼‰å…¨é¢çš„è¯­éŸ³ä¿¡æ¯æ ‡è®°æµç¨‹ï¼›ï¼ˆ2ï¼‰åŠ¨æ€åˆ†ç±»å™¨é“¾ï¼Œèƒ½å¤Ÿæ˜ç¡®æ•æ‰æŒ‡æ ‡é—´çš„ä¾èµ–å…³ç³»ï¼›ï¼ˆ3ï¼‰ä¸¤æ­¥ä»¥ä¿¡å¿ƒä¸ºå¯¼å‘çš„è§£ç ç®—æ³•ï¼Œæé«˜æ¨ç†å¯é æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§è¯„ä¼°åœºæ™¯ä¸­ï¼ŒARECHOæ˜¾è‘—ä¼˜äºåŸºå‡†æ¡†æ¶ï¼ŒåŒ…æ‹¬å¢å¼ºçš„è¯­éŸ³åˆ†æã€è¯­éŸ³ç”Ÿæˆè¯„ä¼°å’Œå™ªå£°è¯­éŸ³è¯„ä¼°ã€‚æ­¤å¤–ï¼Œå…¶åŠ¨æ€ä¾èµ–å»ºæ¨¡é€šè¿‡æ•æ‰æŒ‡æ ‡ä¹‹é—´çš„å…³ç³»æé«˜äº†å¯è§£é‡Šæ€§ã€‚åœ¨å„ç§ä»»åŠ¡ä¸­ï¼ŒARECHOä½¿ç”¨å…¶åŠ¨æ€åˆ†ç±»å™¨é“¾è¿›è¡Œæ— å‚è€ƒè¯„ä¼°ï¼Œæ”¯æŒå­é›†æŸ¥è¯¢ï¼ˆå•ä¸ªæˆ–å¤šä¸ªæŒ‡æ ‡ï¼‰ï¼Œå¹¶é€šè¿‡ä¿¡å¿ƒå¯¼å‘çš„è§£ç å‡å°‘é”™è¯¯ä¼ æ’­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24518v2">PDF</a> NeurIPS 2025 Spotlight</p>
<p><strong>Summary</strong>ï¼š<br>è¯­éŸ³ä¿¡å·åˆ†æé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹å¤šç§æ„ŸçŸ¥å’Œå®¢è§‚æŒ‡æ ‡çš„ä»»åŠ¡ä¸­ï¼Œå¦‚è¯­éŸ³è´¨é‡è¯„ä¼°å’Œè¯­éŸ³ç‰¹å¾æè¿°ã€‚ä¸ºäº†å…‹æœä¸åŒè¯„ä»·æŒ‡æ ‡ä¹‹é—´çš„å·®å¼‚å’Œå¤æ‚æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé“¾å¼å‡è®¾ä¼˜åŒ–çš„è‡ªå›å½’è¯„ä¼°ç³»ç»ŸARECHOã€‚è¯¥ç³»ç»Ÿå…·æœ‰å…¨é¢çš„è¯­éŸ³ä¿¡æ¯æ ‡è®°æµç¨‹ã€åŠ¨æ€åˆ†ç±»å™¨é“¾å’Œä¸¤æ­¥ç½®ä¿¡å¯¼å‘è§£ç ç®—æ³•ç­‰ä¸‰å¤§åˆ›æ–°ç‚¹ï¼Œå¯æ˜¾è‘—æé«˜è¯­éŸ³è¯„ä¼°çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒARECHOåœ¨ä¸åŒè¯„ä¼°åœºæ™¯ä¸­å‡ä¼˜äºåŸºå‡†æ¡†æ¶ï¼Œå¹¶æä¾›äº†å­é›†æŸ¥è¯¢çš„å‚è€ƒæ— å…³è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­éŸ³ä¿¡å·åˆ†æå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹å¤šä¸ªæ„ŸçŸ¥å’Œå®¢è§‚æŒ‡æ ‡çš„ä»»åŠ¡ä¸­ã€‚</li>
<li>ARECHOæ˜¯ä¸€ä¸ªåŸºäºé“¾å¼çš„é€šç”¨è¯­éŸ³è¯„ä¼°ç³»ç»Ÿï¼Œå…·æœ‰ä¸‰å¤§åˆ›æ–°ç‚¹ã€‚</li>
<li>ARECHOé€šè¿‡å…¨é¢çš„è¯­éŸ³ä¿¡æ¯æ ‡è®°æµç¨‹æé«˜äº†è¯­éŸ³è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
<li>åŠ¨æ€åˆ†ç±»å™¨é“¾èƒ½å¤Ÿæ˜¾å¼æ•è·æŒ‡æ ‡é—´çš„ä¾èµ–å…³ç³»ï¼Œæé«˜ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>ä¸¤æ­¥ç½®ä¿¡å¯¼å‘è§£ç ç®—æ³•å¢å¼ºäº†æ¨æ–­çš„å¯é æ€§ã€‚</li>
<li>ARECHOåœ¨å¤šç§è¯„ä¼°åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸºå‡†æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f91e1bd5a759561c1db71b9fa855a3be~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376591&auth_key=1762376591-0-0-1856f12ce60224c0776ae4459517abd9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OmniResponse-Online-Multimodal-Conversational-Response-Generation-in-Dyadic-Interactions"><a href="#OmniResponse-Online-Multimodal-Conversational-Response-Generation-in-Dyadic-Interactions" class="headerlink" title="OmniResponse: Online Multimodal Conversational Response Generation in   Dyadic Interactions"></a>OmniResponse: Online Multimodal Conversational Response Generation in   Dyadic Interactions</h2><p><strong>Authors:Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem</strong></p>
<p>In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task designed to produce synchronized verbal and non-verbal listener feedback online, based on the speakerâ€™s multimodal inputs. OMCRG captures natural dyadic interactions and introduces new challenges in aligning generated audio with listenersâ€™ facial responses. To tackle these challenges, we incorporate text as an intermediate modality to connect audio and facial responses. We propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates accurate multimodal listener responses. OmniResponse leverages a pretrained LLM enhanced with two core components: Chrono-Text Markup, which precisely timestamps generated text tokens, and TempoVoice, a controllable online text-to-speech (TTS) module that outputs speech synchronized with facial responses. To advance OMCRG research, we offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and annotated facial behaviors. Comprehensive evaluations on ResponseNet demonstrate that OmniResponse outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality. Our dataset, code, and models are publicly available. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åœ¨çº¿å¤šæ¨¡å¼å¯¹è¯å“åº”ç”Ÿæˆï¼ˆOMCRGï¼‰è¿™ä¸€æ–°ä»»åŠ¡ï¼Œå®ƒçš„è®¾è®¡ç›®çš„æ˜¯åŸºäºè¯´è¯è€…çš„å¤šæ¨¡å¼è¾“å…¥ï¼Œåœ¨çº¿äº§ç”ŸåŒæ­¥çš„è¨€è¯­å’Œéè¨€è¯­å¬ä¼—åé¦ˆã€‚OMCRGæ•æ‰è‡ªç„¶çš„äºŒå…ƒäº¤äº’ï¼Œå¹¶åœ¨å°†ç”Ÿæˆçš„éŸ³é¢‘ä¸å¬ä¼—çš„é¢éƒ¨å“åº”å¯¹é½æ–¹é¢å¼•å…¥æ–°çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨æ–‡æœ¬ä½œä¸ºè¿æ¥éŸ³é¢‘å’Œé¢éƒ¨å“åº”çš„ä¸­é—´æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†OmniResponseï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå‡†ç¡®çš„å¤šæ¨¡å¼å¬ä¼—å“åº”ã€‚OmniResponseåˆ©ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶å¢å¼ºäº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šChrono-Textæ ‡è®°ï¼Œå®ƒä¸ºç”Ÿæˆçš„æ–‡æœ¬æ ‡è®°æä¾›ç²¾ç¡®çš„æ—¶é—´æˆ³ï¼Œä»¥åŠTempoVoiceï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ§çš„åœ¨çº¿æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ï¼Œè¾“å‡ºä¸é¢éƒ¨å“åº”åŒæ­¥çš„è¯­éŸ³ã€‚ä¸ºäº†æ¨è¿›OMCRGç ”ç©¶ï¼Œæˆ‘ä»¬æä¾›äº†ResponseNetæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«696ä¸ªè¯¦ç»†çš„äºŒå…ƒäº¤äº’ç‰¹å¾ï¼ŒåŒ…æ‹¬åŒæ­¥åˆ†å±è§†é¢‘ã€å¤šé€šé“éŸ³é¢‘ã€å­—å¹•å’Œæ³¨é‡Šçš„é¢éƒ¨è¡Œä¸ºã€‚åœ¨ResponseNetä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniResponseåœ¨è¯­ä¹‰è¯­éŸ³å†…å®¹ã€è§†å¬åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹éƒ½æ˜¯å…¬å¼€å¯ç”¨çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21724v2">PDF</a> 25 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨çº¿å¤šæ¨¡æ€å¯¹è¯å“åº”ç”Ÿæˆï¼ˆOMCRGï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨åŸºäºè¯´è¯è€…çš„å¤šæ¨¡æ€è¾“å…¥ï¼Œåœ¨çº¿ç”ŸæˆåŒæ­¥çš„è¨€è¯­å’Œéè¨€è¯­å¬ä¼—åé¦ˆã€‚OMCRGæ•æ‰è‡ªç„¶åŒå‘äº’åŠ¨ï¼Œå¹¶ä¸ºç”ŸæˆéŸ³é¢‘ä¸å¬ä¼—çš„é¢éƒ¨å“åº”å¯¹é½å¸¦æ¥æ–°æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡é‡‡ç”¨æ–‡æœ¬ä½œä¸ºè¿æ¥éŸ³é¢‘å’Œé¢éƒ¨å“åº”çš„ä¸­é—´æ¨¡æ€ã€‚æå‡ºOmniResponseå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå‡†ç¡®çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ã€‚OmniResponseåˆ©ç”¨å¸¦æœ‰ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶çš„é¢„è®­ç»ƒLLMï¼šç²¾ç¡®æ—¶é—´æˆ³ç”Ÿæˆæ–‡æœ¬æ ‡è®°çš„Chrono-Textå’Œå¯æ§çš„åœ¨çº¿æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—TempoVoiceï¼Œè¾“å‡ºä¸é¢éƒ¨å“åº”åŒæ­¥çš„è¯­éŸ³ã€‚ä¸ºæ¨è¿›OMCRGç ”ç©¶ï¼Œæœ¬æ–‡æä¾›äº†ResponseNetæ•°æ®é›†ï¼ŒåŒ…å«696ä¸ªè¯¦ç»†çš„åŒå‘äº’åŠ¨ç‰¹å¾ï¼ŒåŒæ­¥åˆ†å±è§†é¢‘ã€å¤šé€šé“éŸ³é¢‘ã€æ–‡å­—è®°å½•ä»¥åŠæ³¨é‡Šçš„é¢éƒ¨è¡Œä¸ºã€‚åœ¨ResponseNetä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniResponseåœ¨è¯­ä¹‰è¯­éŸ³å†…å®¹ã€éŸ³è§†é¢‘åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å‡å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OMCRGä»»åŠ¡æ—¨åœ¨åœ¨çº¿ç”ŸæˆåŸºäºè¯´è¯è€…å¤šæ¨¡æ€è¾“å…¥çš„åŒæ­¥è¨€è¯­å’Œéè¨€è¯­å¬ä¼—åé¦ˆã€‚</li>
<li>OMCRGæ•æ‰è‡ªç„¶åŒå‘äº’åŠ¨ï¼Œå¹¶å¸¦æ¥å¯¹é½ç”ŸæˆéŸ³é¢‘ä¸å¬ä¼—é¢éƒ¨å“åº”çš„æ–°æŒ‘æˆ˜ã€‚</li>
<li>OmniResponseå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå‡†ç¡®çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ï¼Œåˆ©ç”¨é¢„è®­ç»ƒLLMåŠå…¶ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶Chrono-Textå’ŒTempoVoiceã€‚</li>
<li>ResponseNetæ•°æ®é›†åŒ…å«è¯¦ç»†åŒå‘äº’åŠ¨ç‰¹å¾ï¼Œå¦‚åŒæ­¥åˆ†å±è§†é¢‘ã€å¤šé€šé“éŸ³é¢‘ã€æ–‡å­—è®°å½•åŠæ³¨é‡Šçš„é¢éƒ¨è¡Œä¸ºã€‚</li>
<li>OmniResponseåœ¨è¯­ä¹‰è¯­éŸ³å†…å®¹ã€éŸ³è§†é¢‘åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡å¼ºè°ƒæ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹çš„å…¬å¼€å¯ç”¨æ€§ä»¥ä¿ƒè¿›ç ”ç©¶å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0df4a9ac575b127e34446fcd7a50c026~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376599&auth_key=1762376599-0-0-d258f40eb70895ab6345ea1ae5a608fb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d896a6eac8a0f723e85c074d728ae289~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376606&auth_key=1762376606-0-0-1e606a4364e153bbb314dcbca5c04581&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-32a0bd84439c93acc23ae8db2fea79a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376613&auth_key=1762376613-0-0-fc622a8db3aba8074dfa6155cf0f07aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fdd417182d8fd617532a5348dc4b862~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376619&auth_key=1762376619-0-0-60b3e02c401082ccfadcaa0afd5433a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-head-Temporal-Latent-Attention"><a href="#Multi-head-Temporal-Latent-Attention" class="headerlink" title="Multi-head Temporal Latent Attention"></a>Multi-head Temporal Latent Attention</h2><p><strong>Authors:Keqi Deng, Philip C. Woodland</strong></p>
<p>While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality. </p>
<blockquote>
<p>è™½ç„¶Transformerçš„è‡ªæ³¨æ„åŠ›æä¾›äº†å¼ºå¤§çš„å¹¶è¡Œæ€§ï¼Œä½†é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜éšç€åºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼Œæˆä¸ºæ¨æ–­æ•ˆç‡ç“¶é¢ˆã€‚æœ€è¿‘å¼€å‘äº†å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼Œç”¨äºå°†KVç¼“å­˜å‹ç¼©åˆ°ä½é˜¶æ½œåœ¨ç©ºé—´ã€‚æœ¬æ–‡æå‡ºäº†å¤šå¤´æ—¶é—´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMTLAï¼‰ï¼Œå®ƒæ²¿æ—¶é—´ç»´åº¦è¿›ä¸€æ­¥å‡å°KVç¼“å­˜å¤§å°ï¼Œå¤§å¤§é™ä½äº†è‡ªæ³¨æ„åŠ›æ¨æ–­çš„å†…å­˜å ç”¨ã€‚MTLAé‡‡ç”¨è¶…ç½‘ç»œæ¥åŠ¨æ€åˆå¹¶æ—¶é—´ä¸Šç›¸é‚»çš„KVç¼“å­˜å‘é‡ã€‚ä¸ºäº†è§£å†³å‹ç¼©KVç¼“å­˜ä¸å¤„ç†åºåˆ—é•¿åº¦ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è·¨æ­¥æ„ŸçŸ¥å› æœæ©ç ï¼Œä»¥ç¡®ä¿æœ‰æ•ˆçš„å¹¶è¡Œè®­ç»ƒå’Œä¸æ¨æ–­è¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚å®éªŒæ¶µç›–è¯­éŸ³ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³ç†è§£å’Œæ–‡æœ¬æ‘˜è¦ç­‰å¤šé¡¹ä»»åŠ¡ï¼Œç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ç›¸æ¯”ï¼ŒMTLAåœ¨å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¤§æé«˜äº†æ¨æ–­é€Ÿåº¦å’ŒGPUå†…å­˜ä½¿ç”¨ç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è‹±æ–‡åˆ°å¾·æ–‡çš„è¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œä¸MHAç›¸æ¯”ï¼ŒMTLAå®ç°äº†5.3å€çš„åŠ é€Ÿå’ŒGPUå†…å­˜ä½¿ç”¨é‡çš„8.3å€å‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†ç¿»è¯‘è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13544v3">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong><br>     è®ºæ–‡æå‡ºMulti-head Temporal Latent Attentionï¼ˆMTLAï¼‰ï¼Œé€šè¿‡åŠ¨æ€åˆå¹¶ç›¸é‚»æ—¶åºKVç¼“å­˜å‘é‡ï¼Œæ²¿æ—¶åºç»´åº¦å‡å°KVç¼“å­˜å¤§å°ï¼Œæ˜¾è‘—é™ä½è‡ªæ³¨æ„åŠ›æ¨ç†çš„å†…å­˜å ç”¨ã€‚ä¸ºè§£å†³å‹ç¼©KVç¼“å­˜ä¸å¤„ç†åºåˆ—é•¿åº¦ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œè®ºæ–‡è¿˜æå‡ºäº†æ­¥é•¿æ„ŸçŸ¥å› æœæ©ç ï¼Œä»¥ç¡®ä¿é«˜æ•ˆçš„å¹¶è¡Œè®­ç»ƒå’Œæ¨ç†è¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMTLAåœ¨è¯­éŸ³ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³ç†è§£å’Œæ–‡æœ¬æ‘˜è¦ç­‰ä»»åŠ¡ä¸Šï¼Œä¸æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ç›¸æ¯”ï¼Œå®ç°äº†ç«äº‰æ€§çš„æ€§èƒ½å’Œæ˜¾è‘—æ”¹è¿›çš„æ¨ç†é€Ÿåº¦å’ŒGPUå†…å­˜ä½¿ç”¨ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MTLAé€šè¿‡åŠ¨æ€åˆå¹¶æ—¶åºç›¸é‚»çš„KVç¼“å­˜å‘é‡ï¼Œè¿›ä¸€æ­¥å‡å°KVç¼“å­˜å¤§å°ã€‚</li>
<li>MTLAèƒ½å¤Ÿæ˜¾è‘—é™ä½è‡ªæ³¨æ„åŠ›æ¨ç†çš„å†…å­˜å ç”¨ã€‚</li>
<li>æ­¥é•¿æ„ŸçŸ¥å› æœæ©ç è§£å†³äº†å‹ç¼©KVç¼“å­˜ä¸å¤„ç†åºåˆ—é•¿åº¦ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>MTLAåœ¨ä¿è¯ç¿»è¯‘è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›çš„5.3å€é€Ÿåº¦æå‡å’ŒGPUå†…å­˜ä½¿ç”¨å‡å°‘8.3å€ã€‚</li>
<li>MTLAåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼ŒåŒ…æ‹¬è¯­éŸ³ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³ç†è§£å’Œæ–‡æœ¬æ‘˜è¦ç­‰ã€‚</li>
<li>MTLAèƒ½å¤Ÿæé«˜æ¨ç†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d11c4004dc9bc925c7d7fb1feafda445~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376626&auth_key=1762376626-0-0-265cddc342a053d2aa36e4b20e245168&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb5eb8f5f37e79768a58ef42f872e39e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376634&auth_key=1762376634-0-0-731348bc6bcfaf701ae7f9c2c17b6f33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8189c67fd57aaf4e4607c107adc3d1d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376640&auth_key=1762376640-0-0-4cf52132ab3826f95c3544151b080555&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MultiMed-ST-Large-scale-Many-to-many-Multilingual-Medical-Speech-Translation"><a href="#MultiMed-ST-Large-scale-Many-to-many-Multilingual-Medical-Speech-Translation" class="headerlink" title="MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech   Translation"></a>MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech   Translation</h2><p><strong>Authors:Khai Le-Duc, Tuyen Tran, Bach Phan Tat, Nguyen Kim Hai Bui, Quan Dang, Hung-Phong Tran, Thanh-Thuy Nguyen, Ly Nguyen, Tuan-Minh Phan, Thi Thu Phuong Tran, Chris Ngo, Nguyen X. Khanh, Thanh Nguyen-Tang</strong></p>
<p>Multilingual speech translation (ST) and machine translation (MT) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, and Simplified&#x2F;Traditional Chinese, together with the models. With 290,000 samples, this is the largest medical MT dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most comprehensive ST analysis in the fieldâ€™s history, to our best knowledge, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed-ST">https://github.com/leduckhai/MultiMed-ST</a> </p>
<blockquote>
<p>åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ï¼ˆSTï¼‰å’Œæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰é€šè¿‡æ‰“ç ´è¯­è¨€éšœç¢å®ç°é«˜æ•ˆæ²Ÿé€šã€ç¼“è§£ä¸“ä¸šäººå‘˜çŸ­ç¼ºé—®é¢˜ã€ä¿ƒè¿›è¯Šæ–­å’Œæ²»ç–—çš„æ”¹å–„ï¼Œç‰¹åˆ«æ˜¯åœ¨ç–«æƒ…æœŸé—´æå‡äº†ç—…æ‚£æŠ¤ç†çš„æ•ˆæœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶åŒ»ç–—STæŠ€æœ¯ï¼Œå¹¶å‘å¸ƒäº†MultiMed-STæ•°æ®é›†ã€‚è¿™æ˜¯åŒ»ç–—é¢†åŸŸçš„å¤§å‹STæ•°æ®é›†ï¼Œæ¶µç›–äº†äº”ç§è¯­è¨€çš„å…¨éƒ¨ç¿»è¯‘æ–¹å‘ï¼šè¶Šå—è¯­ã€è‹±è¯­ã€å¾·è¯­ã€æ³•è¯­å’Œç®€ä½“ä¸­æ–‡&#x2F;ç¹ä½“ä¸­æ–‡ã€‚è¯¥æ•°æ®é›†åŒ…å«29ä¸‡ä¸ªæ ·æœ¬ï¼Œæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åŒ»ç–—MTæ•°æ®é›†å’Œæ‰€æœ‰é¢†åŸŸä¸­æœ€å…¨é¢çš„å¤šè¯­ç§STæ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æä¾›äº†è¿„ä»Šä¸ºæ­¢è¯¥é¢†åŸŸæœ€å…¨é¢çš„STåˆ†æï¼ŒåŒ…æ‹¬å®è¯ç ”ç©¶åŸºå‡†çº¿ã€åŒè¯­-å¤šè¯­å¯¹æ¯”ç ”ç©¶ã€ç«¯åˆ°ç«¯ä¸çº§è”å¯¹æ¯”ç ”ç©¶ã€ä»»åŠ¡ç‰¹å®šä¸å¤šä»»åŠ¡åºåˆ—åˆ°åºåˆ—å¯¹æ¯”ç ”ç©¶ã€ä»£ç åˆ‡æ¢åˆ†æå’Œå®šé‡å®šæ€§é”™è¯¯åˆ†æã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‡å¯åœ¨ç½‘ä¸Šè·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed-ST">https://github.com/leduckhai/MultiMed-ST</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03546v2">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»ç–—é¢†åŸŸä¸­çš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSTï¼‰å’Œæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰å¦‚ä½•æå‡æ‚£è€…æŠ¤ç†çš„è´¨é‡ã€‚é€šè¿‡å…‹æœè¯­è¨€éšœç¢ã€ç¼“è§£ä¸“ä¸šåŠ³åŠ¨åŠ›çŸ­ç¼ºé—®é¢˜ï¼Œä»¥åŠä¿ƒè¿›è¯Šæ–­å’Œæ²»ç–—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯ç–«æƒ…æœŸé—´ï¼Œå…¶é‡è¦æ€§å°¤ä¸ºçªå‡ºã€‚æ–‡ä¸­é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶äº†åŒ»ç–—STï¼Œå¹¶å‘å¸ƒäº†MultiMed-STæ•°æ®é›†ï¼ŒåŒ…å«äº”ç§è¯­è¨€çš„å¤§è§„æ¨¡STæ•°æ®é›†ï¼Œä»¥åŠç›¸åº”çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å…¨é¢çš„STåˆ†æï¼ŒåŒ…æ‹¬å®è¯åŸºå‡†ã€åŒè¯­-å¤šè¯­å¯¹æ¯”ç ”ç©¶ç­‰ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‡å¯åœ¨ç½‘ä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSTï¼‰å’Œæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰åœ¨åŒ»ç–—é¢†åŸŸèƒ½å¤Ÿæå‡æ‚£è€…æŠ¤ç†çš„è´¨é‡ï¼Œé€šè¿‡å…‹æœè¯­è¨€éšœç¢ã€ç¼“è§£ä¸“ä¸šåŠ³åŠ¨åŠ›çŸ­ç¼ºã€ä¿ƒè¿›è¯Šæ–­å’Œæ²»ç–—çš„æ”¹è¿›ã€‚</li>
<li>é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶åŒ»ç–—STï¼Œå¹¶å‘å¸ƒäº†MultiMed-STæ•°æ®é›†ï¼ŒåŒ…å«äº”ç§è¯­è¨€çš„å¤§è§„æ¨¡STæ•°æ®é›†ï¼Œä¸ºåŒ»ç–—é¢†åŸŸçš„ç¿»è¯‘æä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚</li>
<li>MultiMed-STæ•°æ®é›†æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åŒ»ç–—æœºå™¨ç¿»è¯‘æ•°æ®é›†ï¼Œæ¶µç›–äº†æ‰€æœ‰ç¿»è¯‘æ–¹å‘ï¼Œå¹¶ä¸”åŒ…å«äº†å¤šç§è¯­è¨€çš„æ ·æœ¬ã€‚</li>
<li>å…¨é¢çš„STåˆ†æåŒ…æ‹¬å®è¯åŸºå‡†ã€åŒè¯­-å¤šè¯­å¯¹æ¯”ç ”ç©¶ã€ç«¯åˆ°ç«¯ä¸çº§è”å¯¹æ¯”ç ”ç©¶ã€ä»»åŠ¡ç‰¹å®šä¸å¤šä»»åŠ¡åºåˆ—åˆ°åºåˆ—å¯¹æ¯”ç ”ç©¶ã€ä»£ç åˆ‡æ¢åˆ†æå’Œå®šé‡-å®šæ€§è¯¯å·®åˆ†æã€‚</li>
<li>æ‰€æœ‰ç›¸å…³çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‡å¯åœ¨ç½‘ä¸Šè·å–ï¼Œä¾¿äºç ”ç©¶äººå‘˜ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹äºæ”¹è¿›åŒ»ç–—é¢†åŸŸçš„å¤šè¯­è¨€æ²Ÿé€šå…·æœ‰é‡å¤§æ„ä¹‰ï¼Œæœ‰åŠ©äºæå‡åŒ»ç–—æœåŠ¡çš„è´¨é‡å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0be63387c5041bf364cc5eb7641d2181~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376648&auth_key=1762376648-0-0-97e37d52e8ac2f4bf8d4d61293dcb5c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34b25e4889b717b026935494ce929d8d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376655&auth_key=1762376655-0-0-93ce8fda644efaf149f5cccc140db7e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2247af5c09629ce4fe3c3b333d75ecd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376662&auth_key=1762376662-0-0-3fd43a0565c017dd3c5838294f3dbbe4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e383c9c6d34921104f097b44d113d902~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376669&auth_key=1762376669-0-0-4f13b4c35b6a121a3661523ef8efd53f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-80247dd2ae739a2cc27cfad94b02e9a4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376675&auth_key=1762376675-0-0-57a3aaf59c3f9c62550e4fb33e56dea5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf4374239637be2f1dfacfbadaff8b29~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376683&auth_key=1762376683-0-0-827aa94a203491405410e6a94bbdbba7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Modelling-Emotions-in-Face-to-Face-Setting-The-Interplay-of-Eye-Tracking-Personality-and-Temporal-Dynamics"><a href="#Modelling-Emotions-in-Face-to-Face-Setting-The-Interplay-of-Eye-Tracking-Personality-and-Temporal-Dynamics" class="headerlink" title="Modelling Emotions in Face-to-Face Setting: The Interplay of   Eye-Tracking, Personality, and Temporal Dynamics"></a>Modelling Emotions in Face-to-Face Setting: The Interplay of   Eye-Tracking, Personality, and Temporal Dynamics</h2><p><strong>Authors:Meisam Jamshidi Seikavandi, Jostein Fimland, Maria Barrett, Paolo Burelli</strong></p>
<p>Accurate emotion recognition is pivotal for nuanced and engaging human-computer interactions, yet remains difficult to achieve, especially in dynamic, conversation-like settings. In this study, we showcase how integrating eye-tracking data, temporal dynamics, and personality traits can substantially enhance the detection of both perceived and felt emotions. Seventy-three participants viewed short, speech-containing videos from the CREMA-D dataset, while being recorded for eye-tracking signals (pupil size, fixation patterns), Big Five personality assessments, and self-reported emotional states. Our neural network models combined these diverse inputs including stimulus emotion labels for contextual cues and yielded marked performance gains compared to the state-of-the-art. Specifically, perceived valence predictions reached a macro F1-score of 0.76, and models incorporating personality traits and stimulus information demonstrated significant improvements in felt emotion accuracy. These results highlight the benefit of unifying physiological, individual and contextual factors to address the subjectivity and complexity of emotional expression. Beyond validating the role of user-specific data in capturing subtle internal states, our findings inform the design of future affective computing and human-agent systems, paving the way for more adaptive and cross-individual emotional intelligence in real-world interactions. </p>
<blockquote>
<p>ç²¾ç¡®çš„æƒ…ç»ªè¯†åˆ«å¯¹äºç»†å¾®ä¸”å¼•äººå…¥èƒœçš„äººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œç„¶è€Œä»éš¾ä»¥å®ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€ã€ç±»ä¼¼å¯¹è¯çš„ç¯å¢ƒä¸­ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•æ•´åˆçœ¼åŠ¨æ•°æ®ã€æ—¶é—´åŠ¨æ€å’Œäººæ ¼ç‰¹å¾ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ„ŸçŸ¥æƒ…ç»ªå’Œæ„Ÿå—æƒ…ç»ªçš„è¯†åˆ«ã€‚73åå‚ä¸è€…è§‚çœ‹äº†æ¥è‡ªCREMA-Dæ•°æ®é›†çš„ç®€çŸ­è¯­éŸ³è§†é¢‘ï¼ŒåŒæ—¶è®°å½•çœ¼åŠ¨ä¿¡å·ï¼ˆç³å­”å¤§å°ã€æ³¨è§†æ¨¡å¼ï¼‰ã€å¤§äº”äººæ ¼è¯„ä¼°å’Œè‡ªæŠ¥æƒ…ç»ªçŠ¶æ€ã€‚æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ¨¡å‹ç»“åˆäº†è¿™äº›ä¸åŒçš„è¾“å…¥ï¼ŒåŒ…æ‹¬åˆºæ¿€æƒ…ç»ªæ ‡ç­¾ä½œä¸ºä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œæ„ŸçŸ¥ä»·å€¼é¢„æµ‹çš„å®è§‚F1åˆ†æ•°è¾¾åˆ°äº†0.76ï¼Œç»“åˆäººæ ¼ç‰¹å¾å’Œåˆºæ¿€ä¿¡æ¯çš„æ¨¡å‹åœ¨æ„Ÿå—æƒ…ç»ªå‡†ç¡®æ€§æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹å–„ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†ç»Ÿä¸€ç”Ÿç†ã€ä¸ªä½“å’Œä¸Šä¸‹æ–‡å› ç´ çš„å¥½å¤„ï¼Œä»¥åº”å¯¹æƒ…ç»ªè¡¨è¾¾çš„ä¸»è§‚æ€§å’Œå¤æ‚æ€§ã€‚é™¤äº†éªŒè¯ç”¨æˆ·ç‰¹å®šæ•°æ®åœ¨æ•æ‰å¾®å¦™å†…éƒ¨çŠ¶æ€ä¸­çš„ä½œç”¨å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜ä¸ºæœªæ¥æƒ…æ„Ÿè®¡ç®—å’Œäººæœºä»£ç†ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†ä¿¡æ¯ï¼Œä¸ºç°å®ä¸–ç•Œä¸­æ›´è‡ªé€‚åº”å’Œè·¨ä¸ªä½“çš„æƒ…ç»ªæ™ºèƒ½é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16532v2">PDF</a> The paper has been significantly revised and my colleague has already   submitted the updated version in the link below: arXiv:2510.24720</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å°†çœ¼åŠ¨æ•°æ®ã€æ—¶é—´åŠ¨æ€å’Œä¸ªäººæ€§æ ¼ç‰¹è´¨ç›¸ç»“åˆï¼Œå¯¹äºæé«˜æ„ŸçŸ¥å’Œæƒ…æ„Ÿä½“éªŒè¯†åˆ«å‡†ç¡®æ€§çš„é‡è¦æ€§ã€‚é€šè¿‡æ”¶é›†çœ¼åŠ¨ä¿¡å·ã€ä¸ªäººæ€§æ ¼è¯„ä¼°å’Œè‡ªæˆ‘æŠ¥å‘Šæƒ…æ„ŸçŠ¶æ€ç­‰å¤šæ–¹é¢çš„æ•°æ®ï¼Œç»“åˆç¥ç»ç½‘ç»œæ¨¡å‹å¯¹æƒ…å¢ƒçº¿ç´¢è¿›è¡Œå¤„ç†ï¼Œåœ¨æƒ…æ„Ÿè¯†åˆ«ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©æå‡ã€‚ç‰¹åˆ«åœ°ï¼Œæ„ŸçŸ¥ä»·å€¼é¢„æµ‹è¾¾åˆ°äº†å®è§‚F1åˆ†æ•°ä¸º0.76ï¼Œè€Œç»“åˆä¸ªäººæ€§æ ¼ç‰¹è´¨å’Œæƒ…å¢ƒä¿¡æ¯çš„æ¨¡å‹åœ¨æƒ…æ„Ÿå‡†ç¡®æ€§ä¸Šä¹Ÿæœ‰äº†æ˜¾è‘—æå‡ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†æ•´åˆç”Ÿç†ã€ä¸ªäººå’Œæƒ…å¢ƒå› ç´ çš„é‡è¦æ€§ï¼Œä¸ºè§£å†³æƒ…æ„Ÿè¡¨è¾¾çš„å¤æ‚æ€§å’Œä¸»è§‚æ€§æä¾›äº†æœ‰ç›Šçš„æ€è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœ¼åŠ¨æ•°æ®åœ¨æƒ…æ„Ÿè¯†åˆ«ä¸­çš„ä½œç”¨æ—¥ç›Šé‡è¦ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŠ¨æ€ç¯å¢ƒä¸‹çš„æƒ…ç»ªæ£€æµ‹ã€‚</li>
<li>ç»“åˆçœ¼åŠ¨æ•°æ®ã€æ—¶é—´åŠ¨æ€å’Œä¸ªäººæ€§æ ¼ç‰¹è´¨èƒ½æ˜¾è‘—æé«˜æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨å¤„ç†æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡ä½¿ç”¨ç”¨æˆ·ç‰¹å®šæ•°æ®æ•è·å¾®å¦™çš„å†…éƒ¨çŠ¶æ€éªŒè¯äº†ä¸ªäººå› ç´ å¯¹æƒ…æ„Ÿè¡¨è¾¾çš„å½±å“ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºæé«˜æœªæ¥æƒ…æ„Ÿè®¡ç®—å’Œäººæœºäº¤äº’ç³»ç»Ÿçš„é€‚åº”æ€§æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</li>
<li>ç»“åˆæƒ…å¢ƒçº¿ç´¢å’Œç”¨æˆ·ç‰¹å®šä¿¡æ¯å¯¹äºæé«˜æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0e0f5ff70734fca53693b8e2dc5cdb32~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376690&auth_key=1762376690-0-0-e47b4aed911c1c481c4df2251888f054&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-75b8c9e333203edc8670e6c83f3152b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376697&auth_key=1762376697-0-0-863b8aab86a69c91564c68b858818a9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-63e4bbbc27c8811467306417c051110f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376703&auth_key=1762376703-0-0-a808fce0b115ddf1cf3b93f206dc9210&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a874755b58c9b5a52e0a2843afe763b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376710&auth_key=1762376710-0-0-07a62c3df0e8bd45ac37eec15620f2f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-159ab7c4c54ccce0a290e33591095e39~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376716&auth_key=1762376716-0-0-5bc58f56d9df5016f959d35ace1cc46e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="As-Good-as-It-KAN-Get-High-Fidelity-Audio-Representation"><a href="#As-Good-as-It-KAN-Get-High-Fidelity-Audio-Representation" class="headerlink" title="As Good as It KAN Get: High-Fidelity Audio Representation"></a>As Good as It KAN Get: High-Fidelity Audio Representation</h2><p><strong>Authors:Patryk MarszaÅ‚ek, Maciej Rut, Piotr Kawa, PrzemysÅ‚aw Spurek, Piotr Syga</strong></p>
<p>Implicit neural representations (INR) have gained prominence for efficiently encoding multimedia data, yet their applications in audio signals remain limited. This study introduces the Kolmogorov-Arnold Network (KAN), a novel architecture using learnable activation functions, as an effective INR model for audio representation. KAN demonstrates superior perceptual performance over previous INRs, achieving the lowest Log-SpectralDistance of 1.29 and the highest Perceptual Evaluation of Speech Quality of 3.57 for 1.5 s audio. To extend KANâ€™s utility, we propose FewSound, a hypernetwork-based architecture that enhances INR parameter updates. FewSound outperforms the state-of-the-art HyperSound, with a 33.3% improvement in MSE and 60.87% in SI-SNR. These results show KAN as a robust and adaptable audio representation with the potential for scalability and integration into various hypernetwork frameworks. The source code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git">https://github.com/gmum/fewsound.git</a>. </p>
<blockquote>
<p>éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰åœ¨é«˜æ•ˆç¼–ç å¤šåª’ä½“æ•°æ®æ–¹é¢å·²å¤‡å—å…³æ³¨ï¼Œä½†åœ¨éŸ³é¢‘ä¿¡å·ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨å¯å­¦ä¹ æ¿€æ´»å‡½æ•°çš„æ–°å‹æ¶æ„ï¼Œä½œä¸ºéŸ³é¢‘è¡¨ç¤ºçš„æœ‰æ•ˆINRæ¨¡å‹ã€‚KANåœ¨æ„ŸçŸ¥æ€§èƒ½ä¸Šä¼˜äºå…ˆå‰çš„INRï¼Œåœ¨1.5ç§’éŸ³é¢‘ä¸Šå®ç°äº†æœ€ä½çš„Log-SpectralDistanceä¸º1.29å’Œæœ€é«˜çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ä¸º3.57ã€‚ä¸ºäº†æ‰©å±•KANçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¶…ç½‘ç»œçš„FewSoundæ¶æ„ï¼Œå®ƒå¢å¼ºäº†INRå‚æ•°æ›´æ–°ã€‚FewSoundçš„è¡¨ç°ä¼˜äºæœ€æ–°çš„HyperSoundï¼Œåœ¨MSEä¸Šæé«˜äº†33.3%ï¼Œåœ¨SI-SNRä¸Šæé«˜äº†60.87%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒKANæ˜¯ä¸€ç§ç¨³å¥ä¸”é€‚åº”æ€§å¼ºçš„éŸ³é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œèå…¥å„ç§è¶…ç½‘ç»œæ¡†æ¶çš„æ½œåŠ›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/gmum/fewsound.gitè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02585v3">PDF</a> Accepted to the 34th ACM International Conference on Information and   Knowledge Management (CIKM â€˜25)</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡å¼•å…¥Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨å¯å­¦ä¹ æ¿€æ´»å‡½æ•°çš„æ–°å‹æ¶æ„ï¼Œä½œä¸ºéŸ³é¢‘è¡¨ç¤ºçš„æœ‰æ•ˆéšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ¨¡å‹ã€‚KANåœ¨éŸ³é¢‘ä¿¡å·ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„INRæ¨¡å‹ï¼Œå¯¹1.5ç§’éŸ³é¢‘å®ç°äº†æœ€ä½çš„é€»è¾‘è°±è·ç¦»å’Œæœ€é«˜çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ã€‚ä¸ºæ‰©å±•KANçš„å®ç”¨æ€§ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†åŸºäºè¶…ç½‘ç»œçš„FewSoundæ¶æ„ï¼Œå¯ä¼˜åŒ–INRå‚æ•°æ›´æ–°ã€‚FewSoundä¼˜äºå½“å‰ä¸»æµçš„HyperSoundï¼Œåœ¨MSEå’ŒSI-SNRä¸Šåˆ†åˆ«æé«˜äº†33.3%å’Œ60.87%ã€‚ç»“æœè¯æ˜ï¼ŒKANæ˜¯ä¸€ç§ç¨³å¥ä¸”å¯é€‚åº”çš„éŸ³é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œé›†æˆåˆ°å„ç§è¶…ç½‘ç»œæ¡†æ¶çš„æ½œåŠ›ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¼•å…¥Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ä½œä¸ºéšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ¨¡å‹ï¼Œç”¨äºéŸ³é¢‘è¡¨ç¤ºã€‚</li>
<li>KANä½¿ç”¨å¯å­¦ä¹ æ¿€æ´»å‡½æ•°ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–INRæ¨¡å‹ã€‚</li>
<li>KANåœ¨éŸ³é¢‘ä¿¡å·å¤„ç†ä¸­å®ç°äº†ä½é€»è¾‘è°±è·ç¦»å’Œé«˜è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ã€‚</li>
<li>æå‡ºåŸºäºè¶…ç½‘ç»œçš„FewSoundæ¶æ„ï¼Œä¼˜åŒ–INRå‚æ•°æ›´æ–°ã€‚</li>
<li>FewSoundåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„HyperSoundï¼Œåœ¨MSEå’ŒSI-SNRæ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>KANå…·æœ‰å¯æ‰©å±•æ€§å’Œé›†æˆåˆ°å„ç§è¶…ç½‘ç»œæ¡†æ¶çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7008a04d7fe027bc1a4f9f370ce8aec3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376723&auth_key=1762376723-0-0-33c4444952301d9e0053782e4373fdf3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-093f67c54a4f10f3981cb4f707d4d0ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376730&auth_key=1762376730-0-0-ee18a1dd8c7a1864101aff89c3f2c402&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9cfc9cc939ea5cc32030caad8a0c9c77~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376737&auth_key=1762376737-0-0-4378c647d4810c6b546ff07a4ea8a101&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f4c302ace6937a3419c9fc7d7ebcbdd2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376743&auth_key=1762376743-0-0-ffb94c65f7a08b94c913bd012997228f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc03670156ad9157f7c7b76edb258300~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376750&auth_key=1762376750-0-0-3a8c1f6bbf56f4d585c45dd73cc1e5bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f8743e6aa0f21619ef8836b5262558f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376757&auth_key=1762376757-0-0-9c3b50556b2e1a8857e4639d7d9cc791&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a3ecb78d215cab5a190646afcdf26a34~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376763&auth_key=1762376763-0-0-8b280e8c4eea7373b155c46048f63c6f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de2b7690f30877216a8f4bb44614e676~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376790&auth_key=1762376790-0-0-61218054de1973cbf4635d981adab0a6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AnyEnhance-A-Unified-Generative-Model-with-Prompt-Guidance-and-Self-Critic-for-Voice-Enhancement"><a href="#AnyEnhance-A-Unified-Generative-Model-with-Prompt-Guidance-and-Self-Critic-for-Voice-Enhancement" class="headerlink" title="AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement"></a>AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement</h2><p><strong>Authors:Junan Zhang, Jing Yang, Zihao Fang, Yuancheng Wang, Zehua Zhang, Zhuo Wang, Fan Fan, Zhizheng Wu</strong></p>
<p>We introduce AnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, AnyEnhance is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. AnyEnhance introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speakerâ€™s timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate AnyEnhance outperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available at <a target="_blank" rel="noopener" href="https://amphionspace.github.io/anyenhance">https://amphionspace.github.io/anyenhance</a>. An open-source implementation is provided at <a target="_blank" rel="noopener" href="https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc">https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†AnyEnhanceï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºå¤„ç†è¯­éŸ³å’Œæ­Œå”±å£°éŸ³çš„å£°éŸ³å¢å¼ºã€‚åŸºäºæ©è”½ç”Ÿæˆæ¨¡å‹ï¼ŒAnyEnhanceèƒ½å¤ŸåŒæ—¶å¤„ç†è¯­éŸ³å’Œæ­Œå”±å£°éŸ³ï¼Œæ”¯æŒå¹¿æ³›çš„å¢å¼ºä»»åŠ¡ï¼ŒåŒ…æ‹¬å»å™ªã€å»æ··å“ã€å»å‰ªè¾‘ã€è¶…åˆ†è¾¨ç‡å’Œç›®æ ‡è¯´è¯äººæå–ï¼Œè€Œä¸”æ— éœ€å¾®è°ƒã€‚AnyEnhanceå¼•å…¥äº†ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æç¤ºæŒ‡å¯¼æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸç”Ÿåœ°æ¥å—å‚è€ƒè¯´è¯äººçš„éŸ³è‰²ã€‚è¿™æ ·ï¼Œå½“æä¾›å‚è€ƒéŸ³é¢‘æ—¶ï¼Œå®ƒå¯ä»¥æé«˜å¢å¼ºæ€§èƒ½ï¼Œå¹¶åœ¨ä¸æ”¹å˜åº•å±‚æ¶æ„çš„æƒ…å†µä¸‹å®ç°ç›®æ ‡è¯´è¯äººæå–ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼•å…¥äº†è‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶ï¼Œé€šè¿‡è¿­ä»£è‡ªæˆ‘è¯„ä¼°å’Œç»†åŒ–ï¼Œäº§ç”Ÿæ›´é«˜è´¨é‡çš„è¾“å‡ºã€‚å¯¹å„ç§å¢å¼ºä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAnyEnhanceåœ¨å®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚å¬è§‰æµ‹è¯•æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ¼”ç¤ºéŸ³é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://amphionspace.github.io/anyenhance%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82%E5%BC%80%E6%BA%90%E5%AE%9E%E7%8E%B0%E5%8F%AF%E5%9C%A8https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc%E6%89%BE%E5%88%B0%E3%80%82">https://amphionspace.github.io/anyenhanceå…¬å¼€è·å–ã€‚å¼€æºå®ç°å¯åœ¨https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatcæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15417v3">PDF</a> Accepted by IEEE TASLP 2025. Demopage:   <a target="_blank" rel="noopener" href="https://amphionspace.github.io/anyenhance">https://amphionspace.github.io/anyenhance</a>. Open-source implementation:   <a target="_blank" rel="noopener" href="https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc">https://github.com/viewfinder-annn/anyenhance-v1-ccf-aatc</a></p>
<p><strong>Summary</strong><br>è¯­éŸ³å¢å¼ºç»Ÿä¸€ç”Ÿæˆæ¨¡å‹AnyEnhanceé—®ä¸–ï¼Œæ”¯æŒè¯­éŸ³å’Œæ­Œå£°çš„å¤šç§å¢å¼ºä»»åŠ¡ï¼ŒåŒ…æ‹¬é™å™ªã€å»æ··å“ã€å»å‰ªè¾‘ã€è¶…åˆ†è¾¨ç‡å’Œç›®æ ‡è¯´è¯äººæå–ç­‰ã€‚æ¨¡å‹é‡‡ç”¨æ©ç ç”Ÿæˆæœºåˆ¶ï¼Œå…·å¤‡è‡ªé€‚åº”å­¦ä¹ èƒ½åŠ›ï¼Œå¯å€ŸåŠ©å‚è€ƒéŸ³é¢‘æå‡å¢å¼ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡è‡ªæˆ‘æ‰¹è¯„æœºåˆ¶æé«˜ç”Ÿæˆè´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒAnyEnhanceåœ¨å®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚å¬è§‰æµ‹è¯•ä¸Šå‡è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnyEnhanceæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†è¯­éŸ³å’Œæ­Œå£°çš„å¤šç§å¢å¼ºä»»åŠ¡ã€‚</li>
<li>åŸºäºæ©ç ç”Ÿæˆæ¨¡å‹ï¼Œæ— éœ€å¾®è°ƒå³å¯åŒæ—¶å¤„ç†å¤šç§ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥æç¤ºæŒ‡å¯¼æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶åœ°æ¥å—å‚è€ƒæ¼”è®²è€…çš„éŸ³è‰²ã€‚</li>
<li>å€ŸåŠ©å‚è€ƒéŸ³é¢‘æå‡å¢å¼ºæ€§èƒ½ï¼Œå®ç°ç›®æ ‡è¯´è¯äººæå–ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥è‡ªæˆ‘æ‰¹è¯„æœºåˆ¶ï¼Œé€šè¿‡è¿­ä»£è‡ªæˆ‘è¯„ä¼°å’Œç²¾ç‚¼æé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒAnyEnhanceåœ¨å®¢è§‚å’Œä¸»è§‚æµ‹è¯•ä¸Šå‡è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15417">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0c6b6134f9879bcb5aaf93f57b524c6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376798&auth_key=1762376798-0-0-fba39f7eb95701a7161fc87c030dece4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce4ceb1946b1ce3e1717ddc3863a11c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376805&auth_key=1762376805-0-0-d99b563d4a6bd36a7955ebfb2f1aa0b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8372fd40fbfbeb9ccaf1da6504bd8856~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376812&auth_key=1762376812-0-0-25da868df4cad4e4181a885fadf911df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-589e28c5365001faf45fb73f265ff456~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376819&auth_key=1762376819-0-0-db3c43d2fa0fa08a9b33665dc9ddae91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9aa9519f11d537c0162b2d5eff58903a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376825&auth_key=1762376825-0-0-5d4463fc9f624112fe434c45fb446e9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-653cbcecd11f55fe2a7f78087d311a37~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376853&auth_key=1762376853-0-0-d999c6105ae7df5d9d3d4b57e171982a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. Code has been released at <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­äºæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¾ƒå°‘å¼ºè°ƒè¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç”±äºåœ¨æ ¹æœ¬æ¨¡å¼ä¸Šçš„å·®å¼‚ï¼Œåœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­éƒ½å®ç°é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œè¯¥æ–¹æ³•è®ºé€æ­¥è®­ç»ƒLLMä»¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆä½¿æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜å®ç°äº†é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼Œæ— éœ€å•ç‹¬çš„ASRå’ŒTTSæ¨¡å—ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°åŸºå‡†æµ‹è¯•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œå¯å®ç°è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA%E3%80%82">https://github.com/VITA-MLLM/VITAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v4">PDF</a> NeurIPS 2025 Spotlight, Code 2.4K Stars:   <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œå®ç°æµç•…çš„è§†å¬äº¤äº’ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€èƒ½åŠ›ï¼Œè¿˜èƒ½å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—ï¼Œæ˜¾è‘—åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡æ–¹é¢å‡å…·å¤‡å‡ºè‰²çš„æ€§èƒ½ï¼Œå®ç°äº†è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šå¸¸èšç„¦äºè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„èåˆï¼Œä½†è¯­éŸ³åœ¨äº¤äº’ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚</li>
<li>å®ç°è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡çš„é«˜æ€§èƒ½æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºè¿™ä¸¤ç§æ¨¡æ€çš„æ ¹æœ¬å·®å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé€æ­¥è®­ç»ƒLLMç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œä»¥å®ç°æµç•…çš„è§†å¬äº¤äº’ã€‚</li>
<li>è¯¥æ–¹æ³•ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€èƒ½åŠ›ï¼Œå¹¶å®ç°äº†é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹æ— éœ€é¢å¤–çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—ï¼Œæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜è¯¥æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a862c6482be13d4067b24007a2c6afae~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376860&auth_key=1762376860-0-0-1ffb359888ac97f9bf5fb447cdbf5f1f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcced7eae39d85540050428837191e16~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376867&auth_key=1762376867-0-0-97d2f2c09054023a5c4f7613b9060549&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af64631cf671fc440e48a2e406545035~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376873&auth_key=1762376873-0-0-856b2341eb772d6dccaf0aec81be600a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed82f2ce1ffcecc27be32cc14be801b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376879&auth_key=1762376879-0-0-19814ae13dc3cdfdd5be86d032014f85&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Memristive-Nanowire-Network-for-Energy-Efficient-Audio-Classification-Pre-Processing-Free-Reservoir-Computing-with-Reduced-Latency"><a href="#Memristive-Nanowire-Network-for-Energy-Efficient-Audio-Classification-Pre-Processing-Free-Reservoir-Computing-with-Reduced-Latency" class="headerlink" title="Memristive Nanowire Network for Energy Efficient Audio Classification:   Pre-Processing-Free Reservoir Computing with Reduced Latency"></a>Memristive Nanowire Network for Energy Efficient Audio Classification:   Pre-Processing-Free Reservoir Computing with Reduced Latency</h2><p><strong>Authors:Akshaya Rajesh, Pavithra Ananthasubramanian, Nagarajan Raghavan, Ankush Kumar</strong></p>
<p>Efficient audio feature extraction is critical for low-latency, resource-constrained speech recognition. Conventional preprocessing techniques, such as Mel Spectrogram, Perceptual Linear Prediction (PLP), and Learnable Spectrogram, achieve high classification accuracy but require large feature sets and significant computation. The low-latency and power efficiency benefits of neuromorphic computing offer a strong potential for audio classification. Here, we introduce memristive nanowire networks as a neuromorphic hardware preprocessing layer for spoken-digit classification, a capability not previously demonstrated. Nanowire networks extract compact, informative features directly from raw audio, achieving a favorable trade-off between accuracy, dimensionality reduction from the original audio size (data compression) , and training time efficiency. Compared with state-of-the-art software techniques, nanowire features reach 98.95% accuracy with 66 times data compression (XGBoost) and 97.9% accuracy with 255 times compression (Random Forest) in sub-second training latency. Across multiple classifiers nanowire features consistently achieve more than 90% accuracy with more than 62.5 times compression, outperforming features extracted by conventional state-of-the-art techniques such as MFCC in efficiency without loss of performance. Moreover, nanowire features achieve 96.5% accuracy classifying multispeaker audios, outperforming all state-of-the-art feature accuracies while achieving the highest data compression and lowest training time. Nanowire network preprocessing also enhances linear separability of audio data, improving simple classifier performance and generalizing across speakers. These results demonstrate that memristive nanowire networks provide a novel, low-latency, and data-efficient feature extraction approach, enabling high-performance neuromorphic audio classification. </p>
<blockquote>
<p>æœ‰æ•ˆçš„éŸ³é¢‘ç‰¹å¾æå–å¯¹äºä½å»¶è¿Ÿã€èµ„æºå—é™çš„è¯­éŸ³è¯†åˆ«è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„é¢„å¤„ç†æŠ€æœ¯ï¼Œå¦‚æ¢…å°”é¢‘è°±å›¾ã€æ„ŸçŸ¥çº¿æ€§é¢„æµ‹ï¼ˆPLPï¼‰å’Œå­¦ä¹ é¢‘è°±å›¾ï¼Œè™½ç„¶å¯ä»¥å®ç°è¾ƒé«˜çš„åˆ†ç±»ç²¾åº¦ï¼Œä½†éœ€è¦å¤§é‡çš„ç‰¹å¾é›†å’Œå¤§é‡çš„è®¡ç®—ã€‚ç¥ç»å½¢æ€è®¡ç®—çš„ä½å»¶è¿Ÿå’ŒèŠ‚èƒ½ä¼˜åŠ¿ä¸ºéŸ³é¢‘åˆ†ç±»æä¾›äº†å¼ºå¤§çš„æ½œåŠ›ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºè¯­éŸ³æ•°å­—åˆ†ç±»çš„ç¥ç»å½¢æ€ç¡¬ä»¶é¢„å¤„ç†å±‚â€”â€”å¿†é˜»çº³ç±³çº¿ç½‘ç»œï¼Œè¿™æ˜¯ä»¥å‰æœªæ›¾å±•ç¤ºè¿‡çš„èƒ½åŠ›ã€‚çº³ç±³çº¿ç½‘ç»œç›´æ¥ä»åŸå§‹éŸ³é¢‘ä¸­æå–ç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„ç‰¹å¾ï¼Œåœ¨å®ç°ç²¾åº¦ã€ä»åŸå§‹éŸ³é¢‘å¤§å°é™ä½çš„ç»´åº¦ï¼ˆæ•°æ®å‹ç¼©ï¼‰å’Œè®­ç»ƒæ—¶é—´æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ‰åˆ©çš„å¹³è¡¡ã€‚ä¸æœ€å…ˆè¿›çš„è½¯ä»¶æŠ€æœ¯ç›¸æ¯”ï¼Œçº³ç±³çº¿ç‰¹å¾åœ¨å­ç§’çº§è®­ç»ƒå»¶è¿Ÿä¸‹å®ç°äº†98.95%çš„å‡†ç¡®ç‡ï¼Œæ•°æ®å‹ç¼©æ¯”ä¸º66å€ï¼ˆXGBoostï¼‰ï¼Œä»¥åŠ97.9%çš„å‡†ç¡®ç‡ï¼Œå‹ç¼©æ¯”ä¸º255å€ï¼ˆéšæœºæ£®æ—ï¼‰ã€‚åœ¨å¤šä¸ªåˆ†ç±»å™¨ä¸­ï¼Œçº³ç±³çº¿ç‰¹å¾çš„å‹ç¼©æ¯”è¶…è¿‡62.5å€æ—¶ï¼Œå‡†ç¡®ç‡å§‹ç»ˆè¶…è¿‡90%ï¼Œåœ¨æ•ˆç‡ä¸Šä¼˜äºMFCCç­‰ä¼ ç»Ÿå…ˆè¿›æŠ€æœ¯æå–çš„ç‰¹å¾ï¼ŒåŒæ—¶ä¸æŸå¤±æ€§èƒ½ã€‚æ­¤å¤–ï¼Œçº³ç±³çº¿ç‰¹å¾åœ¨åˆ†ç±»å¤šè¯´è¯è€…éŸ³é¢‘æ—¶è¾¾åˆ°äº†96.5%çš„å‡†ç¡®ç‡ï¼Œåœ¨æ•°æ®å‹ç¼©æ¯”æœ€é«˜ã€è®­ç»ƒæ—¶é—´æœ€çŸ­çš„åŒæ—¶ï¼Œä¼˜äºæ‰€æœ‰å…ˆè¿›ç‰¹å¾å‡†ç¡®ç‡ã€‚çº³ç±³çº¿ç½‘ç»œé¢„å¤„ç†è¿˜æé«˜äº†éŸ³é¢‘æ•°æ®çš„çº¿æ€§å¯åˆ†æ€§ï¼Œæé«˜äº†ç®€å•åˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹¶èƒ½åœ¨ä¸åŒè¯´è¯è€…ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¿†é˜»çº³ç±³çº¿ç½‘ç»œæä¾›äº†ä¸€ç§æ–°é¢–ã€ä½å»¶è¿Ÿå’Œæ•°æ®é«˜æ•ˆçš„ç‰¹å¾æå–æ–¹æ³•ï¼Œå¯å®ç°é«˜æ€§èƒ½çš„ç¥ç»å½¢æ€éŸ³é¢‘åˆ†ç±»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19611v2">PDF</a> 14 pages, 5 Figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨çº³ç±³çº¿ç½‘ç»œä½œä¸ºç¥ç»å½¢æ€ç¡¬ä»¶é¢„å¤„ç†å±‚è¿›è¡Œè¯­éŸ³æ•°å­—åˆ†ç±»çš„æŠ€æœ¯ã€‚çº³ç±³çº¿ç½‘ç»œèƒ½å¤Ÿä»åŸå§‹éŸ³é¢‘ä¸­ç›´æ¥æå–ç´§å‡‘ä¸”å¯Œå«ä¿¡æ¯ç‰¹å¾ï¼Œå®ç°äº†åœ¨å‡†ç¡®æ€§ã€éŸ³é¢‘æ•°æ®å‹ç¼©å’Œè®­ç»ƒæ—¶é—´æ•ˆç‡ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚ç›¸è¾ƒäºä¼ ç»Ÿè½¯ä»¶æŠ€æœ¯ï¼Œçº³ç±³çº¿ç‰¹å¾åœ¨åˆ†ç±»ç²¾åº¦ã€æ•°æ®å‹ç¼©å’Œè®­ç»ƒæ—¶é—´ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œçº³ç±³çº¿ç½‘ç»œä¸ºä½å»¶è¿Ÿå’Œæ•°æ®é«˜æ•ˆéŸ³é¢‘åˆ†ç±»æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘ç‰¹å¾æå–å¯¹äºä½å»¶è¿Ÿã€èµ„æºå—é™çš„è¯­éŸ³è¯†åˆ«è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿé¢„å¤„ç†æŠ€æœ¯è™½ç„¶åˆ†ç±»å‡†ç¡®ç‡é«˜ï¼Œä½†éœ€è¦å¤§é‡çš„ç‰¹å¾é›†å’Œè®¡ç®—èµ„æºã€‚</li>
<li>ç¥ç»å½¢æ€è®¡ç®—å¯æä¾›ä½å»¶è¿Ÿå’Œé«˜æ•ˆèƒ½ä¼˜åŠ¿ï¼Œç”¨äºéŸ³é¢‘åˆ†ç±»ã€‚</li>
<li>çº³ç±³çº¿ç½‘ç»œä½œä¸ºç¥ç»å½¢æ€ç¡¬ä»¶é¢„å¤„ç†å±‚ï¼Œå¯ç›´æ¥ä»åŸå§‹éŸ³é¢‘ä¸­æå–ç‰¹å¾ã€‚</li>
<li>çº³ç±³çº¿ç‰¹å¾åœ¨æ•°æ®å‹ç¼©å’Œè®­ç»ƒæ—¶é—´æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œç›¸è¾ƒäºæœ€æ–°è½¯ä»¶æŠ€æœ¯ï¼Œè¾¾åˆ°é«˜åˆ†ç±»ç²¾åº¦ã€‚</li>
<li>çº³ç±³çº¿ç‰¹å¾åœ¨å¤šåˆ†ç±»å™¨ä¸­çš„è¡¨ç°ç¨³å®šï¼Œä¸”åœ¨å¤šè¯´è¯è€…éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a91969353ea22332dbe87ff79b80bd4b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376888&auth_key=1762376888-0-0-8f92ad1c382668f4add6fa1e17ececd9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ea064bc31df631ce34eafa7de3336aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376896&auth_key=1762376896-0-0-a8af799bd140c56b59874f5a320b2961&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f20803e59d4bb468e9480be34d464dd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762376903&auth_key=1762376903-0-0-46e899114467df888d4c729e83222a82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-06/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-45cf9f873f52513770175215513c7a0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762377371&auth_key=1762377371-0-0-5544255a171a68c610254b4b728c1ff2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  PercHead Perceptual Head Model for Single-Image 3D Head Reconstruction   & Editing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-06/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-81375a39c01f7d469b04d5d40a10a611~resize:0:q75.jpg?source=1f5c5e47&expiration=1762375412&auth_key=1762375412-0-0-bfca0535e61b4b6d64acb288a8059a68&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-06  DIsoN Decentralized Isolation Networks for Out-of-Distribution   Detection in Medical Imaging
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
