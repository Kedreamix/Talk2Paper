<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  SEKE Specialised Experts for Keyword Extraction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-25becc6ca17512c6389bde9859232d21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-19-æ›´æ–°"><a href="#2024-12-19-æ›´æ–°" class="headerlink" title="2024-12-19 æ›´æ–°"></a>2024-12-19 æ›´æ–°</h1><h2 id="SEKE-Specialised-Experts-for-Keyword-Extraction"><a href="#SEKE-Specialised-Experts-for-Keyword-Extraction" class="headerlink" title="SEKE: Specialised Experts for Keyword Extraction"></a>SEKE: Specialised Experts for Keyword Extraction</h2><p><strong>Authors:Matej Martinc, Hanh Thi Hong Tran, Senja Pollak, Boshko Koloski</strong></p>
<p>Keyword extraction involves identifying the most descriptive words in a document, allowing automatic categorisation and summarisation of large quantities of diverse textual data. Relying on the insight that real-world keyword detection often requires handling of diverse content, we propose a novel supervised keyword extraction approach based on the mixture of experts (MoE) technique. MoE uses a learnable routing sub-network to direct information to specialised experts, allowing them to specialize in distinct regions of the input space. SEKE, a mixture of Specialised Experts for supervised Keyword Extraction, uses DeBERTa as the backbone model and builds on the MoE framework, where experts attend to each token, by integrating it with a recurrent neural network (RNN), to allow successful extraction even on smaller corpora, where specialisation is harder due to lack of training data. The MoE framework also provides an insight into inner workings of individual experts, enhancing the explainability of the approach. We benchmark SEKE on multiple English datasets, achieving state-of-the-art performance compared to strong supervised and unsupervised baselines. Our analysis reveals that depending on data size and type, experts specialize in distinct syntactic and semantic components, such as punctuation, stopwords, parts-of-speech, or named entities. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/matejMartinc/SEKE_keyword_extraction">https://github.com/matejMartinc/SEKE_keyword_extraction</a> </p>
<blockquote>
<p>å…³é”®è¯æå–æ¶‰åŠè¯†åˆ«æ–‡æ¡£ä¸­æœ€å…·æè¿°æ€§çš„å•è¯ï¼Œä»è€Œå®ç°å¤§é‡ä¸åŒæ–‡æœ¬æ•°æ®çš„è‡ªåŠ¨åˆ†ç±»å’Œæ‘˜è¦ã€‚æˆ‘ä»¬ä¾èµ–äºä¸€ç§è§è§£ï¼Œå³ç°å®ä¸–ç•Œä¸­çš„å…³é”®è¯æ£€æµ‹é€šå¸¸éœ€è¦å¤„ç†å¤šæ ·åŒ–çš„å†…å®¹ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰æŠ€æœ¯çš„æ–°å‹ç›‘ç£å…³é”®è¯æå–æ–¹æ³•ã€‚MoEä½¿ç”¨ä¸€ä¸ªå¯å­¦ä¹ çš„è·¯ç”±å­ç½‘ç»œæ¥æŒ‡å¯¼ä¿¡æ¯æµå‘ä¸“ä¸šä¸“å®¶ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿä¸“æ³¨äºè¾“å…¥ç©ºé—´çš„ç‰¹å®šåŒºåŸŸã€‚SEKEï¼ˆç”¨äºç›‘ç£å…³é”®è¯æå–çš„ä¸“ç”¨ä¸“å®¶æ··åˆï¼‰ä½¿ç”¨DeBERTaä½œä¸ºéª¨å¹²æ¨¡å‹ï¼ŒåŸºäºMoEæ¡†æ¶æ„å»ºï¼Œå…¶ä¸­çš„ä¸“å®¶ä¼šå…³æ³¨æ¯ä¸ªæ ‡è®°ï¼Œå¹¶é€šè¿‡å°†å…¶ä¸å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰é›†æˆï¼Œå³ä½¿åœ¨è¾ƒå°çš„è¯­æ–™åº“ä¸Šä¹Ÿèƒ½å®ç°æˆåŠŸçš„æå–ã€‚ç”±äºç¼ºä¹è®­ç»ƒæ•°æ®ï¼Œä½¿å¾—ä¸“ä¸šåŒ–å˜å¾—æ›´åŠ å›°éš¾ã€‚MoEæ¡†æ¶è¿˜æä¾›äº†å¯¹ä¸ªåˆ«ä¸“å®¶å†…éƒ¨å·¥ä½œçš„æ·±å…¥äº†è§£ï¼Œå¢å¼ºäº†è¯¥æ–¹æ³•çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè‹±æ–‡æ•°æ®é›†ä¸Šå¯¹SEKEè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œä¸å¼ºå¤§çš„æœ‰ç›‘ç£å’Œæ— ç›‘ç£åŸºå‡†ç›¸æ¯”ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ ¹æ®æ•°æ®çš„å¤§å°å’Œç±»å‹ï¼Œä¸“å®¶ä¼šåœ¨ä¸åŒçš„å¥æ³•å’Œè¯­ä¹‰æˆåˆ†ä¸Šä¸“ä¸šåŒ–ï¼Œå¦‚æ ‡ç‚¹ç¬¦å·ã€åœç”¨è¯ã€çŸ­è¯­æˆåˆ†æˆ–å‘½åå®ä½“ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/matejMartinc/SEKE_keyword_extraction">https://github.com/matejMartinc/SEKE_keyword_extraction</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14087v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå…³é”®è¯æå–çš„é‡è¦æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ€æœ¯çš„å…³é”®è¯æå–æ–¹æ³•ã€‚ç»“åˆå¤šç§æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä½¿ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•è¯†åˆ«æè¿°æ€§è¯æ±‡ä»¥è¿›è¡Œå¤§è§„æ¨¡æ–‡æœ¬çš„è‡ªåŠ¨åˆ†ç±»å’Œæ‘˜è¦ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªè‹±è¯­æ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ï¼Œä¸”ä¸“å®¶æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹ä¸åŒç±»å‹çš„æ–‡æœ¬è¿›è¡Œä¸“ä¸šå¤„ç†ï¼Œæé«˜äº†è§£é‡Šæ€§ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å…³é”®è¯æå–æ˜¯é€šè¿‡è¯†åˆ«æ–‡æ¡£ä¸­æœ€å…·æè¿°æ€§çš„è¯æ±‡ï¼Œå®ç°å¯¹å¤§è§„æ¨¡å¤šæ ·æ–‡æœ¬æ•°æ®çš„è‡ªåŠ¨åˆ†ç±»å’Œæ‘˜è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ€æœ¯çš„å…³é”®è¯æå–æ–¹æ³•ï¼Œç»“åˆå¤šç§æ¨¡å‹ä¼˜åŠ¿è¿›è¡Œæ›´å‡†ç¡®çš„åˆ†æã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db4288f8086a130a67cce77dc911172c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20158001ff17639281cba8e690dc1858.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7fd82acac1f6dba7360a642f8f98723.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Investigating-the-Effects-of-Diffusion-based-Conditional-Generative-Speech-Models-Used-for-Speech-Enhancement-on-Dysarthric-Speech"><a href="#Investigating-the-Effects-of-Diffusion-based-Conditional-Generative-Speech-Models-Used-for-Speech-Enhancement-on-Dysarthric-Speech" class="headerlink" title="Investigating the Effects of Diffusion-based Conditional Generative   Speech Models Used for Speech Enhancement on Dysarthric Speech"></a>Investigating the Effects of Diffusion-based Conditional Generative   Speech Models Used for Speech Enhancement on Dysarthric Speech</h2><p><strong>Authors:Joanna Reszka, Parvaneh Janbakhshi, Tilak Purohit, Sadegh Mohammadi</strong></p>
<p>In this study, we aim to explore the effect of pre-trained conditional generative speech models for the first time on dysarthric speech due to Parkinsonâ€™s disease recorded in an ideal&#x2F;non-noisy condition. Considering one category of generative models, i.e., diffusion-based speech enhancement, these models are previously trained to learn the distribution of clean (i.e, recorded in a noise-free environment) typical speech signals. Therefore, we hypothesized that when being exposed to dysarthric speech they might remove the unseen atypical paralinguistic cues during the enhancement process. By considering the automatic dysarthric speech detection task, in this study, we experimentally show that during the enhancement process of dysarthric speech data recorded in an ideal non-noisy environment, some of the acoustic dysarthric speech cues are lost. Therefore such pre-trained models are not yet suitable in the context of dysarthric speech enhancement since they manipulate the pathological speech cues when they process clean dysarthric speech. Furthermore, we show that the removed acoustics cues by the enhancement models in the form of residue speech signal can provide complementary dysarthric cues when fused with the original input speech signal in the feature space. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†é¢„è®­ç»ƒçš„æ¡ä»¶ç”Ÿæˆè¯­éŸ³æ¨¡å‹å¯¹å¸•é‡‘æ£®ç—…å¼•èµ·çš„æ„éŸ³éšœç¢è¯­éŸ³çš„å½±å“ï¼Œè¿™äº›è¯­éŸ³æ˜¯åœ¨ç†æƒ³&#x2F;æ— å™ªéŸ³çš„æ¡ä»¶ä¸‹å½•åˆ¶çš„ã€‚è€ƒè™‘åˆ°ä¸€ç±»ç”Ÿæˆæ¨¡å‹ï¼Œå³åŸºäºæ‰©æ•£çš„è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä¹‹å‰æ¥å—è¿‡è®­ç»ƒï¼Œå­¦ä¹ æ¸…æ´ï¼ˆå³åœ¨æ— å™ªéŸ³ç¯å¢ƒä¸­å½•åˆ¶çš„ï¼‰å…¸å‹è¯­éŸ³ä¿¡å·çš„åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å‡è®¾å½“æš´éœ²äºæ„éŸ³éšœç¢è¯­éŸ³æ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šåœ¨å¢å¼ºè¿‡ç¨‹ä¸­æ¶ˆé™¤æœªè§è¿‡çš„éè¯­è¨€æç¤ºã€‚é€šè¿‡è€ƒè™‘è‡ªåŠ¨æ„éŸ³éšœç¢è¯­éŸ³æ£€æµ‹ä»»åŠ¡ï¼Œæœ¬ç ”ç©¶å®éªŒè¡¨æ˜ï¼Œåœ¨ç†æƒ³æ— å™ªéŸ³ç¯å¢ƒä¸‹å½•åˆ¶çš„æ„éŸ³éšœç¢è¯­éŸ³æ•°æ®çš„å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œä¸€äº›å£°å­¦æ„éŸ³éšœç¢è¯­éŸ³çº¿ç´¢ä¼šä¸¢å¤±ã€‚å› æ­¤ï¼Œè¿™æ ·çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨æ„éŸ³éšœç¢è¯­éŸ³å¢å¼ºæ–¹é¢å°šä¸é€‚ç”¨ï¼Œå› ä¸ºå®ƒä»¬åœ¨å¤„ç†å¹²å‡€çš„æ„éŸ³éšœç¢è¯­éŸ³æ—¶ä¼šæ“ä½œç—…ç†è¯­éŸ³çº¿ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå¢å¼ºæ¨¡å‹ä»¥æ®‹ç•™è¯­éŸ³ä¿¡å·çš„å½¢å¼å»é™¤çš„å£°å­¦çº¿ç´¢ï¼Œå½“ä¸åŸå§‹è¾“å…¥è¯­éŸ³ä¿¡å·èåˆåœ¨ç‰¹å¾ç©ºé—´æ—¶ï¼Œå¯ä»¥æä¾›è¡¥å……çš„æ„éŸ³éšœç¢çº¿ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13933v1">PDF</a> Accepted at ICASSP 2025 Satellite Workshop: Workshop on Speech   Pathology Analysis and DEtection (SPADE)</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢åœ¨éå™ªå£°ç¯å¢ƒä¸‹é¢„è®­ç»ƒçš„åŸºäºæ¡ä»¶çš„ç”Ÿæˆè¯­éŸ³æ¨¡å‹å¯¹å¸•é‡‘æ£®æ°ç—…å¼•èµ·çš„è¨€è¯­éšœç¢ï¼ˆåˆç§°è¨€è¯­å›°éš¾ç—‡ï¼‰çš„å½±å“ã€‚å…ˆå‰å·²è®­ç»ƒè¿‡è¿™äº›æ¨¡å‹ä»¥å­¦ä¹ å¹²å‡€ï¼ˆå³åœ¨æ— å™ªå£°ç¯å¢ƒä¸­å½•åˆ¶ï¼‰çš„å…¸å‹è¯­éŸ³ä¿¡å·çš„åˆ†å¸ƒã€‚æˆ‘ä»¬å‡è®¾ï¼Œå½“è¿™äº›æ¨¡å‹æ¥è§¦åˆ°è¨€è¯­éšœç¢è¯­éŸ³æ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šåœ¨å¢å¼ºè¿‡ç¨‹ä¸­æ¶ˆé™¤æœªè§‚å¯Ÿåˆ°çš„å¼‚å¸¸å‰¯è¯­è¨€çº¿ç´¢ã€‚æœ¬ç ”ç©¶é€šè¿‡å®éªŒè¡¨æ˜ï¼Œåœ¨ç†æƒ³éå™ªå£°ç¯å¢ƒä¸‹å½•åˆ¶çš„è¨€è¯­éšœç¢è¯­éŸ³æ•°æ®çš„å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œéƒ¨åˆ†å£°å­¦éšœç¢çº¿ç´¢ä¼šä¸¢å¤±ã€‚å› æ­¤ï¼Œè¿™ç§é¢„è®­ç»ƒçš„æ¨¡å‹å°šä¸é€‚ç”¨äºå¢å¼ºè¨€è¯­éšœç¢è¯­éŸ³çš„èƒŒæ™¯ï¼Œå› ä¸ºå®ƒä»¬åœ¨å¤„ç†å¹²å‡€çš„è¨€è¯­éšœç¢è¯­éŸ³æ—¶ä¼šå¹²æ‰°ç—…ç†è¯­éŸ³çº¿ç´¢ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¿˜å‘ç°ï¼Œå¢å¼ºæ¨¡å‹ä»¥æ®‹ç•™è¯­éŸ³ä¿¡å·çš„å½¢å¼å»é™¤çš„å£°å­¦çº¿ç´¢å¯ä»¥ä¸åŸå§‹è¾“å…¥è¯­éŸ³ä¿¡å·èåˆï¼Œä¸ºè¯†åˆ«è¨€è¯­éšœç¢æä¾›é¢å¤–çš„çº¿ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†é¢„è®­ç»ƒçš„åŸºäºæ¡ä»¶çš„ç”Ÿæˆè¯­éŸ³æ¨¡å‹å¯¹å¸•é‡‘æ£®æ°ç—…å¯¼è‡´çš„è¨€è¯­éšœç¢è¯­éŸ³çš„å½±å“ã€‚</li>
<li>è¿™äº›é¢„è®­ç»ƒæ¨¡å‹æ—¨åœ¨å­¦ä¹ å¹²å‡€ï¼ˆæ— å™ªå£°ç¯å¢ƒä¸­å½•åˆ¶çš„ï¼‰å…¸å‹è¯­éŸ³ä¿¡å·çš„åˆ†å¸ƒã€‚</li>
<li>åœ¨éå™ªå£°ç¯å¢ƒä¸‹å½•åˆ¶çš„è¨€è¯­éšœç¢è¯­éŸ³æ•°æ®çš„å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œè¿™äº›æ¨¡å‹ä¼šæ¶ˆé™¤ä¸€äº›å£°å­¦éšœç¢çº¿ç´¢ã€‚</li>
<li>é¢„è®­ç»ƒçš„æ¨¡å‹å°šä¸é€‚ç”¨äºå¢å¼ºè¨€è¯­éšœç¢è¯­éŸ³ï¼Œå› ä¸ºå®ƒä»¬ä¼šå¹²æ‰°ç—…ç†è¯­éŸ³çº¿ç´¢ã€‚</li>
<li>å¢å¼ºæ¨¡å‹å»é™¤çš„å£°å­¦çº¿ç´¢ï¼ˆä»¥æ®‹ç•™è¯­éŸ³ä¿¡å·çš„å½¢å¼ï¼‰ä¸åŸå§‹è¾“å…¥è¯­éŸ³ä¿¡å·èåˆåï¼Œå¯ä¸ºè¯†åˆ«è¨€è¯­éšœç¢æä¾›é¢å¤–çš„ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›é¢„è®­ç»ƒæ¨¡å‹éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´å’Œä¼˜åŒ–ï¼Œä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†è¨€è¯­éšœç¢è¯­éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81e3af4599c3760beb7f5f68b79cf773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6837a733efbcdae2193c381d661d904d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4000d53c43eb4d257fdd847df8e8772f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ba364240e9e43e8f96985f1bb24c496.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Typhoon-2-A-Family-of-Open-Text-and-Multimodal-Thai-Large-Language-Models"><a href="#Typhoon-2-A-Family-of-Open-Text-and-Multimodal-Thai-Large-Language-Models" class="headerlink" title="Typhoon 2: A Family of Open Text and Multimodal Thai Large Language   Models"></a>Typhoon 2: A Family of Open Text and Multimodal Thai Large Language   Models</h2><p><strong>Authors:Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai</strong></p>
<p>This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ various post-training techniques to enhance Thai language performance while preserving the base modelsâ€™ original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs simultaneously. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†å°é£2ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é’ˆå¯¹æ³°è¯­ä¼˜åŒ–çš„æ–‡æœ¬å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘æ¨¡å‹ã€‚Typhoon2-Textå»ºç«‹åœ¨æœ€å‰æ²¿çš„å¼€æ”¾æ¨¡å‹ä¸Šï¼Œå¦‚Llama 3å’ŒQwen2ï¼Œæˆ‘ä»¬å¯¹è‹±è¯­å’Œæ³°è¯­æ•°æ®çš„æ··åˆè¿›è¡ŒæŒç»­é¢„è®­ç»ƒã€‚æˆ‘ä»¬é‡‡ç”¨å„ç§åè®­ç»ƒæŠ€æœ¯ï¼Œåœ¨æé«˜æ³°è¯­æ€§èƒ½çš„åŒæ—¶ï¼Œä¿ç•™åŸºç¡€æ¨¡å‹çš„åŸå§‹åŠŸèƒ½ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ç³»åˆ—æ–‡æœ¬æ¨¡å‹ï¼Œå‚æ•°ä»1äº¿åˆ°70äº¿ä¸ç­‰ï¼Œæ—¢æœ‰åŸºç¡€æ¨¡å‹ä¹Ÿæœ‰æŒ‡ä»¤è°ƒä¼˜çš„å˜ä½“ã€‚Typhoon2-Visionåœ¨ä¿ç•™é€šç”¨è§†è§‰åŠŸèƒ½ï¼ˆå¦‚å›¾åƒæ ‡é¢˜ï¼‰çš„åŒæ—¶ï¼Œæé«˜äº†å¯¹æ³°è¯­æ–‡æ¡£çš„ç†è§£èƒ½åŠ›ã€‚Typhoon2-Audioå¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³çš„æ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†éŸ³é¢‘ã€è¯­éŸ³å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶åŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13702v1">PDF</a> technical report, 55 pages</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†Typhoon 2ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥ç³»åˆ—æ¨¡å‹é’ˆå¯¹æ³°è¯­è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘æ¨¡å‹ã€‚Typhoon2-TextåŸºäºæœ€å‰æ²¿çš„å¼€æ”¾æ¨¡å‹è¿›è¡Œæ„å»ºï¼Œå¦‚Llama 3å’ŒQwen2ï¼Œå¹¶åœ¨æ··åˆçš„è‹±è¯­å’Œæ³°è¯­æ•°æ®ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒã€‚é€šè¿‡é‡‡ç”¨å„ç§åè®­ç»ƒæŠ€æœ¯ï¼Œæé«˜äº†æ³°è¯­æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†åŸºç¡€æ¨¡å‹çš„åŸå§‹èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†ä¸åŒè§„æ¨¡çš„æ–‡æœ¬æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ã€‚Typhoon2-Visionæé«˜äº†å¯¹æ³°è¯­æ–‡æ¡£çš„ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†é€šç”¨çš„è§†è§‰åŠŸèƒ½ï¼Œå¦‚å›¾åƒæ ‡é¢˜ç”Ÿæˆã€‚Typhoon2-Audioåˆ™å¼•å…¥äº†ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†éŸ³é¢‘ã€è¯­éŸ³å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶åŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Typhoon 2ç³»åˆ—æ¨¡å‹æ˜¯é’ˆå¯¹æ³°è¯­ä¼˜åŒ–çš„æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>Typhoon2-TextåŸºäºLlama 3å’ŒQwen2æ„å»ºï¼Œå¹¶åœ¨æ··åˆè¯­è¨€æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>é€šè¿‡åè®­ç»ƒæŠ€æœ¯æé«˜æ³°è¯­æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„åŸå§‹èƒ½åŠ›ã€‚</li>
<li>å‘å¸ƒä¸åŒè§„æ¨¡çš„æ–‡æœ¬æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ã€‚</li>
<li>Typhoon2-Visionæé«˜äº†å¯¹æ³°è¯­æ–‡æ¡£çš„ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™é€šç”¨çš„è§†è§‰åŠŸèƒ½ã€‚</li>
<li>Typhoon2-Audioå…·å¤‡ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤„ç†å¤šç§è¾“å…¥å¹¶ç”Ÿæˆå¤šç§è¾“å‡ºã€‚</li>
<li>æ¨¡å‹å…·å¤‡å¼ºå¤§çš„å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºå¤šç§åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bc776d60c3f5267b207b3f0dcadc1ef.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Efficient-Speech-Command-Recognition-Leveraging-Spiking-Neural-Network-and-Curriculum-Learning-based-Knowledge-Distillation"><a href="#Efficient-Speech-Command-Recognition-Leveraging-Spiking-Neural-Network-and-Curriculum-Learning-based-Knowledge-Distillation" class="headerlink" title="Efficient Speech Command Recognition Leveraging Spiking Neural Network   and Curriculum Learning-based Knowledge Distillation"></a>Efficient Speech Command Recognition Leveraging Spiking Neural Network   and Curriculum Learning-based Knowledge Distillation</h2><p><strong>Authors:Jiaqi Wang, Liutao Yu, Liwei Huang, Chenlin Zhou, Han Zhang, Zhenxi Song, Min Zhang, Zhengyu Ma, Zhiguo Zhang</strong></p>
<p>The intrinsic dynamics and event-driven nature of spiking neural networks (SNNs) make them excel in processing temporal information by naturally utilizing embedded time sequences as time steps. Recent studies adopting this approach have demonstrated SNNsâ€™ effectiveness in speech command recognition, achieving high performance by employing large time steps for long time sequences. However, the large time steps lead to increased deployment burdens for edge computing applications. Thus, it is important to balance high performance and low energy consumption when detecting temporal patterns in edge devices. Our solution comprises two key components. 1). We propose a high-performance fully spike-driven framework termed SpikeSCR, characterized by a global-local hybrid structure for efficient representation learning, which exhibits long-term learning capabilities with extended time steps. 2). To further fully embrace low energy consumption, we propose an effective knowledge distillation method based on curriculum learning (KDCL), where valuable representations learned from the easy curriculum are progressively transferred to the hard curriculum with minor loss, striking a trade-off between power efficiency and high performance. We evaluate our method on three benchmark datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC), and the Google Speech Commands (GSC) V2. Our experimental results demonstrate that SpikeSCR outperforms current state-of-the-art (SOTA) methods across these three datasets with the same time steps. Furthermore, by executing KDCL, we reduce the number of time steps by 60% and decrease energy consumption by 54.8% while maintaining comparable performance to recent SOTA results. Therefore, this work offers valuable insights for tackling temporal processing challenges with long time sequences in edge neuromorphic computing systems. </p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„å†…åœ¨åŠ¨æ€å’Œäº‹ä»¶é©±åŠ¨ç‰¹æ€§ä½¿å®ƒä»¬èƒ½å¤Ÿè‡ªç„¶åœ°åˆ©ç”¨åµŒå…¥çš„æ—¶é—´åºåˆ—ä½œä¸ºæ—¶é—´æ­¥é•¿ï¼Œä»è€Œæ“…é•¿å¤„ç†æ—¶åºä¿¡æ¯ã€‚æœ€è¿‘é‡‡ç”¨è¿™ç§æ–¹æ³•çš„ç ”ç©¶å·²ç»è¯æ˜äº†SNNsåœ¨è¯­éŸ³å‘½ä»¤è¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡é‡‡ç”¨å¤§çš„æ—¶é—´æ­¥é•¿æ¥å¤„ç†é•¿æ—¶é—´åºåˆ—ï¼Œå®ç°äº†é«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§æ­¥é•¿ç»™è¾¹ç¼˜è®¡ç®—åº”ç”¨çš„éƒ¨ç½²å¸¦æ¥äº†æ›´å¤§çš„è´Ÿæ‹…ã€‚å› æ­¤ï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸­æ£€æµ‹æ—¶åºæ¨¡å¼æ—¶ï¼Œå¹³è¡¡é«˜æ€§èƒ½å’Œä½èƒ½è€—æ˜¯è‡³å…³é‡è¦çš„ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ã€‚1. æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ€§èƒ½çš„å…¨è„‰å†²é©±åŠ¨æ¡†æ¶ï¼Œåä¸ºSpikeSCRï¼Œå…¶ç‰¹ç‚¹æ˜¯å…·æœ‰å…¨å±€-å±€éƒ¨æ··åˆç»“æ„ï¼Œç”¨äºæœ‰æ•ˆè¡¨ç¤ºå­¦ä¹ ï¼Œå…·æœ‰é•¿æœŸå­¦ä¹ èƒ½åŠ›å¹¶æ‰©å±•æ—¶é—´æ­¥é•¿ã€‚2. ä¸ºäº†è¿›ä¸€æ­¥æ‹¥æŠ±ä½èƒ½è€—ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„æœ‰æ•ˆçŸ¥è¯†è’¸é¦æ–¹æ³•ï¼ˆKDCLï¼‰ï¼Œä»ç®€å•çš„è¯¾ç¨‹ä¸­å­¦ä¹ åˆ°çš„æœ‰ä»·å€¼è¡¨ç¤ºé€æ­¥è½¬ç§»åˆ°å›°éš¾çš„è¯¾ç¨‹ä¸­ï¼Œå‡ ä¹æ²¡æœ‰æŸå¤±ï¼Œåœ¨åŠŸç‡æ•ˆç‡ä¸é«˜æ€§èƒ½ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šæ–¯æ´¾å…‹æµ·å¾·å ¡æ•°æ®é›†ï¼ˆSHDï¼‰ã€æ–¯æ´¾å…‹è¯­éŸ³å‘½ä»¤ï¼ˆSSCï¼‰å’Œè°·æ­Œè¯­éŸ³å‘½ä»¤ï¼ˆGSCï¼‰V2ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSpikeSCRåœ¨è¿™ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰§è¡ŒKDCLï¼Œæˆ‘ä»¬å°†æ—¶é—´æ­¥é•¿å‡å°‘äº†60%ï¼Œèƒ½é‡æ¶ˆè€—é™ä½äº†54.8%ï¼ŒåŒæ—¶ä¿æŒä¸æœ€è¿‘çš„æœ€å…ˆè¿›ç»“æœç›¸å½“çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œä¸ºè§£å†³è¾¹ç¼˜ç¥ç»å½¢æ€è®¡ç®—ç³»ç»Ÿä¸­é•¿æ—¶é—´åºåˆ—çš„æ—¶åºå¤„ç†æŒ‘æˆ˜æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12858v1">PDF</a> Under Review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„å†…åœ¨åŠ¨æ€å’Œäº‹ä»¶é©±åŠ¨ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿåˆ©ç”¨åµŒå…¥çš„æ—¶é—´åºåˆ—ä½œä¸ºæ—¶é—´æ­¥é•¿ï¼Œåœ¨æ—¶åºä¿¡æ¯å¤„ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚è¿‘æœŸé‡‡ç”¨æ­¤æ–¹æ³•çš„ç ”ç©¶åœ¨è¯­éŸ³å‘½ä»¤è¯†åˆ«ä¸­å±•ç¤ºäº†SNNsçš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ä½¿ç”¨å¤§çš„æ—¶é—´æ­¥é•¿æ¥å¤„ç†é•¿æ—¶é—´åºåˆ—ï¼Œå®ç°äº†é«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§æ—¶é—´æ­¥é•¿ç»™è¾¹ç¼˜è®¡ç®—åº”ç”¨å¸¦æ¥äº†éƒ¨ç½²è´Ÿæ‹…ã€‚å› æ­¤ï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæ£€æµ‹æ—¶åºæ¨¡å¼æ—¶ï¼Œéœ€è¦å¹³è¡¡é«˜æ€§èƒ½ä¸ä½èƒ½è€—ã€‚æœ¬ç ”ç©¶è§£å†³æ–¹æ¡ˆåŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ€§èƒ½çš„å…¨è„‰å†²é©±åŠ¨æ¡†æ¶SpikeSCRï¼Œå…·æœ‰å…¨å±€-å±€éƒ¨æ··åˆç»“æ„ï¼Œç”¨äºé«˜æ•ˆè¡¨ç¤ºå­¦ä¹ ï¼Œå…·æœ‰é•¿æ—¶é—´æ­¥é•¿çš„é•¿æœŸå­¦ä¹ èƒ½åŠ›ã€‚2ï¼‰ä¸ºäº†è¿›ä¸€æ­¥å®ç°ä½èƒ½è€—ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„æœ‰æ•ˆçŸ¥è¯†è’¸é¦æ–¹æ³•ï¼ˆKDCLï¼‰ï¼Œä»ç®€å•è¯¾ç¨‹ä¸­å­¦ä¹ çš„æœ‰ä»·å€¼è¡¨ç¤ºé€æ­¥è½¬ç§»åˆ°å›°éš¾è¯¾ç¨‹ï¼Œå®ç°äº†åŠŸç‡æ•ˆç‡ä¸é«˜æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šSpiking Heidelberg Datasetï¼ˆSHDï¼‰ã€Spiking Speech Commandsï¼ˆSSCï¼‰å’ŒGoogle Speech Commandsï¼ˆGSCï¼‰V2ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpikeSCRåœ¨è¿™ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰§è¡ŒKDCLï¼Œæˆ‘ä»¬å°†æ—¶é—´æ­¥é•¿å‡å°‘äº†60%ï¼Œå¹¶é™ä½äº†54.8%çš„èƒ½è€—ï¼ŒåŒæ—¶ä¿æŒä¸æœ€æ–°å…ˆè¿›ç»“æœçš„ç›¸å½“æ€§èƒ½ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œä¸ºè§£å†³è¾¹ç¼˜ç¥ç»å½¢æ€è®¡ç®—ç³»ç»Ÿä¸­é•¿æ—¶é—´åºåˆ—çš„æ—¶åºå¤„ç†æŒ‘æˆ˜æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰èƒ½åˆ©ç”¨æ—¶é—´åºåˆ—è¿›è¡Œé«˜æ•ˆæ—¶åºä¿¡æ¯å¤„ç†ã€‚</li>
<li>SNNsåœ¨è¯­éŸ³å‘½ä»¤è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¤§æ—¶é—´æ­¥é•¿å¢åŠ äº†è¾¹ç¼˜è®¡ç®—çš„éƒ¨ç½²è´Ÿæ‹…ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é«˜æ€§èƒ½å…¨è„‰å†²é©±åŠ¨æ¡†æ¶SpikeSCRï¼Œå…·æœ‰å…¨å±€-å±€éƒ¨æ··åˆç»“æ„ï¼Œå®ç°é•¿æœŸå­¦ä¹ ã€‚</li>
<li>é€šè¿‡çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼ˆKDCLï¼‰å‡å°‘æ—¶é—´æ­¥é•¿å’Œèƒ½é‡æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜SpikeSCRä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>KDCLæ–¹æ³•èƒ½æœ‰æ•ˆå¹³è¡¡åŠŸç‡æ•ˆç‡ä¸é«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c5c0282738ff70c6b0cc388284ddfeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3f2cf485a41ca70fc86f62737528745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e59fc826ac68169aec2216c325bdce3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04f825182cbe12bf0e0b956ccd6f098a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99c2d7388fca819ca4b9466b56518a4f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CAMEL-Cross-Attention-Enhanced-Mixture-of-Experts-and-Language-Bias-for-Code-Switching-Speech-Recognition"><a href="#CAMEL-Cross-Attention-Enhanced-Mixture-of-Experts-and-Language-Bias-for-Code-Switching-Speech-Recognition" class="headerlink" title="CAMEL: Cross-Attention Enhanced Mixture-of-Experts and Language Bias for   Code-Switching Speech Recognition"></a>CAMEL: Cross-Attention Enhanced Mixture-of-Experts and Language Bias for   Code-Switching Speech Recognition</h2><p><strong>Authors:He Wang, Xucheng Wan, Naijun Zheng, Kai Liu, Huan Zhou, Guojian Li, Lei Xie</strong></p>
<p>Code-switching automatic speech recognition (ASR) aims to transcribe speech that contains two or more languages accurately. To better capture language-specific speech representations and address language confusion in code-switching ASR, the mixture-of-experts (MoE) architecture and an additional language diarization (LD) decoder are commonly employed. However, most researches remain stagnant in simple operations like weighted summation or concatenation to fuse language-specific speech representations, leaving significant opportunities to explore the enhancement of integrating language bias information. In this paper, we introduce CAMEL, a cross-attention-based MoE and language bias approach for code-switching ASR. Specifically, after each MoE layer, we fuse language-specific speech representations with cross-attention, leveraging its strong contextual modeling abilities. Additionally, we design a source attention-based mechanism to incorporate the language information from the LD decoder output into text embeddings. Experimental results demonstrate that our approach achieves state-of-the-art performance on the SEAME, ASRU200, and ASRU700+LibriSpeech460 Mandarin-English code-switching ASR datasets. </p>
<blockquote>
<p>ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ—¨åœ¨å‡†ç¡®è½¬å½•åŒ…å«ä¸¤ç§æˆ–å¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚ä¸ºäº†æ›´å¥½åœ°æ•è·ç‰¹å®šè¯­è¨€çš„è¯­éŸ³è¡¨ç¤ºå¹¶è§£å†³ä»£ç åˆ‡æ¢ASRä¸­çš„è¯­è¨€æ··æ·†é—®é¢˜ï¼Œé€šå¸¸é‡‡ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„å’Œé¢å¤–çš„è¯­è¨€æ—¥è®°åŒ–ï¼ˆLDï¼‰è§£ç å™¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶ä»ç„¶åœç•™åœ¨åŠ æƒæ±‚å’Œæˆ–ä¸²è”ç­‰ç®€å•æ“ä½œæ¥èåˆç‰¹å®šè¯­è¨€çš„è¯­éŸ³è¡¨ç¤ºï¼Œè¿™ç•™ä¸‹äº†æ¢ç´¢æ•´åˆè¯­è¨€åå‘ä¿¡æ¯ä»¥å¢å¼ºæ€§èƒ½çš„å·¨å¤§æœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CAMELï¼Œè¿™æ˜¯ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›çš„MoEå’Œè¯­è¨€åå‘æ–¹æ³•ï¼Œç”¨äºä»£ç åˆ‡æ¢ASRã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯ä¸ªMoEå±‚ä¹‹åï¼Œæˆ‘ä»¬åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›èåˆç‰¹å®šè¯­è¨€çš„è¯­éŸ³è¡¨ç¤ºï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæºæ³¨æ„åŠ›çš„æœºåˆ¶ï¼Œå°†LDè§£ç å™¨çš„è¯­è¨€ä¿¡æ¯èå…¥æ–‡æœ¬åµŒå…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨SEAMEã€ASRU200ã€ASRU700+LibriSpeech460çš„æ±‰è¯­-è‹±è¯­ä»£ç åˆ‡æ¢ASRæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12760v1">PDF</a> Accepted by ICASSP 2025. 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒè¯­æˆ–å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„ä»£ç åˆ‡æ¢é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›å’Œè¯­è¨€åå¥½çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„æ–¹æ³•CAMELã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤å‰æ³¨æ„åŠ›èåˆè¯­è¨€ç‰¹å®šè¯­éŸ³è¡¨ç¤ºï¼Œå¹¶å°†è¯­è¨€ä¿¡æ¯ä»è¯­è¨€æ—¥è®°è§£ç å™¨è¾“å‡ºèå…¥æ–‡æœ¬åµŒå…¥ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAMELåœ¨SEAMEã€ASRU200å’ŒASRU700+LibriSpeech460çš„æ™®é€šè¯-è‹±è¯­ä»£ç åˆ‡æ¢ASRæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ—¨åœ¨å‡†ç¡®è½¬å½•åŒ…å«ä¸¤ç§æˆ–å¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚</li>
<li>ç›®å‰ç ”ç©¶åœ¨èåˆè¯­è¨€ç‰¹å®šè¯­éŸ³è¡¨ç¤ºæ—¶ä»åœç•™åœ¨ç®€å•æ“ä½œï¼Œå¦‚åŠ æƒæ±‚å’Œæˆ–æ‹¼æ¥ï¼Œå­˜åœ¨æ¢ç´¢å¢å¼ºæ•´åˆè¯­è¨€åè§ä¿¡æ¯çš„ç©ºé—´ã€‚</li>
<li>å¼•å…¥CAMELæ–¹æ³•ï¼Œç»“åˆæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ”¹è¿›ä»£ç åˆ‡æ¢ASRçš„æ€§èƒ½ã€‚</li>
<li>CAMELåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›èåˆè¯­è¨€ç‰¹å®šè¯­éŸ³è¡¨ç¤ºï¼Œå¹¶åœ¨æ¯ä¸ªMoEå±‚åè¿›è¡Œã€‚</li>
<li>CAMELè®¾è®¡äº†ä¸€ç§åŸºäºæºæ³¨æ„åŠ›çš„æœºåˆ¶ï¼Œå°†è¯­è¨€ä¿¡æ¯ä»è¯­è¨€æ—¥è®°è§£ç å™¨è¾“å‡ºèå…¥æ–‡æœ¬åµŒå…¥ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCAMELåœ¨å¤šä¸ªæ™®é€šè¯-è‹±è¯­ä»£ç åˆ‡æ¢ASRæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99e4b5a39ba63de4492ac74da5c64394.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9200d26abba95f21f8bf122eeb95a0dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a948c134d02d66d48bb96fb34a1a0fc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64a3ccec7a8dcf161e253e2697b0de63.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Streaming-Keyword-Spotting-Boosted-by-Cross-layer-Discrimination-Consistency"><a href="#Streaming-Keyword-Spotting-Boosted-by-Cross-layer-Discrimination-Consistency" class="headerlink" title="Streaming Keyword Spotting Boosted by Cross-layer Discrimination   Consistency"></a>Streaming Keyword Spotting Boosted by Cross-layer Discrimination   Consistency</h2><p><strong>Authors:Yu Xi, Haoyu Li, Xiaoyu Gu, Hao Li, Yidi Jiang, Kai Yu</strong></p>
<p>Connectionist Temporal Classification (CTC), a non-autoregressive training criterion, is widely used in online keyword spotting (KWS). However, existing CTC-based KWS decoding strategies either rely on Automatic Speech Recognition (ASR), which performs suboptimally due to its broad search over the acoustic space without keyword-specific optimization, or on KWS-specific decoding graphs, which are complex to implement and maintain. In this work, we propose a streaming decoding algorithm enhanced by Cross-layer Discrimination Consistency (CDC), tailored for CTC-based KWS. Specifically, we introduce a streamlined yet effective decoding algorithm capable of detecting the start of the keyword at any arbitrary position. Furthermore, we leverage discrimination consistency information across layers to better differentiate between positive and false alarm samples. Our experiments on both clean and noisy Hey Snips datasets show that the proposed streaming decoding strategy outperforms ASR-based and graph-based KWS baselines. The CDC-boosted decoding further improves performance, yielding an average absolute recall improvement of 6.8% and a 46.3% relative reduction in the miss rate compared to the graph-based KWS baseline, with a very low false alarm rate of 0.05 per hour. </p>
<blockquote>
<p>è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰æ˜¯ä¸€ç§éè‡ªå›å½’è®­ç»ƒå‡†åˆ™ï¼Œå¹¿æ³›åº”ç”¨äºåœ¨çº¿å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºCTCçš„KWSè§£ç ç­–ç•¥è¦ä¹ˆä¾èµ–äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼Œç”±äºå…¶åœ¨å£°å­¦ç©ºé—´ä¸Šçš„å¹¿æ³›æœç´¢è€Œæ²¡æœ‰é’ˆå¯¹å…³é”®è¯çš„ç‰¹å®šä¼˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ï¼›è¦ä¹ˆä¾èµ–äºç‰¹å®šçš„KWSè§£ç å›¾ï¼Œè¿™äº›å›¾å¤æ‚ä¸”éš¾ä»¥å®æ–½å’Œç»´æŠ¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡è·¨å±‚é‰´åˆ«ä¸€è‡´æ€§ï¼ˆCDCï¼‰å¢å¼ºçš„æµå¼è§£ç ç®—æ³•ï¼Œé€‚ç”¨äºåŸºäºCTCçš„KWSã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€æ´æœ‰æ•ˆçš„è§£ç ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„ä½ç½®æ£€æµ‹å…³é”®è¯çš„å¼€å§‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨è·¨å±‚çš„é‰´åˆ«ä¸€è‡´æ€§ä¿¡æ¯æ¥æ›´å¥½åœ°åŒºåˆ†æ­£è´Ÿæ ·æœ¬ã€‚æˆ‘ä»¬åœ¨å¹²å‡€å’Œå˜ˆæ‚çš„Hey Snipsæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æµå¼è§£ç ç­–ç•¥ä¼˜äºåŸºäºASRå’ŒåŸºäºå›¾çš„KWSåŸºçº¿ã€‚é€šè¿‡CDCå¢å¼ºçš„è§£ç è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œä¸åŸºäºå›¾çš„KWSåŸºçº¿ç›¸æ¯”ï¼Œå¹³å‡ç»å¯¹å¬å›ç‡æé«˜äº†6.8%ï¼Œæ¼æŠ¥ç‡ç›¸å¯¹é™ä½äº†46.3%ï¼Œä¸”æ¯å°æ—¶è¯¯æŠ¥ç‡éå¸¸ä½ï¼Œä¸º0.05ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12635v1">PDF</a> Submitted to ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>CTCï¼ˆè¿æ¥æ—¶åºåˆ†ç±»ï¼‰åœ¨éè‡ªå›å½’è®­ç»ƒå‡†åˆ™ä¸­å¹¿æ³›åº”ç”¨äºåœ¨çº¿å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CTC-based KWSè§£ç ç­–ç•¥ä¾èµ–äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼Œæ€§èƒ½ä¸ä½³ï¼Œå› ä¸ºå®ƒåœ¨å£°å­¦ç©ºé—´è¿›è¡Œå¹¿æ³›æœç´¢è€Œæ²¡æœ‰å…³é”®è¯ç‰¹å®šä¼˜åŒ–ã€‚æˆ–ä¾èµ–äºå¤æ‚çš„KWSç‰¹å®šè§£ç å›¾ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”±è·¨å±‚é‰´åˆ«ä¸€è‡´æ€§ï¼ˆCDCï¼‰å¢å¼ºçš„æµå¼è§£ç ç®—æ³•ï¼Œé€‚ç”¨äºCTC-based KWSã€‚è¯¥ç®—æ³•èƒ½æœ‰æ•ˆæ£€æµ‹å…³é”®è¯çš„èµ·å§‹ä½ç½®ï¼Œå¹¶å€ŸåŠ©è·¨å±‚çš„é‰´åˆ«ä¸€è‡´æ€§ä¿¡æ¯ï¼Œæ›´å¥½åœ°åŒºåˆ†æ­£æ ·æœ¬å’Œè¯¯è­¦æ ·æœ¬ã€‚åœ¨Hey Snipsæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€ææµå¼è§£ç ç­–ç•¥ä¼˜äºåŸºäºASRå’ŒåŸºäºå›¾çš„KWSåŸºçº¿æ–¹æ³•ã€‚CDCå¢å¼ºçš„è§£ç è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œä¸åŸºäºå›¾çš„KWSåŸºçº¿ç›¸æ¯”ï¼Œå¹³å‡ç»å¯¹å¬å›ç‡æé«˜äº†6.8%ï¼Œè¯¯æŠ¥ç‡é™ä½äº†46.3%ï¼Œä¸”æ¯å°æ—¶è¯¯è­¦ç‡å¾ˆä½ï¼Œä¸ºæ¯å°æ—¶0.05æ¬¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CTCåœ¨éè‡ªå›å½’è®­ç»ƒå‡†åˆ™ä¸­å¹¿æ³›ç”¨äºåœ¨çº¿å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ã€‚</li>
<li>ç°æœ‰CTC-based KWSè§£ç ç­–ç•¥å­˜åœ¨ä¾èµ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¤æ‚å®æ–½çš„KWSç‰¹å®šè§£ç å›¾çš„é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”±è·¨å±‚é‰´åˆ«ä¸€è‡´æ€§ï¼ˆCDCï¼‰å¢å¼ºçš„æµå¼è§£ç ç®—æ³•ï¼Œæœ‰æ•ˆæ£€æµ‹å…³é”®è¯èµ·å§‹ä½ç½®å¹¶åŒºåˆ†æ­£ã€è¯¯è­¦æ ·æœ¬ã€‚</li>
<li>æ‰€ææµå¼è§£ç ç­–ç•¥ä¼˜äºåŸºäºASRå’ŒåŸºäºå›¾çš„KWSåŸºçº¿æ–¹æ³•ã€‚</li>
<li>CDCå¢å¼ºçš„è§£ç æé«˜äº†æ€§èƒ½ï¼Œå¹³å‡ç»å¯¹å¬å›ç‡æé«˜ï¼Œè¯¯æŠ¥ç‡é™ä½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨Hey Snipsæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯å…·æœ‰è¾ƒä½è¯¯è­¦ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29f8756f7dc8371bd07075d6f6530a2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25becc6ca17512c6389bde9859232d21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-729e4fdd3eabb88bbcff0997af20c29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e24f5c79b89e9a15fd52b096f632537b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bbe6980727f7156eb6a3ce658e9f476e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Phoneme-Level-Feature-Discrepancies-A-Key-to-Detecting-Sophisticated-Speech-Deepfakes"><a href="#Phoneme-Level-Feature-Discrepancies-A-Key-to-Detecting-Sophisticated-Speech-Deepfakes" class="headerlink" title="Phoneme-Level Feature Discrepancies: A Key to Detecting Sophisticated   Speech Deepfakes"></a>Phoneme-Level Feature Discrepancies: A Key to Detecting Sophisticated   Speech Deepfakes</h2><p><strong>Authors:Kuiyuan Zhang, Zhongyun Hua, Rushi Lan, Yushu Zhang, Yifang Guo</strong></p>
<p>Recent advancements in text-to-speech and speech conversion technologies have enabled the creation of highly convincing synthetic speech. While these innovations offer numerous practical benefits, they also cause significant security challenges when maliciously misused. Therefore, there is an urgent need to detect these synthetic speech signals. Phoneme features provide a powerful speech representation for deepfake detection. However, previous phoneme-based detection approaches typically focused on specific phonemes, overlooking temporal inconsistencies across the entire phoneme sequence. In this paper, we develop a new mechanism for detecting speech deepfakes by identifying the inconsistencies of phoneme-level speech features. We design an adaptive phoneme pooling technique that extracts sample-specific phoneme-level features from frame-level speech data. By applying this technique to features extracted by pre-trained audio models on previously unseen deepfake datasets, we demonstrate that deepfake samples often exhibit phoneme-level inconsistencies when compared to genuine speech. To further enhance detection accuracy, we propose a deepfake detector that uses a graph attention network to model the temporal dependencies of phoneme-level features. Additionally, we introduce a random phoneme substitution augmentation technique to increase feature diversity during training. Extensive experiments on four benchmark datasets demonstrate the superior performance of our method over existing state-of-the-art detection methods. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬è½¬è¯­éŸ³å’Œè¯­éŸ³è½¬æ¢æŠ€æœ¯çš„è¿›å±•ä½¿å¾—åˆ›å»ºé«˜åº¦é€¼çœŸçš„åˆæˆè¯­éŸ³æˆä¸ºå¯èƒ½ã€‚è™½ç„¶è¿™äº›åˆ›æ–°æä¾›äº†è®¸å¤šå®é™…å¥½å¤„ï¼Œä½†å®ƒä»¬åœ¨è¢«æ¶æ„è¯¯ç”¨æ—¶ä¹Ÿä¼šå¸¦æ¥å·¨å¤§çš„å®‰å…¨æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæ£€æµ‹è¿™äº›åˆæˆè¯­éŸ³ä¿¡å·å˜å¾—æä¸ºè¿«åˆ‡ã€‚éŸ³ç´ ç‰¹å¾ä¸ºæ·±åº¦ä¼ªé€ æ£€æµ‹æä¾›äº†å¼ºå¤§çš„è¯­éŸ³è¡¨ç¤ºã€‚ç„¶è€Œï¼ŒåŸºäºéŸ³ç´ çš„æ£€æµ‹æ–¹æ³•é€šå¸¸ä¾§é‡äºç‰¹å®šçš„éŸ³ç´ ï¼Œå¿½ç•¥äº†æ•´ä¸ªéŸ³ç´ åºåˆ—çš„æ—¶é—´ä¸ä¸€è‡´æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€šè¿‡è¯†åˆ«éŸ³ç´ çº§è¯­éŸ³ç‰¹å¾çš„ä¸ä¸€è‡´æ€§æ¥æ£€æµ‹è¯­éŸ³æ·±åº¦ä¼ªé€ çš„æ–°æœºåˆ¶ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”çš„éŸ³ç´ æ± æŠ€æœ¯ï¼Œä»å¸§çº§è¯­éŸ³æ•°æ®ä¸­æå–æ ·æœ¬ç‰¹å®šçš„éŸ³ç´ çº§ç‰¹å¾ã€‚æˆ‘ä»¬å°†è¯¥æŠ€æœ¯åº”ç”¨äºåœ¨ä¹‹å‰æœªè§è¿‡çš„æ·±åº¦ä¼ªé€ æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„éŸ³é¢‘æ¨¡å‹æ‰€æå–çš„ç‰¹å¾ï¼Œç»“æœè¡¨æ˜ï¼Œä¸çœŸå®è¯­éŸ³ç›¸æ¯”ï¼Œæ·±åº¦ä¼ªé€ æ ·æœ¬åœ¨éŸ³ç´ çº§ä¸Šé€šå¸¸è¡¨ç°å‡ºä¸ä¸€è‡´æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ£€æµ‹ç²¾åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œå¯¹éŸ³ç´ çº§ç‰¹å¾çš„æ—¶é—´ä¾èµ–æ€§è¿›è¡Œå»ºæ¨¡çš„æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§éšæœºéŸ³ç´ æ›¿æ¢å¢å¼ºæŠ€æœ¯ï¼Œä»¥å¢åŠ è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç‰¹å¾å¤šæ ·æ€§ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ£€æµ‹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ–‡æœ¬è½¬è¯­éŸ³å’Œè¯­éŸ³è½¬æ¢æŠ€æœ¯çš„è¿›å±•ä¸ºåˆ›å»ºé«˜åº¦é€¼çœŸçš„åˆæˆè¯­éŸ³æä¾›äº†å¯èƒ½ã€‚è¿™äº›æŠ€æœ¯å¸¦æ¥è¯¸å¤šå®ç”¨ä¼˜åŠ¿çš„åŒæ—¶ï¼Œä¹Ÿå­˜åœ¨æ¶æ„æ»¥ç”¨å¸¦æ¥çš„é‡å¤§å®‰å…¨éšæ‚£ã€‚å› æ­¤ï¼Œæ£€æµ‹åˆæˆè¯­éŸ³ä¿¡å·å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºéŸ³ç´ çº§è¯­éŸ³ç‰¹å¾ä¸ä¸€è‡´æ€§çš„æ–°å‹æ·±åº¦ä¼ªé€ æ£€æµ‹æœºåˆ¶ã€‚è®¾è®¡è‡ªé€‚åº”éŸ³ç´ æ± åŒ–æŠ€æœ¯ï¼Œä»å¸§çº§è¯­éŸ³æ•°æ®ä¸­æå–æ ·æœ¬ç‰¹å®šçš„éŸ³ç´ çº§ç‰¹å¾ã€‚é€šè¿‡å°†æ­¤é¡¹æŠ€æœ¯åº”ç”¨äºé¢„è®­ç»ƒéŸ³é¢‘æ¨¡å‹æå–çš„ç‰¹å¾ä¸Šï¼Œæˆ‘ä»¬è¯æ˜ä¸çœŸå®è¯­éŸ³ç›¸æ¯”ï¼Œæ·±åº¦ä¼ªé€ æ ·æœ¬çš„éŸ³ç´ çº§å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚ä¸ºæé«˜æ£€æµ‹å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œçš„æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨ï¼Œå¯¹éŸ³ç´ çº§ç‰¹å¾çš„æ—¶åºä¾èµ–æ€§è¿›è¡Œå»ºæ¨¡ã€‚æ­¤å¤–ï¼Œå¼•å…¥éšæœºéŸ³ç´ æ›¿æ¢å¢å¼ºæŠ€æœ¯ä»¥æé«˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç‰¹å¾å¤šæ ·æ€§ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸæ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯å¸¦æ¥å®ç”¨ä¼˜åŠ¿ï¼Œä½†ä¹Ÿå­˜åœ¨å®‰å…¨éšæ‚£ã€‚</li>
<li>æ£€æµ‹åˆæˆè¯­éŸ³ä¿¡å·çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ·±åº¦ä¼ªé€ æ£€æµ‹æœºåˆ¶ï¼ŒåŸºäºéŸ³ç´ çº§è¯­éŸ³ç‰¹å¾çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡è‡ªé€‚åº”éŸ³ç´ æ± åŒ–æŠ€æœ¯ï¼Œæå–æ ·æœ¬ç‰¹å®šçš„éŸ³ç´ çº§ç‰¹å¾ã€‚</li>
<li>æ·±åº¦ä¼ªé€ æ ·æœ¬ä¸çœŸå®è¯­éŸ³åœ¨éŸ³ç´ çº§å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ä½¿ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œå¯¹éŸ³ç´ çº§ç‰¹å¾çš„æ—¶åºä¾èµ–æ€§è¿›è¡Œå»ºæ¨¡ï¼Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥éšæœºéŸ³ç´ æ›¿æ¢å¢å¼ºæŠ€æœ¯ï¼Œå¢åŠ è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç‰¹å¾å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-19\./crop_Speech/2412.12619v1/page_0_0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00bb35e2a988cc91bda518f9a6481d49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f707af58709fdcc35a6a4e36063be29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b05844da076cda2e754ca3c6da10b9bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe5a85918dd32d3ac733c344793edb4c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Libri2Vox-Dataset-Target-Speaker-Extraction-with-Diverse-Speaker-Conditions-and-Synthetic-Data"><a href="#Libri2Vox-Dataset-Target-Speaker-Extraction-with-Diverse-Speaker-Conditions-and-Synthetic-Data" class="headerlink" title="Libri2Vox Dataset: Target Speaker Extraction with Diverse Speaker   Conditions and Synthetic Data"></a>Libri2Vox Dataset: Target Speaker Extraction with Diverse Speaker   Conditions and Synthetic Data</h2><p><strong>Authors:Yun Liu, Xuechen Liu, Xiaoxiao Miao, Junichi Yamagishi</strong></p>
<p>Target speaker extraction (TSE) is essential in speech processing applications, particularly in scenarios with complex acoustic environments. Current TSE systems face challenges in limited data diversity and a lack of robustness in real-world conditions, primarily because they are trained on artificially mixed datasets with limited speaker variability and unrealistic noise profiles. To address these challenges, we propose Libri2Vox, a new dataset that combines clean target speech from the LibriTTS dataset with interference speech from the noisy VoxCeleb2 dataset, providing a large and diverse set of speakers under realistic noisy conditions. We also augment Libri2Vox with synthetic speakers generated using state-of-the-art speech generative models to enhance speaker diversity. Additionally, to further improve the effectiveness of incorporating synthetic data, curriculum learning is implemented to progressively train TSE models with increasing levels of difficulty. Extensive experiments across multiple TSE architectures reveal varying degrees of improvement, with SpeakerBeam demonstrating the most substantial gains: a 1.39 dB improvement in signal-to-distortion ratio (SDR) on the Libri2Talker test set compared to baseline training. Building upon these results, we further enhanced performance through our speaker similarity-based curriculum learning approach with the Conformer architecture, achieving an additional 0.78 dB improvement over conventional random sampling methods in which data samples are randomly selected from the entire dataset. These results demonstrate the complementary benefits of diverse real-world data, synthetic speaker augmentation, and structured training strategies in building robust TSE systems. </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰åœ¨è¯­éŸ³å¤„ç†åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å£°å­¦ç¯å¢ƒåœºæ™¯ä¸­ã€‚å½“å‰çš„TSEç³»ç»Ÿé¢ä¸´æ•°æ®å¤šæ ·æ€§æœ‰é™å’Œåœ¨å®é™…ç¯å¢ƒä¸­çš„ç¨³å¥æ€§ä¸è¶³çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬æ˜¯åœ¨äººå·¥æ··åˆæ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œå…·æœ‰æœ‰é™çš„è¯´è¯äººå˜ä½“å’Œä¸å¤ªç°å®çš„å™ªå£°åˆ†å¸ƒã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Libri2Voxæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å°†LibriTTSçš„å¹²å‡€ç›®æ ‡è¯­éŸ³ä¸VoxCeleb2çš„å¹²æ‰°è¯­éŸ³ç›¸ç»“åˆï¼Œåœ¨çœŸå®å™ªå£°æ¡ä»¶ä¸‹æä¾›äº†å¤§é‡ä¸”å¤šæ ·çš„è¯´è¯äººæ•°æ®ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æœ€å…ˆè¿›çš„è¯­éŸ³ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„åˆæˆè¯´è¯äººæ¥å¢å¼ºLibri2Voxçš„è¯´è¯äººå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥æé«˜åˆæˆæ•°æ®çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å®æ–½äº†è¯¾ç¨‹å­¦ä¹ ï¼Œä»¥é€æ­¥è®­ç»ƒå…·æœ‰ä¸åŒéš¾åº¦çš„TSEæ¨¡å‹ã€‚è·¨å¤šä¸ªTSEæ¶æ„çš„å¹¿æ³›å®éªŒæ˜¾ç¤ºå‡ºäº†ä¸åŒç¨‹åº¦çš„æ”¹è¿›ï¼Œå…¶ä¸­SpeakerBeamçš„æ”¹è¿›æœ€ä¸ºæ˜¾è‘—ï¼šä¸åŸºçº¿è®­ç»ƒç›¸æ¯”ï¼Œåœ¨Libri2Talkeræµ‹è¯•é›†ä¸Šï¼Œä¿¡å·å¤±çœŸç‡ï¼ˆSDRï¼‰æé«˜äº†1.39åˆ†è´ã€‚åŸºäºè¿™äº›ç»“æœï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨åŸºäºè¯´è¯äººç›¸ä¼¼æ€§çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œé‡‡ç”¨Conformeræ¶æ„ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„éšæœºé‡‡æ ·æ–¹æ³•ï¼Œåœ¨æ•°æ®æ ·æœ¬ä»æ•´ä¸ªæ•°æ®é›†ä¸­éšæœºé€‰æ‹©çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†é¢å¤–çš„0.78åˆ†è´çš„æ”¹è¿›ã€‚è¿™äº›ç»“æœè¯æ˜äº†ç°å®ä¸–ç•Œçš„å¤šæ ·æ•°æ®ã€åˆæˆè¯´è¯äººå¢å¼ºå’Œç»“æ„åŒ–è®­ç»ƒç­–ç•¥åœ¨æ„å»ºç¨³å¥çš„TSEç³»ç»Ÿä¸­çš„äº’è¡¥æ•ˆç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12512v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªæ–°çš„æ•°æ®é›†Libri2Voxï¼Œç”¨äºè§£å†³ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸‹çš„æŒ‘æˆ˜ã€‚Libri2Voxç»“åˆäº†LibriTTSçš„å¹²å‡€ç›®æ ‡è¯­éŸ³å’ŒVoxCeleb2çš„å¹²æ‰°è¯­éŸ³ï¼Œæä¾›äº†å¤§é‡çœŸå®å™ªå£°æ¡ä»¶ä¸‹çš„ä¸åŒè¯´è¯äººæ•°æ®ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨å…ˆè¿›çš„è¯­éŸ³ç”Ÿæˆæ¨¡å‹ç”Ÿæˆåˆæˆè¯´è¯äººæ¥å¢å¼ºè¯´è¯äººçš„å¤šæ ·æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åˆæˆæ•°æ®çš„æœ‰æ•ˆæ€§ï¼Œå®æ–½è¯¾ç¨‹å­¦ä¹ é€æ­¥è®­ç»ƒä¸åŒéš¾åº¦çš„TSEæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒSpeakerBeamè¡¨ç°æœ€ä½³ï¼Œåœ¨Libri2Talkeræµ‹è¯•é›†ä¸Šä¿¡å·å¤±çœŸæ¯”ï¼ˆSDRï¼‰æé«˜äº†1.39åˆ†è´ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œé‡‡ç”¨åŸºäºè¯´è¯äººç›¸ä¼¼æ€§çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ä¸Conformeræ¶æ„è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œä¸ä¼ ç»Ÿéšæœºé‡‡æ ·æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†é¢å¤–çš„0.78åˆ†è´çš„æ”¹è¿›ã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ ·ç°å®æ•°æ®ã€åˆæˆè¯´è¯äººå¢å¼ºå’Œç»“æ„åŒ–è®­ç»ƒç­–ç•¥åœ¨æ„å»ºç¨³å¥çš„TSEç³»ç»Ÿä¸­å…·æœ‰äº’è¡¥ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥Libri2Voxæ•°æ®é›†ï¼Œç»“åˆçœŸå®å™ªå£°æ¡ä»¶ä¸‹çš„å¹²å‡€ç›®æ ‡è¯­éŸ³å’Œå¹²æ‰°è¯­éŸ³ï¼Œæé«˜ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰çš„é²æ£’æ€§ã€‚</li>
<li>é€šè¿‡ç»“åˆå…ˆè¿›çš„è¯­éŸ³ç”Ÿæˆæ¨¡å‹ï¼Œå¢å¼ºè¯´è¯äººå¤šæ ·æ€§ã€‚</li>
<li>å®æ–½è¯¾ç¨‹å­¦ä¹ ï¼Œé€æ­¥è®­ç»ƒTSEæ¨¡å‹ï¼Œæé«˜æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºSpeakerBeamåœ¨Libri2Talkeræµ‹è¯•é›†ä¸Šçš„SDRæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>åŸºäºè¯´è¯äººç›¸ä¼¼æ€§çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ä¸Conformeræ¶æ„ç»“åˆï¼Œå®ç°äº†æ€§èƒ½çš„æå‡ã€‚</li>
<li>å¯¹æ¯”ä¼ ç»Ÿéšæœºé‡‡æ ·æ–¹æ³•ï¼ŒåŸºäºè¯´è¯äººç›¸ä¼¼æ€§çš„é‡‡æ ·å±•ç°å‡ºæ›´å¥½çš„æ•ˆæœã€‚</li>
<li>å¤šæ ·åŒ–çš„ç°å®æ•°æ®ã€åˆæˆè¯´è¯äººå¢å¼ºå’Œç»“æ„åŒ–è®­ç»ƒç­–ç•¥åœ¨æ„å»ºç¨³å¥TSEç³»ç»Ÿä¸­å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2133cd661180134cd3b86e8059e32691.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93dab40b4538e966b6941de4d06a7b41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa345ce0893ceb9ace8b1664ed4c5872.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2e47ea144c92648469e7d9d865882c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4ac1dd843374b543869d74272315e4b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Imagined-Speech-State-Classification-for-Robust-Brain-Computer-Interface"><a href="#Imagined-Speech-State-Classification-for-Robust-Brain-Computer-Interface" class="headerlink" title="Imagined Speech State Classification for Robust Brain-Computer Interface"></a>Imagined Speech State Classification for Robust Brain-Computer Interface</h2><p><strong>Authors:Byung-Kwan Ko, Jun-Young Kim, Seo-Hyun Lee</strong></p>
<p>This study examines the effectiveness of traditional machine learning classifiers versus deep learning models for detecting the imagined speech using electroencephalogram data. Specifically, we evaluated conventional machine learning techniques such as CSP-SVM and LDA-SVM classifiers alongside deep learning architectures such as EEGNet, ShallowConvNet, and DeepConvNet. Machine learning classifiers exhibited significantly lower precision and recall, indicating limited feature extraction capabilities and poor generalization between imagined speech and idle states. In contrast, deep learning models, particularly EEGNet, achieved the highest accuracy of 0.7080 and an F1 score of 0.6718, demonstrating their enhanced ability in automatic feature extraction and representation learning, essential for capturing complex neurophysiological patterns. These findings highlight the limitations of conventional machine learning approaches in brain-computer interface (BCI) applications and advocate for adopting deep learning methodologies to achieve more precise and reliable classification of detecting imagined speech. This foundational research contributes to the development of imagined speech-based BCI systems. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ©ç”¨è„‘ç”µå›¾æ•°æ®æ£€æµ‹æƒ³è±¡ä¸­çš„è¯­éŸ³æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå¦‚CSP-SVMå’ŒLDA-SVMåˆ†ç±»å™¨ï¼Œä»¥åŠæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¦‚EEGNetã€ShallowConvNetå’ŒDeepConvNetã€‚æœºå™¨å­¦ä¹ åˆ†ç±»å™¨çš„ç²¾åº¦å’Œå¬å›ç‡æ˜æ˜¾è¾ƒä½ï¼Œè¡¨æ˜å…¶åœ¨ç‰¹å¾æå–èƒ½åŠ›æ–¹é¢æœ‰é™ï¼Œä¸”åœ¨æƒ³è±¡è¯­è¨€å’Œç©ºé—²çŠ¶æ€ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶æ˜¯EEGNetï¼Œè¾¾åˆ°äº†æœ€é«˜çš„0.7080çš„å‡†ç¡®ç‡å’Œ0.6718çš„F1åˆ†æ•°ï¼Œè¯æ˜äº†å…¶åœ¨è‡ªåŠ¨ç‰¹å¾æå–å’Œè¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„å¢å¼ºèƒ½åŠ›ï¼Œè¿™å¯¹äºæ•æ‰å¤æ‚çš„ç¥ç»ç”Ÿç†æ¨¡å¼è‡³å…³é‡è¦ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨è„‘æœºæ¥å£ï¼ˆBCIï¼‰åº”ç”¨ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå€¡é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æ¥å®ç°æ›´ç²¾ç¡®ã€æ›´å¯é çš„æƒ³è±¡è¯­è¨€æ£€æµ‹åˆ†ç±»ã€‚è¿™é¡¹åŸºç¡€ç ”ç©¶ä¸ºåŸºäºæƒ³è±¡è¯­è¨€çš„BCIç³»ç»Ÿçš„å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12215v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ©ç”¨è„‘ç”µå›¾æ•°æ®æ£€æµ‹æƒ³è±¡è¯­è¨€æ–¹é¢çš„æœ‰æ•ˆæ€§å¯¹æ¯”ã€‚ç ”ç©¶è¯„ä¼°äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚CSP-SVMå’ŒLDA-SVMåˆ†ç±»å™¨ï¼‰å’Œæ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆå¦‚EEGNetã€ShallowConvNetå’ŒDeepConvNetï¼‰çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœºå™¨å­¦ä¹ åˆ†ç±»å™¨çš„ç²¾åº¦å’Œå¬å›ç‡è¾ƒä½ï¼Œè¡¨æ˜å…¶åœ¨ç‰¹å¾æå–å’Œæƒ³è±¡è¯­è¨€ä¸é™æ­¢çŠ¶æ€é—´çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶æ˜¯EEGNetï¼Œè¾¾åˆ°äº†æœ€é«˜çš„0.7080çš„å‡†ç¡®ç‡å’Œ0.6718çš„F1åˆ†æ•°ï¼Œè¯æ˜äº†å…¶åœ¨è‡ªåŠ¨ç‰¹å¾æå–å’Œè¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¿™å¯¹äºæ•æ‰å¤æ‚çš„ç¥ç»ç”Ÿç†æ¨¡å¼è‡³å…³é‡è¦ã€‚ç ”ç©¶å¼ºè°ƒäº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨è„‘æœºæ¥å£ï¼ˆBCIï¼‰åº”ç”¨ä¸­çš„å±€é™æ€§ï¼Œå¹¶ä¸»å¼ é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ä»¥å®ç°æ›´ç²¾ç¡®ã€å¯é çš„æƒ³è±¡è¯­è¨€æ£€æµ‹åˆ†ç±»ã€‚è¯¥ç ”ç©¶ä¸ºåŸºäºæƒ³è±¡è¯­è¨€çš„BCIç³»ç»Ÿå‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å¯¹æ¯”äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æƒ³è±¡è¯­è¨€æ£€æµ‹ä¸­çš„è¡¨ç°ã€‚</li>
<li>æœºå™¨å­¦ä¹ åˆ†ç±»å™¨åœ¨ç‰¹å¾æå–å’Œæ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°æœ‰é™ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶æ˜¯EEGNetï¼Œè¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®ç‡å’ŒF1åˆ†æ•°ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹å…·æœ‰è‡ªåŠ¨ç‰¹å¾æå–å’Œè¡¨ç¤ºå­¦ä¹ çš„ä¼˜åŠ¿ï¼Œèƒ½æ•æ‰å¤æ‚çš„ç¥ç»ç”Ÿç†æ¨¡å¼ã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨è„‘æœºæ¥å£ï¼ˆBCIï¼‰åº”ç”¨ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶ä¸»å¼ é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ä»¥å®ç°æ›´ç²¾ç¡®ã€å¯é çš„æƒ³è±¡è¯­è¨€æ£€æµ‹åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdf1506df8ae0615261381e4a92d00d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a4f0906d0d59f63a51b04e468030062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08ba71c232c8a96b5379eb8264878403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2de2b3fcde88d2987e49b9512e0042d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24c12fd70b443a611f4e1c701b05f9c1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Greek2MathTex-A-Greek-Speech-to-Text-Framework-for-LaTeX-Equations-Generation"><a href="#Greek2MathTex-A-Greek-Speech-to-Text-Framework-for-LaTeX-Equations-Generation" class="headerlink" title="Greek2MathTex: A Greek Speech-to-Text Framework for LaTeX Equations   Generation"></a>Greek2MathTex: A Greek Speech-to-Text Framework for LaTeX Equations   Generation</h2><p><strong>Authors:Evangelia Gkritzali, Panagiotis Kaliosis, Sofia Galanaki, Elisavet Palogiannidi, Theodoros Giannakopoulos</strong></p>
<p>In the vast majority of the academic and scientific domains, LaTeX has established itself as the de facto standard for typesetting complex mathematical equations and formulae. However, LaTeXâ€™s complex syntax and code-like appearance present accessibility barriers for individuals with disabilities, as well as those unfamiliar with coding conventions. In this paper, we present a novel solution to this challenge through the development of a novel speech-to-LaTeX equations system specifically designed for the Greek language. We propose an end-to-end system that harnesses the power of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP) techniques to enable users to verbally dictate mathematical expressions and equations in natural language, which are subsequently converted into LaTeX format. We present the architecture and design principles of our system, highlighting key components such as the ASR engine, the LLM-based prompt-driven equations generation mechanism, as well as the application of a custom evaluation metric employed throughout the development process. We have made our system open source and available at <a target="_blank" rel="noopener" href="https://github.com/magcil/greek-speech-to-math">https://github.com/magcil/greek-speech-to-math</a>. </p>
<blockquote>
<p>åœ¨å­¦æœ¯å’Œç§‘å­¦é¢†åŸŸçš„ç»å¤§å¤šæ•°åœºæ™¯ä¸­ï¼ŒLaTeXå·²ç¡®ç«‹äº†è‡ªå·±åœ¨æ’ç‰ˆå¤æ‚çš„æ•°å­¦æ–¹ç¨‹å’Œå…¬å¼æ–¹é¢çš„å®é™…æ ‡å‡†åœ°ä½ã€‚ç„¶è€Œï¼ŒLaTeXçš„å¤æ‚è¯­æ³•å’Œç±»ä¼¼äºä»£ç çš„ç•Œé¢ä¸ºæ®‹ç–¾äººä»¥åŠä¸ç†Ÿæ‚‰ç¼–ç è§„èŒƒçš„äººè®¾ç½®äº†è®¿é—®éšœç¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£å†³æ–¹æ¡ˆï¼Œå¼€å‘äº†ä¸€ç§ä¸“é—¨ç”¨äºå¸Œè…Šè¯­çš„æ–°å‹è¯­éŸ³åˆ°LaTeXæ–¹ç¨‹ç³»ç»Ÿã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„ç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯çš„åŠ›é‡ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç”¨è‡ªç„¶è¯­è¨€å£å¤´è¡¨è¿°æ•°å­¦è¡¨è¾¾å¼å’Œæ–¹ç¨‹ï¼Œéšåå°†å…¶è½¬æ¢ä¸ºLaTeXæ ¼å¼ã€‚æˆ‘ä»¬ä»‹ç»äº†ç³»ç»Ÿçš„æ¶æ„å’Œè®¾è®¡åŸåˆ™ï¼Œé‡ç‚¹ä»‹ç»äº†å…³é”®ç»„ä»¶ï¼Œå¦‚ASRå¼•æ“ã€åŸºäºLLMçš„æç¤ºé©±åŠ¨æ–¹ç¨‹ç”Ÿæˆæœºåˆ¶ï¼Œä»¥åŠå¼€å‘è¿‡ç¨‹ä¸­ä½¿ç”¨çš„è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå·²å¼€æºï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/magcil/greek-speech-to-math%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/magcil/greek-speech-to-mathæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12167v1">PDF</a> 4 pages, 2 figures, SETN2024: 13th EETN Conference on Artificial   Intelligence</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹å¸Œè…Šè¯­çš„æ–°å‹è¯­éŸ³åˆ°LaTeXæ–¹ç¨‹ç³»ç»Ÿã€‚æ­¤ç³»ç»Ÿåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç”¨è‡ªç„¶è¯­è¨€å£è¿°æ•°å­¦è¡¨è¾¾å¼å’Œæ–¹ç¨‹ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºLaTeXæ ¼å¼ã€‚æ–‡ä¸­ä»‹ç»äº†ç³»ç»Ÿçš„æ¶æ„å’Œè®¾è®¡åŸåˆ™ï¼ŒåŒ…æ‹¬ASRå¼•æ“ã€åŸºäºLLMçš„æç¤ºé©±åŠ¨æ–¹ç¨‹ç”Ÿæˆæœºåˆ¶ä»¥åŠè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LaTeXæ˜¯å­¦æœ¯å’Œç§‘å­¦é¢†åŸŸä¸­ç”¨äºæ’ç‰ˆå¤æ‚æ•°å­¦æ–¹ç¨‹å’Œå…¬å¼çš„æ ‡å‡†ã€‚</li>
<li>LaTeXçš„å¤æ‚è¯­æ³•å’Œç±»ä¼¼ä»£ç çš„å¤–è§‚å¯¹æ®‹éšœäººå£«å’Œä¸ç†Ÿæ‚‰ç¼–ç è§„èŒƒçš„äººé€ æˆäº†è®¿é—®éšœç¢ã€‚</li>
<li>é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹è¯­éŸ³åˆ°LaTeXæ–¹ç¨‹ç³»ç»Ÿï¼Œç‰¹åˆ«é€‚ç”¨äºå¸Œè…Šè¯­ã€‚</li>
<li>ç³»ç»Ÿåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç”¨è‡ªç„¶è¯­è¨€è¡¨è¿°æ•°å­¦æ–¹ç¨‹ã€‚</li>
<li>ç³»ç»Ÿå°†ç”¨æˆ·çš„å£è¯­æ•°å­¦è¡¨è¾¾å¼å’Œæ–¹ç¨‹è½¬æ¢ä¸ºLaTeXæ ¼å¼ã€‚</li>
<li>ä»‹ç»äº†ç³»ç»Ÿçš„æ¶æ„ã€è®¾è®¡åŸåˆ™ã€å…³é”®ç»„ä»¶ï¼ˆå¦‚ASRå¼•æ“å’ŒåŸºäºLLMçš„æç¤ºé©±åŠ¨æ–¹ç¨‹ç”Ÿæˆæœºåˆ¶ï¼‰ã€‚</li>
<li>ç³»ç»Ÿå·²å¼€æºï¼Œå¹¶å¯é€šè¿‡ç‰¹å®šé“¾æ¥è¿›è¡Œè®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2181aeb9825d5b523884fb6a4e1f1dfd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82461142b4d3c51af48d1bcf33450d40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18760d91a6c2c8b013682e7d2eeaef7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed38204b5a28cfcd2d23df8347d9eb93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bd02d0e2611d23684103fecf8708b87.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ProsodyFM-Unsupervised-Phrasing-and-Intonation-Control-for-Intelligible-Speech-Synthesis"><a href="#ProsodyFM-Unsupervised-Phrasing-and-Intonation-Control-for-Intelligible-Speech-Synthesis" class="headerlink" title="ProsodyFM: Unsupervised Phrasing and Intonation Control for Intelligible   Speech Synthesis"></a>ProsodyFM: Unsupervised Phrasing and Intonation Control for Intelligible   Speech Synthesis</h2><p><strong>Authors:Xiangheng He, Junjie Chen, Zixing Zhang, BjÃ¶rn W. Schuller</strong></p>
<p>Prosody contains rich information beyond the literal meaning of words, which is crucial for the intelligibility of speech. Current models still fall short in phrasing and intonation; they not only miss or misplace breaks when synthesizing long sentences with complex structures but also produce unnatural intonation. We propose ProsodyFM, a prosody-aware text-to-speech synthesis (TTS) model with a flow-matching (FM) backbone that aims to enhance the phrasing and intonation aspects of prosody. ProsodyFM introduces two key components: a Phrase Break Encoder to capture initial phrase break locations, followed by a Duration Predictor for the flexible adjustment of break durations; and a Terminal Intonation Encoder which integrates a set of intonation shape tokens combined with a novel Pitch Processor for more robust modeling of human-perceived intonation change. ProsodyFM is trained with no explicit prosodic labels and yet can uncover a broad spectrum of break durations and intonation patterns. Experimental results demonstrate that ProsodyFM can effectively improve the phrasing and intonation aspects of prosody, thereby enhancing the overall intelligibility compared to four state-of-the-art (SOTA) models. Out-of-distribution experiments show that this prosody improvement can further bring ProsodyFM superior generalizability for unseen complex sentences and speakers. Our case study intuitively illustrates the powerful and fine-grained controllability of ProsodyFM over phrasing and intonation. </p>
<blockquote>
<p>éŸµå¾‹åŒ…å«è¶…è¶Šå•è¯å­—é¢æ„ä¹‰çš„ä¸°å¯Œä¿¡æ¯ï¼Œè¿™å¯¹äºè¯­éŸ³çš„æ¸…æ™°åº¦è‡³å…³é‡è¦ã€‚å½“å‰æ¨¡å‹åœ¨çŸ­è¯­å’Œè¯­è°ƒæ–¹é¢ä»ç„¶ä¸è¶³ï¼›å®ƒä»¬åœ¨åˆæˆå…·æœ‰å¤æ‚ç»“æ„çš„é•¿å¥å­æ—¶ï¼Œä¸ä»…ä¼šé—æ¼æˆ–é”™è¯¯æ”¾ç½®æ–­ç‚¹ï¼Œè€Œä¸”ä¼šäº§ç”Ÿä¸è‡ªç„¶çš„è¯­è°ƒã€‚æˆ‘ä»¬æå‡ºäº†ProsodyFMï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰æµåŒ¹é…ï¼ˆFMï¼‰éª¨å¹²çš„éŸµå¾‹æ„ŸçŸ¥æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºéŸµå¾‹çš„çŸ­è¯­å’Œè¯­è°ƒæ–¹é¢ã€‚ProsodyFMå¼•å…¥äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ä¸ªçŸ­è¯­æ–­è£‚ç¼–ç å™¨ï¼Œç”¨äºæ•æ‰åˆå§‹çŸ­è¯­æ–­è£‚ä½ç½®ï¼Œéšåæ˜¯ä¸€ä¸ªæŒç»­æ—¶é—´é¢„æµ‹å™¨ï¼Œç”¨äºçµæ´»åœ°è°ƒæ•´æ–­è£‚æŒç»­æ—¶é—´ï¼›ä»¥åŠä¸€ä¸ªç»ˆç«¯è¯­è°ƒç¼–ç å™¨ï¼Œå®ƒç»“åˆäº†è¯­è°ƒå½¢çŠ¶æ ‡è®°å’Œä¸€ç§æ–°çš„éŸ³è°ƒå¤„ç†å™¨ï¼Œä»¥æ›´ç¨³å¥åœ°æ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥åˆ°çš„è¯­è°ƒå˜åŒ–ã€‚ProsodyFMä¸éœ€è¦æ˜ç¡®çš„éŸµå¾‹æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œä½†èƒ½å¤Ÿå‘ç°å¹¿æ³›çš„æ–­è£‚æŒç»­æ—¶é—´å’Œè¯­è°ƒæ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å››ç§æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼ŒProsodyFMå¯ä»¥æœ‰æ•ˆåœ°æ”¹å–„éŸµå¾‹çš„çŸ­è¯­å’Œè¯­è°ƒæ–¹é¢ï¼Œä»è€Œæé«˜æ•´ä½“æ¸…æ™°åº¦ã€‚åˆ†å¸ƒå¤–å®éªŒè¡¨æ˜ï¼Œè¿™ç§éŸµå¾‹æ”¹è¿›å¯ä»¥è¿›ä¸€æ­¥æé«˜ProsodyFMå¯¹æœªè§è¿‡çš„å¤æ‚å¥å­å’Œè¯´è¯äººçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶ç›´è§‚åœ°è¯´æ˜äº†ProsodyFMåœ¨çŸ­è¯­å’Œè¯­è°ƒä¸Šçš„å¼ºå¤§å’Œç²¾ç»†å¯æ§æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11795v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬å¼ºè°ƒäº†åœ¨è¯­éŸ³ä¸­ï¼Œé™¤äº†å­—é¢æ„ä¹‰å¤–ï¼ŒéŸµå¾‹è¿˜åŒ…å«ä¸°å¯Œçš„ä¿¡æ¯ï¼Œè¿™å¯¹è¯­éŸ³çš„å¯æ‡‚åº¦è‡³å…³é‡è¦ã€‚å½“å‰æ¨¡å‹åœ¨åˆæˆå…·æœ‰å¤æ‚ç»“æ„çš„é•¿å¥å­æ—¶ï¼Œå­˜åœ¨æ–­å¥å’Œè¯­è°ƒä¸Šçš„ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ProsodyFMï¼Œä¸€ä¸ªåŸºäºéŸµå¾‹æ„ŸçŸ¥çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆTTSï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºæ–­å¥å’Œè¯­è°ƒæ–¹é¢çš„éŸµå¾‹ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šçŸ­è¯­æ–­å¥ç¼–ç å™¨ç”¨äºæ•æ‰åˆå§‹æ–­å¥ä½ç½®ï¼ŒæŒç»­æ—¶é—´é¢„æµ‹å™¨ç”¨äºçµæ´»è°ƒæ•´æ–­å¥æŒç»­æ—¶é—´ï¼›ç»ˆç«¯è¯­è°ƒç¼–ç å™¨é›†æˆäº†è¯­è°ƒå½¢çŠ¶æ ‡è®°å’Œæ–°é¢–çš„éŸ³è°ƒå¤„ç†å™¨ï¼Œä»¥æ›´ç¨³å¥åœ°æ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥çš„è¯­è°ƒå˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å››ç§æœ€æ–°æŠ€æœ¯æ¨¡å‹ç›¸æ¯”ï¼ŒProsodyFMèƒ½æœ‰æ•ˆæ”¹å–„æ–­å¥å’Œè¯­è°ƒæ–¹é¢çš„éŸµå¾‹ï¼Œä»è€Œæé«˜æ•´ä½“å¯æ‡‚åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æœªè§å¤æ‚å¥å­å’Œè¯´è¯äººçš„å®éªŒä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶ç›´è§‚åœ°å±•ç¤ºäº†ProsodyFMåœ¨æ–­å¥å’Œè¯­è°ƒä¸Šçš„å¼ºå¤§å’Œç²¾ç»†å¯æ§æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³ä¸­çš„éŸµå¾‹åŒ…å«è¶…è¶Šå•è¯å­—é¢æ„ä¹‰çš„ä¿¡æ¯ï¼Œå¯¹è¯­éŸ³çš„å¯æ‡‚åº¦è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¨¡å‹åœ¨æ–­å¥å’Œè¯­è°ƒæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚é•¿å¥æ—¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„TTSæ¨¡å‹â€”â€”ProsodyFMï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæé«˜è¯­éŸ³çš„æ–­å¥å’Œè¯­è°ƒè¡¨ç°ã€‚</li>
<li>ProsodyFMå¼•å…¥çŸ­è¯­æ–­å¥ç¼–ç å™¨å’ŒæŒç»­æ—¶é—´é¢„æµ‹å™¨ï¼Œç”¨äºæ•æ‰å’Œè°ƒæ•´æ–­å¥ä½ç½®å’ŒæŒç»­æ—¶é—´ã€‚</li>
<li>ç»ˆç«¯è¯­è°ƒç¼–ç å™¨é›†æˆäº†è¯­è°ƒå½¢çŠ¶æ ‡è®°å’ŒéŸ³è°ƒå¤„ç†å™¨ï¼Œæ¨¡æ‹Ÿäººç±»è¯­è°ƒå˜åŒ–æ„ŸçŸ¥ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºProsodyFMèƒ½æé«˜è¯­éŸ³çš„æ™ºå¬æ€§å’Œæ•´ä½“å¯æ‡‚åº¦ï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc2700e586faf6c530d1991b60e0070b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-512fd61d7c098a56063450b371b8474d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a87b34706eca829194ec8aa82370d56c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b46770780f9c00a34107b6ee55fc5722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c32f83692e4bdf11e40a8cccac8d520.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Multilingual-and-Explainable-Text-Detoxification-with-Parallel-Corpora"><a href="#Multilingual-and-Explainable-Text-Detoxification-with-Parallel-Corpora" class="headerlink" title="Multilingual and Explainable Text Detoxification with Parallel Corpora"></a>Multilingual and Explainable Text Detoxification with Parallel Corpora</h2><p><strong>Authors:Daryna Dementieva, Nikolay Babakov, Amit Ronen, Abinew Ali Ayele, Naquee Rizwan, Florian Schneider, Xintong Wang, Seid Muhie Yimam, Daniil Moskovskiy, Elisei Stakovskii, Eran Kaufman, Ashraf Elnagar, Animesh Mukherjee, Alexander Panchenko</strong></p>
<p>Even with various regulations in place across countries and social media platforms (Government of India, 2021; European Parliament and Council of the European Union, 2022, digital abusive speech remains a significant issue. One potential approach to address this challenge is automatic text detoxification, a text style transfer (TST) approach that transforms toxic language into a more neutral or non-toxic form. To date, the availability of parallel corpora for the text detoxification task (Logachevavet al., 2022; Atwell et al., 2022; Dementievavet al., 2024a) has proven to be crucial for state-of-the-art approaches. With this work, we extend parallel text detoxification corpus to new languages â€“ German, Chinese, Arabic, Hindi, and Amharic â€“ testing in the extensive multilingual setup TST baselines. Next, we conduct the first of its kind an automated, explainable analysis of the descriptive features of both toxic and non-toxic sentences, diving deeply into the nuances, similarities, and differences of toxicity and detoxification across 9 languages. Finally, based on the obtained insights, we experiment with a novel text detoxification method inspired by the Chain-of-Thoughts reasoning approach, enhancing the prompting process through clustering on relevant descriptive attributes. </p>
<blockquote>
<p>å°½ç®¡å„å›½å’Œç¤¾äº¤åª’ä½“å¹³å°ï¼ˆå°åº¦æ”¿åºœï¼Œ2021å¹´ï¼›æ¬§æ´²è®®ä¼šå’Œæ¬§æ´²è”ç›Ÿç†äº‹ä¼šï¼Œ2022å¹´ï¼‰å·²ç»åˆ¶å®šäº†å„ç§è§„å®šï¼Œç½‘ç»œæ¬ºå‡Œè¨€è®ºä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§é—®é¢˜ã€‚è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„ä¸€ç§æ½œåœ¨æ–¹æ³•æ˜¯è‡ªåŠ¨æ–‡æœ¬å‡€åŒ–ï¼Œè¿™æ˜¯ä¸€ç§æ–‡æœ¬é£æ ¼è½¬æ¢ï¼ˆTSTï¼‰æ–¹æ³•ï¼Œå¯ä»¥å°†æœ‰æ¯’è¯­è¨€è½¬æ¢ä¸ºæ›´ä¸­æ€§æˆ–éæœ‰æ¯’çš„å½¢å¼ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œæ–‡æœ¬å‡€åŒ–ä»»åŠ¡çš„å¹³è¡Œè¯­æ–™åº“çš„å¯ç”¨æ€§ï¼ˆLogachevavetç­‰äººï¼Œ2022å¹´ï¼›Atwellç­‰äººï¼Œ2022å¹´ï¼›Dementievavetç­‰äººï¼Œ2024aï¼‰å·²è¢«è¯æ˜å¯¹æœ€æ–°æ–¹æ³•è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†å¹³è¡Œæ–‡æœ¬å‡€åŒ–è¯­æ–™åº“æ‰©å±•åˆ°æ–°è¯­è¨€â€”â€”å¾·è¯­ã€ç®€ä½“ä¸­æ–‡ã€é˜¿æ‹‰ä¼¯è¯­ã€å°åœ°è¯­å’Œé˜¿å§†å“ˆæ‹‰è¯­â€”â€”å¹¶åœ¨å¹¿æ³›çš„å¤šè¯­è¨€è®¾ç½®TSTåŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿›è¡Œäº†é¦–ä¾‹è‡ªåŠ¨åŒ–ã€å¯è§£é‡Šæ€§åˆ†æï¼Œç ”ç©¶æœ‰æ¯’å’Œæ— æ¯’å¥å­çš„æè¿°æ€§ç‰¹å¾ï¼Œæ·±å…¥æ¢ç©¶9ç§è¯­è¨€ä¸­æ¯’æ€§å’Œå‡€åŒ–ä¹‹é—´çš„ç»†å¾®å·®åˆ«ã€ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ã€‚æœ€åï¼ŒåŸºäºæ‰€è·å¾—çš„è§è§£ï¼Œæˆ‘ä»¬å°è¯•äº†ä¸€ç§å—æ€ç»´é“¾æ¨ç†æ–¹æ³•å¯å‘çš„æ–°å‹æ–‡æœ¬å‡€åŒ–æ–¹æ³•ï¼Œé€šè¿‡èšç±»ç›¸å…³çš„æè¿°æ€§å±æ€§æ¥å¢å¼ºæç¤ºè¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11691v1">PDF</a> COLING 2025, main conference, long</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºæ•°å­—æ»¥ç”¨è¨€è®ºä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§é—®é¢˜ï¼Œå°½ç®¡ä¸åŒå›½å®¶å’Œç¤¾äº¤åª’ä½“å¹³å°å·²ç»å®æ–½äº†å„ç§è§„å®šã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨æ–‡æœ¬å‡€åŒ–æ–¹æ³•ï¼Œå³æ–‡æœ¬é£æ ¼è½¬ç§»ï¼ˆTSTï¼‰ï¼Œå°†æœ‰æ¯’è¯­è¨€è½¬åŒ–ä¸ºæ›´ä¸­æ€§æˆ–éæœ‰æ¯’çš„å½¢å¼ã€‚æœ¬æ–‡æ‰©å±•äº†å¹³è¡Œæ–‡æœ¬å‡€åŒ–è¯­æ–™åº“ï¼Œæ”¯æŒå¤šç§æ–°è¯­è¨€ï¼Œå¹¶è¿›è¡Œäº†æœ‰æ¯’å’Œéå¸¸æœ‰æ¯’å¥å­çš„æè¿°ç‰¹å¾çš„è‡ªåŠ¨åŒ–å’Œå¯è§£é‡Šæ€§åˆ†æã€‚æœ€åï¼ŒåŸºäºæ‰€å¾—è§è§£ï¼Œå°è¯•äº†ä¸€ç§å—æ€ç»´é“¾å¯å‘çš„æ–°å‹æ–‡æœ¬å‡€åŒ–æ–¹æ³•ï¼Œé€šè¿‡èšç±»ç›¸å…³æè¿°å±æ€§æ¥å¢å¼ºæç¤ºè¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—æ»¥ç”¨è¨€è®ºä»ç„¶æ˜¯ä¸€ä¸ªæ™®éå­˜åœ¨çš„é—®é¢˜ï¼Œå°½ç®¡å„ä¸ªå›½å®¶å’Œç¤¾äº¤åª’ä½“å¹³å°å·²ç»å®æ–½äº†ç›¸å…³æ³•è§„ã€‚</li>
<li>è‡ªåŠ¨æ–‡æœ¬å‡€åŒ–æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³ç­–ç•¥ï¼Œå…¶ä¸­æ–‡æœ¬é£æ ¼è½¬ç§»ï¼ˆTSTï¼‰æ˜¯å°†æœ‰æ¯’è¯­è¨€è½¬åŒ–ä¸ºä¸­æ€§è¯­è¨€çš„æ–¹æ³•ã€‚</li>
<li>å¹³è¡Œè¯­æ–™åº“å¯¹äºæ–‡æœ¬å‡€åŒ–ä»»åŠ¡è‡³å…³é‡è¦ï¼Œæœ¬æ–‡æ‰©å±•äº†æ”¯æŒå¤šç§æ–°è¯­è¨€çš„å¹³è¡Œæ–‡æœ¬å‡€åŒ–è¯­æ–™åº“ã€‚</li>
<li>å¯¹æœ‰æ¯’å’Œéå¸¸æœ‰æ¯’å¥å­çš„æè¿°ç‰¹å¾è¿›è¡Œäº†è‡ªåŠ¨åŒ–å’Œå¯è§£é‡Šæ€§åˆ†æã€‚</li>
<li>æ·±å…¥ç ”ç©¶äº†ä¸åŒè¯­è¨€ä¸­æ¯’æ€§å’Œå‡€åŒ–ç»†å¾®å·®åˆ«å’Œç›¸ä¼¼ä¹‹å¤„ã€‚</li>
<li>åŸºäºæ‰€å¾—è§è§£ï¼Œå°è¯•äº†ä¸€ç§æ–°å‹æ–‡æœ¬å‡€åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å—æ€ç»´é“¾å¯å‘ï¼Œå¢å¼ºäº†æç¤ºè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11691">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48c965348786cafaef03c21849a74be7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cec37220e3b90e25654d7077e540e0a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03750e35336cf9957f0887061e20ee1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d15d72c2caef5bcb18d7a5c58982ca4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b157d2fd4c20255ac96c2a2c2f54684.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Region-Based-Optimization-in-Continual-Learning-for-Audio-Deepfake-Detection"><a href="#Region-Based-Optimization-in-Continual-Learning-for-Audio-Deepfake-Detection" class="headerlink" title="Region-Based Optimization in Continual Learning for Audio Deepfake   Detection"></a>Region-Based Optimization in Continual Learning for Audio Deepfake   Detection</h2><p><strong>Authors:Yujie Chen, Jiangyan Yi, Cunhang Fan, Jianhua Tao, Yong Ren, Siding Zeng, Chu Yuan Zhang, Xinrui Yan, Hao Gu, Jun Xue, Chenglong Wang, Zhao Lv, Xiaohui Zhang</strong></p>
<p>Rapid advancements in speech synthesis and voice conversion bring convenience but also new security risks, creating an urgent need for effective audio deepfake detection. Although current models perform well, their effectiveness diminishes when confronted with the diverse and evolving nature of real-world deepfakes. To address this issue, we propose a continual learning method named Region-Based Optimization (RegO) for audio deepfake detection. Specifically, we use the Fisher information matrix to measure important neuron regions for real and fake audio detection, dividing them into four regions. First, we directly fine-tune the less important regions to quickly adapt to new tasks. Next, we apply gradient optimization in parallel for regions important only to real audio detection, and in orthogonal directions for regions important only to fake audio detection. For regions that are important to both, we use sample proportion-based adaptive gradient optimization. This region-adaptive optimization ensures an appropriate trade-off between memory stability and learning plasticity. Additionally, to address the increase of redundant neurons from old tasks, we further introduce the Ebbinghaus forgetting mechanism to release them, thereby promoting the capability of the model to learn more generalized discriminative features. Experimental results show our method achieves a 21.3% improvement in EER over the state-of-the-art continual learning approach RWM for audio deepfake detection. Moreover, the effectiveness of RegO extends beyond the audio deepfake detection domain, showing potential significance in other tasks, such as image recognition. The code is available at <a target="_blank" rel="noopener" href="https://github.com/cyjie429/RegO">https://github.com/cyjie429/RegO</a> </p>
<blockquote>
<p>éšç€è¯­éŸ³åˆæˆå’Œè¯­éŸ³è½¬æ¢çš„å¿«é€Ÿå‘å±•ï¼Œè™½ç„¶ä¸ºäººä»¬çš„ç”Ÿæ´»å¸¦æ¥äº†ä¾¿åˆ©ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨é£é™©ï¼Œè¿™å¼•å‘äº†å¯¹äºæœ‰æ•ˆéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æŠ€æœ¯çš„è¿«åˆ‡éœ€æ±‚ã€‚å½“å‰æ¨¡å‹åœ¨å®é™…åº”å¯¹å¤šæ ·åŒ–ä¸”ä¸æ–­å‘å±•çš„æ·±ä¼ªé€ éŸ³é¢‘æ—¶ï¼Œå…¶æ•ˆæœæœ‰æ‰€å‡å¼±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼Œåä¸ºåŸºäºåŒºåŸŸçš„ä¼˜åŒ–ï¼ˆRegOï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨Fisherä¿¡æ¯çŸ©é˜µæ¥è¡¡é‡çœŸå®å’Œä¼ªé€ éŸ³é¢‘æ£€æµ‹çš„é‡è¦ç¥ç»å…ƒåŒºåŸŸï¼Œå¹¶å°†å…¶åˆ’åˆ†ä¸ºå››ä¸ªåŒºåŸŸã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç›´æ¥å¾®è°ƒä¸å¤ªé‡è¦çš„åŒºåŸŸï¼Œä»¥å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¯¹ä»…å¯¹çœŸå®éŸ³é¢‘æ£€æµ‹é‡è¦çš„åŒºåŸŸè¿›è¡Œå¹¶è¡Œæ¢¯åº¦ä¼˜åŒ–ï¼Œè€Œå¯¹ä»…å¯¹ä¼ªé€ éŸ³é¢‘æ£€æµ‹é‡è¦çš„åŒºåŸŸè¿›è¡Œæ­£äº¤æ–¹å‘ä¸Šçš„æ¢¯åº¦ä¼˜åŒ–ã€‚å¯¹äºä¸¤è€…éƒ½é‡è¦çš„åŒºåŸŸï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºæ ·æœ¬æ¯”ä¾‹çš„è‡ªé€‚åº”æ¢¯åº¦ä¼˜åŒ–ã€‚è¿™ç§åŒºåŸŸè‡ªé€‚åº”ä¼˜åŒ–ç¡®ä¿äº†å†…å­˜ç¨³å®šæ€§å’Œå­¦ä¹ å¯å¡‘æ€§ä¹‹é—´çš„é€‚å½“å¹³è¡¡ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³æ—§ä»»åŠ¡ä¸­å†—ä½™ç¥ç»å…ƒçš„å¢åŠ é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è‰¾å®¾æµ©æ–¯é—å¿˜æœºåˆ¶æ¥é‡Šæ”¾å®ƒä»¬ï¼Œä»è€Œä¿ƒè¿›æ¨¡å‹å­¦ä¹ æ›´é€šç”¨çš„è¾¨åˆ«ç‰¹å¾çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢æ¯”æœ€æ–°çš„æŒç»­å­¦ä¹ æ–¹æ³•RWMåœ¨EERä¸Šæé«˜äº†21.3%ã€‚æ­¤å¤–ï¼ŒRegOçš„æœ‰æ•ˆæ€§ä¸ä»…é€‚ç”¨äºéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸï¼Œåœ¨å›¾åƒè¯†åˆ«ç­‰å…¶ä»–ä»»åŠ¡ä¸­ä¹Ÿæ˜¾ç¤ºå‡ºæ½œåœ¨çš„é‡è¦æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cyjie429/RegO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cyjie429/RegOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11551v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€è¯­éŸ³åˆæˆå’Œè¯­éŸ³è½¬æ¢æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹é¢ä¸´æ–°çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰æ¨¡å‹é¢å¯¹å¤šæ ·åŒ–å’Œä¸æ–­å‘å±•çš„ç°å®æ·±åº¦ä¼ªé€ æ—¶æ•ˆæœä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºRegion-Based Optimizationï¼ˆRegOï¼‰çš„æŒç»­å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡Fisherä¿¡æ¯çŸ©é˜µè¡¡é‡çœŸå®å’Œä¼ªé€ éŸ³é¢‘æ£€æµ‹çš„é‡è¦ç¥ç»å…ƒåŒºåŸŸï¼Œå¹¶å¯¹å…¶è¿›è¡Œåˆ†åŒºä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºæœ€æ–°æŒç»­å­¦ä¹ æ–¹æ³•RWMï¼ŒRegOåœ¨éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸Šçš„EERé™ä½äº†21.3%ï¼Œå¹¶ä¸”åœ¨å›¾åƒè¯†åˆ«ç­‰å…¶ä»–ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºæ½œåœ¨çš„é‡è¦æ€§ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åˆæˆå’Œè¯­éŸ³è½¬æ¢æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ–°æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹é¢å¯¹ä¸æ–­æ¼”å˜çš„ä¼ªé€ æŠ€æœ¯æ—¶æ•ˆæœæœ‰å¾…æé«˜ã€‚</li>
<li>Region-Based Optimizationï¼ˆRegOï¼‰æ˜¯ä¸€ç§é’ˆå¯¹éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æå‡ºçš„æŒç»­å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>RegOä½¿ç”¨Fisherä¿¡æ¯çŸ©é˜µè¡¡é‡çœŸå®å’Œä¼ªé€ éŸ³é¢‘æ£€æµ‹çš„é‡è¦ç¥ç»å…ƒåŒºåŸŸï¼Œå¹¶è¿›è¡Œåˆ†åŒºä¼˜åŒ–ã€‚</li>
<li>RegOåœ¨éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸Šçš„æ€§èƒ½è¾ƒæœ€æ–°æŒç»­å­¦ä¹ æ–¹æ³•æœ‰æ‰€æå‡ï¼Œä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</li>
<li>RegOçš„æ•ˆèƒ½ä¸ä»…é™äºéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼Œè¿˜å¯èƒ½åº”ç”¨äºå…¶ä»–ä»»åŠ¡å¦‚å›¾åƒè¯†åˆ«ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ec3916cda4db7fb3ba8a07bec82bb82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce4233cfb3e86f813abe9b7524e07414.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53d98381ea03abefb8a4eb8305f085b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d11926a2480a9f1568c949c3bfc92879.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e460503f63d6a801c5ea8127a342ce19.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-a-Speech-Foundation-Model-for-Singapore-and-Beyond"><a href="#Towards-a-Speech-Foundation-Model-for-Singapore-and-Beyond" class="headerlink" title="Towards a Speech Foundation Model for Singapore and Beyond"></a>Towards a Speech Foundation Model for Singapore and Beyond</h2><p><strong>Authors:Muhammad Huzaifah, Tianchi Liu, Hardik B. Sailor, Kye Min Tan, Tarun K. Vangani, Qiongqiong Wang, Jeremy H. M. Wong, Nancy F. Chen, Ai Ti Aw</strong></p>
<p>This technical report describes the MERaLiON Speech Encoder, a foundation model designed to support a wide range of downstream speech applications. Developed as part of Singaporeâ€™s National Multimodal Large Language Model Programme, the MERaLiON Speech Encoder is tailored to address the speech processing needs in Singapore and the surrounding Southeast Asian region. The model currently supports mainly English, including the variety spoken in Singapore. We are actively expanding our datasets to gradually cover other languages in subsequent releases. The MERaLiON Speech Encoder was pre-trained from scratch on 200K hours of unlabelled speech data using a self-supervised learning approach based on masked language modelling. We describe our training procedure and hyperparameter tuning experiments in detail below. Our evaluation demonstrates improvements to spontaneous and Singapore speech benchmarks for speech recognition, while remaining competitive to other state-of-the-art speech encoders across ten other speech tasks. We commit to releasing our model, supporting broader research endeavours, both in Singapore and beyond. </p>
<blockquote>
<p>æœ¬æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†MERaLiONè¯­éŸ³ç¼–ç å™¨ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨æ”¯æŒå¹¿æ³›çš„ä¸‹æ¸¸è¯­éŸ³åº”ç”¨ç¨‹åºã€‚ä½œä¸ºæ–°åŠ å¡å›½å®¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è®¡åˆ’çš„ä¸€éƒ¨åˆ†è€Œå¼€å‘ï¼ŒMERaLiONè¯­éŸ³ç¼–ç å™¨é’ˆå¯¹æ–°åŠ å¡åŠå‘¨è¾¹ä¸œå—äºšåœ°åŒºçš„è¯­éŸ³å¤„ç†éœ€æ±‚è¿›è¡Œäº†å®šåˆ¶ã€‚è¯¥æ¨¡å‹ç›®å‰ä¸»è¦æ”¯æŒè‹±è¯­ï¼ŒåŒ…æ‹¬æ–°åŠ å¡æ‰€è¯´çš„è‹±è¯­ã€‚æˆ‘ä»¬æ­£åœ¨ç§¯ææ‰©å……æ•°æ®é›†ï¼Œåœ¨åç»­ç‰ˆæœ¬ä¸­é€æ­¥è¦†ç›–å…¶ä»–è¯­è¨€ã€‚MERaLiONè¯­éŸ³ç¼–ç å™¨ä½¿ç”¨åŸºäºæ©ç è¯­è¨€å»ºæ¨¡çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œåœ¨20ä¸‡å°æ—¶çš„æ— æ ‡ç­¾è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œä»å¤´è®­ç»ƒã€‚æˆ‘ä»¬ä¸‹é¢è¯¦ç»†æè¿°äº†æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹å’Œè¶…å‚æ•°è°ƒæ•´å®éªŒã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œåœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢ï¼Œå¯¹è‡ªå‘è¯­éŸ³å’Œæ–°åŠ å¡è¯­éŸ³åŸºå‡†æµ‹è¯•æœ‰äº†æ”¹è¿›ï¼ŒåŒæ—¶åœ¨å…¶ä»–åä¸ªè¯­éŸ³ä»»åŠ¡ä¸Šä¸å…¶ä»–æœ€å…ˆç«¯çš„è¯­éŸ³ç¼–ç å™¨ä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬è‡´åŠ›äºå‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥æ”¯æŒæ–°åŠ å¡å†…å¤–çš„æ›´å¹¿æ³›ç ”ç©¶åŠªåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11538v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MERaLiONè¯­éŸ³ç¼–ç å™¨æ˜¯ä¸€æ¬¾åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨æ”¯æŒå„ç§ä¸‹æ¸¸è¯­éŸ³åº”ç”¨ã€‚è¯¥æ¨¡å‹ä¸“ä¸ºæ–°åŠ å¡åŠå‘¨è¾¹ä¸œå—äºšåœ°åŒºçš„è¯­éŸ³å¤„ç†éœ€æ±‚è®¾è®¡ï¼Œç›®å‰ä¸»è¦æ”¯æŒè‹±è¯­ï¼ŒåŒ…æ‹¬æ–°åŠ å¡è‹±è¯­ã€‚æ¨¡å‹é‡‡ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ŒåŸºäºæ©ç è¯­è¨€å»ºæ¨¡ï¼Œåœ¨20ä¸‡å°æ—¶çš„æ— æ ‡ç­¾è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢å¯¹è‡ªç„¶å’Œæ–°åŠ å¡è¯­éŸ³åŸºå‡†æµ‹è¯•æœ‰æ‰€æ”¹å–„ï¼ŒåŒæ—¶åœ¨å…¶ä»–åä¸ªè¯­éŸ³ä»»åŠ¡ä¸Šä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬æ‰¿è¯ºå‘å¸ƒæ¨¡å‹ï¼Œä»¥æ”¯æŒæ–°åŠ å¡åŠå…¨çƒæ›´å¹¿æ³›çš„ç ”ç©¶å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MERaLiONè¯­éŸ³ç¼–ç å™¨æ˜¯ä¸€ä¸ªä¸ºæ–°åŠ å¡å’Œä¸œå—äºšåœ°åŒºè®¾è®¡çš„è¯­éŸ³å¤„ç†åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ä¸»è¦æ”¯æŒè‹±è¯­ï¼Œå¹¶æ­£åœ¨ç§¯ææ‰©å±•ä»¥è¦†ç›–å…¶ä»–è¯­è¨€ã€‚</li>
<li>é‡‡ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ŒåŸºäºæ©ç è¯­è¨€å»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿ç”¨å¤§é‡æ— æ ‡ç­¾è¯­éŸ³æ•°æ®ã€‚</li>
<li>æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯¹è‡ªç„¶å’Œæ–°åŠ å¡è¯­éŸ³åŸºå‡†æµ‹è¯•æœ‰æ‰€æ”¹å–„ã€‚</li>
<li>åœ¨å…¶ä»–åä¸ªè¯­éŸ³ä»»åŠ¡ä¸Šï¼Œè¯¥æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>MERaLiONè¯­éŸ³ç¼–ç å™¨å°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„ç ”ç©¶å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11538">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c139562300768c4a64576df466401a42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6112112f6c4fa2c2ce6d85e6cb361186.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-154dc4e1feeeabc682b3228afd5de5a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73fa6932ae0b55250cc32ae7844615f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f6684c46714115f26171b98273cadf3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Transliterated-Zero-Shot-Domain-Adaptation-for-Automatic-Speech-Recognition"><a href="#Transliterated-Zero-Shot-Domain-Adaptation-for-Automatic-Speech-Recognition" class="headerlink" title="Transliterated Zero-Shot Domain Adaptation for Automatic Speech   Recognition"></a>Transliterated Zero-Shot Domain Adaptation for Automatic Speech   Recognition</h2><p><strong>Authors:Han Zhu, Gaofeng Cheng, Qingwei Zhao, Pengyuan Zhang</strong></p>
<p>The performance of automatic speech recognition models often degenerates on domains not covered by the training data. Domain adaptation can address this issue, assuming the availability of the target domain data in the target language. However, such assumption does not stand in many real-world applications. To make domain adaptation more applicable, we address the problem of zero-shot domain adaptation (ZSDA), where target domain data is unavailable in the target language. Instead, we transfer the target domain knowledge from another source language where the target domain data is more accessible. To do that, we first perform cross-lingual pre-training (XLPT) to share domain knowledge across languages, then use target language fine-tuning to build the final model. One challenge in this practice is that the pre-trained knowledge can be forgotten during fine-tuning, resulting in sub-optimal adaptation performance. To address this issue, we propose transliterated ZSDA to achieve consistent pre-training and fine-tuning labels, leading to maximum preservation of the pre-trained knowledge. Experimental results show that transliterated ZSDA relatively decreases the word error rate by 9.2% compared with a wav2vec 2.0 baseline. Moreover, transliterated ZSDA consistently outperforms self-supervised ZSDA and performs on par with supervised ZSDA, proving the superiority of transliteration-based pre-training labels. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®æœªæ¶µç›–çš„é¢†åŸŸä¸Šçš„æ€§èƒ½å¾€å¾€ä¼šé€€åŒ–ã€‚é¢†åŸŸé€‚é…ï¼ˆDomain Adaptationï¼‰å¯ä»¥è§£å†³æ­¤é—®é¢˜ï¼Œå‰ææ˜¯ç›®æ ‡é¢†åŸŸæ•°æ®ä»¥ç›®æ ‡è¯­è¨€çš„å½¢å¼å¯ç”¨ã€‚ç„¶è€Œï¼Œè¿™ä¸€å‡è®¾å¹¶ä¸é€‚ç”¨äºè®¸å¤šç°å®ä¸–ç•Œçš„åº”ç”¨åœºæ™¯ã€‚ä¸ºäº†ä½¿é¢†åŸŸé€‚é…æ›´å…·é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬è§£å†³äº†é›¶å°„å‡»é¢†åŸŸé€‚é…ï¼ˆZero-Shot Domain Adaptationï¼ŒZSDAï¼‰çš„é—®é¢˜ï¼Œå…¶ä¸­ç›®æ ‡é¢†åŸŸçš„æ•°æ®åœ¨ç›®æ ‡è¯­è¨€ä¸­æ— æ³•ä½¿ç”¨ã€‚ç›¸åï¼Œæˆ‘ä»¬ä»å¦ä¸€ç§æºè¯­è¨€ä¸­è½¬ç§»ç›®æ ‡é¢†åŸŸçš„çŸ¥è¯†ï¼Œå…¶ä¸­ç›®æ ‡é¢†åŸŸçš„æ•°æ®æ›´å®¹æ˜“è·å¾—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆè¦æ‰§è¡Œè·¨è¯­è¨€é¢„è®­ç»ƒï¼ˆCross-Lingual Pre-Trainingï¼ŒXLPTï¼‰ï¼Œä»¥è·¨è¯­è¨€å…±äº«é¢†åŸŸçŸ¥è¯†ï¼Œç„¶åä½¿ç”¨ç›®æ ‡è¯­è¨€å¾®è°ƒæ¥æ„å»ºæœ€ç»ˆæ¨¡å‹ã€‚è¿™ä¸€å®è·µä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼Œé¢„è®­ç»ƒçŸ¥è¯†å¯èƒ½ä¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¢«é—å¿˜ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„é€‚é…æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºéŸ³è¯‘ZSDAæ–¹æ³•ï¼Œä»¥å®ç°é¢„è®­ç»ƒå’Œå¾®è°ƒæ ‡ç­¾çš„ä¸€è‡´æ€§ï¼Œæœ€å¤§é™åº¦åœ°ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸wav2vec 2.0åŸºçº¿ç›¸æ¯”ï¼ŒéŸ³è¯‘ZSDAç›¸å¯¹é™ä½äº†9.2%çš„å•è¯é”™è¯¯ç‡ã€‚æ­¤å¤–ï¼ŒéŸ³è¯‘ZSDAå§‹ç»ˆä¼˜äºè‡ªç›‘ç£ZSDAï¼Œå¹¶ä¸ç›‘ç£ZSDAè¡¨ç°ç›¸å½“ï¼Œè¯æ˜äº†åŸºäºéŸ³è¯‘çš„é¢„è®­ç»ƒæ ‡ç­¾çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11185v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è·¨é¢†åŸŸè¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨æœªè¢«è®­ç»ƒæ•°æ®è¦†ç›–çš„é¢†åŸŸæ€§èƒ½ä¼šé€€åŒ–ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶æå‡ºé›¶æ ·æœ¬é¢†åŸŸè‡ªé€‚åº”ï¼ˆZSDAï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¦ä¸€ç§æºè¯­è¨€è½¬ç§»ç›®æ ‡é¢†åŸŸçŸ¥è¯†ï¼Œå½“ç›®æ ‡é¢†åŸŸæ•°æ®åœ¨è¯¥æºè¯­è¨€ä¸­æ›´å®¹æ˜“è·å–æ—¶ã€‚é€šè¿‡è·¨è¯­è¨€é¢„è®­ç»ƒï¼ˆXLPTï¼‰å…±äº«é¢†åŸŸçŸ¥è¯†ï¼Œç„¶åé’ˆå¯¹ç›®æ ‡è¯­è¨€è¿›è¡Œå¾®è°ƒæ„å»ºæœ€ç»ˆæ¨¡å‹ã€‚é¢„è®­ç»ƒçŸ¥è¯†åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šé—å¿˜çš„é—®é¢˜æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå¯¼è‡´é€‚åº”æ€§èƒ½ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºé‡‡ç”¨éŸ³è¯‘ZSDAæ–¹æ³•ï¼Œå®ç°é¢„è®­ç»ƒå’Œå¾®è°ƒæ ‡ç­¾çš„ä¸€è‡´æ€§ï¼Œæœ€å¤§é™åº¦åœ°ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒéŸ³è¯‘ZSDAç›¸è¾ƒäºwav2vec 2.0åŸºçº¿é™ä½äº†9.2%çš„è¯é”™è¯¯ç‡ã€‚æ­¤å¤–ï¼ŒéŸ³è¯‘ZSDAè¡¨ç°ä¼˜äºè‡ªç›‘ç£ZSDAï¼Œä¸ç›‘ç£ZSDAè¡¨ç°ç›¸å½“ï¼Œè¯æ˜äº†éŸ³è¯‘é¢„è®­ç»ƒæ ‡ç­¾çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨æœªçŸ¥é¢†åŸŸæ€§èƒ½é€€åŒ–é—®é¢˜å¯é€šè¿‡é¢†åŸŸè‡ªé€‚åº”è§£å†³ã€‚</li>
<li>ZSDAæ–¹æ³•æ—¨åœ¨è§£å†³ç›®æ ‡é¢†åŸŸæ•°æ®åœ¨ç›®æ ‡è¯­è¨€ä¸­ä¸å¯ç”¨çš„é—®é¢˜ï¼Œé€šè¿‡å¦ä¸€ç§æºè¯­è¨€è½¬ç§»çŸ¥è¯†ã€‚</li>
<li>è·¨è¯­è¨€é¢„è®­ç»ƒï¼ˆXLPTï¼‰è¢«ç”¨äºåœ¨ä¸åŒè¯­è¨€é—´å…±äº«é¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>å¾®è°ƒè¿‡ç¨‹ä¸­çš„é¢„è®­ç»ƒçŸ¥è¯†é—å¿˜æ˜¯ZSDAå®è·µä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„éŸ³è¯‘ZSDAæ–¹æ³•èƒ½æœ€å¤§ç¨‹åº¦ä¸Šä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ï¼Œæé«˜é€‚åº”æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºéŸ³è¯‘ZSDAç›¸æ¯”åŸºçº¿é™ä½äº†è¯é”™è¯¯ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-846467b0a09ad4318b5527181bc261fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8feba5d2d7d064a95f24eea0ce49904f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e9f312a130de2a8ac7e86057ee0a912.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3792a32af376d913d530207351c44bcb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Generative-Modeling-with-Residual-Vector-Quantization-Based-Tokens"><a href="#Efficient-Generative-Modeling-with-Residual-Vector-Quantization-Based-Tokens" class="headerlink" title="Efficient Generative Modeling with Residual Vector Quantization-Based   Tokens"></a>Efficient Generative Modeling with Residual Vector Quantization-Based   Tokens</h2><p><strong>Authors:Jaehyeon Kim, Taehong Moon, Keon Lee, Jaewoong Cho</strong></p>
<p>We explore the use of Residual Vector Quantization (RVQ) for high-fidelity generation in vector-quantized generative models. This quantization technique maintains higher data fidelity by employing more in-depth tokens. However, increasing the token number in generative models leads to slower inference speeds. To this end, we introduce ResGen, an efficient RVQ-based discrete diffusion model that generates high-fidelity samples without compromising sampling speed. Our key idea is a direct prediction of vector embedding of collective tokens rather than individual ones. Moreover, we demonstrate that our proposed token masking and multi-token prediction method can be formulated within a principled probabilistic framework using a discrete diffusion process and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation} on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models. The project page can be found at <a target="_blank" rel="noopener" href="https://resgen-genai.github.io/">https://resgen-genai.github.io</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢äº†æ®‹å·®å‘é‡é‡åŒ–ï¼ˆRVQï¼‰åœ¨å‘é‡é‡åŒ–ç”Ÿæˆæ¨¡å‹ä¸­çš„é«˜ä¿çœŸç”Ÿæˆåº”ç”¨ã€‚è¿™ç§é‡åŒ–æŠ€æœ¯é€šè¿‡é‡‡ç”¨æ›´æ·±å…¥çš„ä»¤ç‰Œæ¥ä¿æŒæ›´é«˜çš„æ•°æ®ä¿çœŸåº¦ã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿæˆæ¨¡å‹ä¸­å¢åŠ ä»¤ç‰Œæ•°é‡ä¼šå¯¼è‡´æ¨ç†é€Ÿåº¦å˜æ…¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ResGenï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºRVQçš„é«˜æ•ˆç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²é‡‡æ ·é€Ÿåº¦çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜ä¿çœŸæ ·æœ¬ã€‚æˆ‘ä»¬çš„å…³é”®æƒ³æ³•æ˜¯é¢„æµ‹é›†ä½“ä»¤ç‰Œçš„å‘é‡åµŒå…¥ï¼Œè€Œä¸æ˜¯å•ä¸ªä»¤ç‰Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡ç¦»æ•£æ‰©æ•£è¿‡ç¨‹å’Œå˜åˆ†æ¨æ–­ï¼Œæˆ‘ä»¬æå‡ºçš„ä»¤ç‰Œæ©ç å’Œå¤šä»¤ç‰Œé¢„æµ‹æ–¹æ³•å¯ä»¥åœ¨æœ‰åŸåˆ™çš„æ¦‚ç‡æ¡†æ¶å†…åˆ¶å®šã€‚æˆ‘ä»¬åœ¨ä¸åŒæ¨¡æ€çš„ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸ŠéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼šåœ¨ImageNet 256x256ä¸Šè¿›è¡Œæ¡ä»¶å›¾åƒç”Ÿæˆå’Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒResGenåœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸­çš„è¡¨ç°éƒ½ä¼˜äºè‡ªå›å½’æ¨¡å‹ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶æ²¡æœ‰ç‰ºç‰²é‡‡æ ·é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œéšç€æˆ‘ä»¬æ‰©å¤§RVQçš„æ·±åº¦ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹ä¸ç±»ä¼¼è§„æ¨¡çš„åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„ç”Ÿæˆä¿çœŸåº¦æˆ–æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://resgen-genai.github.ioæ‰¾åˆ°./">https://resgen-genai.github.ioæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10208v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºResidual Vector Quantizationï¼ˆRVQï¼‰çš„é«˜ä¿çœŸç”Ÿæˆåœ¨å‘é‡é‡åŒ–ç”Ÿæˆæ¨¡å‹ä¸­å¾—åˆ°äº†æ¢ç´¢ã€‚è¯¥æŠ€æœ¯é€šè¿‡é‡‡ç”¨æ›´æ·±å…¥çš„ä»¤ç‰Œæ¥ä¿æŒæ›´é«˜çš„æ•°æ®ä¿çœŸåº¦ï¼Œä½†å¢åŠ ç”Ÿæˆæ¨¡å‹ä¸­çš„ä»¤ç‰Œæ•°é‡ä¼šå¯¼è‡´æ¨ç†é€Ÿåº¦å˜æ…¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ResGenï¼Œè¿™æ˜¯ä¸€ç§åŸºäºRVQçš„é«˜æ•ˆç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œå¯åœ¨ä¸ç‰ºç‰²é‡‡æ ·é€Ÿåº¦çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜ä¿çœŸæ ·æœ¬ã€‚æˆ‘ä»¬çš„å…³é”®æƒ³æ³•æ˜¯é¢„æµ‹é›†ä½“ä»¤ç‰Œçš„å‘é‡åµŒå…¥ï¼Œè€Œä¸æ˜¯å•ç‹¬çš„ä»¤ç‰Œã€‚æˆ‘ä»¬åœ¨ç¦»æ•£æ‰©æ•£è¿‡ç¨‹å’Œå˜åˆ†æ¨ç†çš„æ¦‚ç‡æ¡†æ¶ä¸‹ï¼Œè¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ç”¨äºè·¨ä¸åŒæ¨¡æ€çš„ä¸¤ç§æŒ‘æˆ˜ä»»åŠ¡ï¼šåœ¨ImageNet 256x256ä¸Šè¿›è¡Œæ¡ä»¶å›¾åƒç”Ÿæˆå’Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒResGenåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºè‡ªå›å½’æ¨¡å‹ï¼Œä¸”åœ¨ä¸ç‰ºç‰²é‡‡æ ·é€Ÿåº¦çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚éšç€RVQæ·±åº¦çš„å¢åŠ ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆæ¨¡å‹çš„ç”Ÿæˆä¿çœŸåº¦å¾—åˆ°äº†æé«˜æˆ–é‡‡æ ·é€Ÿåº¦å¾—åˆ°äº†åŠ å¿«ï¼Œä¸ç±»ä¼¼è§„æ¨¡çš„åŸºå‡†æ¨¡å‹ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Residual Vector Quantizationï¼ˆRVQï¼‰è¢«ç”¨äºå‘é‡é‡åŒ–ç”Ÿæˆæ¨¡å‹ä¸­ä»¥å®ç°é«˜ä¿çœŸç”Ÿæˆã€‚</li>
<li>å¢åŠ ä»¤ç‰Œæ•°é‡å¯ä»¥æé«˜æ•°æ®ä¿çœŸåº¦ï¼Œä½†ä¼šå¯¼è‡´æ¨ç†é€Ÿåº¦ä¸‹é™ã€‚</li>
<li>ResGenæ˜¯ä¸€ç§åŸºäºRVQçš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨åœ¨ä¸ç‰ºç‰²é‡‡æ ·é€Ÿåº¦çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜ä¿çœŸæ ·æœ¬ã€‚</li>
<li>ResGené€šè¿‡é¢„æµ‹é›†ä½“ä»¤ç‰Œçš„å‘é‡åµŒå…¥æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨æ¡ä»¶å›¾åƒç”Ÿæˆå’Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸¤é¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>ResGenç›¸è¾ƒäºè‡ªå›å½’æ¨¡å‹æœ‰ä¼˜è¶Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31cd55db7de65f71324b8e938087bd00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15d33db3d4919e0e7ddfb506a4b75f12.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-Content-and-Acoustic-Representations-for-Speech-Emotion-Recognition"><a href="#Leveraging-Content-and-Acoustic-Representations-for-Speech-Emotion-Recognition" class="headerlink" title="Leveraging Content and Acoustic Representations for Speech Emotion   Recognition"></a>Leveraging Content and Acoustic Representations for Speech Emotion   Recognition</h2><p><strong>Authors:Soumya Dutta, Sriram Ganapathy</strong></p>
<p>Speech emotion recognition (SER), the task of identifying the expression of emotion from spoken content, is challenging due to the difficulty in extracting representations that capture emotional attributes from speech. The scarcity of labeled datasets further complicates the challenge where large models are prone to over-fitting. In this paper, we propose CARE (Content and Acoustic Representations of Emotions), where we design a dual encoding scheme which emphasizes semantic and acoustic factors of speech. While the semantic encoder is trained using distillation from utterance-level text representations, the acoustic encoder is trained to predict low-level frame-wise features of the speech signal. The proposed dual encoding scheme is a base-sized model trained only on unsupervised raw speech. With a simple light-weight classification model trained on the downstream task, we show that the CARE embeddings provide effective emotion recognition on a variety of datasets. We compare the proposal with several other self-supervised models as well as recent large-language model based approaches. In these evaluations, the proposed CARE is shown to be the best performing model based on average performance across 8 diverse datasets. We also conduct several ablation studies to analyze the importance of various design choices. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ˜¯ä»å£è¯­å†…å®¹ä¸­è¯†åˆ«æƒ…æ„Ÿè¡¨è¾¾çš„ä»»åŠ¡ï¼Œç”±äºä»è¯­éŸ³ä¸­æå–èƒ½æ•æ‰æƒ…æ„Ÿå±æ€§çš„è¡¨ç¤ºå½¢å¼å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› æ­¤è¿™ä¸€ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ ‡è®°æ•°æ®é›†çš„ç¨€ç¼ºè¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œå¤§å‹æ¨¡å‹å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºCAREï¼ˆæƒ…æ„Ÿçš„å†…å®¹å’Œå£°å­¦è¡¨ç¤ºï¼‰ï¼Œè®¾è®¡äº†ä¸€ç§åŒç¼–ç æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä¾§é‡äºè¯­éŸ³çš„è¯­ä¹‰å’Œå£°å­¦å› ç´ ã€‚è¯­ä¹‰ç¼–ç å™¨ä½¿ç”¨è’¸é¦æ³•ä»è¯è¯­çº§åˆ«çš„æ–‡æœ¬è¡¨ç¤ºä¸­è¿›è¡Œè®­ç»ƒï¼Œè€Œå£°å­¦ç¼–ç å™¨åˆ™ç»è¿‡è®­ç»ƒä»¥é¢„æµ‹è¯­éŸ³ä¿¡å·çš„ä½çº§å¸§çº§ç‰¹å¾ã€‚æ‰€æå‡ºçš„åŒç¼–ç æ–¹æ¡ˆæ˜¯ä¸€ä¸ªåŸºç¡€å¤§å°çš„æ¨¡å‹ï¼Œä»…åœ¨æœ‰ç›‘ç£çš„åŸå§‹è¯­éŸ³ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè®­ç»ƒçš„ç®€å•è½»é‡çº§åˆ†ç±»æ¨¡å‹ï¼Œæˆ‘ä»¬è¯æ˜äº†CAREåµŒå…¥åœ¨å„ç§æ•°æ®é›†ä¸Šæä¾›äº†æœ‰æ•ˆçš„æƒ…æ„Ÿè¯†åˆ«ã€‚æˆ‘ä»¬å°†ææ¡ˆä¸å…¶ä»–è‡ªç›‘ç£æ¨¡å‹ä»¥åŠæœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨è¿™äº›è¯„ä¼°ä¸­ï¼ŒCAREè¢«è¯æ˜æ˜¯åœ¨8ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å‡ é¡¹æ¶ˆèç ”ç©¶ï¼Œä»¥åˆ†æå„ç§è®¾è®¡é€‰æ‹©çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05566v2">PDF</a> 11 pages, 5 figures, 6 tables</p>
<p><strong>Summary</strong><br>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ˜¯ä»å£è¯­å†…å®¹ä¸­è¯†åˆ«æƒ…æ„Ÿè¡¨è¾¾çš„ä»»åŠ¡ï¼Œç”±äºä»è¯­éŸ³ä¸­æå–èƒ½æ•æ‰æƒ…æ„Ÿå±æ€§çš„è¡¨ç¤ºå½¢å¼å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› æ­¤è¯¥ä»»åŠ¡å……æ»¡è‰°è¾›ã€‚æ­¤å¤–ï¼Œç¼ºä¹æ ‡è®°æ•°æ®é›†ä¹Ÿä½¿è¿™ä¸€æŒ‘æˆ˜æ›´åŠ å¤æ‚ï¼Œå¤§å‹æ¨¡å‹å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚æœ¬æ–‡æå‡ºäº†CAREï¼ˆæƒ…æ„Ÿçš„å†…å®¹å’Œå£°å­¦è¡¨ç¤ºï¼‰ï¼Œè®¾è®¡äº†ä¸€ç§åŒç¼–ç æ–¹æ¡ˆï¼Œå¼ºè°ƒè¯­éŸ³çš„è¯­ä¹‰å’Œå£°å­¦å› ç´ ã€‚è¯­ä¹‰ç¼–ç å™¨é€šè¿‡è’¸é¦çš„æ–¹å¼ä½¿ç”¨è¯è¯­çº§åˆ«çš„æ–‡æœ¬è¡¨ç¤ºè¿›è¡Œè®­ç»ƒï¼Œè€Œå£°å­¦ç¼–ç å™¨åˆ™è®­ç»ƒç”¨äºé¢„æµ‹è¯­éŸ³ä¿¡å·çš„ä½çº§å¸§çº§ç‰¹å¾ã€‚æ‰€æå‡ºçš„åŒç¼–ç æ–¹æ¡ˆæ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œä»…ä½¿ç”¨æ— ç›‘ç£çš„åŸå§‹è¯­éŸ³è¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨ç®€å•çš„è½»é‡çº§åˆ†ç±»æ¨¡å‹å¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å±•ç¤ºäº†CAREåµŒå…¥åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæƒ…æ„Ÿè¯†åˆ«ã€‚æˆ‘ä»¬å°†ææ¡ˆä¸å…¶ä»–çš„è‡ªæˆ‘ç›‘ç£æ¨¡å‹ä»¥åŠæœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨è¿™äº›è¯„ä¼°ä¸­ï¼Œæ‰€æå‡ºçš„CAREè¢«è¯æ˜æ˜¯åœ¨8ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„æœ€ä½³æ€§èƒ½æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å‡ é¡¹åºŸé™¤ç ”ç©¶ï¼Œä»¥åˆ†æå„ç§è®¾è®¡é€‰æ‹©çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é¢ä¸´ä»è¯­éŸ³ä¸­æå–æƒ…æ„Ÿå±æ€§è¡¨ç¤ºçš„å›°éš¾ã€‚</li>
<li>ç¼ºä¹æ ‡è®°æ•°æ®é›†ä½¿SERä»»åŠ¡æ›´åŠ å¤æ‚ï¼Œå¤§å‹æ¨¡å‹å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†CAREï¼ˆæƒ…æ„Ÿçš„å†…å®¹å’Œå£°å­¦è¡¨ç¤ºï¼‰ï¼Œé‡‡ç”¨åŒç¼–ç æ–¹æ¡ˆç»“åˆè¯­ä¹‰å’Œå£°éŸ³å› ç´ ã€‚</li>
<li>è¯­ä¹‰ç¼–ç å™¨é€šè¿‡è’¸é¦æ–¹å¼ä½¿ç”¨æ–‡æœ¬è¡¨ç¤ºè¿›è¡Œè®­ç»ƒï¼Œå£°å­¦ç¼–ç å™¨åˆ™é¢„æµ‹è¯­éŸ³ä¿¡å·çš„å¸§çº§ç‰¹å¾ã€‚</li>
<li>CAREæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ‰æ•ˆçš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>ä¸å…¶ä»–è‡ªæˆ‘ç›‘ç£æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒCAREåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4bb9752da3b460a67070f5304bcc551.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-979cfd04d38424db69c053d74a33d01c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e982644af90f4e0a0d93b0f68b44b863.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73d8b919e05b01f0d05b5b053f2f069c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27e62f69254b0f46e17efecb75b1b842.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="NEST-Self-supervised-Fast-Conformer-as-All-purpose-Seasoning-to-Speech-Processing-Tasks"><a href="#NEST-Self-supervised-Fast-Conformer-as-All-purpose-Seasoning-to-Speech-Processing-Tasks" class="headerlink" title="NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech   Processing Tasks"></a>NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech   Processing Tasks</h2><p><strong>Authors:He Huang, Taejin Park, Kunal Dhawan, Ivan Medennikov, Krishna C. Puvvada, Nithin Rao Koluguri, Weiqing Wang, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition&#x2F;translation, speaker verification and diarization, etc. However, most of current approaches are computationally expensive. In this paper, we propose a simplified and more efficient self-supervised learning framework termed as NeMo Encoder for Speech Tasks (NEST). Specifically, we adopt the FastConformer architecture with 8x sub-sampling rate, which is faster than Transformer or Conformer architectures. Instead of clustering-based quantization, we use fixed random projection for its simplicity and effectiveness. We also implement a generalized noisy speech augmentation that teaches the model to disentangle the main speaker from noise or other speakers. Experiments show that \model improves over existing self-supervised models and achieves new state-of-the-art performance on a variety of speech processing tasks, such as speech recognition&#x2F;translation, speaker diarization, spoken language understanding, etc. Code and checkpoints are publicly available via NVIDIA NeMo framework. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ å·²è¢«è¯æ˜å¯¹å¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡ï¼ˆå¦‚è¯­éŸ³è¯†åˆ«&#x2F;ç¿»è¯‘ã€è¯´è¯äººéªŒè¯å’Œæ—¥è®°åŒ–ç­‰ï¼‰éƒ½æœ‰ç›Šã€‚ç„¶è€Œï¼Œå½“å‰å¤§å¤šæ•°æ–¹æ³•çš„è®¡ç®—æˆæœ¬éƒ½å¾ˆé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€åŒ–ä¸”æ›´é«˜æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºç”¨äºè¯­éŸ³ä»»åŠ¡çš„NeMoç¼–ç å™¨ï¼ˆNESTï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å…·æœ‰8å€å­é‡‡æ ·ç‡çš„FastConformeræ¶æ„ï¼Œå…¶é€Ÿåº¦æ¯”Transformeræˆ–Conformeræ¶æ„æ›´å¿«ã€‚æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨åŸºäºèšç±»çš„é‡åŒ–æ–¹æ³•ï¼Œè€Œæ˜¯é‡‡ç”¨ç®€å•æœ‰æ•ˆçš„å›ºå®šéšæœºæŠ•å½±æ³•ã€‚æˆ‘ä»¬è¿˜å®ç°äº†ä¸€ç§é€šç”¨çš„å™ªå£°è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œä»¥è®­ç»ƒæ¨¡å‹ä»å™ªå£°æˆ–å…¶ä»–è¯´è¯äººä¸­åˆ†ç¦»å‡ºä¸»è¯´è¯äººçš„å£°éŸ³ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è‡ªç›‘ç£æ¨¡å‹ï¼Œå¹¶å®ç°äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«&#x2F;ç¿»è¯‘ã€è¯´è¯äººæ—¥è®°åŒ–ã€è¯­éŸ³è¯­è¨€ç†è§£ç­‰ä»»åŠ¡ã€‚æ¨¡å‹å’Œæ£€æŸ¥ç‚¹å¯é€šè¿‡NVIDIA NeMoæ¡†æ¶å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13106v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€åŒ–ä¸”é«˜æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºNeMo Encoder for Speech Tasks (NEST)ã€‚é‡‡ç”¨FastConformeræ¶æ„ï¼Œå®ç°8å€å­é‡‡æ ·ç‡ï¼Œæ¯”Transformeræˆ–Conformeræ¶æ„æ›´å¿«ã€‚ä½¿ç”¨å›ºå®šéšæœºæŠ•å½±ä»£æ›¿èšç±»é‡åŒ–ï¼Œå¹¶å®ç°äº†é€šç”¨çš„å™ªå£°è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†ä¸»è¦è¯´è¯è€…çš„å£°éŸ³ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šè¶…è¿‡äº†ç°æœ‰çš„è‡ªç›‘ç£æ¨¡å‹ï¼Œå¹¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å¯é€šè¿‡NVIDIA NeMoæ¡†æ¶å…¬å¼€è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ç®€åŒ–çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶NESTï¼Œé’ˆå¯¹è¯­éŸ³å¤„ç†ä»»åŠ¡é«˜æ•ˆè¿è¡Œã€‚</li>
<li>ä½¿ç”¨FastConformeræ¶æ„å®ç°äº†è¾ƒé«˜çš„é‡‡æ ·ç‡ä¸æ›´å¿«çš„è¿ç®—é€Ÿåº¦ã€‚</li>
<li>é‡‡ç”¨å›ºå®šéšæœºæŠ•å½±æ–¹æ³•æ›¿ä»£äº†å¤æ‚çš„èšç±»é‡åŒ–è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é€šç”¨çš„å™ªå£°è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰è‡ªç›‘ç£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>NESTæ¡†æ¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶æˆåŠŸåº”ç”¨äºå¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€ç¿»è¯‘ã€è¯´è¯äººåˆ†æç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1622803d0200287aaa4be28ea34775da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b408bfa3d29990f327c705bca2ec2b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-513982dd64c21a2f1b940f29dff76184.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09d6b9b9f8375e47a0cb6976a8d901bf.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Towards-High-Quality-and-Efficient-Speech-Bandwidth-Extension-with-Parallel-Amplitude-and-Phase-Prediction"><a href="#Towards-High-Quality-and-Efficient-Speech-Bandwidth-Extension-with-Parallel-Amplitude-and-Phase-Prediction" class="headerlink" title="Towards High-Quality and Efficient Speech Bandwidth Extension with   Parallel Amplitude and Phase Prediction"></a>Towards High-Quality and Efficient Speech Bandwidth Extension with   Parallel Amplitude and Phase Prediction</h2><p><strong>Authors:Ye-Xin Lu, Yang Ai, Hui-Peng Du, Zhen-Hua Ling</strong></p>
<p>Speech bandwidth extension (BWE) refers to widening the frequency bandwidth range of speech signals, enhancing the speech quality towards brighter and fuller. This paper proposes a generative adversarial network (GAN) based BWE model with parallel prediction of Amplitude and Phase spectra, named AP-BWE, which achieves both high-quality and efficient wideband speech waveform generation. The proposed AP-BWE generator is entirely based on convolutional neural networks (CNNs). It features a dual-stream architecture with mutual interaction, where the amplitude stream and the phase stream communicate with each other and respectively extend the high-frequency components from the input narrowband amplitude and phase spectra. To improve the naturalness of the extended speech signals, we employ a multi-period discriminator at the waveform level and design a pair of multi-resolution amplitude and phase discriminators at the spectral level, respectively. Experimental results demonstrate that our proposed AP-BWE achieves state-of-the-art performance in terms of speech quality for BWE tasks targeting sampling rates of both 16 kHz and 48 kHz. In terms of generation efficiency, due to the all-convolutional architecture and all-frame-level operations, the proposed AP-BWE can generate 48 kHz waveform samples 292.3 times faster than real-time on a single RTX 4090 GPU and 18.1 times faster than real-time on a single CPU. Notably, to our knowledge, AP-BWE is the first to achieve the direct extension of the high-frequency phase spectrum, which is beneficial for improving the effectiveness of existing BWE methods. </p>
<blockquote>
<p>è¯­éŸ³å¸¦å®½æ‰©å±•ï¼ˆBWEï¼‰æ˜¯æŒ‡æ‰©å¤§è¯­éŸ³ä¿¡å·çš„é¢‘ç‡å¸¦å®½èŒƒå›´ï¼Œæé«˜è¯­éŸ³è´¨é‡ï¼Œä½¿å…¶æ›´åŠ æ˜äº®å’Œä¸°æ»¡ã€‚æœ¬æ–‡é’ˆå¯¹è¯­éŸ³ä¿¡å·çš„å®½å¸¦æ³¢å½¢ç”Ÿæˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„BWEæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åŒæ—¶é¢„æµ‹æŒ¯å¹…å’Œç›¸ä½è°±ï¼Œç§°ä¸ºAP-BWEã€‚è¯¥æ¨¡å‹å®ç°äº†é«˜è´¨é‡ä¸”é«˜æ•ˆçš„å®½å¸¦è¯­éŸ³æ³¢å½¢ç”Ÿæˆã€‚æå‡ºçš„AP-BWEç”Ÿæˆå™¨å®Œå…¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€‚å®ƒé‡‡ç”¨åŒæµä¼ è¾“æ¶æ„ï¼Œç›¸äº’äº¤äº’ï¼Œå…¶ä¸­æŒ¯å¹…æµå’Œç›¸ä½æµç›¸äº’é€šä¿¡ï¼Œåˆ†åˆ«ä»è¾“å…¥çš„çª„å¸¦æŒ¯å¹…å’Œç›¸ä½è°±æ‰©å±•é«˜é¢‘åˆ†é‡ã€‚ä¸ºäº†æé«˜æ‰©å±•è¯­éŸ³ä¿¡å·çš„è‡ªç„¶æ€§ï¼Œæˆ‘ä»¬åœ¨æ³¢å½¢çº§åˆ«é‡‡ç”¨äº†å¤šå‘¨æœŸé‰´åˆ«å™¨ï¼Œå¹¶åœ¨å…‰è°±çº§åˆ«è®¾è®¡äº†ä¸€å¯¹å¤šåˆ†è¾¨ç‡æŒ¯å¹…å’Œç›¸ä½é‰´åˆ«å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„AP-BWEåœ¨é’ˆå¯¹16kHzå’Œ48kHzé‡‡æ ·ç‡çš„BWEä»»åŠ¡æ—¶ï¼Œè¯­éŸ³è´¨é‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚åœ¨æé«˜ç”Ÿæˆæ•ˆç‡æ–¹é¢ï¼Œç”±äºé‡‡ç”¨äº†å…¨å·ç§¯æ¶æ„å’Œå¸§çº§æ“ä½œï¼ŒAP-BWEåœ¨å•ä¸ªRTX 4090 GPUä¸Šç”Ÿæˆ48kHzæ³¢å½¢æ ·æœ¬çš„é€Ÿåº¦æ˜¯å®æ—¶é€Ÿåº¦çš„292.3å€ï¼Œåœ¨å•ä¸ªCPUä¸Šæ˜¯å®æ—¶é€Ÿåº¦çš„18.1å€ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒAP-BWEé¦–æ¬¡å®ç°äº†é«˜é¢‘ç›¸ä½è°±çš„ç›´æ¥æ‰©å±•ï¼Œè¿™æœ‰åˆ©äºæé«˜ç°æœ‰BWEæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.06387v2">PDF</a> Accepted by IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„è¯­éŸ³å¸¦å®½æ‰©å±•ï¼ˆBWEï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯å®ç°æŒ¯å¹…å’Œç›¸ä½è°±çš„å¹¶è¡Œé¢„æµ‹ï¼Œç§°ä¸ºAP-BWEã€‚å®ƒå®Œå…¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ï¼Œå…·æœ‰åŒæµä¼ æ„Ÿå™¨æ¶æ„ï¼Œå®ç°æŒ¯å¹…å’Œç›¸ä½ä¼ æ„Ÿå™¨ä¹‹é—´çš„ç›¸äº’äº¤æµï¼Œåˆ†åˆ«ä»è¾“å…¥çª„å¸¦æŒ¯å¹…å’Œç›¸ä½è°±æ‰©å±•é«˜é¢‘åˆ†é‡ã€‚ä¸ºæé«˜æ‰©å±•è¯­éŸ³ä¿¡å·çš„è‡ªç„¶æ€§ï¼Œé‡‡ç”¨æ³¢å½¢çº§åˆ«çš„å¤šå‘¨æœŸé‰´åˆ«å™¨å’Œå…‰è°±çº§åˆ«çš„å¤šåˆ†è¾¨ç‡æŒ¯å¹…ä¸ç›¸ä½é‰´åˆ«å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹é‡‡æ ·ç‡ä¸º16kHzå’Œ48kHzçš„BWEä»»åŠ¡ï¼Œæ‰€æå‡ºçš„AP-BWEåœ¨è¯­éŸ³è´¨é‡æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚åœ¨ç”Ÿæˆæ•ˆç‡æ–¹é¢ï¼Œç”±äºå…¨å·ç§¯æ¶æ„å’Œå¸§çº§æ“ä½œï¼ŒAP-BWEåœ¨å•ä¸ªRTX 4090 GPUä¸Šç”Ÿæˆ48kHzæ³¢å½¢æ ·æœ¬çš„é€Ÿåº¦æ˜¯å®æ—¶é€Ÿåº¦çš„292.3å€ï¼Œåœ¨å•ä¸ªCPUä¸Šæ˜¯å®æ—¶çš„18.1å€ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒAP-BWEé¦–æ¬¡å®ç°äº†é«˜é¢‘ç›¸ä½è°±çš„ç›´æ¥æ‰©å±•ï¼Œè¿™æœ‰åŠ©äºæé«˜ç°æœ‰BWEæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„è¯­éŸ³å¸¦å®½æ‰©å±•ï¼ˆBWEï¼‰æ¨¡å‹AP-BWEï¼Œæ—¨åœ¨å¢å¼ºè¯­éŸ³è´¨é‡å¹¶æ‰©å±•é¢‘ç‡å¸¦å®½èŒƒå›´ã€‚</li>
<li>AP-BWEé‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å®Œå…¨æ„å»ºï¼Œå…·æœ‰åŒæµæ¶æ„ï¼Œå®ç°æŒ¯å¹…å’Œç›¸ä½ä¼ æ„Ÿå™¨é—´çš„ç›¸äº’äº¤æµã€‚</li>
<li>é€šè¿‡é‡‡ç”¨å¤šå‘¨æœŸé‰´åˆ«å™¨å’Œå¤šåˆ†è¾¨ç‡æŒ¯å¹…ä¸ç›¸ä½é‰´åˆ«å™¨ï¼Œæé«˜äº†æ‰©å±•è¯­éŸ³ä¿¡å·çš„è‡ªç„¶æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAP-BWEåœ¨è¯­éŸ³è´¨é‡å’Œç”Ÿæˆæ•ˆç‡æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>AP-BWEå®ç°äº†é«˜é¢‘ç›¸ä½è°±çš„ç›´æ¥æ‰©å±•ï¼Œæœ‰åŠ©äºæé«˜ç°æœ‰BWEæ–¹æ³•çš„æ•ˆæœã€‚</li>
<li>AP-BWEèƒ½å¤Ÿåœ¨å•ä¸ªé«˜æ€§èƒ½GPUä¸Šå®ç°å¿«é€Ÿè¯­éŸ³æ³¢å½¢ç”Ÿæˆï¼Œæ˜¾è‘—æé«˜äº†å®æ—¶æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.06387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-710b5141009f390ec54ec17b2a467f42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd99e692e8966b091d3d2ff50dc400a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96fa25fb3445303ecea8205e3d06abf9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a90cc327cb07ee42fb909b59408bf305.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Towards a Universal Synthetic Video Detector From Face or Background   Manipulations to Fully AI-Generated Content
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a539199c69b3da6f402c4e3f197bbf9f.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  DiffBoost Enhancing Medical Image Segmentation via Text-Guided   Diffusion Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">8052.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
