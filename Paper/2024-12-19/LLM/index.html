<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  SafeAgentBench A Benchmark for Safe Task Planning of Embodied LLM   Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fc2ed00fed1f090b2b5f5376e2dbecd0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    23.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    97 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-19-æ›´æ–°"><a href="#2024-12-19-æ›´æ–°" class="headerlink" title="2024-12-19 æ›´æ–°"></a>2024-12-19 æ›´æ–°</h1><h2 id="SafeAgentBench-A-Benchmark-for-Safe-Task-Planning-of-Embodied-LLM-Agents"><a href="#SafeAgentBench-A-Benchmark-for-Safe-Task-Planning-of-Embodied-LLM-Agents" class="headerlink" title="SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM   Agents"></a>SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM   Agents</h2><p><strong>Authors:Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen</strong></p>
<p>With the integration of large language models (LLMs), embodied agents have strong capabilities to execute complicated instructions in natural language, paving a way for the potential deployment of embodied robots. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in real world. To study this issue, we present SafeAgentBench â€“ a new benchmark for safety-aware task planning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset with 750 tasks, covering 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 8 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that the best-performing baseline gets 69% success rate for safe tasks, but only 5% rejection rate for hazardous tasks, indicating significant safety risks. More details and codes are available at <a target="_blank" rel="noopener" href="https://github.com/shengyin1224/SafeAgentBench">https://github.com/shengyin1224/SafeAgentBench</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›†æˆï¼Œå®ä½“ä»£ç†å…·å¤‡äº†æ‰§è¡Œè‡ªç„¶è¯­è¨€ä¸­çš„å¤æ‚æŒ‡ä»¤çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ºå®ä½“æœºå™¨äººçš„æ½œåœ¨éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå¯é¢„è§çš„é—®é¢˜æ˜¯ï¼Œè¿™äº›å®ä½“ä»£ç†ä¹Ÿå¯ä»¥å®Œç¾åœ°æ‰§è¡Œä¸€äº›å±é™©ä»»åŠ¡ï¼Œå¯èƒ½ä¼šåœ¨ç°å®ä¸–ç•Œé€ æˆæŸå®³ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SafeAgentBenchâ€”â€”ä¸€ä¸ªç”¨äºå®ä½“LLMä»£ç†çš„å®‰å…¨æ„è¯†ä»»åŠ¡è§„åˆ’çš„æ–°åŸºå‡†ã€‚SafeAgentBenchåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«750ä¸ªä»»åŠ¡ï¼Œæ¶µç›–10ç§æ½œåœ¨å±é™©å’Œ3ç§ä»»åŠ¡ç±»å‹ï¼›ï¼ˆ2ï¼‰SafeAgentEnvï¼Œä¸€ä¸ªé€šç”¨å®ä½“ç¯å¢ƒï¼Œå…·æœ‰åº•å±‚æ§åˆ¶å™¨ï¼Œæ”¯æŒå¤šä»£ç†æ‰§è¡Œï¼Œå…·æœ‰æ”¯æŒ8ç§æœ€æ–°åŸºçº¿æ¨¡å‹çš„17ä¸ªé«˜çº§åŠ¨ä½œï¼›ï¼ˆ3ï¼‰ä»æ‰§è¡Œå’Œè¯­ä¹‰ä¸¤ä¸ªæ–¹é¢çš„å¯é è¯„ä¼°æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä½³çš„åŸºçº¿æ¨¡å‹åœ¨å®‰å…¨ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ä¸º69%ï¼Œä½†åœ¨å±é™©ä»»åŠ¡ä¸Šçš„æ‹’ç»ç‡ä¸ºä»…5%ï¼Œè¡¨æ˜å­˜åœ¨æ˜¾è‘—çš„å®‰å…¨é£é™©ã€‚æ›´å¤šè¯¦æƒ…å’Œä»£ç å¯é€šè¿‡è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/shengyin1224/SafeAgentBench">https://github.com/shengyin1224/SafeAgentBench</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13178v2">PDF</a> 21 pages, 14 tables, 7 figures, submitted to ICRA 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›†æˆä½¿å¾—å®ä½“ä»£ç†å…·å¤‡äº†æ‰§è¡Œè‡ªç„¶è¯­è¨€å¤æ‚æŒ‡ä»¤çš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸ºå®ä½“æœºå™¨äººçš„æ½œåœ¨éƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚ç„¶è€Œï¼Œå®ä½“ä»£ç†äººä¹Ÿèƒ½å®Œç¾æ‰§è¡Œä¸€äº›å±é™©ä»»åŠ¡ï¼Œå¯èƒ½é€ æˆç°å®ä¸–ç•Œä¸­çš„æŸå®³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†SafeAgentBenchâ€”â€”ä¸€ä¸ªé’ˆå¯¹å…·æœ‰å®‰å…¨æ„è¯†çš„å®ä½“LLMä»£ç†ä»»åŠ¡è§„åˆ’çš„æ–°åŸºå‡†æµ‹è¯•ã€‚SafeAgentBenchåŒ…æ‹¬ï¼š1ï¼‰åŒ…å«750ä¸ªä»»åŠ¡çš„æ–°æ•°æ®é›†ï¼Œæ¶µç›–10ç§æ½œåœ¨å±å®³å’Œ3ç§ä»»åŠ¡ç±»å‹ï¼›2ï¼‰SafeAgentEnvï¼Œä¸€ä¸ªé€šç”¨å®ä½“ç¯å¢ƒï¼Œå…·æœ‰ä½çº§æ§åˆ¶å™¨ï¼Œæ”¯æŒå¤šä»£ç†æ‰§è¡Œï¼Œä¸º8ç§æœ€æ–°æŠ€æœ¯åŸºçº¿æä¾›17ç§é«˜çº§æ“ä½œï¼›ä»¥åŠä»æ‰§è¡Œå’Œè¯­ä¹‰è§’åº¦çš„å¯é è¯„ä¼°æ–¹æ³•ã€‚å®éªŒç»“æœæç¤ºå­˜åœ¨æ˜¾è‘—çš„å®‰å…¨é£é™©ï¼Œæœ€ä½³æ€§èƒ½çš„åŸºçº¿åœ¨å®‰å…¨ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ä»…ä¸º69%ï¼Œè€Œåœ¨å±é™©ä»»åŠ¡ä¸Šçš„æ‹’ç»ç‡ä»…ä¸º5%ã€‚æ›´å¤šè¯¦æƒ…å’Œä»£ç è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/shengyin1224/SafeAgentBench%E3%80%82">https://github.com/shengyin1224/SafeAgentBenchã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›†æˆå¢å¼ºäº†å®ä½“ä»£ç†æ‰§è¡Œè‡ªç„¶è¯­è¨€å¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œä¸ºå®ä½“æœºå™¨äººçš„éƒ¨ç½²æä¾›äº†å¯èƒ½ã€‚</li>
<li>å®ä½“ä»£ç†äººèƒ½å¤Ÿå®Œç¾æ‰§è¡Œå±é™©ä»»åŠ¡ï¼Œå­˜åœ¨æ½œåœ¨çš„ç°å®ä¸–ç•Œé£é™©ã€‚</li>
<li>SafeAgentBenchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å®ä½“LLMä»£ç†çš„ä»»åŠ¡è§„åˆ’å®‰å…¨æ€§ã€‚</li>
<li>SafeAgentBenchåŒ…æ‹¬ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§æ½œåœ¨å±å®³å’Œä»»åŠ¡ç±»å‹ã€‚</li>
<li>SafeAgentEnvæ˜¯ä¸€ä¸ªé€šç”¨å®ä½“ç¯å¢ƒï¼Œæ”¯æŒå¤šä»£ç†æ‰§è¡Œï¼Œå¹¶æä¾›é«˜çº§æ“ä½œã€‚</li>
<li>è¯„ä¼°æ–¹æ³•ä»æ‰§è¡Œå’Œè¯­ä¹‰ä¸¤ä¸ªè§’åº¦è¿›è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e13e57e3979c9c5b1ce769ac2c819ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bfd9426d725cbe359056203be83a925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc2ed00fed1f090b2b5f5376e2dbecd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08e2895a318dd12f07ce9551d82799a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8fe10df61c7d83148a53067563ddb91.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Are-Your-LLMs-Capable-of-Stable-Reasoning"><a href="#Are-Your-LLMs-Capable-of-Stable-Reasoning" class="headerlink" title="Are Your LLMs Capable of Stable Reasoning?"></a>Are Your LLMs Capable of Stable Reasoning?</h2><p><strong>Authors:Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, Kai Chen</strong></p>
<p>The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the modelâ€™s peak performance potential and its stability. Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMsâ€™ â€œrealisticâ€ reasoning capabilities, highlighting the need for more robust evaluation methods. The benchmark and detailed results are available at: <a target="_blank" rel="noopener" href="https://github.com/open-compass/GPassK">https://github.com/open-compass/GPassK</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼ŒåŸºå‡†æµ‹è¯•æ€§èƒ½ä¸å®é™…åº”ç”¨ä¹‹é—´ä»å­˜åœ¨è¾ƒå¤§å·®å¼‚ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸€å·®è·ä¸»è¦æºäºå½“å‰çš„è¯„ä¼°åè®®å’ŒæŒ‡æ ‡ï¼Œå®ƒä»¬ä¸èƒ½å……åˆ†åœ°åæ˜ LLMçš„å…¨éƒ¨èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å‡†ç¡®æ€§å’Œä¸€è‡´æ€§çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚è¿™é¡¹å·¥ä½œæœ‰ä¸¤ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†G-Pass@kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå®ƒå¯ä»¥åœ¨å¤šæ¬¡é‡‡æ ·å°è¯•ä¸­æŒç»­è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œé‡åŒ–æ¨¡å‹çš„å³°å€¼æ€§èƒ½æ½œåŠ›åŠå…¶ç¨³å®šæ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LiveMathBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°ä»£æ•°å­¦é—®é¢˜ï¼Œæ—¨åœ¨é™ä½è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ•°æ®æ³„éœ²é£é™©ã€‚é€šè¿‡åœ¨å¤§è§„æ¨¡LLMä¸Šä½¿ç”¨G-Pass@kå¯¹LiveMathBenchè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å¯¹å®ƒä»¬çš„æœ€å¤§èƒ½åŠ›å’Œæ“ä½œä¸€è‡´æ€§æœ‰äº†å…¨é¢çš„äº†è§£ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨â€œç°å®â€æ¨ç†èƒ½åŠ›æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥çš„è¯„ä¼°æ–¹æ³•ã€‚åŸºå‡†æµ‹è¯•å’Œè¯¦ç»†ç»“æœå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/open-compass/GPassK%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/open-compass/GPassKä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13147v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åŸºå‡†æµ‹è¯•æ€§èƒ½ä¸å®é™…åº”ç”¨ä¹‹é—´ä»å­˜åœ¨å·®è·ã€‚æœ¬æ–‡æå‡ºG-Pass@kè¯„ä¼°æŒ‡æ ‡å’ŒLiveMathBenchåŠ¨æ€åŸºå‡†æµ‹è¯•ï¼Œå‰è€…å¯è¿ç»­è¯„ä¼°æ¨¡å‹å¤šæ¬¡é‡‡æ ·çš„æ€§èƒ½ï¼Œé‡åŒ–æ¨¡å‹çš„å³°å€¼æ€§èƒ½å’Œç¨³å®šæ€§ï¼›åè€…åŒ…å«å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°ä»£æ•°å­¦é—®é¢˜ï¼Œæ—¨åœ¨å‡å°‘è¯„ä¼°ä¸­çš„æ•°æ®æ³„éœ²é£é™©ã€‚å®éªŒè¡¨æ˜ï¼ŒLLMåœ¨â€œç°å®â€æ¨ç†èƒ½åŠ›æ–¹é¢ä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ï¼Œéœ€è¦æ›´ç¨³å¥çš„è¯„ä¼°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å®é™…åº”ç”¨ä¸åŸºå‡†æµ‹è¯•æ€§èƒ½é—´å­˜åœ¨å·®è·ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åè®®å’ŒæŒ‡æ ‡æ— æ³•å…¨é¢æ•æ‰LLMçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®æ€§å’Œä¸€è‡´æ€§éƒ½è‡³å…³é‡è¦çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚</li>
<li>å¼•å…¥G-Pass@kè¯„ä¼°æŒ‡æ ‡ï¼Œå¯è¿ç»­è¯„ä¼°æ¨¡å‹å¤šæ¬¡é‡‡æ ·çš„æ€§èƒ½ï¼Œåæ˜ æ¨¡å‹çš„å³°å€¼å’Œç¨³å®šæ€§ã€‚</li>
<li>æå‡ºLiveMathBenchåŠ¨æ€åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ç°ä»£æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ—¨åœ¨å‡å°‘æ•°æ®æ³„éœ²é£é™©ã€‚</li>
<li>å®éªŒè¡¨æ˜LLMåœ¨â€œç°å®â€æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨æå‡ç©ºé—´ã€‚</li>
<li>éœ€è¦æ›´ç¨³å¥çš„è¯„ä¼°æ–¹æ³•æ¥å…¨é¢è¯„ä¼°LLMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0c619deb7efc12790f0d48dbbef960d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2142cd0e3fd6c12a215447265c0b8e49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e4fcbe71cbd038831486048689919bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420c24d4dcae6348add1de43565217ce.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AIR-Bench-Automated-Heterogeneous-Information-Retrieval-Benchmark"><a href="#AIR-Bench-Automated-Heterogeneous-Information-Retrieval-Benchmark" class="headerlink" title="AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark"></a>AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark</h2><p><strong>Authors:Jianlyu Chen, Nan Wang, Chaofan Li, Bo Wang, Shitao Xiao, Han Xiao, Hao Liao, Defu Lian, Zheng Liu</strong></p>
<p>Evaluation plays a crucial role in the advancement of information retrieval (IR) models. However, current benchmarks, which are based on predefined domains and human-labeled data, face limitations in addressing evaluation needs for emerging domains both cost-effectively and efficiently. To address this challenge, we propose the Automated Heterogeneous Information Retrieval Benchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1) Automated. The testing data in AIR-Bench is automatically generated by large language models (LLMs) without human intervention. 2) Heterogeneous. The testing data in AIR-Bench is generated with respect to diverse tasks, domains and languages. 3) Dynamic. The domains and languages covered by AIR-Bench are constantly augmented to provide an increasingly comprehensive evaluation benchmark for community developers. We develop a reliable and robust data generation pipeline to automatically create diverse and high-quality evaluation datasets based on real-world corpora. Our findings demonstrate that the generated testing data in AIR-Bench aligns well with human-labeled testing data, making AIR-Bench a dependable benchmark for evaluating IR models. The resources in AIR-Bench are publicly available at <a target="_blank" rel="noopener" href="https://github.com/AIR-Bench/AIR-Bench">https://github.com/AIR-Bench/AIR-Bench</a>. </p>
<blockquote>
<p>åœ¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ¨¡å‹çš„è¿›æ­¥ä¸­ï¼Œè¯„ä¼°èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºå‡†æµ‹è¯•ä¾èµ–äºé¢„å®šä¹‰é¢†åŸŸå’Œäººç±»æ ‡æ³¨çš„æ•°æ®ï¼Œåœ¨é’ˆå¯¹æ–°å…´é¢†åŸŸçš„è¯„ä¼°éœ€æ±‚æ—¶ï¼Œæ—¢éš¾ä»¥å®ç°æˆæœ¬æ•ˆç›Šï¼Œä¹Ÿéš¾ä»¥æé«˜æ•ˆç‡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªåŠ¨åŒ–å¼‚è´¨ä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•ï¼ˆAIR-Benchï¼‰ã€‚AIR-Benchä»¥ä¸‰ä¸ªå…³é”®ç‰¹å¾ä¸ºç‰¹è‰²ï¼š1ï¼‰è‡ªåŠ¨åŒ–ã€‚AIR-Benchä¸­çš„æµ‹è¯•æ•°æ®æ˜¯ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚2ï¼‰å¤šå…ƒåŒ–ã€‚AIR-Benchä¸­çš„æµ‹è¯•æ•°æ®æ˜¯é’ˆå¯¹å„ç§ä»»åŠ¡ã€é¢†åŸŸå’Œè¯­è¨€ç”Ÿæˆçš„ã€‚3ï¼‰åŠ¨æ€åŒ–ã€‚AIR-Benchæ¶µç›–çš„é¢†åŸŸå’Œè¯­è¨€ä¸æ–­æ‰©å……ï¼Œä¸ºç¤¾åŒºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªè¶Šæ¥è¶Šå…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯é ä¸”ç¨³å¥çš„æ•°æ®ç”Ÿæˆç®¡é“ï¼ŒåŸºäºçœŸå®è¯­æ–™åº“è‡ªåŠ¨åˆ›å»ºå¤šæ ·ä¸”é«˜è´¨é‡çš„è¯„ä»·æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒAIR-Benchç”Ÿæˆçš„æµ‹è¯•æ•°æ®ä¸äººå·¥æ ‡æ³¨çš„æµ‹è¯•æ•°æ®å¯¹é½è‰¯å¥½ï¼Œä½¿å…¶æˆä¸ºè¯„ä¼°IRæ¨¡å‹çš„å¯ä¿¡åŸºå‡†ã€‚AIR-Benchçš„èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIR-Bench/AIR-Bench%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AIR-Bench/AIR-Benchå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13102v2">PDF</a> 31 pages, 6 figures; Update Table 5</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¿¡æ¯æ£€ç´¢æ¨¡å‹è¯„ä¼°çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°åŸºå‡†çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–å¼‚æ„ä¿¡æ¯æ£€ç´¢åŸºå‡†ï¼ˆAIR-Benchï¼‰ã€‚å…¶ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬è‡ªåŠ¨åŒ–ã€å¼‚æ„æ€§å’ŒåŠ¨æ€æ€§ã€‚AIR-Benché€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼Œè¦†ç›–ä¸åŒçš„ä»»åŠ¡ã€é¢†åŸŸå’Œè¯­è¨€ï¼Œå¹¶æŒç»­æ‰©å……ï¼Œä¸ºå¼€å‘è€…æä¾›æ—¥ç›Šå…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚æ­¤å¤–ï¼Œå…¶æ•°æ®ç”Ÿæˆç®¡é“å¯é ä¸”ç¨³å¥ï¼Œèƒ½å¤ŸåŸºäºçœŸå®è¯­æ–™åº“è‡ªåŠ¨åˆ›å»ºå¤šæ ·ä¸”é«˜è´¨é‡çš„è¯„ä»·æ•°æ®é›†ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒAIR-Benchç”Ÿæˆçš„æµ‹è¯•æ•°æ®ä¸äººå·¥æ ‡æ³¨çš„æµ‹è¯•æ•°æ®å¯¹é½è‰¯å¥½ï¼Œæ˜¯ä¸€ä¸ªå¯é çš„è¯„ä¼°ä¿¡æ¯æ£€ç´¢æ¨¡å‹çš„åŸºå‡†ã€‚ç›¸å…³èµ„æºå·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯æ£€ç´¢æ¨¡å‹çš„è¯„ä¼°åœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>å½“å‰è¯„ä¼°åŸºå‡†é¢ä¸´å¯¹æ–°å…´é¢†åŸŸè¯„ä¼°éœ€æ±‚æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–å¼‚æ„ä¿¡æ¯æ£€ç´¢åŸºå‡†ï¼ˆAIR-Benchï¼‰ä»¥åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>AIR-Benchçš„å…³é”®ç‰¹ç‚¹åŒ…æ‹¬è‡ªåŠ¨åŒ–ã€å¼‚æ„æ€§å’ŒåŠ¨æ€æ€§ã€‚</li>
<li>AIR-Benchä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚</li>
<li>AIR-Benchèƒ½å¤Ÿè¦†ç›–ä¸åŒçš„ä»»åŠ¡ã€é¢†åŸŸå’Œè¯­è¨€ï¼Œå¹¶ä¸ºå¼€å‘è€…æä¾›æ—¥ç›Šå…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e7cb3b3ea8294990a44f62df64a29857.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a2694ffef0bd8f060e115b7ac522b34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95084f46c88ca64b4cdb429e8b975c7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72f5c615538bca5da09452a33cf24b66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30c35f1a5a0365abf25266051236cbe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1693e34501112df5fb893828a9530aee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Modality-Inconsistent-Continual-Learning-of-Multimodal-Large-Language-Models"><a href="#Modality-Inconsistent-Continual-Learning-of-Multimodal-Large-Language-Models" class="headerlink" title="Modality-Inconsistent Continual Learning of Multimodal Large Language   Models"></a>Modality-Inconsistent Continual Learning of Multimodal Large Language   Models</h2><p><strong>Authors:Weiguo Pian, Shijian Deng, Shentong Mo, Yunhui Guo, Yapeng Tian</strong></p>
<p>In this paper, we introduce Modality-Inconsistent Continual Learning (MICL), a new continual learning scenario for Multimodal Large Language Models (MLLMs) that involves tasks with inconsistent modalities (image, audio, or video) and varying task types (captioning or question-answering). Unlike existing vision-only or modality-incremental settings, MICL combines modality and task type shifts, both of which drive catastrophic forgetting. To address these challenges, we propose MoInCL, which employs a Pseudo Targets Generation Module to mitigate forgetting caused by task type shifts in previously seen modalities. It also incorporates Instruction-based Knowledge Distillation to preserve the modelâ€™s ability to handle previously learned modalities when new ones are introduced. We benchmark MICL using a total of six tasks and conduct experiments to validate the effectiveness of our proposed MoInCL. The experimental results highlight the superiority of MoInCL, showing significant improvements over representative and state-of-the-art continual learning baselines. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æ¨¡æ€ä¸ä¸€è‡´çš„æŒç»­å­¦ä¹ ï¼ˆMICLï¼‰ï¼Œè¿™æ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ä¸€ç§æ–°çš„æŒç»­å­¦ä¹ åœºæ™¯ï¼Œæ¶‰åŠæ¨¡æ€ï¼ˆå›¾åƒã€éŸ³é¢‘æˆ–è§†é¢‘ï¼‰ä¸ä¸€è‡´å’Œä»»åŠ¡ç±»å‹ï¼ˆæè¿°æˆ–é—®ç­”ï¼‰å„å¼‚çš„ä»»åŠ¡ã€‚ä¸ç°æœ‰çš„ä»…é’ˆå¯¹è§†è§‰æˆ–æ¨¡æ€é€’å¢çš„è®¾ç½®ä¸åŒï¼ŒMICLç»“åˆäº†æ¨¡æ€å’Œä»»åŠ¡ç±»å‹çš„è½¬å˜ï¼Œä¸¤è€…éƒ½ä¼šå¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MoInCLï¼Œå®ƒé‡‡ç”¨ä¼ªç›®æ ‡ç”Ÿæˆæ¨¡å—æ¥ç¼“è§£å› ä»»åŠ¡ç±»å‹è½¬å˜è€Œå¯¼è‡´çš„é—å¿˜ã€‚å®ƒè¿˜ç»“åˆäº†åŸºäºæŒ‡ä»¤çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥ä¿ç•™æ¨¡å‹åœ¨å¤„ç†æ–°å¼•å…¥çš„æ¨¡æ€æ—¶å¯¹å…ˆå‰å­¦ä¹ æ¨¡æ€çš„å¤„ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨å…­ä¸ªä»»åŠ¡å¯¹MICLè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„MoInCLçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœçªå‡ºäº†MoInCLçš„ä¼˜è¶Šæ€§ï¼Œç›¸è¾ƒäºä»£è¡¨æ€§å’Œæœ€å…ˆè¿›çš„æŒç»­å­¦ä¹ åŸºå‡†æµ‹è¯•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13050v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨¡æ€ä¸ä¸€è‡´æŒç»­å­¦ä¹ ï¼ˆMICLï¼‰æ–°åœºæ™¯ï¼Œæ¶‰åŠæ¨¡æ€ï¼ˆå›¾åƒã€éŸ³é¢‘æˆ–è§†é¢‘ï¼‰å’Œä»»åŠ¡ç±»å‹ï¼ˆæè¿°æˆ–é—®ç­”ï¼‰ä¸ä¸€è‡´çš„ä»»åŠ¡ã€‚é’ˆå¯¹æ¨¡æ€å’Œä»»åŠ¡ç±»å‹è½¬å˜å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºMoInCLæ–¹æ³•ï¼Œé€šè¿‡ä¼ªç›®æ ‡ç”Ÿæˆæ¨¡å—ç¼“è§£ä»»åŠ¡ç±»å‹è½¬å˜å¯¼è‡´çš„é—å¿˜é—®é¢˜ï¼Œå¹¶èå…¥æŒ‡ä»¤çŸ¥è¯†è’¸é¦ï¼Œä»¥ç»´æŒæ¨¡å‹å¤„ç†æ–°å¼•å…¥æ¨¡æ€æ—¶çš„èƒ½åŠ›ã€‚é€šè¿‡å…­ä¸ªä»»åŠ¡è¿›è¡ŒåŸºå‡†æµ‹è¯•ä¸å®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºMoInCLä¼˜äºä»£è¡¨æ€§å’Œæœ€å…ˆè¿›çš„æŒç»­å­¦ä¹ åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ¨¡æ€ä¸ä¸€è‡´æŒç»­å­¦ä¹ ï¼ˆMICLï¼‰æ¦‚å¿µï¼Œæ¶‰åŠå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ¨¡æ€å’Œä»»åŠ¡ç±»å‹çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ä¸€æ¨¡æ€æˆ–æ¨¡æ€å¢é‡å­¦ä¹ ï¼ŒMICLç»“åˆäº†æ¨¡æ€å’Œä»»åŠ¡ç±»å‹è½¬å˜çš„æŒ‘æˆ˜ã€‚</li>
<li>MoInCLæ–¹æ³•é€šè¿‡ä¼ªç›®æ ‡ç”Ÿæˆæ¨¡å—ç¼“è§£ä»»åŠ¡ç±»å‹è½¬å˜å¯¼è‡´çš„é—å¿˜é—®é¢˜ã€‚</li>
<li>MoInCLèå…¥æŒ‡ä»¤çŸ¥è¯†è’¸é¦ï¼Œä»¥ç»´æŒæ¨¡å‹å¤„ç†æ–°å¼•å…¥æ¨¡æ€æ—¶çš„èƒ½åŠ›ã€‚</li>
<li>åŸºå‡†æµ‹è¯•ä½¿ç”¨äº†å…­ä¸ªä»»åŠ¡æ¥è¯„ä¼°MICLåœºæ™¯ã€‚</li>
<li>å®éªŒç»“æœéªŒè¯äº†MoInCLçš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ˜¾ç¤ºå…¶ä¼˜äºå…¶ä»–æŒç»­å­¦ä¹ åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-06321bfc637b053d0764f9e6fcb65e56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66b6679ceca510dedb741d6e93429671.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OmniEval-An-Omnidirectional-and-Automatic-RAG-Evaluation-Benchmark-in-Financial-Domain"><a href="#OmniEval-An-Omnidirectional-and-Automatic-RAG-Evaluation-Benchmark-in-Financial-Domain" class="headerlink" title="OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in   Financial Domain"></a>OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in   Financial Domain</h2><p><strong>Authors:Shuting Wang, Jiejun Tan, Zhicheng Dou, Ji-Rong Wen</strong></p>
<p>As a typical and practical application of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) techniques have gained extensive attention, particularly in vertical domains where LLMs may lack domain-specific knowledge. In this paper, we introduce an omnidirectional and automatic RAG benchmark, OmniEval, in the financial domain. Our benchmark is characterized by its multi-dimensional evaluation framework, including (1) a matrix-based RAG scenario evaluation system that categorizes queries into five task classes and 16 financial topics, leading to a structured assessment of diverse query scenarios; (2) a multi-dimensional evaluation data generation approach, which combines GPT-4-based automatic generation and human annotation, achieving an 87.47% acceptance ratio in human evaluations on generated instances; (3) a multi-stage evaluation system that evaluates both retrieval and generation performance, result in a comprehensive evaluation on the RAG pipeline; and (4) robust evaluation metrics derived from rule-based and LLM-based ones, enhancing the reliability of assessments through manual annotations and supervised fine-tuning of an LLM evaluator. Our experiments demonstrate the comprehensiveness of OmniEval, which includes extensive test datasets and highlights the performance variations of RAG systems across diverse topics and tasks, revealing significant opportunities for RAG models to improve their capabilities in vertical domains. We open source the code of our benchmark in \href{<a target="_blank" rel="noopener" href="https://github.com/RUC-NLPIR/OmniEval%7D%7Bhttps://github.com/RUC-NLPIR/OmniEval%7D">https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}</a>. </p>
<blockquote>
<p>ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…¸å‹å’Œå®é™…åº”ç”¨ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å·²å¼•èµ·å¹¿æ³›å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨LLMå¯èƒ½ç¼ºä¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„å‚ç›´é¢†åŸŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨é‡‘èé¢†åŸŸå¼•å…¥äº†ä¸€ä¸ªå…¨æ–¹ä½çš„è‡ªåŠ¨RAGåŸºå‡†æµ‹è¯•OmniEvalã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä»¥å…¶å¤šç»´è¯„ä¼°æ¡†æ¶ä¸ºç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åŸºäºçŸ©é˜µçš„RAGåœºæ™¯è¯„ä¼°ç³»ç»Ÿï¼Œå°†æŸ¥è¯¢åˆ†ç±»ä¸ºäº”ä¸ªä»»åŠ¡ç±»åˆ«å’Œ16ä¸ªé‡‘èä¸»é¢˜ï¼Œå®ç°å¯¹å¤šæ ·åŒ–æŸ¥è¯¢åœºæ™¯çš„ç»“æ„åŒ–è¯„ä¼°ï¼›ï¼ˆ2ï¼‰å¤šç»´è¯„ä¼°æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆGPT-4åŸºäºçš„è‡ªåŠ¨ç”Ÿæˆå’Œäººå·¥æ ‡æ³¨ï¼Œç”Ÿæˆå®ä¾‹çš„äººç±»è¯„ä¼°ä¸­è¾¾åˆ°87.47%çš„æ¥å—ç‡ï¼›ï¼ˆ3ï¼‰å¤šé˜¶æ®µè¯„ä¼°ç³»ç»Ÿï¼Œè¯„ä¼°æ£€ç´¢å’Œç”Ÿæˆæ€§èƒ½ï¼Œå¯¹RAGç®¡é“è¿›è¡Œå…¨é¢è¯„ä¼°ï¼›ï¼ˆ4ï¼‰åŸºäºè§„åˆ™å’ŒLLMçš„ç¨³å¥è¯„ä¼°æŒ‡æ ‡ï¼Œé€šè¿‡äººå·¥æ ‡æ³¨å’Œç›‘ç£è°ƒæ•´LLMè¯„ä¼°å™¨ï¼Œæé«˜è¯„ä¼°çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†OmniEvalçš„å…¨é¢æ€§ï¼Œå®ƒåŒ…æ‹¬å¹¿æ³›çš„æµ‹è¯•æ•°æ®é›†ï¼Œå¹¶çªå‡ºäº†RAGç³»ç»Ÿåœ¨å¤šä¸ªä¸»é¢˜å’Œä»»åŠ¡ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œæ­ç¤ºäº†RAGæ¨¡å‹åœ¨å‚ç›´é¢†åŸŸæé«˜èƒ½åŠ›çš„æ˜¾è‘—æœºä¼šã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUC-NLPIR/OmniEval">https://github.com/RUC-NLPIR/OmniEval</a>å¼€æºæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13018v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯åœ¨ä¸“ä¸šé¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé‡‘èé¢†åŸŸçš„å…¨è‡ªåŠ¨RAGåŸºå‡†æµ‹è¯•å¹³å°OmniEvalï¼Œå…¶ç‰¹ç‚¹åŒ…æ‹¬å¤šç»´è¯„ä¼°æ¡†æ¶ã€çŸ©é˜µå¼RAGåœºæ™¯è¯„ä¼°ç³»ç»Ÿã€å¤šç»´è¯„ä¼°æ•°æ®ç”Ÿæˆæ–¹æ³•ã€å¤šé˜¶æ®µè¯„ä¼°ç³»ç»Ÿä»¥åŠå¯é çš„è¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒè¡¨æ˜ï¼ŒOmniEvalèƒ½å¤Ÿå…¨é¢è¯„ä¼°RAGç³»ç»Ÿåœ¨å¤šæ ·åŒ–ä¸»é¢˜å’Œä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®å¼‚ï¼Œä¸ºRAGæ¨¡å‹æå‡ä¸“ä¸šèƒ½åŠ›æä¾›äº†é‡è¦æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniEvalæ˜¯ä¸€ä¸ªåœ¨é‡‘èé¢†åŸŸçš„å…¨è‡ªåŠ¨RAGåŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>å®ƒå…·æœ‰å¤šç»´è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬çŸ©é˜µå¼RAGåœºæ™¯è¯„ä¼°ã€æ•°æ®ç”Ÿæˆã€å¤šé˜¶æ®µè¯„ä¼°åŠè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>OmniEvalèƒ½å¤Ÿè¯„ä¼°RAGæŠ€æœ¯åœ¨ä¸åŒä¸»é¢˜å’Œä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>å¹³å°é‡‡ç”¨äº†GPT-4è‡ªåŠ¨ç”Ÿæˆå’Œäººå·¥æ ‡æ³¨ç›¸ç»“åˆçš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œæ¥å—ç‡é«˜è¾¾87.47%ã€‚</li>
<li>OmniEvalé€šè¿‡ç»“åˆè§„åˆ™åŸºç¡€å’ŒLLMåŸºç¡€çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæé«˜äº†è¯„ä¼°çš„å¯é æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRAGæŠ€æœ¯åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d972ab3ab626168aea59dbb85083ef4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee4428c0f5a65d88a531d45c3356f69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e05c0b98bd6993742fdd9da2f0807216.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64f7651489436f4f26831f843659482f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e750015c5ff9995c4a0512a72afff5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9432de1598fa35eba74f4521daafa8a4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-Emergence-of-Strategic-Reasoning-of-Large-Language-Models"><a href="#The-Emergence-of-Strategic-Reasoning-of-Large-Language-Models" class="headerlink" title="The Emergence of Strategic Reasoning of Large Language Models"></a>The Emergence of Strategic Reasoning of Large Language Models</h2><p><strong>Authors:Dongwoo Lee, Gavin Kader</strong></p>
<p>As Large Language Models (LLMs) are increasingly used for a variety of complex and critical tasks, it is vital to assess their logical capabilities in strategic environments. This paper examines their ability in strategic reasoning â€“ the process of choosing an optimal course of action by predicting and adapting to other agentsâ€™ behavior. Using six LLMs, we analyze responses from play in classical games from behavioral economics (p-Beauty Contest, 11-20 Money Request Game, and Guessing Game) and evaluate their performance through hierarchical models of reasoning (level-$k$ theory and cognitive hierarchy theory). Our findings reveal that while LLMs show understanding of the games, the majority struggle with higher-order strategic reasoning. Although most LLMs did demonstrate learning ability with games involving repeated interactions, they still consistently fall short of the reasoning levels demonstrated by typical behavior from human subjects. The exception to these overall findings is with OpenAIâ€™s GPT-o1 â€“ specifically trained to solve complex reasoning tasks â€“ which consistently outperforms other LLMs and human subjects. These findings highlight the challenges and pathways in advancing LLMs toward robust strategic reasoning from the perspective of behavioral economics. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§å¤æ‚å’Œå…³é”®ä»»åŠ¡ä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œè¯„ä¼°å…¶åœ¨æˆ˜ç•¥ç¯å¢ƒä¸­çš„é€»è¾‘èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æˆ˜ç•¥æ¨ç†æ–¹é¢çš„èƒ½åŠ›â€”â€”é€šè¿‡é¢„æµ‹å’Œé€‚åº”å…¶ä»–å®ä½“çš„è¡Œä¸ºæ¥é€‰æ‹©æœ€ä½³è¡ŒåŠ¨æ–¹æ¡ˆçš„è¿‡ç¨‹ã€‚æˆ‘ä»¬ä½¿ç”¨å…­ç§å¤§å‹è¯­è¨€æ¨¡å‹åˆ†æäº†è¡Œä¸ºç»æµå­¦ç»å…¸æ¸¸æˆçš„ç©æ³•ï¼ˆåŒ…æ‹¬åšå¼ˆç¾èµ›æ¸¸æˆã€â€œç»ˆæäº¤æ˜“â€æ¸¸æˆå’ŒçŒœè°œæ¸¸æˆï¼‰ï¼Œå¹¶é€šè¿‡å±‚æ¬¡åŒ–çš„æ¨ç†æ¨¡å‹ï¼ˆå¦‚kçº§ç†è®ºå’Œè®¤çŸ¥å±‚æ¬¡ç†è®ºï¼‰å¯¹å®ƒä»¬çš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç†è§£æ¸¸æˆè§„åˆ™ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹åœ¨è§£å†³é«˜é˜¶æˆ˜ç•¥æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚å°½ç®¡å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¶‰åŠé‡å¤äº¤äº’çš„æ¸¸æˆä¸­è¡¨ç°å‡ºå­¦ä¹ èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶æœªèƒ½è¾¾åˆ°äººç±»å—è¯•è€…è¡¨ç°å‡ºçš„æ¨ç†æ°´å¹³ã€‚æœ¬æ¬¡ç ”ç©¶çš„ä¾‹å¤–æƒ…å†µæ˜¯OpenAIçš„GPT-o1æ¨¡å‹â€”â€”è¯¥æ¨¡å‹ç»è¿‡ä¸“é—¨è®­ç»ƒä»¥è§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œå…¶è¡¨ç°å§‹ç»ˆä¼˜äºå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹å’Œäººç±»å—è¯•è€…ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡Œä¸ºç»æµå­¦è§†è§’ä¸‹çš„ç¨³å¥æˆ˜ç•¥æ¨ç†æ‰€é¢ä¸´çš„æŒ‘æˆ˜å’Œå‰è¿›é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13013v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æˆ˜ç•¥ç¯å¢ƒä¸‹çš„é€»è¾‘èƒ½åŠ›è¯„ä¼°è‡³å…³é‡è¦ã€‚æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—è¡Œä¸ºç»æµå­¦æ¸¸æˆè€ƒå¯ŸLLMçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›ï¼Œå‘ç°å¤šæ•°LLMåœ¨é«˜çº§æˆ˜ç•¥æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½†OpenAIçš„GPT-o1å› ä¸“é—¨è®­ç»ƒç”¨äºè§£å†³å¤æ‚æ¨ç†ä»»åŠ¡è€Œè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æˆ˜ç•¥æ¨ç†æ–¹é¢éœ€è¦è¿›è¡Œè¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å’Œå…³é”®ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>é€šè¿‡è¡Œä¸ºç»æµå­¦æ¸¸æˆè¿›è¡ŒLLMçš„æˆ˜ç•¥æ¨ç†èƒ½åŠ›åˆ†æã€‚</li>
<li>å¤§å¤šæ•°LLMåœ¨é«˜çº§æˆ˜ç•¥æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>LLMåœ¨æŸäº›æ¶‰åŠé‡å¤äº’åŠ¨çš„æ¸¸æˆä¸­å±•ç°å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>ä¸äººç±»ä¸»ä½“ç›¸æ¯”ï¼ŒLLMåœ¨æ¨ç†å±‚æ¬¡ä¸Šä»æœ‰å·®è·ã€‚</li>
<li>GPT-o1åœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡å…¶ä»–LLMå’Œäººç±»ä¸»ä½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-167dc11726aedd4c7eb424cef23921a3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unlocking-LLMs-Addressing-Scarce-Data-and-Bias-Challenges-in-Mental-Health"><a href="#Unlocking-LLMs-Addressing-Scarce-Data-and-Bias-Challenges-in-Mental-Health" class="headerlink" title="Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental   Health"></a>Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental   Health</h2><p><strong>Authors:Vivek Kumar, Eirini Ntoutsi, Pushpraj Singh Rajawat, Giacomo Medda, Diego Reforgiato Recupero</strong></p>
<p>Large language models (LLMs) have shown promising capabilities in healthcare analysis but face several challenges like hallucinations, parroting, and bias manifestation. These challenges are exacerbated in complex, sensitive, and low-resource domains. Therefore, in this work we introduce IC-AnnoMI, an expert-annotated motivational interviewing (MI) dataset built upon AnnoMI by generating in-context conversational dialogues leveraging LLMs, particularly ChatGPT. IC-AnnoMI employs targeted prompts accurately engineered through cues and tailored information, taking into account therapy style (empathy, reflection), contextual relevance, and false semantic change. Subsequently, the dialogues are annotated by experts, strictly adhering to the Motivational Interviewing Skills Code (MISC), focusing on both the psychological and linguistic dimensions of MI dialogues. We comprehensively evaluate the IC-AnnoMI dataset and ChatGPTâ€™s emotional reasoning ability and understanding of domain intricacies by modeling novel classification tasks employing several classical machine learning and current state-of-the-art transformer approaches. Finally, we discuss the effects of progressive prompting strategies and the impact of augmented data in mitigating the biases manifested in IC-AnnoM. Our contributions provide the MI community with not only a comprehensive dataset but also valuable insights for using LLMs in empathetic text generation for conversational therapy in supervised settings. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—å¥åº·åˆ†ææ–¹é¢å±•ç°å‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ï¼Œä½†é¢ä¸´ç€å¹»è§‰ã€é¹¦é¹‰å­¦èˆŒå’Œåè§è¡¨ç°ç­‰æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜åœ¨å¤æ‚ã€æ•æ„Ÿå’Œä½èµ„æºçš„é¢†åŸŸä¸­è¢«åŠ å‰§ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†IC-AnnoMIï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºAnnoMIçš„ä¸“å®¶æ³¨é‡ŠåŠ¨æœºè®¿è°ˆï¼ˆMIï¼‰æ•°æ®é›†ï¼Œé€šè¿‡åˆ©ç”¨LLMï¼Œç‰¹åˆ«æ˜¯ChatGPTç”Ÿæˆä¸Šä¸‹æ–‡å¯¹è¯æ¥å®ç°ã€‚IC-AnnoMIé‡‡ç”¨ç›®æ ‡æç¤ºï¼Œé€šè¿‡çº¿ç´¢å’Œå®šåˆ¶ä¿¡æ¯ç²¾ç¡®æ„å»ºï¼ŒåŒæ—¶è€ƒè™‘åˆ°æ²»ç–—é£æ ¼ï¼ˆåŒç†å¿ƒã€åæ€ï¼‰ã€ä¸Šä¸‹æ–‡ç›¸å…³æ€§å’Œè¯­ä¹‰çš„è™šå‡å˜åŒ–ã€‚éšåï¼Œå¯¹è¯ç”±ä¸“å®¶è¿›è¡Œæ³¨é‡Šï¼Œä¸¥æ ¼éµå¾ªåŠ¨æœºè®¿è°ˆæŠ€èƒ½ä»£ç ï¼ˆMISCï¼‰ï¼Œé‡ç‚¹å…³æ³¨MIå¯¹è¯çš„å¿ƒç†å’Œè¯­è¨€å­¦ç»´åº¦ã€‚æˆ‘ä»¬é€šè¿‡å»ºç«‹æ–°å‹åˆ†ç±»ä»»åŠ¡ï¼Œé‡‡ç”¨å¤šç§ç»å…¸æœºå™¨å­¦ä¹ å’Œå½“å‰æœ€å…ˆè¿›çš„è½¬æ¢å™¨æ–¹æ³•ï¼Œå…¨é¢è¯„ä¼°äº†IC-AnnoMIæ•°æ®é›†å’ŒChatGPTçš„æƒ…æ„Ÿæ¨ç†èƒ½åŠ›ä»¥åŠå¯¹é¢†åŸŸç»†èŠ‚çš„ç†è§£ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æ¸è¿›å¼æç¤ºç­–ç•¥çš„å½±å“ä»¥åŠå¢å¼ºæ•°æ®å¯¹ç¼“è§£IC-AnnoMIä¸­è¡¨ç°å‡ºçš„åè§çš„å½±å“ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸ºMIç¤¾åŒºä¸ä»…æä¾›äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œè€Œä¸”ä¸ºåœ¨ç›‘ç£ç¯å¢ƒä¸­ä½¿ç”¨LLMè¿›è¡Œå…±æƒ…æ–‡æœ¬ç”Ÿæˆè¿›è¡Œå¯¹è¯æ²»ç–—æä¾›äº†æœ‰ä»·å€¼è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12981v1">PDF</a> International Conference on Natural Language Processing and   Artificial Intelligence for Cyber Security (NLPAICS) 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—åˆ†æé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´å¦‚å¹»è§‰ã€é¹¦é¹‰å­¦èˆŒå’Œåè§æ˜¾ç°ç­‰æŒ‘æˆ˜ã€‚åœ¨å¤æ‚ã€æ•æ„Ÿå’Œä½èµ„æºçš„é¢†åŸŸä¸­ï¼Œè¿™äº›æŒ‘æˆ˜æ›´åŠ ä¸¥é‡ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶å¼•å…¥IC-AnnoMIæ•°æ®é›†ï¼Œå®ƒæ˜¯åŸºäºAnnoMIæ„å»ºçš„ä¸“å®¶æ ‡æ³¨åŠ¨æœºé¢è¯•ï¼ˆMIï¼‰æ•°æ®é›†ï¼Œé€šè¿‡åˆ©ç”¨LLMï¼ˆç‰¹åˆ«æ˜¯ChatGPTï¼‰ç”Ÿæˆä¸Šä¸‹æ–‡å¯¹è¯æ¥å®ç°ã€‚IC-AnnoMIé‡‡ç”¨ç²¾å‡†çš„ç›®æ ‡æç¤ºï¼Œè€ƒè™‘æ²»ç–—é£æ ¼ã€ä¸Šä¸‹æ–‡ç›¸å…³æ€§åŠè¯­ä¹‰å˜åŒ–ç­‰å› ç´ ã€‚å¯¹è¯ç”±ä¸“å®¶æŒ‰ç…§åŠ¨æœºé¢è¯•æŠ€èƒ½ä»£ç ï¼ˆMISCï¼‰è¿›è¡Œæ ‡æ³¨ï¼Œä¾§é‡äºMIå¯¹è¯çš„å¿ƒç†å’Œè¯­è¨€å­¦ç»´åº¦ã€‚æœ¬ç ”ç©¶å…¨é¢è¯„ä¼°äº†IC-AnnoMIæ•°æ®é›†å’ŒChatGPTçš„æƒ…æ„Ÿç†è§£èƒ½åŠ›ä»¥åŠå¯¹é¢†åŸŸç»†èŠ‚çš„æŠŠæ¡èƒ½åŠ›ï¼Œé€šè¿‡æ„å»ºæ–°å‹åˆ†ç±»ä»»åŠ¡æ¨¡å‹ï¼Œé‡‡ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œæœ€æ–°è½¬æ¢å™¨æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ¸è¿›å¼æç¤ºç­–ç•¥å’Œå¢å¼ºæ•°æ®å¯¹ç¼“è§£IC-AnnoMä¸­è¡¨ç°å‡ºçš„åè§çš„å½±å“ã€‚ç ”ç©¶è´¡çŒ®ä¸ä»…ä¸ºMIç¤¾åŒºæä¾›äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œè¿˜ä¸ºåœ¨ç›‘ç£ç¯å¢ƒä¸­ä½¿ç”¨LLMè¿›è¡ŒåŒç†å¿ƒæ–‡æœ¬ç”Ÿæˆå’Œå¯¹è¯æ²»ç–—æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨åŒ»ç–—ä¿å¥åˆ†æä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å¹»è§‰ã€æ¨¡ä»¿å’Œåè§ã€‚</li>
<li>IC-AnnoMIæ•°æ®é›†é€šè¿‡åˆ©ç”¨LLMsï¼ˆç‰¹åˆ«æ˜¯ChatGPTï¼‰ç”Ÿæˆä¸Šä¸‹æ–‡å¯¹è¯æ„å»ºè€Œæˆã€‚</li>
<li>IC-AnnoMIé‡‡ç”¨ç²¾å‡†çš„ç›®æ ‡æç¤ºï¼Œè€ƒè™‘æ²»ç–—é£æ ¼ã€ä¸Šä¸‹æ–‡ç›¸å…³æ€§ç­‰å› ç´ ã€‚</li>
<li>å¯¹è¯ç”±ä¸“å®¶æŒ‰ç…§åŠ¨æœºé¢è¯•æŠ€èƒ½ä»£ç ï¼ˆMISCï¼‰è¿›è¡Œæ ‡æ³¨ï¼Œä¾§é‡å¿ƒç†å’Œè¯­è¨€å­¦ç»´åº¦ã€‚</li>
<li>ç ”ç©¶å…¨é¢è¯„ä¼°äº†IC-AnnoMIæ•°æ®é›†å’ŒChatGPTçš„æƒ…æ„Ÿç†è§£èƒ½åŠ›åŠå¯¹é¢†åŸŸç»†èŠ‚çš„æŠŠæ¡èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œæœ€æ–°è½¬æ¢å™¨æ–¹æ³•æ„å»ºæ–°å‹åˆ†ç±»ä»»åŠ¡æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4290acdb23290ba8fa8a48219590de1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b36a8939997145e58f4d120bf37c88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3eddc93a665b8e4af052213679f49ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d6603f24157f5b76966dee0e5521348.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b9df522478d28a6cb214df72e90e35f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c1544e5ee077066be94d1d64d304ca6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Adaptations-of-AI-models-for-querying-the-LandMatrix-database-in-natural-language"><a href="#Adaptations-of-AI-models-for-querying-the-LandMatrix-database-in-natural-language" class="headerlink" title="Adaptations of AI models for querying the LandMatrix database in natural   language"></a>Adaptations of AI models for querying the LandMatrix database in natural   language</h2><p><strong>Authors:Fatiha Ait Kbir, JÃ©rÃ©my Bourgoin, RÃ©my Decoupes, Marie Gradeler, Roberto Interdonato</strong></p>
<p>The Land Matrix initiative (<a target="_blank" rel="noopener" href="https://landmatrix.org/">https://landmatrix.org</a>) and its global observatory aim to provide reliable data on large-scale land acquisitions to inform debates and actions in sectors such as agriculture, extraction, or energy in low- and middle-income countries. Although these data are recognized in the academic world, they remain underutilized in public policy, mainly due to the complexity of access and exploitation, which requires technical expertise and a good understanding of the database schema.   The objective of this work is to simplify access to data from different database systems. The methods proposed in this article are evaluated using data from the Land Matrix. This work presents various comparisons of Large Language Models (LLMs) as well as combinations of LLM adaptations (Prompt Engineering, RAG, Agents) to query different database systems (GraphQL and REST queries). The experiments are reproducible, and a demonstration is available online: <a target="_blank" rel="noopener" href="https://github.com/tetis-nlp/landmatrix-graphql-python">https://github.com/tetis-nlp/landmatrix-graphql-python</a>. </p>
<blockquote>
<p>Land Matrixå€¡è®®ï¼ˆ<a target="_blank" rel="noopener" href="https://landmatrix.org)åŠå…¶å…¨çƒè§‚æµ‹ç«™æ—¨åœ¨æä¾›æœ‰å…³å¤§è§„æ¨¡åœŸåœ°æ”¶è´­çš„å¯é æ•°æ®,ä»¥æ”¯æŒå†œä¸šã€é‡‡çŸ¿æˆ–èƒ½æºç­‰é¢†åŸŸçš„è¾©è®ºå’Œè¡ŒåŠ¨,ç›®æ ‡é’ˆå¯¹ä¸­ä½æ”¶å…¥å’Œæ–°å…´å¸‚åœºå›½å®¶.è™½ç„¶è¿™äº›æ•°æ®åœ¨å­¦æœ¯ç•Œå·²è·å¾—è®¤å¯,ä½†åœ¨å…¬å…±æ”¿ç­–æ–¹é¢çš„åˆ©ç”¨ç‡ä»ç„¶å¾ˆä½,ä¸»è¦æ˜¯å› ä¸ºæ•°æ®è·å–å’Œåˆ©ç”¨çš„å¤æ‚æ€§,è¿™éœ€è¦æŠ€æœ¯ä¸“é•¿å’Œè‰¯å¥½çš„æ•°æ®åº“æ¶æ„ç†è§£.è¿™é¡¹å·¥ä½œçš„ç›®æ ‡æ˜¯ç®€åŒ–ä»ä¸åŒæ•°æ®åº“ç³»ç»Ÿè®¿é—®æ•°æ®çš„è¿‡ç¨‹.æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä½¿ç”¨land/">https://landmatrix.orgï¼‰åŠå…¶å…¨çƒè§‚æµ‹ç«™æ—¨åœ¨æä¾›æœ‰å…³å¤§è§„æ¨¡åœŸåœ°æ”¶è´­çš„å¯é æ•°æ®ï¼Œä»¥æ”¯æŒå†œä¸šã€é‡‡çŸ¿æˆ–èƒ½æºç­‰é¢†åŸŸçš„è¾©è®ºå’Œè¡ŒåŠ¨ï¼Œç›®æ ‡é’ˆå¯¹ä¸­ä½æ”¶å…¥å’Œæ–°å…´å¸‚åœºå›½å®¶ã€‚è™½ç„¶è¿™äº›æ•°æ®åœ¨å­¦æœ¯ç•Œå·²è·å¾—è®¤å¯ï¼Œä½†åœ¨å…¬å…±æ”¿ç­–æ–¹é¢çš„åˆ©ç”¨ç‡ä»ç„¶å¾ˆä½ï¼Œä¸»è¦æ˜¯å› ä¸ºæ•°æ®è·å–å’Œåˆ©ç”¨çš„å¤æ‚æ€§ï¼Œè¿™éœ€è¦æŠ€æœ¯ä¸“é•¿å’Œè‰¯å¥½çš„æ•°æ®åº“æ¶æ„ç†è§£ã€‚è¿™é¡¹å·¥ä½œçš„ç›®æ ‡æ˜¯ç®€åŒ–ä»ä¸åŒæ•°æ®åº“ç³»ç»Ÿè®¿é—®æ•°æ®çš„è¿‡ç¨‹ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä½¿ç”¨Land</a> Matrixçš„æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å¯¹å„ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¯”è¾ƒï¼Œä»¥åŠLLMé€‚åº”çš„ç»„åˆï¼ˆæç¤ºå·¥ç¨‹ã€RAGã€ä»£ç†ï¼‰æ¥æŸ¥è¯¢ä¸åŒçš„æ•°æ®åº“ç³»ç»Ÿï¼ˆGraphQLå’ŒRESTæŸ¥è¯¢ï¼‰ã€‚å®éªŒå…·æœ‰å¯é‡å¤æ€§ï¼Œåœ¨çº¿æ¼”ç¤ºå¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/tetis-nlp/landmatrix-graphql-python%E3%80%82">https://github.com/tetis-nlp/landmatrix-graphql-pythonã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12961v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Land Matrixå€¡è®®åŠå…¶å…¨çƒè§‚æµ‹ç«™çš„ç›®æ ‡ï¼Œæ—¨åœ¨æä¾›æœ‰å…³å¤§è§„æ¨¡åœŸåœ°æ”¶è´­çš„å¯é æ•°æ®ï¼Œä»¥æ”¯æŒä½æ”¶å…¥å’Œä¸­ç­‰æ”¶å…¥å›½å®¶åœ¨å†œä¸šã€é‡‡çŸ¿æˆ–èƒ½æºç­‰éƒ¨é—¨çš„è¾©è®ºå’Œè¡ŒåŠ¨ã€‚å°½ç®¡è¿™äº›æ•°æ®åœ¨å­¦æœ¯ç•Œå¾—åˆ°è®¤å¯ï¼Œä½†åœ¨å…¬å…±æ”¿ç­–ä¸­çš„åˆ©ç”¨ç‡å´å¾ˆä½ï¼Œä¸»è¦åŸå› æ˜¯æ•°æ®è®¿é—®å’Œåˆ©ç”¨çš„å¤æ‚æ€§ï¼Œéœ€è¦æŠ€æœ¯ä¸“é•¿å’Œè‰¯å¥½çš„æ•°æ®åº“æ¶æ„ç†è§£ã€‚æœ¬æ–‡çš„ç›®æ ‡æ˜¯é€šè¿‡ç®€åŒ–å¯¹ä¸åŒæ•°æ®åº“ç³»ç»Ÿçš„æ•°æ®è®¿é—®æ¥æ”¹è¿›æ­¤æƒ…å†µã€‚æœ¬æ–‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•ä»¥åŠä½¿ç”¨GraphQLå’ŒRESTæŸ¥è¯¢è¿›è¡Œæ•°æ®åº“æŸ¥è¯¢çš„LLMé€‚åº”ç»„åˆï¼ˆPrompt Engineeringã€RAGã€Agentsï¼‰è¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ã€‚å®éªŒå…·æœ‰å¯é‡å¤æ€§ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ç½‘ä¸Šè¿›è¡Œæ¼”ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Land Matrixå€¡è®®è‡´åŠ›äºæä¾›å¤§è§„æ¨¡åœŸåœ°æ”¶è´­çš„å¯é æ•°æ®ï¼Œä»¥æ”¯æŒä½æ”¶å…¥å’Œä¸­ç­‰æ”¶å…¥å›½å®¶çš„å†œä¸šã€é‡‡çŸ¿å’Œèƒ½æºç­‰é¢†åŸŸçš„å†³ç­–ã€‚</li>
<li>è™½ç„¶Land Matrixæ•°æ®åœ¨å­¦æœ¯ç•Œå—åˆ°è®¤å¯ï¼Œä½†åœ¨å…¬å…±æ”¿ç­–ä¸­çš„åˆ©ç”¨ç‡è¾ƒä½ï¼ŒåŸå› åœ¨äºæ•°æ®è®¿é—®å’Œåˆ©ç”¨çš„å¤æ‚æ€§ã€‚</li>
<li>æœ¬æ–‡çš„ç›®æ ‡æ˜¯é€šè¿‡ç®€åŒ–å¯¹ä¸åŒæ•°æ®åº“ç³»ç»Ÿçš„æ•°æ®è®¿é—®æ¥æ”¹è¿›è¿™ä¸€çŠ¶å†µã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŸ¥è¯¢æ•°æ®åº“ç³»ç»Ÿæ—¶æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼ŒåŒ…æ‹¬é€šè¿‡Prompt Engineeringã€RAGå’ŒAgentsç­‰é€‚åº”æ–¹å¼ã€‚</li>
<li>é€šè¿‡GraphQLå’ŒRESTæŸ¥è¯¢è¿›è¡Œæ•°æ®åº“æŸ¥è¯¢æ˜¯æœ¬æ–‡ç ”ç©¶çš„é‡è¦å†…å®¹ã€‚</li>
<li>å®éªŒå…·æœ‰å¯é‡å¤æ€§ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ç½‘ä¸Šè¿›è¡Œæ¼”ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-36f73502ab3a8722966a2481b93909c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4167b7ae28a984aaca0dfc5ff6f5e369.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62395ae9325bdd8ab39ea4f188a06fdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3ed90f75415de4e880584272ec59325.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e7f628ccf801cb5d0bbbf0cf0f3c057.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fa34c51c24ff60b6841d4c0be2aa74b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SnakModel-Lessons-Learned-from-Training-an-Open-Danish-Large-Language-Model"><a href="#SnakModel-Lessons-Learned-from-Training-an-Open-Danish-Large-Language-Model" class="headerlink" title="SnakModel: Lessons Learned from Training an Open Danish Large Language   Model"></a>SnakModel: Lessons Learned from Training an Open Danish Large Language   Model</h2><p><strong>Authors:Mike Zhang, Max MÃ¼ller-Eberstein, Elisa Bassignana, Rob van der Goot</strong></p>
<p>We present SnakModel, a Danish large language model (LLM) based on Llama2-7B, which we continuously pre-train on 13.6B Danish words, and further tune on 3.7M Danish instructions. As best practices for creating LLMs for smaller language communities have yet to be established, we examine the effects of early modeling and training decisions on downstream performance throughout the entire training pipeline, including (1) the creation of a strictly curated corpus of Danish text from diverse sources; (2) the language modeling and instruction-tuning training process itself, including the analysis of intermediate training dynamics, and ablations across different hyperparameters; (3) an evaluation on eight language and culturally-specific tasks. Across these experiments SnakModel achieves the highest overall performance, outperforming multiple contemporary Llama2-7B-based models. By making SnakModel, the majority of our pre-training corpus, and the associated code available under open licenses, we hope to foster further research and development in Danish Natural Language Processing, and establish training guidelines for languages with similar resource constraints. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†SnakModelï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºLlama2-7Bçš„ä¸¹éº¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬å¯¹13.6Bä¸¹éº¦å•è¯è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œå¹¶è¿›ä¸€æ­¥ä¼˜åŒ–äº†370ä¸‡æ¡ä¸¹éº¦æŒ‡ä»¤ã€‚ç”±äºå°šæœªå»ºç«‹é’ˆå¯¹å°å‹è¯­è¨€ç¤¾åŒºçš„åˆ›å»ºLLMçš„æœ€ä½³å®è·µï¼Œæˆ‘ä»¬åœ¨æ•´ä¸ªè®­ç»ƒç®¡é“ä¸­æ£€æŸ¥äº†æ—©æœŸå»ºæ¨¡å’Œè®­ç»ƒå†³ç­–å¯¹ä¸‹æ¸¸æ€§èƒ½çš„å½±å“ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰åˆ›å»ºæ¥è‡ªä¸åŒæ¥æºçš„ä¸¥æ ¼ç¼–è¾‘çš„ä¸¹éº¦è¯­æ–‡æœ¬è¯­æ–™åº“ï¼›ï¼ˆ2ï¼‰è¯­è¨€å»ºæ¨¡å’ŒæŒ‡ä»¤è°ƒæ•´è®­ç»ƒè¿‡ç¨‹æœ¬èº«ï¼ŒåŒ…æ‹¬å¯¹ä¸­é—´è®­ç»ƒåŠ¨æ€çš„åˆ†æå’Œä¸åŒè¶…å‚æ•°çš„æ¶ˆèç ”ç©¶ï¼›ï¼ˆ3ï¼‰åœ¨å…«ä¸ªè¯­è¨€å’Œå…·æœ‰æ–‡åŒ–ç‰¹è‰²çš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨è¿™äº›å®éªŒä¸­ï¼ŒSnakModelå–å¾—äº†æœ€é«˜æ€»ä½“æ€§èƒ½ï¼Œè¶…è¿‡äº†å¤šä¸ªåŸºäºå½“ä»£Llama2-7Bçš„æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡å…¬å¼€è®¸å¯è¯æä¾›SnakModelã€å¤§éƒ¨åˆ†é¢„è®­ç»ƒè¯­æ–™åº“å’Œç›¸å…³ä»£ç ï¼Œå¸Œæœ›ä¿ƒè¿›ä¸¹éº¦è‡ªç„¶è¯­è¨€å¤„ç†çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ï¼Œå¹¶ä¸ºå…·æœ‰ç±»ä¼¼èµ„æºçº¦æŸçš„è¯­è¨€å»ºç«‹è®­ç»ƒæŒ‡å—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12956v1">PDF</a> Accepted at NoDaLiDa 2025 (oral)</p>
<p><strong>Summary</strong></p>
<p>SnakModelæ˜¯ä¸¹éº¦çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŸºäºLlama2-7Bè¿›è¡Œè¿ç»­é¢„è®­ç»ƒï¼Œåœ¨ä¸¹éº¦è¯­æ•°æ®ä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡çš„é¢„è®­ç»ƒä¸å¾®è°ƒã€‚è¯¥æ¨¡å‹å…³æ³¨äºå»ºç«‹é’ˆå¯¹å°å‹è¯­è¨€ç¤¾åŒºçš„æœ€ä½³å®è·µï¼Œç ”ç©¶äº†æ—©æœŸå»ºæ¨¡å’Œè®­ç»ƒå†³ç­–å¯¹ä¸‹æ¸¸æ€§èƒ½çš„å½±å“ï¼ŒåŒ…æ‹¬è¯­æ–™åº“çš„å»ºç«‹ã€è®­ç»ƒæµç¨‹å’Œå‚æ•°åˆ†æç­‰ã€‚SnakModelåœ¨å¤šä¸ªä¸¹éº¦æ–‡åŒ–å’Œè¯­è¨€ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¶…è¶Šäº†å…¶ä»–åŸºäºLlama2-7Bçš„æ¨¡å‹ã€‚æ¨¡å‹å’Œé¢„è®­ç»ƒè¯­æ–™åº“å·²å…¬å¼€æä¾›ï¼Œä»¥æ¨åŠ¨ä¸¹éº¦è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶å’Œå‘å±•ï¼Œå¹¶ä¸ºç±»ä¼¼çš„è¯­è¨€æä¾›è®­ç»ƒæŒ‡å—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SnakModelæ˜¯ä¸€ä¸ªåŸºäºLlama2-7Bçš„ä¸¹éº¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>SnakModelåœ¨å¤§é‡ä¸¹éº¦è¯­æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒå’Œå¾®è°ƒã€‚</li>
<li>ç ”ç©¶äº†å»ºæ¨¡å’Œè®­ç»ƒå†³ç­–å¯¹ä¸‹æ¸¸æ€§èƒ½çš„å½±å“ï¼ŒåŒ…æ‹¬è¯­æ–™åº“çš„å»ºç«‹ã€è®­ç»ƒæµç¨‹å’Œå‚æ•°åˆ†æã€‚</li>
<li>SnakModelåœ¨å¤šä¸ªæ–‡åŒ–å’Œè¯­è¨€ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>SnakModelè¶…è¶Šäº†å…¶ä»–åŸºäºLlama2-7Bçš„æ¨¡å‹åœ¨ä¸¹éº¦è¯­ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>SnakModelæ¨¡å‹å’Œé¢„è®­ç»ƒè¯­æ–™åº“å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ¨åŠ¨ä¸¹éº¦è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-049e2008e64d33f868ffea398d965334.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-360fbcb938f895d6ee65d162e83c514f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee5e986d6210387603e01bb37ffb3cf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-355ec5c6d2731ba7e0792bed0c07834b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-759fa9a927586594770615c733ccbc94.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image"><a href="#CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image" class="headerlink" title="CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image"></a>CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image</h2><p><strong>Authors:Wonseok Roh, Hwanhee Jung, Jong Wook Kim, Seunggwan Lee, Innfarn Yoo, Andreas Lugmayr, Seunggeun Chi, Karthik Ramani, Sangpil Kim</strong></p>
<p>Recently, generalizable feed-forward methods based on 3D Gaussian Splatting have gained significant attention for their potential to reconstruct 3D scenes using finite resources. These approaches create a 3D radiance field, parameterized by per-pixel 3D Gaussian primitives, from just a few images in a single forward pass. However, unlike multi-view methods that benefit from cross-view correspondences, 3D scene reconstruction with a single-view image remains an underexplored area. In this work, we introduce CATSplat, a novel generalizable transformer-based framework designed to break through the inherent constraints in monocular settings. First, we propose leveraging textual guidance from a visual-language model to complement insufficient information from a single image. By incorporating scene-specific contextual details from text embeddings through cross-attention, we pave the way for context-aware 3D scene reconstruction beyond relying solely on visual cues. Moreover, we advocate utilizing spatial guidance from 3D point features toward comprehensive geometric understanding under single-view settings. With 3D priors, image features can capture rich structural insights for predicting 3D Gaussians without multi-view techniques. Extensive experiments on large-scale datasets demonstrate the state-of-the-art performance of CATSplat in single-view 3D scene reconstruction with high-quality novel view synthesis. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºä¸‰ç»´é«˜æ–¯å±•å¼€çš„ä¸€èˆ¬æ€§å‰é¦ˆæ–¹æ³•å› å…¶åˆ©ç”¨æœ‰é™èµ„æºé‡å»ºä¸‰ç»´åœºæ™¯çš„æ½œåŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚è¿™äº›æ–¹æ³•ä»…é€šè¿‡ä¸€æ¬¡å‰å‘ä¼ é€’ï¼Œä»å‡ å¼ å›¾åƒä¸­åˆ›å»ºä¸€ä¸ªç”±åƒç´ çº§ä¸‰ç»´é«˜æ–¯åŸºæœ¬ä½“å‚æ•°åŒ–çš„ä¸‰ç»´è¾å°„åœºã€‚ç„¶è€Œï¼Œä¸åŒäºå—ç›Šäºè·¨è§†å›¾å¯¹åº”å…³ç³»çš„å¤šè§†å›¾æ–¹æ³•ï¼Œå•è§†å›¾å›¾åƒçš„3Dåœºæ™¯é‡å»ºä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†ç ”ç©¶çš„é¢†åŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†CATSplatï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¯æ‰©å±•çš„åŸºäºtransformerçš„æ¡†æ¶ï¼Œæ—¨åœ¨çªç ´å•ç›®è®¾ç½®ä¸­çš„å›ºæœ‰çº¦æŸã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æŒ‡å¯¼æ¥è¡¥å……å•ä¸€å›¾åƒä¸­çš„ä¿¡æ¯ä¸è¶³ã€‚é€šè¿‡ç»“åˆæ–‡æœ¬åµŒå…¥çš„åœºæ™¯ç‰¹å®šä¸Šä¸‹æ–‡ç»†èŠ‚è¿›è¡Œäº¤å‰æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬ä¸ºåŸºäºæ–‡æœ¬æŒ‡å¯¼çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸‰ç»´åœºæ™¯é‡å»ºé“ºå¹³äº†é“è·¯ï¼Œè€Œä¸ä»…ä»…ä¾èµ–äºè§†è§‰çº¿ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸»å¼ åˆ©ç”¨ä¸‰ç»´ç‚¹ç‰¹å¾çš„ç©ºé—´æŒ‡å¯¼æ¥å®ç°å•è§†å›¾è®¾ç½®ä¸‹çš„å…¨é¢å‡ ä½•ç†è§£ã€‚å€ŸåŠ©ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ï¼Œå›¾åƒç‰¹å¾å¯ä»¥æ•æ‰ä¸°å¯Œçš„ç»“æ„æ´å¯Ÿä¿¡æ¯ï¼Œä»¥é¢„æµ‹ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼Œæ— éœ€ä½¿ç”¨å¤šè§†å›¾æŠ€æœ¯ã€‚åœ¨å¤§å‹æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCATSplatåœ¨å•è§†å›¾ä¸‰ç»´åœºæ™¯é‡å»ºä¸­å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½è¿›è¡Œé«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12906v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>åŸºäº3Dé«˜æ–¯Splattingçš„é€šç”¨å‰é¦ˆæ–¹æ³•å› å…¶åˆ©ç”¨æœ‰é™èµ„æºé‡å»º3Dåœºæ™¯çš„æ½œåŠ›è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œå•è§†å›¾é‡å»ºä»æ˜¯ä¸€ä¸ªæœªå……åˆ†ç ”ç©¶çš„é¢†åŸŸã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CATSplatæ¡†æ¶ï¼Œæ—¨åœ¨çªç ´å•ç›®è®¾ç½®çš„å›ºæœ‰çº¦æŸã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æŒ‡å¯¼æ¥è¡¥å……å•å¼ å›¾åƒçš„ä¿¡æ¯ä¸è¶³ï¼Œå¹¶é€šè¿‡å¼•å…¥è·¨æ³¨æ„åŠ›æœºåˆ¶æ¥ç»“åˆåœºæ™¯ç‰¹å®šçš„ä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œä¸ºåªä¾èµ–è§†è§‰çº¿ç´¢çš„æƒ…å¢ƒæ„ŸçŸ¥çš„3Dåœºæ™¯é‡å»ºæä¾›äº†æ–°æ€è·¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¸»å¼ åˆ©ç”¨æ¥è‡ªä¸‰ç»´ç‚¹ç‰¹å¾çš„ç©ºé—´æŒ‡å¯¼ï¼Œä»¥å®ç°å•è§†å›¾è®¾ç½®ä¸‹çš„å…¨é¢å‡ ä½•ç†è§£ã€‚åœ¨å¤§å‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCATSplatåœ¨å•è§†å›¾3Dåœºæ™¯é‡å»ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†è§’åˆæˆå›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŸºäº3Dé«˜æ–¯Splattingçš„é€šç”¨å‰é¦ˆæ–¹æ³•å¯ç”¨äºé‡å»º3Dåœºæ™¯ã€‚</li>
<li>å•è§†å›¾é‡å»ºæ˜¯ä¸€ä¸ªæœªå……åˆ†ç ”ç©¶çš„é¢†åŸŸï¼Œè€ŒCATSplatæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>CATSplatåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æŒ‡å¯¼æ¥è¡¥å……å•å¼ å›¾åƒçš„ä¿¡æ¯ä¸è¶³ã€‚</li>
<li>é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶ç»“åˆåœºæ™¯ç‰¹å®šçš„ä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œä¸ºæƒ…å¢ƒæ„ŸçŸ¥çš„3Dåœºæ™¯é‡å»ºæä¾›æ–°æ€è·¯ã€‚</li>
<li>åˆ©ç”¨ä¸‰ç»´ç‚¹ç‰¹å¾çš„ç©ºé—´æŒ‡å¯¼å®ç°å•è§†å›¾è®¾ç½®ä¸‹çš„å…¨é¢å‡ ä½•ç†è§£ã€‚</li>
<li>CATSplatåœ¨å¤§å‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a36283c050fe8e7e8612d5d0f7b3ff3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f4a64437f03222250060d6f25a2e4e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84dce1e5ef85f6648fdb1164e1585b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e5cdf7b6fbf0b81ed13dd728405b419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933704d558b0afca6370cdf480d21167.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GIRAFFE-Design-Choices-for-Extending-the-Context-Length-of-Visual-Language-Models"><a href="#GIRAFFE-Design-Choices-for-Extending-the-Context-Length-of-Visual-Language-Models" class="headerlink" title="GIRAFFE: Design Choices for Extending the Context Length of Visual   Language Models"></a>GIRAFFE: Design Choices for Extending the Context Length of Visual   Language Models</h2><p><strong>Authors:Mukai Li, Lei Li, Shansan Gong, Qi Liu</strong></p>
<p>Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range modeling. Moreover, existing open-source VLMs lack systematic exploration into extending their context length, and commercial models often provide limited details. To tackle this, we aim to establish an effective solution that enhances long context performance of VLMs while preserving their capacities in short context scenarios. Towards this goal, we make the best design choice through extensive experiment settings from data curation to context window extending and utilizing: (1) we analyze data sources and length distributions to construct ETVLM - a data recipe to balance the performance across scenarios; (2) we examine existing position extending methods, identify their limitations and propose M-RoPE++ as an enhanced approach; we also choose to solely instruction-tune the backbone with mixed-source data; (3) we discuss how to better utilize extended context windows and propose hybrid-resolution training. Built on the Qwen-VL series model, we propose Giraffe, which is effectively extended to 128K lengths. Evaluated on extensive long context VLM benchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves state-of-the-art performance among similarly sized open-source long VLMs and is competitive with commercial model GPT-4V. We will open-source the code, data, and models. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å¯¹äºéœ€è¦å¤„ç†å¤šä¸ªå›¾åƒå’Œé«˜åˆ†è¾¨ç‡è§†é¢‘çš„åº”ç”¨ï¼ˆå¦‚è§†è§‰ä»£ç†ï¼‰æ¥è¯´ï¼Œå®ƒä»¬éœ€è¦å¢å¼ºé•¿ç¨‹å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„å¼€æºVLMsç¼ºä¹å¯¹æ‰©å±•å…¶ä¸Šä¸‹æ–‡é•¿åº¦çš„ç³»ç»Ÿæ€§æ¢ç´¢ï¼Œè€Œå•†ä¸šæ¨¡å‹å¾€å¾€æä¾›æœ‰é™çš„ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæé«˜VLMsåœ¨é•¿ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå…¶åœ¨çŸ­ä¸Šä¸‹æ–‡åœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ç›®æ ‡ï¼Œæˆ‘ä»¬é€šè¿‡ä»æ•°æ®æ”¶é›†åˆ°ä¸Šä¸‹æ–‡çª—å£æ‰©å±•çš„å¹¿æ³›å®éªŒè®¾ç½®æ¥åšå‡ºæœ€ä½³è®¾è®¡é€‰æ‹©ï¼šé¦–å…ˆåˆ†ææ•°æ®æºå’Œé•¿åº¦åˆ†å¸ƒä»¥æ„å»ºETVLMâ€”â€”ä¸€ç§å¹³è¡¡ä¸åŒåœºæ™¯æ€§èƒ½çš„åŸºå‡†ï¼›å…¶æ¬¡å¯¹ç°æœ‰ä½ç½®æ‰©å±•æ–¹æ³•è¿›è¡Œè¯„ä¼°å¹¶æå‡ºæ”¹è¿›çš„M-RoPE++æ–¹æ³•ï¼›æˆ‘ä»¬è¿˜é€‰æ‹©ä½¿ç”¨æ··åˆæºæ•°æ®å¯¹ä¸»å¹²è¿›è¡Œå•ç‹¬æŒ‡ä»¤å¾®è°ƒï¼›æœ€åè®¨è®ºå¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨æ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£å¹¶æå‡ºæ··åˆåˆ†è¾¨ç‡è®­ç»ƒã€‚åŸºäºQwen-VLç³»åˆ—æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†é•¿é¢ˆé¹¿æ¨¡å‹ï¼ˆGiraffeï¼‰ï¼Œå…¶é•¿åº¦æœ‰æ•ˆæ‰©å±•åˆ°128Kã€‚åœ¨è¯¸å¦‚VideoMMEå’ŒViusal Haystacksç­‰å¹¿æ³›çš„é•¿ä¸Šä¸‹æ–‡VLMåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGiraffeåœ¨åŒç±»è§„æ¨¡çš„å¼€æºé•¿VLMä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸å•†ä¸šæ¨¡å‹GPT-4Vç›¸æŠ—è¡¡ã€‚æˆ‘ä»¬å°†å¼€æºä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12735v1">PDF</a> Working in progress</p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å¯¹äºéœ€è¦å¤„ç†å¤šå¼ å›¾åƒå’Œé«˜åˆ†è¾¨ç‡è§†é¢‘çš„åº”ç”¨æ¥è¯´ï¼Œä»éœ€è¦æé«˜é•¿æœŸå»ºæ¨¡èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸€ç³»åˆ—æ–¹æ³•å¢å¼ºäº†å¤§æ¨¡å‹åœ¨é•¿æ–‡æœ¬è¯­å¢ƒä¸‹çš„æ€§èƒ½ï¼Œå¹¶èƒ½åœ¨çŸ­è¯­å¢ƒåœºæ™¯ä¸­ä¿æŒåŸæœ‰èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ETVLMæ•°æ®é…æ–¹æ¥å¹³è¡¡ä¸åŒåœºæ™¯çš„æ€§èƒ½ï¼Œæå‡ºäº†M-RoPE++æ–¹æ³•æ¥è§£å†³ç°æœ‰ä½ç½®æ‰©å±•æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶é€‰æ‹©äº†æ··åˆæºæ•°æ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨æ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£å¹¶æå‡ºäº†æ··åˆåˆ†è¾¨ç‡è®­ç»ƒç­–ç•¥ã€‚åŸºäºQwen-VLç³»åˆ—æ¨¡å‹æ„å»ºçš„Giraffeæ¨¡å‹è¢«æœ‰æ•ˆåœ°æ‰©å±•åˆ°äº†128Ké•¿åº¦ï¼Œåœ¨å¤šä¸ªé•¿è¯­å¢ƒè§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç ”ç©¶å›¢é˜Ÿå°†å¼€æºä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§æ¨¡å‹åœ¨å¤„ç†è§†è§‰è¯­è¨€ä»»åŠ¡æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†æ¶‰åŠå¤šä¸ªå›¾åƒå’Œé«˜åˆ†è¾¨ç‡è§†é¢‘çš„é•¿æ–‡æœ¬è¯­å¢ƒä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸€ç³»åˆ—ç­–ç•¥å¢å¼ºäº†æ¨¡å‹åœ¨é•¿æ–‡æœ¬è¯­å¢ƒä¸‹çš„æ€§èƒ½ï¼Œè¿™äº›ç­–ç•¥åŒ…æ‹¬è®¾è®¡ETVLMæ•°æ®é…æ–¹ã€æå‡ºM-RoPE++æ–¹æ³•å’Œæ··åˆæºæ•°æ®æŒ‡ä»¤å¾®è°ƒç­‰ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ¢è®¨äº†å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨æ‰©å±•çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶æå‡ºäº†æ··åˆåˆ†è¾¨ç‡è®­ç»ƒç­–ç•¥æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>Giraffeæ¨¡å‹æ˜¯åŸºäºQwen-VLç³»åˆ—æ¨¡å‹æ„å»ºçš„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†é•¿è¾¾128Kçš„æ–‡æœ¬é•¿åº¦ã€‚</li>
<li>Giraffeæ¨¡å‹åœ¨å¤šä¸ªé•¿æ–‡æœ¬è¯­å¢ƒè§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿè®¡åˆ’å°†ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¼€æºå…±äº«ã€‚è¿™å¯¹äºè§†è§‰è¯­è¨€ç ”ç©¶é¢†åŸŸæ˜¯ä¸€ä¸ªç§¯æçš„è´¡çŒ®ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c61d85f05ddb07edb69678a4988d06cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acdefa9fd97ead77a9892d709a5da9d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8ea217aa5ed1ec990189829d2553241.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5584682e34f54cf7c8ad3f9097505f92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-146936f77d70050bffc5ef9a0936013c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ChatDiT-A-Training-Free-Baseline-for-Task-Agnostic-Free-Form-Chatting-with-Diffusion-Transformers"><a href="#ChatDiT-A-Training-Free-Baseline-for-Task-Agnostic-Free-Form-Chatting-with-Diffusion-Transformers" class="headerlink" title="ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting   with Diffusion Transformers"></a>ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting   with Diffusion Transformers</h2><p><strong>Authors:Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Chen Liang, Tong Shen, Han Zhang, Huanzhang Dou, Yu Liu, Jingren Zhou</strong></p>
<p>Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the inherent in-context generation capabilities of pretrained diffusion transformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks with minimal or no architectural modifications. These capabilities are unlocked by concatenating self-attention tokens across multiple input and target images, combined with grouped and masked generation pipelines. Building upon this foundation, we present ChatDiT, a zero-shot, general-purpose, and interactive visual generation framework that leverages pretrained diffusion transformers in their original form, requiring no additional tuning, adapters, or modifications. Users can interact with ChatDiT to create interleaved text-image articles, multi-page picture books, edit images, design IP derivatives, or develop character design settings, all through free-form natural language across one or more conversational rounds. At its core, ChatDiT employs a multi-agent system comprising three key components: an Instruction-Parsing agent that interprets user-uploaded images and instructions, a Strategy-Planning agent that devises single-step or multi-step generation actions, and an Execution agent that performs these actions using an in-context toolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench arXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with diverse instructions and varying numbers of input and target images. Despite its simplicity and training-free approach, ChatDiT surpasses all competitors, including those specifically designed and trained on extensive multi-task datasets. We further identify key limitations of pretrained DiTs in zero-shot adapting to tasks. We release all code, agents, results, and intermediate outputs to facilitate further research at <a target="_blank" rel="noopener" href="https://github.com/ali-vilab/ChatDiT">https://github.com/ali-vilab/ChatDiT</a> </p>
<blockquote>
<p>æœ€è¿‘çš„arXiv:2410.15027å’ŒarXiv:2410.23775ç ”ç©¶çªå‡ºäº†é¢„è®­ç»ƒæ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰çš„å†…åœ¨ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡æ‹¼æ¥å¤šä¸ªè¾“å…¥å’Œç›®æ ‡å›¾åƒçš„è‡ªæ³¨æ„ä»¤ç‰Œï¼Œç»“åˆåˆ†ç»„å’Œæ©ç ç”Ÿæˆç®¡é“ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿæ— ç¼é€‚åº”å„ç§è§†è§‰ä»»åŠ¡ï¼Œè€Œæ— éœ€è¿›è¡Œæœ€å°çš„æ¶æ„ä¿®æ”¹ã€‚</p>
</blockquote>
<p>åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ChatDiTï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶æ ·æœ¬ã€é€šç”¨ã€äº¤äº’å¼çš„è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£å˜å‹å™¨ï¼Œæ— éœ€é¢å¤–çš„è°ƒæ•´ã€é€‚é…å™¨æˆ–ä¿®æ”¹ã€‚ç”¨æˆ·å¯ä»¥ä¸ChatDiTäº’åŠ¨ï¼Œåˆ›å»ºäº¤äº’å¼çš„æ–‡æœ¬-å›¾åƒæ–‡ç« ã€å¤šé¡µå›¾ç”»ä¹¦ã€ç¼–è¾‘å›¾åƒã€è®¾è®¡çŸ¥è¯†äº§æƒè¡ç”Ÿå“æˆ–å¼€å‘è§’è‰²è®¾è®¡è®¾ç½®ï¼Œæ‰€æœ‰è¿™äº›éƒ½å¯ä»¥é€šè¿‡ä¸€è½®æˆ–å¤šè½®å¯¹è¯çš„è‡ªç”±å½¢å¼è‡ªç„¶è¯­è¨€æ¥å®ç°ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12571v1">PDF</a> Tech report. Project page: <a target="_blank" rel="noopener" href="https://ali-vilab.github.io/ChatDiT-Page/">https://ali-vilab.github.io/ChatDiT-Page/</a></p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰å…·æœ‰å†…åœ¨ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œå¯è½»æ¾é€‚åº”å¤šç§è§†è§‰ä»»åŠ¡ï¼Œæ— éœ€è¿›è¡Œä»»ä½•æ¶æ„ä¿®æ”¹ã€‚åŸºäºæ­¤ï¼Œæå‡ºé›¶æ ·æœ¬ã€é€šç”¨ã€äº¤äº’å¼è§†è§‰ç”Ÿæˆæ¡†æ¶ChatDiTï¼Œåˆ©ç”¨åŸå§‹é¢„è®­ç»ƒæ‰©æ•£è½¬æ¢å™¨ï¼Œæ— éœ€é¢å¤–è°ƒæ•´ã€é€‚é…æˆ–ä¿®æ”¹ã€‚ç”¨æˆ·å¯ä¸ChatDiTäº¤äº’ï¼Œåˆ›å»ºæ–‡æœ¬å›¾åƒæ–‡ç« ã€å¤šé¡µå›¾ç”»ä¹¦ã€ç¼–è¾‘å›¾åƒã€è®¾è®¡IPè¡ç”Ÿå“æˆ–å¼€å‘è§’è‰²è®¾è®¡è®¾ç½®ç­‰ï¼Œå‡é€šè¿‡ä¸€è½®æˆ–å¤šè½®å¯¹è¯çš„è‡ªç”±å½¢å¼è‡ªç„¶è¯­è¨€å®ç°ã€‚å…¶æ ¸å¿ƒé‡‡ç”¨å¤šä»£ç†ç³»ç»Ÿï¼ŒåŒ…æ‹¬è§£é‡Šç”¨æˆ·ä¸Šä¼ çš„å›¾åƒå’ŒæŒ‡ä»¤çš„æŒ‡ä»¤è§£æä»£ç†ã€è®¾è®¡å•æ­¥æˆ–å¤šæ­¥ç”ŸæˆåŠ¨ä½œçš„è§„åˆ’ä»£ç†ä»¥åŠä½¿ç”¨æ‰©æ•£è½¬æ¢å™¨å·¥å…·é›†æ‰§è¡Œè¿™äº›åŠ¨ä½œçš„æ‰§è¡Œä»£ç†ã€‚åœ¨IDEA-Benchä¸Šå…¨é¢è¯„ä¼°ChatDiTï¼ŒåŒ…å«100ä¸ªçœŸå®ä¸–ç•Œè®¾è®¡ä»»åŠ¡å’Œ275ä¸ªå…·æœ‰ä¸åŒæŒ‡ä»¤å’Œè¾“å…¥ç›®æ ‡å›¾åƒçš„æƒ…å†µã€‚å°½ç®¡é‡‡ç”¨ç®€å•ä¸”æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼ŒChatDiTä»è¶…è¶Šæ‰€æœ‰ç«äº‰å¯¹æ‰‹ï¼ŒåŒ…æ‹¬é‚£äº›ä¸“é—¨è®¾è®¡å’Œè®­ç»ƒçš„å¤šä»»åŠ¡æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒæ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰å…·æœ‰ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œå¯é€‚åº”å¤šç§è§†è§‰ä»»åŠ¡ã€‚</li>
<li>ChatDiTæ˜¯ä¸€ä¸ªé›¶æ ·æœ¬ã€é€šç”¨ã€äº¤äº’å¼çš„è§†è§‰ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£è½¬æ¢å™¨ã€‚</li>
<li>ChatDiTå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’åˆ›å»ºæ–‡æœ¬å›¾åƒæ–‡ç« ã€å›¾ç”»ä¹¦ã€ç¼–è¾‘å›¾åƒç­‰ã€‚</li>
<li>ChatDiTé‡‡ç”¨å¤šä»£ç†ç³»ç»Ÿï¼ŒåŒ…æ‹¬æŒ‡ä»¤è§£æã€ç­–ç•¥è§„åˆ’å’Œæ‰§è¡Œä»£ç†ã€‚</li>
<li>ChatDiTåœ¨å¤šç§çœŸå®ä¸–ç•Œè®¾è®¡ä»»åŠ¡ä¸Šè¡¨ç°è¶…è¶Šç«äº‰å¯¹æ‰‹ã€‚</li>
<li>ChatDiTæ— éœ€ç‰¹æ®Šè®­ç»ƒæˆ–è°ƒæ•´ï¼Œå³å¯æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£è½¬æ¢å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a7ba58de4d6536e3e41584c9e13b2c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a1ed74b9e74940b3016d9b2704828e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c174ddf5de654a233fbc89583c508e79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cda8137ac38d2210b1c095a3700528.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7893d4470168de00e332f2331c0ad915.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LinguaLIFT-An-Effective-Two-stage-Instruction-Tuning-Framework-for-Low-Resource-Language-Tasks"><a href="#LinguaLIFT-An-Effective-Two-stage-Instruction-Tuning-Framework-for-Low-Resource-Language-Tasks" class="headerlink" title="LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for   Low-Resource Language Tasks"></a>LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for   Low-Resource Language Tasks</h2><p><strong>Authors:Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang</strong></p>
<p>Large language models (LLMs) have demonstrated impressive multilingual understanding and reasoning capabilities, driven by extensive pre-training multilingual corpora and fine-tuning instruction data. However, a performance gap persists between high-resource and low-resource language tasks due to language imbalance in the pre-training corpus, even using more low-resource data during fine-tuning. To alleviate this issue, we propose LinguaLIFT, a two-stage instruction tuning framework for advancing low-resource language tasks. An additional language alignment layer is first integrated into the LLM to adapt a pre-trained multilingual encoder, thereby enhancing multilingual alignment through code-switched fine-tuning. The second stage fine-tunes LLM with English-only instruction data while freezing the language alignment layer, allowing LLM to transfer task-specific capabilities from English to low-resource language tasks. Additionally, we introduce the Multilingual Math World Problem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and 10 high-resource languages, enabling comprehensive evaluation of multilingual reasoning. Experimental results show that LinguaLIFT outperforms several competitive baselines across MMWP and other widely used benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å¤šè¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™å¾—ç›Šäºå¤§é‡çš„é¢„è®­ç»ƒå¤šè¯­è¨€è¯­æ–™åº“å’Œç²¾ç»†è°ƒæ•´æŒ‡ä»¤æ•°æ®çš„é©±åŠ¨ã€‚ç„¶è€Œï¼Œç”±äºé¢„è®­ç»ƒè¯­æ–™åº“ä¸­çš„è¯­è¨€ä¸å¹³è¡¡ï¼Œé«˜èµ„æºè¯­è¨€ä»»åŠ¡å’Œä½èµ„æºè¯­è¨€ä»»åŠ¡ä¹‹é—´ä»ç„¶å­˜åœ¨æ€§èƒ½å·®è·ï¼Œå³ä½¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†æ›´å¤šçš„ä½èµ„æºæ•°æ®ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LinguaLIFTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æŒ‡ä»¤è°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨æ¨è¿›ä½èµ„æºè¯­è¨€ä»»åŠ¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªé¢å¤–çš„è¯­è¨€å¯¹é½å±‚é›†æˆåˆ°LLMä¸­ï¼Œä»¥é€‚åº”é¢„è®­ç»ƒçš„å¤šè¯­è¨€ç¼–ç å™¨ï¼Œä»è€Œé€šè¿‡ä»£ç åˆ‡æ¢å¾®è°ƒå¢å¼ºå¤šè¯­è¨€å¯¹é½ã€‚ç¬¬äºŒé˜¶æ®µä½¿ç”¨ä»…è‹±è¯­çš„æŒ‡ä»¤æ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å†»ç»“è¯­è¨€å¯¹é½å±‚ï¼Œä½¿LLMèƒ½å¤Ÿä»è‹±è¯­å‘ä½èµ„æºè¯­è¨€ä»»åŠ¡è½¬ç§»ç‰¹å®šä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†å¤šè¯­è¨€æ•°å­¦ä¸–ç•Œé—®é¢˜ï¼ˆMMWPï¼‰åŸºå‡†æµ‹è¯•ï¼Œå®ƒæ¶µç›–21ç§ä½èµ„æºã€17ç§ä¸­ç­‰èµ„æºå’Œ10ç§é«˜èµ„æºè¯­è¨€ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°å¤šè¯­è¨€æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLinguaLIFTåœ¨MMWPå’Œå…¶ä»–å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†å‡ ä¸ªç«äº‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12499v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¼ºå¤§çš„è·¨è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™å¾—ç›Šäºå¤§è§„æ¨¡é¢„è®­ç»ƒå¤šè¯­è¨€è¯­æ–™åº“å’Œç²¾ç»†è°ƒæ•´æŒ‡ä»¤æ•°æ®çš„æ”¯æŒã€‚ç„¶è€Œï¼Œç”±äºé¢„è®­ç»ƒè¯­æ–™åº“ä¸­çš„è¯­è¨€ä¸å¹³è¡¡ï¼Œé«˜èµ„æºè¯­è¨€ä¸ä½èµ„æºè¯­è¨€ä»»åŠ¡ä¹‹é—´ä»å­˜åœ¨æ€§èƒ½å·®è·ï¼Œå³ä½¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†æ›´å¤šçš„ä½èµ„æºæ•°æ®ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†LinguaLIFTè¿™ä¸€ä¸¤é˜¶æ®µæŒ‡ä»¤è°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ä½èµ„æºè¯­è¨€ä»»åŠ¡çš„è¡¨ç°ã€‚é¦–å…ˆï¼Œé€šè¿‡é›†æˆé¢å¤–çš„è¯­è¨€å¯¹é½å±‚åˆ°LLMä¸­ï¼Œä»¥é€‚åº”é¢„è®­ç»ƒçš„å¤šè¯­è¨€ç¼–ç å™¨ï¼Œé€šè¿‡ä»£ç åˆ‡æ¢å¾®è°ƒå¢å¼ºå¤šè¯­è¨€å¯¹é½ã€‚ç¬¬äºŒé˜¶æ®µä»…ä½¿ç”¨è‹±è¯­æŒ‡ä»¤æ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å†»ç»“è¯­è¨€å¯¹é½å±‚ï¼Œä½¿LLMèƒ½å¤Ÿå°†ç‰¹å®šä»»åŠ¡çš„èƒ½åŠ›ä»è‹±è¯­è½¬ç§»åˆ°ä½èµ„æºè¯­è¨€ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è·¨21ç§ä½èµ„æºã€17ç§ä¸­ç­‰èµ„æºå’Œ10ç§é«˜èµ„æºè¯­è¨€çš„Multilingual Math World Problemï¼ˆMMWPï¼‰åŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLinguaLIFTåœ¨MMWPå’Œå…¶ä»–å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºå‡ ä¸ªç«äº‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡å¼ºå¤§çš„å¤šè¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨é«˜èµ„æºä¸ä½èµ„æºè¯­è¨€ä»»åŠ¡é—´çš„æ€§èƒ½å·®è·ã€‚</li>
<li>LinguaLIFTæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæŒ‡ä»¤è°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ä½èµ„æºè¯­è¨€ä»»åŠ¡çš„è¡¨ç°ã€‚</li>
<li>LinguaLIFTé€šè¿‡é›†æˆè¯­è¨€å¯¹é½å±‚é€‚åº”é¢„è®­ç»ƒçš„å¤šè¯­è¨€ç¼–ç å™¨ï¼Œå¢å¼ºå¤šè¯­è¨€å¯¹é½ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå¾®è°ƒä»…ä½¿ç”¨è‹±è¯­æŒ‡ä»¤æ•°æ®ï¼Œå¹¶å†»ç»“è¯­è¨€å¯¹é½å±‚ï¼Œå®ç°ä»»åŠ¡èƒ½åŠ›ä»è‹±è¯­åˆ°ä½èµ„æºè¯­è¨€ä»»åŠ¡çš„è½¬ç§»ã€‚</li>
<li>å¼•å…¥äº†MMWPåŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LinguaLIFTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea4fce6aea3f7579a1fff7d097851cfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d73223e5c799e6d66b45940885e2c610.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd3a364b9fe693619580050cf0f15860.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5f77e7dd75a7f577023263ad84559fe.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Visual-Instruction-Tuning-with-500x-Fewer-Parameters-through-Modality-Linear-Representation-Steering"><a href="#Visual-Instruction-Tuning-with-500x-Fewer-Parameters-through-Modality-Linear-Representation-Steering" class="headerlink" title="Visual Instruction Tuning with 500x Fewer Parameters through Modality   Linear Representation-Steering"></a>Visual Instruction Tuning with 500x Fewer Parameters through Modality   Linear Representation-Steering</h2><p><strong>Authors:Jinhe Bi, Yujun Wang, Haokun Chen, Xun Xiao, Artur Hecker, Volker Tresp, Yunpu Ma</strong></p>
<p>Multimodal Large Language Models (MLLMs) have significantly advanced visual tasks by integrating visual representations into large language models (LLMs). The textual modality, inherited from LLMs, equips MLLMs with abilities like instruction following and in-context learning. In contrast, the visual modality enhances performance in downstream tasks by leveraging rich semantic content, spatial information, and grounding capabilities. These intrinsic modalities work synergistically across various visual tasks. Our research initially reveals a persistent imbalance between these modalities, with text often dominating output generation during visual instruction tuning. This imbalance occurs when using both full fine-tuning and parameter-efficient fine-tuning (PEFT) methods. We then found that re-balancing these modalities can significantly reduce the number of trainable parameters required, inspiring a direction for further optimizing visual instruction tuning. We introduce Modality Linear Representation-Steering (MoReS) to achieve the goal. MoReS effectively re-balances the intrinsic modalities throughout the model, where the key idea is to steer visual representations through linear transformations in the visual subspace across each model layer. To validate our solution, we composed LLaVA Steering, a suite of models integrated with the proposed MoReS method. Evaluation results show that the composed LLaVA Steering models require, on average, 500 times fewer trainable parameters than LoRA needs while still achieving comparable performance across three visual benchmarks and eight visual question-answering tasks. Last, we present the LLaVA Steering Factory, an in-house developed platform that enables researchers to quickly customize various MLLMs with component-based architecture for seamlessly integrating state-of-the-art models, and evaluate their intrinsic modality imbalance. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡å°†è§†è§‰è¡¨ç¤ºé›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œæ˜¾è‘—åœ°æ¨è¿›äº†è§†è§‰ä»»åŠ¡çš„å‘å±•ã€‚æ–‡æœ¬æ¨¡æ€ç»§æ‰¿è‡ªLLMsï¼Œä½¿MLLMså…·å¤‡æŒ‡ä»¤éµå¾ªå’Œä¸Šä¸‹æ–‡å­¦ä¹ ç­‰åŠŸèƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§†è§‰æ¨¡æ€é€šè¿‡åˆ©ç”¨ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹ã€ç©ºé—´ä¿¡æ¯å’Œå®šä½èƒ½åŠ›ï¼Œæé«˜äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚è¿™äº›å†…åœ¨æ¨¡æ€åœ¨ä¸åŒçš„è§†è§‰ä»»åŠ¡ä¸­ååŒå·¥ä½œã€‚æˆ‘ä»¬çš„ç ”ç©¶æœ€åˆæ­ç¤ºäº†è¿™äº›æ¨¡æ€ä¹‹é—´æŒä¹…çš„å¤±è¡¡ï¼Œæ–‡æœ¬é€šå¸¸åœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ä¸­ä¸»å¯¼è¾“å‡ºç”Ÿæˆã€‚è¿™ç§ä¸å¹³è¡¡åœ¨ä½¿ç”¨å…¨å¾®è°ƒï¼ˆFull Fine-tuningï¼‰å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•æ—¶éƒ½ä¼šå‘ç”Ÿã€‚ç„¶åæˆ‘ä»¬å‘ç°é‡æ–°å¹³è¡¡è¿™äº›æ¨¡æ€å¯ä»¥å¤§å¤§å‡å°‘æ‰€éœ€çš„è®­ç»ƒå‚æ•°æ•°é‡ï¼Œè¿™ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–è§†è§‰æŒ‡ä»¤è°ƒæ•´æä¾›äº†æ–¹å‘ã€‚æˆ‘ä»¬å¼•å…¥æ¨¡æ€çº¿æ€§è¡¨ç¤ºè½¬å‘ï¼ˆMoReSï¼‰æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚MoReSæœ‰æ•ˆåœ°åœ¨æ¨¡å‹ä¸­é‡æ–°å¹³è¡¡äº†å†…åœ¨æ¨¡æ€ï¼Œå…¶å…³é”®æ€æƒ³æ˜¯é€šè¿‡æ¯ä¸ªæ¨¡å‹å±‚çš„è§†è§‰å­ç©ºé—´çš„çº¿æ€§å˜æ¢æ¥å¼•å¯¼è§†è§‰è¡¨ç¤ºã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¼€å‘äº†LLaVAè½¬å‘å¥—ä»¶ï¼Œè¿™æ˜¯ä¸€å¥—ç»“åˆäº†æ‰€æå‡ºçš„MoReSæ–¹æ³•çš„æ¨¡å‹ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒLLaVAè½¬å‘å¥—ä»¶æ¨¡å‹å¹³å‡éœ€è¦æ¯”LoRAå°‘500å€çš„å¯è®­ç»ƒå‚æ•°ï¼ŒåŒæ—¶åœ¨ä¸‰ä¸ªè§†è§‰åŸºå‡†æµ‹è¯•å’Œå…«ä¸ªè§†è§‰é—®ç­”ä»»åŠ¡ä¸­ä»è¡¨ç°å‡ºç›¸å½“çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†LLaVAè½¬å‘å·¥å‚ï¼Œè¿™æ˜¯ä¸€ä¸ªå†…éƒ¨å¼€å‘çš„å¹³å°ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿè¿…é€Ÿå®šåˆ¶å„ç§MLLMsï¼Œé€šè¿‡åŸºäºç»„ä»¶çš„æ¶æ„æ— ç¼é›†æˆæœ€æ–°æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…¶å†…åœ¨æ¨¡æ€å¤±è¡¡æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12359v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é›†æˆè§†è§‰è¡¨å¾åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åå¯¹è§†è§‰ä»»åŠ¡çš„æ˜¾è‘—æ”¹è¿›ã€‚æ–‡æœ¬æ¨¡æ€èµ‹äºˆMLLMsæŒ‡ä»¤éµå¾ªå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ï¼Œè€Œè§†è§‰æ¨¡æ€åˆ™é€šè¿‡ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹ã€ç©ºé—´ä¿¡æ¯å’Œæ¥åœ°èƒ½åŠ›å¢å¼ºä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è§†è§‰æŒ‡ä»¤å¾®è°ƒä¸­å­˜åœ¨æ¨¡æ€ä¹‹é—´çš„æŒä¹…ä¸å¹³è¡¡ï¼Œæ–‡æœ¬å¾€å¾€ä¸»å¯¼è¾“å‡ºç”Ÿæˆã€‚é€šè¿‡é‡æ–°å¹³è¡¡è¿™äº›æ¨¡æ€ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘æ‰€éœ€çš„è®­ç»ƒå‚æ•°æ•°é‡ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†æ¨¡æ€çº¿æ€§è¡¨ç¤ºè½¬å‘ï¼ˆMoReSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°å¹³è¡¡äº†æ¨¡å‹ä¸­çš„å†…åœ¨æ¨¡æ€ï¼Œå¹¶é€šè¿‡çº¿æ€§å˜æ¢å¼•å¯¼è§†è§‰è¡¨ç¤ºã€‚æå‡ºçš„LLaVA Steeringæ–¹æ³•ç»“åˆäº†MoReSï¼Œå¹¶åœ¨ä¸‰ä¸ªè§†è§‰åŸºå‡†æµ‹è¯•å’Œå…«ä¸ªè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå®ç°äº†è‰¯å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†æ‰€éœ€çš„è®­ç»ƒå‚æ•°ã€‚æœ€åï¼Œå¼€å‘äº†LLaVA Steering Factoryå¹³å°ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜å¿«é€Ÿå®šåˆ¶å„ç§MLLMsï¼Œå¹¶è¯„ä¼°å…¶å†…åœ¨æ¨¡æ€ä¸å¹³è¡¡æƒ…å†µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsé€šè¿‡é›†æˆè§†è§‰è¡¨å¾æ˜¾è‘—æé«˜äº†è§†è§‰ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>MLLMså…·å¤‡æ–‡æœ¬å’Œè§†è§‰ä¸¤ç§æ¨¡æ€ï¼Œåˆ†åˆ«èµ‹äºˆæ¨¡å‹ä¸åŒçš„èƒ½åŠ›ã€‚</li>
<li>åœ¨è§†è§‰æŒ‡ä»¤å¾®è°ƒä¸­å­˜åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€çš„ä¸å¹³è¡¡ã€‚</li>
<li>æ¨¡æ€é‡æ–°å¹³è¡¡å¯ä»¥æ˜¾è‘—å‡å°‘è®­ç»ƒå‚æ•°éœ€æ±‚ã€‚</li>
<li>å¼•å…¥äº†MoReSæ–¹æ³•æ¥å®ç°æ¨¡æ€çš„å¹³è¡¡ï¼Œå¹¶é€šè¿‡çº¿æ€§å˜æ¢å¼•å¯¼è§†è§‰è¡¨ç¤ºã€‚</li>
<li>LLaVA Steeringæ–¹æ³•ç»“åˆäº†MoReSï¼Œåœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸Šå®ç°äº†è‰¯å¥½çš„æ€§èƒ½å¹¶å‡å°‘äº†è®­ç»ƒå‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12359">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4e6a70ca2b6ee9f15176703595af38b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-075864893a383bec9e4a3292438bfba6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-113ca071c69e490cb778f5af5c73008e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7854dbc881a567a1051b407344200167.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-Open-Source-Advantage-in-Large-Language-Models-LLMs"><a href="#The-Open-Source-Advantage-in-Large-Language-Models-LLMs" class="headerlink" title="The Open Source Advantage in Large Language Models (LLMs)"></a>The Open Source Advantage in Large Language Models (LLMs)</h2><p><strong>Authors:Jiya Manchanda, Laura Boettcher, Matheus Westphalen, Jasser Jasser</strong></p>
<p>Large language models (LLMs) mark a key shift in natural language processing (NLP), having advanced text generation, translation, and domain-specific reasoning. Closed-source models like GPT-4, powered by proprietary datasets and extensive computational resources, lead with state-of-the-art performance today. However, they face criticism for their â€œblack boxâ€ nature and for limiting accessibility in a manner that hinders reproducibility and equitable AI development. By contrast, open-source initiatives like LLaMA and BLOOM prioritize democratization through community-driven development and computational efficiency. These models have significantly reduced performance gaps, particularly in linguistic diversity and domain-specific applications, while providing accessible tools for global researchers and developers. Notably, both paradigms rely on foundational architectural innovations, such as the Transformer framework by Vaswani et al. (2017). Closed-source models excel by scaling effectively, while open-source models adapt to real-world applications in underrepresented languages and domains. Techniques like Low-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source models to achieve competitive results despite limited resources. To be sure, the tension between closed-source and open-source approaches underscores a broader debate on transparency versus proprietary control in AI. Ethical considerations further highlight this divide. Closed-source systems restrict external scrutiny, while open-source models promote reproducibility and collaboration but lack standardized auditing documentation frameworks to mitigate biases. Hybrid approaches that leverage the strengths of both paradigms are likely to shape the future of LLM innovation, ensuring accessibility, competitive technical performance, and ethical deployment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ ‡å¿—ç€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å…³é”®è½¬å˜ï¼Œå…·å¤‡å…ˆè¿›çš„æ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘å’Œé¢†åŸŸç‰¹å®šæ¨ç†èƒ½åŠ›ã€‚åƒGPT-4è¿™æ ·çš„å°é—­æºæ¨¡å‹ï¼Œä¾é ä¸“æœ‰æ•°æ®é›†å’Œå¤§é‡çš„è®¡ç®—èµ„æºï¼Œç›®å‰å¤„äºæœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚ç„¶è€Œï¼Œå®ƒä»¬å› â€œé»‘ç®±â€æ€§è´¨å’Œè®¿é—®é™åˆ¶è€Œå—åˆ°æ‰¹è¯„ï¼Œé˜»ç¢äº†å¯é‡å¤æ€§å’Œå…¬å¹³çš„AIå‘å±•ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLLaMAå’ŒBLOOMç­‰å¼€æºå€¡è®®é€šè¿‡ç¤¾åŒºé©±åŠ¨çš„å¼€å‘å’Œè®¡ç®—æ•ˆç‡æ¥å®ç°æ°‘ä¸»åŒ–ã€‚è¿™äº›æ¨¡å‹åœ¨è¯­è¨€å¤šæ ·æ€§å’Œé¢†åŸŸç‰¹å®šåº”ç”¨ç­‰æ–¹é¢æ˜¾è‘—ç¼©å°äº†æ€§èƒ½å·®è·ï¼Œä¸ºå…¨çƒç ”ç©¶è€…å’Œå¼€å‘è€…æä¾›äº†å¯è®¿é—®çš„å·¥å…·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸¤ç§èŒƒå¼éƒ½ä¾èµ–äºåŸºç¡€æ¶æ„åˆ›æ–°ï¼Œå¦‚Vaswaniç­‰äººæå‡ºçš„Transformeræ¡†æ¶ï¼ˆ2017å¹´ï¼‰ã€‚å°é—­æºæ¨¡å‹é€šè¿‡æœ‰æ•ˆæ‰©å±•è€Œå“è¶Šï¼Œè€Œå¼€æºæ¨¡å‹åˆ™é€‚åº”äºä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€å’Œé¢†åŸŸçš„å®é™…åº”ç”¨ã€‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å’ŒæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ç­‰æŠ€æœ¯ä½¿å¼€æºæ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„èµ„æºä¸‹å®ç°å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚å¯ä»¥è‚¯å®šçš„æ˜¯ï¼Œå°é—­æºå’Œå¼€æºæ–¹æ³•ä¹‹é—´çš„ç´§å¼ å…³ç³»çªæ˜¾äº†äººå·¥æ™ºèƒ½ä¸­é€æ˜åº¦å’Œä¸“æœ‰æ§åˆ¶ä¹‹é—´æ›´å¹¿æ³›çš„è¾©è®ºã€‚é“å¾·è€ƒé‡è¿›ä¸€æ­¥çªæ˜¾äº†è¿™ä¸€åˆ†æ­§ã€‚å°é—­æºç³»ç»Ÿé™åˆ¶å¤–éƒ¨å®¡æŸ¥ï¼Œè€Œå¼€æºæ¨¡å‹ä¿ƒè¿›å¯é‡å¤æ€§å’Œåä½œï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–çš„å®¡è®¡æ–‡æ¡£æ¡†æ¶æ¥å‡è½»åè§ã€‚å¾ˆå¯èƒ½åˆ©ç”¨ä¸¤ç§èŒƒå¼çš„ä¼˜åŠ¿ç›¸ç»“åˆçš„æ··åˆæ–¹æ³•å°†å¡‘é€ LLMåˆ›æ–°çš„æœªæ¥ï¼Œç¡®ä¿å¯è®¿é—®æ€§ã€å…·æœ‰ç«äº‰åŠ›çš„æŠ€æœ¯æ€§èƒ½å’Œé“å¾·éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12004v1">PDF</a> 7 pages, 0 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸæ€èµ·äº†ä¸€åœºé‡å¤§å˜é©ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘å’Œé¢†åŸŸç‰¹å®šæ¨ç†èƒ½åŠ›ã€‚ç›®å‰ï¼Œä»¥GPT-4ä¸ºä»£è¡¨çš„å°é—­æºæ¨¡å‹å‡­å€Ÿä¸“æœ‰æ•°æ®é›†å’Œå¼ºå¤§çš„è®¡ç®—èµ„æºè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ€§èƒ½ï¼Œä½†ä¹Ÿå› â€œé»‘ç®±â€æ€§è´¨å’Œé™åˆ¶å¯è®¿é—®æ€§è€Œå—åˆ°æ‰¹è¯„ï¼Œé˜»ç¢äº†å¯é‡å¤æ€§å’Œå…¬å¹³çš„äººå·¥æ™ºèƒ½å‘å±•ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»¥LLaMAå’ŒBLOOMä¸ºä»£è¡¨çš„å¼€æºå€¡è®®é€šè¿‡ç¤¾åŒºé©±åŠ¨å¼€å‘å’Œè®¡ç®—æ•ˆç‡æ¥å®ç°æ°‘ä¸»åŒ–ï¼Œåœ¨ç¼©å°æ€§èƒ½å·®è·æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€å¤šæ ·æ€§å’Œé¢†åŸŸç‰¹å®šåº”ç”¨æ–¹é¢ã€‚ä¸¤è€…éƒ½ä¾èµ–äºåŸºç¡€æ¶æ„åˆ›æ–°ï¼Œå¦‚Vaswaniç­‰äººåœ¨2017å¹´æå‡ºçš„Transformeræ¡†æ¶ã€‚å°½ç®¡å­˜åœ¨äº‰è®ºå’Œä¼¦ç†è€ƒé‡ï¼Œä½†å°é—­æºæ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„å¼ åŠ›ä½“ç°äº†é€æ˜åº¦å’Œä¸“æœ‰æ§åˆ¶ä¹‹é—´çš„æ›´å¹¿æ³›è¾©è®ºã€‚æœªæ¥ï¼Œèåˆä¸¤ç§æ¨¡å¼çš„ä¼˜åŠ¿ï¼Œç¡®ä¿å¯è®¿é—®æ€§ã€æŠ€æœ¯ç«äº‰åŠ›å’Œé“å¾·éƒ¨ç½²çš„æ··åˆæ–¹æ³•å¯èƒ½å¡‘é€ LLMåˆ›æ–°çš„æœªæ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ ‡å¿—ç€NLPé¢†åŸŸçš„å…³é”®è½¬å˜ï¼Œå…·æœ‰å…ˆè¿›çš„æ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘å’Œé¢†åŸŸç‰¹å®šæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å°é—­æºæ¨¡å‹å¦‚GPT-4å‡­å€Ÿä¸“æœ‰æ•°æ®å’Œè®¡ç®—èµ„æºè¾¾åˆ°ä¸šç•Œé¢†å…ˆï¼Œä½†é¢ä¸´â€œé»‘ç®±â€å’Œå¯è®¿é—®æ€§æ‰¹è¯„ã€‚</li>
<li>å¼€æºå€¡è®®å¦‚LLaMAå’ŒBLOOMé€šè¿‡ç¤¾åŒºé©±åŠ¨å’Œè®¡ç®—æ•ˆç‡å®ç°æ°‘ä¸»åŒ–ï¼Œç¼©å°æ€§èƒ½å·®è·ã€‚</li>
<li>å¼€æºæ¨¡å‹åœ¨è¯­è¨€å¤šæ ·æ€§å’Œé¢†åŸŸç‰¹å®šåº”ç”¨æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—è¿›å±•ã€‚</li>
<li>å°é—­æºå’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„å¼ åŠ›åæ˜ äº†é€æ˜åº¦å’Œä¸“æœ‰æ§åˆ¶ä¹‹é—´çš„æ›´å¹¿æ³›è¾©è®ºã€‚</li>
<li>å°é—­æºç³»ç»Ÿé™åˆ¶å¤–éƒ¨å®¡æŸ¥ï¼Œè€Œå¼€æºæ¨¡å‹ä¿ƒè¿›å¯é‡å¤æ€§å’Œåä½œï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–å®¡è®¡æ–‡æ¡£æ¡†æ¶æ¥å‡è½»åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c42176f0acde77cc475f23de17176335.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Advancing-Comprehensive-Aesthetic-Insight-with-Multi-Scale-Text-Guided-Self-Supervised-Learning"><a href="#Advancing-Comprehensive-Aesthetic-Insight-with-Multi-Scale-Text-Guided-Self-Supervised-Learning" class="headerlink" title="Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided   Self-Supervised Learning"></a>Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided   Self-Supervised Learning</h2><p><strong>Authors:Yuti Liu, Shice Liu, Junyuan Gao, Pengtao Jiang, Hao Zhang, Jinwei Chen, Bo Li</strong></p>
<p>Image Aesthetic Assessment (IAA) is a vital and intricate task that entails analyzing and assessing an imageâ€™s aesthetic values, and identifying its highlights and areas for improvement. Traditional methods of IAA often concentrate on a single aesthetic task and suffer from inadequate labeled datasets, thus impairing in-depth aesthetic comprehension. Despite efforts to overcome this challenge through the application of Multi-modal Large Language Models (MLLMs), such models remain underdeveloped for IAA purposes. To address this, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic insight. Central to our approach is an innovative multi-scale text-guided self-supervised learning technique. This technique features a multi-scale feature alignment module and capitalizes on a wealth of unlabeled data in a self-supervised manner to structurally and functionally enhance aesthetic ability. The empirical evidence indicates that accompanied with extensive instruct-tuning, our model sets new state-of-the-art benchmarks across multiple tasks, including aesthetic scoring, aesthetic commenting, and personalized image aesthetic assessment. Remarkably, it also demonstrates zero-shot learning capabilities in the emerging task of aesthetic suggesting. Furthermore, for personalized image aesthetic assessment, we harness the potential of in-context learning and showcase its inherent advantages. </p>
<blockquote>
<p>å›¾åƒç¾å­¦è¯„ä¼°ï¼ˆIAAï¼‰æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä¸”å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦åˆ†æå’Œè¯„ä¼°å›¾åƒçš„ç¾å­¦ä»·å€¼ï¼Œå¹¶è¯†åˆ«å…¶äº®ç‚¹å’Œæ”¹è¿›é¢†åŸŸã€‚ä¼ ç»Ÿçš„IAAæ–¹æ³•é€šå¸¸é›†ä¸­äºå•ä¸€çš„ç¾å­¦ä»»åŠ¡ï¼Œå¹¶å—åˆ°æ ‡æ³¨æ•°æ®é›†ä¸è¶³çš„å›°æ‰°ï¼Œä»è€Œå½±å“æ·±åº¦ç¾å­¦ç†è§£ã€‚å°½ç®¡æœ‰äººå°è¯•é€šè¿‡åº”ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨IAAç›®çš„æ–¹é¢ä»ç„¶å‘å±•ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨é¢çš„ç¾å­¦MLLMæ¨¡å‹ï¼Œèƒ½å¤Ÿæ´å¯Ÿç»†å¾®çš„ç¾å­¦å·®å¼‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ç§åˆ›æ–°çš„å¤šå°ºåº¦æ–‡æœ¬å¼•å¯¼è‡ªæˆ‘ç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯å…·æœ‰å¤šå°ºåº¦ç‰¹å¾å¯¹é½æ¨¡å—ï¼Œä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼åˆ©ç”¨å¤§é‡æ— æ ‡ç­¾æ•°æ®ï¼Œä»ç»“æ„å’ŒåŠŸèƒ½ä¸Šå¢å¼ºç¾å­¦èƒ½åŠ›ã€‚ç»éªŒè¯æ®è¡¨æ˜ï¼Œé€šè¿‡å¹¿æ³›çš„æŒ‡ä»¤å¾®è°ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè®¾å®šäº†æ–°çš„æœ€æ–°åŸºå‡†ï¼ŒåŒ…æ‹¬ç¾å­¦è¯„åˆ†ã€ç¾å­¦è¯„è®ºå’Œä¸ªæ€§åŒ–å›¾åƒç¾å­¦è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒè¿˜åœ¨æ–°å…´çš„ç¾å­¦å»ºè®®ä»»åŠ¡ä¸­å±•ç¤ºäº†é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¯¹äºä¸ªæ€§åŒ–å›¾åƒç¾å­¦è¯„ä¼°ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ½œåŠ›ï¼Œå¹¶å±•ç¤ºäº†å…¶å†…åœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11952v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>å›¾åƒç¾å­¦è¯„ä¼°ï¼ˆIAAï¼‰æ˜¯ä¸€é¡¹é‡è¦è€Œå¤æ‚çš„ä»»åŠ¡ï¼Œæ¶‰åŠåˆ†æå›¾åƒçš„ç¾å­¦ä»·å€¼å¹¶è¯†åˆ«å…¶ä¼˜ç‚¹å’Œæ”¹è¿›ç‚¹ã€‚ä¼ ç»Ÿæ–¹æ³•é›†ä¸­åœ¨å•ä¸€ç¾å­¦ä»»åŠ¡ä¸Šï¼Œç”±äºç¼ºå°‘è¶³å¤Ÿçš„æ ‡è®°æ•°æ®é›†ï¼Œæ— æ³•è¿›è¡Œæ·±å…¥çš„ç¾å­¦ç†è§£ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒç¾å­¦è¯„ä¼°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åˆ›æ–°çš„å¤šå°ºåº¦æ–‡æœ¬å¼•å¯¼è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œå…·æœ‰å¤šå°ºåº¦ç‰¹å¾å¯¹é½æ¨¡å—ï¼Œåˆ©ç”¨å¤§é‡æ— æ ‡ç­¾æ•°æ®è¿›è¡Œè‡ªç›‘ç£è®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„ç¾å­¦èƒ½åŠ›ã€‚å®éªŒè¯æ®è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼ŒåŒ…æ‹¬ç¾å­¦è¯„åˆ†ã€ç¾å­¦è¯„è®ºå’Œä¸ªæ€§åŒ–å›¾åƒç¾å­¦è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨ç¾å­¦å»ºè®®ç­‰æ–°å…´ä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶æ¢è®¨äº†ä¸ªæ€§åŒ–å›¾åƒç¾å­¦è¯„ä¼°ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç¾å­¦è¯„ä¼°ï¼ˆIAAï¼‰æ˜¯åˆ†æå›¾åƒç¾å­¦ä»·å€¼çš„å…³é”®ä»»åŠ¡ï¼Œæ¶‰åŠè¯†åˆ«å’Œè¯„ä¼°å›¾åƒçš„ä¼˜ç¼ºç‚¹ã€‚</li>
<li>ä¼ ç»ŸIAAæ–¹æ³•é›†ä¸­äºå•ä¸€ä»»åŠ¡ï¼Œå¹¶å—é™äºç¼ºä¹è¶³å¤Ÿçš„æ ‡è®°æ•°æ®é›†ï¼Œå½±å“äº†æ·±åº¦ç¾å­¦ç†è§£ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”¨äºå›¾åƒç¾å­¦è¯„ä¼°ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨åˆ›æ–°çš„å¤šå°ºåº¦æ–‡æœ¬å¼•å¯¼è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤šå°ºåº¦ç‰¹å¾å¯¹é½æ¨¡å—ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å¤§é‡æ— æ ‡ç­¾æ•°æ®è¿›è¡Œè‡ªç›‘ç£è®­ç»ƒï¼Œä»¥æé«˜ç¾å­¦è¯„ä¼°èƒ½åŠ›ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç¾å­¦è¯„åˆ†ã€ç¾å­¦è¯„è®ºå’Œä¸ªæ€§åŒ–å›¾åƒç¾å­¦è¯„ä¼°ç­‰å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3371a0ad85573763d5e80e3b56ca05f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b24b44334d6747011160b9875a3a52b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eca52326b5952845dcb99ed6a0a7f56f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0af7274e26d607054a721848d1beb9ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20a1f99dd6ee2f984e8cac7201640545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac11eedacf7e5369601285eb6536c1d0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GeoX-Geometric-Problem-Solving-Through-Unified-Formalized-Vision-Language-Pre-training"><a href="#GeoX-Geometric-Problem-Solving-Through-Unified-Formalized-Vision-Language-Pre-training" class="headerlink" title="GeoX: Geometric Problem Solving Through Unified Formalized   Vision-Language Pre-training"></a>GeoX: Geometric Problem Solving Through Unified Formalized   Vision-Language Pre-training</h2><p><strong>Authors:Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, Conghui He, Botian Shi, Tao Chen, Junchi Yan, Bo Zhang</strong></p>
<p>Despite their proficiency in general tasks, Multi-modal Large Language Models (MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands understanding diagrams, interpreting symbols, and performing complex reasoning. This limitation arises from their pre-training on natural images and texts, along with the lack of automated verification in the problem-solving process. Besides, current geometric specialists are limited by their task-specific designs, making them less effective for broader geometric problems. To this end, we present GeoX, a multi-modal large model focusing on geometric understanding and reasoning tasks. Given the significant differences between geometric diagram-symbol and natural image-text, we introduce unimodal pre-training to develop a diagram encoder and symbol decoder, enhancing the understanding of geometric images and corpora. Furthermore, we introduce geometry-language alignment, an effective pre-training paradigm that bridges the modality gap between unimodal geometric experts. We propose a Generator-And-Sampler Transformer (GS-Former) to generate discriminative queries and eliminate uninformative representations from unevenly distributed geometric signals. Finally, GeoX benefits from visual instruction tuning, empowering it to take geometric images and questions as input and generate verifiable solutions. Experiments show that GeoX outperforms both generalists and geometric specialists on publicly recognized benchmarks, such as GeoQA, UniGeo, Geometry3K, and PGPS9k. </p>
<blockquote>
<p>å°½ç®¡åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è‡ªåŠ¨å‡ ä½•é—®é¢˜æ±‚è§£ï¼ˆGPSï¼‰æ–¹é¢ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å´é‡åˆ°äº†å›°éš¾ã€‚GPSè¦æ±‚ç†è§£å›¾è¡¨ã€è§£é‡Šç¬¦å·å’Œè¿›è¡Œå¤æ‚æ¨ç†ã€‚è¿™ä¸€å±€é™æ€§æºäºå®ƒä»¬å¯¹è‡ªç„¶å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒï¼Œä»¥åŠé—®é¢˜æ±‚è§£è¿‡ç¨‹ä¸­ç¼ºä¹è‡ªåŠ¨åŒ–éªŒè¯ã€‚æ­¤å¤–ï¼Œå½“å‰çš„å‡ ä½•ä¸“å®¶å—é™äºå…¶ç‰¹å®šä»»åŠ¡è®¾è®¡ï¼Œä½¿å¾—ä»–ä»¬å¯¹äºæ›´å¹¿æ³›çš„å‡ ä½•é—®é¢˜æ•ˆæœè¾ƒå·®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GeoXï¼Œä¸€ä¸ªä¸“æ³¨äºå‡ ä½•ç†è§£å’Œæ¨ç†ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ã€‚è€ƒè™‘åˆ°å‡ ä½•å›¾è¡¨ç¬¦å·ä¸è‡ªç„¶å›¾åƒæ–‡æœ¬ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†å•æ¨¡æ€é¢„è®­ç»ƒï¼Œä»¥å¼€å‘å›¾è¡¨ç¼–ç å™¨å’Œç¬¦å·è§£ç å™¨ï¼Œæé«˜å¯¹å‡ ä½•å›¾åƒå’Œè¯­æ–™çš„ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å‡ ä½•è¯­è¨€å¯¹é½ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„é¢„è®­ç»ƒèŒƒå¼ï¼Œå¯ä»¥å¼¥åˆå•æ¨¡æ€å‡ ä½•ä¸“å®¶ä¹‹é—´çš„æ¨¡æ€å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”Ÿæˆå™¨ä¸é‡‡æ ·å™¨è½¬æ¢å™¨ï¼ˆGS-Formerï¼‰ï¼Œç”¨äºç”Ÿæˆåˆ¤åˆ«æŸ¥è¯¢å¹¶æ¶ˆé™¤ä¸å‡åŒ€åˆ†å¸ƒçš„å‡ ä½•ä¿¡å·ä¸­çš„éä¿¡æ¯è¡¨ç¤ºã€‚æœ€åï¼ŒGeoXå—ç›Šäºè§†è§‰æŒ‡ä»¤å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿä»¥å‡ ä½•å›¾åƒå’Œé—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆå¯éªŒè¯çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒGeoXåœ¨å…¬è®¤çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºé€šç”¨å‹å’Œå‡ ä½•å‹æ¨¡å‹ï¼Œå¦‚GeoQAã€UniGeoã€Geometry3Kå’ŒPGPS9kã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11863v1">PDF</a> Our code is available at <a target="_blank" rel="noopener" href="https://github.com/UniModal4Reasoning/GeoX">https://github.com/UniModal4Reasoning/GeoX</a></p>
<p><strong>Summary</strong>ï¼šå°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è‡ªåŠ¨å‡ ä½•é—®é¢˜æ±‚è§£ï¼ˆGPSï¼‰æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™è¦æ±‚ç†è§£å›¾è¡¨ã€è§£é‡Šç¬¦å·å’Œè¿›è¡Œå¤æ‚æ¨ç†ã€‚å…¶å±€é™æ€§æºäºå¯¹è‡ªç„¶å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒï¼Œä»¥åŠé—®é¢˜æ±‚è§£è¿‡ç¨‹ä¸­ç¼ºä¹è‡ªåŠ¨éªŒè¯ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºGeoXæ¨¡å‹ï¼Œä¸“æ³¨äºå‡ ä½•ç†è§£å’Œæ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥å•æ¨¡æ€é¢„è®­ç»ƒã€å‡ ä½•è¯­è¨€å¯¹é½å’Œç”Ÿæˆé‡‡æ ·å˜å‹å™¨ï¼ˆGS-Formerï¼‰ç­‰æ–¹æ³•ï¼Œæé«˜å‡ ä½•å›¾åƒå’Œè¯­æ–™çš„ç†è§£èƒ½åŠ›ï¼Œç¼©å°æ¨¡æ€é—´å·®è·ã€‚å®éªŒè¡¨æ˜ï¼ŒGeoXåœ¨å…¬è®¤çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºé€šç”¨æ¨¡å‹å’Œå‡ ä½•ä¸“å®¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªåŠ¨å‡ ä½•é—®é¢˜æ±‚è§£ï¼ˆGPSï¼‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç°æœ‰æ¨¡å‹çš„å±€é™æ€§æºäºå…¶è‡ªç„¶å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒï¼Œä»¥åŠé—®é¢˜æ±‚è§£è¿‡ç¨‹ä¸­ç¼ºä¹è‡ªåŠ¨éªŒè¯ã€‚</li>
<li>æå‡ºGeoXæ¨¡å‹ï¼Œä¸“æ³¨äºå‡ ä½•ç†è§£å’Œæ¨ç†ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥å•æ¨¡æ€é¢„è®­ç»ƒï¼Œæé«˜å‡ ä½•å›¾åƒå’Œè¯­æ–™çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>ä»‹ç»å‡ ä½•è¯­è¨€å¯¹é½ï¼Œç¼©å°ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚</li>
<li>æå‡ºç”Ÿæˆé‡‡æ ·å˜å‹å™¨ï¼ˆGS-Formerï¼‰ï¼Œç”¨äºç”Ÿæˆåˆ¤åˆ«æ€§æŸ¥è¯¢å¹¶æ¶ˆé™¤ä¸å‡åŒ€åˆ†å¸ƒçš„å‡ ä½•ä¿¡å·ä¸­çš„éè¡¨å¾è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d9797b596019effbe0e02dae165c9a95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9073f51406a4face60b735adc2d839dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26d4b89e6980cf38e191a469e2c205a8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="UnMA-CapSumT-Unified-and-Multi-Head-Attention-driven-Caption-Summarization-Transformer"><a href="#UnMA-CapSumT-Unified-and-Multi-Head-Attention-driven-Caption-Summarization-Transformer" class="headerlink" title="UnMA-CapSumT: Unified and Multi-Head Attention-driven Caption   Summarization Transformer"></a>UnMA-CapSumT: Unified and Multi-Head Attention-driven Caption   Summarization Transformer</h2><p><strong>Authors:Dhruv Sharma, Chhavi Dhiman, Dinesh Kumar</strong></p>
<p>Image captioning is the generation of natural language descriptions of images which have increased immense popularity in the recent past. With this different deep-learning techniques are devised for the development of factual and stylized image captioning models. Previous models focused more on the generation of factual and stylized captions separately providing more than one caption for a single image. The descriptions generated from these suffer from out-of-vocabulary and repetition issues. To the best of our knowledge, no such work exists that provided a description that integrates different captioning methods to describe the contents of an image with factual and stylized (romantic and humorous) elements. To overcome these limitations, this paper presents a novel Unified Attention and Multi-Head Attention-driven Caption Summarization Transformer (UnMA-CapSumT) based Captioning Framework. It utilizes both factual captions and stylized captions generated by the Modified Adaptive Attention-based factual image captioning model (MAA-FIC) and Style Factored Bi-LSTM with attention (SF-Bi-ALSTM) driven stylized image captioning model respectively. SF-Bi-ALSTM-based stylized IC model generates two prominent styles of expression- {romance, and humor}. The proposed summarizer UnMHA-ST combines both factual and stylized descriptions of an input image to generate styled rich coherent summarized captions. The proposed UnMHA-ST transformer learns and summarizes different linguistic styles efficiently by incorporating proposed word embedding fastText with Attention Word Embedding (fTA-WE) and pointer-generator network with coverage mechanism concept to solve the out-of-vocabulary issues and repetition problem. Extensive experiments are conducted on Flickr8K and a subset of FlickrStyle10K with supporting ablation studies to prove the efficiency and efficacy of the proposed framework. </p>
<blockquote>
<p>å›¾ç‰‡æè¿°ç”Ÿæˆæ˜¯å¯¹å›¾ç‰‡ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°çš„ä¸€ç§æŠ€æœ¯ï¼Œå®ƒåœ¨æœ€è¿‘è¿‡å»å¾—åˆ°äº†æå¤§çš„æ™®åŠã€‚ä¸ºäº†å¼€å‘äº‹å®å’Œé£æ ¼åŒ–çš„å›¾ç‰‡æè¿°ç”Ÿæˆæ¨¡å‹ï¼Œå·²ç»ç ”å‘å‡ºäº†ä¸åŒçš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚ä¹‹å‰çš„æ¨¡å‹ä¸»è¦å…³æ³¨äºäº‹å®å’Œé£æ ¼åŒ–æè¿°çš„åˆ†æ­¥ç”Ÿæˆï¼Œä¸ºå•å¼ å›¾ç‰‡æä¾›ä¸æ­¢ä¸€ä¸ªæè¿°ã€‚ä»è¿™äº›æè¿°ä¸­äº§ç”Ÿçš„æ–‡æœ¬å­˜åœ¨è¯æ±‡è¡¨å’Œé‡å¤æ€§çš„é—®é¢˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿˜æ²¡æœ‰å·¥ä½œèƒ½å¤Ÿæä¾›ä¸€ç§èåˆä¸åŒæè¿°æ–¹æ³•çš„æè¿°ï¼Œä»¥é˜è¿°å›¾ç‰‡çš„å†…å®¹ï¼Œå¹¶åŒ…å«äº‹å®å’Œé£æ ¼åŒ–ï¼ˆæµªæ¼«å’Œå¹½é»˜ï¼‰å…ƒç´ ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç»Ÿä¸€æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šå¤´æ³¨æ„åŠ›é©±åŠ¨çš„æè¿°æ‘˜è¦è½¬æ¢å™¨ï¼ˆUnMA-CapSumTï¼‰çš„æè¿°ç”Ÿæˆæ¡†æ¶ã€‚å®ƒåˆ©ç”¨ç”±æ”¹è‰¯çš„è‡ªé€‚åº”æ³¨æ„åŠ›åŸºç¡€äº‹å®å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹ï¼ˆMAA-FICï¼‰å’Œå¸¦æœ‰æ³¨æ„åŠ›çš„é£æ ¼åŒ–åŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆSF-Bi-ALSTMï¼‰é©±åŠ¨çš„å¸¦æœ‰é£æ ¼åŒ–çš„å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹åˆ†åˆ«ç”Ÿæˆçš„äº‹å®å’Œé£æ ¼åŒ–æè¿°ã€‚SF-Bi-ALSTMé£æ ¼çš„å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹èƒ½ç”Ÿæˆä¸¤ç§ä¸»è¦é£æ ¼çš„è¡¨ç°â€”â€”æµªæ¼«å’Œå¹½é»˜ã€‚æå‡ºçš„æ‘˜è¦å™¨UnMHA-STç»“åˆäº†è¾“å…¥å›¾ç‰‡çš„äº‹å®å’Œé£æ ¼åŒ–æè¿°ï¼Œä»¥ç”Ÿæˆé£æ ¼ä¸°å¯Œã€è¿è´¯çš„æ‘˜è¦æè¿°ã€‚UnMHA-STè½¬æ¢å™¨é€šè¿‡ç»“åˆæå‡ºçš„å¿«é€Ÿæ–‡æœ¬è¯åµŒå…¥ä¸æ³¨æ„åŠ›è¯åµŒå…¥ï¼ˆfTA-WEï¼‰ä»¥åŠå¸¦æœ‰è¦†ç›–æœºåˆ¶çš„æŒ‡é’ˆç”Ÿæˆå™¨ç½‘ç»œæ¦‚å¿µï¼Œæœ‰æ•ˆåœ°å­¦ä¹ å’Œæ€»ç»“ä¸åŒçš„è¯­è¨€é£æ ¼ï¼Œä»¥è§£å†³è¯æ±‡è¡¨é—®é¢˜å’Œé‡å¤é—®é¢˜ã€‚åœ¨Flickr8Kå’ŒFlickrStyle10Kå­é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå¹¶é€šè¿‡æ”¯æŒæ€§çš„æ¶ˆèç ”ç©¶è¯æ˜äº†æ‰€æå‡ºæ¡†æ¶çš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11836v1">PDF</a> </p>
<p><strong>Summary</strong><br>å›¾åƒæè¿°ç”Ÿæˆï¼ˆImage Captioningï¼‰æ˜¯ä¸€ä¸ªè‡ªç„¶è¯­è¨€æè¿°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ï¼Œè¿‘æœŸå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚æ­¤å‰æ¨¡å‹å¸¸å•ç‹¬ç”Ÿæˆäº‹å®æ€§å’Œé£æ ¼åŒ–æè¿°ï¼Œå­˜åœ¨è¯æ±‡è¡¨å¤–é—®é¢˜å’Œé‡å¤é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªåŸºäºç»Ÿä¸€æ³¨æ„åŠ›ä¸å¤šå¤´æ³¨æ„åŠ›é©±åŠ¨çš„å›¾åƒæè¿°æ‘˜è¦è½¬æ¢å™¨ï¼ˆUnMA-CapSumTï¼‰çš„æ¡†æ¶ï¼Œç»“åˆäº‹å®æ€§å’Œé£æ ¼åŒ–ï¼ˆæµªæ¼«å’Œå¹½é»˜ï¼‰å›¾åƒæè¿°ï¼Œå…‹æœä¸Šè¿°é™åˆ¶ã€‚å®ƒé€šè¿‡èåˆåŸºäºæ”¹è¿›è‡ªé€‚åº”æ³¨æ„åŠ›çš„äº‹å®æ€§å›¾åƒæè¿°æ¨¡å‹ï¼ˆMAA-FICï¼‰å’Œé£æ ¼åŒ–åŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆSF-Bi-LSTMï¼‰çš„é£æ ¼åŒ–å›¾åƒæè¿°æ¨¡å‹æ¥ç”Ÿæˆæè¿°å†…å®¹ã€‚å®éªŒåœ¨Flickr8Kå’ŒFlickrStyle10Kå­é›†ä¸Šè¿›è¡Œï¼Œè¯æ˜äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæè¿°ç”Ÿæˆï¼ˆImage Captioningï¼‰å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œèƒ½ç”Ÿæˆå›¾åƒçš„è‡ªç„¶è¯­è¨€æè¿°ã€‚</li>
<li>æ­¤å‰æ¨¡å‹åœ¨ç”Ÿæˆäº‹å®æ€§å’Œé£æ ¼åŒ–æè¿°æ—¶å­˜åœ¨è¯æ±‡è¡¨å¤–å’Œé‡å¤é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºUnMA-CapSumTæ¡†æ¶ï¼Œç»“åˆäº‹å®æ€§å’Œé£æ ¼åŒ–å›¾åƒæè¿°ï¼Œç”ŸæˆåŒ…å«é£æ ¼ä¸°å¯Œçš„ä¸€è‡´æ‘˜è¦æè¿°ã€‚</li>
<li>ä½¿ç”¨MAA-FICå’ŒSF-Bi-LSTMæ¨¡å‹åˆ†åˆ«ç”Ÿæˆäº‹å®æ€§å’Œé£æ ¼åŒ–æè¿°ã€‚</li>
<li>SF-Bi-LSTMæ¨¡å‹èƒ½ç”Ÿæˆä¸¤ç§ä¸»è¦é£æ ¼ï¼šæµªæ¼«å’Œå¹½é»˜ã€‚</li>
<li>UnMA-CapSumTé€šè¿‡èåˆfTA-WEå’ŒæŒ‡é’ˆç”Ÿæˆå™¨ç½‘ç»œè§£å†³è¯æ±‡è¡¨å¤–å’Œé‡å¤é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30b3b61270d218ee74e11d8fd3eb1533.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68a81e4a54435688ce3d4a37ae6c63d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a0649d6c013708d2b0df66a3cd2b7da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-969297f9fad7e0d03796ee363e9e64e9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SP-2-T-Sparse-Proxy-Attention-for-Dual-stream-Point-Transformer"><a href="#SP-2-T-Sparse-Proxy-Attention-for-Dual-stream-Point-Transformer" class="headerlink" title="SP$^2$T: Sparse Proxy Attention for Dual-stream Point Transformer"></a>SP$^2$T: Sparse Proxy Attention for Dual-stream Point Transformer</h2><p><strong>Authors:Jiaxu Wan, Hong Zhang, Ziqi He, Qishu Wang, Ding Yuan, Yifan Yang</strong></p>
<p>In 3D understanding, point transformers have yielded significant advances in broadening the receptive field. However, further enhancement of the receptive field is hindered by the constraints of grouping attention. The proxy-based model, as a hot topic in image and language feature extraction, uses global or local proxies to expand the modelâ€™s receptive field. But global proxy-based methods fail to precisely determine proxy positions and are not suited for tasks like segmentation and detection in the point cloud, and exist local proxy-based methods for image face difficulties in global-local balance, proxy sampling in various point clouds, and parallel cross-attention computation for sparse association. In this paper, we present SP$^2$T, a local proxy-based dual stream point transformer, which promotes global receptive field while maintaining a balance between local and global information. To tackle robust 3D proxy sampling, we propose a spatial-wise proxy sampling with vertex-based point proxy associations, ensuring robust point-cloud sampling in many scales of point cloud. To resolve economical association computation, we introduce sparse proxy attention combined with table-based relative bias, which enables low-cost and precise interactions between proxy and point features. Comprehensive experiments across multiple datasets reveal that our model achieves SOTA performance in downstream tasks. The code has been released in <a target="_blank" rel="noopener" href="https://github.com/TerenceWallel/Sparse-Proxy-Point-Transformer">https://github.com/TerenceWallel/Sparse-Proxy-Point-Transformer</a> . </p>
<blockquote>
<p>åœ¨3Dç†è§£é¢†åŸŸï¼Œç‚¹å˜å‹å™¨åœ¨æ‰©å¤§æ„Ÿå—é‡æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œåˆ†ç»„æ³¨æ„åŠ›çš„é™åˆ¶é˜»ç¢äº†æ„Ÿå—é‡çš„è¿›ä¸€æ­¥æ‰©å±•ã€‚åŸºäºä»£ç†çš„æ¨¡å‹ä½œä¸ºå›¾åƒå’Œè¯­è¨€ç‰¹å¾æå–ä¸­çš„çƒ­é—¨è¯é¢˜ï¼Œä½¿ç”¨å…¨å±€æˆ–å±€éƒ¨ä»£ç†æ¥æ‰©å±•æ¨¡å‹çš„æ„Ÿå—é‡ã€‚ç„¶è€Œï¼ŒåŸºäºå…¨å±€ä»£ç†çš„æ–¹æ³•æ— æ³•ç²¾ç¡®ç¡®å®šä»£ç†ä½ç½®ï¼Œä¸é€‚ç”¨äºç‚¹äº‘ä¸­çš„åˆ†å‰²å’Œæ£€æµ‹ç­‰ä»»åŠ¡ã€‚ç°æœ‰çš„åŸºäºå±€éƒ¨ä»£ç†çš„æ–¹æ³•åœ¨å…¨å±€-å±€éƒ¨å¹³è¡¡ã€åœ¨ä¸åŒç‚¹äº‘ä¸­çš„ä»£ç†é‡‡æ ·ä»¥åŠç¨€ç–å…³è”çš„å¹¶è¡Œäº¤å‰æ³¨æ„åŠ›è®¡ç®—æ–¹é¢é‡åˆ°å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†SP$^2$Tï¼Œä¸€ç§åŸºäºå±€éƒ¨ä»£ç†çš„åŒæµç‚¹å˜å‹å™¨ï¼Œå®ƒåœ¨ä¿æŒå±€éƒ¨å’Œå…¨å±€ä¿¡æ¯å¹³è¡¡çš„åŒæ—¶ï¼Œä¿ƒè¿›äº†å…¨å±€æ„Ÿå—é‡çš„æå‡ã€‚ä¸ºäº†è§£å†³é²æ£’çš„3Dä»£ç†é‡‡æ ·é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¡¶ç‚¹çš„ç‚¹ä»£ç†å…³è”çš„ç©ºé—´ä»£ç†é‡‡æ ·æ–¹æ³•ï¼Œç¡®ä¿åœ¨å¤šå°ºåº¦ç‚¹äº‘ä¸­çš„é²æ£’ç‚¹äº‘é‡‡æ ·ã€‚ä¸ºäº†è§£å†³ç»æµé«˜æ•ˆçš„å…³è”è®¡ç®—é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨€ç–ä»£ç†æ³¨æ„åŠ›ç»“åˆè¡¨å¼ç›¸å¯¹åå·®ï¼Œä½¿ä»£ç†å’Œç‚¹ç‰¹å¾ä¹‹é—´çš„ä½æˆæœ¬ç²¾ç¡®äº¤äº’æˆä¸ºå¯èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/TerenceWallel/Sparse-Proxy-Point-Transformer%E3%80%82">https://github.com/TerenceWallel/Sparse-Proxy-Point-Transformerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11540v1">PDF</a> 13 pages, 14 figures, 14 tables</p>
<p><strong>Summary</strong>ï¼šSP^2Tä¸ºåŸºäºå±€éƒ¨ä»£ç†çš„åŒæµç‚¹å˜æ¢å™¨ï¼Œæœ‰åŠ©äºå…¨å±€æ„Ÿå—é‡çš„åŒæ—¶ä¿æŒå±€éƒ¨ä¸å…¨å±€ä¿¡æ¯çš„å¹³è¡¡ã€‚é‡‡ç”¨åŸºäºç©ºé—´åŒ–çš„ç¨³å¥ç‚¹äº‘é‡‡æ ·æ–¹æ³•ï¼Œç»“åˆè¡¨æ ¼çš„ç›¸å¯¹åå·®è¿›è¡Œç¨€ç–ä»£ç†æ³¨æ„åŠ›è®¡ç®—ï¼Œå®ç°ä½æˆæœ¬ä¸”ç²¾ç¡®çš„ä»£ç†ä¸ç‚¹ç‰¹å¾äº¤äº’ã€‚å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ä»£ç å·²å‘å¸ƒåœ¨ç›¸åº”é“¾æ¥ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç‚¹å˜æ¢å™¨åœ¨æ‰©å¤§æ„Ÿå—é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å—åˆ°åˆ†ç»„æ³¨æ„åŠ›çº¦æŸçš„é™åˆ¶ã€‚</li>
<li>åŸºäºä»£ç†çš„æ¨¡å‹æ˜¯å›¾åƒå’Œè¯­è¨€ç‰¹å¾æå–ä¸­çš„çƒ­é—¨è¯é¢˜ï¼Œå¯é€šè¿‡å…¨å±€æˆ–å±€éƒ¨ä»£ç†æ‰©å±•æ¨¡å‹çš„æ„Ÿå—é‡ã€‚</li>
<li>ç°æœ‰å…¨å±€ä»£ç†æ–¹æ³•æ— æ³•å‡†ç¡®ç¡®å®šä»£ç†ä½ç½®ï¼Œä¸é€‚åˆç‚¹äº‘ä¸­çš„åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡ã€‚å±€éƒ¨ä»£ç†æ–¹æ³•åˆ™åœ¨å›¾åƒé¢ä¸´å…¨çƒä¸åœ°æ–¹çš„å¹³è¡¡é—®é¢˜ã€ä¸åŒç‚¹äº‘çš„ä»£ç†é‡‡æ ·ä»¥åŠç¨€ç–å…³è”å¹¶è¡Œäº¤å‰æ³¨æ„åŠ›è®¡ç®—æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>SP^2Tæ¨¡å‹é‡‡ç”¨åŸºäºç©ºé—´åŒ–çš„ä»£ç†é‡‡æ ·ä¸ç‚¹äº‘ç›¸ç»“åˆçš„ç­–ç•¥ï¼Œä¿ƒè¿›å…¨å±€æ„Ÿå—é‡çš„æ¨å¹¿ã€‚å¼•å…¥è¡¨æ ¼çš„ç›¸å¯¹åå·®å’Œç¨€ç–ä»£ç†æ³¨æ„åŠ›æŠ€æœ¯æ¥å®ç°ç»æµå’Œç²¾ç¡®çš„äº¤äº’è®¡ç®—ã€‚</li>
<li>SP^2Tæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>æ¨¡å‹ä»£ç å·²å…¬å¼€å‘å¸ƒä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-60f6e710d4d3198f15b729a2256017e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c9b17bed1b6f2511b18d9161a424b4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba953d29e4a007a314322c321cb5a568.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73550ad0fa177e904819e10174aab410.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-075cc422da7bb0ab76e356f2ea57af6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-821aefbbebeff11c1c6de54f99c3f422.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2386ea0e2ff2952565407bb026bc35a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Biased-or-Flawed-Mitigating-Stereotypes-in-Generative-Language-Models-by-Addressing-Task-Specific-Flaws"><a href="#Biased-or-Flawed-Mitigating-Stereotypes-in-Generative-Language-Models-by-Addressing-Task-Specific-Flaws" class="headerlink" title="Biased or Flawed? Mitigating Stereotypes in Generative Language Models   by Addressing Task-Specific Flaws"></a>Biased or Flawed? Mitigating Stereotypes in Generative Language Models   by Addressing Task-Specific Flaws</h2><p><strong>Authors:Akshita Jha, Sanchit Kabra, Chandan K. Reddy</strong></p>
<p>Recent studies have shown that generative language models often reflect and amplify societal biases in their outputs. However, these studies frequently conflate observed biases with other task-specific shortcomings, such as comprehension failure. For example, when a model misinterprets a text and produces a response that reinforces a stereotype, it becomes difficult to determine whether the issue arises from inherent bias or from a misunderstanding of the given content. In this paper, we conduct a multi-faceted evaluation that distinctly disentangles bias from flaws within the reading comprehension task. We propose a targeted stereotype mitigation framework that implicitly mitigates observed stereotypes in generative models through instruction-tuning on general-purpose datasets. We reduce stereotypical outputs by over 60% across multiple dimensions â€“ including nationality, age, gender, disability, and physical appearance â€“ by addressing comprehension-based failures, and without relying on explicit debiasing techniques. We evaluate several state-of-the-art generative models to demonstrate the effectiveness of our approach while maintaining the overall utility. Our findings highlight the need to critically disentangle the concept of &#96;biasâ€™ from other types of errors to build more targeted and effective mitigation strategies. CONTENT WARNING: Some examples contain offensive stereotypes. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç”Ÿæˆå¼è¯­è¨€æ¨¡å‹åœ¨å…¶è¾“å‡ºä¸­å¸¸å¸¸åæ˜ å¹¶æ”¾å¤§ç¤¾ä¼šåè§ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶ç»å¸¸å°†è§‚å¯Ÿåˆ°çš„åè§ä¸å…¶ä»–ç‰¹å®šä»»åŠ¡çš„ç¼ºé™·æ··æ·†ï¼Œå¦‚ç†è§£å¤±è´¥ã€‚ä¾‹å¦‚ï¼Œå½“æ¨¡å‹è¯¯è§£æ–‡æœ¬å¹¶äº§ç”Ÿå¼ºåŒ–åˆ»æ¿çš„å›åº”æ—¶ï¼Œå¾ˆéš¾ç¡®å®šé—®é¢˜æ˜¯ç”±äºå›ºæœ‰åè§è¿˜æ˜¯ç”±äºå¯¹ç»™å®šå†…å®¹çš„è¯¯è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¤šæ–¹é¢çš„è¯„ä¼°ï¼Œæ˜ç¡®åŒºåˆ†äº†é˜…è¯»ç†è§£ä»»åŠ¡ä¸­çš„åè§å’Œç¼ºé™·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæœ‰é’ˆå¯¹æ€§çš„åˆ»æ¿å°è±¡ç¼“è§£æ¡†æ¶ï¼Œé€šè¿‡é€šç”¨æ•°æ®é›†ä¸Šçš„æŒ‡ä»¤å¾®è°ƒï¼Œéšå¼åœ°å‡è½»äº†ç”Ÿæˆæ¨¡å‹ä¸­çš„è§‚å¯Ÿåˆ°çš„åˆ»æ¿å°è±¡ã€‚æˆ‘ä»¬é€šè¿‡è§£å†³åŸºäºç†è§£çš„å¤±è´¥ï¼Œå‡å°‘äº†è¶…è¿‡60%çš„åˆ»æ¿è¾“å‡ºï¼ŒåŒ…æ‹¬å›½ç±ã€å¹´é¾„ã€æ€§åˆ«ã€æ®‹ç–¾å’Œå¤–è¡¨ç­‰å¤šä¸ªæ–¹é¢ï¼Œå¹¶ä¸”ä¸ä¾èµ–æ˜ç¡®çš„å»åè§æŠ€æœ¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¿æŒæ€»ä½“å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œéœ€è¦æ‰¹åˆ¤æ€§åœ°åŒºåˆ†â€œåè§â€ä¸å…¶ä»–ç±»å‹çš„é”™è¯¯æ¦‚å¿µï¼Œä»¥å»ºç«‹æ›´æœ‰é’ˆå¯¹æ€§å’Œæœ‰æ•ˆçš„ç¼“è§£ç­–ç•¥ã€‚å†…å®¹è­¦å‘Šï¼šä¸€äº›ä¾‹å­åŒ…å«å†’çŠ¯æ€§çš„åˆ»æ¿å°è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11414v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å¯èƒ½ä¼šåæ˜ å’Œæ”¾å¤§ç¤¾ä¼šåè§ã€‚ä½†ç°æœ‰ç ”ç©¶å¾€å¾€å°†è§‚å¯Ÿåˆ°çš„åè§ä¸å…¶ä»–ç‰¹å®šä»»åŠ¡çš„ç¼ºé™·æ··æ·†ï¼Œå¦‚ç†è§£å¤±è´¥ã€‚æœ¬æ–‡è¿›è¡Œäº†ä¸€é¡¹å¤šå…ƒè¯„ä»·ï¼Œæ˜ç¡®åŒºåˆ†äº†åè§å’Œç†è§£é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªæœ‰é’ˆå¯¹æ€§çš„åˆ»æ¿å°è±¡ç¼“è§£æ¡†æ¶ï¼Œé€šè¿‡ä¸€èˆ¬æ•°æ®é›†ä¸Šçš„æŒ‡ä»¤å¾®è°ƒï¼Œéšå¼åœ°ç¼“è§£ç”Ÿæˆæ¨¡å‹ä¸­çš„åˆ»æ¿å°è±¡ã€‚æˆ‘ä»¬å‡å°‘äº†ä¸å¤šä¸ªç»´åº¦ç›¸å…³çš„åˆ»æ¿å°è±¡è¾“å‡ºè¶…è¿‡60%ï¼ŒåŒ…æ‹¬å›½ç±ã€å¹´é¾„ã€æ€§åˆ«ã€æ®‹ç–¾å’Œå¤–è¡¨ç­‰ï¼Œé€šè¿‡è§£å†³åŸºäºç†è§£çš„å¤±è´¥ï¼Œä¸”ä¸ä¾èµ–æ˜ç¡®çš„å»åè§æŠ€æœ¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ä¸ªå…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°éœ€è¦ä»ç†è®ºä¸Šåˆ†æ¸…åè§ä¸å…¶ä»–é”™è¯¯ç±»å‹æ¥æ„å»ºæ›´æœ‰æ•ˆçš„ç¼“è§£ç­–ç•¥ã€‚è­¦å‘Šï¼šæŸäº›ä¾‹å­å«æœ‰å†’çŠ¯æ€§çš„åˆ»æ¿å°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”Ÿæˆæ€§è¯­è¨€æ¨¡å‹å¯èƒ½åæ˜ å’Œæ”¾å¤§ç¤¾ä¼šåè§ã€‚</li>
<li>ç ”ç©¶ä¸­ç»å¸¸æ··æ·†è§‚å¯Ÿåˆ°çš„åè§ä¸å…¶ä»–ä»»åŠ¡ç‰¹å®šé—®é¢˜ï¼ˆå¦‚ç†è§£å¤±è´¥ï¼‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šå…ƒè¯„ä»·æ–¹æ³•æ¥åŒºåˆ†åè§å’Œç†è§£é”™è¯¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹åˆ»æ¿å°è±¡çš„ç¼“è§£æ¡†æ¶ï¼Œé€šè¿‡æŒ‡ä»¤å¾®è°ƒåœ¨ä¸€èˆ¬æ•°æ®é›†ä¸Šéšå¼ç¼“è§£åˆ»æ¿å°è±¡ã€‚</li>
<li>è¯¥æ¡†æ¶å‡å°‘äº†å¤šä¸ªç»´åº¦ä¸Šçš„åˆ»æ¿å°è±¡è¾“å‡ºè¶…è¿‡60%ã€‚</li>
<li>æ¡†æ¶æ—¨åœ¨è§£å†³åŸºäºç†è§£çš„å¤±è´¥ï¼Œå¹¶ä¸ä¾èµ–æ˜¾å¼å»åè§æŠ€æœ¯ã€‚</li>
<li>è¯„ä¼°äº†å¤šä¸ªå…ˆè¿›æ¨¡å‹æ¥è¯æ˜æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-38a50b8dbb116ecb9ed57a8cd965ad66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a97f325e44e3a30ad070851c1eb88844.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70e7b371316c060cc5551f8784891c59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74900f383fefeb9bb8d65e651bc5ee40.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1><h2 id="Smaller-Language-Models-Are-Better-Instruction-Evolvers"><a href="#Smaller-Language-Models-Are-Better-Instruction-Evolvers" class="headerlink" title="Smaller Language Models Are Better Instruction Evolvers"></a>Smaller Language Models Are Better Instruction Evolvers</h2><p><strong>Authors:Tingfeng Hui, Lulu Zhao, Guanting Dong, Yaqi Zhang, Hua Zhou, Sen Su</strong></p>
<p>Instruction tuning has been widely used to unleash the complete potential of large language models. Notably, complex and diverse instructions are of significant importance as they can effectively align models with various downstream tasks. However, current approaches to constructing large-scale instructions predominantly favour powerful models such as GPT-4 or those with over 70 billion parameters, under the empirical presumption that such larger language models (LLMs) inherently possess enhanced capabilities. In this study, we question this prevalent assumption and conduct an in-depth exploration into the potential of smaller language models (SLMs) in the context of instruction evolution. Extensive experiments across three scenarios of instruction evolution reveal that smaller language models (SLMs) can synthesize more effective instructions than LLMs. Further analysis demonstrates that SLMs possess a broader output space during instruction evolution, resulting in more complex and diverse variants. We also observe that the existing metrics fail to focus on the impact of the instructions. Thus, we propose Instruction Complex-Aware IFD (IC-IFD), which introduces instruction complexity in the original IFD score to evaluate the effectiveness of instruction data more accurately. Our source code is available at: \href{<a target="_blank" rel="noopener" href="https://github.com/HypherX/Evolution-Analysis%7D%7Bhttps://github.com/HypherX/Evolution-Analysis%7D">https://github.com/HypherX/Evolution-Analysis}{https://github.com/HypherX/Evolution-Analysis}</a> </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå·²è¢«å¹¿æ³›åº”ç”¨äºé‡Šæ”¾å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¨éƒ¨æ½œåŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤æ‚å’Œå¤šæ ·çš„æŒ‡ä»¤éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æœ‰æ•ˆåœ°ä½¿æ¨¡å‹ä¸å„ç§ä¸‹æ¸¸ä»»åŠ¡å¯¹é½ã€‚ç„¶è€Œï¼Œå½“å‰æ„å»ºå¤§è§„æ¨¡æŒ‡ä»¤çš„æ–¹æ³•ä¸»è¦åå‘äºå¼ºå¤§çš„æ¨¡å‹ï¼Œå¦‚GPT-4æˆ–é‚£äº›æ‹¥æœ‰è¶…è¿‡70äº¿å‚æ•°æ¨¡å‹çš„å®è¯å‡è®¾ï¼Œå³æ­¤ç±»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤©ç”Ÿå…·å¤‡å¢å¼ºèƒ½åŠ›ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è´¨ç–‘è¿™ä¸€æ™®éå‡è®¾ï¼Œå¹¶åœ¨æŒ‡ä»¤è¿›åŒ–çš„èƒŒæ™¯ä¸‹å¯¹å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ½œåŠ›è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚è·¨è¶Šä¸‰ç§æŒ‡ä»¤è¿›åŒ–åœºæ™¯çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å¯ä»¥åˆæˆæ¯”LLMæ›´æœ‰æ•ˆçš„æŒ‡ä»¤ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œåœ¨æŒ‡ä»¤è¿›åŒ–è¿‡ç¨‹ä¸­ï¼ŒSLMå…·æœ‰æ›´å¹¿é˜”çš„è¾“å‡ºç©ºé—´ï¼Œä»è€Œäº§ç”Ÿæ›´å¤æ‚å’Œå¤šæ ·åŒ–çš„å˜ä½“ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œç°æœ‰æŒ‡æ ‡æœªèƒ½å…³æ³¨æŒ‡ä»¤çš„å½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æŒ‡ä»¤å¤æ‚æ„ŸçŸ¥IFDï¼ˆIC-IFDï¼‰ï¼Œåœ¨åŸå§‹IFDåˆ†æ•°ä¸­å¼•å…¥æŒ‡ä»¤å¤æ‚æ€§ï¼Œä»¥æ›´å‡†ç¡®è¯„ä¼°æŒ‡ä»¤æ•°æ®çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/HypherX/Evolution-Analysis">https://github.com/HypherX/Evolution-Analysis</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11231v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶å¯¹æŒ‡ä»¤è¿›åŒ–è¿‡ç¨‹ä¸­å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰çš„æ½œåŠ›è¿›è¡Œäº†æ·±å…¥æ¢ç´¢ï¼Œå‘ç°ç›¸è¾ƒäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒSLMsåœ¨æŒ‡ä»¤è¿›åŒ–è¿‡ç¨‹ä¸­èƒ½å¤Ÿåˆæˆæ›´æœ‰æ•ˆçš„æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ªæ›´åŠ å‡†ç¡®çš„æŒ‡ä»¤æ•°æ®è¯„ä¼°æ–¹æ³•â€”â€”Instruction Complex-Aware IFDï¼ˆIC-IFDï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤è¿›åŒ–åœ¨é‡Šæ”¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ½œåŠ›æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨äºä½¿ç”¨å¼ºå¤§æ¨¡å‹å¦‚GPT-4æˆ–å‚æ•°è¶…è¿‡70äº¿çš„æ¨¡å‹ï¼Œä½†æœ¬ç ”ç©¶å¯¹æ­¤å‡è®¾æå‡ºè´¨ç–‘ã€‚</li>
<li>ç ”ç©¶å‘ç°å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨æŒ‡ä»¤è¿›åŒ–è¿‡ç¨‹ä¸­èƒ½åˆæˆæ›´æœ‰æ•ˆçš„æŒ‡ä»¤ã€‚</li>
<li>SLMsåœ¨æŒ‡ä»¤è¿›åŒ–è¿‡ç¨‹ä¸­å±•ç°å‡ºæ›´å¹¿é˜”çš„è¾“å‡ºç©ºé—´ï¼Œèƒ½ç”Ÿæˆæ›´å¤æ‚ã€æ›´å¤šæ ·åŒ–çš„å˜ä½“ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡æœªèƒ½å……åˆ†å…³æ³¨æŒ‡ä»¤çš„å½±å“ï¼Œå› æ­¤æå‡ºäº†Instruction Complex-Aware IFDï¼ˆIC-IFDï¼‰æ¥æ›´å‡†ç¡®åœ°è¯„ä¼°æŒ‡ä»¤æ•°æ®çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æœ¬ç ”ç©¶çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œå¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/HypherX/Evolution-Analysis">é“¾æ¥</a>ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf0e1c1be199c5206550813ed117a92c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a5ee7097af75f8553f609465ad105ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d53af9ac25b28bd7eaa255c13abbd998.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e203aba8f0e413b75af2be74b0df921e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0bbdddc50312039e6054f7ab40d5b11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55caa538323b5bb91f38751ddbc8cb70.jpg" align="middle">
</details>


<h1 id="-20"><a href="#-20" class="headerlink" title=""></a></h1><h2 id="FinGPT-Enhancing-Sentiment-Based-Stock-Movement-Prediction-with-Dissemination-Aware-and-Context-Enriched-LLMs"><a href="#FinGPT-Enhancing-Sentiment-Based-Stock-Movement-Prediction-with-Dissemination-Aware-and-Context-Enriched-LLMs" class="headerlink" title="FinGPT: Enhancing Sentiment-Based Stock Movement Prediction with   Dissemination-Aware and Context-Enriched LLMs"></a>FinGPT: Enhancing Sentiment-Based Stock Movement Prediction with   Dissemination-Aware and Context-Enriched LLMs</h2><p><strong>Authors:Yixuan Liang, Yuncong Liu, Boyu Zhang, Christina Dan Wang, Hongyang Yang</strong></p>
<p>Financial sentiment analysis is crucial for understanding the influence of news on stock prices. Recently, large language models (LLMs) have been widely adopted for this purpose due to their advanced text analysis capabilities. However, these models often only consider the news content itself, ignoring its dissemination, which hampers accurate prediction of short-term stock movements. Additionally, current methods often lack sufficient contextual data and explicit instructions in their prompts, limiting LLMsâ€™ ability to interpret news. In this paper, we propose a data-driven approach that enhances LLM-powered sentiment-based stock movement predictions by incorporating news dissemination breadth, contextual data, and explicit instructions. We cluster recent company-related news to assess its reach and influence, enriching prompts with more specific data and precise instructions. This data is used to construct an instruction tuning dataset to fine-tune an LLM for predicting short-term stock price movements. Our experimental results show that our approach improves prediction accuracy by 8% compared to existing methods. </p>
<blockquote>
<p>é‡‘èæƒ…æ„Ÿåˆ†æå¯¹äºç†è§£æ–°é—»å¯¹è‚¡ç¥¨ä»·æ ¼çš„å½±å“è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å…ˆè¿›çš„æ–‡æœ¬åˆ†æèƒ½åŠ›ï¼Œå› æ­¤è¢«å¹¿æ³›åº”ç”¨äºæ­¤ç›®çš„ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åªè€ƒè™‘æ–°é—»å†…å®¹æœ¬èº«ï¼Œè€Œå¿½ç•¥äº†å…¶ä¼ æ’­æƒ…å†µï¼Œè¿™é˜»ç¢äº†çŸ­æœŸè‚¡ç¥¨èµ°åŠ¿çš„å‡†ç¡®é¢„æµ‹ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ•°æ®å’Œæ˜ç¡®çš„æŒ‡ä»¤æç¤ºï¼Œé™åˆ¶äº†LLMè§£é‡Šæ–°é—»çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ–°é—»ä¼ æ’­çš„å¹¿åº¦ã€ä¸Šä¸‹æ–‡æ•°æ®å’Œæ˜ç¡®çš„æŒ‡ä»¤ï¼Œæé«˜äº†åŸºäºLLMçš„æƒ…æ„Ÿé©±åŠ¨è‚¡ç¥¨èµ°åŠ¿é¢„æµ‹ã€‚æˆ‘ä»¬å¯¹æœ€è¿‘çš„ä¸å…¬å¸ç›¸å…³çš„æ–°é—»è¿›è¡Œèšç±»ï¼Œä»¥è¯„ä¼°å…¶ä¼ æ’­èŒƒå›´å’Œå½±å“åŠ›ï¼Œå¹¶é€šè¿‡æ›´å…·ä½“çš„æ•°æ®å’Œç²¾ç¡®æŒ‡ä»¤ä¸°å¯Œæç¤ºã€‚è¿™äº›æ•°æ®ç”¨äºæ„å»ºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œä»¥å¾®è°ƒLLMï¼Œä»¥é¢„æµ‹çŸ­æœŸè‚¡ç¥¨ä»·æ ¼èµ°åŠ¿ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†8%çš„é¢„æµ‹ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10823v1">PDF</a> 1st Workshop on Preparing Good Data for Generative AI: Challenges and   Approaches@ AAAI 2025</p>
<p><strong>Summary</strong><br>è´¢ç»æƒ…æ„Ÿåˆ†æå¯¹äºç†è§£æ–°é—»å¯¹è‚¡ä»·çš„å½±å“è‡³å…³é‡è¦ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶å…ˆè¿›çš„æ–‡æœ¬åˆ†æèƒ½åŠ›è€Œå¹¿æ³›åº”ç”¨äºæ­¤é¢†åŸŸã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸ä»…å…³æ³¨æ–°é—»å†…å®¹æœ¬èº«ï¼Œå¿½ç•¥äº†æ–°é—»çš„æ‰©æ•£ï¼Œè¿™é˜»ç¢äº†çŸ­æœŸè‚¡å¸‚èµ°åŠ¿çš„å‡†ç¡®é¢„æµ‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œé€šè¿‡èå…¥æ–°é—»ä¼ æ’­å¹¿åº¦ã€ä¸Šä¸‹æ–‡æ•°æ®å’Œæ˜ç¡®æŒ‡ä»¤ï¼Œæé«˜LLMåœ¨æƒ…æ„ŸåŸºç¡€ä¸Šçš„è‚¡å¸‚é¢„æµ‹èƒ½åŠ›ã€‚é€šè¿‡èšç±»å…¬å¸ç›¸å…³æ–°é—»æ¥è¯„ä¼°å…¶ä¼ æ’­èŒƒå›´å’Œå½±å“åŠ›ï¼Œä¸°å¯Œäº†å…·æœ‰æ›´å¤šç‰¹å®šæ•°æ®å’Œç²¾ç¡®æŒ‡ä»¤çš„æç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æé«˜äº†8%çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è´¢ç»æƒ…æ„Ÿåˆ†æå¯¹äºé¢„æµ‹è‚¡å¸‚èµ°åŠ¿éå¸¸é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è´¢ç»æƒ…æ„Ÿåˆ†æé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç°æœ‰LLMåœ¨é¢„æµ‹çŸ­æœŸè‚¡å¸‚èµ°åŠ¿æ—¶ï¼Œå¸¸å¸¸ä»…å…³æ³¨æ–°é—»å†…å®¹æœ¬èº«ï¼Œå¿½ç•¥äº†æ–°é—»æ‰©æ•£ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œèåˆäº†æ–°é—»ä¼ æ’­å¹¿åº¦ã€ä¸Šä¸‹æ–‡æ•°æ®å’Œæ˜ç¡®æŒ‡ä»¤ï¼Œä»¥æé«˜LLMçš„è‚¡å¸‚é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡èšç±»å…¬å¸ç›¸å…³æ–°é—»è¯„ä¼°å…¶ä¼ æ’­èŒƒå›´å’Œå½±å“åŠ›ã€‚</li>
<li>ä¸°å¯Œäº†å…·æœ‰æ›´å¤šç‰¹å®šæ•°æ®å’Œç²¾ç¡®æŒ‡ä»¤çš„æç¤ºï¼Œæ„å»ºäº†æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34fb24c8e815e376a8a80ab8580b32fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0fbb5143fcbec193f48b35545f515e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ff33bb2edc39a3d71ac7198957de718.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2ede9e6bd19551df6f47fe39e567a63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2b33247fdf4fd6de0009f37fcabc11f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dccc3fa4dc3b6354945d9af3d8beabef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-341390bb3917c9b563716050c585e673.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0469eff4eb091deead82cb88e7e96c6e.jpg" align="middle">
</details>


<h1 id="-21"><a href="#-21" class="headerlink" title=""></a></h1><h2 id="Evaluation-of-GPT-4o-GPT-4o-miniâ€™s-Vision-Capabilities-for-Salt-Evaporite-Identification"><a href="#Evaluation-of-GPT-4o-GPT-4o-miniâ€™s-Vision-Capabilities-for-Salt-Evaporite-Identification" class="headerlink" title="Evaluation of GPT-4o &amp; GPT-4o-miniâ€™s Vision Capabilities for Salt   Evaporite Identification"></a>Evaluation of GPT-4o &amp; GPT-4o-miniâ€™s Vision Capabilities for Salt   Evaporite Identification</h2><p><strong>Authors:Deven B. Dangi, Beni B. Dangi, Oliver Steinbock</strong></p>
<p>Identifying salts from images of their â€˜stainsâ€™ has diverse practical applications. While specialized AI models are being developed, this paper explores the potential of OpenAIâ€™s state-of-the-art vision models (GPT-4o and GPT-4o-mini) as an immediate solution. Testing with 12 different types of salts, the GPT-4o model achieved 57% accuracy and a 0.52 F1 score, significantly outperforming both random chance (8%) and GPT-4o mini (11% accuracy). Results suggest that current vision models could serve as an interim solution for salt identification from stain images. </p>
<blockquote>
<p>ä»ç›çš„â€œæ±¡æ¸â€å›¾åƒä¸­è¯†åˆ«ç›å…·æœ‰å¤šç§å®é™…åº”ç”¨ã€‚è™½ç„¶æ­£åœ¨å¼€å‘ä¸“é—¨çš„AIæ¨¡å‹ï¼Œä½†æœ¬æ–‡æ¢è®¨äº†OpenAIæœ€å…ˆè¿›çš„è§†è§‰æ¨¡å‹ï¼ˆGPT-4oå’ŒGPT-4o-miniï¼‰ä½œä¸ºå³æ—¶è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚é€šè¿‡å¯¹12ç§ä¸åŒç±»å‹çš„ç›è¿›è¡Œæµ‹è¯•ï¼ŒGPT-4oæ¨¡å‹è¾¾åˆ°äº†57%çš„å‡†ç¡®ç‡å’Œ0.52çš„F1åˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºéšæœºçŒœæµ‹ï¼ˆ8%ï¼‰å’ŒGPT-4o miniï¼ˆ11%çš„å‡†ç¡®ç‡ï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„è§†è§‰æ¨¡å‹å¯ä»¥ä½œä¸ºä»æ±¡æ¸å›¾åƒä¸­è¯†åˆ«ç›çš„ä¸´æ—¶è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10587v1">PDF</a> 11 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºOpenAIå…ˆè¿›è§†è§‰æ¨¡å‹çš„ç›ç±»å›¾åƒè¯†åˆ«ç ”ç©¶ï¼Œå°è¯•ä½¿ç”¨GPT-4oæ¨¡å‹å¯¹åäºŒç§ä¸åŒç›ç±»è¿›è¡Œè¯†åˆ«ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†57%ï¼ŒF1åˆ†æ•°ä¸º0.52ï¼Œè¿œè¶…éšæœºçŒœæµ‹å’ŒGPT-4o miniæ¨¡å‹çš„è¡¨ç°ã€‚ç»“æœæš—ç¤ºå½“å‰è§†è§‰æ¨¡å‹å¯ä½œä¸ºç›ç±»æ±¡æ¸å›¾åƒè¯†åˆ«çš„ä¸´æ—¶è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å°è¯•ä½¿ç”¨OpenAIçš„å…ˆè¿›è§†è§‰æ¨¡å‹ï¼ˆGPT-4oï¼‰è¿›è¡Œç›ç±»è¯†åˆ«ã€‚</li>
<li>ç ”ç©¶å¯¹è±¡ä¸ºåäºŒç§ä¸åŒç±»å‹çš„ç›ã€‚</li>
<li>GPT-4oæ¨¡å‹å–å¾—äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ï¼ˆ57%ï¼‰å’ŒF1åˆ†æ•°ï¼ˆ0.52ï¼‰ã€‚</li>
<li>GPT-4oæ¨¡å‹çš„è¡¨ç°ä¼˜äºéšæœºçŒœæµ‹å’ŒGPT-4o miniæ¨¡å‹ã€‚</li>
<li>å½“å‰è§†è§‰æ¨¡å‹åœ¨ç›ç±»æ±¡æ¸å›¾åƒè¯†åˆ«ä¸Šå…·æœ‰å®ç”¨ä»·å€¼ã€‚</li>
<li>æ­¤é¡¹ç ”ç©¶ä¸ºç›ç±»è¯†åˆ«æä¾›äº†ä¸€ç§ä¸´æ—¶è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-194610202820475e2d94842798ec1f57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d7d4bbbca96afb18259645a1ae2c519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3c2a9a231218a5ddc4f6cb0bd3ad243.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1cd3a79b2d5f9bfc87b673323486111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0de8623e3f5e7693d1adf1b25019b95f.jpg" align="middle">
</details>


<h1 id="-22"><a href="#-22" class="headerlink" title=""></a></h1><h2 id="Performance-of-ChatGPT-on-tasks-involving-physics-visual-representations-the-case-of-the-Brief-Electricity-and-Magnetism-Assessment"><a href="#Performance-of-ChatGPT-on-tasks-involving-physics-visual-representations-the-case-of-the-Brief-Electricity-and-Magnetism-Assessment" class="headerlink" title="Performance of ChatGPT on tasks involving physics visual   representations: the case of the Brief Electricity and Magnetism Assessment"></a>Performance of ChatGPT on tasks involving physics visual   representations: the case of the Brief Electricity and Magnetism Assessment</h2><p><strong>Authors:Giulia Polverini, Jakob Melin, Elias Onerud, Bor Gregorcic</strong></p>
<p>Artificial intelligence-based chatbots are increasingly influencing physics education due to their ability to interpret and respond to textual and visual inputs. This study evaluates the performance of two large multimodal model-based chatbots, ChatGPT-4 and ChatGPT-4o on the Brief Electricity and Magnetism Assessment (BEMA), a conceptual physics inventory rich in visual representations such as vector fields, circuit diagrams, and graphs. Quantitative analysis shows that ChatGPT-4o outperforms both ChatGPT-4 and a large sample of university students, and demonstrates improvements in ChatGPT-4oâ€™s vision interpretation ability over its predecessor ChatGPT-4. However, qualitative analysis of ChatGPT-4oâ€™s responses reveals persistent challenges. We identified three types of difficulties in the chatbotâ€™s responses to tasks on BEMA: (1) difficulties with visual interpretation, (2) difficulties in providing correct physics laws or rules, and (3) difficulties with spatial coordination and application of physics representations. Spatial reasoning tasks, particularly those requiring the use of the right-hand rule, proved especially problematic. These findings highlight that the most broadly used large multimodal model-based chatbot, ChatGPT-4o, still exhibits significant difficulties in engaging with physics tasks involving visual representations. While the chatbot shows potential for educational applications, including personalized tutoring and accessibility support for students who are blind or have low vision, its limitations necessitate caution. On the other hand, our findings can also be leveraged to design assessments that are difficult for chatbots to solve. </p>
<blockquote>
<p>åŸºäºäººå·¥æ™ºèƒ½çš„èŠå¤©æœºå™¨äººç”±äºå…¶è§£é‡Šå’Œå“åº”æ–‡æœ¬å’Œè§†è§‰è¾“å…¥çš„èƒ½åŠ›ï¼Œæ­£è¶Šæ¥è¶Šå½±å“ç‰©ç†æ•™è‚²ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸¤ä¸ªå¤§å‹å¤šæ¨¡å¼æ¨¡å‹åŸºç¡€èŠå¤©æœºå™¨äººChatGPT-4å’ŒChatGPT-4oåœ¨ã€Šç®€çŸ­çš„ç”µå­¦ä¸ç£å­¦è¯„ä¼°ã€‹(BEMA)ä¸Šçš„è¡¨ç°ã€‚BEMAæ˜¯ä¸€ä¸ªæ¦‚å¿µä¸°å¯Œçš„ç‰©ç†åº“å­˜ï¼ŒåŒ…å«çŸ¢é‡åœºã€ç”µè·¯å›¾å’Œå›¾è¡¨ç­‰è§†è§‰è¡¨ç°å½¢å¼ã€‚å®šé‡åˆ†æè¡¨æ˜ï¼ŒChatGPT-4oåœ¨ChatGPT-4å’Œå¤§é‡å¤§å­¦ç”Ÿæ ·æœ¬ä¸­çš„è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è§†è§‰è§£é‡Šèƒ½åŠ›æ–¹é¢ç›¸è¾ƒäºå‰ä»£ChatGPT-4çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¯¹ChatGPT-4oçš„å›åº”è¿›è¡Œå®šæ€§åˆ†ææ­ç¤ºäº†æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†chatbotåœ¨BEMAä»»åŠ¡ä¸Šçš„å›°éš¾åˆ†ä¸ºä¸‰ä¸ªç±»åˆ«ï¼š(1)è§†è§‰è§£é‡Šçš„å›°éš¾ï¼Œ(2)æä¾›æ­£ç¡®çš„ç‰©ç†å®šå¾‹æˆ–è§„åˆ™çš„å›°éš¾ï¼Œ(3)ç©ºé—´åè°ƒå’Œç‰©ç†è¡¨è¾¾åº”ç”¨çš„å›°éš¾ã€‚ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é‚£äº›éœ€è¦ä½¿ç”¨å³æ‰‹è§„åˆ™çš„ä»»åŠ¡ï¼Œè¯æ˜å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæœ€å¹¿æ³›ä½¿ç”¨çš„å¤§å‹å¤šæ¨¡å¼æ¨¡å‹åŸºç¡€èŠå¤©æœºå™¨äººChatGPT-4oåœ¨ä¸æ¶‰åŠè§†è§‰è¡¨è¾¾çš„ç‰©ç†ä»»åŠ¡äº¤äº’æ—¶ä»é¢ä¸´é‡å¤§å›°éš¾ã€‚è™½ç„¶èŠå¤©æœºå™¨äººåœ¨æ•™è‚²åº”ç”¨æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–è¾…å¯¼å’Œæ”¯æŒè§†åŠ›éšœç¢æˆ–è§†åŠ›ä¸ä½³çš„å­¦ç”Ÿï¼Œä½†å…¶å±€é™æ€§éœ€è¦è°¨æ…å¯¹å¾…ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„å‘ç°ä¹Ÿå¯ä»¥ç”¨æ¥è®¾è®¡èŠå¤©æœºå™¨äººéš¾ä»¥è§£å†³çš„è¯„ä¼°é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10019v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºäººå·¥æ™ºèƒ½çš„èŠå¤©æœºå™¨äººå› èƒ½è§£è¯»å’Œå›åº”æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ï¼Œå¯¹ç‰©ç†æ•™è‚²äº§ç”Ÿè¶Šæ¥è¶Šå¤§çš„å½±å“ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸¤ä¸ªå¤§å‹å¤šæ¨¡å¼æ¨¡å‹èŠå¤©æœºå™¨äººâ€”â€”ChatGPT-4å’ŒChatGPT-4oåœ¨å¯Œå«çŸ¢é‡åœºã€ç”µè·¯å›¾å’Œå›¾è¡¨ç­‰è§†è§‰è¡¨ç°çš„ç®€çŸ­ç”µåŠ›ä¸ç£å­¦è¯„ä¼°ï¼ˆBEMAï¼‰ä¸Šçš„è¡¨ç°ã€‚å®šé‡åˆ†ææ˜¾ç¤ºï¼ŒChatGPT-4oåœ¨ChatGPT-4ä»¥åŠå¤§é‡å¤§å­¦ç”Ÿæ ·æœ¬ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨è§†è§‰è§£è¯»èƒ½åŠ›ä¸Šæœ‰æ‰€è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¯¹ChatGPT-4oçš„å›åº”è¿›è¡Œå®šæ€§åˆ†ææ­ç¤ºäº†æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç¡®å®šäº†èŠå¤©æœºå™¨äººåœ¨BEMAä»»åŠ¡å›åº”ä¸­çš„ä¸‰ç§å›°éš¾ï¼š1ï¼‰è§†è§‰è§£è¯»å›°éš¾ï¼Œ2ï¼‰æä¾›æ­£ç¡®çš„ç‰©ç†å®šå¾‹æˆ–è§„åˆ™æ–¹é¢çš„å›°éš¾ï¼Œä»¥åŠ3ï¼‰ç©ºé—´åè°ƒå’Œç‰©ç†è¡¨å¾åº”ç”¨æ–¹é¢çš„å›°éš¾ã€‚ç‰¹åˆ«æ˜¯éœ€è¦ä½¿ç”¨å³æ‰‹å®šåˆ™çš„ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œè¯æ˜å°¤å…¶å­˜åœ¨é—®é¢˜ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæœ€å¹¿æ³›ä½¿ç”¨çš„åŸºäºå¤§å‹å¤šæ¨¡å¼æ¨¡å‹çš„èŠå¤©æœºå™¨äººChatGPT-4oåœ¨æ¶‰åŠè§†è§‰è¡¨ç°çš„ç‰©ç†ä»»åŠ¡ä¸­ä»å­˜åœ¨æ˜¾è‘—å›°éš¾ã€‚è™½ç„¶èŠå¤©æœºå™¨äººåœ¨æ•™è‚²åº”ç”¨æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–è¾…å¯¼å’Œå¤±æ˜æˆ–ä½è§†åŠ›å­¦ç”Ÿçš„è¾…åŠ©æ”¯æŒï¼Œä½†å…¶å±€é™æ€§éœ€è¦è°¨æ…å¯¹å¾…ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„å‘ç°ä¹Ÿå¯ä»¥ç”¨æ¥è®¾è®¡èŠå¤©æœºå™¨äººéš¾ä»¥è§£å†³çš„è¯„ä¼°é¢˜ç›®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºäººå·¥æ™ºèƒ½çš„èŠå¤©æœºå™¨äººåœ¨ç‰©ç†æ•™è‚²ä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼Œå°¤å…¶åœ¨è§£è¯»å’Œå›åº”æ–‡æœ¬å’Œè§†è§‰è¾“å…¥æ–¹é¢ã€‚</li>
<li>ChatGPT-4oåœ¨è§†è§‰è§£è¯»èƒ½åŠ›ä¸Šç›¸è¾ƒäºå…¶å‰èº«ChatGPT-4æœ‰æ‰€æå‡ã€‚</li>
<li>åœ¨æ¶‰åŠè§†è§‰è¡¨ç°çš„ç‰©ç†ä»»åŠ¡ä¸­ï¼ŒèŠå¤©æœºå™¨äººé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šè§†è§‰è§£è¯»ã€æä¾›æ­£ç¡®ç‰©ç†çŸ¥è¯†å’Œåº”ç”¨ç©ºé—´æ¨ç†çš„å›°éš¾ã€‚</li>
<li>ç©ºé—´æ¨ç†ä»»åŠ¡å¯¹èŠå¤©æœºå™¨äººæ¥è¯´å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦ä½¿ç”¨å³æ‰‹å®šåˆ™çš„ä»»åŠ¡å°¤ä¸ºå›°éš¾ã€‚</li>
<li>èŠå¤©æœºå™¨äººåœ¨æ•™è‚²åº”ç”¨æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¦‚ä¸ªæ€§åŒ–è¾…å¯¼å’Œä½è§†åŠ›å­¦ç”Ÿçš„æ”¯æŒã€‚</li>
<li>èŠå¤©æœºå™¨äººå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦åœ¨åº”ç”¨æ—¶è°¨æ…å¯¹å¾…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef09dc7ae2a1e2ae3725b98cc74eecb1.jpg" align="middle">
</details>


<h1 id="-23"><a href="#-23" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c30275e38a084ae99a2feb9c3b5590ab.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Proposer-Agent-Evaluator(PAE) Autonomous Skill Discovery For Foundation   Model Internet Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-18/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4598a50ddf55431f836ff352496ce154.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-18  Proposer-Agent-Evaluator(PAE) Autonomous Skill Discovery For Foundation   Model Internet Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14773.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
