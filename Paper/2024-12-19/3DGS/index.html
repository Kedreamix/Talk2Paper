<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  GraphAvatar Compact Head Avatars with GNN-Generated 3D Gaussians">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a07829a49dd81bb2569cf072e3346f5a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-19-æ›´æ–°"><a href="#2024-12-19-æ›´æ–°" class="headerlink" title="2024-12-19 æ›´æ–°"></a>2024-12-19 æ›´æ–°</h1><h2 id="GraphAvatar-Compact-Head-Avatars-with-GNN-Generated-3D-Gaussians"><a href="#GraphAvatar-Compact-Head-Avatars-with-GNN-Generated-3D-Gaussians" class="headerlink" title="GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians"></a>GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians</h2><p><strong>Authors:Xiaobao Wei, Peng Chen, Ming Lu, Hui Chen, Feng Tian</strong></p>
<p>Rendering photorealistic head avatars from arbitrary viewpoints is crucial for various applications like virtual reality. Although previous methods based on Neural Radiance Fields (NeRF) can achieve impressive results, they lack fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have improved rendering quality and real-time performance but still require significant storage overhead. In this paper, we introduce a method called GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an appearance GNN to generate the attributes of the 3D Gaussians from the tracked mesh. Therefore, our method can store the GNN models instead of the 3D Gaussians, significantly reducing the storage overhead to just 10MB. To reduce the impact of face-tracking errors, we also present a novel graph-guided optimization module to refine face-tracking parameters during training. Finally, we introduce a 3D-aware enhancer for post-processing to enhance the rendering quality. We conduct comprehensive experiments to demonstrate the advantages of GraphAvatar, surpassing existing methods in visual fidelity and storage consumption. The ablation study sheds light on the trade-offs between rendering quality and model size. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ucwxb/GraphAvatar">https://github.com/ucwxb/GraphAvatar</a> </p>
<blockquote>
<p>ä»ä»»æ„è§†è§’æ¸²æŸ“é€¼çœŸçš„å¤´éƒ¨åŒ–èº«å¯¹äºè™šæ‹Ÿç°å®ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„å…ˆå‰æ–¹æ³•å¯ä»¥è¾¾åˆ°ä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœï¼Œä½†å®ƒä»¬ç¼ºä¹çœŸå®æ€§å’Œæ•ˆç‡ã€‚æœ€è¿‘ä½¿ç”¨3Dé«˜æ–¯æ‘Šé“ºï¼ˆ3DGSï¼‰çš„æ–¹æ³•æé«˜äº†æ¸²æŸ“è´¨é‡å’Œå®æ—¶æ€§èƒ½ï¼Œä½†ä»ç„¶éœ€è¦å·¨å¤§çš„å­˜å‚¨å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºGraphAvatarçš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç”Ÿæˆå¤´éƒ¨åŒ–èº«çš„3Dé«˜æ–¯ã€‚å…·ä½“æ¥è¯´ï¼ŒGraphAvatarè®­ç»ƒäº†ä¸€ä¸ªå‡ ä½•GNNå’Œä¸€ä¸ªå¤–è§‚GNNï¼Œä»è·Ÿè¸ªçš„ç½‘æ ¼ç”Ÿæˆ3Dé«˜æ–¯å±æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å­˜å‚¨GNNæ¨¡å‹ï¼Œè€Œä¸æ˜¯3Dé«˜æ–¯ï¼Œå°†å­˜å‚¨å¼€é”€å¤§å¹…é™ä½åˆ°ä»…10MBã€‚ä¸ºäº†å‡å°‘é¢éƒ¨è·Ÿè¸ªè¯¯å·®çš„å½±å“ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„å›¾å¼•å¯¼ä¼˜åŒ–æ¨¡å—ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é¢éƒ¨è·Ÿè¸ªå‚æ•°è¿›è¡Œå¾®è°ƒã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç”¨äºåå¤„ç†çš„3Dæ„ŸçŸ¥å¢å¼ºå™¨ï¼Œä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œå±•ç¤ºäº†GraphAvatarçš„ä¼˜åŠ¿ï¼Œåœ¨è§†è§‰é€¼çœŸåº¦å’Œå­˜å‚¨æ¶ˆè€—æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚æ¶ˆèç ”ç©¶æ­ç¤ºäº†æ¸²æŸ“è´¨é‡å’Œæ¨¡å‹å¤§å°ä¹‹é—´çš„æƒè¡¡ã€‚ä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/ucwxb/GraphAvatar">https://github.com/ucwxb/GraphAvatar</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13983v1">PDF</a> accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç”Ÿæˆå¤´éƒ¨è§’è‰²ä¸‰ç»´é«˜æ–¯æ¸²æŸ“çš„æ–¹æ³•ï¼Œç§°ä¸ºGraphAvatarã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒå‡ ä½•GNNå’Œå¤–è§‚GNNç”Ÿæˆä¸‰ç»´é«˜æ–¯å±æ€§ï¼Œä»è¿½è¸ªç½‘æ ¼ä¸­ç”Ÿæˆå¤´åƒã€‚ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼ŒGraphAvataræ˜¾è‘—å‡å°‘äº†å­˜å‚¨éœ€æ±‚ï¼Œä»…éœ€è¦å­˜å‚¨GNNæ¨¡å‹ï¼Œå¤§å¤§å‡å°‘äº†å­˜å‚¨å¼€é”€ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»é¢éƒ¨è·Ÿè¸ªè¯¯å·®çš„å½±å“ï¼Œå¼•å…¥äº†æ–°å‹çš„å›¾å¼•å¯¼ä¼˜åŒ–æ¨¡å—ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–é¢éƒ¨è·Ÿè¸ªå‚æ•°ã€‚æœ€åé€šè¿‡ç»¼åˆå®éªŒè¯æ˜äº†GraphAvataråœ¨è§†è§‰ä¿çœŸåº¦å’Œå­˜å‚¨æ¶ˆè€—æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GraphAvataråˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç”Ÿæˆå¤´éƒ¨è§’è‰²çš„ä¸‰ç»´é«˜æ–¯æ¸²æŸ“ã€‚</li>
<li>é€šè¿‡è®­ç»ƒå‡ ä½•GNNå’Œå¤–è§‚GNNç”Ÿæˆä¸‰ç»´é«˜æ–¯å±æ€§ã€‚</li>
<li>ä»…éœ€å­˜å‚¨GNNæ¨¡å‹ï¼Œæ˜¾è‘—å‡å°‘å­˜å‚¨éœ€æ±‚ã€‚</li>
<li>å¼•å…¥å›¾å¼•å¯¼ä¼˜åŒ–æ¨¡å—ä»¥ä¼˜åŒ–é¢éƒ¨è·Ÿè¸ªå‚æ•°ï¼Œå‡è½»é¢éƒ¨è·Ÿè¸ªè¯¯å·®çš„å½±å“ã€‚</li>
<li>æä¾›äº†ç»¼åˆå®éªŒè¯æ˜GraphAvataråœ¨è§†è§‰ä¿çœŸåº¦å’Œå­˜å‚¨æ¶ˆè€—æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>å…¬å¼€äº†ä»£ç ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-499727917bec8913fec8dee0d29c0265.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-334573154ce596e4d0d70d1cf06d6c47.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf870dad7b6fa78ed96a65941a326f27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75eda6d48b2a3d0fedaa25367e8d5cb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-555c65c663223607d9c0758ab83bc605.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GAGS-Granularity-Aware-Feature-Distillation-for-Language-Gaussian-Splatting"><a href="#GAGS-Granularity-Aware-Feature-Distillation-for-Language-Gaussian-Splatting" class="headerlink" title="GAGS: Granularity-Aware Feature Distillation for Language Gaussian   Splatting"></a>GAGS: Granularity-Aware Feature Distillation for Language Gaussian   Splatting</h2><p><strong>Authors:Yuning Peng, Haiping Wang, Yuan Liu, Chenglu Wen, Zhen Dong, Bisheng Yang</strong></p>
<p>3D open-vocabulary scene understanding, which accurately perceives complex semantic properties of objects in space, has gained significant attention in recent years. In this paper, we propose GAGS, a framework that distills 2D CLIP features into 3D Gaussian splatting, enabling open-vocabulary queries for renderings on arbitrary viewpoints. The main challenge of distilling 2D features for 3D fields lies in the multiview inconsistency of extracted 2D features, which provides unstable supervision for the 3D feature field. GAGS addresses this challenge with two novel strategies. First, GAGS associates the prompt point density of SAM with the camera distances, which significantly improves the multiview consistency of segmentation results. Second, GAGS further decodes a granularity factor to guide the distillation process and this granularity factor can be learned in a unsupervised manner to only select the multiview consistent 2D features in the distillation process. Experimental results on two datasets demonstrate significant performance and stability improvements of GAGS in visual grounding and semantic segmentation, with an inference speed 2$\times$ faster than baseline methods. The code and additional results are available at <a target="_blank" rel="noopener" href="https://pz0826.github.io/GAGS-Webpage/">https://pz0826.github.io/GAGS-Webpage/</a> . </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œèƒ½å¤Ÿå‡†ç¡®æ„ŸçŸ¥ç©ºé—´ä¸­ç‰©ä½“å¤æ‚è¯­ä¹‰å±æ€§çš„ä¸‰ç»´å¼€æ”¾è¯æ±‡åœºæ™¯ç†è§£å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GAGSæ¡†æ¶ï¼Œå®ƒå°†äºŒç»´CLIPç‰¹å¾è½¬åŒ–ä¸ºä¸‰ç»´é«˜æ–¯èåˆæŠ€æœ¯ï¼Œä¸ºä»»æ„è§†è§’çš„æ¸²æŸ“å®ç°å¼€æ”¾è¯æ±‡æŸ¥è¯¢ã€‚è’¸é¦äºŒç»´ç‰¹å¾ç”¨äºä¸‰ç»´é¢†åŸŸçš„æŒ‘æˆ˜åœ¨äºæå–çš„äºŒç»´ç‰¹å¾çš„å¤šè§†è§’ä¸ä¸€è‡´æ€§ï¼Œè¿™ä¸ºä¸‰ç»´ç‰¹å¾é¢†åŸŸæä¾›äº†ä¸ç¨³å®šçš„ç›‘ç£ã€‚GAGSé€šè¿‡ä¸¤ç§æ–°ç­–ç•¥æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚é¦–å…ˆï¼ŒGAGSå°†SAMçš„æç¤ºç‚¹å¯†åº¦ä¸ç›¸æœºè·ç¦»ç›¸å…³è”ï¼Œè¿™å¤§å¤§æé«˜äº†åˆ†å‰²ç»“æœçš„å¤šè§†è§’ä¸€è‡´æ€§ã€‚å…¶æ¬¡ï¼ŒGAGSè¿›ä¸€æ­¥è§£ç ç²’åº¦å› å­æ¥æŒ‡å¯¼è’¸é¦è¿‡ç¨‹ï¼Œè¯¥ç²’åº¦å› å­å¯ä»¥ä»¥æ— ç›‘ç£çš„æ–¹å¼å­¦ä¹ ï¼Œä»…åœ¨è’¸é¦è¿‡ç¨‹ä¸­é€‰æ‹©å¤šè§†è§’ä¸€è‡´çš„äºŒç»´ç‰¹å¾ã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è§†è§‰å®šä½å’Œç›®æ ‡åˆ†å‰²æ–¹é¢ï¼ŒGAGSçš„æ€§èƒ½å’Œç¨³å®šæ€§æ˜¾è‘—æé«˜ï¼Œå…¶æ¨ç†é€Ÿåº¦æ¯”åŸºçº¿æ–¹æ³•å¿«ä¸¤å€ã€‚ä»£ç å’Œæ›´å¤šç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://pz0826.github.io/GAGS-Webpage/%E8%8E%B7%E5%BE%97%E3%80%82">https://pz0826.github.io/GAGS-Webpage/è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13654v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://pz0826.github.io/GAGS-Webpage/">https://pz0826.github.io/GAGS-Webpage/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGAGSçš„æ¡†æ¶ï¼Œå®ƒå°†äºŒç»´CLIPç‰¹å¾è½¬åŒ–ä¸ºä¸‰ç»´é«˜æ–¯æ··åˆè¡¨ç¤ºï¼Œå®ç°å¯¹ä»»æ„è§†è§’æ¸²æŸ“çš„ä¸‰ç»´åœºæ™¯è¿›è¡Œå¼€æ”¾è¯æ±‡æŸ¥è¯¢çš„ä»»åŠ¡ã€‚è¯¥æ¡†æ¶è§£å†³äº†äºŒç»´ç‰¹å¾åœ¨ä¸‰ç»´åœºæ™¯ä¸­è’¸é¦é¢ä¸´çš„å¤šè§†è§’ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæé«˜äº†è¯­ä¹‰åˆ†å‰²å’Œè§†è§‰å®šä½çš„æ€§èƒ½åŠç¨³å®šæ€§ã€‚å…¶åˆ›æ–°æ€§åœ¨äºé€šè¿‡å…³è”é‡‡æ ·ç‚¹å¯†åº¦ä¸ç›¸æœºè·ç¦»ã€è§£ç ç²’åº¦å› å­ç­‰ç­–ç•¥æ”¹å–„å¤šè§†è§’ä¸€è‡´æ€§ã€‚ç›¸å…³ä»£ç å’Œé¢å¤–ç»“æœå·²å‘å¸ƒäºç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://pz0826.github.io/GAGS-Webpage/%E3%80%82">https://pz0826.github.io/GAGS-Webpage/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GAGSæ¡†æ¶å®ç°äº†äºŒç»´CLIPç‰¹å¾åˆ°ä¸‰ç»´é«˜æ–¯æ··åˆè¡¨ç¤ºçš„è½¬åŒ–ï¼Œä¸ºä¸‰ç»´åœºæ™¯ç†è§£å¸¦æ¥æ–°æ€è·¯ã€‚</li>
<li>è§£å†³äº†äºŒç»´ç‰¹å¾åœ¨ä¸‰ç»´åœºæ™¯è’¸é¦è¿‡ç¨‹ä¸­çš„å¤šè§†è§’ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡å…³è”é‡‡æ ·ç‚¹å¯†åº¦ä¸ç›¸æœºè·ç¦»çš„ç­–ç•¥ï¼Œæé«˜äº†åˆ†å‰²ç»“æœçš„å¤šè§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥ç²’åº¦å› å­ä»¥æŒ‡å¯¼è’¸é¦è¿‡ç¨‹ï¼Œèƒ½å¤Ÿé€‰æ‹©å¤šè§†è§’ä¸€è‡´çš„äºŒç»´ç‰¹å¾ã€‚</li>
<li>åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGAGSåœ¨è§†è§‰å®šä½ä¸è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>GAGSæ¡†æ¶çš„æ¨ç†é€Ÿåº¦æ¯”åŸºå‡†æ–¹æ³•å¿«ä¸¤å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a30ced81a9e4e1c29f474bdea436307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c423d88c806e0bd8f0bdcac307fbf095.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-994fc598d0ef3bb4402c84f5dd54fbe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4da64d6f2bd9977b6e796417c7a51e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb7be5c810927402300c526ce992d58e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54151abae142dd745476461c460433d3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="4D-Radar-Inertial-Odometry-based-on-Gaussian-Modeling-and-Multi-Hypothesis-Scan-Matching"><a href="#4D-Radar-Inertial-Odometry-based-on-Gaussian-Modeling-and-Multi-Hypothesis-Scan-Matching" class="headerlink" title="4D Radar-Inertial Odometry based on Gaussian Modeling and   Multi-Hypothesis Scan Matching"></a>4D Radar-Inertial Odometry based on Gaussian Modeling and   Multi-Hypothesis Scan Matching</h2><p><strong>Authors:Fernando Amodeo, Luis Merino, Fernando Caballero</strong></p>
<p>4D millimeter-wave (mmWave) radars are sensors that provide robustness against adverse weather conditions (rain, snow, fog, etc.), and as such they are increasingly being used for odometry and SLAM applications. However, the noisy and sparse nature of the returned scan data proves to be a challenging obstacle for existing point cloud matching based solutions, especially those originally intended for more accurate sensors such as LiDAR. Inspired by visual odometry research around 3D Gaussian Splatting, in this paper we propose using freely positioned 3D Gaussians to create a summarized representation of a radar point cloud tolerant to sensor noise, and subsequently leverage its inherent probability distribution function for registration (similar to NDT). Moreover, we propose simultaneously optimizing multiple scan matching hypotheses in order to further increase the robustness of the system against local optima of the function. Finally, we fuse our Gaussian modeling and scan matching algorithms into an EKF radar-inertial odometry system designed after current best practices. Experiments show that our Gaussian-based odometry is able to outperform current baselines on a well-known 4D radar dataset used for evaluation. </p>
<blockquote>
<p>å››ç»´æ¯«ç±³æ³¢é›·è¾¾ä¼ æ„Ÿå™¨åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ï¼ˆå¦‚é›¨ã€é›ªã€é›¾ç­‰ï¼‰è¡¨ç°å‡ºç¨³å¥æ€§ï¼Œå› æ­¤è¶Šæ¥è¶Šå¤šåœ°ç”¨äºæµ‹è·å’ŒSLAMåº”ç”¨ã€‚ç„¶è€Œï¼Œè¿”å›çš„æ‰«ææ•°æ®å…·æœ‰å™ªå£°å¤§å’Œç¨€ç–æ€§çš„ç‰¹ç‚¹ï¼Œè¯æ˜è¿™å¯¹ç°æœ‰çš„åŸºäºç‚¹äº‘åŒ¹é…çš„è§£å†³æ–¹æ¡ˆæ„æˆäº†ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„éšœç¢ï¼Œå°¤å…¶æ˜¯é‚£äº›æœ€åˆä¸ºæ¿€å…‰é›·è¾¾ç­‰æ›´ç²¾ç¡®ä¼ æ„Ÿå™¨è®¾è®¡çš„è§£å†³æ–¹æ¡ˆã€‚å—å›´ç»•ä¸‰ç»´é«˜æ–¯å–·æº…çš„è§†è§‰æµ‹è·ç ”ç©¶å¯å‘ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨è‡ªç”±å®šä½çš„3Dé«˜æ–¯æ¥åˆ›å»ºä¸€ç§å¯¹ä¼ æ„Ÿå™¨å™ªå£°å…·æœ‰å®¹å¿åº¦çš„é›·è¾¾ç‚¹äº‘æ‘˜è¦è¡¨ç¤ºï¼Œç„¶ååˆ©ç”¨å…¶å›ºæœ‰çš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°è¿›è¡Œæ³¨å†Œï¼ˆç±»ä¼¼äºNDTï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºåŒæ—¶ä¼˜åŒ–å¤šä¸ªæ‰«æåŒ¹é…å‡è®¾ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç³»ç»Ÿå¯¹å‡½æ•°å±€éƒ¨æœ€ä¼˜è§£çš„ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å°†é«˜æ–¯å»ºæ¨¡å’Œæ‰«æåŒ¹é…ç®—æ³•èåˆåˆ°ä¸€ä¸ªå€Ÿé‰´å½“å‰æœ€ä½³å®è·µçš„é›·è¾¾æƒ¯æ€§æµ‹è·ç³»ç»Ÿçš„æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åŸºäºé«˜æ–¯æ¨¡å‹çš„æµ‹è·ç³»ç»Ÿåœ¨ä¼—æ‰€å‘¨çŸ¥çš„å››ç»´é›·è¾¾æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå½“å‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13639v1">PDF</a> Our code and results can be publicly accessed at:   <a target="_blank" rel="noopener" href="https://github.com/robotics-upo/gaussian-rio">https://github.com/robotics-upo/gaussian-rio</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸‰ç»´é«˜æ–¯æŠ€æœ¯çš„é›·è¾¾ç‚¹äº‘æ‘˜è¦è¡¨ç¤ºå¯æŠ—å¹²æ‰°ä¼ æ„Ÿå™¨å™ªå£°ï¼Œå¹¶åˆ©ç”¨å…¶å›ºæœ‰æ¦‚ç‡åˆ†å¸ƒå‡½æ•°è¿›è¡Œæ³¨å†Œï¼Œç±»ä¼¼äºNDTã€‚åŒæ—¶ä¼˜åŒ–å¤šä¸ªæ‰«æåŒ¹é…å‡è®¾ï¼Œæé«˜ç³»ç»Ÿå¯¹å±€éƒ¨æœ€ä¼˜è§£çš„ç¨³å¥æ€§ã€‚èåˆé«˜æ–¯å»ºæ¨¡å’Œæ‰«æåŒ¹é…ç®—æ³•ï¼Œæ„å»ºé›·è¾¾æƒ¯æ€§é‡Œç¨‹è®¡ç³»ç»Ÿï¼Œåœ¨4Dé›·è¾¾æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4Dæ¯«ç±³æ³¢é›·è¾¾èƒ½åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ç¨³å¥è¿è¡Œï¼Œé€‚ç”¨äºé‡Œç¨‹è®¡å’ŒSLAMåº”ç”¨ã€‚</li>
<li>é›·è¾¾è¿”å›æ‰«ææ•°æ®å…·æœ‰å™ªå£°å¤§å’Œç¨€ç–æ€§ï¼Œå¯¹ç‚¹äº‘åŒ¹é…è§£å†³æ–¹æ¡ˆæ„æˆæŒ‘æˆ˜ã€‚</li>
<li>çµæ„Ÿæ¥è‡ªè§†è§‰é‡Œç¨‹è®¡ä¸­çš„ä¸‰ç»´é«˜æ–¯å±•å¼€ç ”ç©¶ï¼Œæå‡ºä½¿ç”¨è‡ªç”±å®šä½çš„ä¸‰ç»´é«˜æ–¯å¯¹é›·è¾¾ç‚¹äº‘è¿›è¡Œæ‘˜è¦è¡¨ç¤ºï¼Œä»¥æŠ—å¹²æ‰°ä¼ æ„Ÿå™¨å™ªå£°ã€‚</li>
<li>åˆ©ç”¨æ¦‚ç‡åˆ†å¸ƒå‡½æ•°è¿›è¡Œæ³¨å†Œï¼Œç±»ä¼¼äºNDTæ–¹æ³•ã€‚</li>
<li>åŒæ—¶ä¼˜åŒ–å¤šä¸ªæ‰«æåŒ¹é…å‡è®¾ï¼Œå¢å¼ºç³»ç»Ÿç¨³å¥æ€§ï¼Œå‡å°‘å±€éƒ¨æœ€ä¼˜è§£å¹²æ‰°ã€‚</li>
<li>å°†é«˜æ–¯å»ºæ¨¡å’Œæ‰«æåŒ¹é…ç®—æ³•èåˆè¿›EKFé›·è¾¾æƒ¯æ€§é‡Œç¨‹è®¡ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5ab95e6bb781935a6a674884105860e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eada150562ba530f98329cbb926b72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cefc9aad5252caf323dbb23a040c939a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c7b20705c80a60fe1945a3911a64c9d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Turbo-GS-Accelerating-3D-Gaussian-Fitting-for-High-Quality-Radiance-Fields"><a href="#Turbo-GS-Accelerating-3D-Gaussian-Fitting-for-High-Quality-Radiance-Fields" class="headerlink" title="Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance   Fields"></a>Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance   Fields</h2><p><strong>Authors:Tao Lu, Ankit Dhiman, R Srinath, Emre Arslan, Angela Xing, Yuanbo Xiangli, R Venkatesh Babu, Srinath Sridhar</strong></p>
<p>Novel-view synthesis is an important problem in computer vision with applications in 3D reconstruction, mixed reality, and robotics. Recent methods like 3D Gaussian Splatting (3DGS) have become the preferred method for this task, providing high-quality novel views in real time. However, the training time of a 3DGS model is slow, often taking 30 minutes for a scene with 200 views. In contrast, our goal is to reduce the optimization time by training for fewer steps while maintaining high rendering quality. Specifically, we combine the guidance from both the position error and the appearance error to achieve a more effective densification. To balance the rate between adding new Gaussians and fitting old Gaussians, we develop a convergence-aware budget control mechanism. Moreover, to make the densification process more reliable, we selectively add new Gaussians from mostly visited regions. With these designs, we reduce the Gaussian optimization steps to one-third of the previous approach while achieving a comparable or even better novel view rendering quality. To further facilitate the rapid fitting of 4K resolution images, we introduce a dilation-based rendering technique. Our method, Turbo-GS, speeds up optimization for typical scenes and scales well to high-resolution (4K) scenarios on standard datasets. Through extensive experiments, we show that our method is significantly faster in optimization than other methods while retaining quality. Project page: <a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/turbo-gs">https://ivl.cs.brown.edu/research/turbo-gs</a>. </p>
<blockquote>
<p>æ–°é¢–è§†è§’åˆæˆæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªé‡è¦é—®é¢˜ï¼Œåœ¨3Dé‡å»ºã€æ··åˆç°å®å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚æœ€è¿‘çš„æ–¹æ³•å¦‚3Dé«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰å·²æˆä¸ºè¯¥ä»»åŠ¡çš„é¦–é€‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶ç”Ÿæˆé«˜è´¨é‡çš„æ–°é¢–è§†è§’ã€‚ç„¶è€Œï¼Œ3DGSæ¨¡å‹çš„è®­ç»ƒæ—¶é—´è¾ƒæ…¢ï¼Œå¯¹äºåŒ…å«200ä¸ªè§†è§’çš„åœºæ™¯é€šå¸¸éœ€è¦30åˆ†é’Ÿã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡å‡å°‘è®­ç»ƒæ­¥éª¤æ¥ç¼©çŸ­ä¼˜åŒ–æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒé«˜æ¸²æŸ“è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç»“åˆäº†ä½ç½®è¯¯å·®å’Œå¤–è§‚è¯¯å·®çš„æŒ‡å¯¼æ¥å®ç°æ›´æœ‰æ•ˆçš„å¯†é›†åŒ–ã€‚ä¸ºäº†å¹³è¡¡æ·»åŠ æ–°é«˜æ–¯å’Œæ‹Ÿåˆæ—§é«˜æ–¯ä¹‹é—´çš„æ¯”ç‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ”¶æ•›æ„ŸçŸ¥é¢„ç®—æ§åˆ¶æœºåˆ¶ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä½¿å¯†é›†åŒ–è¿‡ç¨‹æ›´åŠ å¯é ï¼Œæˆ‘ä»¬ä»è®¿é—®è¾ƒå¤šçš„åŒºåŸŸä¸­é€‰æ‹©æ€§åœ°æ·»åŠ æ–°çš„é«˜æ–¯å€¼ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæˆ‘ä»¬å°†é«˜æ–¯ä¼˜åŒ–æ­¥éª¤å‡å°‘åˆ°å…ˆå‰æ–¹æ³•çš„ä¸‰åˆ†ä¹‹ä¸€ï¼ŒåŒæ—¶å®ç°ç›¸å½“æˆ–æ›´å¥½çš„æ–°é¢–è§†è§’æ¸²æŸ“è´¨é‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¿ƒè¿›4Kåˆ†è¾¨ç‡å›¾åƒçš„å¿«é€Ÿæ‹Ÿåˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºè†¨èƒ€çš„æ¸²æŸ“æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•Turbo-GSåŠ é€Ÿå…¸å‹åœºæ™¯çš„ä¼˜åŒ–ï¼Œåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šå¾ˆå¥½åœ°æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡ï¼ˆ4Kï¼‰åœºæ™¯ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ˜¾è‘—æ›´å¿«ä¸”ä¿ç•™è´¨é‡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/turbo-gs">https://ivl.cs.brown.edu/research/turbo-gs</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13547v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ivl.cs.brown.edu/research/turbo-gs">https://ivl.cs.brown.edu/research/turbo-gs</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„æ–°å‹è§†å›¾åˆæˆé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨3Dé‡å»ºã€æ··åˆç°å®å’Œæœºå™¨äººæŠ€æœ¯ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹å½“å‰æ–¹æ³•å¦‚3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è®­ç»ƒæ—¶é—´é•¿çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•Turbo-GSã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆä½ç½®è¯¯å·®å’Œå¤–è§‚è¯¯å·®çš„æŒ‡å¯¼ï¼Œå®ç°æ›´æœ‰æ•ˆçš„å¯†é›†åŒ–ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼€å‘æ”¶æ•›æ„ŸçŸ¥é¢„ç®—æ§åˆ¶æœºåˆ¶å’Œé€‰æ‹©æ€§æ·»åŠ æ–°é«˜æ–¯çš„æ–¹å¼ï¼Œå°†é«˜æ–¯ä¼˜åŒ–æ­¥éª¤å‡å°‘è‡³åŸæ–¹æ³•çš„ä¸‰åˆ†ä¹‹ä¸€ï¼Œå®ç°äº†ç›¸å½“æˆ–æ›´å¥½çš„æ–°å‹è§†å›¾æ¸²æŸ“è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åŸºäºè†¨èƒ€çš„æ¸²æŸ“æŠ€æœ¯ï¼Œä»¥åŠ å¿«4Kåˆ†è¾¨ç‡å›¾åƒçš„å¿«é€Ÿæ‹Ÿåˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¼˜åŒ–é€Ÿåº¦ä¸Šæ˜¾è‘—å¿«äºå…¶ä»–æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–°å‹è§†å›¾åˆæˆæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦é—®é¢˜ï¼Œåº”ç”¨äº3Dé‡å»ºã€æ··åˆç°å®å’Œæœºå™¨äººæŠ€æœ¯ã€‚</li>
<li>3DGSç­‰æ–¹æ³•è™½ç„¶èƒ½æä¾›é«˜è´¨é‡çš„æ–°å‹è§†å›¾ï¼Œä½†è®­ç»ƒæ—¶é—´é•¿ã€‚</li>
<li>Turbo-GSæ–¹æ³•é€šè¿‡ç»“åˆä½ç½®è¯¯å·®å’Œå¤–è§‚è¯¯å·®æŒ‡å¯¼ï¼Œå®ç°æ›´æœ‰æ•ˆçš„å¯†é›†åŒ–ã€‚</li>
<li>æ”¶æ•›æ„ŸçŸ¥é¢„ç®—æ§åˆ¶æœºåˆ¶å’Œé€‰æ‹©æ€§æ·»åŠ æ–°é«˜æ–¯çš„æ–¹å¼ï¼Œå¤§å¹…å‡å°‘ä¼˜åŒ–æ­¥éª¤ã€‚</li>
<li>Turbo-GSæ–¹æ³•å®ç°ç›¸å½“æˆ–æ›´å¥½çš„æ–°å‹è§†å›¾æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶æ˜¾è‘—åŠ å¿«ä¼˜åŒ–é€Ÿåº¦ã€‚</li>
<li>å¼•å…¥åŸºäºè†¨èƒ€çš„æ¸²æŸ“æŠ€æœ¯ï¼Œä»¥åŠ å¿«4Kåˆ†è¾¨ç‡å›¾åƒçš„å¿«é€Ÿæ‹Ÿåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13547">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20b0477f556c30ffedf499546db8211f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a41962c7515a584245039e30796fd52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-063f4dfb9a1b434603b0f9427b772f98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94f08016f18ccb40739f6b7c431ed703.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3cc34467125f9fdf9839a3eec0d1618.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GaussTR-Foundation-Model-Aligned-Gaussian-Transformer-for-Self-Supervised-3D-Spatial-Understanding"><a href="#GaussTR-Foundation-Model-Aligned-Gaussian-Transformer-for-Self-Supervised-3D-Spatial-Understanding" class="headerlink" title="GaussTR: Foundation Model-Aligned Gaussian Transformer for   Self-Supervised 3D Spatial Understanding"></a>GaussTR: Foundation Model-Aligned Gaussian Transformer for   Self-Supervised 3D Spatial Understanding</h2><p><strong>Authors:Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong Su, Wenyu Liu, Xinggang Wang</strong></p>
<p>3D Semantic Occupancy Prediction is fundamental for spatial understanding as it provides a comprehensive semantic cognition of surrounding environments. However, prevalent approaches primarily rely on extensive labeled data and computationally intensive voxel-based modeling, restricting the scalability and generalizability of 3D representation learning. In this paper, we introduce GaussTR, a novel Gaussian Transformer that leverages alignment with foundation models to advance self-supervised 3D spatial understanding. GaussTR adopts a Transformer architecture to predict sparse sets of 3D Gaussians that represent scenes in a feed-forward manner. Through aligning rendered Gaussian features with diverse knowledge from pre-trained foundation models, GaussTR facilitates the learning of versatile 3D representations and enables open-vocabulary occupancy prediction without explicit annotations. Empirical evaluations on the Occ3D-nuScenes dataset showcase GaussTRâ€™s state-of-the-art zero-shot performance, achieving 11.70 mIoU while reducing training duration by approximately 50%. These experimental results highlight the significant potential of GaussTR for scalable and holistic 3D spatial understanding, with promising implications for autonomous driving and embodied agents. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hustvl/GaussTR">https://github.com/hustvl/GaussTR</a>. </p>
<blockquote>
<p>3Dè¯­ä¹‰å ç”¨é¢„æµ‹å¯¹äºç©ºé—´ç†è§£è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒæä¾›äº†å¯¹å‘¨å›´ç¯å¢ƒçš„å…¨é¢è¯­ä¹‰è®¤çŸ¥ã€‚ç„¶è€Œï¼Œæµè¡Œçš„æ–¹æ³•ä¸»è¦ä¾èµ–äºå¤§é‡çš„æ ‡è®°æ•°æ®å’Œè®¡ç®—å¯†é›†å‹çš„åŸºäºä½“ç´ ï¼ˆvoxel-basedï¼‰å»ºæ¨¡ï¼Œè¿™é™åˆ¶äº†3Dè¡¨ç¤ºå­¦ä¹ çš„å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GaussTRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é«˜æ–¯å˜æ¢å™¨ï¼ˆGaussian Transformerï¼‰ï¼Œå®ƒåˆ©ç”¨ä¸åŸºç¡€æ¨¡å‹çš„å¯¹é½æ¥æ¨è¿›è‡ªæˆ‘ç›‘ç£çš„3Dç©ºé—´ç†è§£ã€‚GaussTRé‡‡ç”¨Transformeræ¶æ„ï¼Œä»¥å‰é¦ˆæ–¹å¼é¢„æµ‹ä»£è¡¨åœºæ™¯çš„ç¨€ç–3Dé«˜æ–¯é›†ã€‚é€šè¿‡ä½¿æ¸²æŸ“çš„é«˜æ–¯ç‰¹å¾ä¸æ¥è‡ªé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„å¤šæ ·åŒ–çŸ¥è¯†å¯¹é½ï¼ŒGaussTRä¿ƒè¿›äº†å¤šç§3Dè¡¨ç¤ºçš„å­¦ä¹ ï¼Œå¹¶èƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜¾å¼æ³¨é‡Šçš„æƒ…å†µä¸‹å®ç°å¼€æ”¾è¯æ±‡è¡¨çš„å ç”¨é¢„æµ‹ã€‚åœ¨Occ3D-nuScenesæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°å±•ç¤ºäº†GaussTRçš„é›¶æ ·æœ¬æ€§èƒ½å¤„äºæœ€æ–°æ°´å¹³ï¼Œå®ç°äº†11.70 mIoUï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†çº¦50%ã€‚è¿™äº›å®éªŒç»“æœçªå‡ºäº†GaussTRåœ¨å¯æ‰©å±•å’Œå…¨é¢çš„3Dç©ºé—´ç†è§£æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶å’Œå®ä½“ä»£ç†å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hustvl/GaussTR%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hustvl/GaussTRè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13193v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†GaussTRæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨é«˜æ–¯å˜æ¢å™¨è¿›è¡Œä¸‰ç»´è¯­ä¹‰å ç”¨é¢„æµ‹çš„æ–¹æ³•ã€‚è¯¥æ¨¡å‹é€šè¿‡é¢„æµ‹ç¨€ç–ä¸‰ç»´é«˜æ–¯é›†æ¥ä»¥è‡ªé€‚åº”æ–¹å¼å‘ˆç°åœºæ™¯ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„ä¸°å¯ŒçŸ¥è¯†å®ç°è‡ªæˆ‘ç›‘ç£çš„ä¸‰ç»´ç©ºé—´ç†è§£ã€‚é«˜æ–¯TRæ¨¡å‹æ— éœ€æ˜ç¡®æ³¨é‡Šå³å¯è¿›è¡Œå¼€æ”¾å¼è¯æ±‡å ç”¨é¢„æµ‹ï¼Œå¹¶åœ¨Occ3D-nuScenesæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå®ç°äº†11.70 mIoUï¼ŒåŒæ—¶å‡å°‘äº†å¤§çº¦50%çš„è®­ç»ƒæ—¶é—´ã€‚è¿™ä¸ºå¯æ‰©å±•å’Œå…¨é¢çš„ä¸‰ç»´ç©ºé—´ç†è§£æä¾›äº†æ˜¾è‘—æ½œåŠ›ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½ä½“å…·æœ‰é‡è¦å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GaussTRæ¨¡å‹åˆ©ç”¨é«˜æ–¯å˜æ¢å™¨è¿›è¡Œä¸‰ç»´è¯­ä¹‰å ç”¨é¢„æµ‹ï¼Œä¸ºå‘¨å›´ç¯å¢ƒæä¾›å…¨é¢çš„è¯­ä¹‰è®¤çŸ¥ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨Transformeræ¶æ„é¢„æµ‹ç¨€ç–ä¸‰ç»´é«˜æ–¯é›†æ¥å‘ˆç°åœºæ™¯ã€‚</li>
<li>GaussTRé€šè¿‡ä¸é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¯¹é½æ¥ä¸°å¯Œå…¶çŸ¥è¯†ï¼Œå¹¶å®ç°è‡ªæˆ‘ç›‘ç£çš„ä¸‰ç»´è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>GaussTRæ”¯æŒå¼€æ”¾å¼è¯æ±‡å ç”¨é¢„æµ‹ï¼Œæ— éœ€æ˜ç¡®æ³¨é‡Šã€‚</li>
<li>GaussTRåœ¨Occ3D-nuScenesæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œè¾¾åˆ°äº†11.70 mIoUã€‚</li>
<li>GaussTRæ¨¡å‹å‡å°‘äº†å¤§çº¦50%çš„è®­ç»ƒæ—¶é—´ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-54d2416a8165c9f385d61712def97c10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb0c29decf6385424cffdaca5b6a3ea0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5157c5a79b737eab53e6269cde7e0a90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a07829a49dd81bb2569cf072e3346f5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91d012bdabb8d0c24ca213fab5316f0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9637537dcfd31e22abbd61238fef5da.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Real-time-Free-view-Human-Rendering-from-Sparse-view-RGB-Videos-using-Double-Unprojected-Textures"><a href="#Real-time-Free-view-Human-Rendering-from-Sparse-view-RGB-Videos-using-Double-Unprojected-Textures" class="headerlink" title="Real-time Free-view Human Rendering from Sparse-view RGB Videos using   Double Unprojected Textures"></a>Real-time Free-view Human Rendering from Sparse-view RGB Videos using   Double Unprojected Textures</h2><p><strong>Authors:Guoxing Sun, Rishabh Dabral, Heming Zhu, Pascal Fua, Christian Theobalt, Marc Habermann</strong></p>
<p>Real-time free-view human rendering from sparse-view RGB inputs is a challenging task due to the sensor scarcity and the tight time budget. To ensure efficiency, recent methods leverage 2D CNNs operating in texture space to learn rendering primitives. However, they either jointly learn geometry and appearance, or completely ignore sparse image information for geometry estimation, significantly harming visual quality and robustness to unseen body poses. To address these issues, we present Double Unprojected Textures, which at the core disentangles coarse geometric deformation estimation from appearance synthesis, enabling robust and photorealistic 4K rendering in real-time. Specifically, we first introduce a novel image-conditioned template deformation network, which estimates the coarse deformation of the human template from a first unprojected texture. This updated geometry is then used to apply a second and more accurate texture unprojection. The resulting texture map has fewer artifacts and better alignment with input views, which benefits our learning of finer-level geometry and appearance represented by Gaussian splats. We validate the effectiveness and efficiency of the proposed method in quantitative and qualitative experiments, which significantly surpasses other state-of-the-art methods. </p>
<blockquote>
<p>ä»ç¨€ç–è§†å›¾RGBè¾“å…¥è¿›è¡Œå®æ—¶è‡ªç”±è§†è§’äººç±»æ¸²æŸ“æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä¼ æ„Ÿå™¨ç¨€ç¼ºå’Œä¸¥æ ¼çš„æ—¶é—´é¢„ç®—é™åˆ¶ã€‚ä¸ºäº†ä¿è¯æ•ˆç‡ï¼Œæœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨åœ¨çº¹ç†ç©ºé—´æ“ä½œçš„2Då·ç§¯ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ¸²æŸ“åŸç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬è¦ä¹ˆè”åˆå­¦ä¹ å‡ ä½•å’Œå¤–è§‚ï¼Œè¦ä¹ˆå®Œå…¨å¿½ç•¥ç¨€ç–å›¾åƒä¿¡æ¯è¿›è¡Œå‡ ä½•ä¼°è®¡ï¼Œè¿™ä¸¥é‡æŸå®³äº†è§†è§‰è´¨é‡å’Œå¯¹æœªè§å§¿æ€çš„é²æ£’æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒæœªæŠ•å½±çº¹ç†ï¼ˆDouble Unprojected Texturesï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å°†ç²—ç³™çš„å‡ ä½•å˜å½¢ä¼°è®¡ä»å¤–è§‚åˆæˆä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå®ç°å®æ—¶ã€ç¨³å¥å’Œé€¼çœŸçš„4Kæ¸²æŸ“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§æ–°å‹å›¾åƒæ¡ä»¶æ¨¡æ¿å˜å½¢ç½‘ç»œï¼Œè¯¥ç½‘ç»œä»ç¬¬ä¸€ä¸ªæœªæŠ•å½±çº¹ç†ä¼°è®¡äººç±»æ¨¡æ¿çš„ç²—ç³™å˜å½¢ã€‚ç„¶åï¼Œä½¿ç”¨è¿™ä¸ªæ›´æ–°çš„å‡ ä½•æ¥åº”ç”¨ç¬¬äºŒä¸ªæ›´åŠ ç²¾ç¡®çš„çº¹ç†åæŠ•å½±ã€‚ç»“æœå¾—åˆ°çš„çº¹ç†è´´å›¾å…·æœ‰è¾ƒå°‘çš„ä¼ªå½±å¹¶ä¸”ä¸è¾“å…¥è§†å›¾å¯¹é½æ›´å¥½ï¼Œè¿™æœ‰åˆ©äºæˆ‘ä»¬å­¦ä¹ ç”±é«˜æ–¯ç‚¹è¡¨ç¤ºçš„æ›´ç²¾ç»†çº§åˆ«çš„å‡ ä½•å’Œå¤–è§‚ã€‚æˆ‘ä»¬åœ¨å®šé‡å’Œå®šæ€§å®éªŒä¸­éƒ½éªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13183v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://vcai.mpi-inf.mpg.de/projects/DUT/">https://vcai.mpi-inf.mpg.de/projects/DUT/</a></p>
<p><strong>Summary</strong></p>
<p>å®æ—¶ä»ç¨€ç–è§†è§’RGBè¾“å…¥è¿›è¡Œè‡ªç”±è§†è§’äººç±»æ¸²æŸ“æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œè¿‘æœŸæ–¹æ³•åˆ©ç”¨äºŒç»´å·ç§¯ç¥ç»ç½‘ç»œåœ¨çº¹ç†ç©ºé—´å­¦ä¹ æ¸²æŸ“åŸç†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆè”åˆå­¦ä¹ å‡ ä½•å’Œå¤–è§‚ï¼Œè¦ä¹ˆåœ¨ä¼°è®¡å‡ ä½•æ—¶å®Œå…¨å¿½ç•¥ç¨€ç–å›¾åƒä¿¡æ¯ï¼Œä¸¥é‡æŸå®³è§†è§‰è´¨é‡å’Œå¯¹æœªè§å§¿æ€çš„é²æ£’æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºDouble Unprojected Texturesæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¸å¿ƒå°†ç²—ç•¥å‡ ä½•å˜å½¢ä¼°è®¡ä¸å¤–è§‚åˆæˆè§£è€¦ï¼Œå®ç°å®æ—¶é²æ£’ä¸”é€¼çœŸçš„4Kæ¸²æŸ“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥å›¾åƒæ¡ä»¶æ¨¡æ¿å˜å½¢ç½‘ç»œï¼Œä»ç¬¬ä¸€æ¬¡æœªæŠ•å½±çº¹ç†ä¼°è®¡äººç±»æ¨¡æ¿çš„ç²—ç•¥å˜å½¢ã€‚æ›´æ–°çš„å‡ ä½•å†ç”¨äºåº”ç”¨ç¬¬äºŒæ¬¡æ›´å‡†ç¡®çº¹ç†çš„æœªæŠ•å½±ã€‚ç»“æœçº¹ç†å›¾å…·æœ‰è¾ƒå°‘çš„ä¼ªå½±å¹¶ä¸è¾“å…¥è§†å›¾å¯¹é½æ›´å¥½ï¼Œè¿™æœ‰åˆ©äºæˆ‘ä»¬å­¦ä¹ ç”±é«˜æ–¯æ–‘è¡¨ç¤ºçš„æ›´ç²¾ç»†çº§åˆ«çš„å‡ ä½•å’Œå¤–è§‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶è‡ªç”±è§†è§’äººç±»æ¸²æŸ“æ˜¯é¡¹è‰°å·¨ä»»åŠ¡ï¼Œå¾—ç›ŠäºäºŒç»´å·ç§¯ç¥ç»ç½‘ç»œåœ¨çº¹ç†ç©ºé—´çš„å­¦ä¹ ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è”åˆå­¦ä¹ å‡ ä½•å’Œå¤–è§‚æˆ–å¿½ç•¥ç¨€ç–å›¾åƒä¿¡æ¯ï¼Œå¯¼è‡´è§†è§‰è´¨é‡å’Œé²æ£’æ€§å—æŸã€‚</li>
<li>æå‡ºDouble Unprojected Texturesæ–¹æ³•ï¼Œå°†å‡ ä½•å˜å½¢ä¸å¤–è§‚åˆæˆè§£è€¦ï¼Œå®ç°æ›´å‡†ç¡®ã€é€¼çœŸçš„æ¸²æŸ“ã€‚</li>
<li>å¼•å…¥å›¾åƒæ¡ä»¶æ¨¡æ¿å˜å½¢ç½‘ç»œï¼Œä»ç¬¬ä¸€æ¬¡æœªæŠ•å½±çº¹ç†ä¼°è®¡ç²—ç•¥å˜å½¢ã€‚</li>
<li>æ›´æ–°å‡ ä½•ç”¨äºç¬¬äºŒæ¬¡æ›´å‡†ç¡®çº¹ç†æœªæŠ•å½±ï¼Œå‡å°‘ä¼ªå½±å¹¶ä¸è¾“å…¥è§†å›¾å¯¹é½æ›´å¥½ã€‚</li>
<li>ç»“æœçº¹ç†å›¾æœ‰åŠ©äºå­¦ä¹ æ›´ç²¾ç»†çº§åˆ«çš„å‡ ä½•å’Œå¤–è§‚è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6dc5fac88eef97a64b1f9761fa46c439.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c28f3445ea04918729632c1bfccb420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5edbdfa3844f164ad53eabe4314ab846.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b2accd7f2b87c585fe1a041c323fde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aeaa370c60365a33ffc79433fd2f351.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="NFL-BA-Improving-Endoscopic-SLAM-with-Near-Field-Light-Bundle-Adjustment"><a href="#NFL-BA-Improving-Endoscopic-SLAM-with-Near-Field-Light-Bundle-Adjustment" class="headerlink" title="NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle   Adjustment"></a>NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle   Adjustment</h2><p><strong>Authors:Andrea Dunn Beltran, Daniel Rho, Marc Niethammer, Roni Sengupta</strong></p>
<p>Simultaneous Localization And Mapping (SLAM) from a monocular endoscopy video can enable autonomous navigation, guidance to unsurveyed regions, and 3D visualizations, which can significantly improve endoscopy experience for surgeons and patient outcomes. Existing dense SLAM algorithms often assume distant and static lighting and textured surfaces, and alternate between optimizing scene geometry and camera parameters by minimizing a photometric rendering loss, often called Photometric Bundle Adjustment. However, endoscopic environments exhibit dynamic near-field lighting due to the co-located light and camera moving extremely close to the surface, textureless surfaces, and strong specular reflections due to mucus layers. When not considered, these near-field lighting effects can cause significant performance reductions for existing SLAM algorithms from indoor&#x2F;outdoor scenes when applied to endoscopy videos. To mitigate this problem, we introduce a new Near-Field Lighting Bundle Adjustment Loss $(L_{NFL-BA})$ that can also be alternatingly optimized, along with the Photometric Bundle Adjustment loss, such that the captured imagesâ€™ intensity variations match the relative distance and orientation between the surface and the co-located light and camera. We derive a general NFL-BA loss function for 3D Gaussian surface representations and demonstrate that adding $L_{NFL-BA}$ can significantly improve the tracking and mapping performance of two state-of-the-art 3DGS-SLAM systems, MonoGS (35% improvement in tracking, 48% improvement in mapping with predicted depth maps) and EndoGSLAM (22% improvement in tracking, marginal improvement in mapping with predicted depths), on the C3VD endoscopy dataset for colons. The project page is available at <a target="_blank" rel="noopener" href="https://asdunnbe.github.io/NFL-BA/">https://asdunnbe.github.io/NFL-BA/</a> </p>
<blockquote>
<p>ä»å•ç›®å†…çª¥é•œè§†é¢‘è¿›è¡Œçš„åŒæ­¥å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰å¯ä»¥å®ç°è‡ªä¸»å¯¼èˆªã€å¯¹æœªå‹˜æµ‹åŒºåŸŸçš„æŒ‡å¯¼ä»¥åŠ3Då¯è§†åŒ–ï¼Œè¿™å¯ä»¥æ˜¾è‘—æ”¹å–„å¤–ç§‘åŒ»ç”Ÿçš„å†…çª¥é•œä½“éªŒå¹¶æ”¹å–„æ‚£è€…ç»“æœã€‚ç°æœ‰çš„å¯†é›†SLAMç®—æ³•é€šå¸¸å‡è®¾å…‰ç…§è·ç¦»è¾ƒè¿œä¸”é™æ€ï¼Œè¡¨é¢æœ‰çº¹ç†ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–å…‰åº¦æ¸²æŸ“æŸå¤±æ¥ä¼˜åŒ–åœºæ™¯å‡ ä½•å’Œç›¸æœºå‚æ•°ï¼Œè¿™é€šå¸¸è¢«ç§°ä¸ºå…‰åº¦æ†ç»‘è°ƒæ•´ã€‚ç„¶è€Œï¼Œå†…çª¥é•œç¯å¢ƒè¡¨ç°å‡ºç”±äºå…‰æºå’Œç›¸æœºæè¿‘è·ç¦»è¡¨é¢è€Œäº§ç”Ÿçš„åŠ¨æ€è¿‘åœºå…‰ç…§ã€æ— çº¹ç†çš„è¡¨é¢ä»¥åŠç”±äºç²˜æ¶²å±‚äº§ç”Ÿçš„å¼ºçƒˆé•œé¢åå°„ã€‚å½“ä¸è€ƒè™‘è¿™äº›å› ç´ æ—¶ï¼Œè¿™äº›è¿‘åœºç…§æ˜æ•ˆæœä¼šå¯¹åº”ç”¨äºå†…çª¥é•œè§†é¢‘æ—¶çš„å®¤å†…&#x2F;å®¤å¤–åœºæ™¯çš„ç°æœ‰SLAMç®—æ³•é€ æˆæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è¿‘åœºç…§æ˜æ†ç»‘è°ƒæ•´æŸå¤±ï¼ˆL_NFL-BAï¼‰ï¼Œå®ƒå¯ä»¥ä¸å…‰åº¦æ†ç»‘è°ƒæ•´æŸå¤±äº¤æ›¿ä¼˜åŒ–ï¼Œä»¥ä½¿æ•è·çš„å›¾åƒå¼ºåº¦å˜åŒ–ä¸è¡¨é¢ä¸ååŒå®šä½çš„å…‰æºå’Œç›¸æœºä¹‹é—´çš„ç›¸å¯¹è·ç¦»å’Œæ–¹å‘ç›¸åŒ¹é…ã€‚æˆ‘ä»¬ä¸º3Dé«˜æ–¯è¡¨é¢è¡¨ç¤ºæ³•æ¨å¯¼äº†ä¸€ä¸ªé€šç”¨çš„NFL-BAæŸå¤±å‡½æ•°ï¼Œå¹¶è¯æ˜æ·»åŠ L_NFL-BAå¯ä»¥æ˜¾ç€æé«˜ä¸¤ç§æœ€æ–°3DGS-SLAMç³»ç»Ÿï¼ˆMonoGSï¼ˆè·Ÿè¸ªæé«˜35%ï¼Œä½¿ç”¨é¢„æµ‹æ·±åº¦å›¾è¿›è¡Œæ˜ å°„æé«˜48%ï¼‰å’ŒEndoGSLAMï¼ˆè·Ÿè¸ªæé«˜22%ï¼Œä½¿ç”¨é¢„æµ‹æ·±åº¦è¿›è¡Œæ˜ å°„è½»å¾®æ”¹è¿›ï¼‰ï¼‰çš„è·Ÿè¸ªå’Œæ˜ å°„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://asdunnbe.github.io/NFL-BA/">https://asdunnbe.github.io/NFL-BA/</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13176v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å•ç›®å†…çª¥é•œè§†é¢‘ä¸­åˆ©ç”¨åŒæ­¥å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰æŠ€æœ¯å®ç°è‡ªä¸»å¯¼èˆªã€æœªå‹˜æµ‹åŒºåŸŸå¼•å¯¼å’Œä¸‰ç»´å¯è§†åŒ–ã€‚é’ˆå¯¹å†…çª¥é•œç¯å¢ƒä¸­çš„è¿‘åœºç…§æ˜æ•ˆåº”ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„è¿‘åœºç…§æ˜æ†ç»‘è°ƒæ•´æŸå¤±ï¼ˆL_{NFL-BA}ï¼‰ï¼Œå¹¶ä¸å…‰åº¦æ†ç»‘è°ƒæ•´æŸå¤±äº¤æ›¿ä¼˜åŒ–ï¼Œä»¥åŒ¹é…å›¾åƒå¼ºåº¦å˜åŒ–ä¸è¡¨é¢ä¸ååŒç…§æ˜å’Œç›¸æœºä¹‹é—´çš„è·ç¦»å’Œæ–¹ä½ã€‚è¯¥æŸå¤±å‡½æ•°å¯æ˜¾è‘—æé«˜ä¸¤ç§æœ€å…ˆè¿›çš„3DGS-SLAMç³»ç»Ÿï¼ˆMonoGSå’ŒEndoGSLAMï¼‰åœ¨C3VDç»“è‚ å†…çª¥é•œæ•°æ®é›†ä¸Šçš„è·Ÿè¸ªå’Œæ˜ å°„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLAMæŠ€æœ¯åº”ç”¨äºå•ç›®å†…çª¥é•œè§†é¢‘ï¼Œèƒ½æå‡è‡ªä¸»å¯¼èˆªã€æœªå‹˜æµ‹åŒºåŸŸå¼•å¯¼å’Œä¸‰ç»´å¯è§†åŒ–ã€‚</li>
<li>å†…çª¥é•œç¯å¢ƒå­˜åœ¨åŠ¨æ€è¿‘åœºç…§æ˜ã€æ— çº¹ç†è¡¨é¢å’Œç²˜æ¶²å±‚å¼•èµ·çš„å¼ºé•œé¢åå°„ï¼Œå¯¹ç°æœ‰çš„SLAMç®—æ³•æ€§èƒ½é€ æˆä¸¥é‡å½±å“ã€‚</li>
<li>å¼•å…¥æ–°çš„è¿‘åœºç…§æ˜æ†ç»‘è°ƒæ•´æŸå¤±ï¼ˆL_{NFL-BA}ï¼‰ï¼Œä»¥åº”å¯¹å†…çª¥é•œè§†é¢‘çš„è¿‘åœºç…§æ˜æ•ˆåº”ã€‚</li>
<li>L_{NFL-BA}å¯äº¤æ›¿ä¼˜åŒ–ï¼Œä¸å…‰åº¦æ†ç»‘è°ƒæ•´æŸå¤±ç›¸ç»“åˆï¼ŒåŒ¹é…å›¾åƒå¼ºåº¦å˜åŒ–ä¸è¡¨é¢å’Œç›¸æœºä¹‹é—´çš„è·ç¦»å’Œæ–¹ä½ã€‚</li>
<li>L_{NFL-BA}çš„å¼•å…¥æ˜¾è‘—æé«˜äº†MonoGSå’ŒEndoGSLAMä¸¤ç§3DGS-SLAMç³»ç»Ÿåœ¨C3VDç»“è‚ å†…çª¥é•œæ•°æ®é›†ä¸Šçš„è·Ÿè¸ªå’Œæ˜ å°„æ€§èƒ½ã€‚</li>
<li>è¿‘åœºç…§æ˜æ•ˆåº”å¯¹SLAMç®—æ³•æ€§èƒ½çš„å½±å“ï¼Œæ­ç¤ºäº†æœªæ¥ç ”ç©¶ä¸­éœ€è¦æ›´å¤šè€ƒè™‘å†…çª¥é•œç¯å¢ƒçš„ç‰¹æ®Šæ€§è´¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e48e2174549d15bd638e7199a25a2558.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f81d0be54205232f702264e6e5f5886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70cc800d9ae8bc299fd97c7a89220e44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd6d48ea0d9a3b3b47144bf8e9e2967e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EOGS-Gaussian-Splatting-for-Earth-Observation"><a href="#EOGS-Gaussian-Splatting-for-Earth-Observation" class="headerlink" title="EOGS: Gaussian Splatting for Earth Observation"></a>EOGS: Gaussian Splatting for Earth Observation</h2><p><strong>Authors:Luca Savant Aira, Gabriele Facciolo, Thibaud Ehret</strong></p>
<p>Recently, Gaussian splatting has emerged as a strong alternative to NeRF, demonstrating impressive 3D modeling capabilities while requiring only a fraction of the training and rendering time. In this paper, we show how the standard Gaussian splatting framework can be adapted for remote sensing, retaining its high efficiency. This enables us to achieve state-of-the-art performance in just a few minutes, compared to the day-long optimization required by the best-performing NeRF-based Earth observation methods. The proposed framework incorporates remote-sensing improvements from EO-NeRF, such as radiometric correction and shadow modeling, while introducing novel components, including sparsity, view consistency, and opacity regularizations. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé«˜æ–¯æ¶‚æ–‘æŠ€æœ¯ä½œä¸ºNeRFçš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆå´­éœ²å¤´è§’ï¼Œå®ƒåœ¨åªéœ€ä¸€å°éƒ¨åˆ†è®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´çš„æƒ…å†µä¸‹è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„3Då»ºæ¨¡èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†æ ‡å‡†çš„é«˜æ–¯æ¶‚æ–‘æ¡†æ¶é€‚åº”äºé¥æ„Ÿé¢†åŸŸï¼ŒåŒæ—¶ä¿æŒå…¶é«˜æ•ˆç‡ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåœ¨å‡ åˆ†é’Ÿå†…å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œè¡¨ç°æœ€ä½³çš„åŸºäºNeRFçš„åœ°çƒè§‚æµ‹æ–¹æ³•åˆ™éœ€è¦ä¸€æ•´å¤©çš„ä¼˜åŒ–æ—¶é—´ã€‚æ‰€æå‡ºçš„æ¡†æ¶çº³å…¥äº†EO-NeRFçš„é¥æ„Ÿæ”¹è¿›å†…å®¹ï¼Œä¾‹å¦‚è¾å°„æ ¡æ­£å’Œé˜´å½±å»ºæ¨¡ï¼ŒåŒæ—¶å¼•å…¥äº†æ–°é¢–ç»„ä»¶ï¼ŒåŒ…æ‹¬ç¨€ç–æ€§ã€è§†å›¾ä¸€è‡´æ€§å’Œä¸é€æ˜åº¦æ­£åˆ™åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13047v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é«˜æ–¯è´´å›¾æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä½œä¸ºNeRFçš„æ›¿ä»£æ–¹æ¡ˆå±•ç°å‡ºå¼ºå¤§çš„3Då»ºæ¨¡èƒ½åŠ›ï¼Œä¸”è®­ç»ƒä¸æ¸²æŸ“æ—¶é—´å¤§å¹…å‡å°‘ã€‚æ–‡ç« å±•ç¤ºäº†å¦‚ä½•å°†æ ‡å‡†é«˜æ–¯è´´å›¾æ¡†æ¶é€‚åº”äºé¥æ„Ÿé¢†åŸŸï¼Œå¹¶ä¿æŒå…¶é«˜æ•ˆç‡ã€‚ç›¸è¾ƒäºç°æœ‰é¢†å…ˆçš„NeRFåœ°çƒè§‚æµ‹æ–¹æ³•ï¼Œæ–°æ¡†æ¶å¯åœ¨å‡ åˆ†é’Ÿå†…è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæ— éœ€é•¿æ—¶é—´çš„ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶ç»“åˆäº†EO-NeRFçš„é¥æ„Ÿæ”¹è¿›æŠ€æœ¯ï¼Œå¦‚è¾å°„æ ¡æ­£å’Œé˜´å½±å»ºæ¨¡ï¼Œå¹¶å¼•å…¥äº†ç¨€ç–æ€§ã€è§†å›¾ä¸€è‡´æ€§å’Œé€æ˜åº¦æ­£åˆ™åŒ–ç­‰æ–°é¢–ç»„ä»¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ–¯è´´å›¾æŠ€æœ¯ä½œä¸ºNeRFçš„æ›¿ä»£æ–¹æ¡ˆå±•ç°å‡ºå¼ºå¤§çš„3Då»ºæ¨¡èƒ½åŠ›ï¼Œä¸”è®­ç»ƒä¸æ¸²æŸ“æ—¶é—´å¤§å¹…å‡å°‘ã€‚</li>
<li>æ–‡ç« å°†æ ‡å‡†é«˜æ–¯è´´å›¾æ¡†æ¶é€‚åº”äºé¥æ„Ÿé¢†åŸŸï¼Œå¹¶ä¿æŒå…¶é«˜æ•ˆç‡ã€‚</li>
<li>æ–°æ¡†æ¶å¯åœ¨å‡ åˆ†é’Ÿå†…è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæ— éœ€é•¿æ—¶é—´çš„ä¼˜åŒ–ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†EO-NeRFçš„é¥æ„Ÿæ”¹è¿›æŠ€æœ¯ï¼Œå¦‚è¾å°„æ ¡æ­£å’Œé˜´å½±å»ºæ¨¡ã€‚</li>
<li>æ–°æ¡†æ¶å¼•å…¥äº†ç¨€ç–æ€§ã€è§†å›¾ä¸€è‡´æ€§å’Œé€æ˜åº¦æ­£åˆ™åŒ–ç­‰æ–°é¢–ç»„ä»¶ã€‚</li>
<li>é«˜æ–¯è´´å›¾æŠ€æœ¯åœ¨3Då»ºæ¨¡é¢†åŸŸçš„ä¼˜åŠ¿åœ¨äºå…¶é«˜æ•ˆæ€§å’Œå¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5bf2a63195aa0470042e9f469976617.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a4edfd3e37d721a4c8c4f37584cdfe5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83510c6c0d36d26c11fe3a11c1175e46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f69b867dd19d99f84033e3b42f143806.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="4DRGS-4D-Radiative-Gaussian-Splatting-for-Efficient-3D-Vessel-Reconstruction-from-Sparse-View-Dynamic-DSA-Images"><a href="#4DRGS-4D-Radiative-Gaussian-Splatting-for-Efficient-3D-Vessel-Reconstruction-from-Sparse-View-Dynamic-DSA-Images" class="headerlink" title="4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel   Reconstruction from Sparse-View Dynamic DSA Images"></a>4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel   Reconstruction from Sparse-View Dynamic DSA Images</h2><p><strong>Authors:Zhentao Liu, Ruyi Zha, Huangxuan Zhao, Hongdong Li, Zhiming Cui</strong></p>
<p>Reconstructing 3D vessel structures from sparse-view dynamic digital subtraction angiography (DSA) images enables accurate medical assessment while reducing radiation exposure. Existing methods often produce suboptimal results or require excessive computation time. In this work, we propose 4D radiative Gaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently. In detail, we represent the vessels with 4D radiative Gaussian kernels. Each kernel has time-invariant geometry parameters, including position, rotation, and scale, to model static vessel structures. The time-dependent central attenuation of each kernel is predicted from a compact neural network to capture the temporal varying response of contrast agent flow. We splat these Gaussian kernels to synthesize DSA images via X-ray rasterization and optimize the model with real captured ones. The final 3D vessel volume is voxelized from the well-trained kernels. Moreover, we introduce accumulated attenuation pruning and bounded scaling activation to improve reconstruction quality. Extensive experiments on real-world patient data demonstrate that 4DRGS achieves impressive results in 5 minutes training, which is 32x faster than the state-of-the-art method. This underscores the potential of 4DRGS for real-world clinics. </p>
<blockquote>
<p>ä»ç¨€ç–è§†è§’åŠ¨æ€æ•°å­—å‡æ³•è¡€ç®¡é€ å½±ï¼ˆDSAï¼‰å›¾åƒé‡å»º3Dè¡€ç®¡ç»“æ„ï¼Œå¯ä»¥å®ç°å‡†ç¡®çš„åŒ»å­¦è¯„ä¼°ï¼ŒåŒæ—¶å‡å°‘è¾å°„æš´éœ²ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€äº§ç”Ÿä¸ç†æƒ³çš„ç»“æœæˆ–éœ€è¦è¿‡å¤šçš„è®¡ç®—æ—¶é—´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡º4Dè¾å°„é«˜æ–¯å±•å¼€ï¼ˆ4DRGSï¼‰ä»¥å®ç°é«˜æ•ˆçš„é«˜è´¨é‡é‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨4Dè¾å°„é«˜æ–¯æ ¸è¡¨ç¤ºè¡€ç®¡ã€‚æ¯ä¸ªæ ¸éƒ½å…·æœ‰æ—¶é—´ä¸å˜çš„å‡ ä½•å‚æ•°ï¼ŒåŒ…æ‹¬ä½ç½®ã€æ—‹è½¬å’Œå°ºåº¦ï¼Œä»¥æ¨¡æ‹Ÿé™æ€è¡€ç®¡ç»“æ„ã€‚æ¯ä¸ªæ ¸çš„æ—¶é—´ç›¸å…³ä¸­å¿ƒè¡°å‡ç”±ç´§å‡‘ç¥ç»ç½‘ç»œé¢„æµ‹ï¼Œä»¥æ•æ‰é€ å½±å‰‚æµåŠ¨çš„æš‚æ—¶å˜åŒ–å“åº”ã€‚æˆ‘ä»¬é€šè¿‡Xå°„çº¿å…‰æ …åŒ–å±•å¼€è¿™äº›é«˜æ–¯æ ¸æ¥åˆæˆDSAå›¾åƒï¼Œå¹¶ç”¨çœŸå®æ•è·çš„å›¾åƒä¼˜åŒ–æ¨¡å‹ã€‚æœ€ç»ˆçš„3Dè¡€ç®¡ä½“ç§¯æ˜¯ç”±è®­ç»ƒè‰¯å¥½çš„æ ¸ä½“ç´ åŒ–å¾—åˆ°çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç´¯ç§¯è¡°å‡ä¿®å‰ªå’Œæœ‰ç•Œç¼©æ”¾æ¿€æ´»æ¥æé«˜é‡å»ºè´¨é‡ã€‚åœ¨çœŸå®æ‚£è€…æ•°æ®ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œ4DRGSåœ¨5åˆ†é’Ÿå†…è®­ç»ƒå³å¯å–å¾—ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œæ¯”ç°æœ‰æœ€ä½³æ–¹æ³•å¿«32å€ã€‚è¿™çªå‡ºäº†4DRGSåœ¨çœŸå®ä¸–ç•Œä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12919v1">PDF</a> Zhentao Liu and Ruyi Zha made equal contributions</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå››ç»´è¾å°„é«˜æ–¯å±•å¼€ï¼ˆ4DRGSï¼‰çš„æ–¹æ³•ï¼Œç”¨äºä»ç¨€ç–è§†è§’åŠ¨æ€æ•°å­—å‡å½±è¡€ç®¡é€ å½±ï¼ˆDSAï¼‰å›¾åƒé‡å»ºä¸‰ç»´è¡€ç®¡ç»“æ„ã€‚é€šè¿‡ç”¨å››ç»´è¾å°„é«˜æ–¯æ ¸è¡¨ç¤ºè¡€ç®¡ï¼Œç»“åˆç¥ç»ç½‘ç»œé¢„æµ‹å¯¹æ¯”å‰‚æµåŠ¨çš„æ—¶é—´ä¾èµ–ä¸­å¿ƒè¡°å‡ï¼Œé«˜æ•ˆåˆæˆDSAå›¾åƒï¼Œä»è€Œå®ç°äº†é«˜è´¨é‡çš„ä¸‰ç»´é‡å»ºã€‚æ–¹æ³•èƒ½å¤Ÿåœ¨äº”åˆ†é’Ÿå†…å®Œæˆè®­ç»ƒï¼Œé€Ÿåº¦è¿œè¶…ç°æœ‰æŠ€æœ¯ï¼Œå±•ç°äº†åœ¨å®é™…ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨å››ç»´è¾å°„é«˜æ–¯å±•å¼€ï¼ˆ4DRGSï¼‰æ–¹æ³•ä»ç¨€ç–è§†è§’çš„DSAå›¾åƒé‡å»ºä¸‰ç»´è¡€ç®¡ç»“æ„ã€‚</li>
<li>é€šè¿‡æ—¶é—´ä¸å˜çš„å‡ ä½•å‚æ•°ï¼ˆä½ç½®ã€æ—‹è½¬ã€å°ºåº¦ï¼‰å»ºæ¨¡é™æ€è¡€ç®¡ç»“æ„ã€‚</li>
<li>åˆ©ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹å¯¹æ¯”å‰‚æµåŠ¨çš„æ—¶é—´ä¾èµ–ä¸­å¿ƒè¡°å‡ï¼Œæ•æ‰åŠ¨æ€å“åº”ã€‚</li>
<li>åˆ©ç”¨Xå°„çº¿å…‰æ …åŒ–æŠ€æœ¯å°†é«˜æ–¯æ ¸å±•å¼€ä»¥åˆæˆDSAå›¾åƒã€‚</li>
<li>é€šè¿‡çœŸå®æ•è·çš„å›¾åƒä¼˜åŒ–æ¨¡å‹ï¼Œå°†è®­ç»ƒè‰¯å¥½çš„é«˜æ–¯æ ¸ä½“ç´ åŒ–ä¸ºæœ€ç»ˆçš„3Dè¡€ç®¡ä½“ç§¯ã€‚</li>
<li>å¼•å…¥ç´¯ç§¯è¡°å‡ä¿®å‰ªå’Œè¾¹ç•Œç¼©æ”¾æ¿€æ´»æŠ€æœ¯æé«˜é‡å»ºè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-712d180d89900a175ae13c826e870ce5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1abc270b6085d81fecac4232ca210d20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-287cd913a90898dcaea00b03bb46a6ba.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image"><a href="#CATSplat-Context-Aware-Transformer-with-Spatial-Guidance-for-Generalizable-3D-Gaussian-Splatting-from-A-Single-View-Image" class="headerlink" title="CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image"></a>CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image</h2><p><strong>Authors:Wonseok Roh, Hwanhee Jung, Jong Wook Kim, Seunggwan Lee, Innfarn Yoo, Andreas Lugmayr, Seunggeun Chi, Karthik Ramani, Sangpil Kim</strong></p>
<p>Recently, generalizable feed-forward methods based on 3D Gaussian Splatting have gained significant attention for their potential to reconstruct 3D scenes using finite resources. These approaches create a 3D radiance field, parameterized by per-pixel 3D Gaussian primitives, from just a few images in a single forward pass. However, unlike multi-view methods that benefit from cross-view correspondences, 3D scene reconstruction with a single-view image remains an underexplored area. In this work, we introduce CATSplat, a novel generalizable transformer-based framework designed to break through the inherent constraints in monocular settings. First, we propose leveraging textual guidance from a visual-language model to complement insufficient information from a single image. By incorporating scene-specific contextual details from text embeddings through cross-attention, we pave the way for context-aware 3D scene reconstruction beyond relying solely on visual cues. Moreover, we advocate utilizing spatial guidance from 3D point features toward comprehensive geometric understanding under single-view settings. With 3D priors, image features can capture rich structural insights for predicting 3D Gaussians without multi-view techniques. Extensive experiments on large-scale datasets demonstrate the state-of-the-art performance of CATSplat in single-view 3D scene reconstruction with high-quality novel view synthesis. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3D Gaussian Splattingï¼‰çš„é€šç”¨å‰é¦ˆæ–¹æ³•å› å…¶åˆ©ç”¨æœ‰é™èµ„æºé‡å»ºä¸‰ç»´åœºæ™¯çš„æ½œåŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚è¿™äº›æ–¹æ³•é€šè¿‡å•æ¬¡å‰å‘ä¼ é€’ä»…ä»å‡ å¼ å›¾åƒä¸­åˆ›å»ºä¸€ä¸ªä¸‰ç»´è¾å°„åœºï¼Œè¯¥åœºç”±åƒç´ çº§ä¸‰ç»´é«˜æ–¯åŸºå…ƒå‚æ•°åŒ–ã€‚ç„¶è€Œï¼Œä¸åŒäºå—ç›Šäºè·¨è§†å›¾å¯¹åº”å…³ç³»çš„å¤šè§†å›¾æ–¹æ³•ï¼Œå•è§†å›¾å›¾åƒçš„ä¸‰ç»´åœºæ™¯é‡å»ºä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†ç ”ç©¶çš„é¢†åŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†CATSplatï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¯æ³›åŒ–ã€åŸºäºTransformerçš„æ¡†æ¶ï¼Œæ—¨åœ¨çªç ´å•ç›®è§†è§‰è®¾ç½®ä¸­çš„å›ºæœ‰çº¦æŸã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬æŒ‡å¯¼æ¥è¡¥å……å•å¹…å›¾åƒä¸­çš„ä¿¡æ¯ä¸è¶³ã€‚é€šè¿‡è·¨æ³¨æ„åŠ›èå…¥ç‰¹å®šåœºæ™¯çš„ä¸Šä¸‹æ–‡ç»†èŠ‚å’Œæ–‡æœ¬åµŒå…¥ï¼Œæˆ‘ä»¬ä¸ºä»…ä¾èµ–è§†è§‰çº¿ç´¢çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸‰ç»´åœºæ™¯é‡å»ºé“ºå¹³äº†é“è·¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸»å¼ åˆ©ç”¨ä¸‰ç»´ç‚¹ç‰¹å¾çš„ç©ºé—´æŒ‡å¯¼ï¼Œä»¥å®ç°å•è§†å›¾è®¾ç½®ä¸‹çš„å…¨é¢å‡ ä½•ç†è§£ã€‚å€ŸåŠ©ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ï¼Œå›¾åƒç‰¹å¾å¯ä»¥æ•æ‰åˆ°ä¸°å¯Œçš„ç»“æ„æ´å¯ŸåŠ›ï¼Œæ— éœ€å¤šè§†å›¾æŠ€æœ¯å³å¯é¢„æµ‹ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒã€‚å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCATSplatåœ¨å•è§†å›¾ä¸‰ç»´åœºæ™¯é‡å»ºä¸­å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½è¿›è¡Œé«˜è´¨é‡çš„æ–°è§†è§’åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12906v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäº3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯çš„é€šç”¨å‰é¦ˆæ–¹æ³•ï¼Œåœ¨æœ‰é™çš„èµ„æºä¸‹é‡å»º3Dåœºæ™¯å·²å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ä½†å•è§†å›¾é‡å»ºä»æ˜¯ä¸€ä¸ªæœªå……åˆ†ç ”ç©¶çš„é¢†åŸŸã€‚æœ¬ç ”ç©¶æå‡ºåŸºäºæ–‡æœ¬å¼•å¯¼çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é‡å»ºæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹ä¸ç©ºé—´å¼•å¯¼3Dç‚¹ç‰¹å¾è¿›è¡Œå•è§†å›¾åœºæ™¯çš„é‡å»ºï¼Œæé«˜äº†å¯¹åœºæ™¯çš„ä¸°å¯Œå‡ ä½•ç†è§£ã€‚å®éªŒè¯æ˜ï¼ŒCATSplatåœ¨å•è§†å›¾é‡å»ºå’Œé«˜è´¨é‡æ–°è§†è§’åˆæˆæ–¹é¢å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºé€šç”¨å‰é¦ˆæ–¹æ³•çš„é‡å»ºæŠ€æœ¯ä½¿ç”¨æœ‰é™èµ„æºé€šè¿‡å•è§†å›¾å›¾åƒé‡å»º3Dåœºæ™¯ã€‚</li>
<li>ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æä¾›æ–‡æœ¬æŒ‡å¯¼ï¼Œä»¥å¼¥è¡¥å•è§†å›¾ä¿¡æ¯çš„ä¸è¶³ã€‚</li>
<li>é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥åœºæ™¯ç‰¹å®šçš„ä¸Šä¸‹æ–‡ç»†èŠ‚ï¼Œå®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é‡å»ºã€‚</li>
<li>ä½¿ç”¨ç©ºé—´æŒ‡å¯¼æŠ€æœ¯é€šè¿‡å•ç‚¹ç‰¹å¾çš„åæ ‡å’Œç©ºé—´ç‰¹å¾äº†è§£ç»¼åˆå‡ ä½•ã€‚ </li>
<li>ç»“åˆå…ˆè¿›çš„å®éªŒï¼Œå±•ç¤ºäº†åœ¨å•è§†å›¾é‡å»ºå’Œé«˜è´¨é‡æ–°è§†è§’åˆæˆæ–¹é¢çš„æœ€æ–°æ€§èƒ½ã€‚ </li>
<li>CATSplatæ¡†æ¶å…·æœ‰çªç ´å›ºæœ‰çº¦æŸçš„èƒ½åŠ›ï¼Œæœ‰åŠ©äºæ¨è¿›å•è§†å›¾åœºæ™¯é‡å»ºç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a36283c050fe8e7e8612d5d0f7b3ff3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f4a64437f03222250060d6f25a2e4e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84dce1e5ef85f6648fdb1164e1585b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e5cdf7b6fbf0b81ed13dd728405b419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933704d558b0afca6370cdf480d21167.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HyperGS-Hyperspectral-3D-Gaussian-Splatting"><a href="#HyperGS-Hyperspectral-3D-Gaussian-Splatting" class="headerlink" title="HyperGS: Hyperspectral 3D Gaussian Splatting"></a>HyperGS: Hyperspectral 3D Gaussian Splatting</h2><p><strong>Authors:Christopher Thirgood, Oscar Mendez, Erin Chao Ling, Jon Storey, Simon Hadfield</strong></p>
<p>We introduce HyperGS, a novel framework for Hyperspectral Novel View Synthesis (HNVS), based on a new latent 3D Gaussian Splatting (3DGS) technique. Our approach enables simultaneous spatial and spectral renderings by encoding material properties from multi-view 3D hyperspectral datasets. HyperGS reconstructs high-fidelity views from arbitrary perspectives with improved accuracy and speed, outperforming currently existing methods. To address the challenges of high-dimensional data, we perform view synthesis in a learned latent space, incorporating a pixel-wise adaptive density function and a pruning technique for increased training stability and efficiency. Additionally, we introduce the first HNVS benchmark, implementing a number of new baselines based on recent SOTA RGB-NVS techniques, alongside the small number of prior works on HNVS. We demonstrate HyperGSâ€™s robustness through extensive evaluation of real and simulated hyperspectral scenes with a 14db accuracy improvement upon previously published models. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†HyperGSï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ–°å‹æ½œåœ¨ä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰æŠ€æœ¯çš„é«˜å…‰è°±æ–°å‹è§†å›¾åˆæˆï¼ˆHNVSï¼‰çš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç¼–ç å¤šè§†è§’ä¸‰ç»´é«˜å…‰è°±æ•°æ®é›†çš„ææ–™å±æ€§ï¼Œå®ç°äº†ç©ºé—´å’Œå…‰è°±çš„åŒæ—¶æ¸²æŸ“ã€‚HyperGSèƒ½å¤Ÿä»ä»»æ„è§†è§’é‡å»ºé«˜ä¿çœŸè§†å›¾ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œé€Ÿåº¦ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ–¹æ³•ã€‚ä¸ºäº†è§£å†³é«˜ç»´æ•°æ®çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨å­¦ä¹ åˆ°çš„æ½œåœ¨ç©ºé—´è¿›è¡Œè§†å›¾åˆæˆï¼Œç»“åˆåƒç´ çº§è‡ªé€‚åº”å¯†åº¦å‡½æ•°å’Œä¿®å‰ªæŠ€æœ¯ï¼Œä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†é¦–ä¸ªHNVSåŸºå‡†æµ‹è¯•ï¼ŒåŸºäºæœ€æ–°çš„RGB-NVSæŠ€æœ¯å®ç°äº†ä¸€ç³»åˆ—æ–°åŸºçº¿ï¼Œä»¥åŠå°‘æ•°ç°æœ‰çš„HNVSå·¥ä½œã€‚æˆ‘ä»¬é€šè¿‡çœŸå®å’Œæ¨¡æ‹Ÿé«˜å…‰è°±åœºæ™¯çš„å¹¿æ³›è¯„ä¼°ï¼Œè¯æ˜äº†HyperGSçš„ç¨³å¥æ€§ï¼Œå…¶å‡†ç¡®æ€§ç›¸è¾ƒäºå·²å‘å¸ƒçš„æ¨¡å‹æé«˜äº†14dbã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12849v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>HyperGSæ˜¯ä¸€ä¸ªåŸºäºæ–°çš„æ½œåœ¨ä¸‰ç»´é«˜æ–¯æº…å°„æŠ€æœ¯ï¼ˆ3DGSï¼‰çš„ç”¨äºè¶…å…‰è°±æ–°é¢–è§†è§’åˆæˆï¼ˆHNVSï¼‰çš„æ–°æ¡†æ¶ã€‚å®ƒé‡‡ç”¨ç¼–ç å¤šè§†è§’ä¸‰ç»´è¶…å…‰è°±æ•°æ®é›†çš„ææ–™å±æ€§ï¼Œå®ç°ç©ºé—´å’Œå…‰è°±çš„åŒæ—¶æ¸²æŸ“ï¼Œèƒ½é‡å»ºå‡ºé«˜ä¿çœŸåº¦çš„ä»»æ„è§†è§’è§†å›¾ï¼Œå¹¶æé«˜äº†å‡†ç¡®æ€§ä¸é€Ÿåº¦ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HyperGSæ˜¯ä¸€ä¸ªåŸºäºæ½œåœ¨ä¸‰ç»´é«˜æ–¯æº…å°„æŠ€æœ¯ï¼ˆ3DGSï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºè¶…å…‰è°±æ–°é¢–è§†è§’åˆæˆï¼ˆHNVSï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½åŒæ—¶å®ç°ç©ºé—´å’Œå…‰è°±çš„æ¸²æŸ“ã€‚</li>
<li>é€šè¿‡ç¼–ç å¤šè§†è§’ä¸‰ç»´è¶…å…‰è°±æ•°æ®é›†çš„ææ–™å±æ€§ï¼ŒHyperGSèƒ½å¤Ÿé‡å»ºé«˜ä¿çœŸåº¦çš„ä»»æ„è§†è§’è§†å›¾ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒHyperGSæé«˜äº†å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚</li>
<li>ä¸ºäº†è§£å†³é«˜ç»´æ•°æ®çš„æŒ‘æˆ˜ï¼ŒHyperGSåœ¨å­¦ä¹ çš„æ½œåœ¨ç©ºé—´è¿›è¡Œè§†è§’åˆæˆã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨äº†åƒç´ çº§çš„è‡ªé€‚åº”å¯†åº¦å‡½æ•°å’Œä¿®å‰ªæŠ€æœ¯ï¼Œä»¥æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-663c0cead679f9ec7ca5f9cc3c981515.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05f7ad5a21470778768213f662d08838.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-19\./crop_3DGS/2412.12849v1/page_4_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4c286c9861daa3ae304e0d8a4421138.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PanSplat-4K-Panorama-Synthesis-with-Feed-Forward-Gaussian-Splatting"><a href="#PanSplat-4K-Panorama-Synthesis-with-Feed-Forward-Gaussian-Splatting" class="headerlink" title="PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting"></a>PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting</h2><p><strong>Authors:Cheng Zhang, Haofei Xu, Qianyi Wu, Camilo Cruz Gambardella, Dinh Phung, Jianfei Cai</strong></p>
<p>With the advent of portable 360{\deg} cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving. As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential. Nevertheless, existing methods are typically constrained to lower resolutions (512 $\times$ 1024) due to demanding memory and computational requirements. In this paper, we present PanSplat, a generalizable, feed-forward approach that efficiently supports resolution up to 4K (2048 $\times$ 4096). Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy. To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and Gaussian heads with local operations, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU. Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets. Code will be available at \url{<a target="_blank" rel="noopener" href="https://github.com/chengzhag/PanSplat%7D">https://github.com/chengzhag/PanSplat}</a>. </p>
<blockquote>
<p>éšç€ä¾¿æºå¼360Â°ç›¸æœºçš„å‡ºç°ï¼Œå…¨æ™¯æŠ€æœ¯åœ¨è™šæ‹Ÿç°å®ï¼ˆVRï¼‰ã€è™šæ‹Ÿæ¸¸è§ˆã€æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„åº”ç”¨ä¸­å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚å› æ­¤ï¼Œå®½åŸºçº¿å…¨æ™¯è§†å›¾åˆæˆæˆä¸ºäº†ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œå…¶ä¸­é«˜åˆ†è¾¨ç‡ã€å¿«é€Ÿæ¨ç†å’Œå†…å­˜æ•ˆç‡éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚ç„¶è€Œï¼Œç”±äºå†…å­˜å’Œè®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å±€é™äºè¾ƒä½åˆ†è¾¨ç‡ï¼ˆ512Ã—1024ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PanSplatï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„å‰é¦ˆæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°æ”¯æŒé«˜è¾¾4Kï¼ˆ2048Ã—4096ï¼‰çš„åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å®šåˆ¶çš„çƒå½¢3Dé«˜æ–¯é‡‘å­—å¡”ï¼Œå…·æœ‰æ–æ³¢é‚£å¥‘æ ¼å­æ’åˆ—ï¼Œæé«˜äº†å›¾åƒè´¨é‡ï¼ŒåŒæ—¶å‡å°‘äº†ä¿¡æ¯å†—ä½™ã€‚ä¸ºäº†æ»¡è¶³é«˜åˆ†è¾¨ç‡çš„éœ€æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆåˆ†å±‚çƒå½¢æˆæœ¬é‡å’Œé«˜æ–¯å¤´ä¸å½“åœ°æ“ä½œçš„æµç¨‹ï¼Œå®ç°ä¸¤æ­¥å»¶è¿Ÿåå‘ä¼ æ’­ï¼Œå¯åœ¨å•ä¸ªA100 GPUä¸Šè¿›è¡Œå†…å­˜é«˜æ•ˆè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒPanSplatåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„ç»“æœã€æ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/chengzhag/PanSplat%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/chengzhag/PanSplatä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12096v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://chengzhag.github.io/publication/pansplat/">https://chengzhag.github.io/publication/pansplat/</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/chengzhag/PanSplat">https://github.com/chengzhag/PanSplat</a></p>
<p><strong>Summary</strong></p>
<p>å…¨æ™¯æŠ€æœ¯éšç€ä¾¿æºå¼360Â°ç›¸æœºçš„å‡ºç°ï¼Œåœ¨è™šæ‹Ÿç°å®ã€è™šæ‹Ÿæ¸¸è§ˆã€æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ä¸ºæ»¡è¶³é«˜åˆ†è¾¨ç‡ã€å¿«é€Ÿæ¨ç†å’Œå†…å­˜æ•ˆç‡çš„è¦æ±‚ï¼Œå®½åŸºçº¿å…¨æ™¯è§†å›¾åˆæˆæˆä¸ºé‡è¦ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å—é™äºè¾ƒä½åˆ†è¾¨ç‡ï¼ˆå¦‚512Ã—1024ï¼‰ã€‚æœ¬æ–‡æå‡ºPanSplatï¼Œä¸€ç§é€šç”¨å‰é¦ˆæ–¹æ³•ï¼Œæ”¯æŒé«˜è¾¾4Kï¼ˆ2048Ã—4096ï¼‰çš„åˆ†è¾¨ç‡ã€‚é€šè¿‡å®šåˆ¶çƒå½¢3Dé«˜æ–¯é‡‘å­—å¡”å’Œæ–æ³¢é‚£å¥‘æ ¼å­å¸ƒå±€ï¼Œæé«˜å›¾åƒè´¨é‡å¹¶å‡å°‘ä¿¡æ¯å†—ä½™ã€‚ä¸ºæ»¡è¶³é«˜åˆ†è¾¨ç‡éœ€æ±‚ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé›†æˆåˆ†å±‚çƒå½¢æˆæœ¬é‡å’Œé«˜æ–¯å¤´éƒ¨çš„ç®¡é“ï¼Œè¿›è¡Œæœ¬åœ°æ“ä½œï¼Œå®ç°å•A100 GPUä¸Šçš„é«˜æ•ˆå†…å­˜è®­ç»ƒçš„ä¸¤æ­¥å»¶è¿Ÿåå‘ä¼ æ’­ã€‚å®éªŒè¡¨æ˜ï¼ŒPanSplatåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„ç»“æœã€é«˜æ•ˆèƒ½å’Œå›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨æ™¯æŠ€æœ¯åœ¨å¤šä¸ªé¢†åŸŸå—åˆ°å…³æ³¨ï¼Œå°¤å…¶æ˜¯éšç€ä¾¿æºå¼ç›¸æœºçš„æ™®åŠã€‚</li>
<li>å®½åŸºçº¿å…¨æ™¯è§†å›¾åˆæˆè¦æ±‚é«˜åˆ†è¾¨ç‡ã€å¿«é€Ÿæ¨ç†å’Œå†…å­˜æ•ˆç‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºè¾ƒä½åˆ†è¾¨ç‡ï¼Œè€ŒPanSplatæ”¯æŒé«˜è¾¾4Kçš„åˆ†è¾¨ç‡ã€‚</li>
<li>PanSplatä½¿ç”¨å®šåˆ¶çƒå½¢3Dé«˜æ–¯é‡‘å­—å¡”å’Œæ–æ³¢é‚£å¥‘æ ¼å­å¸ƒå±€æé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>é€šè¿‡é›†æˆåˆ†å±‚çƒå½¢æˆæœ¬é‡å’Œé«˜æ–¯å¤´éƒ¨çš„ç®¡é“ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>PanSplaté‡‡ç”¨ä¸¤æ­¥å»¶è¿Ÿåå‘ä¼ æ’­ä»¥é€‚åº”é«˜åˆ†è¾¨ç‡å¹¶ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bc0e75c8344a3cc54055818d53ce1911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1aa4a746180308b971ec3590aa406442.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-916520a64c6d44401465701fa14b9210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e714f7c828382511efbc4d9ccc111a76.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Wonderland-Navigating-3D-Scenes-from-a-Single-Image"><a href="#Wonderland-Navigating-3D-Scenes-from-a-Single-Image" class="headerlink" title="Wonderland: Navigating 3D Scenes from a Single Image"></a>Wonderland: Navigating 3D Scenes from a Single Image</h2><p><strong>Authors:Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, Jian Ren</strong></p>
<p>This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas. We propose a novel pipeline to overcome these limitations. Specifically, we introduce a large-scale reconstruction model that uses latents from a video diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images. For the first time, we demonstrate that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼šå¦‚ä½•ä»ä¸€å¼ ä»»æ„å›¾åƒé«˜æ•ˆåˆ›å»ºé«˜è´¨é‡ã€å¤§èŒƒå›´çš„ä¸‰ç»´åœºæ™¯ï¼Ÿç°æœ‰æ–¹æ³•é¢ä¸´å¤šé‡é™åˆ¶ï¼Œå¦‚éœ€è¦å¤šè§†è§’æ•°æ®ã€è€—æ—¶çš„åœºæ™¯ä¼˜åŒ–ã€èƒŒæ™¯è§†è§‰è´¨é‡ä½ä¸‹ä»¥åŠæœªè§åŒºåŸŸçš„å¤±çœŸé‡å»ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æµç¨‹æ¥å…‹æœè¿™äº›é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤§è§„æ¨¡é‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾æ¥ä»¥æ­£å‘æ–¹å¼é¢„æµ‹åœºæ™¯çš„ä¸‰ç»´é«˜æ–¯Splattingsã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹è¢«è®¾è®¡ä¸ºç²¾ç¡®åœ°éµå¾ªæŒ‡å®šçš„ç›¸æœºè½¨è¿¹æ¥åˆ›å»ºè§†é¢‘ï¼Œä»è€Œç”Ÿæˆå‹ç¼©çš„è§†é¢‘æ½œåœ¨ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾åŒ…å«å¤šè§†è§’ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒä¸‰ç»´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨æ¸è¿›çš„è®­ç»ƒç­–ç•¥æ¥è®­ç»ƒä¸‰ç»´é‡å»ºæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨è§†é¢‘æ½œåœ¨ç©ºé—´ä¸Šè¿è¡Œï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¤§èŒƒå›´ã€é€šç”¨çš„ä¸‰ç»´åœºæ™¯ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å•è§†å›¾ä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸŸå¤–å›¾åƒä¸Šã€‚æˆ‘ä»¬é¦–æ¬¡è¯æ˜ï¼Œå¯ä»¥åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸Šæ„å»ºä¸‰ç»´é‡å»ºæ¨¡å‹ï¼Œä»¥å®ç°é«˜æ•ˆçš„ä¸‰ç»´åœºæ™¯ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12091v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://snap-research.github.io/wonderland/">https://snap-research.github.io/wonderland/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§è§£å†³å¦‚ä½•ä»å•ä¸€ä»»æ„å›¾åƒé«˜æ•ˆåˆ›å»ºé«˜è´¨é‡ã€å¤§èŒƒå›´3Dåœºæ™¯çš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚éœ€è¦å¤šè§†è§’æ•°æ®ã€è€—æ—¶çš„åœºæ™¯ä¼˜åŒ–ã€èƒŒæ™¯è§†è§‰è´¨é‡ä½ä»¥åŠåœ¨æœªè§åŒºåŸŸçš„é‡å»ºå¤±çœŸç­‰ï¼Œè¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æµç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå¼•å…¥äº†ä¸€ç§å¤§è§„æ¨¡é‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾æ¥é¢„æµ‹åœºæ™¯çš„3Dé«˜æ–¯Splattingsï¼Œå¹¶ä»¥å‰é¦ˆæ–¹å¼è¿›è¡Œã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹è¢«è®¾è®¡ä¸ºéµå¾ªæŒ‡å®šçš„ç›¸æœºè½¨è¿¹åˆ›å»ºè§†é¢‘ï¼Œèƒ½å¤Ÿç”ŸæˆåŒ…å«å¤šè§†è§’ä¿¡æ¯çš„å‹ç¼©è§†é¢‘æ½œåœ¨ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒ3Dä¸€è‡´æ€§ã€‚é€šè¿‡æ¸è¿›çš„è®­ç»ƒç­–ç•¥ï¼Œå¯¹3Dé‡å»ºæ¨¡å‹åœ¨è§†é¢‘æ½œåœ¨ç©ºé—´è¿›è¡Œæ“ä½œï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€å¤§èŒƒå›´ã€é€šç”¨çš„3Dåœºæ™¯ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œå¯¹äºå•è§†å›¾3Dåœºæ™¯ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åŸŸå¤–å›¾åƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é¦–æ¬¡å®ç°äº†åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸Šå»ºç«‹æœ‰æ•ˆçš„3Dé‡å»ºæ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆçš„3Dåœºæ™¯ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡è§£å†³äº†ä»å•ä¸€å›¾åƒé«˜æ•ˆåˆ›å»ºé«˜è´¨é‡3Dåœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å¤šè§†è§’æ•°æ®éœ€æ±‚ã€è€—æ—¶ä¼˜åŒ–ã€èƒŒæ™¯è§†è§‰è´¨é‡ä½å’Œæœªè§åŒºåŸŸé‡å»ºå¤±çœŸç­‰é—®é¢˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æµç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾è¿›è¡Œ3Dé«˜æ–¯Splattingsé¢„æµ‹ã€‚</li>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½ç”ŸæˆåŒ…å«å¤šè§†è§’ä¿¡æ¯çš„å‹ç¼©è§†é¢‘æ½œåœ¨ç‰¹å¾ï¼Œå¹¶ç»´æŒ3Dä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨æ¸è¿›çš„è®­ç»ƒç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½åœ¨è§†é¢‘æ½œåœ¨ç©ºé—´æ“ä½œï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¤§èŒƒå›´ã€é€šç”¨çš„3Dåœºæ™¯ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å•è§†å›¾3Dåœºæ™¯ç”Ÿæˆä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¤„ç†åŸŸå¤–å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e1893df1c8744c96edd2008c57ee69ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55e4a8bc8fbda65980e59b9d0248daae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03355e70ff67b3e94a8a1e4b7aed823d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e626360ad87ca9fac2cc5613cb005a25.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GS-ProCams-Gaussian-Splatting-based-Projector-Camera-Systems"><a href="#GS-ProCams-Gaussian-Splatting-based-Projector-Camera-Systems" class="headerlink" title="GS-ProCams: Gaussian Splatting-based Projector-Camera Systems"></a>GS-ProCams: Gaussian Splatting-based Projector-Camera Systems</h2><p><strong>Authors:Qingyue Deng, Jijiang Li, Haibin Ling, Bingyao Huang</strong></p>
<p>We present GS-ProCams, the first Gaussian Splatting-based framework for projector-camera systems (ProCams). GS-ProCams significantly enhances the efficiency of projection mapping (PM) that requires establishing geometric and radiometric mappings between the projector and the camera. Previous CNN-based ProCams are constrained to a specific viewpoint, limiting their applicability to novel perspectives. In contrast, NeRF-based ProCams support view-agnostic projection mapping, however, they require an additional colocated light source and demand significant computational and memory resources. To address this issue, we propose GS-ProCams that employs 2D Gaussian for scene representations, and enables efficient view-agnostic ProCams applications. In particular, we explicitly model the complex geometric and photometric mappings of ProCams using projector responses, the target surfaceâ€™s geometry and materials represented by Gaussians, and global illumination component. Then, we employ differentiable physically-based rendering to jointly estimate them from captured multi-view projections. Compared to state-of-the-art NeRF-based methods, our GS-ProCams eliminates the need for additional devices, achieving superior ProCams simulation quality. It is also 600 times faster and uses only 1&#x2F;10 of the GPU memory. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†GS-ProCamsï¼Œè¿™æ˜¯åŸºäºé«˜æ–¯æ‹¼è´´æŠ€æœ¯çš„æŠ•å½±ä»ªæ‘„åƒå¤´ç³»ç»Ÿï¼ˆProCamsï¼‰çš„é¦–ä¸ªæ¡†æ¶ã€‚GS-ProCamså¤§å¤§æé«˜äº†æŠ•å½±æ˜ å°„ï¼ˆPMï¼‰çš„æ•ˆç‡ï¼Œè¯¥æ•ˆç‡éœ€è¦å»ºç«‹æŠ•å½±ä»ªå’Œæ‘„åƒå¤´ä¹‹é—´çš„å‡ ä½•å’Œè¾å°„åº¦é‡æ˜ å°„ã€‚ä»¥å‰çš„åŸºäºCNNçš„ProCamså—é™äºç‰¹å®šè§†è§’ï¼Œé™åˆ¶äº†å…¶åœ¨æ–°é¢–è§†è§’çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºNeRFçš„ProCamsæ”¯æŒç‹¬ç«‹äºè§†å›¾çš„æŠ•å½±æ˜ å°„ï¼Œç„¶è€Œï¼Œå®ƒä»¬éœ€è¦é¢å¤–çš„å…±ç½®å…‰æºï¼Œå¹¶éœ€è¦å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜èµ„æºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GS-ProCamsï¼Œå®ƒé‡‡ç”¨2Dé«˜æ–¯è¿›è¡Œåœºæ™¯è¡¨ç¤ºï¼Œå¹¶å®ç°äº†é«˜æ•ˆçš„ç‹¬ç«‹äºè§†å›¾çš„ProCamsåº”ç”¨ç¨‹åºã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ•å½±ä»ªå“åº”ã€ç”±é«˜æ–¯è¡¨ç¤ºçš„ç›®æ ‡è¡¨é¢çš„å‡ ä½•å½¢çŠ¶å’Œææ–™ä»¥åŠå…¨å±€ç…§æ˜ç»„ä»¶æ¥æ˜¾å¼å»ºæ¨¡ProCamsçš„å¤æ‚å‡ ä½•å’Œå…‰åº¦æ˜ å°„ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºç‰©ç†çš„å¯å¾®åˆ†æ¸²æŸ“æ¥è”åˆä¼°è®¡ä»æ•è·çš„å¤šè§†è§’æŠ•å½±ã€‚ä¸æœ€æ–°çš„åŸºäºNeRFçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„GS-ProCamsä¸éœ€è¦é¢å¤–çš„è®¾å¤‡ï¼Œå®ç°äº†å“è¶Šçš„ProCamsä»¿çœŸè´¨é‡ã€‚å…¶é€Ÿåº¦ä¹Ÿæ›´å¿«ï¼ˆé«˜è¾¾600å€ï¼‰ï¼Œå¹¶ä¸”åªä½¿ç”¨1&#x2F;10çš„GPUå†…å­˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11762v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GS-ProCamsæ˜¯é¦–ä¸ªåŸºäºé«˜æ–¯å±•å¼€æŠ€æœ¯çš„æŠ•å½±ä»ªç›¸æœºç³»ç»Ÿæ¡†æ¶ï¼Œå®ƒèƒ½æ˜¾è‘—æå‡æŠ•å½±æ˜ å°„çš„æ•ˆç‡ã€‚ä¸ä»…é€‚ç”¨äºç‰¹å®šè§†è§’çš„CNNæ¨¡å‹ç›¸æ¯”ï¼ŒGS-ProCamså®ç°äº†ä¸å—è§†è§’é™åˆ¶çš„æŠ•å½±æ˜ å°„ï¼Œå¹¶ä¸”ä¸éœ€è¦é¢å¤–çš„å…‰æºè®¾å¤‡ã€‚é€šè¿‡å»ºæ¨¡å¤æ‚å‡ ä½•å’Œå…‰åº¦æ˜ å°„å…³ç³»ï¼Œç»“åˆç‰©ç†æ¸²æŸ“æŠ€æœ¯ï¼ŒGS-ProCamså®ç°äº†é«˜è´¨é‡çš„æŠ•å½±æ¨¡æ‹Ÿï¼Œè®¡ç®—é€Ÿåº¦æå‡600å€ï¼ŒGPUå†…å­˜ä½¿ç”¨å‡å°‘ååˆ†ä¹‹ä¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GS-ProCamsæ˜¯é¦–ä¸ªåŸºäºé«˜æ–¯å±•å¼€æŠ€æœ¯çš„æŠ•å½±ä»ªç›¸æœºç³»ç»Ÿæ¡†æ¶ã€‚</li>
<li>GS-ProCamsæé«˜äº†æŠ•å½±æ˜ å°„çš„æ•ˆç‡ã€‚</li>
<li>ç›¸æ¯”äºCNNæ¨¡å‹ï¼ŒGS-ProCamsæ”¯æŒæ›´å¹¿æ³›çš„è§†è§’ã€‚</li>
<li>GS-ProCamsä¸éœ€è¦é¢å¤–çš„å…‰æºè®¾å¤‡ã€‚</li>
<li>GS-ProCamsé€šè¿‡å»ºæ¨¡å¤æ‚å‡ ä½•å’Œå…‰åº¦æ˜ å°„å…³ç³»å®ç°é«˜è´¨é‡çš„æŠ•å½±æ¨¡æ‹Ÿã€‚</li>
<li>GS-ProCamsåˆ©ç”¨ç‰©ç†æ¸²æŸ“æŠ€æœ¯æ¥æå‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-92ded3759ea49dc651403d3718bd5809.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-006456d917848cc493aebd747e46e79f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d0ca752ad7ad76c678521bfda2a66cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-923a998e40a38fa14091d1aa9a310ddf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2bac7cdf3f89ddb434a3d2b0c769f0e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Deformable-Radial-Kernel-Splatting"><a href="#Deformable-Radial-Kernel-Splatting" class="headerlink" title="Deformable Radial Kernel Splatting"></a>Deformable Radial Kernel Splatting</h2><p><strong>Authors:Yi-Hua Huang, Ming-Xian Lin, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, Xiaojuan Qi</strong></p>
<p>Recently, Gaussian splatting has emerged as a robust technique for representing 3D scenes, enabling real-time rasterization and high-fidelity rendering. However, Gaussiansâ€™ inherent radial symmetry and smoothness constraints limit their ability to represent complex shapes, often requiring thousands of primitives to approximate detailed geometry. We introduce Deformable Radial Kernel (DRK), which extends Gaussian splatting into a more general and flexible framework. Through learnable radial bases with adjustable angles and scales, DRK efficiently models diverse shape primitives while enabling precise control over edge sharpness and boundary curvature. iven DRKâ€™s planar nature, we further develop accurate ray-primitive intersection computation for depth sorting and introduce efficient kernel culling strategies for improved rasterization efficiency. Extensive experiments demonstrate that DRK outperforms existing methods in both representation efficiency and rendering quality, achieving state-of-the-art performance while dramatically reducing primitive count. </p>
<blockquote>
<p>è¿‘æœŸï¼Œé«˜æ–¯æ¶‚æ–‘æŠ€æœ¯å·²æˆä¸ºè¡¨ç¤º3Dåœºæ™¯çš„ä¸€ç§ç¨³å¥æŠ€æœ¯ï¼Œå¯å®ç°å®æ—¶å…‰çº¿è¿½è¸ªå’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚ç„¶è€Œï¼Œé«˜æ–¯å›ºæœ‰çš„å¾„å‘å¯¹ç§°æ€§å’Œå¹³æ»‘æ€§çº¦æŸé™åˆ¶äº†å…¶è¡¨ç¤ºå¤æ‚å½¢çŠ¶çš„èƒ½åŠ›ï¼Œé€šå¸¸éœ€è¦æ•°åƒä¸ªåŸºæœ¬å›¾å½¢æ¥è¿‘ä¼¼è¯¦ç»†çš„å‡ ä½•å½¢çŠ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†å¯å˜å½¢å¾„å‘æ ¸ï¼ˆDRKï¼‰ï¼Œå°†é«˜æ–¯æ¶‚æ–‘æ‰©å±•ä¸ºä¸€ä¸ªæ›´é€šç”¨å’Œçµæ´»çš„å¹³å°ã€‚é€šè¿‡å…·æœ‰å¯è°ƒæ•´è§’åº¦å’Œæ¯”ä¾‹çš„å¯å­¦ä¹ å¾„å‘åŸºï¼ŒDRKèƒ½å¤Ÿé«˜æ•ˆåœ°å»ºæ¨¡å„ç§å½¢çŠ¶åŸºæœ¬å›¾å½¢ï¼ŒåŒæ—¶èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶è¾¹ç¼˜é”åº¦å’Œè¾¹ç•Œæ›²ç‡ã€‚é‰´äºDRKçš„å¹³é¢ç‰¹æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ç²¾ç¡®çš„å°„çº¿ä¸åŸºæœ¬å›¾å½¢ç›¸äº¤è®¡ç®—ä»¥å®ç°æ·±åº¦æ’åºï¼Œå¹¶å¼•å…¥äº†é«˜æ•ˆçš„æ ¸å‰”é™¤ç­–ç•¥ä»¥æé«˜å…‰çº¿è¿½è¸ªæ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDRKåœ¨è¡¨ç¤ºæ•ˆç‡å’Œæ¸²æŸ“è´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†åŸºæœ¬å›¾å½¢çš„æ•°é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11752v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œé«˜æ–¯è´´å›¾æŠ€æœ¯å·²æˆä¸ºè¡¨ç¤ºä¸‰ç»´åœºæ™¯çš„ç¨³å¥æ–¹æ³•ï¼Œå¯å®ç°å®æ—¶æ¸²æŸ“å’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚ç„¶è€Œï¼Œé«˜æ–¯æœ¬èº«çš„å¾„å‘å¯¹ç§°æ€§å’Œå¹³æ»‘æ€§çº¦æŸé™åˆ¶äº†å…¶è¡¨ç¤ºå¤æ‚å½¢çŠ¶çš„èƒ½åŠ›ï¼Œé€šå¸¸éœ€è¦æ•°åƒä¸ªåŸºæœ¬å½¢çŠ¶æ¥è¿‘ä¼¼è¯¦ç»†çš„å‡ ä½•å½¢çŠ¶ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•çš„é«˜æ–¯è´´å›¾æŠ€æœ¯â€”â€”å¯å˜å½¢å¾„å‘æ ¸ï¼ˆDRKï¼‰ï¼Œå®ƒæ„å»ºä¸€ä¸ªæ›´é€šç”¨å’Œçµæ´»çš„å¹³å°ã€‚é€šè¿‡å…·æœ‰å¯è°ƒè§’åº¦å’Œæ¯”ä¾‹çš„å¯å­¦ä¹ å¾„å‘åŸºï¼ŒDRKèƒ½å¤Ÿé«˜æ•ˆåœ°æ¨¡æ‹Ÿå„ç§å½¢çŠ¶åŸºæœ¬å½¢çŠ¶ï¼ŒåŒæ—¶å®ç°å¯¹è¾¹ç¼˜é”åº¦å’Œè¾¹ç•Œæ›²ç‡çš„ç²¾ç¡®æ§åˆ¶ã€‚æ­¤å¤–ï¼Œé‰´äºDRKçš„å¹³é¢ç‰¹æ€§ï¼Œæœ¬ç ”ç©¶è¿˜å¼€å‘äº†ç²¾ç¡®çš„å°„çº¿ä¸åŸºæœ¬å½¢çŠ¶çš„äº¤ç‚¹è®¡ç®—æ·±åº¦æ’åºæ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†é«˜æ•ˆçš„æ ¸å‰”é™¤ç­–ç•¥ä»¥æé«˜æ¸²æŸ“æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDRKåœ¨è¡¨ç¤ºæ•ˆç‡å’Œæ¸²æŸ“è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†åŸºæœ¬å½¢çŠ¶çš„æ•°é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ–¯è´´å›¾æŠ€æœ¯å·²ç”¨äºè¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œå…·æœ‰å®æ—¶æ¸²æŸ“å’Œé«˜ä¿çœŸæ¸²æŸ“çš„èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»Ÿé«˜æ–¯æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥è¡¨ç¤ºå¤æ‚å½¢çŠ¶ï¼Œéœ€è¦å¤§é‡åŸºæœ¬å½¢çŠ¶æ¥è¿‘ä¼¼ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†å¯å˜å½¢å¾„å‘æ ¸ï¼ˆDRKï¼‰æŠ€æœ¯ï¼Œæ‰©å±•äº†é«˜æ–¯è´´å›¾çš„é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>DRKé€šè¿‡å¯å­¦ä¹ å¾„å‘åŸºï¼Œèƒ½é«˜æ•ˆæ¨¡æ‹Ÿå¤šç§å½¢çŠ¶ï¼Œå¹¶ç²¾ç¡®æ§åˆ¶è¾¹ç¼˜é”åº¦å’Œè¾¹ç•Œæ›²ç‡ã€‚</li>
<li>DRKå…·æœ‰å¹³é¢ç‰¹æ€§ï¼Œå¼€å‘äº†ç²¾ç¡®çš„å°„çº¿ä¸åŸºæœ¬å½¢çŠ¶äº¤ç‚¹è®¡ç®—æ·±åº¦æ’åºæ–¹æ³•ã€‚</li>
<li>ç ”ç©¶è¿˜å¼•å…¥äº†é«˜æ•ˆçš„æ ¸å‰”é™¤ç­–ç•¥ï¼Œä»¥æé«˜DRKçš„æ¸²æŸ“æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c9dbe41aaa2005216f646d7506b3bb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a564d463a9da737e7511cc8a0806d7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf4d3fa632dc1065f622485ef8d361e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b86a045ff44b5273d64d0ecc267c04cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb06ceac26e5174dc14ba07ab507ae49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b15de55d9472f2f5e1ac25efcd7544.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c1f51fe0439dd283bd20f39c35a8f0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8056cc8a4295fcefcf510482cb3f9f20.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GaussianProperty-Integrating-Physical-Properties-to-3D-Gaussians-with-LMMs"><a href="#GaussianProperty-Integrating-Physical-Properties-to-3D-Gaussians-with-LMMs" class="headerlink" title="GaussianProperty: Integrating Physical Properties to 3D Gaussians with   LMMs"></a>GaussianProperty: Integrating Physical Properties to 3D Gaussians with   LMMs</h2><p><strong>Authors:Xinli Xu, Wenhang Ge, Dicong Qiu, ZhiFei Chen, Dongyu Yan, Zhuoyun Liu, Haoyu Zhao, Hanfeng Zhao, Shunsi Zhang, Junwei Liang, Ying-Cong Chen</strong></p>
<p>Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. To address these challenges, we introduce GaussianProperty, a training-free framework that assigns physical properties of materials to 3D Gaussians. Specifically, we integrate the segmentation capability of SAM with the recognition capability of GPT-4V(ision) to formulate a global-local physical property reasoning module for 2D images. Then we project the physical properties from multi-view 2D images to 3D Gaussians using a voting strategy. We demonstrate that 3D Gaussians with physical property annotations enable applications in physics-based dynamic simulation and robotic grasping. For physics-based dynamic simulation, we leverage the Material Point Method (MPM) for realistic dynamic simulation. For robot grasping, we develop a grasping force prediction strategy that estimates a safe force range required for object grasping based on the estimated physical properties. Extensive experiments on material segmentation, physics-based dynamic simulation, and robotic grasping validate the effectiveness of our proposed method, highlighting its crucial role in understanding physical properties from visual data. Online demo, code, more cases and annotated datasets are available on \href{<a target="_blank" rel="noopener" href="https://gaussian-property.github.io}{this/">https://Gaussian-Property.github.io}{this</a> https URL}. </p>
<blockquote>
<p>ä¼°ç®—è§†è§‰æ•°æ®çš„ç‰©ç†å±æ€§åœ¨è®¡ç®—æœºè§†è§‰ã€å›¾å½¢å’Œæœºå™¨äººæŠ€æœ¯ä¸­æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œå®ƒä¸ºå¢å¼ºç°å®ã€ç‰©ç†æ¨¡æ‹Ÿå’Œæœºå™¨äººæŠ“å–ç­‰åº”ç”¨æä¾›äº†æ”¯æŒã€‚ç„¶è€Œï¼Œç”±äºç‰©ç†å±æ€§ä¼°ç®—å›ºæœ‰çš„æ¨¡ç³Šæ€§ï¼Œè¿™ä¸ªé¢†åŸŸä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GaussianPropertyè¿™ä¸€æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ææ–™çš„ç‰©ç†å±æ€§åˆ†é…ç»™3Dé«˜æ–¯åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†SAMçš„åˆ†å‰²èƒ½åŠ›ä¸GPT-4Vï¼ˆè§†è§‰ï¼‰çš„è¯†åˆ«èƒ½åŠ›ç›¸ç»“åˆï¼Œå½¢æˆäº†é’ˆå¯¹2Då›¾åƒçš„å…¨å±€-å±€éƒ¨ç‰©ç†å±æ€§æ¨ç†æ¨¡å—ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ•ç¥¨ç­–ç•¥å°†å¤šè§†å›¾2Då›¾åƒçš„ç‰©ç†å±æ€§æŠ•å½±åˆ°3Dé«˜æ–¯åˆ†å¸ƒä¸Šã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¸¦æœ‰ç‰©ç†å±æ€§æ³¨é‡Šçš„3Dé«˜æ–¯åˆ†å¸ƒå¯ç”¨äºåŸºäºç‰©ç†çš„åŠ¨æ€æ¨¡æ‹Ÿå’Œæœºå™¨äººæŠ“å–åº”ç”¨ã€‚åœ¨åŸºäºç‰©ç†çš„åŠ¨æ€æ¨¡æ‹Ÿæ–¹é¢ï¼Œæˆ‘ä»¬åˆ©ç”¨ç‰©è´¨ç‚¹æ³•ï¼ˆMPMï¼‰è¿›è¡Œé€¼çœŸçš„åŠ¨æ€æ¨¡æ‹Ÿã€‚åœ¨æœºå™¨äººæŠ“å–æ–¹é¢ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æŠ“å–åŠ›é¢„æµ‹ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®ä¼°è®¡çš„ç‰©ç†å±æ€§æ¥ä¼°è®¡æŠ“å–ç‰©ä½“æ‰€éœ€çš„å®‰å…¨åŠ›èŒƒå›´ã€‚åœ¨ææ–™åˆ†å‰²ã€åŸºäºç‰©ç†çš„åŠ¨æ€æ¨¡æ‹Ÿå’Œæœºå™¨äººæŠ“å–æ–¹é¢çš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå‡¸æ˜¾äº†å…¶åœ¨ç†è§£è§†è§‰æ•°æ®ç‰©ç†å±æ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚åœ¨çº¿æ¼”ç¤ºã€ä»£ç ã€æ›´å¤šæ¡ˆä¾‹å’Œæ³¨é‡Šæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://gaussian-property.github.io/">this https URL</a>ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11258v1">PDF</a> 17 pages, 17 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡†æ¶â€”â€”GaussianPropertyï¼Œè¯¥æ¡†æ¶å¯ä¸º3Dé«˜æ–¯åˆ†é…ææ–™ç‰©ç†å±æ€§ã€‚é€šè¿‡ç»“åˆSAMçš„åˆ†å‰²èƒ½åŠ›å’ŒGPT-4V(ision)çš„è¯†åˆ«èƒ½åŠ›ï¼Œå½¢æˆç”¨äº2Då›¾åƒçš„å…¨å±€-å±€éƒ¨ç‰©ç†å±æ€§æ¨ç†æ¨¡å—ã€‚é‡‡ç”¨æŠ•ç¥¨ç­–ç•¥å°†å¤šè§†è§’2Då›¾åƒçš„ç‰©ç†å±æ€§æŠ•å½±åˆ°3Dé«˜æ–¯ä¸Šã€‚è¯¥æ¡†æ¶åœ¨åŸºäºç‰©ç†çš„åŠ¨æ€æ¨¡æ‹Ÿå’Œæœºå™¨äººæŠ“å–ç­‰æ–¹é¢æœ‰å¹¿æ³›åº”ç”¨ã€‚å¯¹äºåŸºäºç‰©ç†çš„åŠ¨æ€æ¨¡æ‹Ÿï¼Œåˆ©ç”¨ç‰©è´¨ç‚¹æ³•ï¼ˆMPMï¼‰è¿›è¡Œé€¼çœŸçš„åŠ¨æ€æ¨¡æ‹Ÿï¼›å¯¹äºæœºå™¨äººæŠ“å–ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºä¼°è®¡ç‰©ç†å±æ€§çš„å®‰å…¨åŠ›èŒƒå›´é¢„æµ‹ç­–ç•¥ã€‚å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ææ–™åˆ†å‰²ã€åŸºäºç‰©ç†çš„åŠ¨æ€æ¨¡æ‹Ÿå’Œæœºå™¨äººæŠ“å–ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaussianPropertyæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œç”¨äºä¸º3Dé«˜æ–¯åˆ†é…ææ–™ç‰©ç†å±æ€§ï¼Œå¦‚å¼¹æ€§ã€ç¡¬åº¦ç­‰ã€‚</li>
<li>ç»“åˆSAMçš„åˆ†å‰²èƒ½åŠ›å’ŒGPT-4V(ision)çš„è¯†åˆ«èƒ½åŠ›ï¼Œå½¢æˆ2Då›¾åƒçš„ç‰©ç†å±æ€§æ¨ç†æ¨¡å—ã€‚</li>
<li>é‡‡ç”¨æŠ•ç¥¨ç­–ç•¥å°†2Då›¾åƒçš„ç‰©ç†å±æ€§æŠ•å½±åˆ°3Dç©ºé—´çš„é«˜æ–¯åˆ†å¸ƒä¸Šã€‚</li>
<li>è¯¥æ¡†æ¶å¯åº”ç”¨äºåŸºäºç‰©ç†çš„åŠ¨æ€æ¨¡æ‹Ÿå’Œæœºå™¨äººæŠ“å–ã€‚</li>
<li>åœ¨åŠ¨æ€æ¨¡æ‹Ÿæ–¹é¢ï¼Œåˆ©ç”¨ç‰©è´¨ç‚¹æ³•ï¼ˆMPMï¼‰è¿›è¡Œé€¼çœŸçš„æ¨¡æ‹Ÿã€‚</li>
<li>åœ¨æœºå™¨äººæŠ“å–åº”ç”¨ä¸­ï¼Œæå‡ºäº†åŸºäºä¼°è®¡ç‰©ç†å±æ€§çš„å®‰å…¨åŠ›èŒƒå›´é¢„æµ‹ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22936a64f062c9032a287e1f257661bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4222aeefb3df0cf0fcede35f27c3093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8921789d7b90bc8001219887c46f6cb7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SplineGS-Robust-Motion-Adaptive-Spline-for-Real-Time-Dynamic-3D-Gaussians-from-Monocular-Video"><a href="#SplineGS-Robust-Motion-Adaptive-Spline-for-Real-Time-Dynamic-3D-Gaussians-from-Monocular-Video" class="headerlink" title="SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D   Gaussians from Monocular Video"></a>SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D   Gaussians from Monocular Video</h2><p><strong>Authors:Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim</strong></p>
<p>Synthesizing novel views from in-the-wild monocular videos is challenging due to scene dynamics and the lack of multi-view cues. To address this, we propose SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for high-quality reconstruction and fast rendering from monocular videos. At its core is a novel Motion-Adaptive Spline (MAS) method, which represents continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a small number of control points. For MAS, we introduce a Motion-Adaptive Control points Pruning (MACP) method to model the deformation of each dynamic 3D Gaussian across varying motions, progressively pruning control points while maintaining dynamic modeling integrity. Additionally, we present a joint optimization strategy for camera parameter estimation and 3D Gaussian attributes, leveraging photometric and geometric consistency. This eliminates the need for Structure-from-Motion preprocessing and enhances SplineGSâ€™s robustness in real-world conditions. Experiments show that SplineGS significantly outperforms state-of-the-art methods in novel view synthesis quality for dynamic scenes from monocular videos, achieving thousands times faster rendering speed. </p>
<blockquote>
<p>ä»é‡ç”Ÿå•ç›®è§†é¢‘ä¸­åˆæˆæ–°å‹è§†è§’æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºåœºæ™¯åŠ¨æ€å˜åŒ–å’Œç¼ºä¹å¤šè§†è§’çº¿ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SplineGSï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€COLMAPçš„åŠ¨æ€ä¸‰ç»´é«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰æ¡†æ¶ï¼Œç”¨äºä»å•ç›®è§†é¢‘ä¸­å®ç°é«˜è´¨é‡é‡å»ºå’Œå¿«é€Ÿæ¸²æŸ“ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹è¿åŠ¨è‡ªé€‚åº”æ ·æ¡ï¼ˆMASï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å°‘é‡æ§åˆ¶ç‚¹ï¼Œé€šè¿‡ä¸‰æ¬¡Hermiteæ ·æ¡è¡¨ç¤ºè¿ç»­åŠ¨æ€ä¸‰ç»´é«˜æ–¯è½¨è¿¹ã€‚å¯¹äºMASï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¿åŠ¨è‡ªé€‚åº”æ§åˆ¶ç‚¹ä¿®å‰ªï¼ˆMACPï¼‰æ–¹æ³•ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒè¿åŠ¨ä¸‹æ¯ä¸ªåŠ¨æ€ä¸‰ç»´é«˜æ–¯çš„å˜åŒ–ï¼Œåœ¨ä¿æŒåŠ¨æ€å»ºæ¨¡å®Œæ•´æ€§çš„åŒæ—¶é€æ­¥ä¿®å‰ªæ§åˆ¶ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç”¨äºç›¸æœºå‚æ•°ä¼°è®¡å’Œä¸‰ç»´é«˜æ–¯å±æ€§è”åˆä¼˜åŒ–çš„ç­–ç•¥ï¼Œåˆ©ç”¨å…‰åº¦å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚è¿™æ¶ˆé™¤äº†å¯¹ä»è¿åŠ¨ç»“æ„è¿›è¡Œé¢„å¤„ç†çš„éœ€è¦ï¼Œæé«˜äº†SplineGSåœ¨ç°å®æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œå¯¹äºä»å•ç›®è§†é¢‘ä¸­å¾—åˆ°çš„åŠ¨æ€åœºæ™¯çš„æ–°å‹è§†è§’åˆæˆè´¨é‡ï¼ŒSplineGSæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”å®ç°äº†æ•°åƒå€çš„å¿«é€Ÿæ¸²æŸ“é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09982v2">PDF</a> The first two authors contributed equally to this work (equal   contribution). The last two authors advised equally to this work. Please   visit our project page at this <a target="_blank" rel="noopener" href="https://kaist-viclab.github.io/splinegs-site/">https://kaist-viclab.github.io/splinegs-site/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSplineGSçš„æ— éœ€COLMAPçš„åŠ¨æ€3Dé«˜æ–¯æç”»ï¼ˆ3DGSï¼‰æ¡†æ¶ï¼Œç”¨äºä»å•ç›®è§†é¢‘ä¸­å®ç°é«˜è´¨é‡é‡å»ºå’Œå¿«é€Ÿæ¸²æŸ“ã€‚å…¶æ ¸å¿ƒæ˜¯æ–°å‹è¿åŠ¨è‡ªé€‚åº”æ ·æ¡ï¼ˆMASï¼‰æ–¹æ³•ï¼Œä½¿ç”¨å°‘é‡æ§åˆ¶ç‚¹ï¼Œé€šè¿‡ä¸‰æ¬¡Hermiteæ ·æ¡è¡¨ç¤ºè¿ç»­çš„åŠ¨æ€3Dé«˜æ–¯è½¨è¿¹ã€‚ä¸ºMASï¼Œå¼•å…¥è¿åŠ¨è‡ªé€‚åº”æ§åˆ¶ç‚¹ä¿®å‰ªï¼ˆMACPï¼‰æ–¹æ³•ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒè¿åŠ¨ä¸‹æ¯ä¸ªåŠ¨æ€3Dé«˜æ–¯çš„å˜åŒ–ï¼ŒåŒæ—¶ä¿æŒåŠ¨æ€å»ºæ¨¡çš„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨å…‰åº¦å’Œå‡ ä½•ä¸€è‡´æ€§ï¼Œæå‡ºç›¸æœºå‚æ•°ä¼°è®¡å’Œ3Dé«˜æ–¯å±æ€§è”åˆä¼˜åŒ–ç­–ç•¥ï¼Œæ— éœ€ä»è¿åŠ¨ä¸­é‡å»ºé¢„å¤„ç†ï¼Œæé«˜äº†SplineGSåœ¨ç°å®æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSplineGSåœ¨å•ç›®è§†é¢‘åŠ¨æ€åœºæ™¯çš„æ–°å‹è§†å›¾åˆæˆè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ¸²æŸ“é€Ÿåº¦æé«˜æ•°åƒå€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SplineGSæ˜¯ä¸€ä¸ªåŸºäºåŠ¨æ€3Dé«˜æ–¯æç”»ï¼ˆ3DGSï¼‰çš„æ¡†æ¶ï¼Œç”¨äºä»å•ç›®è§†é¢‘ä¸­å®ç°é«˜è´¨é‡é‡å»ºå’Œå¿«é€Ÿæ¸²æŸ“ã€‚</li>
<li>æ¡†æ¶æ ¸å¿ƒä¸ºè¿åŠ¨è‡ªé€‚åº”æ ·æ¡ï¼ˆMASï¼‰æ–¹æ³•ï¼Œä½¿ç”¨Hermiteæ ·æ¡è¡¨ç¤ºåŠ¨æ€3Dé«˜æ–¯è½¨è¿¹ã€‚</li>
<li>å¼•å…¥è¿åŠ¨è‡ªé€‚åº”æ§åˆ¶ç‚¹ä¿®å‰ªï¼ˆMACPï¼‰æ–¹æ³•ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒè¿åŠ¨ä¸‹æ¯ä¸ªåŠ¨æ€3Dé«˜æ–¯çš„å˜åŒ–ã€‚</li>
<li>æå‡ºè”åˆä¼˜åŒ–ç­–ç•¥ï¼Œç”¨äºç›¸æœºå‚æ•°ä¼°è®¡å’Œ3Dé«˜æ–¯å±æ€§ä¼°è®¡ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ç»“æ„ä»è¿åŠ¨ï¼ˆStructure-from-Motionï¼‰é¢„å¤„ç†ï¼Œå¢å¼ºäº†åœ¨ç°å®æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒSplineGSåœ¨å•ç›®è§†é¢‘åŠ¨æ€åœºæ™¯çš„æ–°å‹è§†å›¾åˆæˆè´¨é‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e1ad5ac5834fd209915b64c4a2016ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c561833d872c68ba77982ef610c8bea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9585e17969aadc659e2ff57ef3b7adf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-with-Anisotropic-Gaussian-Splatting-for-Image-Inpainting"><a href="#Diffusion-Models-with-Anisotropic-Gaussian-Splatting-for-Image-Inpainting" class="headerlink" title="Diffusion Models with Anisotropic Gaussian Splatting for Image   Inpainting"></a>Diffusion Models with Anisotropic Gaussian Splatting for Image   Inpainting</h2><p><strong>Authors:Jacob Fein-Ashley, Benjamin Fein-Ashley</strong></p>
<p>Image inpainting is a fundamental task in computer vision, aiming to restore missing or corrupted regions in images realistically. While recent deep learning approaches have significantly advanced the state-of-the-art, challenges remain in maintaining structural continuity and generating coherent textures, particularly in large missing areas. Diffusion models have shown promise in generating high-fidelity images but often lack the structural guidance necessary for realistic inpainting. We propose a novel inpainting method that combines diffusion models with anisotropic Gaussian splatting to capture both local structures and global context effectively. By modeling missing regions using anisotropic Gaussian functions that adapt to local image gradients, our approach provides structural guidance to the diffusion-based inpainting network. The Gaussian splat maps are integrated into the diffusion process, enhancing the modelâ€™s ability to generate high-fidelity and structurally coherent inpainting results. Extensive experiments demonstrate that our method outperforms state-of-the-art techniques, producing visually plausible results with enhanced structural integrity and texture realism. </p>
<blockquote>
<p>å›¾åƒä¿®å¤æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œæ—¨åœ¨ä»¥é€¼çœŸçš„æ–¹å¼æ¢å¤å›¾åƒä¸­ç¼ºå¤±æˆ–æŸåçš„åŒºåŸŸã€‚è™½ç„¶æœ€è¿‘çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å·²ç»æ˜¾è‘—æé«˜äº†æœ€æ–°æ°´å¹³ï¼Œä½†åœ¨ä¿æŒç»“æ„è¿ç»­æ€§å’Œç”Ÿæˆè¿è´¯çº¹ç†æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§é¢ç§¯ç¼ºå¤±çš„æƒ…å†µä¸‹ã€‚æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¾€å¾€ç¼ºä¹ç”¨äºç°å®ä¿®å¤æ‰€éœ€çš„ç»“æ„æŒ‡å¯¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å’Œå®šå‘é«˜æ–¯æ¶‚å¸ƒæŠ€æœ¯çš„æ–°å›¾åƒä¿®å¤æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°æ•æ‰å±€éƒ¨ç»“æ„å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬ä½¿ç”¨é€‚åº”å±€éƒ¨å›¾åƒæ¢¯åº¦çš„å®šå‘é«˜æ–¯å‡½æ•°å¯¹ç¼ºå¤±åŒºåŸŸè¿›è¡Œå»ºæ¨¡ï¼Œä¸ºåŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤ç½‘ç»œæä¾›ç»“æ„æŒ‡å¯¼ã€‚é«˜æ–¯å±•å¹³å›¾é›†æˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå¢å¼ºäº†æ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸå’Œç»“æ„ä¸Šè¿è´¯çš„ä¿®å¤ç»“æœçš„èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œèƒ½å¤Ÿäº§ç”Ÿè§†è§‰ä¸Šçš„å¯ä¿¡ç»“æœï¼Œæé«˜ç»“æ„å®Œæ•´æ€§å’Œçº¹ç†é€¼çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01682v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡ä¸­æå‡ºä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹ä¸å¼‚å‘é«˜æ–¯æ··åˆæŠ€æœ¯çš„å›¾åƒä¿®å¤æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ•æ‰å±€éƒ¨ç»“æ„å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€šè¿‡é€‚åº”å›¾åƒå±€éƒ¨æ¢¯åº¦çš„å¼‚å‘é«˜æ–¯å‡½æ•°å¯¹ç¼ºå¤±åŒºåŸŸè¿›è¡Œå»ºæ¨¡ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›ç»“æ„æŒ‡å¯¼ã€‚æ•´åˆé«˜æ–¯æ··åˆå›¾åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå¢å¼ºäº†æ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸå’Œç»“æ„ä¸Šè¿è´¯çš„å›¾åƒä¿®å¤ç»“æœçš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œèƒ½ç”Ÿæˆè§†è§‰æ•ˆæœå¥½ã€ç»“æ„å®Œæ•´ã€çº¹ç†é€¼çœŸçš„å›¾åƒä¿®å¤ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å’Œå¼‚å‘é«˜æ–¯æ··åˆæŠ€æœ¯çš„å›¾åƒä¿®å¤æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é€‚åº”å±€éƒ¨å›¾åƒæ¢¯åº¦çš„å¼‚å‘é«˜æ–¯å‡½æ•°å»ºæ¨¡ç¼ºå¤±åŒºåŸŸï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›ç»“æ„æŒ‡å¯¼ã€‚</li>
<li>é«˜æ–¯æ··åˆå›¾è¢«æ•´åˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå¢å¼ºäº†ç”Ÿæˆé«˜ä¿çœŸå›¾åƒçš„èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿæ•æ‰å±€éƒ¨ç»“æ„å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå®ç°æ›´çœŸå®çš„çº¹ç†ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤§å‹ç¼ºå¤±åŒºåŸŸä¸­ä¹Ÿèƒ½ä¿æŒç»“æ„è¿ç»­æ€§å’Œçº¹ç†è¿è´¯æ€§ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f568974f88c58ba5c4a5194a7715d1f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-657cf6ed4c728e8c892997668c0324c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-663a7692a38ca5d9433ca5b224d7a43d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25e038425244a8e66ca261d337af83b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5545a14eb3015cff25b5417d60ddc510.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d7378e10dd7791c4d2053d7131ad2b4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AGS-Mesh-Adaptive-Gaussian-Splatting-and-Meshing-with-Geometric-Priors-for-Indoor-Room-Reconstruction-Using-Smartphones"><a href="#AGS-Mesh-Adaptive-Gaussian-Splatting-and-Meshing-with-Geometric-Priors-for-Indoor-Room-Reconstruction-Using-Smartphones" class="headerlink" title="AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors   for Indoor Room Reconstruction Using Smartphones"></a>AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors   for Indoor Room Reconstruction Using Smartphones</h2><p><strong>Authors:Xuqian Ren, Matias Turkulainen, Jiepeng Wang, Otto Seiskari, Iaroslav Melekhov, Juho Kannala, Esa Rahtu</strong></p>
<p>Geometric priors are often used to enhance 3D reconstruction. With many smartphones featuring low-resolution depth sensors and the prevalence of off-the-shelf monocular geometry estimators, incorporating geometric priors as regularization signals has become common in 3D vision tasks. However, the accuracy of depth estimates from mobile devices is typically poor for highly detailed geometry, and monocular estimators often suffer from poor multi-view consistency and precision. In this work, we propose an approach for joint surface depth and normal refinement of Gaussian Splatting methods for accurate 3D reconstruction of indoor scenes. We develop supervision strategies that adaptively filters low-quality depth and normal estimates by comparing the consistency of the priors during optimization. We mitigate regularization in regions where prior estimates have high uncertainty or ambiguities. Our filtering strategy and optimization design demonstrate significant improvements in both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian Splatting-based methods on challenging indoor room datasets. Furthermore, we explore the use of alternative meshing strategies for finer geometry extraction. We develop a scale-aware meshing strategy inspired by TSDF and octree-based isosurface extraction, which recovers finer details from Gaussian models compared to other commonly used open-source meshing tools. Our code is released in <a target="_blank" rel="noopener" href="https://xuqianren.github.io/ags_mesh_website/">https://xuqianren.github.io/ags_mesh_website/</a>. </p>
<blockquote>
<p>å‡ ä½•å…ˆéªŒé€šå¸¸ç”¨äºå¢å¼º3Dé‡å»ºã€‚ç”±äºè®¸å¤šæ™ºèƒ½æ‰‹æœºé…å¤‡äº†ä½åˆ†è¾¨ç‡çš„æ·±åº¦ä¼ æ„Ÿå™¨ï¼Œä»¥åŠç°æˆçš„å•ç›®å‡ ä½•ä¼°è®¡å™¨çš„æ™®åŠï¼Œå°†å‡ ä½•å…ˆéªŒä½œä¸ºæ­£åˆ™åŒ–ä¿¡å·å·²æˆä¸º3Dè§†è§‰ä»»åŠ¡ä¸­çš„å¸¸æ€ã€‚ç„¶è€Œï¼Œå¯¹äºé«˜åº¦è¯¦ç»†çš„å‡ ä½•ç»“æ„ï¼Œæ¥è‡ªç§»åŠ¨è®¾å¤‡çš„æ·±åº¦ä¼°è®¡ç²¾åº¦é€šå¸¸è¾ƒå·®ï¼Œå•ç›®ä¼°è®¡å™¨é€šå¸¸å­˜åœ¨å¤šè§†è§’ä¸€è‡´æ€§å’Œç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆè¡¨é¢æ·±åº¦å’Œæ³•çº¿ç²¾åŒ–çš„é«˜æ–¯å¹³é“ºæ–¹æ³•ï¼Œç”¨äºå®¤å†…åœºæ™¯çš„å‡†ç¡®3Dé‡å»ºã€‚æˆ‘ä»¬å¼€å‘äº†ç›‘ç£ç­–ç•¥ï¼Œé€šè¿‡æ¯”è¾ƒä¼˜åŒ–è¿‡ç¨‹ä¸­å…ˆéªŒçš„ä¸€è‡´æ€§ï¼Œè‡ªé€‚åº”åœ°è¿‡æ»¤ä½è´¨é‡çš„æ·±åº¦å’Œæ³•çº¿ä¼°è®¡ã€‚æˆ‘ä»¬åœ¨å…ˆéªŒä¼°è®¡å­˜åœ¨é«˜ä¸ç¡®å®šæ€§æˆ–æ¨¡ç³Šæ€§çš„åŒºåŸŸå‡è½»æ­£åˆ™åŒ–ã€‚æˆ‘ä»¬çš„è¿‡æ»¤ç­–ç•¥å’Œä¼˜åŒ–è®¾è®¡åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®¤å†…æˆ¿é—´æ•°æ®é›†ä¸Šï¼Œæ— è®ºæ˜¯åŸºäº3Dè¿˜æ˜¯åŸºäº2Dçš„é«˜æ–¯å¹³é“ºæ–¹æ³•ï¼Œåœ¨ç½‘æ ¼ä¼°è®¡å’Œæ–°é¢–è§†å›¾åˆæˆæ–¹é¢éƒ½æ˜¾ç¤ºå‡ºæ˜¾ç€æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç”¨äºæ›´ç²¾ç»†å‡ ä½•æå–çš„æ›¿ä»£ç½‘æ ¼ç­–ç•¥ã€‚æˆ‘ä»¬å—åˆ°æˆªæ–­æœ‰å‘è·ç¦»å‡½æ•°ï¼ˆTSDFï¼‰å’ŒåŸºäºå…«å‰æ ‘çš„ç­‰å€¼é¢æå–å¯å‘çš„è§„æ¨¡æ„ŸçŸ¥ç½‘æ ¼ç­–ç•¥ï¼Œèƒ½å¤Ÿä»é«˜æ–¯æ¨¡å‹ä¸­æ¢å¤æ›´ç²¾ç»†çš„ç»†èŠ‚ï¼Œä¸å…¶ä»–å¸¸ç”¨çš„å¼€æºç½‘æ ¼å·¥å…·ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://xuqianren.github.io/ags_mesh_website/%E3%80%82">https://xuqianren.github.io/ags_mesh_website&#x2F;ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19271v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå‡ ä½•å…ˆéªŒçš„è”åˆè¡¨é¢æ·±åº¦ä¸æ³•çº¿ç²¾ç»†åŒ–çš„æ–¹æ³•ï¼Œç”¨äºå®¤å†…åœºæ™¯çš„3Dé‡å»ºã€‚é€šè¿‡ä¼˜åŒ–ç­–ç•¥ï¼Œè‡ªé€‚åº”è¿‡æ»¤ä½è´¨é‡çš„æ·±åº¦å’Œæ³•çº¿ä¼°è®¡ï¼Œæé«˜ç½‘æ ¼ä¼°è®¡å’Œæ–°é¢–è§†è§’åˆæˆçš„æ•ˆæœã€‚åŒæ—¶ï¼Œé‡‡ç”¨å°ºåº¦æ„ŸçŸ¥çš„ç½‘æ ¼ç­–ç•¥ï¼Œä»é«˜æ–¯æ¨¡å‹ä¸­æ¢å¤æ›´ç²¾ç»†çš„ç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡ ä½•å…ˆéªŒåœ¨3Dé‡å»ºä¸­å¹¿æ³›åº”ç”¨ï¼Œç”¨äºå¢å¼ºæ™ºèƒ½æ‰‹æœºä½åˆ†è¾¨ç‡æ·±åº¦ä¼ æ„Ÿå™¨çš„æ€§èƒ½ä»¥åŠä½¿ç”¨ç°æˆçš„å•çœ¼å‡ ä½•ä¼°è®¡å™¨ã€‚</li>
<li>å¯¹é«˜åº¦è¯¦ç»†çš„å‡ ä½•ç»“æ„ï¼Œç§»åŠ¨è®¾å¤‡çš„æ·±åº¦ä¼°è®¡å‡†ç¡®æ€§é€šå¸¸è¾ƒå·®ï¼Œå•çœ¼ä¼°è®¡å™¨åœ¨å¤šè§†è§’ä¸€è‡´æ€§å’Œç²¾åº¦æ–¹é¢å¸¸å‡ºç°é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§è”åˆè¡¨é¢æ·±åº¦ä¸æ³•çº¿ç²¾ç»†åŒ–çš„æ–¹æ³•ï¼Œç”¨äºå®¤å†…åœºæ™¯çš„3Dé‡å»ºï¼Œé‡‡ç”¨é«˜æ–¯Splattingæ–¹æ³•ã€‚</li>
<li>å¼€å‘ç›‘ç£ç­–ç•¥ï¼Œè‡ªé€‚åº”è¿‡æ»¤ä½è´¨é‡çš„æ·±åº¦å’Œæ³•çº¿ä¼°è®¡ï¼Œé€šè¿‡ä¼˜åŒ–è¿‡ç¨‹ä¸­æ¯”è¾ƒå…ˆéªŒçš„ä¸€è‡´æ€§æ¥å®ç°ã€‚</li>
<li>åœ¨ä¸ç¡®å®šæ€§æˆ–æ¨¡ç³Šåº¦é«˜çš„åŒºåŸŸå‡è½»æ­£åˆ™åŒ–ã€‚</li>
<li>è¿‡æ»¤ç­–ç•¥å’Œä¼˜åŒ–è®¾è®¡æ˜¾è‘—æé«˜äº†ç½‘æ ¼ä¼°è®¡å’Œæ–°é¢–è§†è§’åˆæˆçš„æ•ˆæœï¼Œé€‚ç”¨äºåŸºäº3Då’Œ2Dé«˜æ–¯Splattingçš„æ–¹æ³•ï¼Œå¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å®¤å†…æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc7736b21be34c032be03b17d8bd7451.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6355c0bd18dd56f4ee43cb224548d912.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f737dc91fd1c28291e4a509413000f0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SplatR-Experience-Goal-Visual-Rearrangement-with-3D-Gaussian-Splatting-and-Dense-Feature-Matching"><a href="#SplatR-Experience-Goal-Visual-Rearrangement-with-3D-Gaussian-Splatting-and-Dense-Feature-Matching" class="headerlink" title="SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting   and Dense Feature Matching"></a>SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting   and Dense Feature Matching</h2><p><strong>Authors:Arjun P S, Andrew Melnik, Gora Chand Nandi</strong></p>
<p>Experience Goal Visual Rearrangement task stands as a foundational challenge within Embodied AI, requiring an agent to construct a robust world model that accurately captures the goal state. The agent uses this world model to restore a shuffled scene to its original configuration, making an accurate representation of the world essential for successfully completing the task. In this work, we present a novel framework that leverages on 3D Gaussian Splatting as a 3D scene representation for experience goal visual rearrangement task. Recent advances in volumetric scene representation like 3D Gaussian Splatting, offer fast rendering of high quality and photo-realistic novel views. Our approach enables the agent to have consistent views of the current and the goal setting of the rearrangement task, which enables the agent to directly compare the goal state and the shuffled state of the world in image space. To compare these views, we propose to use a dense feature matching method with visual features extracted from a foundation model, leveraging its advantages of a more universal feature representation, which facilitates robustness, and generalization. We validate our approach on the AI2-THOR rearrangement challenge benchmark and demonstrate improvements over the current state of the art methods </p>
<blockquote>
<p>ä½“éªŒç›®æ ‡è§†è§‰é‡æ„ä»»åŠ¡ä½œä¸ºåµŒå…¥å¼äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªåŸºç¡€æŒ‘æˆ˜ï¼Œè¦æ±‚æ™ºèƒ½ä½“æ„å»ºä¸€ä¸ªç¨³å¥çš„ä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®æ•æ‰ç›®æ ‡çŠ¶æ€ã€‚æ™ºèƒ½ä½“ä½¿ç”¨è¿™ä¸ªä¸–ç•Œæ¨¡å‹æ¥æ¢å¤éšæœºæ‰“ä¹±çš„åœºæ™¯åˆ°å…¶åŸå§‹é…ç½®ï¼Œå› æ­¤å‡†ç¡®çš„ä¸–ç•Œè¡¨ç¤ºå¯¹äºæˆåŠŸå®Œæˆä»»åŠ¡è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ‹¼è´´ä½œä¸ºä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ¥è§£å†³ä½“éªŒç›®æ ‡è§†è§‰é‡æ„ä»»åŠ¡ã€‚åƒä¸‰ç»´é«˜æ–¯æ‹¼è´´è¿™æ ·çš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„æœ€è¿‘è¿›å±•ä¸ºé«˜è´¨é‡å’Œé€¼çœŸçš„æ–°è§†è§’æä¾›äº†å¿«é€Ÿæ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ‹¥æœ‰å…³äºé‡æ„ä»»åŠ¡çš„å½“å‰è®¾å®šå’Œç›®æ ‡è®¾å®šçš„ç»Ÿä¸€è§†è§’ï¼Œè¿™ä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å›¾åƒç©ºé—´ä¸­ç›´æ¥æ¯”è¾ƒç›®æ ‡çŠ¶æ€å’Œä¸–ç•Œçš„éšæœºçŠ¶æ€ã€‚ä¸ºäº†æ¯”è¾ƒè¿™äº›è§†å›¾ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ä¸€ç§å¯†é›†çš„ç‰¹å¾åŒ¹é…æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä»åŸºç¡€æ¨¡å‹ä¸­æå–çš„è§†è§‰ç‰¹å¾ï¼Œåˆ©ç”¨å…¶æ›´é€šç”¨çš„ç‰¹å¾è¡¨ç¤ºçš„ä¼˜åŠ¿ï¼Œæœ‰åŠ©äºå¢å¼ºç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨AI2-THORé‡æ„æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜äº†ä¸å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”æœ‰æ‰€æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14322v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨Embodied AIé¢†åŸŸä¸­çš„ç»éªŒç›®æ ‡è§†è§‰é‡æ„ä»»åŠ¡ä¸­ï¼Œæ„å»ºä¸€ä¸ªå‡†ç¡®æ•æ‰ç›®æ ‡çŠ¶æ€çš„ä¸–ç•Œæ¨¡å‹æ˜¯è‡³å…³é‡è¦çš„ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨3Dé«˜æ–¯æ¶‚æ–‘æŠ€æœ¯ä½œä¸ºè¯¥ä»»åŠ¡çš„3Dåœºæ™¯è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ä»£ç†èƒ½å¤Ÿå…·æœ‰å¯¹å½“å‰å’Œé‡æ„ä»»åŠ¡ç›®æ ‡è®¾ç½®çš„ä¸€è‡´è§†å›¾ï¼Œä»è€Œåœ¨å›¾åƒç©ºé—´ä¸­ç›´æ¥æ¯”è¾ƒç›®æ ‡çŠ¶æ€å’Œæ··ä¹±çŠ¶æ€çš„ä¸–ç•Œã€‚é€šè¿‡å¯†é›†ç‰¹å¾åŒ¹é…æ–¹æ³•å’ŒåŸºç¡€æ¨¡å‹æå–çš„è§†è§‰ç‰¹å¾ï¼ŒéªŒè¯äº†æˆ‘ä»¬åœ¨AI2-THORé‡æ„æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸Šçš„æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»éªŒç›®æ ‡è§†è§‰é‡æ„ä»»åŠ¡æ˜¯Embodied AIé¢†åŸŸçš„åŸºç¡€æŒ‘æˆ˜ï¼Œéœ€è¦æ„å»ºå‡†ç¡®æ•æ‰ç›®æ ‡çŠ¶æ€çš„ä¸–ç•Œæ¨¡å‹ã€‚</li>
<li>é¦–æ¬¡æå‡ºä½¿ç”¨3Dé«˜æ–¯æ¶‚æ–‘æŠ€æœ¯ä½œä¸ºè§†è§‰é‡æ„ä»»åŠ¡çš„3Dåœºæ™¯è¡¨ç¤ºçš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>3Dé«˜æ–¯æ¶‚æ–‘æŠ€æœ¯ç­‰ä½“ç§¯åœºæ™¯è¡¨ç¤ºæ–¹æ³•èƒ½æä¾›é«˜è´¨é‡ã€é€¼çœŸçš„æ–°å‹è§†å›¾å¿«é€Ÿæ¸²æŸ“ã€‚</li>
<li>é€šè¿‡ä¸€è‡´çš„è§‚ç‚¹ï¼Œä»£ç†èƒ½å¤Ÿç›´æ¥æ¯”è¾ƒç›®æ ‡çŠ¶æ€å’Œæ··ä¹±çŠ¶æ€çš„ä¸–ç•Œï¼Œæœ‰åŠ©äºæ›´å¥½åœ°å®Œæˆé‡æ„ä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨å¯†é›†ç‰¹å¾åŒ¹é…æ–¹æ³•ä¸åŸºç¡€æ¨¡å‹æå–çš„è§†è§‰ç‰¹å¾è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>æ–¹æ³•å…·æœ‰é€šç”¨ç‰¹å¾è¡¨ç¤ºçš„ä¼˜è¶Šæ€§ï¼Œå¢å¼ºäº†é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3243364f575602f37558f6c58c70704b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2de4710062b0b143e0631ec9ab66d708.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1><h2 id="Query3D-LLM-Powered-Open-Vocabulary-Scene-Segmentation-with-Language-Embedded-3D-Gaussian"><a href="#Query3D-LLM-Powered-Open-Vocabulary-Scene-Segmentation-with-Language-Embedded-3D-Gaussian" class="headerlink" title="Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language   Embedded 3D Gaussian"></a>Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language   Embedded 3D Gaussian</h2><p><strong>Authors:Amirhosein Chahe, Lifeng Zhou</strong></p>
<p>This paper introduces a novel method for open-vocabulary 3D scene querying in autonomous driving by combining Language Embedded 3D Gaussians with Large Language Models (LLMs). We propose utilizing LLMs to generate both contextually canonical phrases and helping positive words for enhanced segmentation and scene interpretation. Our method leverages GPT-3.5 Turbo as an expert model to create a high-quality text dataset, which we then use to fine-tune smaller, more efficient LLMs for on-device deployment. Our comprehensive evaluation on the WayveScenes101 dataset demonstrates that LLM-guided segmentation significantly outperforms traditional approaches based on predefined canonical phrases. Notably, our fine-tuned smaller models achieve performance comparable to larger expert models while maintaining faster inference times. Through ablation studies, we discover that the effectiveness of helping positive words correlates with model scale, with larger models better equipped to leverage additional semantic information. This work represents a significant advancement towards more efficient, context-aware autonomous driving systems, effectively bridging 3D scene representation with high-level semantic querying while maintaining practical deployment considerations. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆè¯­è¨€åµŒå…¥çš„3Dé«˜æ–¯åˆ†å¸ƒä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¼€æ”¾è¯æ±‡çš„è‡ªåŠ¨é©¾é©¶ä¸­3Dåœºæ™¯æŸ¥è¯¢çš„æ–°å‹æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨LLMç”Ÿæˆä¸Šä¸‹æ–‡è§„èŒƒçŸ­è¯­å’Œå¸®åŠ©ç§¯æè¯æ±‡ï¼Œä»¥æé«˜åˆ†å‰²å’Œåœºæ™¯è§£é‡Šèƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨GPT-3.5 Turboä½œä¸ºä¸“å®¶æ¨¡å‹æ¥åˆ›å»ºé«˜è´¨é‡çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åæˆ‘ä»¬ä½¿ç”¨è¿™äº›æ•°æ®é›†å¯¹æ›´å°ã€æ›´é«˜æ•ˆçš„LLMè¿›è¡Œå¾®è°ƒï¼Œä»¥ç”¨äºè®¾å¤‡éƒ¨ç½²ã€‚æˆ‘ä»¬åœ¨WayveScenes101æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒLLMå¼•å¯¼çš„åˆ†å‰²æ˜¾è‘—ä¼˜äºåŸºäºé¢„å®šä¹‰è§„èŒƒçŸ­è¯­çš„ä¼ ç»Ÿæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ç»è¿‡å¾®è°ƒçš„å°å‹æ¨¡å‹åœ¨ä¿æŒæ›´å¿«çš„æ¨ç†æ—¶é—´çš„åŒæ—¶ï¼Œå®ç°äº†ä¸å¤§å‹ä¸“å®¶æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°å¸®åŠ©ç§¯æè¯æ±‡çš„æœ‰æ•ˆæ€§ä¸æ¨¡å‹è§„æ¨¡æœ‰å…³ï¼Œæ›´å¤§çš„æ¨¡å‹èƒ½æ›´å¥½åœ°åˆ©ç”¨é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯ã€‚è¿™é¡¹å·¥ä½œä»£è¡¨äº†æœç€æ›´é«˜æ•ˆã€æ›´é¢å‘ä¸Šä¸‹æ–‡çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„é‡è¦è¿›å±•ï¼Œæœ‰æ•ˆåœ°å°†3Dåœºæ™¯è¡¨ç¤ºä¸é«˜çº§è¯­ä¹‰æŸ¥è¯¢è”ç³»èµ·æ¥ï¼ŒåŒæ—¶ä¿æŒå®é™…çš„éƒ¨ç½²è€ƒè™‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.03516v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆè¯­è¨€åµŒå…¥ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒå’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€æ”¾è¯æ±‡ä¸‰ç»´åœºæ™¯æŸ¥è¯¢æ–°æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ã€‚æ–‡ç« æå‡ºåˆ©ç”¨LLMç”Ÿæˆä¸Šä¸‹æ–‡è§„èŒƒçŸ­è¯­å’Œå¸®åŠ©ç§¯æè¯æ±‡ï¼Œä»¥æé«˜åœºæ™¯åˆ†å‰²å’Œè§£è¯»èƒ½åŠ›ã€‚æ–‡ç« ä½¿ç”¨GPT-3.5 Turboä½œä¸ºä¸“å®¶æ¨¡å‹åˆ›å»ºé«˜è´¨é‡æ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åå¾®è°ƒæ›´å°ã€æ›´é«˜æ•ˆçš„LLMä»¥è¿›è¡Œè®¾å¤‡éƒ¨ç½²ã€‚åœ¨WayveScenes101æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒLLMå¼•å¯¼çš„åˆ†å‰²æ˜¾è‘—ä¼˜äºåŸºäºé¢„å®šä¹‰è§„èŒƒçŸ­è¯­çš„ä¼ ç»Ÿæ–¹æ³•ã€‚æ­¤å¤–ï¼Œç»è¿‡å¾®è°ƒçš„å°å‹æ¨¡å‹åœ¨ä¿æŒè¾ƒå¿«æ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œå®ç°äº†ä¸å¤§å‹ä¸“å®¶æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œå‘ç°å¸®åŠ©ç§¯æè¯æ±‡çš„æœ‰æ•ˆæ€§ä¸æ¨¡å‹è§„æ¨¡ç›¸å…³ï¼Œè¾ƒå¤§çš„æ¨¡å‹æ›´èƒ½åˆ©ç”¨é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ç»“åˆè¯­è¨€åµŒå…¥ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒå’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€æ”¾è¯æ±‡ä¸‰ç»´åœºæ™¯æŸ¥è¯¢æ–°æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨LLMç”Ÿæˆä¸Šä¸‹æ–‡è§„èŒƒçŸ­è¯­å’Œå¸®åŠ©ç§¯æè¯æ±‡ä»¥æé«˜åœºæ™¯åˆ†å‰²å’Œè§£è¯»èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨GPT-3.5 Turboä½œä¸ºä¸“å®¶æ¨¡å‹åˆ›å»ºé«˜è´¨é‡æ–‡æœ¬æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¾®è°ƒæ›´å°ã€æ›´é«˜æ•ˆçš„LLMæ¨¡å‹ï¼Œå®ç°äº†åœ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚</li>
<li>åœ¨WayveScenes101æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLLMå¼•å¯¼çš„åˆ†å‰²æ–¹æ³•æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¡¨æ˜å¸®åŠ©ç§¯æè¯æ±‡çš„æœ‰æ•ˆæ€§å–å†³äºæ¨¡å‹è§„æ¨¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.03516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-091aeb26da65e8015f34342b082ed447.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ce5c34a59e89ea4f483a3d245d930b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4d2560ccbb236818acfa38f6e9a86c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edf966949fab9e207ff07e16e321d60d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cf5b19a95f492f6759c31e134b7b015.jpg" align="middle">
</details>


<h1 id="-20"><a href="#-20" class="headerlink" title=""></a></h1><h2 id="Human-3Diffusion-Realistic-Avatar-Creation-via-Explicit-3D-Consistent-Diffusion-Models"><a href="#Human-3Diffusion-Realistic-Avatar-Creation-via-Explicit-3D-Consistent-Diffusion-Models" class="headerlink" title="Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent   Diffusion Models"></a>Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent   Diffusion Models</h2><p><strong>Authors:Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll</strong></p>
<p>Creating realistic avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency. We propose Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion. Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other, and by coupling them in a tight manner, we can fully leverage the potential of both models. We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the priors from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process to have better 3D consistency. Experiments show that our proposed framework outperforms state-of-the-art methods and enables the creation of realistic avatars from a single RGB image, achieving high-fidelity in both geometry and appearance. Extensive ablations also validate the efficacy of our design, (1) multi-view 2D priors conditioning in generative 3D reconstruction and (2) consistency refinement of sampling trajectory via the explicit 3D representation. Our code and models will be released on <a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion">https://yuxuan-xue.com/human-3diffusion</a>. </p>
<blockquote>
<p>åˆ›å»ºåŸºäºå•ä¸€RGBå›¾åƒçš„çœŸå®åŒ–èº«æ˜¯ä¸€ä¸ªæœ‰å¸å¼•åŠ›ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç”±äºå…¶è¡¨è¿°ä¸æ˜ç¡®çš„ç‰¹ç‚¹ï¼Œè¿‘æœŸçš„å·¥ä½œåˆ©ç”¨ä»å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„å¼ºå¤§çš„äºŒç»´æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ã€‚è™½ç„¶äºŒç»´æ‰©æ•£æ¨¡å‹è¡¨ç°å‡ºå¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä»¬æ— æ³•æä¾›å…·æœ‰ä¿è¯çš„ä¸‰ç»´ä¸€è‡´æ€§å¤šè§†å›¾å½¢çŠ¶å…ˆéªŒã€‚æˆ‘ä»¬æå‡ºäº†â€œHuman 3Diffusionï¼šé€šè¿‡æ˜¾å¼ä¸‰ç»´ä¸€è‡´æ€§æ‰©æ•£åˆ›å»ºçœŸå®åŒ–èº«â€ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼ŒäºŒç»´å¤šè§†å›¾æ‰©æ•£å’Œä¸‰ç»´é‡å»ºæ¨¡å‹å½¼æ­¤æä¾›äº’è¡¥ä¿¡æ¯ï¼Œé€šè¿‡ç´§å¯†è€¦åˆå®ƒä»¬ï¼Œæˆ‘ä»¬å¯ä»¥å……åˆ†åˆ©ç”¨è¿™ä¸¤ä¸ªæ¨¡å‹çš„æ½œåŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å›¾åƒæ¡ä»¶ç”Ÿæˆä¸‰ç»´é«˜æ–¯ä½“ç´ é‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨äºŒç»´å¤šè§†å›¾æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶æä¾›äº†ä¸€ç§æ˜¾å¼ä¸‰ç»´è¡¨ç¤ºï¼Œè¿›ä¸€æ­¥æŒ‡å¯¼äºŒç»´åå‘é‡‡æ ·è¿‡ç¨‹ï¼Œä»¥å®ç°æ›´å¥½çš„ä¸‰ç»´ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ¡†æ¶ä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•ä¸€RGBå›¾åƒåˆ›å»ºé€¼çœŸçš„åŒ–èº«ï¼Œåœ¨å‡ ä½•å’Œå¤–è§‚ä¸Šéƒ½å®ç°é«˜ä¿çœŸã€‚å¹¿æ³›çš„æ¶ˆèå®éªŒä¹ŸéªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œï¼ˆ1ï¼‰ç”Ÿæˆä¸‰ç»´é‡å»ºä¸­çš„å¤šè§†å›¾äºŒç»´å…ˆéªŒæ¡ä»¶ï¼›ï¼ˆ2ï¼‰é€šè¿‡æ˜¾å¼ä¸‰ç»´è¡¨ç¤ºå¯¹é‡‡æ ·è½¨è¿¹çš„ä¸€è‡´æ€§æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†åœ¨[<a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion]%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://yuxuan-xue.com/human-3diffusion]ä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08475v2">PDF</a> Accepted to NeurIPS2024. Project Page:   <a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion">https://yuxuan-xue.com/human-3diffusion</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ˜ç¡®çš„3Dä¸€è‡´æ€§æ‰©æ•£æ¨¡å‹çš„çœŸå®å¤´åƒåˆ›å»ºæ–¹æ³•ï¼Œé€šè¿‡å°†äºŒç»´å¤šè§†è§’æ‰©æ•£æ¨¡å‹ä¸ä¸‰ç»´é‡å»ºæ¨¡å‹ç›¸ç»“åˆï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„å›¾åƒæ¡ä»¶ç”Ÿæˆä¸‰ç»´é«˜æ–¯splaté‡å»ºæ¨¡å‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äºŒç»´å¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯ï¼Œæä¾›æ˜ç¡®çš„ä¸‰ç»´è¡¨ç¤ºï¼Œè¿›ä¸€æ­¥æŒ‡å¯¼äºŒç»´é€†å‘é‡‡æ ·è¿‡ç¨‹ï¼Œå®ç°æ›´å¥½çš„ä¸‰ç»´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä½•å’Œå¤–è§‚ä¸Šéƒ½å®ç°äº†é«˜ä¿çœŸåº¦ï¼Œå¹¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ›å»ºçœŸå®å¤´åƒæ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œéœ€è¦è§£å†³äºŒç»´æ‰©æ•£æ¨¡å‹çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒæ¡ä»¶ç”Ÿæˆä¸‰ç»´é«˜æ–¯splaté‡å»ºæ¨¡å‹ï¼Œç»“åˆäºŒç»´å¤šè§†è§’æ‰©æ•£æ¨¡å‹å’Œä¸‰ç»´é‡å»ºæ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
<li>åˆ©ç”¨äºŒç»´å¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯ï¼Œæä¾›æ˜ç¡®çš„ä¸‰ç»´è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡æŒ‡å¯¼äºŒç»´é€†å‘é‡‡æ ·è¿‡ç¨‹ï¼Œå®ç°äº†æ›´å¥½çš„ä¸‰ç»´ä¸€è‡´æ€§ã€‚</li>
<li>å®ç°äº†è¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåˆ›å»ºé€¼çœŸçš„å¤´åƒã€‚</li>
<li>æœ‰æ•ˆçš„è®¾è®¡åŒ…æ‹¬å¤šè§†è§’äºŒç»´å…ˆéªŒæ¡ä»¶åœ¨ç”Ÿæˆä¸‰ç»´é‡å»ºä¸­çš„åº”ç”¨å’Œä¸€è‡´æ€§ç²¾ç‚¼é‡‡æ ·è½¨è¿¹é€šè¿‡æ˜ç¡®çš„ä¸‰ç»´è¡¨ç¤ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.08475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-098d04fb98a7383be9fefedaf341e49d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bb8393065d5933b2cfa0352a5506572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35bb47b846f5731cc9a4e3be005d1b01.jpg" align="middle">
</details>


<h1 id="-21"><a href="#-21" class="headerlink" title=""></a></h1><h2 id="MVGamba-Unify-3D-Content-Generation-as-State-Space-Sequence-Modeling"><a href="#MVGamba-Unify-3D-Content-Generation-as-State-Space-Sequence-Modeling" class="headerlink" title="MVGamba: Unify 3D Content Generation as State Space Sequence Modeling"></a>MVGamba: Unify 3D Content Generation as State Space Sequence Modeling</h2><p><strong>Authors:Xuanyu Yi, Zike Wu, Qiuhong Shen, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Shuicheng Yan, Xinchao Wang, Hanwang Zhang</strong></p>
<p>Recent 3D large reconstruction models (LRMs) can generate high-quality 3D content in sub-seconds by integrating multi-view diffusion models with scalable multi-view reconstructors. Current works further leverage 3D Gaussian Splatting as 3D representation for improved visual quality and rendering efficiency. However, we observe that existing Gaussian reconstruction models often suffer from multi-view inconsistency and blurred textures. We attribute this to the compromise of multi-view information propagation in favor of adopting powerful yet computationally intensive architectures (e.g., Transformers). To address this issue, we introduce MVGamba, a general and lightweight Gaussian reconstruction model featuring a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal context containing multi-view information for cross-view self-refinement while generating a long sequence of Gaussians for fine-detail modeling with linear complexity. With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts. Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only $0.1\times$ of the model size. </p>
<blockquote>
<p>æœ€è¿‘çš„ä¸‰ç»´å¤§é‡å»ºæ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡æ•´åˆå¤šè§†è§’æ‰©æ•£æ¨¡å‹ä¸å¯æ‰©å±•çš„å¤šè§†è§’é‡å»ºå™¨ï¼Œèƒ½å¤Ÿåœ¨äºšç§’å†…ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´å†…å®¹ã€‚å½“å‰çš„ç ”ç©¶è¿˜è¿›ä¸€æ­¥åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ‹¼è´´ä½œä¸ºä¸‰ç»´è¡¨ç¤ºï¼Œä»¥æé«˜è§†è§‰è´¨é‡å’Œæ¸²æŸ“æ•ˆç‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç°æœ‰çš„é«˜æ–¯é‡å»ºæ¨¡å‹ç»å¸¸é­å—å¤šè§†è§’ä¸ä¸€è‡´å’Œçº¹ç†æ¨¡ç³Šçš„é—®é¢˜ã€‚æˆ‘ä»¬å°†è¿™å½’å› äºä¸ºäº†é‡‡ç”¨å¼ºå¤§ä½†è®¡ç®—å¯†é›†å‹çš„æ¶æ„ï¼ˆä¾‹å¦‚Transformerï¼‰è€Œå¦¥åäº†å¤šè§†è§’ä¿¡æ¯ä¼ æ’­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MVGambaï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨ä¸”è½»é‡çº§çš„é«˜æ–¯é‡å»ºæ¨¡å‹ï¼Œå®ƒåŸºäºRNNç±»ä¼¼çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰é‡‡ç”¨å¤šè§†è§’é«˜æ–¯é‡å»ºå™¨ã€‚æˆ‘ä»¬çš„é«˜æ–¯é‡å»ºå™¨ä¼ æ’­åŒ…å«å¤šè§†è§’ä¿¡æ¯çš„å› æœä¸Šä¸‹æ–‡ï¼Œç”¨äºè·¨è§†è§’è‡ªæˆ‘ä¼˜åŒ–ï¼ŒåŒæ—¶ç”Ÿæˆä¸€ç³»åˆ—é«˜æ–¯æ•°è¿›è¡Œç²¾ç»†å»ºæ¨¡ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚é€šè¿‡æ•´åˆç°æˆçš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼ŒMVGambaèƒ½å¤Ÿç»Ÿä¸€ä»å•å¼ å›¾åƒã€ç¨€ç–å›¾åƒæˆ–æ–‡æœ¬æç¤ºè¿›è¡Œä¸‰ç»´ç”Ÿæˆä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰çš„ä¸‰ç»´å†…å®¹ç”Ÿæˆåœºæ™¯ä¸­ï¼ŒMVGambaçš„è¡¨ç°éƒ½ä¼˜äºæœ€æ–°çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸”æ¨¡å‹å¤§å°åªæœ‰å¤§çº¦$0.1\times$ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06367v3">PDF</a> Accepted by NeurIPS 2024. Code is included in   <a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/MVGamba">https://github.com/SkyworkAI/MVGamba</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œæ–°ä¸€ä»£ä¸‰ç»´å¤§å‹é‡å»ºæ¨¡å‹èƒ½å¤Ÿåœ¨å‡ ç§’å†…ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´å†…å®¹ã€‚é’ˆå¯¹ç°æœ‰é«˜æ–¯é‡å»ºæ¨¡å‹çš„å¤šè§†è§’ä¸ä¸€è‡´å’Œçº¹ç†æ¨¡ç³Šé—®é¢˜ï¼Œæå‡ºäº†MVGambaæ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œç±»ä¼¼çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå®ç°å¤šè§†è§’é«˜æ–¯é‡å»ºå™¨ï¼Œé€šè¿‡ä¼ æ’­åŒ…å«å¤šè§†è§’ä¿¡æ¯çš„å› æœä¸Šä¸‹æ–‡è¿›è¡Œè·¨è§†è§’è‡ªæˆ‘ä¼˜åŒ–ï¼Œå¹¶ä»¥çº¿æ€§å¤æ‚åº¦ç”Ÿæˆç²¾ç»†çš„é«˜æ–¯åºåˆ—è¿›è¡Œå»ºæ¨¡ã€‚å®éªŒè¡¨æ˜ï¼ŒMVGambaåœ¨å¤šç§ä¸‰ç»´å†…å®¹ç”Ÿæˆåœºæ™¯ä¸­å‡ä¼˜äºæœ€æ–°åŸºçº¿æ¨¡å‹ï¼Œä¸”æ¨¡å‹å¤§å°ä»…çº¦ä¸ºå‰è€…çš„ååˆ†ä¹‹ä¸€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹ä¸‰ç»´å¤§å‹é‡å»ºæ¨¡å‹å¯å€ŸåŠ©å¤šè§†è§’æ‰©æ•£æ¨¡å‹å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡ä¸‰ç»´å†…å®¹ã€‚</li>
<li>å½“å‰é«˜æ–¯é‡å»ºæ¨¡å‹å­˜åœ¨å¤šè§†è§’ä¸ä¸€è‡´å’Œçº¹ç†æ¨¡ç³Šçš„é—®é¢˜ã€‚</li>
<li>MVGambaæ¨¡å‹é€šè¿‡å¼•å…¥å¤šè§†è§’é«˜æ–¯é‡å»ºå™¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œè¯¥é‡å»ºå™¨åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œç±»ä¼¼çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚</li>
<li>MVGambaé€šè¿‡ä¼ æ’­å› æœä¸Šä¸‹æ–‡è¿›è¡Œè·¨è§†è§’è‡ªæˆ‘ä¼˜åŒ–ï¼ŒåŒ…å«å¤šè§†è§’ä¿¡æ¯ã€‚</li>
<li>MVGambaèƒ½å¤Ÿç”Ÿæˆç²¾ç»†çš„é«˜æ–¯åºåˆ—ç”¨äºå»ºæ¨¡ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚</li>
<li>MVGambaåœ¨å¤šç§ä¸‰ç»´å†…å®¹ç”Ÿæˆåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰æœ€æ–°åŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28f4d9a4edbda1db7207681eca6438cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42565eae9e7860088670b75330d9016b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b1f667feefe0f174e95040275265f31.jpg" align="middle">
</details>


<h1 id="-22"><a href="#-22" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f48b889f694388b756223771c0d053df.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  GraphAvatar Compact Head Avatars with GNN-Generated 3D Gaussians
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-72e6f839d76852c919624ff97607ba0c.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  GraphAvatar Compact Head Avatars with GNN-Generated 3D Gaussians
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15437.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
