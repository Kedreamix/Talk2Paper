<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Real-Time Position-Aware View Synthesis from Single-View Input">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b22f0081c44a3181d17141f1ff9c9af7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-19-æ›´æ–°"><a href="#2024-12-19-æ›´æ–°" class="headerlink" title="2024-12-19 æ›´æ–°"></a>2024-12-19 æ›´æ–°</h1><h2 id="Real-Time-Position-Aware-View-Synthesis-from-Single-View-Input"><a href="#Real-Time-Position-Aware-View-Synthesis-from-Single-View-Input" class="headerlink" title="Real-Time Position-Aware View Synthesis from Single-View Input"></a>Real-Time Position-Aware View Synthesis from Single-View Input</h2><p><strong>Authors:Manu Gond, Emin Zerman, Sebastian Knorr, MÃ¥rten SjÃ¶strÃ¶m</strong></p>
<p>Recent advancements in view synthesis have significantly enhanced immersive experiences across various computer graphics and multimedia applications, including telepresence, and entertainment. By enabling the generation of new perspectives from a single input view, view synthesis allows users to better perceive and interact with their environment. However, many state-of-the-art methods, while achieving high visual quality, face limitations in real-time performance, which makes them less suitable for live applications where low latency is critical. In this paper, we present a lightweight, position-aware network designed for real-time view synthesis from a single input image and a target camera pose. The proposed framework consists of a Position Aware Embedding, modeled with a multi-layer perceptron, which efficiently maps positional information from the target pose to generate high dimensional feature maps. These feature maps, along with the input image, are fed into a Rendering Network that merges features from dual encoder branches to resolve both high level semantics and low level details, producing a realistic new view of the scene. Experimental results demonstrate that our method achieves superior efficiency and visual quality compared to existing approaches, particularly in handling complex translational movements without explicit geometric operations like warping. This work marks a step toward enabling real-time view synthesis from a single image for live and interactive applications. </p>
<blockquote>
<p>è¿‘æœŸè§†å›¾åˆæˆæŠ€æœ¯çš„è¿›å±•åœ¨å„ç±»è®¡ç®—æœºå›¾å½¢å’Œå¤šåª’ä½“åº”ç”¨ä¸­å¤§å¹…æå‡äº†æ²‰æµ¸å¼ä½“éªŒï¼ŒåŒ…æ‹¬è¿œç¨‹å­˜åœ¨å’Œå¨±ä¹ã€‚è§†å›¾åˆæˆèƒ½å¤Ÿé€šè¿‡å•ä¸ªè¾“å…¥è§†å›¾ç”Ÿæˆæ–°çš„è§†è§’ï¼Œä½¿ç”¨æˆ·æ›´å¥½åœ°æ„ŸçŸ¥å’Œä¸å…¶ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œè®¸å¤šæœ€å…ˆè¿›çš„æ–¹æ³•è™½ç„¶è§†è§‰è´¨é‡å¾ˆé«˜ï¼Œä½†åœ¨å®æ—¶æ€§èƒ½æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨ä½å»¶è¿Ÿè‡³å…³é‡è¦çš„å®æ—¶åº”ç”¨ä¸­ä¸å¤ªé€‚ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå®æ—¶è§†å›¾åˆæˆçš„è½»é‡çº§ã€ä½ç½®æ„ŸçŸ¥ç½‘ç»œï¼Œè¯¥ç½‘ç»œä»å•ä¸ªè¾“å…¥å›¾åƒå’Œç›®æ ‡ç›¸æœºå§¿æ€è¿›è¡Œè§†å›¾åˆæˆã€‚æ‰€æå‡ºçš„æ¡†æ¶ç”±ä¸€ä¸ªä½ç½®æ„ŸçŸ¥åµŒå…¥ç»„æˆï¼Œé€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨è¿›è¡Œå»ºæ¨¡ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†ç›®æ ‡å§¿æ€çš„ä½ç½®ä¿¡æ¯æ˜ å°„åˆ°ç”Ÿæˆé«˜ç»´ç‰¹å¾å›¾ã€‚è¿™äº›ç‰¹å¾å›¾ä¸è¾“å…¥å›¾åƒä¸€èµ·è¾“å…¥åˆ°æ¸²æŸ“ç½‘ç»œä¸­ï¼Œè¯¥ç½‘ç»œåˆå¹¶æ¥è‡ªåŒç¼–ç å™¨åˆ†æ”¯çš„ç‰¹å¾ä»¥è§£å†³é«˜çº§è¯­ä¹‰å’Œä½çº§ç»†èŠ‚é—®é¢˜ï¼Œä»è€Œç”Ÿæˆåœºæ™¯çš„ç°å®æ–°è§†å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œè§†è§‰è´¨é‡æ–¹é¢å®ç°äº†ä¼˜è¶Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„å¹³ç§»è¿åŠ¨è€Œæ— éœ€æ˜ç¡®çš„å‡ ä½•æ“ä½œï¼ˆå¦‚æ‰­æ›²ï¼‰æ–¹é¢ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€ä¸ºå®æ—¶åº”ç”¨å’Œäº¤äº’å¼åº”ç”¨çš„å•å›¾åƒè§†å›¾åˆæˆè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14005v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€è®¡ç®—æœºå›¾å½¢å­¦å’Œå¤šåª’ä½“åº”ç”¨çš„å¿«é€Ÿå‘å±•ï¼Œè§†å›¾åˆæˆæŠ€æœ¯ä¸æ–­è¿›æ­¥ï¼Œæå¤§åœ°æå‡äº†æ²‰æµ¸å¼ä½“éªŒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ã€ä½ç½®æ„ŸçŸ¥çš„ç½‘ç»œï¼Œç”¨äºä»å•å¼ è¾“å…¥å›¾åƒå’Œç›®æ ‡ç›¸æœºå§¿æ€è¿›è¡Œå®æ—¶è§†å›¾åˆæˆã€‚è¯¥ç½‘ç»œé€šè¿‡ä½ç½®æ„ŸçŸ¥åµŒå…¥å’Œå¤šå±‚æ„ŸçŸ¥å™¨æœ‰æ•ˆåœ°æ˜ å°„ç›®æ ‡å§¿æ€çš„ä½ç½®ä¿¡æ¯ï¼Œç”Ÿæˆé«˜ç»´ç‰¹å¾å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æ•ˆç‡å’Œè§†è§‰è´¨é‡æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶æ“…é•¿å¤„ç†å¤æ‚çš„å¹³ç§»è¿åŠ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†å›¾åˆæˆæŠ€æœ¯å¢å¼ºäº†æ²‰æµ¸å¼ä½“éªŒï¼Œå…è®¸ä»å•ä¸€è§†è§’ç”Ÿæˆæ–°çš„è§†è§’ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è™½ç„¶è§†è§‰è´¨é‡é«˜ï¼Œä½†å®æ—¶æ€§èƒ½æœ‰é™ï¼Œä¸é€‚åˆä½å»¶è¿Ÿçš„å®æ—¶åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ã€ä½ç½®æ„ŸçŸ¥çš„ç½‘ç»œï¼Œç”¨äºå®æ—¶è§†å›¾åˆæˆã€‚</li>
<li>è¯¥ç½‘ç»œé€šè¿‡ä½ç½®æ„ŸçŸ¥åµŒå…¥å’Œå¤šå±‚æ„ŸçŸ¥å™¨æ˜ å°„ç›®æ ‡å§¿æ€çš„ä½ç½®ä¿¡æ¯ã€‚</li>
<li>ç½‘ç»œç”Ÿæˆé«˜ç»´ç‰¹å¾å›¾ï¼Œä¸è¾“å…¥å›¾åƒä¸€èµ·è¾“å…¥æ¸²æŸ“ç½‘ç»œã€‚</li>
<li>æ¸²æŸ“ç½‘ç»œåˆå¹¶æ¥è‡ªä¸¤ä¸ªç¼–ç å™¨åˆ†æ”¯çš„ç‰¹å¾ï¼Œè§£å†³é«˜çº§è¯­ä¹‰å’Œä½çº§ç»†èŠ‚ï¼Œç”Ÿæˆé€¼çœŸçš„æ–°è§†å›¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•ˆç‡å’Œè§†è§‰è´¨é‡æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶æ“…é•¿å¤„ç†å¤æ‚çš„å¹³ç§»è¿åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67583bf22a49e384b21fedc92b864fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0400df8e26f50d6717f090237ee97cc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d203e81407b3a31ccf02989ad453419a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Spatial-Brain-Tumor-Concentration-Estimation-for-Individualized-Radiotherapy-Planning"><a href="#Spatial-Brain-Tumor-Concentration-Estimation-for-Individualized-Radiotherapy-Planning" class="headerlink" title="Spatial Brain Tumor Concentration Estimation for Individualized   Radiotherapy Planning"></a>Spatial Brain Tumor Concentration Estimation for Individualized   Radiotherapy Planning</h2><p><strong>Authors:Jonas Weidner, Michal Balcerak, Ivan Ezhov, AndrÃ© Datchev, Laurin Lux, Lucas Zimmerand Daniel Rueckert, BjÃ¶rn Menze, Benedikt Wiestler</strong></p>
<p>Biophysical modeling of brain tumors has emerged as a promising strategy for personalizing radiotherapy planning by estimating the otherwise hidden distribution of tumor cells within the brain. However, many existing state-of-the-art methods are computationally intensive, limiting their widespread translation into clinical practice. In this work, we propose an efficient and direct method that utilizes soft physical constraints to estimate the tumor cell concentration from preoperative MRI of brain tumor patients. Our approach optimizes a 3D tumor concentration field by simultaneously minimizing the difference between the observed MRI and a physically informed loss function. Compared to existing state-of-the-art techniques, our method significantly improves predicting tumor recurrence on two public datasets with a total of 192 patients while maintaining a clinically viable runtime of under one minute - a substantial reduction from the 30 minutes required by the current best approach. Furthermore, we showcase the generalizability of our framework by incorporating additional imaging information and physical constraints, highlighting its potential to translate to various medical diffusion phenomena with imperfect data. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤çš„ç”Ÿç‰©ç‰©ç†å»ºæ¨¡å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„ç­–ç•¥ï¼Œé€šè¿‡ä¼°è®¡è„‘å†…è‚¿ç˜¤ç»†èƒçš„éšè”½åˆ†å¸ƒæ¥ä¸ªæ€§åŒ–æ”¾å°„æ²»ç–—è®¡åˆ’ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠå®è·µä¸­çš„å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”ç›´æ¥çš„æ–¹æ³•ï¼Œåˆ©ç”¨è½¯ç‰©ç†çº¦æŸæ¥ä¼°è®¡è„‘è‚¿ç˜¤æ‚£è€…çš„æœ¯å‰MRIä¸­çš„è‚¿ç˜¤ç»†èƒæµ“åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŒæ—¶æœ€å°åŒ–è§‚å¯Ÿåˆ°çš„MRIå’Œä¸€ä¸ªç‰©ç†ä¿¡æ¯æŸå¤±å‡½æ•°ä¹‹é—´çš„å·®å¼‚æ¥ä¼˜åŒ–ä¸‰ç»´è‚¿ç˜¤æµ“åº¦åœºã€‚ä¸ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå¯¹192åæ‚£è€…çš„è‚¿ç˜¤å¤å‘é¢„æµ‹è¿›è¡Œäº†æ˜¾è‘—æ”¹å–„ï¼ŒåŒæ—¶ä¿æŒäº†ä¸€åˆ†é’Ÿä»¥å†…çš„ä¸´åºŠå¯è¡Œè¿è¡Œæ—¶é—´â€”â€”è¿™å¤§å¤§å‡å°‘äº†å½“å‰æœ€ä½³æ–¹æ³•æ‰€éœ€çš„30åˆ†é’Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡èå…¥é¢å¤–çš„æˆåƒä¿¡æ¯å’Œç‰©ç†çº¦æŸæ¥å±•ç¤ºæˆ‘ä»¬æ¡†æ¶çš„æ™®éæ€§ï¼Œçªæ˜¾å…¶åœ¨å…·æœ‰ä¸å®Œç¾æ•°æ®çš„å„ç§åŒ»å­¦æ‰©æ•£ç°è±¡ä¸­çš„ç¿»è¯‘æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13811v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”ç›´æ¥çš„æ–¹æ³•ï¼Œåˆ©ç”¨è½¯ç‰©ç†çº¦æŸï¼Œé€šè¿‡è„‘è‚¿ç˜¤æ‚£è€…çš„æœ¯å‰MRIæ¥ä¼°è®¡è‚¿ç˜¤ç»†èƒæµ“åº¦ã€‚è¯¥æ–¹æ³•ä¼˜åŒ–äº†ä¸€ä¸ªä¸‰ç»´è‚¿ç˜¤æµ“åº¦åœºï¼ŒåŒæ—¶æœ€å°åŒ–è§‚å¯Ÿåˆ°çš„MRIä¸ç‰©ç†ä¿¡æ¯æŸå¤±å‡½æ•°ä¹‹é—´çš„å·®å¼‚ã€‚ç›¸è¾ƒäºç°æœ‰çš„å…ˆè¿›æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹è‚¿ç˜¤å¤å‘æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼ŒåŒæ—¶åœ¨ç»´æŒä¸´åºŠå¯è¡Œçš„è¿è¡Œæ—¶é—´ï¼ˆå°‘äºä¸€åˆ†é’Ÿï¼‰çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—æ—¶é—´ï¼ˆä»å½“å‰æœ€ä½³æ–¹æ³•çš„ä¸‰ååˆ†é’Ÿå‡å°‘åˆ°ä¸€åˆ†é’Ÿï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†è¯¥æ¡†æ¶çš„é€šç”¨æ€§ï¼Œå¯èå…¥é¢å¤–çš„æˆåƒä¿¡æ¯å’Œç‰©ç†çº¦æŸï¼Œçªæ˜¾å…¶åœ¨ä¸å®Œç¾æ•°æ®ä¸‹çš„åŒ»å­¦æ‰©æ•£ç°è±¡ç¿»è¯‘æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨è½¯ç‰©ç†çº¦æŸç›´æ¥ä¼°è®¡è„‘è‚¿ç˜¤ç»†èƒæµ“åº¦çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ä¸‰ç»´è‚¿ç˜¤æµ“åº¦åœºï¼Œæœ€å°åŒ–è§‚å¯Ÿåˆ°çš„MRIä¸ç‰©ç†ä¿¡æ¯æŸå¤±å‡½æ•°ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>æ–¹æ³•æ˜¾è‘—æé«˜äº†åœ¨å…¬å…±æ•°æ®é›†ä¸Šé¢„æµ‹è‚¿ç˜¤å¤å‘çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç»´æŒäº†ä¸´åºŠå¯è¡Œçš„è¿è¡Œæ—¶é—´ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—æ—¶é—´ã€‚</li>
<li>æ¡†æ¶å…·æœ‰é€šç”¨æ€§ï¼Œå¯èå…¥é¢å¤–çš„æˆåƒä¿¡æ¯å’Œç‰©ç†çº¦æŸã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æœ›ä¸ºä¸´åºŠæ”¾å°„æ²»ç–—è®¡åˆ’æä¾›ä¸ªæ€§åŒ–ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d2e317c2bec8b6cc279663129a7c5c39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15bbba5136b33a28b9ea8805e4434090.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Automatic-Evaluation-for-Image-Transcreation"><a href="#Towards-Automatic-Evaluation-for-Image-Transcreation" class="headerlink" title="Towards Automatic Evaluation for Image Transcreation"></a>Towards Automatic Evaluation for Image Transcreation</h2><p><strong>Authors:Simran Khanuja, Vivek Iyer, Claire He, Graham Neubig</strong></p>
<p>Beyond conventional paradigms of translating speech and text, recently, there has been interest in automated transcreation of images to facilitate localization of visual content across different cultures. Attempts to define this as a formal Machine Learning (ML) problem have been impeded by the lack of automatic evaluation mechanisms, with previous work relying solely on human evaluation. In this paper, we seek to close this gap by proposing a suite of automatic evaluation metrics inspired by machine translation (MT) metrics, categorized into: a) Object-based, b) Embedding-based, and c) VLM-based. Drawing on theories from translation studies and real-world transcreation practices, we identify three critical dimensions of image transcreation: cultural relevance, semantic equivalence and visual similarity, and design our metrics to evaluate systems along these axes. Our results show that proprietary VLMs best identify cultural relevance and semantic equivalence, while vision-encoder representations are adept at measuring visual similarity. Meta-evaluation across 7 countries shows our metrics agree strongly with human ratings, with average segment-level correlations ranging from 0.55-0.87. Finally, through a discussion of the merits and demerits of each metric, we offer a robust framework for automated image transcreation evaluation, grounded in both theoretical foundations and practical application. Our code can be found here: <a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/automatic-eval-transcreation">https://github.com/simran-khanuja/automatic-eval-transcreation</a> </p>
<blockquote>
<p>åœ¨ä¼ ç»Ÿè¯­éŸ³å’Œæ–‡æœ¬ç¿»è¯‘çš„èŒƒå¼ä¹‹å¤–ï¼Œæœ€è¿‘äººä»¬å¯¹è‡ªåŠ¨åˆ›å»ºå›¾åƒä»¥æ¨åŠ¨ä¸åŒæ–‡åŒ–é—´çš„è§†è§‰å†…å®¹æœ¬åœ°åŒ–äº§ç”Ÿäº†å…´è¶£ã€‚å°†è¿™ä¸€é¢†åŸŸå®šä¹‰ä¸ºæ­£å¼çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰é—®é¢˜çš„å°è¯•å—åˆ°äº†ç¼ºä¹è‡ªåŠ¨è¯„ä¼°æœºåˆ¶çš„é˜»ç¢ï¼Œæ—©æœŸçš„å·¥ä½œå®Œå…¨ä¾èµ–äºäººå·¥è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯•å›¾é€šè¿‡å€Ÿé‰´æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æŒ‡æ ‡æå‡ºä¸€å¥—è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè¿™äº›æŒ‡æ ‡å¯åˆ†ä¸ºä¸‰ç±»ï¼šaï¼‰åŸºäºå¯¹è±¡çš„ï¼Œbï¼‰åŸºäºåµŒå…¥çš„ï¼Œä»¥åŠcï¼‰åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ã€‚æˆ‘ä»¬ä»ç¿»è¯‘ç ”ç©¶ç†è®ºå’Œç°å®ä¸–ç•Œçš„å›¾åƒè½¬åˆ›å®è·µå‡ºå‘ï¼Œç¡®å®šäº†å›¾åƒè½¬åˆ›çš„ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šæ–‡åŒ–ç›¸å…³æ€§ã€è¯­ä¹‰ç­‰ä»·æ€§å’Œè§†è§‰ç›¸ä¼¼æ€§ï¼Œå¹¶è®¾è®¡äº†æˆ‘ä»¬çš„æŒ‡æ ‡æ¥æ²¿ç€è¿™äº›è½´è¯„ä¼°ç³»ç»Ÿã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸“æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«æ–‡åŒ–ç›¸å…³æ€§å’Œè¯­ä¹‰ç­‰ä»·æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œè§†è§‰ç¼–ç å™¨è¡¨ç¤ºåœ¨è¡¡é‡è§†è§‰ç›¸ä¼¼æ€§æ–¹é¢å¾ˆåœ¨è¡Œã€‚åœ¨7ä¸ªå›½å®¶çš„å…ƒè¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æŒ‡æ ‡ä¸äººç±»è¯„åˆ†é«˜åº¦ä¸€è‡´ï¼Œå¹³å‡åˆ†æ®µç›¸å…³æ€§åœ¨0.55-0.87ä¹‹é—´ã€‚æœ€åï¼Œé€šè¿‡å¯¹æ¯ç§æŒ‡æ ‡çš„ä¼˜ç¼ºç‚¹è¿›è¡Œè®¨è®ºï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ—¢åŸºäºç†è®ºä¹Ÿé€‚ç”¨äºå®é™…åº”ç”¨çš„ç¨³å¥æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–å›¾åƒè½¬åˆ›è¯„ä¼°ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/automatic-eval-transcreation">https://github.com/simran-khanuja/automatic-eval-transcreation</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13717v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³å›¾åƒè½¬åˆ›ï¼ˆImage Transcreationï¼‰çš„è‡ªåŠ¨è¯„ä¼°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç³»åˆ—åŸºäºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å¯¹è±¡åŸºç¡€ã€åµŒå…¥åŸºç¡€å’ŒVLMåŸºç¡€ä¸‰ç§ç±»å‹ã€‚è¿™äº›æŒ‡æ ‡æ—¨åœ¨ä»æ–‡åŒ–ç›¸å…³æ€§ã€è¯­ä¹‰ç­‰ä»·æ€§å’Œè§†è§‰ç›¸ä¼¼æ€§ä¸‰ä¸ªå…³é”®ç»´åº¦è¯„ä¼°å›¾åƒè½¬åˆ›ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸“æœ‰VLMsåœ¨è¯†åˆ«æ–‡åŒ–ç›¸å…³æ€§å’Œè¯­ä¹‰ç­‰ä»·æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œè§†è§‰ç¼–ç å™¨è¡¨ç¤ºåœ¨æµ‹é‡è§†è§‰ç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œå¯¹ä¸ƒä¸ªå›½å®¶çš„å…ƒè¯„ä¼°æ˜¾ç¤ºï¼Œæœ¬æ–‡æå‡ºçš„æŒ‡æ ‡ä¸äººç±»è¯„åˆ†é«˜åº¦ä¸€è‡´ï¼Œå¹³å‡åˆ†æ®µç›¸å…³æ€§åœ¨0.55è‡³0.87ä¹‹é—´ã€‚æœ€åï¼Œæœ¬æ–‡è®¨è®ºäº†å„é¡¹æŒ‡æ ‡çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºåŸºäºç†è®ºå’Œå®è·µçš„è‡ªåŠ¨åŒ–å›¾åƒè½¬åˆ›è¯„ä¼°æä¾›äº†ç¨³å¥æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒè½¬åˆ›ï¼ˆImage Transcreationï¼‰æ˜¯ç¿»è¯‘é¢†åŸŸçš„æ–°å‘å±•æ–¹å‘ï¼Œæ—¨åœ¨å®ç°ä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹è§†è§‰å†…å®¹çš„æœ¬åœ°åŒ–ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸­ç¼ºä¹å›¾åƒè½¬åˆ›çš„è‡ªåŠ¨è¯„ä¼°æœºåˆ¶ï¼Œå› æ­¤æœ¬æ–‡æå‡ºäº†ä¸€ç³»åˆ—åŸºäºæœºå™¨ç¿»è¯‘çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>è¿™äº›æŒ‡æ ‡ä»æ–‡åŒ–ç›¸å…³æ€§ã€è¯­ä¹‰ç­‰ä»·æ€§å’Œè§†è§‰ç›¸ä¼¼æ€§ä¸‰ä¸ªå…³é”®ç»´åº¦å¯¹å›¾åƒè½¬åˆ›ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œä¸“æœ‰VLMsåœ¨è¯†åˆ«æ–‡åŒ–ç›¸å…³æ€§å’Œè¯­ä¹‰ç­‰ä»·æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè§†è§‰ç¼–ç å™¨æ“…é•¿æµ‹é‡è§†è§‰ç›¸ä¼¼æ€§ã€‚</li>
<li>è·¨ä¸ƒå›½çš„å…ƒè¯„ä¼°è¯å®ï¼Œæœ¬æ–‡æå‡ºçš„æŒ‡æ ‡ä¸äººç±»è¯„åˆ†é«˜åº¦ä¸€è‡´ã€‚</li>
<li>æœ¬æ–‡è¯¦ç»†è®¨è®ºäº†å„é¡¹æŒ‡æ ‡çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26d343d14aacd616d62cf66b5927d90e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b38550d58440cff37dfe8d4301e20261.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ba0132b4f1076ffac25ff78434e5967.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34244154a057a06450da5c57ab3ffc84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d974c5c222fff2634c34fba597b368be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81d305b3326d9d77a5220a93c85cc686.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Make-Imagination-Clearer-Stable-Diffusion-based-Visual-Imagination-for-Multimodal-Machine-Translation"><a href="#Make-Imagination-Clearer-Stable-Diffusion-based-Visual-Imagination-for-Multimodal-Machine-Translation" class="headerlink" title="Make Imagination Clearer! Stable Diffusion-based Visual Imagination for   Multimodal Machine Translation"></a>Make Imagination Clearer! Stable Diffusion-based Visual Imagination for   Multimodal Machine Translation</h2><p><strong>Authors:Andong Chen, Yuchen Song, Kehai Chen, Muyun Yang, Tiejun Zhao, Min Zhang</strong></p>
<p>Visual information has been introduced for enhancing machine translation (MT), and its effectiveness heavily relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we introduce a stable diffusion-based imagination network into a multimodal large language model (MLLM) to explicitly generate an image for each source sentence, thereby advancing the multimodel MT. Particularly, we build heuristic human feedback with reinforcement learning to ensure the consistency of the generated image with the source sentence without the supervision of image annotation, which breaks the bottleneck of using visual information in MT. Furthermore, the proposed method enables imaginative visual information to be integrated into large-scale text-only MT in addition to multimodal MT. Experimental results show that our model significantly outperforms existing multimodal MT and text-only MT, especially achieving an average improvement of more than 14 BLEU points on Multi30K multimodal MT benchmarks. </p>
<blockquote>
<p>å°†è§†è§‰ä¿¡æ¯å¼•å…¥æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ä»¥å¢å¼ºå…¶åŠŸèƒ½ï¼Œå…¶æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¤§é‡å¸¦æœ‰æ‰‹åŠ¨å›¾åƒæ³¨é‡Šçš„åŒè¯­å¹³è¡Œå¥å­å¯¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†åŸºäºç¨³å®šæ‰©æ•£çš„æƒ³è±¡ç½‘ç»œå¼•å…¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä»¥æ˜ç¡®ä¸ºæ¯å¥æºå¥ç”Ÿæˆç›¸åº”çš„å›¾åƒï¼Œä»è€Œä¿ƒè¿›å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘çš„å‘å±•ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ„å»ºå¯å‘å¼äººç±»åé¦ˆï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸æºå¥çš„ä¸€è‡´æ€§ï¼Œæ— éœ€å›¾åƒæ³¨é‡Šçš„ç›‘ç£ï¼Œä»è€Œçªç ´äº†æœºå™¨ç¿»è¯‘ä¸­ä½¿ç”¨è§†è§‰ä¿¡æ¯çš„ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é™¤äº†å¤šæ¨¡æ€MTä¹‹å¤–ï¼Œè¿˜å¯ä»¥å°†æƒ³è±¡æ€§è§†è§‰ä¿¡æ¯é›†æˆåˆ°å¤§è§„æ¨¡çº¯æ–‡æœ¬MTä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç°æœ‰çš„å¤šæ¨¡æ€MTå’Œçº¯æ–‡æœ¬MTä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨Multi30Kå¤šæ¨¡æ€MTåŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æé«˜äº†è¶…è¿‡14ä¸ªBLEUç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12627v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŸºäºç¨³å®šæ‰©æ•£çš„æƒ³è±¡ç½‘ç»œåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œèƒ½å¤Ÿé’ˆå¯¹æ¯ä¸ªæºå¥å­ç”Ÿæˆå¯¹åº”çš„å›¾åƒï¼Œä»è€Œæ¨è¿›å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘çš„å‘å±•ã€‚é€šè¿‡ç»“åˆå¯å‘å¼äººå·¥åé¦ˆå’Œå¼ºåŒ–å­¦ä¹ ï¼Œç¡®ä¿ç”Ÿæˆå›¾åƒä¸æºå¥å­çš„ä¸€è‡´æ€§ï¼Œæ— éœ€æ‰‹åŠ¨å›¾åƒæ³¨é‡Šçš„ç›‘ç£ï¼Œæ‰“ç ´äº†æœºå™¨ç¿»è¯‘ä¸­ä½¿ç”¨è§†è§‰ä¿¡æ¯çš„ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿåº”ç”¨äºå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼Œè¿˜èƒ½å¤Ÿå°†æƒ³è±¡æ€§çš„è§†è§‰ä¿¡æ¯é›†æˆåˆ°å¤§è§„æ¨¡çš„çº¯æ–‡æœ¬æœºå™¨ç¿»è¯‘ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨Multi30Kå¤šæ¨¡æ€æœºå™¨ç¿»è¯‘åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æé«˜äº†è¶…è¿‡14ä¸ªBLEUç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥ç¨³å®šæ‰©æ•£åŸºäºæƒ³è±¡ç½‘ç»œçš„æ¨¡å‹ä»¥å¢å¼ºæœºå™¨ç¿»è¯‘ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆä¸æºå¥å­ç›¸å¯¹åº”çš„å›¾ç‰‡æ¨è¿›å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘çš„å‘å±•ã€‚</li>
<li>ç»“åˆå¯å‘å¼äººå·¥åé¦ˆå’Œå¼ºåŒ–å­¦ä¹ ï¼Œç¡®ä¿ç”Ÿæˆå›¾åƒä¸æºå¥å­çš„ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€æ‰‹åŠ¨å›¾åƒæ³¨é‡Šçš„ç›‘ç£ï¼Œæ‰“ç ´ä½¿ç”¨è§†è§‰ä¿¡æ¯åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„ç“¶é¢ˆã€‚</li>
<li>æƒ³è±¡æ€§çš„è§†è§‰ä¿¡æ¯å¯é›†æˆåˆ°å¤§è§„æ¨¡çš„çº¯æ–‡æœ¬æœºå™¨ç¿»è¯‘ä¸­ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯Multi30KåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9cbea8ac73a1bf1836f7ad23bf59ae37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da617ccd25a73f3719a11187f1ec454b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac39c89b14e8b743c89e1578ca0993f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93e1a4e443505e1bd054d69891d6aaeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e037042959c1f7882d1a4453ba9a04e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89385e803b349f34695373524122818b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LineArt-A-Knowledge-guided-Training-free-High-quality-Appearance-Transfer-for-Design-Drawing-with-Diffusion-Model"><a href="#LineArt-A-Knowledge-guided-Training-free-High-quality-Appearance-Transfer-for-Design-Drawing-with-Diffusion-Model" class="headerlink" title="LineArt: A Knowledge-guided Training-free High-quality Appearance   Transfer for Design Drawing with Diffusion Model"></a>LineArt: A Knowledge-guided Training-free High-quality Appearance   Transfer for Design Drawing with Diffusion Model</h2><p><strong>Authors:Xi Wang, Hongzhen Li, Heng Fang, Yichen Peng, Haoran Xie, Xi Yang, Chuntao Li</strong></p>
<p>Image rendering from line drawings is vital in design and image generation technologies reduce costs, yet professional line drawings demand preserving complex details. Text prompts struggle with accuracy, and image translation struggles with consistency and fine-grained control. We present LineArt, a framework that transfers complex appearance onto detailed design drawings, facilitating design and artistic creation. It generates high-fidelity appearance while preserving structural accuracy by simulating hierarchical visual cognition and integrating human artistic experience to guide the diffusion process. LineArt overcomes the limitations of current methods in terms of difficulty in fine-grained control and style degradation in design drawings. It requires no precise 3D modeling, physical property specs, or network training, making it more convenient for design tasks. LineArt consists of two stages: a multi-frequency lines fusion module to supplement the input design drawing with detailed structural information and a two-part painting process for Base Layer Shaping and Surface Layer Coloring. We also present a new design drawing dataset ProLines for evaluation. The experiments show that LineArt performs better in accuracy, realism, and material precision compared to SOTAs. </p>
<blockquote>
<p>ä»çº¿æ¡å›¾åƒä¸­å‘ˆç°å›¾åƒåœ¨è®¾è®¡åŠå›¾åƒç”ŸæˆæŠ€æœ¯ä¸­è‡³å…³é‡è¦ï¼Œè¯¥æŠ€æœ¯å¯ä»¥é™ä½ç”Ÿäº§æˆæœ¬ï¼Œç„¶è€Œä¸“ä¸šçš„çº¿æ¡å›¾åƒéœ€è¦ä¿ç•™å¤æ‚çš„ç»†èŠ‚ã€‚æ–‡æœ¬æç¤ºåœ¨å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå›¾åƒç¿»è¯‘åœ¨ä¸€è‡´æ€§å’Œç²¾ç»†æ§åˆ¶æ–¹é¢ä¹Ÿå­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†LineArtæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå°†å¤æ‚çš„å¤–è§‚è½¬ç§»åˆ°è¯¦ç»†çš„è®¾è®¡å›¾çº¸ä¸Šï¼Œä¿ƒè¿›è®¾è®¡å’Œè‰ºæœ¯åˆ›ä½œã€‚å®ƒé€šè¿‡æ¨¡æ‹Ÿåˆ†å±‚è§†è§‰è®¤çŸ¥å¹¶æ•´åˆäººç±»è‰ºæœ¯ç»éªŒæ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œåœ¨ä¿ç•™ç»“æ„å‡†ç¡®æ€§çš„åŒæ—¶ç”Ÿæˆé«˜ä¿çœŸå¤–è§‚ã€‚LineArtå…‹æœäº†å½“å‰æ–¹æ³•åœ¨ç²¾ç»†æ§åˆ¶æ–¹é¢çš„éš¾åº¦å’Œè®¾è®¡å›¾çº¸ä¸­çš„é£æ ¼é€€åŒ–ç­‰å±€é™æ€§ã€‚å®ƒä¸éœ€è¦ç²¾ç¡®çš„ä¸‰ç»´å»ºæ¨¡ã€ç‰©ç†å±æ€§è§„æ ¼æˆ–ç½‘ç»œè®­ç»ƒï¼Œä¸ºè®¾è®¡ä»»åŠ¡æä¾›äº†ä¾¿åˆ©ã€‚LineArtç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆï¼šä¸€ä¸ªå¤šé¢‘çº¿æ¡èåˆæ¨¡å—ï¼Œç”¨äºä¸ºè¾“å…¥çš„è®¾è®¡å›¾çº¸è¡¥å……è¯¦ç»†çš„ç»“æ„ä¿¡æ¯ï¼Œä»¥åŠåˆ†ä¸ºä¸¤éƒ¨åˆ†çš„ä¸Šè‰²è¿‡ç¨‹ï¼Œå³åŸºç¡€å±‚å¡‘å½¢å’Œè¡¨å±‚ç€è‰²ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªæ–°çš„è®¾è®¡å›¾çº¸æ•°æ®é›†ProLinesï¼Œç”¨äºè¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒLineArtåœ¨å‡†ç¡®æ€§ã€çœŸå®æ„Ÿå’Œææ–™ç²¾åº¦æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11519v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://meaoxixi.github.io/LineArt/">https://meaoxixi.github.io/LineArt/</a></p>
<p><strong>Summary</strong></p>
<p>çº¿ç”»å›¾åœ¨è®¾è®¡å’Œå›¾åƒç”ŸæˆæŠ€æœ¯ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†ä¸“ä¸šçº¿ç”»å›¾éœ€è¦ä¿ç•™å¤æ‚çš„ç»†èŠ‚ã€‚å½“å‰æŠ€æœ¯é¢ä¸´æ–‡æœ¬æç¤ºå‡†ç¡®æ€§ä¸è¶³å’Œå›¾åƒç¿»è¯‘ä¸€è‡´æ€§åŠç²¾ç»†æ§åˆ¶æ–¹é¢çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LineArtæ¡†æ¶ï¼Œå®ƒèƒ½å°†å¤æ‚çš„å¤–è§‚è½¬ç§»åˆ°è¯¦ç»†çš„è®¾è®¡å›¾çº¸ä¸Šï¼Œä¿ƒè¿›äº†è®¾è®¡å’Œè‰ºæœ¯åˆ›ä½œã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿåˆ†å±‚è§†è§‰è®¤çŸ¥å¹¶æ•´åˆäººç±»è‰ºæœ¯ç»éªŒæ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œèƒ½åœ¨ä¿æŒç»“æ„å‡†ç¡®æ€§çš„åŒæ—¶ç”Ÿæˆé«˜ä¿çœŸå¤–è§‚ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„ç¼ºç‚¹ã€‚LineArtæ— éœ€ç²¾ç¡®çš„3Då»ºæ¨¡ã€ç‰©ç†å±æ€§è§„æ ¼æˆ–ç½‘ç»œè®­ç»ƒï¼Œæ›´é€‚åˆç”¨äºè®¾è®¡ä»»åŠ¡ã€‚å®ƒç”±å¤šé¢‘çº¿æ¡èåˆæ¨¡å—å’Œä¸¤å±‚ç»˜ç”»æµç¨‹ç»„æˆï¼Œåˆ†åˆ«ä¸ºåŸºç¡€å±‚å¡‘å½¢å’Œè¡¨å±‚ç€è‰²ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†æ–°çš„è®¾è®¡ç»˜å›¾æ•°æ®é›†ProLinesè¿›è¡Œè¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒLineArtåœ¨å‡†ç¡®æ€§ã€é€¼çœŸåº¦å’Œææ–™ç²¾åº¦æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº¿ç”»å›¾åœ¨è®¾è®¡åŠå›¾åƒç”ŸæˆæŠ€æœ¯ä¸­è‡³å…³é‡è¦ï¼Œä¸”éœ€ä¿ç•™å¤æ‚ç»†èŠ‚ã€‚</li>
<li>å½“å‰æŠ€æœ¯é¢ä¸´æ–‡æœ¬æç¤ºå‡†ç¡®æ€§åŠå›¾åƒç¿»è¯‘ä¸€è‡´æ€§ã€ç²¾ç»†æ§åˆ¶éš¾é¢˜ã€‚</li>
<li>LineArtæ¡†æ¶èƒ½é€šè¿‡æ¨¡æ‹Ÿåˆ†å±‚è§†è§‰è®¤çŸ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œç”Ÿæˆé«˜ä¿çœŸå¤–è§‚ã€‚</li>
<li>LineArtåœ¨ä¿æŒç»“æ„å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„ç¼ºç‚¹ã€‚</li>
<li>LineArtæ— éœ€ç²¾ç¡®çš„3Då»ºæ¨¡ã€ç‰©ç†å±æ€§è§„æ ¼æˆ–ç½‘ç»œè®­ç»ƒï¼Œé€‚ç”¨äºè®¾è®¡ä»»åŠ¡ã€‚</li>
<li>LineArtç”±å¤šé¢‘çº¿æ¡èåˆæ¨¡å—å’Œä¸¤å±‚ç»˜ç”»æµç¨‹æ„æˆï¼ŒåŒ…æ‹¬åŸºç¡€å±‚å¡‘å½¢å’Œè¡¨å±‚ç€è‰²ã€‚</li>
<li>æ–°æ¨å‡ºçš„è®¾è®¡ç»˜å›¾æ•°æ®é›†ProLinesä¸ºLineArtæä¾›äº†è¯„ä¼°æ ‡å‡†ï¼Œå®éªŒè¡¨æ˜å…¶è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a992e876f22e1c9acabf1067aac3496.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c2230ed5d056c918b9774b2e3d2134c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05730bc094d405a90dc2cb5937cfcf18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df18f7406e109271469122244655562f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6a78ef0a86c20ff1d8e72acfc6cb5e3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SweetTokenizer-Semantic-Aware-Spatial-Temporal-Tokenizer-for-Compact-Visual-Discretization"><a href="#SweetTokenizer-Semantic-Aware-Spatial-Temporal-Tokenizer-for-Compact-Visual-Discretization" class="headerlink" title="SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact   Visual Discretization"></a>SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact   Visual Discretization</h2><p><strong>Authors:Zhentao Tan, Ben Xue, Jian Jia, Junhao Wang, Wencai Ye, Shaoyun Shi, Mingjie Sun, Wenjin Wu, Quan Chen, Peng Jiang</strong></p>
<p>This paper presents the \textbf{S}emantic-a\textbf{W}ar\textbf{E} spatial-t\textbf{E}mporal \textbf{T}okenizer (SweetTokenizer), a compact yet effective discretization approach for vision data. Our goal is to boost tokenizersâ€™ compression ratio while maintaining reconstruction fidelity in the VQ-VAE paradigm. Firstly, to obtain compact latent representations, we decouple images or videos into spatial-temporal dimensions, translating visual information into learnable querying spatial and temporal tokens through a \textbf{C}ross-attention \textbf{Q}uery \textbf{A}uto\textbf{E}ncoder (CQAE). Secondly, to complement visual information during compression, we quantize these tokens via a specialized codebook derived from off-the-shelf LLM embeddings to leverage the rich semantics from language modality. Finally, to enhance training stability and convergence, we also introduce a curriculum learning strategy, which proves critical for effective discrete visual representation learning. SweetTokenizer achieves comparable video reconstruction fidelity with only \textbf{25%} of the tokens used in previous state-of-the-art video tokenizers, and boost video generation results by \textbf{32.9%} w.r.t gFVD. When using the same token number, we significantly improves video and image reconstruction results by \textbf{57.1%} w.r.t rFVD on UCF-101 and \textbf{37.2%} w.r.t rFID on ImageNet-1K. Additionally, the compressed tokens are imbued with semantic information, enabling few-shot recognition capabilities powered by LLMs in downstream applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Semantic-Awareç©ºé—´æ—¶åºä»¤ç‰ŒåŒ–å™¨ï¼ˆSweetTokenizerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç´§å‡‘ä¸”æœ‰æ•ˆçš„é’ˆå¯¹è§†è§‰æ•°æ®çš„ç¦»æ•£åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨VQ-VAEèŒƒå¼ä¸­æé«˜ä»¤çŸ¢é‡åŒ–å™¨çš„å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚é¦–å…ˆï¼Œä¸ºäº†è·å–ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºï¼Œæˆ‘ä»¬å°†å›¾åƒæˆ–è§†é¢‘è§£è€¦ä¸ºç©ºé—´æ—¶é—´ç»´åº¦ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æŸ¥è¯¢è‡ªåŠ¨ç¼–ç å™¨ï¼ˆCQAEï¼‰å°†è§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºå¯å­¦ä¹ çš„æŸ¥è¯¢ç©ºé—´å’Œæ—¶é—´ä»¤ç‰Œã€‚å…¶æ¬¡ï¼Œä¸ºäº†åœ¨å‹ç¼©è¿‡ç¨‹ä¸­è¡¥å……è§†è§‰ä¿¡æ¯ï¼Œæˆ‘ä»¬é€šè¿‡ä»ç°æˆçš„LLMåµŒå…¥ä¸­æ´¾ç”Ÿå‡ºçš„ä¸“ç”¨ä»£ç æœ¬å¯¹è¿™äº›ä»¤ç‰Œè¿›è¡Œé‡åŒ–ï¼Œä»¥åˆ©ç”¨è¯­è¨€æ¨¡æ€çš„ä¸°å¯Œè¯­ä¹‰ã€‚æœ€åï¼Œä¸ºäº†æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œè¿™å¯¹äºæœ‰æ•ˆçš„ç¦»æ•£è§†è§‰è¡¨ç¤ºå­¦ä¹ è‡³å…³é‡è¦ã€‚SweetTokenizeråœ¨ä»…ä½¿ç”¨ç°æœ‰æœ€å…ˆæŠ€æœ¯è§†é¢‘ä»¤ç‰ŒåŒ–å™¨ä¸­ä½¿ç”¨çš„25ï¼…ä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ç›¸å½“çš„è§†é¢‘é‡å»ºä¿çœŸåº¦ï¼Œå¹¶å°†è§†é¢‘ç”Ÿæˆç»“æœæé«˜äº†32.9ï¼…ï¼Œä»¥GFVDä¸ºå‡†ã€‚åœ¨ä½¿ç”¨ç›¸åŒä»¤ç‰Œæ•°é‡çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨UCF-101ä¸Šçš„è§†é¢‘é‡å»ºç»“æœå’ŒImageNet-1Kä¸Šçš„å›¾åƒé‡å»ºç»“æœåˆ†åˆ«æé«˜äº†57.1ï¼…å’Œ37.2ï¼…ï¼Œä»¥rFVDå’ŒrFIDä¸ºå‡†ã€‚æ­¤å¤–ï¼Œå‹ç¼©çš„ä»¤ç‰Œè¢«èµ‹äºˆäº†è¯­ä¹‰ä¿¡æ¯ï¼Œå¯åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­å€ŸåŠ©LLMå®ç°å°‘é‡çš„è¯†åˆ«åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10443v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè¯­ä¹‰æˆ˜äº‰ç¼–ç ï¼ˆSweetTokenizerï¼‰çš„ç´§å‡‘ä¸”æœ‰æ•ˆçš„ç¦»æ•£åŒ–æ–¹æ³•ï¼Œç”¨äºå¤„ç†è§†è§‰æ•°æ®ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æé«˜VQ-VAEæ¡†æ¶ä¸­çš„å‹ç¼©æ¯”å¹¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚å®ƒé€šè¿‡è§£è€¦å›¾åƒæˆ–è§†é¢‘çš„ç©ºé—´æ—¶é—´ç»´åº¦è·å¾—ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è·¨æ³¨æ„åŠ›æŸ¥è¯¢è‡ªåŠ¨ç¼–ç å™¨ï¼ˆCQAEï¼‰å°†è§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºå¯å­¦ä¹ çš„æŸ¥è¯¢ç©ºé—´å’Œæ—¶é—´ä»¤ç‰Œã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨ä»è¯­è¨€æ¨¡æ€ä¸­è·å¾—çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼Œé€šè¿‡ä¸“ç”¨ä»£ç æœ¬å¯¹è¿™äº›ä»¤ç‰Œè¿›è¡Œé‡åŒ–ã€‚æœ€åï¼Œå¼•å…¥è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä»¥å¢å¼ºè®­ç»ƒå’Œæ”¶æ•›ã€‚SweetTokenizerå®ç°äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”çš„è§†é¢‘é‡å»ºä¿çœŸåº¦ï¼Œå¹¶æé«˜äº†è§†é¢‘ç”Ÿæˆç»“æœã€‚æ­¤å¤–ï¼Œå‹ç¼©çš„ä»¤ç‰Œå…·æœ‰è¯­ä¹‰ä¿¡æ¯ï¼Œå¯åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­å®ç°ç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å°‘æ ·æœ¬è¯†åˆ«åŠŸèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SweetTokenizeræ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰æ•°æ®çš„ç´§å‡‘ä¸”æœ‰æ•ˆçš„ç¦»æ•£åŒ–æ–¹æ³•ã€‚</li>
<li>å®ƒé€šè¿‡è§£è€¦å›¾åƒæˆ–è§†é¢‘çš„ç©ºé—´æ—¶é—´ç»´åº¦æ¥æé«˜å‹ç¼©æ¯”å¹¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚</li>
<li>ä½¿ç”¨è·¨æ³¨æ„åŠ›æŸ¥è¯¢è‡ªåŠ¨ç¼–ç å™¨ï¼ˆCQAEï¼‰å°†è§†è§‰ä¿¡æ¯è½¬æ¢ä¸ºä»¤ç‰Œã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸“ç”¨ä»£ç æœ¬å’Œè¯­è¨€æ¨¡æ€çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯æ¥é‡åŒ–è¿™äº›ä»¤ç‰Œã€‚</li>
<li>å¼•å…¥è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ä»¥å¢å¼ºè®­ç»ƒå’Œæ”¶æ•›ã€‚</li>
<li>SweetTokenizerå®ç°äº†é«˜è§†é¢‘é‡å»ºä¿çœŸåº¦å’Œæ”¹è¿›çš„è§†é¢‘ç”Ÿæˆç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6ad1d061dbaae5e46990c4747c2c206.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b2e7c9a443cb65dec8a37331e96472c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aa1f3c554aa1a9d2405ee842ccba949.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-556305bf2235b5cf6b1c193a1264aeb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79e0fb570c18e1e18fc4aebe875771de.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="herakoi-a-sonification-experiment-for-astronomical-data"><a href="#herakoi-a-sonification-experiment-for-astronomical-data" class="headerlink" title="herakoi: a sonification experiment for astronomical data"></a>herakoi: a sonification experiment for astronomical data</h2><p><strong>Authors:Michele Ginolfi, Luca Di Mascolo, Anita Zanella</strong></p>
<p>Recent research is revealing data-sonification as a promising complementary approach to vision, benefiting both data perception and interpretation. We present herakoi, a novel open-source software that uses machine learning to allow real-time image sonification, with a focus on astronomical data. By tracking hand movements via a webcam and mapping them to image coordinates, herakoi translates visual properties into sound, enabling users to â€œhearâ€ images. Its swift responsiveness allows users to access information in astronomical images with short training, demonstrating high reliability and effectiveness. The software has shown promise in educational and outreach settings, making complex astronomical concepts more engaging and accessible to diverse audiences, including blind and visually impaired individuals. We also discuss future developments, such as the integration of large language and vision models to create a more interactive experience in interpreting astronomical data. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ•°æ®å¯è§†åŒ–ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è¡¥å……æ–¹æ³•ï¼Œæœ‰ç›Šäºæ•°æ®æ„ŸçŸ¥å’Œè§£é‡Šã€‚æˆ‘ä»¬æ¨å‡ºäº†herakoiï¼Œè¿™æ˜¯ä¸€æ¬¾æ–°å‹å¼€æºè½¯ä»¶ï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ å®ç°å®æ—¶å›¾åƒå¯è§†åŒ–ï¼Œé‡ç‚¹å¤„ç†å¤©æ–‡æ•°æ®ã€‚herakoié€šè¿‡è¿½è¸ªé€šè¿‡ç½‘ç»œæ‘„åƒå¤´çš„æ‰‹éƒ¨åŠ¨ä½œå¹¶å°†å…¶æ˜ å°„åˆ°å›¾åƒåæ ‡ï¼Œå°†è§†è§‰å±æ€§è½¬æ¢ä¸ºå£°éŸ³ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿâ€œå¬åˆ°â€å›¾åƒã€‚å…¶å¿«é€Ÿå“åº”èƒ½åŠ›ä½¿ç”¨æˆ·åœ¨çŸ­æš‚è®­ç»ƒåå°±èƒ½è®¿é—®å¤©æ–‡å›¾åƒä¸­çš„ä¿¡æ¯ï¼Œè¡¨ç°å‡ºé«˜åº¦çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¯¥è½¯ä»¶åœ¨æ•™è‚²æ™®åŠå’Œå®£ä¼ é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ï¼Œä½¿å¤æ‚çš„å¤©æ–‡æ¦‚å¿µæ›´åŠ å¼•äººå…¥èƒœï¼Œæ›´å®¹æ˜“è¢«ä¸åŒå—ä¼—æ¥å—ï¼ŒåŒ…æ‹¬ç›²äººå’Œè§†è§‰éšœç¢äººå£«ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†æœªæ¥å‘å±•ï¼Œå¦‚æ•´åˆå¤§å‹è¯­è¨€å’Œè§†è§‰æ¨¡å‹ï¼Œä¸ºè§£é‡Šå¤©æ–‡æ•°æ®åˆ›é€ æ›´åŠ äº’åŠ¨çš„ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09152v1">PDF</a> to be published in the proceedings of â€œVarious Innovative   Technological Experiences - VITE IIâ€ by MemSAIt</p>
<p><strong>Summary</strong><br>æ•°æ®éŸ³é¢‘åŒ–æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è¡¥å……è§†è§‰æ–¹æ³•ï¼Œå¯åŠ©åŠ›æ•°æ®æ„ŸçŸ¥å’Œè§£è¯»ã€‚æ¨å‡ºæ–°å‹å¼€æºè½¯ä»¶herakoiï¼Œè¿ç”¨æœºå™¨å­¦ä¹ å®ç°å®æ—¶å›¾åƒéŸ³é¢‘åŒ–ï¼Œä¸“æ³¨äºå¤©æ–‡æ•°æ®ã€‚è½¯ä»¶é€šè¿‡è¿½è¸ªé€šè¿‡æ‘„åƒå¤´çš„æ‰‹éƒ¨åŠ¨ä½œå¹¶æ˜ å°„å›¾åƒåæ ‡ï¼Œå°†è§†è§‰ç‰¹æ€§è½¬åŒ–ä¸ºå£°éŸ³ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿâ€œå¬â€å›¾åƒã€‚è½¯ä»¶å“åº”è¿…é€Ÿï¼ŒçŸ­æ—¶é—´å†…åŸ¹è®­åï¼Œç”¨æˆ·å°±èƒ½å¿«é€Ÿè·å–å¤©æ–‡å›¾åƒä¿¡æ¯ï¼Œè¡¨ç°å‡ºé«˜å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚åœ¨æ•™è‚²æ™®åŠå’Œå¯¹å¤–å®£ä¼ æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œè®©å¤æ‚çš„å¤©æ–‡æ¦‚å¿µæ›´åŠ å¼•äººå…¥èƒœï¼Œæ˜“äºå—ä¼—ç†è§£ï¼ŒåŒ…æ‹¬ç›²äººå’Œè§†éšœäººå£«ã€‚æœªæ¥è¿˜å°†å¼€å‘å¤§å‹è¯­è¨€å’Œè§†è§‰æ¨¡å‹é›†æˆï¼Œåˆ›é€ æ›´åŠ äº’åŠ¨çš„å¤©æ–‡æ•°æ®è§£è¯»ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®éŸ³é¢‘åŒ–æ˜¯ä¸€ç§æ–°å…´çš„è§†è§‰è¾…åŠ©æ–¹æ³•ï¼Œæœ‰åŠ©äºæé«˜æ•°æ®æ„ŸçŸ¥å’Œè§£è¯»èƒ½åŠ›ã€‚</li>
<li>herakoiè½¯ä»¶æ˜¯ä¸€ç§æ–°å‹å¼€æºå·¥å…·ï¼Œå¯å°†å›¾åƒå®æ—¶è½¬åŒ–ä¸ºå£°éŸ³ã€‚</li>
<li>herakoiè½¯ä»¶ä¸“æ³¨äºå¤©æ–‡æ•°æ®çš„éŸ³é¢‘åŒ–å¤„ç†ã€‚</li>
<li>è½¯ä»¶é€šè¿‡è¿½è¸ªæ‰‹éƒ¨åŠ¨ä½œå¹¶æ˜ å°„å›¾åƒåæ ‡æ¥å®ç°å›¾åƒéŸ³é¢‘åŒ–ã€‚</li>
<li>herakoiè½¯ä»¶å“åº”è¿…é€Ÿï¼Œç”¨æˆ·å¯å¿«é€Ÿè·å–å¤©æ–‡å›¾åƒä¿¡æ¯ã€‚</li>
<li>è½¯ä»¶åœ¨æ•™è‚²æ™®åŠå’Œå¯¹ç›²äººå’Œè§†éšœäººå£«çš„å®£ä¼ æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d1b33b6cd4218ae3bdc20c5fe719f1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b233308c1ef1a977f10f0036faf53bc3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Go-With-the-Flow-Fast-Diffusion-for-Gaussian-Mixture-Models"><a href="#Go-With-the-Flow-Fast-Diffusion-for-Gaussian-Mixture-Models" class="headerlink" title="Go With the Flow: Fast Diffusion for Gaussian Mixture Models"></a>Go With the Flow: Fast Diffusion for Gaussian Mixture Models</h2><p><strong>Authors:George Rapakoulias, Ali Reza Pedram, Panagiotis Tsiotras</strong></p>
<p>Schr&quot;{o}dinger Bridges (SB) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. Furthermore, our method generalizes naturally to more general classes of dynamical systems such as controllable Linear Time-Varying systems that cannot currently be solved using traditional neural SB approaches. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, and various other examples. We also benchmark our approach on an Entropic Optimal Transport (EOT) problem and show that it outperforms state-of-the-art methods in cases where the boundary distributions are mixture models while requiring virtually no training. </p>
<blockquote>
<p>è–›å®šè°”æ¡¥ï¼ˆSchrÃ¶dinger Bridgesï¼Œç®€ç§°SBï¼‰æ˜¯ä¸€ç§æ‰©æ•£è¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™æ—¶é—´å†…å°†ç»™å®šçš„åˆå§‹åˆ†å¸ƒå¼•å¯¼åˆ°å¦ä¸€ä¸ªæœ€ç»ˆåˆ†å¸ƒï¼ŒåŒæ—¶æœ€å°åŒ–é€‚å½“çš„æˆæœ¬å‡½æ•°ã€‚å°½ç®¡æœ€è¿‘åœ¨æ–‡çŒ®ä¸­æå‡ºäº†å¤šç§è®¡ç®—SBçš„æ–¹æ³•ï¼Œä½†å¤§å¤šæ•°è¿™äº›æ–¹æ³•éƒ½éœ€è¦è®¡ç®—æ˜‚è´µçš„è®­ç»ƒæ–¹æ¡ˆï¼Œå³ä½¿å¯¹äºè§£å†³ä½ç»´é—®é¢˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç»„å¯è¡Œç­–ç•¥çš„è§£æå‚æ•°åŒ–ï¼Œä»¥å¼•å¯¼åŠ¨åŠ›ç³»ç»Ÿä»ä¸€ä¸ªé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰åˆ†å¸ƒè½¬å‘å¦ä¸€ä¸ªGMMåˆ†å¸ƒã€‚æˆ‘ä»¬å¹¶æ²¡æœ‰ä¾èµ–æ ‡å‡†çš„éå‡¸ä¼˜åŒ–æŠ€æœ¯ï¼Œè€Œæ˜¯å°†é›†åˆä¸­çš„æœ€ä½³ç­–ç•¥è¿‘ä¼¼ä¸ºä½ç»´çº¿æ€§ç¨‹åºçš„è§£ï¼Œå…¶ç»´åº¦ä¸æ¯ä¸ªæ··åˆä¸­çš„ç»„ä»¶æ•°é‡å‘ˆçº¿æ€§å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è‡ªç„¶åœ°æ¨å¹¿åˆ°äº†æ›´ä¸€èˆ¬çš„åŠ¨åŠ›ç³»ç»Ÿç±»ï¼Œå¦‚ç›®å‰æ— æ³•ç”¨ä¼ ç»Ÿç¥ç»SBæ–¹æ³•è§£å†³çš„å¯æ§çº¿æ€§æ—¶å˜ç³»ç»Ÿã€‚æˆ‘ä»¬é€šè¿‡ä½ç»´åˆ°ä¸­ç»´çš„é—®é¢˜å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æ½œåŠ›ï¼Œä¾‹å¦‚åœ¨è‡ªåŠ¨ç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´ä¸­çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»¥åŠå…¶ä»–å„ç§ç¤ºä¾‹ã€‚æˆ‘ä»¬è¿˜ç”¨ç†µæœ€ä¼˜ä¼ è¾“ï¼ˆEOTï¼‰é—®é¢˜æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¡¨æ˜å®ƒåœ¨è¾¹ç•Œåˆ†å¸ƒä¸ºæ··åˆæ¨¡å‹çš„æƒ…å†µä¸‹ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œè€Œå‡ ä¹ä¸éœ€è¦è¿›è¡Œè®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09059v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰çš„SchrÃ¶dingeræ¡¥ï¼ˆSBï¼‰åˆ†å¸ƒè½¬æ¢æ–¹æ³•ã€‚é€šè¿‡è§£æå‚æ•°åŒ–æ–¹æ³•æ±‚è§£çº¿æ€§è§„åˆ’é—®é¢˜ï¼Œèƒ½åœ¨è¾ƒä½è®¡ç®—æˆæœ¬ä¸‹å®ç°SBçš„è®¡ç®—ï¼Œé€‚ç”¨äºä½ç»´åˆ°ä¸­ç­‰ç»´åº¦çš„åŠ¨æ€ç³»ç»Ÿé—®é¢˜ã€‚è¯¥æ–¹æ³•å¯è‡ªç„¶æ¨å¹¿åˆ°æ›´ä¸€èˆ¬çš„å¯æ§çº¿æ€§æ—¶å˜ç³»ç»Ÿï¼Œåœ¨å›¾åƒç¿»è¯‘ç­‰ä»»åŠ¡ä¸­å±•ç°æ½œåŠ›ï¼Œä¸”åœ¨è¾¹ç•Œåˆ†å¸ƒä¸ºæ··åˆæ¨¡å‹çš„æƒ…å†µä¸‹ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»SchrÃ¶dingeræ¡¥ï¼ˆSBï¼‰æ˜¯ä¸€ç§åˆ†å¸ƒè½¬æ¢çš„æ‰©æ•£è¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™æ—¶é—´å†…å°†åˆå§‹åˆ†å¸ƒå¯¼å‘å¦ä¸€ä¸ªæœ€ç»ˆåˆ†å¸ƒï¼ŒåŒæ—¶æœ€å°åŒ–åˆé€‚çš„æˆæœ¬å‡½æ•°ã€‚ä½†ç°æœ‰çš„è®¡ç®—æ–¹æ³•é€šå¸¸è®¡ç®—é‡å¤§ä¸”æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰çš„è§£æå‚æ•°åŒ–æ–¹æ³•è®¡ç®—SBsï¼Œå¯å®ç°é«˜æ•ˆæ±‚è§£ã€‚å¯¹äºå…·æœ‰å°‘é‡ç»„ä»¶çš„æ··åˆæ¨¡å‹ç‰¹åˆ«æœ‰æ•ˆã€‚æ­¤æ–¹æ³•å…‹æœäº†æ ‡å‡†éå‡¸ä¼˜åŒ–æŠ€æœ¯çš„å›°éš¾ã€‚</li>
<li>é€šè¿‡ä½ç»´çº¿æ€§è§„åˆ’è¿‘ä¼¼æœ€ä¼˜ç­–ç•¥çš„è®¡ç®—è¿‡ç¨‹è¾ƒä¸ºç®€ä¾¿ã€‚é—®é¢˜çš„ç»´åº¦ä¼šéšç€æ¯ä¸ªæ··åˆæ¨¡å‹ä¸­ç»„ä»¶æ•°é‡çš„å¢åŠ è€Œçº¿æ€§å¢é•¿ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºæ›´å¹¿æ³›çš„åŠ¨æ€ç³»ç»Ÿç±»å‹ï¼ŒåŒ…æ‹¬å¯æ§çº¿æ€§æ—¶å˜ç³»ç»Ÿï¼Œæ— æ³•ç”¨ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œSBæ–¹æ³•è§£å†³çš„ç³»ç»Ÿä¹Ÿå¯ä»¥å¤„ç†ã€‚æ­¤æ–¹æ³•å…·æœ‰ä¸€å®šçš„é€šç”¨æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨å®é™…åº”ç”¨å¦‚å›¾åƒåˆ°å›¾åƒçš„æ½œç©ºé—´ç¿»è¯‘ä»¥åŠåŒ…æ‹¬æ›´é«˜ç»´åº¦ä»»åŠ¡ä¸Šçš„æ½œåŠ›å’Œèƒ½åŠ›æœ‰æ‰€ä½“ç°ã€‚åœ¨ä¸åŒç»´åº¦çš„ä»»åŠ¡ä¸­å±•ç¤ºäº†è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03ebcf0730ec2f4d2c69977114f9e79b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b2775b9e0551c5a311d69b0092580b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92248e3ade3836bcec1b9189101e0ffe.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MIT-10M-A-Large-Scale-Parallel-Corpus-of-Multilingual-Image-Translation"><a href="#MIT-10M-A-Large-Scale-Parallel-Corpus-of-Multilingual-Image-Translation" class="headerlink" title="MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation"></a>MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation</h2><p><strong>Authors:Bo Li, Shaolin Zhu, Lijie Wen</strong></p>
<p>Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 840K images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority. </p>
<blockquote>
<p>å›¾åƒç¿»è¯‘ï¼ˆITï¼‰åœ¨å„ä¸ªé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿå®ç°å›¾åƒå†…æ–‡æœ¬å†…å®¹çš„è·¨è¯­è¨€ç¿»è¯‘ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œè´¨é‡æ–¹é¢å­˜åœ¨è¯¸å¤šå±€é™ï¼Œé˜»ç¢äº†ITæ¨¡å‹çš„å¼€å‘ä¸è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MIT-10Mï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€å›¾åƒç¿»è¯‘å¹³è¡Œè¯­æ–™åº“ï¼ŒåŒ…å«è¶…è¿‡1000ä¸‡ä¸ªå›¾åƒæ–‡æœ¬å¯¹ï¼Œè¿™äº›æ•°æ®å‡æ¥æºäºç°å®ä¸–ç•Œï¼Œå¹¶ç»è¿‡äº†ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—å’Œå¤šè¯­è¨€ç¿»è¯‘éªŒè¯ã€‚å®ƒåŒ…å«3ç§å°ºå¯¸ã€28ä¸ªç±»åˆ«çš„84ä¸‡ä¸ªå›¾åƒï¼Œä»»åŠ¡éš¾åº¦åˆ†ä¸º3ä¸ªçº§åˆ«ï¼Œä»¥åŠ14ç§è¯­è¨€çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œç›¸è¾ƒäºç°æœ‰æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬åœ¨MIT-10Mä¸Šè¿›è¡Œäº†å¤§é‡çš„å®éªŒæ¥è¯„ä¼°å’Œè®­ç»ƒæ¨¡å‹ã€‚å®éªŒç»“æœæ¸…æ¥šåœ°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åœ¨è¯„ä¼°æ¨¡å‹åº”å¯¹ç°å®ä¸–ç•Œä¸­å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¤æ‚æ€§çš„å›¾åƒç¿»è¯‘ä»»åŠ¡çš„æ€§èƒ½æ—¶å…·æœ‰æ›´é«˜çš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨MIT-10Mè¿›è¡Œå¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½æé«˜äº†ä¸‰å€ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07147v2">PDF</a> Accepted in COLING 2025</p>
<p><strong>Summary</strong><br>     å›¾åƒç¿»è¯‘ï¼ˆITï¼‰åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯å®ç°å›¾åƒå†…æ–‡æœ¬çš„è·¨è¯­è¨€ç¿»è¯‘ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œè´¨é‡æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œé˜»ç¢äº†ITæ¨¡å‹çš„å¼€å‘å’Œè¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MIT-10Mï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡1000ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹çš„å¤§å‹å¤šè¯­è¨€å›¾åƒç¿»è¯‘å¹³è¡Œè¯­æ–™åº“ï¼Œæºäºç°å®ä¸–ç•Œæ•°æ®ï¼Œå¹¶ç»è¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—å’Œå¤šè¯­è¨€ç¿»è¯‘éªŒè¯ã€‚å®ƒåœ¨å›¾åƒå¤§å°ã€ç±»åˆ«ã€ä»»åŠ¡éš¾åº¦å’Œè¯­è¨€å¤šæ ·æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹è¿›ã€‚å®éªŒè¡¨æ˜ï¼ŒMIT-10Måœ¨åº”å¯¹ç°å®ä¸–ç•Œä¸­çš„å¤æ‚å›¾åƒç¿»è¯‘ä»»åŠ¡æ—¶è¡¨ç°å‡ºæ›´é«˜çš„é€‚åº”æ€§ï¼Œä½¿ç”¨MIT-10Må¾®è°ƒçš„æ¨¡å‹æ€§èƒ½æ˜¯åŸºçº¿æ¨¡å‹çš„ä¸‰å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç¿»è¯‘ï¼ˆITï¼‰åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨è¯­è¨€ç¿»è¯‘æ–¹é¢ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å­˜åœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œè´¨é‡é—®é¢˜ï¼Œé™åˆ¶äº†ITæ¨¡å‹çš„å‘å±•ã€‚</li>
<li>MIT-10Mæ˜¯ä¸€ä¸ªå¤§å‹å¤šè¯­è¨€å›¾åƒç¿»è¯‘å¹³è¡Œè¯­æ–™åº“ï¼ŒåŒ…å«è¶…è¿‡10Må¼ å›¾åƒæ–‡æœ¬å¯¹ï¼Œæºäºç°å®æ•°æ®å¹¶ç»è¿‡ä¸¥æ ¼éªŒè¯ã€‚</li>
<li>MIT-10Måœ¨å›¾åƒå¤§å°ã€ç±»åˆ«ã€ä»»åŠ¡éš¾åº¦å’Œè¯­è¨€æ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå¢å¼ºäº†æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMIT-10Måœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½å’Œå¤„ç†å¤æ‚å›¾åƒç¿»è¯‘ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„é€‚åº”æ€§ã€‚</li>
<li>ä½¿ç”¨MIT-10Må¾®è°ƒçš„æ¨¡å‹æ€§èƒ½æ˜¯åŸºçº¿æ¨¡å‹çš„ä¸‰å€ï¼Œè¯å®äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>MIT-10Mçš„æ¨å‡ºä¸ºITé¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•æä¾›äº†é‡è¦çš„æ•°æ®é›†æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cacae7a47cade8f86b24dc3d57af3ba1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f79ddea5e0335376407887ffb9b7fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06c903befd571e21804bdbe9a878a09b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b22f0081c44a3181d17141f1ff9c9af7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b02f93a7a822cc0ea3809244981ef4f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffad01de36274cabab6343e55be2746b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model"><a href="#MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model" class="headerlink" title="MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model"></a>MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model</h2><p><strong>Authors:Shan Yang</strong></p>
<p>Text-to-image generation models have revolutionized content creation, but diffusion-based vision-language models still face challenges in precisely controlling the shape, appearance, and positional placement of objects in generated images using text guidance alone. Existing global image editing models rely on additional masks or images as guidance to achieve layout control, often requiring retraining of the model. While local object-editing models allow modifications to object shapes, they lack the capability to control object positions. To address these limitations, we propose the Mask-free Training-free Object-Level Layout Control Diffusion Model (MFTF), which provides precise control over object positions without requiring additional masks or images. The MFTF model supports both single-object and multi-object positional adjustments, such as translation and rotation, while enabling simultaneous layout control and object semantic editing. The MFTF model employs a parallel denoising process for both the source and target diffusion models. During this process, attention masks are dynamically generated from the cross-attention layers of the source diffusion model and applied to queries from the self-attention layers to isolate objects. These queries, generated in the source diffusion model, are then adjusted according to the layout control parameters and re-injected into the self-attention layers of the target diffusion model. This approach ensures accurate and precise positional control of objects. Project source code available at <a target="_blank" rel="noopener" href="https://github.com/syang-genai/MFTF">https://github.com/syang-genai/MFTF</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†å†…å®¹åˆ›ä½œçš„æ–¹å¼ï¼Œä½†æ˜¯åŸºäºæ‰©æ•£çš„è§†è¯­è¨€æ¨¡å‹åœ¨ä»…ä½¿ç”¨æ–‡æœ¬æŒ‡å¯¼æ¥ç²¾ç¡®æ§åˆ¶ç”Ÿæˆå›¾åƒä¸­ç‰©ä½“çš„å½¢çŠ¶ã€å¤–è§‚å’Œä½ç½®æ”¾ç½®æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„å…¨å±€å›¾åƒç¼–è¾‘æ¨¡å‹ä¾èµ–äºé¢å¤–çš„è’™ç‰ˆæˆ–å›¾åƒä½œä¸ºæŒ‡å¯¼æ¥å®ç°å¸ƒå±€æ§åˆ¶ï¼Œé€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚è™½ç„¶å±€éƒ¨ç‰©ä½“ç¼–è¾‘æ¨¡å‹å…è®¸ä¿®æ”¹ç‰©ä½“å½¢çŠ¶ï¼Œä½†å®ƒä»¬ç¼ºä¹æ§åˆ¶ç‰©ä½“ä½ç½®çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è’™ç‰ˆè®­ç»ƒçš„å¯¹è±¡çº§å¸ƒå±€æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼ˆMFTFï¼‰ï¼Œè¯¥æ¨¡å‹æ— éœ€é¢å¤–çš„è’™ç‰ˆæˆ–å›¾åƒå³å¯ç²¾ç¡®æ§åˆ¶ç‰©ä½“ä½ç½®ã€‚MFTFæ¨¡å‹æ”¯æŒå•ç‰©ä½“å’Œå¤šç‰©ä½“çš„ä½ç½®è°ƒæ•´ï¼Œå¦‚å¹³ç§»å’Œæ—‹è½¬ï¼ŒåŒæ—¶å®ç°å¸ƒå±€æ§åˆ¶å’Œç‰©ä½“è¯­ä¹‰ç¼–è¾‘ã€‚MFTFæ¨¡å‹å¯¹æºå’Œç›®æ ‡æ‰©æ•£æ¨¡å‹è¿›è¡Œå¹¶è¡Œå»å™ªå¤„ç†ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä»æºæ‰©æ•£æ¨¡å‹çš„äº¤å‰æ³¨æ„å±‚åŠ¨æ€ç”Ÿæˆæ³¨æ„åŠ›è’™ç‰ˆï¼Œå¹¶åº”ç”¨äºè‡ªæˆ‘æ³¨æ„å±‚çš„æŸ¥è¯¢ä»¥éš”ç¦»ç‰©ä½“ã€‚è¿™äº›åœ¨æºæ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆçš„æŸ¥è¯¢ä¼šæ ¹æ®å¸ƒå±€æ§åˆ¶å‚æ•°è¿›è¡Œè°ƒæ•´ï¼Œç„¶åé‡æ–°æ³¨å…¥ç›®æ ‡æ‰©æ•£æ¨¡å‹çš„è‡ªæˆ‘æ³¨æ„å±‚ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç‰©ä½“çš„ç²¾ç¡®ä½ç½®æ§åˆ¶ã€‚é¡¹ç›®æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/syang-genai/MFTF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/syang-genai/MFTFæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01284v2">PDF</a> 8 pages, 7 figures</p>
<p><strong>Summary</strong><br>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹å·²é©æ–°å†…å®¹åˆ›ä½œï¼Œä½†åŸºäºæ‰©æ•£çš„è§†è¯­è¨€æ¨¡å‹åœ¨ç”¨æ–‡æœ¬æŒ‡å¯¼ç”Ÿæˆå›¾åƒæ—¶ï¼Œä»é¢ä¸´ç²¾ç¡®æ§åˆ¶ç‰©ä½“å½¢çŠ¶ã€å¤–è§‚å’Œä½ç½®æ”¾ç½®çš„æŒ‘æˆ˜ã€‚ç°æœ‰å…¨å±€å›¾åƒç¼–è¾‘æ¨¡å‹éœ€é¢å¤–è’™ç‰ˆæˆ–å›¾åƒæŒ‡å¯¼ä»¥å®ç°å¸ƒå±€æ§åˆ¶ï¼Œä¸”å¸¸éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚å±€éƒ¨ç‰©ä½“ç¼–è¾‘æ¨¡å‹è™½å¯ä¿®æ”¹ç‰©ä½“å½¢çŠ¶ï¼Œä½†æ— æ³•æ§åˆ¶ç‰©ä½“ä½ç½®ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºæ— éœ€è’™ç‰ˆè®­ç»ƒçš„Mask-free Training-free Object-Level Layout Control Diffusion Modelï¼ˆMFTFï¼‰ã€‚è¯¥æ¨¡å‹æ— éœ€é¢å¤–è’™ç‰ˆæˆ–å›¾åƒå³å¯ç²¾ç¡®æ§åˆ¶ç‰©ä½“ä½ç½®ï¼Œæ”¯æŒå•ç‰©ä½“å’Œå¤šç‰©ä½“çš„ä½ç½®è°ƒæ•´ï¼Œå¦‚å¹³ç§»å’Œæ—‹è½¬ï¼ŒåŒæ—¶å®ç°å¸ƒå±€æ§åˆ¶å’Œç‰©ä½“è¯­ä¹‰ç¼–è¾‘ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æºå’Œç›®æ ‡æ‰©æ•£æ¨¡å‹çš„å¹¶è¡Œå»å™ªè¿‡ç¨‹ï¼Œé€šè¿‡åŠ¨æ€ç”Ÿæˆæ³¨æ„åŠ›è’™ç‰ˆå¹¶åº”ç”¨äºæŸ¥è¯¢ï¼Œä»¥éš”ç¦»ç‰©ä½“ï¼Œå¹¶æ ¹æ®å¸ƒå±€æ§åˆ¶å‚æ•°è°ƒæ•´æŸ¥è¯¢ï¼Œå†æ³¨å…¥ç›®æ ‡æ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚ï¼Œç¡®ä¿ç‰©ä½“çš„ç²¾ç¡®ä½ç½®æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å†…å®¹åˆ›ä½œé¢†åŸŸå…·æœ‰é©å‘½æ€§å½±å“ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨ç²¾ç¡®æ§åˆ¶ç‰©ä½“å½¢çŠ¶ã€å¤–è§‚å’Œä½ç½®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å…¨å±€å›¾åƒç¼–è¾‘æ¨¡å‹ä¾èµ–é¢å¤–çš„è’™ç‰ˆæˆ–å›¾åƒè¿›è¡Œå¸ƒå±€æ§åˆ¶ï¼Œå¹¶å¸¸éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>å±€éƒ¨ç‰©ä½“ç¼–è¾‘æ¨¡å‹è™½ç„¶å¯ä»¥ä¿®æ”¹ç‰©ä½“å½¢çŠ¶ï¼Œä½†æ— æ³•æ§åˆ¶ç‰©ä½“ä½ç½®ã€‚</li>
<li>Mask-free Training-free Object-Level Layout Control Diffusion Modelï¼ˆMFTFï¼‰æå‡ºè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>MFTFæ¨¡å‹æ— éœ€é¢å¤–è’™ç‰ˆæˆ–å›¾åƒå³å¯å®ç°ç²¾ç¡®çš„å¯¹è±¡ä½ç½®æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f5307dede9c39a5e6238a4553cd509b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92df01ad425ce71e6a3a7789281fc8d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31d932f273200085ffd85d5c55bc2acd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb08341d38f99b43d2cb48d414f401d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-452bad3796827f922621781276fed702.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7024f4fd43f65c0109fc421d30a29356.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="NBBOX-Noisy-Bounding-Box-Improves-Remote-Sensing-Object-Detection"><a href="#NBBOX-Noisy-Bounding-Box-Improves-Remote-Sensing-Object-Detection" class="headerlink" title="NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection"></a>NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection</h2><p><strong>Authors:Yechan Kim, SooYeon Kim, Moongu Jeon</strong></p>
<p>Data augmentation has shown significant advancements in computer vision to improve model performance over the years, particularly in scenarios with limited and insufficient data. Currently, most studies focus on adjusting the image or its features to expand the size, quality, and variety of samples during training in various tasks including object detection. However, we argue that it is necessary to investigate bounding box transformations as a data augmentation technique rather than image-level transformations, especially in aerial imagery due to potentially inconsistent bounding box annotations. Hence, this letter presents a thorough investigation of bounding box transformation in terms of scaling, rotation, and translation for remote sensing object detection. We call this augmentation strategy NBBOX (Noise Injection into Bounding Box). We conduct extensive experiments on DOTA and DIOR-R, both well-known datasets that include a variety of rotated generic objects in aerial images. Experimental results show that our approach significantly improves remote sensing object detection without whistles and bells and it is more time-efficient than other state-of-the-art augmentation strategies. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²å±•ç°å‡ºæ˜¾è‘—è¿›å±•ï¼Œå¤šå¹´æ¥ä¸€ç›´åœ¨æé«˜æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™å’Œä¸è¶³çš„åœºæ™¯ä¸‹ã€‚ç›®å‰ï¼Œå¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨è°ƒæ•´å›¾åƒæˆ–å…¶ç‰¹å¾ä»¥æ‰©å¤§æ ·æœ¬å¤§å°ã€è´¨é‡å’Œå¤šæ ·æ€§ï¼Œç”¨äºåŒ…æ‹¬ç›®æ ‡æ£€æµ‹åœ¨å†…çš„å„ç§ä»»åŠ¡çš„è®­ç»ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰å¿…è¦ç ”ç©¶è¾¹ç•Œæ¡†å˜æ¢ä½œä¸ºä¸€ç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œè€Œä¸æ˜¯å›¾åƒçº§åˆ«çš„å˜æ¢ï¼Œå°¤å…¶æ˜¯åœ¨èˆªç©ºå›¾åƒä¸­ï¼Œå› ä¸ºè¾¹ç•Œæ¡†æ³¨é‡Šå¯èƒ½å­˜åœ¨æ½œåœ¨çš„ä¸ä¸€è‡´æ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡å…¨é¢ç ”ç©¶äº†è¾¹ç•Œæ¡†å˜æ¢åœ¨ç¼©æ”¾ã€æ—‹è½¬å’Œç¿»è¯‘æ–¹é¢çš„åº”ç”¨ï¼Œç”¨äºé¥æ„Ÿç›®æ ‡æ£€æµ‹ã€‚æˆ‘ä»¬å°†è¿™ç§å¢å¼ºç­–ç•¥ç§°ä¸ºNBBOXï¼ˆå™ªå£°æ³¨å…¥è¾¹ç•Œæ¡†ï¼‰ã€‚æˆ‘ä»¬åœ¨DOTAå’ŒDIOR-Rè¿™ä¸¤ä¸ªåŒ…å«èˆªç©ºå›¾åƒä¸­å„ç§æ—‹è½¬é€šç”¨å¯¹è±¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä½¿ç”¨ä»»ä½•èŠ±å“¨æŠ€å·§çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†é¥æ„Ÿç›®æ ‡æ£€æµ‹çš„ç²¾åº¦ï¼Œå¹¶ä¸”æ¯”å…¶ä»–æœ€å…ˆè¿›çš„å¢å¼ºç­–ç•¥æ›´åŠ çœæ—¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09424v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®å¢å¼ºæŠ€æœ¯åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™å’Œä¸è¶³çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæé«˜æ¨¡å‹æ€§èƒ½ã€‚ç›®å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å›¾åƒæˆ–å…¶ç‰¹å¾çš„è°ƒæ•´ä¸Šï¼Œä»¥æ‰©å¤§æ ·æœ¬çš„å¤§å°ã€è´¨é‡å’Œç§ç±»ã€‚æœ¬æ–‡æå‡ºç ”ç©¶è¾¹ç•Œæ¡†è½¬æ¢ä½œä¸ºæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨èˆªç©ºå›¾åƒä¸­ï¼Œå› ä¸ºè¾¹ç•Œæ¡†æ³¨é‡Šå¯èƒ½å­˜åœ¨ä¸ä¸€è‡´çš„æƒ…å†µã€‚æœ¬æ–‡å¯¹è¾¹ç•Œæ¡†è½¬æ¢çš„ç¼©æ”¾ã€æ—‹è½¬å’Œç¿»è¯‘è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œç”¨äºé¥æ„Ÿç›®æ ‡æ£€æµ‹ã€‚æˆ‘ä»¬ç§°è¿™ç§å¢å¼ºç­–ç•¥ä¸ºNBBOXï¼ˆå™ªå£°æ³¨å…¥è¾¹ç•Œæ¡†ï¼‰ã€‚åœ¨DOTAå’ŒDIOR-Rä¸¤ä¸ªåŒ…å«èˆªç©ºå›¾åƒä¸­æ—‹è½¬é€šç”¨å¯¹è±¡çš„å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸å¢åŠ é¢å¤–æ—¶é—´å’Œæˆæœ¬çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†é¥æ„Ÿç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºæŠ€æœ¯åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦æ€§ï¼šç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™å’Œä¸è¶³æ—¶ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å›¾åƒçº§åˆ«çš„æ•°æ®å¢å¼ºä¸Šï¼Œé€šè¿‡è°ƒæ•´å›¾åƒæˆ–å…¶ç‰¹å¾æ¥æ‰©å¤§æ ·æœ¬è§„æ¨¡ã€æé«˜è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>è®ºæ–‡ä¸»å¼ ç ”ç©¶è¾¹ç•Œæ¡†è½¬æ¢ä½œä¸ºæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨èˆªç©ºå›¾åƒä¸­ï¼Œå› ä¸ºå­˜åœ¨è¾¹ç•Œæ¡†æ³¨é‡Šä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>è®ºæ–‡ä»‹ç»äº†NBBOXç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§å°†å™ªå£°æ³¨å…¥è¾¹ç•Œæ¡†çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ç¼©æ”¾ã€æ—‹è½¬å’Œç¿»è¯‘çš„ç ”ç©¶ã€‚</li>
<li>å®éªŒåœ¨DOTAå’ŒDIOR-Rä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œè¿™ä¸¤ä¸ªæ•°æ®é›†åŒ…å«èˆªç©ºå›¾åƒä¸­çš„æ—‹è½¬é€šç”¨å¯¹è±¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒNBBOXç­–ç•¥åœ¨é¥æ„Ÿç›®æ ‡æ£€æµ‹ä¸­å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”æ¯”ä¸€äº›å…ˆè¿›çš„å¢å¼ºç­–ç•¥æ›´åŠ é«˜æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-35715ef4725aac1b951c3eabaf5c5ed3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5f2e95849f13ebe4b4f717da096fc08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6701815eefdbef184100b57582da634.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88fcf71de3685c7515c363ca58222902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-055430be74fa599b9db20f5d1ce67d9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19001d41d9534ed05204d3341507a4da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1957199965f5c39375b00e43dd37ceaf.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AllWeatherNet-Unified-Image-Enhancement-for-Autonomous-Driving-under-Adverse-Weather-and-Lowlight-conditions"><a href="#AllWeatherNet-Unified-Image-Enhancement-for-Autonomous-Driving-under-Adverse-Weather-and-Lowlight-conditions" class="headerlink" title="AllWeatherNet:Unified Image Enhancement for Autonomous Driving under   Adverse Weather and Lowlight-conditions"></a>AllWeatherNet:Unified Image Enhancement for Autonomous Driving under   Adverse Weather and Lowlight-conditions</h2><p><strong>Authors:Chenghao Qian, Mahdi Rezaei, Saeed Anwar, Wenjing Li, Tanveer Hussain, Mohsen Azarmi, Wei Wang</strong></p>
<p>Adverse conditions like snow, rain, nighttime, and fog, pose challenges for autonomous driving perception systems. Existing methods have limited effectiveness in improving essential computer vision tasks, such as semantic segmentation, and often focus on only one specific condition, such as removing rain or translating nighttime images into daytime ones. To address these limitations, we propose a method to improve the visual quality and clarity degraded by such adverse conditions. Our method, AllWeather-Net, utilizes a novel hierarchical architecture to enhance images across all adverse conditions. This architecture incorporates information at three semantic levels: scene, object, and texture, by discriminating patches at each level. Furthermore, we introduce a Scaled Illumination-aware Attention Mechanism (SIAM) that guides the learning towards road elements critical for autonomous driving perception. SIAM exhibits robustness, remaining unaffected by changes in weather conditions or environmental scenes. AllWeather-Net effectively transforms images into normal weather and daytime scenes, demonstrating superior image enhancement results and subsequently enhancing the performance of semantic segmentation, with up to a 5.3% improvement in mIoU in the trained domain. We also show our modelâ€™s generalization ability by applying it to unseen domains without re-training, achieving up to 3.9% mIoU improvement. Code can be accessed at: <a target="_blank" rel="noopener" href="https://github.com/Jumponthemoon/AllWeatherNet">https://github.com/Jumponthemoon/AllWeatherNet</a>. </p>
<blockquote>
<p>æ¶åŠ£æ¡ä»¶ï¼Œå¦‚é›ªã€é›¨ã€å¤œæ™šå’Œé›¾ï¼Œç»™è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿå¸¦æ¥äº†æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨æ”¹å–„é‡è¦çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰æ–¹é¢çš„æ•ˆæœæœ‰é™ï¼Œè€Œä¸”é€šå¸¸åªå…³æ³¨ä¸€ç§ç‰¹å®šæ¡ä»¶ï¼Œä¾‹å¦‚å»é›¨æˆ–å°†å¤œé—´å›¾åƒè½¬æ¢ä¸ºæ—¥é—´å›¾åƒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œç”¨ä»¥æ”¹å–„ç”±è¿™äº›æ¶åŠ£æ¡ä»¶é€ æˆçš„è§†è§‰è´¨é‡å’Œæ¸…æ™°åº¦ä¸‹é™çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒAllWeather-Netï¼Œåˆ©ç”¨ä¸€ç§æ–°å‹åˆ†å±‚æ¶æ„ï¼Œæå‡å„ç§æ¶åŠ£æ¡ä»¶ä¸‹çš„å›¾åƒã€‚è¯¥æ¶æ„é€šè¿‡åŒºåˆ†ä¸‰ä¸ªè¯­ä¹‰çº§åˆ«çš„ä¿¡æ¯ï¼ˆåœºæ™¯ã€ç‰©ä½“å’Œçº¹ç†ï¼‰æ¥å¢å¼ºå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¼©æ”¾ç…§æ˜æ„ŸçŸ¥æ³¨æ„æœºåˆ¶ï¼ˆSIAMï¼‰ï¼Œå¼•å¯¼å­¦ä¹ é¢å‘å¯¹è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥è‡³å…³é‡è¦çš„é“è·¯å…ƒç´ ã€‚SIAMè¡¨ç°å‡ºç¨³å¥æ€§ï¼Œä¸å—å¤©æ°”æ¡ä»¶æˆ–ç¯å¢ƒåœºæ™¯å˜åŒ–çš„å½±å“ã€‚AllWeather-Netèƒ½æœ‰æ•ˆåœ°å°†å›¾åƒè½¬æ¢ä¸ºæ­£å¸¸å¤©æ°”å’Œæ—¥é—´åœºæ™¯ï¼Œæ˜¾ç¤ºå‡ºå“è¶Šçš„å›¾åƒå¢å¼ºæ•ˆæœï¼Œè¿›è€Œæå‡è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ï¼Œåœ¨è®­ç»ƒåŸŸå†…å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰æé«˜5.3%ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å°†åœ¨æœªè§åŸŸçš„åº”ç”¨å±•ç¤ºäº†æˆ‘ä»¬æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯å®ç°é«˜è¾¾3.9%çš„mIoUæ”¹å–„ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Jumponthemoon/AllWeatherNet%E3%80%82">https://github.com/Jumponthemoon/AllWeatherNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02045v2">PDF</a> ICPR 2024, Piero Zamperoni Overall Best Student Paper Award</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åº”å¯¹æ¶åŠ£å¤©æ°”æ¡ä»¶æŒ‘æˆ˜çš„æ–¹æ³•ï¼Œåä¸ºAllWeather-Netã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œé€šè¿‡åœºæ™¯ã€ç‰©ä½“å’Œçº¹ç†ä¸‰ä¸ªè¯­ä¹‰å±‚æ¬¡çš„ä¿¡æ¯å¤„ç†æ¥æ”¹å–„å›¾åƒè´¨é‡ã€‚å¼•å…¥çš„ç¼©æ”¾å…‰ç…§æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSIAMï¼‰èƒ½æŒ‡å¯¼å­¦ä¹ å…³æ³¨å¯¹è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥è‡³å…³é‡è¦çš„é“è·¯å…ƒç´ ã€‚AllWeather-Netå¯å°†å›¾åƒè½¬åŒ–ä¸ºæ­£å¸¸å¤©æ°”å’Œç™½å¤©åœºæ™¯ï¼Œæå‡è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼Œå¹¶åœ¨è®­ç»ƒå’Œæœªè®­ç»ƒé¢†åŸŸå‡è¡¨ç°å‡ºä¼˜å¼‚çš„å›¾åƒå¢å¼ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¶åŠ£æ¡ä»¶å¦‚é›¨é›ªã€å¤œæ™šå’Œé›¾å¯¹è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿæ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ”¹å–„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰çš„æ•ˆæœæœ‰é™ï¼Œé€šå¸¸åªé’ˆå¯¹å•ä¸€æ¡ä»¶ã€‚</li>
<li>AllWeather-Neté‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œå¤„ç†åœºæ™¯ã€ç‰©ä½“å’Œçº¹ç†ä¸‰ä¸ªè¯­ä¹‰å±‚æ¬¡çš„ä¿¡æ¯ï¼Œæ”¹å–„å›¾åƒè´¨é‡ã€‚</li>
<li>å¼•å…¥ç¼©æ”¾å…‰ç…§æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSIAMï¼‰ï¼ŒæŒ‡å¯¼å­¦ä¹ å…³æ³¨è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ä¸­é‡è¦çš„é“è·¯å…ƒç´ ã€‚</li>
<li>AllWeather-Netå¯å°†å›¾åƒè½¬åŒ–ä¸ºæ­£å¸¸å¤©æ°”å’Œç™½å¤©åœºæ™¯ï¼Œæ˜¾è‘—æå‡è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è®­ç»ƒå’Œæœªè®­ç»ƒé¢†åŸŸå‡å±•ç°å‡ºä¼˜å¼‚çš„å›¾åƒå¢å¼ºæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-37a6a0c7542257f1cb0bd78dbca4480e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa51ab85f01d6d89acf7845b81437fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fad563b40dbf08afd94dd936541bd3f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e1cb02f273db28aa3545031e805e6ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33907cb8fad24662194925b0396f2daf.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3cd5b11aadac42b7c5cdf1fbae433acc.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  FocusChat Text-guided Long Video Understanding via Spatiotemporal   Information Filtering
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-a91205a65b64bc5f5dda409bd8f005a2.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  FarExStance Explainable Stance Detection for Farsi
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
