<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2024-12-19  Real-Time Position-Aware View Synthesis from Single-View Input">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b22f0081c44a3181d17141f1ff9c9af7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-19-更新"><a href="#2024-12-19-更新" class="headerlink" title="2024-12-19 更新"></a>2024-12-19 更新</h1><h2 id="Real-Time-Position-Aware-View-Synthesis-from-Single-View-Input"><a href="#Real-Time-Position-Aware-View-Synthesis-from-Single-View-Input" class="headerlink" title="Real-Time Position-Aware View Synthesis from Single-View Input"></a>Real-Time Position-Aware View Synthesis from Single-View Input</h2><p><strong>Authors:Manu Gond, Emin Zerman, Sebastian Knorr, Mårten Sjöström</strong></p>
<p>Recent advancements in view synthesis have significantly enhanced immersive experiences across various computer graphics and multimedia applications, including telepresence, and entertainment. By enabling the generation of new perspectives from a single input view, view synthesis allows users to better perceive and interact with their environment. However, many state-of-the-art methods, while achieving high visual quality, face limitations in real-time performance, which makes them less suitable for live applications where low latency is critical. In this paper, we present a lightweight, position-aware network designed for real-time view synthesis from a single input image and a target camera pose. The proposed framework consists of a Position Aware Embedding, modeled with a multi-layer perceptron, which efficiently maps positional information from the target pose to generate high dimensional feature maps. These feature maps, along with the input image, are fed into a Rendering Network that merges features from dual encoder branches to resolve both high level semantics and low level details, producing a realistic new view of the scene. Experimental results demonstrate that our method achieves superior efficiency and visual quality compared to existing approaches, particularly in handling complex translational movements without explicit geometric operations like warping. This work marks a step toward enabling real-time view synthesis from a single image for live and interactive applications. </p>
<blockquote>
<p>近期视图合成技术的进展在各类计算机图形和多媒体应用中大幅提升了沉浸式体验，包括远程存在和娱乐。视图合成能够通过单个输入视图生成新的视角，使用户更好地感知和与其环境进行交互。然而，许多最先进的方法虽然视觉质量很高，但在实时性能方面存在局限性，这使得它们在低延迟至关重要的实时应用中不太适用。在本文中，我们提出了一种用于实时视图合成的轻量级、位置感知网络，该网络从单个输入图像和目标相机姿态进行视图合成。所提出的框架由一个位置感知嵌入组成，通过多层感知器进行建模，能够高效地将目标姿态的位置信息映射到生成高维特征图。这些特征图与输入图像一起输入到渲染网络中，该网络合并来自双编码器分支的特征以解决高级语义和低级细节问题，从而生成场景的现实新视图。实验结果表明，与现有方法相比，我们的方法在效率和视觉质量方面实现了优越性，特别是在处理复杂的平移运动而无需明确的几何操作（如扭曲）方面。这项工作标志着为实时应用和交互式应用的单图像视图合成迈出了重要一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14005v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着计算机图形学和多媒体应用的快速发展，视图合成技术不断进步，极大地提升了沉浸式体验。本文提出了一种轻量级、位置感知的网络，用于从单张输入图像和目标相机姿态进行实时视图合成。该网络通过位置感知嵌入和多层感知器有效地映射目标姿态的位置信息，生成高维特征图。实验结果表明，该方法与现有方法相比，在效率和视觉质量方面表现优越，尤其擅长处理复杂的平移运动。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视图合成技术增强了沉浸式体验，允许从单一视角生成新的视角。</li>
<li>现有方法虽然视觉质量高，但实时性能有限，不适合低延迟的实时应用。</li>
<li>本文提出了一种轻量级、位置感知的网络，用于实时视图合成。</li>
<li>该网络通过位置感知嵌入和多层感知器映射目标姿态的位置信息。</li>
<li>网络生成高维特征图，与输入图像一起输入渲染网络。</li>
<li>渲染网络合并来自两个编码器分支的特征，解决高级语义和低级细节，生成逼真的新视图。</li>
<li>实验结果表明，该方法在效率和视觉质量方面表现优越，尤其擅长处理复杂的平移运动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-67583bf22a49e384b21fedc92b864fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0400df8e26f50d6717f090237ee97cc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d203e81407b3a31ccf02989ad453419a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Spatial-Brain-Tumor-Concentration-Estimation-for-Individualized-Radiotherapy-Planning"><a href="#Spatial-Brain-Tumor-Concentration-Estimation-for-Individualized-Radiotherapy-Planning" class="headerlink" title="Spatial Brain Tumor Concentration Estimation for Individualized   Radiotherapy Planning"></a>Spatial Brain Tumor Concentration Estimation for Individualized   Radiotherapy Planning</h2><p><strong>Authors:Jonas Weidner, Michal Balcerak, Ivan Ezhov, André Datchev, Laurin Lux, Lucas Zimmerand Daniel Rueckert, Björn Menze, Benedikt Wiestler</strong></p>
<p>Biophysical modeling of brain tumors has emerged as a promising strategy for personalizing radiotherapy planning by estimating the otherwise hidden distribution of tumor cells within the brain. However, many existing state-of-the-art methods are computationally intensive, limiting their widespread translation into clinical practice. In this work, we propose an efficient and direct method that utilizes soft physical constraints to estimate the tumor cell concentration from preoperative MRI of brain tumor patients. Our approach optimizes a 3D tumor concentration field by simultaneously minimizing the difference between the observed MRI and a physically informed loss function. Compared to existing state-of-the-art techniques, our method significantly improves predicting tumor recurrence on two public datasets with a total of 192 patients while maintaining a clinically viable runtime of under one minute - a substantial reduction from the 30 minutes required by the current best approach. Furthermore, we showcase the generalizability of our framework by incorporating additional imaging information and physical constraints, highlighting its potential to translate to various medical diffusion phenomena with imperfect data. </p>
<blockquote>
<p>脑肿瘤的生物物理建模已成为一种有前途的策略，通过估计脑内肿瘤细胞的隐蔽分布来个性化放射治疗计划。然而，许多现有的最先进的方法计算量大，限制了它们在临床实践中的广泛应用。在这项工作中，我们提出了一种高效且直接的方法，利用软物理约束来估计脑肿瘤患者的术前MRI中的肿瘤细胞浓度。我们的方法通过同时最小化观察到的MRI和一个物理信息损失函数之间的差异来优化三维肿瘤浓度场。与现有的最先进技术相比，我们的方法在两个公共数据集上对192名患者的肿瘤复发预测进行了显著改善，同时保持了一分钟以内的临床可行运行时间——这大大减少了当前最佳方法所需的30分钟。此外，我们通过融入额外的成像信息和物理约束来展示我们框架的普遍性，突显其在具有不完美数据的各种医学扩散现象中的翻译潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13811v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种高效且直接的方法，利用软物理约束，通过脑肿瘤患者的术前MRI来估计肿瘤细胞浓度。该方法优化了一个三维肿瘤浓度场，同时最小化观察到的MRI与物理信息损失函数之间的差异。相较于现有的先进技术，该方法在预测肿瘤复发方面表现更优，同时在维持临床可行的运行时间（少于一分钟）的同时，显著减少了计算时间（从当前最佳方法的三十分钟减少到一分钟）。此外，该研究展示了该框架的通用性，可融入额外的成像信息和物理约束，突显其在不完美数据下的医学扩散现象翻译潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种利用软物理约束直接估计脑肿瘤细胞浓度的方法。</li>
<li>通过优化三维肿瘤浓度场，最小化观察到的MRI与物理信息损失函数之间的差异。</li>
<li>方法显著提高了在公共数据集上预测肿瘤复发的准确性。</li>
<li>维持了临床可行的运行时间，显著减少了计算时间。</li>
<li>框架具有通用性，可融入额外的成像信息和物理约束。</li>
<li>该方法有望为临床放射治疗计划提供个性化策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13811">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d2e317c2bec8b6cc279663129a7c5c39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15bbba5136b33a28b9ea8805e4434090.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Automatic-Evaluation-for-Image-Transcreation"><a href="#Towards-Automatic-Evaluation-for-Image-Transcreation" class="headerlink" title="Towards Automatic Evaluation for Image Transcreation"></a>Towards Automatic Evaluation for Image Transcreation</h2><p><strong>Authors:Simran Khanuja, Vivek Iyer, Claire He, Graham Neubig</strong></p>
<p>Beyond conventional paradigms of translating speech and text, recently, there has been interest in automated transcreation of images to facilitate localization of visual content across different cultures. Attempts to define this as a formal Machine Learning (ML) problem have been impeded by the lack of automatic evaluation mechanisms, with previous work relying solely on human evaluation. In this paper, we seek to close this gap by proposing a suite of automatic evaluation metrics inspired by machine translation (MT) metrics, categorized into: a) Object-based, b) Embedding-based, and c) VLM-based. Drawing on theories from translation studies and real-world transcreation practices, we identify three critical dimensions of image transcreation: cultural relevance, semantic equivalence and visual similarity, and design our metrics to evaluate systems along these axes. Our results show that proprietary VLMs best identify cultural relevance and semantic equivalence, while vision-encoder representations are adept at measuring visual similarity. Meta-evaluation across 7 countries shows our metrics agree strongly with human ratings, with average segment-level correlations ranging from 0.55-0.87. Finally, through a discussion of the merits and demerits of each metric, we offer a robust framework for automated image transcreation evaluation, grounded in both theoretical foundations and practical application. Our code can be found here: <a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/automatic-eval-transcreation">https://github.com/simran-khanuja/automatic-eval-transcreation</a> </p>
<blockquote>
<p>在传统语音和文本翻译的范式之外，最近人们对自动创建图像以推动不同文化间的视觉内容本地化产生了兴趣。将这一领域定义为正式的机器学习（ML）问题的尝试受到了缺乏自动评估机制的阻碍，早期的工作完全依赖于人工评估。在本文中，我们试图通过借鉴机器翻译（MT）指标提出一套自动评估指标来填补这一空白，这些指标可分为三类：a）基于对象的，b）基于嵌入的，以及c）基于视觉语言模型（VLM）的。我们从翻译研究理论和现实世界的图像转创实践出发，确定了图像转创的三个关键维度：文化相关性、语义等价性和视觉相似性，并设计了我们的指标来沿着这些轴评估系统。我们的结果表明，专有视觉语言模型在识别文化相关性和语义等价性方面表现最佳，而视觉编码器表示在衡量视觉相似性方面很在行。在7个国家的元评估显示，我们的指标与人类评分高度一致，平均分段相关性在0.55-0.87之间。最后，通过对每种指标的优缺点进行讨论，我们提供了一个既基于理论也适用于实际应用的稳健框架，用于自动化图像转创评估。我们的代码可以在这里找到：<a target="_blank" rel="noopener" href="https://github.com/simran-khanuja/automatic-eval-transcreation">https://github.com/simran-khanuja/automatic-eval-transcreation</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13717v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文旨在解决图像转创（Image Transcreation）的自动评估问题，提出了一系列基于机器翻译（MT）的自动评估指标，包括对象基础、嵌入基础和VLM基础三种类型。这些指标旨在从文化相关性、语义等价性和视觉相似性三个关键维度评估图像转创系统。实验结果表明，专有VLMs在识别文化相关性和语义等价性方面表现最佳，而视觉编码器表示在测量视觉相似性方面表现良好。此外，对七个国家的元评估显示，本文提出的指标与人类评分高度一致，平均分段相关性在0.55至0.87之间。最后，本文讨论了各项指标的优缺点，为基于理论和实践的自动化图像转创评估提供了稳健框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像转创（Image Transcreation）是翻译领域的新发展方向，旨在实现不同文化背景下视觉内容的本地化。</li>
<li>现有研究中缺乏图像转创的自动评估机制，因此本文提出了一系列基于机器翻译的自动评估指标。</li>
<li>这些指标从文化相关性、语义等价性和视觉相似性三个关键维度对图像转创系统进行评估。</li>
<li>实验表明，专有VLMs在识别文化相关性和语义等价性方面表现最佳，视觉编码器擅长测量视觉相似性。</li>
<li>跨七国的元评估证实，本文提出的指标与人类评分高度一致。</li>
<li>本文详细讨论了各项指标的优缺点，为未来的研究提供了方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13717">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-26d343d14aacd616d62cf66b5927d90e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b38550d58440cff37dfe8d4301e20261.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ba0132b4f1076ffac25ff78434e5967.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34244154a057a06450da5c57ab3ffc84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d974c5c222fff2634c34fba597b368be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81d305b3326d9d77a5220a93c85cc686.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Make-Imagination-Clearer-Stable-Diffusion-based-Visual-Imagination-for-Multimodal-Machine-Translation"><a href="#Make-Imagination-Clearer-Stable-Diffusion-based-Visual-Imagination-for-Multimodal-Machine-Translation" class="headerlink" title="Make Imagination Clearer! Stable Diffusion-based Visual Imagination for   Multimodal Machine Translation"></a>Make Imagination Clearer! Stable Diffusion-based Visual Imagination for   Multimodal Machine Translation</h2><p><strong>Authors:Andong Chen, Yuchen Song, Kehai Chen, Muyun Yang, Tiejun Zhao, Min Zhang</strong></p>
<p>Visual information has been introduced for enhancing machine translation (MT), and its effectiveness heavily relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we introduce a stable diffusion-based imagination network into a multimodal large language model (MLLM) to explicitly generate an image for each source sentence, thereby advancing the multimodel MT. Particularly, we build heuristic human feedback with reinforcement learning to ensure the consistency of the generated image with the source sentence without the supervision of image annotation, which breaks the bottleneck of using visual information in MT. Furthermore, the proposed method enables imaginative visual information to be integrated into large-scale text-only MT in addition to multimodal MT. Experimental results show that our model significantly outperforms existing multimodal MT and text-only MT, especially achieving an average improvement of more than 14 BLEU points on Multi30K multimodal MT benchmarks. </p>
<blockquote>
<p>将视觉信息引入机器翻译（MT）以增强其功能，其有效性在很大程度上依赖于大量带有手动图像注释的双语平行句子对。在本文中，我们将基于稳定扩散的想象网络引入多模态大型语言模型（MLLM），以明确为每句源句生成相应的图像，从而促进多模态机器翻译的发展。特别地，我们利用强化学习构建启发式人类反馈，确保生成的图像与源句的一致性，无需图像注释的监督，从而突破了机器翻译中使用视觉信息的瓶颈。此外，该方法除了多模态MT之外，还可以将想象性视觉信息集成到大规模纯文本MT中。实验结果表明，我们的模型在现有的多模态MT和纯文本MT上表现优越，特别是在Multi30K多模态MT基准测试上平均提高了超过14个BLEU点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12627v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>本文引入了一种基于稳定扩散的想象网络到多模态大型语言模型中，能够针对每个源句子生成对应的图像，从而推进多模态机器翻译的发展。通过结合启发式人工反馈和强化学习，确保生成图像与源句子的一致性，无需手动图像注释的监督，打破了机器翻译中使用视觉信息的瓶颈。此外，该方法不仅能够应用于多模态机器翻译，还能够将想象性的视觉信息集成到大规模的纯文本机器翻译中。实验结果表明，该模型在多模态机器翻译方面显著优于现有技术，特别是在Multi30K多模态机器翻译基准测试上平均提高了超过14个BLEU点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入稳定扩散基于想象网络的模型以增强机器翻译。</li>
<li>通过生成与源句子相对应的图片推进多模态机器翻译的发展。</li>
<li>结合启发式人工反馈和强化学习，确保生成图像与源句子的一致性。</li>
<li>该方法无需手动图像注释的监督，打破使用视觉信息在机器翻译中的瓶颈。</li>
<li>想象性的视觉信息可集成到大规模的纯文本机器翻译中。</li>
<li>模型在多模态机器翻译方面表现优异，特别是Multi30K基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9cbea8ac73a1bf1836f7ad23bf59ae37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da617ccd25a73f3719a11187f1ec454b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac39c89b14e8b743c89e1578ca0993f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93e1a4e443505e1bd054d69891d6aaeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e037042959c1f7882d1a4453ba9a04e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89385e803b349f34695373524122818b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LineArt-A-Knowledge-guided-Training-free-High-quality-Appearance-Transfer-for-Design-Drawing-with-Diffusion-Model"><a href="#LineArt-A-Knowledge-guided-Training-free-High-quality-Appearance-Transfer-for-Design-Drawing-with-Diffusion-Model" class="headerlink" title="LineArt: A Knowledge-guided Training-free High-quality Appearance   Transfer for Design Drawing with Diffusion Model"></a>LineArt: A Knowledge-guided Training-free High-quality Appearance   Transfer for Design Drawing with Diffusion Model</h2><p><strong>Authors:Xi Wang, Hongzhen Li, Heng Fang, Yichen Peng, Haoran Xie, Xi Yang, Chuntao Li</strong></p>
<p>Image rendering from line drawings is vital in design and image generation technologies reduce costs, yet professional line drawings demand preserving complex details. Text prompts struggle with accuracy, and image translation struggles with consistency and fine-grained control. We present LineArt, a framework that transfers complex appearance onto detailed design drawings, facilitating design and artistic creation. It generates high-fidelity appearance while preserving structural accuracy by simulating hierarchical visual cognition and integrating human artistic experience to guide the diffusion process. LineArt overcomes the limitations of current methods in terms of difficulty in fine-grained control and style degradation in design drawings. It requires no precise 3D modeling, physical property specs, or network training, making it more convenient for design tasks. LineArt consists of two stages: a multi-frequency lines fusion module to supplement the input design drawing with detailed structural information and a two-part painting process for Base Layer Shaping and Surface Layer Coloring. We also present a new design drawing dataset ProLines for evaluation. The experiments show that LineArt performs better in accuracy, realism, and material precision compared to SOTAs. </p>
<blockquote>
<p>从线条图像中呈现图像在设计及图像生成技术中至关重要，该技术可以降低生产成本，然而专业的线条图像需要保留复杂的细节。文本提示在准确性方面存在困难，图像翻译在一致性和精细控制方面也存在挑战。我们提出了LineArt框架，该框架能够将复杂的外观转移到详细的设计图纸上，促进设计和艺术创作。它通过模拟分层视觉认知并整合人类艺术经验来指导扩散过程，从而在保留结构准确性的同时生成高保真外观。LineArt克服了当前方法在精细控制方面的难度和设计图纸中的风格退化等局限性。它不需要精确的三维建模、物理属性规格或网络训练，为设计任务提供了便利。LineArt由两个阶段组成：一个多频线条融合模块，用于为输入的设计图纸补充详细的结构信息，以及分为两部分的上色过程，即基础层塑形和表层着色。我们还提供了一个新的设计图纸数据集ProLines，用于评估。实验表明，与最新技术相比，LineArt在准确性、真实感和材料精度方面表现更佳。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11519v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://meaoxixi.github.io/LineArt/">https://meaoxixi.github.io/LineArt/</a></p>
<p><strong>Summary</strong></p>
<p>线画图在设计和图像生成技术中扮演着至关重要的角色，但专业线画图需要保留复杂的细节。当前技术面临文本提示准确性不足和图像翻译一致性及精细控制方面的问题。为此，我们提出了LineArt框架，它能将复杂的外观转移到详细的设计图纸上，促进了设计和艺术创作。该框架通过模拟分层视觉认知并整合人类艺术经验来指导扩散过程，能在保持结构准确性的同时生成高保真外观，克服了现有方法的缺点。LineArt无需精确的3D建模、物理属性规格或网络训练，更适合用于设计任务。它由多频线条融合模块和两层绘画流程组成，分别为基础层塑形和表层着色。我们还推出了新的设计绘图数据集ProLines进行评估。实验表明，LineArt在准确性、逼真度和材料精度方面表现优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>线画图在设计及图像生成技术中至关重要，且需保留复杂细节。</li>
<li>当前技术面临文本提示准确性及图像翻译一致性、精细控制难题。</li>
<li>LineArt框架能通过模拟分层视觉认知指导扩散过程，生成高保真外观。</li>
<li>LineArt在保持结构准确性的同时，克服了现有方法的缺点。</li>
<li>LineArt无需精确的3D建模、物理属性规格或网络训练，适用于设计任务。</li>
<li>LineArt由多频线条融合模块和两层绘画流程构成，包括基础层塑形和表层着色。</li>
<li>新推出的设计绘图数据集ProLines为LineArt提供了评估标准，实验表明其表现优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11519">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a992e876f22e1c9acabf1067aac3496.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c2230ed5d056c918b9774b2e3d2134c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05730bc094d405a90dc2cb5937cfcf18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df18f7406e109271469122244655562f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6a78ef0a86c20ff1d8e72acfc6cb5e3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SweetTokenizer-Semantic-Aware-Spatial-Temporal-Tokenizer-for-Compact-Visual-Discretization"><a href="#SweetTokenizer-Semantic-Aware-Spatial-Temporal-Tokenizer-for-Compact-Visual-Discretization" class="headerlink" title="SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact   Visual Discretization"></a>SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact   Visual Discretization</h2><p><strong>Authors:Zhentao Tan, Ben Xue, Jian Jia, Junhao Wang, Wencai Ye, Shaoyun Shi, Mingjie Sun, Wenjin Wu, Quan Chen, Peng Jiang</strong></p>
<p>This paper presents the \textbf{S}emantic-a\textbf{W}ar\textbf{E} spatial-t\textbf{E}mporal \textbf{T}okenizer (SweetTokenizer), a compact yet effective discretization approach for vision data. Our goal is to boost tokenizers’ compression ratio while maintaining reconstruction fidelity in the VQ-VAE paradigm. Firstly, to obtain compact latent representations, we decouple images or videos into spatial-temporal dimensions, translating visual information into learnable querying spatial and temporal tokens through a \textbf{C}ross-attention \textbf{Q}uery \textbf{A}uto\textbf{E}ncoder (CQAE). Secondly, to complement visual information during compression, we quantize these tokens via a specialized codebook derived from off-the-shelf LLM embeddings to leverage the rich semantics from language modality. Finally, to enhance training stability and convergence, we also introduce a curriculum learning strategy, which proves critical for effective discrete visual representation learning. SweetTokenizer achieves comparable video reconstruction fidelity with only \textbf{25%} of the tokens used in previous state-of-the-art video tokenizers, and boost video generation results by \textbf{32.9%} w.r.t gFVD. When using the same token number, we significantly improves video and image reconstruction results by \textbf{57.1%} w.r.t rFVD on UCF-101 and \textbf{37.2%} w.r.t rFID on ImageNet-1K. Additionally, the compressed tokens are imbued with semantic information, enabling few-shot recognition capabilities powered by LLMs in downstream applications. </p>
<blockquote>
<p>本文介绍了Semantic-Aware空间时序令牌化器（SweetTokenizer），这是一种紧凑且有效的针对视觉数据的离散化方法。我们的目标是在VQ-VAE范式中提高令矢量化器的压缩率，同时保持重建保真度。首先，为了获取紧凑的潜在表示，我们将图像或视频解耦为空间时间维度，通过跨注意力查询自动编码器（CQAE）将视觉信息转换为可学习的查询空间和时间令牌。其次，为了在压缩过程中补充视觉信息，我们通过从现成的LLM嵌入中派生出的专用代码本对这些令牌进行量化，以利用语言模态的丰富语义。最后，为了提高训练稳定性和收敛性，我们还引入了一种课程学习策略，这对于有效的离散视觉表示学习至关重要。SweetTokenizer在仅使用现有最先技术视频令牌化器中使用的25％令牌的情况下，实现了相当的视频重建保真度，并将视频生成结果提高了32.9％，以GFVD为准。在使用相同令牌数量的情况下，我们在UCF-101上的视频重建结果和ImageNet-1K上的图像重建结果分别提高了57.1％和37.2％，以rFVD和rFID为准。此外，压缩的令牌被赋予了语义信息，可在下游应用中借助LLM实现少量的识别功能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10443v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于语义战争编码（SweetTokenizer）的紧凑且有效的离散化方法，用于处理视觉数据。该方法旨在提高VQ-VAE框架中的压缩比并保持重建保真度。它通过解耦图像或视频的空间时间维度获得紧凑的潜在表示，并通过跨注意力查询自动编码器（CQAE）将视觉信息转换为可学习的查询空间和时间令牌。此外，通过利用从语言模态中获得的丰富语义信息，通过专用代码本对这些令牌进行量化。最后，引入课程学习策略以增强训练和收敛。SweetTokenizer实现了与现有技术相比的视频重建保真度，并提高了视频生成结果。此外，压缩的令牌具有语义信息，可在下游应用中实现由大型语言模型驱动的少样本识别功能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SweetTokenizer是一种针对视觉数据的紧凑且有效的离散化方法。</li>
<li>它通过解耦图像或视频的空间时间维度来提高压缩比并保持重建保真度。</li>
<li>使用跨注意力查询自动编码器（CQAE）将视觉信息转换为令牌。</li>
<li>通过使用专用代码本和语言模态的丰富语义信息来量化这些令牌。</li>
<li>引入课程学习策略以增强训练和收敛。</li>
<li>SweetTokenizer实现了高视频重建保真度和改进的视频生成结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10443">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e6ad1d061dbaae5e46990c4747c2c206.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b2e7c9a443cb65dec8a37331e96472c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aa1f3c554aa1a9d2405ee842ccba949.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-556305bf2235b5cf6b1c193a1264aeb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79e0fb570c18e1e18fc4aebe875771de.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="herakoi-a-sonification-experiment-for-astronomical-data"><a href="#herakoi-a-sonification-experiment-for-astronomical-data" class="headerlink" title="herakoi: a sonification experiment for astronomical data"></a>herakoi: a sonification experiment for astronomical data</h2><p><strong>Authors:Michele Ginolfi, Luca Di Mascolo, Anita Zanella</strong></p>
<p>Recent research is revealing data-sonification as a promising complementary approach to vision, benefiting both data perception and interpretation. We present herakoi, a novel open-source software that uses machine learning to allow real-time image sonification, with a focus on astronomical data. By tracking hand movements via a webcam and mapping them to image coordinates, herakoi translates visual properties into sound, enabling users to “hear” images. Its swift responsiveness allows users to access information in astronomical images with short training, demonstrating high reliability and effectiveness. The software has shown promise in educational and outreach settings, making complex astronomical concepts more engaging and accessible to diverse audiences, including blind and visually impaired individuals. We also discuss future developments, such as the integration of large language and vision models to create a more interactive experience in interpreting astronomical data. </p>
<blockquote>
<p>最近的研究表明，数据可视化作为一种有前景的补充方法，有益于数据感知和解释。我们推出了herakoi，这是一款新型开源软件，利用机器学习实现实时图像可视化，重点处理天文数据。herakoi通过追踪通过网络摄像头的手部动作并将其映射到图像坐标，将视觉属性转换为声音，使用户能够“听到”图像。其快速响应能力使用户在短暂训练后就能访问天文图像中的信息，表现出高度的可靠性和有效性。该软件在教育普及和宣传领域展现出潜力，使复杂的天文概念更加引人入胜，更容易被不同受众接受，包括盲人和视觉障碍人士。我们还讨论了未来发展，如整合大型语言和视觉模型，为解释天文数据创造更加互动的体验。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09152v1">PDF</a> to be published in the proceedings of “Various Innovative   Technological Experiences - VITE II” by MemSAIt</p>
<p><strong>Summary</strong><br>数据音频化是一种有前景的补充视觉方法，可助力数据感知和解读。推出新型开源软件herakoi，运用机器学习实现实时图像音频化，专注于天文数据。软件通过追踪通过摄像头的手部动作并映射图像坐标，将视觉特性转化为声音，使用户能够“听”图像。软件响应迅速，短时间内培训后，用户就能快速获取天文图像信息，表现出高可靠性和有效性。在教育普及和对外宣传方面表现出巨大潜力，让复杂的天文概念更加引人入胜，易于受众理解，包括盲人和视障人士。未来还将开发大型语言和视觉模型集成，创造更加互动的天文数据解读体验。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据音频化是一种新兴的视觉辅助方法，有助于提高数据感知和解读能力。</li>
<li>herakoi软件是一种新型开源工具，可将图像实时转化为声音。</li>
<li>herakoi软件专注于天文数据的音频化处理。</li>
<li>软件通过追踪手部动作并映射图像坐标来实现图像音频化。</li>
<li>herakoi软件响应迅速，用户可快速获取天文图像信息。</li>
<li>软件在教育普及和对盲人和视障人士的宣传方面具有巨大潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09152">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d1b33b6cd4218ae3bdc20c5fe719f1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b233308c1ef1a977f10f0036faf53bc3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Go-With-the-Flow-Fast-Diffusion-for-Gaussian-Mixture-Models"><a href="#Go-With-the-Flow-Fast-Diffusion-for-Gaussian-Mixture-Models" class="headerlink" title="Go With the Flow: Fast Diffusion for Gaussian Mixture Models"></a>Go With the Flow: Fast Diffusion for Gaussian Mixture Models</h2><p><strong>Authors:George Rapakoulias, Ali Reza Pedram, Panagiotis Tsiotras</strong></p>
<p>Schr&quot;{o}dinger Bridges (SB) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. Furthermore, our method generalizes naturally to more general classes of dynamical systems such as controllable Linear Time-Varying systems that cannot currently be solved using traditional neural SB approaches. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, and various other examples. We also benchmark our approach on an Entropic Optimal Transport (EOT) problem and show that it outperforms state-of-the-art methods in cases where the boundary distributions are mixture models while requiring virtually no training. </p>
<blockquote>
<p>薛定谔桥（Schrödinger Bridges，简称SB）是一种扩散过程，能够在有限时间内将给定的初始分布引导到另一个最终分布，同时最小化适当的成本函数。尽管最近在文献中提出了多种计算SB的方法，但大多数这些方法都需要计算昂贵的训练方案，即使对于解决低维问题也是如此。在这项工作中，我们提出了一组可行策略的解析参数化，以引导动力系统从一个高斯混合模型（GMM）分布转向另一个GMM分布。我们并没有依赖标准的非凸优化技术，而是将集合中的最佳策略近似为低维线性程序的解，其维度与每个混合中的组件数量呈线性关系。此外，我们的方法自然地推广到了更一般的动力系统类，如目前无法用传统神经SB方法解决的可控线性时变系统。我们通过低维到中维的问题展示了该方法的潜力，例如在自动编码器的潜在空间中的图像到图像翻译以及其他各种示例。我们还用熵最优传输（EOT）问题来评估我们的方法，并表明它在边界分布为混合模型的情况下优于现有技术，而几乎不需要进行训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09059v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于高斯混合模型（GMM）的Schrödinger桥（SB）分布转换方法。通过解析参数化方法求解线性规划问题，能在较低计算成本下实现SB的计算，适用于低维到中等维度的动态系统问题。该方法可自然推广到更一般的可控线性时变系统，在图像翻译等任务中展现潜力，且在边界分布为混合模型的情况下优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍Schrödinger桥（SB）是一种分布转换的扩散过程，能够在有限时间内将初始分布导向另一个最终分布，同时最小化合适的成本函数。但现有的计算方法通常计算量大且成本高昂。</li>
<li>提出了一种基于高斯混合模型（GMM）的解析参数化方法计算SBs，可实现高效求解。对于具有少量组件的混合模型特别有效。此方法克服了标准非凸优化技术的困难。</li>
<li>通过低维线性规划近似最优策略的计算过程较为简便。问题的维度会随着每个混合模型中组件数量的增加而线性增长。</li>
<li>该方法适用于更广泛的动态系统类型，包括可控线性时变系统，无法用传统的神经网络SB方法解决的系统也可以处理。此方法具有一定的通用性。</li>
<li>方法在实际应用如图像到图像的潜空间翻译以及包括更高维度任务上的潜力和能力有所体现。在不同维度的任务中展示了良好的性能表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09059">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-03ebcf0730ec2f4d2c69977114f9e79b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b2775b9e0551c5a311d69b0092580b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92248e3ade3836bcec1b9189101e0ffe.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MIT-10M-A-Large-Scale-Parallel-Corpus-of-Multilingual-Image-Translation"><a href="#MIT-10M-A-Large-Scale-Parallel-Corpus-of-Multilingual-Image-Translation" class="headerlink" title="MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation"></a>MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation</h2><p><strong>Authors:Bo Li, Shaolin Zhu, Lijie Wen</strong></p>
<p>Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 840K images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority. </p>
<blockquote>
<p>图像翻译（IT）在各个领域具有巨大的潜力，能够实现图像内文本内容的跨语言翻译。然而，现有数据集在规模、多样性和质量方面存在诸多局限，阻碍了IT模型的开发与评估。为了解决这个问题，我们推出了MIT-10M，这是一个大规模的多语言图像翻译平行语料库，包含超过1000万个图像文本对，这些数据均来源于现实世界，并经过了严格的数据清洗和多语言翻译验证。它包含3种尺寸、28个类别的84万个图像，任务难度分为3个级别，以及14种语言的图像文本对，相较于现有数据集，这是一个显著的改进。我们在MIT-10M上进行了大量的实验来评估和训练模型。实验结果清楚地表明，我们的数据集在评估模型应对现实世界中具有挑战性和复杂性的图像翻译任务的性能时具有更高的适应性。此外，与基线模型相比，使用MIT-10M进行微调后的模型性能提高了三倍，进一步证明了其优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07147v2">PDF</a> Accepted in COLING 2025</p>
<p><strong>Summary</strong><br>     图像翻译（IT）在多个领域具有巨大潜力，可实现图像内文本的跨语言翻译。然而，现有数据集在规模、多样性和质量方面存在局限性，阻碍了IT模型的开发和评估。为此，我们推出了MIT-10M，这是一个包含超过1000万张图像文本对的大型多语言图像翻译平行语料库，源于现实世界数据，并经过严格的数据清洗和多语言翻译验证。它在图像大小、类别、任务难度和语言多样性方面都有显著改进。实验表明，MIT-10M在应对现实世界中的复杂图像翻译任务时表现出更高的适应性，使用MIT-10M微调的模型性能是基线模型的三倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像翻译（IT）在多个领域具有巨大的应用潜力，特别是在跨语言翻译方面。</li>
<li>现有数据集存在规模、多样性和质量问题，限制了IT模型的发展。</li>
<li>MIT-10M是一个大型多语言图像翻译平行语料库，包含超过10M张图像文本对，源于现实数据并经过严格验证。</li>
<li>MIT-10M在图像大小、类别、任务难度和语言方面都有显著改进，增强了数据集的多样性。</li>
<li>实验表明，MIT-10M在评估模型性能和处理复杂图像翻译任务方面表现出更高的适应性。</li>
<li>使用MIT-10M微调的模型性能是基线模型的三倍，证实了其优越性。</li>
<li>MIT-10M的推出为IT领域的研究和发展提供了重要的数据集支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07147">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cacae7a47cade8f86b24dc3d57af3ba1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f79ddea5e0335376407887ffb9b7fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06c903befd571e21804bdbe9a878a09b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b22f0081c44a3181d17141f1ff9c9af7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b02f93a7a822cc0ea3809244981ef4f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffad01de36274cabab6343e55be2746b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model"><a href="#MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model" class="headerlink" title="MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model"></a>MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model</h2><p><strong>Authors:Shan Yang</strong></p>
<p>Text-to-image generation models have revolutionized content creation, but diffusion-based vision-language models still face challenges in precisely controlling the shape, appearance, and positional placement of objects in generated images using text guidance alone. Existing global image editing models rely on additional masks or images as guidance to achieve layout control, often requiring retraining of the model. While local object-editing models allow modifications to object shapes, they lack the capability to control object positions. To address these limitations, we propose the Mask-free Training-free Object-Level Layout Control Diffusion Model (MFTF), which provides precise control over object positions without requiring additional masks or images. The MFTF model supports both single-object and multi-object positional adjustments, such as translation and rotation, while enabling simultaneous layout control and object semantic editing. The MFTF model employs a parallel denoising process for both the source and target diffusion models. During this process, attention masks are dynamically generated from the cross-attention layers of the source diffusion model and applied to queries from the self-attention layers to isolate objects. These queries, generated in the source diffusion model, are then adjusted according to the layout control parameters and re-injected into the self-attention layers of the target diffusion model. This approach ensures accurate and precise positional control of objects. Project source code available at <a target="_blank" rel="noopener" href="https://github.com/syang-genai/MFTF">https://github.com/syang-genai/MFTF</a>. </p>
<blockquote>
<p>文本到图像生成模型已经彻底改变了内容创作的方式，但是基于扩散的视语言模型在仅使用文本指导来精确控制生成图像中物体的形状、外观和位置放置方面仍然面临挑战。现有的全局图像编辑模型依赖于额外的蒙版或图像作为指导来实现布局控制，通常需要重新训练模型。虽然局部物体编辑模型允许修改物体形状，但它们缺乏控制物体位置的能力。为了解决这些限制，我们提出了无需蒙版训练的对象级布局控制扩散模型（MFTF），该模型无需额外的蒙版或图像即可精确控制物体位置。MFTF模型支持单物体和多物体的位置调整，如平移和旋转，同时实现布局控制和物体语义编辑。MFTF模型对源和目标扩散模型进行并行去噪处理。在此过程中，从源扩散模型的交叉注意层动态生成注意力蒙版，并应用于自我注意层的查询以隔离物体。这些在源扩散模型中生成的查询会根据布局控制参数进行调整，然后重新注入目标扩散模型的自我注意层。这种方法确保了物体的精确位置控制。项目源代码可在<a target="_blank" rel="noopener" href="https://github.com/syang-genai/MFTF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/syang-genai/MFTF找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01284v2">PDF</a> 8 pages, 7 figures</p>
<p><strong>Summary</strong><br>文本转图像生成模型已革新内容创作，但基于扩散的视语言模型在用文本指导生成图像时，仍面临精确控制物体形状、外观和位置放置的挑战。现有全局图像编辑模型需额外蒙版或图像指导以实现布局控制，且常需重新训练模型。局部物体编辑模型虽可修改物体形状，但无法控制物体位置。为解决这些局限，我们提出无需蒙版训练的Mask-free Training-free Object-Level Layout Control Diffusion Model（MFTF）。该模型无需额外蒙版或图像即可精确控制物体位置，支持单物体和多物体的位置调整，如平移和旋转，同时实现布局控制和物体语义编辑。该模型采用源和目标扩散模型的并行去噪过程，通过动态生成注意力蒙版并应用于查询，以隔离物体，并根据布局控制参数调整查询，再注入目标扩散模型的自注意力层，确保物体的精确位置控制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本转图像生成模型在内容创作领域具有革命性影响。</li>
<li>现有扩散模型在精确控制物体形状、外观和位置方面存在挑战。</li>
<li>全局图像编辑模型依赖额外的蒙版或图像进行布局控制，并常需重新训练模型。</li>
<li>局部物体编辑模型虽然可以修改物体形状，但无法控制物体位置。</li>
<li>Mask-free Training-free Object-Level Layout Control Diffusion Model（MFTF）提出解决上述问题。</li>
<li>MFTF模型无需额外蒙版或图像即可实现精确的对象位置控制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01284">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7f5307dede9c39a5e6238a4553cd509b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92df01ad425ce71e6a3a7789281fc8d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31d932f273200085ffd85d5c55bc2acd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb08341d38f99b43d2cb48d414f401d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-452bad3796827f922621781276fed702.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7024f4fd43f65c0109fc421d30a29356.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="NBBOX-Noisy-Bounding-Box-Improves-Remote-Sensing-Object-Detection"><a href="#NBBOX-Noisy-Bounding-Box-Improves-Remote-Sensing-Object-Detection" class="headerlink" title="NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection"></a>NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection</h2><p><strong>Authors:Yechan Kim, SooYeon Kim, Moongu Jeon</strong></p>
<p>Data augmentation has shown significant advancements in computer vision to improve model performance over the years, particularly in scenarios with limited and insufficient data. Currently, most studies focus on adjusting the image or its features to expand the size, quality, and variety of samples during training in various tasks including object detection. However, we argue that it is necessary to investigate bounding box transformations as a data augmentation technique rather than image-level transformations, especially in aerial imagery due to potentially inconsistent bounding box annotations. Hence, this letter presents a thorough investigation of bounding box transformation in terms of scaling, rotation, and translation for remote sensing object detection. We call this augmentation strategy NBBOX (Noise Injection into Bounding Box). We conduct extensive experiments on DOTA and DIOR-R, both well-known datasets that include a variety of rotated generic objects in aerial images. Experimental results show that our approach significantly improves remote sensing object detection without whistles and bells and it is more time-efficient than other state-of-the-art augmentation strategies. </p>
<blockquote>
<p>数据增强在计算机视觉领域已展现出显著进展，多年来一直在提高模型性能，特别是在数据有限和不足的场景下。目前，大多数研究集中在调整图像或其特征以扩大样本大小、质量和多样性，用于包括目标检测在内的各种任务的训练。然而，我们认为有必要研究边界框变换作为一种数据增强技术，而不是图像级别的变换，尤其是在航空图像中，因为边界框注释可能存在潜在的不一致性。因此，本文全面研究了边界框变换在缩放、旋转和翻译方面的应用，用于遥感目标检测。我们将这种增强策略称为NBBOX（噪声注入边界框）。我们在DOTA和DIOR-R这两个包含航空图像中各种旋转通用对象的数据集上进行了大量实验。实验结果表明，我们的方法在不使用任何花哨技巧的情况下显著提高了遥感目标检测的精度，并且比其他最先进的增强策略更加省时。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09424v2">PDF</a> </p>
<p><strong>Summary</strong><br>数据增强技术在计算机视觉领域已取得显著进展，特别是在数据有限和不足的情况下，能够提高模型性能。目前研究主要集中在图像或其特征的调整上，以扩大样本的大小、质量和种类。本文提出研究边界框转换作为数据增强技术，特别是在航空图像中，因为边界框注释可能存在不一致的情况。本文对边界框转换的缩放、旋转和翻译进行了深入研究，用于遥感目标检测。我们称这种增强策略为NBBOX（噪声注入边界框）。在DOTA和DIOR-R两个包含航空图像中旋转通用对象的大型数据集上进行了大量实验。实验结果表明，我们的方法在不增加额外时间和成本的情况下显著提高了遥感目标检测的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据增强技术在计算机视觉领域的重要性：特别是在数据有限和不足时，有助于提高模型性能。</li>
<li>当前研究主要集中在图像级别的数据增强上，通过调整图像或其特征来扩大样本规模、提高质量和多样性。</li>
<li>论文主张研究边界框转换作为数据增强技术，特别是在航空图像中，因为存在边界框注释不一致的问题。</li>
<li>论文介绍了NBBOX策略，这是一种将噪声注入边界框的数据增强方法，包括缩放、旋转和翻译的研究。</li>
<li>实验在DOTA和DIOR-R两个数据集上进行，这两个数据集包含航空图像中的旋转通用对象。</li>
<li>实验结果表明，NBBOX策略在遥感目标检测中具有显著的优势，并且比一些先进的增强策略更加高效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09424">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-35715ef4725aac1b951c3eabaf5c5ed3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5f2e95849f13ebe4b4f717da096fc08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6701815eefdbef184100b57582da634.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88fcf71de3685c7515c363ca58222902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-055430be74fa599b9db20f5d1ce67d9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19001d41d9534ed05204d3341507a4da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1957199965f5c39375b00e43dd37ceaf.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AllWeatherNet-Unified-Image-Enhancement-for-Autonomous-Driving-under-Adverse-Weather-and-Lowlight-conditions"><a href="#AllWeatherNet-Unified-Image-Enhancement-for-Autonomous-Driving-under-Adverse-Weather-and-Lowlight-conditions" class="headerlink" title="AllWeatherNet:Unified Image Enhancement for Autonomous Driving under   Adverse Weather and Lowlight-conditions"></a>AllWeatherNet:Unified Image Enhancement for Autonomous Driving under   Adverse Weather and Lowlight-conditions</h2><p><strong>Authors:Chenghao Qian, Mahdi Rezaei, Saeed Anwar, Wenjing Li, Tanveer Hussain, Mohsen Azarmi, Wei Wang</strong></p>
<p>Adverse conditions like snow, rain, nighttime, and fog, pose challenges for autonomous driving perception systems. Existing methods have limited effectiveness in improving essential computer vision tasks, such as semantic segmentation, and often focus on only one specific condition, such as removing rain or translating nighttime images into daytime ones. To address these limitations, we propose a method to improve the visual quality and clarity degraded by such adverse conditions. Our method, AllWeather-Net, utilizes a novel hierarchical architecture to enhance images across all adverse conditions. This architecture incorporates information at three semantic levels: scene, object, and texture, by discriminating patches at each level. Furthermore, we introduce a Scaled Illumination-aware Attention Mechanism (SIAM) that guides the learning towards road elements critical for autonomous driving perception. SIAM exhibits robustness, remaining unaffected by changes in weather conditions or environmental scenes. AllWeather-Net effectively transforms images into normal weather and daytime scenes, demonstrating superior image enhancement results and subsequently enhancing the performance of semantic segmentation, with up to a 5.3% improvement in mIoU in the trained domain. We also show our model’s generalization ability by applying it to unseen domains without re-training, achieving up to 3.9% mIoU improvement. Code can be accessed at: <a target="_blank" rel="noopener" href="https://github.com/Jumponthemoon/AllWeatherNet">https://github.com/Jumponthemoon/AllWeatherNet</a>. </p>
<blockquote>
<p>恶劣条件，如雪、雨、夜晚和雾，给自动驾驶感知系统带来了挑战。现有方法在改善重要的计算机视觉任务（如语义分割）方面的效果有限，而且通常只关注一种特定条件，例如去雨或将夜间图像转换为日间图像。为了解决这些局限性，我们提出了一种方法，用以改善由这些恶劣条件造成的视觉质量和清晰度下降的问题。我们的方法，AllWeather-Net，利用一种新型分层架构，提升各种恶劣条件下的图像。该架构通过区分三个语义级别的信息（场景、物体和纹理）来增强图像。此外，我们引入了一种缩放照明感知注意机制（SIAM），引导学习面向对自动驾驶感知至关重要的道路元素。SIAM表现出稳健性，不受天气条件或环境场景变化的影响。AllWeather-Net能有效地将图像转换为正常天气和日间场景，显示出卓越的图像增强效果，进而提升语义分割的性能，在训练域内平均交并比（mIoU）提高5.3%。我们还通过将在未见域的应用展示了我们模型的泛化能力，无需重新训练即可实现高达3.9%的mIoU改善。相关代码可访问：<a target="_blank" rel="noopener" href="https://github.com/Jumponthemoon/AllWeatherNet%E3%80%82">https://github.com/Jumponthemoon/AllWeatherNet。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02045v2">PDF</a> ICPR 2024, Piero Zamperoni Overall Best Student Paper Award</p>
<p><strong>Summary</strong><br>    本文提出一种应对恶劣天气条件挑战的方法，名为AllWeather-Net。该方法采用分层架构，通过场景、物体和纹理三个语义层次的信息处理来改善图像质量。引入的缩放光照感知注意力机制（SIAM）能指导学习关注对自动驾驶感知至关重要的道路元素。AllWeather-Net可将图像转化为正常天气和白天场景，提升语义分割性能，并在训练和未训练领域均表现出优异的图像增强效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>恶劣条件如雨雪、夜晚和雾对自动驾驶感知系统构成挑战。</li>
<li>现有方法改善计算机视觉任务（如语义分割）的效果有限，通常只针对单一条件。</li>
<li>AllWeather-Net采用分层架构，处理场景、物体和纹理三个语义层次的信息，改善图像质量。</li>
<li>引入缩放光照感知注意力机制（SIAM），指导学习关注自动驾驶感知中重要的道路元素。</li>
<li>AllWeather-Net可将图像转化为正常天气和白天场景，显著提升语义分割性能。</li>
<li>该方法在训练和未训练领域均展现出优异的图像增强效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-37a6a0c7542257f1cb0bd78dbca4480e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa51ab85f01d6d89acf7845b81437fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fad563b40dbf08afd94dd936541bd3f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e1cb02f273db28aa3545031e805e6ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33907cb8fad24662194925b0396f2daf.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3cd5b11aadac42b7c5cdf1fbae433acc.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2024-12-19  FocusChat Text-guided Long Video Understanding via Spatiotemporal   Information Filtering
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-a91205a65b64bc5f5dda409bd8f005a2.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2024-12-19  FarExStance Explainable Stance Detection for Farsi
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
