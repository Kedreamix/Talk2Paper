<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Prompt Categories Cluster for Weakly Supervised Semantic Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-341bda71b5849f8db8e8fbfcf65e57fa.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    23.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    97 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-19-æ›´æ–°"><a href="#2024-12-19-æ›´æ–°" class="headerlink" title="2024-12-19 æ›´æ–°"></a>2024-12-19 æ›´æ–°</h1><h2 id="Prompt-Categories-Cluster-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Prompt-Categories-Cluster-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Prompt Categories Cluster for Weakly Supervised Semantic Segmentation"></a>Prompt Categories Cluster for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level labels, has garnered significant attention due to its cost-effectiveness. The previous methods mainly strengthen the inter-class differences to avoid class semantic ambiguity which may lead to erroneous activation. However, they overlook the positive function of some shared information between similar classes. Categories within the same cluster share some similar features. Allowing the model to recognize these features can further relieve the semantic ambiguity between these classes. To effectively identify and utilize this shared information, in this paper, we introduce a novel WSSS framework called Prompt Categories Clustering (PCC). Specifically, we explore the ability of Large Language Models (LLMs) to derive category clusters through prompts. These clusters effectively represent the intrinsic relationships between categories. By integrating this relational information into the training network, our model is able to better learn the hidden connections between categories. Experimental results demonstrate the effectiveness of our approach, showing its ability to enhance performance on the PASCAL VOC 2012 dataset and surpass existing state-of-the-art methods in WSSS. </p>
<blockquote>
<p>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰åˆ©ç”¨å›¾åƒçº§åˆ«çš„æ ‡ç­¾ï¼Œå› å…¶æˆæœ¬æ•ˆç›Šè€Œå¤‡å—å…³æ³¨ã€‚ä¹‹å‰çš„æ–¹æ³•ä¸»è¦æ˜¯åŠ å¼ºç±»ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥é¿å…å¯èƒ½å¯¼è‡´é”™è¯¯æ¿€æ´»çš„ç±»è¯­ä¹‰æ¨¡ç³Šã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½è§†äº†ç›¸ä¼¼ç±»åˆ«ä¹‹é—´å…±äº«ä¿¡æ¯çš„ç§¯æä½œç”¨ã€‚åŒä¸€èšç±»ä¸­çš„ç±»åˆ«å…±äº«ä¸€äº›ç›¸ä¼¼ç‰¹å¾ã€‚å…è®¸æ¨¡å‹è¯†åˆ«è¿™äº›ç‰¹å¾å¯ä»¥è¿›ä¸€æ­¥ç¼“è§£è¿™äº›ç±»åˆ«ä¹‹é—´çš„è¯­ä¹‰æ¨¡ç³Šã€‚ä¸ºäº†æœ‰æ•ˆåœ°è¯†åˆ«å’Œåˆ©ç”¨è¿™äº›å…±äº«ä¿¡æ¯ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹çš„WSSSæ¡†æ¶ï¼Œç§°ä¸ºæç¤ºç±»åˆ«èšç±»ï¼ˆPCCï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æç¤ºæ¨å¯¼ç±»åˆ«èšç±»çš„èƒ½åŠ›ã€‚è¿™äº›èšç±»æœ‰æ•ˆåœ°ä»£è¡¨äº†ç±»åˆ«ä¹‹é—´çš„å†…åœ¨å…³ç³»ã€‚é€šè¿‡å°†è¿™ç§å…³ç³»ä¿¡æ¯é›†æˆåˆ°è®­ç»ƒç½‘ç»œä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ ç±»åˆ«ä¹‹é—´çš„éšè—è¿æ¥ã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶åœ¨PASCAL VOC 2012æ•°æ®é›†ä¸Šçš„æ€§èƒ½æå‡ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„WSSSå…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13823v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åˆ©ç”¨å›¾åƒçº§æ ‡ç­¾çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰å› å…¶æˆæœ¬æ•ˆç›Šè€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ä»¥å¾€æ–¹æ³•ä¸»è¦å¼ºåŒ–ç±»é—´å·®å¼‚ä»¥é¿å…è¯­ä¹‰æ¨¡ç³Šå¯¼è‡´çš„é”™è¯¯æ¿€æ´»ï¼Œä½†å¿½è§†äº†ç›¸ä¼¼ç±»åˆ«é—´å…±äº«ä¿¡æ¯çš„ç§¯æä½œç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPrompt Categories Clusteringï¼ˆPCCï¼‰çš„æ–°å‹WSSSæ¡†æ¶ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æç¤ºèƒ½åŠ›æ¥è¯†åˆ«ç±»åˆ«é›†ç¾¤ï¼Œæœ‰æ•ˆä»£è¡¨ç±»åˆ«ä¹‹é—´çš„å†…åœ¨å…³ç³»ã€‚é€šè¿‡å°†æ­¤å…³ç³»ä¿¡æ¯æ•´åˆåˆ°è®­ç»ƒç½‘ç»œä¸­ï¼Œæ¨¡å‹èƒ½æ›´å¥½åœ°å­¦ä¹ ç±»åˆ«ä¹‹é—´çš„éšè—è”ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨PASCAL VOC 2012æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>WSSSåˆ©ç”¨å›¾åƒçº§æ ‡ç­¾ï¼Œæˆæœ¬æ•ˆç›Šé«˜ï¼Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ä»¥å¾€æ–¹æ³•ä¸»è¦å¼ºåŒ–ç±»é—´å·®å¼‚ï¼Œé¿å…è¯­ä¹‰æ¨¡ç³Šå¯¼è‡´çš„é”™è¯¯æ¿€æ´»ã€‚</li>
<li>ç›¸ä¼¼ç±»åˆ«é—´å­˜åœ¨å…±äº«ä¿¡æ¯ï¼Œå¯æœ‰æ•ˆç¼“è§£è¯­ä¹‰æ¨¡ç³Šã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„WSSSæ¡†æ¶â€”â€”Prompt Categories Clusteringï¼ˆPCCï¼‰ã€‚</li>
<li>PCCåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºèƒ½åŠ›æ¥è¯†åˆ«ç±»åˆ«é›†ç¾¤ï¼Œä»£è¡¨ç±»åˆ«é—´çš„å†…åœ¨å…³ç³»ã€‚</li>
<li>å°†å…³ç³»ä¿¡æ¯æ•´åˆåˆ°è®­ç»ƒç½‘ç»œä¸­ï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ ç±»åˆ«é—´çš„éšè—è”ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ea8b4a3830902aaa3bb92fe36664d661.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-908109cb9b407b23ee25a4ba6bcf4118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-738015d46a0e90dc353722a18c35f429.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c14ab5196abb4cba8957ac648d3e53b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-488b1a6470c5bf78da2cf9027aa1f594.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f94ae199cbb1ce23612ee22e021f7ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba38560197f0921d0249f5071cf3d72b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c693a61d1baabb497cad65acb625cec0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="S2S2-Semantic-Stacking-for-Robust-Semantic-Segmentation-in-Medical-Imaging"><a href="#S2S2-Semantic-Stacking-for-Robust-Semantic-Segmentation-in-Medical-Imaging" class="headerlink" title="S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical   Imaging"></a>S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical   Imaging</h2><p><strong>Authors:Yimu Pan, Sitao Zhang, Alison D. Gernand, Jeffery A. Goldstein, James Z. Wang</strong></p>
<p>Robustness and generalizability in medical image segmentation are often hindered by scarcity and limited diversity of training data, which stands in contrast to the variability encountered during inference. While conventional strategies â€“ such as domain-specific augmentation, specialized architectures, and tailored training procedures â€“ can alleviate these issues, they depend on the availability and reliability of domain knowledge. When such knowledge is unavailable, misleading, or improperly applied, performance may deteriorate. In response, we introduce a novel, domain-agnostic, add-on, and data-driven strategy inspired by image stacking in image denoising. Termed &#96;&#96;semantic stacking,â€™â€™ our method estimates a denoised semantic representation that complements the conventional segmentation loss during training. This method does not depend on domain-specific assumptions, making it broadly applicable across diverse image modalities, model architectures, and augmentation techniques. Through extensive experiments, we validate the superiority of our approach in improving segmentation performance under diverse conditions. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ymp5078/Semantic-Stacking">https://github.com/ymp5078/Semantic-Stacking</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§å¸¸å¸¸å—åˆ°è®­ç»ƒæ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§æœ‰é™çš„é˜»ç¢ï¼Œè¿™ä¸æ¨ç†è¿‡ç¨‹ä¸­é‡åˆ°çš„å˜é‡å½¢æˆå¯¹æ¯”ã€‚è™½ç„¶ä¼ ç»Ÿçš„ç­–ç•¥â€”â€”å¦‚ç‰¹å®šé¢†åŸŸçš„å¢å¼ºã€ä¸“ç”¨æ¶æ„å’Œå®šåˆ¶è®­ç»ƒç¨‹åºâ€”â€”å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†å®ƒä»¬ä¾èµ–äºé¢†åŸŸçŸ¥è¯†çš„å¯ç”¨æ€§å’Œå¯é æ€§ã€‚å½“è¿™ç§çŸ¥è¯†ä¸å¯ç”¨ã€è¯¯å¯¼æˆ–åº”ç”¨ä¸å½“æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¼šæ¶åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ã€éç‰¹å®šé¢†åŸŸçš„é™„åŠ æ•°æ®é©±åŠ¨ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å—åˆ°å›¾åƒå»å™ªä¸­å›¾åƒå †å çš„å¯å‘ã€‚æˆ‘ä»¬å°†å…¶ç§°ä¸ºâ€œè¯­ä¹‰å †å â€ï¼Œè¯¥æ–¹æ³•ä¼°è®¡å»å™ªçš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»¥è¡¥å……è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¸¸è§„åˆ†å‰²æŸå¤±ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„å‡è®¾ï¼Œå› æ­¤å¯å¹¿æ³›åº”ç”¨äºä¸åŒçš„å›¾åƒæ¨¡å¼ã€æ¨¡å‹æ¶æ„å’Œå¢å¼ºæŠ€æœ¯ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ”¹å–„ä¸åŒæ¡ä»¶ä¸‹çš„åˆ†å‰²æ€§èƒ½æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ymp5078/Semantic-Stacking%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/ymp5078/Semantic-Stackingè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13156v1">PDF</a> AAAI2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºå’Œæœ‰é™å¤šæ ·æ€§å¸¸å¯¼è‡´æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›å—é™ã€‚ä¸ºåº”å¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ã€é¢†åŸŸæ— å…³çš„ã€é™„åŠ çš„ã€æ•°æ®é©±åŠ¨ç­–ç•¥ï¼Œå—å›¾åƒå»å™ªä¸­çš„å›¾åƒå †å å¯å‘ï¼Œç§°ä¸ºâ€œè¯­ä¹‰å †å â€ã€‚è¯¥æ–¹æ³•ä¼°è®¡å»å™ªçš„è¯­ä¹‰è¡¨ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¥å……ä¼ ç»Ÿåˆ†å‰²æŸå¤±ï¼Œå¯å¹¿æ³›åº”ç”¨äºä¸åŒå›¾åƒæ¨¡æ€ã€æ¨¡å‹æ¶æ„å’Œå¢å¼ºæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´è®­ç»ƒæ•°æ®ç¨€ç¼ºå’Œæœ‰é™å¤šæ ·æ€§é—®é¢˜ï¼Œå½±å“æ¨¡å‹ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»Ÿç­–ç•¥å¦‚é¢†åŸŸç‰¹å®šå¢å¼ºã€ä¸“é—¨æ¶æ„å’Œå®šåˆ¶è®­ç»ƒç¨‹åºå¯ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†ä¾èµ–é¢†åŸŸçŸ¥è¯†çš„å¯ç”¨æ€§ã€å¯é æ€§ã€‚</li>
<li>å½“é¢†åŸŸçŸ¥è¯†ä¸å¯ç”¨ã€è¯¯å¯¼æˆ–ä¸å½“åº”ç”¨æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ã€é¢†åŸŸæ— å…³çš„ã€é™„åŠ çš„ã€æ•°æ®é©±åŠ¨ç­–ç•¥â€”â€”â€œè¯­ä¹‰å †å â€ï¼Œä¼°è®¡å»å™ªè¯­ä¹‰è¡¨ç¤ºï¼Œè¡¥å……ä¼ ç»Ÿåˆ†å‰²æŸå¤±ã€‚</li>
<li>â€œè¯­ä¹‰å †å â€æ–¹æ³•ä¸å—ç‰¹å®šé¢†åŸŸå‡è®¾çš„é™åˆ¶ï¼Œå¯å¹¿æ³›åº”ç”¨äºä¸åŒå›¾åƒæ¨¡æ€ã€æ¨¡å‹æ¶æ„å’Œå¢å¼ºæŠ€æœ¯ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨æ”¹å–„åˆ†å‰²æ€§èƒ½æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0e6ac961a052458d69a5156dd111aedd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-702c1a90068880d011c40846d2566095.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1590868d6db7b83739fce58c98b24b4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fbaff335b864b3a9397e7272dd86454.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-New-Adversarial-Perspective-for-LiDAR-based-3D-Object-Detection"><a href="#A-New-Adversarial-Perspective-for-LiDAR-based-3D-Object-Detection" class="headerlink" title="A New Adversarial Perspective for LiDAR-based 3D Object Detection"></a>A New Adversarial Perspective for LiDAR-based 3D Object Detection</h2><p><strong>Authors:Shijun Zheng, Weiquan Liu, Yu Guo, Yu Zang, Siqi Shen, Cheng Wang</strong></p>
<p>Autonomous vehicles (AVs) rely on LiDAR sensors for environmental perception and decision-making in driving scenarios. However, ensuring the safety and reliability of AVs in complex environments remains a pressing challenge. To address this issue, we introduce a real-world dataset (ROLiD) comprising LiDAR-scanned point clouds of two random objects: water mist and smoke. In this paper, we introduce a novel adversarial perspective by proposing an attack framework that utilizes water mist and smoke to simulate environmental interference. Specifically, we propose a point cloud sequence generation method using a motion and content decomposition generative adversarial network named PCS-GAN to simulate the distribution of random objects. Furthermore, leveraging the simulated LiDAR scanning characteristics implemented with Range Image, we examine the effects of introducing random object perturbations at various positions on the target vehicle. Extensive experiments demonstrate that adversarial perturbations based on random objects effectively deceive vehicle detection and reduce the recognition rate of 3D object detection models. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVï¼‰åœ¨é©¾é©¶åœºæ™¯ä¸­ä¾èµ–äºæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨è¿›è¡Œç¯å¢ƒæ„ŸçŸ¥å’Œå†³ç­–ã€‚ç„¶è€Œï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­ç¡®ä¿è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å®‰å…¨å’Œå¯é æ€§ä»ç„¶æ˜¯ä¸€ä¸ªç´§è¿«çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«æ¿€å…‰é›·è¾¾æ‰«æçš„ä¸¤ç§éšæœºå¯¹è±¡ç‚¹äº‘æ•°æ®çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼ˆROLiDï¼‰ï¼šæ°´é›¾å’ŒçƒŸé›¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ°´é›¾å’ŒçƒŸé›¾æ¨¡æ‹Ÿç¯å¢ƒå¹²æ‰°çš„æ”»å‡»æ¡†æ¶ï¼Œä»è€Œå¼•å…¥äº†å¯¹æŠ—æ€§è§†è§’ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨åä¸ºPCS-GANçš„è¿åŠ¨å’Œå†…å®¹åˆ†è§£ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥ç‚¹äº‘åºåˆ—ç”Ÿæˆæ–¹æ³•ï¼Œä»¥æ¨¡æ‹Ÿéšæœºå¯¹è±¡çš„åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æ¨¡æ‹Ÿçš„æ¿€å…‰é›·è¾¾æ‰«æç‰¹æ€§ç»“åˆèŒƒå›´å›¾åƒï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨ç›®æ ‡è½¦è¾†çš„ä¸åŒä½ç½®å¼•å…¥éšæœºå¯¹è±¡æ‰°åŠ¨çš„å½±å“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒåŸºäºéšæœºå¯¹è±¡çš„å¯¹æŠ—æ€§æ‰°åŠ¨å¯ä»¥æœ‰æ•ˆåœ°æ¬ºéª—è½¦è¾†æ£€æµ‹ï¼Œé™ä½3Då¯¹è±¡æ£€æµ‹æ¨¡å‹çš„è¯†åˆ«ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13017v1">PDF</a> 11 pages, 7 figures, AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰åœ¨å¤æ‚ç¯å¢ƒä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºæ­¤å¼•å…¥äº†çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼ˆROLiDï¼‰ï¼ŒåŒ…å«æ¿€å…‰é›·è¾¾æ‰«æçš„æ°´é›¾å’ŒçƒŸé›¾ç‚¹äº‘ã€‚æ–‡ç« æå‡ºä¸€ç§åˆ©ç”¨æ°´é›¾å’ŒçƒŸé›¾æ¨¡æ‹Ÿç¯å¢ƒå¹²æ‰°çš„å¯¹æŠ—æ€§è§†è§’ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åä¸ºPCS-GANçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥æ¨¡æ‹Ÿéšæœºå¯¹è±¡çš„ç‚¹äº‘åºåˆ—ç”Ÿæˆã€‚è¯¥ç½‘ç»œç»“åˆè¿åŠ¨å’Œå†…å®¹åˆ†è§£æŠ€æœ¯ï¼Œæ¨¡æ‹Ÿéšæœºå¯¹è±¡åˆ†å¸ƒã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºéšæœºå¯¹è±¡çš„å¯¹æŠ—æ€§æ‰°åŠ¨å¯æœ‰æ•ˆæ¬ºéª—è½¦è¾†æ£€æµ‹å¹¶é™ä½ä¸‰ç»´ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„è¯†åˆ«ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨å¤æ‚ç¯å¢ƒä¸­é¢ä¸´å®‰å…¨å¯é æ€§æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼ˆROLiDï¼‰ï¼ŒåŒ…å«æ°´é›¾å’ŒçƒŸé›¾çš„æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®ã€‚</li>
<li>æå‡ºåˆ©ç”¨æ°´é›¾å’ŒçƒŸé›¾æ¨¡æ‹Ÿç¯å¢ƒå¹²æ‰°çš„å¯¹æŠ—æ€§è§†è§’ã€‚</li>
<li>å¼€å‘åä¸ºPCS-GANçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥æ¨¡æ‹Ÿéšæœºå¯¹è±¡çš„ç‚¹äº‘åºåˆ—ç”Ÿæˆã€‚</li>
<li>è¯¥ç½‘ç»œé€šè¿‡è¿åŠ¨å’Œå†…å®¹åˆ†è§£æŠ€æœ¯æ¨¡æ‹Ÿéšæœºå¯¹è±¡çš„åˆ†å¸ƒç‰¹æ€§ã€‚</li>
<li>å¯¹æŠ—æ€§æ‰°åŠ¨èƒ½æ¨¡æ‹Ÿéšæœºå¯¹è±¡å¹²æ‰°è½¦è¾†æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7e8ea71c09a29fcc8799627770f34539.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6175f94525aa963f6a1ca8d2f88f3307.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a1075a0efae11ba9ddebf78003ad364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c35f411ea6af5ebc688d727c1d167df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db0bbedbfdc4ba90ab6c2a25a446ccaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22ba44e2fbf5647bbb6c6a36fe15a826.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Efficient-Event-based-Semantic-Segmentation-with-Spike-driven-Lightweight-Transformer-based-Networks"><a href="#Efficient-Event-based-Semantic-Segmentation-with-Spike-driven-Lightweight-Transformer-based-Networks" class="headerlink" title="Efficient Event-based Semantic Segmentation with Spike-driven   Lightweight Transformer-based Networks"></a>Efficient Event-based Semantic Segmentation with Spike-driven   Lightweight Transformer-based Networks</h2><p><strong>Authors:Xiaxin Zhu, Fangming Guo, Xianlei Long, Qingyi Gu, Chao Chen, Fuqiang Gu</strong></p>
<p>Event-based semantic segmentation has great potential in autonomous driving and robotics due to the advantages of event cameras, such as high dynamic range, low latency, and low power cost. Unfortunately, current artificial neural network (ANN)-based segmentation methods suffer from high computational demands, the requirements for image frames, and massive energy consumption, limiting their efficiency and application on resource-constrained edge&#x2F;mobile platforms. To address these problems, we introduce SLTNet, a spike-driven lightweight transformer-based network designed for event-based semantic segmentation. Specifically, SLTNet is built on efficient spike-driven convolution blocks (SCBs) to extract rich semantic features while reducing the modelâ€™s parameters. Then, to enhance the long-range contextural feature interaction, we propose novel spike-driven transformer blocks (STBs) with binary mask operations. Based on these basic blocks, SLTNet employs a high-efficiency single-branch architecture while maintaining the low energy consumption of the Spiking Neural Network (SNN). Finally, extensive experiments on DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms state-of-the-art (SOTA) SNN-based methods by at least 7.30% and 3.30% mIoU, respectively, with extremely 5.48x lower energy consumption and 1.14x faster inference speed. </p>
<blockquote>
<p>åŸºäºäº‹ä»¶çš„è¯­ä¹‰åˆ†å‰²åœ¨è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ï¼Œè¿™æ˜¯ç”±äºäº‹ä»¶ç›¸æœºå…·æœ‰è¯¸å¦‚é«˜åŠ¨æ€èŒƒå›´ã€ä½å»¶è¿Ÿå’Œä½åŠŸè€—ç­‰ä¼˜ç‚¹ã€‚ç„¶è€Œï¼Œç›®å‰åŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰çš„åˆ†å‰²æ–¹æ³•å­˜åœ¨ç€è®¡ç®—é‡å¤§ã€å¯¹å›¾åƒå¸§çš„è¦æ±‚é«˜ä»¥åŠèƒ½è€—å·¨å¤§çš„é—®é¢˜ï¼Œè¿™åœ¨èµ„æºå—é™çš„è¾¹ç¼˜&#x2F;ç§»åŠ¨å¹³å°ä¸Šé™åˆ¶äº†å…¶æ•ˆç‡å’Œåº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SLTNetï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè„‰å†²é©±åŠ¨çš„è½»é‡çº§å˜å‹å™¨ç½‘ç»œï¼Œç”¨äºåŸºäºäº‹ä»¶çš„è¯­ä¹‰åˆ†å‰²ã€‚å…·ä½“è€Œè¨€ï¼ŒSLTNetæ˜¯å»ºç«‹åœ¨é«˜æ•ˆçš„è„‰å†²é©±åŠ¨å·ç§¯å—ï¼ˆSCBï¼‰åŸºç¡€ä¸Šçš„ï¼Œæ—¨åœ¨æå–ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾ï¼ŒåŒæ—¶å‡å°‘æ¨¡å‹çš„å‚æ•°ã€‚ç„¶åï¼Œä¸ºäº†å¢å¼ºé•¿ç¨‹çº¹ç†ç‰¹å¾çš„äº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹è„‰å†²é©±åŠ¨å˜å‹å™¨å—ï¼ˆSTBï¼‰ï¼Œé‡‡ç”¨äºŒè¿›åˆ¶æ©ç æ“ä½œã€‚åŸºäºè¿™äº›åŸºæœ¬å—ï¼ŒSLTNeté‡‡ç”¨äº†é«˜æ•ˆçš„å•åˆ†æ”¯æ¶æ„ï¼ŒåŒæ—¶ä¿æŒäº†è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰çš„ä½èƒ½è€—ã€‚æœ€ååœ¨DDD17å’ŒDSEC-Semanticæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSLTNetæ¯”æœ€å…ˆè¿›çš„SNNæ–¹æ³•è‡³å°‘é«˜å‡º7.30%å’Œ3.30%çš„mIoUï¼ŒåŒæ—¶èƒ½è€—é™ä½äº†5.48å€ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†1.14å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12843v1">PDF</a> Submitted to IEEE ICRA 2025</p>
<p><strong>Summary</strong></p>
<p>äº‹ä»¶é©±åŠ¨çš„è¯­ä¹‰åˆ†å‰²åœ¨è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººé¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¾—ç›Šäºäº‹ä»¶ç›¸æœºçš„é«˜åŠ¨æ€èŒƒå›´ã€ä½å»¶è¿Ÿå’Œä½èƒ½è€—ä¼˜åŠ¿ã€‚ä¸ºè§£å†³å½“å‰äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰åˆ†å‰²æ–¹æ³•è®¡ç®—é‡å¤§ã€å¯¹å›¾åƒå¸§çš„è¦æ±‚ä»¥åŠèƒ½è€—é«˜çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SLTNetï¼Œä¸€ä¸ªåŸºäºè„‰å†²é©±åŠ¨è½»é‡çº§Transformerçš„äº‹ä»¶é©±åŠ¨è¯­ä¹‰åˆ†å‰²ç½‘ç»œã€‚é€šè¿‡é«˜æ•ˆçš„è„‰å†²é©±åŠ¨å·ç§¯å—ï¼ˆSCBï¼‰æå–ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾å¹¶å‡å°‘æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶æå‡ºæ–°é¢–çš„è„‰å†²é©±åŠ¨Transformerå—ï¼ˆSTBï¼‰è¿›è¡Œé•¿è·ç¦»ä¸Šä¸‹æ–‡ç‰¹å¾äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼ŒSLTNetåœ¨DDD17å’ŒDSEC-Semanticæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€æ–°SNNæ–¹æ³•ï¼Œè‡³å°‘æé«˜äº†7.30%å’Œ3.30%çš„mIoUï¼ŒåŒæ—¶èƒ½è€—é™ä½äº†5.48å€ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†1.14å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>äº‹ä»¶é©±åŠ¨çš„è¯­ä¹‰åˆ†å‰²åœ¨è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººé¢†åŸŸæœ‰æ½œåŠ›ï¼Œå› äº‹ä»¶ç›¸æœºçš„é«˜åŠ¨æ€èŒƒå›´ã€ä½å»¶è¿Ÿå’Œä½èƒ½è€—ä¼˜åŠ¿ã€‚</li>
<li>å½“å‰ANNæ–¹æ³•åœ¨è®¡ç®—éœ€æ±‚ã€å›¾åƒå¸§è¦æ±‚å’Œèƒ½è€—æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚</li>
<li>SLTNetæ˜¯ä¸€ä¸ªåŸºäºè„‰å†²é©±åŠ¨çš„è½»é‡çº§Transformerç½‘ç»œï¼Œç”¨äºäº‹ä»¶é©±åŠ¨çš„è¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>SLTNetä½¿ç”¨é«˜æ•ˆçš„è„‰å†²é©±åŠ¨å·ç§¯å—ï¼ˆSCBï¼‰æå–è¯­ä¹‰ç‰¹å¾å¹¶å‡å°‘æ¨¡å‹å‚æ•°ã€‚</li>
<li>æå‡ºäº†è„‰å†²é©±åŠ¨Transformerå—ï¼ˆSTBï¼‰è¿›è¡Œé•¿è·ç¦»ä¸Šä¸‹æ–‡ç‰¹å¾äº¤äº’ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-960cb3b69c760eaccfef80094f028500.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-341bda71b5849f8db8e8fbfcf65e57fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b204c75c244535c8838a408c28d979e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-caaed203cfbc789d2b9f8be2fec052c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46f52478b00c91164d8ae7b66eeb9ba2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-352f1acb5e49df211bc5d5032cee3632.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-437254e28e9506cc1e3576e50a480abd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd2428b2bc77239ae5eb22b0f343d671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c1640d862aa1fbee05c66eac2a605e4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Differential-Alignment-for-Domain-Adaptive-Object-Detection"><a href="#Differential-Alignment-for-Domain-Adaptive-Object-Detection" class="headerlink" title="Differential Alignment for Domain Adaptive Object Detection"></a>Differential Alignment for Domain Adaptive Object Detection</h2><p><strong>Authors:Xinyu He, Xinhui Li, Xiaojie Guo</strong></p>
<p>Domain adaptive object detection (DAOD) aims to generalize an object detector trained on labeled source-domain data to a target domain without annotations, the core principle of which is \emph{source-target feature alignment}. Typically, existing approaches employ adversarial learning to align the distributions of the source and target domains as a whole, barely considering the varying significance of distinct regions, say instances under different circumstances and foreground \emph{vs} background areas, during feature alignment. To overcome the shortcoming, we investigates a differential feature alignment strategy. Specifically, a prediction-discrepancy feedback instance alignment module (dubbed PDFA) is designed to adaptively assign higher weights to instances of higher teacher-student detection discrepancy, effectively handling heavier domain-specific information. Additionally, an uncertainty-based foreground-oriented image alignment module (UFOA) is proposed to explicitly guide the model to focus more on regions of interest. Extensive experiments on widely-used DAOD datasets together with ablation studies are conducted to demonstrate the efficacy of our proposed method and reveal its superiority over other SOTA alternatives. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/EstrellaXyu/Differential-Alignment-for-DAOD">https://github.com/EstrellaXyu/Differential-Alignment-for-DAOD</a>. </p>
<blockquote>
<p>é¢†åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼ˆDAODï¼‰æ—¨åœ¨å°†è®­ç»ƒæœ‰æ ‡ç­¾çš„æºåŸŸæ•°æ®çš„ç›®æ ‡æ£€æµ‹å™¨æ¨å¹¿åˆ°æ— æ³¨é‡Šçš„ç›®æ ‡åŸŸã€‚å…¶æ ¸å¿ƒåŸç†æ˜¯â€œæºåŸŸåˆ°ç›®æ ‡åŸŸçš„ç‰¹å¾å¯¹é½â€ã€‚é€šå¸¸ï¼Œç°æœ‰çš„æ–¹æ³•é‡‡ç”¨å¯¹æŠ—æ€§å­¦ä¹ æ¥æ•´ä½“å¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸçš„æ•´ä½“åˆ†å¸ƒï¼Œå‡ ä¹ä¸è€ƒè™‘ä¸åŒåŒºåŸŸåœ¨ç‰¹å¾å¯¹é½è¿‡ç¨‹ä¸­çš„ä¸åŒé‡è¦æ€§ï¼Œä¾‹å¦‚åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„å®ä¾‹ä»¥åŠå‰æ™¯ä¸èƒŒæ™¯åŒºåŸŸä¹‹é—´çš„å·®å¼‚ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç¼ºç‚¹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§å·®å¼‚ç‰¹å¾å¯¹é½ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ä¸ªé¢„æµ‹å·®å¼‚åé¦ˆå®ä¾‹å¯¹é½æ¨¡å—ï¼ˆç®€ç§°PDFAï¼‰ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°ä¸ºå…·æœ‰è¾ƒé«˜æ•™å¸ˆ-å­¦ç”Ÿæ£€æµ‹å·®å¼‚çš„å®ä¾‹åˆ†é…æ›´é«˜çš„æƒé‡ï¼Œä»è€Œæœ‰æ•ˆåœ°å¤„ç†æ›´é‡çš„ç‰¹å®šé¢†åŸŸä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„é¢å‘å‰æ™¯çš„å›¾åƒå¯¹é½æ¨¡å—ï¼ˆUFOAï¼‰ï¼Œä»¥æ˜ç¡®æŒ‡å¯¼æ¨¡å‹æ›´å¤šåœ°å…³æ³¨æ„Ÿå…´è¶£åŒºåŸŸã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„DAODæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒæ—¶è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥è¯æ˜æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºå…¶åœ¨å…¶ä»–æœ€æ–°æ›¿ä»£æ–¹æ¡ˆä¸­çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EstrellaXyu/Differential-Alignment-for-DAOD">https://github.com/EstrellaXyu/Differential-Alignment-for-DAOD</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12830v1">PDF</a> 11 pages, 8 figures, accepted by aaai25</p>
<p><strong>Summary</strong><br>é¢†åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼ˆDAODï¼‰æ—¨åœ¨å°†è®­ç»ƒæœ‰æ ‡ç­¾çš„æºåŸŸæ•°æ®çš„ç›®æ ‡æ£€æµ‹å™¨æ¨å¹¿åˆ°æ— æ ‡æ³¨çš„ç›®æ ‡åŸŸã€‚å…¶æ ¸å¿ƒåŸç†æ˜¯æºåŸŸå’Œç›®æ ‡åŸŸçš„ç‰¹å¾å¯¹é½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨å¯¹æŠ—æ€§å­¦ä¹ æ¥æ•´ä½“å¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸåˆ†å¸ƒï¼Œä½†å¾ˆå°‘è€ƒè™‘ä¸åŒåŒºåŸŸï¼ˆå¦‚ä¸åŒæƒ…å¢ƒä¸‹çš„å®ä¾‹å’Œå‰æ™¯ä¸èƒŒæ™¯åŒºåŸŸï¼‰çš„ä¸åŒé‡è¦æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å·®å¼‚åŒ–ç‰¹å¾å¯¹é½ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†é¢„æµ‹å·®å¼‚åé¦ˆå®ä¾‹å¯¹é½æ¨¡å—ï¼ˆPDFAï¼‰ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°ä¸ºå…·æœ‰è¾ƒé«˜æ•™å¸ˆ-å­¦ç”Ÿæ£€æµ‹å·®å¼‚çš„å®ä¾‹åˆ†é…æ›´é«˜çš„æƒé‡ï¼Œæœ‰æ•ˆå¤„ç†ç‰¹å®šåŸŸä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†åŸºäºä¸ç¡®å®šæ€§çš„é¢å‘å‰æ™¯çš„å›¾åƒå¯¹é½æ¨¡å—ï¼ˆUFOAï¼‰ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ›´å¤šåœ°å…³æ³¨æ„Ÿå…´è¶£åŒºåŸŸã€‚é€šè¿‡å¹¿æ³›çš„DAODæ•°æ®é›†å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼ŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å…¶ä»–å…ˆè¿›æ›¿ä»£æ–¹æ¡ˆä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢†åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼ˆDAODï¼‰å…³æ³¨æºåŸŸæ£€æµ‹å™¨åœ¨ç›®æ ‡åŸŸçš„æ¨å¹¿ã€‚</li>
<li>æ ¸å¿ƒæ˜¯æºåŸŸå’Œç›®æ ‡åŸŸçš„ç‰¹å¾å¯¹é½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ•´ä½“å¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸåˆ†å¸ƒï¼Œå¿½ç•¥äº†ä¸åŒåŒºåŸŸçš„é‡è¦æ€§å·®å¼‚ã€‚</li>
<li>æå‡ºäº†å·®å¼‚åŒ–ç‰¹å¾å¯¹é½ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„æµ‹å·®å¼‚åé¦ˆå®ä¾‹å¯¹é½æ¨¡å—ï¼ˆPDFAï¼‰ã€‚</li>
<li>PDFAèƒ½è‡ªé€‚åº”åœ°å¤„ç†ç‰¹å®šåŸŸä¿¡æ¯å¹¶ä¸ºé‡è¦å®ä¾‹åˆ†é…æ›´é«˜æƒé‡ã€‚</li>
<li>æå‡ºäº†åŸºäºä¸ç¡®å®šæ€§çš„é¢å‘å‰æ™¯çš„å›¾åƒå¯¹é½æ¨¡å—ï¼ˆUFOAï¼‰ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨æ„Ÿå…´è¶£åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2093a3bc9db42ca6c4dc2361b545b264.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-422950265c270257938e3e6a502020e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0475b7197b846b7dc285f7c2be5a93b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7832ed460219ff36a62e8d13dac9d347.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dce60303dba472d1d43512f0ac1a22ce.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RCTrans-Radar-Camera-Transformer-via-Radar-Densifier-and-Sequential-Decoder-for-3D-Object-Detection"><a href="#RCTrans-Radar-Camera-Transformer-via-Radar-Densifier-and-Sequential-Decoder-for-3D-Object-Detection" class="headerlink" title="RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential   Decoder for 3D Object Detection"></a>RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential   Decoder for 3D Object Detection</h2><p><strong>Authors:Yiheng Li, Yang Yang, Zhen Lei</strong></p>
<p>In radar-camera 3D object detection, the radar point clouds are sparse and noisy, which causes difficulties in fusing camera and radar modalities. To solve this, we introduce a novel query-based detection method named Radar-Camera Transformer (RCTrans). Specifically, we first design a Radar Dense Encoder to enrich the sparse valid radar tokens, and then concatenate them with the image tokens. By doing this, we can fully explore the 3D information of each interest region and reduce the interference of empty tokens during the fusing stage. We then design a Pruning Sequential Decoder to predict 3D boxes based on the obtained tokens and random initialized queries. To alleviate the effect of elevation ambiguity in radar point clouds, we gradually locate the position of the object via a sequential fusion structure. It helps to get more precise and flexible correspondences between tokens and queries. A pruning training strategy is adopted in the decoder, which can save much time during inference and inhibit queries from losing their distinctiveness. Extensive experiments on the large-scale nuScenes dataset prove the superiority of our method, and we also achieve new state-of-the-art radar-camera 3D detection results. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/liyih/RCTrans">https://github.com/liyih/RCTrans</a>. </p>
<blockquote>
<p>åœ¨é›·è¾¾æ‘„åƒå¤´3Dç›®æ ‡æ£€æµ‹ä¸­ï¼Œé›·è¾¾ç‚¹äº‘ç¨€ç–ä¸”å­˜åœ¨å™ªå£°ï¼Œè¿™ç»™æ‘„åƒå¤´å’Œé›·è¾¾æ¨¡å¼çš„èåˆå¸¦æ¥äº†å›°éš¾ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæŸ¥è¯¢çš„æ–°å‹æ£€æµ‹æ–¹æ³•ï¼Œåä¸ºRadar-Camera Transformerï¼ˆRCTransï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªRadar Dense Encoderæ¥ä¸°å¯Œç¨€ç–æœ‰æ•ˆçš„é›·è¾¾ä»¤ç‰Œï¼Œç„¶åå°†å…¶ä¸å›¾åƒä»¤ç‰Œè¿æ¥èµ·æ¥ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥å……åˆ†æ¢ç´¢æ¯ä¸ªæ„Ÿå…´è¶£åŒºåŸŸçš„3Dä¿¡æ¯ï¼Œå¹¶åœ¨èåˆé˜¶æ®µå‡å°‘ç©ºä»¤ç‰Œçš„å½±å“ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªPruning Sequential Decoderï¼ŒåŸºäºè·å¾—çš„ä»¤ç‰Œå’Œéšæœºåˆå§‹åŒ–çš„æŸ¥è¯¢æ¥é¢„æµ‹3Dè¾¹ç•Œæ¡†ã€‚ä¸ºäº†å‡è½»é›·è¾¾ç‚¹äº‘ä¸­é«˜åº¦æ¨¡ç³Šçš„å½±å“ï¼Œæˆ‘ä»¬é€šè¿‡é¡ºåºèåˆç»“æ„é€æ¸å®šä½ç›®æ ‡ä½ç½®ã€‚è¿™æœ‰åŠ©äºè·å¾—ä»¤ç‰Œå’ŒæŸ¥è¯¢ä¹‹é—´æ›´å‡†ç¡®ã€æ›´çµæ´»çš„å¯¹é½å…³ç³»ã€‚è§£ç å™¨é‡‡ç”¨äº†ä¸€ç§ä¿®å‰ªè®­ç»ƒç­–ç•¥ï¼Œè¿™å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­èŠ‚çœå¤§é‡æ—¶é—´å¹¶é˜²æ­¢æŸ¥è¯¢å¤±å»å…¶ç‹¬ç‰¹æ€§ã€‚åœ¨å¤§è§„æ¨¡nuScenesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†æœ€æ–°çš„é›·è¾¾æ‘„åƒå¤´3Dæ£€æµ‹ç»“æœã€‚æˆ‘ä»¬çš„å®ç°å¯è®¿é—®äº<a target="_blank" rel="noopener" href="https://github.com/liyih/RCTrans%E3%80%82">https://github.com/liyih/RCTransã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12799v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹é›·è¾¾ç‚¹äº‘ç¨€ç–å’Œå™ªå£°é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºæŸ¥è¯¢çš„é›·è¾¾-ç›¸æœºTransformerï¼ˆRCTransï¼‰èåˆæ–¹æ³•ã€‚è®¾è®¡é›·è¾¾å¯†é›†ç¼–ç å™¨ï¼ˆRadar Dense Encoderï¼‰å¯¹ç¨€ç–é›·è¾¾ç‚¹äº‘è¿›è¡Œå¢å¼ºå¤„ç†ï¼Œä¸å›¾åƒæ ‡è®°ç¬¦è¿›è¡Œç»“åˆï¼Œåˆ©ç”¨3Dä¿¡æ¯æ¢æµ‹å…´è¶£åŒºåŸŸå¹¶å‡å°‘ç©ºæ ‡è®°ç¬¦çš„å¹²æ‰°ã€‚åŒæ—¶é‡‡ç”¨ä¿®å‰ªåºåˆ—è§£ç å™¨ï¼ˆPruning Sequential Decoderï¼‰é¢„æµ‹åŸºäºè·å¾—çš„æ ‡è®°ç¬¦å’Œéšæœºåˆå§‹åŒ–æŸ¥è¯¢çš„3Dè¾¹ç•Œæ¡†ã€‚é€šè¿‡åºåˆ—èåˆç»“æ„é€æ­¥å®šä½ç‰©ä½“ä½ç½®ï¼Œæé«˜äº†æ ‡è®°ç¬¦ä¸æŸ¥è¯¢ä¹‹é—´çš„ç²¾ç¡®æ€§å’Œçµæ´»æ€§å¯¹åº”ã€‚é‡‡ç”¨ä¿®å‰ªè®­ç»ƒç­–ç•¥åœ¨è§£ç å™¨ä¸­èŠ‚çœæ¨ç†æ—¶é—´å¹¶é˜²æ­¢æŸ¥è¯¢å¤±å»ç‹¬ç‰¹æ€§ã€‚åœ¨å¤§å‹nuScenesæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨é›·è¾¾ç›¸æœº3Dæ£€æµ‹ä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>é›·è¾¾ç‚¹äº‘ç¨€ç–æ€§å’Œå™ªå£°å¯¼è‡´é›·è¾¾å’Œç›¸æœºæ¨¡æ€èåˆå›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæŸ¥è¯¢çš„é›·è¾¾-ç›¸æœºTransformerï¼ˆRCTransï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è®¾è®¡äº†é›·è¾¾å¯†é›†ç¼–ç å™¨æ¥å¢å¼ºç¨€ç–é›·è¾¾ç‚¹äº‘ï¼Œå¹¶å°†å…¶ä¸å›¾åƒæ ‡è®°ç¬¦ç»“åˆï¼Œä»¥åˆ©ç”¨æ¯ä¸ªå…´è¶£åŒºåŸŸçš„3Dä¿¡æ¯å¹¶å‡å°‘å¹²æ‰°ã€‚</li>
<li>é‡‡ç”¨ä¿®å‰ªåºåˆ—è§£ç å™¨é¢„æµ‹åŸºäºæ ‡è®°ç¬¦å’ŒéšæœºæŸ¥è¯¢çš„3Dè¾¹ç•Œæ¡†ã€‚</li>
<li>é€šè¿‡åºåˆ—èåˆç»“æ„é€æ­¥å®šä½ç‰©ä½“ä½ç½®ï¼Œæé«˜äº†æ ‡è®°ç¬¦ä¸æŸ¥è¯¢ä¹‹é—´çš„ç²¾ç¡®æ€§å’Œçµæ´»æ€§å¯¹åº”ã€‚</li>
<li>é‡‡ç”¨ä¿®å‰ªè®­ç»ƒç­–ç•¥ä»¥æé«˜æ¨ç†æ•ˆç‡å¹¶é˜²æ­¢æŸ¥è¯¢å¤±å»ç‹¬ç‰¹æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bc78fff8a31ce4f88d046bcf885ef87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24889bd886066adfdc3ee95e64ad0870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b4c4101078d3a0525dfaeeee369a92f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00e004de03c4815d78e038fee58bbbde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fd7ede296586deea9a59b42579eb72d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ZoRI-Towards-Discriminative-Zero-Shot-Remote-Sensing-Instance-Segmentation"><a href="#ZoRI-Towards-Discriminative-Zero-Shot-Remote-Sensing-Instance-Segmentation" class="headerlink" title="ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance   Segmentation"></a>ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance   Segmentation</h2><p><strong>Authors:Shiqi Huang, Shuting He, Bihan Wen</strong></p>
<p>Instance segmentation algorithms in remote sensing are typically based on conventional methods, limiting their application to seen scenarios and closed-set predictions. In this work, we propose a novel task called zero-shot remote sensing instance segmentation, aimed at identifying aerial objects that are absent from training data. Challenges arise when classifying aerial categories with high inter-class similarity and intra-class variance. Besides, the domain gap between vision-language modelsâ€™ pretraining datasets and remote sensing datasets hinders the zero-shot capabilities of the pretrained model when it is directly applied to remote sensing images. To address these challenges, we propose a $\textbf{Z}$ero-Sh$\textbf{o}$t $\textbf{R}$emote Sensing $\textbf{I}$nstance Segmentation framework, dubbed $\textbf{ZoRI}$. Our approach features a discrimination-enhanced classifier that uses refined textual embeddings to increase the awareness of class disparities. Instead of direct fine-tuning, we propose a knowledge-maintained adaptation strategy that decouples semantic-related information to preserve the pretrained vision-language alignment while adjusting features to capture remote sensing domain-specific visual cues. Additionally, we introduce a prior-injected prediction with cache bank of aerial visual prototypes to supplement the semantic richness of text embeddings and seamlessly integrate aerial representations, adapting to the remote sensing domain. We establish new experimental protocols and benchmarks, and extensive experiments convincingly demonstrate that ZoRI achieves the state-of-art performance on the zero-shot remote sensing instance segmentation task. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HuangShiqi128/ZoRI">https://github.com/HuangShiqi128/ZoRI</a>. </p>
<blockquote>
<p>é¥æ„Ÿé¢†åŸŸçš„å®ä¾‹åˆ†å‰²ç®—æ³•é€šå¸¸åŸºäºä¼ ç»Ÿæ–¹æ³•ï¼Œå°†å…¶åº”ç”¨é™åˆ¶åœ¨å·²çŸ¥åœºæ™¯å’Œå°é—­é›†é¢„æµ‹ä¸Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€é¡¹åä¸ºé›¶æ ·æœ¬é¥æ„Ÿå®ä¾‹åˆ†å‰²çš„æ–°å‹ä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨çš„èˆªç©ºç‰©ä½“ã€‚åœ¨åˆ†ç±»å…·æœ‰é«˜åº¦ç±»é—´ç›¸ä¼¼æ€§å’Œç±»å†…å·®å¼‚çš„èˆªç©ºç±»åˆ«æ—¶ï¼Œä¼šå‡ºç°æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ•°æ®é›†å’Œé¥æ„Ÿæ•°æ®é›†ä¹‹é—´çš„åŸŸå·®è·ï¼Œé˜»ç¢äº†é¢„è®­ç»ƒæ¨¡å‹åœ¨ç›´æ¥åº”ç”¨äºé¥æ„Ÿå›¾åƒæ—¶çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º$\textbf{ZoRI}$ï¼ˆé›¶æ ·æœ¬é¥æ„Ÿå®ä¾‹åˆ†å‰²ï¼‰çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¢å¼ºåˆ¤åˆ«åŠ›çš„åˆ†ç±»å™¨ï¼Œåˆ©ç”¨ç²¾ç»†æ–‡æœ¬åµŒå…¥æ¥æé«˜å¯¹ç±»åˆ«å·®å¼‚çš„è®¤è¯†ã€‚æˆ‘ä»¬å¹¶æœªé‡‡ç”¨ç›´æ¥å¾®è°ƒçš„æ–¹æ³•ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ç§çŸ¥è¯†ä¿æŒé€‚åº”ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥è§£è€¦è¯­ä¹‰ç›¸å…³ä¿¡æ¯ï¼Œåœ¨ä¿æŒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€å¯¹é½çš„åŒæ—¶è°ƒæ•´ç‰¹å¾ä»¥æ•è·é¥æ„Ÿé¢†åŸŸç‰¹å®šçš„è§†è§‰çº¿ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºèˆªç©ºè§†è§‰åŸå‹çš„å…ˆéªŒæ³¨å…¥é¢„æµ‹ç¼“å­˜åº“ï¼Œä»¥è¡¥å……æ–‡æœ¬åµŒå…¥çš„è¯­ä¹‰ä¸°å¯Œæ€§å¹¶æ— ç¼é›†æˆèˆªç©ºè¡¨ç¤ºï¼Œé€‚åº”é¥æ„Ÿé¢†åŸŸã€‚æˆ‘ä»¬å»ºç«‹äº†æ–°çš„å®éªŒåè®®å’ŒåŸºå‡†æµ‹è¯•ï¼Œå¤§é‡å®éªŒæœ‰åŠ›åœ°è¯æ˜ï¼Œåœ¨é›¶æ ·æœ¬é¥æ„Ÿå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒZoRIè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/HuangShiqi128/ZoRI%E3%80%82">https://github.com/HuangShiqi128/ZoRIã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12798v1">PDF</a> AAAI 2025, code see <a target="_blank" rel="noopener" href="https://github.com/HuangShiqi128/ZoRI">https://github.com/HuangShiqi128/ZoRI</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹é¥æ„Ÿå®ä¾‹åˆ†å‰²ä»»åŠ¡â€”â€”é›¶æ ·æœ¬é¥æ„Ÿå®ä¾‹åˆ†å‰²ï¼Œæ—¨åœ¨è¯†åˆ«è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨çš„ç©ºä¸­ç›®æ ‡ã€‚é’ˆå¯¹é¥æ„Ÿå›¾åƒä¸­çš„ç±»åˆ«åˆ†ç±»éš¾é¢˜å’Œé¢†åŸŸå·®è·é—®é¢˜ï¼Œæå‡ºäº†åä¸ºZoRIçš„é›¶æ ·æœ¬é¥æ„Ÿå®ä¾‹åˆ†å‰²æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¢å¼ºåˆ¤åˆ«åŠ›çš„åˆ†ç±»å™¨ï¼Œä½¿ç”¨ç²¾ç»†æ–‡æœ¬åµŒå…¥æé«˜ç±»åˆ«å·®å¼‚çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§çŸ¥è¯†ä¿æŒé€‚åº”ç­–ç•¥ï¼Œä»¥è§£è€¦è¯­ä¹‰ç›¸å…³ä¿¡æ¯å¹¶ä¿ç•™é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€å¯¹é½ï¼Œè°ƒæ•´ç‰¹å¾ä»¥æ•æ‰é¥æ„Ÿç‰¹å®šè§†è§‰çº¿ç´¢ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†åŸºäºç©ºä¸­è§†è§‰åŸå‹çš„å…ˆéªŒæ³¨å…¥é¢„æµ‹ç¼“å­˜åº“ï¼Œä»¥è¡¥å……æ–‡æœ¬åµŒå…¥çš„è¯­ä¹‰ä¸°å¯Œæ€§å¹¶æ— ç¼é›†æˆç©ºä¸­è¡¨ç¤ºï¼Œé€‚åº”é¥æ„Ÿé¢†åŸŸã€‚å®éªŒè¯æ˜ZoRIåœ¨é›¶æ ·æœ¬é¥æ„Ÿå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹é¥æ„Ÿå®ä¾‹åˆ†å‰²ä»»åŠ¡â€”â€”é›¶æ ·æœ¬é¥æ„Ÿå®ä¾‹åˆ†å‰²ï¼Œé’ˆå¯¹è®­ç»ƒæ•°æ®ä¸­æœªå‡ºç°çš„ç©ºä¸­ç›®æ ‡è¿›è¡Œè¯†åˆ«ã€‚</li>
<li>æå‡ºäº†åä¸ºZoRIçš„æ¡†æ¶æ¥è§£å†³é›¶æ ·æœ¬é¥æ„Ÿå®ä¾‹åˆ†å‰²çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç±»åˆ«åˆ†ç±»éš¾é¢˜å’Œé¢†åŸŸå·®è·é—®é¢˜ã€‚</li>
<li>ZoRIæ¡†æ¶é‡‡ç”¨å¢å¼ºåˆ¤åˆ«åŠ›çš„åˆ†ç±»å™¨ï¼Œåˆ©ç”¨ç²¾ç»†æ–‡æœ¬åµŒå…¥æ¥æé«˜ç±»åˆ«å·®å¼‚çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§çŸ¥è¯†ä¿æŒé€‚åº”ç­–ç•¥ï¼Œé€šè¿‡è§£è€¦è¯­ä¹‰ç›¸å…³ä¿¡æ¯æ¥ä¿ç•™é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€å¯¹é½ï¼Œå¹¶è°ƒæ•´ç‰¹å¾ä»¥æ•æ‰é¥æ„Ÿç‰¹å®šè§†è§‰çº¿ç´¢ã€‚</li>
<li>å¼•å…¥äº†åŸºäºç©ºä¸­è§†è§‰åŸå‹çš„å…ˆéªŒæ³¨å…¥é¢„æµ‹ç¼“å­˜åº“ï¼Œä»¥ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯å’Œæ— ç¼é›†æˆç©ºä¸­è¡¨ç¤ºï¼Œé€‚åº”é¥æ„Ÿé¢†åŸŸã€‚</li>
<li>å®éªŒè¯æ˜ZoRIåœ¨é›¶æ ·æœ¬é¥æ„Ÿå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e91af805b1618a4747e07fb90d68b110.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7b45dcd579a4422f06d70b385a24748.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73028b73b3e441f041a42ab6abf1dae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f5e583e4c6d0e8c2ed5da7cef9beeeb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Structural-Pruning-via-Spatial-aware-Information-Redundancy-for-Semantic-Segmentation"><a href="#Structural-Pruning-via-Spatial-aware-Information-Redundancy-for-Semantic-Segmentation" class="headerlink" title="Structural Pruning via Spatial-aware Information Redundancy for Semantic   Segmentation"></a>Structural Pruning via Spatial-aware Information Redundancy for Semantic   Segmentation</h2><p><strong>Authors:Dongyue Wu, Zilin Guo, Li Yu, Nong Sang, Changxin Gao</strong></p>
<p>In recent years, semantic segmentation has flourished in various applications. However, the high computational cost remains a significant challenge that hinders its further adoption. The filter pruning method for structured network slimming offers a direct and effective solution for the reduction of segmentation networks. Nevertheless, we argue that most existing pruning methods, originally designed for image classification, overlook the fact that segmentation is a location-sensitive task, which consequently leads to their suboptimal performance when applied to segmentation networks. To address this issue, this paper proposes a novel approach, denoted as Spatial-aware Information Redundancy Filter Pruning<del>(SIRFP), which aims to reduce feature redundancy between channels. First, we formulate the pruning process as a maximum edge weight clique problem</del>(MEWCP) in graph theory, thereby minimizing the redundancy among the remaining features after pruning. Within this framework, we introduce a spatial-aware redundancy metric based on feature maps, thus endowing the pruning process with location sensitivity to better adapt to pruning segmentation networks. Additionally, based on the MEWCP, we propose a low computational complexity greedy strategy to solve this NP-hard problem, making it feasible and efficient for structured pruning. To validate the effectiveness of our method, we conducted extensive comparative experiments on various challenging datasets. The results demonstrate the superior performance of SIRFP for semantic segmentation tasks. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè¯­ä¹‰åˆ†å‰²åœ¨å„ç§åº”ç”¨ä¸­è“¬å‹ƒå‘å±•ã€‚ç„¶è€Œï¼Œé«˜è®¡ç®—æˆæœ¬ä»ç„¶æ˜¯é˜»ç¢å…¶è¿›ä¸€æ­¥é‡‡ç”¨çš„ä¸€å¤§æŒ‘æˆ˜ã€‚é’ˆå¯¹ç»“æ„åŒ–ç½‘ç»œç˜¦èº«ï¼ˆstructured network slimmingï¼‰çš„æ»¤æ³¢å™¨å‰ªææ–¹æ³•ä¸ºè§£å†³åˆ†å‰²ç½‘ç»œçš„å‡å°‘é—®é¢˜æä¾›äº†ç›´æ¥æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºå¤§å¤šæ•°ç°æœ‰çš„å‰ªææ–¹æ³•æœ€åˆæ˜¯ä¸ºå›¾åƒåˆ†ç±»è€Œè®¾è®¡çš„ï¼Œå¿½ç•¥äº†åˆ†å‰²æ˜¯ä¸€ä¸ªä½ç½®æ•æ„Ÿçš„ä»»åŠ¡è¿™ä¸€äº‹å®ï¼Œå› æ­¤å½“åº”ç”¨äºåˆ†å‰²ç½‘ç»œæ—¶ï¼Œå…¶æ€§èƒ½è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºç©ºé—´æ„ŸçŸ¥ä¿¡æ¯å†—ä½™æ»¤æ³¢å™¨å‰ªæï¼ˆSIRFPï¼‰ï¼Œæ—¨åœ¨å‡å°‘é€šé“ä¹‹é—´çš„ç‰¹å¾å†—ä½™ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†å‰ªæè¿‡ç¨‹å…¬å¼åŒ–ä¸ºå›¾è®ºä¸­çš„æœ€å¤§è¾¹æƒé‡é›†å›¢é—®é¢˜ï¼ˆMEWCPï¼‰ï¼Œä»è€Œæœ€å°åŒ–å‰ªæåå‰©ä½™ç‰¹å¾ä¹‹é—´çš„å†—ä½™ã€‚åœ¨è¿™ä¸ªæ¡†æ¶å†…ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºç‰¹å¾å›¾çš„å…·æœ‰ç©ºé—´æ„ŸçŸ¥çš„å†—ä½™åº¦é‡æŒ‡æ ‡ï¼Œä»è€Œä½¿å‰ªæè¿‡ç¨‹å…·æœ‰ä½ç½®æ•æ„Ÿæ€§ï¼Œæ›´å¥½åœ°é€‚åº”åˆ†å‰²ç½‘ç»œçš„å‰ªæã€‚æ­¤å¤–ï¼ŒåŸºäºMEWCPï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½è®¡ç®—å¤æ‚åº¦çš„è´ªå©ªç­–ç•¥æ¥è§£å†³è¿™ä¸ªNPéš¾é¢˜ï¼Œä½¿ç»“æ„åŒ–å‰ªæå˜å¾—å¯è¡Œä¸”é«˜æ•ˆã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å¯¹æ¯”å®éªŒã€‚ç»“æœè¡¨æ˜SIRFPåœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12672v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSIRFPçš„ç©ºé—´æ„ŸçŸ¥ä¿¡æ¯å†—ä½™æ»¤æ³¢å™¨å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘åˆ†å‰²ç½‘ç»œä¸­çš„ç‰¹å¾å†—ä½™ã€‚é€šè¿‡å›¾è®ºä¸­çš„æœ€å¤§è¾¹æƒé‡é›†å›¢é—®é¢˜æ¥è¡¨è¿°å‰ªæè¿‡ç¨‹ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºç‰¹å¾å›¾çš„ç©ºæ„ŸçŸ¥å†—ä½™åº¦é‡ï¼Œä½¿å‰ªæè¿‡ç¨‹å…·æœ‰ä½ç½®æ•æ„Ÿæ€§ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”åˆ†å‰²ç½‘ç»œçš„å‰ªæã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSIRFPåœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­ä¹‰åˆ†å‰²åœ¨å¤šä¸ªåº”ç”¨é¢†åŸŸä¸­è“¬å‹ƒå‘å±•ï¼Œä½†é«˜è®¡ç®—æˆæœ¬ä»æ˜¯å…¶è¿›ä¸€æ­¥åº”ç”¨çš„é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>è¿‡æ»¤å™¨å‰ªææ–¹æ³•å¯ä½œä¸ºç»“æ„åŒ–ç½‘ç»œç˜¦èº«çš„ä¸€ç§ç›´æ¥å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰çš„å‰ªææ–¹æ³•å¿½ç•¥äº†åˆ†å‰²æ˜¯ä½ç½®æ•æ„Ÿçš„ä»»åŠ¡ï¼Œå¯¼è‡´åœ¨åˆ†å‰²ç½‘ç»œä¸­çš„åº”ç”¨è¡¨ç°ä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„SIRFPæ–¹æ³•æ—¨åœ¨å‡å°‘é€šé“é—´çš„ç‰¹å¾å†—ä½™ï¼Œå°†å‰ªæè¿‡ç¨‹è¡¨è¿°ä¸ºå›¾è®ºä¸­çš„æœ€å¤§è¾¹æƒé‡é›†å›¢é—®é¢˜ã€‚</li>
<li>å¼•å…¥åŸºäºç‰¹å¾å›¾çš„ç©ºæ„ŸçŸ¥å†—ä½™åº¦é‡ï¼Œä½¿å‰ªæè¿‡ç¨‹å…·æœ‰ä½ç½®æ•æ„Ÿæ€§ã€‚</li>
<li>æå‡ºä¸€ç§ä½è®¡ç®—å¤æ‚åº¦çš„è´ªå©ªç­–ç•¥æ¥è§£å†³NPéš¾é¢˜ï¼Œå®ç°ç»“æ„åŒ–å‰ªæçš„å¯è¡Œæ€§å’Œæ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff1d4c09d4e9b2e914f1ad96bdecce0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b69e8a046602f183bdf9a552b9bf62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37dac4ddb602c1bc8e68d78f8e8bf2c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b917711dbcc526cf422c059135d593d4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SEG-SAM-Semantic-Guided-SAM-for-Unified-Medical-Image-Segmentation"><a href="#SEG-SAM-Semantic-Guided-SAM-for-Unified-Medical-Image-Segmentation" class="headerlink" title="SEG-SAM: Semantic-Guided SAM for Unified Medical Image Segmentation"></a>SEG-SAM: Semantic-Guided SAM for Unified Medical Image Segmentation</h2><p><strong>Authors:Shuangping Huang, Hao Liang, Qingfeng Wang, Chulong Zhong, Zijian Zhou, Miaojing Shi</strong></p>
<p>Recently, developing unified medical image segmentation models gains increasing attention, especially with the advent of the Segment Anything Model (SAM). SAM has shown promising binary segmentation performance in natural domains, however, transferring it to the medical domain remains challenging, as medical images often possess substantial inter-category overlaps. To address this, we propose the SEmantic-Guided SAM (SEG-SAM), a unified medical segmentation model that incorporates semantic medical knowledge to enhance medical segmentation performance. First, to avoid the potential conflict between binary and semantic predictions, we introduce a semantic-aware decoder independent of SAMâ€™s original decoder, specialized for both semantic segmentation on the prompted object and classification on unprompted objects in images. To further enhance the modelâ€™s semantic understanding, we solicit key characteristics of medical categories from large language models and incorporate them into SEG-SAM through a text-to-vision semantic module, adaptively transferring the language information into the visual segmentation task. In the end, we introduce the cross-mask spatial alignment strategy to encourage greater overlap between the predicted masks from SEG-SAMâ€™s two decoders, thereby benefiting both predictions. Extensive experiments demonstrate that SEG-SAM outperforms state-of-the-art SAM-based methods in unified binary medical segmentation and task-specific methods in semantic medical segmentation, showcasing promising results and potential for broader medical applications. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¼€å‘ç»Ÿä¸€åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå°¤å…¶æ˜¯éšç€Segment Anything Modelï¼ˆSAMï¼‰çš„å‡ºç°ã€‚SAMåœ¨è‡ªç„¶é¢†åŸŸçš„äºŒè¿›åˆ¶åˆ†å‰²æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„å‰æ™¯ï¼Œç„¶è€Œå°†å…¶è½¬ç§»åˆ°åŒ»å­¦é¢†åŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºåŒ»å­¦å›¾åƒé€šå¸¸å­˜åœ¨å¤§é‡çš„ç±»åˆ«é—´é‡å ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SEmantic-Guided SAMï¼ˆSEG-SAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¯­ä¹‰åŒ»å­¦çŸ¥è¯†æ¥å¢å¼ºåŒ»å­¦åˆ†å‰²æ€§èƒ½çš„ç»Ÿä¸€çš„åŒ»å­¦åˆ†å‰²æ¨¡å‹ã€‚é¦–å…ˆï¼Œä¸ºäº†é¿å…äºŒè¿›åˆ¶å’Œè¯­ä¹‰é¢„æµ‹ä¹‹é—´çš„æ½œåœ¨å†²çªï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç‹¬ç«‹äºSAMåŸå§‹è§£ç å™¨çš„è¯­ä¹‰æ„ŸçŸ¥è§£ç å™¨ï¼Œä¸“é—¨ç”¨äºå›¾åƒä¸­å¯¹æç¤ºå¯¹è±¡çš„è¯­ä¹‰åˆ†å‰²ä»¥åŠå¯¹æœªæç¤ºå¯¹è±¡çš„åˆ†ç±»ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰ç†è§£ï¼Œæˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–åŒ»å­¦ç±»åˆ«çš„å…³é”®ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ–‡æœ¬åˆ°è§†è§‰è¯­ä¹‰æ¨¡å—å°†å…¶èå…¥SEG-SAMä¸­ï¼Œè‡ªé€‚åº”åœ°å°†è¯­è¨€ä¿¡æ¯è½¬ç§»åˆ°è§†è§‰åˆ†å‰²ä»»åŠ¡ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ©è†œç©ºé—´å¯¹é½ç­–ç•¥ï¼Œä»¥é¼“åŠ±SEG-SAMçš„ä¸¤ä¸ªè§£ç å™¨äº§ç”Ÿçš„é¢„æµ‹æ©è†œä¹‹é—´æ›´å¤§çš„é‡å ï¼Œä»è€Œæœ‰åˆ©äºä¸¤ç§é¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSEG-SAMåœ¨ç»Ÿä¸€çš„äºŒå…ƒåŒ»å­¦åˆ†å‰²æ–¹é¢ä¼˜äºæœ€å…ˆè¿›SAMæ–¹æ³•ï¼Œåœ¨ç‰¹å®šä»»åŠ¡çš„è¯­ä¹‰åŒ»å­¦åˆ†å‰²æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œæ½œåœ¨çš„åŒ»å­¦åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12660v1">PDF</a> 12 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹åœ¨è‡ªç„¶é¢†åŸŸå·²å±•ç°å‡ºæœ‰å‰æ™¯çš„äºŒå…ƒåˆ†å‰²æ€§èƒ½ï¼Œä½†å°†å…¶åº”ç”¨äºåŒ»å­¦é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSEG-SAMæ¨¡å‹ï¼Œç»“åˆè¯­ä¹‰åŒ»å­¦çŸ¥è¯†æå‡åŒ»å­¦åˆ†å‰²æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥è¯­ä¹‰æ„ŸçŸ¥è§£ç å™¨é¿å…äºŒå…ƒå’Œè¯­ä¹‰é¢„æµ‹ä¹‹é—´çš„æ½œåœ¨å†²çªï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹è·å–åŒ»å­¦ç±»åˆ«çš„å…³é”®ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ–‡æœ¬åˆ°è§†è§‰çš„è¯­ä¹‰æ¨¡å—å°†å…¶èå…¥SEG-SAMã€‚å®éªŒè¡¨æ˜ï¼ŒSEG-SAMåœ¨ç»Ÿä¸€äºŒå…ƒåŒ»å­¦åˆ†å‰²å’Œè¯­ä¹‰åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMæ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸåº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºåŒ»å­¦å›¾åƒå­˜åœ¨å¤§é‡çš„ç±»åˆ«é—´é‡å ã€‚</li>
<li>SEG-SAMæ¨¡å‹é€šè¿‡ç»“åˆè¯­ä¹‰åŒ»å­¦çŸ¥è¯†æå‡åŒ»å­¦åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>SEG-SAMå¼•å…¥è¯­ä¹‰æ„ŸçŸ¥è§£ç å™¨ï¼Œç‹¬ç«‹äºSAMåŸå§‹è§£ç å™¨ï¼Œèƒ½åŒæ—¶è¿›è¡Œè¯­ä¹‰åˆ†å‰²å’Œæœªæç¤ºå¯¹è±¡çš„åˆ†ç±»ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è·å–åŒ»å­¦ç±»åˆ«çš„å…³é”®ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ–‡æœ¬åˆ°è§†è§‰çš„è¯­ä¹‰æ¨¡å—èå…¥SEG-SAMï¼Œå¢å¼ºæ¨¡å‹å¯¹è¯­ä¹‰çš„ç†è§£ã€‚</li>
<li>SEG-SAMé‡‡ç”¨è·¨æ©è†œç©ºé—´å¯¹é½ç­–ç•¥ï¼Œé¼“åŠ±ä¸¤ä¸ªè§£ç å™¨äº§ç”Ÿçš„é¢„æµ‹æ©è†œæ›´å¤§ç¨‹åº¦é‡å ï¼Œä»è€Œæœ‰åˆ©äºä¸¤ç§é¢„æµ‹ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒSEG-SAMåœ¨ç»Ÿä¸€äºŒå…ƒåŒ»å­¦åˆ†å‰²å’Œè¯­ä¹‰åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›SAMæ–¹æ³•å’Œç‰¹å®šä»»åŠ¡æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-08a32d1954892759a4d38fd62cdc50d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbe3ba1d12b414f71e943a0842d8e6cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56f3b84a1d972066dd6724985ce37556.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DuSSS-Dual-Semantic-Similarity-Supervised-Vision-Language-Model-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#DuSSS-Dual-Semantic-Similarity-Supervised-Vision-Language-Model-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for   Semi-Supervised Medical Image Segmentation"></a>DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Qingtao Pan, Wenhao Qiao, Jingjiao Lou, Bing Ji, Shuo Li</strong></p>
<p>Semi-supervised medical image segmentation (SSMIS) uses consistency learning to regularize model training, which alleviates the burden of pixel-wise manual annotations. However, it often suffers from error supervision from low-quality pseudo labels. Vision-Language Model (VLM) has great potential to enhance pseudo labels by introducing text prompt guided multimodal supervision information. It nevertheless faces the cross-modal problem: the obtained messages tend to correspond to multiple targets. To address aforementioned problems, we propose a Dual Semantic Similarity-Supervised VLM (DuSSS) for SSMIS. Specifically, 1) a Dual Contrastive Learning (DCL) is designed to improve cross-modal semantic consistency by capturing intrinsic representations within each modality and semantic correlations across modalities. 2) To encourage the learning of multiple semantic correspondences, a Semantic Similarity-Supervision strategy (SSS) is proposed and injected into each contrastive learning process in DCL, supervising semantic similarity via the distribution-based uncertainty levels. Furthermore, a novel VLM-based SSMIS network is designed to compensate for the quality deficiencies of pseudo-labels. It utilizes the pretrained VLM to generate text prompt guided supervision information, refining the pseudo label for better consistency regularization. Experimental results demonstrate that our DuSSS achieves outstanding performance with Dice of 82.52%, 74.61% and 78.03% on three public datasets (QaTa-COV19, BM-Seg and MoNuSeg). </p>
<blockquote>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰åˆ©ç”¨ä¸€è‡´æ€§å­¦ä¹ æ¥è§„èŒƒæ¨¡å‹è®­ç»ƒï¼Œå‡è½»äº†åƒç´ çº§æ‰‹åŠ¨æ ‡æ³¨çš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œå®ƒå¸¸å¸¸å—åˆ°ä½è´¨é‡ä¼ªæ ‡ç­¾çš„é”™è¯¯ç›‘ç£çš„å½±å“ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡å¼•å…¥æ–‡æœ¬æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€ç›‘ç£ä¿¡æ¯æ¥æé«˜ä¼ªæ ‡ç­¾çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒé¢ä¸´ç€è·¨æ¨¡æ€çš„é—®é¢˜ï¼šè·å–çš„ä¿¡æ¯å¾€å¾€å¯¹åº”å¤šä¸ªç›®æ ‡ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºSSMISçš„åŒè¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£VLMï¼ˆDuSSSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œ1ï¼‰è®¾è®¡äº†åŒå¯¹æ¯”å­¦ä¹ ï¼ˆDCLï¼‰ï¼Œé€šè¿‡æ•æ‰æ¯ç§æ¨¡æ€çš„å†…åœ¨è¡¨ç¤ºå’Œè·¨æ¨¡æ€çš„è¯­ä¹‰å…³è”ï¼Œæé«˜è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ã€‚2ï¼‰ä¸ºäº†é¼“åŠ±å­¦ä¹ å¤šä¸ªè¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œæå‡ºäº†è¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£ç­–ç•¥ï¼ˆSSSï¼‰ï¼Œå¹¶å°†å…¶æ³¨å…¥DCLä¸­çš„æ¯ä¸ªå¯¹æ¯”å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡åŸºäºåˆ†å¸ƒçš„çš„ä¸ç¡®å®šæ€§æ°´å¹³ç›‘ç£è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºVLMçš„SSMISç½‘ç»œï¼Œä»¥å¼¥è¡¥ä¼ªæ ‡ç­¾çš„è´¨é‡ç¼ºé™·ã€‚å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„VLMç”Ÿæˆæ–‡æœ¬æç¤ºå¼•å¯¼çš„ç›‘ç£ä¿¡æ¯ï¼Œå¯¹ä¼ªæ ‡ç­¾è¿›è¡Œç»†åŒ–ï¼Œä»¥æ›´å¥½åœ°å®ç°ä¸€è‡´æ€§æ­£åˆ™åŒ–ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„DuSSSåœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆQaTa-COV19ã€BM-Segå’ŒMoNuSegï¼‰ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒDiceåˆ†åˆ«ä¸º82.52%ã€74.61%å’Œ78.03%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12492v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŠç›‘ç£åˆ†å‰²ï¼ˆSSMISï¼‰é€šè¿‡ä¸€è‡´æ€§å­¦ä¹ è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå‡è½»åƒç´ çº§æ‰‹åŠ¨æ ‡æ³¨çš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œå®ƒå¸¸å¸¸å—åˆ°ä½è´¨é‡ä¼ªæ ‡ç­¾çš„é”™è¯¯ç›‘ç£å½±å“ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºåŒé‡è¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£çš„è·¨æ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆDuSSSï¼‰ï¼Œé€šè¿‡åŒé‡å¯¹æ¯”å­¦ä¹ ï¼ˆDCLï¼‰æé«˜è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ï¼Œé€šè¿‡è¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£ç­–ç•¥ï¼ˆSSSï¼‰é¼“åŠ±å­¦ä¹ å¤šä¸ªè¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¹¶è®¾è®¡æ–°å‹SSMISç½‘ç»œä»¥è¡¥å¿ä¼ªæ ‡ç­¾è´¨é‡ä¸è¶³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDuSSSåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²åˆ©ç”¨ä¸€è‡´æ€§å­¦ä¹ å‡è½»æ‰‹åŠ¨æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>ä½è´¨é‡ä¼ªæ ‡ç­¾å¯èƒ½å¯¼è‡´é”™è¯¯ç›‘ç£çš„é—®é¢˜ã€‚</li>
<li>æå‡ºDuSSSæ–¹æ³•ï¼Œç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¢å¼ºä¼ªæ ‡ç­¾è´¨é‡ã€‚</li>
<li>DuSSSä½¿ç”¨åŒé‡å¯¹æ¯”å­¦ä¹ ï¼ˆDCLï¼‰æé«˜è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>è¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£ç­–ç•¥ï¼ˆSSSï¼‰é¼“åŠ±å­¦ä¹ å¤šä¸ªè¯­ä¹‰å¯¹åº”å…³ç³»ã€‚</li>
<li>è®¾è®¡æ–°å‹SSMISç½‘ç»œä»¥è¡¥å¿ä¼ªæ ‡ç­¾è´¨é‡ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a7239e0c9bcb715b290fbf64ebabc295.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f89b040b8a468f5823b755eadd1ef7bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-198c30e94c70e5a3f4feeba5d1a3cb07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-235ce8abb7612cf23263f02516ed03aa.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Exploring-Semantic-Consistency-and-Style-Diversity-for-Domain-Generalized-Semantic-Segmentation"><a href="#Exploring-Semantic-Consistency-and-Style-Diversity-for-Domain-Generalized-Semantic-Segmentation" class="headerlink" title="Exploring Semantic Consistency and Style Diversity for Domain   Generalized Semantic Segmentation"></a>Exploring Semantic Consistency and Style Diversity for Domain   Generalized Semantic Segmentation</h2><p><strong>Authors:Hongwei Niu, Linhuang Xie, Jianghang Lin, Shengchuan Zhang</strong></p>
<p>Domain Generalized Semantic Segmentation (DGSS) seeks to utilize source domain data exclusively to enhance the generalization of semantic segmentation across unknown target domains. Prevailing studies predominantly concentrate on feature normalization and domain randomization, these approaches exhibit significant limitations. Feature normalization-based methods tend to confuse semantic features in the process of constraining the feature space distribution, resulting in classification misjudgment. Domain randomization-based methods frequently incorporate domain-irrelevant noise due to the uncontrollability of style transformations, resulting in segmentation ambiguity. To address these challenges, we introduce a novel framework, named SCSD for Semantic Consistency prediction and Style Diversity generalization. It comprises three pivotal components: Firstly, a Semantic Query Booster is designed to enhance the semantic awareness and discrimination capabilities of object queries in the mask decoder, enabling cross-domain semantic consistency prediction. Secondly, we develop a Text-Driven Style Transform module that utilizes domain difference text embeddings to controllably guide the style transformation of image features, thereby increasing inter-domain style diversity. Lastly, to prevent the collapse of similar domain feature spaces, we introduce a Style Synergy Optimization mechanism that fortifies the separation of inter-domain features and the aggregation of intra-domain features by synergistically weighting style contrastive loss and style aggregation loss. Extensive experiments demonstrate that the proposed SCSD significantly outperforms existing state-of-theart methods. Notably, SCSD trained on GTAV achieved an average of 49.11 mIoU on the four unseen domain datasets, surpassing the previous state-of-the-art method by +4.08 mIoU. Code is available at <a target="_blank" rel="noopener" href="https://github.com/nhw649/SCSD">https://github.com/nhw649/SCSD</a>. </p>
<blockquote>
<p>é¢†åŸŸé€šç”¨è¯­ä¹‰åˆ†å‰²ï¼ˆDGSSï¼‰æ—¨åœ¨ä»…åˆ©ç”¨æºåŸŸæ•°æ®æ¥æé«˜åœ¨æœªçŸ¥ç›®æ ‡åŸŸä¸Šè¯­ä¹‰åˆ†å‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å¾å½’ä¸€åŒ–å’Œé¢†åŸŸéšæœºåŒ–ä¸Šï¼Œä½†è¿™äº›æ–¹æ³•å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚åŸºäºç‰¹å¾å½’ä¸€åŒ–çš„æ–¹æ³•å¾€å¾€ä¼šåœ¨çº¦æŸç‰¹å¾ç©ºé—´åˆ†å¸ƒçš„è¿‡ç¨‹ä¸­æ··æ·†è¯­ä¹‰ç‰¹å¾ï¼Œå¯¼è‡´åˆ†ç±»åˆ¤æ–­é”™è¯¯ã€‚è€ŒåŸºäºé¢†åŸŸéšæœºåŒ–çš„æ–¹æ³•ç”±äºé£æ ¼è½¬æ¢çš„ä¸å¯æ§æ€§ï¼Œç»å¸¸å¼•å…¥ä¸é¢†åŸŸæ— å…³çš„å™ªå£°ï¼Œå¯¼è‡´åˆ†å‰²æ¨¡ç³Šã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºSCSDï¼ˆè¯­ä¹‰ä¸€è‡´æ€§é¢„æµ‹å’Œé£æ ¼å¤šæ ·æ€§æ³›åŒ–ï¼‰çš„æ–°æ¡†æ¶ã€‚å®ƒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šé¦–å…ˆï¼Œè®¾è®¡äº†ä¸€ç§è¯­ä¹‰æŸ¥è¯¢å¢å¼ºå™¨ï¼Œç”¨äºæé«˜æ©è†œè§£ç å™¨ä¸­å¯¹è±¡æŸ¥è¯¢çš„è¯­ä¹‰æ„è¯†å’Œè¾¨åˆ«èƒ½åŠ›ï¼Œä»è€Œå®ç°è·¨åŸŸè¯­ä¹‰ä¸€è‡´æ€§é¢„æµ‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ–‡æœ¬é©±åŠ¨çš„é£æ ¼è½¬æ¢æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨é¢†åŸŸå·®å¼‚æ–‡æœ¬åµŒå…¥æ¥å¯æ§åœ°æŒ‡å¯¼å›¾åƒç‰¹å¾çš„æ ·å¼è½¬æ¢ï¼Œä»è€Œå¢åŠ è·¨é¢†åŸŸçš„é£æ ¼å¤šæ ·æ€§ã€‚æœ€åï¼Œä¸ºäº†é˜²æ­¢ç±»ä¼¼é¢†åŸŸç‰¹å¾ç©ºé—´çš„å´©æºƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é£æ ¼ååŒä¼˜åŒ–æœºåˆ¶ï¼Œé€šè¿‡ååŒæƒé‡é£æ ¼å¯¹æ¯”æŸå¤±å’Œé£æ ¼èšåˆæŸå¤±ï¼ŒåŠ å¼ºäº†è·¨é¢†åŸŸç‰¹å¾çš„åˆ†ç¦»å’Œé¢†åŸŸå†…ç‰¹å¾çš„èšåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SCSDæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSCSDåœ¨GTAVä¸Šè®­ç»ƒï¼Œåœ¨å››ä¸ªæœªè§è¿‡çš„é¢†åŸŸæ•°æ®é›†ä¸Šå¹³å‡è¾¾åˆ°49.11 mIoUï¼Œæ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•é«˜å‡º+4.08 mIoUã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nhw649/SCSD">https://github.com/nhw649/SCSD</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12050v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>     é¢†åŸŸé€šç”¨è¯­ä¹‰åˆ†å‰²ï¼ˆDGSSï¼‰åˆ©ç”¨æºåŸŸæ•°æ®æå‡åœ¨æœªçŸ¥ç›®æ ‡åŸŸä¸Šçš„è¯­ä¹‰åˆ†å‰²æ³›åŒ–èƒ½åŠ›ã€‚å½“å‰ä¸»æµæ–¹æ³•é›†ä¸­åœ¨ç‰¹å¾å½’ä¸€åŒ–å’Œé¢†åŸŸéšæœºåŒ–ä¸Šï¼Œä½†å­˜åœ¨æ··æ·†è¯­ä¹‰ç‰¹å¾å’Œå¼•å…¥é¢†åŸŸä¸ç›¸å…³å™ªå£°çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†SCSDæ¡†æ¶ï¼ŒåŒ…å«è¯­ä¹‰æŸ¥è¯¢å¢å¼ºå™¨ã€æ–‡æœ¬é©±åŠ¨é£æ ¼è½¬æ¢æ¨¡å—ä»¥åŠé£æ ¼ååŒä¼˜åŒ–æœºåˆ¶ã€‚SCSDåœ¨å››ä¸ªæœªè§åŸŸæ•°æ®é›†ä¸Šçš„å¹³å‡mIoUè¾¾åˆ°49.11ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DGSSæ—¨åœ¨åˆ©ç”¨æºåŸŸæ•°æ®æå‡è¯­ä¹‰åˆ†å‰²åœ¨æœªçŸ¥ç›®æ ‡åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨ç‰¹å¾å½’ä¸€åŒ–å’Œé¢†åŸŸéšæœºåŒ–ï¼Œä½†å­˜åœ¨è¯­ä¹‰ç‰¹å¾æ··æ·†å’Œå¼•å…¥ä¸ç›¸å…³å™ªå£°çš„é—®é¢˜ã€‚</li>
<li>SCSDæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šè¯­ä¹‰æŸ¥è¯¢å¢å¼ºå™¨ã€æ–‡æœ¬é©±åŠ¨é£æ ¼è½¬æ¢æ¨¡å—å’Œé£æ ¼ååŒä¼˜åŒ–æœºåˆ¶ã€‚</li>
<li>è¯­ä¹‰æŸ¥è¯¢å¢å¼ºå™¨æå‡å¯¹è±¡æŸ¥è¯¢çš„è¯­ä¹‰æ„è¯†å’Œè¾¨åˆ«èƒ½åŠ›ï¼Œå®ç°è·¨åŸŸè¯­ä¹‰ä¸€è‡´æ€§é¢„æµ‹ã€‚</li>
<li>æ–‡æœ¬é©±åŠ¨é£æ ¼è½¬æ¢æ¨¡å—åˆ©ç”¨é¢†åŸŸå·®å¼‚æ–‡æœ¬åµŒå…¥æ¥å¯æ§åœ°å¼•å¯¼å›¾åƒç‰¹å¾çš„æ ·å¼è½¬æ¢ï¼Œå¢åŠ è·¨åŸŸé£æ ¼å¤šæ ·æ€§ã€‚</li>
<li>é£æ ¼ååŒä¼˜åŒ–æœºåˆ¶é€šè¿‡ååŒåŠ æƒé£æ ¼å¯¹æ¯”æŸå¤±å’Œé£æ ¼èšåˆæŸå¤±ï¼Œå¼ºåŒ–åŸŸé—´ç‰¹å¾åˆ†ç¦»å’ŒåŸŸå†…ç‰¹å¾èšåˆã€‚</li>
<li>SCSDåœ¨å››ä¸ªæœªè§åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡mIoUè¾¾åˆ°49.11ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-113b36cf6253c86d091442ea8f5af80f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a40d0b04c99ecd14211ec14bf105bafe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6faa6d78539c9a19a2628994dd3dff9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8913c048e936ccdcba061549922c2d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8500f43f4ae35b44b9d74b2c24b8e7b4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Coconut-Palm-Tree-Counting-on-Drone-Images-with-Deep-Object-Detection-and-Synthetic-Training-Data"><a href="#Coconut-Palm-Tree-Counting-on-Drone-Images-with-Deep-Object-Detection-and-Synthetic-Training-Data" class="headerlink" title="Coconut Palm Tree Counting on Drone Images with Deep Object Detection   and Synthetic Training Data"></a>Coconut Palm Tree Counting on Drone Images with Deep Object Detection   and Synthetic Training Data</h2><p><strong>Authors:Tobias Rohe, Barbara BÃ¶hm, Michael KÃ¶lle, Jonas Stein, Robert MÃ¼ller, Claudia Linnhoff-Popien</strong></p>
<p>Drones have revolutionized various domains, including agriculture. Recent advances in deep learning have propelled among other things object detection in computer vision. This study utilized YOLO, a real-time object detector, to identify and count coconut palm trees in Ghanaian farm drone footage. The farm presented has lost track of its trees due to different planting phases. While manual counting would be very tedious and error-prone, accurately determining the number of trees is crucial for efficient planning and management of agricultural processes, especially for optimizing yields and predicting production. We assessed YOLO for palm detection within a semi-automated framework, evaluated accuracy augmentations, and pondered its potential for farmers. Data was captured in September 2022 via drones. To optimize YOLO with scarce data, synthetic images were created for model training and validation. The YOLOv7 model, pretrained on the COCO dataset (excluding coconut palms), was adapted using tailored data. Trees from footage were repositioned on synthetic images, with testing on distinct authentic images. In our experiments, we adjusted hyperparameters, improving YOLOâ€™s mean average precision (mAP). We also tested various altitudes to determine the best drone height. From an initial mAP@.5 of $0.65$, we achieved 0.88, highlighting the value of synthetic images in agricultural scenarios. </p>
<blockquote>
<p>æ— äººæœºå·²ç»å½»åº•æ”¹å˜äº†åŒ…æ‹¬å†œä¸šåœ¨å†…çš„å„ä¸ªé¢†åŸŸã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰ä¸­çš„ç‰©ä½“æ£€æµ‹ç­‰æŠ€æœ¯ã€‚æœ¬ç ”ç©¶ä½¿ç”¨YOLOå®æ—¶ç‰©ä½“æ£€æµ‹å™¨æ¥è¯†åˆ«å’Œè®¡æ•°åŠ çº³å†œåœºæ— äººæœºå½±åƒä¸­çš„æ¤°å­æ ‘ã€‚ç”±äºç§æ¤é˜¶æ®µä¸åŒï¼Œè¯¥å†œåœºæ— æ³•è¿½è¸ªå…¶æ ‘æœ¨ã€‚è™½ç„¶æ‰‹åŠ¨è®¡æ•°éå¸¸ç¹çä¸”å®¹æ˜“å‡ºé”™ï¼Œä½†å‡†ç¡®ç¡®å®šæ ‘æœ¨æ•°é‡å¯¹äºé«˜æ•ˆè§„åˆ’å’Œå†œä¸šè¿‡ç¨‹ç®¡ç†è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¼˜åŒ–äº§é‡å’Œé¢„æµ‹ç”Ÿäº§ã€‚æˆ‘ä»¬åœ¨åŠè‡ªåŠ¨åŒ–æ¡†æ¶å†…è¯„ä¼°äº†YOLOæ£€æµ‹æ£•æ¦ˆæ ‘çš„æ€§èƒ½ï¼Œè¯„ä¼°äº†å‡†ç¡®æ€§å¢å¼ºæƒ…å†µï¼Œå¹¶è€ƒè™‘äº†å…¶å¯¹å†œæ°‘çš„æ½œåŠ›ã€‚æ•°æ®æ˜¯é€šè¿‡æ— äººæœºåœ¨2022å¹´9æœˆæ•è·çš„ã€‚ä¸ºäº†åˆ©ç”¨ç¨€ç¼ºæ•°æ®ä¼˜åŒ–YOLOï¼Œæˆ‘ä»¬åˆ›å»ºäº†åˆæˆå›¾åƒç”¨äºæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„COCOæ•°æ®é›†ï¼ˆä¸åŒ…æ‹¬æ¤°å­æ ‘ï¼‰ä¸Šçš„YOLOv7æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶æ•°æ®è¿›è¡Œäº†é€‚åº”ã€‚å°†å½±åƒä¸­çš„æ ‘æœ¨é‡æ–°å®šä½åˆ°åˆæˆå›¾åƒä¸Šï¼Œå¹¶åœ¨ä¸åŒçš„çœŸå®å›¾åƒä¸Šè¿›è¡Œæµ‹è¯•ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬è°ƒæ•´äº†è¶…å‚æ•°ï¼Œæé«˜äº†YOLOçš„å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰ã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†å„ç§é«˜åº¦ä»¥ç¡®å®šæœ€ä½³çš„æ— äººæœºé«˜åº¦ã€‚ä»æœ€åˆçš„mAP@.5çš„0.65ï¼Œæˆ‘ä»¬è¾¾åˆ°äº†0.88ï¼Œçªæ˜¾äº†åˆæˆå›¾åƒåœ¨å†œä¸šåœºæ™¯ä¸­çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11949v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ ä¸­çš„YOLOå®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•ï¼Œåœ¨åŠ çº³çš„å†œä¸šæ— äººæœºå½±åƒä¸­å¯¹æ¤°å­æ ‘è¿›è¡Œæ£€æµ‹å’Œè®¡æ•°ã€‚é€šè¿‡åˆæˆå›¾åƒä¼˜åŒ–YOLOæ¨¡å‹ï¼Œæé«˜æ¨¡å‹ç²¾åº¦ï¼Œè§£å†³äº†å› ç§æ¤é˜¶æ®µä¸åŒå¯¼è‡´çš„æ ‘æœ¨è·Ÿè¸ªä¸¢å¤±é—®é¢˜ã€‚å‡†ç¡®è®¡æ ‘å¯¹å†œä¸šç®¡ç†å’Œç”Ÿäº§ä¼˜åŒ–è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶èƒŒæ™¯ï¼šç ”ç©¶å±•ç¤ºäº†æ— äººæœºåœ¨å†œä¸šé¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¤°å­æ ‘çš„è®¡æ•°æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>æŠ€æœ¯åº”ç”¨ï¼šä½¿ç”¨YOLOå®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•å¯¹æ¤°å­æ ‘è¿›è¡Œè¯†åˆ«å’Œè®¡æ•°ã€‚</li>
<li>æ•°æ®æŒ‘æˆ˜ä¸å¯¹ç­–ï¼šé’ˆå¯¹æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç ”ç©¶é€šè¿‡åˆæˆå›¾åƒæ¥ä¼˜åŒ–YOLOæ¨¡å‹ï¼Œæé«˜æ¨¡å‹çš„ç²¾åº¦ã€‚</li>
<li>æ¨¡å‹ä¼˜åŒ–ç»“æœï¼šä»åˆå§‹çš„mAP@.5å€¼ä¸º0.65æé«˜åˆ°0.88ï¼Œè¯æ˜äº†åˆæˆå›¾åƒåœ¨å†œä¸šåœºæ™¯ä¸­çš„ä»·å€¼ã€‚</li>
<li>æ¨¡å‹é€‚ç”¨æ€§ï¼šç ”ç©¶äº†ä¸åŒæµ·æ‹”ä¸‹çš„æ— äººæœºæ‹æ‘„æ•ˆæœï¼Œä»¥ç¡®å®šæœ€ä½³çš„æ— äººæœºé«˜åº¦ã€‚</li>
<li>ç ”ç©¶æ„ä¹‰ï¼šå‡†ç¡®è®¡æ ‘å¯¹å†œä¸šç®¡ç†å’Œç”Ÿäº§ä¼˜åŒ–è‡³å…³é‡è¦ï¼Œè¯¥ç ”ç©¶çš„æˆæœå¯ä¸ºå†œæ°‘æä¾›æœ‰åŠ›çš„å·¥å…·æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2b7bf6328d6dda735054ee3026262af1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35da49d74690e4d9dd7c8f3826892783.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c139045edf1de32005f74ed1ffa542e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b969c383a4bdcaaa33fd61907ed997f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-840a90ba8553ebea9ae71abaa2c0355c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b08facae39ce66e508e869960685f2b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1383f465188312e2bac222c5499328e4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PhysAug-A-Physical-guided-and-Frequency-based-Data-Augmentation-for-Single-Domain-Generalized-Object-Detection"><a href="#PhysAug-A-Physical-guided-and-Frequency-based-Data-Augmentation-for-Single-Domain-Generalized-Object-Detection" class="headerlink" title="PhysAug: A Physical-guided and Frequency-based Data Augmentation for   Single-Domain Generalized Object Detection"></a>PhysAug: A Physical-guided and Frequency-based Data Augmentation for   Single-Domain Generalized Object Detection</h2><p><strong>Authors:Xiaoran Xu, Jiangang Yang, Wenhui Shi, Siyuan Ding, Luqing Luo, Jian Liu</strong></p>
<p>Single-Domain Generalized Object Detection~(S-DGOD) aims to train on a single source domain for robust performance across a variety of unseen target domains by taking advantage of an object detector. Existing S-DGOD approaches often rely on data augmentation strategies, including a composition of visual transformations, to enhance the detectorâ€™s generalization ability. However, the absence of real-world prior knowledge hinders data augmentation from contributing to the diversity of training data distributions. To address this issue, we propose PhysAug, a novel physical model-based non-ideal imaging condition data augmentation method, to enhance the adaptability of the S-DGOD tasks. Drawing upon the principles of atmospheric optics, we develop a universal perturbation model that serves as the foundation for our proposed PhysAug. Given that visual perturbations typically arise from the interaction of light with atmospheric particles, the image frequency spectrum is harnessed to simulate real-world variations during training. This approach fosters the detector to learn domain-invariant representations, thereby enhancing its ability to generalize across various settings. Without altering the network architecture or loss function, our approach significantly outperforms the state-of-the-art across various S-DGOD datasets. In particular, it achieves a substantial improvement of $7.3%$ and $7.2%$ over the baseline on DWD and Cityscape-C, highlighting its enhanced generalizability in real-world settings. </p>
<blockquote>
<p>å•åŸŸå¹¿ä¹‰å¯¹è±¡æ£€æµ‹ï¼ˆS-DGODï¼‰æ—¨åœ¨é€šè¿‡å¯¹è±¡æ£€æµ‹å™¨åœ¨å•ä¸€æºåŸŸä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ä¾¿åœ¨å„ç§æœªè§è¿‡çš„ç›®æ ‡åŸŸä¸Šå®ç°ç¨³å¥æ€§èƒ½ã€‚ç°æœ‰çš„S-DGODæ–¹æ³•é€šå¸¸ä¾èµ–äºæ•°æ®å¢å¼ºç­–ç•¥ï¼ŒåŒ…æ‹¬è§†è§‰è½¬æ¢çš„ç»„åˆï¼Œä»¥å¢å¼ºæ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç¼ºä¹çœŸå®ä¸–ç•Œçš„å…ˆéªŒçŸ¥è¯†é˜»ç¢äº†æ•°æ®å¢å¼ºå¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒå¤šæ ·æ€§çš„è´¡çŒ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PhysAugï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç‰©ç†æ¨¡å‹çš„æ–°å‹éç†æƒ³æˆåƒæ¡ä»¶æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä»¥æé«˜S-DGODä»»åŠ¡çš„é€‚åº”æ€§ã€‚åŸºäºå¤§æ°”å…‰å­¦åŸç†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé€šç”¨æ‰°åŠ¨æ¨¡å‹ï¼Œä½œä¸ºæˆ‘ä»¬æå‡ºçš„PhysAugçš„åŸºç¡€ã€‚é‰´äºè§†è§‰æ‰°åŠ¨é€šå¸¸æ˜¯ç”±å…‰ä¸å¤§æ°”ç²’å­çš„ç›¸äº’ä½œç”¨è€Œäº§ç”Ÿçš„ï¼Œå› æ­¤æˆ‘ä»¬åˆ©ç”¨å›¾åƒé¢‘è°±æ¥æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„çœŸå®ä¸–ç•Œå˜åŒ–ã€‚è¿™ç§æ–¹æ³•ä¿ƒè¿›äº†æ£€æµ‹å™¨å­¦ä¹ åŸŸä¸å˜è¡¨ç¤ºï¼Œä»è€Œæé«˜äº†å…¶åœ¨å„ç§è®¾ç½®ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸æ”¹å˜ç½‘ç»œæ¶æ„æˆ–æŸå¤±å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªS-DGODæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯ã€‚ç‰¹åˆ«æ˜¯åœ¨DWDå’ŒCityscape-Cä¸Šï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œå®ƒå®ç°äº†7.3%å’Œ7.2%çš„å¤§å¹…æ”¹è¿›ï¼Œå‡¸æ˜¾äº†å…¶åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11807v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å•åŸŸå¹¿ä¹‰ç›®æ ‡æ£€æµ‹ï¼ˆS-DGODï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†åŸºäºç‰©ç†æ¨¡å‹çš„éç†æƒ³æˆåƒæ¡ä»¶æ•°æ®å¢å¼ºæ–¹æ³•PhysAugï¼Œæ—¨åœ¨æé«˜S-DGODä»»åŠ¡çš„é€‚åº”æ€§ã€‚é€šè¿‡æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„è§†è§‰æ‰°åŠ¨ï¼Œè¯¥æ–¹æ³•ä¿ƒè¿›æ£€æµ‹å™¨å­¦ä¹ åŸŸä¸å˜è¡¨ç¤ºï¼Œä»è€Œæé«˜å…¶åœ¨ä¸åŒè®¾ç½®ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸æ”¹å˜ç½‘ç»œæ¶æ„æˆ–æŸå¤±å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªS-DGODæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>S-DGODæ—¨åœ¨é€šè¿‡å•ä¸€æºåŸŸè®­ç»ƒï¼Œå®ç°åœ¨å¤šç§æœªè§ç›®æ ‡åŸŸä¸Šçš„ç¨³å¥æ€§èƒ½ã€‚</li>
<li>ç°æœ‰S-DGODæ–¹æ³•å¸¸ä¾èµ–æ•°æ®å¢å¼ºç­–ç•¥æ¥æé«˜æ£€æµ‹å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>çœŸå®ä¸–ç•Œå…ˆéªŒçŸ¥è¯†çš„ç¼ºå¤±é˜»ç¢äº†æ•°æ®å¢å¼ºå¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒå¤šæ ·æ€§çš„è´¡çŒ®ã€‚</li>
<li>æå‡ºäº†åŸºäºç‰©ç†æ¨¡å‹çš„éç†æƒ³æˆåƒæ¡ä»¶æ•°æ®å¢å¼ºæ–¹æ³•PhysAugï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„è§†è§‰æ‰°åŠ¨ã€‚</li>
<li>PhysAugåˆ©ç”¨å¤§æ°”å…‰å­¦åŸç†ï¼Œå»ºç«‹é€šç”¨æ‰°åŠ¨æ¨¡å‹ï¼Œä¿ƒè¿›æ£€æµ‹å™¨å­¦ä¹ åŸŸä¸å˜è¡¨ç¤ºã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸æ”¹å˜ç½‘ç»œæ¶æ„æˆ–æŸå¤±å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†S-DGODä»»åŠ¡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2653748f8d775e0af9c6b1c11164605a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87102dfa25f6cf98635c67edeffe7a2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c1cc8293e563fb7865d6155effdfa47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd88107106008d3c7e02c2bb3d35ca56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f31a99c2f102ead83f8b40f0692f7dd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="HGSFusion-Radar-Camera-Fusion-with-Hybrid-Generation-and-Synchronization-for-3D-Object-Detection"><a href="#HGSFusion-Radar-Camera-Fusion-with-Hybrid-Generation-and-Synchronization-for-3D-Object-Detection" class="headerlink" title="HGSFusion: Radar-Camera Fusion with Hybrid Generation and   Synchronization for 3D Object Detection"></a>HGSFusion: Radar-Camera Fusion with Hybrid Generation and   Synchronization for 3D Object Detection</h2><p><strong>Authors:Zijian Gu, Jianwei Ma, Yan Huang, Honghao Wei, Zhanye Chen, Hui Zhang, Wei Hong</strong></p>
<p>Millimeter-wave radar plays a vital role in 3D object detection for autonomous driving due to its all-weather and all-lighting-condition capabilities for perception. However, radar point clouds suffer from pronounced sparsity and unavoidable angle estimation errors. To address these limitations, incorporating a camera may partially help mitigate the shortcomings. Nevertheless, the direct fusion of radar and camera data can lead to negative or even opposite effects due to the lack of depth information in images and low-quality image features under adverse lighting conditions. Hence, in this paper, we present the radar-camera fusion network with Hybrid Generation and Synchronization (HGSFusion), designed to better fuse radar potentials and image features for 3D object detection. Specifically, we propose the Radar Hybrid Generation Module (RHGM), which fully considers the Direction-Of-Arrival (DOA) estimation errors in radar signal processing. This module generates denser radar points through different Probability Density Functions (PDFs) with the assistance of semantic information. Meanwhile, we introduce the Dual Sync Module (DSM), comprising spatial sync and modality sync, to enhance image features with radar positional information and facilitate the fusion of distinct characteristics in different modalities. Extensive experiments demonstrate the effectiveness of our approach, outperforming the state-of-the-art methods in the VoD and TJ4DRadSet datasets by $6.53%$ and $2.03%$ in RoI AP and BEV AP, respectively. The code is available at <a target="_blank" rel="noopener" href="https://github.com/garfield-cpp/HGSFusion">https://github.com/garfield-cpp/HGSFusion</a>. </p>
<blockquote>
<p>æ¯«ç±³æ³¢é›·è¾¾å› å…¶å…¨å¤©å€™å’Œå…¨å…‰ç…§æ¡ä»¶ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶çš„3Dç›®æ ‡æ£€æµ‹ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œé›·è¾¾ç‚¹äº‘å­˜åœ¨æ˜æ˜¾çš„ç¨€ç–æ€§å’Œä¸å¯é¿å…çš„è§’åº¦ä¼°è®¡è¯¯å·®ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œèå…¥ç›¸æœºå¯èƒ½æœ‰åŠ©äºéƒ¨åˆ†å¼¥è¡¥è¿™äº›ä¸è¶³ã€‚ç„¶è€Œï¼Œé›·è¾¾å’Œç›¸æœºæ•°æ®çš„ç›´æ¥èåˆå¯èƒ½å¯¼è‡´è´Ÿé¢ç”šè‡³ç›¸åçš„æ•ˆæœï¼Œå› ä¸ºå›¾åƒç¼ºä¹æ·±åº¦ä¿¡æ¯ï¼Œä¸”åœ¨ä¸è‰¯å…‰ç…§æ¡ä»¶ä¸‹å›¾åƒç‰¹å¾è´¨é‡è¾ƒä½ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†èåˆé›·è¾¾å’Œç›¸æœºçš„ç½‘ç»œHybrid Generation and Synchronizationï¼ˆHGSFusionï¼‰ï¼Œæ—¨åœ¨æ›´å¥½åœ°èåˆé›·è¾¾æ½œåŠ›å’Œå›¾åƒç‰¹å¾è¿›è¡Œ3Dç›®æ ‡æ£€æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†Radar Hybrid Generation Moduleï¼ˆRHGMï¼‰ï¼Œå®ƒå……åˆ†è€ƒè™‘äº†é›·è¾¾ä¿¡å·å¤„ç†ä¸­çš„åˆ°è¾¾æ–¹å‘ï¼ˆDOAï¼‰ä¼°è®¡è¯¯å·®ã€‚è¯¥æ¨¡å—é€šè¿‡ä¸åŒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ç”Ÿæˆæ›´å¯†é›†çš„é›·è¾¾ç‚¹ï¼Œå¹¶å€ŸåŠ©è¯­ä¹‰ä¿¡æ¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å«ç©ºé—´åŒæ­¥å’Œæ¨¡æ€åŒæ­¥çš„Dual Sync Moduleï¼ˆDSMï¼‰ï¼Œä»¥å¢å¼ºå›¾åƒç‰¹å¾ä¸­çš„é›·è¾¾ä½ç½®ä¿¡æ¯ï¼Œå¹¶ä¿ƒè¿›ä¸åŒæ¨¡æ€ä¸­ä¸åŒç‰¹å¾çš„èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éå¸¸æœ‰æ•ˆï¼Œåœ¨VoDå’ŒTJ4DRadSetæ•°æ®é›†ä¸Šçš„RoI APå’ŒBEV APåˆ†åˆ«æé«˜äº†6.53%å’Œ2.03%ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/garfield-cpp/HGSFusion%E3%80%82">https://github.com/garfield-cpp/HGSFusionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11489v1">PDF</a> 12 pages, 8 figures, 7 tables. Accepted by AAAI 2025 , the 39th   Annual AAAI Conference on Artificial Intelligence</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ¯«ç±³æ³¢é›·è¾¾åœ¨è‡ªä¸»é©¾é©¶çš„3Dç›®æ ‡æ£€æµ‹ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå¾—ç›Šäºå…¶åœ¨å„ç§å¤©æ°”å’Œå…‰ç…§æ¡ä»¶ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚ä½†é›·è¾¾ç‚¹äº‘å­˜åœ¨æ˜¾è‘—çš„ç¨€ç–æ€§å’Œè§’åº¦ä¼°è®¡è¯¯å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥æ‘„åƒæœºæœ‰åŠ©äºéƒ¨åˆ†å¼¥è¡¥ä¸è¶³ã€‚é’ˆå¯¹é›·è¾¾ä¸æ‘„åƒæœºæ•°æ®ç›´æ¥èåˆå¯èƒ½å¸¦æ¥çš„è´Ÿé¢æ•ˆæœï¼Œæœ¬æ–‡æå‡ºäº†èåˆé›·è¾¾ä¸æ‘„åƒæœºæ•°æ®çš„é›·è¾¾-æ‘„åƒæœºèåˆç½‘ç»œï¼ˆHGSFusionï¼‰ã€‚è¯¥ç½‘ç»œåŒ…æ‹¬é›·è¾¾æ··åˆç”Ÿæˆæ¨¡å—ï¼ˆRHGMï¼‰å’ŒåŒé‡åŒæ­¥æ¨¡å—ï¼ˆDSMï¼‰ï¼Œæ—¨åœ¨æ›´å¥½åœ°èåˆé›·è¾¾æ½œåŠ›å’Œå›¾åƒç‰¹å¾è¿›è¡Œ3Dç›®æ ‡æ£€æµ‹ã€‚RHGMæ¨¡å—è€ƒè™‘äº†é›·è¾¾ä¿¡å·å¤„ç†ä¸­çš„æ–¹å‘åˆ°è¾¾ï¼ˆDOAï¼‰ä¼°è®¡è¯¯å·®ï¼Œé€šè¿‡ä¸åŒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ç”Ÿæˆæ›´å¯†é›†çš„é›·è¾¾ç‚¹ã€‚DSMæ¨¡å—åˆ™é€šè¿‡ç©ºé—´åŒæ­¥å’Œæ¨¡æ€åŒæ­¥å¢å¼ºå›¾åƒç‰¹å¾å¹¶ä¿ƒè¿›ä¸åŒæ¨¡æ€çš„èåˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨VoDå’ŒTJ4DRadSetæ•°æ®é›†ä¸Šçš„æ£€æµ‹ç²¾åº¦åˆ†åˆ«æé«˜äº†$6.53%$å’Œ$2.03%$ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡GitHubè·å–ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¯«ç±³æ³¢é›·è¾¾åœ¨è‡ªä¸»é©¾é©¶çš„3Dç›®æ ‡æ£€æµ‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œå› å…¶å…·å¤‡å…¨å¤©å€™å’Œå…¨å…‰ç…§æ¡ä»¶ä¸‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>é›·è¾¾ç‚¹äº‘å­˜åœ¨ç¨€ç–æ€§å’Œè§’åº¦ä¼°è®¡è¯¯å·®çš„é—®é¢˜ï¼Œéœ€è¦å¯»æ‰¾è§£å†³æ–¹æ¡ˆæ¥æ”¹è¿›ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ‘„åƒæœºæ¥èåˆæ•°æ®æœ‰åŠ©äºéƒ¨åˆ†å¼¥è¡¥é›·è¾¾çš„ä¸è¶³ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ³¨æ„äºŒè€…ç›´æ¥èåˆå¯èƒ½å¸¦æ¥çš„é—®é¢˜ï¼Œå¦‚ç¼ºä¹æ·±åº¦ä¿¡æ¯å’Œä¸è‰¯å…‰ç…§æ¡ä»¶ä¸‹å›¾åƒè´¨é‡å·®ç­‰ã€‚</li>
<li>æå‡ºäº†é›·è¾¾-æ‘„åƒæœºèåˆç½‘ç»œHGSFusionï¼ŒåŒ…æ‹¬é›·è¾¾æ··åˆç”Ÿæˆæ¨¡å—RHGMå’ŒåŒé‡åŒæ­¥æ¨¡å—DSMï¼Œä»¥æ›´å¥½åœ°èåˆé›·è¾¾å’Œå›¾åƒç‰¹å¾è¿›è¡Œ3Dç›®æ ‡æ£€æµ‹ã€‚</li>
<li>RHGMè€ƒè™‘äº†é›·è¾¾ä¿¡å·å¤„ç†ä¸­çš„DOAä¼°è®¡è¯¯å·®ï¼Œå¹¶å¯é€šè¿‡ä¸åŒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ç”Ÿæˆæ›´å¯†é›†çš„é›·è¾¾ç‚¹ã€‚</li>
<li>DSMæ¨¡å—æ—¨åœ¨é€šè¿‡ç©ºé—´åŒæ­¥å’Œæ¨¡æ€åŒæ­¥å¢å¼ºå›¾åƒç‰¹å¾å¹¶ä¿ƒè¿›ä¸åŒæ¨¡æ€æ•°æ®çš„èåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11489">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-446b9edbfc5517c3b1f635efab4ac92c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9842df75b045c8c225fd4c8ebe3d5141.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85d7acc75986dacac9f3dbd3d88207d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-313d28b3c765f8bba11b52754dad6665.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9b7f1c38bccf0768b0c22f5c66aae6e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Redefining-Normal-A-Novel-Object-Level-Approach-for-Multi-Object-Novelty-Detection"><a href="#Redefining-Normal-A-Novel-Object-Level-Approach-for-Multi-Object-Novelty-Detection" class="headerlink" title="Redefining Normal: A Novel Object-Level Approach for Multi-Object   Novelty Detection"></a>Redefining Normal: A Novel Object-Level Approach for Multi-Object   Novelty Detection</h2><p><strong>Authors:Mohammadreza Salehi, Nikolaos Apostolikas, Efstratios Gavves, Cees G. M. Snoek, Yuki M. Asano</strong></p>
<p>In the realm of novelty detection, accurately identifying outliers in data without specific class information poses a significant challenge. While current methods excel in single-object scenarios, they struggle with multi-object situations due to their focus on individual objects. Our paper suggests a novel approach: redefining <code>normal&#39; at the object level in training datasets. Rather than the usual image-level view, we consider the most dominant object in a dataset as the norm, offering a perspective that is more effective for real-world scenarios. Adapting to our object-level definition of </code>normalâ€™, we modify knowledge distillation frameworks, where a student network learns from a pre-trained teacher network. Our first contribution, DeFeND(Dense Feature Fine-tuning on Normal Data), integrates dense feature fine-tuning into the distillation process, allowing the teacher network to focus on object-level features with a self-supervised loss. The second is masked knowledge distillation, where the student network works with partially hidden inputs, honing its ability to deduce and generalize from incomplete data. This approach not only fares well in single-object novelty detection but also considerably surpasses existing methods in multi-object contexts. The implementation is available at: <a target="_blank" rel="noopener" href="https://github.com/SMSD75/Redefining_Normal_ACCV24/tree/main">https://github.com/SMSD75/Redefining_Normal_ACCV24/tree/main</a> </p>
<blockquote>
<p>åœ¨æ–°å‹æ£€æµ‹é¢†åŸŸï¼Œæ²¡æœ‰ç‰¹å®šç±»åˆ«ä¿¡æ¯çš„æƒ…å†µä¸‹å‡†ç¡®è¯†åˆ«æ•°æ®ä¸­çš„å¼‚å¸¸å€¼æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•åœ¨å•ä¸€å¯¹è±¡åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤šå¯¹è±¡åœºæ™¯ä¸­å´è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬ä¾§é‡äºå•ä¸ªå¯¹è±¡ã€‚æˆ‘ä»¬çš„è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼šåœ¨è®­ç»ƒæ•°æ®é›†ä¸­é‡æ–°å®šä¹‰å¯¹è±¡çº§åˆ«çš„â€œæ­£å¸¸â€ã€‚æˆ‘ä»¬ä¸å†é‡‡ç”¨é€šå¸¸çš„å›¾åƒçº§åˆ«è§†è§’ï¼Œè€Œæ˜¯è€ƒè™‘æ•°æ®é›†ä¸­æœ€ä¸»å¯¼çš„å¯¹è±¡ä¸ºæ­£å¸¸æ ‡å‡†ï¼Œè¿™ä¸ºç°å®ä¸–ç•Œåœºæ™¯æä¾›äº†æ›´æœ‰æ•ˆçš„è§†è§’ã€‚æ ¹æ®æˆ‘ä»¬å¯¹â€œæ­£å¸¸â€çš„å¯¹è±¡çº§åˆ«å®šä¹‰ï¼Œæˆ‘ä»¬ä¿®æ”¹äº†çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œä»ä¸­å­¦ä¹ é¢„è®­ç»ƒçš„æ•™å¸ˆç½‘ç»œã€‚æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªè´¡çŒ®æ˜¯DeFeNDï¼ˆåœ¨æ­£å¸¸æ•°æ®ä¸Šè¿›è¡Œå¯†é›†ç‰¹å¾å¾®è°ƒï¼‰ï¼Œå®ƒå°†å¯†é›†ç‰¹å¾å¾®è°ƒé›†æˆåˆ°è’¸é¦è¿‡ç¨‹ä¸­ï¼Œä½¿æ•™å¸ˆç½‘ç»œèƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘ç›‘ç£æŸå¤±å…³æ³¨å¯¹è±¡çº§åˆ«ç‰¹å¾ã€‚ç¬¬äºŒä¸ªæ˜¯æ©è†œçŸ¥è¯†è’¸é¦ï¼Œå­¦ç”Ÿç½‘ç»œå¯¹éƒ¨åˆ†éšè—è¾“å…¥è¿›è¡Œæ“ä½œï¼Œæé«˜äº†ä»ä¸å®Œæ•´æ•°æ®ä¸­æ¨æ–­å’Œæ¦‚æ‹¬çš„èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä¸ä»…åœ¨å•ä¸€å¯¹è±¡çš„æ–°é¢–æ€§æ£€æµ‹ä¸­è¡¨ç°è‰¯å¥½ï¼Œè€Œä¸”åœ¨å¤šå¯¹è±¡ç¯å¢ƒä¸­ä¹Ÿå¤§å¤§è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚å…·ä½“å®ç°å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/SMSD75/Redefining_Normal_ACCV24/tree/main">https://github.com/SMSD75/Redefining_Normal_ACCV24/tree/main</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11148v1">PDF</a> Accepted at ACCV24(Oral)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³æ— ç‰¹å®šç±»åˆ«ä¿¡æ¯ä¸‹çš„å¼‚å¸¸æ£€æµ‹é—®é¢˜ã€‚åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šé‡æ–°å®šä¹‰å¯¹è±¡çº§åˆ«çš„â€œæ­£å¸¸â€ï¼Œä»¥é€‚åº”çœŸå®åœºæ™¯ä¸­çš„å¤šå¯¹è±¡æƒ…å†µã€‚é€šè¿‡ä¿®æ”¹çŸ¥è¯†è’¸é¦æ¡†æ¶å¹¶å¼•å…¥å¯†é›†ç‰¹å¾å¾®è°ƒï¼Œå®ç°å¯¹è±¡çº§åˆ«çš„ç‰¹å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ©è†œçŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸ä»…é€‚ç”¨äºå•å¯¹è±¡å¼‚å¸¸æ£€æµ‹ï¼Œè€Œä¸”åœ¨å¤šå¯¹è±¡åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰æ–¹æ³•åœ¨å¼‚å¸¸æ£€æµ‹ä¸­é¢ä¸´å¤„ç†å¤šå¯¹è±¡æƒ…å†µçš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡é‡æ–°å®šä¹‰å¯¹è±¡çº§åˆ«çš„â€œæ­£å¸¸â€æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å¼•å…¥å¯†é›†ç‰¹å¾å¾®è°ƒçš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œå¸®åŠ©ç½‘ç»œè¿›è¡Œå¯¹è±¡çº§åˆ«çš„ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>æå‡ºæ©è†œçŸ¥è¯†è’¸é¦ï¼Œæé«˜ç½‘ç»œä»ä¸å®Œæ•´æ•°æ®ä¸­æ¨æ–­å’Œæ³›åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…åœ¨å•å¯¹è±¡å¼‚å¸¸æ£€æµ‹ä¸­è¡¨ç°è‰¯å¥½ï¼Œè€Œä¸”åœ¨å¤šå¯¹è±¡åœºæ™¯ä¸­ä¹Ÿæœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>è®ºæ–‡ä¸­çš„å®ç°ç»†èŠ‚å’Œä»£ç å·²å…¬å¼€ï¼Œä¾¿äºä»–äººå‚è€ƒå’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd1bac981d0fbce2a50478dadc103487.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78637471a66c9b1a2702b5d629122c83.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation"><a href="#MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation"></a>MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation</h2><p><strong>Authors:Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels typically uses Class Activation Maps (CAM) to achieve dense predictions. Recently, Vision Transformer (ViT) has provided an alternative to generate localization maps from class-patch attention. However, due to insufficient constraints on modeling such attention, we observe that the Localization Attention Maps (LAM) often struggle with the artifact issue, i.e., patch regions with minimal semantic relevance are falsely activated by class tokens. In this work, we propose MoRe to address this issue and further explore the potential of LAM. Our findings suggest that imposing additional regularization on class-patch attention is necessary. To this end, we first view the attention as a novel directed graph and propose the Graph Category Representation module to implicitly regularize the interaction among class-patch entities. It ensures that class tokens dynamically condense the related patch information and suppress unrelated artifacts at a graph level. Second, motivated by the observation that CAM from classification weights maintains smooth localization of objects, we devise the Localization-informed Regularization module to explicitly regularize the class-patch attention. It directly mines the token relations from CAM and further supervises the consistency between class and patch tokens in a learnable manner. Extensive experiments are conducted on PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact issue and achieves state-of-the-art performance, surpassing recent single-stage and even multi-stage methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe">https://github.com/zwyang6/MoRe</a>. </p>
<blockquote>
<p>ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰é€šå¸¸ä½¿ç”¨ç±»æ¿€æ´»å›¾ï¼ˆCAMï¼‰æ¥å®ç°å¯†é›†é¢„æµ‹ã€‚æœ€è¿‘ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æä¾›äº†ä¸€ç§ä»ç±»è¡¥ä¸æ³¨æ„åŠ›ç”Ÿæˆå®šä½å›¾çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºå¯¹è¿™ç±»æ³¨æ„åŠ›çš„å»ºæ¨¡çº¦æŸä¸è¶³ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å®šä½æ³¨æ„åŠ›å›¾ï¼ˆLAMï¼‰ç»å¸¸é¢ä¸´ä¼ªå½±é—®é¢˜ï¼Œå³è¯­ä¹‰ç›¸å…³æ€§æå°çš„è¡¥ä¸åŒºåŸŸä¼šè¢«ç±»ä»¤ç‰Œé”™è¯¯æ¿€æ´»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºMoReæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶LAMçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹ç±»è¡¥ä¸æ³¨æ„åŠ›æ–½åŠ é¢å¤–çš„æ­£åˆ™åŒ–æ˜¯å¿…è¦çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ³¨æ„åŠ›è§†ä¸ºä¸€ç§æ–°å‹çš„æœ‰å‘å›¾ï¼Œå¹¶æå‡ºå›¾ç±»åˆ«è¡¨ç¤ºæ¨¡å—ï¼Œä»¥éšå¼åœ°è§„èŒƒç±»è¡¥ä¸å®ä½“ä¹‹é—´çš„äº¤äº’ã€‚å®ƒç¡®ä¿ç±»ä»¤ç‰Œèƒ½å¤ŸåŠ¨æ€åœ°å‡èšç›¸å…³çš„è¡¥ä¸ä¿¡æ¯ï¼Œå¹¶åœ¨å›¾çº§åˆ«æŠ‘åˆ¶æ— å…³ä¼ªå½±ã€‚å…¶æ¬¡ï¼Œå—åˆ†ç±»æƒé‡CAMèƒ½ä¿æŒå¯¹è±¡å®šä½å¹³æ»‘çš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†å®šä½æ„ŸçŸ¥æ­£åˆ™åŒ–æ¨¡å—ï¼Œä»¥æ˜¾å¼åœ°è§„èŒƒç±»è¡¥ä¸æ³¨æ„åŠ›ã€‚å®ƒç›´æ¥ä»CAMæŒ–æ˜ä»¤ç‰Œå…³ç³»ï¼Œå¹¶ä»¥å¯å­¦ä¹ çš„æ–¹å¼ç›‘ç£ç±»å’Œè¡¥ä¸ä»¤ç‰Œä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨PASCAL VOCå’ŒMS COCOä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†MoReæœ‰æ•ˆåœ°è§£å†³äº†ä¼ªå½±é—®é¢˜ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€è¿‘çš„å•é˜¶æ®µç”šè‡³å¤šé˜¶æ®µæ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zwyang6/MoReæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11076v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå›¾åƒçº§åˆ«æ ‡ç­¾çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä½¿ç”¨Vision Transformerï¼ˆViTï¼‰ç”Ÿæˆçš„Localization Attention Mapsï¼ˆLAMï¼‰å­˜åœ¨è¯¯æ¿€æ´»éè¯­ä¹‰ç›¸å…³åŒºåŸŸçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MoReæ–¹æ³•ï¼Œé€šè¿‡é¢å¤–çš„æ­£åˆ™åŒ–æ¥ä¼˜åŒ–LAMçš„æ€§èƒ½ã€‚MoReåŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šGraph Category Representationæ¨¡å—å’ŒLocalization-informed Regularizationæ¨¡å—ï¼Œåˆ†åˆ«é€šè¿‡éšå¼å’Œæ˜¾å¼çš„æ–¹å¼å¯¹ç±»è¡¥ä¸æ³¨æ„åŠ›è¿›è¡Œæ­£åˆ™åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒMoReåœ¨PASCAL VOCå’ŒMS COCOæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰é€šå¸¸ä½¿ç”¨Class Activation Mapsï¼ˆCAMï¼‰å®ç°å¯†é›†é¢„æµ‹ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰å¯ç”ŸæˆLocalization Attention Mapsï¼ˆLAMï¼‰ã€‚</li>
<li>LAMå­˜åœ¨è¯¯æ¿€æ´»éè¯­ä¹‰ç›¸å…³åŒºåŸŸçš„é—®é¢˜ã€‚</li>
<li>MoReæ–¹æ³•é€šè¿‡é¢å¤–çš„æ­£åˆ™åŒ–æ¥è§£å†³LAMçš„é—®é¢˜ã€‚</li>
<li>MoReåŒ…æ‹¬Graph Category Representationæ¨¡å—å’ŒLocalization-informed Regularizationæ¨¡å—ã€‚</li>
<li>MoReåœ¨PASCAL VOCå’ŒMS COCOæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6510c136895fa0abdc11ee0a807819f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-948b191c21d0a7790db8f2c48fcfc40e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3e4750dfaf5c1d3fd23143f2d50fec0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-699cfbb8aa515db4cbb28229318109c5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Heterogeneous-Graph-Transformer-for-Multiple-Tiny-Object-Tracking-in-RGB-T-Videos"><a href="#Heterogeneous-Graph-Transformer-for-Multiple-Tiny-Object-Tracking-in-RGB-T-Videos" class="headerlink" title="Heterogeneous Graph Transformer for Multiple Tiny Object Tracking in   RGB-T Videos"></a>Heterogeneous Graph Transformer for Multiple Tiny Object Tracking in   RGB-T Videos</h2><p><strong>Authors:Qingyu Xu, Longguang Wang, Weidong Sheng, Yingqian Wang, Chao Xiao, Chao Ma, Wei An</strong></p>
<p>Tracking multiple tiny objects is highly challenging due to their weak appearance and limited features. Existing multi-object tracking algorithms generally focus on single-modality scenes, and overlook the complementary characteristics of tiny objects captured by multiple remote sensors. To enhance tracking performance by integrating complementary information from multiple sources, we propose a novel framework called {HGT-Track (Heterogeneous Graph Transformer based Multi-Tiny-Object Tracking)}. Specifically, we first employ a Transformer-based encoder to embed images from different modalities. Subsequently, we utilize Heterogeneous Graph Transformer to aggregate spatial and temporal information from multiple modalities to generate detection and tracking features. Additionally, we introduce a target re-detection module (ReDet) to ensure tracklet continuity by maintaining consistency across different modalities. Furthermore, this paper introduces the first benchmark VT-Tiny-MOT (Visible-Thermal Tiny Multi-Object Tracking) for RGB-T fused multiple tiny object tracking. Extensive experiments are conducted on VT-Tiny-MOT, and the results have demonstrated the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of MOTA (Multiple-Object Tracking Accuracy) and ID-F1 score. The code and dataset will be made available at <a target="_blank" rel="noopener" href="https://github.com/xuqingyu26/HGTMT">https://github.com/xuqingyu26/HGTMT</a>. </p>
<blockquote>
<p>é’ˆå¯¹å¤šä¸ªå¾®å°ç›®æ ‡çš„è·Ÿè¸ªç”±äºå…¶å¤–è§‚å¾®å¼±ã€ç‰¹å¾æœ‰é™è€Œé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤šç›®æ ‡è·Ÿè¸ªç®—æ³•é€šå¸¸ä¸“æ³¨äºå•æ¨¡æ€åœºæ™¯ï¼Œå¿½ç•¥äº†å¤šä¸ªè¿œç¨‹ä¼ æ„Ÿå™¨æ•è·çš„å¾®å°å¯¹è±¡çš„äº’è¡¥ç‰¹æ€§ã€‚ä¸ºäº†é€šè¿‡æ•´åˆæ¥è‡ªå¤šä¸ªæºå¤´çš„äº’è¡¥ä¿¡æ¯æ¥æé«˜è·Ÿè¸ªæ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç§°ä¸ºHGT-Trackï¼ˆåŸºäºå¼‚æ„å›¾å˜æ¢å™¨çš„å¤šå¾®å°ç›®æ ‡è·Ÿè¸ªï¼‰ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨åŸºäºTransformerçš„ç¼–ç å™¨å¯¹ä¸åŒæ¨¡æ€çš„å›¾åƒè¿›è¡ŒåµŒå…¥ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨å¼‚æ„å›¾å˜æ¢å™¨èšåˆå¤šä¸ªæ¨¡æ€çš„ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆæ£€æµ‹å’Œè·Ÿè¸ªç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç›®æ ‡å†æ£€æµ‹æ¨¡å—ï¼ˆReDetï¼‰ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´ä¿æŒä¸€è‡´æ€§ï¼Œä»è€Œå®ç°è½¨è¿¹è¿ç»­æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†é¦–ä¸ªRGB-Tèåˆå¤šå¾®å°ç›®æ ‡è·Ÿè¸ªçš„åŸºå‡†æµ‹è¯•VT-Tiny-MOTï¼ˆå¯è§çƒ­å¾®å°å¤šç›®æ ‡è·Ÿè¸ªï¼‰ã€‚åœ¨VT-Tiny-MOTä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç›®æ ‡è·Ÿè¸ªå‡†ç¡®åº¦ï¼ˆMOTAï¼‰å’ŒID-F1åˆ†æ•°æ–¹é¢å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/xuqingyu26/HGTMT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/xuqingyu26/HGTMTä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10861v1">PDF</a> N&#x2F;A</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å¤šæ¨¡æ€åœºæ™¯ä¸­çš„å¤šä¸ªå¾®å°ç›®æ ‡è·Ÿè¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºHGT-Trackçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¼‚è´¨å›¾å˜æ¢æŠ€æœ¯é›†æˆå¤šæºä¿¡æ¯ä»¥æå‡è·Ÿè¸ªæ€§èƒ½ã€‚æ¡†æ¶åŒ…æ‹¬å¤šæ¨¡æ€å›¾åƒåµŒå…¥ã€å¼‚è´¨å›¾å˜æ¢å™¨è¿›è¡Œæ—¶ç©ºä¿¡æ¯èšåˆã€ç›®æ ‡é‡æ–°æ£€æµ‹æ¨¡å—ç¡®ä¿è½¨è¿¹è¿ç»­æ€§ï¼Œå¹¶åœ¨RGB-Tèåˆçš„å¤šå¾®å°ç›®æ ‡è·Ÿè¸ªä¸Šå¼•å…¥é¦–ä¸ªåŸºå‡†æµ‹è¯•VT-Tiny-MOTã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MOTAå’ŒID-F1åˆ†æ•°ä¸Šè¾ƒå…¶ä»–å…ˆè¿›æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡é’ˆå¯¹å¤šæ¨¡æ€åœºæ™¯ä¸­çš„å¤šä¸ªå¾®å°ç›®æ ‡è·Ÿè¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶HGT-Trackã€‚</li>
<li>HGT-Trackåˆ©ç”¨Transformerç¼–ç å™¨åµŒå…¥ä¸åŒæ¨¡æ€çš„å›¾åƒã€‚</li>
<li>å¼•å…¥å¼‚è´¨å›¾å˜æ¢å™¨æ¥èšåˆå¤šæ¨¡æ€çš„æ—¶ç©ºä¿¡æ¯ï¼Œä»¥ç”Ÿæˆæ£€æµ‹å’Œè·Ÿè¸ªç‰¹å¾ã€‚</li>
<li>ç›®æ ‡é‡æ–°æ£€æµ‹æ¨¡å—ï¼ˆReDetï¼‰ç¡®ä¿è½¨è¿¹è¿ç»­æ€§ï¼Œç»´æŒä¸åŒæ¨¡æ€é—´çš„ä¸€è‡´æ€§ã€‚</li>
<li>ä»‹ç»äº†é¦–ä¸ªRGB-Tèåˆçš„å¤šå¾®å°ç›®æ ‡è·Ÿè¸ªåŸºå‡†æµ‹è¯•VT-Tiny-MOTã€‚</li>
<li>åœ¨VT-Tiny-MOTä¸Šçš„å®éªŒè¯æ˜äº†HGT-Trackçš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨MOTAå’ŒID-F1åˆ†æ•°ä¸Šè¾ƒå…¶ä»–æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10861">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f2246c1bdb4b6b52006eafaea112c527.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d724d2c81c912ca814da620472772611.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb00655e85754c9f17174fcc1cd49a67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b7da8f0505a6cdce75e4d9e091f244d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d76b22d5ab92db2d4f3785b321153757.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Analysis-of-Object-Detection-Models-for-Tiny-Object-in-Satellite-Imagery-A-Dataset-Centric-Approach"><a href="#Analysis-of-Object-Detection-Models-for-Tiny-Object-in-Satellite-Imagery-A-Dataset-Centric-Approach" class="headerlink" title="Analysis of Object Detection Models for Tiny Object in Satellite   Imagery: A Dataset-Centric Approach"></a>Analysis of Object Detection Models for Tiny Object in Satellite   Imagery: A Dataset-Centric Approach</h2><p><strong>Authors:Kailas PS, Selvakumaran R, Palani Murugan, Ramesh Kumar V, Malaya Kumar Biswal M</strong></p>
<p>In recent years, significant advancements have been made in deep learning-based object detection algorithms, revolutionizing basic computer vision tasks, notably in object detection, tracking, and segmentation. This paper delves into the intricate domain of Small-Object-Detection (SOD) within satellite imagery, highlighting the unique challenges stemming from wide imaging ranges, object distribution, and their varying appearances in birdâ€™s-eye-view satellite images. Traditional object detection models face difficulties in detecting small objects due to limited contextual information and class imbalances. To address this, our research presents a meticulously curated dataset comprising 3000 images showcasing cars, ships, and airplanes in satellite imagery. Our study aims to provide valuable insights into small object detection in satellite imagery by empirically evaluating state-of-the-art models. Furthermore, we tackle the challenges of satellite video-based object tracking, employing the Byte Track algorithm on the SAT-MTB dataset. Through rigorous experimentation, we aim to offer a comprehensive understanding of the efficacy of state-of-the-art models in Small-Object-Detection for satellite applications. Our findings shed light on the effectiveness of these models and pave the way for future advancements in satellite imagery analysis. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹ç®—æ³•å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä¸ºè®¡ç®—æœºè§†è§‰çš„åŸºæœ¬ä»»åŠ¡ï¼ˆç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡æ£€æµ‹ã€è·Ÿè¸ªå’Œåˆ†å‰²æ–¹é¢ï¼‰å¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†å«æ˜Ÿå›¾åƒä¸­çš„å°ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰çš„å¤æ‚é¢†åŸŸï¼Œé‡ç‚¹ä»‹ç»äº†ç”±äºæˆåƒèŒƒå›´å¹¿æ³›ã€ç›®æ ‡åˆ†å¸ƒåŠå…¶ä¿¯è§†å›¾å«æ˜Ÿå›¾åƒä¸­å¤–è§‚å˜åŒ–æ‰€å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ¨¡å‹ç”±äºä¸Šä¸‹æ–‡ä¿¡æ¯æœ‰é™å’Œç±»åˆ«ä¸å¹³è¡¡ï¼Œåœ¨æ£€æµ‹å°ç›®æ ‡æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ˆç°äº†ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ï¼ŒåŒ…å«3000å¼ å±•ç¤ºå«æ˜Ÿå›¾åƒä¸­çš„æ±½è½¦ã€èˆ¹åªå’Œé£æœºçš„å›¾åƒã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å®è¯è¯„ä¼°æœ€æ–°æ¨¡å‹ï¼Œä¸ºå«æ˜Ÿå›¾åƒä¸­çš„å°ç›®æ ‡æ£€æµ‹æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§£å†³äº†åŸºäºå«æ˜Ÿè§†é¢‘çš„è·Ÿè¸ªæŒ‘æˆ˜ï¼Œåœ¨SAT-MTBæ•°æ®é›†ä¸Šé‡‡ç”¨Byte Trackç®—æ³•ã€‚é€šè¿‡ä¸¥æ ¼çš„å®éªŒï¼Œæˆ‘ä»¬æ—¨åœ¨å…¨é¢ç†è§£æœ€æ–°æ¨¡å‹åœ¨å°ç›®æ ‡æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå«æ˜Ÿåº”ç”¨æä¾›å®è´µæ´å¯Ÿã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè¿™äº›æ¨¡å‹çš„æœ‰æ•ˆæ€§æä¾›äº†æœ‰åŠ›çš„è¯æ®ï¼Œå¹¶ä¸ºæœªæ¥çš„å«æ˜Ÿå›¾åƒåˆ†æè¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10453v1">PDF</a> Conference Proceesings of AIAA SciTech Forum 2025 and Exposition</p>
<p><strong>Summary</strong><br>éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å‘å±•ï¼Œå°ç›®æ ‡æ£€æµ‹å·²æˆä¸ºå«æ˜Ÿå›¾åƒåˆ†æä¸­çš„ç ”ç©¶çƒ­ç‚¹ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªä¸“é—¨çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡å®éªŒè¯„ä¼°äº†å½“å‰å…ˆè¿›æ¨¡å‹åœ¨å°ç›®æ ‡æ£€æµ‹ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æ¢è®¨äº†å«æ˜Ÿè§†é¢‘ä¸­çš„ç›®æ ‡è·Ÿè¸ªé—®é¢˜ï¼Œé‡‡ç”¨äº†Byte Trackç®—æ³•åœ¨SAT-MTBæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒã€‚ç ”ç©¶ç»“æœè¡¨æ˜è¿™äº›æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåç»­å«æ˜Ÿå›¾åƒåˆ†æçš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å°ç›®æ ‡æ£€æµ‹é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶åœ¨å«æ˜Ÿå›¾åƒåˆ†æä¸­åº”ç”¨å¹¿æ³›ã€‚</li>
<li>ä»‹ç»äº†ä¸“é—¨ç”¨äºå«æ˜Ÿå›¾åƒä¸­å°ç›®æ ‡æ£€æµ‹çš„æ•°æ®é›†ï¼ŒåŒ…å«3000å¼ å›¾åƒã€‚</li>
<li>æŒ‡å‡ºå°ç›®æ ‡æ£€æµ‹åœ¨å«æ˜Ÿå›¾åƒä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚æˆåƒèŒƒå›´å¹¿æ³›ã€ç›®æ ‡åˆ†å¸ƒå’Œå¤–è§‚å˜åŒ–ç­‰ã€‚</li>
<li>ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨æ£€æµ‹å°ç›®æ ‡æ—¶é¢ä¸´å›°éš¾ï¼Œå¦‚ä¸Šä¸‹æ–‡ä¿¡æ¯æœ‰é™å’Œç±»åˆ«ä¸å¹³è¡¡ç­‰é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨Byte Trackç®—æ³•åœ¨SAT-MTBæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œä»¥æ¢è®¨å«æ˜Ÿè§†é¢‘ä¸­çš„ç›®æ ‡è·Ÿè¸ªé—®é¢˜ã€‚</li>
<li>é€šè¿‡å®éªŒè¯„ä¼°äº†å½“å‰å…ˆè¿›æ¨¡å‹åœ¨å°ç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ªä¸­çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b759223d60ee0e78dd8f56d7c7bbd462.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e37ca6b57574dafcf228801112753e4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RemDet-Rethinking-Efficient-Model-Design-for-UAV-Object-Detection"><a href="#RemDet-Rethinking-Efficient-Model-Design-for-UAV-Object-Detection" class="headerlink" title="RemDet: Rethinking Efficient Model Design for UAV Object Detection"></a>RemDet: Rethinking Efficient Model Design for UAV Object Detection</h2><p><strong>Authors:Chen Li, Rui Zhao, Zeyu Wang, Huiying Xu, Xinzhong Zhu</strong></p>
<p>Object detection in Unmanned Aerial Vehicle (UAV) images has emerged as a focal area of research, which presents two significant challenges: i) objects are typically small and dense within vast images; ii) computational resource constraints render most models unsuitable for real-time deployment. Current real-time object detectors are not optimized for UAV images, and complex methods designed for small object detection often lack real-time capabilities. To address these challenges, we propose a novel detector, RemDet (Reparameter efficient multiplication Detector). Our contributions are as follows: 1) Rethinking the challenges of existing detectors for small and dense UAV images, and proposing information loss as a design guideline for efficient models. 2) We introduce the ChannelC2f module to enhance small object detection performance, demonstrating that high-dimensional representations can effectively mitigate information loss. 3) We design the GatedFFN module to provide not only strong performance but also low latency, effectively addressing the challenges of real-time detection. Our research reveals that GatedFFN, through the use of multiplication, is more cost-effective than feed-forward networks for high-dimensional representation. 4) We propose the CED module, which combines the advantages of ViT and CNN downsampling to effectively reduce information loss. It specifically enhances context information for small and dense objects. Extensive experiments on large UAV datasets, Visdrone and UAVDT, validate the real-time efficiency and superior performance of our methods. On the challenging UAV dataset VisDrone, our methods not only provided state-of-the-art results, improving detection by more than 3.4%, but also achieve 110 FPS on a single 4090. </p>
<blockquote>
<p>æ— äººæœºï¼ˆUAVï¼‰å›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹å·²æˆä¸ºç ”ç©¶çš„é‡è¦é¢†åŸŸï¼Œè¿™å¸¦æ¥äº†ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯å¯¹å¤§éƒ¨åˆ†ç›®æ ‡é€šå¸¸å¾ˆå°ä¸”å¯†é›†åœ°åˆ†å¸ƒåœ¨å¤§å‹å›¾åƒä¸­ï¼›äºŒæ˜¯è®¡ç®—èµ„æºé™åˆ¶ä½¿å¾—å¤§å¤šæ•°æ¨¡å‹ä¸é€‚åˆå®æ—¶éƒ¨ç½²ã€‚ç°æœ‰çš„å®æ—¶ç›®æ ‡æ£€æµ‹å™¨å¹¶ä¸é€‚ç”¨äºæ— äººæœºå›¾åƒï¼Œè€Œé’ˆå¯¹å°ç›®æ ‡æ£€æµ‹è®¾è®¡çš„å¤æ‚æ–¹æ³•å¾€å¾€ç¼ºä¹å®æ—¶æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ£€æµ‹å™¨RemDetï¼ˆé‡å‚æ•°é«˜æ•ˆä¹˜æ³•æ£€æµ‹å™¨ï¼‰ã€‚æˆ‘ä»¬çš„è´¡çŒ®å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬é‡æ–°æ€è€ƒäº†ç°æœ‰æ£€æµ‹å™¨åœ¨å°å‹ä¸”å¯†é›†çš„æ— äººæœºå›¾åƒä¸Šæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¿¡æ¯æŸå¤±ä½œä¸ºé«˜æ•ˆæ¨¡å‹è®¾è®¡çš„æŒ‡å¯¼åŸåˆ™ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ChannelC2fæ¨¡å—ä»¥æé«˜å°ç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œè¯æ˜äº†é«˜ç»´è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆåœ°å‡è½»ä¿¡æ¯æŸå¤±ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è®¾è®¡äº†GatedFFNæ¨¡å—ï¼Œä¸ä»…æä¾›å¼ºå¤§çš„æ€§èƒ½ï¼Œè€Œä¸”å…·æœ‰ä½å»¶è¿Ÿï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å®æ—¶æ£€æµ‹çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä¹˜æ³•è¿ç®—çš„GatedFFNæ¯”å‰é¦ˆç½‘ç»œæ›´ç»æµé«˜æ•ˆï¼Œç”¨äºé«˜ç»´è¡¨ç¤ºã€‚ç¬¬å››ï¼Œæˆ‘ä»¬æå‡ºäº†CEDæ¨¡å—ï¼Œå®ƒç»“åˆäº†ViTå’ŒCNNä¸‹é‡‡æ ·çš„ä¼˜ç‚¹ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†ä¿¡æ¯æŸå¤±ã€‚ç‰¹åˆ«æ˜¯æé«˜äº†å°ä¸”å¯†é›†ç›®æ ‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨å¤§è§„æ¨¡æ— äººæœºæ•°æ®é›†Visdroneå’ŒUAVDTä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„å®æ—¶æ•ˆç‡å’Œå“è¶Šæ€§èƒ½ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VisDroneæ— äººæœºæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æä¾›äº†ä¼˜äºå…¶ä»–æ–¹æ³•çš„æœ€æ–°ç»“æœï¼Œæé«˜äº†è¶…è¿‡3.4%çš„æ£€æµ‹æ€§èƒ½ï¼Œè€Œä¸”åœ¨å•ä¸ª4090ä¸Šå®ç°äº†110 FPSã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10040v2">PDF</a> Accepted to AAAI25</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹æ— äººæœºå›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹é¢ä¸´çš„å°ç›®æ ‡æ£€æµ‹ä¸å®æ—¶è®¡ç®—èµ„æºé™åˆ¶ä¸¤å¤§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ£€æµ‹å™¨RemDetã€‚é€šè¿‡å¼•å…¥ChannelC2fæ¨¡å—å¢å¼ºå°ç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œè®¾è®¡GatedFFNæ¨¡å—å®ç°å¼ºæ€§èƒ½ä¸ä½å»¶è¿Ÿï¼Œæå‡ºCEDæ¨¡å—ç»“åˆViTå’ŒCNNä¸‹é‡‡æ ·çš„ä¼˜åŠ¿ä»¥å‡å°‘ä¿¡æ¯æŸå¤±ã€‚åœ¨å¤§å‹æ— äººæœºæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶æ–¹æ³•å’Œæ€§èƒ½çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ— äººæœºå›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹é¢ä¸´å°ç›®æ ‡æ£€æµ‹å’Œå®æ—¶è®¡ç®—èµ„æºé™åˆ¶ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ£€æµ‹å™¨RemDetï¼Œé€šè¿‡ä¿¡æ¯æŸå¤±ä½œä¸ºè®¾è®¡é«˜æ•ˆæ¨¡å‹çš„åŸåˆ™ã€‚</li>
<li>é€šè¿‡å¼•å…¥ChannelC2fæ¨¡å—å¢å¼ºå°ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>GatedFFNæ¨¡å—è®¾è®¡å®ç°äº†é«˜æ€§èƒ½ä¸ä½å»¶è¿Ÿï¼Œè§£å†³äº†å®æ—¶æ£€æµ‹çš„æŒ‘æˆ˜ã€‚</li>
<li>CEDæ¨¡å—ç»“åˆäº†ViTå’ŒCNNä¸‹é‡‡æ ·çš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨å‡å°‘ä¿¡æ¯æŸå¤±å¹¶å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åœ¨å¤§å‹æ— äººæœºæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†RemDetçš„ä¼˜è¶Šæ€§èƒ½å’Œå®æ—¶æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-88a68a5e31cfcadad3f08bd2ab577688.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29a85ecb9005bb56ffb63b86687b3f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c3363759e74e24decb4f1687e52c969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58071f781b9a5fce03689e840084f268.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b5ba8dac80b6baac5d6eaed57c25a88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d380bf947769afb5f51c667662926b21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aacbd78ceb8ee4d8723af562fca5cc7a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="No-Annotations-for-Object-Detection-in-Art-through-Stable-Diffusion"><a href="#No-Annotations-for-Object-Detection-in-Art-through-Stable-Diffusion" class="headerlink" title="No Annotations for Object Detection in Art through Stable Diffusion"></a>No Annotations for Object Detection in Art through Stable Diffusion</h2><p><strong>Authors:Patrick Ramos, Nicolas Gonthier, Selina Khan, Yuta Nakashima, Noa Garcia</strong></p>
<p>Object detection in art is a valuable tool for the digital humanities, as it allows for faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion modelsâ€™ art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art. Code is available at <a target="_blank" rel="noopener" href="https://github.com/patrick-john-ramos/nada">https://github.com/patrick-john-ramos/nada</a> </p>
<blockquote>
<p>è‰ºæœ¯å“ä¸­çš„ç›®æ ‡æ£€æµ‹å¯¹äºæ•°å­—äººæ–‡æ¥è¯´æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ï¼Œå› ä¸ºå®ƒä¸äººç±»ç›¸æ¯”ï¼Œå¯ä»¥æ›´å¿«åœ°è¯†åˆ«è‰ºæœ¯å’Œå†å²å›¾åƒä¸­çš„ç›®æ ‡ã€‚ç„¶è€Œï¼Œå¯¹è¿™äº›å›¾åƒè¿›è¡Œæ ‡æ³¨å´å¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦ä¸“ä¸šçš„é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†NADAï¼ˆè‰ºæœ¯å“æ£€æµ‹æ— éœ€æ ‡æ³¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€å®Œæ•´è¾¹ç•Œæ¡†ç›‘ç£å³å¯åœ¨ç»˜ç”»ä¸­è¿›è¡Œç›®æ ‡æ£€æµ‹çš„ç®¡é“ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¸è‰ºæœ¯ç›¸å…³çš„çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå¼±ç›‘ç£å’Œæ— æºåœºæ™¯ï¼Œå¹¶ä¸”ä¸éœ€è¦å¯¹å…¶é¢„è®­ç»ƒç»„ä»¶è¿›è¡Œä»»ä½•å¾®è°ƒï¼Œå®ƒç”±åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»æå‡ºè€…å’ŒåŸºäºStable Diffusionçš„ç±»æ¡ä»¶æ£€æµ‹å™¨ç»„æˆã€‚NADAåœ¨ArtDL 2.0å’ŒIconArtä¸¤ä¸ªè‰ºæœ¯å“æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨å¼±ç›‘ç£æ£€æµ‹æ–¹é¢ä¼˜äºå…ˆå‰çš„å·¥ä½œï¼ŒåŒæ—¶æ˜¯è‰ºæœ¯å“é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹çš„ç¬¬ä¸€é¡¹å·¥ä½œã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/patrick-john-ramos/nada%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/patrick-john-ramos/nadaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06286v2">PDF</a> 8 pages, 6 figures, to be published in WACV 2025</p>
<p><strong>Summary</strong></p>
<p>å¯¹è±¡æ£€æµ‹åœ¨è‰ºæœ¯é¢†åŸŸæ˜¯æ•°å­—äººæ–‡çš„é‡è¦å·¥å…·ï¼Œèƒ½æ›´å¿«åœ°åœ¨è‰ºæœ¯å’Œå†å²å›¾åƒä¸­è¯†åˆ«ç‰©ä½“ã€‚ç„¶è€Œï¼Œå¯¹è¿™ç±»å›¾åƒè¿›è¡Œæ ‡æ³¨éœ€è¦ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå­˜åœ¨å·¨å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†NADAï¼ˆè‰ºæœ¯ä¸­æ— éœ€æ ‡æ³¨çš„æ£€æµ‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¸è‰ºæœ¯ç›¸å…³çš„çŸ¥è¯†ï¼Œåœ¨ç”»ä½œä¸­è¿›è¡Œç‰©ä½“æ£€æµ‹çš„æ–¹æ³•ï¼Œæ— éœ€å®Œæ•´çš„è¾¹ç•Œæ¡†ç›‘ç£ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå¼±ç›‘ç£å’Œæ— æºåœºæ™¯ï¼Œä¸”ä¸éœ€è¦å¯¹é¢„è®­ç»ƒç»„ä»¶è¿›è¡Œå¾®è°ƒï¼Œå®ƒç”±åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»æè®®å™¨å’ŒåŸºäºStable Diffusionçš„ç±»æ¡ä»¶æ£€æµ‹å™¨ç»„æˆã€‚NADAåœ¨ArtDL 2.0å’ŒIconArtä¸¤ä¸ªè‰ºæœ¯å“æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨å¼±ç›‘ç£æ£€æµ‹æ–¹é¢ä¼˜äºä»¥å‰çš„å·¥ä½œï¼Œå¹¶ä¸”æ˜¯è‰ºæœ¯ä¸­é›¶æ ·æœ¬å¯¹è±¡æ£€æµ‹çš„é¦–æ¬¡å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¯¹è±¡æ£€æµ‹åœ¨è‰ºæœ¯é¢†åŸŸå…·æœ‰é‡è¦ä»·å€¼ï¼Œå¯ä»¥å¿«é€Ÿè¯†åˆ«è‰ºæœ¯å’Œå†å²å›¾åƒä¸­çš„ç‰©ä½“ã€‚</li>
<li>æ ‡æ³¨è‰ºæœ¯å›¾åƒé¢ä¸´éœ€è¦ç‰¹å®šé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„æŒ‘æˆ˜ã€‚</li>
<li>NADAæ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¸è‰ºæœ¯ç›¸å…³çš„çŸ¥è¯†ï¼Œå®ç°äº†åœ¨ç”»ä½œä¸­çš„ç‰©ä½“æ£€æµ‹ï¼Œæ— éœ€å®Œæ•´çš„è¾¹ç•Œæ¡†ç›‘ç£ã€‚</li>
<li>NADAæ–¹æ³•æ”¯æŒå¼±ç›‘ç£å’Œæ— æºåœºæ™¯ï¼Œä¸”é¢„è®­ç»ƒç»„ä»¶æ— éœ€å¾®è°ƒã€‚</li>
<li>NADAç”±åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»æè®®å™¨å’ŒåŸºäºStable Diffusionçš„ç±»æ¡ä»¶æ£€æµ‹å™¨ç»„æˆã€‚</li>
<li>NADAåœ¨ArtDL 2.0å’ŒIconArtä¸¤ä¸ªè‰ºæœ¯å“æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¼˜äºå…ˆå‰çš„å¼±ç›‘ç£æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>NADAæ˜¯é¦–æ¬¡å°è¯•åœ¨è‰ºæœ¯å“æ£€æµ‹ä¸­å®ç°é›¶æ ·æœ¬å¯¹è±¡æ£€æµ‹çš„å·¥ä½œã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57c17d24733537fe1886250560c883fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4139a38513f0b2f267b75456dd3dfdfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b04ddd24cf532a8716dc0a4011c96116.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9735771e9b15404efe8fd522bb09352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21873767c7771c024dd0a90f4d9774c2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1><h2 id="NBBOX-Noisy-Bounding-Box-Improves-Remote-Sensing-Object-Detection"><a href="#NBBOX-Noisy-Bounding-Box-Improves-Remote-Sensing-Object-Detection" class="headerlink" title="NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection"></a>NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection</h2><p><strong>Authors:Yechan Kim, SooYeon Kim, Moongu Jeon</strong></p>
<p>Data augmentation has shown significant advancements in computer vision to improve model performance over the years, particularly in scenarios with limited and insufficient data. Currently, most studies focus on adjusting the image or its features to expand the size, quality, and variety of samples during training in various tasks including object detection. However, we argue that it is necessary to investigate bounding box transformations as a data augmentation technique rather than image-level transformations, especially in aerial imagery due to potentially inconsistent bounding box annotations. Hence, this letter presents a thorough investigation of bounding box transformation in terms of scaling, rotation, and translation for remote sensing object detection. We call this augmentation strategy NBBOX (Noise Injection into Bounding Box). We conduct extensive experiments on DOTA and DIOR-R, both well-known datasets that include a variety of rotated generic objects in aerial images. Experimental results show that our approach significantly improves remote sensing object detection without whistles and bells and it is more time-efficient than other state-of-the-art augmentation strategies. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¤šå¹´æ¥åœ¨æé«˜æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™å’Œä¸è¶³çš„åœºæ™¯ä¸‹ï¼Œå‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚ç›®å‰ï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨è°ƒæ•´å›¾åƒæˆ–å…¶ç‰¹å¾ä¸Šï¼Œä»¥æ‰©å¤§æ ·æœ¬çš„å¤§å°ã€è´¨é‡å’Œå¤šæ ·æ€§ï¼Œç”¨äºå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç›®æ ‡æ£€æµ‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰å¿…è¦ç ”ç©¶è¾¹ç•Œæ¡†å˜æ¢ä½œä¸ºä¸€ç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œè€Œä¸æ˜¯å›¾åƒçº§åˆ«çš„å˜æ¢ï¼Œç‰¹åˆ«æ˜¯åœ¨èˆªç©ºå›¾åƒä¸­ï¼Œå› ä¸ºè¾¹ç•Œæ¡†æ³¨é‡Šå¯èƒ½å­˜åœ¨æ½œåœ¨çš„ä¸ä¸€è‡´æ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡å…¨é¢ç ”ç©¶äº†è¾¹ç•Œæ¡†å˜æ¢åœ¨ç¼©æ”¾ã€æ—‹è½¬å’Œå¹³ç§»æ–¹é¢çš„é¥æ„Ÿç›®æ ‡æ£€æµ‹ã€‚æˆ‘ä»¬å°†è¿™ç§å¢å¼ºç­–ç•¥ç§°ä¸ºNBBOXï¼ˆå™ªå£°æ³¨å…¥è¾¹ç•Œæ¡†ï¼‰ã€‚æˆ‘ä»¬åœ¨DOTAå’ŒDIOR-Rè¿™ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¿™ä¸¤ä¸ªæ•°æ®é›†éƒ½åŒ…å«èˆªç©ºå›¾åƒä¸­çš„å¤šç§æ—‹è½¬é€šç”¨å¯¹è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸éœ€è¦è¿‡å¤šä¿®é¥°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†é¥æ„Ÿç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œå¹¶ä¸”ä¸å…¶ä»–æœ€å…ˆè¿›çš„å¢å¼ºç­–ç•¥ç›¸æ¯”ï¼Œæ›´åŠ é«˜æ•ˆçœæ—¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09424v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ•°æ®å¢å¼ºåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™å’Œä¸è¶³çš„æƒ…å¢ƒä¸‹æé«˜æ¨¡å‹æ€§èƒ½ã€‚å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è°ƒæ•´å›¾åƒæˆ–å…¶ç‰¹å¾ä»¥æ‰©å¤§æ ·æœ¬å¤§å°ã€è´¨é‡å’Œå¤šæ ·æ€§ï¼Œç”¨äºå„ç§ä»»åŠ¡ä¸­çš„è®­ç»ƒï¼ŒåŒ…æ‹¬ç›®æ ‡æ£€æµ‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰å¿…è¦ç ”ç©¶è¾¹ç•Œæ¡†å˜æ¢ä½œä¸ºä¸€ç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œè€Œä¸æ˜¯å›¾åƒçº§åˆ«çš„å˜æ¢ï¼Œç‰¹åˆ«æ˜¯åœ¨èˆªç©ºå›¾åƒä¸­ï¼Œå› ä¸ºè¾¹ç•Œæ¡†æ³¨é‡Šå¯èƒ½å­˜åœ¨ä¸ä¸€è‡´çš„æƒ…å†µã€‚å› æ­¤ï¼Œæœ¬æ–‡å…¨é¢ç ”ç©¶äº†è¾¹ç•Œæ¡†å˜æ¢åœ¨ç¼©æ”¾ã€æ—‹è½¬å’Œç¿»è¯‘æ–¹é¢çš„é¥æ„Ÿç›®æ ‡æ£€æµ‹ã€‚æˆ‘ä»¬å°†è¿™ç§å¢å¼ºç­–ç•¥ç§°ä¸ºNBBOXï¼ˆå™ªå£°æ³¨å…¥è¾¹ç•Œæ¡†ï¼‰ã€‚æˆ‘ä»¬åœ¨DOTAå’ŒDIOR-Rè¿™ä¸¤ä¸ªåŒ…å«èˆªç©ºå›¾åƒä¸­å„ç§æ—‹è½¬é€šç”¨ç›®æ ‡çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸éœ€è¦å¤æ‚æ“ä½œçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†é¥æ„Ÿç›®æ ‡æ£€æµ‹çš„ç²¾åº¦ï¼Œå¹¶ä¸”ä¸å…¶ä»–æœ€å…ˆè¿›çš„å¢å¼ºç­–ç•¥ç›¸æ¯”æ›´åŠ çœæ—¶é«˜æ•ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>æ•°æ®å¢å¼ºåœ¨æ”¹è¿›æ¨¡å‹æ€§èƒ½ä¸Šå‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å›¾åƒçº§åˆ«çš„æ•°æ®å¢å¼ºï¼Œä½†è¾¹ç•Œæ¡†å˜æ¢åŒæ ·é‡è¦ã€‚</li>
<li>è¾¹ç•Œæ¡†æ³¨é‡Šåœ¨èˆªç©ºå›¾åƒä¸­å¯èƒ½å­˜åœ¨ä¸ä¸€è‡´çš„æƒ…å†µï¼Œéœ€è¦è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚</li>
<li>ä»‹ç»äº†NBBOXç­–ç•¥ï¼Œå®ƒé€šè¿‡è¾¹ç•Œæ¡†çš„ç¼©æ”¾ã€æ—‹è½¬å’Œç¿»è¯‘æ¥è¿›è¡Œæ•°æ®å¢å¼ºã€‚</li>
<li>åœ¨DOTAå’ŒDIOR-Ræ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNBBOXç­–ç•¥åœ¨é¥æ„Ÿç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-35715ef4725aac1b951c3eabaf5c5ed3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5f2e95849f13ebe4b4f717da096fc08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6701815eefdbef184100b57582da634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88fcf71de3685c7515c363ca58222902.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-055430be74fa599b9db20f5d1ce67d9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19001d41d9534ed05204d3341507a4da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1957199965f5c39375b00e43dd37ceaf.jpg" align="middle">
</details>


<h1 id="-20"><a href="#-20" class="headerlink" title=""></a></h1><h2 id="Replay-Consolidation-with-Label-Propagation-for-Continual-Object-Detection"><a href="#Replay-Consolidation-with-Label-Propagation-for-Continual-Object-Detection" class="headerlink" title="Replay Consolidation with Label Propagation for Continual Object   Detection"></a>Replay Consolidation with Label Propagation for Continual Object   Detection</h2><p><strong>Authors:Riccardo De Monte, Davide Dalle Pezze, Marina Ceccon, Francesco Pasti, Francesco Paissan, Elisabetta Farella, Gian Antonio Susto, Nicola Bellotto</strong></p>
<p>Continual Learning (CL) aims to learn new data while remembering previously acquired knowledge. In contrast to CL for image classification, CL for Object Detection faces additional challenges such as the missing annotations problem. In this scenario, images from previous tasks may contain instances of unknown classes that could reappear as labeled in future tasks, leading to task interference in replay-based approaches. Consequently, most approaches in the literature have focused on distillation-based techniques, which are effective when there is a significant class overlap between tasks. In our work, we propose an alternative to distillation-based approaches with a novel approach called Replay Consolidation with Label Propagation for Object Detection (RCLPOD). RCLPOD enhances the replay memory by improving the quality of the stored samples through a technique that promotes class balance while also improving the quality of the ground truth associated with these samples through a technique called label propagation. RCLPOD outperforms existing techniques on well-established benchmarks such as VOC and COC. Moreover, our approach is developed to work with modern architectures like YOLOv8, making it suitable for dynamic, real-world applications such as autonomous driving and robotics, where continuous learning and resource efficiency are essential. </p>
<blockquote>
<p>æŒç»­å­¦ä¹ ï¼ˆCLï¼‰æ—¨åœ¨å­¦ä¹ æ–°æ•°æ®çš„åŒæ—¶ä¿ç•™ä¹‹å‰è·å¾—çš„çŸ¥è¯†ã€‚ä¸ç”¨äºå›¾åƒåˆ†ç±»çš„CLä¸åŒï¼Œç”¨äºç›®æ ‡æ£€æµ‹çš„CLé¢ä¸´é¢å¤–çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç¼ºå¤±æ³¨é‡Šé—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¥è‡ªå…ˆå‰ä»»åŠ¡çš„å›¾åƒå¯èƒ½åŒ…å«æœªçŸ¥ç±»çš„å®ä¾‹ï¼Œè¿™äº›å®ä¾‹å¯èƒ½åœ¨æœªæ¥ä»»åŠ¡ä¸­è¢«æ ‡è®°ä¸ºé‡ç°ï¼Œå¯¼è‡´åŸºäºé‡æ’­çš„æ–¹æ³•å‡ºç°ä»»åŠ¡å¹²æ‰°ã€‚å› æ­¤ï¼Œæ–‡çŒ®ä¸­çš„å¤§å¤šæ•°æ–¹æ³•éƒ½é›†ä¸­åœ¨åŸºäºè’¸é¦çš„æŠ€æœ¯ä¸Šï¼Œè¿™åœ¨ä»»åŠ¡ä¹‹é—´å­˜åœ¨å¤§é‡ç±»é‡å æ—¶éå¸¸æœ‰æ•ˆã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè’¸é¦æ–¹æ³•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œç§°ä¸ºç”¨äºç›®æ ‡æ£€æµ‹çš„å›æ”¾å·©å›ºä¸æ ‡ç­¾ä¼ æ’­ï¼ˆRCLPODï¼‰ã€‚RCLPODé€šè¿‡æé«˜å­˜å‚¨æ ·æœ¬çš„è´¨é‡æ¥å¢å¼ºå›æ”¾è®°å¿†ï¼Œé‡‡ç”¨ä¸€ç§ä¿ƒè¿›ç±»å¹³è¡¡çš„æŠ€æœ¯ï¼ŒåŒæ—¶é€šè¿‡æ ‡ç­¾ä¼ æ’­æŠ€æœ¯æé«˜ä¸è¿™äº›æ ·æœ¬ç›¸å…³çš„åœ°é¢çœŸå®æ€§çš„è´¨é‡ã€‚RCLPODåœ¨VOCå’ŒCOCç­‰æ—¢å®šåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸ºç°ä»£æ¶æ„ï¼ˆå¦‚YOLOv8ï¼‰è€Œå¼€å‘çš„ï¼Œå› æ­¤éå¸¸é€‚åˆåŠ¨æ€ã€ç°å®ä¸–ç•Œçš„åº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ï¼‰ï¼Œåœ¨è¿™é‡Œï¼ŒæŒç»­å­¦ä¹ å’Œèµ„æºæ•ˆç‡è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05650v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æŒç»­å­¦ä¹ ï¼ˆCLï¼‰åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸçš„åº”ç”¨åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹å›¾åƒåˆ†ç±»çš„CLæ–¹æ³•ä¸é€‚ç”¨äºç›®æ ‡æ£€æµ‹ï¼Œå› ä¸ºç›®æ ‡æ£€æµ‹é¢ä¸´ç¼ºå°‘æ³¨é‡Šçš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°æ–‡çŒ®ä¸­çš„æ–¹æ³•éƒ½é›†ä¸­åœ¨åŸºäºè’¸é¦çš„æŠ€æœ¯ä¸Šï¼Œä½†å½“ä»»åŠ¡é—´å­˜åœ¨å¤§é‡ç±»åˆ«é‡å æ—¶ï¼Œè¿™äº›æ–¹æ³•çš„æ•ˆæœå¯èƒ½æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ›¿ä»£æ–¹æ³•â€”â€”å›æ”¾æ•´åˆä¸æ ‡ç­¾ä¼ æ’­ç›¸ç»“åˆçš„ç›®æ ‡æ£€æµ‹ï¼ˆRCLPODï¼‰ã€‚RCLPODé€šè¿‡æé«˜å­˜å‚¨æ ·æœ¬çš„è´¨é‡å’Œä¸å…¶å…³è”çš„åœ°é¢çœŸå®åº¦æ¥æ”¹å–„å›æ”¾è®°å¿†ã€‚å®ƒæ¨å¹¿äº†ç±»åˆ«å¹³è¡¡ï¼Œæ”¹è¿›äº†æ ·æœ¬æ ‡è®°è¿‡ç¨‹ï¼Œå¹¶å–å¾—äº†ç°æœ‰æŠ€æœ¯åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„å‡ºè‰²è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¸ç°ä»£æ¶æ„ï¼ˆå¦‚YOLOv8ï¼‰å…¼å®¹ï¼Œé€‚ç”¨äºè‡ªä¸»é©¾é©¶å’Œæœºå™¨äººç­‰åŠ¨æ€å®é™…åº”ç”¨åœºæ™¯ã€‚è¿™äº›åœºæ™¯ä¸­ï¼ŒæŒç»­å­¦ä¹ å’Œèµ„æºæ•ˆç‡è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>æŒç»­å­¦ä¹ ï¼ˆCLï¼‰æ—¨åœ¨å­¦ä¹ æ–°æ•°æ®çš„åŒæ—¶ä¿ç•™å…ˆå‰è·å¾—çš„çŸ¥è¯†ã€‚åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸåº”ç”¨æ—¶é¢ä¸´ç¼ºå°‘æ³¨é‡Šç­‰æŒ‘æˆ˜ã€‚</p>
</li>
<li><p>ä¼ ç»ŸåŸºäºè’¸é¦çš„æ–¹æ³•åœ¨å¤„ç†å­˜åœ¨å¤§é‡ç±»åˆ«é‡å çš„ä»»åŠ¡æ—¶æ•ˆæœæœ‰é™ã€‚è¿™æ„å‘³ç€å•çº¯ä¾èµ–è¿™äº›æ–¹æ³•æ— æ³•åœ¨æ‰€æœ‰åœºæ™¯ä¸­å–å¾—ç†æƒ³ç»“æœã€‚ç°æœ‰çš„ç ”ç©¶å¤šé›†ä¸­äºè¿™ç§èƒŒæ™¯ä¸‹è¿›è¡Œç›®æ ‡æ£€æµ‹çš„æŠ€æœ¯çªç ´ã€‚åœ¨éœ€è¦è·¨ä¸åŒä»»åŠ¡è¿›è¡Œå­¦ä¹ çš„æƒ…å†µä¸‹ï¼ŒæŸäº›é¢†åŸŸæŠ€æœ¯é¢ä¸´ç€è¯†åˆ«å‡†ç¡®ç‡ä¸é«˜çš„é—®é¢˜ã€‚æˆ‘ä»¬é¢ä¸´å¯¹é€‚ç”¨äºçœŸå®ä¸–ç•Œçš„è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚å¢é•¿è¶‹åŠ¿ä»¥åŠåˆ›æ–°å‹åŠ›å’ŒæŒ‘æˆ˜åœ¨ä¸æ–­åŠ å‰§çš„æƒ…å½¢ä¸‹è¿›è¡Œè¿™äº›æ¢ç´¢å’ŒæŠ€æœ¯ç ”ç©¶æ˜¯ä¸€å¤§å…³é”®é¢†åŸŸï¼ˆç‚¹ï¼‰ï¼ˆå› ä¸ºè¿«åˆ‡éœ€è¦æ»¡è¶³ç¤¾ä¼šå®é™…åº”ç”¨çš„è¿ç»­å­¦ä¹ å’Œé¢„æµ‹å‡†ç¡®åº¦çš„åŒé‡è¦æ±‚ï¼‰ã€‚æ–‡ä¸­å¼ºè°ƒè‡ªä¸»é©¾é©¶å’Œæœºå™¨äººé¢†åŸŸç­‰å¯¹æŒç»­å­¦ä¹ å’Œèµ„æºæ•ˆç‡çš„æé«˜è¦æ±‚è¡¨æ˜å®é™…ç”Ÿæ´»ä¸­è§£å†³è¯¥é¢†åŸŸçš„é—®é¢˜ç´§è¿«æ€§ä¸æ–­æé«˜ä¹Ÿæ˜¾ç¤ºç°å®éœ€æ±‚å’Œä¸æ–­å¢é•¿çš„å‹åŠ›è¡¨æ˜ç¼ºä¹å®æ—¶ç²¾å‡†å’Œç²¾ç¡®æ ‡æ³¨çš„é«˜è´¨é‡æ•°æ®æºæ­£åœ¨é™åˆ¶è¿™ç±»ç³»ç»Ÿçš„æˆåŠŸå› æ­¤é¢ä¸´çš„æŒ‘æˆ˜ä¹Ÿå¾ˆå¤§æ„å‘³ç€åº”å¯¹åœºæ™¯é€‚åº”æ€§å¼±é«˜å‡†ç¡®ç‡å¯é›†æˆç°å®ç¯å¢ƒçš„è‡ªåŠ¨åŒ–ç¨‹åº¦å°šéœ€è¿›ä¸€æ­¥åŠ å¼ºæ•´ä½“å‘å±•è¶‹åŠ¿æ¥çœ‹å®ç°é«˜è´¨é‡æŒç»­å­¦ä¹ åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸæ˜¯å½“ä¸‹è¿«åˆ‡çš„éœ€æ±‚å’Œå‘å±•è¶‹åŠ¿çš„é‡è¦ä½“ç°ç‚¹ï¼‰ã€‚å…·ä½“æ¥è¯´åœ¨è§£å†³ç¼ºå°‘æ³¨é‡Šé—®é¢˜æ—¶æ„å»ºä¸€å¥—çµæ´»å¯é çš„æ ·æœ¬æ”¶é›†å’Œæ ‡è®°æµç¨‹ä»¥åŠåº”å¯¹æ¨¡å‹è¿‡åº¦é€‚åº”å½“å‰ä»»åŠ¡çš„ç­–ç•¥å°±æ˜¾å¾—å°¤ä¸ºé‡è¦åŒæ—¶ä¹Ÿä¸ºæœªæ¥çš„æŠ€æœ¯å‘å±•å’Œå®é™…åº”ç”¨æä¾›äº†é‡è¦çš„æ€è·¯å’Œæ–¹å‘å› æ­¤å»ºç«‹æ›´åŠ æ™ºèƒ½çµæ´»çš„æ ‡æ³¨ç³»ç»Ÿå¯¹äºæœªæ¥çš„æŒç»­å­¦ä¹ å’Œç›®æ ‡æ£€æµ‹æŠ€æœ¯çš„å‘å±•è‡³å…³é‡è¦å¯¹äºå®é™…åº”ç”¨åœºæ™¯è€Œè¨€åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œç²¾å‡†çš„ç›®æ ‡æ£€æµ‹å’ŒæŒç»­å­¦ä¹ æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚ç°æœ‰åŸºäºå›æ”¾çš„æŠ€æœ¯å¯é€šè¿‡å¤šç§é€”å¾„æé«˜å‡†ç¡®æ€§å’Œé€‚ç”¨æ€§æ„å‘³ç€åˆ›æ–°çš„ç›®æ ‡æ£€æµ‹æ–¹æ³•ä¸æˆç†Ÿåº”ç”¨çš„é›†æˆæ˜¯éå¸¸é‡è¦çš„è¿™åœ¨æ‰©å¤§å¸‚åœºåº”ç”¨èŒƒå›´æ¨åŠ¨æŠ€æœ¯åˆ›æ–°å‘å±•å¢å¼ºå¯¹ç¤¾ä¼šçš„é€‚åº”æ€§æ–¹é¢æœ‰ç€é‡è¦æ½œåŠ›æˆ‘ä»¬éœ€è¦è‡´åŠ›äºåœ¨ç»§ç»­åº”å¯¹å…³é”®é¢†åŸŸé—®é¢˜çš„åŒæ—¶ç ”ç©¶å…ˆè¿›çš„æœºå™¨å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨ä»¥æ— äººé©¾é©¶ç³»ç»Ÿä¸ºé¦–çš„æ–°æŠ€æœ¯å’Œç°ä»£åŒ–å¹³å°ä¸Šç¡®ä¿å¯é æ€§ç»´æŠ¤æªæ–½ç­‰å› ç´ åœ¨æ•´ä¸ªç ”ç©¶å’Œäº§å“å¼€å‘è¿‡ç¨‹ä¸­çš„ç»¼åˆè€ƒé‡ç¡®ä¿äº†å…¶æŒç»­å­¦ä¹ ç³»ç»Ÿçš„æœ‰æ•ˆæ€§è¿™äº›åˆ›æ–°å¯¹äºä¿æŒæ¨¡å‹æ•ˆèƒ½åŒæ—¶ç¡®ä¿æ€§èƒ½ä¸€è‡´æ€§ä»¥ç¬¦åˆç”¨æˆ·æœŸæœ›å°†å‘æŒ¥å…³é”®ä½œç”¨åœ¨å¤æ‚å¤šå˜çš„ç¯å¢ƒä¸­ä¿è¯æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§å¹¶å‡å°‘äººä¸ºå¹²é¢„çš„éœ€æ±‚ä»¥ä¼˜åŒ–æ•´ä¸ªç³»ç»Ÿçš„æ€§èƒ½å®ç°æœªæ¥æŠ€æœ¯åº”ç”¨çš„å…³é”®ç›®æ ‡ä¹‹ä¸€æ˜¯ä¼˜åŒ–å’Œè‡ªåŠ¨åŒ–å†³ç­–è¿‡ç¨‹çš„å¤æ‚æ€§é™ä½å¯¹ç”¨æˆ·å‚ä¸åº¦çš„ä¾èµ–ä»¥ä¾¿èƒ½é«˜æ•ˆè¿è¡Œå¹¶ä¿è¯é«˜å¯ç”¨æ€§è¿™æ„å‘³ç€è¯¥é¢†åŸŸæœªæ¥ä¼šå‘æ™ºèƒ½åŒ–å‘å±•å¹¶è§£å†³ç›¸å…³å®é™…é—®é¢˜è¿›ä¸€æ­¥æ¨è¿›è¯¥æŠ€æœ¯çš„å•†ä¸šåŒ–è¿›ç¨‹å’Œå¸‚åœºæ™®åŠåº¦æ¨åŠ¨å…¶åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸçš„å•†ä¸šåŒ–æ¨å¹¿ä»ç¤¾ä¼šå‘å±•è§†è§’è¡¨æ˜å…¬ä¼—å’ŒæŠ€æœ¯å¸‚åœºå¯¹äºç»§ç»­å®ç°å…³é”®æŠ€æœ¯é£è·ƒçš„é‡è§†æ˜¯éå¸¸ç§¯æçš„æ”¹è¿›è¿›æ­¥ä¹ƒè‡³è´¨çš„é£è·ƒè¡Œä¸šæ€»ä½“æœç€æ›´é«˜çš„é›†æˆåº¦è‡ªåŠ¨åŒ–ä»¥åŠçµæ´»æ€§å’Œæ™ºèƒ½åŒ–çš„æ–¹å‘å‘å±•å¹¶ä¸ºé€‚åº”ä¸åŒéœ€æ±‚æä¾›æ›´ä¸°å¯Œçš„é€‰æ‹©è¿™ç¬¦åˆæ•´ä¸ªè¡Œä¸šçš„æœªæ¥å‘å±•è¶‹åŠ¿å¹¶æœ‰æœ›æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥çªç ´å’Œåˆ›æ–°å‘å±•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ”¹è¿›å›æ”¾è®°å¿†å’Œæ ‡ç­¾ä¼ æ’­ç­–ç•¥ä»è€Œæœ‰æ•ˆåº”å¯¹ä»¥ä¸ŠæŒ‘æˆ˜ä¸ä»…ä½¿æ–°åº”ç”¨åœºæ™¯å’ŒæŠ€æœ¯çš„è¿›æ­¥ç›¸è¾…ç›¸æˆè¿˜å°†æ¿€å‘è¿›ä¸€æ­¥çš„ç§‘æŠ€åˆ›æ–°ä¿ƒä½¿è¯¥æŠ€æœ¯è¿›å…¥æ›´é«˜å±‚æ¬¡çš„æ™ºèƒ½åº”ç”¨é˜¶æ®µä¸ºå®ç°æ›´åŠ æ™ºèƒ½é«˜æ•ˆçš„ç¤¾ä¼šç”Ÿæ´»æä¾›äº†å¯èƒ½æ€§ã€‚<strong>Key Takeawaysçš„å…·ä½“å†…å®¹éœ€è¦åŸºäºåŸæ–‡å†…å®¹ç²¾ç®€æç‚¼å‡ºæœ€é‡è¦çš„å‡ ç‚¹å³å¯ã€‚</strong>ä»¥ä¸‹æ˜¯æ ¹æ®åŸæ–‡æç‚¼å‡ºçš„å…³é”®è¦ç‚¹ï¼š</p>
</li>
<li><p>æŒç»­å­¦ä¹ åœ¨ç›®æ ‡æ£€æµ‹é¢†åŸŸé¢ä¸´ç¼ºå°‘æ³¨é‡Šçš„æŒ‘æˆ˜ã€‚</p>
</li>
<li><p>åŸºäºè’¸é¦çš„æ–¹æ³•åœ¨å¤„ç†ç±»åˆ«é‡å ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</p>
</li>
<li><p>æœ¬æ–‡æå‡ºçš„RCLPODæ–¹æ³•é€šè¿‡æ”¹è¿›å›æ”¾è®°å¿†å’Œæ ‡ç­¾ä¼ æ’­ç­–ç•¥æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-995e3bef6160254e37d916d6605ca8c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4750bd5a3e48983a3ef51213eecf7b65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-435016e1d12d2eedb2e2c1e25ffebbec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0f6b0436c67298d10c0d231aea616e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a024b26c3595d530bfe5a65e7951bf46.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fe30539d8cf3716de9d85039b6cae09.jpg" align="middle">
</details>


<h1 id="-21"><a href="#-21" class="headerlink" title=""></a></h1><h2 id="ESOD-Efficient-Small-Object-Detection-on-High-Resolution-Images"><a href="#ESOD-Efficient-Small-Object-Detection-on-High-Resolution-Images" class="headerlink" title="ESOD: Efficient Small Object Detection on High-Resolution Images"></a>ESOD: Efficient Small Object Detection on High-Resolution Images</h2><p><strong>Authors:Kai Liu, Zhihang Fu, Sheng Jin, Ze Chen, Fan Zhou, Rongxin Jiang, Yaowu Chen, Jieping Ye</strong></p>
<p>Enlarging input images is a straightforward and effective approach to promote small object detection. However, simple image enlargement is significantly expensive on both computations and GPU memory. In fact, small objects are usually sparsely distributed and locally clustered. Therefore, massive feature extraction computations are wasted on the non-target background area of images. Recent works have tried to pick out target-containing regions using an extra network and perform conventional object detection, but the newly introduced computation limits their final performance. In this paper, we propose to reuse the detectorâ€™s backbone to conduct feature-level object-seeking and patch-slicing, which can avoid redundant feature extraction and reduce the computation cost. Incorporating a sparse detection head, we are able to detect small objects on high-resolution inputs (e.g., 1080P or larger) for superior performance. The resulting Efficient Small Object Detection (ESOD) approach is a generic framework, which can be applied to both CNN- and ViT-based detectors to save the computation and GPU memory costs. Extensive experiments demonstrate the efficacy and efficiency of our method. In particular, our method consistently surpasses the SOTA detectors by a large margin (e.g., 8% gains on AP) on the representative VisDrone, UAVDT, and TinyPerson datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/alibaba/esod">https://github.com/alibaba/esod</a>. </p>
<blockquote>
<p>æ”¾å¤§è¾“å…¥å›¾åƒæ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¿ƒè¿›å°ç›®æ ‡æ£€æµ‹çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç®€å•çš„å›¾åƒæ”¾å¤§åœ¨è®¡ç®—å’ŒGPUå†…å­˜æ–¹é¢æˆæœ¬é«˜æ˜‚ã€‚å®é™…ä¸Šï¼Œå°ç›®æ ‡é€šå¸¸ç¨€ç–åˆ†å¸ƒä¸”å±€éƒ¨èšé›†ã€‚å› æ­¤ï¼Œåœ¨éç›®æ ‡èƒŒæ™¯åŒºåŸŸçš„å›¾åƒä¸Šè¿›è¡Œäº†å¤§é‡ç‰¹å¾æå–è®¡ç®—ï¼Œé€ æˆäº†æµªè´¹ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œå°è¯•ä½¿ç”¨é¢å¤–çš„ç½‘ç»œæ¥æŒ‘é€‰å‡ºåŒ…å«ç›®æ ‡çš„åŒºåŸŸï¼Œå¹¶è¿›è¡Œä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹ï¼Œä½†æ–°å¼•å…¥çš„è®¡ç®—é™åˆ¶äº†å…¶æœ€ç»ˆæ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºé‡ç”¨æ£€æµ‹å™¨çš„ä¸»å¹²ç½‘ç»œè¿›è¡Œç‰¹å¾çº§çš„ç›®æ ‡æœç´¢å’Œè¡¥ä¸åˆ‡ç‰‡ï¼Œè¿™å¯ä»¥é¿å…å†—ä½™çš„ç‰¹å¾æå–ï¼Œå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚é€šè¿‡ç»“åˆç¨€ç–æ£€æµ‹å¤´ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ï¼ˆä¾‹å¦‚1080Pæˆ–æ›´å¤§ï¼‰ä¸Šæ£€æµ‹å°ç›®æ ‡ï¼Œä»¥å®ç°å“è¶Šçš„æ€§èƒ½ã€‚æ‰€å¾—çš„Efficient Small Object Detectionï¼ˆESODï¼‰æ–¹æ³•æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯åº”ç”¨äºåŸºäºCNNå’ŒViTçš„æ£€æµ‹å™¨ï¼Œä»¥èŠ‚çœè®¡ç®—å’ŒGPUå†…å­˜æˆæœ¬ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»£è¡¨æ€§çš„VisDroneã€UAVDTå’ŒTinyPersonæ•°æ®é›†ä¸Šå§‹ç»ˆå¤§å¹…åº¦è¶…è¶Šæœ€æ–°æ£€æµ‹å™¨ï¼ˆä¾‹å¦‚åœ¨APä¸Šè·å¾—8%çš„å¢ç›Šï¼‰ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/alibaba/esod">https://github.com/alibaba/esod</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16424v2">PDF</a> This paper has been recerived by IEEE TIP 2024. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/alibaba/esod">https://github.com/alibaba/esod</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ£€æµ‹å°å¯¹è±¡çš„æ–¹æ³•ï¼Œé€šè¿‡é‡ç”¨æ£€æµ‹å™¨çš„ä¸»å¹²ç½‘ç»œè¿›è¡Œç‰¹å¾å±‚é¢çš„ç›®æ ‡æœç´¢å’Œè¡¥ä¸åˆ‡ç‰‡ï¼Œé¿å…äº†å†—ä½™çš„ç‰¹å¾æå–ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç¨€ç–æ£€æµ‹å¤´ï¼Œèƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸Šæ£€æµ‹å°ç›®æ ‡ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚æå‡ºçš„Efficient Small Object Detectionï¼ˆESODï¼‰æ–¹æ³•æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯åº”ç”¨äºCNNå’ŒViTåŸºæ£€æµ‹å™¨ï¼ŒèŠ‚çœè®¡ç®—å’ŒGPUå†…å­˜æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»£è¡¨æ€§æ•°æ®é›†ä¸Šè¶…è¿‡äº†æœ€å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>ç®€å•å›¾åƒæ”¾å¤§å¯¹å°ç›®æ ‡æ£€æµ‹å¹¶ä¸é«˜æ•ˆï¼Œå­˜åœ¨è®¡ç®—å’ŒGPUå†…å­˜æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>å°ç›®æ ‡åœ¨å›¾åƒä¸­é€šå¸¸ç¨€ç–åˆ†å¸ƒä¸”å±€éƒ¨èšé›†ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡é¢å¤–çš„ç½‘ç»œæŒ‘é€‰å‡ºåŒ…å«ç›®æ ‡çš„åŒºåŸŸè¿›è¡Œå¸¸è§„ç›®æ ‡æ£€æµ‹ï¼Œä½†æ–°å¼•å…¥çš„è®¡ç®—é™åˆ¶äº†å…¶æœ€ç»ˆæ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„æ£€æµ‹æ–¹æ³•â€”â€”Efficient Small Object Detectionï¼ˆESODï¼‰ã€‚è¯¥æ–¹æ³•é‡ç”¨æ£€æµ‹å™¨çš„ä¸»å¹²ç½‘ç»œè¿›è¡Œç‰¹å¾å±‚é¢çš„ç›®æ ‡æœç´¢å’Œè¡¥ä¸åˆ‡ç‰‡ï¼Œé¿å…å†—ä½™ç‰¹å¾æå–å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>ESODæ–¹æ³•ç»“åˆäº†ç¨€ç–æ£€æµ‹å¤´ï¼Œèƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸Šæ£€æµ‹å°ç›®æ ‡ï¼Œå®ç°ä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>ESODæ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œé€‚ç”¨äºCNNå’ŒViTåŸºæ£€æµ‹å™¨ï¼Œèƒ½æœ‰æ•ˆèŠ‚çœè®¡ç®—å’ŒGPUå†…å­˜æˆæœ¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a60b0ed01ade625070c9fbb5ca055696.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-908cfa3c8ea1627021acccf93083ae4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f18cabe98c00361a831a740bf95fe599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8186842f6822de735cd898a182db38f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3b5163b34289362ef664ddbe47b8969.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-599a605ca6f67107234a2245b9185bd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14a846a868c7bef42a48d51aa53ccb18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db9aeac240970425782a8a29a641ec72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54c3ffda3a833c90d4323d0c084dc698.jpg" align="middle">
</details>


<h1 id="-22"><a href="#-22" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-086aa25bf58de799c30818fe145912f2.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  GLCF A Global-Local Multimodal Coherence Analysis Framework for Talking   Face Generation Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0bf82c8a9b7c59eb8562c67e28067af8.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  FastVLM Efficient Vision Encoding for Vision Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14643.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
