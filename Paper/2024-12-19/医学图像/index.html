<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Parameter-efficient Fine-tuning for improved Convolutional Baseline for   Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-62816a2ba15fa317d81e7a9f8172f741.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    26.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    107 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-19-æ›´æ–°"><a href="#2024-12-19-æ›´æ–°" class="headerlink" title="2024-12-19 æ›´æ–°"></a>2024-12-19 æ›´æ–°</h1><h2 id="Parameter-efficient-Fine-tuning-for-improved-Convolutional-Baseline-for-Brain-Tumor-Segmentation-in-Sub-Saharan-Africa-Adult-Glioma-Dataset"><a href="#Parameter-efficient-Fine-tuning-for-improved-Convolutional-Baseline-for-Brain-Tumor-Segmentation-in-Sub-Saharan-Africa-Adult-Glioma-Dataset" class="headerlink" title="Parameter-efficient Fine-tuning for improved Convolutional Baseline for   Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset"></a>Parameter-efficient Fine-tuning for improved Convolutional Baseline for   Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset</h2><p><strong>Authors:Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju, Laxmi Kanta Poudel, Confidence Raymond, Dong Zhang, Udunna C Anazodo, Bishesh Khanal, Mahesh Shakya</strong></p>
<p>Automating brain tumor segmentation using deep learning methods is an ongoing challenge in medical imaging. Multiple lingering issues exist including domain-shift and applications in low-resource settings which brings a unique set of challenges including scarcity of data. As a step towards solving these specific problems, we propose Convolutional adapter-inspired Parameter-efficient Fine-tuning (PEFT) of MedNeXt architecture. To validate our idea, we show our method performs comparable to full fine-tuning with the added benefit of reduced training compute using BraTS-2021 as pre-training dataset and BraTS-Africa as the fine-tuning dataset. BraTS-Africa consists of a small dataset (60 train &#x2F; 35 validation) from the Sub-Saharan African population with marked shift in the MRI quality compared to BraTS-2021 (1251 train samples). We first show that models trained on BraTS-2021 dataset do not generalize well to BraTS-Africa as shown by 20% reduction in mean dice on BraTS-Africa validation samples. Then, we show that PEFT can leverage both the BraTS-2021 and BraTS-Africa dataset to obtain mean dice of 0.8 compared to 0.72 when trained only on BraTS-Africa. Finally, We show that PEFT (0.80 mean dice) results in comparable performance to full fine-tuning (0.77 mean dice) which may show PEFT to be better on average but the boxplots show that full finetuning results is much lesser variance in performance. Nevertheless, on disaggregation of the dice metrics, we find that the model has tendency to oversegment as shown by high specificity (0.99) compared to relatively low sensitivity(0.75). The source code is available at <a target="_blank" rel="noopener" href="https://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt">https://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt</a> </p>
<blockquote>
<p>ä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œè„‘è‚¿ç˜¤åˆ†å‰²è‡ªåŠ¨åŒ–æ˜¯åŒ»å­¦å½±åƒé¢†åŸŸçš„ä¸€ä¸ªæŒç»­æŒ‘æˆ˜ã€‚å­˜åœ¨è®¸å¤šæ‚¬è€Œæœªå†³çš„é—®é¢˜ï¼ŒåŒ…æ‹¬é¢†åŸŸè¿ç§»å’Œåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œè¿™å¸¦æ¥äº†ä¸€ç³»åˆ—ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå…¶ä¸­åŒ…æ‹¬æ•°æ®ç¨€ç¼ºã€‚ä½œä¸ºè§£å†³è¿™äº›ç‰¹å®šé—®é¢˜çš„ä¸€æ­¥ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå·ç§¯é€‚é…å™¨å¯å‘çš„MedNeXtæ¶æ„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æƒ³æ³•ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ä¸å…¨å¾®è°ƒç›¸æ¯”å…·æœ‰å‡å°‘è®­ç»ƒè®¡ç®—é‡çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ä½¿ç”¨BraTS-2021ä½œä¸ºé¢„è®­ç»ƒæ•°æ®é›†ï¼ŒBraTS-Africaä½œä¸ºå¾®è°ƒæ•°æ®é›†ã€‚BraTS-Africaç”±æ¥è‡ªæ’’å“ˆæ‹‰ä»¥å—éæ´²äººç¾¤çš„å°æ•°æ®é›†ï¼ˆ60ä¸ªè®­ç»ƒæ ·æœ¬&#x2F; 35ä¸ªéªŒè¯æ ·æœ¬ï¼‰ç»„æˆï¼Œä¸BraTS-2021ç›¸æ¯”ï¼ˆ1251ä¸ªè®­ç»ƒæ ·æœ¬ï¼‰ï¼Œå…¶MRIè´¨é‡æœ‰æ˜æ˜¾å·®å¼‚ã€‚æˆ‘ä»¬é¦–å…ˆè¡¨æ˜ï¼Œåœ¨BraTS-2021æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨BraTS-Africaä¸Šè¡¨ç°ä¸ä½³ï¼Œåœ¨BraTS-AfricaéªŒè¯æ ·æœ¬ä¸Šçš„å¹³å‡Diceå¾—åˆ†é™ä½äº†20%ã€‚ç„¶åï¼Œæˆ‘ä»¬æ˜¾ç¤ºPEFTå¯ä»¥åˆ©ç”¨BraTS-2021å’ŒBraTS-Africaæ•°æ®é›†è·å¾—å¹³å‡Diceå¾—åˆ†0.8ï¼ˆä»…åœ¨BraTS-Africaä¸Šè®­ç»ƒæ—¶ä¸º0.72ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜PEFTï¼ˆå¹³å‡Diceå¾—åˆ†0.80ï¼‰çš„æ€§èƒ½ä¸å…¨å¾®è°ƒï¼ˆå¹³å‡Diceå¾—åˆ†0.77ï¼‰ç›¸å½“ï¼Œè¿™å¯èƒ½è¡¨æ˜PEFTåœ¨å¹³å‡æ€§èƒ½ä¸Šæ›´å¥½ï¼Œä½†ç®±çº¿å›¾æ˜¾ç¤ºå…¨å¾®è°ƒçš„ç»“æœæ€§èƒ½æ³¢åŠ¨æ›´å°ã€‚ç„¶è€Œï¼Œåœ¨å¯¹DiceæŒ‡æ ‡è¿›è¡Œç»†åˆ†åï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹å€¾å‘äºè¿‡åº¦åˆ†å‰²ï¼Œè¡¨ç°ä¸ºç‰¹å¼‚æ€§è¾ƒé«˜ï¼ˆ0.99ï¼‰ï¼Œè€Œæ•æ„Ÿæ€§ç›¸å¯¹è¾ƒä½ï¼ˆ0.75ï¼‰ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXtæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14100v1">PDF</a> Accepted to â€œThe International Brain Tumor Segmentation (BraTS)   challenge organized at MICCAI 2024 conferenceâ€</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ è‡ªåŠ¨åŒ–è„‘è‚¿ç˜¤åˆ†å‰²çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºç¯å¢ƒä¸­çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå·ç§¯é€‚é…å™¨å¯å‘çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„MedNeXtæ¶æ„ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨BraTS-Africaæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸å…¨å¾®è°ƒç›¸å½“ï¼Œå¹¶å‡å°‘äº†è®­ç»ƒè®¡ç®—é‡ã€‚PEFTå¯åˆ©ç”¨BraTS-2021å’ŒBraTS-Africaæ•°æ®é›†è·å¾—å¹³å‡diceç³»æ•°0.8ï¼Œè€Œä»…åœ¨BraTS-Africaæ•°æ®é›†ä¸Šè®­ç»ƒæ—¶åˆ™ä¸º0.72ã€‚è™½ç„¶PEFTçš„å¹³å‡æ€§èƒ½ä¸å…¨å¾®è°ƒç›¸å½“ï¼Œä½†å…¨å¾®è°ƒçš„ç»“æœè¡¨ç°å‡ºè¾ƒå°çš„æ€§èƒ½å·®å¼‚å˜åŒ–ã€‚æ¨¡å‹çš„æ€§èƒ½åå‘è¿‡åº¦åˆ†å‰²ï¼Œç‰¹å¼‚æ€§è¾ƒé«˜ï¼ˆ0.99ï¼‰ï¼Œè€Œæ•æ„Ÿæ€§è¾ƒä½ï¼ˆ0.75ï¼‰ã€‚æºä»£ç å·²å…¬å¼€åœ¨ç›¸å…³GitHubé“¾æ¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é’ˆå¯¹ä½èµ„æºç¯å¢ƒä¸‹çš„æ·±åº¦å­¦ä¹ è„‘è‚¿ç˜¤åˆ†å‰²å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®ç¨€ç¼ºå’Œé¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå·ç§¯é€‚é…å™¨å¯å‘çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰çš„MedNeXtæ¶æ„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>PEFTåœ¨BraTS-Africaæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸å…¨å¾®è°ƒç›¸å½“ï¼Œå¹¶é™ä½äº†è®­ç»ƒè®¡ç®—æˆæœ¬ã€‚</li>
<li>PEFTèƒ½å¤Ÿåˆ©ç”¨BraTS-2021å’ŒBraTS-Africaæ•°æ®é›†æ¥æé«˜æ€§èƒ½ï¼Œè·å¾—å¹³å‡diceç³»æ•°0.8ã€‚</li>
<li>PEFTä¸å…¨å¾®è°ƒçš„æ€§èƒ½å·®å¼‚è¾ƒå°ï¼Œä½†å…¨å¾®è°ƒè¡¨ç°å‡ºæ›´ç¨³å®šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>æ¨¡å‹æœ‰è¿‡åº¦åˆ†å‰²çš„å€¾å‘ï¼Œè¡¨ç°å‡ºé«˜ç‰¹å¼‚æ€§å’Œè¾ƒä½çš„æ•æ„Ÿæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0499e0d172d25b8564ad20ec07c6e710.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8de4e64a7e5ac9a5a6d84ea69b4ea1ac.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CAD-Recode-Reverse-Engineering-CAD-Code-from-Point-Clouds"><a href="#CAD-Recode-Reverse-Engineering-CAD-Code-from-Point-Clouds" class="headerlink" title="CAD-Recode: Reverse Engineering CAD Code from Point Clouds"></a>CAD-Recode: Reverse Engineering CAD Code from Point Clouds</h2><p><strong>Authors:Danila Rukhovich, Elona Dupont, Dimitrios Mallis, Kseniya Cherenkova, Anis Kacem, Djamila Aouada</strong></p>
<p>Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and dataset. In particular, we represent CAD sketch-extrude sequences as Python code. The proposed CAD-Recode translates a point cloud into Python code that, when executed, reconstructs the CAD model. Taking advantage of the exposure of pre-trained Large Language Models (LLMs) to Python code, we leverage a relatively small LLM as a decoder for CAD-Recode and combine it with a lightweight point cloud projector. CAD-Recode is trained solely on a proposed synthetic dataset of one million diverse CAD sequences. CAD-Recode significantly outperforms existing methods across three datasets while requiring fewer input points. Notably, it achieves 10 times lower mean Chamfer distance than state-of-the-art methods on DeepCAD and Fusion360 datasets. Furthermore, we show that our CAD Python code output is interpretable by off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering from point clouds. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡ä¾æ¬¡ç»˜åˆ¶å‚æ•°è‰å›¾å¹¶åº”ç”¨CADæ“ä½œæ¥è·å¾—çš„ä¸‰ç»´æ¨¡å‹ã€‚3D CADé€†å‘å·¥ç¨‹çš„é—®é¢˜åœ¨äºä»ç‚¹äº‘ç­‰3Dè¡¨ç¤ºé‡å»ºè‰å›¾å’ŒCADæ“ä½œåºåˆ—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªå±‚æ¬¡çš„å…¨æ–°è´¡çŒ®æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼šCADåºåˆ—è¡¨ç¤ºã€ç½‘ç»œè®¾è®¡å’Œæ•°æ®é›†ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†CADè‰å›¾æŒ¤å‹åºåˆ—è¡¨ç¤ºä¸ºPythonä»£ç ã€‚æ‰€æå‡ºçš„CAD-Recodeå°†ç‚¹äº‘è½¬æ¢ä¸ºPythonä»£ç ï¼Œæ‰§è¡Œæ—¶å¯é‡å»ºCADæ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„ å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹Pythonä»£ç çš„æš´éœ²ï¼Œåˆ©ç”¨ç›¸å¯¹è¾ƒå°çš„LLMä½œä¸ºCAD-Recodeçš„è§£ç å™¨ï¼Œå¹¶å°†å…¶ä¸è½»é‡çº§çš„ç‚¹äº‘æŠ•å½±ä»ªç›¸ç»“åˆã€‚CAD-Recodeä»…åœ¨ä¸€ç™¾ä¸‡ä¸ªå¤šæ ·åŒ–çš„CADåºåˆ—æ‰€æ„æˆæ‹Ÿåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸­ï¼ŒCAD-Recodeæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶éœ€è¦çš„è¾“å…¥ç‚¹æ•°æ›´å°‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šå®ç°äº†æ¯”æœ€æ–°æŠ€æœ¯ä½10å€çš„å¹³å‡Chamferè·ç¦»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„CAD Pythonä»£ç è¾“å‡ºèƒ½è¢«å¸‚é¢ä¸Šçš„LLMæ‰€è§£é‡Šï¼Œä»è€Œèƒ½å¤Ÿæœ‰ç‚¹äº‘è¿›è¡ŒCADç¼–è¾‘å’Œç‰¹å®šçš„é—®ç­”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14042v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†è®¡ç®—æœºä¸‰ç»´CADæ¨¡å‹çš„é€†å‘å·¥ç¨‹é—®é¢˜ï¼Œé€šè¿‡åˆ›æ–°çš„CADåºåˆ—è¡¨ç¤ºæ–¹æ³•ã€ç½‘ç»œè®¾è®¡å’Œæ•°æ®é›†ï¼Œå®ç°ä»ç‚¹äº‘é‡å»ºCADæ¨¡å‹ã€‚æå‡ºCAD-Recodeæ–¹æ³•ï¼Œå°†ç‚¹äº‘è½¬åŒ–ä¸ºPythonä»£ç ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„LLMè§£ç é‡å»ºCADæ¨¡å‹ã€‚åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°ä½è¯¯å·®ã€é«˜æ•ˆç‡çš„CADæ¨¡å‹é‡å»ºã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„CAD Pythonä»£ç æ˜“äºè§£è¯»å’Œç¼–è¾‘ï¼Œå¢å¼ºäº†CADæ¨¡å‹çš„äº¤äº’æ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CADæ¨¡å‹é€†å‘å·¥ç¨‹æ—¨åœ¨ä»ä¸‰ç»´è¡¨ç¤ºï¼ˆå¦‚ç‚¹äº‘ï¼‰é‡å»ºè‰å›¾åŠCADæ“ä½œåºåˆ—ã€‚</li>
<li>CAD-Recodeæ–¹æ³•å°†ç‚¹äº‘è½¬åŒ–ä¸ºPythonä»£ç ï¼Œé‡ç°CADæ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„LLMä½œä¸ºè§£ç å™¨ä¸è½»é‡çº§ç‚¹äº‘æŠ•å½±ä»ªç»“åˆã€‚</li>
<li>åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ¶µç›–ä¸€ç™¾ä¸‡ä¸ªä¸åŒçš„CADåºåˆ—ã€‚</li>
<li>åœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°æ›´ä½çš„Chamferè·ç¦»ï¼ˆå¹³å‡é™ä½äº†åå€ï¼‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e25d7eb0cc8854f180ad0774b6b068b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22b5fe8fec241094bc912f8d9be7a1de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25b7780bc0f0fc4b40cf19519278cc37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b82201bed22ed54cd867c9af5861d923.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e03a5ce6205dac33a10bf58a58af4c83.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Memorizing-SAM-3D-Medical-Segment-Anything-Model-with-Memorizing-Transformer"><a href="#Memorizing-SAM-3D-Medical-Segment-Anything-Model-with-Memorizing-Transformer" class="headerlink" title="Memorizing SAM: 3D Medical Segment Anything Model with Memorizing   Transformer"></a>Memorizing SAM: 3D Medical Segment Anything Model with Memorizing   Transformer</h2><p><strong>Authors:Xinyuan Shao, Yiqing Shen, Mathias Unberath</strong></p>
<p>Segment Anything Models (SAMs) have gained increasing attention in medical image analysis due to their zero-shot generalization capability in segmenting objects of unseen classes and domains when provided with appropriate user prompts. Addressing this performance gap is important to fully leverage the pre-trained weights of SAMs, particularly in the domain of volumetric medical image segmentation, where accuracy is important but well-annotated 3D medical data for fine-tuning is limited. In this work, we investigate whether introducing the memory mechanism as a plug-in, specifically the ability to memorize and recall internal representations of past inputs, can improve the performance of SAM with limited computation cost. To this end, we propose Memorizing SAM, a novel 3D SAM architecture incorporating a memory Transformer as a plug-in. Unlike conventional memorizing Transformers that save the internal representation during training or inference, our Memorizing SAM utilizes existing highly accurate internal representation as the memory source to ensure the quality of memory. We evaluate the performance of Memorizing SAM in 33 categories from the TotalSegmentator dataset, which indicates that Memorizing SAM can outperform state-of-the-art 3D SAM variant i.e., FastSAM3D with an average Dice increase of 11.36% at the cost of only 4.38 millisecond increase in inference time. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/swedfr/memorizingSAM">https://github.com/swedfr/memorizingSAM</a> </p>
<blockquote>
<p>åˆ†æ®µä»»ä½•äº‹æƒ…æ¨¡å‹ï¼ˆSAMï¼‰å› å…¶é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è€Œå—åˆ°åŒ»å­¦å›¾åƒåˆ†æçš„å¹¿æ³›å…³æ³¨ã€‚å½“æä¾›é€‚å½“çš„ç”¨æˆ·æç¤ºæ—¶ï¼Œå®ƒä»¬èƒ½å¤Ÿåˆ†å‰²æœªè§ç±»åˆ«å’Œé¢†åŸŸçš„å¯¹è±¡ã€‚ç¼©å°è¿™ä¸€æ€§èƒ½å·®è·å¯¹äºå……åˆ†åˆ©ç”¨SAMçš„é¢„è®­ç»ƒæƒé‡è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œå‡†ç¡®æ€§è‡³å…³é‡è¦ï¼Œä½†ç”¨äºå¾®è°ƒçš„é«˜ç²¾åº¦ä¸‰ç»´åŒ»å­¦æ•°æ®æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å¼•å…¥è®°å¿†æœºåˆ¶ä½œä¸ºæ’ä»¶æ˜¯å¦å¯ä»¥æ”¹å–„SAMçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯è®°å¿†å’Œå›å¿†è¿‡å»è¾“å…¥çš„å†…éƒ¨è¡¨ç¤ºçš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è®°å¿†SAMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸‰ç»´SAMæ¶æ„ï¼Œå®ƒç»“åˆäº†è®°å¿†Transformerä½œä¸ºæ’ä»¶ã€‚ä¸ä¼ ç»Ÿçš„ä¿å­˜è®­ç»ƒæˆ–æ¨ç†è¿‡ç¨‹ä¸­å†…éƒ¨è¡¨ç¤ºçš„è®°å¿†Transformerä¸åŒï¼Œæˆ‘ä»¬çš„è®°å¿†SAMåˆ©ç”¨ç°æœ‰çš„é«˜ç²¾åº¦å†…éƒ¨è¡¨ç¤ºä½œä¸ºå†…å­˜æºï¼Œä»¥ç¡®ä¿å†…å­˜çš„è´¨é‡ã€‚æˆ‘ä»¬åœ¨TotalSegmentatoræ•°æ®é›†çš„33ä¸ªç±»åˆ«ä¸­è¯„ä¼°äº†è®°å¿†SAMçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜ï¼Œè®°å¿†SAMå¯ä»¥è¶…è¶Šæœ€æ–°çš„ä¸‰ç»´SAMå˜ä½“ï¼ˆå³FastSAM3Dï¼‰ï¼Œå¹³å‡Diceç³»æ•°æé«˜äº†11.36%ï¼Œè€Œæ¨ç†æ—¶é—´ä»…å¢åŠ äº†4.38æ¯«ç§’ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/swedfr/memorizingSAM%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/swedfr/memorizingSAMå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13908v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œé€šè¿‡å¼•å…¥è®°å¿†æœºåˆ¶ä½œä¸ºæ’ä»¶æ¥æ”¹è¿›Segment Anything Modelsï¼ˆSAMsï¼‰çš„æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†Memorizing SAMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„3D SAMæ¶æ„ï¼Œå®ƒç»“åˆäº†è®°å¿†Transformerä½œä¸ºæ’ä»¶ã€‚è¯¥æ¶æ„åˆ©ç”¨ç°æœ‰çš„é«˜è´¨é‡å†…éƒ¨è¡¨ç¤ºä½œä¸ºè®°å¿†æºï¼Œç¡®ä¿è®°å¿†çš„è´¨é‡ã€‚åœ¨TotalSegmentatoræ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMemorizing SAMåœ¨å¹³å‡Diceç³»æ•°ä¸Šä¼˜äºFastSAM3Dç­‰å…ˆè¿›æ¨¡å‹ï¼Œä¸”æ¨ç†æ—¶é—´ä»…å¢åŠ æ¯«ç§’çº§åˆ«ã€‚ä»£ç å·²å…¬å¼€åˆ†äº«åœ¨ç›¸å…³ç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAMså› å…¶é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è€Œå—åˆ°åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„å…³æ³¨ã€‚é’ˆå¯¹æ­¤ç±»æ¨¡å‹å¼•å…¥è®°å¿†æœºåˆ¶ä»¥æé«˜æ€§èƒ½çš„é—®é¢˜è¿›è¡Œæ¢ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹3D SAMæ¶æ„â€”â€”Memorizing SAMï¼Œç»“åˆäº†è®°å¿†Transformeræ’ä»¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1896c3d0de0cabfb0c61105fac41b0ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c63b9ea3ddeea7732293405c8481a54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e8a6aad616fff6ba300df47f4c78fee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fae1db67e69c4adfd7b137f6917cc8a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Navigating-limitations-with-precision-A-fine-grained-ensemble-approach-to-wrist-pathology-recognition-on-a-limited-x-ray-dataset"><a href="#Navigating-limitations-with-precision-A-fine-grained-ensemble-approach-to-wrist-pathology-recognition-on-a-limited-x-ray-dataset" class="headerlink" title="Navigating limitations with precision: A fine-grained ensemble approach   to wrist pathology recognition on a limited x-ray dataset"></a>Navigating limitations with precision: A fine-grained ensemble approach   to wrist pathology recognition on a limited x-ray dataset</h2><p><strong>Authors:Ammar Ahmed, Ali Shariq Imran, Mohib Ullah, Zenun Kastrati, Sher Muhammad Daudpota</strong></p>
<p>The exploration of automated wrist fracture recognition has gained considerable research attention in recent years. In practical medical scenarios, physicians and surgeons may lack the specialized expertise required for accurate X-ray interpretation, highlighting the need for machine vision to enhance diagnostic accuracy. However, conventional recognition techniques face challenges in discerning subtle differences in X-rays when classifying wrist pathologies, as many of these pathologies, such as fractures, can be small and hard to distinguish. This study tackles wrist pathology recognition as a fine-grained visual recognition (FGVR) problem, utilizing a limited, custom-curated dataset that mirrors real-world medical constraints, relying solely on image-level annotations. We introduce a specialized FGVR-based ensemble approach to identify discriminative regions within X-rays. We employ an Explainable AI (XAI) technique called Grad-CAM to pinpoint these regions. Our ensemble approach outperformed many conventional SOTA and FGVR techniques, underscoring the effectiveness of our strategy in enhancing accuracy in wrist pathology recognition. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè‡ªåŠ¨è…•éƒ¨éª¨æŠ˜è¯†åˆ«çš„ç ”ç©¶å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚åœ¨å®é™…åŒ»ç–—åœºæ™¯ä¸­ï¼ŒåŒ»ç”Ÿå’Œå¤–ç§‘åŒ»ç”Ÿå¯èƒ½ç¼ºä¹å‡†ç¡®è§£è¯»Xå…‰ç‰‡æ‰€éœ€çš„ä¸“ä¸šçŸ¥è¯†ï¼Œè¿™çªæ˜¾äº†éœ€è¦å€ŸåŠ©æœºå™¨è§†è§‰æ¥æé«˜è¯Šæ–­å‡†ç¡®æ€§çš„å¿…è¦æ€§ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è¯†åˆ«æŠ€æœ¯åœ¨è¾¨è¯†Xå…‰ç‰‡ä¸­ç»†å¾®å·®å¼‚ä»¥åˆ†ç±»è…•éƒ¨ç–¾ç—…æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›ç–¾ç—…ï¼ˆå¦‚éª¨æŠ˜ï¼‰å¯èƒ½å¾ˆå°ä¸”éš¾ä»¥åŒºåˆ†ã€‚æœ¬ç ”ç©¶å°†è…•éƒ¨ç–¾ç—…è¯†åˆ«è§†ä¸ºä¸€é¡¹ç²¾ç»†çš„è§†è§‰è¯†åˆ«ï¼ˆFGVRï¼‰é—®é¢˜ï¼Œåˆ©ç”¨ä¸€ä¸ªåæ˜ ç°å®åŒ»ç–—çº¦æŸçš„å®šåˆ¶æœ‰é™æ•°æ®é›†ï¼Œä»…ä¾èµ–å›¾åƒçº§åˆ«çš„æ³¨é‡Šã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºFGVRçš„é›†æˆæ–¹æ³•ï¼Œç”¨äºè¯†åˆ«Xå…‰ç‰‡ä¸­çš„é‰´åˆ«åŒºåŸŸã€‚æˆ‘ä»¬é‡‡ç”¨åä¸ºGrad-CAMçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯æ¥å®šä½è¿™äº›åŒºåŸŸã€‚æˆ‘ä»¬çš„é›†æˆæ–¹æ³•åœ¨è®¸å¤šä¼ ç»Ÿçš„SOTAå’ŒFGVRæŠ€æœ¯ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¿™çªæ˜¾äº†æˆ‘ä»¬çš„ç­–ç•¥åœ¨æé«˜è…•éƒ¨ç–¾ç—…è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13884v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è‡ªåŠ¨åŒ–æ‰‹è…•éª¨æŠ˜è¯†åˆ«æŠ€æœ¯çš„é‡è¦æ€§åŠå…¶åœ¨å®é™…åŒ»ç–—åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ç”±äºåŒ»ç”Ÿå¯èƒ½åœ¨Xå…‰è§£è¯»æ–¹é¢ç¼ºä¹ä¸“ä¸šçŸ¥è¯†ï¼Œå› æ­¤éœ€è¦å€ŸåŠ©æœºå™¨è§†è§‰æŠ€æœ¯æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚æ–‡ç« é‡‡ç”¨äº†ä¸€ç§ç²¾ç»†ç²’åº¦è§†è§‰è¯†åˆ«ï¼ˆFGVRï¼‰çš„æ–¹æ³•ï¼Œè§£å†³æ‰‹è…•ç—…ç†è¯†åˆ«çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨é™å®šä¸”åæ˜ çœŸå®ä¸–ç•ŒåŒ»ç–—é™åˆ¶çš„å®šåˆ¶æ•°æ®é›†ï¼Œä»…ä¾èµ–å›¾åƒçº§åˆ«çš„æ³¨é‡Šã€‚é€šè¿‡é‡‡ç”¨Grad-CAMçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿè¯†åˆ«å‡ºXå…‰ç‰‡ä¸­çš„é‰´åˆ«åŒºåŸŸã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥é›†æˆæ–¹æ³•ä¼˜äºè®¸å¤šä¼ ç»Ÿçš„SOTAå’ŒFGVRæŠ€æœ¯ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜æ‰‹è…•ç—…ç†è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–æ‰‹è…•éª¨æŠ˜è¯†åˆ«è¿‘å¹´æ¥å—åˆ°ç ”ç©¶å…³æ³¨ã€‚</li>
<li>åœ¨å®é™…åŒ»ç–—åœºæ™¯ä¸­ï¼ŒåŒ»ç”Ÿå¯èƒ½ç¼ºä¹å‡†ç¡®è§£è¯»Xå…‰çš„ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>æ‰‹è…•ç—…ç†è¯†åˆ«è¢«è§†ä¸ºä¸€ç§ç²¾ç»†ç²’åº¦è§†è§‰è¯†åˆ«ï¼ˆFGVRï¼‰é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å®šåˆ¶æ•°æ®é›†ï¼Œåæ˜ çœŸå®ä¸–ç•ŒåŒ»ç–—é™åˆ¶ï¼Œä»…ä¾èµ–å›¾åƒçº§åˆ«æ³¨é‡Šã€‚</li>
<li>ä½¿ç”¨Grad-CAMçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯æ¥è¯†åˆ«Xå…‰ç‰‡ä¸­çš„é‰´åˆ«åŒºåŸŸã€‚</li>
<li>é›†æˆæ–¹æ³•ä¼˜äºè®¸å¤šä¼ ç»Ÿå’ŒFGVRæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34fc61e9f8162a6c367661910f99babb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87e4c446f78d6299a2880cd16926c8ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9cdda8752ad0d8c7d313b8e85a8598f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20305da72a76d52e141815b3142e6faa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55e4bf5c89bfc3dbc7fd631f0ae6f2dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df7e711defb80def6e8bc158b2df264b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf7e5079fda99b1483c575b9ca8872f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66a85e4eb4c221d69443b299b8617822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3358c4115cbad4884993de3fdfba9ee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26e2494af66f6b9862d541a2ae0ebcaa.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Spatial-Brain-Tumor-Concentration-Estimation-for-Individualized-Radiotherapy-Planning"><a href="#Spatial-Brain-Tumor-Concentration-Estimation-for-Individualized-Radiotherapy-Planning" class="headerlink" title="Spatial Brain Tumor Concentration Estimation for Individualized   Radiotherapy Planning"></a>Spatial Brain Tumor Concentration Estimation for Individualized   Radiotherapy Planning</h2><p><strong>Authors:Jonas Weidner, Michal Balcerak, Ivan Ezhov, AndrÃ© Datchev, Laurin Lux, Lucas Zimmerand Daniel Rueckert, BjÃ¶rn Menze, Benedikt Wiestler</strong></p>
<p>Biophysical modeling of brain tumors has emerged as a promising strategy for personalizing radiotherapy planning by estimating the otherwise hidden distribution of tumor cells within the brain. However, many existing state-of-the-art methods are computationally intensive, limiting their widespread translation into clinical practice. In this work, we propose an efficient and direct method that utilizes soft physical constraints to estimate the tumor cell concentration from preoperative MRI of brain tumor patients. Our approach optimizes a 3D tumor concentration field by simultaneously minimizing the difference between the observed MRI and a physically informed loss function. Compared to existing state-of-the-art techniques, our method significantly improves predicting tumor recurrence on two public datasets with a total of 192 patients while maintaining a clinically viable runtime of under one minute - a substantial reduction from the 30 minutes required by the current best approach. Furthermore, we showcase the generalizability of our framework by incorporating additional imaging information and physical constraints, highlighting its potential to translate to various medical diffusion phenomena with imperfect data. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤çš„ç”Ÿç‰©ç‰©ç†å»ºæ¨¡å·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„ç­–ç•¥ï¼Œé€šè¿‡ä¼°è®¡è„‘å†…è‚¿ç˜¤ç»†èƒçš„éšè—åˆ†å¸ƒæ¥ä¸ªæ€§åŒ–æ”¾å°„æ²»ç–—è®¡åˆ’ã€‚ç„¶è€Œï¼Œè®¸å¤šç›®å‰æœ€å…ˆè¿›çš„æ–¹æ³•è®¡ç®—å¯†é›†ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠå®è·µä¸­çš„å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”ç›´æ¥çš„æ–¹æ³•ï¼Œåˆ©ç”¨è½¯ç‰©ç†çº¦æŸæ¥ä¼°è®¡è„‘è‚¿ç˜¤æ‚£è€…çš„æœ¯å‰MRIä¸­çš„è‚¿ç˜¤ç»†èƒæµ“åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŒæ—¶æœ€å°åŒ–è§‚å¯Ÿåˆ°çš„MRIä¸ç‰©ç†ä¿¡æ¯æŸå¤±å‡½æ•°ä¹‹é—´çš„å·®å¼‚æ¥ä¼˜åŒ–ä¸‰ç»´è‚¿ç˜¤æµ“åº¦åœºã€‚ä¸ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå¯¹192åæ‚£è€…çš„è‚¿ç˜¤å¤å‘é¢„æµ‹è¿›è¡Œäº†æ˜¾è‘—æ”¹å–„ï¼ŒåŒæ—¶ä¿æŒäº†ä¸€åˆ†é’Ÿçš„ä¸´åºŠå¯è¡Œè¿è¡Œæ—¶é—´â€”â€”ç›¸è¾ƒäºå½“å‰æœ€ä½³æ–¹æ³•çš„30åˆ†é’Ÿï¼Œå¤§å¤§ç¼©çŸ­äº†æ—¶é—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡èå…¥é¢å¤–çš„æˆåƒä¿¡æ¯å’Œç‰©ç†çº¦æŸå±•ç¤ºäº†æ¡†æ¶çš„æ³›åŒ–èƒ½åŠ›ï¼Œçªæ˜¾äº†å…¶åœ¨å¤„ç†å¸¦æœ‰ç¼ºé™·æ•°æ®çš„å„ç§åŒ»å­¦æ‰©æ•£ç°è±¡ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13811v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨è½¯ç‰©ç†çº¦æŸä»è„‘è‚¿ç˜¤æ‚£è€…çš„æœ¯å‰MRIä¼°è®¡è‚¿ç˜¤ç»†èƒæµ“åº¦çš„é«˜æ•ˆç›´æ¥æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–3Dè‚¿ç˜¤æµ“åº¦åœºï¼ŒåŒæ—¶æœ€å°åŒ–è§‚æµ‹MRIä¸ç‰©ç†ä¿¡æ¯æŸå¤±å‡½æ•°ä¹‹é—´çš„å·®å¼‚ï¼Œæ˜¾è‘—æé«˜äº†åœ¨å…¬å…±æ•°æ®é›†ä¸Šå¯¹192ä¾‹æ‚£è€…çš„è‚¿ç˜¤å¤å‘é¢„æµ‹èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†ä¸´åºŠå¯è¡Œçš„è¿è¡Œæ—¶é—´åœ¨ä¸€åˆ†é’Ÿä»¥å†…ï¼Œå¤§å¤§ä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•çš„30åˆ†é’Ÿã€‚æ­¤å¤–ï¼Œé€šè¿‡èå…¥é¢å¤–çš„æˆåƒä¿¡æ¯å’Œç‰©ç†çº¦æŸï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾å…¶åœ¨ä¸å®Œç¾æ•°æ®ä¸‹åº”ç”¨äºå¤šç§åŒ»å­¦æ‰©æ•£ç°è±¡çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿç‰©ç‰©ç†å»ºæ¨¡æ˜¯ä¸ªæ€§åŒ–æ”¾å°„æ²»ç–—è®¡åˆ’çš„ä¸€ç§æœ‰å‰é€”çš„ç­–ç•¥ï¼Œå¯ä¼°è®¡è„‘å†…éšè—çš„è‚¿ç˜¤ç»†èƒåˆ†å¸ƒã€‚</li>
<li>å½“å‰å…ˆè¿›æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œéš¾ä»¥å¹¿æ³›åº”ç”¨äºä¸´åºŠå®è·µã€‚</li>
<li>æå‡ºäº†ä¸€ç§é«˜æ•ˆç›´æ¥çš„æ–¹æ³•ï¼Œåˆ©ç”¨è½¯ç‰©ç†çº¦æŸå’Œæœ¯å‰MRIæ¥ä¼°è®¡è‚¿ç˜¤ç»†èƒæµ“åº¦ã€‚</li>
<li>æ–¹æ³•åœ¨å…¬å…±æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†è‚¿ç˜¤å¤å‘çš„é¢„æµ‹èƒ½åŠ›ï¼Œæ¶‰åŠ192ä¾‹æ‚£è€…ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æŠ€æœ¯ï¼Œæ–°æ–¹æ³•è¿è¡Œæ—¶é—´å¤§å¹…ç¼©çŸ­è‡³ä¸€åˆ†é’Ÿä»¥å†…ã€‚</li>
<li>æ¡†æ¶å¯èå…¥é¢å¤–æˆåƒä¿¡æ¯å’Œç‰©ç†çº¦æŸï¼Œå±•ç¤ºå…¶è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d2e317c2bec8b6cc279663129a7c5c39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15bbba5136b33a28b9ea8805e4434090.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learnable-Prompting-SAM-induced-Knowledge-Distillation-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Learnable-Prompting-SAM-induced-Knowledge-Distillation-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Learnable Prompting SAM-induced Knowledge Distillation for   Semi-supervised Medical Image Segmentation"></a>Learnable Prompting SAM-induced Knowledge Distillation for   Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Kaiwen Huang, Tao Zhou, Huazhu Fu, Yizhe Zhang, Yi Zhou, Chen Gong, Dong Liang</strong></p>
<p>The limited availability of labeled data has driven advancements in semi-supervised learning for medical image segmentation. Modern large-scale models tailored for general segmentation, such as the Segment Anything Model (SAM), have revealed robust generalization capabilities. However, applying these models directly to medical image segmentation still exposes performance degradation. In this paper, we propose a learnable prompting SAM-induced Knowledge distillation framework (KnowSAM) for semi-supervised medical image segmentation. Firstly, we propose a Multi-view Co-training (MC) strategy that employs two distinct sub-networks to employ a co-teaching paradigm, resulting in more robust outcomes. Secondly, we present a Learnable Prompt Strategy (LPS) to dynamically produce dense prompts and integrate an adapter to fine-tune SAM specifically for medical image segmentation tasks. Moreover, we propose SAM-induced Knowledge Distillation (SKD) to transfer useful knowledge from SAM to two sub-networks, enabling them to learn from SAMâ€™s predictions and alleviate the effects of incorrect pseudo-labels during training. Notably, the predictions generated by our subnets are used to produce mask prompts for SAM, facilitating effective inter-module information exchange. Extensive experimental results on various medical segmentation tasks demonstrate that our model outperforms the state-of-the-art semi-supervised segmentation approaches. Crucially, our SAM distillation framework can be seamlessly integrated into other semi-supervised segmentation methods to enhance performance. The code will be released upon acceptance of this manuscript at: <a target="_blank" rel="noopener" href="https://github.com/taozh2017/KnowSAM">https://github.com/taozh2017/KnowSAM</a> </p>
<blockquote>
<p>å—é™äºæ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§æ¨åŠ¨äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ çš„è¿›æ­¥ã€‚é’ˆå¯¹ä¸€èˆ¬åˆ†å‰²çš„ç°ä»£åŒ–å¤§è§„æ¨¡æ¨¡å‹ï¼Œå¦‚Segment Anything Modelï¼ˆSAMï¼‰ï¼Œå·²ç»æ˜¾ç¤ºå‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ç›´æ¥åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»ç„¶ä¼šå‡ºç°æ€§èƒ½ä¸‹é™çš„æƒ…å†µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¯å­¦ä¹ æç¤ºSAMè¯±å¯¼çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼ˆKnowSAMï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šè§†è§’ååŒè®­ç»ƒï¼ˆMCï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é‡‡ç”¨ä¸¤ä¸ªä¸åŒçš„å­ç½‘ç»œæ¥é‡‡ç”¨ååŒæ•™å­¦èŒƒå¼ï¼Œä»è€Œå¾—åˆ°æ›´ç¨³å¥çš„ç»“æœã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯å­¦ä¹ æç¤ºç­–ç•¥ï¼ˆLPSï¼‰ï¼Œä»¥åŠ¨æ€ç”Ÿæˆå¯†é›†æç¤ºå¹¶å°†é€‚é…å™¨é›†æˆï¼Œä»¥ä¾¿å¯¹SAMè¿›è¡Œå¾®è°ƒï¼Œä¸“é—¨ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†SAMè¯±å¯¼çŸ¥è¯†è’¸é¦ï¼ˆSKDï¼‰ï¼Œä»¥å°†SAMçš„æœ‰ç”¨çŸ¥è¯†è½¬ç§»åˆ°ä¸¤ä¸ªå­ç½‘ç»œä¸­ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿä»SAMçš„é¢„æµ‹ä¸­å­¦ä¹ å¹¶å‡è½»è®­ç»ƒè¿‡ç¨‹ä¸­é”™è¯¯ä¼ªæ ‡ç­¾çš„å½±å“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å­ç½‘äº§ç”Ÿçš„é¢„æµ‹ç”¨äºä¸ºSAMç”Ÿæˆæ©è†œæç¤ºï¼Œä¿ƒè¿›æ¨¡å—é—´çš„æœ‰æ•ˆä¿¡æ¯äº¤æ¢ã€‚åœ¨å„ç§åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºæœ€æ–°çš„åŠç›‘ç£åˆ†å‰²æ–¹æ³•ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬çš„SAMè’¸é¦æ¡†æ¶å¯ä»¥æ— ç¼é›†æˆåˆ°å…¶ä»–åŠç›‘ç£åˆ†å‰²æ–¹æ³•ä¸­ä»¥æé«˜æ€§èƒ½ã€‚è¯¥ä»£ç å°†åœ¨æœ¬æ‰‹ç¨¿è¢«æ¥å—åå‘å¸ƒåœ¨ï¼š[<a target="_blank" rel="noopener" href="https://github.com/taozh2017/KnowSAM]">https://github.com/taozh2017/KnowSAM]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13742v1">PDF</a> 12 pages, 7 figures</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸé¢ä¸´æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œä¿ƒè¿›äº†åŠç›‘ç£å­¦ä¹ çš„å‘å±•ã€‚ç°ä»£å¤§è§„æ¨¡é€šç”¨åˆ†å‰²æ¨¡å‹ï¼Œå¦‚SAMï¼ˆSegment Anything Modelï¼‰ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ç›´æ¥åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½ä¼šä¸‹é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºSAMè¯±å¯¼çŸ¥è¯†è’¸é¦çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶KnowSAMã€‚åŒ…æ‹¬å¤šè§†è§’ååŒè®­ç»ƒç­–ç•¥ã€å¯å­¦ä¹ æç¤ºç­–ç•¥ä»¥åŠSAMè¯±å¯¼çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å…ˆè¿›åŠç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œä¸”å¯æ— ç¼é›†æˆåˆ°å…¶ä»–åŠç›‘ç£åˆ†å‰²æ–¹æ³•ä¸­æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œä¿ƒè¿›äº†åŠç›‘ç£å­¦ä¹ æŠ€æœ¯çš„å‘å±•ã€‚</li>
<li>SAMç­‰å¤§è§„æ¨¡é€šç”¨åˆ†å‰²æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ç›´æ¥åº”ç”¨æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æå‡ºçš„KnowSAMæ¡†æ¶åŒ…å«å¤šè§†è§’ååŒè®­ç»ƒç­–ç•¥ï¼Œé‡‡ç”¨ä¸¤ä¸ªå­ç½‘ç»œè¿›è¡ŒååŒæ•™å­¦ï¼Œæé«˜ç»“æœç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥å¯å­¦ä¹ æç¤ºç­–ç•¥ï¼ŒåŠ¨æ€ç”Ÿæˆå¯†é›†æç¤ºå¹¶å¾®è°ƒSAMä»¥é€‚åº”åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>SAMè¯±å¯¼çŸ¥è¯†è’¸é¦æ–¹æ³•ç”¨äºä»SAMè½¬ç§»æœ‰ç”¨çŸ¥è¯†ç»™å­ç½‘ç»œï¼Œä½¿å®ƒä»¬ä»SAMçš„é¢„æµ‹ä¸­å­¦ä¹ å¹¶å‡è½»é”™è¯¯ä¼ªæ ‡ç­¾çš„å½±å“ã€‚</li>
<li>å­ç½‘çš„é¢„æµ‹ç”¨äºä¸ºSAMç”Ÿæˆæ©è†œæç¤ºï¼Œå®ç°æ¨¡å—é—´æœ‰æ•ˆä¿¡æ¯å…±äº«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d89d69ad2b7331750d780d64710ccb83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5db4df2c064b83ef7c87170756948e69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1250ff9fc16975c4003036e035e9e50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e0595371e1d75eb3c8a679064364f6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa31674475828ce31153da75f682b753.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Plug-and-Play-Tri-Branch-Invertible-Block-for-Image-Rescaling"><a href="#Plug-and-Play-Tri-Branch-Invertible-Block-for-Image-Rescaling" class="headerlink" title="Plug-and-Play Tri-Branch Invertible Block for Image Rescaling"></a>Plug-and-Play Tri-Branch Invertible Block for Image Rescaling</h2><p><strong>Authors:Jingwei Bao, Jinhua Hao, Pengcheng Xu, Ming Sun, Chao Zhou, Shuyuan Zhu</strong></p>
<p>High-resolution (HR) images are commonly downscaled to low-resolution (LR) to reduce bandwidth, followed by upscaling to restore their original details. Recent advancements in image rescaling algorithms have employed invertible neural networks (INNs) to create a unified framework for downscaling and upscaling, ensuring a one-to-one mapping between LR and HR images. Traditional methods, utilizing dual-branch based vanilla invertible blocks, process high-frequency and low-frequency information separately, often relying on specific distributions to model high-frequency components. However, processing the low-frequency component directly in the RGB domain introduces channel redundancy, limiting the efficiency of image reconstruction. To address these challenges, we propose a plug-and-play tri-branch invertible block (T-InvBlocks) that decomposes the low-frequency branch into luminance (Y) and chrominance (CbCr) components, reducing redundancy and enhancing feature processing. Additionally, we adopt an all-zero mapping strategy for high-frequency components during upscaling, focusing essential rescaling information within the LR image. Our T-InvBlocks can be seamlessly integrated into existing rescaling models, improving performance in both general rescaling tasks and scenarios involving lossy compression. Extensive experiments confirm that our method advances the state of the art in HR image reconstruction. </p>
<blockquote>
<p>é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å›¾åƒé€šå¸¸ä¼šè¢«é™å°ºåº¦å¤„ç†æˆä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰ä»¥å‡å°‘å¸¦å®½ï¼Œéšåå†å¯¹å…¶è¿›è¡Œä¸Šé‡‡æ ·ä»¥æ¢å¤å…¶åŸå§‹ç»†èŠ‚ã€‚è¿‘æœŸåœ¨å›¾åƒé‡ç¼©æ”¾ç®—æ³•æ–¹é¢çš„è¿›å±•é‡‡ç”¨äº†å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNsï¼‰æ¥åˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ç”¨äºé™å°ºåº¦å’Œä¸Šé‡‡æ ·ï¼Œç¡®ä¿LRå’ŒHRå›¾åƒä¹‹é—´çš„ä¸€ä¸€æ˜ å°„å…³ç³»ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä½¿ç”¨åŸºäºåŒåˆ†æ”¯çš„æ™®é€šå¯é€†å—ï¼Œåˆ†åˆ«å¤„ç†é«˜é¢‘å’Œä½é¢‘ä¿¡æ¯ï¼Œé€šå¸¸ä¾èµ–äºç‰¹å®šçš„åˆ†å¸ƒæ¥æ¨¡æ‹Ÿé«˜é¢‘æˆåˆ†ã€‚ç„¶è€Œï¼Œç›´æ¥åœ¨RGBåŸŸå¤„ç†ä½é¢‘æˆåˆ†ä¼šå¯¼è‡´é€šé“å†—ä½™ï¼Œé™åˆ¶äº†å›¾åƒé‡å»ºçš„æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„ä¸‰åˆ†æ”¯å¯é€†å—ï¼ˆT-InvBlocksï¼‰ï¼Œå®ƒå°†ä½é¢‘åˆ†æ”¯åˆ†è§£ä¸ºäº®åº¦ï¼ˆYï¼‰å’Œè‰²åº¦ï¼ˆCbCrï¼‰æˆåˆ†ï¼Œä»¥å‡å°‘å†—ä½™å¹¶å¢å¼ºç‰¹å¾å¤„ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­é‡‡ç”¨å…¨é›¶æ˜ å°„ç­–ç•¥æ¥å¤„ç†é«˜é¢‘æˆåˆ†ï¼Œä¸“æ³¨äºåœ¨LRå›¾åƒå†…éƒ¨çš„å…³é”®é‡ç¼©æ”¾ä¿¡æ¯ã€‚æˆ‘ä»¬çš„T-InvBlockså¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é‡ç¼©æ”¾æ¨¡å‹ä¸­ï¼Œåœ¨ä¸€èˆ¬çš„é‡ç¼©æ”¾ä»»åŠ¡å’Œæ¶‰åŠæœ‰æŸå‹ç¼©çš„åœºæ™¯ä¸­éƒ½èƒ½æé«˜æ€§èƒ½ã€‚å¤§é‡å®éªŒè¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨HRå›¾åƒé‡å»ºæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯çš„å‰æ²¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13508v1">PDF</a> Accepted by AAAI 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/Jingwei-Bao/T-InvBlocks">https://github.com/Jingwei-Bao/T-InvBlocks</a></p>
<p><strong>Summary</strong></p>
<p>é«˜åˆ†è¾¨ç‡å›¾åƒå¸¸é€šè¿‡é™åˆ†è¾¨ç‡ä»¥å‡å°‘å¸¦å®½ï¼Œå†é€šè¿‡å›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯æ¢å¤å…¶åŸå§‹ç»†èŠ‚ã€‚æœ€æ–°å›¾åƒç¼©æ”¾ç®—æ³•åˆ©ç”¨å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNsï¼‰ä¸ºé™åˆ†è¾¨ç‡å’Œå‡åˆ†è¾¨ç‡åˆ›å»ºç»Ÿä¸€æ¡†æ¶ã€‚ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨åŒåˆ†æ”¯åŸºç¡€å¯é€†å—ï¼Œåˆ†åˆ«å¤„ç†é«˜é¢‘å’Œä½é¢‘ä¿¡æ¯ï¼Œå¸¸ä¾èµ–ç‰¹å®šåˆ†å¸ƒæ¥æ¨¡æ‹Ÿé«˜é¢‘æˆåˆ†ã€‚ç›´æ¥åœ¨RGBåŸŸå¤„ç†ä½é¢‘æˆåˆ†å¼•å…¥é€šé“å†—ä½™ï¼Œé™åˆ¶äº†å›¾åƒé‡å»ºçš„æ•ˆç‡ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºå³æ’å³ç”¨çš„ä¸‰åˆ†æ”¯å¯é€†å—ï¼ˆT-InvBlocksï¼‰ï¼Œå°†ä½é¢‘åˆ†æ”¯åˆ†è§£ä¸ºäº®åº¦ï¼ˆYï¼‰å’Œè‰²åº¦ï¼ˆCbCrï¼‰æˆåˆ†ï¼Œå‡å°‘å†—ä½™å¹¶å¢å¼ºç‰¹å¾å¤„ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å‡åˆ†è¾¨ç‡æ—¶é‡‡ç”¨é«˜é¢‘æˆåˆ†å…¨é›¶æ˜ å°„ç­–ç•¥ï¼Œé›†ä¸­å…³é”®ç¼©æ”¾ä¿¡æ¯äºä½åˆ†è¾¨ç‡å›¾åƒå†…ã€‚T-InvBlockså¯æ— ç¼é›†æˆåˆ°ç°æœ‰ç¼©æ”¾æ¨¡å‹ä¸­ï¼Œæå‡é€šç”¨ç¼©æ”¾ä»»åŠ¡å’Œæ¶‰åŠæœ‰æŸå‹ç¼©åœºæ™¯çš„æ€§èƒ½ã€‚å®éªŒè¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºæ–¹é¢è¾¾åˆ°ä¸šç•Œé¢†å…ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜åˆ†è¾¨ç‡å›¾åƒé€šè¿‡é™åˆ†è¾¨ç‡å‡å°‘å¸¦å®½ï¼Œå†é€šè¿‡è¶…åˆ†è¾¨ç‡æŠ€æœ¯æ¢å¤ç»†èŠ‚ã€‚</li>
<li>è¿‘æœŸå›¾åƒç¼©æ”¾ç®—æ³•ä½¿ç”¨å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNsï¼‰ä¸ºé™åˆ†è¾¨ç‡å’Œå‡åˆ†è¾¨ç‡åˆ›å»ºç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¤„ç†é«˜é¢‘å’Œä½é¢‘ä¿¡æ¯æ—¶å­˜åœ¨é€šé“å†—ä½™é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„ä¸‰åˆ†æ”¯å¯é€†å—ï¼ˆT-InvBlocksï¼‰å°†ä½é¢‘æˆåˆ†è¿›ä¸€æ­¥ç»†åˆ†ä¸ºäº®åº¦ï¼ˆYï¼‰å’Œè‰²åº¦ï¼ˆCbCrï¼‰ï¼Œä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>é‡‡ç”¨é«˜é¢‘æˆåˆ†å…¨é›¶æ˜ å°„ç­–ç•¥ï¼Œé›†ä¸­å…³é”®ç¼©æ”¾ä¿¡æ¯äºä½åˆ†è¾¨ç‡å›¾åƒå†…ã€‚</li>
<li>T-InvBlockså¯é›†æˆåˆ°ç°æœ‰ç¼©æ”¾æ¨¡å‹ï¼Œæå‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f5bdd4e7313db292469f2f814fe99a7b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96fa05c2e7a34e702954d45d6e917ad9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d53957d6c269d72b3d38db9c3bc2a4f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c79d56ba93ab203e0aacbee381b57762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fc36e6100b687b1c87b3e93d4125562.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Image-registration-is-a-geometric-deep-learning-task"><a href="#Image-registration-is-a-geometric-deep-learning-task" class="headerlink" title="Image registration is a geometric deep learning task"></a>Image registration is a geometric deep learning task</h2><p><strong>Authors:Vasiliki Sideri-Lampretsa, Nil Stolt-AnsÃ³, Martin Menten, Huaqi Qiu, Julian McGinnis, Daniel Rueckert</strong></p>
<p>Data-driven deformable image registration methods predominantly rely on operations that process grid-like inputs. However, applying deformable transformations to an image results in a warped space that deviates from a rigid grid structure. Consequently, data-driven approaches with sequential deformations have to apply grid resampling operations between each deformation step. While artifacts caused by resampling are negligible in high-resolution images, the resampling of sparse, high-dimensional feature grids introduces errors that affect the deformation modeling process. Taking inspiration from Lagrangian reference frames of deformation fields, our work introduces a novel paradigm for data-driven deformable image registration that utilizes geometric deep-learning principles to model deformations without grid requirements. Specifically, we model image features as a set of nodes that freely move in Euclidean space, update their coordinates under graph operations, and dynamically readjust their local neighborhoods. We employ this formulation to construct a multi-resolution deformable registration model, where deformation layers iteratively refine the overall transformation at each resolution without intermediate resampling operations on the feature grids. We investigate our methodâ€™s ability to fully deformably capture large deformations across a number of medical imaging registration tasks. In particular, we apply our approach (GeoReg) to the registration of inter-subject brain MR images and inhale-exhale lung CT images, showing on par performance with the current state-of-the-art methods. We believe our contribution open up avenues of research to reduce the black-box nature of current learned registration paradigms by explicitly modeling the transformation within the architecture. </p>
<blockquote>
<p>æ•°æ®é©±åŠ¨çš„å¯å˜å½¢å›¾åƒæ³¨å†Œæ–¹æ³•ä¸»è¦ä¾èµ–äºå¤„ç†ç½‘æ ¼çŠ¶è¾“å…¥çš„æ“ä½œã€‚ç„¶è€Œï¼Œå°†å¯å˜å½¢è½¬æ¢åº”ç”¨äºå›¾åƒä¼šäº§ç”Ÿåç¦»åˆšæ€§ç½‘æ ¼ç»“æ„çš„æ‰­æ›²ç©ºé—´ã€‚å› æ­¤ï¼Œå…·æœ‰é¡ºåºå˜å½¢çš„æ•°æ®é©±åŠ¨æ–¹æ³•å¿…é¡»åœ¨æ¯æ¬¡å˜å½¢æ­¥éª¤ä¹‹é—´è¿›è¡Œç½‘æ ¼é‡é‡‡æ ·æ“ä½œã€‚è™½ç„¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­é‡é‡‡æ ·å¼•èµ·çš„ä¼ªå½±å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œä½†å¯¹ç¨€ç–ã€é«˜ç»´ç‰¹å¾ç½‘æ ¼çš„é‡é‡‡æ ·ä¼šå¼•å…¥å½±å“å˜å½¢å»ºæ¨¡è¿‡ç¨‹çš„è¯¯å·®ã€‚æˆ‘ä»¬çš„å·¥ä½œä»æ‹‰æ ¼æœ—æ—¥å‚è€ƒç³»å˜å½¢åœºä¸­è·å¾—çµæ„Ÿï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹æ•°æ®é©±åŠ¨å¯å˜å½¢å›¾åƒæ³¨å†ŒèŒƒä¾‹ï¼Œè¯¥èŒƒä¾‹åˆ©ç”¨å‡ ä½•æ·±åº¦å­¦ä¹ åŸç†è¿›è¡Œå˜å½¢å»ºæ¨¡ï¼Œæ— éœ€ç½‘æ ¼è¦æ±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å›¾åƒç‰¹å¾å»ºæ¨¡ä¸ºä¸€ç³»åˆ—èŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­è‡ªç”±ç§»åŠ¨ï¼Œåœ¨å›¾å½¢æ“ä½œä¸‹æ›´æ–°å…¶åæ ‡å¹¶åŠ¨æ€è°ƒæ•´å…¶å±€éƒ¨é‚»åŸŸã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€å…¬å¼æ„å»ºäº†ä¸€ç§å¤šåˆ†è¾¨ç‡å¯å˜å½¢æ³¨å†Œæ¨¡å‹ï¼Œå…¶ä¸­å˜å½¢å±‚ä»¥è¿­ä»£æ–¹å¼å®Œå–„æ¯ä¸ªåˆ†è¾¨ç‡çš„æ•´ä½“è½¬æ¢ï¼Œæ— éœ€åœ¨ç‰¹å¾ç½‘æ ¼ä¸Šè¿›è¡Œä¸­é—´é‡é‡‡æ ·æ“ä½œã€‚æˆ‘ä»¬ç ”ç©¶äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ³¨å†Œä»»åŠ¡ä¸­å®Œå…¨å¯å˜å½¢åœ°æ•è·å¤§å˜å½¢çš„èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆGeoRegï¼‰åº”ç”¨äºè·¨ä¸»ä½“ä¹‹é—´çš„å¤§è„‘MRå›¾åƒå’Œå¸æ°”-å‘¼æ°”è‚ºéƒ¨CTå›¾åƒçš„æ³¨å†Œï¼Œæ˜¾ç¤ºå‡ºä¸å½“å‰æœ€æ–°æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„ç ”ç©¶å¼€è¾Ÿäº†å‡å°‘å½“å‰å­¦ä¹ æ³¨å†ŒèŒƒä¾‹çš„é»‘ç®±æ€§è´¨çš„é€”å¾„ï¼Œé€šè¿‡åœ¨æ¶æ„å†…æ˜¾å¼å»ºæ¨¡è½¬æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13294v1">PDF</a> 22 Pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ•°æ®é©±åŠ¨çš„å¯å˜å½¢å›¾åƒæ³¨å†Œæ–¹æ³•çš„ä¸»è¦æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºç½‘æ ¼è¾“å…¥å¤„ç†ï¼Œä½†åœ¨åº”ç”¨å¯å˜å½¢è½¬æ¢æ—¶ä¼šäº§ç”Ÿåç¦»åˆšæ€§ç½‘æ ¼ç»“æ„çš„ç©ºé—´æ‰­æ›²ã€‚æœ¬æ–‡å—æ‹‰æ ¼æœ—æ—¥å‚è€ƒç³»å˜å½¢åœºçš„å¯å‘ï¼Œåˆ©ç”¨å‡ ä½•æ·±åº¦å­¦ä¹ åŸç†ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€ç½‘æ ¼è¦æ±‚çš„æ•°æ®é©±åŠ¨å¯å˜å½¢å›¾åƒæ³¨å†Œæ–°èŒƒå¼ã€‚è¯¥æ–¹æ³•å°†å›¾åƒç‰¹å¾å»ºæ¨¡ä¸ºä¸€ç»„èŠ‚ç‚¹ï¼Œåœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­è‡ªç”±ç§»åŠ¨ï¼Œåœ¨å›¾å½¢æ“ä½œä¸‹æ›´æ–°å…¶åæ ‡ï¼Œå¹¶åŠ¨æ€è°ƒæ•´å…¶å±€éƒ¨é‚»åŸŸã€‚åœ¨å¤šé¡¹åŒ»å­¦æˆåƒæ³¨å†Œä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®Œå…¨å¯å˜å½¢åœ°æ•è·å¤§å˜å½¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿæ•°æ®é©±åŠ¨çš„å¯å˜å½¢å›¾åƒæ³¨å†Œæ–¹æ³•ä¸»è¦ä¾èµ–ç½‘æ ¼è¾“å…¥å¤„ç†ï¼Œä½†åœ¨åº”ç”¨å¯å˜å½¢è½¬æ¢æ—¶äº§ç”Ÿç©ºé—´æ‰­æ›²ã€‚</li>
<li>ç½‘æ ¼é‡é‡‡æ ·æ“ä½œåœ¨æ¯ä¸€æ­¥å˜å½¢ä¸­éƒ½ä¼šå¼•å…¥è¯¯å·®ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–ã€é«˜ç»´ç‰¹å¾ç½‘æ ¼ä¸­ã€‚</li>
<li>æœ¬æ–‡å—æ‹‰æ ¼æœ—æ—¥å‚è€ƒç³»å˜å½¢åœºçš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é©±åŠ¨å¯å˜å½¢å›¾åƒæ³¨å†Œæ–¹æ³•ã€‚</li>
<li>æ–°æ–¹æ³•åˆ©ç”¨å‡ ä½•æ·±åº¦å­¦ä¹ åŸç†ï¼Œå°†å›¾åƒç‰¹å¾å»ºæ¨¡ä¸ºåœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­è‡ªç”±ç§»åŠ¨çš„èŠ‚ç‚¹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å›¾å½¢æ“ä½œæ›´æ–°èŠ‚ç‚¹åæ ‡ï¼Œå¹¶åŠ¨æ€è°ƒæ•´å±€éƒ¨é‚»åŸŸï¼Œæ— éœ€ç½‘æ ¼è¦æ±‚ã€‚</li>
<li>å¤šåˆ†è¾¨ç‡å¯å˜å½¢æ³¨å†Œæ¨¡å‹èƒ½å¤Ÿé€å±‚ç²¾ç»†è°ƒæ•´è½¬æ¢ï¼Œæ— éœ€ä¸­é—´é‡é‡‡æ ·æ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-20e0b2017a56d2c06209a287502170bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d5a375a9f212f2c01ef21a5f4bc984e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0cff3567f93564d28ee3bf6563a2926.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="S2S2-Semantic-Stacking-for-Robust-Semantic-Segmentation-in-Medical-Imaging"><a href="#S2S2-Semantic-Stacking-for-Robust-Semantic-Segmentation-in-Medical-Imaging" class="headerlink" title="S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical   Imaging"></a>S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical   Imaging</h2><p><strong>Authors:Yimu Pan, Sitao Zhang, Alison D. Gernand, Jeffery A. Goldstein, James Z. Wang</strong></p>
<p>Robustness and generalizability in medical image segmentation are often hindered by scarcity and limited diversity of training data, which stands in contrast to the variability encountered during inference. While conventional strategies â€“ such as domain-specific augmentation, specialized architectures, and tailored training procedures â€“ can alleviate these issues, they depend on the availability and reliability of domain knowledge. When such knowledge is unavailable, misleading, or improperly applied, performance may deteriorate. In response, we introduce a novel, domain-agnostic, add-on, and data-driven strategy inspired by image stacking in image denoising. Termed &#96;&#96;semantic stacking,â€™â€™ our method estimates a denoised semantic representation that complements the conventional segmentation loss during training. This method does not depend on domain-specific assumptions, making it broadly applicable across diverse image modalities, model architectures, and augmentation techniques. Through extensive experiments, we validate the superiority of our approach in improving segmentation performance under diverse conditions. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ymp5078/Semantic-Stacking">https://github.com/ymp5078/Semantic-Stacking</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œé²æ£’æ€§å’Œé€šç”¨æ€§å¸¸å¸¸å—åˆ°è®­ç»ƒæ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§æœ‰é™çš„é˜»ç¢ï¼Œè¿™ä¸æ¨ç†è¿‡ç¨‹ä¸­é‡åˆ°çš„å˜é‡å½¢æˆå¯¹æ¯”ã€‚è™½ç„¶ä¼ ç»Ÿçš„ç­–ç•¥â€”â€”å¦‚é¢†åŸŸç‰¹å®šçš„å¢å¼ºã€ä¸“ç”¨æ¶æ„å’Œå®šåˆ¶è®­ç»ƒç¨‹åºâ€”â€”å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†å®ƒä»¬ä¾èµ–äºé¢†åŸŸçŸ¥è¯†çš„å¯ç”¨æ€§å’Œå¯é æ€§ã€‚å½“è¿™ç§çŸ¥è¯†ä¸å¯ç”¨ã€è¯¯å¯¼æˆ–åº”ç”¨ä¸å½“æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¼šæ¶åŒ–ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ã€é¢†åŸŸæ— å…³ã€é™„åŠ çš„ã€æ•°æ®é©±åŠ¨çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å—åˆ°å›¾åƒå»å™ªä¸­å›¾åƒå †å çš„å¯å‘ã€‚æˆ‘ä»¬å°†å…¶ç§°ä¸ºâ€œè¯­ä¹‰å †å â€ï¼Œè¯¥æ–¹æ³•ä¼°è®¡å»å™ªçš„è¯­ä¹‰è¡¨ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¥å……ä¼ ç»Ÿçš„åˆ†å‰²æŸå¤±ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„å‡è®¾ï¼Œå› æ­¤å¯å¹¿æ³›åº”ç”¨äºä¸åŒçš„å›¾åƒæ¨¡å¼ã€æ¨¡å‹æ¶æ„å’Œå¢å¼ºæŠ€æœ¯ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ”¹å–„ä¸åŒæ¡ä»¶ä¸‹çš„åˆ†å‰²æ€§èƒ½æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/ymp5078/Semantic-Stacking">https://github.com/ymp5078/Semantic-Stacking</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13156v1">PDF</a> AAAI2025</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§å’Œå¤šæ ·æ€§é™åˆ¶å¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›é€ æˆå½±å“ï¼Œé’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ã€é¢†åŸŸæ— å…³çš„ã€é™„åŠ çš„ã€æ•°æ®é©±åŠ¨çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å€Ÿé‰´äº†å›¾åƒå»å™ªä¸­çš„å›¾åƒå †å æŠ€æœ¯ï¼Œè¢«ç§°ä¸ºâ€œè¯­ä¹‰å †å â€ã€‚è¯¥æ–¹æ³•ä¼°è®¡å»å™ªçš„è¯­ä¹‰è¡¨ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¥å……ä¼ ç»Ÿçš„åˆ†å‰²æŸå¤±ï¼Œä¸ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„å‡è®¾ï¼Œå› æ­¤å¯å¹¿æ³›åº”ç”¨äºä¸åŒçš„å›¾åƒæ¨¡æ€ã€æ¨¡å‹æ¶æ„å’Œå¢å¼ºæŠ€æœ¯ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¡ä»¶ä¸‹æ”¹è¿›äº†åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´è®­ç»ƒæ•°æ®ç¨€ç¼ºå’Œå¤šæ ·æ€§é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿç­–ç•¥ä¾èµ–äºé¢†åŸŸçŸ¥è¯†çš„å¯ç”¨æ€§ã€å¯é æ€§å’Œé€‚å½“åº”ç”¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ã€é¢†åŸŸæ— å…³çš„ã€é™„åŠ çš„ã€æ•°æ®é©±åŠ¨çš„ç­–ç•¥â€”â€”â€œè¯­ä¹‰å †å â€ã€‚</li>
<li>â€œè¯­ä¹‰å †å â€æ–¹æ³•å€Ÿé‰´äº†å›¾åƒå»å™ªä¸­çš„å›¾åƒå †å æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼°è®¡å»å™ªçš„è¯­ä¹‰è¡¨ç¤ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¥å……ä¼ ç»Ÿçš„åˆ†å‰²æŸå¤±ã€‚</li>
<li>â€œè¯­ä¹‰å †å â€æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œé€‚ç”¨äºä¸åŒçš„å›¾åƒæ¨¡æ€ã€æ¨¡å‹æ¶æ„å’Œå¢å¼ºæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e6ac961a052458d69a5156dd111aedd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-702c1a90068880d011c40846d2566095.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1590868d6db7b83739fce58c98b24b4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4fbaff335b864b3a9397e7272dd86454.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Knowledge-enhanced-Pathology-Vision-language-Foundation-Model-for-Cancer-Diagnosis"><a href="#A-Knowledge-enhanced-Pathology-Vision-language-Foundation-Model-for-Cancer-Diagnosis" class="headerlink" title="A Knowledge-enhanced Pathology Vision-language Foundation Model for   Cancer Diagnosis"></a>A Knowledge-enhanced Pathology Vision-language Foundation Model for   Cancer Diagnosis</h2><p><strong>Authors:Xiao Zhou, Luoyi Sun, Dexuan He, Wenbin Guan, Ruifen Wang, Lifeng Wang, Xin Sun, Kun Sun, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Deep learning has enabled the development of highly robust foundation models for various pathological tasks across diverse diseases and patient cohorts. Among these models, vision-language pre-training, which leverages large-scale paired data to align pathology image and text embedding spaces, and provides a novel zero-shot paradigm for downstream tasks. However, existing models have been primarily data-driven and lack the incorporation of domain-specific knowledge, which limits their performance in cancer diagnosis, especially for rare tumor subtypes. To address this limitation, we establish a Knowledge-enhanced Pathology (KEEP) foundation model that harnesses disease knowledge to facilitate vision-language pre-training. Specifically, we first construct a disease knowledge graph (KG) that covers 11,454 human diseases with 139,143 disease attributes, including synonyms, definitions, and hypernym relations. We then systematically reorganize the millions of publicly available noisy pathology image-text pairs, into 143K well-structured semantic groups linked through the hierarchical relations of the disease KG. To derive more nuanced image and text representations, we propose a novel knowledge-enhanced vision-language pre-training approach that integrates disease knowledge into the alignment within hierarchical semantic groups instead of unstructured image-text pairs. Validated on 18 diverse benchmarks with more than 14,000 whole slide images (WSIs), KEEP achieves state-of-the-art performance in zero-shot cancer diagnostic tasks. Notably, for cancer detection, KEEP demonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7 cancer types. For cancer subtyping, KEEP achieves a median balanced accuracy of 0.456 in subtyping 30 rare brain cancers, indicating strong generalizability for diagnosing rare tumors. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ä¸ºå„ç§ç–¾ç—…å’Œä¸åŒæ‚£è€…ç¾¤ä½“çš„å¤šç§ç—…ç†ä»»åŠ¡å¼€å‘é«˜åº¦ç¨³å¥çš„åŸºç¡€æ¨¡å‹æä¾›äº†å¯èƒ½ã€‚åœ¨è¿™äº›æ¨¡å‹ä¸­ï¼Œè§†è§‰è¯­è¨€é¢„è®­ç»ƒé€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡é…å¯¹æ•°æ®æ¥å¯¹é½ç—…ç†å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ç©ºé—´ï¼Œå¹¶ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°å‹é›¶æ ·æœ¬èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹ä¸»è¦æ˜¯æ•°æ®é©±åŠ¨ï¼Œç¼ºä¹ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†çš„èå…¥ï¼Œè¿™åœ¨ç™Œç—‡è¯Šæ–­ä¸­ï¼Œå°¤å…¶æ˜¯ç½•è§è‚¿ç˜¤äºšå‹çš„è¯Šæ–­ä¸­é™åˆ¶äº†å…¶æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªçŸ¥è¯†å¢å¼ºçš„ç—…ç†ï¼ˆKEEPï¼‰åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ç–¾ç—…çŸ¥è¯†æ¥ä¿ƒè¿›è§†è§‰è¯­è¨€é¢„è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªç–¾ç—…çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ï¼Œæ¶µç›–11454ç§äººç±»ç–¾ç—…å’Œ139143ä¸ªç–¾ç—…å±æ€§ï¼ŒåŒ…æ‹¬åŒä¹‰è¯ã€å®šä¹‰å’Œä¸Šä½è¯å…³ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹æ•°ç™¾ä¸‡ä¸ªå…¬å¼€å¯ç”¨çš„å˜ˆæ‚ç—…ç†å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œç³»ç»ŸåŒ–é‡ç»„ï¼Œé€šè¿‡ç–¾ç—…çŸ¥è¯†å›¾è°±çš„å±‚æ¬¡å…³ç³»å°†å…¶åˆ†ä¸º14.3ä¸‡ç»„ç»“æ„è‰¯å¥½çš„è¯­ä¹‰ç»„ã€‚ä¸ºäº†å¾—å‡ºæ›´å¾®å¦™çš„å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„çŸ¥è¯†å¢å¼ºè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ç–¾ç—…çŸ¥è¯†é›†æˆåˆ°å±‚æ¬¡è¯­ä¹‰ç»„å†…çš„å¯¹é½ä¸­ï¼Œè€Œä¸æ˜¯ç®€å•çš„å›¾åƒæ–‡æœ¬å¯¹ã€‚åœ¨è¶…è¿‡1.4ä¸‡å¼ å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰çš„18ä¸ªä¸åŒåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼ŒKEEPåœ¨é›¶æ ·æœ¬ç™Œç—‡è¯Šæ–­ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç‰¹åˆ«æ˜¯åœ¨ç™Œç—‡æ£€æµ‹æ–¹é¢ï¼ŒKEEPåœ¨7ç§ç™Œç—‡ç±»å‹ä¸Šè¡¨ç°å‡ºäº†å¹³å‡æ•æ„Ÿæ€§ä¸º89.8%ï¼Œç‰¹å¼‚æ€§ä¸º95.0%ã€‚åœ¨ç™Œç—‡äºšå‹åˆ†æä¸­ï¼ŒKEEPåœ¨30ç§ç½•è§è„‘ç™Œçš„äºšå‹åˆ†æä¸­è¾¾åˆ°äº†ä¸­ä½æ•°å¹³è¡¡ç²¾åº¦ä¸º0.456ï¼Œæ˜¾ç¤ºå‡ºè¯Šæ–­ç½•è§è‚¿ç˜¤çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13126v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æ·±åº¦å­¦ä¹ å·²æ¨åŠ¨é’ˆå¯¹å¤šç§ç–¾ç—…å’Œæ‚£è€…ç¾¤ä½“çš„ç¨³å¥åŸºç¡€æ¨¡å‹å‘å±•ã€‚ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›é›¶æ ·æœ¬èŒƒå¼çš„æ–°å‹é¢„è®­ç»ƒæŠ€æœ¯å·²å‡ºç°ï¼Œä½†ç°æœ‰æ¨¡å‹ä¸»è¦ä¾èµ–æ•°æ®ï¼Œç¼ºä¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„èå…¥ï¼Œåœ¨ç™Œç—‡è¯Šæ–­ï¼Œå°¤å…¶æ˜¯ç½•è§è‚¿ç˜¤äºšå‹æ–¹é¢çš„æ€§èƒ½å—é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†èåˆç–¾ç—…çŸ¥è¯†å¢å¼ºï¼ˆKEEPï¼‰çš„åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç–¾ç—…çŸ¥è¯†ä¿ƒè¿›è§†è§‰è¯­è¨€é¢„è®­ç»ƒã€‚æˆ‘ä»¬åœ¨ç–¾ç—…çŸ¥è¯†å›¾è°±ä¸­è¦†ç›–1.1ä¸‡å¤šç§äººç±»ç–¾ç—…å’Œ13.9ä¸‡å¤šç§ç–¾ç—…å±æ€§ï¼Œå¯¹ç™¾ä¸‡çº§å…¬å¼€ç—…ç†å­¦å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œç³»ç»Ÿé‡ç»„ã€‚åœ¨18ä¸ªå¤šæ ·åŸºå‡†æµ‹è¯•è¶…è¿‡1.4ä¸‡å¼ å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰éªŒè¯ä¸‹ï¼ŒKEEPåœ¨é›¶æ ·æœ¬ç™Œç—‡è¯Šæ–­ä»»åŠ¡ä¸­å–å¾—æœ€ä½³æ€§èƒ½ã€‚å°¤å…¶åœ¨ç™Œç—‡æ£€æµ‹å’Œç™Œç—‡äºšå‹é‰´åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±åº¦å­¦ä¹ ä¸ºå„ç§ç—…ç†ä»»åŠ¡æ„å»ºäº†ç¨³å¥çš„åŸºç¡€æ¨¡å‹ã€‚</li>
<li>è§†è§‰è¯­è¨€é¢„è®­ç»ƒåˆ©ç”¨å¤§è§„æ¨¡é…å¯¹æ•°æ®å¯¹é½ç—…ç†å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¸»è¦ä¾èµ–æ•°æ®ï¼Œç¼ºä¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†èå…¥ï¼Œå½±å“åœ¨ç™Œç—‡è¯Šæ–­å°¤å…¶æ˜¯ç½•è§è‚¿ç˜¤äºšå‹æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>KEEPæ¨¡å‹é€šè¿‡æ„å»ºç–¾ç—…çŸ¥è¯†å›¾è°±èå…¥ç–¾ç—…çŸ¥è¯†ï¼Œä¿ƒè¿›è§†è§‰è¯­è¨€é¢„è®­ç»ƒã€‚</li>
<li>ç–¾ç—…çŸ¥è¯†å›¾è°±è¦†ç›–1.1ä¸‡å¤šç§äººç±»ç–¾ç—…å’Œå¤§é‡ç–¾ç—…å±æ€§ã€‚</li>
<li>KEEPæ¨¡å‹å¯¹ç—…ç†å­¦å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œç³»ç»Ÿæ€§é‡ç»„ï¼Œé€šè¿‡ç–¾ç—…çŸ¥è¯†çš„å±‚æ¬¡å…³ç³»è¿›è¡Œå¯¹é½ã€‚</li>
<li>KEEPæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ç™Œç—‡æ£€æµ‹å’Œç½•è§è‚¿ç˜¤äºšå‹é‰´åˆ«æ–¹é¢ã€‚</li>
<li>KEEPæ¨¡å‹åœ¨é›¶æ ·æœ¬ç™Œç—‡è¯Šæ–­ä»»åŠ¡ä¸­å¹³å‡æ•æ„Ÿç‡ä¸º89.8%ï¼Œç‰¹å¼‚æ€§ä¸º95.0%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c701e119329a86c017a4de13532063d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a7a5bf9f858fc6b1e3956f9375687c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a3bf59eeaf40fb2a79c7830e0f2f45b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c58738ece1588d790a73c376a4869539.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Learning-of-Patch-Based-Smooth-Plus-Sparse-Models-for-Image-Reconstruction"><a href="#Learning-of-Patch-Based-Smooth-Plus-Sparse-Models-for-Image-Reconstruction" class="headerlink" title="Learning of Patch-Based Smooth-Plus-Sparse Models for Image   Reconstruction"></a>Learning of Patch-Based Smooth-Plus-Sparse Models for Image   Reconstruction</h2><p><strong>Authors:Stanislas Ducotterd, Sebastian Neumayer, Michael Unser</strong></p>
<p>We aim at the solution of inverse problems in imaging, by combining a penalized sparse representation of image patches with an unconstrained smooth one. This allows for a straightforward interpretation of the reconstruction. We formulate the optimization as a bilevel problem. The inner problem deploys classical algorithms while the outer problem optimizes the dictionary and the regularizer parameters through supervised learning. The process is carried out via implicit differentiation and gradient-based optimization. We evaluate our method for denoising, super-resolution, and compressed-sensing magnetic-resonance imaging. We compare it to other classical models as well as deep-learning-based methods and show that it always outperforms the former and also the latter in some instances. </p>
<blockquote>
<p>æˆ‘ä»¬æ—¨åœ¨é€šè¿‡ç»“åˆå›¾åƒå—çš„æƒ©ç½šç¨€ç–è¡¨ç¤ºå’Œæ— çº¦æŸå¹³æ»‘è¡¨ç¤ºæ¥è§£å†³æˆåƒä¸­çš„åé—®é¢˜ã€‚è¿™å…è®¸å¯¹é‡å»ºè¿›è¡Œç›´æ¥è§£é‡Šã€‚æˆ‘ä»¬å°†ä¼˜åŒ–é—®é¢˜åˆ¶å®šä¸ºä¸€ä¸ªåŒå±‚é—®é¢˜ã€‚å†…å±‚é—®é¢˜é‡‡ç”¨ç»å…¸ç®—æ³•ï¼Œå¤–å±‚é—®é¢˜é€šè¿‡ç›‘ç£å­¦ä¹ ä¼˜åŒ–å­—å…¸å’Œæ­£åˆ™åŒ–å‚æ•°ã€‚è¯¥è¿‡ç¨‹é€šè¿‡éšå¼å¾®åˆ†å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ¥å®Œæˆã€‚æˆ‘ä»¬é’ˆå¯¹å»å™ªã€è¶…åˆ†è¾¨ç‡å’Œå‹ç¼©æ„ŸçŸ¥ç£å…±æŒ¯æˆåƒè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†å®ƒä¸å…¶å®ƒç»å…¸æ¨¡å‹ä»¥åŠåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¯æ˜å®ƒåœ¨æŸäº›æƒ…å†µä¸‹æ€»æ˜¯ä¼˜äºå‰è€…ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ä¹Ÿä¼˜äºåè€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13070v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ—¨åœ¨è§£å†³æˆåƒä¸­çš„åé—®é¢˜ï¼Œç»“åˆæƒ©ç½šç¨€ç–å›¾åƒå—è¡¨ç¤ºå’Œæ— çº¦æŸå¹³æ»‘è¡¨ç¤ºï¼Œæ˜“äºè§£é‡Šé‡å»ºç»“æœã€‚ä¼˜åŒ–é—®é¢˜è¢«è¡¨è¿°ä¸ºåŒå±‚é—®é¢˜ï¼Œå†…å±‚é—®é¢˜é‡‡ç”¨ç»å…¸ç®—æ³•ï¼Œå¤–å±‚é—®é¢˜é€šè¿‡ç›‘ç£å­¦ä¹ ä¼˜åŒ–å­—å…¸å’Œæ­£åˆ™åŒ–å‚æ•°ã€‚è¿‡ç¨‹é€šè¿‡éšå¼åˆ†åŒ–å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ¥å®ç°ã€‚ç»å»å™ªã€è¶…åˆ†è¾¨ç‡å’Œå‹ç¼©æ„ŸçŸ¥ç£å…±æŒ¯æˆåƒè¯„ä¼°ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–ç»å…¸æ¨¡å‹å’ŒæŸäº›æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“åˆæƒ©ç½šç¨€ç–å›¾åƒå—è¡¨ç¤ºå’Œæ— çº¦æŸå¹³æ»‘è¡¨ç¤ºï¼Œè§£å†³æˆåƒä¸­çš„åé—®é¢˜ã€‚</li>
<li>ä¼˜åŒ–é—®é¢˜è¡¨è¿°ä¸ºåŒå±‚é—®é¢˜ï¼Œå†…å±‚é‡‡ç”¨ç»å…¸ç®—æ³•ï¼Œå¤–å±‚ä¼˜åŒ–å­—å…¸å’Œæ­£åˆ™åŒ–å‚æ•°ã€‚</li>
<li>é€šè¿‡ç›‘ç£å­¦ä¹ å’Œéšå¼åˆ†åŒ–åŠåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å®ç°è¿‡ç¨‹ã€‚</li>
<li>æ–¹æ³•åœ¨å»å™ªã€è¶…åˆ†è¾¨ç‡å’Œå‹ç¼©æ„ŸçŸ¥ç£å…±æŒ¯æˆåƒåº”ç”¨ä¸­å¾—åˆ°è¯„ä¼°ã€‚</li>
<li>ä¸å…¶ä»–ç»å…¸æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ç”šè‡³æ¯”æ·±åº¦å­¦ä¹ æ–¹æ³•è¡¨ç°æ›´å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-42651ad09ac5ab8816c0b08009c30f07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48dc2781366c34a8b74636d5f81d091d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb1cf91abd9e9bdf3aa1f3da73aafabd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57db75d5c42abef42531ba8c97eb8c00.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="3D-MedDiffusion-A-3D-Medical-Diffusion-Model-for-Controllable-and-High-quality-Medical-Image-Generation"><a href="#3D-MedDiffusion-A-3D-Medical-Diffusion-Model-for-Controllable-and-High-quality-Medical-Image-Generation" class="headerlink" title="3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and   High-quality Medical Image Generation"></a>3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and   High-quality Medical Image Generation</h2><p><strong>Authors:Haoshen Wang, Zhentao Liu, Kaicong Sun, Xiaodong Wang, Dinggang Shen, Zhiming Cui</strong></p>
<p>The generation of medical images presents significant challenges due to their high-resolution and three-dimensional nature. Existing methods often yield suboptimal performance in generating high-quality 3D medical images, and there is currently no universal generative framework for medical imaging. In this paper, we introduce the 3D Medical Diffusion (3D MedDiffusion) model for controllable, high-quality 3D medical image generation. 3D MedDiffusion incorporates a novel, highly efficient Patch-Volume Autoencoder that compresses medical images into latent space through patch-wise encoding and recovers back into image space through volume-wise decoding. Additionally, we design a new noise estimator to capture both local details and global structure information during diffusion denoising process. 3D MedDiffusion can generate fine-detailed, high-resolution images (up to 512x512x512) and effectively adapt to various downstream tasks as it is trained on large-scale datasets covering CT and MRI modalities and different anatomical regions (from head to leg). Experimental results demonstrate that 3D MedDiffusion surpasses state-of-the-art methods in generative quality and exhibits strong generalizability across tasks such as sparse-view CT reconstruction, fast MRI reconstruction, and data augmentation. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒç”Ÿæˆå› å…¶é«˜åˆ†è¾¨ç‡å’Œä¸‰ç»´ç‰¹æ€§è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¸¸åœ¨ç”Ÿæˆé«˜è´¨é‡3DåŒ»å­¦å›¾åƒæ—¶è¡¨ç°ä¸ä½³ï¼Œç›®å‰åŒ»å­¦æˆåƒé¢†åŸŸå°šæœªæœ‰é€šç”¨çš„ç”Ÿæˆæ¡†æ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç”¨äºå¯æ§ã€é«˜è´¨é‡3DåŒ»å­¦å›¾åƒç”Ÿæˆçš„3DåŒ»å­¦æ‰©æ•£ï¼ˆ3D MedDiffusionï¼‰æ¨¡å‹ã€‚3D MedDiffusionèå…¥äº†ä¸€ç§æ–°é¢–ã€é«˜æ•ˆçš„Patch-Volumeè‡ªç¼–ç å™¨ï¼Œè¯¥è‡ªç¼–ç å™¨é€šè¿‡patchçº§ç¼–ç å°†åŒ»å­¦å›¾åƒå‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œå†é€šè¿‡volumeçº§è§£ç æ¢å¤åˆ°å›¾åƒç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„å™ªå£°ä¼°è®¡å™¨ï¼Œåœ¨æ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­æ•æ‰å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ç»“æ„ä¿¡æ¯ã€‚3D MedDiffusionå¯ä»¥ç”Ÿæˆç»†èŠ‚ç²¾ç»†ã€é«˜åˆ†è¾¨ç‡çš„å›¾åƒï¼ˆé«˜è¾¾512x512x512ï¼‰ï¼Œå¹¶ä¸”åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¦†ç›–CTå’ŒMRIæ¨¡æ€ä»¥åŠä¸åŒçš„è§£å‰–åŒºåŸŸï¼ˆä»å¤´è‡³è„šï¼‰ï¼Œå› æ­¤å¯æœ‰æ•ˆé€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3D MedDiffusionåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶åœ¨ç¨€ç–è§†å›¾CTé‡å»ºã€å¿«é€ŸMRIé‡å»ºå’Œæ•°æ®å¢å¼ºç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13059v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå¯æ§ã€é«˜è´¨é‡çš„ä¸‰ç»´åŒ»å­¦å›¾åƒç”Ÿæˆçš„æ–°å‹æ¡†æ¶ï¼Œåä¸ºä¸‰ç»´åŒ»å­¦æ‰©æ•£æ¨¡å‹ï¼ˆ3D MedDiffusionï¼‰ã€‚å®ƒèåˆäº†æ–°é¢–çš„Patch-Volume AutoencoderæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œç”Ÿæˆç²¾ç»†ã€é«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´åŒ»å­¦å›¾åƒï¼ˆåˆ†è¾¨ç‡é«˜è¾¾512x512x512ï¼‰ï¼Œå¹¶é€‚åº”å¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚ç¨€ç–è§†å›¾CTé‡å»ºã€å¿«é€ŸMRIé‡å»ºå’Œæ•°æ®å¢å¼ºç­‰ã€‚é€šè¿‡è¿™ä¸€è¿‡ç¨‹ï¼ŒåŒ»å­¦å›¾åƒèƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­ä»¥è¡¥ä¸çš„æ–¹å¼è¿›è¡Œç¼–ç å’Œè§£ç ï¼Œä»è€Œé«˜æ•ˆåœ°å‹ç¼©å’Œæ¢å¤ã€‚è®ºæ–‡è®¾è®¡äº†ä¸€ç§æ–°çš„å™ªå£°ä¼°è®¡å™¨ï¼Œèƒ½å¤Ÿåœ¨æ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­æ•æ‰å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ç»“æ„ä¿¡æ¯ã€‚æ€»ä½“è€Œè¨€ï¼Œè®ºæ–‡çš„åˆ›æ–°æˆæœæœ‰æœ›åœ¨åŒ»å­¦å›¾åƒç”Ÿæˆé¢†åŸŸå¸¦æ¥æ˜¾è‘—çš„æå‡å’Œæ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒç”Ÿæˆé¢ä¸´é«˜åˆ†è¾¨ç‡å’Œä¸‰ç»´ç‰¹æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡ä¸‰ç»´åŒ»å­¦å›¾åƒçš„æ€§èƒ½å¹¶ä¸ç†æƒ³ï¼Œç¼ºä¹é€šç”¨çš„åŒ»å­¦æˆåƒç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸‰ç»´åŒ»å­¦æ‰©æ•£ï¼ˆ3D MedDiffusionï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å¯æ§ã€é«˜è´¨é‡çš„ä¸‰ç»´åŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>3D MedDiffusionæ¨¡å‹èåˆäº†æ–°é¢–çš„Patch-Volume AutoencoderæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­ä»¥è¡¥ä¸çš„æ–¹å¼è¿›è¡Œç¼–ç å’Œè§£ç ï¼Œä»è€Œé«˜æ•ˆåœ°å‹ç¼©å’Œæ¢å¤åŒ»å­¦å›¾åƒã€‚</li>
<li>è®ºæ–‡è®¾è®¡äº†ä¸€ç§æ–°çš„å™ªå£°ä¼°è®¡å™¨ï¼Œç”¨äºåœ¨æ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­æ•æ‰å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ç»“æ„ä¿¡æ¯ã€‚</li>
<li>3D MedDiffusionæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡ï¼ˆé«˜è¾¾512x512x512ï¼‰çš„ç²¾ç»†å›¾åƒï¼Œå¹¶é€‚åº”å¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚ç¨€ç–è§†å›¾CTé‡å»ºã€å¿«é€ŸMRIé‡å»ºå’Œæ•°æ®å¢å¼ºç­‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d025b77ec5e648abbcc6e4a69194f66b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6cdd62b4b1f21590ea05ab4df342c3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08a95f03efe51a43612bf8c222d3ad31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ef15d5c75a72b3ff427278fa10a201a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e71222a735c90dbfaae69f3fb1d79f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de8d3c61d7b9a2641a924a07a236768c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SEG-SAM-Semantic-Guided-SAM-for-Unified-Medical-Image-Segmentation"><a href="#SEG-SAM-Semantic-Guided-SAM-for-Unified-Medical-Image-Segmentation" class="headerlink" title="SEG-SAM: Semantic-Guided SAM for Unified Medical Image Segmentation"></a>SEG-SAM: Semantic-Guided SAM for Unified Medical Image Segmentation</h2><p><strong>Authors:Shuangping Huang, Hao Liang, Qingfeng Wang, Chulong Zhong, Zijian Zhou, Miaojing Shi</strong></p>
<p>Recently, developing unified medical image segmentation models gains increasing attention, especially with the advent of the Segment Anything Model (SAM). SAM has shown promising binary segmentation performance in natural domains, however, transferring it to the medical domain remains challenging, as medical images often possess substantial inter-category overlaps. To address this, we propose the SEmantic-Guided SAM (SEG-SAM), a unified medical segmentation model that incorporates semantic medical knowledge to enhance medical segmentation performance. First, to avoid the potential conflict between binary and semantic predictions, we introduce a semantic-aware decoder independent of SAMâ€™s original decoder, specialized for both semantic segmentation on the prompted object and classification on unprompted objects in images. To further enhance the modelâ€™s semantic understanding, we solicit key characteristics of medical categories from large language models and incorporate them into SEG-SAM through a text-to-vision semantic module, adaptively transferring the language information into the visual segmentation task. In the end, we introduce the cross-mask spatial alignment strategy to encourage greater overlap between the predicted masks from SEG-SAMâ€™s two decoders, thereby benefiting both predictions. Extensive experiments demonstrate that SEG-SAM outperforms state-of-the-art SAM-based methods in unified binary medical segmentation and task-specific methods in semantic medical segmentation, showcasing promising results and potential for broader medical applications. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¼€å‘ç»Ÿä¸€çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå°¤å…¶æ˜¯éšç€Segment Anything Modelï¼ˆSAMï¼‰çš„å‡ºç°ã€‚SAMåœ¨è‡ªç„¶é¢†åŸŸçš„äºŒè¿›åˆ¶åˆ†å‰²æ€§èƒ½æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ï¼Œç„¶è€Œï¼Œå°†å…¶è½¬ç§»åˆ°åŒ»å­¦é¢†åŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºåŒ»å­¦å›¾åƒé€šå¸¸å­˜åœ¨å¤§é‡çš„ç±»åˆ«é—´é‡å ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SEmantic-Guided SAMï¼ˆSEG-SAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¯­ä¹‰åŒ»å­¦çŸ¥è¯†å¢å¼ºåŒ»å­¦åˆ†å‰²æ€§èƒ½çš„ç»Ÿä¸€çš„åŒ»å­¦åˆ†å‰²æ¨¡å‹ã€‚é¦–å…ˆï¼Œä¸ºäº†é¿å…äºŒè¿›åˆ¶å’Œè¯­ä¹‰é¢„æµ‹ä¹‹é—´çš„æ½œåœ¨å†²çªï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç‹¬ç«‹äºSAMåŸå§‹è§£ç å™¨çš„è¯­ä¹‰æ„ŸçŸ¥è§£ç å™¨ï¼Œä¸“é—¨ç”¨äºå›¾åƒä¸­æç¤ºå¯¹è±¡çš„è¯­ä¹‰åˆ†å‰²å’Œæœªæç¤ºå¯¹è±¡çš„åˆ†ç±»ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹å¯¹è¯­ä¹‰çš„ç†è§£ï¼Œæˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–åŒ»å­¦ç±»åˆ«çš„å…³é”®ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ–‡æœ¬åˆ°è§†è§‰è¯­ä¹‰æ¨¡å—å°†å…¶èå…¥SEG-SAMä¸­ï¼Œè‡ªé€‚åº”åœ°å°†è¯­è¨€ä¿¡æ¯è½¬åŒ–ä¸ºè§†è§‰åˆ†å‰²ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ©è†œç©ºé—´å¯¹é½ç­–ç•¥ï¼Œä»¥é¼“åŠ±SEG-SAMçš„ä¸¤ä¸ªè§£ç å™¨äº§ç”Ÿçš„é¢„æµ‹æ©è†œä¹‹é—´æ›´å¤§çš„é‡å ï¼Œä»è€Œæœ‰åˆ©äºä¸¤ç§é¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSEG-SAMåœ¨ç»Ÿä¸€çš„äºŒè¿›åˆ¶åŒ»å­¦åˆ†å‰²ä¸Šä¼˜äºæœ€å…ˆè¿›çš„SAMæ–¹æ³•ï¼Œåœ¨ç‰¹å®šçš„è¯­ä¹‰åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œå±•ç°å‡ºå¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œæ½œåœ¨çš„åŒ»å­¦åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12660v1">PDF</a> 12 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»å­¦å›¾åƒç»Ÿä¸€åˆ†å‰²æ¨¡å‹çš„å‘å±•èƒŒæ™¯ï¼Œä»‹ç»äº†SEMANTIC-GUIDED SEGMENT ANYTHING MODELï¼ˆSEG-SAMï¼‰æ¨¡å‹çš„æå‡ºåŠå…¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¼˜åŠ¿ã€‚é€šè¿‡å¼•å…¥è¯­ä¹‰æ„ŸçŸ¥è§£ç å™¨ï¼Œèåˆå¤§å‹è¯­è¨€æ¨¡å‹çš„åŒ»å­¦ç±»åˆ«å…³é”®ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨è·¨æ©è†œç©ºé—´å¯¹é½ç­–ç•¥ï¼ŒSEG-SAMæ¨¡å‹åœ¨ç»Ÿä¸€äºŒå…ƒåŒ»å­¦åˆ†å‰²å’Œè¯­ä¹‰åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SEG-SAMæ¨¡å‹æ˜¯åŸºäºSegment Anything Modelï¼ˆSAMï¼‰å¼€å‘çš„ç»Ÿä¸€åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œé’ˆå¯¹åŒ»å­¦å›¾åƒç‰¹æœ‰çš„é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
<li>å¼•å…¥è¯­ä¹‰æ„ŸçŸ¥è§£ç å™¨ï¼Œè§£å†³äºŒå…ƒå’Œè¯­ä¹‰é¢„æµ‹ä¹‹é—´çš„æ½œåœ¨å†²çªã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–åŒ»å­¦ç±»åˆ«çš„å…³é”®ç‰¹å¾ï¼Œå¹¶èå…¥SEG-SAMæ¨¡å‹ä¸­ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬åˆ°è§†è§‰çš„è¯­ä¹‰æ¨¡å—ï¼Œè‡ªé€‚åº”åœ°å°†è¯­è¨€ä¿¡æ¯è½¬åŒ–ä¸ºè§†è§‰åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨è·¨æ©è†œç©ºé—´å¯¹é½ç­–ç•¥ï¼Œæå‡ä¸¤ä¸ªè§£ç å™¨é¢„æµ‹çš„æ©è†œé‡å åº¦ã€‚</li>
<li>SEG-SAMæ¨¡å‹åœ¨ç»Ÿä¸€äºŒå…ƒåŒ»å­¦åˆ†å‰²å’Œè¯­ä¹‰åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-08a32d1954892759a4d38fd62cdc50d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbe3ba1d12b414f71e943a0842d8e6cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56f3b84a1d972066dd6724985ce37556.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DuSSS-Dual-Semantic-Similarity-Supervised-Vision-Language-Model-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#DuSSS-Dual-Semantic-Similarity-Supervised-Vision-Language-Model-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for   Semi-Supervised Medical Image Segmentation"></a>DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Qingtao Pan, Wenhao Qiao, Jingjiao Lou, Bing Ji, Shuo Li</strong></p>
<p>Semi-supervised medical image segmentation (SSMIS) uses consistency learning to regularize model training, which alleviates the burden of pixel-wise manual annotations. However, it often suffers from error supervision from low-quality pseudo labels. Vision-Language Model (VLM) has great potential to enhance pseudo labels by introducing text prompt guided multimodal supervision information. It nevertheless faces the cross-modal problem: the obtained messages tend to correspond to multiple targets. To address aforementioned problems, we propose a Dual Semantic Similarity-Supervised VLM (DuSSS) for SSMIS. Specifically, 1) a Dual Contrastive Learning (DCL) is designed to improve cross-modal semantic consistency by capturing intrinsic representations within each modality and semantic correlations across modalities. 2) To encourage the learning of multiple semantic correspondences, a Semantic Similarity-Supervision strategy (SSS) is proposed and injected into each contrastive learning process in DCL, supervising semantic similarity via the distribution-based uncertainty levels. Furthermore, a novel VLM-based SSMIS network is designed to compensate for the quality deficiencies of pseudo-labels. It utilizes the pretrained VLM to generate text prompt guided supervision information, refining the pseudo label for better consistency regularization. Experimental results demonstrate that our DuSSS achieves outstanding performance with Dice of 82.52%, 74.61% and 78.03% on three public datasets (QaTa-COV19, BM-Seg and MoNuSeg). </p>
<blockquote>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰åˆ©ç”¨ä¸€è‡´æ€§å­¦ä¹ æ¥è§„èŒƒæ¨¡å‹è®­ç»ƒï¼Œå‡è½»äº†é€åƒç´ æ‰‹åŠ¨æ ‡æ³¨çš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œå®ƒå¸¸å¸¸å—åˆ°ä½è´¨é‡ä¼ªæ ‡ç­¾çš„é”™è¯¯ç›‘ç£çš„å½±å“ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡å¼•å…¥æ–‡æœ¬æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€ç›‘ç£ä¿¡æ¯ï¼Œåœ¨å¢å¼ºä¼ªæ ‡ç­¾æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒé¢ä¸´è·¨æ¨¡æ€é—®é¢˜ï¼šè·å–çš„ä¿¡æ¯å¾€å¾€å¯¹åº”å¤šä¸ªç›®æ ‡ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºSSMISçš„åŒé‡è¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£VLMï¼ˆDuSSSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œ1ï¼‰è®¾è®¡äº†åŒé‡å¯¹æ¯”å­¦ä¹ ï¼ˆDCLï¼‰ï¼Œé€šè¿‡æ•æ‰æ¯ç§æ¨¡æ€çš„å†…åœ¨è¡¨ç¤ºå’Œè·¨æ¨¡æ€çš„è¯­ä¹‰å…³è”ï¼Œæé«˜è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ã€‚2ï¼‰ä¸ºäº†é¼“åŠ±å­¦ä¹ å¤šä¸ªè¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œæå‡ºäº†ä¸€ç§è¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£ç­–ç•¥ï¼ˆSSSï¼‰ï¼Œå¹¶å°†å…¶æ³¨å…¥DCLä¸­çš„æ¯ä¸ªå¯¹æ¯”å­¦ä¹ è¿‡ç¨‹ï¼Œé€šè¿‡åŸºäºåˆ†å¸ƒçš„ä¸ç¡®å®šæ€§æ°´å¹³ç›‘ç£è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„åŸºäºVLMçš„SSMISç½‘ç»œï¼Œä»¥å¼¥è¡¥ä¼ªæ ‡ç­¾è´¨é‡ç¼ºé™·ã€‚å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„VLMç”Ÿæˆæ–‡æœ¬æç¤ºå¼•å¯¼çš„ç›‘ç£ä¿¡æ¯ï¼Œå¯¹ä¼ªæ ‡ç­¾è¿›è¡Œç»†åŒ–ï¼Œä»¥å¾—åˆ°æ›´å¥½çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„DuSSSåœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆQaTa-COV19ã€BM-Segå’ŒMoNuSegï¼‰ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒDiceåˆ†åˆ«ä¸º82.52%ã€74.61%å’Œ78.03%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12492v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰ä¸­çš„ä¸€è‡´æ€§å­¦ä¹ åŠé¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä½è´¨é‡ä¼ªæ ‡ç­¾å¯¼è‡´çš„è¯¯å·®ç›‘ç£é—®é¢˜ã€‚æå‡ºä½¿ç”¨Dual Semantic Similarity-Supervised Vision-Language Modelï¼ˆDuSSSï¼‰è§£å†³ä¸Šè¿°é—®é¢˜ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬æ”¹è¿›è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§çš„Dual Contrastive Learningï¼ˆDCLï¼‰ï¼Œé¼“åŠ±å­¦ä¹ å¤šé‡è¯­ä¹‰å¯¹åº”å…³ç³»çš„Semantic Similarity-Supervisionç­–ç•¥ï¼ˆSSSï¼‰ï¼Œå¹¶è®¾è®¡åŸºäºVLMçš„SSMISç½‘ç»œä»¥è¡¥å¿ä¼ªæ ‡ç­¾çš„è´¨é‡ç¼ºé™·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDuSSSåœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„Diceè¡¨ç°ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²åˆ©ç”¨ä¸€è‡´æ€§å­¦ä¹ è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå‡å°‘åƒç´ çº§æ‰‹åŠ¨æ ‡æ³¨çš„è´Ÿæ‹…ã€‚</li>
<li>å­˜åœ¨ä½è´¨é‡ä¼ªæ ‡ç­¾å¼•èµ·çš„è¯¯å·®ç›‘ç£é—®é¢˜ã€‚</li>
<li>å¼•å…¥Vision-Language Modelï¼ˆVLMï¼‰é€šè¿‡æ–‡æœ¬æç¤ºå¼•å¯¼å¤šæ¨¡æ€ç›‘ç£ä¿¡æ¯æé«˜ä¼ªæ ‡ç­¾è´¨é‡ã€‚</li>
<li>é¢ä¸´è·¨æ¨¡æ€é—®é¢˜ï¼šè·å–çš„ä¿¡æ¯å¾€å¾€å¯¹åº”å¤šä¸ªç›®æ ‡ã€‚</li>
<li>æå‡ºDual Semantic Similarity-Supervised VLMï¼ˆDuSSSï¼‰è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒåŒ…æ‹¬æ”¹è¿›è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§çš„Dual Contrastive Learningï¼ˆDCLï¼‰ã€‚</li>
<li>å¼•å…¥Semantic Similarity-Supervisionç­–ç•¥ï¼ˆSSSï¼‰é¼“åŠ±å­¦ä¹ å¤šä¸ªè¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¹¶æ³¨å…¥åˆ°DCLçš„å¯¹æ¯”å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>è®¾è®¡åŸºäºVLMçš„SSMISç½‘ç»œï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„VLMç”Ÿæˆæ–‡æœ¬æç¤ºå¼•å¯¼çš„ç›‘ç£ä¿¡æ¯ï¼Œæ”¹å–„ä¼ªæ ‡ç­¾è´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7239e0c9bcb715b290fbf64ebabc295.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f89b040b8a468f5823b755eadd1ef7bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-198c30e94c70e5a3f4feeba5d1a3cb07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-235ce8abb7612cf23263f02516ed03aa.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DLSOM-A-Deep-learning-based-strategy-for-liver-cancer-subtyping"><a href="#DLSOM-A-Deep-learning-based-strategy-for-liver-cancer-subtyping" class="headerlink" title="DLSOM: A Deep learning-based strategy for liver cancer subtyping"></a>DLSOM: A Deep learning-based strategy for liver cancer subtyping</h2><p><strong>Authors:Fabio Zamio</strong></p>
<p>Liver cancer is a leading cause of cancer-related mortality worldwide, with its high genetic heterogeneity complicating diagnosis and treatment. This study introduces DLSOM, a deep learning framework utilizing stacked autoencoders to analyze the complete somatic mutation landscape of 1,139 liver cancer samples, covering 20,356 protein-coding genes. By transforming high-dimensional mutation data into three low-dimensional features, DLSOM enables robust clustering and identifies five distinct liver cancer subtypes with unique mutational, functional, and biological profiles. Subtypes SC1 and SC2 exhibit higher mutational loads, while SC3 has the lowest, reflecting mutational heterogeneity. Novel and COSMIC-associated mutational signatures reveal subtype-specific molecular mechanisms, including links to hypermutation and chemotherapy resistance. Functional analyses further highlight the biological relevance of each subtype. This comprehensive framework advances precision medicine in liver cancer by enabling the development of subtype-specific diagnostics, biomarkers, and therapies, showcasing the potential of deep learning in addressing cancer complexity. </p>
<blockquote>
<p>è‚ç™Œæ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå…¶é«˜åº¦çš„é—ä¼ å¼‚è´¨æ€§ä½¿å¾—è¯Šæ–­å’Œæ²»ç–—å˜å¾—å¤æ‚ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†DLSOMï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å †å è‡ªç¼–ç å™¨è¿›è¡Œæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ï¼Œç”¨äºåˆ†æ1139ä¸ªè‚ç™Œæ ·æœ¬çš„å®Œæ•´ä½“ç»†èƒçªå˜å›¾è°±ï¼Œè¦†ç›–20356ä¸ªè›‹ç™½è´¨ç¼–ç åŸºå› ã€‚é€šè¿‡å°†é«˜ç»´çªå˜æ•°æ®è½¬æ¢ä¸ºä¸‰ä¸ªä½ç»´ç‰¹å¾ï¼ŒDLSOMèƒ½å¤Ÿå®ç°ç¨³å¥çš„èšç±»ï¼Œå¹¶è¯†åˆ«å‡ºå…·æœ‰ç‹¬ç‰¹çªå˜ã€åŠŸèƒ½å’Œç”Ÿç‰©å­¦ç‰¹å¾çš„äº”ç§ä¸åŒçš„è‚ç™Œäºšå‹ã€‚SC1å’ŒSC2äºšå‹è¡¨ç°å‡ºè¾ƒé«˜çš„çªå˜è´Ÿè·ï¼Œè€ŒSC3çš„çªå˜è´Ÿè·æœ€ä½ï¼Œåæ˜ äº†çªå˜å¼‚è´¨æ€§ã€‚æ–°çš„å’ŒCOSMICç›¸å…³çš„çªå˜ç‰¹å¾æ­ç¤ºäº†äºšå‹ç‰¹å®šçš„åˆ†å­æœºåˆ¶ï¼ŒåŒ…æ‹¬ä¸è¶…çªå˜å’ŒåŒ–ç–—æŠµæŠ—çš„è”ç³»ã€‚åŠŸèƒ½åˆ†æè¿›ä¸€æ­¥çªå‡ºäº†æ¯ä¸ªäºšå‹çš„ç”Ÿç‰©å­¦æ„ä¹‰ã€‚è¿™ä¸€ç»¼åˆæ¡†æ¶é€šè¿‡å¼€å‘é’ˆå¯¹äºšå‹çš„è¯Šæ–­ã€ç”Ÿç‰©æ ‡å¿—å’Œæ²»ç–—æ‰‹æ®µï¼Œæ¨åŠ¨äº†è‚ç™Œç²¾å‡†åŒ»å­¦çš„å‘å±•ï¼Œå±•ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨è§£å†³ç™Œç—‡å¤æ‚æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12214v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶DLSOMåˆ†æè‚è„ç™Œç—‡çš„ä½“ç»†èƒçªå˜æƒ…å†µã€‚é€šè¿‡å¯¹1,139ä¸ªè‚è„ç™Œç—‡æ ·æœ¬çš„20,356ä¸ªè›‹ç™½ç¼–ç åŸºå› è¿›è¡Œå…¨é¢åˆ†æï¼ŒDLSOMèƒ½å°†é«˜ç»´åº¦çš„çªå˜æ•°æ®è½¬åŒ–ä¸ºä¸‰ä¸ªä½ç»´åº¦çš„ç‰¹å¾ï¼Œä»è€Œè¿›è¡Œç¨³å¥çš„èšç±»ï¼Œå¹¶è¯†åˆ«å‡ºå…·æœ‰ç‹¬ç‰¹çªå˜ã€åŠŸèƒ½å’Œç”Ÿç‰©å­¦ç‰¹å¾çš„äº”ç§è‚è„ç™Œç—‡äºšå‹ã€‚è¿™äº›äºšå‹åœ¨çªå˜è´Ÿè·æ–¹é¢å­˜åœ¨å·®å¼‚ï¼Œæ­ç¤ºäº†è‚è„ç™Œç—‡çš„é—ä¼ å¼‚è´¨æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜å‘ç°äº†ä¸€äº›äºšå‹ç‰¹æœ‰çš„åˆ†å­æœºåˆ¶ï¼Œå¦‚è¶…çªå˜å’ŒåŒ–ç–—æŠµæŠ—ç­‰ã€‚æ­¤ç ”ç©¶ä¸ºè‚è„ç™Œç—‡çš„ç²¾å‡†åŒ»ç–—æä¾›äº†çªç ´æ€§çš„è¿›å±•ï¼Œæœ‰æœ›æ¨åŠ¨å¼€å‘é’ˆå¯¹ç‰¹å®šäºšå‹çš„è¯Šæ–­ã€ç”Ÿç‰©æ ‡å¿—ç‰©å’Œç–—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼•å…¥æ·±åº¦å­¦ä¹ æ¡†æ¶DLSOMï¼Œåˆ†æè‚è„ç™Œç—‡çš„ä½“ç»†èƒçªå˜æƒ…å†µã€‚</li>
<li>DLSOMèƒ½è½¬åŒ–é«˜ç»´åº¦çªå˜æ•°æ®ä¸ºä½ç»´åº¦ç‰¹å¾ï¼Œå®ç°ç¨³å¥çš„èšç±»ã€‚</li>
<li>ç ”ç©¶è¯†åˆ«å‡ºäº”ç§å…·æœ‰ä¸åŒçªå˜ã€åŠŸèƒ½å’Œç”Ÿç‰©å­¦ç‰¹å¾çš„è‚è„ç™Œç—‡äºšå‹ã€‚</li>
<li>ä¸åŒäºšå‹åœ¨çªå˜è´Ÿè·ä¸Šå­˜åœ¨å·®å¼‚ï¼Œåæ˜ äº†è‚è„ç™Œç—‡çš„é—ä¼ å¼‚è´¨æ€§ã€‚</li>
<li>å‘ç°äº†ä¸€äº›äºšå‹ç‰¹æœ‰çš„åˆ†å­æœºåˆ¶ï¼Œå¦‚è¶…çªå˜å’ŒåŒ–ç–—æŠµæŠ—ã€‚</li>
<li>DLSOMçš„ç ”ç©¶ä¸ºè‚è„ç™Œç—‡çš„ç²¾å‡†åŒ»ç–—æä¾›äº†çªç ´æ€§çš„è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-585c6c5559884092e66cbf7014865bee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db1a3d7fae00c4410a1f3fdafd4b5a93.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CPath-Omni-A-Unified-Multimodal-Foundation-Model-for-Patch-and-Whole-Slide-Image-Analysis-in-Computational-Pathology"><a href="#CPath-Omni-A-Unified-Multimodal-Foundation-Model-for-Patch-and-Whole-Slide-Image-Analysis-in-Computational-Pathology" class="headerlink" title="CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole   Slide Image Analysis in Computational Pathology"></a>CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole   Slide Image Analysis in Computational Pathology</h2><p><strong>Authors:Yuxuan Sun, Yixuan Si, Chenglu Zhu, Xuan Gong, Kai Zhang, Pingyi Chen, Ye Zhang, Zhongyi Shui, Tao Lin, Lin Yang</strong></p>
<p>The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the integration of learned knowledge across patches and WSIs, and resulting in redundant models. In this work, we introduce CPath-Omni, the first 15-billion-parameter LMM designed to unify both patch and WSI level image analysis, consolidating a variety of tasks at both levels, including classification, visual question answering, captioning, and visual referring prompting. Extensive experiments demonstrate that CPath-Omni achieves state-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42 datasets, outperforming or matching task-specific models trained for individual tasks. Additionally, we develop a specialized pathology CLIP-based visual processor for CPath-Omni, CPath-CLIP, which, for the first time, integrates different vision models and incorporates a large language model as a text encoder to build a more powerful CLIP model, which achieves SOTA performance on nine zero-shot and four few-shot datasets. Our findings highlight CPath-Omniâ€™s ability to unify diverse pathology tasks, demonstrating its potential to streamline and advance the field of foundation model in pathology. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆLMMï¼‰çš„å‡ºç°ç»™ç—…ç†å­¦å¸¦æ¥äº†é‡å¤§è¿›æ­¥ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ†åˆ«è®­ç»ƒè¡¥ä¸çº§åˆ«å’Œå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çº§åˆ«çš„æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†è¡¥ä¸å’ŒWSIsä¹‹é—´å­¦ä¹ çŸ¥è¯†çš„æ•´åˆï¼Œå¹¶å¯¼è‡´äº†å†—ä½™æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CPath-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–ä¸ªè®¾è®¡ç”¨äºç»Ÿä¸€è¡¥ä¸å’ŒWSIçº§åˆ«å›¾åƒåˆ†æçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é›†æˆäº†è¿™ä¸¤ä¸ªçº§åˆ«çš„å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€è§†è§‰é—®ç­”ã€æè¿°å’Œè§†è§‰æç¤ºå¼•å¯¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCPath-Omniåœ¨42ä¸ªæ•°æ®é›†çš„39ä¸ªæ•°æ®é›†ä¸Šçš„ä¸ƒä¸ªä¸åŒä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºæˆ–ä¸é’ˆå¯¹ä¸ªåˆ«ä»»åŠ¡è®­ç»ƒçš„ç‰¹å®šä»»åŠ¡æ¨¡å‹ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºCPath-Omniå¼€å‘äº†ä¸€ä¸ªåŸºäºCLIPçš„ä¸“ç”¨ç—…ç†è§†è§‰å¤„ç†å™¨CPath-CLIPï¼Œå®ƒé¦–æ¬¡æ•´åˆäº†ä¸åŒçš„è§†è§‰æ¨¡å‹ï¼Œå¹¶çº³å…¥å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œä»¥æ„å»ºæ›´å¼ºå¤§çš„CLIPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¹ä¸ªé›¶æ ·æœ¬å’Œå››ä¸ªå°‘æ ·æœ¬æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†CPath-Omniç»Ÿä¸€å¤šæ ·åŒ–ç—…ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨åŠ¨ç—…ç†å­¦é¢†åŸŸåŸºç¡€æ¨¡å‹å‘å±•çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12077v1">PDF</a> 22 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‡ºç°ï¼Œç—…ç†å­¦é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ†åˆ«è®­ç»ƒè¡¥ä¸çº§åˆ«å’Œå…¨å¹»ç¯ç‰‡å›¾åƒçº§åˆ«çš„æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†è·¨è¡¥ä¸å’Œå…¨å¹»ç¯ç‰‡å›¾åƒçš„çŸ¥è¯†æ•´åˆï¼Œå¹¶å¯¼è‡´æ¨¡å‹å†—ä½™ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†CPath-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç»Ÿä¸€è¡¥ä¸å’ŒWSIçº§åˆ«å›¾åƒåˆ†æçš„é¦–ä¸ªæ‹¥æœ‰15äº¿å‚æ•°çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚å®ƒèåˆäº†å„çº§åˆ«çš„å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€è§†è§‰é—®ç­”ã€æè¿°å’Œè§†è§‰å¼•ç”¨æç¤ºç­‰ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒCPath-Omniåœ¨42ä¸ªæ•°æ®é›†çš„å…¶ä¸­39ä¸ªæ•°æ®é›†çš„ä¸ƒä¸ªä¸åŒä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºCPath-Omniå¼€å‘äº†ä¸€ä¸ªåŸºäºCLIPçš„ä¸“ç”¨ç—…ç†è§†è§‰å¤„ç†å™¨CPath-CLIPï¼Œé¦–æ¬¡æ•´åˆäº†ä¸åŒçš„è§†è§‰æ¨¡å‹ï¼Œå¹¶å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œæ„å»ºäº†æ›´å¼ºå¤§çš„CLIPæ¨¡å‹ï¼Œåœ¨ä¹ä¸ªé›¶æ ·æœ¬å’Œå››ä¸ªå°‘æ ·æœ¬æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†CPath-Omniç»Ÿä¸€å¤šç§ç—…ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨ç—…ç†å­¦é¢†åŸŸåŸºç¡€æ¨¡å‹åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‡ºç°åœ¨ç—…ç†å­¦é¢†åŸŸå¸¦æ¥æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ä»¥å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨å•ç‹¬è®­ç»ƒè¡¥ä¸çº§åˆ«å’Œå…¨å¹»ç¯ç‰‡å›¾åƒçº§åˆ«çš„æ¨¡å‹ï¼Œå¯¼è‡´çŸ¥è¯†æ•´åˆå—é™å’Œæ¨¡å‹å†—ä½™ã€‚</li>
<li>CPath-Omniæ˜¯é¦–ä¸ªç»Ÿä¸€è¡¥ä¸å’ŒWSIçº§åˆ«å›¾åƒåˆ†æçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ•´åˆäº†å„çº§åˆ«çš„å¤šç§ä»»åŠ¡ã€‚</li>
<li>CPath-Omniåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ä¸ƒä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>CPath-CLIPæ˜¯ä¸“é—¨ä¸ºCPath-Omniå¼€å‘çš„ç—…ç†CLIPè§†è§‰å¤„ç†å™¨ï¼Œèåˆäº†ä¸åŒçš„è§†è§‰æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¢å¼ºäº†CLIPæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>CPath-CLIPåœ¨å¤šä¸ªé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40de384d2746dc54b07326c3069a9984.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70eee2a1e7dbe5812606dcab76373ed1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3362b0067b35a06feb442ad7607822f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6d2e833df182c1666107f7b0044b3f3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LLM-RG4-Flexible-and-Factual-Radiology-Report-Generation-across-Diverse-Input-Contexts"><a href="#LLM-RG4-Flexible-and-Factual-Radiology-Report-Generation-across-Diverse-Input-Contexts" class="headerlink" title="LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse   Input Contexts"></a>LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse   Input Contexts</h2><p><strong>Authors:Zhuhao Wang, Yihua Sun, Zihan Li, Xuan Yang, Fang Chen, Hongen Liao</strong></p>
<p>Drafting radiology reports is a complex task requiring flexibility, where radiologists tail content to available information and particular clinical demands. However, most current radiology report generation (RRG) models are constrained to a fixed task paradigm, such as predicting the full &#96;&#96;findingâ€™â€™ section from a single image, inherently involving a mismatch between inputs and outputs. The trained models lack the flexibility for diverse inputs and could generate harmful, input-agnostic hallucinations. To bridge the gap between current RRG models and the clinical demands in practice, we first develop a data generation pipeline to create a new MIMIC-RG4 dataset, which considers four common radiology report drafting scenarios and has perfectly corresponded input and output. Secondly, we propose a novel large language model (LLM) based RRG framework, namely LLM-RG4, which utilizes LLMâ€™s flexible instruction-following capabilities and extensive general knowledge. We further develop an adaptive token fusion module that offers flexibility to handle diverse scenarios with different input combinations, while minimizing the additional computational burden associated with increased input volumes. Besides, we propose a token-level loss weighting strategy to direct the modelâ€™s attention towards positive and uncertain descriptions. Experimental results demonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical efficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR datasets. We quantitatively demonstrate that our model has minimal input-agnostic hallucinations, whereas current open-source models commonly suffer from this problem. </p>
<blockquote>
<p>èµ·è‰æ”¾å°„å­¦æŠ¥å‘Šæ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦çµæ´»åº¦ï¼Œå…¶ä¸­æ”¾å°„ç§‘åŒ»ç”Ÿæ ¹æ®ç°æœ‰ä¿¡æ¯å’Œç‰¹å®šä¸´åºŠéœ€æ±‚å®šåˆ¶å†…å®¹ã€‚ç„¶è€Œï¼Œå½“å‰å¤§å¤šæ•°çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ¨¡å‹å—é™äºå›ºå®šçš„ä»»åŠ¡èŒƒå¼ï¼Œä¾‹å¦‚ä»…æ ¹æ®å•å¼ å›¾åƒé¢„æµ‹å®Œæ•´çš„â€œå‘ç°â€éƒ¨åˆ†ï¼Œè¿™æœ¬è´¨ä¸Šå¯¼è‡´äº†è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„ä¸åŒ¹é…ã€‚è®­ç»ƒè¿‡çš„æ¨¡å‹åœ¨åº”å¯¹ä¸åŒè¾“å…¥æ—¶ç¼ºä¹çµæ´»æ€§ï¼Œå¹¶å¯èƒ½äº§ç”Ÿæœ‰å®³çš„ã€ä¸è¾“å…¥æ— å…³çš„å¹»è§‰ã€‚ä¸ºäº†å¼¥åˆå½“å‰RRGæ¨¡å‹ä¸å®é™…åº”ç”¨ä¸­ä¸´åºŠéœ€æ±‚ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªæ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆ›å»ºäº†æ–°çš„MIMIC-RG4æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†è€ƒè™‘äº†å››ç§å¸¸è§çš„æ”¾å°„å­¦æŠ¥å‘Šèµ·è‰åœºæ™¯ï¼Œå¹¶å®ç°äº†å®Œç¾çš„è¾“å…¥å’Œè¾“å‡ºå¯¹åº”ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„RRGæ¡†æ¶ï¼Œå³LLM-RG4ï¼Œå®ƒåˆ©ç”¨LLMçš„çµæ´»æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œå¹¿æ³›çš„ä¸€èˆ¬çŸ¥è¯†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªè‡ªé€‚åº”ä»¤ç‰Œèåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿæä¾›çµæ´»æ€§ï¼Œä»¥å¤„ç†ä¸åŒåœºæ™¯ä¸‹çš„ä¸åŒè¾“å…¥ç»„åˆï¼ŒåŒæ—¶æœ€å°åŒ–å¢åŠ è¾“å…¥é‡æ‰€å¸¦æ¥çš„é¢å¤–è®¡ç®—è´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¤ç‰Œçº§æŸå¤±åŠ æƒç­–ç•¥ï¼Œä»¥å¼•å¯¼æ¨¡å‹å…³æ³¨æ­£é¢å’Œä¸ç¡®å®šçš„æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM-RG4åœ¨MIMIC-RG4å’ŒMIMIC-CXRæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„ä¸´åºŠæ•ˆç‡å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆæ€§èƒ½ã€‚æˆ‘ä»¬å®šé‡è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰æœ€å°çš„ä¸è¾“å…¥æ— å…³çš„å¹»è§‰ï¼Œè€Œå½“å‰å¼€æºæ¨¡å‹é€šå¸¸å­˜åœ¨è¿™ä¸ªé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12001v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆã€‚ç”±äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ¨¡å‹å—é™äºå›ºå®šçš„ä»»åŠ¡æ¨¡å¼ï¼Œå¯¼è‡´åœ¨å®é™…åº”ç”¨ä¸­ä¸ä¸´åºŠéœ€æ±‚å­˜åœ¨å·®è·ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªå…¨æ–°çš„æ•°æ®ç”Ÿæˆç®¡é“æ¥æ„å»ºMIMIC-RG4æ•°æ®é›†ï¼Œå¹¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡ºäº†ä¸€ä¸ªçµæ´»çš„RRGæ¡†æ¶â€”â€”LLM-RG4ã€‚è¯¥æ¡†æ¶èƒ½åº”å¯¹ä¸åŒçš„è¾“å…¥ç»„åˆåœºæ™¯ï¼ŒåŒæ—¶é€šè¿‡ä¸€ä¸ªè‡ªé€‚åº”çš„ä»¤ç‰Œèåˆæ¨¡å—å‡å°‘äº†è®¡ç®—è´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§ä»¤ç‰Œçº§æŸå¤±åŠ æƒç­–ç•¥ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨æ­£é¢å’Œä¸ç¡®å®šçš„æè¿°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLM-RG4åœ¨MIMIC-RG4å’ŒMIMIC-CXRæ•°æ®é›†ä¸Šè¾¾åˆ°äº†ä¸´åºŠæ•ˆç‡å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆçš„æœ€æ–°æ°´å¹³ï¼Œä¸”æ˜¾è‘—å‡å°‘äº†è¾“å…¥æ— å…³çš„å¹»è§‰ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ¨¡å‹å—é™äºå›ºå®šä»»åŠ¡æ¨¡å¼ï¼Œå¯¼è‡´ä¸ä¸´åºŠéœ€æ±‚ä¸åŒ¹é…ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ç”Ÿæˆç®¡é“åˆ›å»ºMIMIC-RG4æ•°æ®é›†ï¼Œæ¶µç›–å››ç§å¸¸è§çš„æ”¾å°„å­¦æŠ¥å‘Šèµ·è‰åœºæ™¯ã€‚</li>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡ºäº†LLM-RG4æ¡†æ¶ï¼Œå…·æœ‰çµæ´»çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œå¹¿æ³›çš„ä¸€èˆ¬çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”ä»¤ç‰Œèåˆæ¨¡å—å¤„ç†ä¸åŒçš„è¾“å…¥ç»„åˆåœºæ™¯ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>é‡‡ç”¨ä»¤ç‰Œçº§æŸå¤±åŠ æƒç­–ç•¥ï¼Œä½¿æ¨¡å‹æ›´å…³æ³¨æ­£é¢å’Œä¸ç¡®å®šçš„æè¿°ã€‚</li>
<li>å®éªŒè¯æ˜LLM-RG4åœ¨ä¸´åºŠæ•ˆç‡å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆæ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6972d6bdf291897fced313d2dea7e841.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62816a2ba15fa317d81e7a9f8172f741.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f7af6a5e5597df43506834d14cf10c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d95455ae4a579db26ebbe538338b6ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3f6755e4aaaf1399331c0d1cd2ca262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a85827dca3c64dc37d1ff154125f4992.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37342a340f10ad587995002df75a2ddc.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="From-2D-CAD-Drawings-to-3D-Parametric-Models-A-Vision-Language-Approach"><a href="#From-2D-CAD-Drawings-to-3D-Parametric-Models-A-Vision-Language-Approach" class="headerlink" title="From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach"></a>From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach</h2><p><strong>Authors:Xilin Wang, Jia Zheng, Yuanchao Hu, Hao Zhu, Qian Yu, Zihan Zhou</strong></p>
<p>In this paper, we present CAD2Program, a new method for reconstructing 3D parametric models from 2D CAD drawings. Our proposed method is inspired by recent successes in vision-language models (VLMs), and departs from traditional methods which rely on task-specific data representations and&#x2F;or algorithms. Specifically, on the input side, we simply treat the 2D CAD drawing as a raster image, regardless of its original format, and encode the image with a standard ViT model. We show that such an encoding scheme achieves competitive performance against existing methods that operate on vector-graphics inputs, while imposing substantially fewer restrictions on the 2D drawings. On the output side, our method auto-regressively predicts a general-purpose language describing 3D parametric models in text form. Compared to other sequence modeling methods for CAD which use domain-specific sequence representations with fixed-size slots, our text-based representation is more flexible, and can be easily extended to arbitrary geometric entities and semantic or functional properties. Experimental results on a large-scale dataset of cabinet models demonstrate the effectiveness of our method. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CAD2Programï¼Œè¿™æ˜¯ä¸€ç§ä»äºŒç»´CADå›¾çº¸é‡å»ºä¸‰ç»´å‚æ•°æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å—åˆ°æœ€è¿‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æˆåŠŸçš„å¯å‘ï¼Œä¸ä¼ ç»Ÿçš„ä¾èµ–äºç‰¹å®šä»»åŠ¡æ•°æ®è¡¨ç¤ºå’Œ&#x2F;æˆ–ç®—æ³•çš„æ¨¡å‹æˆªç„¶ä¸åŒã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è¾“å…¥æ–¹é¢ï¼Œæˆ‘ä»¬ç®€å•åœ°å°†äºŒç»´CADå›¾çº¸è§†ä¸ºå…‰æ …å›¾åƒï¼Œæ— è®ºå…¶åŸå§‹æ ¼å¼å¦‚ä½•ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†çš„ViTæ¨¡å‹å¯¹å…¶è¿›è¡Œç¼–ç ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ç§ç¼–ç æ–¹æ¡ˆä¸åœ¨çŸ¢é‡å›¾å½¢è¾“å…¥ä¸Šè¿è¡Œçš„ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶å¯¹äºŒç»´å›¾çº¸çš„çº¦æŸå¤§å¤§å‡å°‘ã€‚åœ¨è¾“å‡ºæ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è‡ªå›å½’é¢„æµ‹ä»¥æ–‡æœ¬å½¢å¼æè¿°ä¸‰ç»´å‚æ•°æ¨¡å‹çš„é€šç”¨è¯­è¨€ã€‚ä¸å…¶ä»–ä½¿ç”¨å…·æœ‰å›ºå®šå¤§å°æ’æ§½çš„ç‰¹å®šé¢†åŸŸåºåˆ—è¡¨ç¤ºçš„CADåºåˆ—å»ºæ¨¡æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–‡æœ¬è¡¨ç¤ºæ›´åŠ çµæ´»ï¼Œå¹¶ä¸”å¯è½»æ¾æ‰©å±•åˆ°ä»»æ„å‡ ä½•å®ä½“å’Œè¯­ä¹‰æˆ–åŠŸèƒ½å±æ€§ã€‚åœ¨å¤§å‹æ©±æŸœæ¨¡å‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11892v2">PDF</a> To Appear in AAAI 2025. The project page is at   <a target="_blank" rel="noopener" href="https://manycore-research.github.io/CAD2Program">https://manycore-research.github.io/CAD2Program</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºCAD2Programæ–¹æ³•ï¼Œé€šè¿‡å€Ÿé‰´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æˆåŠŸç»éªŒï¼Œå®ç°ä»äºŒç»´CADå›¾çº¸åˆ°ä¸‰ç»´å‚æ•°æ¨¡å‹çš„é‡å»ºã€‚è¯¥æ–¹æ³•å°†äºŒç»´CADå›¾çº¸è§†ä¸ºå›¾åƒè¾“å…¥ï¼Œä½¿ç”¨æ ‡å‡†ViTæ¨¡å‹è¿›è¡Œç¼–ç ï¼Œèƒ½å¤Ÿå…¼å®¹ä¸åŒæ ¼å¼çš„å›¾çº¸ï¼Œé¢„æµ‹è¾“å‡ºæè¿°ä¸‰ç»´å‚æ•°æ¨¡å‹çš„æ–‡æœ¬ä¿¡æ¯ã€‚ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å’Œå…¶ä»–åºåˆ—å»ºæ¨¡æ–¹æ³•ï¼ŒCAD2Programæ›´å…·çµæ´»æ€§ï¼Œå¯ä»¥è½»æ˜“æ‰©å±•åˆ°ä»»æ„å‡ ä½•å®ä½“å’Œè¯­ä¹‰åŠŸèƒ½å±æ€§ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤§å‹æ©±æŸœæ¨¡å‹æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CAD2Programæ˜¯ä¸€ç§åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä»äºŒç»´CADå›¾çº¸é‡å»ºä¸‰ç»´å‚æ•°æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚</li>
<li>CADå›¾çº¸ä»¥å›¾åƒå½¢å¼è¾“å…¥ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†ViTæ¨¡å‹è¿›è¡Œç¼–ç ï¼Œå…¼å®¹å¤šç§æ ¼å¼ã€‚</li>
<li>è¾“å‡ºä¸ºæè¿°ä¸‰ç»´å‚æ•°æ¨¡å‹çš„æ–‡æœ¬ä¿¡æ¯ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å…·çµæ´»æ€§ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿæ‰©å±•åˆ°ä»»æ„å‡ ä½•å®ä½“å’Œè¯­ä¹‰åŠŸèƒ½å±æ€§ã€‚</li>
<li>åœ¨å¤§å‹æ©±æŸœæ¨¡å‹æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>CAD2Programå—åˆ°è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯å‘ï¼Œä¸ç°æœ‰çš„CADå¤„ç†æ–¹æ³•ä¸åŒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-699808d85c570298cd34317940a6a711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ad0ee65a0f5e345198656bf70a4bf6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e2952627374f1b1221a5d4afa429f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14f3dc0055e7abf63827e934fdf0f150.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83c9578f6b88c8a88bc39d0b4f104175.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b46a0f8d039c10dab7a6b6ba501dc5b5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Ensemble-Learning-and-3D-Pix2Pix-for-Comprehensive-Brain-Tumor-Analysis-in-Multimodal-MRI"><a href="#Ensemble-Learning-and-3D-Pix2Pix-for-Comprehensive-Brain-Tumor-Analysis-in-Multimodal-MRI" class="headerlink" title="Ensemble Learning and 3D Pix2Pix for Comprehensive Brain Tumor Analysis   in Multimodal MRI"></a>Ensemble Learning and 3D Pix2Pix for Comprehensive Brain Tumor Analysis   in Multimodal MRI</h2><p><strong>Authors:Ramy A. Zeineldin, Franziska Mathis-Ullrich</strong></p>
<p>Motivated by the need for advanced solutions in the segmentation and inpainting of glioma-affected brain regions in multi-modal magnetic resonance imaging (MRI), this study presents an integrated approach leveraging the strengths of ensemble learning with hybrid transformer models and convolutional neural networks (CNNs), alongside the innovative application of 3D Pix2Pix Generative Adversarial Network (GAN). Our methodology combines robust tumor segmentation capabilities, utilizing axial attention and transformer encoders for enhanced spatial relationship modeling, with the ability to synthesize biologically plausible brain tissue through 3D Pix2Pix GAN. This integrated approach addresses the BraTS 2023 cluster challenges by offering precise segmentation and realistic inpainting, tailored for diverse tumor types and sub-regions. The results demonstrate outstanding performance, evidenced by quantitative evaluations such as the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD95) for segmentation, and Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean-Square Error (MSE) for inpainting. Qualitative assessments further validate the high-quality, clinically relevant outputs. In conclusion, this study underscores the potential of combining advanced machine learning techniques for comprehensive brain tumor analysis, promising significant advancements in clinical decision-making and patient care within the realm of medical imaging. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å—åˆ°å¤šæ¨¡æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­èƒ¶è´¨æ¯ç»†èƒç˜¤å½±å“çš„å¤§è„‘åŒºåŸŸåˆ†å‰²å’Œä¿®å¤éœ€è¦å…ˆè¿›è§£å†³æ–¹æ¡ˆçš„é©±åŠ¨ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆé›†æˆå­¦ä¹ ã€æ··åˆTransformeræ¨¡å‹å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ–¹æ³•ï¼Œå¹¶åˆ›æ–°æ€§åœ°åº”ç”¨äº†3D Pix2Pixç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åˆ©ç”¨è½´å‘æ³¨æ„åŠ›å’ŒTransformerç¼–ç å™¨è¿›è¡Œå¢å¼ºç©ºé—´å…³ç³»å»ºæ¨¡çš„ç¨³å¥è‚¿ç˜¤åˆ†å‰²èƒ½åŠ›ï¼Œä»¥åŠé€šè¿‡3D Pix2Pix GANåˆæˆç”Ÿç‰©ä¸Šåˆç†çš„è„‘ç»„ç»‡çš„èƒ½åŠ›ã€‚è¿™ä¸€ç»¼åˆæ–¹æ³•é€šè¿‡æä¾›ç²¾ç¡®çš„åˆ†å‰²å’Œé€¼çœŸçš„ä¿®å¤ï¼Œè§£å†³äº†BraTS 2023é›†ç¾¤æŒ‘æˆ˜ï¼Œé€‚ç”¨äºå¤šç§è‚¿ç˜¤ç±»å‹å’Œå­åŒºåŸŸã€‚ç»“æœè¯æ˜äº†å…¶å“è¶Šæ€§èƒ½ï¼Œå®šé‡è¯„ä¼°å¦‚ç”¨äºåˆ†å‰²çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰å’ŒHausdorffè·ç¦»ï¼ˆHD95ï¼‰ï¼Œä»¥åŠç”¨äºä¿®å¤çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰ã€å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚å®šæ€§è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†é«˜è´¨é‡ã€ä¸´åºŠç›¸å…³çš„è¾“å‡ºã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶å¼ºè°ƒäº†ç»“åˆå…ˆè¿›æœºå™¨å­¦ä¹ æŠ€æœ¯è¿›è¡Œå…¨é¢çš„è„‘è‚¿ç˜¤åˆ†æçš„å¯èƒ½æ€§ï¼Œæœ‰æœ›åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„ä¸´åºŠå†³ç­–å’Œæ‚£è€…æŠ¤ç†æ–¹é¢å–å¾—é‡å¤§è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11849v1">PDF</a> Accepted at the MICCAI BraTS Challenge 2023</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­èƒ¶è´¨ç˜¤å—ç´¯è„‘åŒºåŸŸåˆ†å‰²å’Œä¿®å¤çš„éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆé›†æˆå­¦ä¹ ã€æ··åˆå˜å‹å™¨æ¨¡å‹ã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä»¥åŠ3D Pix2Pixç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„ç»¼åˆæ€§æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ä»…å…·å¤‡åˆ©ç”¨è½´å‘æ³¨æ„åŠ›å’Œå˜å‹å™¨ç¼–ç å™¨è¿›è¡Œå¢å¼ºç©ºé—´å…³ç³»å»ºæ¨¡çš„ç¨³å¥è‚¿ç˜¤åˆ†å‰²èƒ½åŠ›ï¼Œè€Œä¸”èƒ½å¤Ÿé€šè¿‡3D Pix2Pix GANåˆæˆå…·æœ‰ç”Ÿç‰©å­¦å¯è¡Œæ€§çš„è„‘ç»„ç»‡ã€‚è¯¥ç»¼åˆæ€§æ–¹æ³•é’ˆå¯¹BraTS 2023é›†ç¾¤æŒ‘æˆ˜ï¼Œæä¾›ç²¾ç¡®çš„åˆ†å‰²å’Œé€¼çœŸçš„ä¿®å¤ï¼Œé€‚ç”¨äºå„ç§è‚¿ç˜¤ç±»å‹å’Œå­åŒºåŸŸã€‚ç»“æœé€šè¿‡å®šé‡è¯„ä¼°å’Œå®šæ€§è¯„ä¼°è¯æ˜äº†å…¶å“è¶Šæ€§èƒ½ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†ç»“åˆå…ˆè¿›æœºå™¨å­¦ä¹ æŠ€æœ¯è¿›è¡Œå…¨é¢çš„è„‘è‚¿ç˜¤åˆ†æçš„æ½œåŠ›ï¼Œæœ‰æœ›åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„ä¸´åºŠå†³ç­–å’Œæ‚£è€…æŠ¤ç†æ–¹é¢å®ç°é‡å¤§è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¤šç§æœºå™¨å­¦ä¹ æŠ€æœ¯çš„ç»¼åˆæ€§æ–¹æ³•ï¼Œç”¨äºå¤šæ¨¡æ€MRIä¸­èƒ¶è´¨ç˜¤è„‘åŒºåŸŸçš„åˆ†å‰²å’Œä¿®å¤ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†é›†æˆå­¦ä¹ ã€æ··åˆå˜å‹å™¨æ¨¡å‹ã€CNNä»¥åŠ3D Pix2Pix GANã€‚</li>
<li>è¯¥æ–¹æ³•å…·å¤‡ç¨³å¥çš„è‚¿ç˜¤åˆ†å‰²èƒ½åŠ›ï¼Œå¹¶å¯é€šè¿‡è½´å‘æ³¨æ„åŠ›å’Œå˜å‹å™¨ç¼–ç å™¨è¿›è¡Œç©ºé—´å…³ç³»å»ºæ¨¡ã€‚</li>
<li>é€šè¿‡åˆæˆç”Ÿç‰©å­¦å¯è¡Œçš„è„‘ç»„ç»‡ï¼Œè¯¥æ–¹æ³•å…·æœ‰å¼ºå¤§çš„ä¿®å¤èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨BraTS 2023é›†ç¾¤æŒ‘æˆ˜ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›ç²¾ç¡®åˆ†å‰²å’Œé€¼çœŸä¿®å¤ï¼Œé€‚ç”¨äºå¤šç§è‚¿ç˜¤ç±»å‹å’Œå­åŒºåŸŸã€‚</li>
<li>ç»“æœé€šè¿‡å®šé‡è¯„ä¼°ï¼ˆå¦‚Diceç›¸ä¼¼ç³»æ•°ã€Hausdorffè·ç¦»ç­‰ï¼‰å’Œå®šæ€§è¯„ä¼°å¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9825be7cab399ba7264ffc74af4c493.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73ce182a270e64f920ec72e4f611d254.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MaskCLIP-A-Mask-Based-CLIP-Fine-tuning-Framework-for-Open-Vocabulary-Image-Segmentation"><a href="#MaskCLIP-A-Mask-Based-CLIP-Fine-tuning-Framework-for-Open-Vocabulary-Image-Segmentation" class="headerlink" title="MaskCLIP++: A Mask-Based CLIP Fine-tuning Framework for Open-Vocabulary   Image Segmentation"></a>MaskCLIP++: A Mask-Based CLIP Fine-tuning Framework for Open-Vocabulary   Image Segmentation</h2><p><strong>Authors:Quan-Sheng Zeng, Yunheng Li, Daquan Zhou, Guanbin Li, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>Open-vocabulary image segmentation has been advanced through the synergy between mask generators and vision-language models like Contrastive Language-Image Pre-training (CLIP). Previous approaches focus on generating masks while aligning mask features with text embeddings during training. In this paper, we observe that relying on generated low-quality masks can weaken the alignment of vision and language in regional representations. This motivates us to present a new fine-tuning framework, named MaskCLIP++, which uses ground-truth masks instead of generated masks to enhance the mask classification capability of CLIP. Due to the limited diversity of image segmentation datasets with mask annotations, we propose incorporating a consistency alignment constraint during fine-tuning, which alleviates categorical bias toward the fine-tuning dataset. After low-cost fine-tuning, combining with the mask generator in previous state-of-the-art mask-based open vocabulary segmentation methods, we achieve performance improvements of +1.7, +2.3, +2.1, +3.1, and +0.3 mIoU on the A-847, PC-459, A-150, PC-59, and PAS-20 datasets, respectively. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²æŠ€æœ¯é€šè¿‡æ©è†œç”Ÿæˆå™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ï¼‰ä¹‹é—´çš„ååŒä½œç”¨å–å¾—äº†è¿›å±•ã€‚ä»¥å‰çš„æ–¹æ³•ä¾§é‡äºç”Ÿæˆæ©è†œï¼ŒåŒæ—¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†æ©è†œç‰¹å¾ä¸æ–‡æœ¬åµŒå…¥å¯¹é½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¾èµ–ç”Ÿæˆçš„ä½è´¨é‡æ©è†œä¼šå‰Šå¼±åŒºåŸŸè¡¨ç¤ºä¸­çš„è§†è§‰å’Œè¯­è¨€å¯¹é½ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºä¸€ä¸ªæ–°çš„å¾®è°ƒæ¡†æ¶ï¼Œåä¸ºMaskCLIP++ï¼Œå®ƒä½¿ç”¨çœŸå®æ©è†œä»£æ›¿ç”Ÿæˆçš„æ©è†œï¼Œä»¥æé«˜CLIPçš„æ©è†œåˆ†ç±»èƒ½åŠ›ã€‚ç”±äºå¸¦æœ‰æ©è†œæ ‡æ³¨çš„å›¾åƒåˆ†å‰²æ•°æ®é›†å¤šæ ·æ€§æœ‰é™ï¼Œæˆ‘ä»¬æå‡ºåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥ä¸€è‡´æ€§å¯¹é½çº¦æŸï¼Œè¿™å‡è½»äº†å¯¹å¾®è°ƒæ•°æ®é›†çš„ç±»åˆ«åè§ã€‚ç»è¿‡ä½æˆæœ¬çš„å¾®è°ƒåï¼Œç»“åˆå…ˆå‰æœ€å…ˆè¿›çš„åŸºäºæ©è†œå¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•çš„æ©è†œç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬åœ¨A-847ã€PC-459ã€A-150ã€PC-59å’ŒPAS-20æ•°æ®é›†ä¸Šçš„mIoUåˆ†åˆ«æé«˜äº†+1.7ã€+2.3ã€+2.1ã€+3.1å’Œ+0.3ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11464v1">PDF</a> 20 pages, 8 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMaskCLIP++çš„æ–°å¾®è°ƒæ¡†æ¶ï¼Œå®ƒä½¿ç”¨çœŸå®æ©è†œä»£æ›¿ç”Ÿæˆçš„æ©è†œä»¥æå‡CLIPçš„æ©è†œåˆ†ç±»èƒ½åŠ›ã€‚ä¸ºæé«˜å›¾åƒåˆ†å‰²æ•°æ®é›†æ©è†œæ ‡æ³¨çš„å¤šæ ·æ€§ï¼Œå¼•å…¥ä¸€è‡´æ€§å¯¹é½çº¦æŸè¿›è¡Œå¾®è°ƒã€‚ç»“åˆå…ˆè¿›çš„åŸºäºæ©è†œå¼€æ”¾è¯æ±‡è¡¨åˆ†å‰²æ–¹æ³•ä¸­çš„æ©è†œç”Ÿæˆå™¨ï¼Œå¯åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaskCLIP++ä½¿ç”¨çœŸå®æ©è†œæ›¿ä»£ç”Ÿæˆçš„æ©è†œï¼Œä»¥å¢å¼ºCLIPçš„æ©è†œåˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>çœŸå®æ©è†œçš„ä½¿ç”¨æœ‰åŠ©äºæé«˜å›¾åƒåˆ†å‰²çš„ç²¾åº¦å’Œæ€§èƒ½ã€‚</li>
<li>ä¸€è‡´æ€§å¯¹é½çº¦æŸåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¢«å¼•å…¥ï¼Œä»¥æé«˜å›¾åƒåˆ†å‰²æ•°æ®é›†çš„å¤šæ ·æ€§å¹¶ç¼“è§£åˆ†ç±»åå‘é—®é¢˜ã€‚</li>
<li>MaskCLIP++åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œä½æˆæœ¬çš„å¾®è°ƒã€‚</li>
<li>ç»“åˆå…ˆè¿›çš„åŸºäºæ©è†œå¼€æ”¾è¯æ±‡è¡¨åˆ†å‰²æ–¹æ³•çš„æ©è†œç”Ÿæˆå™¨ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ï¼ˆå¦‚A-847ã€PC-459ã€A-150ã€PC-59å’ŒPAS-20ï¼‰ä¸Šå®ç°äº†æ€§èƒ½æ”¹è¿›ï¼ŒåŒ…æ‹¬mIoUæŒ‡æ ‡çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-871985a6d63254b8e9444ba3cf5ca5f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80c05a729c27f44fbcd568b4eac4cbe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20d6f62612372e9ccd67cfdf8959dbec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ce850d0612e0ef2833af25f3f12743.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e459efecb94ed01fc6976d02caba439.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1><h2 id="HResFormer-Hybrid-Residual-Transformer-for-Volumetric-Medical-Image-Segmentation"><a href="#HResFormer-Hybrid-Residual-Transformer-for-Volumetric-Medical-Image-Segmentation" class="headerlink" title="HResFormer: Hybrid Residual Transformer for Volumetric Medical Image   Segmentation"></a>HResFormer: Hybrid Residual Transformer for Volumetric Medical Image   Segmentation</h2><p><strong>Authors:Sucheng Ren, Xiaomeng Li</strong></p>
<p>Vision Transformer shows great superiority in medical image segmentation due to the ability in learning long-range dependency. For medical image segmentation from 3D data, such as computed tomography (CT), existing methods can be broadly classified into 2D-based and 3D-based methods. One key limitation in 2D-based methods is that the intra-slice information is ignored, while the limitation in 3D-based methods is the high computation cost and memory consumption, resulting in a limited feature representation for inner-slice information. During the clinical examination, radiologists primarily use the axial plane and then routinely review both axial and coronal planes to form a 3D understanding of anatomy. Motivated by this fact, our key insight is to design a hybrid model which can first learn fine-grained inner-slice information and then generate a 3D understanding of anatomy by incorporating 3D information. We present a novel \textbf{H}ybrid \textbf{Res}idual trans\textbf{Former} \textbf{(HResFormer)} for 3D medical image segmentation. Building upon standard 2D and 3D Transformer backbones, HResFormer involves two novel key designs: \textbf{(1)} a \textbf{H}ybrid \textbf{L}ocal-\textbf{G}lobal fusion \textbf{M}odule \textbf{(HLGM)} to effectively and adaptively fuse inner-slice information from 2D Transformer and intra-slice information from 3D volumes for 3D Transformer with local fine-grained and global long-range representation. \textbf{(2)} a residual learning of the hybrid model, which can effectively leverage the inner-slice and intra-slice information for better 3D understanding of anatomy. Experiments show that our HResFormer outperforms prior art on widely-used medical image segmentation benchmarks. This paper sheds light on an important but neglected way to design Transformers for 3D medical image segmentation. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œç”±äºå…·å¤‡å­¦ä¹ é•¿è·ç¦»ä¾èµ–å…³ç³»çš„èƒ½åŠ›ï¼ŒVision Transformerè¡¨ç°å‡ºäº†å·¨å¤§çš„ä¼˜è¶Šæ€§ã€‚å¯¹äºæ¥è‡ªè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç­‰3Dæ•°æ®çš„åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œç°æœ‰æ–¹æ³•å¤§è‡´å¯åˆ†ä¸ºåŸºäºäºŒç»´çš„æ–¹æ³•å’ŒåŸºäºä¸‰ç»´çš„æ–¹æ³•ã€‚åŸºäºäºŒç»´çš„æ–¹æ³•çš„ä¸€ä¸ªå…³é”®å±€é™æ€§åœ¨äºå¿½ç•¥äº†åˆ‡ç‰‡å†…çš„ä¿¡æ¯ï¼Œè€ŒåŸºäºä¸‰ç»´çš„æ–¹æ³•çš„å±€é™æ€§åœ¨äºè®¡ç®—æˆæœ¬é«˜å’Œå†…å­˜æ¶ˆè€—å¤§ï¼Œå¯¼è‡´å¯¹åˆ‡ç‰‡å†…ä¿¡æ¯çš„ç‰¹å¾è¡¨ç¤ºæœ‰é™ã€‚åœ¨ä¸´åºŠæ£€æŸ¥è¿‡ç¨‹ä¸­ï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿä¸»è¦ä½¿ç”¨è½´å¹³é¢ï¼Œç„¶åå¸¸è§„åœ°åŒæ—¶æŸ¥çœ‹è½´å¹³é¢å’Œå† çŠ¶å¹³é¢ï¼Œä»¥å½¢æˆå¯¹è§£å‰–ç»“æ„çš„ä¸‰ç»´ç†è§£ã€‚å—æ­¤äº‹å®çš„å¯å‘ï¼Œæˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯è®¾è®¡ä¸€ç§æ··åˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é¦–å…ˆå­¦ä¹ ç²¾ç»†çš„åˆ‡ç‰‡å†…ä¿¡æ¯ï¼Œç„¶åé€šè¿‡ç»“åˆä¸‰ç»´ä¿¡æ¯äº§ç”Ÿå¯¹è§£å‰–ç»“æ„çš„ä¸‰ç»´ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆæ®‹å·®Transformerï¼ˆHResFormerï¼‰ï¼Œç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚åŸºäºæ ‡å‡†çš„äºŒç»´å’Œä¸‰ç»´Transformeréª¨å¹²ç½‘ï¼ŒHResFormeråŒ…å«ä¸¤ä¸ªæ–°é¢–çš„å…³é”®è®¾è®¡ï¼šï¼ˆ1ï¼‰æ··åˆå±€éƒ¨å…¨å±€èåˆæ¨¡å—ï¼ˆHLGMï¼‰ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°è‡ªé€‚åº”åœ°èåˆæ¥è‡ªäºŒç»´Transformerçš„åˆ‡ç‰‡å†…ä¿¡æ¯å’Œæ¥è‡ªä¸‰ç»´ä½“ç§¯çš„åˆ‡ç‰‡é—´ä¿¡æ¯ï¼Œä»è€Œä¸ºä¸‰ç»´Transformeræä¾›å±€éƒ¨ç²¾ç»†å’Œå…¨å±€é•¿è·ç¦»è¡¨ç¤ºã€‚ï¼ˆ2ï¼‰æ··åˆæ¨¡å‹çš„æ®‹å·®å­¦ä¹ å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨åˆ‡ç‰‡å†…å’Œåˆ‡ç‰‡é—´çš„ä¿¡æ¯ï¼Œä»¥æ›´å¥½åœ°å¯¹è§£å‰–ç»“æ„è¿›è¡Œä¸‰ç»´ç†è§£ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„HResFormeråœ¨å¹¿æ³›ä½¿ç”¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…ˆå‰æŠ€æœ¯ã€‚æœ¬æ–‡æ­ç¤ºäº†è®¾è®¡ç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„Transformerçš„ä¸€ç§é‡è¦ä½†è¢«å¿½è§†çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11458v1">PDF</a> Accepted by TNNLS</p>
<p><strong>Summary</strong><br>    é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼ŒVision Transformerå› èƒ½å­¦ä¹ é•¿è·ç¦»ä¾èµ–å…³ç³»å±•ç°å‡ºä¼˜è¶Šæ€§ã€‚ç°æœ‰æ–¹æ³•å¯åˆ†ä¸ºåŸºäºäºŒç»´å’ŒåŸºäºä¸‰ç»´çš„æ–¹æ³•ï¼Œä½†å­˜åœ¨å¿½ç•¥åˆ‡ç‰‡å†…ä¿¡æ¯å’Œé«˜è®¡ç®—æˆæœ¬ç­‰å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ··åˆæ¨¡å‹HResFormerï¼ŒèåˆäºŒç»´å’Œä¸‰ç»´Transformeréª¨å¹²ç½‘ç»œï¼Œè®¾è®¡Hybrid Local-Global fusion Moduleè‡ªé€‚åº”èåˆåˆ‡ç‰‡å†…å’Œä½“ç§¯å†…ä¿¡æ¯ï¼Œå¹¶ç»“åˆæ®‹å·®å­¦ä¹ ï¼Œæ—¨åœ¨å®ç°æ›´å¥½çš„ä¸‰ç»´è§£å‰–å­¦ç†è§£ã€‚å®éªŒè¡¨æ˜ï¼ŒHResFormeråœ¨å¹¿æ³›ä½¿ç”¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformeråœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå› èƒ½å­¦ä¹ é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•å¯åˆ†ä¸ºåŸºäºäºŒç»´å’ŒåŸºäºä¸‰ç»´ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>HResFormeræ˜¯ä¸€ç§æ–°å‹æ··åˆæ¨¡å‹ï¼ŒèåˆäºŒç»´å’Œä¸‰ç»´Transformerç½‘ç»œã€‚</li>
<li>HResFormerè®¾è®¡åŒ…æ‹¬Hybrid Local-Global fusion Moduleï¼Œè‡ªé€‚åº”èåˆåˆ‡ç‰‡å†…å’Œä½“ç§¯å†…ä¿¡æ¯ã€‚</li>
<li>HResFormerç»“åˆæ®‹å·®å­¦ä¹ ï¼Œå®ç°æ›´å¥½çš„ä¸‰ç»´è§£å‰–å­¦ç†è§£ã€‚</li>
<li>å®éªŒè¡¨æ˜HResFormeråœ¨åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8056f9fd4442fbbc81dc0d27218c95ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fae2f9757fa4ab43f7721a3a44bb92c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e0e2243f0b2920c9d981f75cd879e8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22c7bbe217396beb9e328748673bf05d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8789d561237aa2e07fa9c3fb31c8ed32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f718168e1b9a29f0aa5b1bb09fb7d02.jpg" align="middle">
</details>


<h1 id="-20"><a href="#-20" class="headerlink" title=""></a></h1><h2 id="Adapting-Segment-Anything-Model-SAM-to-Experimental-Datasets-via-Fine-Tuning-on-GAN-based-Simulation-A-Case-Study-in-Additive-Manufacturing"><a href="#Adapting-Segment-Anything-Model-SAM-to-Experimental-Datasets-via-Fine-Tuning-on-GAN-based-Simulation-A-Case-Study-in-Additive-Manufacturing" class="headerlink" title="Adapting Segment Anything Model (SAM) to Experimental Datasets via   Fine-Tuning on GAN-based Simulation: A Case Study in Additive Manufacturing"></a>Adapting Segment Anything Model (SAM) to Experimental Datasets via   Fine-Tuning on GAN-based Simulation: A Case Study in Additive Manufacturing</h2><p><strong>Authors:Anika Tabassum, Amirkoushyar Ziabari</strong></p>
<p>Industrial X-ray computed tomography (XCT) is a powerful tool for non-destructive characterization of materials and manufactured components. XCT commonly accompanied by advanced image analysis and computer vision algorithms to extract relevant information from the images. Traditional computer vision models often struggle due to noise, resolution variability, and complex internal structures, particularly in scientific imaging applications. State-of-the-art foundational models, like the Segment Anything Model (SAM)-designed for general-purpose image segmentation-have revolutionized image segmentation across various domains, yet their application in specialized fields like materials science remains under-explored. In this work, we explore the application and limitations of SAM for industrial X-ray CT inspection of additive manufacturing components. We demonstrate that while SAM shows promise, it struggles with out-of-distribution data, multiclass segmentation, and computational efficiency during fine-tuning. To address these issues, we propose a fine-tuning strategy utilizing parameter-efficient techniques, specifically Conv-LoRa, to adapt SAM for material-specific datasets. Additionally, we leverage generative adversarial network (GAN)-generated data to enhance the training process and improve the modelâ€™s segmentation performance on complex X-ray CT data. Our experimental results highlight the importance of tailored segmentation models for accurate inspection, showing that fine-tuning SAM on domain-specific scientific imaging data significantly improves performance. However, despite improvements, the modelâ€™s ability to generalize across diverse datasets remains limited, highlighting the need for further research into robust, scalable solutions for domain-specific segmentation tasks. </p>
<blockquote>
<p>å·¥ä¸šXå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆXCTï¼‰æ˜¯éç ´åæ€§è¡¨å¾ææ–™å’Œåˆ¶é€ éƒ¨ä»¶çš„å¼ºå¤§å·¥å…·ã€‚XCTé€šå¸¸ä¸å…ˆè¿›çš„å›¾åƒåˆ†æå’Œè®¡ç®—æœºè§†è§‰ç®—æ³•ç›¸ç»“åˆï¼Œä»å›¾åƒä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ¨¡å‹ç»å¸¸å› å™ªå£°ã€åˆ†è¾¨ç‡å¯å˜æ€§å’Œå¤æ‚çš„å†…éƒ¨ç»“æ„è€Œé¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦æˆåƒåº”ç”¨ä¸­ã€‚æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚ç”¨äºé€šç”¨å›¾åƒåˆ†å‰²çš„Segment Anything Modelï¼ˆSAMï¼‰ï¼Œå·²ç»å½»åº•æ”¹å˜äº†å„ä¸ªé¢†åŸŸçš„å›¾åƒåˆ†å‰²ï¼Œä½†å®ƒä»¬åœ¨ææ–™ç§‘å­¦ç­‰ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†SAMåœ¨å¢æåˆ¶é€ éƒ¨ä»¶çš„å·¥ä¸šXå°„çº¿CTæ£€æµ‹ä¸­çš„åº”ç”¨å’Œå±€é™æ€§ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè™½ç„¶SAMå…·æœ‰æ½œåŠ›ï¼Œä½†åœ¨å¤„ç†åˆ†å¸ƒå¤–æ•°æ®ã€å¤šç±»åˆ†å‰²å’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—æ•ˆç‡æ—¶ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å‚æ•°é«˜æ•ˆæŠ€æœ¯çš„å¾®è°ƒç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯Conv-LoRaï¼Œä»¥é€‚åº”ææ–™ç‰¹å®šçš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”Ÿæˆçš„æ•°æ®æ¥å¢å¼ºè®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹å¯¹å¤æ‚Xå°„çº¿CTæ•°æ®çš„åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœå¼ºè°ƒäº†å®šåˆ¶åˆ†å‰²æ¨¡å‹å¯¹äºå‡†ç¡®æ£€æŸ¥çš„é‡è¦æ€§ï¼Œå¹¶è¡¨æ˜åœ¨ç‰¹å®šé¢†åŸŸçš„ç§‘å­¦æˆåƒæ•°æ®ä¸Šå¾®è°ƒSAMå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰æ‰€æ”¹è¿›ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œè¿™çªæ˜¾äº†å¯¹é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åˆ†å‰²ä»»åŠ¡çš„ç¨³å¥ã€å¯æ‰©å±•è§£å†³æ–¹æ¡ˆçš„è¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11381v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†å°†Segment Anything Modelï¼ˆSAMï¼‰åº”ç”¨äºå·¥ä¸šXå°„çº¿CTæ£€æµ‹å¢æåˆ¶é€ éƒ¨ä»¶çš„æ½œåŠ›ä¸å±€é™æ€§ã€‚è™½ç„¶SAMæ¨¡å‹è¡¨ç°å‡ºä¸€å®šæ½œåŠ›ï¼Œä½†åœ¨é¢ä¸´è¶…å‡ºèŒƒå›´çš„æ•°æ®ã€å¤šç±»åˆ«åˆ†å‰²å’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—æ•ˆç‡é—®é¢˜æ—¶ï¼Œå…¶åœ¨ææ–™ç‰¹å®šæ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰å¾…æé«˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ç§åˆ©ç”¨Conv-LoRaç­‰å‚æ•°ä¼˜åŒ–æŠ€æœ¯çš„å¾®è°ƒç­–ç•¥ï¼ŒåŒæ—¶å€ŸåŠ©ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”Ÿæˆçš„æ•°æ®å¢å¼ºè®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹å¯¹å¤æ‚Xå°„çº¿CTæ•°æ®çš„åˆ†å‰²æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹ç‰¹å®šç§‘å­¦æˆåƒæ•°æ®çš„SAMæ¨¡å‹å¾®è°ƒèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä½†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä»æœ‰å¾…æå‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ç¨³å¥ã€å¯æ‰©å±•è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å·¥ä¸šXå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆXCTï¼‰æ˜¯éç ´åæ€§ææ–™è¡¨å¾å’Œç»„ä»¶åˆ¶é€ çš„æœ‰åŠ›å·¥å…·ï¼Œå¸¸ä¸å…ˆè¿›çš„å›¾åƒåˆ†æå’Œè®¡ç®—æœºè§†è§‰ç®—æ³•ç»“åˆä½¿ç”¨ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸå…·æœ‰é©å‘½æ€§å½±å“ï¼Œä½†åœ¨ææ–™ç§‘å­¦ç­‰ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚</li>
<li>SAMåœ¨åº”å¯¹å·¥ä¸šXå°„çº¿CTæ£€æµ‹å¢æåˆ¶é€ éƒ¨ä»¶æ—¶é¢ä¸´è¶…å‡ºèŒƒå›´çš„æ•°æ®ã€å¤šç±»åˆ«åˆ†å‰²å’Œè®¡ç®—æ•ˆç‡çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨å‚æ•°ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚Conv-LoRaï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”Ÿæˆçš„æ•°æ®ï¼Œå¯ä»¥æé«˜SAMåœ¨å¤æ‚Xå°„çº¿CTæ•°æ®ä¸Šçš„åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œé’ˆå¯¹ç‰¹å®šç§‘å­¦æˆåƒæ•°æ®å¾®è°ƒSAMèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä½†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä»æœ‰å¾…æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68869de46c0cdb7e886a40641ab951d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-732024ecc4fba477e7740c4dac3ffab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dbbc6e53d60f16b7772eeb25fe38a3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32c059254fe838a182067649ce3ecdf8.jpg" align="middle">
</details>


<h1 id="-21"><a href="#-21" class="headerlink" title=""></a></h1><h2 id="Efficient-Quantization-Aware-Training-on-Segment-Anything-Model-in-Medical-Images-and-Its-Deployment"><a href="#Efficient-Quantization-Aware-Training-on-Segment-Anything-Model-in-Medical-Images-and-Its-Deployment" class="headerlink" title="Efficient Quantization-Aware Training on Segment Anything Model in   Medical Images and Its Deployment"></a>Efficient Quantization-Aware Training on Segment Anything Model in   Medical Images and Its Deployment</h2><p><strong>Authors:Haisheng Lu, Yujie Fu, Fan Zhang, Le Zhang</strong></p>
<p>Medical image segmentation is a critical component of clinical practice, and the state-of-the-art MedSAM model has significantly advanced this field. Nevertheless, critiques highlight that MedSAM demands substantial computational resources during inference. To address this issue, the CVPR 2024 MedSAM on Laptop Challenge was established to find an optimal balance between accuracy and processing speed. In this paper, we introduce a quantization-aware training pipeline designed to efficiently quantize the Segment Anything Model for medical images and deploy it using the OpenVINO inference engine. This pipeline optimizes both training time and disk storage. Our experimental results confirm that this approach considerably enhances processing speed over the baseline, while still achieving an acceptable accuracy level. The training script, inference script, and quantized model are publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/AVC2-UESTC/QMedSAM">https://github.com/AVC2-UESTC/QMedSAM</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯ä¸´åºŠå®è·µçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œè€Œæœ€æ–°çš„MedSAMæ¨¡å‹å·²å¤§å¤§æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚ç„¶è€Œï¼Œæ‰¹è¯„è€…æŒ‡å‡ºï¼ŒMedSAMåœ¨æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒCVPR 2024 MedSAMç¬”è®°æœ¬ç”µè„‘æŒ‘æˆ˜èµ›æ—¨åœ¨æ‰¾åˆ°å‡†ç¡®æ€§ä¸å¤„ç†é€Ÿåº¦ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªé‡åŒ–æ„ŸçŸ¥è®­ç»ƒç®¡é“ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°å¯¹åŒ»å­¦å›¾åƒçš„Segment Anything Modelè¿›è¡Œé‡åŒ–ï¼Œå¹¶ä½¿ç”¨OpenVINOæ¨ç†å¼•æ“è¿›è¡Œéƒ¨ç½²ã€‚è¯¥ç®¡é“ä¼˜åŒ–äº†è®­ç»ƒæ—¶é—´å’Œç£ç›˜å­˜å‚¨ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¯å®ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†é€Ÿåº¦ä¸Šæ˜æ˜¾ä¼˜äºåŸºçº¿ï¼ŒåŒæ—¶ä»èƒ½è¾¾åˆ°å¯æ¥å—çš„å‡†ç¡®æ€§æ°´å¹³ã€‚è®­ç»ƒè„šæœ¬ã€æ¨ç†è„šæœ¬å’Œé‡åŒ–æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AVC2-UESTC/QMedSAM">https://github.com/AVC2-UESTC/QMedSAM</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11186v1">PDF</a> 14 pages, 3 figures, to be published in LNCS</p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—å›¾åƒåˆ†å‰²æ˜¯ä¸´åºŠå®è·µçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼ŒMedSAMæ¨¡å‹ä¸ºè¿™ä¸€é¢†åŸŸå¸¦æ¥äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæ‰¹è¯„è€…æŒ‡å‡ºMedSAMåœ¨æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒCVPR 2024 MedSAM on Laptop Challengeæ—¨åœ¨æ‰¾åˆ°å‡†ç¡®æ€§ä¸å¤„ç†é€Ÿåº¦ä¹‹é—´çš„å¹³è¡¡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé‡åŒ–æ„ŸçŸ¥è®­ç»ƒç®¡é“ï¼Œç”¨äºæœ‰æ•ˆåœ°é‡åŒ–åŒ»ç–—å›¾åƒçš„Segment Anything Modelï¼Œå¹¶ä½¿ç”¨OpenVINOæ¨ç†å¼•æ“è¿›è¡Œéƒ¨ç½²ã€‚è¯¥ç®¡é“ä¼˜åŒ–äº†è®­ç»ƒæ—¶é—´å’Œç£ç›˜å­˜å‚¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜å¤„ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œä»ä¿æŒäº†å¯æ¥å—çš„å‡†ç¡®æ€§ã€‚è®­ç»ƒè„šæœ¬ã€æ¨ç†è„šæœ¬å’Œé‡åŒ–æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AVC2-UESTC/QMedSAM">AVC2-UESTC&#x2F;QMedSAM</a>å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»ç–—å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå®è·µä¸­è‡³å…³é‡è¦ï¼ŒMedSAMæ¨¡å‹ä¸ºè¯¥é¢†åŸŸå¸¦æ¥äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>MedSAMæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œå¼•å‘å…³æ³¨ã€‚</li>
<li>CVPR 2024 MedSAM on Laptop Challengeæ—¨åœ¨å¹³è¡¡æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¤„ç†é€Ÿåº¦ã€‚</li>
<li>ä»‹ç»äº†ä¸€ä¸ªé‡åŒ–æ„ŸçŸ¥è®­ç»ƒç®¡é“ï¼Œç”¨äºä¼˜åŒ–åŒ»ç–—å›¾åƒçš„Segment Anything Modelçš„è®­ç»ƒæ•ˆç‡å’Œéƒ¨ç½²æ–¹å¼ã€‚</li>
<li>è®­ç»ƒç®¡é“çš„ä¼˜åŒ–å‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œç£ç›˜å­˜å‚¨éœ€æ±‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥é‡åŒ–æ–¹æ³•åœ¨æé«˜å¤„ç†é€Ÿåº¦çš„åŒæ—¶ç»´æŒäº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-129b3341fd0f50a647e931b9422c2f93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7613603b28a6995de6d8c0903456a948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73df9aa0582e0a568c00e6e68b16162b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab1822dd382f53b56c1b194a0e64c789.jpg" align="middle">
</details>


<h1 id="-22"><a href="#-22" class="headerlink" title=""></a></h1><h2 id="SweetTokenizer-Semantic-Aware-Spatial-Temporal-Tokenizer-for-Compact-Visual-Discretization"><a href="#SweetTokenizer-Semantic-Aware-Spatial-Temporal-Tokenizer-for-Compact-Visual-Discretization" class="headerlink" title="SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact   Visual Discretization"></a>SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact   Visual Discretization</h2><p><strong>Authors:Zhentao Tan, Ben Xue, Jian Jia, Junhao Wang, Wencai Ye, Shaoyun Shi, Mingjie Sun, Wenjin Wu, Quan Chen, Peng Jiang</strong></p>
<p>This paper presents the \textbf{S}emantic-a\textbf{W}ar\textbf{E} spatial-t\textbf{E}mporal \textbf{T}okenizer (SweetTokenizer), a compact yet effective discretization approach for vision data. Our goal is to boost tokenizersâ€™ compression ratio while maintaining reconstruction fidelity in the VQ-VAE paradigm. Firstly, to obtain compact latent representations, we decouple images or videos into spatial-temporal dimensions, translating visual information into learnable querying spatial and temporal tokens through a \textbf{C}ross-attention \textbf{Q}uery \textbf{A}uto\textbf{E}ncoder (CQAE). Secondly, to complement visual information during compression, we quantize these tokens via a specialized codebook derived from off-the-shelf LLM embeddings to leverage the rich semantics from language modality. Finally, to enhance training stability and convergence, we also introduce a curriculum learning strategy, which proves critical for effective discrete visual representation learning. SweetTokenizer achieves comparable video reconstruction fidelity with only \textbf{25%} of the tokens used in previous state-of-the-art video tokenizers, and boost video generation results by \textbf{32.9%} w.r.t gFVD. When using the same token number, we significantly improves video and image reconstruction results by \textbf{57.1%} w.r.t rFVD on UCF-101 and \textbf{37.2%} w.r.t rFID on ImageNet-1K. Additionally, the compressed tokens are imbued with semantic information, enabling few-shot recognition capabilities powered by LLMs in downstream applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­ä¹‰æ„ŸçŸ¥æ—¶ç©ºä»¤ç‰ŒåŒ–å™¨ï¼ˆSweetTokenizerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç´§å‡‘ä¸”æœ‰æ•ˆçš„è§†è§‰æ•°æ®ç¦»æ•£åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æé«˜VQ-VAEæ¶æ„ä¸­çš„tokenå‹ç¼©æ¯”ä¾‹çš„åŒæ—¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚é¦–å…ˆï¼Œä¸ºäº†è·å¾—ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºï¼Œæˆ‘ä»¬å°†å›¾åƒæˆ–è§†é¢‘è§£è€¦ä¸ºæ—¶ç©ºç»´åº¦ï¼Œå¹¶é€šè¿‡è·¨æŸ¥è¯¢æ³¨æ„åŠ›è‡ªåŠ¨ç¼–ç å™¨ï¼ˆCQAEï¼‰å°†è§†è§‰ä¿¡æ¯è½¬åŒ–ä¸ºå¯å­¦ä¹ çš„æŸ¥è¯¢ç©ºé—´å’Œæ—¶é—´ä»¤ç‰Œã€‚å…¶æ¬¡ï¼Œä¸ºäº†åœ¨å‹ç¼©è¿‡ç¨‹ä¸­è¡¥å……è§†è§‰ä¿¡æ¯ï¼Œæˆ‘ä»¬é€šè¿‡ä»ç°æˆçš„LLMåµŒå…¥ä¸­æ´¾ç”Ÿå‡ºçš„ä¸“ç”¨ä»£ç æœ¬å¯¹è¿™äº›ä»¤ç‰Œè¿›è¡Œé‡åŒ–ï¼Œä»¥åˆ©ç”¨è¯­è¨€æ¨¡æ€çš„ä¸°å¯Œè¯­ä¹‰ã€‚æœ€åï¼Œä¸ºäº†æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œè¿™å¯¹äºæœ‰æ•ˆçš„ç¦»æ•£è§†è§‰è¡¨ç¤ºå­¦ä¹ è‡³å…³é‡è¦ã€‚SweetTokenizeråœ¨ä»…ä½¿ç”¨å½“å‰è§†é¢‘tokenizerçš„ç™¾åˆ†ä¹‹äºŒåäº”æ ‡è®°çš„æƒ…å†µä¸‹å–å¾—äº†ç›¸å½“çš„è§†é¢‘é‡å»ºä¿çœŸåº¦ï¼Œå¹¶åœ¨GFVDæŒ‡æ ‡ä¸Šå°†è§†é¢‘ç”Ÿæˆç»“æœæé«˜äº†ç™¾åˆ†ä¹‹ä¸‰åå…«ç‚¹ä¹ã€‚å½“ä½¿ç”¨ç›¸åŒçš„æ ‡è®°æ•°é‡æ—¶ï¼Œæˆ‘ä»¬åœ¨UCF-101ä¸Šçš„è§†é¢‘é‡å»ºç»“æœç›¸è¾ƒäºrFVDæŒ‡æ ‡æå‡äº†ç™¾åˆ†ä¹‹äº”åä¸ƒç‚¹ä¸€ï¼Œåœ¨ImageNet-1Kä¸Šçš„å›¾åƒé‡å»ºç»“æœç›¸è¾ƒäºrFIDæŒ‡æ ‡æå‡äº†ç™¾åˆ†ä¹‹ä¸‰åä¸ƒç‚¹äºŒã€‚æ­¤å¤–ï¼Œå‹ç¼©çš„ä»¤ç‰Œä¸­è•´å«äº†è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿å¾—ä¸‹æ¸¸åº”ç”¨ä¸­èƒ½å¤Ÿå€ŸåŠ©LLMå®ç°å°æ ·æœ¬è¯†åˆ«åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10443v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Semantic-Awareç©ºé—´æ—¶åºæ ‡è®°åŒ–ï¼ˆSweetTokenizerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç´§å‡‘è€Œæœ‰æ•ˆçš„é’ˆå¯¹è§†è§‰æ•°æ®çš„ç¦»æ•£åŒ–æ–¹æ³•ã€‚ç›®æ ‡æ˜¯æé«˜VQ-VAEèŒƒå¼ä¸­çš„ä»¤ç‰Œå‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚é¦–å…ˆï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æŸ¥è¯¢è‡ªåŠ¨ç¼–ç å™¨ï¼ˆCQAEï¼‰å°†å›¾åƒæˆ–è§†é¢‘è½¬æ¢ä¸ºå¯å­¦ä¹ çš„æŸ¥è¯¢ç©ºé—´å’Œæ—¶é—´ä»¤ç‰Œï¼Œè·å¾—ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºã€‚å…¶æ¬¡ï¼Œé€šè¿‡æ¥è‡ªç°æˆçš„LLMåµŒå…¥çš„ä¸“ç”¨ä»£ç æœ¬å¯¹è¿™äº›ä»¤ç‰Œè¿›è¡Œé‡åŒ–ï¼Œä»¥åˆ©ç”¨è¯­è¨€æ¨¡æ€çš„ä¸°å¯Œè¯­ä¹‰æ¥è¡¥å……è§†è§‰ä¿¡æ¯ã€‚æœ€åï¼Œä¸ºäº†æé«˜è®­ç»ƒå’Œæ”¶æ•›çš„ç¨³å®šæ€§ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œè¿™å¯¹äºæœ‰æ•ˆçš„ç¦»æ•£è§†è§‰è¡¨ç¤ºå­¦ä¹ è‡³å…³é‡è¦ã€‚SweetTokenizerä»¥å…ˆå‰æœ€å…ˆè¿›çš„è§†é¢‘ä»¤ç‰Œå™¨ä¸­ä½¿ç”¨çš„ä»¤ç‰Œçš„ä»…å››åˆ†ä¹‹ä¸€å®ç°äº†å¯æ¯”çš„è§†é¢‘é‡å»ºä¿çœŸåº¦ï¼Œå¹¶é€šè¿‡GFVDå°†è§†é¢‘ç”Ÿæˆç»“æœæé«˜äº†32.9ï¼…ã€‚åœ¨ä½¿ç”¨ç›¸åŒä»¤ç‰Œæ•°é‡çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨UCF-101ä¸Šçš„è§†é¢‘å’Œå›¾åƒé‡å»ºç»“æœåˆ†åˆ«æé«˜äº†57.1ï¼…å’ŒFIDä»¥åŠåœ¨ImageNet-1Kä¸Šçš„æé«˜äº†FID 37.2ï¼…ã€‚æ­¤å¤–ï¼Œå‹ç¼©çš„ä»¤ç‰Œè•´å«ç€è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä»¥åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­å€ŸåŠ©LLMå®ç°å°‘é‡æ ·æœ¬è¯†åˆ«åŠŸèƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SweetTokenizeræ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰æ•°æ®çš„ç´§å‡‘ä¸”æœ‰æ•ˆçš„ç¦»æ•£åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜VQ-VAEèŒƒå¼ä¸­çš„ä»¤ç‰Œå‹ç¼©ç‡å¹¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚</li>
<li>é€šè¿‡CQAEå°†å›¾åƒæˆ–è§†é¢‘è½¬æ¢ä¸ºç©ºé—´æ—¶åºä»¤ç‰Œï¼Œè·å¾—ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>åˆ©ç”¨LLMåµŒå…¥çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯é€šè¿‡ä¸“ç”¨ä»£ç æœ¬å¯¹ä»¤ç‰Œè¿›è¡Œé‡åŒ–ã€‚</li>
<li>é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥æé«˜ç¦»æ•£è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>SweetTokenizeråœ¨è§†é¢‘é‡å»ºå’Œç”Ÿæˆæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰çš„æŠ€æœ¯ï¼ŒåŒæ—¶ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡å¤§å¹…å‡å°‘ã€‚åœ¨è§†é¢‘é‡å»ºä¸­ï¼Œè¾¾åˆ°äº†åŒç­‰æ•ˆæœä½¿ç”¨äº†ä»…å››åˆ†ä¹‹ä¸€çš„ä»¤ç‰Œæ•°é‡ï¼›åœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢ï¼Œç›¸æ¯”GFVDæ”¹è¿›äº†çº¦3æˆæ¯”ä¾‹çš„ç»“æœï¼›å›¾åƒç”Ÿæˆä¹Ÿå–å¾—æ˜¾è‘—æ”¹å–„ç»“æœã€‚è¯æ˜äº†å°‘é‡çš„ç›¸åŒæ•°ç›®çš„æ ‡è®°å¯ä»¥æœ‰æ•ˆåœ°ä¼˜åŒ–å’Œæ”¹å–„å¤„ç†è´¨é‡å’Œå¤„ç†æ€§èƒ½æé«˜ç ”ç©¶æ•ˆç‡çš„æ•ˆç‡åœ¨å„ç§ç‰¹å®šä»»åŠ¡ä¸­éƒ½èƒ½å®ç°æ˜¾è‘—çš„æ”¹è¿›å’Œæå‡åœ¨å›¾åƒå¤„ç†æ–¹é¢è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ•ˆæœå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ä¸”åº”ç”¨å‰æ™¯å¹¿é˜”å¯å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„å„ä¸ªå­é¢†åŸŸä»å¼€å‘æµç¨‹å’Œé«˜æ•ˆç ”ç©¶å‡ºå‘åœ¨ä¸åŒæ¨¡å‹çš„ç»“æ„é€‰æ‹©è¯„ä¼°ä¸è®¾è®¡å·¥ä½œä¸­æä¾›æœ‰åŠ›çš„æ”¯æ’‘å’ŒæŠ€æœ¯ä¿éšœ</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6ad1d061dbaae5e46990c4747c2c206.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b2e7c9a443cb65dec8a37331e96472c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aa1f3c554aa1a9d2405ee842ccba949.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-556305bf2235b5cf6b1c193a1264aeb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79e0fb570c18e1e18fc4aebe875771de.jpg" align="middle">
</details>


<h1 id="-23"><a href="#-23" class="headerlink" title=""></a></h1><h2 id="FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation"></a>FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation</h2><p><strong>Authors:Yuntian Bo, Yazhou Zhu, Lunbo Li, Haofeng Zhang</strong></p>
<p>Existing few-shot medical image segmentation (FSMIS) models fail to address a practical issue in medical imaging: the domain shift caused by different imaging techniques, which limits the applicability to current FSMIS tasks. To overcome this limitation, we focus on the cross-domain few-shot medical image segmentation (CD-FSMIS) task, aiming to develop a generalized model capable of adapting to a broader range of medical image segmentation scenarios with limited labeled data from the novel target domain. Inspired by the characteristics of frequency domain similarity across different domains, we propose a Frequency-aware Matching Network (FAMNet), which includes two key components: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion (MSF) module. The FAM module tackles two problems during the meta-learning phase: 1) intra-domain variance caused by the inherent support-query bias, due to the different appearances of organs and lesions, and 2) inter-domain variance caused by different medical imaging techniques. Additionally, we design an MSF module to integrate the different frequency features decoupled by the FAM module, and further mitigate the impact of inter-domain variance on the modelâ€™s segmentation performance. Combining these two modules, our FAMNet surpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation models on three cross-domain datasets, achieving state-of-the-art performance in the CD-FSMIS task. </p>
<blockquote>
<p>ç°æœ‰çš„å°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ¨¡å‹æ— æ³•è§£å†³åŒ»å­¦æˆåƒä¸­çš„ä¸€ä¸ªå®é™…é—®é¢˜ï¼šç”±ä¸åŒæˆåƒæŠ€æœ¯å¼•èµ·çš„åŸŸåç§»ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å½“å‰FSMISä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸“æ³¨äºè·¨åŸŸå°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§é€šç”¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ–°çš„ç›®æ ‡åŸŸä¸­æœ‰é™æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€‚åº”æ›´å¹¿æ³›çš„åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ã€‚å—ä¸åŒé¢†åŸŸé¢‘ç‡åŸŸç›¸ä¼¼æ€§ç‰¹å¾çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ã€‚FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰ç”±äºå™¨å®˜å’Œç—…å˜çš„ä¸åŒå¤–è§‚é€ æˆçš„åŸŸå†…å·®å¼‚å¯¼è‡´çš„å›ºæœ‰æ”¯æŒæŸ¥è¯¢åè§ï¼›2ï¼‰ç”±äºä¸åŒçš„åŒ»å­¦æˆåƒæŠ€æœ¯é€ æˆçš„è·¨åŸŸå·®å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªMSFæ¨¡å—ï¼Œä»¥æ•´åˆFAMæ¨¡å—è§£è€¦çš„ä¸åŒé¢‘ç‡ç‰¹å¾ï¼Œå¹¶è¿›ä¸€æ­¥å‡è½»è·¨åŸŸå·®å¼‚å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚ç»“åˆè¿™ä¸¤ä¸ªæ¨¡å—ï¼Œæˆ‘ä»¬çš„FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œåœ¨CD-FSMISä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09319v2">PDF</a> Accepted by the 39th Annual AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç°æœ‰åŒ»ç–—å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨ä¸åŒæˆåƒæŠ€æœ¯å¯¼è‡´çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œæå‡ºä¸€ç§é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼ŒåŒ…æ‹¬é¢‘ç‡æ„ŸçŸ¥åŒ¹é…æ¨¡å—å’Œå¤šå…‰è°±èåˆæ¨¡å—ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿé€‚åº”æ›´å¹¿æ³›åŒ»ç–—å›¾åƒåˆ†å‰²åœºæ™¯çš„é€šç”¨æ¨¡å‹ï¼Œåªéœ€å°‘é‡ç›®æ ‡é¢†åŸŸçš„æ•°æ®ã€‚FAMNetåœ¨ä¸¤ä¸ªå…³é”®æ¨¡å—ä¸­è§£å†³è·¨åŸŸå’Œå†…éƒ¨åŸŸæ–¹å·®é—®é¢˜ï¼Œå¹¶åœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¢†åŸŸåç§»é—®é¢˜æ˜¯åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­çš„ä¸€ä¸ªå®é™…é—®é¢˜ï¼Œç”±äºä¸åŒæˆåƒæŠ€æœ¯å¯¼è‡´ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰æ¥è§£å†³è·¨åŸŸå°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ã€‚</li>
<li>FAMNetåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢‘ç‡æ„ŸçŸ¥åŒ¹é…æ¨¡å—å’Œå¤šå…‰è°±èåˆæ¨¡å—ã€‚</li>
<li>FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„å†…éƒ¨å’Œå¤–éƒ¨é¢†åŸŸå·®å¼‚é—®é¢˜ã€‚</li>
<li>MSFæ¨¡å—ç”¨äºæ•´åˆä¸åŒé¢‘ç‡ç‰¹å¾ï¼Œè¿›ä¸€æ­¥å‡è½»é¢†åŸŸå·®å¼‚å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚</li>
<li>FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†åœ¨CD-FSMISä»»åŠ¡ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-247281a683520d1a403f40d75c810e43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa5e909180014c3b3e9b82c59c9fd06f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bebbe679237a61c8f674f9fcadf5a543.jpg" align="middle">
</details>


<h1 id="-24"><a href="#-24" class="headerlink" title=""></a></h1><h2 id="DiffBoost-Enhancing-Medical-Image-Segmentation-via-Text-Guided-Diffusion-Model"><a href="#DiffBoost-Enhancing-Medical-Image-Segmentation-via-Text-Guided-Diffusion-Model" class="headerlink" title="DiffBoost: Enhancing Medical Image Segmentation via Text-Guided   Diffusion Model"></a>DiffBoost: Enhancing Medical Image Segmentation via Text-Guided   Diffusion Model</h2><p><strong>Authors:Zheyuan Zhang, Lanhong Yao, Bin Wang, Debesh Jha, Gorkem Durak, Elif Keles, Alpay Medetalibeyoglu, Ulas Bagci</strong></p>
<p>Large-scale, big-variant, high-quality data are crucial for developing robust and successful deep-learning models for medical applications since they potentially enable better generalization performance and avoid overfitting. However, the scarcity of high-quality labeled data always presents significant challenges. This paper proposes a novel approach to address this challenge by developing controllable diffusion models for medical image synthesis, called DiffBoost. We leverage recent diffusion probabilistic models to generate realistic and diverse synthetic medical image data that preserve the essential characteristics of the original medical images by incorporating edge information of objects to guide the synthesis process. In our approach, we ensure that the synthesized samples adhere to medically relevant constraints and preserve the underlying structure of imaging data. Due to the random sampling process by the diffusion model, we can generate an arbitrary number of synthetic images with diverse appearances. To validate the effectiveness of our proposed method, we conduct an extensive set of medical image segmentation experiments on multiple datasets, including Ultrasound breast (+13.87%), CT spleen (+0.38%), and MRI prostate (+7.78%), achieving significant improvements over the baseline segmentation methods. The promising results demonstrate the effectiveness of our \textcolor{black}{DiffBoost} for medical image segmentation tasks and show the feasibility of introducing a first-ever text-guided diffusion model for general medical image segmentation tasks. With carefully designed ablation experiments, we investigate the influence of various data augmentations, hyper-parameter settings, patch size for generating random merging mask settings, and combined influence with different network architectures. Source code are available at <a target="_blank" rel="noopener" href="https://github.com/NUBagciLab/DiffBoost">https://github.com/NUBagciLab/DiffBoost</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡ã€å¤šå˜ä½“ã€é«˜è´¨é‡çš„æ•°æ®å¯¹äºå¼€å‘ç”¨äºåŒ»å­¦åº”ç”¨çš„ç¨³å¥ä¸”æˆåŠŸçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿæ½œåœ¨åœ°å®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½å¹¶é¿å…è¿‡æ‹Ÿåˆã€‚ç„¶è€Œï¼Œé«˜è´¨é‡æ ‡è®°æ•°æ®çš„ç¨€ç¼ºæ€§å§‹ç»ˆæ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼€å‘ç”¨äºåŒ»å­¦å›¾åƒåˆæˆçš„å¯æ§æ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸ºDiffBoostã€‚æˆ‘ä»¬åˆ©ç”¨æœ€æ–°çš„æ‰©æ•£æ¦‚ç‡æ¨¡å‹ç”Ÿæˆé€¼çœŸä¸”å¤šæ ·åŒ–çš„åˆæˆåŒ»å­¦å›¾åƒæ•°æ®ï¼Œé€šè¿‡èå…¥å¯¹è±¡çš„è¾¹ç¼˜ä¿¡æ¯æ¥æŒ‡å¯¼åˆæˆè¿‡ç¨‹ï¼Œä»è€Œä¿ç•™åŸå§‹åŒ»å­¦å›¾åƒçš„åŸºæœ¬ç‰¹å¾ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ç¡®ä¿åˆæˆæ ·æœ¬ç¬¦åˆåŒ»å­¦ç›¸å…³çš„çº¦æŸå¹¶ä¿ç•™æˆåƒæ•°æ®çš„åŸºæœ¬ç»“æ„ã€‚ç”±äºæ‰©æ•£æ¨¡å‹çš„éšæœºé‡‡æ ·è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå…·æœ‰å„ç§å¤–è§‚çš„ä»»æ„æ•°é‡çš„åˆæˆå›¾åƒã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡çš„åŒ»å­¦å›¾åƒåˆ†å‰²å®éªŒï¼ŒåŒ…æ‹¬è¶…å£°ä¹³è…ºï¼ˆ+13.87%ï¼‰ã€CTè„¾è„ï¼ˆ+0.38%ï¼‰å’ŒMRIå‰åˆ—è…ºï¼ˆ+7.78%ï¼‰ï¼Œåœ¨åŸºå‡†åˆ†å‰²æ–¹æ³•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä»¤äººé¼“èˆçš„ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„DiffBooståœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å¼•å…¥é¦–ä¸ªæ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ç”¨äºä¸€èˆ¬åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„å¯è¡Œæ€§ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶ˆèå®éªŒï¼Œæˆ‘ä»¬ç ”ç©¶äº†å„ç§æ•°æ®å¢å¼ºã€è¶…å‚æ•°è®¾ç½®ã€ç”¨äºç”Ÿæˆéšæœºåˆå¹¶è’™ç‰ˆçš„è¡¥ä¸å¤§å°ä»¥åŠä¸åŒç½‘ç»œæ¶æ„çš„ç»„åˆå½±å“ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NUBagciLab/DiffBoost%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NUBagciLab/DiffBoostæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12868v2">PDF</a> Accepted by IEEE TRANSACTIONS ON MEDICAL IMAGING</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºå¤§è§„æ¨¡ã€å¤šå˜ä½“ã€é«˜è´¨é‡çš„æ•°æ®å¯¹äºå¼€å‘ç”¨äºåŒ»å­¦åº”ç”¨çš„ç¨³å¥å’ŒæˆåŠŸçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯æ§æ‰©æ•£æ¨¡å‹æ¥è§£å†³é«˜è´¨é‡æ ‡ç­¾æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆæˆï¼Œç§°ä¸ºDiffBoostã€‚è¯¥æ¨¡å‹åˆ©ç”¨æœ€æ–°çš„æ‰©æ•£æ¦‚ç‡æ¨¡å‹ç”Ÿæˆé€¼çœŸçš„ã€å¤šæ ·åŒ–çš„åˆæˆåŒ»å­¦å›¾åƒæ•°æ®ï¼Œé€šè¿‡èå…¥å¯¹è±¡çš„è¾¹ç¼˜ä¿¡æ¯æ¥å¼•å¯¼åˆæˆè¿‡ç¨‹ï¼Œä¿ç•™åŸå§‹åŒ»å­¦å›¾åƒçš„åŸºæœ¬ç‰¹å¾ã€‚é€šè¿‡å¹¿æ³›çš„åŒ»å­¦å›¾åƒåˆ†å‰²å®éªŒéªŒè¯ï¼ŒDiffBooståœ¨è¶…å£°ä¹³æˆ¿ï¼ˆ+13.87ï¼…ï¼‰ã€CTè„¾è„ï¼ˆ+0.38ï¼…ï¼‰å’ŒMRIå‰åˆ—è…ºï¼ˆ+7.78ï¼…ï¼‰ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç»“æœè¡¨æ˜DiffBooståœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å¼•å…¥é¦–ä¸ªæ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ç”¨äºä¸€èˆ¬åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§è§„æ¨¡ã€é«˜è´¨é‡æ•°æ®å¯¹å¼€å‘ç¨³å¥çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½å’Œé¿å…è¿‡æ‹Ÿåˆã€‚</li>
<li>é¢ä¸´é«˜è´¨é‡æ ‡ç­¾æ•°æ®çš„ç¨€ç¼ºæ€§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¯æ§æ‰©æ•£æ¨¡å‹DiffBoostï¼Œç”¨äºç”ŸæˆåˆæˆåŒ»å­¦å›¾åƒæ•°æ®ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¦‚ç‡æ¨¡å‹ç”Ÿæˆé€¼çœŸçš„ã€å¤šæ ·åŒ–çš„åŒ»å­¦å›¾åƒï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„åŸºæœ¬ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¹¿æ³›çš„åŒ»å­¦å›¾åƒåˆ†å‰²å®éªŒéªŒè¯ï¼ŒDiffBooståœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>ç¬¬ä¸€ä¸ªå¼•å…¥æ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.12868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c7be1d980e17d308f4e0d439962e99e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90857f8ad4ac62cb7c57f03ad18d3dd1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a539199c69b3da6f402c4e3f197bbf9f.jpg" align="middle">
</details>


<h1 id="-25"><a href="#-25" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bf6e7db4f0253c8902f2ba241b051a88.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Synthetic Speech Classification IEEE Signal Processing Cup 2022   challenge
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-92df01ad425ce71e6a3a7789281fc8d0.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  AniDoc Animation Creation Made Easier
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
