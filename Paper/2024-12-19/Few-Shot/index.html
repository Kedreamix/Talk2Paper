<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  FarExStance Explainable Stance Detection for Farsi">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-a91205a65b64bc5f5dda409bd8f005a2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-19-æ›´æ–°"><a href="#2024-12-19-æ›´æ–°" class="headerlink" title="2024-12-19 æ›´æ–°"></a>2024-12-19 æ›´æ–°</h1><h2 id="FarExStance-Explainable-Stance-Detection-for-Farsi"><a href="#FarExStance-Explainable-Stance-Detection-for-Farsi" class="headerlink" title="FarExStance: Explainable Stance Detection for Farsi"></a>FarExStance: Explainable Stance Detection for Farsi</h2><p><strong>Authors:Majid Zarharan, Maryam Hashemi, Malika Behroozrazegh, Sauleh Eetemadi, Mohammad Taher Pilehvar, Jennifer Foster</strong></p>
<p>We introduce FarExStance, a new dataset for explainable stance detection in Farsi. Each instance in this dataset contains a claim, the stance of an article or social media post towards that claim, and an extractive explanation which provides evidence for the stance label. We compare the performance of a fine-tuned multilingual RoBERTa model to several large language models in zero-shot, few-shot, and parameter-efficient fine-tuned settings on our new dataset. On stance detection, the most accurate models are the fine-tuned RoBERTa model, the LLM Aya-23-8B which has been fine-tuned using parameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the quality of the explanations, our automatic evaluation metrics indicate that few-shot GPT-4o generates the most coherent explanations, while our human evaluation reveals that the best Overall Explanation Score (OES) belongs to few-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced explanations most closely aligned with the reference explanations. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†FarExStanceæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ³¢æ–¯è¯­çš„å¯è§£é‡Šç«‹åœºæ£€æµ‹çš„æ–°æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä¸­çš„æ¯ä¸ªå®ä¾‹éƒ½åŒ…å«ä¸€ä¸ªå£°æ˜ã€ä¸€ç¯‡æ–‡ç« æˆ–ç¤¾äº¤åª’ä½“å¸–å­å¯¹è¯¥å£°æ˜çš„ç«‹åœºä»¥åŠæä¾›ç«‹åœºæ ‡ç­¾è¯æ®çš„è§£é‡Šæ€§æ‘˜è¦ã€‚æˆ‘ä»¬åœ¨æ–°çš„æ•°æ®é›†ä¸Šå¯¹æ¯”äº†å¾®è°ƒçš„å¤šè¯­è¨€RoBERTaæ¨¡å‹ä¸å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒè®¾ç½®ä¸­çš„è¡¨ç°ã€‚åœ¨ç«‹åœºæ£€æµ‹æ–¹é¢ï¼Œè¡¨ç°æœ€å‡†ç¡®çš„æ¨¡å‹æ˜¯å¾®è°ƒçš„RoBERTaæ¨¡å‹ã€ç»è¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹Aya-23-8Bå’Œå°‘æ ·æœ¬çš„Claude-3.5-Sonnetã€‚å…³äºè§£é‡Šçš„è´¨é‡ï¼Œæˆ‘ä»¬çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ˜¾ç¤ºï¼Œå°‘æ ·æœ¬çš„GPT-4oç”Ÿæˆçš„è§£é‡Šæœ€ä¸ºè¿è´¯ï¼Œè€Œæˆ‘ä»¬çš„äººå·¥è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€å¥½çš„æ•´ä½“è§£é‡Šå¾—åˆ†ï¼ˆOESï¼‰å±äºå°‘æ ·æœ¬çš„Claude-3.5-Sonnetã€‚ç»è¿‡å¾®è°ƒçš„Aya-32-8Bæ¨¡å‹ç”Ÿæˆçš„è§£é‡Šä¸å‚è€ƒè§£é‡Šæœ€ä¸ºä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14008v1">PDF</a> Accepted in COLING 2025</p>
<p><strong>Summary</strong></p>
<p>FarExStanceæ•°æ®é›†ç”¨äºæ³¢æ–¯è¯­çš„è§£é‡Šæ€§ç«‹åœºæ£€æµ‹ã€‚ç ”ç©¶å¯¹æ¯”äº†å¾®è°ƒè¿‡çš„å¤šè¯­è¨€RoBERTaæ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒè®¾ç½®ä¸Šçš„è¡¨ç°ã€‚åœ¨ç«‹åœºæ£€æµ‹æ–¹é¢ï¼Œå¾®è°ƒçš„RoBERTaæ¨¡å‹ã€ç»è¿‡å‚æ•°é«˜æ•ˆè°ƒæ•™çš„LLM aya-23-8Bå’Œå°‘æ ·æœ¬çš„Claude-3.5-Sonnetè¡¨ç°æœ€å‡†ç¡®ã€‚åœ¨è§£é‡Šè´¨é‡æ–¹é¢ï¼ŒGPT-4oçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ˜¾ç¤ºå…¶è§£é‡Šæœ€è¿è´¯ï¼Œè€Œäººå·¥è¯„ä¼°æ˜¾ç¤ºæœ€å¥½çš„æ•´ä½“è§£é‡Šåˆ†æ•°æ¥è‡ªå°‘æ ·æœ¬çš„Claude-3.5-Sonnetã€‚åŒæ—¶ï¼Œç»è¿‡å¾®è°ƒçš„aya-32-8Bæ¨¡å‹ç”Ÿæˆçš„è§£é‡Šä¸å‚è€ƒè§£é‡Šæœ€ä¸ºæ¥è¿‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FarExStanceæ˜¯ä¸€ä¸ªç”¨äºæ³¢æ–¯è¯­è§£é‡Šæ€§ç«‹åœºæ£€æµ‹çš„æ–°æ•°æ®é›†ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†å¤šç§æ¨¡å‹åœ¨FarExStanceæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>åœ¨ç«‹åœºæ£€æµ‹æ–¹é¢ï¼Œå¾®è°ƒçš„RoBERTaæ¨¡å‹ã€LLM aya-23-8Bï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰å’Œå°‘æ ·æœ¬çš„Claude-3.5-Sonnetè¡¨ç°æœ€ä½³ã€‚</li>
<li>åœ¨è§£é‡Šè´¨é‡æ–¹é¢ï¼ŒGPT-4oçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ˜¾ç¤ºå…¶è§£é‡Šæœ€è¿è´¯ã€‚</li>
<li>äººå·¥è¯„ä¼°æ˜¾ç¤ºæœ€å¥½çš„æ•´ä½“è§£é‡Šåˆ†æ•°æ¥è‡ªå°‘æ ·æœ¬çš„Claude-3.5-Sonnetæ¨¡å‹ã€‚</li>
<li>ç›¸æ¯”å…¶ä»–æ¨¡å‹ï¼Œaya-32-8Bæ¨¡å‹ç”Ÿæˆçš„è§£é‡Šä¸å‚è€ƒè§£é‡Šæœ€ä¸ºæ¥è¿‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14008">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67e37634e11ac5ea39f3c8312d0000eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-251163be1bdfa11a3fa8a4d30185cd80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb64ac1dfb43540806096b88b9c90ed6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Few-shot-Steerable-Alignment-Adapting-Rewards-and-LLM-Policies-with-Neural-Processes"><a href="#Few-shot-Steerable-Alignment-Adapting-Rewards-and-LLM-Policies-with-Neural-Processes" class="headerlink" title="Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with   Neural Processes"></a>Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with   Neural Processes</h2><p><strong>Authors:Katarzyna Kobalczyk, Claudio Fanconi, Hao Sun, Mihaela van der Schaar</strong></p>
<p>As large language models (LLMs) become increasingly embedded in everyday applications, ensuring their alignment with the diverse preferences of individual users has become a critical challenge. Currently deployed approaches typically assume homogeneous user objectives and rely on single-objective fine-tuning. However, human preferences are inherently heterogeneous, influenced by various unobservable factors, leading to conflicting signals in preference data. Existing solutions addressing this diversity often require costly datasets labelled for specific objectives and involve training multiple reward models or LLM policies, which is computationally expensive and impractical. In this work, we present a novel framework for few-shot steerable alignment, where usersâ€™ underlying preferences are inferred from a small sample of their choices. To achieve this, we extend the Bradley-Terry-Luce model to handle heterogeneous preferences with unobserved variability factors and propose its practical implementation for reward modelling and LLM fine-tuning. Thanks to our proposed approach of functional parameter-space conditioning, LLMs trained with our framework can be adapted to individual preferences at inference time, generating outputs over a continuum of behavioural modes. We empirically validate the effectiveness of methods, demonstrating their ability to capture and align with diverse human preferences in a data-efficient manner. Our code is made available at: <a target="_blank" rel="noopener" href="https://github.com/kasia-kobalczyk/few-shot-steerable-alignment">https://github.com/kasia-kobalczyk/few-shot-steerable-alignment</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­çš„åµŒå…¥ç¨‹åº¦è¶Šæ¥è¶Šé«˜ï¼Œç¡®ä¿å®ƒä»¬ä¸ä¸ªåˆ«ç”¨æˆ·çš„å¤šæ ·åŒ–åå¥½ä¿æŒä¸€è‡´å·²æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚å½“å‰éƒ¨ç½²çš„æ–¹æ³•é€šå¸¸å‡è®¾ç”¨æˆ·ç›®æ ‡æ˜¯å‡è´¨çš„ï¼Œå¹¶ä¾èµ–äºå•ç›®æ ‡å¾®è°ƒã€‚ç„¶è€Œï¼Œäººç±»åå¥½æœ¬è´¨ä¸Šæ˜¯å¼‚è´¨çš„ï¼Œå—åˆ°å„ç§ä¸å¯è§‚å¯Ÿå› ç´ çš„å½±å“ï¼Œå¯¼è‡´åå¥½æ•°æ®ä¸­çš„å†²çªä¿¡å·ã€‚è§£å†³è¿™ç§å¤šæ ·æ€§çš„ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸éœ€è¦ä¸ºç‰¹å®šç›®æ ‡æ ‡è®°çš„æ˜‚è´µæ•°æ®é›†ï¼Œå¹¶æ¶‰åŠè®­ç»ƒå¤šä¸ªå¥–åŠ±æ¨¡å‹æˆ–LLMç­–ç•¥ï¼Œè¿™åœ¨è®¡ç®—ä¸Šå¾ˆæ˜‚è´µä¸”ä¸åˆ‡å®é™…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå°‘æ ·æœ¬å¯æ§å¯¹é½çš„æ–°å‹æ¡†æ¶ï¼Œå…¶ä¸­ç”¨æˆ·çš„æ½œåœ¨åå¥½æ˜¯ä»å…¶é€‰æ‹©çš„å°æ ·æœ¬ä¸­æ¨æ–­å‡ºæ¥çš„ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ‰©å±•äº†Bradley-Terry-Luceæ¨¡å‹ä»¥å¤„ç†å…·æœ‰æœªè§‚å¯Ÿåˆ°çš„å˜å¼‚æ€§å› ç´ çš„å¼‚è´¨åå¥½ï¼Œå¹¶æå‡ºäº†å…¶ç”¨äºå¥–åŠ±å»ºæ¨¡å’ŒLLMè°ƒå‚çš„å®é™…å®ç°ã€‚ç”±äºæˆ‘ä»¬æå‡ºçš„åŠŸèƒ½å‚æ•°ç©ºé—´è°ƒèŠ‚æ–¹æ³•ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶è®­ç»ƒçš„LLMå¯ä»¥åœ¨æ¨ç†æ—¶é€‚åº”ä¸ªäººåå¥½ï¼Œåœ¨è¿ç»­çš„è¡Œä¸ºæ¨¡å¼ä¸Šç”Ÿæˆè¾“å‡ºã€‚æˆ‘ä»¬é€šè¿‡å®è¯éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å®ƒä»¬åœ¨æ•°æ®é«˜æ•ˆçš„æ–¹å¼ä¸‹æ•æ‰å’Œä¸äººç±»å¤šæ ·åŒ–åå¥½ä¿æŒä¸€è‡´çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/kasia-kobalczyk/few-shot-steerable-alignment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kasia-kobalczyk/few-shot-steerable-alignmentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13998v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­çš„æ™®åŠå¯¹ç¡®ä¿å…¶ä¸ç”¨æˆ·å¤šæ ·åŒ–åå¥½çš„å¯¹é½æå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å‡è®¾ç”¨æˆ·ç›®æ ‡çš„ä¸€è‡´æ€§å¹¶ä¾èµ–äºå•ä¸€ç›®æ ‡çš„å¾®è°ƒã€‚ç„¶è€Œï¼Œäººç±»åå¥½æœ¬è´¨ä¸Šæ˜¯å¤šæ ·åŒ–çš„ï¼Œå—åˆ°å„ç§ä¸å¯è§‚å¯Ÿå› ç´ çš„å½±å“ï¼Œå¯¼è‡´åå¥½æ•°æ®ä¸­çš„å†²çªä¿¡å·ã€‚ä¸ºè§£å†³è¿™ä¸€å¤šæ ·æ€§ï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šç›®æ ‡è¿›è¡Œæ˜‚è´µçš„æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶æ¶‰åŠè®­ç»ƒå¤šä¸ªå¥–åŠ±æ¨¡å‹æˆ–LLMç­–ç•¥ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”ä¸åˆ‡å®é™…ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬å¯æ“æ§å¯¹é½æ¡†æ¶ï¼Œé€šè¿‡å°‘é‡ç”¨æˆ·é€‰æ‹©æ ·æœ¬æ¨æ–­ç”¨æˆ·çš„æ½œåœ¨åå¥½ã€‚æˆ‘ä»¬æ‰©å±•äº†Bradley-Terry-Luceæ¨¡å‹ä»¥å¤„ç†å…·æœ‰æœªè§‚å¯Ÿåˆ°çš„å˜å¼‚å› ç´ çš„å¼‚è´¨åå¥½ï¼Œå¹¶æå‡ºäº†å…¶å®ç”¨äºå¥–åŠ±å»ºæ¨¡å’ŒLLMå¾®è°ƒçš„å®é™…å®ç°ã€‚å€ŸåŠ©æˆ‘ä»¬çš„åŠŸèƒ½å‚æ•°ç©ºé—´æ¡ä»¶å¤„ç†æ–¹æ³•ï¼Œç”¨æˆ‘ä»¬çš„æ¡†æ¶è®­ç»ƒçš„LLMå¯ä»¥åœ¨æ¨ç†æ—¶é—´é€‚åº”ä¸ªäººåå¥½ï¼Œç”Ÿæˆä¸€ç³»åˆ—è¡Œä¸ºæ¨¡å¼çš„è¾“å‡ºã€‚æˆ‘ä»¬éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æ•°æ®é«˜æ•ˆæ–¹å¼ä¸‹æ•æ‰å’Œä¸äººç±»å¤šæ ·åŒ–åå¥½å¯¹é½çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç”¨æˆ·çš„å¤šæ ·åŒ–åå¥½å¯¹é½æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸åŸºäºå‡è®¾ç”¨æˆ·ç›®æ ‡çš„åŒè´¨æ€§å¹¶ä¾èµ–å•ä¸€ç›®æ ‡å¾®è°ƒï¼Œä½†è¿™å¹¶ä¸é€‚ç”¨äºäººç±»åå¥½çš„å¤šæ ·æ€§ã€‚</li>
<li>äººç±»åå¥½å—åˆ°å¤šç§ä¸å¯è§‚å¯Ÿå› ç´ çš„å½±å“ï¼Œå¯¼è‡´åå¥½æ•°æ®ä¸­çš„å†²çªä¿¡å·ã€‚</li>
<li>è§£å†³æ­¤é—®é¢˜éœ€è¦é€‚åº”å°‘é‡ç”¨æˆ·é€‰æ‹©çš„æ ·æœ¬æ•°æ®ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬å¯æ“æ§å¯¹é½æ¡†æ¶ï¼Œæ‰©å±•äº†Bradley-Terry-Luceæ¨¡å‹ä»¥å¤„ç†å…·æœ‰æœªè§‚å¯Ÿåˆ°çš„å˜å¼‚å› ç´ çš„å¼‚è´¨åå¥½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å®é™…å®ç°ç”¨äºå¥–åŠ±å»ºæ¨¡å’ŒLLMå¾®è°ƒçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6fe174039e4aaa57f5c8cf8d7806c26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58e109d601c98b7d17841e2c7f519242.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-954a0c9d8bac53512f36968a758ded8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fc96e12e3406a3e03c5374ebf33d2b0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Prompting-and-Few-Shot-Fine-Tuning-Revisiting-Document-Image-Classification-Using-Large-Language-Models"><a href="#Zero-Shot-Prompting-and-Few-Shot-Fine-Tuning-Revisiting-Document-Image-Classification-Using-Large-Language-Models" class="headerlink" title="Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image   Classification Using Large Language Models"></a>Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image   Classification Using Large Language Models</h2><p><strong>Authors:Anna Scius-Bertrand, Michael Jungo, Lars VÃ¶gtlin, Jean-Marc Spat, Andreas Fischer</strong></p>
<p>Classifying scanned documents is a challenging problem that involves image, layout, and text analysis for document understanding. Nevertheless, for certain benchmark datasets, notably RVL-CDIP, the state of the art is closing in to near-perfect performance when considering hundreds of thousands of training samples. With the advent of large language models (LLMs), which are excellent few-shot learners, the question arises to what extent the document classification problem can be addressed with only a few training samples, or even none at all. In this paper, we investigate this question in the context of zero-shot prompting and few-shot model fine-tuning, with the aim of reducing the need for human-annotated training samples as much as possible. </p>
<blockquote>
<p>åˆ†ç±»æ‰«ææ–‡æ¡£æ˜¯ä¸€ä¸ªæ¶‰åŠå›¾åƒã€å¸ƒå±€å’Œæ–‡æœ¬åˆ†æä»¥ç†è§£æ–‡æ¡£çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚ç„¶è€Œï¼Œå¯¹äºæŸäº›åŸºå‡†æ•°æ®é›†ï¼ˆå°¤å…¶æ˜¯RVL-CDIPï¼‰ï¼Œè€ƒè™‘åˆ°æ•°åä¸‡ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå½“å‰æŠ€æœ¯çš„çŠ¶æ€æ­£åœ¨æ¥è¿‘è¿‘ä¹å®Œç¾çš„æ€§èƒ½ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œè¿™äº›æ¨¡å‹æ˜¯å‡ºè‰²çš„å°‘æ ·æœ¬å­¦ä¹ è€…ï¼Œå› æ­¤å‡ºç°çš„é—®é¢˜æ˜¯ï¼Œä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬ï¼Œç”šè‡³å®Œå…¨ä¸ä½¿ç”¨æ ·æœ¬ï¼Œå¯ä»¥åœ¨å¤šå¤§ç¨‹åº¦ä¸Šè§£å†³æ–‡æ¡£åˆ†ç±»é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨é›¶æ ·æœ¬æç¤ºå’Œå°‘æ ·æœ¬æ¨¡å‹å¾®è°ƒçš„èƒŒæ™¯ä¸‹è°ƒæŸ¥äº†è¿™ä¸ªé—®é¢˜ï¼Œæ—¨åœ¨å°½å¯èƒ½å‡å°‘å¯¹äººå·¥æ³¨é‡Šè®­ç»ƒæ ·æœ¬çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13859v1">PDF</a> ICPR 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ–‡æ¡£åˆ†ç±»çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬æç¤ºå’Œå°‘é‡æ¨¡å‹å¾®è°ƒçš„ç¯å¢ƒä¸‹ã€‚æ–‡ç« æ—¨åœ¨é€šè¿‡å‡å°‘å¯¹äººç±»æ³¨é‡Šè®­ç»ƒæ ·æœ¬çš„éœ€æ±‚æ¥è§£å†³æ–‡æ¡£åˆ†ç±»é—®é¢˜ï¼Œå¯¹é›¶æ ·æœ¬å­¦ä¹ å’Œå°‘é‡è®­ç»ƒæ ·æœ¬çš„åœºæ™¯è¿›è¡Œæ·±å…¥åˆ†æã€‚é€šè¿‡æ­¤æ–¹å¼æå‡æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æ¡£åˆ†ç±»æ˜¯ä¸€ä¸ªæ¶‰åŠå›¾åƒã€å¸ƒå±€å’Œæ–‡æœ¬åˆ†æçš„é—®é¢˜ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°ï¼Œå°¤å…¶æ˜¯å…¶ä½œä¸ºä¼˜ç§€çš„å°‘æ ·æœ¬å­¦ä¹ è€…ï¼Œäººä»¬å¼€å§‹æ¢è®¨æ–‡æ¡£åˆ†ç±»é—®é¢˜æ˜¯å¦å¯ä»¥é€šè¿‡å°‘é‡æˆ–é›¶è®­ç»ƒæ ·æœ¬è§£å†³ã€‚</li>
<li>è¯¥è®ºæ–‡æ—¨åœ¨å‡å°‘æ–‡æ¡£åˆ†ç±»å¯¹å¤§é‡äººå·¥æ ‡æ³¨è®­ç»ƒæ ·æœ¬çš„ä¾èµ–ï¼Œæ¢ç©¶åœ¨é›¶æ ·æœ¬æç¤ºå’Œå°‘æ ·æœ¬æ¨¡å‹å¾®è°ƒæƒ…å¢ƒä¸‹çš„è§£å†³æ–¹æ¡ˆã€‚å…¶ä¸­â€œé›¶æ ·æœ¬å­¦ä¹ â€æŠ€æœ¯è‡³å…³é‡è¦ï¼Œæœºå™¨èƒ½ç†è§£å’Œæ¨æ–­äººç±»è¯­è¨€çš„é—´æ¥æ„å›¾ä¸æ¦‚å¿µè½¬ç§»çŸ¥è¯†ï¼Œä»è€Œå®ç°è·¨åœºæ™¯å­¦ä¹ ä¸ç†è§£ä»»åŠ¡è¿ç§»èƒ½åŠ›ã€‚è¿™ä¹Ÿç¬¦åˆå°‘æ ·æœ¬å­¦ä¹ çš„æ€æƒ³ç²¾é«“â€”â€”ä½¿ç”¨æœ‰é™æ ·æœ¬å®Œæˆå¤æ‚çš„ä»»åŠ¡å­¦ä¹ ã€‚è¿™ä¹Ÿæ˜¯å½“å‰äººå·¥æ™ºèƒ½ç ”ç©¶çš„é‡è¦è¶‹åŠ¿ä¹‹ä¸€ã€‚è¿™ä¸€æ–¹æ³•æœ‰æœ›è¿›ä¸€æ­¥æ¨è¿›æœºå™¨å­¦ä¹ çš„å®é™…åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨è®­ç»ƒæ ·æœ¬çš„é¢†åŸŸå¦‚æ–‡æ¡£åˆ†ç±»ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d545620c8036701c634f5f0f9266997e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-414d1c3268f18702de6a5061906b166b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2130c6755888cf58752c4485e8d3b385.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81f3193936bb969baccc0e9769154ef0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Extreme-Multi-label-Completion-for-Semantic-Document-Labelling-with-Taxonomy-Aware-Parallel-Learning"><a href="#Extreme-Multi-label-Completion-for-Semantic-Document-Labelling-with-Taxonomy-Aware-Parallel-Learning" class="headerlink" title="Extreme Multi-label Completion for Semantic Document Labelling with   Taxonomy-Aware Parallel Learning"></a>Extreme Multi-label Completion for Semantic Document Labelling with   Taxonomy-Aware Parallel Learning</h2><p><strong>Authors:Julien Audiffren, Christophe Broillet, Ljiljana Dolamic, Philippe CudrÃ©-Mauroux</strong></p>
<p>In Extreme Multi Label Completion (XMLCo), the objective is to predict the missing labels of a collection of documents. Together with XML Classification, XMLCo is arguably one of the most challenging document classification tasks, as the very high number of labels (at least ten of thousands) is generally very large compared to the number of available labelled documents in the training dataset. Such a task is often accompanied by a taxonomy that encodes the labels organic relationships, and many methods have been proposed to leverage this hierarchy to improve the results of XMLCo algorithms. In this paper, we propose a new approach to this problem, TAMLEC (Taxonomy-Aware Multi-task Learning for Extreme multi-label Completion). TAMLEC divides the problem into several Taxonomy-Aware Tasks, i.e. subsets of labels adapted to the hierarchical paths of the taxonomy, and trains on these tasks using a dynamic Parallel Feature sharing approach, where some parts of the model are shared between tasks while others are task-specific. Then, at inference time, TAMLEC uses the labels available in a document to infer the appropriate tasks and to predict missing labels. To achieve this result, TAMLEC uses a modified transformer architecture that predicts ordered sequences of labels on a Weak-Semilattice structure that is naturally induced by the tasks. This approach yields multiple advantages. First, our experiments on real-world datasets show that TAMLEC outperforms state-of-the-art methods for various XMLCo problems. Second, TAMLEC is by construction particularly suited for few-shots XML tasks, where new tasks or labels are introduced with only few examples, and extensive evaluations highlight its strong performance compared to existing methods. </p>
<blockquote>
<p>åœ¨æç«¯å¤šæ ‡ç­¾è¡¥å…¨ï¼ˆXMLCoï¼‰ä¸­ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ä¸€ç»„æ–‡æ¡£ä¸­çš„ç¼ºå¤±æ ‡ç­¾ã€‚ä¸XMLåˆ†ç±»ä¸€èµ·ï¼ŒXMLCoå¯ä»¥è¯´æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„æ–‡æ¡£åˆ†ç±»ä»»åŠ¡ä¹‹ä¸€ï¼Œå› ä¸ºæ ‡ç­¾æ•°é‡ï¼ˆè‡³å°‘æ•°ä¸‡ï¼‰é€šå¸¸æ¯”è®­ç»ƒæ•°æ®é›†ä¸­å¯ç”¨çš„å¸¦æ ‡ç­¾æ–‡æ¡£çš„æ•°é‡è¦å¤§å¾—å¤šã€‚æ­¤ç±»ä»»åŠ¡é€šå¸¸ä¼´éšç€å¯¹æ ‡ç­¾æœ‰æœºå…³ç³»è¿›è¡Œç¼–ç çš„åˆ†ç±»æ³•ï¼Œå·²ç»æå‡ºäº†è®¸å¤šæ–¹æ³•æ¥åˆ©ç”¨è¿™ä¸ªå±‚æ¬¡ç»“æ„æ¥æ”¹å–„XMLCoç®—æ³•çš„ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹è¿™ä¸ªé—®é¢˜æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼šTAMLECï¼ˆç”¨äºæç«¯å¤šæ ‡ç­¾è¡¥å…¨çš„åˆ†å±‚æ„ŸçŸ¥å¤šä»»åŠ¡å­¦ä¹ ï¼‰ã€‚TAMLECå°†é—®é¢˜åˆ†ä¸ºå¤šä¸ªå±‚æ¬¡æ„ŸçŸ¥ä»»åŠ¡ï¼Œå³é€‚åº”åˆ†ç±»å±‚æ¬¡è·¯å¾„çš„æ ‡ç­¾å­é›†ï¼Œå¹¶ä½¿ç”¨åŠ¨æ€å¹¶è¡Œç‰¹å¾å…±äº«æ–¹æ³•åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­æ¨¡å‹çš„éƒ¨åˆ†ç»„ä»¶åœ¨ä»»åŠ¡ä¹‹é—´æ˜¯å…±äº«çš„ï¼Œè€Œå…¶ä»–ä¸€äº›æ˜¯ç‰¹å®šäºä»»åŠ¡çš„ã€‚ç„¶åï¼Œåœ¨æ¨ç†æ—¶ï¼ŒTAMLECä½¿ç”¨æ–‡æ¡£ä¸­å¯ç”¨çš„æ ‡ç­¾æ¥æ¨æ–­é€‚å½“çš„ä»»åŠ¡å¹¶é¢„æµ‹ç¼ºå¤±çš„æ ‡ç­¾ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç»“æœï¼ŒTAMLECä½¿ç”¨äº†ä¸€ç§æ”¹è¿›çš„è½¬æ¢å™¨æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨å¼±åŠæ ¼ç»“æ„ä¸Šé¢„æµ‹æ ‡ç­¾çš„æœ‰åºåºåˆ—ï¼Œè¯¥ç»“æ„è‡ªç„¶æ˜¯ç”±ä»»åŠ¡å¼•èµ·çš„ã€‚è¿™ç§æ–¹æ³•å…·æœ‰å¤šä¸ªä¼˜ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTAMLECä¼˜äºå„ç§XMLCoé—®é¢˜çš„æœ€æ–°æ–¹æ³•ã€‚å…¶æ¬¡ï¼ŒTAMLECåœ¨æ„å»ºæ—¶ç‰¹åˆ«é€‚åˆå°æ ·æœ¬çš„XMLä»»åŠ¡ï¼Œå…¶ä¸­å¼•å…¥çš„æ–°ä»»åŠ¡æˆ–æ ‡ç­¾åªæœ‰å°‘æ•°å‡ ä¸ªç¤ºä¾‹ï¼Œå¹¶ä¸”å¹¿æ³›çš„è¯„ä¼°çªå‡ºäº†å…¶ä¸ç°æœ‰æ–¹æ³•çš„å¼ºå¤§æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13809v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹æç«¯å¤šæ ‡ç­¾è¡¥å…¨ä»»åŠ¡ï¼ˆXMLCoï¼‰çš„æ–°æ–¹æ³•TAMLECã€‚TAMLECå°†é—®é¢˜åˆ†ä¸ºå¤šä¸ªä¸ç¨æ”¶ç­‰çº§ç›¸ç¬¦çš„å­ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€å¹¶è¡Œç‰¹å¾å…±äº«æ–¹æ³•å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚åœ¨æ¨æ–­æ—¶ï¼ŒTAMLECåˆ©ç”¨æ–‡æ¡£ä¸­çš„ç°æœ‰æ ‡ç­¾æ¥æ¨æ–­é€‚å½“çš„ä»»åŠ¡å¹¶é¢„æµ‹ç¼ºå¤±çš„æ ‡ç­¾ã€‚è¯¥æ–¹æ³•åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œå°¤å…¶é€‚ç”¨äºåªæœ‰å°‘é‡ç¤ºä¾‹çš„æ–°ä»»åŠ¡æˆ–æ ‡ç­¾çš„XMLä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Extreme Multi Label Completion (XMLCo)æ—¨åœ¨é¢„æµ‹ä¸€ç»„æ–‡æ¡£ç¼ºå¤±çš„æ ‡ç­¾ï¼Œæ˜¯æå…·æŒ‘æˆ˜æ€§çš„æ–‡æ¡£åˆ†ç±»ä»»åŠ¡ä¹‹ä¸€ã€‚<br>2.TAMLECï¼ˆTaxonomies-Aware Multi-task Learning for Extreme multi-label Completionï¼‰æ˜¯ä¸€ç§æ–°çš„è§£å†³æ–¹æ³•ï¼Œå®ƒå°†é—®é¢˜åˆ’åˆ†ä¸ºå¤šä¸ªä¸ç¨æ”¶ç­‰çº§ç›¸ç¬¦çš„å­ä»»åŠ¡ã€‚<br>3.TAMLECåˆ©ç”¨åŠ¨æ€å¹¶è¡Œç‰¹å¾å…±äº«æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œéƒ¨åˆ†æ¨¡å‹éƒ¨ä»¶æ˜¯å…±äº«ä»»åŠ¡ï¼Œè€Œå…¶ä»–éƒ¨ä»¶æ˜¯ç‰¹å®šä»»åŠ¡çš„ã€‚<br>4.åœ¨æ¨æ–­æ—¶ï¼ŒTAMLECä½¿ç”¨æ–‡æ¡£ä¸­çš„ç°æœ‰æ ‡ç­¾æ¥æ¨æ–­é€‚å½“çš„ä»»åŠ¡å¹¶é¢„æµ‹ç¼ºå¤±çš„æ ‡ç­¾ã€‚<br>5.TAMLECé‡‡ç”¨ä¿®æ”¹åçš„è½¬æ¢å™¨æ¶æ„ï¼Œåœ¨å¼±åŠæ ¼ç»“æ„ä¸Šé¢„æµ‹æ ‡ç­¾çš„æœ‰åºåºåˆ—ï¼Œè¿™æ˜¯ç”±ä»»åŠ¡è‡ªç„¶å¼•å‘çš„ã€‚<br>6.å®éªŒè¡¨æ˜ï¼ŒTAMLECåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-064c5f737071fc0d4a8070bc4a6a9358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a54bd87c5258bd087da10faf01fb353a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-683c10f5d119fb1ffee31705bc2a559e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a73d8c514c1d64e6961093305fd3eace.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Modelling-Multi-modal-Cross-interaction-for-ML-FSIC-Based-on-Local-Feature-Selection"><a href="#Modelling-Multi-modal-Cross-interaction-for-ML-FSIC-Based-on-Local-Feature-Selection" class="headerlink" title="Modelling Multi-modal Cross-interaction for ML-FSIC Based on Local   Feature Selection"></a>Modelling Multi-modal Cross-interaction for ML-FSIC Based on Local   Feature Selection</h2><p><strong>Authors:Kun Yan, Zied Bouraoui, Fangyun Wei, Chang Xu, Ping Wang, Shoaib Jameel, Steven Schockaert</strong></p>
<p>The aim of multi-label few-shot image classification (ML-FSIC) is to assign semantic labels to images, in settings where only a small number of training examples are available for each label. A key feature of the multi-label setting is that images often have several labels, which typically refer to objects appearing in different regions of the image. When estimating label prototypes, in a metric-based setting, it is thus important to determine which regions are relevant for which labels, but the limited amount of training data and the noisy nature of local features make this highly challenging. As a solution, we propose a strategy in which label prototypes are gradually refined. First, we initialize the prototypes using word embeddings, which allows us to leverage prior knowledge about the meaning of the labels. Second, taking advantage of these initial prototypes, we then use a Loss Change Measurement~(LCM) strategy to select the local features from the training images (i.e.\ the support set) that are most likely to be representative of a given label. Third, we construct the final prototype of the label by aggregating these representative local features using a multi-modal cross-interaction mechanism, which again relies on the initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC, NUS-WIDE, and iMaterialist show that our model substantially improves the current state-of-the-art. </p>
<blockquote>
<p>å¤šæ ‡ç­¾å°æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆML-FSICï¼‰çš„ç›®æ ‡æ˜¯åœ¨æ¯ä¸ªæ ‡ç­¾åªæœ‰å°‘é‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œä¸ºå›¾åƒåˆ†é…è¯­ä¹‰æ ‡ç­¾ã€‚å¤šæ ‡ç­¾è®¾ç½®çš„ä¸€ä¸ªå…³é”®ç‰¹ç‚¹æ˜¯å›¾åƒé€šå¸¸å…·æœ‰å¤šä¸ªæ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾é€šå¸¸æŒ‡çš„æ˜¯å›¾åƒä¸­ä¸åŒåŒºåŸŸå‡ºç°çš„å¯¹è±¡ã€‚åœ¨åŸºäºåº¦é‡çš„ç¯å¢ƒä¸­ä¼°è®¡æ ‡ç­¾åŸå‹æ—¶ï¼Œç¡®å®šå“ªäº›åŒºåŸŸä¸å“ªäº›æ ‡ç­¾ç›¸å…³éå¸¸é‡è¦ï¼Œä½†è®­ç»ƒæ•°æ®çš„æœ‰é™æ€§å’Œå±€éƒ¨ç‰¹å¾çš„å™ªå£°æ€§è´¨ä½¿å¾—è¿™æå…·æŒ‘æˆ˜æ€§ã€‚ä½œä¸ºä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€æ¸å®Œå–„æ ‡ç­¾åŸå‹çš„ç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¯åµŒå…¥æ¥åˆå§‹åŒ–åŸå‹ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨æœ‰å…³æ ‡ç­¾å«ä¹‰çš„å…ˆéªŒçŸ¥è¯†ã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨è¿™äº›åˆå§‹åŸå‹ï¼Œæˆ‘ä»¬é‡‡ç”¨æŸå¤±å˜åŒ–æµ‹é‡ï¼ˆLCMï¼‰ç­–ç•¥æ¥é€‰æ‹©è®­ç»ƒå›¾åƒï¼ˆå³æ”¯æŒé›†ï¼‰ä¸­æœ€å¯èƒ½ä»£è¡¨ç»™å®šæ ‡ç­¾çš„å±€éƒ¨ç‰¹å¾ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡æ€äº¤å‰äº¤äº’æœºåˆ¶èšåˆè¿™äº›ä»£è¡¨æ€§å±€éƒ¨ç‰¹å¾ï¼Œæ„å»ºæ ‡ç­¾çš„æœ€ç»ˆåŸå‹ï¼Œè¿™åŒæ ·ä¾èµ–äºæœ€åˆçš„åŸºäºè¯åµŒå…¥çš„åŸå‹ã€‚åœ¨COCOã€PASCAL VOCã€NUS-WIDEå’ŒiMaterialistä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¤§å¹…æå‡äº†å½“å‰çš„æœ€ä½³æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13732v1">PDF</a> Accepted in Transactions on Multimedia Computing Communications and   Applications</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ ‡ç­¾å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆML-FSICï¼‰çš„ç›®æ ‡ï¼Œå³åœ¨æ¯ä¸ªæ ‡ç­¾åªæœ‰å°‘é‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå¯¹å›¾åƒè¿›è¡Œè¯­ä¹‰æ ‡ç­¾åˆ†é…ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºåº¦é‡æ–¹æ³•çš„æ ‡ç­¾åŸå‹ä¼°è®¡ç­–ç•¥ï¼Œé€šè¿‡é€æ­¥ä¼˜åŒ–æ ‡ç­¾åŸå‹æ¥è§£å†³è®­ç»ƒæ•°æ®æœ‰é™å’Œå±€éƒ¨ç‰¹å¾å™ªå£°çš„é—®é¢˜ã€‚é¦–å…ˆï¼Œä½¿ç”¨è¯åµŒå…¥åˆå§‹åŒ–æ ‡ç­¾åŸå‹ä»¥åˆ©ç”¨æ ‡ç­¾çš„å…ˆéªŒçŸ¥è¯†ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨æŸå¤±å˜åŒ–æµ‹é‡ï¼ˆLCMï¼‰ç­–ç•¥é€‰æ‹©æœ€å¯èƒ½ä»£è¡¨ç»™å®šæ ‡ç­¾çš„è®­ç»ƒå›¾åƒå±€éƒ¨ç‰¹å¾ï¼›æœ€åï¼Œé€šè¿‡å¤šæ¨¡æ€äº¤å‰äº¤äº’æœºåˆ¶èšåˆè¿™äº›ä»£è¡¨æ€§å±€éƒ¨ç‰¹å¾æ¥æ„å»ºæœ€ç»ˆçš„æ ‡ç­¾åŸå‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨COCOã€PASCAL VOCã€NUS-WIDEå’ŒiMaterialistæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†å½“å‰æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ ‡ç­¾å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆML-FSICï¼‰çš„ç›®æ ‡æ˜¯åœ¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬ä¸‹ï¼Œä¸ºå›¾åƒåˆ†é…å¤šä¸ªè¯­ä¹‰æ ‡ç­¾ã€‚</li>
<li>æ ‡ç­¾åŸå‹ä¼°è®¡åœ¨ML-FSICä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºå›¾åƒé€šå¸¸å…·æœ‰å¤šä¸ªæ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾é€šå¸¸æŒ‡ä»£å›¾åƒä¸åŒåŒºåŸŸçš„ç‰©ä½“ã€‚</li>
<li>æå‡ºçš„ç­–ç•¥é€šè¿‡é€æ­¥ä¼˜åŒ–æ ‡ç­¾åŸå‹æ¥è§£å†³è®­ç»ƒæ•°æ®é™åˆ¶å’Œå±€éƒ¨ç‰¹å¾å™ªå£°é—®é¢˜ã€‚</li>
<li>åˆå§‹åŒ–æ ‡ç­¾åŸå‹æ—¶ä½¿ç”¨è¯åµŒå…¥ï¼Œä»¥åˆ©ç”¨æ ‡ç­¾çš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é‡‡ç”¨æŸå¤±å˜åŒ–æµ‹é‡ï¼ˆLCMï¼‰ç­–ç•¥é€‰æ‹©æœ€å¯èƒ½ä»£è¡¨ç»™å®šæ ‡ç­¾çš„è®­ç»ƒå›¾åƒå±€éƒ¨ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€äº¤å‰äº¤äº’æœºåˆ¶èšåˆä»£è¡¨æ€§å±€éƒ¨ç‰¹å¾æ¥æ„å»ºæœ€ç»ˆçš„æ ‡ç­¾åŸå‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-640f13acf14d7b037c48a4f37ae50135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02bd4166c52127f4dcc44d1e0849b94.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AnchorInv-Few-Shot-Class-Incremental-Learning-of-Physiological-Signals-via-Representation-Space-Guided-Inversion"><a href="#AnchorInv-Few-Shot-Class-Incremental-Learning-of-Physiological-Signals-via-Representation-Space-Guided-Inversion" class="headerlink" title="AnchorInv: Few-Shot Class-Incremental Learning of Physiological Signals   via Representation Space Guided Inversion"></a>AnchorInv: Few-Shot Class-Incremental Learning of Physiological Signals   via Representation Space Guided Inversion</h2><p><strong>Authors:Chenqi Li, Boyan Gao, Gabriel Jones, Timothy Denison, Tingting Zhu</strong></p>
<p>Deep learning models have demonstrated exceptional performance in a variety of real-world applications. These successes are often attributed to strong base models that can generalize to novel tasks with limited supporting data while keeping prior knowledge intact. However, these impressive results are based on the availability of a large amount of high-quality data, which is often lacking in specialized biomedical applications. In such fields, models are usually developed with limited data that arrive incrementally with novel categories. This requires the model to adapt to new information while preserving existing knowledge. Few-Shot Class-Incremental Learning (FSCIL) methods offer a promising approach to addressing these challenges, but they also depend on strong base models that face the same aforementioned limitations. To overcome these constraints, we propose AnchorInv following the straightforward and efficient buffer-replay strategy. Instead of selecting and storing raw data, AnchorInv generates synthetic samples guided by anchor points in the feature space. This approach protects privacy and regularizes the model for adaptation. When evaluated on three public physiological time series datasets, AnchorInv exhibits efficient knowledge forgetting prevention and improved adaptation to novel classes, surpassing state-of-the-art baselines. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤šç§å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›æˆåŠŸå¾€å¾€å½’åŠŸäºèƒ½å¤Ÿæ¨å¹¿åˆ°æ–°å‹ä»»åŠ¡ä¸”ä¿æŒå…ˆéªŒçŸ¥è¯†ä¸å˜çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœæ˜¯åŸºäºå¤§é‡é«˜è´¨é‡æ•°æ®çš„å¯ç”¨æ€§ï¼Œè¿™åœ¨ä¸“é—¨çš„ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­å¾€å¾€ç¼ºä¹ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ¨¡å‹é€šå¸¸æ˜¯åœ¨æœ‰é™çš„æ•°æ®ä¸Šå¼€å‘çš„ï¼Œè¿™äº›æ•°æ®ä¼šéšç€æ—¶é—´çš„æ¨ç§»é™†ç»­å‡ºç°æ–°çš„ç±»åˆ«ã€‚è¿™è¦æ±‚æ¨¡å‹åœ¨é€‚åº”æ–°ä¿¡æ¯çš„åŒæ—¶ä¿æŒç°æœ‰çŸ¥è¯†ã€‚å°æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ–¹æ³•ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ€è·¯ï¼Œä½†å®ƒä»¬åŒæ ·ä¾èµ–äºä¸Šè¿°æåˆ°çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç®€å•é«˜æ•ˆç¼“å†²å›æ”¾ç­–ç•¥çš„AnchorInvã€‚AnchorInvä¸é€‰æ‹©å’Œå­˜å‚¨åŸå§‹æ•°æ®ï¼Œè€Œæ˜¯æ ¹æ®ç‰¹å¾ç©ºé—´ä¸­çš„é”šç‚¹ç”Ÿæˆåˆæˆæ ·æœ¬ã€‚è¿™ç§æ–¹æ³•ä¿æŠ¤äº†éšç§å¹¶ä¿ƒè¿›äº†æ¨¡å‹çš„é€‚åº”ã€‚åœ¨ä¸‰ä¸ªå…¬å…±ç”Ÿç†æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°æ—¶ï¼ŒAnchorInvè¡¨ç°å‡ºäº†æœ‰æ•ˆçš„çŸ¥è¯†é—å¿˜é¢„é˜²å’Œå¯¹æ–°ç±»åˆ«çš„è‰¯å¥½é€‚åº”èƒ½åŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€ä½³åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13714v1">PDF</a> AAAI-25 Extended Version</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤šç§å®é™…åº”ç”¨ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¿™å½’åŠŸäºèƒ½å¤Ÿåœ¨æœ‰é™æ•°æ®æ”¯æŒä¸‹æ³›åŒ–åˆ°æ–°ä»»åŠ¡å¹¶ä¿ç•™å…ˆéªŒçŸ¥è¯†çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æˆæœå»ºç«‹åœ¨å¤§é‡é«˜è´¨é‡æ•°æ®å¯ç”¨æ€§çš„åŸºç¡€ä¸Šï¼Œè¿™åœ¨ä¸“é—¨çš„ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­å¾€å¾€ç¼ºä¹ã€‚åœ¨è¿™äº›é¢†åŸŸï¼Œæ¨¡å‹é€šå¸¸ä½¿ç”¨æœ‰é™ä¸”é™†ç»­åˆ°æ¥çš„æ–°ç±»åˆ«æ•°æ®è¿›è¡Œå¼€å‘ï¼Œè¿™è¦æ±‚æ¨¡å‹åœ¨é€‚åº”æ–°ä¿¡æ¯çš„åŒæ—¶ä¿ç•™ç°æœ‰çŸ¥è¯†ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒFew-Shot Class-Incremental Learningï¼ˆFSCILï¼‰æ–¹æ³•æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬ä¹Ÿä¾èµ–äºé¢ä¸´ç›¸åŒé™åˆ¶çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ã€‚ä¸ºäº†å…‹æœè¿™äº›åˆ¶çº¦å› ç´ ï¼Œæå‡ºäº†AnchorInvæ–¹æ³•ï¼Œé‡‡ç”¨ç®€å•é«˜æ•ˆçš„ç¼“å†²åŒºå›æ”¾ç­–ç•¥ã€‚AnchorInvé€šè¿‡ç‰¹å¾ç©ºé—´ä¸­çš„é”šç‚¹ç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œè€Œä¸æ˜¯é€‰æ‹©å’Œå­˜å‚¨åŸå§‹æ•°æ®ï¼Œè¿™ç§æ–¹æ³•ä¿æŠ¤äº†éšç§å¹¶ä¿ƒè¿›äº†æ¨¡å‹çš„é€‚åº”æ€§è°ƒæ•´ã€‚åœ¨ä¸‰ä¸ªå…¬å…±ç”Ÿç†æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒAnchorInvæœ‰æ•ˆåœ°é˜²æ­¢äº†çŸ¥è¯†é—å¿˜ï¼Œå¹¶æé«˜äº†å¯¹æ–°ç±»åˆ«çš„é€‚åº”èƒ½åŠ›ï¼Œè¶…è¶Šäº†æœ€æ–°çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤šç§åº”ç”¨ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå½’åŠŸäºå¼ºå¤§çš„åŸºç¡€æ¨¡å‹ã€‚</li>
<li>åœ¨ç”Ÿç‰©åŒ»å­¦ç­‰ç‰¹å®šé¢†åŸŸï¼Œæ•°æ®æœ‰é™ä¸”é™†ç»­åˆ°æ¥ï¼Œè¦æ±‚æ¨¡å‹èƒ½é€‚åº”æ–°ä¿¡æ¯å¹¶ä¿ç•™ç°æœ‰çŸ¥è¯†ã€‚</li>
<li>Few-Shot Class-Incremental Learningï¼ˆFSCILï¼‰æ–¹æ³•ç”¨äºè§£å†³æ­¤æŒ‘æˆ˜ã€‚</li>
<li>AnchorInvæ–¹æ³•é‡‡ç”¨ç¼“å†²åŒºå›æ”¾ç­–ç•¥ï¼Œé€šè¿‡ç‰¹å¾ç©ºé—´ä¸­çš„é”šç‚¹ç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œä»¥ä¿æŠ¤éšç§å¹¶ä¿ƒè¿›æ¨¡å‹é€‚åº”æ€§è°ƒæ•´ã€‚</li>
<li>AnchorInvåœ¨ç”Ÿç†æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨ç°ä¼˜è¶Šï¼Œæœ‰æ•ˆé˜²æ­¢çŸ¥è¯†é—å¿˜ï¼Œæé«˜å¯¹æ–°ç±»åˆ«çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>AnchorInvæ–¹æ³•ä¾èµ–äºå¼ºå¤§åŸºç¡€æ¨¡å‹ï¼Œä½†ä»å­˜åœ¨æ•°æ®é™åˆ¶çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f6779e8c9baf5cefc545dd99cae3e41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12654225ce6a14df6c590759f90b6af3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de54d3694115c842cb0d2395abebeb69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6d5d29141579371abf5270e291ab5c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cffc6247f383099fd42d2b7fe266e40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa0ee4e1e9f9c2c4546e47a429f76ee3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d12735a7ef970b7c504f3f383a666503.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Fine-Tuning-of-Single-Cell-Foundation-Models-Enables-Zero-Shot-Molecular-Perturbation-Prediction"><a href="#Efficient-Fine-Tuning-of-Single-Cell-Foundation-Models-Enables-Zero-Shot-Molecular-Perturbation-Prediction" class="headerlink" title="Efficient Fine-Tuning of Single-Cell Foundation Models Enables Zero-Shot   Molecular Perturbation Prediction"></a>Efficient Fine-Tuning of Single-Cell Foundation Models Enables Zero-Shot   Molecular Perturbation Prediction</h2><p><strong>Authors:Sepideh Maleki, Jan-Christian Huetter, Kangway V. Chuang, Gabriele Scalia, Tommaso Biancalani</strong></p>
<p>Predicting transcriptional responses to novel drugs provides a unique opportunity to accelerate biomedical research and advance drug discovery efforts. However, the inherent complexity and high dimensionality of cellular responses, combined with the extremely limited available experimental data, makes the task challenging. In this study, we leverage single-cell foundation models (FMs) pre-trained on tens of millions of single cells, encompassing multiple cell types, states, and disease annotations, to address molecular perturbation prediction. We introduce a drug-conditional adapter that allows efficient fine-tuning by training less than 1% of the original foundation model, thus enabling molecular conditioning while preserving the rich biological representation learned during pre-training. The proposed strategy allows not only the prediction of cellular responses to novel drugs, but also the zero-shot generalization to unseen cell lines. We establish a robust evaluation framework to assess model performance across different generalization tasks, demonstrating state-of-the-art results across all settings, with significant improvements in the few-shot and zero-shot generalization to new cell lines compared to existing baselines. </p>
<blockquote>
<p>é¢„æµ‹æ–°å‹è¯ç‰©çš„è½¬å½•ååº”ä¸ºåŠ é€Ÿç”Ÿç‰©åŒ»å­¦ç ”ç©¶å’Œæ¨è¿›è¯ç‰©å‘ç°åŠªåŠ›æä¾›äº†ç‹¬ç‰¹çš„æœºä¼šã€‚ç„¶è€Œï¼Œç»†èƒååº”çš„å›ºæœ‰å¤æ‚æ€§å’Œé«˜ç»´æ€§ï¼Œä»¥åŠå¯ç”¨çš„å®éªŒæ•°æ®æä¸ºæœ‰é™ï¼Œä½¿å¾—è¿™ä¸€ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºäºå•ç»†èƒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆFMsï¼‰ï¼Œè¯¥æ¨¡å‹åŸºäºæ•°åƒä¸‡ä¸ªå•ç»†èƒè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–å¤šç§ç»†èƒç±»å‹ã€çŠ¶æ€å’Œç–¾ç—…æ³¨é‡Šï¼Œæ¥è§£å†³åˆ†å­æ‰°åŠ¨é¢„æµ‹é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯ç‰©æ¡ä»¶é€‚é…å™¨ï¼Œé€šè¿‡è®­ç»ƒä¸åˆ°åŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„1%ï¼Œå®ç°äº†é«˜æ•ˆçš„å¾®è°ƒï¼Œä»è€Œåœ¨ä¿æŒé¢„è®­ç»ƒæœŸé—´å­¦ä¹ çš„ä¸°å¯Œç”Ÿç‰©å­¦è¡¨å¾çš„åŒæ—¶ï¼Œå®ç°äº†åˆ†å­è°ƒèŠ‚ã€‚æ‰€æå‡ºçš„ç­–ç•¥ä¸ä»…å…è®¸é¢„æµ‹æ–°å‹è¯ç‰©çš„ç»†èƒååº”ï¼Œè€Œä¸”å¯ä»¥åœ¨æœªè§è¿‡çš„ç»†èƒç³»ä¸­å®ç°é›¶æ ·æœ¬æ³›åŒ–ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç¨³å¥çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ³›åŒ–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œåœ¨æ‰€æœ‰è®¾ç½®ä¸­éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼Œåœ¨æ–°ç»†èƒç³»çš„å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„æµ‹æ–°å‹è¯ç‰©çš„è½¬å½•ååº”ä¸ºåŠ é€Ÿç”Ÿç‰©åŒ»å­¦ç ”ç©¶å’Œæ¨åŠ¨è¯ç‰©å‘ç°æä¾›äº†ç‹¬ç‰¹çš„æœºä¼šã€‚æœ¬ç ”ç©¶åˆ©ç”¨åŸºäºå•ç»†èƒé¢„è®­ç»ƒæ¨¡å‹åº”å¯¹åˆ†å­æ‰°åŠ¨é¢„æµ‹çš„æŒ‘æˆ˜ï¼Œé€šè¿‡å¼•å…¥è¯ç‰©æ¡ä»¶é€‚é…å™¨å®ç°é«˜æ•ˆå¾®è°ƒï¼Œåªéœ€è®­ç»ƒåŸé¢„è®­ç»ƒæ¨¡å‹çš„ä¸åˆ°ç™¾åˆ†ä¹‹ä¸€ï¼Œä»è€Œåœ¨ä¿ç•™é¢„è®­ç»ƒä¸°å¯Œç”Ÿç‰©å­¦è¡¨å¾çš„åŒæ—¶å®ç°åˆ†å­æ¡ä»¶åŒ–ã€‚è¯¥ç­–ç•¥ä¸ä»…å¯é¢„æµ‹æ–°å‹è¯ç‰©çš„ç»†èƒååº”ï¼Œè¿˜å¯å®ç°é›¶æ ·æœ¬æ³›åŒ–è‡³æœªè§è¿‡çš„ç»†èƒç³»ã€‚æˆ‘ä»¬å»ºç«‹äº†ç¨³å¥çš„è¯„ä¼°æ¡†æ¶ä»¥è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ³›åŒ–ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºåœ¨å„ç§è®¾ç½®ä¸­å‡è¾¾åˆ°æœ€æ–°ç»“æœï¼Œä¸æ–°ç»†èƒç³»çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ³›åŒ–æ–¹é¢è¾ƒç°æœ‰åŸºçº¿æœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å•ç»†èƒé¢„è®­ç»ƒæ¨¡å‹é¢„æµ‹æ–°å‹è¯ç‰©çš„è½¬å½•ååº”ï¼ŒåŠ é€Ÿç”Ÿç‰©åŒ»å­¦ç ”ç©¶å’Œè¯ç‰©å‘ç°ã€‚</li>
<li>å¼•å…¥è¯ç‰©æ¡ä»¶é€‚é…å™¨ï¼Œå®ç°é«˜æ•ˆå¾®è°ƒå¹¶ä¿ç•™é¢„è®­ç»ƒçš„ä¸°å¯Œç”Ÿç‰©å­¦è¡¨å¾ã€‚</li>
<li>è¯¥ç­–ç•¥ä¸ä»…èƒ½é¢„æµ‹æ–°å‹è¯ç‰©çš„ç»†èƒååº”ï¼Œè¿˜èƒ½å®ç°é›¶æ ·æœ¬æ³›åŒ–è‡³æœªè§è¿‡çš„ç»†èƒç³»ã€‚</li>
<li>å»ºç«‹ç¨³å¥çš„è¯„ä¼°æ¡†æ¶ä»¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å„ç§è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºæœ€æ–°ç»“æœã€‚</li>
<li>ä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼Œåœ¨æ–°ç»†èƒç³»çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ³›åŒ–æ–¹é¢æœ‰æ˜æ˜¾æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0a4a0fe20de97e4dae88efc59f2dd545.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb5b73f82061e4f6cf9c4bf6100b1ffc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-280dc160f4b379de2046d44cb36c23e7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="An-Agentic-Approach-to-Automatic-Creation-of-P-ID-Diagrams-from-Natural-Language-Descriptions"><a href="#An-Agentic-Approach-to-Automatic-Creation-of-P-ID-Diagrams-from-Natural-Language-Descriptions" class="headerlink" title="An Agentic Approach to Automatic Creation of P&amp;ID Diagrams from Natural   Language Descriptions"></a>An Agentic Approach to Automatic Creation of P&amp;ID Diagrams from Natural   Language Descriptions</h2><p><strong>Authors:Shreeyash Gowaikar, Srinivasan Iyengar, Sameer Segal, Shivkumar Kalyanaraman</strong></p>
<p>The Piping and Instrumentation Diagrams (P&amp;IDs) are foundational to the design, construction, and operation of workflows in the engineering and process industries. However, their manual creation is often labor-intensive, error-prone, and lacks robust mechanisms for error detection and correction. While recent advancements in Generative AI, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), have demonstrated significant potential across various domains, their application in automating generation of engineering workflows remains underexplored. In this work, we introduce a novel copilot for automating the generation of P&amp;IDs from natural language descriptions. Leveraging a multi-step agentic workflow, our copilot provides a structured and iterative approach to diagram creation directly from Natural Language prompts. We demonstrate the feasibility of the generation process by evaluating the soundness and completeness of the workflow, and show improved results compared to vanilla zero-shot and few-shot generation approaches. </p>
<blockquote>
<p>ç®¡é“ä¸ä»ªè¡¨å›¾ï¼ˆP&amp;IDsï¼‰æ˜¯å·¥ç¨‹å’Œå·¥è‰ºè¡Œä¸šå·¥ä½œæµç¨‹è®¾è®¡ã€æ„å»ºå’Œè¿è¥çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œå…¶æ‰‹åŠ¨åˆ›å»ºå¾€å¾€åŠ³åŠ¨å¼ºåº¦å¤§ã€æ˜“å‡ºé”™ï¼Œä¸”ç¼ºä¹ç¨³å¥çš„è¯¯å·®æ£€æµ‹å’Œæ ¡æ­£æœºåˆ¶ã€‚è™½ç„¶æœ€è¿‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²ç»åœ¨å„ä¸ªé¢†åŸŸå±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆå·¥ç¨‹å·¥ä½œæµç¨‹æ–¹é¢çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹åŠ©æ‰‹ï¼Œç”¨äºæ ¹æ®è‡ªç„¶è¯­è¨€æè¿°è‡ªåŠ¨ç”Ÿæˆç®¡é“ä¸ä»ªè¡¨å›¾ã€‚é€šè¿‡åˆ©ç”¨å¤šæ­¥éª¤ä»£ç†å·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬çš„åŠ©æ‰‹æä¾›äº†ç»“æ„åŒ–ã€è¿­ä»£åŒ–çš„æ–¹æ³•ï¼Œç›´æ¥ä»è‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå›¾è¡¨ã€‚æˆ‘ä»¬é€šè¿‡è¯„ä¼°å·¥ä½œæµç¨‹çš„å¥å…¨æ€§å’Œå®Œæ•´æ€§æ¥éªŒè¯ç”Ÿæˆè¿‡ç¨‹çš„å¯è¡Œæ€§ï¼Œå¹¶å±•ç¤ºäº†ç›¸è¾ƒäºæ™®é€šé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ›´å¥½çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12898v1">PDF</a> Accepted at the AAAIâ€™25 Workshop on AI to Accelerate Science and   Engineering (AI2ASE)</p>
<p><strong>Summary</strong></p>
<p>å·¥ç¨‹æµç¨‹å›¾å’Œä»ªå™¨æµç¨‹å›¾ï¼ˆP&amp;IDsï¼‰æ˜¯å·¥ç¨‹å’Œå·¥è‰ºè¡Œä¸šå·¥ä½œæµç¨‹è®¾è®¡ã€æ„å»ºå’Œè¿è¥çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œå…¶æ‰‹åŠ¨åˆ›å»ºå¾€å¾€åŠ³åŠ¨å¼ºåº¦å¤§ã€æ˜“å‡ºé”™ï¼Œä¸”ç¼ºä¹å¯é çš„é”™è¯¯æ£€æµ‹å’Œçº æ­£æœºåˆ¶ã€‚æœ€è¿‘ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆå°¤å…¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰åœ¨å„é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆå·¥ç¨‹å·¥ä½œæµç¨‹æ–¹é¢çš„åº”ç”¨ä»è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹P&amp;IDsè‡ªåŠ¨ç”ŸæˆåŠ©æ‰‹ï¼Œå®ƒèƒ½ç›´æ¥ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­ç”Ÿæˆæµç¨‹å›¾ã€‚å€ŸåŠ©å¤šæ­¥éª¤çš„ä»£ç†å·¥ä½œæµç¨‹ï¼Œè¯¥åŠ©æ‰‹æä¾›ç»“æ„åŒ–å’Œè¿­ä»£å¼çš„å›¾è¡¨åˆ›å»ºæ–¹æ³•ã€‚é€šè¿‡è¯„ä¼°å·¥ä½œæµç¨‹çš„å¥å…¨æ€§å’Œå®Œæ•´æ€§ï¼Œå±•ç¤ºäº†ç”Ÿæˆè¿‡ç¨‹çš„å¯è¡Œæ€§ï¼Œå¹¶è¡¨ç°å‡ºç›¸è¾ƒäºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç”Ÿæˆæ–¹æ³•çš„æ”¹è¿›ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>P&amp;IDsåœ¨å·¥ç¨‹å’Œå·¥è‰ºè¡Œä¸šä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶æ‰‹åŠ¨åˆ›å»ºè¿‡ç¨‹åŠ³åŠ¨å¼ºåº¦å¤§ã€æ˜“å‡ºé”™ã€‚</li>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨è‡ªåŠ¨åŒ–ç”Ÿæˆå·¥ç¨‹å·¥ä½œæµç¨‹æ–¹é¢çš„åº”ç”¨æ½œåŠ›å·¨å¤§ï¼Œä½†ç›¸å…³ç ”ç©¶ä»æœ‰é™ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„P&amp;IDsè‡ªåŠ¨ç”ŸæˆåŠ©æ‰‹ï¼Œèƒ½ç›´æ¥ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­ç”Ÿæˆæµç¨‹å›¾ã€‚</li>
<li>è¯¥åŠ©æ‰‹é‡‡ç”¨å¤šæ­¥éª¤çš„ä»£ç†å·¥ä½œæµç¨‹ï¼Œæä¾›ç»“æ„åŒ–å’Œè¿­ä»£å¼çš„å›¾è¡¨åˆ›å»ºæ–¹æ³•ã€‚</li>
<li>è¯¥è‡ªåŠ¨ç”ŸæˆåŠ©æ‰‹çš„å¯è¡Œæ€§é€šè¿‡è¯„ä¼°å·¥ä½œæµç¨‹çš„å¥å…¨æ€§å’Œå®Œæ•´æ€§å¾—ä»¥éªŒè¯ã€‚</li>
<li>ä¸é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥åŠ©æ‰‹å±•ç°å‡ºæ˜æ˜¾çš„æ”¹è¿›ã€‚</li>
<li>è¿™ä¸€ç ”ç©¶ä¸ºå·¥ç¨‹å’Œå·¥è‰ºè¡Œä¸šçš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–å‘å±•å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-009e6b6d1a78b3704998a50af84e3715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be680d39e5abf37802c1b4facffb31cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e13cd02d350c010a8f3b31abef93997a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d7e3ef8c1b5ddbcb01598af390fcffb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2651ce9b407106e31d9045d602d8af7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Question-How-do-Large-Language-Models-perform-on-the-Question-Answering-tasks-Answer"><a href="#Question-How-do-Large-Language-Models-perform-on-the-Question-Answering-tasks-Answer" class="headerlink" title="Question: How do Large Language Models perform on the Question Answering   tasks? Answer:"></a>Question: How do Large Language Models perform on the Question Answering   tasks? Answer:</h2><p><strong>Authors:Kevin Fischer, Darren FÃ¼rst, Sebastian Steindl, Jakob Lindner, Ulrich SchÃ¤fer</strong></p>
<p>Large Language Models (LLMs) have been showing promising results for various NLP-tasks without the explicit need to be trained for these tasks by using few-shot or zero-shot prompting techniques. A common NLP-task is question-answering (QA). In this study, we propose a comprehensive performance comparison between smaller fine-tuned models and out-of-the-box instruction-following LLMs on the Stanford Question Answering Dataset 2.0 (SQuAD2), specifically when using a single-inference prompting technique. Since the dataset contains unanswerable questions, previous work used a double inference method. We propose a prompting style which aims to elicit the same ability without the need for double inference, saving compute time and resources. Furthermore, we investigate their generalization capabilities by comparing their performance on similar but different QA datasets, without fine-tuning neither model, emulating real-world uses where the context and questions asked may differ from the original training distribution, for example swapping Wikipedia for news articles.   Our results show that smaller, fine-tuned models outperform current State-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are able to close this gap on the out-of-distribution test and even outperform the fine-tuned models on 3 of the 5 tested QA datasets. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§NLPä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººç©ç›®çš„ç»“æœï¼Œè¿™äº›ä»»åŠ¡æ— éœ€é€šè¿‡å°‘é‡æ ·æœ¬æˆ–é›¶æ ·æœ¬æç¤ºæŠ€æœ¯è¿›è¡Œç‰¹å®šè®­ç»ƒã€‚ä¸€ä¸ªå¸¸è§çš„NLPä»»åŠ¡æ˜¯é—®ç­”ï¼ˆQAï¼‰ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹ç»è¿‡å¾®è°ƒçš„å°å‹æ¨¡å‹ä¸å³æ’å³ç”¨çš„æŒ‡ä»¤éµå¾ªå‹LLMåœ¨Stanford Question Answering Dataset 2.0ï¼ˆSQuAD2ï¼‰ä¸Šçš„ç»¼åˆæ€§èƒ½è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒï¼Œç‰¹åˆ«æ˜¯å½“ä½¿ç”¨å•ä¸€æ¨ç†æç¤ºæŠ€æœ¯æ—¶ã€‚ç”±äºè¯¥æ•°æ®é›†åŒ…å«æ— æ³•å›ç­”çš„é—®é¢˜ï¼Œä¹‹å‰çš„å·¥ä½œä½¿ç”¨äº†åŒé‡æ¨ç†æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æç¤ºé£æ ¼ï¼Œæ—¨åœ¨åœ¨ä¸ä½¿ç”¨åŒé‡æ¨ç†çš„æƒ…å†µä¸‹æ¿€å‘ç›¸åŒçš„èƒ½åŠ›ï¼Œä»è€ŒèŠ‚çœè®¡ç®—æ—¶é—´å’Œèµ„æºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒä»–ä»¬åœ¨ç±»ä¼¼ä½†ä¸åŒçš„QAæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ¥è°ƒæŸ¥ä»–ä»¬çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ— éœ€å¯¹ä»»ä½•æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„ä½¿ç”¨æƒ…å†µï¼Œå…¶ä¸­ä¸Šä¸‹æ–‡å’Œæ‰€æé—®é¢˜å¯èƒ½ä¸åŸå§‹è®­ç»ƒåˆ†å¸ƒæœ‰æ‰€ä¸åŒï¼Œä¾‹å¦‚å°†Wikipediaæ›¿æ¢ä¸ºæ–°é—»æ–‡ç« ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„å°å‹æ¨¡å‹åœ¨å½“å‰å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¾®è°ƒä»»åŠ¡ä¸Šçš„è¡¨ç°æ›´å‡ºè‰²ï¼Œä½†æœ€è¿‘å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨è¶…å‡ºåˆ†å¸ƒæµ‹è¯•ä¸Šç¼©å°è¿™ä¸€å·®è·ï¼Œç”šè‡³åœ¨äº”ä¸ªæµ‹è¯•é—®ç­”æ•°æ®é›†çš„ä¸‰ä¸ªä¸Šè¡¨ç°è¶…è¿‡ç»è¿‡å¾®è°ƒçš„å°å‹æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12893v1">PDF</a> Accepted at SAI Computing Conference 2025</p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°å…¨é¢æ¯”è¾ƒã€‚ç ”ç©¶å‘ç°åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œç²¾ç»†è°ƒæ•´çš„å°å‹æ¨¡å‹è¡¨ç°ä¼˜äºå½“å‰å…ˆè¿›çš„LLMï¼Œä½†åœ¨è„±ç¦»åˆ†å¸ƒæµ‹è¯•çš„ç¯å¢ƒä¸‹ï¼Œå…ˆè¿›çš„LLMèƒ½å¤Ÿç¼©å°å·®è·ï¼Œç”šè‡³åœ¨ä¸‰ä¸ªæµ‹è¯•ä¸­çš„äº”ä¸ªé—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç²¾ç»†è°ƒæ•´æ¨¡å‹ã€‚æå‡ºæ–°çš„æç¤ºé£æ ¼ä»¥ç®€åŒ–æ¨ç†è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šç§NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¯é€šè¿‡å°‘æ ·æœ¬æˆ–é›¶æ ·æœ¬æç¤ºæŠ€æœ¯æ— éœ€ç‰¹å®šè®­ç»ƒå³å¯å®Œæˆã€‚</li>
<li>åœ¨Stanford Question Answering Dataset 2.0ï¼ˆSQuAD2ï¼‰ä¸Šè¿›è¡Œäº†å°å‹ç²¾ç»†è°ƒæ•´æ¨¡å‹ä¸å³æ’å³ç”¨å‹LLMçš„æ€§èƒ½æ¯”è¾ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºé£æ ¼ï¼Œæ—¨åœ¨æ— éœ€åŒé‡æ¨ç†å³å¯å¼•å‡ºç›¸åŒèƒ½åŠ›ï¼Œä»è€ŒèŠ‚çœè®¡ç®—æ—¶é—´å’Œèµ„æºã€‚</li>
<li>åˆ†æäº†LLMåœ¨ç±»ä¼¼ä½†ä¸åŒçš„é—®ç­”æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œæ¨¡æ‹Ÿäº†çœŸå®ä¸–ç•Œçš„ä½¿ç”¨æƒ…å†µï¼Œå…¶ä¸­ä¸Šä¸‹æ–‡å’Œæ‰€æé—®é¢˜å¯èƒ½ä¸åŸå§‹è®­ç»ƒåˆ†å¸ƒä¸åŒã€‚</li>
<li>ç²¾ç»†è°ƒæ•´çš„å°å‹æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå½“å‰å…ˆè¿›çš„LLMã€‚</li>
<li>åœ¨è„±ç¦»åˆ†å¸ƒæµ‹è¯•çš„ç¯å¢ƒä¸‹ï¼Œå…ˆè¿›çš„LLMèƒ½å¤Ÿç¼©å°ä¸ç²¾ç»†è°ƒæ•´æ¨¡å‹çš„æ€§èƒ½å·®è·ï¼Œç”šè‡³åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa696a3dabcc1d66005d4def7cc8d474.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-104eec9a81136b6cb215064dbde2aa7b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Selective-Shot-Learning-for-Code-Explanation"><a href="#Selective-Shot-Learning-for-Code-Explanation" class="headerlink" title="Selective Shot Learning for Code Explanation"></a>Selective Shot Learning for Code Explanation</h2><p><strong>Authors:Paheli Bhattacharya, Rishabh Gupta</strong></p>
<p>Code explanation plays a crucial role in the software engineering domain, aiding developers in grasping code functionality efficiently. Recent work shows that the performance of LLMs for code explanation improves in a few-shot setting, especially when the few-shot examples are selected intelligently. State-of-the-art approaches for such Selective Shot Learning (SSL) include token-based and embedding-based methods. However, these SSL approaches have been evaluated on proprietary LLMs, without much exploration on open-source Code-LLMs. Additionally, these methods lack consideration for programming language syntax. To bridge these gaps, we present a comparative study and propose a novel SSL method (SSL_ner) that utilizes entity information for few-shot example selection. We present several insights and show the effectiveness of SSL_ner approach over state-of-the-art methods across two datasets. To the best of our knowledge, this is the first systematic benchmarking of open-source Code-LLMs while assessing the performances of the various few-shot examples selection approaches for the code explanation task. </p>
<blockquote>
<p>ä»£ç è§£é‡Šåœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸæ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒå¸®åŠ©å¼€å‘è€…é«˜æ•ˆåœ°æŒæ¡ä»£ç åŠŸèƒ½ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸‹çš„ä»£ç è§£é‡Šæ€§èƒ½æœ‰æ‰€æå‡ï¼Œå°¤å…¶æ˜¯å½“é€‰æ‹©çš„å°‘é‡æ ·æœ¬æ˜¯æ˜æ™ºçš„æ—¶å€™ã€‚é’ˆå¯¹æ­¤ç±»é€‰æ‹©æ€§å°„å‡»å­¦ä¹ ï¼ˆSSLï¼‰çš„æœ€æ–°æ–¹æ³•åŒ…æ‹¬åŸºäºä»¤ç‰Œå’ŒåŸºäºåµŒå…¥çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›SSLæ–¹æ³•ä¸»è¦æ˜¯åœ¨ä¸“æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè€Œå¯¹å¼€æºä»£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCode-LLMsï¼‰çš„æ¢ç´¢å¹¶ä¸å¤šã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•æ²¡æœ‰è€ƒè™‘åˆ°ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ¯”è¾ƒç ”ç©¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„SSLæ–¹æ³•ï¼ˆSSL_nerï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å®ä½“ä¿¡æ¯è¿›è¡Œå°‘é‡æ ·æœ¬é€‰æ‹©ã€‚æˆ‘ä»¬è·å¾—äº†ä¸€äº›è§è§£ï¼Œå¹¶å±•ç¤ºäº†SSL_neræ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯åœ¨å¯¹å„ç§å°‘é‡æ ·æœ¬é€‰æ‹©æ–¹æ³•è¿›è¡Œæ€§èƒ½è¯„ä¼°çš„åŒæ—¶ï¼Œé¦–æ¬¡å¯¹å¼€æºä»£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12852v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè½¯ä»¶å·¥ç¨‹é¢†åŸŸä¸­çš„ä»£ç è§£é‡Šçš„é‡è¦æ€§ï¼ŒLLMsï¼ˆå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰åœ¨å°‘æ•°æ ·æœ¬ä¸‹çš„è¡¨ç°æå‡å—åˆ°å…³æ³¨ã€‚è¿‘æœŸå·¥ä½œè¡¨æ˜ï¼Œåˆç†é€‰æ‹©å°‘æ•°æ ·æœ¬å¯¹äºæå‡LLMsåœ¨ä»£ç è§£é‡Šä»»åŠ¡ä¸­çš„æ€§èƒ½å°¤ä¸ºå…³é”®ã€‚æœ¬ç ”ç©¶é’ˆå¯¹å¼€æºCode-LLMsè¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°ï¼Œå¹¶æå‡ºä¸€ç§åˆ©ç”¨å®ä½“ä¿¡æ¯çš„æ–°å‹Selective Shot Learningï¼ˆSSL_nerï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨è€ƒè™‘ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ç»“æ„åŸºç¡€ä¸Šï¼Œå®ç°äº†å¯¹SSLæ–¹æ³•çš„æ”¹è¿›ã€‚ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„è§è§£ï¼Œå¹¶åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†SSL_neræ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¼€æºCode-LLMsè¯„ä¼°ä¸åŒå°‘æ•°æ ·æœ¬é€‰æ‹©æ–¹æ³•åœ¨ä»£ç è§£é‡Šä»»åŠ¡ä¸Šçš„è¡¨ç°çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç è§£é‡Šåœ¨è½¯ä»¶å·¥ç¨‹ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œæœ‰åŠ©äºå¼€å‘è€…é«˜æ•ˆç†è§£ä»£ç åŠŸèƒ½ã€‚</li>
<li>LLMsåœ¨å°‘æ•°æ ·æœ¬ä¸‹çš„æ€§èƒ½æå‡å—åˆ°å…³æ³¨ï¼Œå°¤å…¶æ˜¯åˆç†é€‰æ‹©å°‘æ•°æ ·æœ¬çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰çš„SSLæ–¹æ³•åœ¨è¯„ä¼°ä¸­ä¸»è¦é’ˆå¯¹ç§æœ‰LLMsï¼Œè€Œè¾ƒå°‘æ¢ç´¢å¼€æºCode-LLMsã€‚</li>
<li>SSLæ–¹æ³•åœ¨è€ƒè™‘ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ç»“æ„ä¸Šå­˜åœ¨ä¸è¶³ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„SSLæ–¹æ³•â€”â€”SSL_nerï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å®ä½“ä¿¡æ¯è¿›è¡Œå°‘æ•°æ ·æœ¬é€‰æ‹©ã€‚</li>
<li>ç ”ç©¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†SSL_neræ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶ç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d74734e7284d67d321af1f3113e9ec8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-473358974a34046089cefd28c8927df7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55aae6885c8a0a8a7a2e0575ccd05ecf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc0d168bcb0916bc19d81e3e8d2aa36e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b2053b921984d833f6d3e3e3cd4d8e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88d45e2a660a844ee4260309ce32dd10.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FocusChat-Text-guided-Long-Video-Understanding-via-Spatiotemporal-Information-Filtering"><a href="#FocusChat-Text-guided-Long-Video-Understanding-via-Spatiotemporal-Information-Filtering" class="headerlink" title="FocusChat: Text-guided Long Video Understanding via Spatiotemporal   Information Filtering"></a>FocusChat: Text-guided Long Video Understanding via Spatiotemporal   Information Filtering</h2><p><strong>Authors:Zheng Cheng, Rendong Wang, Zhicheng Wang</strong></p>
<p>Recently, multi-modal large language models have made significant progress. However, visual information lacking of guidance from the userâ€™s intention may lead to redundant computation and involve unnecessary visual noise, especially in long, untrimmed videos. To address this issue, we propose FocusChat, a text-guided multi-modal large language model (LLM) that emphasizes visual information correlated to the userâ€™s prompt. In detail, Our model first undergoes the semantic extraction module, which comprises a visual semantic branch and a text semantic branch to extract image and text semantics, respectively. The two branches are combined using the Spatial-Temporal Filtering Module (STFM). STFM enables explicit spatial-level information filtering and implicit temporal-level feature filtering, ensuring that the visual tokens are closely aligned with the userâ€™s query. It lowers the essential number of visual tokens inputted into the LLM. FocusChat significantly outperforms Video-LLaMA in zero-shot experiments, using an order of magnitude less training data with only 16 visual tokens occupied. It achieves results comparable to the state-of-the-art in few-shot experiments, with only 0.72M pre-training data. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç¼ºä¹ç”¨æˆ·æ„å›¾æŒ‡å¯¼çš„è§†è§‰ä¿¡æ¯å¯èƒ½å¯¼è‡´å†—ä½™è®¡ç®—å¹¶å¼•å…¥ä¸å¿…è¦çš„è§†è§‰å™ªå£°ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿ä¸”æœªä¿®å‰ªçš„è§†é¢‘ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FocusChatï¼Œè¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬å¼•å¯¼çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ƒå¼ºè°ƒä¸ç”¨æˆ·æç¤ºç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é¦–å…ˆç»è¿‡è¯­ä¹‰æå–æ¨¡å—ï¼Œè¯¥æ¨¡å—åŒ…æ‹¬è§†è§‰è¯­ä¹‰åˆ†æ”¯å’Œæ–‡æœ¬è¯­ä¹‰åˆ†æ”¯ï¼Œåˆ†åˆ«æå–å›¾åƒå’Œæ–‡æœ¬è¯­ä¹‰ã€‚è¿™ä¸¤ä¸ªåˆ†æ”¯é€šè¿‡æ—¶ç©ºæ»¤æ³¢æ¨¡å—ï¼ˆSTFMï¼‰è¿›è¡Œç»„åˆã€‚STFMå®ç°äº†æ˜¾å¼çš„ç©ºé—´çº§ä¿¡æ¯æ»¤æ³¢å’Œéšå¼çš„æ—¶é—´çº§ç‰¹å¾æ»¤æ³¢ï¼Œç¡®ä¿è§†è§‰ä»¤ç‰Œä¸ç”¨æˆ·æŸ¥è¯¢ç´§å¯†å¯¹é½ã€‚å®ƒé™ä½äº†è¾“å…¥åˆ°LLMä¸­çš„å¿…è¦è§†è§‰ä»¤ç‰Œæ•°é‡ã€‚FocusChatåœ¨é›¶æ ·æœ¬å®éªŒä¸­æ˜¾è‘—ä¼˜äºVideo-LLaMAï¼Œä½¿ç”¨æ•°é‡çº§æ›´å°‘çš„è®­ç»ƒæ•°æ®ï¼Œä»…å ç”¨16ä¸ªè§†è§‰ä»¤ç‰Œã€‚åœ¨å°‘é‡æ ·æœ¬å®éªŒä¸­ï¼Œå®ƒè¾¾åˆ°äº†ä¸å›½å®¶æœ€æ–°æŠ€æœ¯ç›¸å½“çš„ç»“æœï¼Œä»…æœ‰0.72Mçš„é¢„è®­ç»ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12833v1">PDF</a> 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºFocusChatçš„æ–‡æœ¬å¼•å¯¼å¼å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¼ºè°ƒä¸ç”¨æˆ·æç¤ºç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚å®ƒé€šè¿‡è¯­ä¹‰æå–æ¨¡å—å’Œç©ºé—´æ—¶é—´è¿‡æ»¤æ¨¡å—ï¼Œé™ä½äº†è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ ‡è®°æ•°é‡ï¼Œä»è€Œæé«˜æ•ˆç‡å¹¶å‡å°‘å†—ä½™è®¡ç®—ã€‚FocusChatåœ¨é›¶å°„å‡»å®éªŒä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºVideo-LLaMAï¼Œä½¿ç”¨çš„è®­ç»ƒæ•°æ®é‡å‡å°‘äº†ä¸€ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶åœ¨å°‘å‡»å®éªŒä¸­å–å¾—äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„ç»“æœï¼Œä»…ä½¿ç”¨0.72Mçš„é¢„è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FocusChatæ˜¯ä¸€ä¸ªæ–‡æœ¬å¼•å¯¼çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åœ¨è§†é¢‘å¤„ç†ä¸­ç”±äºç”¨æˆ·æ„å›¾ç¼ºä¹æŒ‡å¯¼è€Œå¯¼è‡´çš„å†—ä½™è®¡ç®—å’Œè§†è§‰å™ªéŸ³é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡è¯­ä¹‰æå–æ¨¡å—å’Œç©ºé—´æ—¶é—´è¿‡æ»¤æ¨¡å—æ¥å¼ºè°ƒä¸ç”¨æˆ·æç¤ºç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>FocusChaté€šè¿‡é™ä½è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ ‡è®°æ•°é‡æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>FocusChatåœ¨é›¶å°„å‡»å®éªŒä¸­çš„è¡¨ç°ä¼˜äºVideo-LLaMAï¼Œä½¿ç”¨çš„è®­ç»ƒæ•°æ®é‡å¤§å¤§å‡å°‘ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å°‘å‡»å®éªŒä¸­å–å¾—äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„ç»“æœã€‚</li>
<li>FocusChatçš„é¢„è®­ç»ƒæ•°æ®é‡ä»…ä¸º0.72Mã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5e440689d57f774b6ceb66fccc97d9e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480aeb25e3e355af1748f85383e00656.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e9084bcc4258f9e6f2a2b19f21b9400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a1cd3d554e7b2b242a5ed7ab7187bad.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CRoF-CLIP-based-Robust-Few-shot-Learning-on-Noisy-Labels"><a href="#CRoF-CLIP-based-Robust-Few-shot-Learning-on-Noisy-Labels" class="headerlink" title="CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels"></a>CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels</h2><p><strong>Authors:Shizhuo Deng, Bowen Han, Jiaqi Chen, Hao Wang, Dongyue Chen, Tong Jia</strong></p>
<p>Noisy labels threaten the robustness of few-shot learning (FSL) due to the inexact features in a new domain. CLIP, a large-scale vision-language model, performs well in FSL on image-text embedding similarities, but it is susceptible to misclassification caused by noisy labels. How to enhance domain generalization of CLIP on noisy data within FSL tasks is a critical challenge. In this paper, we provide a novel view to mitigate the influence of noisy labels, CLIP-based Robust Few-shot learning (CRoF). CRoF is a general plug-in module for CLIP-based models. To avoid misclassification and confused label embedding, we design the few-shot task-oriented prompt generator to give more discriminative descriptions of each category. The proposed prompt achieves larger distances of inter-class textual embedding. Furthermore, rather than fully trusting zero-shot classification by CLIP, we fine-tune CLIP on noisy few-shot data in a new domain with a weighting strategy like label-smooth. The weights for multiple potentially correct labels consider the relationship between CLIPâ€™s prior knowledge and original label information to ensure reliability. Our multiple label loss function further supports robust training under this paradigm. Comprehensive experiments show that CRoF, as a plug-in, outperforms fine-tuned and vanilla CLIP models on different noise types and noise ratios. </p>
<blockquote>
<p>å¸¦å™ªå£°çš„æ ‡ç­¾ç”±äºæ–°é¢†åŸŸçš„ç‰¹å¾ä¸å‡†ç¡®è€Œå¨èƒå°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰çš„ç¨³å¥æ€§ã€‚CLIPæ˜¯ä¸€ç§å¤§è§„æ¨¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨å›¾åƒæ–‡æœ¬åµŒå…¥ç›¸ä¼¼æ€§æ–¹é¢ï¼ŒFSLä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒå®¹æ˜“å—åˆ°å¸¦å™ªå£°æ ‡ç­¾å¯¼è‡´çš„è¯¯åˆ†ç±»å½±å“ã€‚å¦‚ä½•åœ¨FSLä»»åŠ¡ä¸­å¸¦å™ªå£°æ•°æ®ä¸Šå¢å¼ºCLIPçš„é¢†åŸŸæ³›åŒ–æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å‡è½»å¸¦å™ªå£°æ ‡ç­¾å½±å“çš„å…¨æ–°è§‚ç‚¹ï¼Œå³åŸºäºCLIPçš„é²æ£’å°‘æ ·æœ¬å­¦ä¹ ï¼ˆCRoFï¼‰ã€‚CRoFæ˜¯ä¸€ä¸ªé€‚ç”¨äºCLIPæ¨¡å‹çš„é€šç”¨æ’ä»¶æ¨¡å—ã€‚ä¸ºäº†é¿å…è¯¯åˆ†ç±»å’Œæ··æ·†æ ‡ç­¾åµŒå…¥ï¼Œæˆ‘ä»¬è®¾è®¡äº†é¢å‘å°‘æ ·æœ¬ä»»åŠ¡çš„æç¤ºç”Ÿæˆå™¨ï¼Œä¸ºæ¯ä¸ªç±»åˆ«æä¾›æ›´å…·åŒºåˆ†åº¦çš„æè¿°ã€‚æ‰€æå‡ºçš„æç¤ºå®ç°äº†è¾ƒå¤§çš„ç±»é—´æ–‡æœ¬åµŒå…¥è·ç¦»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸æ˜¯å®Œå…¨ä¿¡ä»»CLIPçš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œè€Œæ˜¯ä½¿ç”¨æ ‡ç­¾å¹³æ»‘ç­‰åŠ æƒç­–ç•¥å¯¹æ–°é¢†åŸŸä¸­çš„å¸¦å™ªå£°å°‘æ ·æœ¬æ•°æ®è¿›è¡Œå¾®è°ƒã€‚å¤šä¸ªå¯èƒ½æ­£ç¡®çš„æ ‡ç­¾çš„æƒé‡è€ƒè™‘äº†CLIPçš„å…ˆéªŒçŸ¥è¯†å’ŒåŸå§‹æ ‡ç­¾ä¿¡æ¯ä¹‹é—´çš„å…³ç³»ï¼Œä»¥ç¡®ä¿å¯é æ€§ã€‚æˆ‘ä»¬çš„å¤šæ ‡ç­¾æŸå¤±å‡½æ•°è¿›ä¸€æ­¥æ”¯æŒæ­¤æ¨¡å¼ä¸‹çš„ç¨³å¥è®­ç»ƒã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œä½œä¸ºæ’ä»¶çš„CRoFåœ¨ä¸åŒå™ªå£°ç±»å‹å’Œå™ªå£°æ¯”ç‡ä¸Šä¼˜äºç»è¿‡ç²¾ç»†è°ƒæ•´å’ŒåŸå§‹çš„CLIPæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12793v1">PDF</a> </p>
<p><strong>Summary</strong><br>    CLIPæ¨¡å‹åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­å› æ–°åŸŸçš„ä¸ç²¾ç¡®ç‰¹å¾è€Œå—åˆ°å™ªå£°æ ‡ç­¾çš„å¨èƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„CRoFæ¨¡å—æ¥å‡å°‘å™ªå£°æ ‡ç­¾çš„å½±å“ï¼Œå®ƒå¢å¼ºäº†CLIPæ¨¡å‹çš„åŸŸæ³›åŒ–èƒ½åŠ›ã€‚CRoFæ˜¯ä¸€ç§é’ˆå¯¹CLIPæ¨¡å‹çš„é€šç”¨æ’ä»¶æ¨¡å—ï¼Œå®ƒé€šè¿‡è®¾è®¡é¢å‘ä»»åŠ¡çš„å°‘é‡æç¤ºç”Ÿæˆå™¨æ¥é¿å…è¯¯åˆ†ç±»å’Œæ··æ·†æ ‡ç­¾åµŒå…¥ã€‚æ­¤å¤–ï¼ŒCRoFè¿˜é‡‡ç”¨äº†ä¸€ç§åŠ æƒç­–ç•¥å¯¹CLIPè¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ–°åŸŸçš„å™ªå£°æ•°æ®ã€‚é€šè¿‡ç»¼åˆå®éªŒè¯æ˜ï¼ŒCRoFä½œä¸ºæ’ä»¶åœ¨ä¸åŒå™ªå£°ç±»å‹å’Œå™ªå£°æ¯”ç‡ä¸Šå‡ä¼˜äºå¾®è°ƒåçš„CLIPæ¨¡å‹å’ŒåŸå§‹CLIPæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­æ˜“å—å™ªå£°æ ‡ç­¾å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>CRoFä½œä¸ºä¸€ç§æ–°å‹æ’ä»¶æ¨¡å—ï¼Œæ—¨åœ¨æé«˜CLIPæ¨¡å‹åœ¨æ–°åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CRoFé€šè¿‡è®¾è®¡é¢å‘ä»»åŠ¡çš„å°‘é‡æç¤ºç”Ÿæˆå™¨ï¼Œå‡å°‘è¯¯åˆ†ç±»å’Œæ··æ·†æ ‡ç­¾åµŒå…¥çš„é—®é¢˜ã€‚</li>
<li>CRoFé‡‡ç”¨åŠ æƒç­–ç•¥å¯¹CLIPè¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ–°åŸŸçš„å™ªå£°æ•°æ®ï¼Œæé«˜æ¨¡å‹çš„å¯é æ€§ã€‚</li>
<li>CRoFåˆ©ç”¨å¤šé‡æ ‡ç­¾æŸå¤±å‡½æ•°è¿›è¡Œç¨³å¥è®­ç»ƒã€‚</li>
<li>ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCRoFåœ¨å¤šç§å™ªå£°ç±»å‹å’Œæ¯”ç‡ä¸‹æ€§èƒ½ä¼˜äºåŸå§‹CLIPæ¨¡å‹å’Œå¾®è°ƒåçš„CLIPæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae206aa32a9795d211ebc72fe46a3237.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-516b5ba9897b35ab1fcb44ce145e4f82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d67809d7d0a7b88e56e75bba1c02c5ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54aecf5daf027ba6d0c0c84fe0979d13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6805ce8784547a01100de0f51f51940d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0949bb5f2161c80881f270612e2bc64a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4251e14cf28fa612741195f3bdaf2185.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Adapting-Unsigned-Graph-Neural-Networks-for-Signed-Graphs-A-Few-Shot-Prompt-Tuning-Approach"><a href="#Adapting-Unsigned-Graph-Neural-Networks-for-Signed-Graphs-A-Few-Shot-Prompt-Tuning-Approach" class="headerlink" title="Adapting Unsigned Graph Neural Networks for Signed Graphs: A Few-Shot   Prompt Tuning Approach"></a>Adapting Unsigned Graph Neural Networks for Signed Graphs: A Few-Shot   Prompt Tuning Approach</h2><p><strong>Authors:Zian Zhai, Sima Qing, Xiaoyang Wang, Wenjie Zhang</strong></p>
<p>Signed Graph Neural Networks (SGNNs) are powerful tools for signed graph representation learning but struggle with limited generalization and heavy dependence on labeled data. While recent advancements in â€œgraph pre-training and prompt tuningâ€ have reduced label dependence in Graph Neural Networks (GNNs) and improved their generalization abilities by leveraging pre-training knowledge, these efforts have focused exclusively on unsigned graphs. The scarcity of publicly available signed graph datasets makes it essential to transfer knowledge from unsigned graphs to signed graph tasks. However, this transfer introduces significant challenges due to the graph-level and task-level divergences between the pre-training and downstream phases. To address these challenges, we propose Signed Graph Prompt Tuning (SGPT) in this paper. Specifically, SGPT employs a graph template and a semantic prompt to segregate mixed link semantics in the signed graph and then adaptively integrate the distinctive semantic information according to the needs of downstream tasks, thereby unifying the pre-training and downstream graphs. Additionally, SGPT utilizes a task template and a feature prompt to reformulate the downstream signed graph tasks, aligning them with pre-training tasks to ensure a unified optimization objective and consistent feature space across tasks. Finally, extensive experiments are conducted on popular signed graph datasets, demonstrating the superiority of SGPT over state-of-the-art methods. </p>
<blockquote>
<p>æœ‰ç¬¦å·å›¾ç¥ç»ç½‘ç»œï¼ˆSGNNsï¼‰æ˜¯æœ‰ç¬¦å·å›¾è¡¨ç¤ºå­¦ä¹ çš„å¼ºå¤§å·¥å…·ï¼Œä½†åœ¨æ³›åŒ–æœ‰é™å’Œä¸¥é‡ä¾èµ–æ ‡ç­¾æ•°æ®æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘çš„â€œå›¾é¢„è®­ç»ƒå’Œæç¤ºè°ƒæ•´â€è¿›å±•å‡å°‘äº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å¯¹æ ‡ç­¾çš„ä¾èµ–ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†æé«˜äº†å…¶æ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿™äº›åŠªåŠ›éƒ½é›†ä¸­åœ¨æ— ç¬¦å·å›¾ä¸Šã€‚æœ‰ç¬¦å·å›¾æ•°æ®é›†å…¬å¼€å¯ç”¨çš„ç¨€ç¼ºæ€§ä½¿å¾—ä»æ— ç¬¦å·å›¾å‘æœ‰ç¬¦å·å›¾ä»»åŠ¡è½¬ç§»çŸ¥è¯†å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºé¢„è®­ç»ƒå’Œä¸‹æµé˜¶æ®µä¹‹é—´çš„å›¾çº§åˆ«å’Œä»»åŠ¡çº§åˆ«å·®å¼‚ï¼Œè¿™ç§è½¬ç§»å¼•å…¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†æœ‰ç¬¦å·å›¾æç¤ºè°ƒæ•´ï¼ˆSGPTï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒSGPTé‡‡ç”¨å›¾æ¨¡æ¿å’Œè¯­ä¹‰æç¤ºæ¥åˆ†ç¦»æœ‰ç¬¦å·å›¾ä¸­çš„æ··åˆé“¾æ¥è¯­ä¹‰ï¼Œç„¶åæ ¹æ®ä¸‹æ¸¸ä»»åŠ¡çš„éœ€æ±‚è‡ªé€‚åº”åœ°é›†æˆä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œç»Ÿä¸€é¢„è®­ç»ƒå’Œä¸‹æµå›¾ã€‚æ­¤å¤–ï¼ŒSGPTåˆ©ç”¨ä»»åŠ¡æ¨¡æ¿å’Œç‰¹å¾æç¤ºæ¥é‡æ–°åˆ¶å®šä¸‹æ¸¸æœ‰ç¬¦å·å›¾ä»»åŠ¡ï¼Œä½¿å®ƒä»¬ä¸é¢„è®­ç»ƒä»»åŠ¡å¯¹é½ï¼Œä»¥ç¡®ä¿ç»Ÿä¸€ä¼˜åŒ–ç›®æ ‡å’Œä»»åŠ¡ä¹‹é—´ä¸€è‡´çš„ç‰¹å¾ç©ºé—´ã€‚æœ€åï¼Œåœ¨æµè¡Œçš„æœ‰ç¬¦å·å›¾æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†SGPTä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12155v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SGNNçš„å¼ºå¤§å·¥å…·ç”¨äºæœ‰ç¬¦å·å›¾è¡¨ç¤ºå­¦ä¹ ï¼Œä½†å­˜åœ¨æ³›åŒ–å—é™å’Œä¾èµ–å¤§é‡æ ‡ç­¾æ•°æ®çš„é—®é¢˜ã€‚æœ€è¿‘å›¾é¢„è®­ç»ƒå’Œæç¤ºè°ƒæ•´æŠ€æœ¯çš„è¿›æ­¥å‡å°‘äº†å›¾ç¥ç»ç½‘ç»œå¯¹æ ‡ç­¾çš„ä¾èµ–ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†æé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼Œä½†è¿™äº›åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨æ— ç¬¦å·å›¾ä¸Šã€‚æœ‰ç¬¦å·å›¾æ•°æ®é›†çš„ç¨€ç¼ºæ€§ä½¿å¾—ä»æ— ç¬¦å·å›¾å‘æœ‰ç¬¦å·å›¾ä»»åŠ¡è½¬ç§»çŸ¥è¯†å˜å¾—è‡³å…³é‡è¦ï¼Œä½†ç”±äºé¢„è®­ç»ƒå’Œä¸‹æ¸¸é˜¶æ®µçš„å›¾çº§åˆ«å’Œä»»åŠ¡çº§åˆ«çš„å·®å¼‚ï¼Œè¿™ç§è½¬ç§»å¼•å…¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†æœ‰ç¬¦å·å›¾æç¤ºè°ƒæ•´ï¼ˆSGPTï¼‰æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚SGPTä½¿ç”¨å›¾æ¨¡æ¿å’Œè¯­ä¹‰æç¤ºæ¥åˆ†ç¦»æ··åˆé“¾æ¥è¯­ä¹‰ï¼Œå¹¶æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡çš„éœ€è¦è‡ªé€‚åº”åœ°é›†æˆä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œç»Ÿä¸€é¢„è®­ç»ƒå’Œä¸‹æ¸¸å›¾ã€‚æ­¤å¤–ï¼ŒSGPTè¿˜ä½¿ç”¨ä»»åŠ¡æ¨¡æ¿å’Œç‰¹å¾æç¤ºæ¥é‡æ–°åˆ¶å®šä¸‹æ¸¸æœ‰ç¬¦å·å›¾ä»»åŠ¡ï¼Œä½¿å…¶ä¸é¢„è®­ç»ƒä»»åŠ¡å¯¹é½ï¼Œç¡®ä¿ç»Ÿä¸€çš„ä¼˜åŒ–ç›®æ ‡å’Œè·¨ä»»åŠ¡çš„ä¸€è‡´ç‰¹å¾ç©ºé—´ã€‚åœ¨æµè¡Œçš„æœ‰ç¬¦å·å›¾æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œè¯æ˜SGPTä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Signed Graph Neural Networks (SGNNs) é¢ä¸´æ³›åŒ–æœ‰é™å’Œä¾èµ–å¤§é‡æ ‡ç­¾æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>æœ€è¿‘å›¾é¢„è®­ç»ƒå’Œæç¤ºè°ƒæ•´æŠ€æœ¯çš„è¿›æ­¥å·²åº”ç”¨äºæ— ç¬¦å·å›¾ä¸Šï¼Œä»¥æé«˜æ³›åŒ–èƒ½åŠ›å’Œå‡å°‘æ ‡ç­¾ä¾èµ–ã€‚</li>
<li>æœ‰ç¬¦å·å›¾æ•°æ®é›†çš„ç¨€ç¼ºæ€§å¯¼è‡´éœ€è¦ä»æ— ç¬¦å·å›¾å‘æœ‰ç¬¦å·å›¾ä»»åŠ¡è½¬ç§»çŸ¥è¯†ã€‚</li>
<li>è¿™ç§è½¬ç§»é¢ä¸´é¢„è®­ç»ƒå’Œä¸‹æ¸¸é˜¶æ®µçš„å›¾çº§åˆ«å’Œä»»åŠ¡çº§åˆ«çš„å·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>Signed Graph Prompt Tuning (SGPT) è¢«æå‡ºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡å›¾æ¨¡æ¿å’Œè¯­ä¹‰æç¤ºæ¥åˆ†ç¦»æ··åˆé“¾æ¥è¯­ä¹‰å¹¶é›†æˆä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>SGPTè¿˜ä½¿ç”¨ä»»åŠ¡æ¨¡æ¿å’Œç‰¹å¾æç¤ºæ¥é‡æ–°åˆ¶å®šä¸‹æ¸¸æœ‰ç¬¦å·å›¾ä»»åŠ¡ï¼Œç¡®ä¿ç»Ÿä¸€çš„ä¼˜åŒ–ç›®æ ‡å’Œä¸€è‡´çš„ç‰¹å¾ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a7b258e2313a3191cf6a50a8f997472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c9f488b25507d6f88c8c1b99b99f4e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b742996795bc271dbc1edadf87014b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f90ed6dcde7b94e16e0060c821265e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-214f9fe2a46d7fb18f70ac9480b1da35.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Meta-Controller-Few-Shot-Imitation-of-Unseen-Embodiments-and-Tasks-in-Continuous-Control"><a href="#Meta-Controller-Few-Shot-Imitation-of-Unseen-Embodiments-and-Tasks-in-Continuous-Control" class="headerlink" title="Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in   Continuous Control"></a>Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in   Continuous Control</h2><p><strong>Authors:Seongwoong Cho, Donggyun Kim, Jinwoo Lee, Seunghoon Hong</strong></p>
<p>Generalizing across robot embodiments and tasks is crucial for adaptive robotic systems. Modular policy learning approaches adapt to new embodiments but are limited to specific tasks, while few-shot imitation learning (IL) approaches often focus on a single embodiment. In this paper, we introduce a few-shot behavior cloning framework to simultaneously generalize to unseen embodiments and tasks using a few (\emph{e.g.,} five) reward-free demonstrations. Our framework leverages a joint-level input-output representation to unify the state and action spaces of heterogeneous embodiments and employs a novel structure-motion state encoder that is parameterized to capture both shared knowledge across all embodiments and embodiment-specific knowledge. A matching-based policy network then predicts actions from a few demonstrations, producing an adaptive policy that is robust to over-fitting. Evaluated in the DeepMind Control suite, our framework termed \modelname{} demonstrates superior few-shot generalization to unseen embodiments and tasks over modular policy learning and few-shot IL approaches. Codes are available at \href{<a target="_blank" rel="noopener" href="https://github.com/SeongwoongCho/meta-controller%7D%7Bhttps://github.com/SeongwoongCho/meta-controller%7D">https://github.com/SeongwoongCho/meta-controller}{https://github.com/SeongwoongCho/meta-controller}</a>. </p>
<blockquote>
<p>å¯¹äºè‡ªé€‚åº”æœºå™¨äººç³»ç»Ÿæ¥è¯´ï¼Œè·¨æœºå™¨äººå®ä½“å’Œä»»åŠ¡çš„ä¸€èˆ¬åŒ–è‡³å…³é‡è¦ã€‚æ¨¡å—åŒ–ç­–ç•¥å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿé€‚åº”æ–°çš„å®ä½“ï¼Œä½†ä»…é™äºç‰¹å®šä»»åŠ¡ï¼Œè€Œå°‘æ ·æœ¬æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ–¹æ³•é€šå¸¸ä¾§é‡äºå•ä¸€å®ä½“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å°‘æ ·æœ¬è¡Œä¸ºå…‹éš†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨å°‘é‡ï¼ˆä¾‹å¦‚äº”ä¸ªï¼‰æ— å¥–åŠ±ç¤ºèŒƒæ¥åŒæ—¶æ¨å¹¿åˆ°æœªè§è¿‡çš„å®ä½“å’Œä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨å…³èŠ‚çº§è¾“å…¥è¾“å‡ºè¡¨ç¤ºæ¥ç»Ÿä¸€å¼‚è´¨å®ä½“çš„çŠ¶æ€å’Œè¡Œä¸ºç©ºé—´ï¼Œå¹¶é‡‡ç”¨æ–°å‹ç»“æ„è¿åŠ¨çŠ¶æ€ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨ç»è¿‡å‚æ•°åŒ–è®¾ç½®ï¼Œèƒ½å¤Ÿæ•æ‰æ‰€æœ‰å®ä½“ä¹‹é—´çš„å…±äº«çŸ¥è¯†ä»¥åŠé’ˆå¯¹å®ä½“çš„ç‰¹å®šçŸ¥è¯†ã€‚ç„¶åï¼ŒåŸºäºåŒ¹é…çš„ç­–ç•¥ç½‘ç»œä»å°‘æ•°ç¤ºèŒƒä¸­é¢„æµ‹è¡Œä¸ºï¼Œäº§ç”Ÿèƒ½å¤Ÿé€‚åº”è¿‡åº¦æ‹Ÿåˆçš„ç¨³å¥ç­–ç•¥ã€‚åœ¨DeepMind Controlå¥—ä»¶ä¸­è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ï¼ˆç§°ä¸º\modelnameï¼‰åœ¨æœªè§è¿‡çš„å®ä½“å’Œä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºæ¨¡å—åŒ–ç­–ç•¥å­¦ä¹ å’Œå°‘æ ·æœ¬ILæ–¹æ³•çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SeongwoongCho/meta-controller">https://github.com/SeongwoongCho/meta-controller</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12147v1">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¡Œä¸ºå…‹éš†çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å°‘é‡å¥–åŠ±æ— å…³æ¼”ç¤ºçš„åŸºç¡€ä¸Šï¼ŒåŒæ—¶æ¨å¹¿åˆ°æœªè§è¿‡çš„æœºå™¨äººå®ä½“å’Œä»»åŠ¡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å…³èŠ‚çº§è¾“å…¥è¾“å‡ºè¡¨ç¤ºæ¥ç»Ÿä¸€ä¸åŒå®ä½“çš„çŠ¶æ€å’Œè¡Œä¸ºç©ºé—´ï¼Œå¹¶é‡‡ç”¨æ–°å‹ç»“æ„è¿åŠ¨çŠ¶æ€ç¼–ç å™¨æ•æ‰è·¨å®ä½“çš„å…±äº«çŸ¥è¯†å’Œç‰¹å®šå®ä½“çš„çŸ¥è¯†ã€‚åŒ¹é…ç­–ç•¥ç½‘ç»œæ ¹æ®å°‘é‡æ¼”ç¤ºé¢„æµ‹è¡Œä¸ºï¼Œäº§ç”Ÿé€‚åº”æ€§ç­–ç•¥ï¼Œå‡å°‘è¿‡åº¦æ‹Ÿåˆã€‚åœ¨DeepMindæ§åˆ¶å¥—ä»¶ä¸­è¯„ä¼°ï¼Œè¯¥æ¡†æ¶å±•ç°å‡ºå‡ºè‰²çš„æœªè§å®ä½“å’Œä»»åŠ¡çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºæ¨¡å—åŒ–æ”¿ç­–å­¦ä¹ å’Œå°‘æ ·æœ¬æ¨¡ä»¿å­¦ä¹ çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºè¡Œä¸ºå…‹éš†çš„æ¡†æ¶ï¼Œå¯ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„æœºå™¨äººå®ä½“å’Œä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨å…³èŠ‚çº§è¾“å…¥è¾“å‡ºè¡¨ç¤ºæ¥ç»Ÿä¸€ä¸åŒå®ä½“çš„çŠ¶æ€å’Œè¡Œä¸ºç©ºé—´ã€‚</li>
<li>é‡‡ç”¨æ–°å‹ç»“æ„è¿åŠ¨çŠ¶æ€ç¼–ç å™¨æ•æ‰è·¨å®ä½“çš„å…±äº«çŸ¥è¯†å’Œç‰¹å®šå®ä½“çš„çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡åŒ¹é…ç­–ç•¥ç½‘ç»œé¢„æµ‹è¡Œä¸ºï¼Œäº§ç”Ÿé€‚åº”æ€§ç­–ç•¥ï¼Œå‡å°‘è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>åœ¨DeepMindæ§åˆ¶å¥—ä»¶ä¸­è¯„ä¼°ï¼Œå±•ç°å‡ºå‡ºè‰²çš„æœªè§å®ä½“å’Œä»»åŠ¡çš„å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶ä¼˜äºæ¨¡å—åŒ–æ”¿ç­–å­¦ä¹ å’Œå°‘æ ·æœ¬æ¨¡ä»¿å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95103570d0d65736f1b64bd1f9438267.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce28d52bb8a50d23bfc49dd1aa69e5d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8aa2396b6cb98c127ff4670be3844c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11c9c31165b30ee71d37e47c49e445d1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Text-and-Image-Are-Mutually-Beneficial-Enhancing-Training-Free-Few-Shot-Classification-with-CLIP"><a href="#Text-and-Image-Are-Mutually-Beneficial-Enhancing-Training-Free-Few-Shot-Classification-with-CLIP" class="headerlink" title="Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot   Classification with CLIP"></a>Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot   Classification with CLIP</h2><p><strong>Authors:Yayuan Li, Jintao Guo, Lei Qi, Wenbin Li, Yinghuan Shi</strong></p>
<p>Contrastive Language-Image Pretraining (CLIP) has been widely used in vision tasks. Notably, CLIP has demonstrated promising performance in few-shot learning (FSL). However, existing CLIP-based methods in training-free FSL (i.e., without the requirement of additional training) mainly learn different modalities independently, leading to two essential issues: 1) severe anomalous match in image modality; 2) varying quality of generated text prompts. To address these issues, we build a mutual guidance mechanism, that introduces an Image-Guided-Text (IGT) component to rectify varying quality of text prompts through image representations, and a Text-Guided-Image (TGI) component to mitigate the anomalous match of image modality through text representations. By integrating IGT and TGI, we adopt a perspective of Text-Image Mutual guidance Optimization, proposing TIMO. Extensive experiments show that TIMO significantly outperforms the state-of-the-art (SOTA) training-free method. Additionally, by exploring the extent of mutual guidance, we propose an enhanced variant, TIMO-S, which even surpasses the best training-required methods by 0.33% with approximately 100 times less time cost. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/lyymuwu/TIMO">https://github.com/lyymuwu/TIMO</a>. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å·²åœ¨è§†è§‰ä»»åŠ¡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç‰¹åˆ«æ˜¯ï¼ŒCLIPåœ¨å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­è¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFree-training Few-Shot Learning, FT-FSLï¼‰ä¸­ï¼Œç°æœ‰çš„åŸºäºCLIPçš„æ–¹æ³•ä¸»è¦ç‹¬ç«‹å­¦ä¹ ä¸åŒçš„æ¨¡æ€ï¼Œå¯¼è‡´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š1ï¼‰å›¾åƒæ¨¡æ€ä¸­å­˜åœ¨ä¸¥é‡çš„å¼‚å¸¸åŒ¹é…ï¼›2ï¼‰ç”Ÿæˆçš„æ–‡æœ¬æç¤ºè´¨é‡ä¸ä¸€ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ç§ç›¸äº’å¼•å¯¼æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¼•å…¥äº†ä¸€ä¸ªå›¾åƒå¼•å¯¼æ–‡æœ¬ï¼ˆIGTï¼‰ç»„ä»¶ï¼Œé€šè¿‡å›¾åƒè¡¨ç¤ºæ¥æ ¡æ­£æ–‡æœ¬æç¤ºçš„è´¨é‡ä¸ä¸€é—®é¢˜ï¼Œä»¥åŠä¸€ä¸ªæ–‡æœ¬å¼•å¯¼å›¾åƒï¼ˆTGIï¼‰ç»„ä»¶ï¼Œé€šè¿‡æ–‡æœ¬è¡¨ç¤ºæ¥ç¼“è§£å›¾åƒæ¨¡æ€çš„å¼‚å¸¸åŒ¹é…é—®é¢˜ã€‚é€šè¿‡æ•´åˆIGTå’ŒTGIï¼Œæˆ‘ä»¬ä»æ–‡æœ¬å›¾åƒç›¸äº’å¼•å¯¼ä¼˜åŒ–çš„è§’åº¦å…¥æ‰‹ï¼Œæå‡ºäº†TIMOã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTIMOæ˜¾è‘—ä¼˜äºæœ€æ–°çš„æ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ¢ç´¢ç›¸äº’å¼•å¯¼çš„ç¨‹åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†å¢å¼ºå‹TIMO-Sï¼Œå…¶ç”šè‡³ä»¥çº¦100å€çš„æ—¶é—´æˆæœ¬è¶…è¶Šäº†æœ€ä½³éœ€è¦è®­ç»ƒçš„æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†0.33%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lyymuwu/TIMO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lyymuwu/TIMOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11375v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰çš„æŠ€æœ¯åœ¨è§†è§‰ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·å­¦ä¹ ï¼ˆFSLï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CLIPåœ¨é›¶è®­ç»ƒå°æ ·å­¦ä¹ ï¼ˆæ— é¢å¤–è®­ç»ƒéœ€æ±‚ï¼‰çš„æ–¹æ³•ä¸»è¦ç‹¬ç«‹å­¦ä¹ ä¸åŒæ¨¡æ€ï¼Œå¯¼è‡´å›¾åƒæ¨¡æ€çš„å¼‚å¸¸åŒ¹é…å’Œç”Ÿæˆæ–‡æœ¬æç¤ºçš„è´¨é‡ä¸ä¸€ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ–‡æœ¬å›¾åƒç›¸äº’å¼•å¯¼ä¼˜åŒ–ï¼ˆTIMOï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å›¾åƒå¼•å¯¼æ–‡æœ¬ï¼ˆIGTï¼‰ç»„ä»¶çº æ­£æ–‡æœ¬æç¤ºçš„è´¨é‡ï¼Œä»¥åŠæ–‡æœ¬å¼•å¯¼å›¾åƒï¼ˆTGIï¼‰ç»„ä»¶å‡è½»å›¾åƒæ¨¡æ€çš„å¼‚å¸¸åŒ¹é…ã€‚å®éªŒæ˜¾ç¤ºï¼ŒTIMOæ˜¾è‘—ä¼˜äºç°æœ‰é›¶è®­ç»ƒæ–¹æ³•ï¼Œå…¶å¢å¼ºç‰ˆTIMO-Sç”šè‡³è¶…è¶Šäº†æœ€ä½³éœ€è®­ç»ƒæ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒæ—¶æ—¶é—´æˆæœ¬å¤§å¹…é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæŠ€æœ¯åœ¨è§†è§‰ä»»åŠ¡å’Œå°æ ·å­¦ä¹ ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰CLIPåœ¨é›¶è®­ç»ƒå°æ ·å­¦ä¹ ä¸­å­˜åœ¨å›¾åƒæ¨¡æ€å¼‚å¸¸åŒ¹é…å’Œæ–‡æœ¬æç¤ºè´¨é‡ä¸ä¸€çš„é—®é¢˜ã€‚</li>
<li>TIMOé€šè¿‡æ–‡æœ¬å›¾åƒç›¸äº’å¼•å¯¼æœºåˆ¶è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>IGTç»„ä»¶ç”¨äºçº æ­£æ–‡æœ¬æç¤ºçš„è´¨é‡ï¼ŒTGIç»„ä»¶å‡è½»å›¾åƒæ¨¡æ€çš„å¼‚å¸¸åŒ¹é…ã€‚</li>
<li>TIMOæ˜¾è‘—ä¼˜äºç°æœ‰é›¶è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>TIMOçš„å¢å¼ºç‰ˆTIMO-Såœ¨æ—¶é—´æˆæœ¬å¤§å¹…é™ä½çš„åŒæ—¶ï¼Œæ€§èƒ½è¶…è¶Šæœ€ä½³éœ€è®­ç»ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7404aaa6248977ee707d7e09d45d9dcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d3b4683ab5343196b2b3b6315d020a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a18a74487ca58be48aefc640927e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24d004e40c9a3694a95bbf969bf36ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9a096878db179f4a39019eb30b179ae.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="On-Distilling-the-Displacement-Knowledge-for-Few-Shot-Class-Incremental-Learning"><a href="#On-Distilling-the-Displacement-Knowledge-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="On Distilling the Displacement Knowledge for Few-Shot Class-Incremental   Learning"></a>On Distilling the Displacement Knowledge for Few-Shot Class-Incremental   Learning</h2><p><strong>Authors:Pengfei Fang, Yongchun Qin, Hui Xue</strong></p>
<p>Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of evolving data distributions and the difficulty of data acquisition in real-world scenarios. To counteract the catastrophic forgetting typically encountered in FSCIL, knowledge distillation is employed as a way to maintain the knowledge from learned data distribution. Recognizing the limitations of generating discriminative feature representations in a few-shot context, our approach incorporates structural information between samples into knowledge distillation. This structural information serves as a remedy for the low quality of features. Diverging from traditional structured distillation methods that compute sample similarity, we introduce the Displacement Knowledge Distillation (DKD) method. DKD utilizes displacement rather than similarity between samples, incorporating both distance and angular information to significantly enhance the information density retained through knowledge distillation. Observing performance disparities in feature distribution between base and novel classes, we propose the Dual Distillation Network (DDNet). This network applies traditional knowledge distillation to base classes and DKD to novel classes, challenging the conventional integration of novel classes with base classes. Additionally, we implement an instance-aware sample selector during inference to dynamically adjust dual branch weights, thereby leveraging the complementary strengths of each approach. Extensive testing on three benchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover, through rigorous experimentation and comparison, we establish the robustness and general applicability of our proposed DKD method. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰è§£å†³äº†æ•°æ®åˆ†å¸ƒæ¼”å˜å’Œç°å®ä¸–ç•Œåœºæ™¯ä¸­æ•°æ®è·å–å›°éš¾çš„é—®é¢˜ã€‚ä¸ºäº†æŠµæ¶ˆFSCILä¸­é€šå¸¸é‡åˆ°çš„ç¾éš¾æ€§é—å¿˜ï¼Œé‡‡ç”¨çŸ¥è¯†è’¸é¦ä½œä¸ºä¸€ç§ä¿æŒå·²å­¦ä¹ æ•°æ®åˆ†å¸ƒçŸ¥è¯†çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è®¤è¯†åˆ°åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹ç”Ÿæˆåˆ¤åˆ«ç‰¹å¾è¡¨ç¤ºçš„å±€é™æ€§ï¼Œå°†æ ·æœ¬ä¹‹é—´çš„ç»“æ„ä¿¡æ¯çº³å…¥çŸ¥è¯†è’¸é¦ä¸­ã€‚è¿™ç§ç»“æ„ä¿¡æ¯ä½œä¸ºå¯¹ç‰¹å¾è´¨é‡ä½çš„è¡¥æ•‘æªæ–½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä½ç§»çŸ¥è¯†è’¸é¦ï¼ˆDKDï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ ·æœ¬ä¹‹é—´çš„ä½ç§»è€Œä¸æ˜¯ç›¸ä¼¼æ€§ï¼Œç»“åˆè·ç¦»å’Œè§’åº¦ä¿¡æ¯ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æ˜¾è‘—å¢å¼ºä¿ç•™çš„ä¿¡æ¯å¯†åº¦ã€‚è§‚å¯Ÿåˆ°åŸºç¡€ç±»å’Œæ–°é¢–ç±»ä¹‹é—´ç‰¹å¾åˆ†å¸ƒçš„æ€§èƒ½å·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†åŒè’¸é¦ç½‘ç»œï¼ˆDDNetï¼‰ã€‚è¯¥ç½‘ç»œå¯¹åŸºç¡€ç±»åº”ç”¨ä¼ ç»ŸçŸ¥è¯†è’¸é¦ï¼Œå¯¹æ–°é¢–ç±»åº”ç”¨DKDï¼ŒæŒ‘æˆ˜äº†æ–°é¢–ç±»ä¸åŸºç¡€ç±»çš„ä¼ ç»Ÿèåˆæ–¹å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°äº†å®ä¾‹æ„ŸçŸ¥æ ·æœ¬é€‰æ‹©å™¨ï¼Œä»¥åŠ¨æ€è°ƒæ•´åŒåˆ†æ”¯æƒé‡ï¼Œä»è€Œå……åˆ†åˆ©ç”¨æ¯ç§æ–¹æ³•çš„äº’è¡¥ä¼˜åŠ¿ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›æµ‹è¯•è¡¨æ˜ï¼ŒDDNetè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è€Œä¸”ï¼Œé€šè¿‡ä¸¥æ ¼çš„å®éªŒå’Œæ¯”è¾ƒï¼Œæˆ‘ä»¬ç¡®å®šäº†æˆ‘ä»¬æ‰€æå‡ºçš„DKDæ–¹æ³•çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11017v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Few-shotç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®åˆ†å¸ƒçš„å˜åŒ–å’ŒçœŸå®åœºæ™¯ä¸­æ•°æ®è·å–çš„å›°éš¾ã€‚ä¸ºè§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œé‡‡ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ä¿ç•™å·²å­¦ä¹ æ•°æ®åˆ†å¸ƒçš„çŸ¥è¯†ã€‚é’ˆå¯¹å°‘æ ·æœ¬æƒ…å¢ƒä¸‹ç”Ÿæˆåˆ¤åˆ«ç‰¹å¾è¡¨ç¤ºçš„é™åˆ¶ï¼Œç»“åˆæ ·æœ¬é—´çš„ç»“æ„ä¿¡æ¯æ”¹è¿›çŸ¥è¯†è’¸é¦ã€‚å¼•å…¥ä½ç§»çŸ¥è¯†è’¸é¦ï¼ˆDKDï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ ·æœ¬é—´çš„ä½ç§»è€Œéç›¸ä¼¼æ€§ï¼Œç»“åˆè·ç¦»å’Œè§’åº¦ä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜é€šè¿‡çŸ¥è¯†è’¸é¦ä¿ç•™çš„ä¿¡æ¯å¯†åº¦ã€‚ä¸ºè§£å†³åŸºç¡€ç±»å’Œæ–°å‹ç±»åœ¨ç‰¹å¾åˆ†å¸ƒä¸Šçš„æ€§èƒ½å·®å¼‚ï¼Œæå‡ºåŒè’¸é¦ç½‘ç»œï¼ˆDDNetï¼‰ï¼Œè¯¥ç½‘ç»œå¯¹åŸºç¡€ç±»åº”ç”¨ä¼ ç»ŸçŸ¥è¯†è’¸é¦ï¼Œå¯¹æ–°å‹ç±»åº”ç”¨DKDï¼Œå¹¶åŠ¨æ€è°ƒæ•´åŒåˆ†æ”¯æƒé‡ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æµ‹è¯•è¡¨æ˜DDNetå–å¾—äº†æœ€æ–°æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shotç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é¢ä¸´æ•°æ®åˆ†å¸ƒå˜åŒ–å’ŒçœŸå®åœºæ™¯æ•°æ®è·å–å›°éš¾ç­‰æŒ‘æˆ˜ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æŠ€æœ¯ç”¨äºè§£å†³FSCILä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¿ç•™å·²å­¦ä¹ æ•°æ®åˆ†å¸ƒçš„çŸ¥è¯†ã€‚</li>
<li>å¼•å…¥ä½ç§»çŸ¥è¯†è’¸é¦ï¼ˆDKDï¼‰æ–¹æ³•ï¼Œç»“åˆè·ç¦»å’Œè§’åº¦ä¿¡æ¯ï¼Œæé«˜ä¿¡æ¯å¯†åº¦ã€‚</li>
<li>åŒè’¸é¦ç½‘ç»œï¼ˆDDNetï¼‰é’ˆå¯¹åŸºç¡€ç±»å’Œæ–°å‹ç±»åœ¨ç‰¹å¾åˆ†å¸ƒä¸Šçš„æ€§èƒ½å·®å¼‚ï¼Œå®æ–½ä¸åŒçš„è’¸é¦ç­–ç•¥ã€‚</li>
<li>DDNeté€šè¿‡åœ¨åŠ¨æ€è°ƒæ•´åŒåˆ†æ”¯æƒé‡çš„æ–¹æ³•ä¸­èåˆä¼ ç»ŸçŸ¥è¯†è’¸é¦ä¸DKDçš„ä¼˜åŠ¿ã€‚</li>
<li>å®éªŒè¡¨æ˜DDNetåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b01e4d4fe75e49050f94c8a51a0675b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-502d642dec3c97f567abdfcdc3b91a8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b6fbb289370db809316603c24c6f7e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14fe9d721ecf53ca7dea6abe7a34a0bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a3ff24f3791ee44bad5bb32a18d63b8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="HEP-NAS-Towards-Efficient-Few-shot-Neural-Architecture-Search-via-Hierarchical-Edge-Partitioning"><a href="#HEP-NAS-Towards-Efficient-Few-shot-Neural-Architecture-Search-via-Hierarchical-Edge-Partitioning" class="headerlink" title="HEP-NAS: Towards Efficient Few-shot Neural Architecture Search via   Hierarchical Edge Partitioning"></a>HEP-NAS: Towards Efficient Few-shot Neural Architecture Search via   Hierarchical Edge Partitioning</h2><p><strong>Authors:Jianfeng Li, Jiawen Zhang, Feng Wang, Lianbo Ma</strong></p>
<p>One-shot methods have significantly advanced the field of neural architecture search (NAS) by adopting weight-sharing strategy to reduce search costs. However, the accuracy of performance estimation can be compromised by co-adaptation. Few-shot methods divide the entire supernet into individual sub-supernets by splitting edge by edge to alleviate this issue, yet neglect relationships among edges and result in performance degradation on huge search space. In this paper, we introduce HEP-NAS, a hierarchy-wise partition algorithm designed to further enhance accuracy. To begin with, HEP-NAS treats edges sharing the same end node as a hierarchy, permuting and splitting edges within the same hierarchy to directly search for the optimal operation combination for each intermediate node. This approach aligns more closely with the ultimate goal of NAS. Furthermore, HEP-NAS selects the most promising sub-supernet after each segmentation, progressively narrowing the search space in which the optimal architecture may exist. To improve performance evaluation of sub-supernets, HEP-NAS employs search space mutual distillation, stabilizing the training process and accelerating the convergence of each individual sub-supernet. Within a given budget, HEP-NAS enables the splitting of all edges and gradually searches for architectures with higher accuracy. Experimental results across various datasets and search spaces demonstrate the superiority of HEP-NAS compared to state-of-the-art methods. </p>
<blockquote>
<p>ä¸€shotæ–¹æ³•é€šè¿‡é‡‡ç”¨æƒé‡å…±äº«ç­–ç•¥æ¥é™ä½æœç´¢æˆæœ¬ï¼Œä»è€Œæ˜¾è‘—æ¨åŠ¨äº†ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰é¢†åŸŸçš„å‘å±•ã€‚ç„¶è€Œï¼ŒååŒé€‚åº”å¯èƒ½ä¼šæŸå®³æ€§èƒ½ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚Few-shotæ–¹æ³•é€šè¿‡é€è¾¹åˆ†å‰²å°†æ•´ä¸ªè¶…ç½‘åˆ†æˆå•ä¸ªçš„å­è¶…ç½‘æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†å´å¿½è§†äº†è¾¹ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åœ¨å·¨å¤§çš„æœç´¢ç©ºé—´ä¸Šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HEP-NASï¼Œè¿™æ˜¯ä¸€ç§å±‚æ¬¡åˆ’åˆ†ç®—æ³•ï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æé«˜å‡†ç¡®æ€§ã€‚é¦–å…ˆï¼ŒHEP-NASå°†å…±äº«ç›¸åŒæœ«ç«¯èŠ‚ç‚¹çš„è¾¹è§†ä¸ºä¸€ä¸ªå±‚æ¬¡ç»“æ„ï¼Œå¯¹åŒä¸€å±‚æ¬¡å†…çš„è¾¹è¿›è¡Œæ’åˆ—å’Œåˆ†å‰²ï¼Œä»¥ç›´æ¥æœç´¢æ¯ä¸ªä¸­é—´èŠ‚ç‚¹çš„æœ€ä½³æ“ä½œç»„åˆã€‚è¿™ç§æ–¹æ³•æ›´è´´è¿‘NASçš„æœ€ç»ˆç›®æ ‡ã€‚æ­¤å¤–ï¼ŒHEP-NASä¼šåœ¨æ¯æ¬¡åˆ†å‰²åé€‰æ‹©æœ€æœ‰å¸Œæœ›çš„å­è¶…ç½‘ï¼Œé€æ­¥ç¼©å°å¯èƒ½å­˜åœ¨æœ€ä¼˜æ¶æ„çš„æœç´¢ç©ºé—´ã€‚ä¸ºäº†æ”¹è¿›å­è¶…ç½‘çš„æ€§èƒ½è¯„ä¼°ï¼ŒHEP-NASé‡‡ç”¨æœç´¢ç©ºé—´ç›¸äº’è’¸é¦æŠ€æœ¯ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶åŠ é€Ÿæ¯ä¸ªå­è¶…ç½‘çš„æ”¶æ•›ã€‚åœ¨ç»™å®šé¢„ç®—å†…ï¼ŒHEP-NASèƒ½å¤Ÿå®ç°æ‰€æœ‰è¾¹çš„åˆ†å‰²å¹¶é€æ­¥æœç´¢æ›´é«˜ç²¾åº¦çš„æ¶æ„ã€‚åœ¨ä¸åŒæ•°æ®é›†å’Œæœç´¢ç©ºé—´ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†HEP-NASç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10723v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰é¢†åŸŸå·²ç»é€šè¿‡é‡‡ç”¨ä¸€æ¬¡æ€§æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™äº›æ–¹æ³•é€šè¿‡é‡‡ç”¨æƒé‡å…±äº«ç­–ç•¥æ¥é™ä½æœç´¢æˆæœ¬ã€‚ç„¶è€Œï¼Œæ€§èƒ½ä¼°è®¡çš„å‡†ç¡®æ€§å¯èƒ½ä¼šå—åˆ°ååŒé€‚åº”æ€§çš„å¨èƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å±‚æ¬¡åˆ’åˆ†ç®—æ³•â€”â€”HEP-NASï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æé«˜å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡è¾¹ç¼˜å±‚æ¬¡åˆ’åˆ†ï¼Œæ›´ç›´æ¥åœ°æœç´¢æ¯ä¸ªä¸­é—´èŠ‚ç‚¹çš„æœ€ä½³æ“ä½œç»„åˆï¼Œä»¥ç¼©å°æœç´¢ç©ºé—´å¹¶æ‰¾åˆ°æœ€å…·æ½œåŠ›çš„å­è¶…ç½‘ç»œã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨æœç´¢ç©ºé—´ç›¸äº’è’¸é¦æŠ€æœ¯ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶åŠ é€Ÿæ¯ä¸ªå­è¶…ç½‘ç»œçš„æ”¶æ•›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰é™çš„é¢„ç®—å†…ï¼ŒHEP-NASèƒ½å¤Ÿåœ¨å„ç§æ•°æ®é›†å’Œæœç´¢ç©ºé—´ä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸€é”®å¼æ–¹æ³•åœ¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡æƒé‡å…±äº«ç­–ç•¥é™ä½æœç´¢æˆæœ¬ã€‚</li>
<li>æ€§èƒ½ä¼°è®¡çš„å‡†ç¡®æ€§å¯èƒ½å—åˆ°ååŒé€‚åº”æ€§çš„å¨èƒã€‚</li>
<li>HEP-NASæ˜¯ä¸€ç§å±‚æ¬¡åˆ’åˆ†ç®—æ³•ï¼Œé€šè¿‡åœ¨åŒä¸€å±‚æ¬¡å†…å¯¹è¾¹ç¼˜è¿›è¡Œæ’åˆ—å’Œåˆ†å‰²ï¼Œæ›´ç›´æ¥åœ°æœç´¢æœ€ä½³æ“ä½œç»„åˆã€‚</li>
<li>HEP-NASåœ¨åˆ†å‰²åé€‰æ‹©æœ€æœ‰å‰é€”çš„å­è¶…ç½‘ç»œï¼Œé€æ­¥ç¼©å°å¯èƒ½åŒ…å«æœ€ä½³æ¶æ„çš„æœç´¢ç©ºé—´ã€‚</li>
<li>HEP-NASé‡‡ç”¨æœç´¢ç©ºé—´ç›¸äº’è’¸é¦æŠ€æœ¯ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹å¹¶åŠ é€Ÿå­è¶…ç½‘ç»œçš„æ”¶æ•›ã€‚</li>
<li>HEP-NASèƒ½å¤Ÿåœ¨æœ‰é™çš„é¢„ç®—å†…å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å„ç§æ•°æ®é›†å’Œæœç´¢ç©ºé—´ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0923dc627023568b66b2fe5cf0eb79f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a38f388b64f37b97398413190ad7ec96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa397f2c6a9feaed509436147aeb6247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a9cd39fc344e5833bdf9f1f349b9f4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f25d8c13909ccda26a8873fe14d40fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65ce73e26ec8ce978504d9889b802acc.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation"></a>FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation</h2><p><strong>Authors:Yuntian Bo, Yazhou Zhu, Lunbo Li, Haofeng Zhang</strong></p>
<p>Existing few-shot medical image segmentation (FSMIS) models fail to address a practical issue in medical imaging: the domain shift caused by different imaging techniques, which limits the applicability to current FSMIS tasks. To overcome this limitation, we focus on the cross-domain few-shot medical image segmentation (CD-FSMIS) task, aiming to develop a generalized model capable of adapting to a broader range of medical image segmentation scenarios with limited labeled data from the novel target domain. Inspired by the characteristics of frequency domain similarity across different domains, we propose a Frequency-aware Matching Network (FAMNet), which includes two key components: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion (MSF) module. The FAM module tackles two problems during the meta-learning phase: 1) intra-domain variance caused by the inherent support-query bias, due to the different appearances of organs and lesions, and 2) inter-domain variance caused by different medical imaging techniques. Additionally, we design an MSF module to integrate the different frequency features decoupled by the FAM module, and further mitigate the impact of inter-domain variance on the modelâ€™s segmentation performance. Combining these two modules, our FAMNet surpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation models on three cross-domain datasets, achieving state-of-the-art performance in the CD-FSMIS task. </p>
<blockquote>
<p>ç°æœ‰çš„å°‘æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ¨¡å‹æ— æ³•è§£å†³åŒ»å­¦æˆåƒä¸­çš„ä¸€ä¸ªå®é™…é—®é¢˜ï¼šç”±ä¸åŒæˆåƒæŠ€æœ¯å¼•èµ·çš„åŸŸåç§»ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å½“å‰FSMISä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸“æ³¨äºè·¨åŸŸå°‘æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„æ–°ç›®æ ‡åŸŸçš„æ ‡è®°æ•°æ®ä¸‹ï¼Œé€‚åº”æ›´å¹¿æ³›çš„åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ã€‚æˆ‘ä»¬å—åˆ°ä¸åŒåŸŸä¹‹é—´é¢‘ç‡åŸŸç›¸ä¼¼æ€§ç‰¹å¾çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ã€‚FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰ç”±äºå™¨å®˜å’Œç—…å˜çš„ä¸åŒå¤–è§‚å¯¼è‡´çš„åŸŸå†…æ–¹å·®ï¼Œè¿™æ˜¯å›ºæœ‰çš„æ”¯æŒæŸ¥è¯¢åè§çš„ç»“æœï¼›2ï¼‰ç”±ä¸åŒçš„åŒ»å­¦æˆåƒæŠ€æœ¯å¼•èµ·çš„åŸŸé—´æ–¹å·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªMSFæ¨¡å—ï¼Œä»¥æ•´åˆFAMæ¨¡å—è§£è€¦çš„ä¸åŒé¢‘ç‡ç‰¹å¾ï¼Œå¹¶è¿›ä¸€æ­¥å‡è½»åŸŸé—´æ–¹å·®å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚ç»“åˆè¿™ä¸¤ä¸ªæ¨¡å—ï¼Œæˆ‘ä»¬çš„FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œåœ¨CD-FSMISä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09319v2">PDF</a> Accepted by the 39th Annual AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è·¨åŸŸå°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿé€‚åº”æ›´å¹¿æ³›åŒ»ç–—å›¾åƒåˆ†å‰²åœºæ™¯ã€è§£å†³ä¸åŒæˆåƒæŠ€æœ¯å¼•èµ·çš„é¢†åŸŸåç§»é—®é¢˜çš„é€šç”¨æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼ŒåŒ…æ‹¬é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ï¼Œæé«˜äº†æ¨¡å‹åœ¨å…ƒå­¦ä¹ é˜¶æ®µçš„æ€§èƒ½ã€‚è¿™äº›åˆ›æ–°ç‚¹ä½¿æ¨¡å‹åœ¨è·¨åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œå®ç°äº†åœ¨CD-FSMISä»»åŠ¡ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨åŸŸå°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰æ˜¯åŒ»ç–—å›¾åƒå¤„ç†é¢†åŸŸçš„ä¸€ä¸ªé‡è¦é—®é¢˜ï¼Œæ¶‰åŠåˆ°ä¸åŒæˆåƒæŠ€æœ¯çš„é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰æ¥è§£å†³CD-FSMISä»»åŠ¡ï¼ŒåŒ…æ‹¬é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ã€‚</li>
<li>FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šç”±å™¨å®˜å’Œç—…å˜ä¸åŒå¤–è§‚å¼•èµ·çš„å†…éƒ¨é¢†åŸŸå·®å¼‚å’Œä¸åŒåŒ»å­¦æˆåƒæŠ€æœ¯å¼•èµ·çš„è·¨é¢†åŸŸå·®å¼‚ã€‚</li>
<li>MSFæ¨¡å—ç”¨äºæ•´åˆç”±FAMæ¨¡å—åˆ†ç¦»çš„ä¸åŒé¢‘ç‡ç‰¹å¾ï¼Œè¿›ä¸€æ­¥å‡è½»è·¨é¢†åŸŸå·®å¼‚å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚</li>
<li>FAMNetæ¨¡å‹åœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºåŒ»ç–—å›¾åƒåˆ†å‰²é¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåº”å¯¹æœ‰é™çš„æ ‡ç­¾æ•°æ®å’Œä¸åŒçš„æˆåƒæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-247281a683520d1a403f40d75c810e43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa5e909180014c3b3e9b82c59c9fd06f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bebbe679237a61c8f674f9fcadf5a543.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FM2DS-Few-Shot-Multimodal-Multihop-Data-Synthesis-with-Knowledge-Distillation-for-Question-Answering"><a href="#FM2DS-Few-Shot-Multimodal-Multihop-Data-Synthesis-with-Knowledge-Distillation-for-Question-Answering" class="headerlink" title="FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge   Distillation for Question Answering"></a>FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge   Distillation for Question Answering</h2><p><strong>Authors:Amirhossein Abaskohi, Spandana Gella, Giuseppe Carenini, Issam H. Laradji</strong></p>
<p>Multimodal multihop question answering is a complex task that requires reasoning over multiple sources of information, such as images and text, to answer questions. While there has been significant progress in visual question answering, the multihop setting remains unexplored due to the lack of high-quality datasets. Current methods focus on single-hop question answering or a single modality, which makes them unsuitable for real-world scenarios such as analyzing multimodal educational materials, summarizing lengthy academic articles, or interpreting scientific studies that combine charts, images, and text. To address this gap, we propose a novel methodology, introducing the first framework for creating a high-quality dataset that enables training models for multimodal multihop question answering. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure quality data. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks, our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) on average. We believe our data synthesis method will serve as a strong foundation for training and evaluating multimodal multihop question answering models. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤šè·³é—®ç­”æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå®ƒè¦æ±‚åœ¨å¤šæºä¿¡æ¯ä¸Šè¿›è¡Œæ¨ç†ï¼Œå¦‚å›¾åƒå’Œæ–‡æœ¬ï¼Œä»¥å›ç­”é—®é¢˜ã€‚å°½ç®¡è§†è§‰é—®ç­”å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¤šè·³è®¾ç½®ä»ç„¶æœªè¢«æ¢ç´¢ã€‚å½“å‰çš„æ–¹æ³•ä¾§é‡äºå•è·³é—®ç­”æˆ–å•ä¸€æ¨¡æ€ï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆç°å®åœºæ™¯ï¼Œå¦‚åˆ†æå¤šæ¨¡æ€æ•™è‚²ææ–™ã€æ€»ç»“å†—é•¿çš„å­¦æœ¯è®ºæ–‡æˆ–è§£é‡Šç»“åˆå›¾è¡¨ã€å›¾åƒå’Œæ–‡æœ¬çš„ç§‘å­¦ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè®­ç»ƒå¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªæ¶‰åŠä»ç»´åŸºç™¾ç§‘è·å–ç›¸å…³å¤šæ¨¡æ€æ–‡æ¡£ã€åˆæˆç”Ÿæˆé«˜çº§é—®é¢˜å’Œç­”æ¡ˆã€å¹¶é€šè¿‡ä¸¥æ ¼æ ‡å‡†éªŒè¯ä»¥ç¡®ä¿æ•°æ®è´¨é‡çš„äº”ä¸ªé˜¶æ®µçš„ç®¡é“ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æˆ‘ä»¬çš„åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹å¹¶åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ ·æœ¬å¤§å°ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œåœ¨æˆ‘ä»¬çš„åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¹³å‡ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰ä¸Šæ¯”åœ¨äººç±»æ”¶é›†çš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹é«˜å‡º1.9ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„æ•°æ®åˆæˆæ–¹æ³•å°†ä¸ºè®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹æä¾›åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07030v2">PDF</a> 20 pages, 11 figures, 10 tables, Submitted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¤šæ¨¡æ€å¤šè·³é—®ç­”ä»»åŠ¡çš„é‡è¦æ€§ï¼Œå…¶éœ€è¦èåˆå›¾åƒå’Œæ–‡å­—ç­‰å¤šç§ä¿¡æ¯æ¥æºè¿›è¡Œæ¨ç†ã€‚å°½ç®¡è§†è§‰é—®ç­”ä»»åŠ¡å·²å–å¾—äº†é‡è¦è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹é«˜è´¨é‡æ•°æ®é›†ï¼Œå¤šè·³è®¾ç½®ä»æœªè¢«æ¢ç´¢ã€‚å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨å•è·³é—®ç­”æˆ–å•ä¸€æ¨¡æ€ï¼Œä¸é€‚ç”¨äºåˆ†æå¤šæ¨¡æ€æ•™è‚²ææ–™ã€æ€»ç»“å†—é•¿å­¦æœ¯è®ºæ–‡æˆ–è§£è¯»ç»“åˆå›¾è¡¨ã€æ–‡å­—å’Œå›¾åƒçš„ç§‘å­¦ç ”ç©¶ç­‰ç°å®åœºæ™¯ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé¦–æ¬¡æ„å»ºäº†é«˜è´¨é‡æ•°æ®é›†ï¼Œä¸ºè®­ç»ƒå¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹æä¾›äº†å¯èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªæ¶‰åŠä»ç»´åŸºç™¾ç§‘è·å–ç›¸å…³å¤šæ¨¡æ€æ–‡æ¡£ã€åˆæˆé«˜çº§é—®é¢˜å’Œç­”æ¡ˆã€å¹¶é€šè¿‡ä¸¥æ ¼æ ‡å‡†éªŒè¯æ•°æ®è´¨é‡çš„äº”ä¸ªé˜¶æ®µç®¡é“ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹å¹¶åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨æ ·æœ¬å¤§å°ç›¸ç­‰çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨äººæ”¶é›†çš„åŸºå‡†æ•°æ®ä¸Šçš„å¹³å‡ç²¾ç¡®åŒ¹é…å¾—åˆ†é«˜å‡º1.9åˆ†ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„æ•°æ®åˆæˆæ–¹æ³•å°†ä¸ºè®­ç»ƒå’Œè¯„ä¼°å¤šæ¨¡æ€å¤šè·³é—®ç­”æ¨¡å‹æä¾›åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤šè·³é—®ç­”æ˜¯ä¸€ä¸ªè·¨è¶Šå›¾åƒå’Œæ–‡æœ¬ç­‰å¤šä¸ªä¿¡æ¯æºçš„å¤æ‚ä»»åŠ¡ï¼Œéœ€è¦å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰ç¼ºä¹é«˜è´¨é‡æ•°æ®é›†é™åˆ¶äº†å¤šè·³é—®ç­”ä»»åŠ¡çš„è¿›å±•ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨å•æ¨¡æ€å’Œå•è·³é—®ç­”ï¼Œä¸é€‚ç”¨äºç°å®ä¸–ç•Œçš„å¤æ‚åœºæ™¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œæ„å»ºäº†é«˜è´¨é‡æ•°æ®é›†ä»¥æ”¯æŒå¤šæ¨¡æ€å¤šè·³é—®ç­”çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…æ‹¬äº”ä¸ªé˜¶æ®µçš„ç®¡é“ï¼Œæ¶‰åŠä»ç»´åŸºç™¾ç§‘è·å–å¤šæ¨¡æ€æ–‡æ¡£ã€åˆæˆé«˜çº§é—®é¢˜å’Œç­”æ¡ˆç­‰æ­¥éª¤ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ ·æœ¬å¤§å°ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œåˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¼˜äºåœ¨äººæ”¶é›†çš„åŸºå‡†æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39afa50736a23acafa909748bd95d3e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17398bc33f88f8a6a7fa79dadeef87c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b755a8d1756d2e5395ae5d455f05e72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d2d07d044e8591d3e3ce02da3e5e84f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Efficient-Transfer-Learning-for-Video-language-Foundation-Models"><a href="#Efficient-Transfer-Learning-for-Video-language-Foundation-Models" class="headerlink" title="Efficient Transfer Learning for Video-language Foundation Models"></a>Efficient Transfer Learning for Video-language Foundation Models</h2><p><strong>Authors:Haoxing Chen, Zizheng Huang, Yan Hong, Yanshuo Wang, Zhongcai Lyu, Zhuoer Xu, Jun Lan, Zhangxuan Gu</strong></p>
<p>Pre-trained vision-language models provide a robust foundation for efficient transfer learning across various downstream tasks. In the field of video action recognition, mainstream approaches often introduce additional parameter modules to capture temporal information. While the increased model capacity brought by these additional parameters helps better fit the video-specific inductive biases, existing methods require learning a large number of parameters and are prone to catastrophic forgetting of the original generalizable knowledge. In this paper, we propose a simple yet effective Multi-modal Spatio-Temporal Adapter (MSTA) to improve the alignment between representations in the text and vision branches, achieving a balance between general knowledge and task-specific knowledge. Furthermore, to mitigate over-fitting and enhance generalizability, we introduce a spatio-temporal description-guided consistency constraint. This constraint involves feeding template inputs (i.e., &#96;&#96;a video of ${\textbf{cls}}$â€™â€™) into the trainable language branch, while LLM-generated spatio-temporal descriptions are input into the pre-trained language branch, enforcing consistency between the outputs of the two branches. This mechanism prevents over-fitting to downstream tasks and improves the distinguishability of the trainable branch within the spatio-temporal semantic space. We evaluate the effectiveness of our approach across four tasks: zero-shot transfer, few-shot learning, base-to-novel generalization, and fully-supervised learning. Compared to many state-of-the-art methods, our MSTA achieves outstanding performance across all evaluations, while using only 2-7% of the trainable parameters in the original model. Code will be avaliable at <a target="_blank" rel="noopener" href="https://github.com/chenhaoxing/ETL4Video">https://github.com/chenhaoxing/ETL4Video</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒè¿‡çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºé«˜æ•ˆè¿ç§»å­¦ä¹ åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¿ç§»æä¾›äº†åšå®çš„åŸºç¡€ã€‚åœ¨è§†é¢‘åŠ¨ä½œè¯†åˆ«é¢†åŸŸï¼Œä¸»æµæ–¹æ³•é€šå¸¸å¼•å…¥é¢å¤–çš„å‚æ•°æ¨¡å—æ¥æ•è·æ—¶é—´ä¿¡æ¯ã€‚è™½ç„¶è¿™äº›é¢å¤–å‚æ•°å¸¦æ¥çš„æ¨¡å‹å®¹é‡å¢åŠ æœ‰åŠ©äºæ›´å¥½åœ°é€‚åº”è§†é¢‘ç‰¹å®šçš„å½’çº³åè§ï¼Œä½†ç°æœ‰æ–¹æ³•éœ€è¦å­¦ä¹ å¤§é‡å‚æ•°ï¼Œå¹¶å®¹æ˜“é—å¿˜åŸå§‹çš„é€šç”¨çŸ¥è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¤šæ¨¡æ€æ—¶ç©ºé€‚é…å™¨ï¼ˆMSTAï¼‰ï¼Œä»¥æé«˜æ–‡æœ¬å’Œè§†è§‰åˆ†æ”¯ä¹‹é—´è¡¨ç¤ºçš„å¯¹é½ï¼Œåœ¨é€šç”¨çŸ¥è¯†å’Œä»»åŠ¡ç‰¹å®šçŸ¥è¯†ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£è¿‡æ‹Ÿåˆå¹¶å¢å¼ºé€šç”¨æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å—æ—¶ç©ºæè¿°å¼•å¯¼çš„ä¸€è‡´æ€§çº¦æŸã€‚è¯¥çº¦æŸæ¶‰åŠå°†æ¨¡æ¿è¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œâ€œä¸€ä¸ªè§†é¢‘ä¸­çš„${\textbf{cls}}$â€ï¼‰è¾“å…¥åˆ°å¯è®­ç»ƒçš„è¯­è¨€åˆ†æ”¯ï¼ŒåŒæ—¶å°†LLMç”Ÿæˆçš„æ—¶ç©ºæè¿°è¾“å…¥åˆ°é¢„è®­ç»ƒçš„è¯­è¨€åˆ†æ”¯ï¼Œå¼ºåˆ¶ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡ºä¿æŒä¸€è‡´ã€‚è¿™ç§æœºåˆ¶é˜²æ­¢äº†å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„è¿‡åº¦æ‹Ÿåˆï¼Œæé«˜äº†å¯è®­ç»ƒåˆ†æ”¯åœ¨æ—¶ç©ºè¯­ä¹‰ç©ºé—´ä¸­çš„å¯åŒºåˆ†æ€§ã€‚æˆ‘ä»¬åœ¨å››ç§ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šé›¶æ ·æœ¬è¿ç§»ã€å°æ ·å­¦ä¹ ã€åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–å’Œå…¨ç›‘ç£å­¦ä¹ ã€‚ä¸è®¸å¤šå…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„MSTAåœ¨æ‰€æœ‰è¯„ä¼°ä¸­éƒ½å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒæ—¶åªä½¿ç”¨äº†åŸå§‹æ¨¡å‹ä¸­2-7%çš„å¯è®­ç»ƒå‚æ•°ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/chenhaoxing/ETL4Video%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/chenhaoxing/ETL4Videoä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11223v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¤šæ¨¡æ€æ—¶ç©ºé€‚é…å™¨ï¼ˆMSTAï¼‰ï¼Œç”¨äºæ”¹å–„æ–‡æœ¬å’Œè§†è§‰åˆ†æ”¯ä¹‹é—´çš„è¡¨ç¤ºå¯¹é½ï¼Œå®ç°äº†é€šç”¨çŸ¥è¯†å’Œä»»åŠ¡ç‰¹å®šçŸ¥è¯†ä¹‹é—´çš„å¹³è¡¡ã€‚ä¸ºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶å‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¼•å…¥äº†æ—¶ç©ºæè¿°å¼•å¯¼çš„ä¸€è‡´æ€§çº¦æŸã€‚åœ¨å››ä¸ªä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMSTAåœ¨é›¶æ ·æœ¬è¿ç§»ã€å°‘æ ·æœ¬å­¦ä¹ ã€åŸºç¡€åˆ°æ–°é¢–æ³›åŒ–ä»¥åŠå…¨ç›‘ç£å­¦ä¹ æ–¹é¢éƒ½å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨åŸå§‹æ¨¡å‹ä¸­2-7%çš„å¯è®­ç»ƒå‚æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ—¶ç©ºé€‚é…å™¨ï¼ˆMSTAï¼‰å¢å¼ºäº†é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºå¯¹é½ï¼Œæé«˜äº†åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¿ç§»å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>MSTAå®ç°äº†é€šç”¨çŸ¥è¯†å’Œä»»åŠ¡ç‰¹å®šçŸ¥è¯†ä¹‹é—´çš„å¹³è¡¡ï¼Œé€šè¿‡å¼•å…¥å°‘é‡é¢å¤–å‚æ•°æ¨¡å—æ¥æ•æ‰æ—¶ç©ºä¿¡æ¯ã€‚</li>
<li>å¼•å…¥çš„æ—¶ç©ºæè¿°å¼•å¯¼çš„ä¸€è‡´æ€§çº¦æŸæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å‡è½»äº†è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æ¨¡æ¿è¾“å…¥å’ŒLLMç”Ÿæˆçš„æ—¶ç©ºæè¿°ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨æ—¶ç©ºè¯­ä¹‰ç©ºé—´ä¸­çš„å¯è¾¨è¯†æ€§ã€‚</li>
<li>MSTAåœ¨å››ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡ä¼˜äºè®¸å¤šæœ€æ–°æ–¹æ³•ï¼Œä¸”ä»…ä½¿ç”¨åŸå§‹æ¨¡å‹çš„å¾ˆå°éƒ¨åˆ†å¯è®­ç»ƒå‚æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œé€‚ç”¨äºé›¶æ ·æœ¬è¿ç§»ã€å°‘æ ·æœ¬å­¦ä¹ ã€åŸºç¡€åˆ°æ–°é¢–æ³›åŒ–ä»¥åŠå…¨ç›‘ç£å­¦ä¹ ç­‰ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-462c8bd756b977acd3244a7e2ca31863.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79bda175dcb2bbf09576150edab05e1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c2044ceba096091f3f1263a8f0800e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2102ca4e507be97cbcb2117b41e290.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1><h2 id="Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models"><a href="#Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models" class="headerlink" title="Revisiting In-context Learning Inference Circuit in Large Language   Models"></a>Revisiting In-context Learning Inference Circuit in Large Language   Models</h2><p><strong>Authors:Hakaze Cho, Mariko Kato, Yoshihiro Sakai, Naoya Inoue</strong></p>
<p>In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Input Text Encode: LMs encode every input text (demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ˜¯ä¸€ç§æ–°å…´çš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å°æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œå…¶å†…éƒ¨æœºåˆ¶å°šæœªè¢«æ¢ç´¢ã€‚å·²æœ‰å·¥ä½œæè¿°äº†ICLçš„å†…éƒ¨å¤„ç†è¿‡ç¨‹ï¼Œä½†å¾ˆéš¾æ•æ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨ç†ç°è±¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»¼åˆç”µè·¯æ¥æ¨¡æ‹Ÿæ¨ç†åŠ¨æ€ï¼Œå¹¶è¯•å›¾è§£é‡Šè§‚å¯Ÿåˆ°çš„ICLç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ICLæ¨ç†è¿‡ç¨‹åˆ†ä¸ºä¸‰å¤§æ“ä½œï¼šï¼ˆ1ï¼‰è¾“å…¥æ–‡æœ¬ç¼–ç ï¼šLMå°†æ¯ä¸ªè¾“å…¥æ–‡æœ¬ï¼ˆæ¼”ç¤ºå’ŒæŸ¥è¯¢ï¼‰ç¼–ç ä¸ºéšè—çŠ¶æ€ä¸­çš„çº¿æ€§è¡¨ç¤ºï¼Œå…¶ä¸­åŒ…å«è§£å†³ICLä»»åŠ¡æ‰€éœ€çš„ä¿¡æ¯ã€‚ï¼ˆ2ï¼‰è¯­ä¹‰åˆå¹¶ï¼šLMå°†æ¼”ç¤ºçš„ç¼–ç è¡¨ç¤ºä¸å…¶ç›¸åº”çš„æ ‡ç­¾ä»¤ç‰Œåˆå¹¶ï¼Œä»¥äº§ç”Ÿæ ‡ç­¾å’Œæ¼”ç¤ºçš„è”åˆè¡¨ç¤ºã€‚ï¼ˆ3ï¼‰ç‰¹å¾æ£€ç´¢å’Œå¤åˆ¶ï¼šLMåœ¨ä»»åŠ¡å­ç©ºé—´ä¸­æœç´¢ä¸æŸ¥è¯¢è¡¨ç¤ºç›¸ä¼¼çš„è”åˆè¡¨ç¤ºï¼Œå¹¶å°†æ‰€æœç´¢çš„è¡¨ç¤ºå¤åˆ¶åˆ°æŸ¥è¯¢ä¸­ã€‚ç„¶åï¼Œè¯­è¨€æ¨¡å‹å¤´éƒ¨åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ•è·è¿™äº›å¤åˆ¶çš„æ ‡ç­¾è¡¨ç¤ºï¼Œå¹¶å°†å…¶è§£ç ä¸ºé¢„æµ‹æ ‡ç­¾ã€‚æ‰€æå‡ºçš„æ¨ç†ç”µè·¯æˆåŠŸåœ°æ•æ‰åˆ°äº†ICLè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„è®¸å¤šç°è±¡ï¼Œæ˜¯å¯¹ICLæ¨ç†è¿‡ç¨‹çš„å…¨é¢å’Œå®é™…è§£é‡Šã€‚æ­¤å¤–ï¼Œé€šè¿‡ç¦ç”¨æ‰€æå‡ºçš„æ­¥éª¤è¿›è¡Œçš„åˆ†æä¸¥é‡æŸå®³äº†ICLçš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜æ‰€æå‡ºçš„æ¨ç†ç”µè·¯æ˜¯ä¸»å¯¼æœºåˆ¶ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ç¡®è®¤å¹¶åˆ—å‡ºäº†ä¸€äº›ä¸æ‰€æå‡ºçš„ç”µè·¯å¹¶è¡Œè§£å†³ICLä»»åŠ¡çš„æ—è·¯æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04468v2">PDF</a> 37 pages, 41 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹è¯­è¨€æ¨¡å‹ä¸­çš„In-context Learningï¼ˆICLï¼‰æå‡ºäº†ä¸€ç§å…¨é¢çš„æ¨ç†ç”µè·¯æ¨¡å‹ï¼Œç”¨ä»¥è§£é‡ŠICLè¿‡ç¨‹ä¸­çš„æ¨ç†ç°è±¡ã€‚è¯¥ç”µè·¯å°†ICLæ¨ç†åˆ†ä¸ºä¸‰å¤§æ“ä½œï¼šè¾“å…¥æ–‡æœ¬ç¼–ç ã€è¯­ä¹‰åˆå¹¶ä»¥åŠç‰¹å¾æ£€ç´¢ä¸å¤åˆ¶ã€‚è¯¥æ¨¡å‹æˆåŠŸæ•æ‰äº†ICLè¿‡ç¨‹ä¸­çš„è®¸å¤šç°è±¡ï¼Œä¸ºICLæä¾›äº†å…¨é¢è€Œå®ç”¨çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œé€šè¿‡å»é™¤è¯¥ç”µè·¯ä¸­çš„æŸäº›æ­¥éª¤ï¼ŒICLæ€§èƒ½å—åˆ°ä¸¥é‡å½±å“ï¼Œè¯æ˜äº†è¯¥ç”µè·¯çš„ä¸»å¯¼ä½œç”¨ã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜ç¡®è®¤äº†ä¸ç”µè·¯å¹¶è¡Œè§£å†³ICLä»»åŠ¡çš„æŸäº›æ—è·¯æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>In-context Learningï¼ˆICLï¼‰æ˜¯ä¸€ç§æ–°å…´çš„è¯­è¨€æ¨¡å‹ä¸­çš„å°æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œå…¶å†…éƒ¨æœºåˆ¶å°šæœªè¢«å®Œå…¨æ¢ç´¢ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨æè¿°ICLçš„å†…éƒ¨å¤„ç†æ–¹é¢å·²æœ‰æˆæœï¼Œä½†éš¾ä»¥æ•æ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨ç†ç°è±¡ã€‚</li>
<li>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„æ¨ç†ç”µè·¯æ¨¡å‹ï¼Œç”¨ä»¥è§£é‡ŠICLè¿‡ç¨‹ä¸­çš„æ¨ç†ç°è±¡ï¼ŒåŒ…æ‹¬è¾“å…¥æ–‡æœ¬ç¼–ç ã€è¯­ä¹‰åˆå¹¶ä»¥åŠç‰¹å¾æ£€ç´¢ä¸å¤åˆ¶ä¸‰å¤§æ“ä½œã€‚</li>
<li>è¯¥ç”µè·¯æ¨¡å‹æˆåŠŸæ•æ‰äº†ICLè¿‡ç¨‹ä¸­çš„è®¸å¤šç°è±¡ï¼Œä¸ºç†è§£è¿™ä¸€è¿‡ç¨‹æä¾›äº†å…¨é¢è€Œå®ç”¨çš„è§£é‡Šã€‚</li>
<li>é€šè¿‡å»é™¤ç”µè·¯ä¸­çš„æŸäº›æ­¥éª¤ï¼ŒICLæ€§èƒ½å—åˆ°ä¸¥é‡å½±å“ï¼Œè¯æ˜äº†è¯¥ç”µè·¯çš„ä¸»å¯¼ä½œç”¨ã€‚</li>
<li>è®ºæ–‡è¿˜ç¡®è®¤äº†ä¸ç”µè·¯å¹¶è¡Œè§£å†³ICLä»»åŠ¡çš„æŸäº›æ—è·¯æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-abe9169fc4d79ec3bcf6aaa83cee09a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2671ca5bf8a8f7ced3f326fe11d206e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9f2bc2ba4df900d15bff2a53d7fa5a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c759ab722c13c3d7958c15492bca666.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a91205a65b64bc5f5dda409bd8f005a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f54f25de98cb55cea65795548facddaf.jpg" align="middle">
</details>


<h1 id="-20"><a href="#-20" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b22f0081c44a3181d17141f1ff9c9af7.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Real-Time Position-Aware View Synthesis from Single-View Input
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-da617ccd25a73f3719a11187f1ec454b.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Make Imagination Clearer! Stable Diffusion-based Visual Imagination for   Multimodal Machine Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">9113.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
