<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2024-12-19  FarExStance Explainable Stance Detection for Farsi">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-a91205a65b64bc5f5dda409bd8f005a2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    86 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-19-更新"><a href="#2024-12-19-更新" class="headerlink" title="2024-12-19 更新"></a>2024-12-19 更新</h1><h2 id="FarExStance-Explainable-Stance-Detection-for-Farsi"><a href="#FarExStance-Explainable-Stance-Detection-for-Farsi" class="headerlink" title="FarExStance: Explainable Stance Detection for Farsi"></a>FarExStance: Explainable Stance Detection for Farsi</h2><p><strong>Authors:Majid Zarharan, Maryam Hashemi, Malika Behroozrazegh, Sauleh Eetemadi, Mohammad Taher Pilehvar, Jennifer Foster</strong></p>
<p>We introduce FarExStance, a new dataset for explainable stance detection in Farsi. Each instance in this dataset contains a claim, the stance of an article or social media post towards that claim, and an extractive explanation which provides evidence for the stance label. We compare the performance of a fine-tuned multilingual RoBERTa model to several large language models in zero-shot, few-shot, and parameter-efficient fine-tuned settings on our new dataset. On stance detection, the most accurate models are the fine-tuned RoBERTa model, the LLM Aya-23-8B which has been fine-tuned using parameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the quality of the explanations, our automatic evaluation metrics indicate that few-shot GPT-4o generates the most coherent explanations, while our human evaluation reveals that the best Overall Explanation Score (OES) belongs to few-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced explanations most closely aligned with the reference explanations. </p>
<blockquote>
<p>我们介绍了FarExStance数据集，这是一个用于波斯语的可解释立场检测的新数据集。该数据集中的每个实例都包含一个声明、一篇文章或社交媒体帖子对该声明的立场以及提供立场标签证据的解释性摘要。我们在新的数据集上对比了微调的多语言RoBERTa模型与多种大型语言模型在零样本、少样本和参数高效微调设置中的表现。在立场检测方面，表现最准确的模型是微调的RoBERTa模型、经过参数高效微调的大型语言模型Aya-23-8B和少样本的Claude-3.5-Sonnet。关于解释的质量，我们的自动评估指标显示，少样本的GPT-4o生成的解释最为连贯，而我们的人工评估显示，最好的整体解释得分（OES）属于少样本的Claude-3.5-Sonnet。经过微调的Aya-32-8B模型生成的解释与参考解释最为一致。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14008v1">PDF</a> Accepted in COLING 2025</p>
<p><strong>Summary</strong></p>
<p>FarExStance数据集用于波斯语的解释性立场检测。研究对比了微调过的多语言RoBERTa模型、大型语言模型在零样本、少样本和参数高效微调设置上的表现。在立场检测方面，微调的RoBERTa模型、经过参数高效调教的LLM aya-23-8B和少样本的Claude-3.5-Sonnet表现最准确。在解释质量方面，GPT-4o的自动评估指标显示其解释最连贯，而人工评估显示最好的整体解释分数来自少样本的Claude-3.5-Sonnet。同时，经过微调的aya-32-8B模型生成的解释与参考解释最为接近。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FarExStance是一个用于波斯语解释性立场检测的新数据集。</li>
<li>研究对比了多种模型在FarExStance数据集上的表现。</li>
<li>在立场检测方面，微调的RoBERTa模型、LLM aya-23-8B（参数高效微调）和少样本的Claude-3.5-Sonnet表现最佳。</li>
<li>在解释质量方面，GPT-4o的自动评估指标显示其解释最连贯。</li>
<li>人工评估显示最好的整体解释分数来自少样本的Claude-3.5-Sonnet模型。</li>
<li>相比其他模型，aya-32-8B模型生成的解释与参考解释最为接近。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14008">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-67e37634e11ac5ea39f3c8312d0000eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-251163be1bdfa11a3fa8a4d30185cd80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb64ac1dfb43540806096b88b9c90ed6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Few-shot-Steerable-Alignment-Adapting-Rewards-and-LLM-Policies-with-Neural-Processes"><a href="#Few-shot-Steerable-Alignment-Adapting-Rewards-and-LLM-Policies-with-Neural-Processes" class="headerlink" title="Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with   Neural Processes"></a>Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with   Neural Processes</h2><p><strong>Authors:Katarzyna Kobalczyk, Claudio Fanconi, Hao Sun, Mihaela van der Schaar</strong></p>
<p>As large language models (LLMs) become increasingly embedded in everyday applications, ensuring their alignment with the diverse preferences of individual users has become a critical challenge. Currently deployed approaches typically assume homogeneous user objectives and rely on single-objective fine-tuning. However, human preferences are inherently heterogeneous, influenced by various unobservable factors, leading to conflicting signals in preference data. Existing solutions addressing this diversity often require costly datasets labelled for specific objectives and involve training multiple reward models or LLM policies, which is computationally expensive and impractical. In this work, we present a novel framework for few-shot steerable alignment, where users’ underlying preferences are inferred from a small sample of their choices. To achieve this, we extend the Bradley-Terry-Luce model to handle heterogeneous preferences with unobserved variability factors and propose its practical implementation for reward modelling and LLM fine-tuning. Thanks to our proposed approach of functional parameter-space conditioning, LLMs trained with our framework can be adapted to individual preferences at inference time, generating outputs over a continuum of behavioural modes. We empirically validate the effectiveness of methods, demonstrating their ability to capture and align with diverse human preferences in a data-efficient manner. Our code is made available at: <a target="_blank" rel="noopener" href="https://github.com/kasia-kobalczyk/few-shot-steerable-alignment">https://github.com/kasia-kobalczyk/few-shot-steerable-alignment</a>. </p>
<blockquote>
<p>随着大型语言模型（LLM）在日常应用中的嵌入程度越来越高，确保它们与个别用户的多样化偏好保持一致已成为一项关键挑战。当前部署的方法通常假设用户目标是均质的，并依赖于单目标微调。然而，人类偏好本质上是异质的，受到各种不可观察因素的影响，导致偏好数据中的冲突信号。解决这种多样性的现有解决方案通常需要为特定目标标记的昂贵数据集，并涉及训练多个奖励模型或LLM策略，这在计算上很昂贵且不切实际。在这项工作中，我们提出了一种用于少样本可控对齐的新型框架，其中用户的潜在偏好是从其选择的小样本中推断出来的。为了实现这一点，我们扩展了Bradley-Terry-Luce模型以处理具有未观察到的变异性因素的异质偏好，并提出了其用于奖励建模和LLM调参的实际实现。由于我们提出的功能参数空间调节方法，使用我们的框架训练的LLM可以在推理时适应个人偏好，在连续的行为模式上生成输出。我们通过实证验证了方法的有效性，证明了它们在数据高效的方式下捕捉和与人类多样化偏好保持一致的能力。我们的代码可在：<a target="_blank" rel="noopener" href="https://github.com/kasia-kobalczyk/few-shot-steerable-alignment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kasia-kobalczyk/few-shot-steerable-alignment找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13998v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在日常应用中的普及对确保其与用户多样化偏好的对齐提出了巨大挑战。现有方法假设用户目标的一致性并依赖于单一目标的微调。然而，人类偏好本质上是多样化的，受到各种不可观察因素的影响，导致偏好数据中的冲突信号。为解决这一多样性，现有解决方案通常需要针对特定目标进行昂贵的标注数据集，并涉及训练多个奖励模型或LLM策略，计算成本高且不切实际。本研究提出了一种新的少样本可操控对齐框架，通过少量用户选择样本推断用户的潜在偏好。我们扩展了Bradley-Terry-Luce模型以处理具有未观察到的变异因素的异质偏好，并提出了其实用于奖励建模和LLM微调的实际实现。借助我们的功能参数空间条件处理方法，用我们的框架训练的LLM可以在推理时间适应个人偏好，生成一系列行为模式的输出。我们验证了方法的有效性，并展示了其在数据高效方式下捕捉和与人类多样化偏好对齐的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）与用户的多样化偏好对齐是一个关键挑战。</li>
<li>现有方法通常基于假设用户目标的同质性并依赖单一目标微调，但这并不适用于人类偏好的多样性。</li>
<li>人类偏好受到多种不可观察因素的影响，导致偏好数据中的冲突信号。</li>
<li>解决此问题需要适应少量用户选择的样本数据。</li>
<li>本研究提出了一种新的少样本可操控对齐框架，扩展了Bradley-Terry-Luce模型以处理具有未观察到的变异因素的异质偏好。</li>
<li>提出了一种实际实现用于奖励建模和LLM微调的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13998">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a6fe174039e4aaa57f5c8cf8d7806c26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58e109d601c98b7d17841e2c7f519242.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-954a0c9d8bac53512f36968a758ded8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fc96e12e3406a3e03c5374ebf33d2b0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Prompting-and-Few-Shot-Fine-Tuning-Revisiting-Document-Image-Classification-Using-Large-Language-Models"><a href="#Zero-Shot-Prompting-and-Few-Shot-Fine-Tuning-Revisiting-Document-Image-Classification-Using-Large-Language-Models" class="headerlink" title="Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image   Classification Using Large Language Models"></a>Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image   Classification Using Large Language Models</h2><p><strong>Authors:Anna Scius-Bertrand, Michael Jungo, Lars Vögtlin, Jean-Marc Spat, Andreas Fischer</strong></p>
<p>Classifying scanned documents is a challenging problem that involves image, layout, and text analysis for document understanding. Nevertheless, for certain benchmark datasets, notably RVL-CDIP, the state of the art is closing in to near-perfect performance when considering hundreds of thousands of training samples. With the advent of large language models (LLMs), which are excellent few-shot learners, the question arises to what extent the document classification problem can be addressed with only a few training samples, or even none at all. In this paper, we investigate this question in the context of zero-shot prompting and few-shot model fine-tuning, with the aim of reducing the need for human-annotated training samples as much as possible. </p>
<blockquote>
<p>分类扫描文档是一个涉及图像、布局和文本分析以理解文档的挑战性问题。然而，对于某些基准数据集（尤其是RVL-CDIP），考虑到数十万个训练样本，当前技术的状态正在接近近乎完美的性能。随着大型语言模型（LLM）的出现，这些模型是出色的少样本学习者，因此出现的问题是，仅使用少量训练样本，甚至完全不使用样本，可以在多大程度上解决文档分类问题。在本文中，我们在零样本提示和少样本模型微调的背景下调查了这个问题，旨在尽可能减少对人工注释训练样本的需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13859v1">PDF</a> ICPR 2024</p>
<p><strong>Summary</strong></p>
<p>本文探讨了使用大型语言模型（LLMs）进行文档分类的问题，尤其是在零样本提示和少量模型微调的环境下。文章旨在通过减少对人类注释训练样本的需求来解决文档分类问题，对零样本学习和少量训练样本的场景进行深入分析。通过此方式提升机器学习模型的效率与准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文档分类是一个涉及图像、布局和文本分析的问题。随着大型语言模型（LLMs）的出现，尤其是其作为优秀的少样本学习者，人们开始探讨文档分类问题是否可以通过少量或零训练样本解决。</li>
<li>该论文旨在减少文档分类对大量人工标注训练样本的依赖，探究在零样本提示和少样本模型微调情境下的解决方案。其中“零样本学习”技术至关重要，机器能理解和推断人类语言的间接意图与概念转移知识，从而实现跨场景学习与理解任务迁移能力。这也符合少样本学习的思想精髓——使用有限样本完成复杂的任务学习。这也是当前人工智能研究的重要趋势之一。这一方法有望进一步推进机器学习的实际应用，尤其是在需要大量人工标注训练样本的领域如文档分类等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13859">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d545620c8036701c634f5f0f9266997e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-414d1c3268f18702de6a5061906b166b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2130c6755888cf58752c4485e8d3b385.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81f3193936bb969baccc0e9769154ef0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Extreme-Multi-label-Completion-for-Semantic-Document-Labelling-with-Taxonomy-Aware-Parallel-Learning"><a href="#Extreme-Multi-label-Completion-for-Semantic-Document-Labelling-with-Taxonomy-Aware-Parallel-Learning" class="headerlink" title="Extreme Multi-label Completion for Semantic Document Labelling with   Taxonomy-Aware Parallel Learning"></a>Extreme Multi-label Completion for Semantic Document Labelling with   Taxonomy-Aware Parallel Learning</h2><p><strong>Authors:Julien Audiffren, Christophe Broillet, Ljiljana Dolamic, Philippe Cudré-Mauroux</strong></p>
<p>In Extreme Multi Label Completion (XMLCo), the objective is to predict the missing labels of a collection of documents. Together with XML Classification, XMLCo is arguably one of the most challenging document classification tasks, as the very high number of labels (at least ten of thousands) is generally very large compared to the number of available labelled documents in the training dataset. Such a task is often accompanied by a taxonomy that encodes the labels organic relationships, and many methods have been proposed to leverage this hierarchy to improve the results of XMLCo algorithms. In this paper, we propose a new approach to this problem, TAMLEC (Taxonomy-Aware Multi-task Learning for Extreme multi-label Completion). TAMLEC divides the problem into several Taxonomy-Aware Tasks, i.e. subsets of labels adapted to the hierarchical paths of the taxonomy, and trains on these tasks using a dynamic Parallel Feature sharing approach, where some parts of the model are shared between tasks while others are task-specific. Then, at inference time, TAMLEC uses the labels available in a document to infer the appropriate tasks and to predict missing labels. To achieve this result, TAMLEC uses a modified transformer architecture that predicts ordered sequences of labels on a Weak-Semilattice structure that is naturally induced by the tasks. This approach yields multiple advantages. First, our experiments on real-world datasets show that TAMLEC outperforms state-of-the-art methods for various XMLCo problems. Second, TAMLEC is by construction particularly suited for few-shots XML tasks, where new tasks or labels are introduced with only few examples, and extensive evaluations highlight its strong performance compared to existing methods. </p>
<blockquote>
<p>在极端多标签补全（XMLCo）中，目标是预测一组文档中的缺失标签。与XML分类一起，XMLCo可以说是最具挑战性的文档分类任务之一，因为标签数量（至少数万）通常比训练数据集中可用的带标签文档的数量要大得多。此类任务通常伴随着对标签有机关系进行编码的分类法，已经提出了许多方法来利用这个层次结构来改善XMLCo算法的结果。在本文中，我们针对这个问题提出了一种新方法：TAMLEC（用于极端多标签补全的分层感知多任务学习）。TAMLEC将问题分为多个层次感知任务，即适应分类层次路径的标签子集，并使用动态并行特征共享方法在这些任务上进行训练，其中模型的部分组件在任务之间是共享的，而其他一些是特定于任务的。然后，在推理时，TAMLEC使用文档中可用的标签来推断适当的任务并预测缺失的标签。为了实现这一结果，TAMLEC使用了一种改进的转换器架构，该架构在弱半格结构上预测标签的有序序列，该结构自然是由任务引起的。这种方法具有多个优点。首先，我们在真实数据集上的实验表明，TAMLEC优于各种XMLCo问题的最新方法。其次，TAMLEC在构建时特别适合小样本的XML任务，其中引入的新任务或标签只有少数几个示例，并且广泛的评估突出了其与现有方法的强大性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13809v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了针对极端多标签补全任务（XMLCo）的新方法TAMLEC。TAMLEC将问题分为多个与税收等级相符的子任务，并利用动态并行特征共享方法对其进行训练。在推断时，TAMLEC利用文档中的现有标签来推断适当的任务并预测缺失的标签。该方法在真实数据集上的实验表现优于其他先进方法，尤其适用于只有少量示例的新任务或标签的XML任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Extreme Multi Label Completion (XMLCo)旨在预测一组文档缺失的标签，是极具挑战性的文档分类任务之一。<br>2.TAMLEC（Taxonomies-Aware Multi-task Learning for Extreme multi-label Completion）是一种新的解决方法，它将问题划分为多个与税收等级相符的子任务。<br>3.TAMLEC利用动态并行特征共享方法进行训练，部分模型部件是共享任务，而其他部件是特定任务的。<br>4.在推断时，TAMLEC使用文档中的现有标签来推断适当的任务并预测缺失的标签。<br>5.TAMLEC采用修改后的转换器架构，在弱半格结构上预测标签的有序序列，这是由任务自然引发的。<br>6.实验表明，TAMLEC在真实数据集上的表现优于其他最先进的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13809">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-064c5f737071fc0d4a8070bc4a6a9358.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a54bd87c5258bd087da10faf01fb353a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-683c10f5d119fb1ffee31705bc2a559e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a73d8c514c1d64e6961093305fd3eace.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Modelling-Multi-modal-Cross-interaction-for-ML-FSIC-Based-on-Local-Feature-Selection"><a href="#Modelling-Multi-modal-Cross-interaction-for-ML-FSIC-Based-on-Local-Feature-Selection" class="headerlink" title="Modelling Multi-modal Cross-interaction for ML-FSIC Based on Local   Feature Selection"></a>Modelling Multi-modal Cross-interaction for ML-FSIC Based on Local   Feature Selection</h2><p><strong>Authors:Kun Yan, Zied Bouraoui, Fangyun Wei, Chang Xu, Ping Wang, Shoaib Jameel, Steven Schockaert</strong></p>
<p>The aim of multi-label few-shot image classification (ML-FSIC) is to assign semantic labels to images, in settings where only a small number of training examples are available for each label. A key feature of the multi-label setting is that images often have several labels, which typically refer to objects appearing in different regions of the image. When estimating label prototypes, in a metric-based setting, it is thus important to determine which regions are relevant for which labels, but the limited amount of training data and the noisy nature of local features make this highly challenging. As a solution, we propose a strategy in which label prototypes are gradually refined. First, we initialize the prototypes using word embeddings, which allows us to leverage prior knowledge about the meaning of the labels. Second, taking advantage of these initial prototypes, we then use a Loss Change Measurement~(LCM) strategy to select the local features from the training images (i.e.\ the support set) that are most likely to be representative of a given label. Third, we construct the final prototype of the label by aggregating these representative local features using a multi-modal cross-interaction mechanism, which again relies on the initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC, NUS-WIDE, and iMaterialist show that our model substantially improves the current state-of-the-art. </p>
<blockquote>
<p>多标签小样本图像分类（ML-FSIC）的目标是在每个标签只有少量训练样本的情况下，为图像分配语义标签。多标签设置的一个关键特点是图像通常具有多个标签，这些标签通常指的是图像中不同区域出现的对象。在基于度量的环境中估计标签原型时，确定哪些区域与哪些标签相关非常重要，但训练数据的有限性和局部特征的噪声性质使得这极具挑战性。作为一种解决方案，我们提出了一种逐渐完善标签原型的策略。首先，我们使用词嵌入来初始化原型，这使我们能够利用有关标签含义的先验知识。其次，利用这些初始原型，我们采用损失变化测量（LCM）策略来选择训练图像（即支持集）中最可能代表给定标签的局部特征。第三，我们通过多模态交叉交互机制聚合这些代表性局部特征，构建标签的最终原型，这同样依赖于最初的基于词嵌入的原型。在COCO、PASCAL VOC、NUS-WIDE和iMaterialist上的实验表明，我们的模型大幅提升了当前的最佳水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13732v1">PDF</a> Accepted in Transactions on Multimedia Computing Communications and   Applications</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多标签少样本图像分类（ML-FSIC）的目标，即在每个标签只有少量训练样本的情况下，对图像进行语义标签分配。文章提出了一种基于度量方法的标签原型估计策略，通过逐步优化标签原型来解决训练数据有限和局部特征噪声的问题。首先，使用词嵌入初始化标签原型以利用标签的先验知识；其次，利用损失变化测量（LCM）策略选择最可能代表给定标签的训练图像局部特征；最后，通过多模态交叉交互机制聚合这些代表性局部特征来构建最终的标签原型。实验结果表明，该方法在COCO、PASCAL VOC、NUS-WIDE和iMaterialist数据集上显著提高了当前技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多标签少样本图像分类（ML-FSIC）的目标是在有限的训练样本下，为图像分配多个语义标签。</li>
<li>标签原型估计在ML-FSIC中至关重要，因为图像通常具有多个标签，这些标签通常指代图像不同区域的物体。</li>
<li>提出的策略通过逐步优化标签原型来解决训练数据限制和局部特征噪声问题。</li>
<li>初始化标签原型时使用词嵌入，以利用标签的先验知识。</li>
<li>采用损失变化测量（LCM）策略选择最可能代表给定标签的训练图像局部特征。</li>
<li>通过多模态交叉交互机制聚合代表性局部特征来构建最终的标签原型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13732">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-640f13acf14d7b037c48a4f37ae50135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02bd4166c52127f4dcc44d1e0849b94.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AnchorInv-Few-Shot-Class-Incremental-Learning-of-Physiological-Signals-via-Representation-Space-Guided-Inversion"><a href="#AnchorInv-Few-Shot-Class-Incremental-Learning-of-Physiological-Signals-via-Representation-Space-Guided-Inversion" class="headerlink" title="AnchorInv: Few-Shot Class-Incremental Learning of Physiological Signals   via Representation Space Guided Inversion"></a>AnchorInv: Few-Shot Class-Incremental Learning of Physiological Signals   via Representation Space Guided Inversion</h2><p><strong>Authors:Chenqi Li, Boyan Gao, Gabriel Jones, Timothy Denison, Tingting Zhu</strong></p>
<p>Deep learning models have demonstrated exceptional performance in a variety of real-world applications. These successes are often attributed to strong base models that can generalize to novel tasks with limited supporting data while keeping prior knowledge intact. However, these impressive results are based on the availability of a large amount of high-quality data, which is often lacking in specialized biomedical applications. In such fields, models are usually developed with limited data that arrive incrementally with novel categories. This requires the model to adapt to new information while preserving existing knowledge. Few-Shot Class-Incremental Learning (FSCIL) methods offer a promising approach to addressing these challenges, but they also depend on strong base models that face the same aforementioned limitations. To overcome these constraints, we propose AnchorInv following the straightforward and efficient buffer-replay strategy. Instead of selecting and storing raw data, AnchorInv generates synthetic samples guided by anchor points in the feature space. This approach protects privacy and regularizes the model for adaptation. When evaluated on three public physiological time series datasets, AnchorInv exhibits efficient knowledge forgetting prevention and improved adaptation to novel classes, surpassing state-of-the-art baselines. </p>
<blockquote>
<p>深度学习模型在多种实际应用中表现出了卓越的性能。这些成功往往归功于能够推广到新型任务且保持先验知识不变的强大基础模型。然而，这些令人印象深刻的结果是基于大量高质量数据的可用性，这在专门的生物医学应用中往往缺乏。在这些领域中，模型通常是在有限的数据上开发的，这些数据会随着时间的推移陆续出现新的类别。这要求模型在适应新信息的同时保持现有知识。小样本类增量学习（FSCIL）方法为解决这些挑战提供了一种有前途的解决思路，但它们同样依赖于上述提到的强大基础模型。为了克服这些限制，我们提出了基于简单高效缓冲回放策略的AnchorInv。AnchorInv不选择和存储原始数据，而是根据特征空间中的锚点生成合成样本。这种方法保护了隐私并促进了模型的适应。在三个公共生理时间序列数据集上进行评估时，AnchorInv表现出了有效的知识遗忘预防和对新类别的良好适应能力，超越了现有的最佳基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13714v1">PDF</a> AAAI-25 Extended Version</p>
<p><strong>Summary</strong></p>
<p>深度学习模型在多种实际应用中展现出卓越性能，这归功于能够在有限数据支持下泛化到新任务并保留先验知识的强大基础模型。然而，这些成果建立在大量高质量数据可用性的基础上，这在专门的生物医学应用中往往缺乏。在这些领域，模型通常使用有限且陆续到来的新类别数据进行开发，这要求模型在适应新信息的同时保留现有知识。针对这些挑战，Few-Shot Class-Incremental Learning（FSCIL）方法提供了一个有前景的解决方案，但它们也依赖于面临相同限制的强大基础模型。为了克服这些制约因素，提出了AnchorInv方法，采用简单高效的缓冲区回放策略。AnchorInv通过特征空间中的锚点生成合成样本，而不是选择和存储原始数据，这种方法保护了隐私并促进了模型的适应性调整。在三个公共生理时间序列数据集上的评估表明，AnchorInv有效地防止了知识遗忘，并提高了对新类别的适应能力，超越了最新的基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习模型在多种应用中表现出卓越性能，归功于强大的基础模型。</li>
<li>在生物医学等特定领域，数据有限且陆续到来，要求模型能适应新信息并保留现有知识。</li>
<li>Few-Shot Class-Incremental Learning（FSCIL）方法用于解决此挑战。</li>
<li>AnchorInv方法采用缓冲区回放策略，通过特征空间中的锚点生成合成样本，以保护隐私并促进模型适应性调整。</li>
<li>AnchorInv在生理时间序列数据集上的评估表现优越，有效防止知识遗忘，提高对新类别的适应能力。</li>
<li>AnchorInv方法依赖于强大基础模型，但仍存在数据限制的问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1f6779e8c9baf5cefc545dd99cae3e41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12654225ce6a14df6c590759f90b6af3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de54d3694115c842cb0d2395abebeb69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6d5d29141579371abf5270e291ab5c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cffc6247f383099fd42d2b7fe266e40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa0ee4e1e9f9c2c4546e47a429f76ee3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d12735a7ef970b7c504f3f383a666503.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Fine-Tuning-of-Single-Cell-Foundation-Models-Enables-Zero-Shot-Molecular-Perturbation-Prediction"><a href="#Efficient-Fine-Tuning-of-Single-Cell-Foundation-Models-Enables-Zero-Shot-Molecular-Perturbation-Prediction" class="headerlink" title="Efficient Fine-Tuning of Single-Cell Foundation Models Enables Zero-Shot   Molecular Perturbation Prediction"></a>Efficient Fine-Tuning of Single-Cell Foundation Models Enables Zero-Shot   Molecular Perturbation Prediction</h2><p><strong>Authors:Sepideh Maleki, Jan-Christian Huetter, Kangway V. Chuang, Gabriele Scalia, Tommaso Biancalani</strong></p>
<p>Predicting transcriptional responses to novel drugs provides a unique opportunity to accelerate biomedical research and advance drug discovery efforts. However, the inherent complexity and high dimensionality of cellular responses, combined with the extremely limited available experimental data, makes the task challenging. In this study, we leverage single-cell foundation models (FMs) pre-trained on tens of millions of single cells, encompassing multiple cell types, states, and disease annotations, to address molecular perturbation prediction. We introduce a drug-conditional adapter that allows efficient fine-tuning by training less than 1% of the original foundation model, thus enabling molecular conditioning while preserving the rich biological representation learned during pre-training. The proposed strategy allows not only the prediction of cellular responses to novel drugs, but also the zero-shot generalization to unseen cell lines. We establish a robust evaluation framework to assess model performance across different generalization tasks, demonstrating state-of-the-art results across all settings, with significant improvements in the few-shot and zero-shot generalization to new cell lines compared to existing baselines. </p>
<blockquote>
<p>预测新型药物的转录反应为加速生物医学研究和推进药物发现努力提供了独特的机会。然而，细胞反应的固有复杂性和高维性，以及可用的实验数据极为有限，使得这一任务具有挑战性。在这项研究中，我们利用基于单细胞的预训练模型（FMs），该模型基于数千万个单细胞进行预训练，涵盖多种细胞类型、状态和疾病注释，来解决分子扰动预测问题。我们引入了一种药物条件适配器，通过训练不到原始预训练模型的1%，实现了高效的微调，从而在保持预训练期间学习的丰富生物学表征的同时，实现了分子调节。所提出的策略不仅允许预测新型药物的细胞反应，而且可以在未见过的细胞系中实现零样本泛化。我们建立了一个稳健的评估框架，以评估模型在不同泛化任务上的性能，在所有设置中都取得了最先进的结果，与现有基线相比，在新细胞系的少样本和零样本泛化方面取得了显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预测新型药物的转录反应为加速生物医学研究和推动药物发现提供了独特的机会。本研究利用基于单细胞预训练模型应对分子扰动预测的挑战，通过引入药物条件适配器实现高效微调，只需训练原预训练模型的不到百分之一，从而在保留预训练丰富生物学表征的同时实现分子条件化。该策略不仅可预测新型药物的细胞反应，还可实现零样本泛化至未见过的细胞系。我们建立了稳健的评估框架以评估模型在不同泛化任务中的性能，显示在各种设置中均达到最新结果，与新细胞系的零样本和少样本泛化方面较现有基线有显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用单细胞预训练模型预测新型药物的转录反应，加速生物医学研究和药物发现。</li>
<li>引入药物条件适配器，实现高效微调并保留预训练的丰富生物学表征。</li>
<li>该策略不仅能预测新型药物的细胞反应，还能实现零样本泛化至未见过的细胞系。</li>
<li>建立稳健的评估框架以评估模型性能。</li>
<li>模型在各种设置下均表现出最新结果。</li>
<li>与现有基线相比，在新细胞系的零样本和少样本泛化方面有明显改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13478">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0a4a0fe20de97e4dae88efc59f2dd545.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb5b73f82061e4f6cf9c4bf6100b1ffc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-280dc160f4b379de2046d44cb36c23e7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="An-Agentic-Approach-to-Automatic-Creation-of-P-ID-Diagrams-from-Natural-Language-Descriptions"><a href="#An-Agentic-Approach-to-Automatic-Creation-of-P-ID-Diagrams-from-Natural-Language-Descriptions" class="headerlink" title="An Agentic Approach to Automatic Creation of P&amp;ID Diagrams from Natural   Language Descriptions"></a>An Agentic Approach to Automatic Creation of P&amp;ID Diagrams from Natural   Language Descriptions</h2><p><strong>Authors:Shreeyash Gowaikar, Srinivasan Iyengar, Sameer Segal, Shivkumar Kalyanaraman</strong></p>
<p>The Piping and Instrumentation Diagrams (P&amp;IDs) are foundational to the design, construction, and operation of workflows in the engineering and process industries. However, their manual creation is often labor-intensive, error-prone, and lacks robust mechanisms for error detection and correction. While recent advancements in Generative AI, particularly Large Language Models (LLMs) and Vision-Language Models (VLMs), have demonstrated significant potential across various domains, their application in automating generation of engineering workflows remains underexplored. In this work, we introduce a novel copilot for automating the generation of P&amp;IDs from natural language descriptions. Leveraging a multi-step agentic workflow, our copilot provides a structured and iterative approach to diagram creation directly from Natural Language prompts. We demonstrate the feasibility of the generation process by evaluating the soundness and completeness of the workflow, and show improved results compared to vanilla zero-shot and few-shot generation approaches. </p>
<blockquote>
<p>管道与仪表图（P&amp;IDs）是工程和工艺行业工作流程设计、构建和运营的基础。然而，其手动创建往往劳动强度大、易出错，且缺乏稳健的误差检测和校正机制。虽然最近生成式人工智能的进展，特别是大型语言模型（LLMs）和视觉语言模型（VLMs）已经在各个领域展现出显著潜力，但它们在自动化生成工程工作流程方面的应用仍然被探索得不够深入。在这项工作中，我们引入了一种新型助手，用于根据自然语言描述自动生成管道与仪表图。通过利用多步骤代理工作流程，我们的助手提供了结构化、迭代化的方法，直接从自然语言提示生成图表。我们通过评估工作流程的健全性和完整性来验证生成过程的可行性，并展示了相较于普通零样本和少样本生成方法，我们的方法具有更好的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12898v1">PDF</a> Accepted at the AAAI’25 Workshop on AI to Accelerate Science and   Engineering (AI2ASE)</p>
<p><strong>Summary</strong></p>
<p>工程流程图和仪器流程图（P&amp;IDs）是工程和工艺行业工作流程设计、构建和运营的基础。然而，其手动创建往往劳动强度大、易出错，且缺乏可靠的错误检测和纠正机制。最近，生成式人工智能（尤其是大型语言模型和视觉语言模型）在各领域展现出巨大潜力，但在自动化生成工程工作流程方面的应用仍被忽视。本研究引入了一种新型P&amp;IDs自动生成助手，它能直接从自然语言描述中生成流程图。借助多步骤的代理工作流程，该助手提供结构化和迭代式的图表创建方法。通过评估工作流程的健全性和完整性，展示了生成过程的可行性，并表现出相较于零样本和少样本生成方法的改进结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>P&amp;IDs在工程和工艺行业中至关重要，但其手动创建过程劳动强度大、易出错。</li>
<li>生成式人工智能，特别是大型语言模型和视觉语言模型，在自动化生成工程工作流程方面的应用潜力巨大，但相关研究仍有限。</li>
<li>研究提出了一种新型的P&amp;IDs自动生成助手，能直接从自然语言描述中生成流程图。</li>
<li>该助手采用多步骤的代理工作流程，提供结构化和迭代式的图表创建方法。</li>
<li>该自动生成助手的可行性通过评估工作流程的健全性和完整性得以验证。</li>
<li>与零样本和少样本生成方法相比，该助手展现出明显的改进。</li>
<li>这一研究为工程和工艺行业的自动化和智能化发展开辟了新的道路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12898">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-009e6b6d1a78b3704998a50af84e3715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be680d39e5abf37802c1b4facffb31cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e13cd02d350c010a8f3b31abef93997a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d7e3ef8c1b5ddbcb01598af390fcffb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2651ce9b407106e31d9045d602d8af7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Question-How-do-Large-Language-Models-perform-on-the-Question-Answering-tasks-Answer"><a href="#Question-How-do-Large-Language-Models-perform-on-the-Question-Answering-tasks-Answer" class="headerlink" title="Question: How do Large Language Models perform on the Question Answering   tasks? Answer:"></a>Question: How do Large Language Models perform on the Question Answering   tasks? Answer:</h2><p><strong>Authors:Kevin Fischer, Darren Fürst, Sebastian Steindl, Jakob Lindner, Ulrich Schäfer</strong></p>
<p>Large Language Models (LLMs) have been showing promising results for various NLP-tasks without the explicit need to be trained for these tasks by using few-shot or zero-shot prompting techniques. A common NLP-task is question-answering (QA). In this study, we propose a comprehensive performance comparison between smaller fine-tuned models and out-of-the-box instruction-following LLMs on the Stanford Question Answering Dataset 2.0 (SQuAD2), specifically when using a single-inference prompting technique. Since the dataset contains unanswerable questions, previous work used a double inference method. We propose a prompting style which aims to elicit the same ability without the need for double inference, saving compute time and resources. Furthermore, we investigate their generalization capabilities by comparing their performance on similar but different QA datasets, without fine-tuning neither model, emulating real-world uses where the context and questions asked may differ from the original training distribution, for example swapping Wikipedia for news articles.   Our results show that smaller, fine-tuned models outperform current State-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are able to close this gap on the out-of-distribution test and even outperform the fine-tuned models on 3 of the 5 tested QA datasets. </p>
<blockquote>
<p>大型语言模型（LLM）在多种NLP任务中展现出令人瞩目的结果，这些任务无需通过少量样本或零样本提示技术进行特定训练。一个常见的NLP任务是问答（QA）。在这项研究中，我们对经过微调的小型模型与即插即用的指令遵循型LLM在Stanford Question Answering Dataset 2.0（SQuAD2）上的综合性能进行了全面比较，特别是当使用单一推理提示技术时。由于该数据集包含无法回答的问题，之前的工作使用了双重推理方法。我们提出了一种提示风格，旨在在不使用双重推理的情况下激发相同的能力，从而节省计算时间和资源。此外，我们通过比较他们在类似但不同的QA数据集上的性能来调查他们的泛化能力，且无需对任何模型进行微调，以模拟现实世界的使用情况，其中上下文和所提问题可能与原始训练分布有所不同，例如将Wikipedia替换为新闻文章。我们的结果表明，经过微调的小型模型在当前先进的大型语言模型在微调任务上的表现更出色，但最近先进的大型语言模型能够在超出分布测试上缩小这一差距，甚至在五个测试问答数据集的三个上表现超过经过微调的小型模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12893v1">PDF</a> Accepted at SAI Computing Conference 2025</p>
<p><strong>Summary</strong></p>
<p>LLM在问答任务上的性能表现全面比较。研究发现在特定任务上，精细调整的小型模型表现优于当前先进的LLM，但在脱离分布测试的环境下，先进的LLM能够缩小差距，甚至在三个测试中的五个问答数据集上表现优于精细调整模型。提出新的提示风格以简化推理过程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在多种NLP任务中表现出色，可通过少样本或零样本提示技术无需特定训练即可完成。</li>
<li>在Stanford Question Answering Dataset 2.0（SQuAD2）上进行了小型精细调整模型与即插即用型LLM的性能比较。</li>
<li>提出了一种新的提示风格，旨在无需双重推理即可引出相同能力，从而节省计算时间和资源。</li>
<li>分析了LLM在类似但不同的问答数据集上的泛化能力，模拟了真实世界的使用情况，其中上下文和所提问题可能与原始训练分布不同。</li>
<li>精细调整的小型模型在特定任务上表现优于当前先进的LLM。</li>
<li>在脱离分布测试的环境下，先进的LLM能够缩小与精细调整模型的性能差距，甚至在多个数据集上表现更优。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12893">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fa696a3dabcc1d66005d4def7cc8d474.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-104eec9a81136b6cb215064dbde2aa7b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Selective-Shot-Learning-for-Code-Explanation"><a href="#Selective-Shot-Learning-for-Code-Explanation" class="headerlink" title="Selective Shot Learning for Code Explanation"></a>Selective Shot Learning for Code Explanation</h2><p><strong>Authors:Paheli Bhattacharya, Rishabh Gupta</strong></p>
<p>Code explanation plays a crucial role in the software engineering domain, aiding developers in grasping code functionality efficiently. Recent work shows that the performance of LLMs for code explanation improves in a few-shot setting, especially when the few-shot examples are selected intelligently. State-of-the-art approaches for such Selective Shot Learning (SSL) include token-based and embedding-based methods. However, these SSL approaches have been evaluated on proprietary LLMs, without much exploration on open-source Code-LLMs. Additionally, these methods lack consideration for programming language syntax. To bridge these gaps, we present a comparative study and propose a novel SSL method (SSL_ner) that utilizes entity information for few-shot example selection. We present several insights and show the effectiveness of SSL_ner approach over state-of-the-art methods across two datasets. To the best of our knowledge, this is the first systematic benchmarking of open-source Code-LLMs while assessing the performances of the various few-shot examples selection approaches for the code explanation task. </p>
<blockquote>
<p>代码解释在软件工程领域扮演着至关重要的角色，它帮助开发者高效地掌握代码功能。最近的研究表明，大型语言模型（LLMs）在少量样本场景下的代码解释性能有所提升，尤其是当选择的少量样本是明智的时候。针对此类选择性射击学习（SSL）的最新方法包括基于令牌和基于嵌入的方法。然而，这些SSL方法主要是在专有的大型语言模型上进行了评估，而对开源代码的大型语言模型（Code-LLMs）的探索并不多。此外，这些方法没有考虑到编程语言的语法。为了弥补这些差距，我们进行了一项比较研究，并提出了一种新的SSL方法（SSL_ner），该方法利用实体信息进行少量样本选择。我们获得了一些见解，并展示了SSL_ner方法在两个数据集上的性能优于最新方法。据我们所知，这是在对各种少量样本选择方法进行性能评估的同时，首次对开源代码的大型语言模型进行系统的基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12852v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于软件工程领域中的代码解释的重要性，LLMs（大型预训练语言模型）在少数样本下的表现提升受到关注。近期工作表明，合理选择少数样本对于提升LLMs在代码解释任务中的性能尤为关键。本研究针对开源Code-LLMs进行系统性评估，并提出一种利用实体信息的新型Selective Shot Learning（SSL_ner）方法。该方法在考虑编程语言的语法结构基础上，实现了对SSL方法的改进。研究提供了丰富的见解，并在两个数据集上验证了SSL_ner方法的有效性。这是首个针对开源Code-LLMs评估不同少数样本选择方法在代码解释任务上的表现的基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码解释在软件工程中扮演着至关重要的角色，有助于开发者高效理解代码功能。</li>
<li>LLMs在少数样本下的性能提升受到关注，尤其是合理选择少数样本的重要性。</li>
<li>现有的SSL方法在评估中主要针对私有LLMs，而较少探索开源Code-LLMs。</li>
<li>SSL方法在考虑编程语言的语法结构上存在不足。</li>
<li>研究提出了一种新的SSL方法——SSL_ner，该方法利用实体信息进行少数样本选择。</li>
<li>研究在多个数据集上验证了SSL_ner方法的有效性，表明其相较于现有方法的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12852">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4d74734e7284d67d321af1f3113e9ec8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-473358974a34046089cefd28c8927df7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55aae6885c8a0a8a7a2e0575ccd05ecf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc0d168bcb0916bc19d81e3e8d2aa36e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b2053b921984d833f6d3e3e3cd4d8e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88d45e2a660a844ee4260309ce32dd10.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FocusChat-Text-guided-Long-Video-Understanding-via-Spatiotemporal-Information-Filtering"><a href="#FocusChat-Text-guided-Long-Video-Understanding-via-Spatiotemporal-Information-Filtering" class="headerlink" title="FocusChat: Text-guided Long Video Understanding via Spatiotemporal   Information Filtering"></a>FocusChat: Text-guided Long Video Understanding via Spatiotemporal   Information Filtering</h2><p><strong>Authors:Zheng Cheng, Rendong Wang, Zhicheng Wang</strong></p>
<p>Recently, multi-modal large language models have made significant progress. However, visual information lacking of guidance from the user’s intention may lead to redundant computation and involve unnecessary visual noise, especially in long, untrimmed videos. To address this issue, we propose FocusChat, a text-guided multi-modal large language model (LLM) that emphasizes visual information correlated to the user’s prompt. In detail, Our model first undergoes the semantic extraction module, which comprises a visual semantic branch and a text semantic branch to extract image and text semantics, respectively. The two branches are combined using the Spatial-Temporal Filtering Module (STFM). STFM enables explicit spatial-level information filtering and implicit temporal-level feature filtering, ensuring that the visual tokens are closely aligned with the user’s query. It lowers the essential number of visual tokens inputted into the LLM. FocusChat significantly outperforms Video-LLaMA in zero-shot experiments, using an order of magnitude less training data with only 16 visual tokens occupied. It achieves results comparable to the state-of-the-art in few-shot experiments, with only 0.72M pre-training data. </p>
<blockquote>
<p>近期，多模态大型语言模型取得了显著进展。然而，缺乏用户意图指导的视觉信息可能导致冗余计算并引入不必要的视觉噪声，特别是在长且未修剪的视频中。为了解决这个问题，我们提出了FocusChat，这是一个文本引导的多模态大型语言模型（LLM），它强调与用户提示相关的视觉信息。具体来说，我们的模型首先经过语义提取模块，该模块包括视觉语义分支和文本语义分支，分别提取图像和文本语义。这两个分支通过时空滤波模块（STFM）进行组合。STFM实现了显式的空间级信息滤波和隐式的时间级特征滤波，确保视觉令牌与用户查询紧密对齐。它降低了输入到LLM中的必要视觉令牌数量。FocusChat在零样本实验中显著优于Video-LLaMA，使用数量级更少的训练数据，仅占用16个视觉令牌。在少量样本实验中，它达到了与国家最新技术相当的结果，仅有0.72M的预训练数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12833v1">PDF</a> 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>文本提出了一种名为FocusChat的文本引导式多模态大型语言模型，该模型强调与用户提示相关的视觉信息。它通过语义提取模块和空间时间过滤模块，降低了输入到语言模型中的视觉标记数量，从而提高效率并减少冗余计算。FocusChat在零射击实验中的表现显著优于Video-LLaMA，使用的训练数据量减少了一个数量级，同时在少击实验中取得了与最新技术相当的结果，仅使用0.72M的预训练数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FocusChat是一个文本引导的多模态大型语言模型，旨在解决在视频处理中由于用户意图缺乏指导而导致的冗余计算和视觉噪音问题。</li>
<li>该模型通过语义提取模块和空间时间过滤模块来强调与用户提示相关的视觉信息。</li>
<li>FocusChat通过降低输入到语言模型中的视觉标记数量来提高效率。</li>
<li>FocusChat在零射击实验中的表现优于Video-LLaMA，使用的训练数据量大大减少。</li>
<li>该模型在少击实验中取得了与最新技术相当的结果。</li>
<li>FocusChat的预训练数据量仅为0.72M。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12833">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5e440689d57f774b6ceb66fccc97d9e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480aeb25e3e355af1748f85383e00656.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e9084bcc4258f9e6f2a2b19f21b9400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a1cd3d554e7b2b242a5ed7ab7187bad.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CRoF-CLIP-based-Robust-Few-shot-Learning-on-Noisy-Labels"><a href="#CRoF-CLIP-based-Robust-Few-shot-Learning-on-Noisy-Labels" class="headerlink" title="CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels"></a>CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels</h2><p><strong>Authors:Shizhuo Deng, Bowen Han, Jiaqi Chen, Hao Wang, Dongyue Chen, Tong Jia</strong></p>
<p>Noisy labels threaten the robustness of few-shot learning (FSL) due to the inexact features in a new domain. CLIP, a large-scale vision-language model, performs well in FSL on image-text embedding similarities, but it is susceptible to misclassification caused by noisy labels. How to enhance domain generalization of CLIP on noisy data within FSL tasks is a critical challenge. In this paper, we provide a novel view to mitigate the influence of noisy labels, CLIP-based Robust Few-shot learning (CRoF). CRoF is a general plug-in module for CLIP-based models. To avoid misclassification and confused label embedding, we design the few-shot task-oriented prompt generator to give more discriminative descriptions of each category. The proposed prompt achieves larger distances of inter-class textual embedding. Furthermore, rather than fully trusting zero-shot classification by CLIP, we fine-tune CLIP on noisy few-shot data in a new domain with a weighting strategy like label-smooth. The weights for multiple potentially correct labels consider the relationship between CLIP’s prior knowledge and original label information to ensure reliability. Our multiple label loss function further supports robust training under this paradigm. Comprehensive experiments show that CRoF, as a plug-in, outperforms fine-tuned and vanilla CLIP models on different noise types and noise ratios. </p>
<blockquote>
<p>带噪声的标签由于新领域的特征不准确而威胁少样本学习（FSL）的稳健性。CLIP是一种大规模的视觉语言模型，在图像文本嵌入相似性方面，FSL中表现良好，但它容易受到带噪声标签导致的误分类影响。如何在FSL任务中带噪声数据上增强CLIP的领域泛化是一个关键挑战。在本文中，我们提供了减轻带噪声标签影响的全新观点，即基于CLIP的鲁棒少样本学习（CRoF）。CRoF是一个适用于CLIP模型的通用插件模块。为了避免误分类和混淆标签嵌入，我们设计了面向少样本任务的提示生成器，为每个类别提供更具区分度的描述。所提出的提示实现了较大的类间文本嵌入距离。此外，我们不是完全信任CLIP的零样本分类，而是使用标签平滑等加权策略对新领域中的带噪声少样本数据进行微调。多个可能正确的标签的权重考虑了CLIP的先验知识和原始标签信息之间的关系，以确保可靠性。我们的多标签损失函数进一步支持此模式下的稳健训练。综合实验表明，作为插件的CRoF在不同噪声类型和噪声比率上优于经过精细调整和原始的CLIP模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12793v1">PDF</a> </p>
<p><strong>Summary</strong><br>    CLIP模型在少量样本学习（FSL）中因新域的不精确特征而受到噪声标签的威胁。本文提出了一种新型的CRoF模块来减少噪声标签的影响，它增强了CLIP模型的域泛化能力。CRoF是一种针对CLIP模型的通用插件模块，它通过设计面向任务的少量提示生成器来避免误分类和混淆标签嵌入。此外，CRoF还采用了一种加权策略对CLIP进行微调，以适应新域的噪声数据。通过综合实验证明，CRoF作为插件在不同噪声类型和噪声比率上均优于微调后的CLIP模型和原始CLIP模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP在少量样本学习（FSL）中易受噪声标签影响，导致性能下降。</li>
<li>CRoF作为一种新型插件模块，旨在提高CLIP模型在新域的泛化能力。</li>
<li>CRoF通过设计面向任务的少量提示生成器，减少误分类和混淆标签嵌入的问题。</li>
<li>CRoF采用加权策略对CLIP进行微调，以适应新域的噪声数据，提高模型的可靠性。</li>
<li>CRoF利用多重标签损失函数进行稳健训练。</li>
<li>综合实验表明，CRoF在多种噪声类型和比率下性能优于原始CLIP模型和微调后的CLIP模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12793">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae206aa32a9795d211ebc72fe46a3237.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-516b5ba9897b35ab1fcb44ce145e4f82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d67809d7d0a7b88e56e75bba1c02c5ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54aecf5daf027ba6d0c0c84fe0979d13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6805ce8784547a01100de0f51f51940d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0949bb5f2161c80881f270612e2bc64a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4251e14cf28fa612741195f3bdaf2185.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Adapting-Unsigned-Graph-Neural-Networks-for-Signed-Graphs-A-Few-Shot-Prompt-Tuning-Approach"><a href="#Adapting-Unsigned-Graph-Neural-Networks-for-Signed-Graphs-A-Few-Shot-Prompt-Tuning-Approach" class="headerlink" title="Adapting Unsigned Graph Neural Networks for Signed Graphs: A Few-Shot   Prompt Tuning Approach"></a>Adapting Unsigned Graph Neural Networks for Signed Graphs: A Few-Shot   Prompt Tuning Approach</h2><p><strong>Authors:Zian Zhai, Sima Qing, Xiaoyang Wang, Wenjie Zhang</strong></p>
<p>Signed Graph Neural Networks (SGNNs) are powerful tools for signed graph representation learning but struggle with limited generalization and heavy dependence on labeled data. While recent advancements in “graph pre-training and prompt tuning” have reduced label dependence in Graph Neural Networks (GNNs) and improved their generalization abilities by leveraging pre-training knowledge, these efforts have focused exclusively on unsigned graphs. The scarcity of publicly available signed graph datasets makes it essential to transfer knowledge from unsigned graphs to signed graph tasks. However, this transfer introduces significant challenges due to the graph-level and task-level divergences between the pre-training and downstream phases. To address these challenges, we propose Signed Graph Prompt Tuning (SGPT) in this paper. Specifically, SGPT employs a graph template and a semantic prompt to segregate mixed link semantics in the signed graph and then adaptively integrate the distinctive semantic information according to the needs of downstream tasks, thereby unifying the pre-training and downstream graphs. Additionally, SGPT utilizes a task template and a feature prompt to reformulate the downstream signed graph tasks, aligning them with pre-training tasks to ensure a unified optimization objective and consistent feature space across tasks. Finally, extensive experiments are conducted on popular signed graph datasets, demonstrating the superiority of SGPT over state-of-the-art methods. </p>
<blockquote>
<p>有符号图神经网络（SGNNs）是有符号图表示学习的强大工具，但在泛化有限和严重依赖标签数据方面存在挑战。虽然最近的“图预训练和提示调整”进展减少了图神经网络（GNNs）对标签的依赖，并利用预训练知识提高了其泛化能力，但这些努力都集中在无符号图上。有符号图数据集公开可用的稀缺性使得从无符号图向有符号图任务转移知识变得至关重要。然而，由于预训练和下流阶段之间的图级别和任务级别差异，这种转移引入了重大挑战。为了应对这些挑战，我们在本文中提出了有符号图提示调整（SGPT）。具体来说，SGPT采用图模板和语义提示来分离有符号图中的混合链接语义，然后根据下游任务的需求自适应地集成不同的语义信息，从而统一预训练和下流图。此外，SGPT利用任务模板和特征提示来重新制定下游有符号图任务，使它们与预训练任务对齐，以确保统一优化目标和任务之间一致的特征空间。最后，在流行的有符号图数据集上进行了大量实验，证明了SGPT优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12155v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SGNN的强大工具用于有符号图表示学习，但存在泛化受限和依赖大量标签数据的问题。最近图预训练和提示调整技术的进步减少了图神经网络对标签的依赖，利用预训练知识提高了泛化能力，但这些努力主要集中在无符号图上。有符号图数据集的稀缺性使得从无符号图向有符号图任务转移知识变得至关重要，但由于预训练和下游阶段的图级别和任务级别的差异，这种转移引入了重大挑战。本文提出了有符号图提示调整（SGPT）来解决这些挑战。SGPT使用图模板和语义提示来分离混合链接语义，并根据下游任务的需要自适应地集成不同的语义信息，从而统一预训练和下游图。此外，SGPT还使用任务模板和特征提示来重新制定下游有符号图任务，使其与预训练任务对齐，确保统一的优化目标和跨任务的一致特征空间。在流行的有符号图数据集上进行了广泛实验，证明SGPT优于最新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Signed Graph Neural Networks (SGNNs) 面临泛化有限和依赖大量标签数据的问题。</li>
<li>最近图预训练和提示调整技术的进步已应用于无符号图上，以提高泛化能力和减少标签依赖。</li>
<li>有符号图数据集的稀缺性导致需要从无符号图向有符号图任务转移知识。</li>
<li>这种转移面临预训练和下游阶段的图级别和任务级别的差异带来的挑战。</li>
<li>Signed Graph Prompt Tuning (SGPT) 被提出来解决这些挑战，通过图模板和语义提示来分离混合链接语义并集成不同的语义信息。</li>
<li>SGPT还使用任务模板和特征提示来重新制定下游有符号图任务，确保统一的优化目标和一致的特征空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12155">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6a7b258e2313a3191cf6a50a8f997472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c9f488b25507d6f88c8c1b99b99f4e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b742996795bc271dbc1edadf87014b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f90ed6dcde7b94e16e0060c821265e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-214f9fe2a46d7fb18f70ac9480b1da35.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Meta-Controller-Few-Shot-Imitation-of-Unseen-Embodiments-and-Tasks-in-Continuous-Control"><a href="#Meta-Controller-Few-Shot-Imitation-of-Unseen-Embodiments-and-Tasks-in-Continuous-Control" class="headerlink" title="Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in   Continuous Control"></a>Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in   Continuous Control</h2><p><strong>Authors:Seongwoong Cho, Donggyun Kim, Jinwoo Lee, Seunghoon Hong</strong></p>
<p>Generalizing across robot embodiments and tasks is crucial for adaptive robotic systems. Modular policy learning approaches adapt to new embodiments but are limited to specific tasks, while few-shot imitation learning (IL) approaches often focus on a single embodiment. In this paper, we introduce a few-shot behavior cloning framework to simultaneously generalize to unseen embodiments and tasks using a few (\emph{e.g.,} five) reward-free demonstrations. Our framework leverages a joint-level input-output representation to unify the state and action spaces of heterogeneous embodiments and employs a novel structure-motion state encoder that is parameterized to capture both shared knowledge across all embodiments and embodiment-specific knowledge. A matching-based policy network then predicts actions from a few demonstrations, producing an adaptive policy that is robust to over-fitting. Evaluated in the DeepMind Control suite, our framework termed \modelname{} demonstrates superior few-shot generalization to unseen embodiments and tasks over modular policy learning and few-shot IL approaches. Codes are available at \href{<a target="_blank" rel="noopener" href="https://github.com/SeongwoongCho/meta-controller%7D%7Bhttps://github.com/SeongwoongCho/meta-controller%7D">https://github.com/SeongwoongCho/meta-controller}{https://github.com/SeongwoongCho/meta-controller}</a>. </p>
<blockquote>
<p>对于自适应机器人系统来说，跨机器人实体和任务的一般化至关重要。模块化策略学习方法能够适应新的实体，但仅限于特定任务，而少样本模仿学习（IL）方法通常侧重于单一实体。在本文中，我们引入了一种少样本行为克隆框架，该框架能够利用少量（例如五个）无奖励示范来同时推广到未见过的实体和任务。我们的框架利用关节级输入输出表示来统一异质实体的状态和行为空间，并采用新型结构运动状态编码器，该编码器经过参数化设置，能够捕捉所有实体之间的共享知识以及针对实体的特定知识。然后，基于匹配的策略网络从少数示范中预测行为，产生能够适应过度拟合的稳健策略。在DeepMind Control套件中进行评估，我们的框架（称为\modelname）在未见过的实体和任务上表现出优于模块化策略学习和少样本IL方法的少样本泛化能力。代码可在<a target="_blank" rel="noopener" href="https://github.com/SeongwoongCho/meta-controller">https://github.com/SeongwoongCho/meta-controller</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12147v1">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于行为克隆的框架，能够在少量奖励无关演示的基础上，同时推广到未见过的机器人实体和任务。该框架利用关节级输入输出表示来统一不同实体的状态和行为空间，并采用新型结构运动状态编码器捕捉跨实体的共享知识和特定实体的知识。匹配策略网络根据少量演示预测行为，产生适应性策略，减少过度拟合。在DeepMind控制套件中评估，该框架展现出出色的未见实体和任务的少样本泛化能力，优于模块化政策学习和少样本模仿学习的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于行为克隆的框架，可以推广到未见过的机器人实体和任务。</li>
<li>利用关节级输入输出表示来统一不同实体的状态和行为空间。</li>
<li>采用新型结构运动状态编码器捕捉跨实体的共享知识和特定实体的知识。</li>
<li>通过匹配策略网络预测行为，产生适应性策略，减少过度拟合。</li>
<li>在DeepMind控制套件中评估，展现出出色的未见实体和任务的少样本泛化能力。</li>
<li>该框架优于模块化政策学习和少样本模仿学习的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12147">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-95103570d0d65736f1b64bd1f9438267.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce28d52bb8a50d23bfc49dd1aa69e5d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8aa2396b6cb98c127ff4670be3844c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11c9c31165b30ee71d37e47c49e445d1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Text-and-Image-Are-Mutually-Beneficial-Enhancing-Training-Free-Few-Shot-Classification-with-CLIP"><a href="#Text-and-Image-Are-Mutually-Beneficial-Enhancing-Training-Free-Few-Shot-Classification-with-CLIP" class="headerlink" title="Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot   Classification with CLIP"></a>Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot   Classification with CLIP</h2><p><strong>Authors:Yayuan Li, Jintao Guo, Lei Qi, Wenbin Li, Yinghuan Shi</strong></p>
<p>Contrastive Language-Image Pretraining (CLIP) has been widely used in vision tasks. Notably, CLIP has demonstrated promising performance in few-shot learning (FSL). However, existing CLIP-based methods in training-free FSL (i.e., without the requirement of additional training) mainly learn different modalities independently, leading to two essential issues: 1) severe anomalous match in image modality; 2) varying quality of generated text prompts. To address these issues, we build a mutual guidance mechanism, that introduces an Image-Guided-Text (IGT) component to rectify varying quality of text prompts through image representations, and a Text-Guided-Image (TGI) component to mitigate the anomalous match of image modality through text representations. By integrating IGT and TGI, we adopt a perspective of Text-Image Mutual guidance Optimization, proposing TIMO. Extensive experiments show that TIMO significantly outperforms the state-of-the-art (SOTA) training-free method. Additionally, by exploring the extent of mutual guidance, we propose an enhanced variant, TIMO-S, which even surpasses the best training-required methods by 0.33% with approximately 100 times less time cost. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/lyymuwu/TIMO">https://github.com/lyymuwu/TIMO</a>. </p>
<blockquote>
<p>对比语言图像预训练（CLIP）已在视觉任务中得到广泛应用。特别是，CLIP在少样本学习（FSL）中表现出了有前景的性能。然而，在无需额外训练的少样本学习（Free-training Few-Shot Learning, FT-FSL）中，现有的基于CLIP的方法主要独立学习不同的模态，导致两个关键问题：1）图像模态中存在严重的异常匹配；2）生成的文本提示质量不一。为了解决这些问题，我们建立了一种相互引导机制，该机制引入了一个图像引导文本（IGT）组件，通过图像表示来校正文本提示的质量不一问题，以及一个文本引导图像（TGI）组件，通过文本表示来缓解图像模态的异常匹配问题。通过整合IGT和TGI，我们从文本图像相互引导优化的角度入手，提出了TIMO。大量实验表明，TIMO显著优于最新的无需训练的方法。此外，通过探索相互引导的程度，我们提出了增强型TIMO-S，其甚至以约100倍的时间成本超越了最佳需要训练的方法，准确率提高了0.33%。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/lyymuwu/TIMO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lyymuwu/TIMO找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11375v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>基于对比语言图像预训练（CLIP）的技术在视觉任务中广泛应用，特别是在小样学习（FSL）中表现优异。然而，现有的CLIP在零训练小样学习（无额外训练需求）的方法主要独立学习不同模态，导致图像模态的异常匹配和生成文本提示的质量不一。为解决这些问题，本文提出了文本图像相互引导优化（TIMO）的方法，通过图像引导文本（IGT）组件纠正文本提示的质量，以及文本引导图像（TGI）组件减轻图像模态的异常匹配。实验显示，TIMO显著优于现有零训练方法，其增强版TIMO-S甚至超越了最佳需训练方法的性能，同时时间成本大幅降低。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP技术在视觉任务和小样学习中表现优异。</li>
<li>现有CLIP在零训练小样学习中存在图像模态异常匹配和文本提示质量不一的问题。</li>
<li>TIMO通过文本图像相互引导机制解决上述问题。</li>
<li>IGT组件用于纠正文本提示的质量，TGI组件减轻图像模态的异常匹配。</li>
<li>TIMO显著优于现有零训练方法。</li>
<li>TIMO的增强版TIMO-S在时间成本大幅降低的同时，性能超越最佳需训练方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11375">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7404aaa6248977ee707d7e09d45d9dcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d3b4683ab5343196b2b3b6315d020a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a18a74487ca58be48aefc640927e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24d004e40c9a3694a95bbf969bf36ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9a096878db179f4a39019eb30b179ae.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="On-Distilling-the-Displacement-Knowledge-for-Few-Shot-Class-Incremental-Learning"><a href="#On-Distilling-the-Displacement-Knowledge-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="On Distilling the Displacement Knowledge for Few-Shot Class-Incremental   Learning"></a>On Distilling the Displacement Knowledge for Few-Shot Class-Incremental   Learning</h2><p><strong>Authors:Pengfei Fang, Yongchun Qin, Hui Xue</strong></p>
<p>Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of evolving data distributions and the difficulty of data acquisition in real-world scenarios. To counteract the catastrophic forgetting typically encountered in FSCIL, knowledge distillation is employed as a way to maintain the knowledge from learned data distribution. Recognizing the limitations of generating discriminative feature representations in a few-shot context, our approach incorporates structural information between samples into knowledge distillation. This structural information serves as a remedy for the low quality of features. Diverging from traditional structured distillation methods that compute sample similarity, we introduce the Displacement Knowledge Distillation (DKD) method. DKD utilizes displacement rather than similarity between samples, incorporating both distance and angular information to significantly enhance the information density retained through knowledge distillation. Observing performance disparities in feature distribution between base and novel classes, we propose the Dual Distillation Network (DDNet). This network applies traditional knowledge distillation to base classes and DKD to novel classes, challenging the conventional integration of novel classes with base classes. Additionally, we implement an instance-aware sample selector during inference to dynamically adjust dual branch weights, thereby leveraging the complementary strengths of each approach. Extensive testing on three benchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover, through rigorous experimentation and comparison, we establish the robustness and general applicability of our proposed DKD method. </p>
<blockquote>
<p>少量样本类增量学习（FSCIL）解决了数据分布演变和现实世界场景中数据获取困难的问题。为了抵消FSCIL中通常遇到的灾难性遗忘，采用知识蒸馏作为一种保持已学习数据分布知识的方法。我们的方法认识到在少量样本情况下生成判别特征表示的局限性，将样本之间的结构信息纳入知识蒸馏中。这种结构信息作为对特征质量低的补救措施。我们引入了位移知识蒸馏（DKD）方法，该方法利用样本之间的位移而不是相似性，结合距离和角度信息，通过知识蒸馏显著增强保留的信息密度。观察到基础类和新颖类之间特征分布的性能差异，我们提出了双蒸馏网络（DDNet）。该网络对基础类应用传统知识蒸馏，对新颖类应用DKD，挑战了新颖类与基础类的传统融合方式。此外，我们在推理过程中实现了实例感知样本选择器，以动态调整双分支权重，从而充分利用每种方法的互补优势。在三个基准测试上的广泛测试表明，DDNet达到了最新技术水平。而且，通过严格的实验和比较，我们确定了我们所提出的DKD方法的稳健性和通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11017v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Few-shot类增量学习（FSCIL）面临的挑战，包括数据分布的变化和真实场景中数据获取的困难。为解决灾难性遗忘问题，采用知识蒸馏技术保留已学习数据分布的知识。针对少样本情境下生成判别特征表示的限制，结合样本间的结构信息改进知识蒸馏。引入位移知识蒸馏（DKD）方法，利用样本间的位移而非相似性，结合距离和角度信息，显著提高通过知识蒸馏保留的信息密度。为解决基础类和新型类在特征分布上的性能差异，提出双蒸馏网络（DDNet），该网络对基础类应用传统知识蒸馏，对新型类应用DKD，并动态调整双分支权重。在三个基准测试上的测试表明DDNet取得了最新成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot类增量学习（FSCIL）面临数据分布变化和真实场景数据获取困难等挑战。</li>
<li>知识蒸馏技术用于解决FSCIL中的灾难性遗忘问题，保留已学习数据分布的知识。</li>
<li>引入位移知识蒸馏（DKD）方法，结合距离和角度信息，提高信息密度。</li>
<li>双蒸馏网络（DDNet）针对基础类和新型类在特征分布上的性能差异，实施不同的蒸馏策略。</li>
<li>DDNet通过在动态调整双分支权重的方法中融合传统知识蒸馏与DKD的优势。</li>
<li>实验表明DDNet在三个基准测试上取得了最新成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11017">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b01e4d4fe75e49050f94c8a51a0675b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-502d642dec3c97f567abdfcdc3b91a8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b6fbb289370db809316603c24c6f7e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14fe9d721ecf53ca7dea6abe7a34a0bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a3ff24f3791ee44bad5bb32a18d63b8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="HEP-NAS-Towards-Efficient-Few-shot-Neural-Architecture-Search-via-Hierarchical-Edge-Partitioning"><a href="#HEP-NAS-Towards-Efficient-Few-shot-Neural-Architecture-Search-via-Hierarchical-Edge-Partitioning" class="headerlink" title="HEP-NAS: Towards Efficient Few-shot Neural Architecture Search via   Hierarchical Edge Partitioning"></a>HEP-NAS: Towards Efficient Few-shot Neural Architecture Search via   Hierarchical Edge Partitioning</h2><p><strong>Authors:Jianfeng Li, Jiawen Zhang, Feng Wang, Lianbo Ma</strong></p>
<p>One-shot methods have significantly advanced the field of neural architecture search (NAS) by adopting weight-sharing strategy to reduce search costs. However, the accuracy of performance estimation can be compromised by co-adaptation. Few-shot methods divide the entire supernet into individual sub-supernets by splitting edge by edge to alleviate this issue, yet neglect relationships among edges and result in performance degradation on huge search space. In this paper, we introduce HEP-NAS, a hierarchy-wise partition algorithm designed to further enhance accuracy. To begin with, HEP-NAS treats edges sharing the same end node as a hierarchy, permuting and splitting edges within the same hierarchy to directly search for the optimal operation combination for each intermediate node. This approach aligns more closely with the ultimate goal of NAS. Furthermore, HEP-NAS selects the most promising sub-supernet after each segmentation, progressively narrowing the search space in which the optimal architecture may exist. To improve performance evaluation of sub-supernets, HEP-NAS employs search space mutual distillation, stabilizing the training process and accelerating the convergence of each individual sub-supernet. Within a given budget, HEP-NAS enables the splitting of all edges and gradually searches for architectures with higher accuracy. Experimental results across various datasets and search spaces demonstrate the superiority of HEP-NAS compared to state-of-the-art methods. </p>
<blockquote>
<p>一shot方法通过采用权重共享策略来降低搜索成本，从而显著推动了神经网络架构搜索（NAS）领域的发展。然而，协同适应可能会损害性能估计的准确性。Few-shot方法通过逐边分割将整个超网分成单个的子超网来缓解这个问题，但却忽视了边之间的关系，并在巨大的搜索空间上导致性能下降。在本文中，我们介绍了HEP-NAS，这是一种层次划分算法，旨在进一步提高准确性。首先，HEP-NAS将共享相同末端节点的边视为一个层次结构，对同一层次内的边进行排列和分割，以直接搜索每个中间节点的最佳操作组合。这种方法更贴近NAS的最终目标。此外，HEP-NAS会在每次分割后选择最有希望的子超网，逐步缩小可能存在最优架构的搜索空间。为了改进子超网的性能评估，HEP-NAS采用搜索空间相互蒸馏技术，稳定训练过程并加速每个子超网的收敛。在给定预算内，HEP-NAS能够实现所有边的分割并逐步搜索更高精度的架构。在不同数据集和搜索空间上的实验结果证明了HEP-NAS相较于最新技术方法的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10723v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>神经网络架构搜索（NAS）领域已经通过采用一次性方法取得了显著进展，这些方法通过采用权重共享策略来降低搜索成本。然而，性能估计的准确性可能会受到协同适应性的威胁。本文提出了一种层次划分算法——HEP-NAS，旨在进一步提高准确性。它通过边缘层次划分，更直接地搜索每个中间节点的最佳操作组合，以缩小搜索空间并找到最具潜力的子超网络。此外，它采用搜索空间相互蒸馏技术，稳定训练过程并加速每个子超网络的收敛。实验结果表明，在有限的预算内，HEP-NAS能够在各种数据集和搜索空间中实现更高的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>一键式方法在神经网络架构搜索领域取得显著进展，通过权重共享策略降低搜索成本。</li>
<li>性能估计的准确性可能受到协同适应性的威胁。</li>
<li>HEP-NAS是一种层次划分算法，通过在同一层次内对边缘进行排列和分割，更直接地搜索最佳操作组合。</li>
<li>HEP-NAS在分割后选择最有前途的子超网络，逐步缩小可能包含最佳架构的搜索空间。</li>
<li>HEP-NAS采用搜索空间相互蒸馏技术，稳定训练过程并加速子超网络的收敛。</li>
<li>HEP-NAS能够在有限的预算内实现更高的准确性，并在各种数据集和搜索空间中表现出优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10723">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0923dc627023568b66b2fe5cf0eb79f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a38f388b64f37b97398413190ad7ec96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa397f2c6a9feaed509436147aeb6247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a9cd39fc344e5833bdf9f1f349b9f4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f25d8c13909ccda26a8873fe14d40fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65ce73e26ec8ce978504d9889b802acc.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation"></a>FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation</h2><p><strong>Authors:Yuntian Bo, Yazhou Zhu, Lunbo Li, Haofeng Zhang</strong></p>
<p>Existing few-shot medical image segmentation (FSMIS) models fail to address a practical issue in medical imaging: the domain shift caused by different imaging techniques, which limits the applicability to current FSMIS tasks. To overcome this limitation, we focus on the cross-domain few-shot medical image segmentation (CD-FSMIS) task, aiming to develop a generalized model capable of adapting to a broader range of medical image segmentation scenarios with limited labeled data from the novel target domain. Inspired by the characteristics of frequency domain similarity across different domains, we propose a Frequency-aware Matching Network (FAMNet), which includes two key components: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion (MSF) module. The FAM module tackles two problems during the meta-learning phase: 1) intra-domain variance caused by the inherent support-query bias, due to the different appearances of organs and lesions, and 2) inter-domain variance caused by different medical imaging techniques. Additionally, we design an MSF module to integrate the different frequency features decoupled by the FAM module, and further mitigate the impact of inter-domain variance on the model’s segmentation performance. Combining these two modules, our FAMNet surpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation models on three cross-domain datasets, achieving state-of-the-art performance in the CD-FSMIS task. </p>
<blockquote>
<p>现有的少样本医学图像分割（FSMIS）模型无法解决医学成像中的一个实际问题：由不同成像技术引起的域偏移，这限制了其在当前FSMIS任务中的应用。为了克服这一局限性，我们专注于跨域少样本医学图像分割（CD-FSMIS）任务，旨在开发一种通用模型，能够在有限的新目标域的标记数据下，适应更广泛的医学图像分割场景。我们受到不同域之间频率域相似性特征的启发，提出了一种频率感知匹配网络（FAMNet），它包括两个关键组件：频率感知匹配（FAM）模块和多光谱融合（MSF）模块。FAM模块解决了元学习阶段的两个问题：1）由于器官和病变的不同外观导致的域内方差，这是固有的支持查询偏见的结果；2）由不同的医学成像技术引起的域间方差。此外，我们设计了一个MSF模块，以整合FAM模块解耦的不同频率特征，并进一步减轻域间方差对模型分割性能的影响。结合这两个模块，我们的FAMNet在三个跨域数据集上超越了现有的FSMIS模型和跨域少样本语义分割模型，在CD-FSMIS任务中达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09319v2">PDF</a> Accepted by the 39th Annual AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong></p>
<p>本文关注跨域少样本医疗图像分割（CD-FSMIS）任务，旨在开发一个能够适应更广泛医疗图像分割场景、解决不同成像技术引起的领域偏移问题的通用模型。为此，提出了频率感知匹配网络（FAMNet），包括频率感知匹配（FAM）模块和多光谱融合（MSF）模块，提高了模型在元学习阶段的性能。这些创新点使模型在跨域数据集上的表现超越了现有的FSMIS模型和跨域少样本语义分割模型，实现了在CD-FSMIS任务中的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>跨域少样本医疗图像分割（CD-FSMIS）是医疗图像处理领域的一个重要问题，涉及到不同成像技术的领域偏移问题。</li>
<li>本文提出了频率感知匹配网络（FAMNet）来解决CD-FSMIS任务，包括频率感知匹配（FAM）模块和多光谱融合（MSF）模块。</li>
<li>FAM模块解决了元学习阶段的两个主要问题：由器官和病变不同外观引起的内部领域差异和不同医学成像技术引起的跨领域差异。</li>
<li>MSF模块用于整合由FAM模块分离的不同频率特征，进一步减轻跨领域差异对模型分割性能的影响。</li>
<li>FAMNet模型在三个跨域数据集上的表现超越了现有的FSMIS模型和跨域少样本语义分割模型。</li>
<li>该研究为医疗图像分割领域提供了一种新的解决方案，能够应对有限的标签数据和不同的成像技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09319">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-247281a683520d1a403f40d75c810e43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa5e909180014c3b3e9b82c59c9fd06f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bebbe679237a61c8f674f9fcadf5a543.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FM2DS-Few-Shot-Multimodal-Multihop-Data-Synthesis-with-Knowledge-Distillation-for-Question-Answering"><a href="#FM2DS-Few-Shot-Multimodal-Multihop-Data-Synthesis-with-Knowledge-Distillation-for-Question-Answering" class="headerlink" title="FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge   Distillation for Question Answering"></a>FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge   Distillation for Question Answering</h2><p><strong>Authors:Amirhossein Abaskohi, Spandana Gella, Giuseppe Carenini, Issam H. Laradji</strong></p>
<p>Multimodal multihop question answering is a complex task that requires reasoning over multiple sources of information, such as images and text, to answer questions. While there has been significant progress in visual question answering, the multihop setting remains unexplored due to the lack of high-quality datasets. Current methods focus on single-hop question answering or a single modality, which makes them unsuitable for real-world scenarios such as analyzing multimodal educational materials, summarizing lengthy academic articles, or interpreting scientific studies that combine charts, images, and text. To address this gap, we propose a novel methodology, introducing the first framework for creating a high-quality dataset that enables training models for multimodal multihop question answering. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure quality data. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks, our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) on average. We believe our data synthesis method will serve as a strong foundation for training and evaluating multimodal multihop question answering models. </p>
<blockquote>
<p>多模态多跳问答是一项复杂的任务，它要求在多源信息上进行推理，如图像和文本，以回答问题。尽管视觉问答已经取得了重大进展，但由于缺乏高质量的数据集，多跳设置仍然未被探索。当前的方法侧重于单跳问答或单一模态，这使得它们不适合现实场景，如分析多模态教育材料、总结冗长的学术论文或解释结合图表、图像和文本的科学研究。为了弥补这一空白，我们提出了一种新的方法，并引入了创建高质量数据集的第一个框架，该框架能够训练多模态多跳问答模型。我们的方法包括一个涉及从维基百科获取相关多模态文档、合成生成高级问题和答案、并通过严格标准验证以确保数据质量的五个阶段的管道。我们通过在我们的合成数据集上训练模型并在两个基准上进行测试来评估我们的方法。结果表明，在样本大小相同的情况下，在我们的合成数据上训练的模型在平均精确匹配（EM）上比在人类收集的数据上训练的模型高出1.9。我们相信我们的数据合成方法将为训练和评估多模态多跳问答模型提供坚实的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07030v2">PDF</a> 20 pages, 11 figures, 10 tables, Submitted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了多模态多跳问答任务的重要性，其需要融合图像和文字等多种信息来源进行推理。尽管视觉问答任务已取得了重要进展，但由于缺乏高质量数据集，多跳设置仍未被探索。当前方法主要关注单跳问答或单一模态，不适用于分析多模态教育材料、总结冗长学术论文或解读结合图表、文字和图像的科学研究等现实场景。为弥补这一空白，我们提出了一种新方法，首次构建了高质量数据集，为训练多模态多跳问答模型提供了可能。我们的方法包括一个涉及从维基百科获取相关多模态文档、合成高级问题和答案、并通过严格标准验证数据质量的五个阶段管道。通过在我们的合成数据集上训练模型并在两个基准上进行测试，结果表明，在样本大小相等的情况下，我们的合成数据训练的模型在人收集的基准数据上的平均精确匹配得分高出1.9分。我们相信我们的数据合成方法将为训练和评估多模态多跳问答模型提供坚实的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态多跳问答是一个跨越图像和文本等多个信息源的复杂任务，需要强大的推理能力。</li>
<li>当前缺乏高质量数据集限制了多跳问答任务的进展。</li>
<li>当前方法主要关注单模态和单跳问答，不适用于现实世界的复杂场景。</li>
<li>提出了一种新的方法，构建了高质量数据集以支持多模态多跳问答的训练和评估。</li>
<li>该方法包括五个阶段的管道，涉及从维基百科获取多模态文档、合成高级问题和答案等步骤。</li>
<li>实验结果表明，在样本大小相同的情况下，合成数据训练的模型性能优于在人收集的基准数据上训练的模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07030">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-39afa50736a23acafa909748bd95d3e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17398bc33f88f8a6a7fa79dadeef87c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b755a8d1756d2e5395ae5d455f05e72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d2d07d044e8591d3e3ce02da3e5e84f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Efficient-Transfer-Learning-for-Video-language-Foundation-Models"><a href="#Efficient-Transfer-Learning-for-Video-language-Foundation-Models" class="headerlink" title="Efficient Transfer Learning for Video-language Foundation Models"></a>Efficient Transfer Learning for Video-language Foundation Models</h2><p><strong>Authors:Haoxing Chen, Zizheng Huang, Yan Hong, Yanshuo Wang, Zhongcai Lyu, Zhuoer Xu, Jun Lan, Zhangxuan Gu</strong></p>
<p>Pre-trained vision-language models provide a robust foundation for efficient transfer learning across various downstream tasks. In the field of video action recognition, mainstream approaches often introduce additional parameter modules to capture temporal information. While the increased model capacity brought by these additional parameters helps better fit the video-specific inductive biases, existing methods require learning a large number of parameters and are prone to catastrophic forgetting of the original generalizable knowledge. In this paper, we propose a simple yet effective Multi-modal Spatio-Temporal Adapter (MSTA) to improve the alignment between representations in the text and vision branches, achieving a balance between general knowledge and task-specific knowledge. Furthermore, to mitigate over-fitting and enhance generalizability, we introduce a spatio-temporal description-guided consistency constraint. This constraint involves feeding template inputs (i.e., &#96;&#96;a video of ${\textbf{cls}}$’’) into the trainable language branch, while LLM-generated spatio-temporal descriptions are input into the pre-trained language branch, enforcing consistency between the outputs of the two branches. This mechanism prevents over-fitting to downstream tasks and improves the distinguishability of the trainable branch within the spatio-temporal semantic space. We evaluate the effectiveness of our approach across four tasks: zero-shot transfer, few-shot learning, base-to-novel generalization, and fully-supervised learning. Compared to many state-of-the-art methods, our MSTA achieves outstanding performance across all evaluations, while using only 2-7% of the trainable parameters in the original model. Code will be avaliable at <a target="_blank" rel="noopener" href="https://github.com/chenhaoxing/ETL4Video">https://github.com/chenhaoxing/ETL4Video</a>. </p>
<blockquote>
<p>预训练过的视觉语言模型为高效迁移学习在各种下游任务中的迁移提供了坚实的基础。在视频动作识别领域，主流方法通常引入额外的参数模块来捕获时间信息。虽然这些额外参数带来的模型容量增加有助于更好地适应视频特定的归纳偏见，但现有方法需要学习大量参数，并容易遗忘原始的通用知识。在本文中，我们提出了一种简单有效的多模态时空适配器（MSTA），以提高文本和视觉分支之间表示的对齐，在通用知识和任务特定知识之间取得平衡。此外，为了缓解过拟合并增强通用性，我们引入了一种受时空描述引导的一致性约束。该约束涉及将模板输入（例如，“一个视频中的${\textbf{cls}}$”）输入到可训练的语言分支，同时将LLM生成的时空描述输入到预训练的语言分支，强制两个分支的输出保持一致。这种机制防止了对下游任务的过度拟合，提高了可训练分支在时空语义空间中的可区分性。我们在四种任务上评估了我们的方法的有效性：零样本迁移、小样学习、基础到新颖的泛化和全监督学习。与许多先进的方法相比，我们的MSTA在所有评估中都取得了出色的性能，同时只使用了原始模型中2-7%的可训练参数。代码将在<a target="_blank" rel="noopener" href="https://github.com/chenhaoxing/ETL4Video%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/chenhaoxing/ETL4Video上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11223v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个简单有效的多模态时空适配器（MSTA），用于改善文本和视觉分支之间的表示对齐，实现了通用知识和任务特定知识之间的平衡。为提高模型的泛化能力并减轻过拟合问题，引入了时空描述引导的一致性约束。在四个任务上的评估表明，MSTA在零样本迁移、少样本学习、基础到新颖泛化以及全监督学习方面都取得了出色的性能，并且仅使用原始模型中2-7%的可训练参数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态时空适配器（MSTA）增强了预训练视觉语言模型的表示对齐，提高了在各种下游任务中的迁移学习效率。</li>
<li>MSTA实现了通用知识和任务特定知识之间的平衡，通过引入少量额外参数模块来捕捉时空信息。</li>
<li>引入的时空描述引导的一致性约束提高了模型的泛化能力，并减轻了过拟合问题。</li>
<li>通过使用模板输入和LLM生成的时空描述，增强了模型在时空语义空间中的可辨识性。</li>
<li>MSTA在四个任务上的性能均优于许多最新方法，且仅使用原始模型的很小部分可训练参数。</li>
<li>该方法具有广泛的应用潜力，适用于零样本迁移、少样本学习、基础到新颖泛化以及全监督学习等任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11223">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-462c8bd756b977acd3244a7e2ca31863.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79bda175dcb2bbf09576150edab05e1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c2044ceba096091f3f1263a8f0800e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2102ca4e507be97cbcb2117b41e290.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1><h2 id="Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models"><a href="#Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models" class="headerlink" title="Revisiting In-context Learning Inference Circuit in Large Language   Models"></a>Revisiting In-context Learning Inference Circuit in Large Language   Models</h2><p><strong>Authors:Hakaze Cho, Mariko Kato, Yoshihiro Sakai, Naoya Inoue</strong></p>
<p>In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Input Text Encode: LMs encode every input text (demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit. </p>
<blockquote>
<p>上下文学习（ICL）是一种新兴的语言模型（LM）小样本学习范式，其内部机制尚未被探索。已有工作描述了ICL的内部处理过程，但很难捕捉大型语言模型中的所有推理现象。因此，本文提出了一个综合电路来模拟推理动态，并试图解释观察到的ICL现象。具体来说，我们将ICL推理过程分为三大操作：（1）输入文本编码：LM将每个输入文本（演示和查询）编码为隐藏状态中的线性表示，其中包含解决ICL任务所需的信息。（2）语义合并：LM将演示的编码表示与其相应的标签令牌合并，以产生标签和演示的联合表示。（3）特征检索和复制：LM在任务子空间中搜索与查询表示相似的联合表示，并将所搜索的表示复制到查询中。然后，语言模型头部在一定程度上捕获这些复制的标签表示，并将其解码为预测标签。所提出的推理电路成功地捕捉到了ICL过程中观察到的许多现象，是对ICL推理过程的全面和实际解释。此外，通过禁用所提出的步骤进行的分析严重损害了ICL的性能，这表明所提出的推理电路是主导机制。另外，我们确认并列出了一些与所提出的电路并行解决ICL任务的旁路机制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04468v2">PDF</a> 37 pages, 41 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>本论文针对语言模型中的In-context Learning（ICL）提出了一种全面的推理电路模型，用以解释ICL过程中的推理现象。该电路将ICL推理分为三大操作：输入文本编码、语义合并以及特征检索与复制。该模型成功捕捉了ICL过程中的许多现象，为ICL提供了全面而实用的解释。此外，通过去除该电路中的某些步骤，ICL性能受到严重影响，证明了该电路的主导作用。同时，论文还确认了与电路并行解决ICL任务的某些旁路机制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>In-context Learning（ICL）是一种新兴的语言模型中的小样本学习范式，其内部机制尚未被完全探索。</li>
<li>现有研究在描述ICL的内部处理方面已有成果，但难以捕捉大型语言模型中的所有推理现象。</li>
<li>本论文提出了一种全面的推理电路模型，用以解释ICL过程中的推理现象，包括输入文本编码、语义合并以及特征检索与复制三大操作。</li>
<li>该电路模型成功捕捉了ICL过程中的许多现象，为理解这一过程提供了全面而实用的解释。</li>
<li>通过去除电路中的某些步骤，ICL性能受到严重影响，证明了该电路的主导作用。</li>
<li>论文还确认了与电路并行解决ICL任务的某些旁路机制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-abe9169fc4d79ec3bcf6aaa83cee09a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2671ca5bf8a8f7ced3f326fe11d206e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9f2bc2ba4df900d15bff2a53d7fa5a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c759ab722c13c3d7958c15492bca666.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a91205a65b64bc5f5dda409bd8f005a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f54f25de98cb55cea65795548facddaf.jpg" align="middle">
</details>


<h1 id="-20"><a href="#-20" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b22f0081c44a3181d17141f1ff9c9af7.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2024-12-19  Real-Time Position-Aware View Synthesis from Single-View Input
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-da617ccd25a73f3719a11187f1ec454b.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2024-12-19  Make Imagination Clearer! Stable Diffusion-based Visual Imagination for   Multimodal Machine Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">9113.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
