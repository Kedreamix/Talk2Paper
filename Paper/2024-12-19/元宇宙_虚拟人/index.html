<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="元宇宙/虚拟人">
    <meta name="description" content="元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-12-19  GraphAvatar Compact Head Avatars with GNN-Generated 3D Gaussians">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>元宇宙/虚拟人 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-72e6f839d76852c919624ff97607ba0c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">元宇宙/虚拟人</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                                <span class="chip bg-color">元宇宙/虚拟人</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                元宇宙/虚拟人
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-19-更新"><a href="#2024-12-19-更新" class="headerlink" title="2024-12-19 更新"></a>2024-12-19 更新</h1><h2 id="GraphAvatar-Compact-Head-Avatars-with-GNN-Generated-3D-Gaussians"><a href="#GraphAvatar-Compact-Head-Avatars-with-GNN-Generated-3D-Gaussians" class="headerlink" title="GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians"></a>GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians</h2><p><strong>Authors:Xiaobao Wei, Peng Chen, Ming Lu, Hui Chen, Feng Tian</strong></p>
<p>Rendering photorealistic head avatars from arbitrary viewpoints is crucial for various applications like virtual reality. Although previous methods based on Neural Radiance Fields (NeRF) can achieve impressive results, they lack fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have improved rendering quality and real-time performance but still require significant storage overhead. In this paper, we introduce a method called GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an appearance GNN to generate the attributes of the 3D Gaussians from the tracked mesh. Therefore, our method can store the GNN models instead of the 3D Gaussians, significantly reducing the storage overhead to just 10MB. To reduce the impact of face-tracking errors, we also present a novel graph-guided optimization module to refine face-tracking parameters during training. Finally, we introduce a 3D-aware enhancer for post-processing to enhance the rendering quality. We conduct comprehensive experiments to demonstrate the advantages of GraphAvatar, surpassing existing methods in visual fidelity and storage consumption. The ablation study sheds light on the trade-offs between rendering quality and model size. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ucwxb/GraphAvatar">https://github.com/ucwxb/GraphAvatar</a> </p>
<blockquote>
<p>从任意角度渲染出真实感头部角色，对虚拟现实等应用至关重要。尽管基于神经辐射场（NeRF）的先前方法可以实现令人印象深刻的效果，但它们缺乏保真度和效率。最近使用三维高斯片材（3DGS）的方法提高了渲染质量和实时性能，但仍需要相当大的存储开销。在本文中，我们提出了一种名为GraphAvatar的方法，它利用图神经网络（GNN）生成头部角色的三维高斯模型。具体来说，GraphAvatar训练了一个几何GNN和一个外观GNN，从跟踪的网格中产生三维高斯模型的属性。因此，我们的方法可以存储GNN模型而不是三维高斯模型，显著减少存储开销至仅10MB。为了减少面部跟踪误差的影响，我们还提供了一个新型的图引导优化模块，用于在训练过程中优化面部跟踪参数。最后，我们引入了一个用于后处理的3D感知增强器，以提高渲染质量。我们进行了全面的实验，展示了GraphAvatar的优势，在视觉保真度和存储消耗方面超越了现有方法。消融研究揭示了渲染质量和模型大小之间的权衡。代码将在<a target="_blank" rel="noopener" href="https://github.com/ucwxb/GraphAvatar">https://github.com/ucwxb/GraphAvatar</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13983v1">PDF</a> accepted by AAAI2025</p>
<p><strong>Summary</strong><br>     本文提出了一种利用图神经网络（GNN）生成头部化身三维高斯分布的方法，称为GraphAvatar。该方法通过训练几何GNN和外观GNN来从跟踪的网格生成三维高斯分布的属性，显著减少存储需求至仅10MB。为减少面部跟踪误差的影响，还提出了图形引导的优化模块，并在后处理中引入了一个三维感知增强器来提高渲染质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GraphAvatar利用图神经网络（GNN）生成头部化身的三维高斯分布，实现了高效且高质量的渲染。</li>
<li>通过训练几何GNN和外观GNN，从跟踪的网格生成属性，显著减少存储需求。</li>
<li>引入图形引导的优化模块，以减小面部跟踪误差的影响，提高渲染质量。</li>
<li>提出了一个三维感知增强器进行后处理，进一步增强渲染质量。</li>
<li>GraphAvatar在视觉保真度和存储消耗方面超越了现有方法。</li>
<li>全面的实验验证了GraphAvatar的优势，包括与现有方法的对比和消融研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13983">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-499727917bec8913fec8dee0d29c0265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-334573154ce596e4d0d70d1cf06d6c47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf870dad7b6fa78ed96a65941a326f27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75eda6d48b2a3d0fedaa25367e8d5cb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-555c65c663223607d9c0758ab83bc605.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Na’vi-or-Knave-Jailbreaking-Language-Models-via-Metaphorical-Avatars"><a href="#Na’vi-or-Knave-Jailbreaking-Language-Models-via-Metaphorical-Avatars" class="headerlink" title="Na’vi or Knave: Jailbreaking Language Models via Metaphorical Avatars"></a>Na’vi or Knave: Jailbreaking Language Models via Metaphorical Avatars</h2><p><strong>Authors:Yu Yan, Sheng Sun, Junqi Tong, Min Liu, Qi Li</strong></p>
<p>Metaphor serves as an implicit approach to convey information, while enabling the generalized comprehension of complex subjects. However, metaphor can potentially be exploited to bypass the safety alignment mechanisms of Large Language Models (LLMs), leading to the theft of harmful knowledge. In our study, we introduce a novel attack framework that exploits the imaginative capacity of LLMs to achieve jailbreaking, the J\underline{\textbf{A}}ilbreak \underline{\textbf{V}}ia \underline{\textbf{A}}dversarial Me\underline{\textbf{TA}} -pho\underline{\textbf{R}} (\textit{AVATAR}). Specifically, to elicit the harmful response, AVATAR extracts harmful entities from a given harmful target and maps them to innocuous adversarial entities based on LLM’s imagination. Then, according to these metaphors, the harmful target is nested within human-like interaction for jailbreaking adaptively. Experimental results demonstrate that AVATAR can effectively and transferablly jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs. Our study exposes a security risk in LLMs from their endogenous imaginative capabilities. Furthermore, the analytical study reveals the vulnerability of LLM to adversarial metaphors and the necessity of developing defense methods against jailbreaking caused by the adversarial metaphor. \textcolor{orange}{ \textbf{Warning: This paper contains potentially harmful content from LLMs.}} </p>
<blockquote>
<p>隐喻作为一种隐性传递信息的方式，能够使复杂的主题得到普遍理解。然而，隐喻可能被用于绕过大型语言模型（LLM）的安全对齐机制，从而导致有害知识的窃取。在我们的研究中，我们引入了一种新的攻击框架，利用LLM的想象力来实现越狱，即J\underline{\textbf{A}}ilbreak \underline{\textbf{V}}ia \underline{\textbf{A}}dversarial Me\underline{\textbf{TA}} -pho\underline{\textbf{R}}（\textit{AVATAR}）。具体来说，为了引发有害的反应，AVATAR会从给定的有害目标中提取有害实体，并根据LLM的想象力将它们映射到无害的对立实体。然后，根据这些隐喻，将有害目标嵌套在人类交互中进行自适应越狱。实验结果表明，AVATAR可以有效地、可迁移地对LLM进行越狱，并在多个高级LLM上达到最先进的攻击成功率。我们的研究揭示了LLM由于其固有的想象力功能存在的安全风险。此外，分析研究表明LLM易受对立隐喻的影响，并有必要开发针对由对立隐喻引起的越狱的防御方法。警告：本文包含可能有害的LLM内容。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12145v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>本研究揭示了隐喻可能绕过大型语言模型的安防机制的风险，导致有害知识的窃取。为此提出了一种新型攻击框架——通过利用大型语言模型的想象力来实现越狱的 AVATAR（冒险超越真实世界）。实验证明，AVATAR能有效越狱多个高级大型语言模型，并达到前所未有的攻击成功率。研究揭示了大型语言模型对隐喻的脆弱性，并警告有必要开发防御方法来应对这种攻击方式带来的风险。这项研究提供了防范该漏洞的技术路线图和对未来发展潜力的探讨。部分学者特别警示表示此文含有大型语言模型潜在的有害内容。 </p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>隐喻可以作为一种绕过大型语言模型安全机制的手段，窃取有害知识。</li>
<li>研究提出新型攻击框架 AVATAR，通过模拟现实实体转移来实现攻击效果。 </li>
<li>实验结果显示 AVATAR 在多个高级大型语言模型上实现有效越狱，攻击成功率领先。 </li>
<li>大型语言模型对隐喻存在脆弱性，需要开发防御方法来应对风险。 </li>
<li>该研究对技术路线图进行明确描述，为未来的技术发展提供了指导方向。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12145">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-72e6f839d76852c919624ff97607ba0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b63e56614198c6fdf125f04e9c794b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af5fc1b65a378f2d6ebb2e387b6f4cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f123f38ac867535dc507f5b3961eb98b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CAP4D-Creating-Animatable-4D-Portrait-Avatars-with-Morphable-Multi-View-Diffusion-Models"><a href="#CAP4D-Creating-Animatable-4D-Portrait-Avatars-with-Morphable-Multi-View-Diffusion-Models" class="headerlink" title="CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View   Diffusion Models"></a>CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View   Diffusion Models</h2><p><strong>Authors:Felix Taubner, Ruihang Zhang, Mathieu Tuli, David B. Lindell</strong></p>
<p>Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints $-$ for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques. </p>
<blockquote>
<p>从图像重建逼真且动态的肖像化身对于广告、视觉效果和虚拟现实等许多应用至关重要。根据应用的不同，化身重建涉及不同的捕获设置和约束——例如，视觉效果工作室使用相机阵列捕获数百张参考图像，而内容创作者可能试图从互联网上下载的单张肖像图像进行动画制作。因此，存在大量且多样化的化身重建方法生态系统。基于多视图立体或神经渲染的技术取得了最高质量的结果，但需要数百张参考图像。最近的生成模型可以从单张参考图像生成令人信服的化身，但在视觉真实感方面仍然落后于多视图技术。在这里，我们提出CAP4D：一种使用形态可变的多视图扩散模型的方法，可以从任何数量的参考图像（即1到100张）重建逼真的4D（动态3D）肖像化身，并以实时方式进行动画和渲染。我们的方法在单张、少量和多张图像4D肖像化身重建方面均表现出最新技术水平，并致力于缩小单图像重建与多视图重建技术之间在视觉真实感方面的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12093v1">PDF</a> 23 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于形态多变的多视角扩散模型（CAP4D）的重建技术，该技术可从任意数量的参考图像（从一到一百张）重建出逼真的四维动态肖像，并实时进行动画渲染。此技术填补了单图像和多视角重建技术之间在视觉真实感方面的差距，为广告、视觉效果和虚拟现实等领域提供了重要的技术支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>重建基于图像的光影现实主义动态肖像对广告、视觉效果和虚拟现实等多个应用至关重要。</li>
<li>目前存在多种头像重建技术，根据应用场景，头像重建有不同的采集设置和限制。</li>
<li>多视角立体或神经渲染技术能提供高质量的结果，但需要参考数百张图像。</li>
<li>基于单一参考图像的生成模型虽然能生成令人信服的头像，但在视觉真实感上仍有待提高。</li>
<li>CAP4D方法使用形态多变的多视角扩散模型进行四维动态肖像头像重建。</li>
<li>CAP4D技术能从任意数量的参考图像（一到一百张）重建头像，并在实时进行动画渲染。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b0c07cb33b1c6197b11b67dba0c6f21a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99b9ac7a01765803cf9a22724eaa54fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2778de1c980b3010aaf86d7b2251eb63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ef638c55b3f767d0633e6d902605853.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="3D-2-Actor-Learning-Pose-Conditioned-3D-Aware-Denoiser-for-Realistic-Gaussian-Avatar-Modeling"><a href="#3D-2-Actor-Learning-Pose-Conditioned-3D-Aware-Denoiser-for-Realistic-Gaussian-Avatar-Modeling" class="headerlink" title="3D$^2$-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic   Gaussian Avatar Modeling"></a>3D$^2$-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic   Gaussian Avatar Modeling</h2><p><strong>Authors:Zichen Tang, Hongyu Yang, Hanchen Zhang, Jiaxin Chen, Di Huang</strong></p>
<p>Advancements in neural implicit representations and differentiable rendering have markedly improved the ability to learn animatable 3D avatars from sparse multi-view RGB videos. However, current methods that map observation space to canonical space often face challenges in capturing pose-dependent details and generalizing to novel poses. While diffusion models have demonstrated remarkable zero-shot capabilities in 2D image generation, their potential for creating animatable 3D avatars from 2D inputs remains underexplored. In this work, we introduce 3D$^2$-Actor, a novel approach featuring a pose-conditioned 3D-aware human modeling pipeline that integrates iterative 2D denoising and 3D rectifying steps. The 2D denoiser, guided by pose cues, generates detailed multi-view images that provide the rich feature set necessary for high-fidelity 3D reconstruction and pose rendering. Complementing this, our Gaussian-based 3D rectifier renders images with enhanced 3D consistency through a two-stage projection strategy and a novel local coordinate representation. Additionally, we propose an innovative sampling strategy to ensure smooth temporal continuity across frames in video synthesis. Our method effectively addresses the limitations of traditional numerical solutions in handling ill-posed mappings, producing realistic and animatable 3D human avatars. Experimental results demonstrate that 3D$^2$-Actor excels in high-fidelity avatar modeling and robustly generalizes to novel poses. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/silence-tang/GaussianActor">https://github.com/silence-tang/GaussianActor</a>. </p>
<blockquote>
<p>随着神经隐式表示和可微分渲染技术的不断进步，从稀疏的多视角RGB视频中学习动画三维头像的能力得到了显著提高。然而，当前将观测空间映射到规范空间的方法在捕捉姿态相关细节和泛化到新姿态时常常面临挑战。尽管扩散模型在二维图像生成中表现出了令人印象深刻的零样本能力，但它们从二维输入创建动画三维头像的潜力仍未被充分探索。在这项工作中，我们引入了3D$^2$-Actor，这是一种新的方法，其特点是具有姿态调节的三维感知人类建模管道，该管道结合了迭代二维去噪和三维校正步骤。二维去噪器在姿态线索的指导下生成详细的多视角图像，提供了用于高保真三维重建和姿态渲染的丰富特征集。作为补充，我们的基于高斯的三维整流器通过两阶段投影策略和一种新的局部坐标表示，生成了具有增强三维一致性的图像。此外，我们提出了一种创新的采样策略，以确保视频合成中帧之间的时间连续性平滑。我们的方法有效地解决了传统数值解决方案在处理病态映射时的局限性，生成了逼真且可动画的三维人类头像。实验结果证明，3D$^2$-Actor在高保真头像建模方面表现出色，并能很好地泛化到新姿态。代码可用在：<a target="_blank" rel="noopener" href="https://github.com/silence-tang/GaussianActor%E3%80%82">https://github.com/silence-tang/GaussianActor。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11599v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>     神经网络隐式表示和可微渲染技术的发展极大地提高了从稀疏多视角RGB视频中学习动画三维人物角色的能力。然而，当前将观测空间映射到规范空间的方法在捕捉姿态相关细节和泛化到新姿态时面临挑战。本研究引入了一种名为3D$^2$-Actor的新方法，通过姿态调节的三维感知人类建模管道，集成了迭代二维去噪和三维校正步骤。在姿态线索的指导下，二维去噪器生成详细的多视角图像，为高质量三维重建和姿态渲染提供了丰富的特征集。基于高斯的三维整流器通过两阶段投影策略和新颖的地方坐标表示来增强图像的渲染三维一致性。此外，我们提出了一种创新的采样策略，以确保视频合成中帧之间的时间连续性平滑。该方法有效地解决了传统数值解决方案在处理不适定映射时的局限性，可生成逼真且可动画的三维人类角色。代码已在GitHub上发布供公开查阅和使用。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络隐式表示和可微渲染技术提高了创建三维人物角色的能力。</li>
<li>当前方法面临从观测空间到规范空间映射的挑战，尤其在捕捉姿态相关细节和泛化到新姿态时。</li>
<li>3D$^2$-Actor方法通过结合二维去噪和三维校正来解决这些问题。</li>
<li>二维去噪器生成详细的多视角图像，为高质量三维重建提供丰富特征。</li>
<li>基于高斯的三维整流器增强了图像的渲染三维一致性。</li>
<li>创新的采样策略确保了视频合成中的时间连续性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11599">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0b63fddf5c7f0f64e51ffe723c8fe090.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9476b38d0ebb9132b5f1c717bd677718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42bc2c11c3d88ef0a81cba1bb32ce510.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dbe8d2a72a149059f0de74aa0062f56a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b55835b2e2d98d2afde59ab7e68b6072.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="StrandHead-Text-to-Strand-Disentangled-3D-Head-Avatars-Using-Hair-Geometric-Priors"><a href="#StrandHead-Text-to-Strand-Disentangled-3D-Head-Avatars-Using-Hair-Geometric-Priors" class="headerlink" title="StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair   Geometric Priors"></a>StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair   Geometric Priors</h2><p><strong>Authors:Xiaokun Sun, Zeyu Cai, Zhenyu Zhang, Ying Tai, Jian Yang</strong></p>
<p>While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the general or entangled representation. We propose StrandHead, a novel text to 3D head avatar generation method capable of generating disentangled 3D hair with strand representation. Without using 3D data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative diffusion models. To this end, we propose a series of reliable priors on shape initialization, geometric primitives, and statistical haircut features, leading to a stable optimization and text-aligned performance. Extensive experiments show that StrandHead achieves the state-of-the-art reality and diversity of generated 3D head and hair. The generated 3D hair can also be easily implemented in the Unreal Engine for physical simulation and other applications. The code will be available at <a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/StrandHead.github.io">https://xiaokunsun.github.io/StrandHead.github.io</a>. </p>
<blockquote>
<p>虽然发型能体现个性，但现有的化身生成方法由于一般性或纠缠的表示而无法对实际发型进行建模。我们提出了StrandHead，这是一种新型的文本到3D头部化身生成方法，能够生成具有细丝表示的解纠缠3D发型。我们展示，不使用3D数据进行监督，可以通过提炼2D生成扩散模型，从提示生成逼真的发束。为此，我们在形状初始化、几何基元和统计发型特征方面提出了一系列可靠的先验知识，实现了稳定的优化和与文本对齐的性能。大量实验表明，StrandHead在生成的3D头部和发型的真实性和多样性方面达到了最新水平。生成的3D发型还可以轻松地在Unreal Engine中进行物理模拟和其他应用。代码将在<a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/StrandHead.github.io%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://xiaokunsun.github.io/StrandHead.github.io上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11586v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/StrandHead.github.io">https://xiaokunsun.github.io/StrandHead.github.io</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为StrandHead的新型文本到3D头像生成方法，能够生成具有发丝级表示的分离式3D头发。该方法无需使用3D数据进行监督，通过提炼2D生成扩散模型，即可从提示生成逼真的发丝。通过形状初始化、几何基本元素和发型特征统计的一系列可靠先验知识，实现了稳定优化和与文本相符的性能。实验表明，StrandHead在生成3D头像和头发方面达到了现实性和多样性的最佳水平，生成的3D头发可轻松应用于Unreal Engine进行物理模拟和其他应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StrandHead是一种新型的文本到3D头像生成方法。</li>
<li>它能够生成具有发丝级表示的分离式3D头发，这是现有方法所无法做到的。</li>
<li>StrandHead不需要使用3D数据进行监督，而是通过提炼2D生成扩散模型来生成头发。</li>
<li>该方法通过一系列可靠先验知识实现稳定优化和与文本相符的性能。</li>
<li>实验证明，StrandHead在生成3D头像和头发的现实性和多样性方面达到了最佳水平。</li>
<li>生成的3D头发可轻松应用于Unreal Engine进行物理模拟和其他应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11586">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5fd8d2b1281ef261bca4aa45e68e75cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d85d2a56283feaedc4991c46075f8347.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29ea6ea331d87575770e4072005785b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56727063359ceb66b7d0560b128a3e7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eff77adbaafcac86f986bb81062ed5a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VQTalker-Towards-Multilingual-Talking-Avatars-through-Facial-Motion-Tokenization"><a href="#VQTalker-Towards-Multilingual-Talking-Avatars-through-Facial-Motion-Tokenization" class="headerlink" title="VQTalker: Towards Multilingual Talking Avatars through Facial Motion   Tokenization"></a>VQTalker: Towards Multilingual Talking Avatars through Facial Motion   Tokenization</h2><p><strong>Authors:Tao Liu, Ziyang Ma, Qi Chen, Feilong Chen, Shuai Fan, Xie Chen, Kai Yu</strong></p>
<p>We present VQTalker, a Vector Quantization-based framework for multilingual talking head generation that addresses the challenges of lip synchronization and natural motion across diverse languages. Our approach is grounded in the phonetic principle that human speech comprises a finite set of distinct sound units (phonemes) and corresponding visual articulations (visemes), which often share commonalities across languages. We introduce a facial motion tokenizer based on Group Residual Finite Scalar Quantization (GRFSQ), which creates a discretized representation of facial features. This method enables comprehensive capture of facial movements while improving generalization to multiple languages, even with limited training data. Building on this quantized representation, we implement a coarse-to-fine motion generation process that progressively refines facial animations. Extensive experiments demonstrate that VQTalker achieves state-of-the-art performance in both video-driven and speech-driven scenarios, particularly in multilingual settings. Notably, our method achieves high-quality results at a resolution of 512*512 pixels while maintaining a lower bitrate of approximately 11 kbps. Our work opens new possibilities for cross-lingual talking face generation. Synthetic results can be viewed at <a target="_blank" rel="noopener" href="https://x-lance.github.io/VQTalker">https://x-lance.github.io/VQTalker</a>. </p>
<blockquote>
<p>我们提出VQTalker，这是一个基于矢量量化的多语言谈话头生成框架，解决了不同语言之间的口型同步和自然动作挑战。我们的方法基于语音学原理，即人类语音由一组特定的声音单元（音素）和相应的视觉发音（动程）组成，这些单元通常在各种语言中共享共性。我们引入了基于组残差有限标量量化（GRFSQ）的面部运动标记器，创建面部特征的离散表示。这种方法能够全面捕捉面部运动，即使在有限的训练数据下也能提高多种语言的泛化能力。基于这种量化表示，我们实现了一种从粗到细的运动生成过程，逐步优化面部动画。大量实验表明，VQTalker在视频驱动和语音驱动场景中均达到最新技术水平，特别是在多语言环境中。值得注意的是，我们的方法在512*512像素的分辨率下达到高质量结果，同时保持约11kbps的较低比特率。我们的工作开辟了跨语言对话面部生成的全新可能性。合成结果可在<a target="_blank" rel="noopener" href="https://x-lance.github.io/VQTalker%E6%9F%A5%E7%9C%8B%E3%80%82">https://x-lance.github.io/VQTalker查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09892v2">PDF</a> 14 pages</p>
<p><strong>Summary</strong><br>高保真多语言虚拟人物生成技术。基于向量量化的框架VQTalker，实现多语言虚拟人物头部生成，解决唇同步和跨语言自然动作挑战。引入面部运动分词器，使用组剩余有限标量量化技术，创建面部特征的离散表示，改善多语言场景下的泛化能力。建立精细化的动作生成流程，逐步优化面部动画。在视频驱动和语音驱动场景下表现优异，分辨率达512*512像素，比特率约11kbps。详情链接：<a target="_blank" rel="noopener" href="https://x-lance.github.io/VQTalker">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VQTalker是一个基于向量量化的多语言虚拟人物头部生成框架。</li>
<li>解决了唇同步和跨语言自然动作的问题。</li>
<li>引入面部运动分词器，使用GRFSQ技术创建面部特征的离散表示。</li>
<li>提高了在多语言场景下的泛化能力，即使训练数据有限。</li>
<li>实施了从粗到细的动作生成流程，逐步优化面部动画。</li>
<li>在视频和语音驱动的场景下表现优异，支持高分辨率（如512*512像素）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09892">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-af3e9dd611abc72a50e2d766b003741e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a722c8a7d82e7ca957d17c3f5f02906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6a388e7890a788231c4e9ac9605307d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84c0d2dc045aceef527f650a890ec0c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afabc3b7e4c72f3491a13a2940a020fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69169fcd7766328ed4c05ecd93b9406a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abef479a0571064a01720444e71aa407.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Barbie-Text-to-Barbie-Style-3D-Avatars"><a href="#Barbie-Text-to-Barbie-Style-3D-Avatars" class="headerlink" title="Barbie: Text to Barbie-Style 3D Avatars"></a>Barbie: Text to Barbie-Style 3D Avatars</h2><p><strong>Authors:Xiaokun Sun, Zhenyu Zhang, Ying Tai, Qian Wang, Hao Tang, Zili Yi, Jian Yang</strong></p>
<p>Recent advances in text-guided 3D avatar generation have made substantial progress by distilling knowledge from diffusion models. Despite the plausible generated appearance, existing methods cannot achieve fine-grained disentanglement or high-fidelity modeling between inner body and outfit. In this paper, we propose Barbie, a novel framework for generating 3D avatars that can be dressed in diverse and high-quality Barbie-like garments and accessories. Instead of relying on a holistic model, Barbie achieves fine-grained disentanglement on avatars by semantic-aligned separated models for human body and outfits. These disentangled 3D representations are then optimized by different expert models to guarantee the domain-specific fidelity. To balance geometry diversity and reasonableness, we propose a series of losses for template-preserving and human-prior evolving. The final avatar is enhanced by unified texture refinement for superior texture consistency. Extensive experiments demonstrate that Barbie outperforms existing methods in both dressed human and outfit generation, supporting flexible apparel combination and animation. Our project page is: <a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/Barbie.github.io">https://xiaokunsun.github.io/Barbie.github.io</a>. </p>
<blockquote>
<p>近期文本引导的3D角色形象生成技术取得了重大进展，通过扩散模型的知识蒸馏实现。尽管生成的外观可能令人信服，但现有方法无法实现精细的分解或内部身体和服装之间的高保真建模。在本文中，我们提出了Barbie，这是一个用于生成3D角色形象的全新框架，可以穿上多样且高质量的Barbie风格的服装和配饰。Barbie不是依赖于整体模型，而是通过语义对齐的分离模型实现角色形象的精细分解，分别用于人体和服装。这些分解的3D表示然后通过不同的专家模型进行优化，以保证特定领域的保真度。为了平衡几何多样性和合理性，我们提出了一系列保留模板和人体先验演化的损失。最终的角色形象通过统一的纹理优化增强纹理的一致性。大量实验表明，Barbie在着装人物和服装生成方面都优于现有方法，支持灵活的服装组合和动画。我们的项目页面是：<a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/Barbie.github.io%E3%80%82">https://xiaokunsun.github.io/Barbie.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09126v5">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://xiaokunsun.github.io/Barbie.github.io">https://xiaokunsun.github.io/Barbie.github.io</a></p>
<p><strong>Summary</strong><br>文本介绍了近期文本引导的3D角色生成技术的新进展，提出了一种名为Barbie的新型框架，用于生成可穿上多样化和高质量服装的角色。Barbie通过语义对齐的分离模型实现精细分离，优化领域特定保真度，平衡几何多样性和合理性，最终通过统一纹理优化增强纹理一致性。该框架在角色服装和服装生成方面表现出优于现有方法的效果，支持灵活的服装组合和动画。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Barbie框架实现了精细分离的角色生成技术，包括身体和服装的独立建模。</li>
<li>Barbie使用语义对齐的模型来确保角色和服装的高保真度建模。</li>
<li>Barbie框架引入了多种损失函数来平衡角色的几何多样性和合理性。</li>
<li>统一纹理优化增强了角色的纹理一致性。</li>
<li>Barbie在角色服装和服装生成方面表现出卓越性能，支持灵活的服装组合和动画。</li>
<li>Barbie框架在文本引导的3D角色生成领域取得了新的进展，进一步提高了生成角色的逼真度和多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.09126">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a78928d71824c54546db1dd2788078d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a15541cb4875d92f75d72250a9d30ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90470584ecd6dd8598d47c34f7ed0299.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a60bb30e29827ed5bc5b6c4b2f0c5bb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Human-3Diffusion-Realistic-Avatar-Creation-via-Explicit-3D-Consistent-Diffusion-Models"><a href="#Human-3Diffusion-Realistic-Avatar-Creation-via-Explicit-3D-Consistent-Diffusion-Models" class="headerlink" title="Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent   Diffusion Models"></a>Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent   Diffusion Models</h2><p><strong>Authors:Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll</strong></p>
<p>Creating realistic avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency. We propose Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion. Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other, and by coupling them in a tight manner, we can fully leverage the potential of both models. We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the priors from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process to have better 3D consistency. Experiments show that our proposed framework outperforms state-of-the-art methods and enables the creation of realistic avatars from a single RGB image, achieving high-fidelity in both geometry and appearance. Extensive ablations also validate the efficacy of our design, (1) multi-view 2D priors conditioning in generative 3D reconstruction and (2) consistency refinement of sampling trajectory via the explicit 3D representation. Our code and models will be released on <a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion">https://yuxuan-xue.com/human-3diffusion</a>. </p>
<blockquote>
<p>创建从单一RGB图像的真实化身是一个有吸引力但具有挑战性的任务。由于其不明确性，最近的工作利用来自大型数据集预训练的强大的二维扩散模型的先验知识。虽然二维扩散模型展现出强大的泛化能力，但它们无法提供具有保证的三维一致性的多视图形状先验。我们提出“Human 3Diffusion”：通过明确的3D一致扩散创建真实化身。我们的关键见解是，二维多视图扩散和三维重建模型彼此提供互补信息，通过紧密耦合它们，我们可以充分利用两者的潜力。我们引入了一种新型图像条件生成三维高斯Splats重建模型，它利用二维多视图扩散模型的先验知识，并提供明确的三维表示，进一步指导二维反向采样过程，以更好地实现三维一致性。实验表明，我们提出的框架优于最先进的方法，并能从单一RGB图像创建逼真的化身，在几何和外观上都实现高保真度。广泛的消融实验也验证了我们的设计的有效性，（1）生成三维重建中的多视图二维先验条件，（2）通过明确的三维表示对采样轨迹的一致性改进。我们的代码和模型将在<a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://yuxuan-xue.com/human-3diffusion上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08475v2">PDF</a> Accepted to NeurIPS2024. Project Page:   <a target="_blank" rel="noopener" href="https://yuxuan-xue.com/human-3diffusion">https://yuxuan-xue.com/human-3diffusion</a></p>
<p><strong>Summary</strong></p>
<p>基于单张RGB图像创建逼真的人像角色是一个吸引人的挑战性问题。该研究提出了一种新的方法Human 3Diffusion，通过将二维多视角扩散模型与三维重建模型紧密结合，实现了具有明确三维一致性的逼真人像角色创建。该方法引入了一种新的图像条件生成三维高斯Splats重建模型，利用二维多视角扩散模型的先验信息，并提供一个明确的三维表示，进一步指导二维反向采样过程，以达到更好的三维一致性。实验表明，该方法在几何和外观上都实现了高保真，超越了现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Human 3Diffusion方法结合了二维多视角扩散模型和三维重建模型，实现了逼真的人像角色创建。</li>
<li>该方法引入了一种新的图像条件生成三维高斯Splats重建模型，利用二维扩散模型的先验信息。</li>
<li>Human 3Diffusion方法提供了一个明确的三维表示，进一步指导二维反向采样过程，以实现更好的三维一致性。</li>
<li>实验表明，该方法在几何和外观上均实现了高保真，超越了现有技术。</li>
<li>该方法通过释放代码和模型，为公众提供了访问和使用的机会。</li>
<li>该研究强调了多视角二维先验条件在生成三维重建中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.08475">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-098d04fb98a7383be9fefedaf341e49d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bb8393065d5933b2cfa0352a5506572.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35bb47b846f5731cc9a4e3be005d1b01.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                                    <span class="chip bg-color">元宇宙/虚拟人</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a07829a49dd81bb2569cf072e3346f5a.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2024-12-19  GraphAvatar Compact Head Avatars with GNN-Generated 3D Gaussians
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9a1075a0efae11ba9ddebf78003ad364.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2024-12-19  A New Adversarial Perspective for LiDAR-based 3D Object Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">8841.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
