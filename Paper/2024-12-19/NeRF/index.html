<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-12-19  GraphAvatar Compact Head Avatars with GNN-Generated 3D Gaussians">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f48b889f694388b756223771c0d053df.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    44 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-19-更新"><a href="#2024-12-19-更新" class="headerlink" title="2024-12-19 更新"></a>2024-12-19 更新</h1><h2 id="GraphAvatar-Compact-Head-Avatars-with-GNN-Generated-3D-Gaussians"><a href="#GraphAvatar-Compact-Head-Avatars-with-GNN-Generated-3D-Gaussians" class="headerlink" title="GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians"></a>GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians</h2><p><strong>Authors:Xiaobao Wei, Peng Chen, Ming Lu, Hui Chen, Feng Tian</strong></p>
<p>Rendering photorealistic head avatars from arbitrary viewpoints is crucial for various applications like virtual reality. Although previous methods based on Neural Radiance Fields (NeRF) can achieve impressive results, they lack fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have improved rendering quality and real-time performance but still require significant storage overhead. In this paper, we introduce a method called GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an appearance GNN to generate the attributes of the 3D Gaussians from the tracked mesh. Therefore, our method can store the GNN models instead of the 3D Gaussians, significantly reducing the storage overhead to just 10MB. To reduce the impact of face-tracking errors, we also present a novel graph-guided optimization module to refine face-tracking parameters during training. Finally, we introduce a 3D-aware enhancer for post-processing to enhance the rendering quality. We conduct comprehensive experiments to demonstrate the advantages of GraphAvatar, surpassing existing methods in visual fidelity and storage consumption. The ablation study sheds light on the trade-offs between rendering quality and model size. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ucwxb/GraphAvatar">https://github.com/ucwxb/GraphAvatar</a> </p>
<blockquote>
<p>从任意视角渲染逼真的头像对于虚拟现实等应用至关重要。尽管基于神经辐射场（NeRF）的先前方法可以实现令人印象深刻的结果，但它们缺乏真实感和效率。使用三维高斯平铺（3DGS）的最近方法提高了渲染质量和实时性能，但仍然需要很大的存储开销。在本文中，我们介绍了一种名为GraphAvatar的方法，该方法利用图神经网络（GNN）生成头像的3D高斯分布。具体来说，GraphAvatar训练了一个几何GNN和一个外观GNN，从跟踪的网格中产生3D高斯分布的属性。因此，我们的方法可以存储GNN模型而不是3D高斯分布，将存储开销大幅降低到仅10MB。为了减少面部跟踪误差的影响，我们还提供了一个新型的图引导优化模块，用于在训练过程中优化面部跟踪参数。最后，我们引入了一个用于后处理的3D感知增强器，以提高渲染质量。我们进行了全面的实验，展示了GraphAvatar的优势，在视觉真实感和存储消耗方面超越了现有方法。消融研究揭示了渲染质量和模型大小之间的权衡。代码将在<a target="_blank" rel="noopener" href="https://github.com/ucwxb/GraphAvatar">https://github.com/ucwxb/GraphAvatar</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13983v1">PDF</a> accepted by AAAI2025</p>
<p><strong>摘要</strong><br>采用图神经网络（GNN）生成头显人物模型，实现高保真度渲染。通过几何GNN和外观GNN生成三维高斯分布属性，降低存储需求至仅10MB。引入图引导优化模块，减少面部跟踪误差对渲染质量的影响，并提出三维感知增强器用于后期处理以提升渲染质量。实验证明GraphAvatar在视觉保真度和存储消耗方面优于现有方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>利用图神经网络（GNN）生成三维高斯分布，实现高质量的头显人物渲染。</li>
<li>通过几何和外观GNN模型生成属性，大幅降低存储需求至仅10MB。</li>
<li>图引导优化模块减少了面部跟踪误差对渲染效果的影响。</li>
<li>提出一种新型的三维感知增强器进行后处理以提升渲染质量。</li>
<li>实验证明GraphAvatar在视觉保真度和存储消耗方面优于现有方法。</li>
<li>消融研究揭示了渲染质量和模型大小之间的权衡关系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13983">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-499727917bec8913fec8dee0d29c0265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-334573154ce596e4d0d70d1cf06d6c47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf870dad7b6fa78ed96a65941a326f27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75eda6d48b2a3d0fedaa25367e8d5cb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-555c65c663223607d9c0758ab83bc605.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RelationField-Relate-Anything-in-Radiance-Fields"><a href="#RelationField-Relate-Anything-in-Radiance-Fields" class="headerlink" title="RelationField: Relate Anything in Radiance Fields"></a>RelationField: Relate Anything in Radiance Fields</h2><p><strong>Authors:Sebastian Koch, Johanna Wald, Mirco Colosi, Narunas Vaskevicius, Pedro Hermosilla, Federico Tombari, Timo Ropinski</strong></p>
<p>Neural radiance fields are an emerging 3D scene representation and recently even been extended to learn features for scene understanding by distilling open-vocabulary features from vision-language models. However, current method primarily focus on object-centric representations, supporting object segmentation or detection, while understanding semantic relationships between objects remains largely unexplored. To address this gap, we propose RelationField, the first method to extract inter-object relationships directly from neural radiance fields. RelationField represents relationships between objects as pairs of rays within a neural radiance field, effectively extending its formulation to include implicit relationship queries. To teach RelationField complex, open-vocabulary relationships, relationship knowledge is distilled from multi-modal LLMs. To evaluate RelationField, we solve open-vocabulary 3D scene graph generation tasks and relationship-guided instance segmentation, achieving state-of-the-art performance in both tasks. See the project website at <a target="_blank" rel="noopener" href="https://relationfield.github.io/">https://relationfield.github.io</a>. </p>
<blockquote>
<p>神经辐射场是一种新兴的3D场景表示方法，最近甚至被扩展为通过学习特征来用于场景理解，通过从视觉语言模型中提炼开放词汇特征。然而，当前的方法主要集中在面向对象的表示上，支持对象分割或检测，而理解对象之间的语义关系仍然在很大程度上未被探索。为了弥补这一空白，我们提出了RelationField，这是第一种直接从神经辐射场中提取对象间关系的方法。RelationField将对象之间的关系表示为神经辐射场内的射线对，有效地将其公式扩展为包括隐式关系查询。为了教导RelationField复杂且开放的词汇关系，关系知识是从多模态大型语言模型中提炼出来的。为了评估RelationField的性能，我们解决了开放式词汇的3D场景图生成任务和关系导向的实例分割任务，在这两项任务中都达到了最先进的性能。更多详情可见项目网站：<a target="_blank" rel="noopener" href="https://relationfield.github.io./">https://relationfield.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13652v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://relationfield.github.io/">https://relationfield.github.io</a></p>
<p><strong>Summary</strong></p>
<p>神经网络辐射场（NeRF）是一种新兴的三维场景表示方法，最近被扩展用于学习场景理解的特征。当前的方法主要关注对象中心的表示，支持对象分割或检测，但在理解对象之间的语义关系方面仍存在很大差距。针对这一问题，我们提出RelationField，这是第一种直接从神经网络辐射场中提取对象间关系的方法。RelationField将对象间的关系表示为神经辐射场内的成对射线，有效地将其公式扩展为包含隐式关系查询。为了教授RelationField复杂、开放词汇的关系，我们从多模态大型语言模型（LLM）中提炼关系知识。在开放词汇3D场景图生成任务和关系引导实例分割任务中，RelationField取得了最佳性能。有关详细信息，请访问项目网站：<a target="_blank" rel="noopener" href="https://relationfield.github.io./">https://relationfield.github.io。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前NeRF主要用于对象中心的表示，支持对象分割和检测。</li>
<li>对象间的语义关系在NeRF中的理解仍然是一个未被充分探索的领域。</li>
<li>RelationField是第一个直接从NeRF提取对象间关系的方法。</li>
<li>RelationField通过将对象间的关系表示为NeRF内的成对射线来有效地处理隐式关系查询。</li>
<li>RelationField从多模态大型语言模型中提炼复杂、开放词汇的关系知识。</li>
<li>在开放词汇3D场景图生成任务和关系引导实例分割任务中，RelationField达到了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13652">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9c15efc8de0f571320f5595c9e1a3338.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ece9247f79c352994ea0555386769b05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64613d7b3775581e6b56e0d93a08c7e6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EOGS-Gaussian-Splatting-for-Earth-Observation"><a href="#EOGS-Gaussian-Splatting-for-Earth-Observation" class="headerlink" title="EOGS: Gaussian Splatting for Earth Observation"></a>EOGS: Gaussian Splatting for Earth Observation</h2><p><strong>Authors:Luca Savant Aira, Gabriele Facciolo, Thibaud Ehret</strong></p>
<p>Recently, Gaussian splatting has emerged as a strong alternative to NeRF, demonstrating impressive 3D modeling capabilities while requiring only a fraction of the training and rendering time. In this paper, we show how the standard Gaussian splatting framework can be adapted for remote sensing, retaining its high efficiency. This enables us to achieve state-of-the-art performance in just a few minutes, compared to the day-long optimization required by the best-performing NeRF-based Earth observation methods. The proposed framework incorporates remote-sensing improvements from EO-NeRF, such as radiometric correction and shadow modeling, while introducing novel components, including sparsity, view consistency, and opacity regularizations. </p>
<blockquote>
<p>最近，高斯涂抹（Gaussian splatting）作为一种强大的NeRF替代方法崭露头角，展现出令人印象深刻的3D建模能力，同时仅需一小部分训练和渲染时间。在本文中，我们展示了如何适应标准高斯涂抹框架进行遥感，同时保持其高效率。这使得我们能够在几分钟内达到最新技术水平，而无需使用基于NeRF的地球观测方法中表现最佳的长时间优化方法。所提出的框架融入了遥感改进的内容，例如EO-NeRF的辐射校正和阴影建模，同时引入了新颖组件，包括稀疏性、视图一致性和不透明度正则化。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13047v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>近期，高斯涂抹技术成为NeRF的有力替代方案，展现了强大的3D建模能力，且只需一小部分训练和渲染时间。本文展示了如何将标准高斯涂抹框架适应于遥感领域，同时保持其高效率。这使我们在短短几分钟内即可实现卓越性能，而最佳性能的NeRF地球观测方法则需要一整天的优化。所提框架融入了EO-NeRF的遥感改进，如辐射校正和阴影建模，同时引入了新颖组件，包括稀疏性、视图一致性和不透明度正则化。</p>
<p><strong>要点</strong></p>
<ol>
<li>高斯涂抹技术作为NeRF的替代方案，展现出强大的3D建模能力，且效率更高。</li>
<li>提出的框架适应于遥感领域，实现了高效性能。</li>
<li>与最佳性能的NeRF地球观测方法相比，该框架在几分钟内即可实现卓越性能，无需长时间的优化。</li>
<li>融合了EO-NeRF的遥感改进，如辐射校正和阴影建模。</li>
<li>引入了新颖组件，包括稀疏性处理、视图一致性以及不透明度正则化。</li>
<li>所提框架具有广泛的应用前景，可应用于遥感领域的不同场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13047">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b5bf2a63195aa0470042e9f469976617.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a4edfd3e37d721a4c8c4f37584cdfe5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83510c6c0d36d26c11fe3a11c1175e46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f69b867dd19d99f84033e3b42f143806.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GS-ProCams-Gaussian-Splatting-based-Projector-Camera-Systems"><a href="#GS-ProCams-Gaussian-Splatting-based-Projector-Camera-Systems" class="headerlink" title="GS-ProCams: Gaussian Splatting-based Projector-Camera Systems"></a>GS-ProCams: Gaussian Splatting-based Projector-Camera Systems</h2><p><strong>Authors:Qingyue Deng, Jijiang Li, Haibin Ling, Bingyao Huang</strong></p>
<p>We present GS-ProCams, the first Gaussian Splatting-based framework for projector-camera systems (ProCams). GS-ProCams significantly enhances the efficiency of projection mapping (PM) that requires establishing geometric and radiometric mappings between the projector and the camera. Previous CNN-based ProCams are constrained to a specific viewpoint, limiting their applicability to novel perspectives. In contrast, NeRF-based ProCams support view-agnostic projection mapping, however, they require an additional colocated light source and demand significant computational and memory resources. To address this issue, we propose GS-ProCams that employs 2D Gaussian for scene representations, and enables efficient view-agnostic ProCams applications. In particular, we explicitly model the complex geometric and photometric mappings of ProCams using projector responses, the target surface’s geometry and materials represented by Gaussians, and global illumination component. Then, we employ differentiable physically-based rendering to jointly estimate them from captured multi-view projections. Compared to state-of-the-art NeRF-based methods, our GS-ProCams eliminates the need for additional devices, achieving superior ProCams simulation quality. It is also 600 times faster and uses only 1&#x2F;10 of the GPU memory. </p>
<blockquote>
<p>我们提出了GS-ProCams，这是基于高斯拼贴技术的投影仪相机系统（ProCams）的首个框架。GS-ProCams极大地提高了投影映射（PM）的效率，该映射需要在投影仪和相机之间建立几何和辐射度量映射。之前的基于CNN的ProCams受限于特定的视角，限制了其在新型视角的应用。相比之下，基于NeRF的ProCams支持视角无关的投影映射，但它们需要额外的共置光源，并需要大量的计算和内存资源。为了解决这一问题，我们提出了GS-ProCams，它采用二维高斯进行场景表示，并实现了高效的视角无关ProCams应用。特别是，我们通过投影仪响应、由高斯表示的目标表面的几何形状和材料以及全局照明组件，明确地建模了ProCams复杂的几何和光度映射。然后，我们采用基于物理的、可微分的渲染方法，从捕获的多视角投影联合估计它们。与最先进的基于NeRF的方法相比，我们的GS-ProCams不需要额外的设备，实现了卓越的ProCams模拟质量。其速度也更快（高达600倍），并且只使用十分之一的GPU内存。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11762v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于GS-ProCams的高效投影仪相机系统研究摘要：该研究提出了一种基于高斯混合的GS-ProCams框架，用于投影仪相机系统（ProCams）。该方法提高了投影映射的效率，并解决了CNN基ProCams视角限制的问题，支持无视角投影映射。此外，它利用二维高斯进行场景表示，并利用基于物理的渲染技术估计几何和光度映射。相比现有NeRF基方法，GS-ProCams无需额外设备即可实现高质量模拟，同时速度快600倍且使用GPU内存仅十分之一。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GS-ProCams是基于高斯混合的投影仪相机系统框架，提高了投影映射效率。</li>
<li>该方法解决了CNN基ProCams的视角限制问题，支持无视角投影映射。</li>
<li>GS-ProCams使用二维高斯进行场景表示。</li>
<li>通过基于物理的渲染技术估计几何和光度映射。</li>
<li>与现有NeRF基方法相比，GS-ProCams实现高质量模拟，速度快且使用内存少。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11762">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-92ded3759ea49dc651403d3718bd5809.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-006456d917848cc493aebd747e46e79f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d0ca752ad7ad76c678521bfda2a66cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-923a998e40a38fa14091d1aa9a310ddf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2bac7cdf3f89ddb434a3d2b0c769f0e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Sequence-Matters-Harnessing-Video-Models-in-3D-Super-Resolution"><a href="#Sequence-Matters-Harnessing-Video-Models-in-3D-Super-Resolution" class="headerlink" title="Sequence Matters: Harnessing Video Models in 3D Super-Resolution"></a>Sequence Matters: Harnessing Video Models in 3D Super-Resolution</h2><p><strong>Authors:Hyun-kyu Ko, Dongheok Park, Youngin Park, Byeonghyeon Lee, Juhee Han, Eunbyung Park</strong></p>
<p>3D super-resolution aims to reconstruct high-fidelity 3D models from low-resolution (LR) multi-view images. Early studies primarily focused on single-image super-resolution (SISR) models to upsample LR images into high-resolution images. However, these methods often lack view consistency because they operate independently on each image. Although various post-processing techniques have been extensively explored to mitigate these inconsistencies, they have yet to fully resolve the issues. In this paper, we perform a comprehensive study of 3D super-resolution by leveraging video super-resolution (VSR) models. By utilizing VSR models, we ensure a higher degree of spatial consistency and can reference surrounding spatial information, leading to more accurate and detailed reconstructions. Our findings reveal that VSR models can perform remarkably well even on sequences that lack precise spatial alignment. Given this observation, we propose a simple yet practical approach to align LR images without involving fine-tuning or generating ‘smooth’ trajectory from the trained 3D models over LR images. The experimental results show that the surprisingly simple algorithms can achieve the state-of-the-art results of 3D super-resolution tasks on standard benchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets. Project page: <a target="_blank" rel="noopener" href="https://ko-lani.github.io/Sequence-Matters">https://ko-lani.github.io/Sequence-Matters</a> </p>
<blockquote>
<p>三维超分辨率旨在从低分辨率（LR）多视角图像重建高保真三维模型。早期研究主要集中在单图像超分辨率（SISR）模型上，将LR图像上采样为高分辨率图像。然而，这些方法通常缺乏视角一致性，因为它们独立地处理每张图像。尽管已经广泛探索了各种后处理技术来缓解这些不一致性，但它们尚未完全解决这些问题。</p>
</blockquote>
<p>在本文中，我们通过对视频超分辨率（VSR）模型的利用，对三维超分辨率进行了全面的研究。通过利用VSR模型，我们确保了更高的空间一致性，并且可以引用周围的空间信息，从而导致更精确和详细的重建。我们的研究发现，即使在缺乏精确空间对齐的序列上，VSR模型也可以表现得非常出色。鉴于此观察，我们提出了一种简单而实用的方法来对齐LR图像，而无需涉及精细调整或从训练的3D模型在LR图像上生成“平滑”轨迹。实验结果表明，这些出人意料的简单算法可以在标准基准数据集上实现最先进的三维超分辨率任务的结果，如NeRF-synthetic和MipNeRF-360数据集。项目页面：<a target="_blank" rel="noopener" href="https://ko-lani.github.io/Sequence-Matters">https://ko-lani.github.io/Sequence-Matters</a></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11525v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ko-lani.github.io/Sequence-Matters">https://ko-lani.github.io/Sequence-Matters</a></p>
<p><strong>Summary</strong></p>
<p>本文研究了如何利用视频超分辨率（VSR）模型进行3D超分辨率重建。通过对VSR模型的利用，确保了较高的空间一致性，并能参考周围的空间信息，从而得到更准确、更详细的重建结果。实验表明，即使在缺乏精确空间对齐的序列上，VSR模型也能表现出优异的性能。本研究提出了一种简单实用的方法，无需微调或生成平滑轨迹，即可对低分辨率图像进行对齐。在标准数据集上，该算法实现了令人惊讶的3D超分辨率任务的最佳结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究利用视频超分辨率（VSR）模型进行3D超分辨率重建，确保空间一致性和详细性。</li>
<li>VSR模型能参考周围的空间信息，从而得到更准确、更详细的重建结果。</li>
<li>实验表明，VSR模型在缺乏精确空间对齐的序列上也能表现出优异的性能。</li>
<li>提出一种简单实用的方法，无需微调或生成平滑轨迹，即可对低分辨率图像进行对齐。</li>
<li>该方法在标准数据集上实现了3D超分辨率任务的最佳结果。</li>
<li>本研究不仅对3D超分辨率技术做出了重要贡献，而且推动了视频超分辨率模型的进一步应用和发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11525">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ac79f7c413bf349955549303ec87dfdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9775abb384746b27a4fc2a194c545d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-989d39f7b9ea8cecdbb712b167039878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a8ddc004ad248304640951934bbc58f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c25d41832f4dd818b862c5db63c445e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3381673e405e5b6c7f8b507bcacd8eaf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Adapting-Segment-Anything-Model-SAM-to-Experimental-Datasets-via-Fine-Tuning-on-GAN-based-Simulation-A-Case-Study-in-Additive-Manufacturing"><a href="#Adapting-Segment-Anything-Model-SAM-to-Experimental-Datasets-via-Fine-Tuning-on-GAN-based-Simulation-A-Case-Study-in-Additive-Manufacturing" class="headerlink" title="Adapting Segment Anything Model (SAM) to Experimental Datasets via   Fine-Tuning on GAN-based Simulation: A Case Study in Additive Manufacturing"></a>Adapting Segment Anything Model (SAM) to Experimental Datasets via   Fine-Tuning on GAN-based Simulation: A Case Study in Additive Manufacturing</h2><p><strong>Authors:Anika Tabassum, Amirkoushyar Ziabari</strong></p>
<p>Industrial X-ray computed tomography (XCT) is a powerful tool for non-destructive characterization of materials and manufactured components. XCT commonly accompanied by advanced image analysis and computer vision algorithms to extract relevant information from the images. Traditional computer vision models often struggle due to noise, resolution variability, and complex internal structures, particularly in scientific imaging applications. State-of-the-art foundational models, like the Segment Anything Model (SAM)-designed for general-purpose image segmentation-have revolutionized image segmentation across various domains, yet their application in specialized fields like materials science remains under-explored. In this work, we explore the application and limitations of SAM for industrial X-ray CT inspection of additive manufacturing components. We demonstrate that while SAM shows promise, it struggles with out-of-distribution data, multiclass segmentation, and computational efficiency during fine-tuning. To address these issues, we propose a fine-tuning strategy utilizing parameter-efficient techniques, specifically Conv-LoRa, to adapt SAM for material-specific datasets. Additionally, we leverage generative adversarial network (GAN)-generated data to enhance the training process and improve the model’s segmentation performance on complex X-ray CT data. Our experimental results highlight the importance of tailored segmentation models for accurate inspection, showing that fine-tuning SAM on domain-specific scientific imaging data significantly improves performance. However, despite improvements, the model’s ability to generalize across diverse datasets remains limited, highlighting the need for further research into robust, scalable solutions for domain-specific segmentation tasks. </p>
<blockquote>
<p>工业X射线计算机断层扫描（XCT）是非破坏性表征材料和制造部件的强大工具。XCT通常与先进的图像分析和计算机视觉算法相结合，从图像中提取相关信息。由于噪声、分辨率变化和复杂内部结构的影响，传统计算机视觉模型在科学成像应用中经常面临挑战。最先进的基础模型，如用于通用图像分割的Segment Anything Model（SAM），已经推动了各个领域图像分割的革命，但它们在材料科学等特定领域的应用仍然被探索不足。在这项工作中，我们探索了SAM在增材制造部件的工业X射线CT检测中的应用和局限性。我们证明，虽然SAM显示出潜力，但在微调过程中，它在处理离群数据、多类分割和计算效率方面存在困难。为了解决这个问题，我们提出了一种利用参数高效技术的微调策略，特别是Conv-LoRa，来适应材料特定的数据集。此外，我们还利用生成对抗网络（GAN）生成的数据来增强训练过程，提高模型在复杂X射线CT数据上的分割性能。我们的实验结果强调了定制分割模型对于准确检测的重要性，并表明在特定领域的科学成像数据上微调SAM可以显著提高性能。然而，尽管有所改进，模型在不同数据集上的泛化能力仍然有限，这凸显了对针对特定领域的分割任务的稳健、可扩展解决方案的进一步研究的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11381v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文探讨了将先进的计算机视觉模型，如Segment Anything Model（SAM），应用于工业X射线计算机断层扫描（XCT）技术，用于分析制造部件的材料特性。研究展示了SAM在应对复杂内部结构、噪声和分辨率变化时的潜力与局限性，并提出了通过利用Conv-LoRa等参数高效技术进行微调，以及使用生成对抗网络（GAN）生成数据以增强训练过程的方法。实验结果表明，针对特定领域的科学成像数据微调SAM可显著提高性能，但模型的泛化能力仍有待提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Segment Anything Model (SAM) 在工业X射线CT检测中显示出潜力，尤其在分析制造部件的材料特性方面。</li>
<li>SAM在面对噪声、分辨率变化和复杂内部结构时存在挑战。</li>
<li>通过利用参数高效技术（如Conv-LoRa）进行微调，可适应材料特定数据集。</li>
<li>利用生成对抗网络（GAN）生成数据可以增强训练过程，提高模型在复杂X射线CT数据上的分割性能。</li>
<li>实验表明，针对特定领域的科学成像数据微调SAM能显著提高性能。</li>
<li>尽管有所改进，但模型的泛化能力仍然有限，需要在不同数据集上进行更多研究以寻找更稳健、可扩展的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11381">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-68869de46c0cdb7e886a40641ab951d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-732024ecc4fba477e7740c4dac3ffab1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dbbc6e53d60f16b7772eeb25fe38a3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32c059254fe838a182067649ce3ecdf8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ViPOcc-Leveraging-Visual-Priors-from-Vision-Foundation-Models-for-Single-View-3D-Occupancy-Prediction"><a href="#ViPOcc-Leveraging-Visual-Priors-from-Vision-Foundation-Models-for-Single-View-3D-Occupancy-Prediction" class="headerlink" title="ViPOcc: Leveraging Visual Priors from Vision Foundation Models for   Single-View 3D Occupancy Prediction"></a>ViPOcc: Leveraging Visual Priors from Vision Foundation Models for   Single-View 3D Occupancy Prediction</h2><p><strong>Authors:Yi Feng, Yu Han, Xijing Zhang, Tanghui Li, Yanting Zhang, Rui Fan</strong></p>
<p>Inferring the 3D structure of a scene from a single image is an ill-posed and challenging problem in the field of vision-centric autonomous driving. Existing methods usually employ neural radiance fields to produce voxelized 3D occupancy, lacking instance-level semantic reasoning and temporal photometric consistency. In this paper, we propose ViPOcc, which leverages the visual priors from vision foundation models (VFMs) for fine-grained 3D occupancy prediction. Unlike previous works that solely employ volume rendering for RGB and depth image reconstruction, we introduce a metric depth estimation branch, in which an inverse depth alignment module is proposed to bridge the domain gap in depth distribution between VFM predictions and the ground truth. The recovered metric depth is then utilized in temporal photometric alignment and spatial geometric alignment to ensure accurate and consistent 3D occupancy prediction. Additionally, we also propose a semantic-guided non-overlapping Gaussian mixture sampler for efficient, instance-aware ray sampling, which addresses the redundant and imbalanced sampling issue that still exists in previous state-of-the-art methods. Extensive experiments demonstrate the superior performance of ViPOcc in both 3D occupancy prediction and depth estimation tasks on the KITTI-360 and KITTI Raw datasets. Our code is available at: \url{<a target="_blank" rel="noopener" href="https://mias.group/ViPOcc%7D">https://mias.group/ViPOcc}</a>. </p>
<blockquote>
<p>从单幅图像推断场景的三维结构是视觉自主驾驶领域中一个设置不当且具有挑战性的课题。现有方法通常采用神经辐射场来生成体素化的三维占用信息，但缺乏实例级别的语义推理和时间上的光度一致性。在本文中，我们提出了ViPOcc，它利用视觉先验知识从视觉基础模型（VFMs）进行精细粒度的三维占用预测。与仅使用体积渲染进行RGB和深度图像重建的先前工作不同，我们引入了度量深度估计分支，其中提出了逆深度对齐模块来弥合视觉基础模型预测和真实值之间深度分布的域差距。然后利用恢复的度量深度进行时间光度对齐和空间几何对齐，以确保准确且一致的三维占用预测。此外，我们还提出了一种语义引导的非重叠高斯混合采样器，用于高效、实例感知的射线采样，解决了先前最先进的方法中仍然存在的不必要的和不平衡的采样问题。大量实验表明，ViPOcc在KITTI-360和KITTI Raw数据集上的三维占用预测和深度估计任务中均表现出卓越的性能。我们的代码可在：<a target="_blank" rel="noopener" href="https://mias.group/ViPOcc%E3%80%82">https://mias.group/ViPOcc。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11210v1">PDF</a> accepted to AAAI25</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于视觉先验的精细粒度三维占用预测方法ViPOcc，利用视觉基础模型（VFMs）进行单图像三维场景推断。方法包括引入度量深度估计分支和逆深度对齐模块，以缩小VFM预测与真实值之间的深度分布域差距。此外，还提出了语义引导的非重叠高斯混合采样器，解决之前的冗余和不平衡采样问题。在KITTI-360和KITTI Raw数据集上的实验表明，ViPOcc在三维占用预测和深度估计任务上表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViPOcc利用视觉基础模型（VFMs）进行精细粒度的三维占用预测。</li>
<li>引入度量深度估计分支和逆深度对齐模块以改善深度预测的准确性。</li>
<li>语义引导的非重叠高斯混合采样器解决了之前的冗余和不平衡采样问题。</li>
<li>ViPOcc在KITTI-360和KITTI Raw数据集上的实验表现出优越性能。</li>
<li>方法结合了视觉先验、深度估计和语义信息，提高了三维占用预测的准确性。</li>
<li>ViPOcc代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11210">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-de4aee2a4e58c254a0d507ee8e2d190b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cb1161d5887d3e30e2b4b7e296f11c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a05f7cf90808059f4cb7561eb4baab6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c44885d55730f0c014ad8c9d9a78af7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ρ-NeRF-Leveraging-Attenuation-Priors-in-Neural-Radiance-Field-for-3D-Computed-Tomography-Reconstruction"><a href="#ρ-NeRF-Leveraging-Attenuation-Priors-in-Neural-Radiance-Field-for-3D-Computed-Tomography-Reconstruction" class="headerlink" title="$ρ$-NeRF: Leveraging Attenuation Priors in Neural Radiance Field for   3D Computed Tomography Reconstruction"></a>$ρ$-NeRF: Leveraging Attenuation Priors in Neural Radiance Field for   3D Computed Tomography Reconstruction</h2><p><strong>Authors:Li Zhou, Changsheng Fang, Bahareh Morovati, Yongtong Liu, Shuo Han, Yongshun Xu, Hengyong Yu</strong></p>
<p>This paper introduces $\rho$-NeRF, a self-supervised approach that sets a new standard in novel view synthesis (NVS) and computed tomography (CT) reconstruction by modeling a continuous volumetric radiance field enriched with physics-based attenuation priors. The $\rho$-NeRF represents a three-dimensional (3D) volume through a fully-connected neural network that takes a single continuous four-dimensional (4D) coordinate, spatial location $(x, y, z)$ and an initialized attenuation value ($\rho$), and outputs the attenuation coefficient at that position. By querying these 4D coordinates along X-ray paths, the classic forward projection technique is applied to integrate attenuation data across the 3D space. By matching and refining pre-initialized attenuation values derived from traditional reconstruction algorithms like Feldkamp-Davis-Kress algorithm (FDK) or conjugate gradient least squares (CGLS), the enriched schema delivers superior fidelity in both projection synthesis and image recognition. </p>
<blockquote>
<p>本文介绍了$\rho$-NeRF，这是一种自监督方法，通过建立一个基于物理衰减先验的连续体积辐射场，为新型视图合成（NVS）和计算机断层扫描（CT）重建设定了新的标准。$\rho$-NeRF通过全连接神经网络代表一个三维（3D）体积，该网络接收一个连续的四维（4D）坐标（空间位置$(x, y, z)$和初始化的衰减值$\rho$），并输出该位置的衰减系数。通过沿着X射线路径查询这些四维坐标，采用经典的前向投影技术将衰减数据集成到三维空间中。通过匹配和细化源自传统重建算法的预初始化衰减值（如Feldkamp-Davis-Kress算法（FDK）或共轭梯度最小二乘法（CGLS）），丰富了模式在投影合成和图像识别方面均实现了较高的保真度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05322v1">PDF</a> The paper was submitted to CVPR 2025</p>
<p><strong>摘要</strong></p>
<p>本文介绍了ρ-NeRF，这是一种自监督方法，通过建立一个包含物理衰减先验的连续体积辐射场，为新型视图合成（NVS）和计算机断层扫描（CT）重建设定了新的标准。ρ-NeRF通过全连接神经网络代表一个三维体积，该网络接受一个连续的四维坐标（包括空间位置（x，y，z）和一个初始化的衰减值ρ），并输出该位置的衰减系数。通过沿着X射线路径查询这些四维坐标，采用经典的前向投影技术来整合三维空间中的衰减数据。通过匹配和细化从传统的重建算法（如Feldkamp-Davis-Kress算法（FDK）或共轭梯度最小二乘法（CGLS））得出的预初始化衰减值，丰富的模式在投影合成和图像识别方面都提供了更高的保真度。</p>
<p><strong>要点</strong></p>
<ol>
<li>$\rho$-NeRF是一种自监督方法，用于新型视图合成（NVS）和计算机断层扫描（CT）重建。</li>
<li>通过建立一个包含物理衰减先验的连续体积辐射场，提升了质量。</li>
<li>使用全连接神经网络代表三维体积，网络输入四维坐标和初始化衰减值来输出衰减系数。</li>
<li>采用经典的前向投影技术整合四维坐标上的衰减数据。</li>
<li>ρ-NeRF匹配并改进了从传统的重建算法获得的预初始化衰减值。</li>
<li>在投影合成和图像识别方面都表现出了更高的保真度。</li>
<li>这种方法有助于提升CT图像的质量并改善诊断的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05322">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-193210774fb16d3eb566a044a48b1eb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5a73b8a079d02f7632466aad8ee8e7b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f40dce0bb462d9b67f643036fa553ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e4aa25809e4f53fb92f5bc5eaf842c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad8932e732403010809ca6ec5fff6e3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecc4723fe301a0cb434e9a9493486185.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b66a9bd8669da64607891cbd7da39b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Expansive-Supervision-for-Neural-Radiance-Field"><a href="#Expansive-Supervision-for-Neural-Radiance-Field" class="headerlink" title="Expansive Supervision for Neural Radiance Field"></a>Expansive Supervision for Neural Radiance Field</h2><p><strong>Authors:Weixiang Zhang, Shuzhao Xie, Shijia Ge, Wei Yao, Chen Tang, Zhi Wang</strong></p>
<p>Neural Radiance Field (NeRF) has achieved remarkable success in creating immersive media representations through its exceptional reconstruction capabilities. However, the computational demands of dense forward passes and volume rendering during training continue to challenge its real-world applications. In this paper, we introduce Expansive Supervision to reduce time and memory costs during NeRF training from the perspective of partial ray selection for supervision. Specifically, we observe that training errors exhibit a long-tail distribution correlated with image content. Based on this observation, our method selectively renders a small but crucial subset of pixels and expands their values to estimate errors across the entire area for each iteration. Compared to conventional supervision, our approach effectively bypasses redundant rendering processes, resulting in substantial reductions in both time and memory consumption. Experimental results demonstrate that integrating Expansive Supervision within existing state-of-the-art acceleration frameworks achieves 52% memory savings and 16% time savings while maintaining comparable visual quality. </p>
<blockquote>
<p>神经辐射场（NeRF）凭借其出色的重建能力，在创建沉浸式媒体表示方面取得了显著的成功。然而，训练过程中的密集前向传递和体积渲染的计算需求仍然对其实际应用提出了挑战。本文引入扩张监督（Expansive Supervision）从部分射线选择监督的角度减少NeRF训练的时间和内存成本。具体来说，我们观察到训练错误与图像内容呈长尾分布相关。基于此观察，我们的方法选择渲染一小部分但至关重要的像素，并扩大它们的值来估计每次迭代的整个区域的误差。与传统的监督方法相比，我们的方法有效地绕过了冗余的渲染过程，导致时间和内存消耗大幅减少。实验结果证明，将扩张监督集成到最先进的加速框架中，可以实现52%的内存节省和16%的时间节省，同时保持相当的可视质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08056v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>神经网络辐射场（NeRF）在创建沉浸式媒体表示方面取得了显著的成功，但其训练过程中的密集正向传递和体积渲染的计算需求仍然挑战其实际应用。本文引入扩展监督（Expansive Supervision）方法，从部分射线选择的监督角度减少NeRF训练的时间和内存成本。通过选择性渲染一小部分关键像素并扩展其值来估算每个迭代中的整体误差，实现时间和内存的显著减少。实验结果表明，在现有最先进的加速框架内整合扩展监督方法可实现52%的内存节省和16%的时间节省，同时保持相当的可视质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF在创建沉浸式媒体表示方面表现出卓越的重构能力。</li>
<li>训练过程中的密集正向传递和体积渲染是NeRF面临的实际应用挑战。</li>
<li>扩展监督方法通过部分射线选择的监督来减少NeRF训练的时间和内存成本。</li>
<li>该方法通过选择性渲染一小部分关键像素并扩展其值来估算整体误差。</li>
<li>扩展监督方法实现了时间和内存的显著减少。</li>
<li>与现有最先进的加速框架结合，扩展监督方法可在保持相当可视质量的同时实现52%的内存节省和16%的时间节省。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08056">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f133ea54cf7c355b489a1e7043fd1f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ef233e15dfe2d0a4b90dd9c015c100c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f48b889f694388b756223771c0d053df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f883494cf5fbfe669b95807b1a2d24a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c010e5dcbef0a55ca3823bf015c3a00d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e835b3940c9d1278585ab7d2509e6b2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a1976482a5117263b46a432c012ed69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69b2da74c157c0e18ad0b1c59f4ef29b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a969f286c786d45cf93dd75fb9cc2c93.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FisherRF-Active-View-Selection-and-Uncertainty-Quantification-for-Radiance-Fields-using-Fisher-Information"><a href="#FisherRF-Active-View-Selection-and-Uncertainty-Quantification-for-Radiance-Fields-using-Fisher-Information" class="headerlink" title="FisherRF: Active View Selection and Uncertainty Quantification for   Radiance Fields using Fisher Information"></a>FisherRF: Active View Selection and Uncertainty Quantification for   Radiance Fields using Fisher Information</h2><p><strong>Authors:Wen Jiang, Boshu Lei, Kostas Daniilidis</strong></p>
<p>This study addresses the challenging problem of active view selection and uncertainty quantification within the domain of Radiance Fields. Neural Radiance Fields (NeRF) have greatly advanced image rendering and reconstruction, but the cost of acquiring images poses the need to select the most informative viewpoints efficiently. Existing approaches depend on modifying the model architecture or hypothetical perturbation field to indirectly approximate the model uncertainty. However, selecting views from indirect approximation does not guarantee optimal information gain for the model. By leveraging Fisher Information, we directly quantify observed information on the parameters of Radiance Fields and select candidate views by maximizing the Expected Information Gain(EIG). Our method achieves state-of-the-art results on multiple tasks, including view selection, active mapping, and uncertainty quantification, demonstrating its potential to advance the field of Radiance Fields. </p>
<blockquote>
<p>本研究旨在解决Radiance Fields领域内主动视图选择和不确定性量化这一具有挑战性的问题。Neural Radiance Fields（NeRF）在图像渲染和重建方面取得了巨大的进步，但获取图像的成本促使我们需要高效地选择最具有信息量的视角。现有方法依赖于修改模型架构或假设扰动场来间接地近似模型的不确定性。然而，从间接近似中选择视图并不能保证模型获得最优的信息增益。通过利用Fisher信息，我们直接量化Radiance Fields参数上的观测信息，并通过最大化预期信息增益（EIG）选择候选视图。我们的方法在多个任务上实现了最先进的成果，包括视图选择、主动映射和不确定性量化，证明了其在Radiance Fields领域的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.17874v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jiangwenpl.github.io/FisherRF/">https://jiangwenpl.github.io/FisherRF/</a></p>
<p><strong>摘要</strong></p>
<p>本研究探讨了活动视角选择与不确定性量化在光场领域中的难题。神经光场（NeRF）极大地推动了图像渲染和重建的发展，但获取图像的成本促使需要有效选择最具信息量的视角。现有方法依赖于修改模型架构或假设扰动场来间接近似模型不确定性。然而，从间接近似中选择视图并不能保证模型的最佳信息增益。本研究利用Fisher信息直接量化光场参数上的观测信息，通过最大化预期信息增益（EIG）选择候选视图。该方法在多任务上均达到了最新技术水准，包括视角选择、活动映射和不确定性量化，显示了其推动光场领域的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究聚焦于光场领域的活动视角选择和不确定性量化难题。</li>
<li>现有方法间接近似模型不确定性，可能导致信息增益不足。</li>
<li>利用Fisher信息直接量化光场参数观测信息。</li>
<li>通过最大化预期信息增益选择候选视角。</li>
<li>方法在多项任务上表现优异，包括视角选择、活动映射和不确定性量化。</li>
<li>研究结果推动了光场领域的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.17874">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f30c387b7ea7afb49fa82492a1ad8deb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a486c91e5fdcbf296b3d71619573c15e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DynaMoN-Motion-Aware-Fast-and-Robust-Camera-Localization-for-Dynamic-Neural-Radiance-Fields"><a href="#DynaMoN-Motion-Aware-Fast-and-Robust-Camera-Localization-for-Dynamic-Neural-Radiance-Fields" class="headerlink" title="DynaMoN: Motion-Aware Fast and Robust Camera Localization for Dynamic   Neural Radiance Fields"></a>DynaMoN: Motion-Aware Fast and Robust Camera Localization for Dynamic   Neural Radiance Fields</h2><p><strong>Authors:Nicolas Schischka, Hannah Schieber, Mert Asim Karaoglu, Melih Görgülü, Florian Grötzner, Alexander Ladikos, Daniel Roth, Nassir Navab, Benjamin Busam</strong></p>
<p>The accurate reconstruction of dynamic scenes with neural radiance fields is significantly dependent on the estimation of camera poses. Widely used structure-from-motion pipelines encounter difficulties in accurately tracking the camera trajectory when faced with separate dynamics of the scene content and the camera movement. To address this challenge, we propose Dynamic Motion-Aware Fast and Robust Camera Localization for Dynamic Neural Radiance Fields (DynaMoN). DynaMoN utilizes semantic segmentation and generic motion masks to handle dynamic content for initial camera pose estimation and statics-focused ray sampling for fast and accurate novel-view synthesis. Our novel iterative learning scheme switches between training the NeRF and updating the pose parameters for an improved reconstruction and trajectory estimation quality. The proposed pipeline shows significant acceleration of the training process. We extensively evaluate our approach on two real-world dynamic datasets, the TUM RGB-D dataset and the BONN RGB-D Dynamic dataset. DynaMoN improves over the state-of-the-art both in terms of reconstruction quality and trajectory accuracy. We plan to make our code public to enhance research in this area. </p>
<blockquote>
<p>利用神经辐射场对动态场景的精确重建在很大程度上依赖于相机姿态的估计。广泛使用的结构运动管道在面对场景内容的独立动态和相机运动时的轨迹跟踪时遇到困难。为了应对这一挑战，我们提出了动态运动感知快速稳健相机定位用于动态神经辐射场（DynaMoN）。DynaMoN利用语义分割和通用运动蒙版来处理动态内容来进行初始相机姿态估计，并侧重于静态射线采样以快速准确地合成新视角。我们的新颖迭代学习方案在训练NeRF和更新姿态参数之间切换，以提高重建和轨迹估计的质量。所提的管道显著加速了训练过程。我们在两个真实世界的动态数据集——TUM RGB-D数据集和BONN RGB-D动态数据集上广泛评估了我们的方法。DynaMoN在重建质量和轨迹准确性方面都超过了现有技术。我们计划公开我们的代码以促进该领域的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08927v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于动态场景神经辐射场重建中相机姿态估计的重要性，提出一种动态运动感知的快速稳健相机定位方法（DynaMoN）。该方法采用语义分割和通用运动掩膜处理动态内容，进行初始相机姿态估计，并采用静态聚焦射线采样实现快速准确的新视角合成。其迭代学习方案在训练NeRF和更新姿态参数之间切换，提高了重建和轨迹估计质量。DynaMoN显著加速了训练过程，并在两个真实动态数据集上进行了广泛评估，提升了重建质量和轨迹准确性。计划公开代码以促进该领域研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动态场景重建中相机姿态估计的重要性。</li>
<li>DynaMoN方法利用语义分割和通用运动掩膜处理动态内容，用于初始相机姿态估计。</li>
<li>DynaMoN采用静态聚焦射线采样实现快速准确的新视角合成。</li>
<li>迭代学习方案在训练NeRF和更新姿态参数之间切换，提升重建和轨迹估计质量。</li>
<li>DynaMoN显著加速了训练过程。</li>
<li>在两个真实动态数据集上进行了广泛评估，表现出优秀的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.08927">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-810a074ac22370d84e53df4e3aa4db59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81b4c627c658399249d3103086665010.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebc6be52865460924021d63491b294cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c8a579754ed1ff5c823b265e1cef3d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6427f445d847548c13907a50d601c63c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a0af7b29b118ac06ee81c1290eb1677.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e00ab92696c588f8f084b21670fbd7ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfd0d6967822d10c362059c2a3d18867.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Deep-learning-based-radiointerferometric-imaging-with-GAN-aided-training"><a href="#Deep-learning-based-radiointerferometric-imaging-with-GAN-aided-training" class="headerlink" title="Deep learning-based radiointerferometric imaging with GAN-aided training"></a>Deep learning-based radiointerferometric imaging with GAN-aided training</h2><p><strong>Authors:F. Geyer, K. Schmidt, J. Kummer, M. Brüggen, H. W. Edler, D. Elsässer, F. Griese, A. Poggenpohl, L. Rustige, W. Rhode</strong></p>
<p>Radio interferometry invariably suffers from an incomplete coverage of the spatial Fourier space, which leads to imaging artifacts. The current state-of-the-art technique is to create an image by Fourier-transforming the incomplete visibility data and to clean the systematic effects originating from incomplete data in Fourier space. Previously, we have shown how super-resolution methods based on convolutional neural networks can reconstruct sparse visibility data. Our previous work has suffered from a low realism of the training data. The aim of this work is to build a whole simulation chain for realistic radio sources that then leads to a vastly improved neural net for the reconstruction of missing visibilities. This method offers considerable improvements in terms of speed, automatization and reproducibility over the standard techniques. Here we generate large amounts of training data by creating images of radio galaxies with a generative adversarial network (GAN) that has been trained on radio survey data. Then, we applied the Radio Interferometer Measurement Equation (RIME) in order to simulate the measurement process of a radio interferometer. We show that our neural network can reconstruct faithfully images of realistic radio galaxies. The reconstructed images agree well with the original images in terms of the source area, integrated flux density, peak flux density, and the multi-scale structural similarity index. Finally, we show how the neural net can be adapted to estimate the uncertainties in the imaging process. </p>
<blockquote>
<p>射电干涉仪经常面临空间傅里叶变换覆盖不全的问题，从而导致成像失真。目前最先进的技术是通过傅里叶变换不完整可见度数据来创建图像，并清理由傅里叶空间不完整数据引起的系统效应。以前，我们已经展示了基于卷积神经网络的超分辨率方法如何重建稀疏可见度数据。我们之前的工作受到了训练数据真实度低的影响。这项工作的目标是建立一个针对真实射电源的整体仿真链，从而建立一个大大改进的神经网络，用于重建缺失的可见度。这种方法在速度、自动化和可重复性方面提供了对标准技术的重大改进。在这里，我们通过使用生成对抗网络（GAN）创建射电星系图像来生成大量训练数据，该网络已经在射电调查数据上进行了训练。然后，我们应用射电干涉仪测量方程（RIME）来模拟射电干涉仪的测量过程。我们证明了我们的神经网络能够忠实地重建真实的射电星系的图像。重建的图像在源区域、积分流量密度、峰值流量密度和多尺度结构相似性指数方面与原始图像吻合良好。最后，我们展示了如何调整神经网络以估计成像过程中的不确定性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14100v2">PDF</a> Accepted for publication in Astronomy &amp; Astrophysics</p>
<p><strong>Summary</strong></p>
<p>本文介绍了射电干涉仪在成像时存在空间傅里叶变换不完全的问题，导致成像出现伪影。为解决此问题，本文构建了一个完整的模拟链以生成真实的射电源，进而使用基于卷积神经网络的方法重建缺失的可见度数据。该方法使用生成对抗网络（GAN）生成射电波源图像，并利用射电干涉仪测量方程模拟测量过程。实验表明，神经网络能够重建真实的射电波源图像，与原始图像在源面积、积分流量密度、峰值流量密度和多尺度结构相似指数等方面达成良好共识。此外，本文还展示了如何调整神经网络以估计成像过程中的不确定性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>射电干涉仪存在空间傅里叶变换不完全的问题，导致成像伪影。</li>
<li>当前先进的方法是通过傅里叶变换生成图像并清理由不完全数据引起的系统性影响。</li>
<li>之前基于卷积神经网络的方法虽能重建稀疏可见度数据，但训练数据缺乏真实感。</li>
<li>本文旨在构建一个完整的模拟链来生成真实的射电源，进而提高神经网络对缺失可见度的重建能力。</li>
<li>使用生成对抗网络（GAN）生成射电波源图像，并模拟测量过程。</li>
<li>神经网络能够重建真实的射电波源图像，与原始图像在多个方面达成良好共识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.14100">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0ec182cef014095e4e8e80305a18d444.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f973ddc9c9cbc99e9c02cda5c062021.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c26f0823ce38b0c108ec8c562bf712a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb032dbdd8c8eb982b10d899dc62fdd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-236d47d5fb192a7cf433a9ed33efac88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f6793ad5991ab4286f32a1ccd83271d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b23a92c6a6d2683d5bda995f332db456.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16362e7d27d76bb84cc1c227dfb96092.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-92df01ad425ce71e6a3a7789281fc8d0.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-19  AniDoc Animation Creation Made Easier
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a07829a49dd81bb2569cf072e3346f5a.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2024-12-19  GraphAvatar Compact Head Avatars with GNN-Generated 3D Gaussians
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15332k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
