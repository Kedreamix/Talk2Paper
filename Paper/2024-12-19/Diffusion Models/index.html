<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-19  AniDoc Animation Creation Made Easier">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-92df01ad425ce71e6a3a7789281fc8d0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    78 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-19-更新"><a href="#2024-12-19-更新" class="headerlink" title="2024-12-19 更新"></a>2024-12-19 更新</h1><h2 id="AniDoc-Animation-Creation-Made-Easier"><a href="#AniDoc-Animation-Creation-Made-Easier" class="headerlink" title="AniDoc: Animation Creation Made Easier"></a>AniDoc: Animation Creation Made Easier</h2><p><strong>Authors:Yihao Meng, Hao Ouyang, Hanlin Wang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Zhiheng Liu, Yujun Shen, Huamin Qu</strong></p>
<p>The production of 2D animation follows an industry-standard workflow, encompassing four essential stages: character design, keyframe animation, in-betweening, and coloring. Our research focuses on reducing the labor costs in the above process by harnessing the potential of increasingly powerful generative AI. Using video diffusion models as the foundation, AniDoc emerges as a video line art colorization tool, which automatically converts sketch sequences into colored animations following the reference character specification. Our model exploits correspondence matching as an explicit guidance, yielding strong robustness to the variations (e.g., posture) between the reference character and each line art frame. In addition, our model could even automate the in-betweening process, such that users can easily create a temporally consistent animation by simply providing a character image as well as the start and end sketches. Our code is available at: <a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo">https://yihao-meng.github.io/AniDoc_demo</a>. </p>
<blockquote>
<p>二维动画的制作遵循行业标准的工作流程，包括四个基本阶段：角色设计、关键帧动画、中间帧生成和上色。我们的研究聚焦于利用日益强大的生成式人工智能的潜力，降低上述流程中的劳动力成本。以视频扩散模型为基础，AniDoc作为一种视频线艺术彩色化工具应运而生，它会自动将草图序列转换为彩色动画，并按照参考角色规范进行。我们的模型利用对应匹配作为明确指导，对参考角色与每个线艺术框架之间的变化（例如姿势）具有很强的稳健性。此外，我们的模型甚至可以自动化中间帧生成过程，这样用户只需提供角色图像以及开始和结束的草图，就可以轻松创建时间连贯的动画。我们的代码可在：<a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo%E8%AE%BF%E9%97%AE%E3%80%82">https://yihao-meng.github.io/AniDoc_demo访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14173v1">PDF</a> Project page and code: <a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo">https://yihao-meng.github.io/AniDoc_demo</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用视频扩散模型开发的动画生产线工具AniDoc，它能自动将草图序列转化为彩色动画，依据参照角色特性进行色彩填充，并支持自动完成中间过程。工具可实现强鲁棒性，适应角色姿态变化等差异，并简化动画创作流程。相关代码可在指定网址下载。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>使用视频扩散模型开发动画工具AniDoc。</li>
<li>AniDoc可将草图序列自动转化为彩色动画。</li>
<li>AniDoc依据参照角色特性进行色彩填充。<br>4.AniDoc具有强鲁棒性，适应角色姿态变化等差异。</li>
<li>AniDoc可自动完成中间过程，简化动画创作流程。</li>
<li>用户只需提供角色图像以及起始和结束草图，即可轻松创建连贯的动画。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14173">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b643bf9e4a016f462ab739b764265090.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8aaa76958c834fd9840cf11cd36be3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d31db743f7a767ab45a653d120968c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fccbeb9b521b4250ae3e1825e3877013.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e982dabbb02cd0d3a1f9e118e5f9243f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Video-Generation-without-Vector-Quantization"><a href="#Autoregressive-Video-Generation-without-Vector-Quantization" class="headerlink" title="Autoregressive Video Generation without Vector Quantization"></a>Autoregressive Video Generation without Vector Quantization</h2><p><strong>Authors:Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, Xinlong Wang</strong></p>
<p>This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA">https://github.com/baaivision/NOVA</a>. </p>
<blockquote>
<p>本文提出了一种新的方法，能够实现高效的自回归视频生成。我们提议将视频生成问题重新表述为未量化的自回归建模，包括时间上的逐帧预测和空间的逐集预测。与先前自回归模型中的扫描预测或扩散模型中固定长度符号的联合分布建模不同，我们的方法保持了GPT风格模型的因果特性，以提供灵活的上下文能力，同时利用单个帧内的双向建模来提高效率。通过这种方法，我们训练了一种名为NOVA的新型视频自回归模型，无需向量量化。结果表明，NOVA在数据效率、推理速度、视觉保真度和视频流畅性方面超越了先前的自回归视频模型，即使其模型容量更小（即0.6B参数）。此外，NOVA在文本到图像生成任务上也超越了最先进的图像扩散模型，并大大降低了训练成本。NOVA还能够适应长时间的视频，在一个统一模型中实现了多样化的零样本应用。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/baaivision/NOVA上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14169v1">PDF</a> 22 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的方法，能够在高效的情况下实现自回归视频生成。该方法重新定义了视频生成问题，采用非量化自回归建模方式，实现了时序逐帧预测和空间逐集预测。该方法保持了GPT风格模型的因果特性，同时利用单个帧内的双向建模来提高效率。所提出的模型称为NOVA，它在数据效率、推理速度、视觉保真度和视频流畅性方面都超越了先前的自回归视频模型，即使模型容量较小（仅为0.6B参数）。此外，NOVA在文本到图像生成任务中也表现出色，训练成本低。它还能够很好地泛化到更长的视频时长，并在统一模型中实现多样化的零样本应用。模型和代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了一种高效自回归视频生成的新方法。</li>
<li>该方法将视频生成问题重新定义为非量化自回归建模问题，实现时序逐帧和空间逐集预测。</li>
<li>该方法结合GPT风格模型的因果特性和单个帧内的双向建模来提高效率。</li>
<li>NOVA模型在数据效率、推理速度、视觉保真度和视频流畅性方面表现优异，超越现有自回归视频模型。</li>
<li>NOVA在文本到图像生成任务中表现突出，训练成本低。</li>
<li>NOVA具有良好的泛化能力，可应用于更长的视频时长。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14169">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8d514d70d0fc5abba757619c0da01eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2be2a967eec50163628b48aa14d4260f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbf00e6bf06193ddb9cb16fe338bb3c9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VideoDPO-Omni-Preference-Alignment-for-Video-Diffusion-Generation"><a href="#VideoDPO-Omni-Preference-Alignment-for-Video-Diffusion-Generation" class="headerlink" title="VideoDPO: Omni-Preference Alignment for Video Diffusion Generation"></a>VideoDPO: Omni-Preference Alignment for Video Diffusion Generation</h2><p><strong>Authors:Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, Qifeng Chen</strong></p>
<p>Recent progress in generative diffusion models has greatly advanced text-to-video generation. While text-to-video models trained on large-scale, diverse datasets can produce varied outputs, these generations often deviate from user preferences, highlighting the need for preference alignment on pre-trained models. Although Direct Preference Optimization (DPO) has demonstrated significant improvements in language and image generation, we pioneer its adaptation to video diffusion models and propose a VideoDPO pipeline by making several key adjustments. Unlike previous image alignment methods that focus solely on either (i) visual quality or (ii) semantic alignment between text and videos, we comprehensively consider both dimensions and construct a preference score accordingly, which we term the OmniScore. We design a pipeline to automatically collect preference pair data based on the proposed OmniScore and discover that re-weighting these pairs based on the score significantly impacts overall preference alignment. Our experiments demonstrate substantial improvements in both visual quality and semantic alignment, ensuring that no preference aspect is neglected. Code and data will be shared at <a target="_blank" rel="noopener" href="https://videodpo.github.io/">https://videodpo.github.io/</a>. </p>
<blockquote>
<p>生成式扩散模型的最新进展极大地推动了文本到视频的生成。虽然基于大规模、多样化数据集的文本到视频模型可以产生多样化的输出，但这些生成往往偏离用户偏好，突显了对预训练模型进行偏好对齐的必要性。尽管直接偏好优化（DPO）在语言和图像生成中显示出显着改进，但我们首创将其适应视频扩散模型，并通过几个关键调整提出了VideoDPO管道。与以往仅侧重于（i）视觉质量或（ii）文本和视频之间语义对齐的图像对齐方法不同，我们全面考虑这两个维度，并相应地构建偏好分数，我们称之为OmniScore。我们设计了一个管道，基于提出的OmniScore自动收集偏好对数据，并发现根据该分数重新权重这些对会显著影响整体的偏好对齐。我们的实验表明，在视觉质量和语义对齐方面都有实质性的改进，确保不会忽略任何偏好方面。相关代码和数据将在<a target="_blank" rel="noopener" href="https://videodpo.github.io/%E5%85%B1%E4%BA%AB%E3%80%82">https://videodpo.github.io/共享。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14167v1">PDF</a> </p>
<p><strong>Summary</strong><br>     生成式扩散模型的最新进展极大地推动了文本到视频的生成。尽管在大型、多样化的数据集上训练的文本到视频模型可以产生多样化的输出，但这些输出常常偏离用户偏好，突显了对预训练模型进行偏好调整的必要性。我们率先将直接偏好优化（DPO）适应到视频扩散模型，并提出VideoDPO管道，通过几个关键调整来实现。不同于之前只关注视觉质量或文本与视频之间语义对齐的图像对齐方法，我们全面考虑这两个维度，并据此构建了一个偏好得分，称为OmniScore。我们设计了一个管道来基于提出的OmniScore自动收集偏好对数据，并发现根据得分重新加权这些对可以显著影响偏好对齐的整体效果。我们的实验表明，在视觉质量和语义对齐方面都有显著改进，确保不会忽略任何偏好方面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到视频生成领域受益于生成式扩散模型的最新进展。</li>
<li>文本到视频模型的输出常偏离用户偏好，需要调整预训练模型的偏好对齐。</li>
<li>直接偏好优化（DPO）被首次适应到视频扩散模型中，形成VideoDPO管道。</li>
<li>VideoDPO通过全面考虑视觉质量和语义对齐来构建OmniScore偏好得分。</li>
<li>自动收集基于OmniScore的偏好对数据显示重新加权对可以影响偏好对齐的整体效果。</li>
<li>实验证明VideoDPO在视觉质量和语义对齐方面都有显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14167">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9e00a6b4509b3bd36128cd87aaf0833e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f876b5c01df4a8b0ff607e4a0e29bae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30787fbeb8f89e953ca8213825486234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-616b9555da1372418f12fc80dc21d17e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78108678dc8ecce823dd3448a991a689.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Text2Relight-Creative-Portrait-Relighting-with-Text-Guidance"><a href="#Text2Relight-Creative-Portrait-Relighting-with-Text-Guidance" class="headerlink" title="Text2Relight: Creative Portrait Relighting with Text Guidance"></a>Text2Relight: Creative Portrait Relighting with Text Guidance</h2><p><strong>Authors:Junuk Cha, Mengwei Ren, Krishna Kumar Singh, He Zhang, Yannick Hold-Geoffroy, Seunghyun Yoon, HyunJoon Jung, Jae Shin Yoon, Seungryul Baek</strong></p>
<p>We present a lighting-aware image editing pipeline that, given a portrait image and a text prompt, performs single image relighting. Our model modifies the lighting and color of both the foreground and background to align with the provided text description. The unbounded nature in creativeness of a text allows us to describe the lighting of a scene with any sensory features including temperature, emotion, smell, time, and so on. However, the modeling of such mapping between the unbounded text and lighting is extremely challenging due to the lack of dataset where there exists no scalable data that provides large pairs of text and relighting, and therefore, current text-driven image editing models does not generalize to lighting-specific use cases. We overcome this problem by introducing a novel data synthesis pipeline: First, diverse and creative text prompts that describe the scenes with various lighting are automatically generated under a crafted hierarchy using a large language model (<em>e.g.,</em> ChatGPT). A text-guided image generation model creates a lighting image that best matches the text. As a condition of the lighting images, we perform image-based relighting for both foreground and background using a single portrait image or a set of OLAT (One-Light-at-A-Time) images captured from lightstage system. Particularly for the background relighting, we represent the lighting image as a set of point lights and transfer them to other background images. A generative diffusion model learns the synthesized large-scale data with auxiliary task augmentation (<em>e.g.,</em> portrait delighting and light positioning) to correlate the latent text and lighting distribution for text-guided portrait relighting. </p>
<blockquote>
<p>我们提出了一种感知照明的图像编辑流程。给定肖像图像和文字提示，该流程可以对单幅图像进行重新照明。我们的模型会修改前景和背景的照明和颜色，以符合提供的文字描述。文本的无界创造性使我们能够描述任何感觉特征的场景照明，包括温度、情感、气味、时间等等。然而，在文本和照明之间进行此类映射的建模极具挑战性，因为缺乏大型数据集提供大量的文本和重新照明配对。因此，当前的文本驱动图像编辑模型并不适用于特定的照明用例。我们通过引入新型数据合成流程来解决这一问题：首先，在一个精心设计的层次结构下，使用大型语言模型（例如ChatGPT）自动生成描述各种照明的场景的各种创意文本提示。文本引导的图像生成模型创建与文本最佳匹配的照明图像。作为照明图像的条件，我们使用单幅肖像图像或从灯光舞台系统捕获的一组OLAT（一次一个灯光）图像对前景和背景进行基于图像的重新照明。特别是针对背景重新照明，我们将照明图像表示为一组点光源并将其转移到其他背景图像上。生成扩散模型通过辅助任务增强（例如肖像照明和灯光定位）来学习合成的大规模数据，从而关联潜在文本和照明分布，实现文本引导的肖像重新照明。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13734v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种基于文本提示的图像编辑流程，它可以根据提供的肖像图像和文本提示进行单图像重照明。该模型能够修改前景和背景的光照和颜色，以符合给定的文本描述。文本描述的创意无界性允许我们描述场景的任何感官特征，如温度、情感、气味、时间等。然而，将这种无界的文本与照明之间的映射建模极具挑战性，因为缺乏相应的数据集，现有的文本驱动图像编辑模型无法推广到照明特定的用例。为解决此问题，我们引入了一种新颖的数据合成流程：使用大型语言模型（如ChatGPT）在精心设计的层次结构下自动生成描述场景各种照明的多样化创意文本提示。基于文本指导的图像生成模型创建与文本最佳匹配的照明图像。作为照明图像的条件，我们对前景和背景进行了基于图像的重新照明，使用单个肖像图像或由光舞台系统捕获的一组OLAT（一次一个灯光）图像。特别是针对背景重新照明，我们将照明图像表示为一组点光源并将其转移到其他背景图像上。生成扩散模型通过辅助任务增强（如肖像重照明和灯光定位）来学习合成的大规模数据，从而关联潜在文本和照明分布，以实现文本引导的肖像重新照明。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>介绍了基于文本提示的照明感知图像编辑流程，能够实现单图像重照明。</li>
<li>模型能够修改前景和背景的光照和颜色以符合文本描述。</li>
<li>创意无界的文本描述允许描述场景的多种感官特征。</li>
<li>数据集缺乏是文本与照明映射建模的主要挑战。</li>
<li>通过数据合成流程克服此挑战，包括使用大型语言模型自动生成文本提示。</li>
<li>使用文本指导的图像生成模型创建与文本匹配的照明图像。</li>
<li>通过图像基于条件的重照明技术处理前景和背景，利用生成扩散模型学习大规模数据以实现文本引导的肖像重新照明。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13734">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a97dbf6bd263cdb96aef5fc1abfb6356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccf4a0b928ec891a2074e47b36508c9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cb443233d9e8a99f1b5980d64d672e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a588c2ad715c2e80adab7bfdd417fed0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d36c3edd7048ce9e3a4480745b1d9200.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03b1e6dd5392d9bb541629e137d8956d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VIIS-Visible-and-Infrared-Information-Synthesis-for-Severe-Low-light-Image-Enhancement"><a href="#VIIS-Visible-and-Infrared-Information-Synthesis-for-Severe-Low-light-Image-Enhancement" class="headerlink" title="VIIS: Visible and Infrared Information Synthesis for Severe Low-light   Image Enhancement"></a>VIIS: Visible and Infrared Information Synthesis for Severe Low-light   Image Enhancement</h2><p><strong>Authors:Chen Zhao, Mengyuan Yu, Fan Yang, Peiguang Jing</strong></p>
<p>Images captured in severe low-light circumstances often suffer from significant information absence. Existing singular modality image enhancement methods struggle to restore image regions lacking valid information. By leveraging light-impervious infrared images, visible and infrared image fusion methods have the potential to reveal information hidden in darkness. However, they primarily emphasize inter-modal complementation but neglect intra-modal enhancement, limiting the perceptual quality of output images. To address these limitations, we propose a novel task, dubbed visible and infrared information synthesis (VIIS), which aims to achieve both information enhancement and fusion of the two modalities. Given the difficulty in obtaining ground truth in the VIIS task, we design an information synthesis pretext task (ISPT) based on image augmentation. We employ a diffusion model as the framework and design a sparse attention-based dual-modalities residual (SADMR) conditioning mechanism to enhance information interaction between the two modalities. This mechanism enables features with prior knowledge from both modalities to adaptively and iteratively attend to each modality’s information during the denoising process. Our extensive experiments demonstrate that our model qualitatively and quantitatively outperforms not only the state-of-the-art methods in relevant fields but also the newly designed baselines capable of both information enhancement and fusion. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Chenz418/VIIS">https://github.com/Chenz418/VIIS</a>. </p>
<blockquote>
<p>在严重低光环境下捕捉的图像通常缺乏重要信息。现有的单一模态图像增强方法在恢复缺乏有效信息图像区域方面表现困难。通过利用不受光线影响的红外图像，可见光和红外图像融合方法具有揭示隐藏在黑暗中的信息的潜力。然而，它们主要侧重于跨模态互补，而忽略了跨模态内的增强，这限制了输出图像的感知质量。为了解决这些局限性，我们提出了一个新的任务，称为可见光和红外信息合成（VIIS），旨在实现两个模态的信息增强和融合。考虑到在VIIS任务中获取真实标签的难度，我们基于图像增强设计了一个信息合成预训练任务（ISPT）。我们采用扩散模型作为框架，并设计了一种基于稀疏注意力的双模态残差（SADMR）条件机制，以增强两个模态之间的信息交互。该机制使来自两个模态的先验知识特征能够自适应地迭代关注每个模态的信息去噪过程。我们的大量实验表明，我们的模型不仅在相关领域最先进的方法上表现优越，而且在同时具备信息增强和融合能力的新设计基准上也有出色表现。代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/Chenz418/VIIS">https://github.com/Chenz418/VIIS</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13655v1">PDF</a> Accepted to WACV 2025</p>
<p><strong>Summary</strong><br>     针对低光环境下图像信息缺失的问题，现有图像增强方法难以恢复缺失信息。通过结合红外图像，可见光和红外图像融合方法能够揭示隐藏信息。但现有方法主要关注跨模态互补，忽视单模态内增强，影响输出图像感知质量。为此，提出可见光和红外信息合成（VIIS）任务，旨在实现信息增强与双模态融合。针对VIIS任务难以获取真实标注的问题，设计基于图像增强的信息合成预训练任务（ISPT）。采用扩散模型框架，并设计基于稀疏注意力的双模态残差（SADMR）条件机制，增强两模态间的信息交互。实验证明，该模型在相关领域中不仅优于最新方法，而且超越仅具备信息增强和融合功能的新设计基准模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>低光环境下的图像存在信息缺失问题，现有图像增强方法难以解决。</li>
<li>通过结合红外图像，可见光和红外图像融合方法可以揭示隐藏信息。</li>
<li>现有方法主要关注跨模态互补，忽视单模态内的增强，影响输出图像质量。</li>
<li>提出可见光和红外信息合成（VIIS）任务，旨在实现信息增强与双模态融合。</li>
<li>针对VIIS任务难以获取真实标注的问题，设计基于图像增强的信息合成预训练任务（ISPT）。</li>
<li>采用扩散模型框架，并设计SADMR条件机制以增强两模态间的信息交互。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-56679ebe286bc200a5de62a2a69de24a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9e1c6deda7aa914b89f17d5bc207f03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e5f7e8ca525fa0277d0790ae1dc9fc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d667ae4a456350597b52a1556071a8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b1a32ad7254776f74a9080364fe4183.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42094ded1273105cc0528506dc8ddb15.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Real-time-One-Step-Diffusion-based-Expressive-Portrait-Videos-Generation"><a href="#Real-time-One-Step-Diffusion-based-Expressive-Portrait-Videos-Generation" class="headerlink" title="Real-time One-Step Diffusion-based Expressive Portrait Videos Generation"></a>Real-time One-Step Diffusion-based Expressive Portrait Videos Generation</h2><p><strong>Authors:Hanzhong Guo, Hongwei Yi, Daquan Zhou, Alexander William Bergman, Michael Lingelbach, Yizhou Yu</strong></p>
<p>Latent diffusion models have made great strides in generating expressive portrait videos with accurate lip-sync and natural motion from a single reference image and audio input. However, these models are far from real-time, often requiring many sampling steps that take minutes to generate even one second of video-significantly limiting practical use. We introduce OSA-LCM (One-Step Avatar Latent Consistency Model), paving the way for real-time diffusion-based avatars. Our method achieves comparable video quality to existing methods but requires only one sampling step, making it more than 10x faster. To accomplish this, we propose a novel avatar discriminator design that guides lip-audio consistency and motion expressiveness to enhance video quality in limited sampling steps. Additionally, we employ a second-stage training architecture using an editing fine-tuned method (EFT), transforming video generation into an editing task during training to effectively address the temporal gap challenge in single-step generation. Experiments demonstrate that OSA-LCM outperforms existing open-source portrait video generation models while operating more efficiently with a single sampling step. </p>
<blockquote>
<p>潜在扩散模型在利用单一参考图像和音频输入生成表情丰富的肖像视频方面取得了巨大的进步，这些视频具有准确的唇音同步和自然动作。然而，这些模型还远远达不到实时标准，通常需要许多采样步骤，甚至生成一秒钟的视频也需要数分钟的时间，从而极大地限制了其实际使用。我们引入了OSA-LCM（一步式化身潜在一致性模型），为基于扩散的实时化身技术铺平了道路。我们的方法达到了与现有方法相当的视频质量，但仅需一个采样步骤，使其速度超过现有方法十倍以上。为了实现这一点，我们提出了一种新型化身鉴别器设计，该设计用于指导唇音一致性和动作表现力，在有限的采样步骤中提高视频质量。此外，我们采用第二阶段训练架构，使用编辑微调方法（EFT），在训练过程中将视频生成转变为编辑任务，以有效解决单步生成中的时间间隔挑战。实验表明，OSA-LCM在性能上超越了现有的开源肖像视频生成模型，同时凭借单个采样步骤实现了更高效的操作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13479v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>基于潜在扩散模型的技术，在仅使用单张参考图像和音频输入的情况下，生成具有精确唇同步和自然动作的表达性肖像视频方面取得了显著进展。然而，这些模型的生成过程并非实时，需要大量采样步骤，甚至生成一秒视频都需要数分钟时间，这极大地限制了其实际使用。我们推出OSA-LCM（一步式化身潜在一致性模型），为基于扩散的实时化身技术铺平了道路。我们的方法实现了与现有方法相当的视频质量，但仅需一个采样步骤，使其速度超过现有技术十倍以上。通过设计新型化身鉴别器，引导唇音频一致性和动作表达性，在有限的采样步骤中提高视频质量。此外，我们采用第二阶段训练架构，使用编辑微调方法（EFT），将视频生成转变为训练过程中的编辑任务，有效解决单步生成中的时间间隔挑战。实验表明，OSA-LCM在肖像视频生成方面优于现有开源模型，同时以单步采样实现更高效的操作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>潜在扩散模型在生成肖像视频方面取得了显著进展，能够基于单张参考图像和音频输入生成具有精确唇同步和自然动作的视频。</li>
<li>现有模型生成过程非实时，需要大量采样步骤，限制了实际应用。</li>
<li>OSA-LCM模型引入一步式化身潜在一致性模型，实现实时扩散式化身技术。</li>
<li>OSA-LCM通过设计新型化身鉴别器，提高视频质量，实现与现有方法相当的视频效果。</li>
<li>OSA-LCM采用编辑微调方法（EFT），将视频生成转变为训练过程中的编辑任务，解决单步生成中的时间间隔挑战。</li>
<li>实验证明，OSA-LCM在肖像视频生成方面优于现有开源模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13479">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d914279391907ce9b69f3053974749b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4eeaedbc45eaf6982ca87257e52026f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eadf55da456f403ed88f8c4788515d0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f30508cf7a229128760bc3f0a00a3179.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8234aa9c31a830b21a5dedaad83ce0d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Low-Light-Image-Enhancement-with-Diffusion-Prior"><a href="#Zero-Shot-Low-Light-Image-Enhancement-with-Diffusion-Prior" class="headerlink" title="Zero-Shot Low Light Image Enhancement with Diffusion Prior"></a>Zero-Shot Low Light Image Enhancement with Diffusion Prior</h2><p><strong>Authors:Joshua Cho, Sara Aghajanzadeh, Zhen Zhu, D. A. Forsyth</strong></p>
<p>Balancing aesthetic quality with fidelity when enhancing images from challenging, degraded sources is a core objective in computational photography. In this paper, we address low light image enhancement (LLIE), a task in which dark images often contain limited visible information. Diffusion models, known for their powerful image enhancement capacities, are a natural choice for this problem. However, their deep generative priors can also lead to hallucinations, introducing non-existent elements or substantially altering the visual semantics of the original scene. In this work, we introduce a novel zero-shot method for controlling and refining the generative behavior of diffusion models for dark-to-light image conversion tasks. Our method demonstrates superior performance over existing state-of-the-art methods in the task of low-light image enhancement, as evidenced by both quantitative metrics and qualitative analysis. </p>
<blockquote>
<p>在计算摄影中，平衡美学质量与保真度，同时对来自具有挑战性的退化源的图像进行增强，是核心目标。本文我们研究低光图像增强（LLIE）问题，暗图像往往包含有限的可视信息。扩散模型以其强大的图像增强能力而著称，因此是此问题的自然选择。然而，它们的深度生成先验也可能导致幻觉，引入不存在的元素或大幅改变原始场景的可视语义。在这项工作中，我们介绍了一种用于控制和细化扩散模型生成行为的新型零样本方法，用于暗到亮的图像转换任务。我们的方法在暗光图像增强任务中的性能优于现有最先进的方法，这由定量指标和定性分析均证明。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13401v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文聚焦于低光图像增强任务，旨在平衡美学质量与保真度。虽然扩散模型在图像增强方面表现出强大的能力，但也可能产生幻觉，引入不存在的元素或大幅改变原始场景的视觉语义。本文提出了一种新型的零样本方法，用于控制和优化扩散模型在暗光图像转换任务中的生成行为，并在低光图像增强任务中展现出优于现有先进方法的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文关注低光图像增强任务，重点是在挑战性的退化图像源中平衡美学质量与保真度。</li>
<li>扩散模型因其强大的图像增强能力而被选为解决此问题的自然选择，但也存在生成幻觉的问题。</li>
<li>本文提出了一种新型的零样本方法，用于控制和优化扩散模型在暗光图像转换任务中的生成行为。</li>
<li>该方法通过定量指标和定性分析证明了在低光图像增强任务中优于现有先进方法的表现。</li>
<li>扩散模型在图像生成方面具有强大的潜力，但也需要进一步研究和改进来控制其生成行为。</li>
<li>对于暗光图像转换任务，未来的研究可以探索更多的控制方法和优化策略来提高图像的质量和保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13401">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a89f5d1b46395c0996cf7593218c332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58775cca4ea2b4808f799d5bea37b75e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dff77926116c4166c96373633f710b32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-415ce6a784ca54ee6b5db2dcd38a33c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-681d37375584438b45fe645abf0b9902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1250d0476c3b60901c31a600537c67e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Marigold-DC-Zero-Shot-Monocular-Depth-Completion-with-Guided-Diffusion"><a href="#Marigold-DC-Zero-Shot-Monocular-Depth-Completion-with-Guided-Diffusion" class="headerlink" title="Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion"></a>Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion</h2><p><strong>Authors:Massimiliano Viola, Kevin Qu, Nando Metzger, Bingxin Ke, Alexander Becker, Konrad Schindler, Anton Obukhov</strong></p>
<p>Depth completion upgrades sparse depth measurements into dense depth maps guided by a conventional image. Existing methods for this highly ill-posed task operate in tightly constrained settings and tend to struggle when applied to images outside the training domain or when the available depth measurements are sparse, irregularly distributed, or of varying density. Inspired by recent advances in monocular depth estimation, we reframe depth completion as an image-conditional depth map generation guided by sparse measurements. Our method, Marigold-DC, builds on a pretrained latent diffusion model for monocular depth estimation and injects the depth observations as test-time guidance via an optimization scheme that runs in tandem with the iterative inference of denoising diffusion. The method exhibits excellent zero-shot generalization across a diverse range of environments and handles even extremely sparse guidance effectively. Our results suggest that contemporary monocular depth priors greatly robustify depth completion: it may be better to view the task as recovering dense depth from (dense) image pixels, guided by sparse depth; rather than as inpainting (sparse) depth, guided by an image. Project website: <a target="_blank" rel="noopener" href="https://marigolddepthcompletion.github.io/">https://MarigoldDepthCompletion.github.io/</a> </p>
<blockquote>
<p>深度补全将稀疏的深度测量值升级为受常规图像引导的密集深度图。针对这一高度不适定的任务，现有方法通常在严格受限的环境中运行，当应用于训练域外的图像或可用的深度测量值稀疏、分布不均或密度不一的情况下，往往会遇到困难。受单目深度估计的最新进展的启发，我们将深度补全重新定位为基于稀疏测量引导的受图像条件控制的深度图生成。我们的方法Marigold-DC建立在用于单目深度估计的预训练潜在扩散模型的基础上，通过一种优化方案在去除噪声扩散的迭代推断过程中注入深度观测值作为测试时的指导。该方法在多种环境中展现出出色的零样本泛化能力，并能有效地处理极其稀疏的指导。我们的结果表明，当代的单目深度先验知识极大地增强了深度补全的稳健性：可能更好的做法是将其视为从（密集）图像像素中恢复密集深度，受稀疏深度引导；而不是将任务视为在图像引导下填充（稀疏）深度。项目网站：<a target="_blank" rel="noopener" href="https://marigolddepthcompletion.github.io/">https://MarigoldDepthCompletion.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13389v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Marigold-DC的深度完成方法，它将深度完成任务重新定义为在稀疏测量指导下，基于图像的深度图生成。该方法利用预训练的潜在扩散模型，通过优化方案在测试时注入深度观测值，与去噪扩散的迭代推理并行运行。该方法具有良好的零样本泛化能力，可处理各种环境，并可有效处理极其稀疏的指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Marigold-DC方法将深度完成定义为在稀疏测量指导下，基于图像的深度图生成。</li>
<li>它利用预训练的潜在扩散模型，注入深度观测值作为测试时的指导。</li>
<li>Marigold-DC通过优化方案与去噪扩散的迭代推理并行运行。</li>
<li>该方法具有良好的零样本泛化能力，能在各种环境中表现良好。</li>
<li>Marigold-DC能有效处理极其稀疏的指导情况。</li>
<li>现有深度完成方法往往局限于特定环境或训练域，而Marigold-DC具有更强的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13389">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8c6f17604407b4d7f0de80b92a9bbea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05169c8bc9fd83097ace96381787fda4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-019bf016d910b28e2358f68a20756030.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Optimized-two-stage-AI-based-Neural-Decoding-for-Enhanced-Visual-Stimulus-Reconstruction-from-fMRI-Data"><a href="#Optimized-two-stage-AI-based-Neural-Decoding-for-Enhanced-Visual-Stimulus-Reconstruction-from-fMRI-Data" class="headerlink" title="Optimized two-stage AI-based Neural Decoding for Enhanced Visual   Stimulus Reconstruction from fMRI Data"></a>Optimized two-stage AI-based Neural Decoding for Enhanced Visual   Stimulus Reconstruction from fMRI Data</h2><p><strong>Authors:Lorenzo Veronese, Andrea Moglia, Luca Mainardi, Pietro Cerveri</strong></p>
<p>AI-based neural decoding reconstructs visual perception by leveraging generative models to map brain activity, measured through functional MRI (fMRI), into latent hierarchical representations. Traditionally, ridge linear models transform fMRI into a latent space, which is then decoded using latent diffusion models (LDM) via a pre-trained variational autoencoder (VAE). Due to the complexity and noisiness of fMRI data, newer approaches split the reconstruction into two sequential steps, the first one providing a rough visual approximation, the second on improving the stimulus prediction via LDM endowed by CLIP embeddings. This work proposes a non-linear deep network to improve fMRI latent space representation, optimizing the dimensionality alike. Experiments on the Natural Scenes Dataset showed that the proposed architecture improved the structural similarity of the reconstructed image by about 2% with respect to the state-of-the-art model, based on ridge linear transform. The reconstructed image’s semantics improved by about 4%, measured by perceptual similarity, with respect to the state-of-the-art. The noise sensitivity analysis of the LDM showed that the role of the first stage was fundamental to predict the stimulus featuring high structural similarity. Conversely, providing a large noise stimulus affected less the semantics of the predicted stimulus, while the structural similarity between the ground truth and predicted stimulus was very poor. The findings underscore the importance of leveraging non-linear relationships between BOLD signal and the latent representation and two-stage generative AI for optimizing the fidelity of reconstructed visual stimuli from noisy fMRI data. </p>
<blockquote>
<p>基于人工智能的神经网络解码通过利用生成模型将功能磁共振成像（fMRI）测量的脑活动映射到潜在层次表示，从而重建视觉感知。传统上，岭线性模型将fMRI转换为潜在空间，然后使用预训练的变分自编码器（VAE）通过潜在扩散模型（LDM）进行解码。由于fMRI数据的复杂性和噪声，更新的方法将重建过程分为两个连续步骤，第一个步骤提供粗略的视觉近似，第二个步骤通过LDM和CLIP嵌入改进刺激预测。这项工作提出了一个非线性深度网络来改善fMRI的潜在空间表示，同时优化维度。在自然场景数据集上的实验表明，与基于岭线性变换的最先进模型相比，所提出的架构改进了重建图像的结构相似性约2%。在感知相似性方面，重建图像的语义改进了约4%。LDM的噪声敏感性分析表明，第一阶段在预测具有高结构相似性的刺激方面起着至关重要的作用。相反，提供大噪声刺激对预测刺激的语义影响较小，而地面真实数据和预测刺激之间的结构相似性非常差。这些发现强调了利用BOLD信号和潜在表示之间的非线性关系以及两阶段生成人工智能在优化从噪声fMRI数据中重建的视觉刺激保真度方面的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13237v1">PDF</a> 14 pages, 5 figures</p>
<p><strong>摘要</strong><br>    基于人工智能的神经网络解码通过利用生成模型将功能磁共振成像（fMRI）测量的脑活动映射到潜在层次表示中，重建视觉感知。本研究提出一种非线性深度网络，优化fMRI的潜在空间表示，在维度上实现优化。实验表明，与传统的基于岭线性变换的模型相比，该架构提高了重建图像的构造相似性约2%，在感知相似性方面也提高了约4%。对LDM的噪声敏感性分析显示，第一阶段在预测具有高强度构造相似性的刺激时起着至关重要的作用。相反，提供大量噪声刺激对预测刺激的语义影响较小，而地面真实数据与预测刺激之间的构造相似性则较差。研究强调了利用BOLD信号和潜在表现之间的非线性关系以及两阶段生成人工智能在优化从噪声fMRI数据中重建的视觉刺激保真度中的重要性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>AI-based neural decoding利用生成模型映射fMRI数据到潜在层次表示，重建视觉感知。</li>
<li>提出非线性深度网络改进fMRI的潜在空间表示，优化维度。</li>
<li>与现有模型相比，新架构提高了重建图像的构造相似性约2%，感知相似性约4%。</li>
<li>LDM的噪声敏感性分析显示第一阶段在预测高强度构造相似性刺激时至关重要。</li>
<li>噪声对预测刺激的语义影响较小，但对构造相似性的影响较大。</li>
<li>研究强调了利用BOLD信号和潜在表现之间的非线性关系的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13237">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-716f23b2c3ee304a44b74deba41e41b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96fdbfb4127e0c56c0ec253bf43cb5bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ab08cae9d525ab76c69706147b5ee74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2c8e1d57dc2f8555303bde065042c9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-058e48cb19ea3cb3fcd7b9ee92061e80.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CoMPaSS-Enhancing-Spatial-Understanding-in-Text-to-Image-Diffusion-Models"><a href="#CoMPaSS-Enhancing-Spatial-Understanding-in-Text-to-Image-Diffusion-Models" class="headerlink" title="CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion   Models"></a>CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion   Models</h2><p><strong>Authors:Gaoyang Zhang, Bingtao Fu, Qingnan Fan, Qi Zhang, Runxing Liu, Hong Gu, Huaqi Zhang, Xinguo Liu</strong></p>
<p>Text-to-image diffusion models excel at generating photorealistic images, but commonly struggle to render accurate spatial relationships described in text prompts. We identify two core issues underlying this common failure: 1) the ambiguous nature of spatial-related data in existing datasets, and 2) the inability of current text encoders to accurately interpret the spatial semantics of input descriptions. We address these issues with CoMPaSS, a versatile training framework that enhances spatial understanding of any T2I diffusion model. CoMPaSS solves the ambiguity of spatial-related data with the Spatial Constraints-Oriented Pairing (SCOP) data engine, which curates spatially-accurate training data through a set of principled spatial constraints. To better exploit the curated high-quality spatial priors, CoMPaSS further introduces a Token ENcoding ORdering (TENOR) module to allow better exploitation of high-quality spatial priors, effectively compensating for the shortcoming of text encoders. Extensive experiments on four popular open-weight T2I diffusion models covering both UNet- and MMDiT-based architectures demonstrate the effectiveness of CoMPaSS by setting new state-of-the-arts with substantial relative gains across well-known benchmarks on spatial relationships generation, including VISOR (+98%), T2I-CompBench Spatial (+67%), and GenEval Position (+131%). Code will be available at <a target="_blank" rel="noopener" href="https://github.com/blurgyy/CoMPaSS">https://github.com/blurgyy/CoMPaSS</a>. </p>
<blockquote>
<p>文本到图像的扩散模型在生成逼真的图像方面表现出色，但在呈现文本提示中描述的空间关系时通常遇到困难。我们确定了这种常见失败背后的两个核心问题：1）现有数据集中空间相关数据的模糊性质；2）当前文本编码器无法准确解释输入描述的空间语义。我们通过CoMPaSS解决这些问题，这是一种通用的训练框架，可以提高任何T2I扩散模型的空间理解能力。CoMPaSS通过面向空间约束配对（SCOP）数据引擎解决空间相关数据的模糊性问题，该引擎通过一系列有原则的空间约束来策划空间精确的训练数据。为了更好地利用精选的高质量空间先验知识，CoMPaSS还引入了Token ENcoding ORdering（TENOR）模块，允许更有效地利用高质量空间先验知识，有效地弥补了文本编码器的不足。在四个流行的开放权重T2I扩散模型上进行的广泛实验，涵盖了基于UNet和MMDiT的架构，证明了CoMPaSS的有效性。通过在空间关系生成方面设置新的最新技术，并在VISOR（+98%）、T2I-CompBench Spatial（+67%）和GenEval Position（+131%）等基准测试上实现相对较大的提升，证明了其效果。代码将在<a target="_blank" rel="noopener" href="https://github.com/blurgyy/CoMPaSS">https://github.com/blurgyy/CoMPaSS</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13195v1">PDF</a> 18 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>文本到图像的扩散模型擅长生成逼真的图像，但在渲染文本提示中描述的空间关系时常常遇到困难。本研究识别出两个核心问题：1）现有数据集中空间相关数据的模糊性；2）当前文本编码器无法准确解释输入描述的空间语义。为了解决这些问题，提出了CoMPaSS，一个通用的训练框架，可增强任何T2I扩散模型的空间理解能力。CoMPaSS通过基于原则的空间约束解决空间相关数据的模糊性，并引入Token ENcoding ORdering（TENOR）模块，以更好地利用高质量的空间先验知识，有效弥补了文本编码器的不足。在四个流行的开放权重T2I扩散模型上的实验表明，CoMPaSS在生成空间关系方面的表现优异，并在VISOR（+98%）、T2I-CompBench Spatial（+67%）和GenEval Position（+131%）等基准测试中取得显著相对增益。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像的扩散模型在生成空间关系时存在困难。</li>
<li>现有数据集中空间相关数据的模糊性和文本编码器对空间语义的解读能力是主要挑战。</li>
<li>CoMPaSS框架通过解决空间相关数据的模糊性和增强空间理解能力来改善模型性能。</li>
<li>CoMPaSS使用Spatial Constraints-Oriented Pairing（SCOP）数据引擎和Token ENcoding ORdering（TENOR）模块来实现上述目标。</li>
<li>CoMPaSS在多种测试和基准测试中表现出色，取得显著相对增益。</li>
<li>该框架适用于多种T2I扩散模型，包括UNet-和MMDiT-based架构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13195">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-255c50838e3dd3c0bd1aa2b5fb7701ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-867ef474db121a27c24f1fe8a97b970e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d805f2c0fe6be9db54979f8e158c6076.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d312d40b35e6ba86c7de9aa57cbd7668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69bf501bc4042201f429806f716384b3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Prompt-Augmentation-for-Self-supervised-Text-guided-Image-Manipulation"><a href="#Prompt-Augmentation-for-Self-supervised-Text-guided-Image-Manipulation" class="headerlink" title="Prompt Augmentation for Self-supervised Text-guided Image Manipulation"></a>Prompt Augmentation for Self-supervised Text-guided Image Manipulation</h2><p><strong>Authors:Rumeysa Bodur, Binod Bhattarai, Tae-Kyun Kim</strong></p>
<p>Text-guided image editing finds applications in various creative and practical fields. While recent studies in image generation have advanced the field, they often struggle with the dual challenges of coherent image transformation and context preservation. In response, our work introduces prompt augmentation, a method amplifying a single input prompt into several target prompts, strengthening textual context and enabling localised image editing. Specifically, we use the augmented prompts to delineate the intended manipulation area. We propose a Contrastive Loss tailored to driving effective image editing by displacing edited areas and drawing preserved regions closer. Acknowledging the continuous nature of image manipulations, we further refine our approach by incorporating the similarity concept, creating a Soft Contrastive Loss. The new losses are incorporated to the diffusion model, demonstrating improved or competitive image editing results on public datasets and generated images over state-of-the-art approaches. </p>
<blockquote>
<p>文本引导的图像编辑在各种创意和实践领域都有应用。虽然最近的图像生成研究已经推动了该领域的发展，但它们经常面临一致的图像转换和上下文保留的双重挑战。针对这一问题，我们的工作引入了提示增强方法，该方法将单个输入提示放大为多个目标提示，增强了文本上下文并实现了局部图像编辑。具体来说，我们使用增强的提示来描绘预期的操纵区域。我们提出了一种针对驱动有效图像编辑的对比损失，通过位移编辑区域并将保留的区域拉近。考虑到图像操作的连续性，我们通过引入相似度的概念进一步改进了我们的方法，创建了软对比损失。新的损失被纳入到扩散模型中，在公共数据集和生成图像上的图像编辑结果有所改进或具有竞争力，超过了最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13081v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了文本引导的图像编辑技术在不同创意和实践领域的应用。针对图像生成中的挑战，本文提出了一种新的方法——提示增强法，通过放大单个输入提示为多个目标提示，强化文本上下文并实现局部图像编辑。具体来说，我们使用增强的提示来描绘预期的修改区域，并提出一种对比损失来驱动有效的图像编辑，通过位移编辑区域并拉近保留区域。考虑到图像操作的连续性，我们进一步通过引入相似性的概念完善了我们的方法，创建了软对比损失。新的损失被纳入扩散模型，在公共数据集和生成图像上展示了改进或具有竞争力的图像编辑结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本引导的图像编辑技术广泛应用于创意和实践领域。</li>
<li>提示增强法能够强化文本上下文并实现局部图像编辑。</li>
<li>增强的提示被用于描绘预期的图像修改区域。</li>
<li>对比损失被用来驱动有效的图像编辑。</li>
<li>位移编辑区域并拉近保留区域以增强图像编辑效果。</li>
<li>考虑到图像操作的连续性，引入了相似性的概念以完善方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13081">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae3f4de5809d67e571220a84942803f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e936ab9a9e68171b92830104214ac23f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f753cf1a7fd37d6dcf0eac2a3e3283.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Attentive-Eraser-Unleashing-Diffusion-Model’s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance"><a href="#Attentive-Eraser-Unleashing-Diffusion-Model’s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance" class="headerlink" title="Attentive Eraser: Unleashing Diffusion Model’s Object Removal Potential   via Self-Attention Redirection Guidance"></a>Attentive Eraser: Unleashing Diffusion Model’s Object Removal Potential   via Self-Attention Redirection Guidance</h2><p><strong>Authors:Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang</strong></p>
<p>Recently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after removal. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pre-trained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>. </p>
<blockquote>
<p>最近，扩散模型作为生成模型领域的新晋者表现出色，尤其在图像生成方面。然而，当用于对象去除任务时，它们仍然面临一些问题，例如产生随机伪影和在去除后无法用适当内容重新绘制前景对象区域。为了解决这些问题，我们提出了“Attentive Eraser”，这是一种无需调整的方法，可以为预训练的扩散模型提供稳定有效的对象去除能力。首先，基于观察到自注意力图影响生成图像的结构和形状细节，我们提出了注意力激活和抑制（ASS），它根据给定的掩膜重新设计预训练扩散模型内的自注意力机制，从而在反向生成过程中优先处理背景而非前景对象。此外，我们引入了自注意力重定向指导（SARG），它利用ASS引导的自注意力来指导生成过程，有效地在掩膜内去除前景对象，同时生成既合理又连贯的内容。实验表明，Attentive Eraser在各种预训练的扩散模型中的对象去除表现稳定且有效，甚至超越了基于训练的方法。此外，Attentive Eraser可在各种扩散模型架构和检查点中实现，具有良好的可扩展性。代码可在<a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>处获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12974v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>     扩散模型在生成模型领域崭露头角，尤其在图像生成方面表现出色。然而，在对象移除任务中，仍存在生成随机瑕疵和移除前景对象后无法重新绘制适当内容的问题。为此，提出无需调整的“Attentive Eraser”方法，使预训练的扩散模型能够进行稳定有效的对象移除。通过利用自我关注地图影响图像结构和形状细节的观察，提出基于给定掩膜的自我关注机制的激活和抑制（ASS），在反向生成过程中优先处理背景而非前景对象。此外，引入自我关注重定向指导（SARG），利用ASS重定向的自我关注来指导生成过程，在掩膜内有效移除前景对象，同时生成既合理又连贯的内容。实验表明，Attentive Eraser在多种预训练扩散模型中的对象移除表现稳定有效，甚至超越基于训练的方法。该方法可在各种扩散模型架构和检查点中实现，展现出卓越的可扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像生成领域展现出巨大潜力。</li>
<li>在对象移除任务中，扩散模型面临生成随机瑕疵和重新绘制问题。</li>
<li>提出Attentive Eraser方法，无需调整即可增强预训练扩散模型进行稳定有效的对象移除。</li>
<li>通过Attention Activation and Suppression (ASS)技术，优先处理背景信息，影响生成图像的结构和形状细节。</li>
<li>引入Self-Attention Redirection Guidance (SARG)，有效移除前景对象并生成连贯内容。</li>
<li>实验证明Attentive Eraser在多种预训练扩散模型中的优异表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12974">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b3653bdad7357fbc27eb73e856f192a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cccfe0d01ac8522e139f080a5dd29b7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d9b80c7275fc5978631c2565f9043ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-daf18f400271dc04dfd192a7b462c011.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3fe8a0715cad758c3774271ce823ca6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c81810756ea7a7a3ba441ce5a37a1067.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ArtAug-Enhancing-Text-to-Image-Generation-through-Synthesis-Understanding-Interaction"><a href="#ArtAug-Enhancing-Text-to-Image-Generation-through-Synthesis-Understanding-Interaction" class="headerlink" title="ArtAug: Enhancing Text-to-Image Generation through   Synthesis-Understanding Interaction"></a>ArtAug: Enhancing Text-to-Image Generation through   Synthesis-Understanding Interaction</h2><p><strong>Authors:Zhongjie Duan, Qianyi Zhao, Cen Chen, Daoyuan Chen, Wenmeng Zhou, Yaliang Li, Yingda Chen</strong></p>
<p>The emergence of diffusion models has significantly advanced image synthesis. The recent studies of model interaction and self-corrective reasoning approach in large language models offer new insights for enhancing text-to-image models. Inspired by these studies, we propose a novel method called ArtAug for enhancing text-to-image models in this paper. To the best of our knowledge, ArtAug is the first one that improves image synthesis models via model interactions with understanding models. In the interactions, we leverage human preferences implicitly learned by image understanding models to provide fine-grained suggestions for image synthesis models. The interactions can modify the image content to make it aesthetically pleasing, such as adjusting exposure, changing shooting angles, and adding atmospheric effects. The enhancements brought by the interaction are iteratively fused into the synthesis model itself through an additional enhancement module. This enables the synthesis model to directly produce aesthetically pleasing images without any extra computational cost. In the experiments, we train the ArtAug enhancement module on existing text-to-image models. Various evaluation metrics consistently demonstrate that ArtAug enhances the generative capabilities of text-to-image models without incurring additional computational costs. The source code and models will be released publicly. </p>
<blockquote>
<p>扩散模型的出现极大地推动了图像合成的发展。近期关于大型语言模型的模型交互和自我纠正推理方法的研究，为改进文本到图像的模型提供了新的见解。受这些研究的启发，我们在本文中提出了一种名为ArtAug的新型方法，用于增强文本到图像的模型。据我们所知，ArtAug是通过模型与理解模型的交互来改进图像合成模型的第一种方法。在交互过程中，我们利用图像理解模型隐含地学习人类偏好，为图像合成模型提供精细的建议。这些交互可以修改图像内容，使其具有美学感，例如调整曝光、改变拍摄角度和添加大气效果。通过额外的增强模块，这些交互带来的增强功能被迭代地融合到合成模型本身中。这使得合成模型能够直接生成具有美学感的图像，而无需任何额外的计算成本。在实验中，我们在现有的文本到图像模型上训练了ArtAug增强模块。各种评估指标一致表明，ArtAug在不增加额外计算成本的情况下提高了文本到图像模型的生成能力。源代码和模型将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12888v2">PDF</a> 18 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了利用大型语言模型的交互和自修正推理方法，提出一种名为ArtAug的新型图像合成增强方法。ArtAug通过模型交互利用图像理解模型的隐含人类偏好，为图像合成模型提供精细建议。这种交互能够修改图像内容，使其更具审美感，如调整曝光、拍摄角度和添加大气效果等。这种交互增强功能通过额外的增强模块迭代地融入合成模型本身，使得合成模型可直接生成具有美感的图像，无需额外的计算成本。实验证明，ArtAug对现有文本到图像模型的生成能力具有增强效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的兴起已大大推动了图像合成的发展。</li>
<li>大型语言模型的交互和自修正推理方法为图像合成模型的改进提供了新的视角。</li>
<li>ArtAug方法利用图像理解模型隐含的人类偏好，为图像合成模型提供精细建议。</li>
<li>ArtAug能够通过修改图像内容使其更具审美感，如调整曝光、拍摄角度和大气效果等。</li>
<li>ArtAug通过额外的增强模块将交互增强功能融入合成模型，无需额外计算成本。</li>
<li>实验证明ArtAug能有效提升文本到图像模型的生成能力。</li>
<li>ArtAug的源代码和模型将公开发布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12888">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b375109d2ac16ad1f88dfede9f9b962e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95c786fcba128a88fa18c86afe9a1a5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8e93f17d6a6a77482cc645aa6b75701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a67fee23981dff08f96381195a89d82d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6ff9dcafbc2018fb6f0680017b93710.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Causal-Diffusion-Transformers-for-Generative-Modeling"><a href="#Causal-Diffusion-Transformers-for-Generative-Modeling" class="headerlink" title="Causal Diffusion Transformers for Generative Modeling"></a>Causal Diffusion Transformers for Generative Modeling</h2><p><strong>Authors:Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, Haoqi Fan</strong></p>
<p>We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion’s multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion’s ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data. </p>
<blockquote>
<p>我们引入因果扩散（Causal Diffusion）作为扩散模型的自回归（AR）对应物。它是一种友好的下一个（或多个）令牌预测框架，适用于离散和连续模式，并与现有的下一个令牌预测模型（如LLaMA和GPT）兼容。尽管最近有研究表明尝试将扩散与AR模型相结合，但我们发现对扩散模型引入序列分解可以显著提高性能，并可以在AR和扩散生成模式之间实现平稳过渡。因此，我们提出了因果融合（CausalFusion）——一种仅解码的变压器，它在序列令牌和扩散噪声级别上双重分解数据，从而在ImageNet生成基准测试上取得了最新结果，同时享受AR生成任意数量令牌的上下文推理优势。我们进一步通过联合图像生成和描述模型来展示因果融合的多模式能力，并展示其在零镜头上下文图像操作中的能力。我们希望这项工作能为社区在离散和连续数据上训练多模式模型提供新的视角。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12095v2">PDF</a> 22 figures, 21 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了因果扩散（Causal Diffusion）作为扩散模型的自回归（AR）对应物。它是一种用于预测下一个或多个符号的框架，适用于离散和连续模态，并与现有的自回归预测模型（如LLaMA和GPT）兼容。本文展示了对扩散模型引入序列分解可以显著提高性能，并在自回归和扩散生成模式之间实现平稳过渡。因此，提出了因果融合（CausalFusion）——一种仅解码的变压器，它在序列标记和扩散噪声级别上双重分解数据，在ImageNet生成基准测试上取得了最新结果，并保留了自回归生成任意数量符号的上下文推理优势。此外，还展示了因果融合的多模式能力和零样本上下文图像操作能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了因果扩散作为自回归模型的扩散模型的对应物。</li>
<li>因果扩散是一个用于预测下一个或多个符号的框架，适用于离散和连续模态。</li>
<li>现有的自回归预测模型（如LLaMA和GPT）与因果扩散兼容。</li>
<li>对扩散模型引入序列分解可以显著提高性能。</li>
<li>因果融合是一个仅解码的变压器，它在序列标记和扩散噪声级别上双重分解数据。</li>
<li>因果融合在ImageNet生成基准测试上取得了最新结果，同时具有自回归的优势。</li>
<li>因果融合具有多模式能力，展示了零样本上下文图像操作能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12095">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8c26c69cb665c939727e3834ef4390a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b9a31efe4ad14498b8fac7006c61471.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00479e06234d725de8863a95f76119f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be3018f479edd2671c8785d0238e02ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fb68eb7080e8144e64e28bcbdd8aa9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-899f6faafe8be6ad0a10f33236ab2807.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SwiftTry-Fast-and-Consistent-Video-Virtual-Try-On-with-Diffusion-Models"><a href="#SwiftTry-Fast-and-Consistent-Video-Virtual-Try-On-with-Diffusion-Models" class="headerlink" title="SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models"></a>SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models</h2><p><strong>Authors:Hung Nguyen, Quang Qui-Vinh Nguyen, Khoi Nguyen, Rang Nguyen</strong></p>
<p>Given an input video of a person and a new garment, the objective of this paper is to synthesize a new video where the person is wearing the specified garment while maintaining spatiotemporal consistency. Although significant advances have been made in image-based virtual try-on, extending these successes to video often leads to frame-to-frame inconsistencies. Some approaches have attempted to address this by increasing the overlap of frames across multiple video chunks, but this comes at a steep computational cost due to the repeated processing of the same frames, especially for long video sequences. To tackle these challenges, we reconceptualize video virtual try-on as a conditional video inpainting task, with garments serving as input conditions. Specifically, our approach enhances image diffusion models by incorporating temporal attention layers to improve temporal coherence. To reduce computational overhead, we propose ShiftCaching, a novel technique that maintains temporal consistency while minimizing redundant computations. Furthermore, we introduce the TikTokDress dataset, a new video try-on dataset featuring more complex backgrounds, challenging movements, and higher resolution compared to existing public datasets. Extensive experiments demonstrate that our approach outperforms current baselines, particularly in terms of video consistency and inference speed. The project page is available at <a target="_blank" rel="noopener" href="https://swift-try.github.io/">https://swift-try.github.io/</a>. </p>
<blockquote>
<p>给定一个输入视频，其中包含一个人和一件新衣服，本文的目标是在保持时空一致性的情况下，合成一个新的视频，其中人物穿着指定的服装。尽管基于图像的虚拟试衣已经取得了重大进展，但这些成功往往难以扩展到视频领域，因为这经常导致帧之间的不一致性。一些方法试图通过增加跨越多个视频片段的帧重叠来解决这个问题，但由于重复处理相同的帧，特别是在处理长视频序列时，这带来了高昂的计算成本。为了应对这些挑战，我们将视频虚拟试衣重新定位为条件视频填充任务，服装作为输入条件。具体来说，我们的方法通过结合时间注意力层来提高图像扩散模型的性能，从而提高时间连贯性。为了减少计算开销，我们提出了ShiftCaching这一新技术，该技术能在保持时间一致性的同时最小化冗余计算。此外，我们还引入了TikTokDress数据集，这是一个新的视频试穿数据集，具有更复杂的背景、更具挑战性的动作和更高的分辨率。大量实验表明，我们的方法优于当前基线方法，特别是在视频一致性和推理速度方面。项目页面可在[<a target="_blank" rel="noopener" href="https://swift-try.github.io/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://swift-try.github.io/]上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10178v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文旨在合成一个新视频，展示人物穿上指定衣物的同时保持时空一致性。为提高视频连贯性并降低计算成本，研究团队将视频虚拟试衣视为条件视频修复任务，通过引入时间注意力层改进图像扩散模型，并提出ShiftCaching技术减少冗余计算。此外，引入TikTokDress数据集，与现有公开数据集相比，背景更复杂、动作更具挑战性且分辨率更高。实验证明，该方法在视频一致性和推理速度方面均优于当前基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文的目标是根据输入的人物视频和新的衣物，合成一个展示人物穿上新衣物的视频，同时保持时空一致性。</li>
<li>尽管图像虚拟试衣已经取得了显著进展，但将其扩展到视频时仍面临帧间不一致的问题。</li>
<li>研究人员将视频虚拟试衣重新构想为条件视频修复任务，其中衣物作为输入条件。</li>
<li>通过引入时间注意力层改进图像扩散模型，以提高视频连贯性。</li>
<li>提出ShiftCaching技术来减少冗余计算，提高效率。</li>
<li>引入TikTokDress数据集，包含更复杂的背景、挑战性和更高分辨率的视频内容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-086aec0a20e565084105162988b57ae8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a31d3029b9d81b879d85f54648c504.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c4c605b443e0cc4a77553a7cd2f4670.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72ff49c42082cab8c706fb0954f8a262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e121e47f535554fc59afe88bab5a990a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fbcefc670f07de03e4e294a4dd19708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77a359f212cba39f20fae311c478cd17.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="No-Annotations-for-Object-Detection-in-Art-through-Stable-Diffusion"><a href="#No-Annotations-for-Object-Detection-in-Art-through-Stable-Diffusion" class="headerlink" title="No Annotations for Object Detection in Art through Stable Diffusion"></a>No Annotations for Object Detection in Art through Stable Diffusion</h2><p><strong>Authors:Patrick Ramos, Nicolas Gonthier, Selina Khan, Yuta Nakashima, Noa Garcia</strong></p>
<p>Object detection in art is a valuable tool for the digital humanities, as it allows for faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion models’ art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art. Code is available at <a target="_blank" rel="noopener" href="https://github.com/patrick-john-ramos/nada">https://github.com/patrick-john-ramos/nada</a> </p>
<blockquote>
<p>艺术品中的目标检测是数字人文领域的一个重要工具，因为它与人类相比，可以更快速地识别艺术和历史图像中的目标。然而，对这些图像进行标注却面临重大挑战，因为需要专业的领域知识。我们提出了NADA（艺术品检测无需标注），这是一种利用扩散模型的与艺术相关的知识，在绘画中进行目标检测，无需完整的边界框监督。我们的方法支持弱监督和无源场景，并且不需要对其预训练组件进行任何微调，它由基于大型视觉语言模型的类提出者和基于Stable Diffusion的类条件检测器组成。NADA在两个艺术品数据集ArtDL 2.0和IconArt上进行了评估，在弱监督检测方面优于先前的工作，同时是艺术品零样本目标检测的首项工作。代码可在<a target="_blank" rel="noopener" href="https://github.com/patrick-john-ramos/nada%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/patrick-john-ramos/nada找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06286v2">PDF</a> 8 pages, 6 figures, to be published in WACV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了艺术领域中的物体检测工具的重要性，它能够快速识别艺术和历史图像中的物体。然而，由于需要特定的专业知识，对这类图像进行标注具有挑战性。研究人员提出了一种利用扩散模型的无需全框监督的绘画检测方案——NADA（无需标注进行艺术检测）。该方法支持弱监督和无源场景，无需对其预训练组件进行微调，包括基于大型视觉语言模型的类提出器和基于Stable Diffusion的类条件检测器。在ArtDL 2.0和IconArt两个艺术品数据集上，NADA的表现超越了现有的弱监督检测方法，同时也是第一个在无源艺术领域实现物体检测的。完整代码可以在GitHub上找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>艺术领域的物体检测工具可以快速识别艺术和历史图像中的物体，具有实用价值。</li>
<li>对艺术图像进行标注需要特定的专业知识，具有挑战性。</li>
<li>NADA是一种利用扩散模型进行艺术检测的解决方案，无需全框监督。</li>
<li>NADA支持弱监督和无源场景的应用。</li>
<li>NADA不需要对其预训练组件进行微调。</li>
<li>NADA包括基于大型视觉语言模型的类提出器和基于Stable Diffusion的类条件检测器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06286">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-57c17d24733537fe1886250560c883fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4139a38513f0b2f267b75456dd3dfdfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b04ddd24cf532a8716dc0a4011c96116.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9735771e9b15404efe8fd522bb09352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21873767c7771c024dd0a90f4d9774c2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model"><a href="#MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model" class="headerlink" title="MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model"></a>MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model</h2><p><strong>Authors:Shan Yang</strong></p>
<p>Text-to-image generation models have revolutionized content creation, but diffusion-based vision-language models still face challenges in precisely controlling the shape, appearance, and positional placement of objects in generated images using text guidance alone. Existing global image editing models rely on additional masks or images as guidance to achieve layout control, often requiring retraining of the model. While local object-editing models allow modifications to object shapes, they lack the capability to control object positions. To address these limitations, we propose the Mask-free Training-free Object-Level Layout Control Diffusion Model (MFTF), which provides precise control over object positions without requiring additional masks or images. The MFTF model supports both single-object and multi-object positional adjustments, such as translation and rotation, while enabling simultaneous layout control and object semantic editing. The MFTF model employs a parallel denoising process for both the source and target diffusion models. During this process, attention masks are dynamically generated from the cross-attention layers of the source diffusion model and applied to queries from the self-attention layers to isolate objects. These queries, generated in the source diffusion model, are then adjusted according to the layout control parameters and re-injected into the self-attention layers of the target diffusion model. This approach ensures accurate and precise positional control of objects. Project source code available at <a target="_blank" rel="noopener" href="https://github.com/syang-genai/MFTF">https://github.com/syang-genai/MFTF</a>. </p>
<blockquote>
<p>文本到图像生成模型已经彻底改变了内容创作的方式，但基于扩散的视语言模型在仅使用文本指导来精确控制生成图像中物体的形状、外观和位置放置方面仍面临挑战。现有的全局图像编辑模型依赖于额外的蒙版或图像作为指导来实现布局控制，通常需要重新训练模型。虽然局部物体编辑模型允许修改物体形状，但它们缺乏控制物体位置的能力。为了解决这些限制，我们提出了无蒙版、无需训练的物体级别布局控制扩散模型（MFTF），该模型能够在无需额外蒙版或图像的情况下，精确控制物体的位置。MFTF模型既支持单物体也支持多物体的位置调整，如平移和旋转，同时实现布局控制和物体语义编辑。MFTF模型采用源和目标扩散模型的并行去噪过程。在此过程中，从源扩散模型的交叉注意层动态生成注意蒙版，并应用于自我注意层的查询以隔离物体。这些在源扩散模型中生成的查询会根据布局控制参数进行调整，然后重新注入目标扩散模型的自我注意层。这种方法确保了物体的精确位置控制。项目源代码可在<a target="_blank" rel="noopener" href="https://github.com/syang-genai/MFTF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/syang-genai/MFTF找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01284v2">PDF</a> 8 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种无需掩膜和训练的对象级布局控制扩散模型（MFTF），该模型可在无需额外掩膜或图像的情况下，实现对生成图像中对象的形状、外观和位置进行精确控制。MFTF模型支持单对象和多对象的位置调整，如平移和旋转，同时实现布局控制和对象语义编辑。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在内容创建中实现了文本到图像的生成，但仍面临精确控制对象形状、外观和位置的挑战。</li>
<li>现有全局图像编辑模型依赖额外的掩膜或图像作为指导来实现布局控制，需要重训模型。</li>
<li>局部对象编辑模型允许修改对象形状，但无法控制对象位置。</li>
<li>提出的MFTF模型提供了对对象位置的精确控制，无需额外的掩膜或图像。</li>
<li>MFTF模型支持单对象和多对象的位置调整，如平移和旋转，同时实现布局控制和对象语义编辑。</li>
<li>MFTF模型采用并行去噪过程，通过交叉注意层生成注意力掩膜，应用于自我注意层的查询，以隔离对象。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01284">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7f5307dede9c39a5e6238a4553cd509b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92df01ad425ce71e6a3a7789281fc8d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31d932f273200085ffd85d5c55bc2acd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb08341d38f99b43d2cb48d414f401d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-452bad3796827f922621781276fed702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7024f4fd43f65c0109fc421d30a29356.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Estimating-Body-and-Hand-Motion-in-an-Ego-sensed-World"><a href="#Estimating-Body-and-Hand-Motion-in-an-Ego-sensed-World" class="headerlink" title="Estimating Body and Hand Motion in an Ego-sensed World"></a>Estimating Body and Hand Motion in an Ego-sensed World</h2><p><strong>Authors:Brent Yi, Vickie Ye, Maya Zheng, Yunqi Li, Lea Müller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa</strong></p>
<p>We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture a device wearer’s actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve hand estimation: the resulting kinematic and temporal constraints can reduce world-frame errors in single-frame estimates by 40%. Project page: <a target="_blank" rel="noopener" href="https://egoallo.github.io/">https://egoallo.github.io/</a> </p>
<blockquote>
<p>我们介绍了EgoAllo系统，该系统通过头戴设备实现人体运动估计。仅使用以自我为中心的SLAM姿势和图像，EgoAllo指导从条件扩散模型中进行采样，以估计三维身体姿势、高度和手部参数，这些参数能够捕获设备佩戴者在场景的中心坐标系中的动作。为实现这一点，我们的关键见解在于表示：我们提出了空间和时间不变性标准以提高模型性能，由此得出头部运动条件参数化，可提高估计精度达18%。我们还展示了我们的系统估计的躯体如何改进手部估计：由此产生的运动学和时间约束可减少单帧估计中的世界框架误差达40%。项目页面：<a target="_blank" rel="noopener" href="https://egoallo.github.io/">https://egoallo.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03665v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://egoallo.github.io/">https://egoallo.github.io/</a></p>
<p><strong>Summary</strong>：</p>
<p>我们推出了EgoAllo系统，该系统通过头戴设备实现人体动作估计。仅使用以自我为中心的SLAM姿态和图像，EgoAllo引导从条件扩散模型中进行采样，以估计三维身体姿态、高度以及手部参数，这些参数捕捉设备佩戴者在场景中的以场景为中心的坐标框架中的动作。我们的关键见解在于表示法：我们提出空间和时间不变性标准以提高模型性能，从中得出头部运动条件参数化，可提高估计精度达18%。我们还展示了我们的系统估计的躯体如何改善手部估计：由此产生的运动学和时间约束可减少单帧估计中的世界框架误差达40%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>EgoAllo系统利用头戴设备实现人体动作估计。</li>
<li>仅使用以自我为中心的SLAM姿态和图像进行动作估计。</li>
<li>通过条件扩散模型采样来估计三维身体姿态、高度和手部参数。</li>
<li>系统的关键在于提出空间和时间不变性标准来提高模型性能。</li>
<li>头部运动条件参数化能提高估计精度达18%。</li>
<li>躯体估计可改善手部估计。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03665">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-81c8492b3b1560f367a0967f27d2c999.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77a7bf23f5bb0f722bdca49c60d6acbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1df460e2931a3ff20eb5ecde3ebdfae3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11a4ae84ce2c96f819c81a77bb23063c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Resolving-Multi-Condition-Confusion-for-Finetuning-Free-Personalized-Image-Generation"><a href="#Resolving-Multi-Condition-Confusion-for-Finetuning-Free-Personalized-Image-Generation" class="headerlink" title="Resolving Multi-Condition Confusion for Finetuning-Free Personalized   Image Generation"></a>Resolving Multi-Condition Confusion for Finetuning-Free Personalized   Image Generation</h2><p><strong>Authors:Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, Jie Song</strong></p>
<p>Personalized text-to-image generation methods can generate customized images based on the reference images, which have garnered wide research interest. Recent methods propose a finetuning-free approach with a decoupled cross-attention mechanism to generate personalized images requiring no test-time finetuning. However, when multiple reference images are provided, the current decoupled cross-attention mechanism encounters the object confusion problem and fails to map each reference image to its corresponding object, thereby seriously limiting its scope of application. To address the object confusion problem, in this work we investigate the relevance of different positions of the latent image features to the target object in diffusion model, and accordingly propose a weighted-merge method to merge multiple reference image features into the corresponding objects. Next, we integrate this weighted-merge method into existing pre-trained models and continue to train the model on a multi-object dataset constructed from the open-sourced SA-1B dataset. To mitigate object confusion and reduce training costs, we propose an object quality score to estimate the image quality for the selection of high-quality training samples. Furthermore, our weighted-merge training framework can be employed on single-object generation when a single object has multiple reference images. The experiments verify that our method achieves superior performance to the state-of-the-arts on the Concept101 dataset and DreamBooth dataset of multi-object personalized image generation, and remarkably improves the performance on single-object personalized image generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/hqhQAQ/MIP-Adapter">https://github.com/hqhQAQ/MIP-Adapter</a>. </p>
<blockquote>
<p>个性化文本到图像生成方法能够根据参考图像生成定制图像，这已引起了广泛的研究兴趣。最近的方法提出了一种无需微调的脱钩交叉注意机制来生成个性化图像，无需测试时的微调。然而，当提供多个参考图像时，当前的脱钩交叉注意机制会遇到对象混淆问题，并且无法将每个参考图像映射到其相应的对象，从而严重限制了其应用范围。为了解决对象混淆问题，在这项工作中，我们研究了潜在图像特征的不同位置与目标对象的相关性，并据此提出了一种加权合并方法，将多个参考图像特征合并到相应的对象中。接下来，我们将这种加权合并方法集成到现有的预训练模型中，并在由开源SA-1B数据集构建的多对象数据集上继续训练模型。为了减轻对象混淆并降低训练成本，我们提出了一种对象质量评分来估计图像质量，以选择高质量的训练样本。此外，我们的加权合并训练框架可以在单个对象有多个参考图像时使用于单个对象的生成。实验验证我们的方法在Concept101数据集和DreamBooth数据集的多对象个性化图像生成方面达到了最新技术的先进水平，并在单对象个性化图像生成方面显著提高了性能。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/hqhQAQ/MIP-Adapter%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hqhQAQ/MIP-Adapter找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17920v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对个性化文本到图像生成方法，研究提出了一种无需微调的解耦跨注意力机制。当提供多个参考图像时，现有方法会遇到对象混淆问题。本研究调查了扩散模型中潜在图像特征与目标对象位置的相关性，并提出加权合并方法将多个参考图像特征合并为对应对象。此外，该研究还构建了多对象数据集，并引入对象质量评分以选择高质量训练样本。实验证明，该方法在多对象个性化图像生成任务上优于现有方法，并在单对象个性化图像生成任务上也有显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了无需微调的个性化文本到图像生成方法，采用解耦跨注意力机制。</li>
<li>当处理多个参考图像时，现有方法存在对象混淆问题。</li>
<li>研究了扩散模型中潜在图像特征与目标对象位置的相关性。</li>
<li>提出了加权合并方法，将多个参考图像特征合并为对应对象。</li>
<li>构建多对象数据集，并引入对象质量评分机制以选择高质量训练样本。</li>
<li>方法在多对象个性化图像生成任务上表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17920">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7e3341fb6f6aed6cfd489083e2d86d16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49ae746042db2df69465711dbdce0cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a37bb9241b3a1fde860929cdd7cff79f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c30307300d0cdff3f5e005406690cc85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a72a7e4eaeda12994df13b176becd37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ddc2fc0ada7bcdcc85a22cac1354452.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation"><a href="#Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation" class="headerlink" title="Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few   Steps Image Generation"></a>Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few   Steps Image Generation</h2><p><strong>Authors:Clément Chadebec, Onur Tasar, Eyal Benaroche, Benjamin Aubin</strong></p>
<p>In this paper, we propose an efficient, fast, and versatile distillation method to accelerate the generation of pre-trained diffusion models: Flash Diffusion. The method reaches state-of-the-art performances in terms of FID and CLIP-Score for few steps image generation on the COCO2014 and COCO2017 datasets, while requiring only several GPU hours of training and fewer trainable parameters than existing methods. In addition to its efficiency, the versatility of the method is also exposed across several tasks such as text-to-image, inpainting, face-swapping, super-resolution and using different backbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\alpha$), as well as adapters. In all cases, the method allowed to reduce drastically the number of sampling steps while maintaining very high-quality image generation. The official implementation is available at <a target="_blank" rel="noopener" href="https://github.com/gojasper/flash-diffusion">https://github.com/gojasper/flash-diffusion</a>. </p>
<blockquote>
<p>本文提出了一种高效、快速、通用的蒸馏方法来加速预训练扩散模型的生成：Flash Diffusion。该方法在COCO2014和COCO2017数据集上的少量步骤图像生成方面，在FID和CLIP分数上达到了最先进的性能，同时只需几小时的GPU训练时间，并且相较于现有方法需要的可训练参数更少。除了高效性之外，该方法还展现出跨多个任务的灵活性，如文字转图像、图像补全、面部替换、超分辨率以及使用不同的主干网络如基于UNet的去噪器（SD1.5、SDXL）或DiT（Pixart-α），以及适配器等。在所有情况下，该方法在大幅减少采样步骤数量的同时，保持了非常高的图像生成质量。官方实现可访问 <a target="_blank" rel="noopener" href="https://github.com/gojasper/flash-diffusion">https://github.com/gojasper/flash-diffusion</a> 获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02347v3">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种高效、快速且通用的蒸馏方法，用于加速预训练扩散模型的生成：Flash Diffusion。该方法在COCO2014和COCO2017数据集上进行少量步骤的图像生成时，达到了FID和CLIP-Score的最新性能水平。它只需要几个小时的GPU训练时间，并且相较于现有方法，所需的训练参数更少。除了高效性外，该方法还具有通用性，可用于文本转图像、图像补全、人脸替换、超分辨率处理等多种任务，并支持不同的骨干网络（如基于UNet的降噪器或DiT）和适配器。该方法大幅减少了采样步骤数量，同时保持了高质量图像生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Flash Diffusion是一种用于加速预训练扩散模型生成的高效、快速和通用的蒸馏方法。</li>
<li>该方法在COCO2014和COCO2017数据集上实现了优异的FID和CLIP-Score性能。</li>
<li>Flash Diffusion训练时间短，仅需几个小时的GPU时间。</li>
<li>相较于其他方法，Flash Diffusion需要的训练参数更少。</li>
<li>该方法具有广泛的应用性，可用于多种任务，如文本转图像、图像补全、人脸替换和超分辨率处理。</li>
<li>Flash Diffusion支持多种骨干网络和适配器，展示了其强大的灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02347">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4cb2686ea1b0bd9c48c139b2516ade5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4860e6d3f9ab853f160ee02cebc4e61e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b83d24033e2a35f1cd6c5bf95160557.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-62816a2ba15fa317d81e7a9f8172f741.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-19  Parameter-efficient Fine-tuning for improved Convolutional Baseline for   Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f48b889f694388b756223771c0d053df.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2024-12-19  GraphAvatar Compact Head Avatars with GNN-Generated 3D Gaussians
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">14773.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
