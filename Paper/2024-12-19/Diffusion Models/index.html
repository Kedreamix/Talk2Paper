<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  AniDoc Animation Creation Made Easier">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-92df01ad425ce71e6a3a7789281fc8d0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-19-æ›´æ–°"><a href="#2024-12-19-æ›´æ–°" class="headerlink" title="2024-12-19 æ›´æ–°"></a>2024-12-19 æ›´æ–°</h1><h2 id="AniDoc-Animation-Creation-Made-Easier"><a href="#AniDoc-Animation-Creation-Made-Easier" class="headerlink" title="AniDoc: Animation Creation Made Easier"></a>AniDoc: Animation Creation Made Easier</h2><p><strong>Authors:Yihao Meng, Hao Ouyang, Hanlin Wang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Zhiheng Liu, Yujun Shen, Huamin Qu</strong></p>
<p>The production of 2D animation follows an industry-standard workflow, encompassing four essential stages: character design, keyframe animation, in-betweening, and coloring. Our research focuses on reducing the labor costs in the above process by harnessing the potential of increasingly powerful generative AI. Using video diffusion models as the foundation, AniDoc emerges as a video line art colorization tool, which automatically converts sketch sequences into colored animations following the reference character specification. Our model exploits correspondence matching as an explicit guidance, yielding strong robustness to the variations (e.g., posture) between the reference character and each line art frame. In addition, our model could even automate the in-betweening process, such that users can easily create a temporally consistent animation by simply providing a character image as well as the start and end sketches. Our code is available at: <a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo">https://yihao-meng.github.io/AniDoc_demo</a>. </p>
<blockquote>
<p>äºŒç»´åŠ¨ç”»çš„åˆ¶ä½œéµå¾ªè¡Œä¸šæ ‡å‡†çš„å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬å››ä¸ªåŸºæœ¬é˜¶æ®µï¼šè§’è‰²è®¾è®¡ã€å…³é”®å¸§åŠ¨ç”»ã€ä¸­é—´å¸§ç”Ÿæˆå’Œä¸Šè‰²ã€‚æˆ‘ä»¬çš„ç ”ç©¶èšç„¦äºåˆ©ç”¨æ—¥ç›Šå¼ºå¤§çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ½œåŠ›ï¼Œé™ä½ä¸Šè¿°æµç¨‹ä¸­çš„åŠ³åŠ¨åŠ›æˆæœ¬ã€‚ä»¥è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸ºåŸºç¡€ï¼ŒAniDocä½œä¸ºä¸€ç§è§†é¢‘çº¿è‰ºæœ¯å½©è‰²åŒ–å·¥å…·åº”è¿è€Œç”Ÿï¼Œå®ƒä¼šè‡ªåŠ¨å°†è‰å›¾åºåˆ—è½¬æ¢ä¸ºå½©è‰²åŠ¨ç”»ï¼Œå¹¶æŒ‰ç…§å‚è€ƒè§’è‰²è§„èŒƒè¿›è¡Œã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨å¯¹åº”åŒ¹é…ä½œä¸ºæ˜ç¡®æŒ‡å¯¼ï¼Œå¯¹å‚è€ƒè§’è‰²ä¸æ¯ä¸ªçº¿è‰ºæœ¯æ¡†æ¶ä¹‹é—´çš„å˜åŒ–ï¼ˆä¾‹å¦‚å§¿åŠ¿ï¼‰å…·æœ‰å¾ˆå¼ºçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç”šè‡³å¯ä»¥è‡ªåŠ¨åŒ–ä¸­é—´å¸§ç”Ÿæˆè¿‡ç¨‹ï¼Œè¿™æ ·ç”¨æˆ·åªéœ€æä¾›è§’è‰²å›¾åƒä»¥åŠå¼€å§‹å’Œç»“æŸçš„è‰å›¾ï¼Œå°±å¯ä»¥è½»æ¾åˆ›å»ºæ—¶é—´è¿è´¯çš„åŠ¨ç”»ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo%E8%AE%BF%E9%97%AE%E3%80%82">https://yihao-meng.github.io/AniDoc_demoè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14173v1">PDF</a> Project page and code: <a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo">https://yihao-meng.github.io/AniDoc_demo</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å¼€å‘çš„åŠ¨ç”»ç”Ÿäº§çº¿å·¥å…·AniDocï¼Œå®ƒèƒ½è‡ªåŠ¨å°†è‰å›¾åºåˆ—è½¬åŒ–ä¸ºå½©è‰²åŠ¨ç”»ï¼Œä¾æ®å‚ç…§è§’è‰²ç‰¹æ€§è¿›è¡Œè‰²å½©å¡«å……ï¼Œå¹¶æ”¯æŒè‡ªåŠ¨å®Œæˆä¸­é—´è¿‡ç¨‹ã€‚å·¥å…·å¯å®ç°å¼ºé²æ£’æ€§ï¼Œé€‚åº”è§’è‰²å§¿æ€å˜åŒ–ç­‰å·®å¼‚ï¼Œå¹¶ç®€åŒ–åŠ¨ç”»åˆ›ä½œæµç¨‹ã€‚ç›¸å…³ä»£ç å¯åœ¨æŒ‡å®šç½‘å€ä¸‹è½½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å¼€å‘åŠ¨ç”»å·¥å…·AniDocã€‚</li>
<li>AniDocå¯å°†è‰å›¾åºåˆ—è‡ªåŠ¨è½¬åŒ–ä¸ºå½©è‰²åŠ¨ç”»ã€‚</li>
<li>AniDocä¾æ®å‚ç…§è§’è‰²ç‰¹æ€§è¿›è¡Œè‰²å½©å¡«å……ã€‚<br>4.AniDocå…·æœ‰å¼ºé²æ£’æ€§ï¼Œé€‚åº”è§’è‰²å§¿æ€å˜åŒ–ç­‰å·®å¼‚ã€‚</li>
<li>AniDocå¯è‡ªåŠ¨å®Œæˆä¸­é—´è¿‡ç¨‹ï¼Œç®€åŒ–åŠ¨ç”»åˆ›ä½œæµç¨‹ã€‚</li>
<li>ç”¨æˆ·åªéœ€æä¾›è§’è‰²å›¾åƒä»¥åŠèµ·å§‹å’Œç»“æŸè‰å›¾ï¼Œå³å¯è½»æ¾åˆ›å»ºè¿è´¯çš„åŠ¨ç”»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b643bf9e4a016f462ab739b764265090.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8aaa76958c834fd9840cf11cd36be3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d31db743f7a767ab45a653d120968c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fccbeb9b521b4250ae3e1825e3877013.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e982dabbb02cd0d3a1f9e118e5f9243f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Video-Generation-without-Vector-Quantization"><a href="#Autoregressive-Video-Generation-without-Vector-Quantization" class="headerlink" title="Autoregressive Video Generation without Vector Quantization"></a>Autoregressive Video Generation without Vector Quantization</h2><p><strong>Authors:Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, Xinlong Wang</strong></p>
<p>This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA">https://github.com/baaivision/NOVA</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„è‡ªå›å½’è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬æè®®å°†è§†é¢‘ç”Ÿæˆé—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæœªé‡åŒ–çš„è‡ªå›å½’å»ºæ¨¡ï¼ŒåŒ…æ‹¬æ—¶é—´ä¸Šçš„é€å¸§é¢„æµ‹å’Œç©ºé—´çš„é€é›†é¢„æµ‹ã€‚ä¸å…ˆå‰è‡ªå›å½’æ¨¡å‹ä¸­çš„æ‰«æé¢„æµ‹æˆ–æ‰©æ•£æ¨¡å‹ä¸­å›ºå®šé•¿åº¦ç¬¦å·çš„è”åˆåˆ†å¸ƒå»ºæ¨¡ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†GPTé£æ ¼æ¨¡å‹çš„å› æœç‰¹æ€§ï¼Œä»¥æä¾›çµæ´»çš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼ŒåŒæ—¶åˆ©ç”¨å•ä¸ªå¸§å†…çš„åŒå‘å»ºæ¨¡æ¥æé«˜æ•ˆç‡ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§åä¸ºNOVAçš„æ–°å‹è§†é¢‘è‡ªå›å½’æ¨¡å‹ï¼Œæ— éœ€å‘é‡é‡åŒ–ã€‚ç»“æœè¡¨æ˜ï¼ŒNOVAåœ¨æ•°æ®æ•ˆç‡ã€æ¨ç†é€Ÿåº¦ã€è§†è§‰ä¿çœŸåº¦å’Œè§†é¢‘æµç•…æ€§æ–¹é¢è¶…è¶Šäº†å…ˆå‰çš„è‡ªå›å½’è§†é¢‘æ¨¡å‹ï¼Œå³ä½¿å…¶æ¨¡å‹å®¹é‡æ›´å°ï¼ˆå³0.6Bå‚æ•°ï¼‰ã€‚æ­¤å¤–ï¼ŒNOVAåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šä¹Ÿè¶…è¶Šäº†æœ€å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¤§å¤§é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚NOVAè¿˜èƒ½å¤Ÿé€‚åº”é•¿æ—¶é—´çš„è§†é¢‘ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­å®ç°äº†å¤šæ ·åŒ–çš„é›¶æ ·æœ¬åº”ç”¨ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/baaivision/NOVA%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/baaivision/NOVAä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14169v1">PDF</a> 22 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é«˜æ•ˆçš„æƒ…å†µä¸‹å®ç°è‡ªå›å½’è§†é¢‘ç”Ÿæˆã€‚è¯¥æ–¹æ³•é‡æ–°å®šä¹‰äº†è§†é¢‘ç”Ÿæˆé—®é¢˜ï¼Œé‡‡ç”¨éé‡åŒ–è‡ªå›å½’å»ºæ¨¡æ–¹å¼ï¼Œå®ç°äº†æ—¶åºé€å¸§é¢„æµ‹å’Œç©ºé—´é€é›†é¢„æµ‹ã€‚è¯¥æ–¹æ³•ä¿æŒäº†GPTé£æ ¼æ¨¡å‹çš„å› æœç‰¹æ€§ï¼ŒåŒæ—¶åˆ©ç”¨å•ä¸ªå¸§å†…çš„åŒå‘å»ºæ¨¡æ¥æé«˜æ•ˆç‡ã€‚æ‰€æå‡ºçš„æ¨¡å‹ç§°ä¸ºNOVAï¼Œå®ƒåœ¨æ•°æ®æ•ˆç‡ã€æ¨ç†é€Ÿåº¦ã€è§†è§‰ä¿çœŸåº¦å’Œè§†é¢‘æµç•…æ€§æ–¹é¢éƒ½è¶…è¶Šäº†å…ˆå‰çš„è‡ªå›å½’è§†é¢‘æ¨¡å‹ï¼Œå³ä½¿æ¨¡å‹å®¹é‡è¾ƒå°ï¼ˆä»…ä¸º0.6Bå‚æ•°ï¼‰ã€‚æ­¤å¤–ï¼ŒNOVAåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒæˆæœ¬ä½ã€‚å®ƒè¿˜èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°æ›´é•¿çš„è§†é¢‘æ—¶é•¿ï¼Œå¹¶åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­å®ç°å¤šæ ·åŒ–çš„é›¶æ ·æœ¬åº”ç”¨ã€‚æ¨¡å‹å’Œä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆè‡ªå›å½’è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å°†è§†é¢‘ç”Ÿæˆé—®é¢˜é‡æ–°å®šä¹‰ä¸ºéé‡åŒ–è‡ªå›å½’å»ºæ¨¡é—®é¢˜ï¼Œå®ç°æ—¶åºé€å¸§å’Œç©ºé—´é€é›†é¢„æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆGPTé£æ ¼æ¨¡å‹çš„å› æœç‰¹æ€§å’Œå•ä¸ªå¸§å†…çš„åŒå‘å»ºæ¨¡æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>NOVAæ¨¡å‹åœ¨æ•°æ®æ•ˆç‡ã€æ¨ç†é€Ÿåº¦ã€è§†è§‰ä¿çœŸåº¦å’Œè§†é¢‘æµç•…æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰è‡ªå›å½’è§†é¢‘æ¨¡å‹ã€‚</li>
<li>NOVAåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°çªå‡ºï¼Œè®­ç»ƒæˆæœ¬ä½ã€‚</li>
<li>NOVAå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯åº”ç”¨äºæ›´é•¿çš„è§†é¢‘æ—¶é•¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d514d70d0fc5abba757619c0da01eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2be2a967eec50163628b48aa14d4260f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbf00e6bf06193ddb9cb16fe338bb3c9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VideoDPO-Omni-Preference-Alignment-for-Video-Diffusion-Generation"><a href="#VideoDPO-Omni-Preference-Alignment-for-Video-Diffusion-Generation" class="headerlink" title="VideoDPO: Omni-Preference Alignment for Video Diffusion Generation"></a>VideoDPO: Omni-Preference Alignment for Video Diffusion Generation</h2><p><strong>Authors:Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing He, Renjie Pi, Qifeng Chen</strong></p>
<p>Recent progress in generative diffusion models has greatly advanced text-to-video generation. While text-to-video models trained on large-scale, diverse datasets can produce varied outputs, these generations often deviate from user preferences, highlighting the need for preference alignment on pre-trained models. Although Direct Preference Optimization (DPO) has demonstrated significant improvements in language and image generation, we pioneer its adaptation to video diffusion models and propose a VideoDPO pipeline by making several key adjustments. Unlike previous image alignment methods that focus solely on either (i) visual quality or (ii) semantic alignment between text and videos, we comprehensively consider both dimensions and construct a preference score accordingly, which we term the OmniScore. We design a pipeline to automatically collect preference pair data based on the proposed OmniScore and discover that re-weighting these pairs based on the score significantly impacts overall preference alignment. Our experiments demonstrate substantial improvements in both visual quality and semantic alignment, ensuring that no preference aspect is neglected. Code and data will be shared at <a target="_blank" rel="noopener" href="https://videodpo.github.io/">https://videodpo.github.io/</a>. </p>
<blockquote>
<p>ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æå¤§åœ°æ¨åŠ¨äº†æ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆã€‚è™½ç„¶åŸºäºå¤§è§„æ¨¡ã€å¤šæ ·åŒ–æ•°æ®é›†çš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹å¯ä»¥äº§ç”Ÿå¤šæ ·åŒ–çš„è¾“å‡ºï¼Œä½†è¿™äº›ç”Ÿæˆå¾€å¾€åç¦»ç”¨æˆ·åå¥½ï¼Œçªæ˜¾äº†å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåå¥½å¯¹é½çš„å¿…è¦æ€§ã€‚å°½ç®¡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨è¯­è¨€å’Œå›¾åƒç”Ÿæˆä¸­æ˜¾ç¤ºå‡ºæ˜¾ç€æ”¹è¿›ï¼Œä½†æˆ‘ä»¬é¦–åˆ›å°†å…¶é€‚åº”è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é€šè¿‡å‡ ä¸ªå…³é”®è°ƒæ•´æå‡ºäº†VideoDPOç®¡é“ã€‚ä¸ä»¥å¾€ä»…ä¾§é‡äºï¼ˆiï¼‰è§†è§‰è´¨é‡æˆ–ï¼ˆiiï¼‰æ–‡æœ¬å’Œè§†é¢‘ä¹‹é—´è¯­ä¹‰å¯¹é½çš„å›¾åƒå¯¹é½æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å…¨é¢è€ƒè™‘è¿™ä¸¤ä¸ªç»´åº¦ï¼Œå¹¶ç›¸åº”åœ°æ„å»ºåå¥½åˆ†æ•°ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºOmniScoreã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®¡é“ï¼ŒåŸºäºæå‡ºçš„OmniScoreè‡ªåŠ¨æ”¶é›†åå¥½å¯¹æ•°æ®ï¼Œå¹¶å‘ç°æ ¹æ®è¯¥åˆ†æ•°é‡æ–°æƒé‡è¿™äº›å¯¹ä¼šæ˜¾è‘—å½±å“æ•´ä½“çš„åå¥½å¯¹é½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨è§†è§‰è´¨é‡å’Œè¯­ä¹‰å¯¹é½æ–¹é¢éƒ½æœ‰å®è´¨æ€§çš„æ”¹è¿›ï¼Œç¡®ä¿ä¸ä¼šå¿½ç•¥ä»»ä½•åå¥½æ–¹é¢ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://videodpo.github.io/%E5%85%B1%E4%BA%AB%E3%80%82">https://videodpo.github.io/å…±äº«ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14167v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æå¤§åœ°æ¨åŠ¨äº†æ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆã€‚å°½ç®¡åœ¨å¤§å‹ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹å¯ä»¥äº§ç”Ÿå¤šæ ·åŒ–çš„è¾“å‡ºï¼Œä½†è¿™äº›è¾“å‡ºå¸¸å¸¸åç¦»ç”¨æˆ·åå¥½ï¼Œçªæ˜¾äº†å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåå¥½è°ƒæ•´çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬ç‡å…ˆå°†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰é€‚åº”åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶æå‡ºVideoDPOç®¡é“ï¼Œé€šè¿‡å‡ ä¸ªå…³é”®è°ƒæ•´æ¥å®ç°ã€‚ä¸åŒäºä¹‹å‰åªå…³æ³¨è§†è§‰è´¨é‡æˆ–æ–‡æœ¬ä¸è§†é¢‘ä¹‹é—´è¯­ä¹‰å¯¹é½çš„å›¾åƒå¯¹é½æ–¹æ³•ï¼Œæˆ‘ä»¬å…¨é¢è€ƒè™‘è¿™ä¸¤ä¸ªç»´åº¦ï¼Œå¹¶æ®æ­¤æ„å»ºäº†ä¸€ä¸ªåå¥½å¾—åˆ†ï¼Œç§°ä¸ºOmniScoreã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç®¡é“æ¥åŸºäºæå‡ºçš„OmniScoreè‡ªåŠ¨æ”¶é›†åå¥½å¯¹æ•°æ®ï¼Œå¹¶å‘ç°æ ¹æ®å¾—åˆ†é‡æ–°åŠ æƒè¿™äº›å¯¹å¯ä»¥æ˜¾è‘—å½±å“åå¥½å¯¹é½çš„æ•´ä½“æ•ˆæœã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨è§†è§‰è´¨é‡å’Œè¯­ä¹‰å¯¹é½æ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œç¡®ä¿ä¸ä¼šå¿½ç•¥ä»»ä½•åå¥½æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆé¢†åŸŸå—ç›Šäºç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ã€‚</li>
<li>æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„è¾“å‡ºå¸¸åç¦»ç”¨æˆ·åå¥½ï¼Œéœ€è¦è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹çš„åå¥½å¯¹é½ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¢«é¦–æ¬¡é€‚åº”åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå½¢æˆVideoDPOç®¡é“ã€‚</li>
<li>VideoDPOé€šè¿‡å…¨é¢è€ƒè™‘è§†è§‰è´¨é‡å’Œè¯­ä¹‰å¯¹é½æ¥æ„å»ºOmniScoreåå¥½å¾—åˆ†ã€‚</li>
<li>è‡ªåŠ¨æ”¶é›†åŸºäºOmniScoreçš„åå¥½å¯¹æ•°æ®æ˜¾ç¤ºé‡æ–°åŠ æƒå¯¹å¯ä»¥å½±å“åå¥½å¯¹é½çš„æ•´ä½“æ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜VideoDPOåœ¨è§†è§‰è´¨é‡å’Œè¯­ä¹‰å¯¹é½æ–¹é¢éƒ½æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e00a6b4509b3bd36128cd87aaf0833e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f876b5c01df4a8b0ff607e4a0e29bae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30787fbeb8f89e953ca8213825486234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-616b9555da1372418f12fc80dc21d17e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78108678dc8ecce823dd3448a991a689.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Text2Relight-Creative-Portrait-Relighting-with-Text-Guidance"><a href="#Text2Relight-Creative-Portrait-Relighting-with-Text-Guidance" class="headerlink" title="Text2Relight: Creative Portrait Relighting with Text Guidance"></a>Text2Relight: Creative Portrait Relighting with Text Guidance</h2><p><strong>Authors:Junuk Cha, Mengwei Ren, Krishna Kumar Singh, He Zhang, Yannick Hold-Geoffroy, Seunghyun Yoon, HyunJoon Jung, Jae Shin Yoon, Seungryul Baek</strong></p>
<p>We present a lighting-aware image editing pipeline that, given a portrait image and a text prompt, performs single image relighting. Our model modifies the lighting and color of both the foreground and background to align with the provided text description. The unbounded nature in creativeness of a text allows us to describe the lighting of a scene with any sensory features including temperature, emotion, smell, time, and so on. However, the modeling of such mapping between the unbounded text and lighting is extremely challenging due to the lack of dataset where there exists no scalable data that provides large pairs of text and relighting, and therefore, current text-driven image editing models does not generalize to lighting-specific use cases. We overcome this problem by introducing a novel data synthesis pipeline: First, diverse and creative text prompts that describe the scenes with various lighting are automatically generated under a crafted hierarchy using a large language model (<em>e.g.,</em> ChatGPT). A text-guided image generation model creates a lighting image that best matches the text. As a condition of the lighting images, we perform image-based relighting for both foreground and background using a single portrait image or a set of OLAT (One-Light-at-A-Time) images captured from lightstage system. Particularly for the background relighting, we represent the lighting image as a set of point lights and transfer them to other background images. A generative diffusion model learns the synthesized large-scale data with auxiliary task augmentation (<em>e.g.,</em> portrait delighting and light positioning) to correlate the latent text and lighting distribution for text-guided portrait relighting. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ„ŸçŸ¥ç…§æ˜çš„å›¾åƒç¼–è¾‘æµç¨‹ã€‚ç»™å®šè‚–åƒå›¾åƒå’Œæ–‡å­—æç¤ºï¼Œè¯¥æµç¨‹å¯ä»¥å¯¹å•å¹…å›¾åƒè¿›è¡Œé‡æ–°ç…§æ˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¼šä¿®æ”¹å‰æ™¯å’ŒèƒŒæ™¯çš„ç…§æ˜å’Œé¢œè‰²ï¼Œä»¥ç¬¦åˆæä¾›çš„æ–‡å­—æè¿°ã€‚æ–‡æœ¬çš„æ— ç•Œåˆ›é€ æ€§ä½¿æˆ‘ä»¬èƒ½å¤Ÿæè¿°ä»»ä½•æ„Ÿè§‰ç‰¹å¾çš„åœºæ™¯ç…§æ˜ï¼ŒåŒ…æ‹¬æ¸©åº¦ã€æƒ…æ„Ÿã€æ°”å‘³ã€æ—¶é—´ç­‰ç­‰ã€‚ç„¶è€Œï¼Œåœ¨æ–‡æœ¬å’Œç…§æ˜ä¹‹é—´è¿›è¡Œæ­¤ç±»æ˜ å°„çš„å»ºæ¨¡æå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¼ºä¹å¤§å‹æ•°æ®é›†æä¾›å¤§é‡çš„æ–‡æœ¬å’Œé‡æ–°ç…§æ˜é…å¯¹ã€‚å› æ­¤ï¼Œå½“å‰çš„æ–‡æœ¬é©±åŠ¨å›¾åƒç¼–è¾‘æ¨¡å‹å¹¶ä¸é€‚ç”¨äºç‰¹å®šçš„ç…§æ˜ç”¨ä¾‹ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥æ–°å‹æ•°æ®åˆæˆæµç¨‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼šé¦–å…ˆï¼Œåœ¨ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å±‚æ¬¡ç»“æ„ä¸‹ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ChatGPTï¼‰è‡ªåŠ¨ç”Ÿæˆæè¿°å„ç§ç…§æ˜çš„åœºæ™¯çš„å„ç§åˆ›æ„æ–‡æœ¬æç¤ºã€‚æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆæ¨¡å‹åˆ›å»ºä¸æ–‡æœ¬æœ€ä½³åŒ¹é…çš„ç…§æ˜å›¾åƒã€‚ä½œä¸ºç…§æ˜å›¾åƒçš„æ¡ä»¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å•å¹…è‚–åƒå›¾åƒæˆ–ä»ç¯å…‰èˆå°ç³»ç»Ÿæ•è·çš„ä¸€ç»„OLATï¼ˆä¸€æ¬¡ä¸€ä¸ªç¯å…‰ï¼‰å›¾åƒå¯¹å‰æ™¯å’ŒèƒŒæ™¯è¿›è¡ŒåŸºäºå›¾åƒçš„é‡æ–°ç…§æ˜ã€‚ç‰¹åˆ«æ˜¯é’ˆå¯¹èƒŒæ™¯é‡æ–°ç…§æ˜ï¼Œæˆ‘ä»¬å°†ç…§æ˜å›¾åƒè¡¨ç¤ºä¸ºä¸€ç»„ç‚¹å…‰æºå¹¶å°†å…¶è½¬ç§»åˆ°å…¶ä»–èƒŒæ™¯å›¾åƒä¸Šã€‚ç”Ÿæˆæ‰©æ•£æ¨¡å‹é€šè¿‡è¾…åŠ©ä»»åŠ¡å¢å¼ºï¼ˆä¾‹å¦‚è‚–åƒç…§æ˜å’Œç¯å…‰å®šä½ï¼‰æ¥å­¦ä¹ åˆæˆçš„å¤§è§„æ¨¡æ•°æ®ï¼Œä»è€Œå…³è”æ½œåœ¨æ–‡æœ¬å’Œç…§æ˜åˆ†å¸ƒï¼Œå®ç°æ–‡æœ¬å¼•å¯¼çš„è‚–åƒé‡æ–°ç…§æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13734v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ–‡æœ¬æç¤ºçš„å›¾åƒç¼–è¾‘æµç¨‹ï¼Œå®ƒå¯ä»¥æ ¹æ®æä¾›çš„è‚–åƒå›¾åƒå’Œæ–‡æœ¬æç¤ºè¿›è¡Œå•å›¾åƒé‡ç…§æ˜ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä¿®æ”¹å‰æ™¯å’ŒèƒŒæ™¯çš„å…‰ç…§å’Œé¢œè‰²ï¼Œä»¥ç¬¦åˆç»™å®šçš„æ–‡æœ¬æè¿°ã€‚æ–‡æœ¬æè¿°çš„åˆ›æ„æ— ç•Œæ€§å…è®¸æˆ‘ä»¬æè¿°åœºæ™¯çš„ä»»ä½•æ„Ÿå®˜ç‰¹å¾ï¼Œå¦‚æ¸©åº¦ã€æƒ…æ„Ÿã€æ°”å‘³ã€æ—¶é—´ç­‰ã€‚ç„¶è€Œï¼Œå°†è¿™ç§æ— ç•Œçš„æ–‡æœ¬ä¸ç…§æ˜ä¹‹é—´çš„æ˜ å°„å»ºæ¨¡æå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¼ºä¹ç›¸åº”çš„æ•°æ®é›†ï¼Œç°æœ‰çš„æ–‡æœ¬é©±åŠ¨å›¾åƒç¼–è¾‘æ¨¡å‹æ— æ³•æ¨å¹¿åˆ°ç…§æ˜ç‰¹å®šçš„ç”¨ä¾‹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ•°æ®åˆæˆæµç¨‹ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰åœ¨ç²¾å¿ƒè®¾è®¡çš„å±‚æ¬¡ç»“æ„ä¸‹è‡ªåŠ¨ç”Ÿæˆæè¿°åœºæ™¯å„ç§ç…§æ˜çš„å¤šæ ·åŒ–åˆ›æ„æ–‡æœ¬æç¤ºã€‚åŸºäºæ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒç”Ÿæˆæ¨¡å‹åˆ›å»ºä¸æ–‡æœ¬æœ€ä½³åŒ¹é…çš„ç…§æ˜å›¾åƒã€‚ä½œä¸ºç…§æ˜å›¾åƒçš„æ¡ä»¶ï¼Œæˆ‘ä»¬å¯¹å‰æ™¯å’ŒèƒŒæ™¯è¿›è¡Œäº†åŸºäºå›¾åƒçš„é‡æ–°ç…§æ˜ï¼Œä½¿ç”¨å•ä¸ªè‚–åƒå›¾åƒæˆ–ç”±å…‰èˆå°ç³»ç»Ÿæ•è·çš„ä¸€ç»„OLATï¼ˆä¸€æ¬¡ä¸€ä¸ªç¯å…‰ï¼‰å›¾åƒã€‚ç‰¹åˆ«æ˜¯é’ˆå¯¹èƒŒæ™¯é‡æ–°ç…§æ˜ï¼Œæˆ‘ä»¬å°†ç…§æ˜å›¾åƒè¡¨ç¤ºä¸ºä¸€ç»„ç‚¹å…‰æºå¹¶å°†å…¶è½¬ç§»åˆ°å…¶ä»–èƒŒæ™¯å›¾åƒä¸Šã€‚ç”Ÿæˆæ‰©æ•£æ¨¡å‹é€šè¿‡è¾…åŠ©ä»»åŠ¡å¢å¼ºï¼ˆå¦‚è‚–åƒé‡ç…§æ˜å’Œç¯å…‰å®šä½ï¼‰æ¥å­¦ä¹ åˆæˆçš„å¤§è§„æ¨¡æ•°æ®ï¼Œä»è€Œå…³è”æ½œåœ¨æ–‡æœ¬å’Œç…§æ˜åˆ†å¸ƒï¼Œä»¥å®ç°æ–‡æœ¬å¼•å¯¼çš„è‚–åƒé‡æ–°ç…§æ˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºæ–‡æœ¬æç¤ºçš„ç…§æ˜æ„ŸçŸ¥å›¾åƒç¼–è¾‘æµç¨‹ï¼Œèƒ½å¤Ÿå®ç°å•å›¾åƒé‡ç…§æ˜ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿä¿®æ”¹å‰æ™¯å’ŒèƒŒæ™¯çš„å…‰ç…§å’Œé¢œè‰²ä»¥ç¬¦åˆæ–‡æœ¬æè¿°ã€‚</li>
<li>åˆ›æ„æ— ç•Œçš„æ–‡æœ¬æè¿°å…è®¸æè¿°åœºæ™¯çš„å¤šç§æ„Ÿå®˜ç‰¹å¾ã€‚</li>
<li>æ•°æ®é›†ç¼ºä¹æ˜¯æ–‡æœ¬ä¸ç…§æ˜æ˜ å°„å»ºæ¨¡çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡æ•°æ®åˆæˆæµç¨‹å…‹æœæ­¤æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬æç¤ºã€‚</li>
<li>ä½¿ç”¨æ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒç”Ÿæˆæ¨¡å‹åˆ›å»ºä¸æ–‡æœ¬åŒ¹é…çš„ç…§æ˜å›¾åƒã€‚</li>
<li>é€šè¿‡å›¾åƒåŸºäºæ¡ä»¶çš„é‡ç…§æ˜æŠ€æœ¯å¤„ç†å‰æ™¯å’ŒèƒŒæ™¯ï¼Œåˆ©ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹å­¦ä¹ å¤§è§„æ¨¡æ•°æ®ä»¥å®ç°æ–‡æœ¬å¼•å¯¼çš„è‚–åƒé‡æ–°ç…§æ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a97dbf6bd263cdb96aef5fc1abfb6356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccf4a0b928ec891a2074e47b36508c9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cb443233d9e8a99f1b5980d64d672e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a588c2ad715c2e80adab7bfdd417fed0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d36c3edd7048ce9e3a4480745b1d9200.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03b1e6dd5392d9bb541629e137d8956d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VIIS-Visible-and-Infrared-Information-Synthesis-for-Severe-Low-light-Image-Enhancement"><a href="#VIIS-Visible-and-Infrared-Information-Synthesis-for-Severe-Low-light-Image-Enhancement" class="headerlink" title="VIIS: Visible and Infrared Information Synthesis for Severe Low-light   Image Enhancement"></a>VIIS: Visible and Infrared Information Synthesis for Severe Low-light   Image Enhancement</h2><p><strong>Authors:Chen Zhao, Mengyuan Yu, Fan Yang, Peiguang Jing</strong></p>
<p>Images captured in severe low-light circumstances often suffer from significant information absence. Existing singular modality image enhancement methods struggle to restore image regions lacking valid information. By leveraging light-impervious infrared images, visible and infrared image fusion methods have the potential to reveal information hidden in darkness. However, they primarily emphasize inter-modal complementation but neglect intra-modal enhancement, limiting the perceptual quality of output images. To address these limitations, we propose a novel task, dubbed visible and infrared information synthesis (VIIS), which aims to achieve both information enhancement and fusion of the two modalities. Given the difficulty in obtaining ground truth in the VIIS task, we design an information synthesis pretext task (ISPT) based on image augmentation. We employ a diffusion model as the framework and design a sparse attention-based dual-modalities residual (SADMR) conditioning mechanism to enhance information interaction between the two modalities. This mechanism enables features with prior knowledge from both modalities to adaptively and iteratively attend to each modalityâ€™s information during the denoising process. Our extensive experiments demonstrate that our model qualitatively and quantitatively outperforms not only the state-of-the-art methods in relevant fields but also the newly designed baselines capable of both information enhancement and fusion. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Chenz418/VIIS">https://github.com/Chenz418/VIIS</a>. </p>
<blockquote>
<p>åœ¨ä¸¥é‡ä½å…‰ç¯å¢ƒä¸‹æ•æ‰çš„å›¾åƒé€šå¸¸ç¼ºä¹é‡è¦ä¿¡æ¯ã€‚ç°æœ‰çš„å•ä¸€æ¨¡æ€å›¾åƒå¢å¼ºæ–¹æ³•åœ¨æ¢å¤ç¼ºä¹æœ‰æ•ˆä¿¡æ¯å›¾åƒåŒºåŸŸæ–¹é¢è¡¨ç°å›°éš¾ã€‚é€šè¿‡åˆ©ç”¨ä¸å—å…‰çº¿å½±å“çš„çº¢å¤–å›¾åƒï¼Œå¯è§å…‰å’Œçº¢å¤–å›¾åƒèåˆæ–¹æ³•å…·æœ‰æ­ç¤ºéšè—åœ¨é»‘æš—ä¸­çš„ä¿¡æ¯çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸»è¦ä¾§é‡äºè·¨æ¨¡æ€äº’è¡¥ï¼Œè€Œå¿½ç•¥äº†è·¨æ¨¡æ€å†…çš„å¢å¼ºï¼Œè¿™é™åˆ¶äº†è¾“å‡ºå›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºå¯è§å…‰å’Œçº¢å¤–ä¿¡æ¯åˆæˆï¼ˆVIISï¼‰ï¼Œæ—¨åœ¨å®ç°ä¸¤ä¸ªæ¨¡æ€çš„ä¿¡æ¯å¢å¼ºå’Œèåˆã€‚è€ƒè™‘åˆ°åœ¨VIISä»»åŠ¡ä¸­è·å–çœŸå®æ ‡ç­¾çš„éš¾åº¦ï¼Œæˆ‘ä»¬åŸºäºå›¾åƒå¢å¼ºè®¾è®¡äº†ä¸€ä¸ªä¿¡æ¯åˆæˆé¢„è®­ç»ƒä»»åŠ¡ï¼ˆISPTï¼‰ã€‚æˆ‘ä»¬é‡‡ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºæ¡†æ¶ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºç¨€ç–æ³¨æ„åŠ›çš„åŒæ¨¡æ€æ®‹å·®ï¼ˆSADMRï¼‰æ¡ä»¶æœºåˆ¶ï¼Œä»¥å¢å¼ºä¸¤ä¸ªæ¨¡æ€ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ã€‚è¯¥æœºåˆ¶ä½¿æ¥è‡ªä¸¤ä¸ªæ¨¡æ€çš„å…ˆéªŒçŸ¥è¯†ç‰¹å¾èƒ½å¤Ÿè‡ªé€‚åº”åœ°è¿­ä»£å…³æ³¨æ¯ä¸ªæ¨¡æ€çš„ä¿¡æ¯å»å™ªè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…åœ¨ç›¸å…³é¢†åŸŸæœ€å…ˆè¿›çš„æ–¹æ³•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè€Œä¸”åœ¨åŒæ—¶å…·å¤‡ä¿¡æ¯å¢å¼ºå’Œèåˆèƒ½åŠ›çš„æ–°è®¾è®¡åŸºå‡†ä¸Šä¹Ÿæœ‰å‡ºè‰²è¡¨ç°ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Chenz418/VIIS">https://github.com/Chenz418/VIIS</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13655v1">PDF</a> Accepted to WACV 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ä½å…‰ç¯å¢ƒä¸‹å›¾åƒä¿¡æ¯ç¼ºå¤±çš„é—®é¢˜ï¼Œç°æœ‰å›¾åƒå¢å¼ºæ–¹æ³•éš¾ä»¥æ¢å¤ç¼ºå¤±ä¿¡æ¯ã€‚é€šè¿‡ç»“åˆçº¢å¤–å›¾åƒï¼Œå¯è§å…‰å’Œçº¢å¤–å›¾åƒèåˆæ–¹æ³•èƒ½å¤Ÿæ­ç¤ºéšè—ä¿¡æ¯ã€‚ä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è·¨æ¨¡æ€äº’è¡¥ï¼Œå¿½è§†å•æ¨¡æ€å†…å¢å¼ºï¼Œå½±å“è¾“å‡ºå›¾åƒæ„ŸçŸ¥è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæå‡ºå¯è§å…‰å’Œçº¢å¤–ä¿¡æ¯åˆæˆï¼ˆVIISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å®ç°ä¿¡æ¯å¢å¼ºä¸åŒæ¨¡æ€èåˆã€‚é’ˆå¯¹VIISä»»åŠ¡éš¾ä»¥è·å–çœŸå®æ ‡æ³¨çš„é—®é¢˜ï¼Œè®¾è®¡åŸºäºå›¾åƒå¢å¼ºçš„ä¿¡æ¯åˆæˆé¢„è®­ç»ƒä»»åŠ¡ï¼ˆISPTï¼‰ã€‚é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œå¹¶è®¾è®¡åŸºäºç¨€ç–æ³¨æ„åŠ›çš„åŒæ¨¡æ€æ®‹å·®ï¼ˆSADMRï¼‰æ¡ä»¶æœºåˆ¶ï¼Œå¢å¼ºä¸¤æ¨¡æ€é—´çš„ä¿¡æ¯äº¤äº’ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç›¸å…³é¢†åŸŸä¸­ä¸ä»…ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œè€Œä¸”è¶…è¶Šä»…å…·å¤‡ä¿¡æ¯å¢å¼ºå’ŒèåˆåŠŸèƒ½çš„æ–°è®¾è®¡åŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½å…‰ç¯å¢ƒä¸‹çš„å›¾åƒå­˜åœ¨ä¿¡æ¯ç¼ºå¤±é—®é¢˜ï¼Œç°æœ‰å›¾åƒå¢å¼ºæ–¹æ³•éš¾ä»¥è§£å†³ã€‚</li>
<li>é€šè¿‡ç»“åˆçº¢å¤–å›¾åƒï¼Œå¯è§å…‰å’Œçº¢å¤–å›¾åƒèåˆæ–¹æ³•å¯ä»¥æ­ç¤ºéšè—ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è·¨æ¨¡æ€äº’è¡¥ï¼Œå¿½è§†å•æ¨¡æ€å†…çš„å¢å¼ºï¼Œå½±å“è¾“å‡ºå›¾åƒè´¨é‡ã€‚</li>
<li>æå‡ºå¯è§å…‰å’Œçº¢å¤–ä¿¡æ¯åˆæˆï¼ˆVIISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å®ç°ä¿¡æ¯å¢å¼ºä¸åŒæ¨¡æ€èåˆã€‚</li>
<li>é’ˆå¯¹VIISä»»åŠ¡éš¾ä»¥è·å–çœŸå®æ ‡æ³¨çš„é—®é¢˜ï¼Œè®¾è®¡åŸºäºå›¾åƒå¢å¼ºçš„ä¿¡æ¯åˆæˆé¢„è®­ç»ƒä»»åŠ¡ï¼ˆISPTï¼‰ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œå¹¶è®¾è®¡SADMRæ¡ä»¶æœºåˆ¶ä»¥å¢å¼ºä¸¤æ¨¡æ€é—´çš„ä¿¡æ¯äº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-56679ebe286bc200a5de62a2a69de24a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9e1c6deda7aa914b89f17d5bc207f03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e5f7e8ca525fa0277d0790ae1dc9fc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d667ae4a456350597b52a1556071a8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b1a32ad7254776f74a9080364fe4183.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42094ded1273105cc0528506dc8ddb15.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Real-time-One-Step-Diffusion-based-Expressive-Portrait-Videos-Generation"><a href="#Real-time-One-Step-Diffusion-based-Expressive-Portrait-Videos-Generation" class="headerlink" title="Real-time One-Step Diffusion-based Expressive Portrait Videos Generation"></a>Real-time One-Step Diffusion-based Expressive Portrait Videos Generation</h2><p><strong>Authors:Hanzhong Guo, Hongwei Yi, Daquan Zhou, Alexander William Bergman, Michael Lingelbach, Yizhou Yu</strong></p>
<p>Latent diffusion models have made great strides in generating expressive portrait videos with accurate lip-sync and natural motion from a single reference image and audio input. However, these models are far from real-time, often requiring many sampling steps that take minutes to generate even one second of video-significantly limiting practical use. We introduce OSA-LCM (One-Step Avatar Latent Consistency Model), paving the way for real-time diffusion-based avatars. Our method achieves comparable video quality to existing methods but requires only one sampling step, making it more than 10x faster. To accomplish this, we propose a novel avatar discriminator design that guides lip-audio consistency and motion expressiveness to enhance video quality in limited sampling steps. Additionally, we employ a second-stage training architecture using an editing fine-tuned method (EFT), transforming video generation into an editing task during training to effectively address the temporal gap challenge in single-step generation. Experiments demonstrate that OSA-LCM outperforms existing open-source portrait video generation models while operating more efficiently with a single sampling step. </p>
<blockquote>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨åˆ©ç”¨å•ä¸€å‚è€ƒå›¾åƒå’ŒéŸ³é¢‘è¾“å…¥ç”Ÿæˆè¡¨æƒ…ä¸°å¯Œçš„è‚–åƒè§†é¢‘æ–¹é¢å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œè¿™äº›è§†é¢‘å…·æœ‰å‡†ç¡®çš„å”‡éŸ³åŒæ­¥å’Œè‡ªç„¶åŠ¨ä½œã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹è¿˜è¿œè¿œè¾¾ä¸åˆ°å®æ—¶æ ‡å‡†ï¼Œé€šå¸¸éœ€è¦è®¸å¤šé‡‡æ ·æ­¥éª¤ï¼Œç”šè‡³ç”Ÿæˆä¸€ç§’é’Ÿçš„è§†é¢‘ä¹Ÿéœ€è¦æ•°åˆ†é’Ÿçš„æ—¶é—´ï¼Œä»è€Œæå¤§åœ°é™åˆ¶äº†å…¶å®é™…ä½¿ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†OSA-LCMï¼ˆä¸€æ­¥å¼åŒ–èº«æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼‰ï¼Œä¸ºåŸºäºæ‰©æ•£çš„å®æ—¶åŒ–èº«æŠ€æœ¯é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„è§†é¢‘è´¨é‡ï¼Œä½†ä»…éœ€ä¸€ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œä½¿å…¶é€Ÿåº¦è¶…è¿‡ç°æœ‰æ–¹æ³•åå€ä»¥ä¸Šã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åŒ–èº«é‰´åˆ«å™¨è®¾è®¡ï¼Œè¯¥è®¾è®¡ç”¨äºæŒ‡å¯¼å”‡éŸ³ä¸€è‡´æ€§å’ŒåŠ¨ä½œè¡¨ç°åŠ›ï¼Œåœ¨æœ‰é™çš„é‡‡æ ·æ­¥éª¤ä¸­æé«˜è§†é¢‘è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¬¬äºŒé˜¶æ®µè®­ç»ƒæ¶æ„ï¼Œä½¿ç”¨ç¼–è¾‘å¾®è°ƒæ–¹æ³•ï¼ˆEFTï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†è§†é¢‘ç”Ÿæˆè½¬å˜ä¸ºç¼–è¾‘ä»»åŠ¡ï¼Œä»¥æœ‰æ•ˆè§£å†³å•æ­¥ç”Ÿæˆä¸­çš„æ—¶é—´é—´éš”æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼ŒOSA-LCMåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºè‚–åƒè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ŒåŒæ—¶å‡­å€Ÿå•ä¸ªé‡‡æ ·æ­¥éª¤å®ç°äº†æ›´é«˜æ•ˆçš„æ“ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13479v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯ï¼Œåœ¨ä»…ä½¿ç”¨å•å¼ å‚è€ƒå›¾åƒå’ŒéŸ³é¢‘è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆå…·æœ‰ç²¾ç¡®å”‡åŒæ­¥å’Œè‡ªç„¶åŠ¨ä½œçš„è¡¨è¾¾æ€§è‚–åƒè§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹å¹¶éå®æ—¶ï¼Œéœ€è¦å¤§é‡é‡‡æ ·æ­¥éª¤ï¼Œç”šè‡³ç”Ÿæˆä¸€ç§’è§†é¢‘éƒ½éœ€è¦æ•°åˆ†é’Ÿæ—¶é—´ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å…¶å®é™…ä½¿ç”¨ã€‚æˆ‘ä»¬æ¨å‡ºOSA-LCMï¼ˆä¸€æ­¥å¼åŒ–èº«æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼‰ï¼Œä¸ºåŸºäºæ‰©æ•£çš„å®æ—¶åŒ–èº«æŠ€æœ¯é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„è§†é¢‘è´¨é‡ï¼Œä½†ä»…éœ€ä¸€ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œä½¿å…¶é€Ÿåº¦è¶…è¿‡ç°æœ‰æŠ€æœ¯åå€ä»¥ä¸Šã€‚é€šè¿‡è®¾è®¡æ–°å‹åŒ–èº«é‰´åˆ«å™¨ï¼Œå¼•å¯¼å”‡éŸ³é¢‘ä¸€è‡´æ€§å’ŒåŠ¨ä½œè¡¨è¾¾æ€§ï¼Œåœ¨æœ‰é™çš„é‡‡æ ·æ­¥éª¤ä¸­æé«˜è§†é¢‘è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¬¬äºŒé˜¶æ®µè®­ç»ƒæ¶æ„ï¼Œä½¿ç”¨ç¼–è¾‘å¾®è°ƒæ–¹æ³•ï¼ˆEFTï¼‰ï¼Œå°†è§†é¢‘ç”Ÿæˆè½¬å˜ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¼–è¾‘ä»»åŠ¡ï¼Œæœ‰æ•ˆè§£å†³å•æ­¥ç”Ÿæˆä¸­çš„æ—¶é—´é—´éš”æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼ŒOSA-LCMåœ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ï¼ŒåŒæ—¶ä»¥å•æ­¥é‡‡æ ·å®ç°æ›´é«˜æ•ˆçš„æ“ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè‚–åƒè§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤ŸåŸºäºå•å¼ å‚è€ƒå›¾åƒå’ŒéŸ³é¢‘è¾“å…¥ç”Ÿæˆå…·æœ‰ç²¾ç¡®å”‡åŒæ­¥å’Œè‡ªç„¶åŠ¨ä½œçš„è§†é¢‘ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹éå®æ—¶ï¼Œéœ€è¦å¤§é‡é‡‡æ ·æ­¥éª¤ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>OSA-LCMæ¨¡å‹å¼•å…¥ä¸€æ­¥å¼åŒ–èº«æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼Œå®ç°å®æ—¶æ‰©æ•£å¼åŒ–èº«æŠ€æœ¯ã€‚</li>
<li>OSA-LCMé€šè¿‡è®¾è®¡æ–°å‹åŒ–èº«é‰´åˆ«å™¨ï¼Œæé«˜è§†é¢‘è´¨é‡ï¼Œå®ç°ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„è§†é¢‘æ•ˆæœã€‚</li>
<li>OSA-LCMé‡‡ç”¨ç¼–è¾‘å¾®è°ƒæ–¹æ³•ï¼ˆEFTï¼‰ï¼Œå°†è§†é¢‘ç”Ÿæˆè½¬å˜ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¼–è¾‘ä»»åŠ¡ï¼Œè§£å†³å•æ­¥ç”Ÿæˆä¸­çš„æ—¶é—´é—´éš”æŒ‘æˆ˜ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒOSA-LCMåœ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d914279391907ce9b69f3053974749b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4eeaedbc45eaf6982ca87257e52026f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eadf55da456f403ed88f8c4788515d0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f30508cf7a229128760bc3f0a00a3179.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8234aa9c31a830b21a5dedaad83ce0d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Low-Light-Image-Enhancement-with-Diffusion-Prior"><a href="#Zero-Shot-Low-Light-Image-Enhancement-with-Diffusion-Prior" class="headerlink" title="Zero-Shot Low Light Image Enhancement with Diffusion Prior"></a>Zero-Shot Low Light Image Enhancement with Diffusion Prior</h2><p><strong>Authors:Joshua Cho, Sara Aghajanzadeh, Zhen Zhu, D. A. Forsyth</strong></p>
<p>Balancing aesthetic quality with fidelity when enhancing images from challenging, degraded sources is a core objective in computational photography. In this paper, we address low light image enhancement (LLIE), a task in which dark images often contain limited visible information. Diffusion models, known for their powerful image enhancement capacities, are a natural choice for this problem. However, their deep generative priors can also lead to hallucinations, introducing non-existent elements or substantially altering the visual semantics of the original scene. In this work, we introduce a novel zero-shot method for controlling and refining the generative behavior of diffusion models for dark-to-light image conversion tasks. Our method demonstrates superior performance over existing state-of-the-art methods in the task of low-light image enhancement, as evidenced by both quantitative metrics and qualitative analysis. </p>
<blockquote>
<p>åœ¨è®¡ç®—æ‘„å½±ä¸­ï¼Œå¹³è¡¡ç¾å­¦è´¨é‡ä¸ä¿çœŸåº¦ï¼ŒåŒæ—¶å¯¹æ¥è‡ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é€€åŒ–æºçš„å›¾åƒè¿›è¡Œå¢å¼ºï¼Œæ˜¯æ ¸å¿ƒç›®æ ‡ã€‚æœ¬æ–‡æˆ‘ä»¬ç ”ç©¶ä½å…‰å›¾åƒå¢å¼ºï¼ˆLLIEï¼‰é—®é¢˜ï¼Œæš—å›¾åƒå¾€å¾€åŒ…å«æœ‰é™çš„å¯è§†ä¿¡æ¯ã€‚æ‰©æ•£æ¨¡å‹ä»¥å…¶å¼ºå¤§çš„å›¾åƒå¢å¼ºèƒ½åŠ›è€Œè‘—ç§°ï¼Œå› æ­¤æ˜¯æ­¤é—®é¢˜çš„è‡ªç„¶é€‰æ‹©ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ·±åº¦ç”Ÿæˆå…ˆéªŒä¹Ÿå¯èƒ½å¯¼è‡´å¹»è§‰ï¼Œå¼•å…¥ä¸å­˜åœ¨çš„å…ƒç´ æˆ–å¤§å¹…æ”¹å˜åŸå§‹åœºæ™¯çš„å¯è§†è¯­ä¹‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºæ§åˆ¶å’Œç»†åŒ–æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¡Œä¸ºçš„æ–°å‹é›¶æ ·æœ¬æ–¹æ³•ï¼Œç”¨äºæš—åˆ°äº®çš„å›¾åƒè½¬æ¢ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æš—å…‰å›¾åƒå¢å¼ºä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¿™ç”±å®šé‡æŒ‡æ ‡å’Œå®šæ€§åˆ†æå‡è¯æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13401v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡èšç„¦äºä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ï¼Œæ—¨åœ¨å¹³è¡¡ç¾å­¦è´¨é‡ä¸ä¿çœŸåº¦ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¢å¼ºæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä¹Ÿå¯èƒ½äº§ç”Ÿå¹»è§‰ï¼Œå¼•å…¥ä¸å­˜åœ¨çš„å…ƒç´ æˆ–å¤§å¹…æ”¹å˜åŸå§‹åœºæ™¯çš„è§†è§‰è¯­ä¹‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œç”¨äºæ§åˆ¶å’Œä¼˜åŒ–æ‰©æ•£æ¨¡å‹åœ¨æš—å…‰å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­çš„ç”Ÿæˆè¡Œä¸ºï¼Œå¹¶åœ¨ä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å…³æ³¨ä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ï¼Œé‡ç‚¹æ˜¯åœ¨æŒ‘æˆ˜æ€§çš„é€€åŒ–å›¾åƒæºä¸­å¹³è¡¡ç¾å­¦è´¨é‡ä¸ä¿çœŸåº¦ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å› å…¶å¼ºå¤§çš„å›¾åƒå¢å¼ºèƒ½åŠ›è€Œè¢«é€‰ä¸ºè§£å†³æ­¤é—®é¢˜çš„è‡ªç„¶é€‰æ‹©ï¼Œä½†ä¹Ÿå­˜åœ¨ç”Ÿæˆå¹»è§‰çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œç”¨äºæ§åˆ¶å’Œä¼˜åŒ–æ‰©æ•£æ¨¡å‹åœ¨æš—å…‰å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­çš„ç”Ÿæˆè¡Œä¸ºã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å®šé‡æŒ‡æ ‡å’Œå®šæ€§åˆ†æè¯æ˜äº†åœ¨ä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ä¸­ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•çš„è¡¨ç°ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å…·æœ‰å¼ºå¤§çš„æ½œåŠ›ï¼Œä½†ä¹Ÿéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›æ¥æ§åˆ¶å…¶ç”Ÿæˆè¡Œä¸ºã€‚</li>
<li>å¯¹äºæš—å…‰å›¾åƒè½¬æ¢ä»»åŠ¡ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å¤šçš„æ§åˆ¶æ–¹æ³•å’Œä¼˜åŒ–ç­–ç•¥æ¥æé«˜å›¾åƒçš„è´¨é‡å’Œä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a89f5d1b46395c0996cf7593218c332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58775cca4ea2b4808f799d5bea37b75e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dff77926116c4166c96373633f710b32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-415ce6a784ca54ee6b5db2dcd38a33c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-681d37375584438b45fe645abf0b9902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1250d0476c3b60901c31a600537c67e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Marigold-DC-Zero-Shot-Monocular-Depth-Completion-with-Guided-Diffusion"><a href="#Marigold-DC-Zero-Shot-Monocular-Depth-Completion-with-Guided-Diffusion" class="headerlink" title="Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion"></a>Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion</h2><p><strong>Authors:Massimiliano Viola, Kevin Qu, Nando Metzger, Bingxin Ke, Alexander Becker, Konrad Schindler, Anton Obukhov</strong></p>
<p>Depth completion upgrades sparse depth measurements into dense depth maps guided by a conventional image. Existing methods for this highly ill-posed task operate in tightly constrained settings and tend to struggle when applied to images outside the training domain or when the available depth measurements are sparse, irregularly distributed, or of varying density. Inspired by recent advances in monocular depth estimation, we reframe depth completion as an image-conditional depth map generation guided by sparse measurements. Our method, Marigold-DC, builds on a pretrained latent diffusion model for monocular depth estimation and injects the depth observations as test-time guidance via an optimization scheme that runs in tandem with the iterative inference of denoising diffusion. The method exhibits excellent zero-shot generalization across a diverse range of environments and handles even extremely sparse guidance effectively. Our results suggest that contemporary monocular depth priors greatly robustify depth completion: it may be better to view the task as recovering dense depth from (dense) image pixels, guided by sparse depth; rather than as inpainting (sparse) depth, guided by an image. Project website: <a target="_blank" rel="noopener" href="https://marigolddepthcompletion.github.io/">https://MarigoldDepthCompletion.github.io/</a> </p>
<blockquote>
<p>æ·±åº¦è¡¥å…¨å°†ç¨€ç–çš„æ·±åº¦æµ‹é‡å€¼å‡çº§ä¸ºå—å¸¸è§„å›¾åƒå¼•å¯¼çš„å¯†é›†æ·±åº¦å›¾ã€‚é’ˆå¯¹è¿™ä¸€é«˜åº¦ä¸é€‚å®šçš„ä»»åŠ¡ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ä¸¥æ ¼å—é™çš„ç¯å¢ƒä¸­è¿è¡Œï¼Œå½“åº”ç”¨äºè®­ç»ƒåŸŸå¤–çš„å›¾åƒæˆ–å¯ç”¨çš„æ·±åº¦æµ‹é‡å€¼ç¨€ç–ã€åˆ†å¸ƒä¸å‡æˆ–å¯†åº¦ä¸ä¸€çš„æƒ…å†µä¸‹ï¼Œå¾€å¾€ä¼šé‡åˆ°å›°éš¾ã€‚å—å•ç›®æ·±åº¦ä¼°è®¡çš„æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬å°†æ·±åº¦è¡¥å…¨é‡æ–°å®šä½ä¸ºåŸºäºç¨€ç–æµ‹é‡å¼•å¯¼çš„å—å›¾åƒæ¡ä»¶æ§åˆ¶çš„æ·±åº¦å›¾ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•Marigold-DCå»ºç«‹åœ¨ç”¨äºå•ç›®æ·±åº¦ä¼°è®¡çš„é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡ä¸€ç§ä¼˜åŒ–æ–¹æ¡ˆåœ¨å»é™¤å™ªå£°æ‰©æ•£çš„è¿­ä»£æ¨æ–­è¿‡ç¨‹ä¸­æ³¨å…¥æ·±åº¦è§‚æµ‹å€¼ä½œä¸ºæµ‹è¯•æ—¶çš„æŒ‡å¯¼ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§ç¯å¢ƒä¸­å±•ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°å¤„ç†æå…¶ç¨€ç–çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“ä»£çš„å•ç›®æ·±åº¦å…ˆéªŒçŸ¥è¯†æå¤§åœ°å¢å¼ºäº†æ·±åº¦è¡¥å…¨çš„ç¨³å¥æ€§ï¼šå¯èƒ½æ›´å¥½çš„åšæ³•æ˜¯å°†å…¶è§†ä¸ºä»ï¼ˆå¯†é›†ï¼‰å›¾åƒåƒç´ ä¸­æ¢å¤å¯†é›†æ·±åº¦ï¼Œå—ç¨€ç–æ·±åº¦å¼•å¯¼ï¼›è€Œä¸æ˜¯å°†ä»»åŠ¡è§†ä¸ºåœ¨å›¾åƒå¼•å¯¼ä¸‹å¡«å……ï¼ˆç¨€ç–ï¼‰æ·±åº¦ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://marigolddepthcompletion.github.io/">https://MarigoldDepthCompletion.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13389v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMarigold-DCçš„æ·±åº¦å®Œæˆæ–¹æ³•ï¼Œå®ƒå°†æ·±åº¦å®Œæˆä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºåœ¨ç¨€ç–æµ‹é‡æŒ‡å¯¼ä¸‹ï¼ŒåŸºäºå›¾åƒçš„æ·±åº¦å›¾ç”Ÿæˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä¼˜åŒ–æ–¹æ¡ˆåœ¨æµ‹è¯•æ—¶æ³¨å…¥æ·±åº¦è§‚æµ‹å€¼ï¼Œä¸å»å™ªæ‰©æ•£çš„è¿­ä»£æ¨ç†å¹¶è¡Œè¿è¡Œã€‚è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¯å¤„ç†å„ç§ç¯å¢ƒï¼Œå¹¶å¯æœ‰æ•ˆå¤„ç†æå…¶ç¨€ç–çš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Marigold-DCæ–¹æ³•å°†æ·±åº¦å®Œæˆå®šä¹‰ä¸ºåœ¨ç¨€ç–æµ‹é‡æŒ‡å¯¼ä¸‹ï¼ŒåŸºäºå›¾åƒçš„æ·±åº¦å›¾ç”Ÿæˆã€‚</li>
<li>å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ³¨å…¥æ·±åº¦è§‚æµ‹å€¼ä½œä¸ºæµ‹è¯•æ—¶çš„æŒ‡å¯¼ã€‚</li>
<li>Marigold-DCé€šè¿‡ä¼˜åŒ–æ–¹æ¡ˆä¸å»å™ªæ‰©æ•£çš„è¿­ä»£æ¨ç†å¹¶è¡Œè¿è¡Œã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨å„ç§ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>Marigold-DCèƒ½æœ‰æ•ˆå¤„ç†æå…¶ç¨€ç–çš„æŒ‡å¯¼æƒ…å†µã€‚</li>
<li>ç°æœ‰æ·±åº¦å®Œæˆæ–¹æ³•å¾€å¾€å±€é™äºç‰¹å®šç¯å¢ƒæˆ–è®­ç»ƒåŸŸï¼Œè€ŒMarigold-DCå…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8c6f17604407b4d7f0de80b92a9bbea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05169c8bc9fd83097ace96381787fda4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-019bf016d910b28e2358f68a20756030.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Optimized-two-stage-AI-based-Neural-Decoding-for-Enhanced-Visual-Stimulus-Reconstruction-from-fMRI-Data"><a href="#Optimized-two-stage-AI-based-Neural-Decoding-for-Enhanced-Visual-Stimulus-Reconstruction-from-fMRI-Data" class="headerlink" title="Optimized two-stage AI-based Neural Decoding for Enhanced Visual   Stimulus Reconstruction from fMRI Data"></a>Optimized two-stage AI-based Neural Decoding for Enhanced Visual   Stimulus Reconstruction from fMRI Data</h2><p><strong>Authors:Lorenzo Veronese, Andrea Moglia, Luca Mainardi, Pietro Cerveri</strong></p>
<p>AI-based neural decoding reconstructs visual perception by leveraging generative models to map brain activity, measured through functional MRI (fMRI), into latent hierarchical representations. Traditionally, ridge linear models transform fMRI into a latent space, which is then decoded using latent diffusion models (LDM) via a pre-trained variational autoencoder (VAE). Due to the complexity and noisiness of fMRI data, newer approaches split the reconstruction into two sequential steps, the first one providing a rough visual approximation, the second on improving the stimulus prediction via LDM endowed by CLIP embeddings. This work proposes a non-linear deep network to improve fMRI latent space representation, optimizing the dimensionality alike. Experiments on the Natural Scenes Dataset showed that the proposed architecture improved the structural similarity of the reconstructed image by about 2% with respect to the state-of-the-art model, based on ridge linear transform. The reconstructed imageâ€™s semantics improved by about 4%, measured by perceptual similarity, with respect to the state-of-the-art. The noise sensitivity analysis of the LDM showed that the role of the first stage was fundamental to predict the stimulus featuring high structural similarity. Conversely, providing a large noise stimulus affected less the semantics of the predicted stimulus, while the structural similarity between the ground truth and predicted stimulus was very poor. The findings underscore the importance of leveraging non-linear relationships between BOLD signal and the latent representation and two-stage generative AI for optimizing the fidelity of reconstructed visual stimuli from noisy fMRI data. </p>
<blockquote>
<p>åŸºäºäººå·¥æ™ºèƒ½çš„ç¥ç»ç½‘ç»œè§£ç é€šè¿‡åˆ©ç”¨ç”Ÿæˆæ¨¡å‹å°†åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æµ‹é‡çš„è„‘æ´»åŠ¨æ˜ å°„åˆ°æ½œåœ¨å±‚æ¬¡è¡¨ç¤ºï¼Œä»è€Œé‡å»ºè§†è§‰æ„ŸçŸ¥ã€‚ä¼ ç»Ÿä¸Šï¼Œå²­çº¿æ€§æ¨¡å‹å°†fMRIè½¬æ¢ä¸ºæ½œåœ¨ç©ºé—´ï¼Œç„¶åä½¿ç”¨é¢„è®­ç»ƒçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰é€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è¿›è¡Œè§£ç ã€‚ç”±äºfMRIæ•°æ®çš„å¤æ‚æ€§å’Œå™ªå£°ï¼Œæ›´æ–°çš„æ–¹æ³•å°†é‡å»ºè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªè¿ç»­æ­¥éª¤ï¼Œç¬¬ä¸€ä¸ªæ­¥éª¤æä¾›ç²—ç•¥çš„è§†è§‰è¿‘ä¼¼ï¼Œç¬¬äºŒä¸ªæ­¥éª¤é€šè¿‡LDMå’ŒCLIPåµŒå…¥æ”¹è¿›åˆºæ¿€é¢„æµ‹ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªéçº¿æ€§æ·±åº¦ç½‘ç»œæ¥æ”¹å–„fMRIçš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼ŒåŒæ—¶ä¼˜åŒ–ç»´åº¦ã€‚åœ¨è‡ªç„¶åœºæ™¯æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºå²­çº¿æ€§å˜æ¢çš„æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ¶æ„æ”¹è¿›äº†é‡å»ºå›¾åƒçš„ç»“æ„ç›¸ä¼¼æ€§çº¦2%ã€‚åœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§æ–¹é¢ï¼Œé‡å»ºå›¾åƒçš„è¯­ä¹‰æ”¹è¿›äº†çº¦4%ã€‚LDMçš„å™ªå£°æ•æ„Ÿæ€§åˆ†æè¡¨æ˜ï¼Œç¬¬ä¸€é˜¶æ®µåœ¨é¢„æµ‹å…·æœ‰é«˜ç»“æ„ç›¸ä¼¼æ€§çš„åˆºæ¿€æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç›¸åï¼Œæä¾›å¤§å™ªå£°åˆºæ¿€å¯¹é¢„æµ‹åˆºæ¿€çš„è¯­ä¹‰å½±å“è¾ƒå°ï¼Œè€Œåœ°é¢çœŸå®æ•°æ®å’Œé¢„æµ‹åˆºæ¿€ä¹‹é—´çš„ç»“æ„ç›¸ä¼¼æ€§éå¸¸å·®ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åˆ©ç”¨BOLDä¿¡å·å’Œæ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„éçº¿æ€§å…³ç³»ä»¥åŠä¸¤é˜¶æ®µç”Ÿæˆäººå·¥æ™ºèƒ½åœ¨ä¼˜åŒ–ä»å™ªå£°fMRIæ•°æ®ä¸­é‡å»ºçš„è§†è§‰åˆºæ¿€ä¿çœŸåº¦æ–¹é¢çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13237v1">PDF</a> 14 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºäººå·¥æ™ºèƒ½çš„ç¥ç»ç½‘ç»œè§£ç é€šè¿‡åˆ©ç”¨ç”Ÿæˆæ¨¡å‹å°†åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æµ‹é‡çš„è„‘æ´»åŠ¨æ˜ å°„åˆ°æ½œåœ¨å±‚æ¬¡è¡¨ç¤ºä¸­ï¼Œé‡å»ºè§†è§‰æ„ŸçŸ¥ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§éçº¿æ€§æ·±åº¦ç½‘ç»œï¼Œä¼˜åŒ–fMRIçš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼Œåœ¨ç»´åº¦ä¸Šå®ç°ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºå²­çº¿æ€§å˜æ¢çš„æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¶æ„æé«˜äº†é‡å»ºå›¾åƒçš„æ„é€ ç›¸ä¼¼æ€§çº¦2%ï¼Œåœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§æ–¹é¢ä¹Ÿæé«˜äº†çº¦4%ã€‚å¯¹LDMçš„å™ªå£°æ•æ„Ÿæ€§åˆ†ææ˜¾ç¤ºï¼Œç¬¬ä¸€é˜¶æ®µåœ¨é¢„æµ‹å…·æœ‰é«˜å¼ºåº¦æ„é€ ç›¸ä¼¼æ€§çš„åˆºæ¿€æ—¶èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç›¸åï¼Œæä¾›å¤§é‡å™ªå£°åˆºæ¿€å¯¹é¢„æµ‹åˆºæ¿€çš„è¯­ä¹‰å½±å“è¾ƒå°ï¼Œè€Œåœ°é¢çœŸå®æ•°æ®ä¸é¢„æµ‹åˆºæ¿€ä¹‹é—´çš„æ„é€ ç›¸ä¼¼æ€§åˆ™è¾ƒå·®ã€‚ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨BOLDä¿¡å·å’Œæ½œåœ¨è¡¨ç°ä¹‹é—´çš„éçº¿æ€§å…³ç³»ä»¥åŠä¸¤é˜¶æ®µç”Ÿæˆäººå·¥æ™ºèƒ½åœ¨ä¼˜åŒ–ä»å™ªå£°fMRIæ•°æ®ä¸­é‡å»ºçš„è§†è§‰åˆºæ¿€ä¿çœŸåº¦ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AI-based neural decodingåˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ˜ å°„fMRIæ•°æ®åˆ°æ½œåœ¨å±‚æ¬¡è¡¨ç¤ºï¼Œé‡å»ºè§†è§‰æ„ŸçŸ¥ã€‚</li>
<li>æå‡ºéçº¿æ€§æ·±åº¦ç½‘ç»œæ”¹è¿›fMRIçš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼Œä¼˜åŒ–ç»´åº¦ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œæ–°æ¶æ„æé«˜äº†é‡å»ºå›¾åƒçš„æ„é€ ç›¸ä¼¼æ€§çº¦2%ï¼Œæ„ŸçŸ¥ç›¸ä¼¼æ€§çº¦4%ã€‚</li>
<li>LDMçš„å™ªå£°æ•æ„Ÿæ€§åˆ†ææ˜¾ç¤ºç¬¬ä¸€é˜¶æ®µåœ¨é¢„æµ‹é«˜å¼ºåº¦æ„é€ ç›¸ä¼¼æ€§åˆºæ¿€æ—¶è‡³å…³é‡è¦ã€‚</li>
<li>å™ªå£°å¯¹é¢„æµ‹åˆºæ¿€çš„è¯­ä¹‰å½±å“è¾ƒå°ï¼Œä½†å¯¹æ„é€ ç›¸ä¼¼æ€§çš„å½±å“è¾ƒå¤§ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨BOLDä¿¡å·å’Œæ½œåœ¨è¡¨ç°ä¹‹é—´çš„éçº¿æ€§å…³ç³»çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-716f23b2c3ee304a44b74deba41e41b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96fdbfb4127e0c56c0ec253bf43cb5bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ab08cae9d525ab76c69706147b5ee74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2c8e1d57dc2f8555303bde065042c9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-058e48cb19ea3cb3fcd7b9ee92061e80.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CoMPaSS-Enhancing-Spatial-Understanding-in-Text-to-Image-Diffusion-Models"><a href="#CoMPaSS-Enhancing-Spatial-Understanding-in-Text-to-Image-Diffusion-Models" class="headerlink" title="CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion   Models"></a>CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion   Models</h2><p><strong>Authors:Gaoyang Zhang, Bingtao Fu, Qingnan Fan, Qi Zhang, Runxing Liu, Hong Gu, Huaqi Zhang, Xinguo Liu</strong></p>
<p>Text-to-image diffusion models excel at generating photorealistic images, but commonly struggle to render accurate spatial relationships described in text prompts. We identify two core issues underlying this common failure: 1) the ambiguous nature of spatial-related data in existing datasets, and 2) the inability of current text encoders to accurately interpret the spatial semantics of input descriptions. We address these issues with CoMPaSS, a versatile training framework that enhances spatial understanding of any T2I diffusion model. CoMPaSS solves the ambiguity of spatial-related data with the Spatial Constraints-Oriented Pairing (SCOP) data engine, which curates spatially-accurate training data through a set of principled spatial constraints. To better exploit the curated high-quality spatial priors, CoMPaSS further introduces a Token ENcoding ORdering (TENOR) module to allow better exploitation of high-quality spatial priors, effectively compensating for the shortcoming of text encoders. Extensive experiments on four popular open-weight T2I diffusion models covering both UNet- and MMDiT-based architectures demonstrate the effectiveness of CoMPaSS by setting new state-of-the-arts with substantial relative gains across well-known benchmarks on spatial relationships generation, including VISOR (+98%), T2I-CompBench Spatial (+67%), and GenEval Position (+131%). Code will be available at <a target="_blank" rel="noopener" href="https://github.com/blurgyy/CoMPaSS">https://github.com/blurgyy/CoMPaSS</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸçš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å‘ˆç°æ–‡æœ¬æç¤ºä¸­æè¿°çš„ç©ºé—´å…³ç³»æ—¶é€šå¸¸é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬ç¡®å®šäº†è¿™ç§å¸¸è§å¤±è´¥èƒŒåçš„ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š1ï¼‰ç°æœ‰æ•°æ®é›†ä¸­ç©ºé—´ç›¸å…³æ•°æ®çš„æ¨¡ç³Šæ€§è´¨ï¼›2ï¼‰å½“å‰æ–‡æœ¬ç¼–ç å™¨æ— æ³•å‡†ç¡®è§£é‡Šè¾“å…¥æè¿°çš„ç©ºé—´è¯­ä¹‰ã€‚æˆ‘ä»¬é€šè¿‡CoMPaSSè§£å†³è¿™äº›é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„è®­ç»ƒæ¡†æ¶ï¼Œå¯ä»¥æé«˜ä»»ä½•T2Iæ‰©æ•£æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚CoMPaSSé€šè¿‡é¢å‘ç©ºé—´çº¦æŸé…å¯¹ï¼ˆSCOPï¼‰æ•°æ®å¼•æ“è§£å†³ç©ºé—´ç›¸å…³æ•°æ®çš„æ¨¡ç³Šæ€§é—®é¢˜ï¼Œè¯¥å¼•æ“é€šè¿‡ä¸€ç³»åˆ—æœ‰åŸåˆ™çš„ç©ºé—´çº¦æŸæ¥ç­–åˆ’ç©ºé—´ç²¾ç¡®çš„è®­ç»ƒæ•°æ®ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç²¾é€‰çš„é«˜è´¨é‡ç©ºé—´å…ˆéªŒçŸ¥è¯†ï¼ŒCoMPaSSè¿˜å¼•å…¥äº†Token ENcoding ORderingï¼ˆTENORï¼‰æ¨¡å—ï¼Œå…è®¸æ›´æœ‰æ•ˆåœ°åˆ©ç”¨é«˜è´¨é‡ç©ºé—´å…ˆéªŒçŸ¥è¯†ï¼Œæœ‰æ•ˆåœ°å¼¥è¡¥äº†æ–‡æœ¬ç¼–ç å™¨çš„ä¸è¶³ã€‚åœ¨å››ä¸ªæµè¡Œçš„å¼€æ”¾æƒé‡T2Iæ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œæ¶µç›–äº†åŸºäºUNetå’ŒMMDiTçš„æ¶æ„ï¼Œè¯æ˜äº†CoMPaSSçš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åœ¨ç©ºé—´å…³ç³»ç”Ÿæˆæ–¹é¢è®¾ç½®æ–°çš„æœ€æ–°æŠ€æœ¯ï¼Œå¹¶åœ¨VISORï¼ˆ+98%ï¼‰ã€T2I-CompBench Spatialï¼ˆ+67%ï¼‰å’ŒGenEval Positionï¼ˆ+131%ï¼‰ç­‰åŸºå‡†æµ‹è¯•ä¸Šå®ç°ç›¸å¯¹è¾ƒå¤§çš„æå‡ï¼Œè¯æ˜äº†å…¶æ•ˆæœã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/blurgyy/CoMPaSS">https://github.com/blurgyy/CoMPaSS</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13195v1">PDF</a> 18 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹æ“…é•¿ç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä½†åœ¨æ¸²æŸ“æ–‡æœ¬æç¤ºä¸­æè¿°çš„ç©ºé—´å…³ç³»æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚æœ¬ç ”ç©¶è¯†åˆ«å‡ºä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š1ï¼‰ç°æœ‰æ•°æ®é›†ä¸­ç©ºé—´ç›¸å…³æ•°æ®çš„æ¨¡ç³Šæ€§ï¼›2ï¼‰å½“å‰æ–‡æœ¬ç¼–ç å™¨æ— æ³•å‡†ç¡®è§£é‡Šè¾“å…¥æè¿°çš„ç©ºé—´è¯­ä¹‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†CoMPaSSï¼Œä¸€ä¸ªé€šç”¨çš„è®­ç»ƒæ¡†æ¶ï¼Œå¯å¢å¼ºä»»ä½•T2Iæ‰©æ•£æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚CoMPaSSé€šè¿‡åŸºäºåŸåˆ™çš„ç©ºé—´çº¦æŸè§£å†³ç©ºé—´ç›¸å…³æ•°æ®çš„æ¨¡ç³Šæ€§ï¼Œå¹¶å¼•å…¥Token ENcoding ORderingï¼ˆTENORï¼‰æ¨¡å—ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨é«˜è´¨é‡çš„ç©ºé—´å…ˆéªŒçŸ¥è¯†ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†æ–‡æœ¬ç¼–ç å™¨çš„ä¸è¶³ã€‚åœ¨å››ä¸ªæµè¡Œçš„å¼€æ”¾æƒé‡T2Iæ‰©æ•£æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoMPaSSåœ¨ç”Ÿæˆç©ºé—´å…³ç³»æ–¹é¢çš„è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨VISORï¼ˆ+98%ï¼‰ã€T2I-CompBench Spatialï¼ˆ+67%ï¼‰å’ŒGenEval Positionï¼ˆ+131%ï¼‰ç­‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—æ˜¾è‘—ç›¸å¯¹å¢ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆç©ºé—´å…³ç³»æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ä¸­ç©ºé—´ç›¸å…³æ•°æ®çš„æ¨¡ç³Šæ€§å’Œæ–‡æœ¬ç¼–ç å™¨å¯¹ç©ºé—´è¯­ä¹‰çš„è§£è¯»èƒ½åŠ›æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>CoMPaSSæ¡†æ¶é€šè¿‡è§£å†³ç©ºé—´ç›¸å…³æ•°æ®çš„æ¨¡ç³Šæ€§å’Œå¢å¼ºç©ºé—´ç†è§£èƒ½åŠ›æ¥æ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>CoMPaSSä½¿ç”¨Spatial Constraints-Oriented Pairingï¼ˆSCOPï¼‰æ•°æ®å¼•æ“å’ŒToken ENcoding ORderingï¼ˆTENORï¼‰æ¨¡å—æ¥å®ç°ä¸Šè¿°ç›®æ ‡ã€‚</li>
<li>CoMPaSSåœ¨å¤šç§æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå–å¾—æ˜¾è‘—ç›¸å¯¹å¢ç›Šã€‚</li>
<li>è¯¥æ¡†æ¶é€‚ç”¨äºå¤šç§T2Iæ‰©æ•£æ¨¡å‹ï¼ŒåŒ…æ‹¬UNet-å’ŒMMDiT-basedæ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13195">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-255c50838e3dd3c0bd1aa2b5fb7701ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-867ef474db121a27c24f1fe8a97b970e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d805f2c0fe6be9db54979f8e158c6076.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d312d40b35e6ba86c7de9aa57cbd7668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69bf501bc4042201f429806f716384b3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Prompt-Augmentation-for-Self-supervised-Text-guided-Image-Manipulation"><a href="#Prompt-Augmentation-for-Self-supervised-Text-guided-Image-Manipulation" class="headerlink" title="Prompt Augmentation for Self-supervised Text-guided Image Manipulation"></a>Prompt Augmentation for Self-supervised Text-guided Image Manipulation</h2><p><strong>Authors:Rumeysa Bodur, Binod Bhattarai, Tae-Kyun Kim</strong></p>
<p>Text-guided image editing finds applications in various creative and practical fields. While recent studies in image generation have advanced the field, they often struggle with the dual challenges of coherent image transformation and context preservation. In response, our work introduces prompt augmentation, a method amplifying a single input prompt into several target prompts, strengthening textual context and enabling localised image editing. Specifically, we use the augmented prompts to delineate the intended manipulation area. We propose a Contrastive Loss tailored to driving effective image editing by displacing edited areas and drawing preserved regions closer. Acknowledging the continuous nature of image manipulations, we further refine our approach by incorporating the similarity concept, creating a Soft Contrastive Loss. The new losses are incorporated to the diffusion model, demonstrating improved or competitive image editing results on public datasets and generated images over state-of-the-art approaches. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘åœ¨å„ç§åˆ›æ„å’Œå®è·µé¢†åŸŸéƒ½æœ‰åº”ç”¨ã€‚è™½ç„¶æœ€è¿‘çš„å›¾åƒç”Ÿæˆç ”ç©¶å·²ç»æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œä½†å®ƒä»¬ç»å¸¸é¢ä¸´ä¸€è‡´çš„å›¾åƒè½¬æ¢å’Œä¸Šä¸‹æ–‡ä¿ç•™çš„åŒé‡æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†æç¤ºå¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å•ä¸ªè¾“å…¥æç¤ºæ”¾å¤§ä¸ºå¤šä¸ªç›®æ ‡æç¤ºï¼Œå¢å¼ºäº†æ–‡æœ¬ä¸Šä¸‹æ–‡å¹¶å®ç°äº†å±€éƒ¨å›¾åƒç¼–è¾‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å¢å¼ºçš„æç¤ºæ¥æç»˜é¢„æœŸçš„æ“çºµåŒºåŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹é©±åŠ¨æœ‰æ•ˆå›¾åƒç¼–è¾‘çš„å¯¹æ¯”æŸå¤±ï¼Œé€šè¿‡ä½ç§»ç¼–è¾‘åŒºåŸŸå¹¶å°†ä¿ç•™çš„åŒºåŸŸæ‹‰è¿‘ã€‚è€ƒè™‘åˆ°å›¾åƒæ“ä½œçš„è¿ç»­æ€§ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ç›¸ä¼¼åº¦çš„æ¦‚å¿µè¿›ä¸€æ­¥æ”¹è¿›äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåˆ›å»ºäº†è½¯å¯¹æ¯”æŸå¤±ã€‚æ–°çš„æŸå¤±è¢«çº³å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œåœ¨å…¬å…±æ•°æ®é›†å’Œç”Ÿæˆå›¾åƒä¸Šçš„å›¾åƒç¼–è¾‘ç»“æœæœ‰æ‰€æ”¹è¿›æˆ–å…·æœ‰ç«äº‰åŠ›ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13081v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æŠ€æœ¯åœ¨ä¸åŒåˆ›æ„å’Œå®è·µé¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹å›¾åƒç”Ÿæˆä¸­çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æç¤ºå¢å¼ºæ³•ï¼Œé€šè¿‡æ”¾å¤§å•ä¸ªè¾“å…¥æç¤ºä¸ºå¤šä¸ªç›®æ ‡æç¤ºï¼Œå¼ºåŒ–æ–‡æœ¬ä¸Šä¸‹æ–‡å¹¶å®ç°å±€éƒ¨å›¾åƒç¼–è¾‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å¢å¼ºçš„æç¤ºæ¥æç»˜é¢„æœŸçš„ä¿®æ”¹åŒºåŸŸï¼Œå¹¶æå‡ºä¸€ç§å¯¹æ¯”æŸå¤±æ¥é©±åŠ¨æœ‰æ•ˆçš„å›¾åƒç¼–è¾‘ï¼Œé€šè¿‡ä½ç§»ç¼–è¾‘åŒºåŸŸå¹¶æ‹‰è¿‘ä¿ç•™åŒºåŸŸã€‚è€ƒè™‘åˆ°å›¾åƒæ“ä½œçš„è¿ç»­æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å¼•å…¥ç›¸ä¼¼æ€§çš„æ¦‚å¿µå®Œå–„äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåˆ›å»ºäº†è½¯å¯¹æ¯”æŸå¤±ã€‚æ–°çš„æŸå¤±è¢«çº³å…¥æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å…¬å…±æ•°æ®é›†å’Œç”Ÿæˆå›¾åƒä¸Šå±•ç¤ºäº†æ”¹è¿›æˆ–å…·æœ‰ç«äº‰åŠ›çš„å›¾åƒç¼–è¾‘ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æŠ€æœ¯å¹¿æ³›åº”ç”¨äºåˆ›æ„å’Œå®è·µé¢†åŸŸã€‚</li>
<li>æç¤ºå¢å¼ºæ³•èƒ½å¤Ÿå¼ºåŒ–æ–‡æœ¬ä¸Šä¸‹æ–‡å¹¶å®ç°å±€éƒ¨å›¾åƒç¼–è¾‘ã€‚</li>
<li>å¢å¼ºçš„æç¤ºè¢«ç”¨äºæç»˜é¢„æœŸçš„å›¾åƒä¿®æ”¹åŒºåŸŸã€‚</li>
<li>å¯¹æ¯”æŸå¤±è¢«ç”¨æ¥é©±åŠ¨æœ‰æ•ˆçš„å›¾åƒç¼–è¾‘ã€‚</li>
<li>ä½ç§»ç¼–è¾‘åŒºåŸŸå¹¶æ‹‰è¿‘ä¿ç•™åŒºåŸŸä»¥å¢å¼ºå›¾åƒç¼–è¾‘æ•ˆæœã€‚</li>
<li>è€ƒè™‘åˆ°å›¾åƒæ“ä½œçš„è¿ç»­æ€§ï¼Œå¼•å…¥äº†ç›¸ä¼¼æ€§çš„æ¦‚å¿µä»¥å®Œå–„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae3f4de5809d67e571220a84942803f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e936ab9a9e68171b92830104214ac23f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f753cf1a7fd37d6dcf0eac2a3e3283.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance"><a href="#Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance" class="headerlink" title="Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance"></a>Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance</h2><p><strong>Authors:Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang</strong></p>
<p>Recently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after removal. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pre-trained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„æ–°æ™‹è€…è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ã€‚ç„¶è€Œï¼Œå½“ç”¨äºå¯¹è±¡å»é™¤ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚äº§ç”Ÿéšæœºä¼ªå½±å’Œåœ¨å»é™¤åæ— æ³•ç”¨é€‚å½“å†…å®¹é‡æ–°ç»˜åˆ¶å‰æ™¯å¯¹è±¡åŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œAttentive Eraserâ€ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è°ƒæ•´çš„æ–¹æ³•ï¼Œå¯ä»¥ä¸ºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æä¾›ç¨³å®šæœ‰æ•ˆçš„å¯¹è±¡å»é™¤èƒ½åŠ›ã€‚é¦–å…ˆï¼ŒåŸºäºè§‚å¯Ÿåˆ°è‡ªæ³¨æ„åŠ›å›¾å½±å“ç”Ÿæˆå›¾åƒçš„ç»“æ„å’Œå½¢çŠ¶ç»†èŠ‚ï¼Œæˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›æ¿€æ´»å’ŒæŠ‘åˆ¶ï¼ˆASSï¼‰ï¼Œå®ƒæ ¹æ®ç»™å®šçš„æ©è†œé‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œåœ¨åå‘ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†èƒŒæ™¯è€Œéå‰æ™¯å¯¹è±¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæ³¨æ„åŠ›é‡å®šå‘æŒ‡å¯¼ï¼ˆSARGï¼‰ï¼Œå®ƒåˆ©ç”¨ASSå¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°åœ¨æ©è†œå†…å»é™¤å‰æ™¯å¯¹è±¡ï¼ŒåŒæ—¶ç”Ÿæˆæ—¢åˆç†åˆè¿è´¯çš„å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒAttentive Eraseråœ¨å„ç§é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­çš„å¯¹è±¡å»é™¤è¡¨ç°ç¨³å®šä¸”æœ‰æ•ˆï¼Œç”šè‡³è¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒAttentive Eraserå¯åœ¨å„ç§æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹ä¸­å®ç°ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12974v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸå´­éœ²å¤´è§’ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œåœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸­ï¼Œä»å­˜åœ¨ç”Ÿæˆéšæœºç‘•ç–µå’Œç§»é™¤å‰æ™¯å¯¹è±¡åæ— æ³•é‡æ–°ç»˜åˆ¶é€‚å½“å†…å®¹çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºæ— éœ€è°ƒæ•´çš„â€œAttentive Eraserâ€æ–¹æ³•ï¼Œä½¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œç¨³å®šæœ‰æ•ˆçš„å¯¹è±¡ç§»é™¤ã€‚é€šè¿‡åˆ©ç”¨è‡ªæˆ‘å…³æ³¨åœ°å›¾å½±å“å›¾åƒç»“æ„å’Œå½¢çŠ¶ç»†èŠ‚çš„è§‚å¯Ÿï¼Œæå‡ºåŸºäºç»™å®šæ©è†œçš„è‡ªæˆ‘å…³æ³¨æœºåˆ¶çš„æ¿€æ´»å’ŒæŠ‘åˆ¶ï¼ˆASSï¼‰ï¼Œåœ¨åå‘ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†èƒŒæ™¯è€Œéå‰æ™¯å¯¹è±¡ã€‚æ­¤å¤–ï¼Œå¼•å…¥è‡ªæˆ‘å…³æ³¨é‡å®šå‘æŒ‡å¯¼ï¼ˆSARGï¼‰ï¼Œåˆ©ç”¨ASSé‡å®šå‘çš„è‡ªæˆ‘å…³æ³¨æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œåœ¨æ©è†œå†…æœ‰æ•ˆç§»é™¤å‰æ™¯å¯¹è±¡ï¼ŒåŒæ—¶ç”Ÿæˆæ—¢åˆç†åˆè¿è´¯çš„å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒAttentive Eraseråœ¨å¤šç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„å¯¹è±¡ç§»é™¤è¡¨ç°ç¨³å®šæœ‰æ•ˆï¼Œç”šè‡³è¶…è¶ŠåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯åœ¨å„ç§æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹ä¸­å®ç°ï¼Œå±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>åœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸­ï¼Œæ‰©æ•£æ¨¡å‹é¢ä¸´ç”Ÿæˆéšæœºç‘•ç–µå’Œé‡æ–°ç»˜åˆ¶é—®é¢˜ã€‚</li>
<li>æå‡ºAttentive Eraseræ–¹æ³•ï¼Œæ— éœ€è°ƒæ•´å³å¯å¢å¼ºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œç¨³å®šæœ‰æ•ˆçš„å¯¹è±¡ç§»é™¤ã€‚</li>
<li>é€šè¿‡Attention Activation and Suppression (ASS)æŠ€æœ¯ï¼Œä¼˜å…ˆå¤„ç†èƒŒæ™¯ä¿¡æ¯ï¼Œå½±å“ç”Ÿæˆå›¾åƒçš„ç»“æ„å’Œå½¢çŠ¶ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥Self-Attention Redirection Guidance (SARG)ï¼Œæœ‰æ•ˆç§»é™¤å‰æ™¯å¯¹è±¡å¹¶ç”Ÿæˆè¿è´¯å†…å®¹ã€‚</li>
<li>å®éªŒè¯æ˜Attentive Eraseråœ¨å¤šç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä¼˜å¼‚è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b3653bdad7357fbc27eb73e856f192a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cccfe0d01ac8522e139f080a5dd29b7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d9b80c7275fc5978631c2565f9043ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-daf18f400271dc04dfd192a7b462c011.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3fe8a0715cad758c3774271ce823ca6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c81810756ea7a7a3ba441ce5a37a1067.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ArtAug-Enhancing-Text-to-Image-Generation-through-Synthesis-Understanding-Interaction"><a href="#ArtAug-Enhancing-Text-to-Image-Generation-through-Synthesis-Understanding-Interaction" class="headerlink" title="ArtAug: Enhancing Text-to-Image Generation through   Synthesis-Understanding Interaction"></a>ArtAug: Enhancing Text-to-Image Generation through   Synthesis-Understanding Interaction</h2><p><strong>Authors:Zhongjie Duan, Qianyi Zhao, Cen Chen, Daoyuan Chen, Wenmeng Zhou, Yaliang Li, Yingda Chen</strong></p>
<p>The emergence of diffusion models has significantly advanced image synthesis. The recent studies of model interaction and self-corrective reasoning approach in large language models offer new insights for enhancing text-to-image models. Inspired by these studies, we propose a novel method called ArtAug for enhancing text-to-image models in this paper. To the best of our knowledge, ArtAug is the first one that improves image synthesis models via model interactions with understanding models. In the interactions, we leverage human preferences implicitly learned by image understanding models to provide fine-grained suggestions for image synthesis models. The interactions can modify the image content to make it aesthetically pleasing, such as adjusting exposure, changing shooting angles, and adding atmospheric effects. The enhancements brought by the interaction are iteratively fused into the synthesis model itself through an additional enhancement module. This enables the synthesis model to directly produce aesthetically pleasing images without any extra computational cost. In the experiments, we train the ArtAug enhancement module on existing text-to-image models. Various evaluation metrics consistently demonstrate that ArtAug enhances the generative capabilities of text-to-image models without incurring additional computational costs. The source code and models will be released publicly. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å‡ºç°æå¤§åœ°æ¨åŠ¨äº†å›¾åƒåˆæˆçš„å‘å±•ã€‚è¿‘æœŸå…³äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨¡å‹äº¤äº’å’Œè‡ªæˆ‘çº æ­£æ¨ç†æ–¹æ³•çš„ç ”ç©¶ï¼Œä¸ºæ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹æä¾›äº†æ–°çš„è§è§£ã€‚å—è¿™äº›ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§åä¸ºArtAugçš„æ–°å‹æ–¹æ³•ï¼Œç”¨äºå¢å¼ºæ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒArtAugæ˜¯é€šè¿‡æ¨¡å‹ä¸ç†è§£æ¨¡å‹çš„äº¤äº’æ¥æ”¹è¿›å›¾åƒåˆæˆæ¨¡å‹çš„ç¬¬ä¸€ç§æ–¹æ³•ã€‚åœ¨äº¤äº’è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å›¾åƒç†è§£æ¨¡å‹éšå«åœ°å­¦ä¹ äººç±»åå¥½ï¼Œä¸ºå›¾åƒåˆæˆæ¨¡å‹æä¾›ç²¾ç»†çš„å»ºè®®ã€‚è¿™äº›äº¤äº’å¯ä»¥ä¿®æ”¹å›¾åƒå†…å®¹ï¼Œä½¿å…¶å…·æœ‰ç¾å­¦æ„Ÿï¼Œä¾‹å¦‚è°ƒæ•´æ›å…‰ã€æ”¹å˜æ‹æ‘„è§’åº¦å’Œæ·»åŠ å¤§æ°”æ•ˆæœã€‚é€šè¿‡é¢å¤–çš„å¢å¼ºæ¨¡å—ï¼Œè¿™äº›äº¤äº’å¸¦æ¥çš„å¢å¼ºåŠŸèƒ½è¢«è¿­ä»£åœ°èåˆåˆ°åˆæˆæ¨¡å‹æœ¬èº«ä¸­ã€‚è¿™ä½¿å¾—åˆæˆæ¨¡å‹èƒ½å¤Ÿç›´æ¥ç”Ÿæˆå…·æœ‰ç¾å­¦æ„Ÿçš„å›¾åƒï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬åœ¨ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸Šè®­ç»ƒäº†ArtAugå¢å¼ºæ¨¡å—ã€‚å„ç§è¯„ä¼°æŒ‡æ ‡ä¸€è‡´è¡¨æ˜ï¼ŒArtAugåœ¨ä¸å¢åŠ é¢å¤–è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚æºä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12888v2">PDF</a> 18 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’å’Œè‡ªä¿®æ­£æ¨ç†æ–¹æ³•ï¼Œæå‡ºä¸€ç§åä¸ºArtAugçš„æ–°å‹å›¾åƒåˆæˆå¢å¼ºæ–¹æ³•ã€‚ArtAugé€šè¿‡æ¨¡å‹äº¤äº’åˆ©ç”¨å›¾åƒç†è§£æ¨¡å‹çš„éšå«äººç±»åå¥½ï¼Œä¸ºå›¾åƒåˆæˆæ¨¡å‹æä¾›ç²¾ç»†å»ºè®®ã€‚è¿™ç§äº¤äº’èƒ½å¤Ÿä¿®æ”¹å›¾åƒå†…å®¹ï¼Œä½¿å…¶æ›´å…·å®¡ç¾æ„Ÿï¼Œå¦‚è°ƒæ•´æ›å…‰ã€æ‹æ‘„è§’åº¦å’Œæ·»åŠ å¤§æ°”æ•ˆæœç­‰ã€‚è¿™ç§äº¤äº’å¢å¼ºåŠŸèƒ½é€šè¿‡é¢å¤–çš„å¢å¼ºæ¨¡å—è¿­ä»£åœ°èå…¥åˆæˆæ¨¡å‹æœ¬èº«ï¼Œä½¿å¾—åˆæˆæ¨¡å‹å¯ç›´æ¥ç”Ÿæˆå…·æœ‰ç¾æ„Ÿçš„å›¾åƒï¼Œæ— éœ€é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚å®éªŒè¯æ˜ï¼ŒArtAugå¯¹ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å…·æœ‰å¢å¼ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„å…´èµ·å·²å¤§å¤§æ¨åŠ¨äº†å›¾åƒåˆæˆçš„å‘å±•ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’å’Œè‡ªä¿®æ­£æ¨ç†æ–¹æ³•ä¸ºå›¾åƒåˆæˆæ¨¡å‹çš„æ”¹è¿›æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
<li>ArtAugæ–¹æ³•åˆ©ç”¨å›¾åƒç†è§£æ¨¡å‹éšå«çš„äººç±»åå¥½ï¼Œä¸ºå›¾åƒåˆæˆæ¨¡å‹æä¾›ç²¾ç»†å»ºè®®ã€‚</li>
<li>ArtAugèƒ½å¤Ÿé€šè¿‡ä¿®æ”¹å›¾åƒå†…å®¹ä½¿å…¶æ›´å…·å®¡ç¾æ„Ÿï¼Œå¦‚è°ƒæ•´æ›å…‰ã€æ‹æ‘„è§’åº¦å’Œå¤§æ°”æ•ˆæœç­‰ã€‚</li>
<li>ArtAugé€šè¿‡é¢å¤–çš„å¢å¼ºæ¨¡å—å°†äº¤äº’å¢å¼ºåŠŸèƒ½èå…¥åˆæˆæ¨¡å‹ï¼Œæ— éœ€é¢å¤–è®¡ç®—æˆæœ¬ã€‚</li>
<li>å®éªŒè¯æ˜ArtAugèƒ½æœ‰æ•ˆæå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ArtAugçš„æºä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12888">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b375109d2ac16ad1f88dfede9f9b962e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95c786fcba128a88fa18c86afe9a1a5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8e93f17d6a6a77482cc645aa6b75701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a67fee23981dff08f96381195a89d82d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6ff9dcafbc2018fb6f0680017b93710.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Causal-Diffusion-Transformers-for-Generative-Modeling"><a href="#Causal-Diffusion-Transformers-for-Generative-Modeling" class="headerlink" title="Causal Diffusion Transformers for Generative Modeling"></a>Causal Diffusion Transformers for Generative Modeling</h2><p><strong>Authors:Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, Haoqi Fan</strong></p>
<p>We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusionâ€™s multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusionâ€™s ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥å› æœæ‰©æ•£ï¼ˆCausal Diffusionï¼‰ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„è‡ªå›å½’ï¼ˆARï¼‰å¯¹åº”ç‰©ã€‚å®ƒæ˜¯ä¸€ç§å‹å¥½çš„ä¸‹ä¸€ä¸ªï¼ˆæˆ–å¤šä¸ªï¼‰ä»¤ç‰Œé¢„æµ‹æ¡†æ¶ï¼Œé€‚ç”¨äºç¦»æ•£å’Œè¿ç»­æ¨¡å¼ï¼Œå¹¶ä¸ç°æœ‰çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æ¨¡å‹ï¼ˆå¦‚LLaMAå’ŒGPTï¼‰å…¼å®¹ã€‚å°½ç®¡æœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜å°è¯•å°†æ‰©æ•£ä¸ARæ¨¡å‹ç›¸ç»“åˆï¼Œä½†æˆ‘ä»¬å‘ç°å¯¹æ‰©æ•£æ¨¡å‹å¼•å…¥åºåˆ—åˆ†è§£å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œå¹¶å¯ä»¥åœ¨ARå’Œæ‰©æ•£ç”Ÿæˆæ¨¡å¼ä¹‹é—´å®ç°å¹³ç¨³è¿‡æ¸¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å› æœèåˆï¼ˆCausalFusionï¼‰â€”â€”ä¸€ç§ä»…è§£ç çš„å˜å‹å™¨ï¼Œå®ƒåœ¨åºåˆ—ä»¤ç‰Œå’Œæ‰©æ•£å™ªå£°çº§åˆ«ä¸ŠåŒé‡åˆ†è§£æ•°æ®ï¼Œä»è€Œåœ¨ImageNetç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼ŒåŒæ—¶äº«å—ARç”Ÿæˆä»»æ„æ•°é‡ä»¤ç‰Œçš„ä¸Šä¸‹æ–‡æ¨ç†ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡è”åˆå›¾åƒç”Ÿæˆå’Œæè¿°æ¨¡å‹æ¥å±•ç¤ºå› æœèåˆçš„å¤šæ¨¡å¼èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºå…¶åœ¨é›¶é•œå¤´ä¸Šä¸‹æ–‡å›¾åƒæ“ä½œä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºç¤¾åŒºåœ¨ç¦»æ•£å’Œè¿ç»­æ•°æ®ä¸Šè®­ç»ƒå¤šæ¨¡å¼æ¨¡å‹æä¾›æ–°çš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12095v2">PDF</a> 22 figures, 21 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å› æœæ‰©æ•£ï¼ˆCausal Diffusionï¼‰ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„è‡ªå›å½’ï¼ˆARï¼‰å¯¹åº”ç‰©ã€‚å®ƒæ˜¯ä¸€ç§ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªæˆ–å¤šä¸ªç¬¦å·çš„æ¡†æ¶ï¼Œé€‚ç”¨äºç¦»æ•£å’Œè¿ç»­æ¨¡æ€ï¼Œå¹¶ä¸ç°æœ‰çš„è‡ªå›å½’é¢„æµ‹æ¨¡å‹ï¼ˆå¦‚LLaMAå’ŒGPTï¼‰å…¼å®¹ã€‚æœ¬æ–‡å±•ç¤ºäº†å¯¹æ‰©æ•£æ¨¡å‹å¼•å…¥åºåˆ—åˆ†è§£å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œå¹¶åœ¨è‡ªå›å½’å’Œæ‰©æ•£ç”Ÿæˆæ¨¡å¼ä¹‹é—´å®ç°å¹³ç¨³è¿‡æ¸¡ã€‚å› æ­¤ï¼Œæå‡ºäº†å› æœèåˆï¼ˆCausalFusionï¼‰â€”â€”ä¸€ç§ä»…è§£ç çš„å˜å‹å™¨ï¼Œå®ƒåœ¨åºåˆ—æ ‡è®°å’Œæ‰©æ•£å™ªå£°çº§åˆ«ä¸ŠåŒé‡åˆ†è§£æ•°æ®ï¼Œåœ¨ImageNetç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œå¹¶ä¿ç•™äº†è‡ªå›å½’ç”Ÿæˆä»»æ„æ•°é‡ç¬¦å·çš„ä¸Šä¸‹æ–‡æ¨ç†ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†å› æœèåˆçš„å¤šæ¨¡å¼èƒ½åŠ›å’Œé›¶æ ·æœ¬ä¸Šä¸‹æ–‡å›¾åƒæ“ä½œèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å› æœæ‰©æ•£ä½œä¸ºè‡ªå›å½’æ¨¡å‹çš„æ‰©æ•£æ¨¡å‹çš„å¯¹åº”ç‰©ã€‚</li>
<li>å› æœæ‰©æ•£æ˜¯ä¸€ä¸ªç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªæˆ–å¤šä¸ªç¬¦å·çš„æ¡†æ¶ï¼Œé€‚ç”¨äºç¦»æ•£å’Œè¿ç»­æ¨¡æ€ã€‚</li>
<li>ç°æœ‰çš„è‡ªå›å½’é¢„æµ‹æ¨¡å‹ï¼ˆå¦‚LLaMAå’ŒGPTï¼‰ä¸å› æœæ‰©æ•£å…¼å®¹ã€‚</li>
<li>å¯¹æ‰©æ•£æ¨¡å‹å¼•å…¥åºåˆ—åˆ†è§£å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>å› æœèåˆæ˜¯ä¸€ä¸ªä»…è§£ç çš„å˜å‹å™¨ï¼Œå®ƒåœ¨åºåˆ—æ ‡è®°å’Œæ‰©æ•£å™ªå£°çº§åˆ«ä¸ŠåŒé‡åˆ†è§£æ•°æ®ã€‚</li>
<li>å› æœèåˆåœ¨ImageNetç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼ŒåŒæ—¶å…·æœ‰è‡ªå›å½’çš„ä¼˜åŠ¿ã€‚</li>
<li>å› æœèåˆå…·æœ‰å¤šæ¨¡å¼èƒ½åŠ›ï¼Œå±•ç¤ºäº†é›¶æ ·æœ¬ä¸Šä¸‹æ–‡å›¾åƒæ“ä½œèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8c26c69cb665c939727e3834ef4390a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b9a31efe4ad14498b8fac7006c61471.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00479e06234d725de8863a95f76119f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be3018f479edd2671c8785d0238e02ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fb68eb7080e8144e64e28bcbdd8aa9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-899f6faafe8be6ad0a10f33236ab2807.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SwiftTry-Fast-and-Consistent-Video-Virtual-Try-On-with-Diffusion-Models"><a href="#SwiftTry-Fast-and-Consistent-Video-Virtual-Try-On-with-Diffusion-Models" class="headerlink" title="SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models"></a>SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models</h2><p><strong>Authors:Hung Nguyen, Quang Qui-Vinh Nguyen, Khoi Nguyen, Rang Nguyen</strong></p>
<p>Given an input video of a person and a new garment, the objective of this paper is to synthesize a new video where the person is wearing the specified garment while maintaining spatiotemporal consistency. Although significant advances have been made in image-based virtual try-on, extending these successes to video often leads to frame-to-frame inconsistencies. Some approaches have attempted to address this by increasing the overlap of frames across multiple video chunks, but this comes at a steep computational cost due to the repeated processing of the same frames, especially for long video sequences. To tackle these challenges, we reconceptualize video virtual try-on as a conditional video inpainting task, with garments serving as input conditions. Specifically, our approach enhances image diffusion models by incorporating temporal attention layers to improve temporal coherence. To reduce computational overhead, we propose ShiftCaching, a novel technique that maintains temporal consistency while minimizing redundant computations. Furthermore, we introduce the TikTokDress dataset, a new video try-on dataset featuring more complex backgrounds, challenging movements, and higher resolution compared to existing public datasets. Extensive experiments demonstrate that our approach outperforms current baselines, particularly in terms of video consistency and inference speed. The project page is available at <a target="_blank" rel="noopener" href="https://swift-try.github.io/">https://swift-try.github.io/</a>. </p>
<blockquote>
<p>ç»™å®šä¸€ä¸ªè¾“å…¥è§†é¢‘ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªäººå’Œä¸€ä»¶æ–°è¡£æœï¼Œæœ¬æ–‡çš„ç›®æ ‡æ˜¯åœ¨ä¿æŒæ—¶ç©ºä¸€è‡´æ€§çš„æƒ…å†µä¸‹ï¼Œåˆæˆä¸€ä¸ªæ–°çš„è§†é¢‘ï¼Œå…¶ä¸­äººç‰©ç©¿ç€æŒ‡å®šçš„æœè£…ã€‚å°½ç®¡åŸºäºå›¾åƒçš„è™šæ‹Ÿè¯•è¡£å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¿™äº›æˆåŠŸå¾€å¾€éš¾ä»¥æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸï¼Œå› ä¸ºè¿™ç»å¸¸å¯¼è‡´å¸§ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚ä¸€äº›æ–¹æ³•è¯•å›¾é€šè¿‡å¢åŠ è·¨è¶Šå¤šä¸ªè§†é¢‘ç‰‡æ®µçš„å¸§é‡å æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ç”±äºé‡å¤å¤„ç†ç›¸åŒçš„å¸§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿è§†é¢‘åºåˆ—æ—¶ï¼Œè¿™å¸¦æ¥äº†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†è§†é¢‘è™šæ‹Ÿè¯•è¡£é‡æ–°å®šä½ä¸ºæ¡ä»¶è§†é¢‘å¡«å……ä»»åŠ¡ï¼Œæœè£…ä½œä¸ºè¾“å…¥æ¡ä»¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç»“åˆæ—¶é—´æ³¨æ„åŠ›å±‚æ¥æé«˜å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œæé«˜æ—¶é—´è¿è´¯æ€§ã€‚ä¸ºäº†å‡å°‘è®¡ç®—å¼€é”€ï¼Œæˆ‘ä»¬æå‡ºäº†ShiftCachingè¿™ä¸€æ–°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½åœ¨ä¿æŒæ—¶é—´ä¸€è‡´æ€§çš„åŒæ—¶æœ€å°åŒ–å†—ä½™è®¡ç®—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†TikTokDressæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è§†é¢‘è¯•ç©¿æ•°æ®é›†ï¼Œå…·æœ‰æ›´å¤æ‚çš„èƒŒæ™¯ã€æ›´å…·æŒ‘æˆ˜æ€§çš„åŠ¨ä½œå’Œæ›´é«˜çš„åˆ†è¾¨ç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå½“å‰åŸºçº¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘ä¸€è‡´æ€§å’Œæ¨ç†é€Ÿåº¦æ–¹é¢ã€‚é¡¹ç›®é¡µé¢å¯åœ¨[<a target="_blank" rel="noopener" href="https://swift-try.github.io/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://swift-try.github.io/]ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10178v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨åˆæˆä¸€ä¸ªæ–°è§†é¢‘ï¼Œå±•ç¤ºäººç‰©ç©¿ä¸ŠæŒ‡å®šè¡£ç‰©çš„åŒæ—¶ä¿æŒæ—¶ç©ºä¸€è‡´æ€§ã€‚ä¸ºæé«˜è§†é¢‘è¿è´¯æ€§å¹¶é™ä½è®¡ç®—æˆæœ¬ï¼Œç ”ç©¶å›¢é˜Ÿå°†è§†é¢‘è™šæ‹Ÿè¯•è¡£è§†ä¸ºæ¡ä»¶è§†é¢‘ä¿®å¤ä»»åŠ¡ï¼Œé€šè¿‡å¼•å…¥æ—¶é—´æ³¨æ„åŠ›å±‚æ”¹è¿›å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶æå‡ºShiftCachingæŠ€æœ¯å‡å°‘å†—ä½™è®¡ç®—ã€‚æ­¤å¤–ï¼Œå¼•å…¥TikTokDressæ•°æ®é›†ï¼Œä¸ç°æœ‰å…¬å¼€æ•°æ®é›†ç›¸æ¯”ï¼ŒèƒŒæ™¯æ›´å¤æ‚ã€åŠ¨ä½œæ›´å…·æŒ‘æˆ˜æ€§ä¸”åˆ†è¾¨ç‡æ›´é«˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ä¸€è‡´æ€§å’Œæ¨ç†é€Ÿåº¦æ–¹é¢å‡ä¼˜äºå½“å‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡çš„ç›®æ ‡æ˜¯æ ¹æ®è¾“å…¥çš„äººç‰©è§†é¢‘å’Œæ–°çš„è¡£ç‰©ï¼Œåˆæˆä¸€ä¸ªå±•ç¤ºäººç‰©ç©¿ä¸Šæ–°è¡£ç‰©çš„è§†é¢‘ï¼ŒåŒæ—¶ä¿æŒæ—¶ç©ºä¸€è‡´æ€§ã€‚</li>
<li>å°½ç®¡å›¾åƒè™šæ‹Ÿè¯•è¡£å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†å…¶æ‰©å±•åˆ°è§†é¢‘æ—¶ä»é¢ä¸´å¸§é—´ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶äººå‘˜å°†è§†é¢‘è™šæ‹Ÿè¯•è¡£é‡æ–°æ„æƒ³ä¸ºæ¡ä»¶è§†é¢‘ä¿®å¤ä»»åŠ¡ï¼Œå…¶ä¸­è¡£ç‰©ä½œä¸ºè¾“å…¥æ¡ä»¶ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ—¶é—´æ³¨æ„åŠ›å±‚æ”¹è¿›å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥æé«˜è§†é¢‘è¿è´¯æ€§ã€‚</li>
<li>æå‡ºShiftCachingæŠ€æœ¯æ¥å‡å°‘å†—ä½™è®¡ç®—ï¼Œæé«˜æ•ˆç‡ã€‚</li>
<li>å¼•å…¥TikTokDressæ•°æ®é›†ï¼ŒåŒ…å«æ›´å¤æ‚çš„èƒŒæ™¯ã€æŒ‘æˆ˜æ€§å’Œæ›´é«˜åˆ†è¾¨ç‡çš„è§†é¢‘å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-086aec0a20e565084105162988b57ae8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a31d3029b9d81b879d85f54648c504.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c4c605b443e0cc4a77553a7cd2f4670.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72ff49c42082cab8c706fb0954f8a262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e121e47f535554fc59afe88bab5a990a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fbcefc670f07de03e4e294a4dd19708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77a359f212cba39f20fae311c478cd17.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="No-Annotations-for-Object-Detection-in-Art-through-Stable-Diffusion"><a href="#No-Annotations-for-Object-Detection-in-Art-through-Stable-Diffusion" class="headerlink" title="No Annotations for Object Detection in Art through Stable Diffusion"></a>No Annotations for Object Detection in Art through Stable Diffusion</h2><p><strong>Authors:Patrick Ramos, Nicolas Gonthier, Selina Khan, Yuta Nakashima, Noa Garcia</strong></p>
<p>Object detection in art is a valuable tool for the digital humanities, as it allows for faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion modelsâ€™ art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art. Code is available at <a target="_blank" rel="noopener" href="https://github.com/patrick-john-ramos/nada">https://github.com/patrick-john-ramos/nada</a> </p>
<blockquote>
<p>è‰ºæœ¯å“ä¸­çš„ç›®æ ‡æ£€æµ‹æ˜¯æ•°å­—äººæ–‡é¢†åŸŸçš„ä¸€ä¸ªé‡è¦å·¥å…·ï¼Œå› ä¸ºå®ƒä¸äººç±»ç›¸æ¯”ï¼Œå¯ä»¥æ›´å¿«é€Ÿåœ°è¯†åˆ«è‰ºæœ¯å’Œå†å²å›¾åƒä¸­çš„ç›®æ ‡ã€‚ç„¶è€Œï¼Œå¯¹è¿™äº›å›¾åƒè¿›è¡Œæ ‡æ³¨å´é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦ä¸“ä¸šçš„é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†NADAï¼ˆè‰ºæœ¯å“æ£€æµ‹æ— éœ€æ ‡æ³¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¸è‰ºæœ¯ç›¸å…³çš„çŸ¥è¯†ï¼Œåœ¨ç»˜ç”»ä¸­è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œæ— éœ€å®Œæ•´çš„è¾¹ç•Œæ¡†ç›‘ç£ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå¼±ç›‘ç£å’Œæ— æºåœºæ™¯ï¼Œå¹¶ä¸”ä¸éœ€è¦å¯¹å…¶é¢„è®­ç»ƒç»„ä»¶è¿›è¡Œä»»ä½•å¾®è°ƒï¼Œå®ƒç”±åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»æå‡ºè€…å’ŒåŸºäºStable Diffusionçš„ç±»æ¡ä»¶æ£€æµ‹å™¨ç»„æˆã€‚NADAåœ¨ä¸¤ä¸ªè‰ºæœ¯å“æ•°æ®é›†ArtDL 2.0å’ŒIconArtä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨å¼±ç›‘ç£æ£€æµ‹æ–¹é¢ä¼˜äºå…ˆå‰çš„å·¥ä½œï¼ŒåŒæ—¶æ˜¯è‰ºæœ¯å“é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹çš„é¦–é¡¹å·¥ä½œã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/patrick-john-ramos/nada%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/patrick-john-ramos/nadaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06286v2">PDF</a> 8 pages, 6 figures, to be published in WACV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‰ºæœ¯é¢†åŸŸä¸­çš„ç‰©ä½“æ£€æµ‹å·¥å…·çš„é‡è¦æ€§ï¼Œå®ƒèƒ½å¤Ÿå¿«é€Ÿè¯†åˆ«è‰ºæœ¯å’Œå†å²å›¾åƒä¸­çš„ç‰©ä½“ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦ç‰¹å®šçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¯¹è¿™ç±»å›¾åƒè¿›è¡Œæ ‡æ³¨å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ— éœ€å…¨æ¡†ç›‘ç£çš„ç»˜ç”»æ£€æµ‹æ–¹æ¡ˆâ€”â€”NADAï¼ˆæ— éœ€æ ‡æ³¨è¿›è¡Œè‰ºæœ¯æ£€æµ‹ï¼‰ã€‚è¯¥æ–¹æ³•æ”¯æŒå¼±ç›‘ç£å’Œæ— æºåœºæ™¯ï¼Œæ— éœ€å¯¹å…¶é¢„è®­ç»ƒç»„ä»¶è¿›è¡Œå¾®è°ƒï¼ŒåŒ…æ‹¬åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»æå‡ºå™¨å’ŒåŸºäºStable Diffusionçš„ç±»æ¡ä»¶æ£€æµ‹å™¨ã€‚åœ¨ArtDL 2.0å’ŒIconArtä¸¤ä¸ªè‰ºæœ¯å“æ•°æ®é›†ä¸Šï¼ŒNADAçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„å¼±ç›‘ç£æ£€æµ‹æ–¹æ³•ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªåœ¨æ— æºè‰ºæœ¯é¢†åŸŸå®ç°ç‰©ä½“æ£€æµ‹çš„ã€‚å®Œæ•´ä»£ç å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‰ºæœ¯é¢†åŸŸçš„ç‰©ä½“æ£€æµ‹å·¥å…·å¯ä»¥å¿«é€Ÿè¯†åˆ«è‰ºæœ¯å’Œå†å²å›¾åƒä¸­çš„ç‰©ä½“ï¼Œå…·æœ‰å®ç”¨ä»·å€¼ã€‚</li>
<li>å¯¹è‰ºæœ¯å›¾åƒè¿›è¡Œæ ‡æ³¨éœ€è¦ç‰¹å®šçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>NADAæ˜¯ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè‰ºæœ¯æ£€æµ‹çš„è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€å…¨æ¡†ç›‘ç£ã€‚</li>
<li>NADAæ”¯æŒå¼±ç›‘ç£å’Œæ— æºåœºæ™¯çš„åº”ç”¨ã€‚</li>
<li>NADAä¸éœ€è¦å¯¹å…¶é¢„è®­ç»ƒç»„ä»¶è¿›è¡Œå¾®è°ƒã€‚</li>
<li>NADAåŒ…æ‹¬åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»æå‡ºå™¨å’ŒåŸºäºStable Diffusionçš„ç±»æ¡ä»¶æ£€æµ‹å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57c17d24733537fe1886250560c883fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4139a38513f0b2f267b75456dd3dfdfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b04ddd24cf532a8716dc0a4011c96116.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9735771e9b15404efe8fd522bb09352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21873767c7771c024dd0a90f4d9774c2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model"><a href="#MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model" class="headerlink" title="MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model"></a>MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model</h2><p><strong>Authors:Shan Yang</strong></p>
<p>Text-to-image generation models have revolutionized content creation, but diffusion-based vision-language models still face challenges in precisely controlling the shape, appearance, and positional placement of objects in generated images using text guidance alone. Existing global image editing models rely on additional masks or images as guidance to achieve layout control, often requiring retraining of the model. While local object-editing models allow modifications to object shapes, they lack the capability to control object positions. To address these limitations, we propose the Mask-free Training-free Object-Level Layout Control Diffusion Model (MFTF), which provides precise control over object positions without requiring additional masks or images. The MFTF model supports both single-object and multi-object positional adjustments, such as translation and rotation, while enabling simultaneous layout control and object semantic editing. The MFTF model employs a parallel denoising process for both the source and target diffusion models. During this process, attention masks are dynamically generated from the cross-attention layers of the source diffusion model and applied to queries from the self-attention layers to isolate objects. These queries, generated in the source diffusion model, are then adjusted according to the layout control parameters and re-injected into the self-attention layers of the target diffusion model. This approach ensures accurate and precise positional control of objects. Project source code available at <a target="_blank" rel="noopener" href="https://github.com/syang-genai/MFTF">https://github.com/syang-genai/MFTF</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†å†…å®¹åˆ›ä½œçš„æ–¹å¼ï¼Œä½†åŸºäºæ‰©æ•£çš„è§†è¯­è¨€æ¨¡å‹åœ¨ä»…ä½¿ç”¨æ–‡æœ¬æŒ‡å¯¼æ¥ç²¾ç¡®æ§åˆ¶ç”Ÿæˆå›¾åƒä¸­ç‰©ä½“çš„å½¢çŠ¶ã€å¤–è§‚å’Œä½ç½®æ”¾ç½®æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„å…¨å±€å›¾åƒç¼–è¾‘æ¨¡å‹ä¾èµ–äºé¢å¤–çš„è’™ç‰ˆæˆ–å›¾åƒä½œä¸ºæŒ‡å¯¼æ¥å®ç°å¸ƒå±€æ§åˆ¶ï¼Œé€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚è™½ç„¶å±€éƒ¨ç‰©ä½“ç¼–è¾‘æ¨¡å‹å…è®¸ä¿®æ”¹ç‰©ä½“å½¢çŠ¶ï¼Œä½†å®ƒä»¬ç¼ºä¹æ§åˆ¶ç‰©ä½“ä½ç½®çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†æ— è’™ç‰ˆã€æ— éœ€è®­ç»ƒçš„ç‰©ä½“çº§åˆ«å¸ƒå±€æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼ˆMFTFï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–è’™ç‰ˆæˆ–å›¾åƒçš„æƒ…å†µä¸‹ï¼Œç²¾ç¡®æ§åˆ¶ç‰©ä½“çš„ä½ç½®ã€‚MFTFæ¨¡å‹æ—¢æ”¯æŒå•ç‰©ä½“ä¹Ÿæ”¯æŒå¤šç‰©ä½“çš„ä½ç½®è°ƒæ•´ï¼Œå¦‚å¹³ç§»å’Œæ—‹è½¬ï¼ŒåŒæ—¶å®ç°å¸ƒå±€æ§åˆ¶å’Œç‰©ä½“è¯­ä¹‰ç¼–è¾‘ã€‚MFTFæ¨¡å‹é‡‡ç”¨æºå’Œç›®æ ‡æ‰©æ•£æ¨¡å‹çš„å¹¶è¡Œå»å™ªè¿‡ç¨‹ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä»æºæ‰©æ•£æ¨¡å‹çš„äº¤å‰æ³¨æ„å±‚åŠ¨æ€ç”Ÿæˆæ³¨æ„è’™ç‰ˆï¼Œå¹¶åº”ç”¨äºè‡ªæˆ‘æ³¨æ„å±‚çš„æŸ¥è¯¢ä»¥éš”ç¦»ç‰©ä½“ã€‚è¿™äº›åœ¨æºæ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆçš„æŸ¥è¯¢ä¼šæ ¹æ®å¸ƒå±€æ§åˆ¶å‚æ•°è¿›è¡Œè°ƒæ•´ï¼Œç„¶åé‡æ–°æ³¨å…¥ç›®æ ‡æ‰©æ•£æ¨¡å‹çš„è‡ªæˆ‘æ³¨æ„å±‚ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç‰©ä½“çš„ç²¾ç¡®ä½ç½®æ§åˆ¶ã€‚é¡¹ç›®æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/syang-genai/MFTF%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/syang-genai/MFTFæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01284v2">PDF</a> 8 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€æ©è†œå’Œè®­ç»ƒçš„å¯¹è±¡çº§å¸ƒå±€æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼ˆMFTFï¼‰ï¼Œè¯¥æ¨¡å‹å¯åœ¨æ— éœ€é¢å¤–æ©è†œæˆ–å›¾åƒçš„æƒ…å†µä¸‹ï¼Œå®ç°å¯¹ç”Ÿæˆå›¾åƒä¸­å¯¹è±¡çš„å½¢çŠ¶ã€å¤–è§‚å’Œä½ç½®è¿›è¡Œç²¾ç¡®æ§åˆ¶ã€‚MFTFæ¨¡å‹æ”¯æŒå•å¯¹è±¡å’Œå¤šå¯¹è±¡çš„ä½ç½®è°ƒæ•´ï¼Œå¦‚å¹³ç§»å’Œæ—‹è½¬ï¼ŒåŒæ—¶å®ç°å¸ƒå±€æ§åˆ¶å’Œå¯¹è±¡è¯­ä¹‰ç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å†…å®¹åˆ›å»ºä¸­å®ç°äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œä½†ä»é¢ä¸´ç²¾ç¡®æ§åˆ¶å¯¹è±¡å½¢çŠ¶ã€å¤–è§‚å’Œä½ç½®çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å…¨å±€å›¾åƒç¼–è¾‘æ¨¡å‹ä¾èµ–é¢å¤–çš„æ©è†œæˆ–å›¾åƒä½œä¸ºæŒ‡å¯¼æ¥å®ç°å¸ƒå±€æ§åˆ¶ï¼Œéœ€è¦é‡è®­æ¨¡å‹ã€‚</li>
<li>å±€éƒ¨å¯¹è±¡ç¼–è¾‘æ¨¡å‹å…è®¸ä¿®æ”¹å¯¹è±¡å½¢çŠ¶ï¼Œä½†æ— æ³•æ§åˆ¶å¯¹è±¡ä½ç½®ã€‚</li>
<li>æå‡ºçš„MFTFæ¨¡å‹æä¾›äº†å¯¹å¯¹è±¡ä½ç½®çš„ç²¾ç¡®æ§åˆ¶ï¼Œæ— éœ€é¢å¤–çš„æ©è†œæˆ–å›¾åƒã€‚</li>
<li>MFTFæ¨¡å‹æ”¯æŒå•å¯¹è±¡å’Œå¤šå¯¹è±¡çš„ä½ç½®è°ƒæ•´ï¼Œå¦‚å¹³ç§»å’Œæ—‹è½¬ï¼ŒåŒæ—¶å®ç°å¸ƒå±€æ§åˆ¶å’Œå¯¹è±¡è¯­ä¹‰ç¼–è¾‘ã€‚</li>
<li>MFTFæ¨¡å‹é‡‡ç”¨å¹¶è¡Œå»å™ªè¿‡ç¨‹ï¼Œé€šè¿‡äº¤å‰æ³¨æ„å±‚ç”Ÿæˆæ³¨æ„åŠ›æ©è†œï¼Œåº”ç”¨äºè‡ªæˆ‘æ³¨æ„å±‚çš„æŸ¥è¯¢ï¼Œä»¥éš”ç¦»å¯¹è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f5307dede9c39a5e6238a4553cd509b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92df01ad425ce71e6a3a7789281fc8d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31d932f273200085ffd85d5c55bc2acd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb08341d38f99b43d2cb48d414f401d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-452bad3796827f922621781276fed702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7024f4fd43f65c0109fc421d30a29356.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Estimating-Body-and-Hand-Motion-in-an-Ego-sensed-World"><a href="#Estimating-Body-and-Hand-Motion-in-an-Ego-sensed-World" class="headerlink" title="Estimating Body and Hand Motion in an Ego-sensed World"></a>Estimating Body and Hand Motion in an Ego-sensed World</h2><p><strong>Authors:Brent Yi, Vickie Ye, Maya Zheng, Yunqi Li, Lea MÃ¼ller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa</strong></p>
<p>We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture a device wearerâ€™s actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve hand estimation: the resulting kinematic and temporal constraints can reduce world-frame errors in single-frame estimates by 40%. Project page: <a target="_blank" rel="noopener" href="https://egoallo.github.io/">https://egoallo.github.io/</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†EgoAlloç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡å¤´æˆ´è®¾å¤‡å®ç°äººä½“è¿åŠ¨ä¼°è®¡ã€‚ä»…ä½¿ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„SLAMå§¿åŠ¿å’Œå›¾åƒï¼ŒEgoAlloæŒ‡å¯¼ä»æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·ï¼Œä»¥ä¼°è®¡ä¸‰ç»´èº«ä½“å§¿åŠ¿ã€é«˜åº¦å’Œæ‰‹éƒ¨å‚æ•°ï¼Œè¿™äº›å‚æ•°èƒ½å¤Ÿæ•è·è®¾å¤‡ä½©æˆ´è€…åœ¨åœºæ™¯çš„ä¸­å¿ƒåæ ‡ç³»ä¸­çš„åŠ¨ä½œã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬çš„å…³é”®è§è§£åœ¨äºè¡¨ç¤ºï¼šæˆ‘ä»¬æå‡ºäº†ç©ºé—´å’Œæ—¶é—´ä¸å˜æ€§æ ‡å‡†ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œç”±æ­¤å¾—å‡ºå¤´éƒ¨è¿åŠ¨æ¡ä»¶å‚æ•°åŒ–ï¼Œå¯æé«˜ä¼°è®¡ç²¾åº¦è¾¾18%ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„ç³»ç»Ÿä¼°è®¡çš„èº¯ä½“å¦‚ä½•æ”¹è¿›æ‰‹éƒ¨ä¼°è®¡ï¼šç”±æ­¤äº§ç”Ÿçš„è¿åŠ¨å­¦å’Œæ—¶é—´çº¦æŸå¯å‡å°‘å•å¸§ä¼°è®¡ä¸­çš„ä¸–ç•Œæ¡†æ¶è¯¯å·®è¾¾40%ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://egoallo.github.io/">https://egoallo.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03665v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://egoallo.github.io/">https://egoallo.github.io/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æ¨å‡ºäº†EgoAlloç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡å¤´æˆ´è®¾å¤‡å®ç°äººä½“åŠ¨ä½œä¼°è®¡ã€‚ä»…ä½¿ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„SLAMå§¿æ€å’Œå›¾åƒï¼ŒEgoAlloå¼•å¯¼ä»æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·ï¼Œä»¥ä¼°è®¡ä¸‰ç»´èº«ä½“å§¿æ€ã€é«˜åº¦ä»¥åŠæ‰‹éƒ¨å‚æ•°ï¼Œè¿™äº›å‚æ•°æ•æ‰è®¾å¤‡ä½©æˆ´è€…åœ¨åœºæ™¯ä¸­çš„ä»¥åœºæ™¯ä¸ºä¸­å¿ƒçš„åæ ‡æ¡†æ¶ä¸­çš„åŠ¨ä½œã€‚æˆ‘ä»¬çš„å…³é”®è§è§£åœ¨äºè¡¨ç¤ºæ³•ï¼šæˆ‘ä»¬æå‡ºç©ºé—´å’Œæ—¶é—´ä¸å˜æ€§æ ‡å‡†ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä»ä¸­å¾—å‡ºå¤´éƒ¨è¿åŠ¨æ¡ä»¶å‚æ•°åŒ–ï¼Œå¯æé«˜ä¼°è®¡ç²¾åº¦è¾¾18%ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„ç³»ç»Ÿä¼°è®¡çš„èº¯ä½“å¦‚ä½•æ”¹å–„æ‰‹éƒ¨ä¼°è®¡ï¼šç”±æ­¤äº§ç”Ÿçš„è¿åŠ¨å­¦å’Œæ—¶é—´çº¦æŸå¯å‡å°‘å•å¸§ä¼°è®¡ä¸­çš„ä¸–ç•Œæ¡†æ¶è¯¯å·®è¾¾40%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>EgoAlloç³»ç»Ÿåˆ©ç”¨å¤´æˆ´è®¾å¤‡å®ç°äººä½“åŠ¨ä½œä¼°è®¡ã€‚</li>
<li>ä»…ä½¿ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„SLAMå§¿æ€å’Œå›¾åƒè¿›è¡ŒåŠ¨ä½œä¼°è®¡ã€‚</li>
<li>é€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹é‡‡æ ·æ¥ä¼°è®¡ä¸‰ç»´èº«ä½“å§¿æ€ã€é«˜åº¦å’Œæ‰‹éƒ¨å‚æ•°ã€‚</li>
<li>ç³»ç»Ÿçš„å…³é”®åœ¨äºæå‡ºç©ºé—´å’Œæ—¶é—´ä¸å˜æ€§æ ‡å‡†æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¤´éƒ¨è¿åŠ¨æ¡ä»¶å‚æ•°åŒ–èƒ½æé«˜ä¼°è®¡ç²¾åº¦è¾¾18%ã€‚</li>
<li>èº¯ä½“ä¼°è®¡å¯æ”¹å–„æ‰‹éƒ¨ä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-81c8492b3b1560f367a0967f27d2c999.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77a7bf23f5bb0f722bdca49c60d6acbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1df460e2931a3ff20eb5ecde3ebdfae3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11a4ae84ce2c96f819c81a77bb23063c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Resolving-Multi-Condition-Confusion-for-Finetuning-Free-Personalized-Image-Generation"><a href="#Resolving-Multi-Condition-Confusion-for-Finetuning-Free-Personalized-Image-Generation" class="headerlink" title="Resolving Multi-Condition Confusion for Finetuning-Free Personalized   Image Generation"></a>Resolving Multi-Condition Confusion for Finetuning-Free Personalized   Image Generation</h2><p><strong>Authors:Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, Jie Song</strong></p>
<p>Personalized text-to-image generation methods can generate customized images based on the reference images, which have garnered wide research interest. Recent methods propose a finetuning-free approach with a decoupled cross-attention mechanism to generate personalized images requiring no test-time finetuning. However, when multiple reference images are provided, the current decoupled cross-attention mechanism encounters the object confusion problem and fails to map each reference image to its corresponding object, thereby seriously limiting its scope of application. To address the object confusion problem, in this work we investigate the relevance of different positions of the latent image features to the target object in diffusion model, and accordingly propose a weighted-merge method to merge multiple reference image features into the corresponding objects. Next, we integrate this weighted-merge method into existing pre-trained models and continue to train the model on a multi-object dataset constructed from the open-sourced SA-1B dataset. To mitigate object confusion and reduce training costs, we propose an object quality score to estimate the image quality for the selection of high-quality training samples. Furthermore, our weighted-merge training framework can be employed on single-object generation when a single object has multiple reference images. The experiments verify that our method achieves superior performance to the state-of-the-arts on the Concept101 dataset and DreamBooth dataset of multi-object personalized image generation, and remarkably improves the performance on single-object personalized image generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/hqhQAQ/MIP-Adapter">https://github.com/hqhQAQ/MIP-Adapter</a>. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•èƒ½å¤Ÿæ ¹æ®å‚è€ƒå›¾åƒç”Ÿæˆå®šåˆ¶å›¾åƒï¼Œè¿™å·²å¼•èµ·äº†å¹¿æ³›çš„ç ”ç©¶å…´è¶£ã€‚æœ€è¿‘çš„æ–¹æ³•æå‡ºäº†ä¸€ç§æ— éœ€å¾®è°ƒçš„è„±é’©äº¤å‰æ³¨æ„æœºåˆ¶æ¥ç”Ÿæˆä¸ªæ€§åŒ–å›¾åƒï¼Œæ— éœ€æµ‹è¯•æ—¶çš„å¾®è°ƒã€‚ç„¶è€Œï¼Œå½“æä¾›å¤šä¸ªå‚è€ƒå›¾åƒæ—¶ï¼Œå½“å‰çš„è„±é’©äº¤å‰æ³¨æ„æœºåˆ¶ä¼šé‡åˆ°å¯¹è±¡æ··æ·†é—®é¢˜ï¼Œå¹¶ä¸”æ— æ³•å°†æ¯ä¸ªå‚è€ƒå›¾åƒæ˜ å°„åˆ°å…¶ç›¸åº”çš„å¯¹è±¡ï¼Œä»è€Œä¸¥é‡é™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚ä¸ºäº†è§£å†³å¯¹è±¡æ··æ·†é—®é¢˜ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ½œåœ¨å›¾åƒç‰¹å¾çš„ä¸åŒä½ç½®ä¸ç›®æ ‡å¯¹è±¡çš„ç›¸å…³æ€§ï¼Œå¹¶æ®æ­¤æå‡ºäº†ä¸€ç§åŠ æƒåˆå¹¶æ–¹æ³•ï¼Œå°†å¤šä¸ªå‚è€ƒå›¾åƒç‰¹å¾åˆå¹¶åˆ°ç›¸åº”çš„å¯¹è±¡ä¸­ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™ç§åŠ æƒåˆå¹¶æ–¹æ³•é›†æˆåˆ°ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œå¹¶åœ¨ç”±å¼€æºSA-1Bæ•°æ®é›†æ„å»ºçš„å¤šå¯¹è±¡æ•°æ®é›†ä¸Šç»§ç»­è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†å‡è½»å¯¹è±¡æ··æ·†å¹¶é™ä½è®­ç»ƒæˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹è±¡è´¨é‡è¯„åˆ†æ¥ä¼°è®¡å›¾åƒè´¨é‡ï¼Œä»¥é€‰æ‹©é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åŠ æƒåˆå¹¶è®­ç»ƒæ¡†æ¶å¯ä»¥åœ¨å•ä¸ªå¯¹è±¡æœ‰å¤šä¸ªå‚è€ƒå›¾åƒæ—¶ä½¿ç”¨äºå•ä¸ªå¯¹è±¡çš„ç”Ÿæˆã€‚å®éªŒéªŒè¯æˆ‘ä»¬çš„æ–¹æ³•åœ¨Concept101æ•°æ®é›†å’ŒDreamBoothæ•°æ®é›†çš„å¤šå¯¹è±¡ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯çš„å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨å•å¯¹è±¡ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ–¹é¢æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hqhQAQ/MIP-Adapter%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hqhQAQ/MIP-Adapteræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17920v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€å¾®è°ƒçš„è§£è€¦è·¨æ³¨æ„åŠ›æœºåˆ¶ã€‚å½“æä¾›å¤šä¸ªå‚è€ƒå›¾åƒæ—¶ï¼Œç°æœ‰æ–¹æ³•ä¼šé‡åˆ°å¯¹è±¡æ··æ·†é—®é¢˜ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†æ‰©æ•£æ¨¡å‹ä¸­æ½œåœ¨å›¾åƒç‰¹å¾ä¸ç›®æ ‡å¯¹è±¡ä½ç½®çš„ç›¸å…³æ€§ï¼Œå¹¶æå‡ºåŠ æƒåˆå¹¶æ–¹æ³•å°†å¤šä¸ªå‚è€ƒå›¾åƒç‰¹å¾åˆå¹¶ä¸ºå¯¹åº”å¯¹è±¡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ„å»ºäº†å¤šå¯¹è±¡æ•°æ®é›†ï¼Œå¹¶å¼•å…¥å¯¹è±¡è´¨é‡è¯„åˆ†ä»¥é€‰æ‹©é«˜è´¨é‡è®­ç»ƒæ ·æœ¬ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šå¯¹è±¡ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å•å¯¹è±¡ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æ— éœ€å¾®è°ƒçš„ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨è§£è€¦è·¨æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>å½“å¤„ç†å¤šä¸ªå‚è€ƒå›¾åƒæ—¶ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å¯¹è±¡æ··æ·†é—®é¢˜ã€‚</li>
<li>ç ”ç©¶äº†æ‰©æ•£æ¨¡å‹ä¸­æ½œåœ¨å›¾åƒç‰¹å¾ä¸ç›®æ ‡å¯¹è±¡ä½ç½®çš„ç›¸å…³æ€§ã€‚</li>
<li>æå‡ºäº†åŠ æƒåˆå¹¶æ–¹æ³•ï¼Œå°†å¤šä¸ªå‚è€ƒå›¾åƒç‰¹å¾åˆå¹¶ä¸ºå¯¹åº”å¯¹è±¡ã€‚</li>
<li>æ„å»ºå¤šå¯¹è±¡æ•°æ®é›†ï¼Œå¹¶å¼•å…¥å¯¹è±¡è´¨é‡è¯„åˆ†æœºåˆ¶ä»¥é€‰æ‹©é«˜è´¨é‡è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šå¯¹è±¡ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7e3341fb6f6aed6cfd489083e2d86d16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49ae746042db2df69465711dbdce0cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a37bb9241b3a1fde860929cdd7cff79f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c30307300d0cdff3f5e005406690cc85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a72a7e4eaeda12994df13b176becd37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ddc2fc0ada7bcdcc85a22cac1354452.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation"><a href="#Flash-Diffusion-Accelerating-Any-Conditional-Diffusion-Model-for-Few-Steps-Image-Generation" class="headerlink" title="Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few   Steps Image Generation"></a>Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few   Steps Image Generation</h2><p><strong>Authors:ClÃ©ment Chadebec, Onur Tasar, Eyal Benaroche, Benjamin Aubin</strong></p>
<p>In this paper, we propose an efficient, fast, and versatile distillation method to accelerate the generation of pre-trained diffusion models: Flash Diffusion. The method reaches state-of-the-art performances in terms of FID and CLIP-Score for few steps image generation on the COCO2014 and COCO2017 datasets, while requiring only several GPU hours of training and fewer trainable parameters than existing methods. In addition to its efficiency, the versatility of the method is also exposed across several tasks such as text-to-image, inpainting, face-swapping, super-resolution and using different backbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\alpha$), as well as adapters. In all cases, the method allowed to reduce drastically the number of sampling steps while maintaining very high-quality image generation. The official implementation is available at <a target="_blank" rel="noopener" href="https://github.com/gojasper/flash-diffusion">https://github.com/gojasper/flash-diffusion</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆã€å¿«é€Ÿã€é€šç”¨çš„è’¸é¦æ–¹æ³•æ¥åŠ é€Ÿé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆï¼šFlash Diffusionã€‚è¯¥æ–¹æ³•åœ¨COCO2014å’ŒCOCO2017æ•°æ®é›†ä¸Šçš„å°‘é‡æ­¥éª¤å›¾åƒç”Ÿæˆæ–¹é¢ï¼Œåœ¨FIDå’ŒCLIPåˆ†æ•°ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åªéœ€å‡ å°æ—¶çš„GPUè®­ç»ƒæ—¶é—´ï¼Œå¹¶ä¸”ç›¸è¾ƒäºç°æœ‰æ–¹æ³•éœ€è¦çš„å¯è®­ç»ƒå‚æ•°æ›´å°‘ã€‚é™¤äº†é«˜æ•ˆæ€§ä¹‹å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å±•ç°å‡ºè·¨å¤šä¸ªä»»åŠ¡çš„çµæ´»æ€§ï¼Œå¦‚æ–‡å­—è½¬å›¾åƒã€å›¾åƒè¡¥å…¨ã€é¢éƒ¨æ›¿æ¢ã€è¶…åˆ†è¾¨ç‡ä»¥åŠä½¿ç”¨ä¸åŒçš„ä¸»å¹²ç½‘ç»œå¦‚åŸºäºUNetçš„å»å™ªå™¨ï¼ˆSD1.5ã€SDXLï¼‰æˆ–DiTï¼ˆPixart-Î±ï¼‰ï¼Œä»¥åŠé€‚é…å™¨ç­‰ã€‚åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å¹…å‡å°‘é‡‡æ ·æ­¥éª¤æ•°é‡çš„åŒæ—¶ï¼Œä¿æŒäº†éå¸¸é«˜çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚å®˜æ–¹å®ç°å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/gojasper/flash-diffusion">https://github.com/gojasper/flash-diffusion</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02347v3">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆã€å¿«é€Ÿä¸”é€šç”¨çš„è’¸é¦æ–¹æ³•ï¼Œç”¨äºåŠ é€Ÿé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆï¼šFlash Diffusionã€‚è¯¥æ–¹æ³•åœ¨COCO2014å’ŒCOCO2017æ•°æ®é›†ä¸Šè¿›è¡Œå°‘é‡æ­¥éª¤çš„å›¾åƒç”Ÿæˆæ—¶ï¼Œè¾¾åˆ°äº†FIDå’ŒCLIP-Scoreçš„æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚å®ƒåªéœ€è¦å‡ ä¸ªå°æ—¶çš„GPUè®­ç»ƒæ—¶é—´ï¼Œå¹¶ä¸”ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ‰€éœ€çš„è®­ç»ƒå‚æ•°æ›´å°‘ã€‚é™¤äº†é«˜æ•ˆæ€§å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å…·æœ‰é€šç”¨æ€§ï¼Œå¯ç”¨äºæ–‡æœ¬è½¬å›¾åƒã€å›¾åƒè¡¥å…¨ã€äººè„¸æ›¿æ¢ã€è¶…åˆ†è¾¨ç‡å¤„ç†ç­‰å¤šç§ä»»åŠ¡ï¼Œå¹¶æ”¯æŒä¸åŒçš„éª¨å¹²ç½‘ç»œï¼ˆå¦‚åŸºäºUNetçš„é™å™ªå™¨æˆ–DiTï¼‰å’Œé€‚é…å™¨ã€‚è¯¥æ–¹æ³•å¤§å¹…å‡å°‘äº†é‡‡æ ·æ­¥éª¤æ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Flash Diffusionæ˜¯ä¸€ç§ç”¨äºåŠ é€Ÿé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é«˜æ•ˆã€å¿«é€Ÿå’Œé€šç”¨çš„è’¸é¦æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨COCO2014å’ŒCOCO2017æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜å¼‚çš„FIDå’ŒCLIP-Scoreæ€§èƒ½ã€‚</li>
<li>Flash Diffusionè®­ç»ƒæ—¶é—´çŸ­ï¼Œä»…éœ€å‡ ä¸ªå°æ—¶çš„GPUæ—¶é—´ã€‚</li>
<li>ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼ŒFlash Diffusionéœ€è¦çš„è®­ç»ƒå‚æ•°æ›´å°‘ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œå¯ç”¨äºå¤šç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬è½¬å›¾åƒã€å›¾åƒè¡¥å…¨ã€äººè„¸æ›¿æ¢å’Œè¶…åˆ†è¾¨ç‡å¤„ç†ã€‚</li>
<li>Flash Diffusionæ”¯æŒå¤šç§éª¨å¹²ç½‘ç»œå’Œé€‚é…å™¨ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02347">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cb2686ea1b0bd9c48c139b2516ade5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4860e6d3f9ab853f160ee02cebc4e61e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b83d24033e2a35f1cd6c5bf95160557.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-62816a2ba15fa317d81e7a9f8172f741.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Parameter-efficient Fine-tuning for improved Convolutional Baseline for   Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f48b889f694388b756223771c0d053df.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  GraphAvatar Compact Head Avatars with GNN-Generated 3D Gaussians
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14773.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
