<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  FastVLM Efficient Vision Encoding for Vision Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-0bf82c8a9b7c59eb8562c67e28067af8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    95 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-19-æ›´æ–°"><a href="#2024-12-19-æ›´æ–°" class="headerlink" title="2024-12-19 æ›´æ–°"></a>2024-12-19 æ›´æ–°</h1><h2 id="FastVLM-Efficient-Vision-Encoding-for-Vision-Language-Models"><a href="#FastVLM-Efficient-Vision-Encoding-for-Vision-Language-Models" class="headerlink" title="FastVLM: Efficient Vision Encoding for Vision Language Models"></a>FastVLM: Efficient Vision Encoding for Vision Language Models</h2><p><strong>Authors:Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari</strong></p>
<p>Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2$\times$ improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152$\times$1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85$\times$ faster TTFT and a vision encoder that is 3.4$\times$ smaller. </p>
<blockquote>
<p>å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥è¯´ï¼Œå¢åŠ è¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡å¯¹äºæå‡æ€§èƒ½è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒç†è§£ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚ViTsï¼‰åœ¨é«˜åˆ†è¾¨ç‡ä¸‹å˜å¾—æ•ˆç‡ä½ä¸‹ï¼Œè¿™æ˜¯ç”±äºå †å çš„è‡ªæ³¨æ„åŠ›å±‚å¯¼è‡´çš„ä»¤ç‰Œæ•°é‡ä¼—å¤šå’Œç¼–ç å»¶è¿Ÿé«˜ã€‚åœ¨ä¸åŒçš„æ“ä½œåˆ†è¾¨ç‡ä¸‹ï¼ŒVLMçš„è§†è§‰ç¼–ç å™¨å¯ä»¥é€šè¿‡ä¸¤ä¸ªè½´è¿›è¡Œä¼˜åŒ–ï¼šå‡å°‘ç¼–ç å»¶è¿Ÿå’Œå‡å°‘ä¼ é€’ç»™LLMçš„è§†è§‰ä»¤ç‰Œæ•°é‡ï¼Œä»è€Œé™ä½æ€»ä½“å»¶è¿Ÿã€‚åŸºäºå¯¹å›¾åƒåˆ†è¾¨ç‡ã€è§†è§‰å»¶è¿Ÿã€ä»¤ç‰Œè®¡æ•°å’ŒLLMå¤§å°ä¹‹é—´äº¤äº’çš„ç»¼åˆæ•ˆç‡åˆ†æï¼Œæˆ‘ä»¬å¼•å…¥äº†FastVLMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å®ç°äº†å»¶è¿Ÿã€æ¨¡å‹å¤§å°å’Œå‡†ç¡®æ€§ä¹‹é—´çš„ä¼˜åŒ–æŠ˜è¡·ã€‚FastVLMç»“åˆäº†FastViTHDè¿™ä¸€æ–°å‹æ··åˆè§†è§‰ç¼–ç å™¨ï¼Œæ—¨åœ¨è¾“å‡ºè¾ƒå°‘çš„ä»¤ç‰Œå¹¶æ˜¾è‘—å‡å°‘é«˜åˆ†è¾¨ç‡å›¾åƒçš„ç¼–ç æ—¶é—´ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼ŒFastVLMé€šè¿‡ä»…ç¼©æ”¾è¾“å…¥å›¾åƒå°±èƒ½å®ç°è§†è§‰ä»¤ç‰Œè®¡æ•°å’Œå›¾åƒåˆ†è¾¨ç‡ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ï¼Œä»è€Œæ— éœ€é¢å¤–çš„ä»¤ç‰Œä¿®å‰ªå¹¶ç®€åŒ–äº†æ¨¡å‹è®¾è®¡ã€‚åœ¨LLaVA-1.5è®¾ç½®ä¸­ï¼ŒFastVLMåœ¨æ—¶é—´åˆ°ç¬¬ä¸€ä¸ªä»¤ç‰Œï¼ˆTTFTï¼‰ä¸Šå®ç°äº†3.2å€çš„æ”¹è¿›ï¼ŒåŒæ—¶åœ¨VLMåŸºå‡†æµ‹è¯•ä¸­ä¿æŒä¸å…ˆå‰ä½œå“ç›¸ä¼¼çš„æ€§èƒ½ã€‚ä¸æœ€é«˜åˆ†è¾¨ç‡ï¼ˆ1152Ã—1152ï¼‰ä¸‹çš„LLaVa-OneVisionç›¸æ¯”ï¼ŒFastVLMåœ¨SeedBenchå’ŒMMMUç­‰å…³é”®åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ç›¸å½“çš„æ€§èƒ½ï¼Œä½¿ç”¨ç›¸åŒçš„0.5B LLMï¼Œä½†TTFTåŠ å¿«äº†85å€ï¼Œå¹¶ä¸”è§†è§‰ç¼–ç å™¨ç¼©å°äº†3.4å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13303v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒæé«˜è¾“å…¥å›¾åƒåˆ†è¾¨ç‡å¯¹æå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒç†è§£ä»»åŠ¡ä¸­ã€‚é’ˆå¯¹é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨å¦‚ViTsä¼šå› å¤§é‡æ ‡è®°å’Œç¼–ç å»¶è¿Ÿè€Œå˜å¾—æ•ˆç‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FastVLMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä¼˜åŒ–è§†è§‰ç¼–ç å™¨ï¼Œå®ç°äº†å»¶è¿Ÿã€æ¨¡å‹å¤§å°å’Œå‡†ç¡®åº¦ä¹‹é—´çš„å¹³è¡¡ã€‚FastVLMé‡‡ç”¨FastViTHDè¿™ä¸€æ–°å‹æ··åˆè§†è§‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿè¾“å‡ºæ›´å°‘æ ‡è®°å¹¶å¤§å¹…å‡å°‘é«˜åˆ†è¾¨ç‡å›¾åƒçš„ç¼–ç æ—¶é—´ã€‚ä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼ŒFastVLMä»…é€šè¿‡è°ƒæ•´è¾“å…¥å›¾åƒå¤§å°å°±èƒ½å®ç°è§†è§‰æ ‡è®°æ•°é‡å’Œå›¾åƒåˆ†è¾¨ç‡ä¹‹é—´çš„æœ€ä¼˜å¹³è¡¡ï¼Œæ— éœ€é¢å¤–çš„æ ‡è®°ä¿®å‰ªï¼Œç®€åŒ–äº†æ¨¡å‹è®¾è®¡ã€‚åœ¨LLaVA-1.5é…ç½®ä¸‹ï¼ŒFastVLMåœ¨ä¿æŒç±»ä¼¼å…ˆå‰ä½œå“æ€§èƒ½çš„åŒæ—¶ï¼Œé¦–æ¬¡ä»¤ç‰Œæ—¶é—´æé«˜äº†3.2å€ã€‚ä¸æœ€é«˜åˆ†è¾¨ç‡ï¼ˆ1152Ã—1152ï¼‰ä¸‹çš„LLaVa-OneVisionç›¸æ¯”ï¼ŒFastVLMåœ¨SeedBenchå’ŒMMMUç­‰å…³é”®åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ç›¸å½“çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶ä½¿ç”¨ç›¸åŒçš„0.5B LLMï¼Œå…¶é¦–æ¬¡ä»¤ç‰Œæ—¶é—´åŠ å¿«äº†85å€ï¼Œå¹¶ä¸”è§†è§‰ç¼–ç å™¨ç¼©å°äº†3.4å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æé«˜è¾“å…¥å›¾åƒåˆ†è¾¨ç‡å¯¹å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒç†è§£ä»»åŠ¡ä¸­ã€‚</li>
<li>æµè¡Œçš„è§†è§‰ç¼–ç å™¨åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸‹ä¼šé¢ä¸´æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä¸»è¦å› ä¸ºå¤§é‡æ ‡è®°å’Œç¼–ç å»¶è¿Ÿã€‚</li>
<li>FastVLMæ¨¡å‹é€šè¿‡ä¼˜åŒ–è§†è§‰ç¼–ç å™¨ï¼Œå®ç°äº†å»¶è¿Ÿã€æ¨¡å‹å¤§å°å’Œå‡†ç¡®åº¦ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>FastVLMé‡‡ç”¨FastViTHDè¿™ä¸€æ–°å‹æ··åˆè§†è§‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå‡å°‘è¾“å‡ºæ ‡è®°å’Œç¼–ç æ—¶é—´ã€‚</li>
<li>FastVLMé€šè¿‡è°ƒæ•´è¾“å…¥å›¾åƒå¤§å°å®ç°äº†è§†è§‰æ ‡è®°æ•°é‡å’Œå›¾åƒåˆ†è¾¨ç‡ä¹‹é—´çš„æœ€ä¼˜å¹³è¡¡ï¼Œæ— éœ€é¢å¤–çš„æ ‡è®°ä¿®å‰ªã€‚</li>
<li>ä¸LLaVA-1.5é…ç½®ä¸‹çš„å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒFastVLMåœ¨ä¿æŒç›¸ä¼¼æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æé«˜äº†é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ccd1ffe9be8fd49ba8129393bd66e91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c021efc0ceee0c95391cecdc133a8dab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d00c581938d560113e415c0e9d05db7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a385caaecc27978cfbff4c2d668f3802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3457368291ac951bb31606ca7936c2da.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CRoF-CLIP-based-Robust-Few-shot-Learning-on-Noisy-Labels"><a href="#CRoF-CLIP-based-Robust-Few-shot-Learning-on-Noisy-Labels" class="headerlink" title="CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels"></a>CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels</h2><p><strong>Authors:Shizhuo Deng, Bowen Han, Jiaqi Chen, Hao Wang, Dongyue Chen, Tong Jia</strong></p>
<p>Noisy labels threaten the robustness of few-shot learning (FSL) due to the inexact features in a new domain. CLIP, a large-scale vision-language model, performs well in FSL on image-text embedding similarities, but it is susceptible to misclassification caused by noisy labels. How to enhance domain generalization of CLIP on noisy data within FSL tasks is a critical challenge. In this paper, we provide a novel view to mitigate the influence of noisy labels, CLIP-based Robust Few-shot learning (CRoF). CRoF is a general plug-in module for CLIP-based models. To avoid misclassification and confused label embedding, we design the few-shot task-oriented prompt generator to give more discriminative descriptions of each category. The proposed prompt achieves larger distances of inter-class textual embedding. Furthermore, rather than fully trusting zero-shot classification by CLIP, we fine-tune CLIP on noisy few-shot data in a new domain with a weighting strategy like label-smooth. The weights for multiple potentially correct labels consider the relationship between CLIPâ€™s prior knowledge and original label information to ensure reliability. Our multiple label loss function further supports robust training under this paradigm. Comprehensive experiments show that CRoF, as a plug-in, outperforms fine-tuned and vanilla CLIP models on different noise types and noise ratios. </p>
<blockquote>
<p>å¸¦å™ªå£°çš„æ ‡ç­¾å¯¹æ–°åŸŸä¸­çš„ä¸å‡†ç¡®ç‰¹å¾äº§ç”Ÿäº†å¨èƒï¼Œä»è€Œå½±å“äº†å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰çš„ç¨³å¥æ€§ã€‚CLIPæ˜¯ä¸€ç§å¤§è§„æ¨¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨å›¾åƒæ–‡æœ¬åµŒå…¥ç›¸ä¼¼æ€§æ–¹é¢ï¼ŒFSLè¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒå®¹æ˜“å—åˆ°å¸¦å™ªå£°æ ‡ç­¾å¯¼è‡´çš„è¯¯åˆ†ç±»çš„å½±å“ã€‚å¦‚ä½•åœ¨FSLä»»åŠ¡ä¸­å¯¹å¸¦å™ªå£°æ•°æ®å¢å¼ºCLIPçš„é¢†åŸŸæ³›åŒ–æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å‡è½»å™ªå£°æ ‡ç­¾å½±å“çš„å…¨æ–°è§‚ç‚¹ï¼ŒåŸºäºCLIPçš„é²æ£’å°æ ·æœ¬å­¦ä¹ ï¼ˆCRoFï¼‰ã€‚CRoFæ˜¯ä¸€ä¸ªé€‚ç”¨äºCLIPæ¨¡å‹çš„é€šç”¨æ’ä»¶æ¨¡å—ã€‚ä¸ºäº†é¿å…è¯¯åˆ†ç±»å’Œæ··æ·†æ ‡ç­¾åµŒå…¥ï¼Œæˆ‘ä»¬è®¾è®¡äº†é¢å‘å°æ ·æœ¬ä»»åŠ¡çš„æç¤ºç”Ÿæˆå™¨ï¼Œä¸ºæ¯ä¸ªç±»åˆ«æä¾›æ›´å…·åŒºåˆ†æ€§çš„æè¿°ã€‚æ‰€æå‡ºçš„æç¤ºå®ç°äº†è¾ƒå¤§çš„ç±»é—´æ–‡æœ¬åµŒå…¥è·ç¦»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸æ˜¯å®Œå…¨ä¿¡ä»»CLIPçš„é›¶æ ·æœ¬åˆ†ç±»ï¼Œè€Œæ˜¯ä½¿ç”¨æ ‡ç­¾å¹³æ»‘ç­‰åŠ æƒç­–ç•¥å¯¹æ–°é¢†åŸŸä¸­çš„å¸¦å™ªå£°çš„å°æ ·æœ¬æ•°æ®è¿›è¡Œå¾®è°ƒã€‚å¤šä¸ªå¯èƒ½æ­£ç¡®çš„æ ‡ç­¾çš„æƒé‡è€ƒè™‘äº†CLIPçš„å…ˆéªŒçŸ¥è¯†ä¸åŸå§‹æ ‡ç­¾ä¿¡æ¯ä¹‹é—´çš„å…³ç³»ï¼Œä»¥ç¡®ä¿å¯é æ€§ã€‚æˆ‘ä»¬çš„å¤šæ ‡ç­¾æŸå¤±å‡½æ•°è¿›ä¸€æ­¥æ”¯æŒæ­¤æ¨¡å¼ä¸‹çš„ç¨³å¥è®­ç»ƒã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œä½œä¸ºæ’ä»¶çš„CRoFåœ¨ä¸åŒå™ªå£°ç±»å‹å’Œå™ªå£°æ¯”ç‡ä¸Šä¼˜äºç»è¿‡å¾®è°ƒçš„å’ŒåŸç”Ÿçš„CLIPæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12793v1">PDF</a> </p>
<p><strong>Summary</strong><br>    CLIPæ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä»»åŠ¡ä¸­ï¼Œå› å™ªå£°æ ‡ç­¾å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºåº”å¯¹æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºCLIPçš„é²æ£’å°‘æ ·æœ¬å­¦ä¹ ï¼ˆCRoFï¼‰æ–¹æ³•ã€‚CRoFè®¾è®¡ä»»åŠ¡å¯¼å‘çš„æç¤ºç”Ÿæˆå™¨ï¼Œå¢å¼ºç±»åˆ«æè¿°é‰´åˆ«åŠ›ï¼Œå¹¶å¾®è°ƒCLIPæ¨¡å‹åœ¨æ–°åŸŸå™ªå£°æ•°æ®ä¸Šçš„æƒé‡ç­–ç•¥ï¼ŒåŒæ—¶é‡‡ç”¨å¤šæ ‡ç­¾æŸå¤±å‡½æ•°æå‡é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCRoFä¼˜äºå¾®è°ƒåŠåŸç”Ÿçš„CLIPæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨FSLä»»åŠ¡ä¸­å—å™ªå£°æ ‡ç­¾å½±å“ï¼Œæ€§èƒ½å—é™ã€‚</li>
<li>æå‡ºCRoFæ–¹æ³•ï¼Œä¸ºCLIPæ¨¡å‹æä¾›é²æ£’æ€§å¢å¼ºã€‚</li>
<li>è®¾è®¡ä»»åŠ¡å¯¼å‘çš„æç¤ºç”Ÿæˆå™¨ï¼Œæé«˜ç±»åˆ«æè¿°çš„é‰´åˆ«åŠ›ã€‚</li>
<li>é‡‡ç”¨å¾®è°ƒç­–ç•¥ï¼Œåœ¨æ–°åŸŸå™ªå£°æ•°æ®ä¸Šä¼˜åŒ–CLIPæ¨¡å‹çš„æƒé‡ã€‚</li>
<li>åˆ©ç”¨å¤šæ ‡ç­¾æŸå¤±å‡½æ•°æå‡æ¨¡å‹åœ¨å™ªå£°æ•°æ®ä¸‹çš„é²æ£’æ€§è®­ç»ƒã€‚</li>
<li>CRoFæ–¹æ³•åœ¨å„ç§å™ªå£°ç±»å‹å’Œæ¯”ä¾‹ä¸‹çš„è¡¨ç°ä¼˜äºåŸCLIPæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae206aa32a9795d211ebc72fe46a3237.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-516b5ba9897b35ab1fcb44ce145e4f82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d67809d7d0a7b88e56e75bba1c02c5ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54aecf5daf027ba6d0c0c84fe0979d13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6805ce8784547a01100de0f51f51940d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0949bb5f2161c80881f270612e2bc64a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4251e14cf28fa612741195f3bdaf2185.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DuSSS-Dual-Semantic-Similarity-Supervised-Vision-Language-Model-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#DuSSS-Dual-Semantic-Similarity-Supervised-Vision-Language-Model-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for   Semi-Supervised Medical Image Segmentation"></a>DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Qingtao Pan, Wenhao Qiao, Jingjiao Lou, Bing Ji, Shuo Li</strong></p>
<p>Semi-supervised medical image segmentation (SSMIS) uses consistency learning to regularize model training, which alleviates the burden of pixel-wise manual annotations. However, it often suffers from error supervision from low-quality pseudo labels. Vision-Language Model (VLM) has great potential to enhance pseudo labels by introducing text prompt guided multimodal supervision information. It nevertheless faces the cross-modal problem: the obtained messages tend to correspond to multiple targets. To address aforementioned problems, we propose a Dual Semantic Similarity-Supervised VLM (DuSSS) for SSMIS. Specifically, 1) a Dual Contrastive Learning (DCL) is designed to improve cross-modal semantic consistency by capturing intrinsic representations within each modality and semantic correlations across modalities. 2) To encourage the learning of multiple semantic correspondences, a Semantic Similarity-Supervision strategy (SSS) is proposed and injected into each contrastive learning process in DCL, supervising semantic similarity via the distribution-based uncertainty levels. Furthermore, a novel VLM-based SSMIS network is designed to compensate for the quality deficiencies of pseudo-labels. It utilizes the pretrained VLM to generate text prompt guided supervision information, refining the pseudo label for better consistency regularization. Experimental results demonstrate that our DuSSS achieves outstanding performance with Dice of 82.52%, 74.61% and 78.03% on three public datasets (QaTa-COV19, BM-Seg and MoNuSeg). </p>
<blockquote>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰é‡‡ç”¨ä¸€è‡´æ€§å­¦ä¹ æ¥è§„èŒƒæ¨¡å‹è®­ç»ƒï¼Œå‡è½»äº†åƒç´ çº§æ‰‹åŠ¨æ³¨é‡Šçš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œå®ƒå¸¸å¸¸å—åˆ°ä½è´¨é‡ä¼ªæ ‡ç­¾çš„é”™è¯¯ç›‘ç£çš„å½±å“ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡å¼•å…¥æ–‡æœ¬æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€ç›‘ç£ä¿¡æ¯ï¼Œåœ¨å¢å¼ºä¼ªæ ‡ç­¾æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒé¢ä¸´è·¨æ¨¡æ€çš„é—®é¢˜ï¼šè·å–çš„ä¿¡æ¯å¾€å¾€å¯¹åº”å¤šä¸ªç›®æ ‡ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºSSMISçš„åŒé‡è¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£VLMï¼ˆDuSSSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œ1ï¼‰è®¾è®¡äº†åŒé‡å¯¹æ¯”å­¦ä¹ ï¼ˆDCLï¼‰ï¼Œé€šè¿‡æ•æ‰æ¯ç§æ¨¡æ€çš„å†…åœ¨è¡¨ç¤ºå’Œè·¨æ¨¡æ€çš„è¯­ä¹‰å…³è”ï¼Œæ¥æé«˜è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ã€‚2ï¼‰ä¸ºäº†é¼“åŠ±å­¦ä¹ å¤šä¸ªè¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œæå‡ºäº†è¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£ç­–ç•¥ï¼ˆSSSï¼‰ï¼Œå¹¶å°†å…¶æ³¨å…¥DCLä¸­çš„æ¯ä¸ªå¯¹æ¯”å­¦ä¹ è¿‡ç¨‹ï¼Œé€šè¿‡åŸºäºåˆ†å¸ƒçš„ä¸ç¡®å®šæ€§æ°´å¹³æ¥ç›‘ç£è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºVLMçš„SSMISç½‘ç»œï¼Œä»¥å¼¥è¡¥ä¼ªæ ‡ç­¾çš„è´¨é‡ç¼ºé™·ã€‚å®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„VLMç”Ÿæˆæ–‡æœ¬æç¤ºå¼•å¯¼çš„ç›‘ç£ä¿¡æ¯ï¼Œå¯¹ä¼ªæ ‡ç­¾è¿›è¡Œç»†åŒ–ï¼Œä»¥å®ç°æ›´å¥½çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„DuSSSåœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆQaTa-COV19ã€BM-Segå’ŒMoNuSegï¼‰ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒDiceç³»æ•°åˆ†åˆ«ä¸º82.52%ã€74.61%å’Œ78.03%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12492v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰åˆ©ç”¨ä¸€è‡´æ€§å­¦ä¹ æ¥è§„èŒƒæ¨¡å‹è®­ç»ƒï¼Œå‡è½»åƒç´ çº§æ‰‹åŠ¨æ³¨é‡Šçš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œå®ƒå¸¸å—åˆ°ä½è´¨é‡ä¼ªæ ‡ç­¾çš„é”™è¯¯ç›‘ç£å½±å“ã€‚ä¸ºå¢å¼ºä¼ªæ ‡ç­¾å¹¶è§£å†³è·¨æ¨¡æ€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŒè¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆDuSSSï¼‰ã€‚é€šè¿‡è®¾è®¡åŒå¯¹æ¯”å­¦ä¹ ï¼ˆDCLï¼‰å’Œè¯­ä¹‰ç›¸ä¼¼æ€§ç›‘ç£ç­–ç•¥ï¼ˆSSSï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSMISåˆ©ç”¨ä¸€è‡´æ€§å­¦ä¹ è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå‡å°‘æ‰‹åŠ¨æ³¨é‡Šå·¥ä½œé‡ï¼Œä½†ä½è´¨é‡ä¼ªæ ‡ç­¾å¯¼è‡´çš„é”™è¯¯ç›‘ç£æ˜¯å…¶æŒ‘æˆ˜ã€‚</li>
<li>VLMå…·æœ‰é€šè¿‡å¼•å…¥æ–‡æœ¬æç¤ºå¼•å¯¼çš„å¤šæ¨¡æ€ç›‘ç£ä¿¡æ¯æ¥å¢å¼ºä¼ªæ ‡ç­¾çš„æ½œåŠ›ã€‚</li>
<li>DuSSSé€šè¿‡DCLæé«˜è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ï¼Œé€šè¿‡æ•è·å„æ¨¡æ€çš„å†…åœ¨è¡¨ç¤ºå’Œè·¨æ¨¡æ€çš„è¯­ä¹‰å…³è”æ¥å®ç°ã€‚</li>
<li>SSSç­–ç•¥è¢«æå‡ºä»¥é¼“åŠ±å­¦ä¹ å¤šä¸ªè¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¹¶æ³¨å…¥DCLçš„æ¯ä¸ªå¯¹æ¯”å­¦ä¹ è¿‡ç¨‹ï¼Œé€šè¿‡åŸºäºåˆ†å¸ƒçš„ä¸ç¡®å®šæ€§æ°´å¹³æ¥ç›‘ç£è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</li>
<li>DuSSSè®¾è®¡äº†ä¸€ä¸ªåŸºäºVLMçš„SSMISç½‘ç»œï¼Œä»¥å¼¥è¡¥ä¼ªæ ‡ç­¾è´¨é‡ç¼ºé™·ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„VLMç”Ÿæˆæ–‡æœ¬æç¤ºå¼•å¯¼çš„ç›‘ç£ä¿¡æ¯ï¼Œå¯¹ä¼ªæ ‡ç­¾è¿›è¡Œç²¾ç‚¼ï¼Œå®ç°æ›´å¥½çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDuSSSåœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„Diceæ€§èƒ½è¾¾åˆ°82.52%ã€74.61%å’Œ78.03%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a7239e0c9bcb715b290fbf64ebabc295.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f89b040b8a468f5823b755eadd1ef7bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-198c30e94c70e5a3f4feeba5d1a3cb07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-235ce8abb7612cf23263f02516ed03aa.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CPath-Omni-A-Unified-Multimodal-Foundation-Model-for-Patch-and-Whole-Slide-Image-Analysis-in-Computational-Pathology"><a href="#CPath-Omni-A-Unified-Multimodal-Foundation-Model-for-Patch-and-Whole-Slide-Image-Analysis-in-Computational-Pathology" class="headerlink" title="CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole   Slide Image Analysis in Computational Pathology"></a>CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole   Slide Image Analysis in Computational Pathology</h2><p><strong>Authors:Yuxuan Sun, Yixuan Si, Chenglu Zhu, Xuan Gong, Kai Zhang, Pingyi Chen, Ye Zhang, Zhongyi Shui, Tao Lin, Lin Yang</strong></p>
<p>The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the integration of learned knowledge across patches and WSIs, and resulting in redundant models. In this work, we introduce CPath-Omni, the first 15-billion-parameter LMM designed to unify both patch and WSI level image analysis, consolidating a variety of tasks at both levels, including classification, visual question answering, captioning, and visual referring prompting. Extensive experiments demonstrate that CPath-Omni achieves state-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42 datasets, outperforming or matching task-specific models trained for individual tasks. Additionally, we develop a specialized pathology CLIP-based visual processor for CPath-Omni, CPath-CLIP, which, for the first time, integrates different vision models and incorporates a large language model as a text encoder to build a more powerful CLIP model, which achieves SOTA performance on nine zero-shot and four few-shot datasets. Our findings highlight CPath-Omniâ€™s ability to unify diverse pathology tasks, demonstrating its potential to streamline and advance the field of foundation model in pathology. </p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‡ºç°ä¸ºç—…ç†å­¦é¢†åŸŸå¸¦æ¥äº†å·¨å¤§çš„è¿›æ­¥ã€‚ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ†åˆ«è®­ç»ƒè¡¥ä¸çº§åˆ«å’Œå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çº§åˆ«çš„æ¨¡å‹ä¸Šï¼Œè¿™é™åˆ¶äº†è·¨è¡¥ä¸å’ŒWSIsæ‰€å­¦çŸ¥è¯†çš„æ•´åˆï¼Œå¹¶å¯¼è‡´äº†å†—ä½™æ¨¡å‹çš„å‡ºç°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CPath-Omniï¼Œè¿™æ˜¯é¦–ä¸ªè®¾è®¡çš„15äº¿å‚æ•°LMMï¼Œæ—¨åœ¨ç»Ÿä¸€è¡¥ä¸å’ŒWSIçº§åˆ«çš„å›¾åƒåˆ†æï¼Œå·©å›ºè¿™ä¸¤ä¸ªçº§åˆ«çš„å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€è§†è§‰é—®ç­”ã€æè¿°å’Œè§†è§‰æç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCPath-Omniåœ¨42ä¸ªæ•°æ®é›†ä¸­çš„39ä¸ªæ•°æ®é›†ä¸Šçš„ä¸ƒä¸ªä¸åŒä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºæˆ–åŒ¹é…äº†é’ˆå¯¹å•ä¸ªä»»åŠ¡è®­ç»ƒçš„ç‰¹å®šä»»åŠ¡æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºCPath-Omniå¼€å‘äº†ä¸€ä¸ªåŸºäºCLIPçš„ä¸“ç”¨ç—…ç†è§†è§‰å¤„ç†å™¨CPath-CLIPï¼Œå®ƒé¦–æ¬¡å°†ä¸åŒçš„è§†è§‰æ¨¡å‹æ•´åˆåœ¨ä¸€èµ·ï¼Œå¹¶èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œä»¥æ„å»ºæ›´å¼ºå¤§çš„CLIPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¹ä¸ªé›¶æ ·æœ¬å’Œå››ä¸ªå°‘æ ·æœ¬æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†CPath-Omniç»Ÿä¸€å„ç§ç—…ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¡¨æ˜äº†å…¶åœ¨ç—…ç†å­¦é¢†åŸŸåŸºç¡€æ¨¡å‹ä¸­çš„æ½œåŠ›å’Œä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12077v1">PDF</a> 22 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‡ºç°åœ¨ç—…ç†å­¦é¢†åŸŸå¸¦æ¥äº†é‡å¤§çªç ´ã€‚è¿‡å»çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ†åˆ«è®­ç»ƒè¡¥ä¸çº§åˆ«å’Œå…¨å¹»ç¯ç‰‡çº§åˆ«æ¨¡å‹ä¸Šï¼Œè¿™é™åˆ¶äº†è·¨è¡¥ä¸å’Œå…¨å¹»ç¯ç‰‡é›†æˆçš„çŸ¥è¯†çš„æ•´åˆï¼Œå¹¶å¯¼è‡´äº†å†—ä½™æ¨¡å‹çš„å‡ºç°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CPath-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–ä¸ªæ—¨åœ¨ç»Ÿä¸€è¡¥ä¸å’Œå¹»ç¯ç‰‡çº§åˆ«å›¾åƒåˆ†æçš„å¤§å‹æ¨¡å‹ï¼Œå®ƒå¯ä»¥åœ¨è¿™ä¸¤ä¸ªå±‚é¢ä¸Šæ•´åˆå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€è§†è§‰é—®ç­”ã€æè¿°å’Œè§†è§‰æç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒCPath-Omniåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„æ°´å¹³ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºCPath-Omniå¼€å‘äº†ä¸€ç§åŸºäºCLIPçš„ä¸“ç”¨ç—…ç†è§†è§‰å¤„ç†å™¨CPath-CLIPï¼Œå®ƒé¦–æ¬¡å°†ä¸åŒçš„è§†è§‰æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸ç»“åˆä½œä¸ºæ–‡æœ¬ç¼–ç å™¨æ¥æ„å»ºæ›´å¼ºå¤§çš„CLIPæ¨¡å‹ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†CPath-Omniç»Ÿä¸€å„ç§ç—…ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç—…ç†å­¦åŸºç¡€æ¨¡å‹é¢†åŸŸæ¨åŠ¨å’Œè¿›æ­¥æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¼•å…¥åœ¨ç—…ç†å­¦é¢†åŸŸå¸¦æ¥äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>CPath-Omniæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼Œç»“åˆäº†è¡¥ä¸çº§åˆ«å’Œå…¨å¹»ç¯ç‰‡çº§åˆ«çš„å›¾åƒåˆ†æã€‚</li>
<li>CPath-Omnièƒ½å¤Ÿåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€è§†è§‰é—®ç­”ç­‰ã€‚</li>
<li>CPath-CLIPæ˜¯CPath-Omniçš„ä¸€ä¸ªä¸“é—¨è§†è§‰å¤„ç†å™¨ï¼Œç»“åˆäº†å¤šç§æ¨¡å‹å¹¶å®ç°äº†é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>CPath-Omniå…·æœ‰ç»Ÿä¸€å¤šç§ç—…ç†ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå…·æœ‰æ¨åŠ¨ç—…ç†å­¦åŸºç¡€æ¨¡å‹é¢†åŸŸå‘å±•çš„æ½œåŠ›ã€‚</li>
<li>æ­¤æ¨¡å‹é€šè¿‡è·¨è¡¥ä¸å’Œå…¨å¹»ç¯ç‰‡çš„é›†æˆçŸ¥è¯†å‡å°‘äº†å†—ä½™æ¨¡å‹çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40de384d2746dc54b07326c3069a9984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70eee2a1e7dbe5812606dcab76373ed1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3362b0067b35a06feb442ad7607822f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6d2e833df182c1666107f7b0044b3f3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FSFM-A-Generalizable-Face-Security-Foundation-Model-via-Self-Supervised-Facial-Representation-Learning"><a href="#FSFM-A-Generalizable-Face-Security-Foundation-Model-via-Self-Supervised-Facial-Representation-Learning" class="headerlink" title="FSFM: A Generalizable Face Security Foundation Model via Self-Supervised   Facial Representation Learning"></a>FSFM: A Generalizable Face Security Foundation Model via Self-Supervised   Facial Representation Learning</h2><p><strong>Authors:Gaojian Wang, Feng Lin, Tong Wu, Zhenguang Liu, Zhongjie Ba, Kui Ren</strong></p>
<p>This work asks: with abundant, unlabeled real faces, how to learn a robust and transferable facial representation that boosts various face security tasks with respect to generalization performance? We make the first attempt and propose a self-supervised pretraining framework to learn fundamental representations of real face images, FSFM, that leverages the synergy between masked image modeling (MIM) and instance discrimination (ID). We explore various facial masking strategies for MIM and present a simple yet powerful CRFR-P masking, which explicitly forces the model to capture meaningful intra-region consistency and challenging inter-region coherency. Furthermore, we devise the ID network that naturally couples with MIM to establish underlying local-to-global correspondence via tailored self-distillation. These three learning objectives, namely 3C, empower encoding both local features and global semantics of real faces. After pretraining, a vanilla ViT serves as a universal vision foundation model for downstream face security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forgery detection. Extensive experiments on 10 public datasets demonstrate that our model transfers better than supervised pretraining, visual and facial self-supervised learning arts, and even outperforms task-specialized SOTA methods. </p>
<blockquote>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºçš„é—®é¢˜æ˜¯ï¼šåœ¨å¤§é‡æ— æ ‡ç­¾çš„çœŸå®äººè„¸æ•°æ®æƒ…å†µä¸‹ï¼Œå¦‚ä½•å­¦ä¹ ä¸€ç§ç¨³å¥ä¸”å¯è¿ç§»çš„äººè„¸è¡¨ç¤ºï¼Œä»¥æé«˜å…³äºæ³›åŒ–æ€§èƒ½çš„å„ç§äººè„¸å®‰å…¨ä»»åŠ¡çš„æ€§èƒ½ï¼Ÿæˆ‘ä»¬é¦–æ¬¡å°è¯•å¹¶æå‡ºä¸€ç§è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå­¦ä¹ çœŸå®äººè„¸å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºï¼ŒFSFMï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹é‰´åˆ«ï¼ˆIDï¼‰ä¹‹é—´çš„ååŒä½œç”¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†MIMçš„å„ç§é¢éƒ¨æ©ç ç­–ç•¥ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„CRFR-Pæ©ç ï¼Œå®ƒæ˜ç¡®åœ°è¿«ä½¿æ¨¡å‹æ•æ‰åŒºåŸŸå†…æœ‰æ„ä¹‰çš„ä¸€è‡´æ€§å’ŒåŒºåŸŸé—´æŒ‘æˆ˜æ€§çš„è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸MIMè‡ªç„¶ç»“åˆçš„IDç½‘ç»œï¼Œé€šè¿‡å®šåˆ¶çš„è‡ªè’¸é¦å»ºç«‹åŸºæœ¬çš„å±€éƒ¨åˆ°å…¨å±€å¯¹åº”å…³ç³»ã€‚è¿™ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡ï¼Œå³3Cï¼Œä½¿å¾—ç¼–ç çœŸå®äººè„¸çš„å±€éƒ¨ç‰¹å¾å’Œå…¨å±€è¯­ä¹‰æˆä¸ºå¯èƒ½ã€‚é¢„è®­ç»ƒåï¼Œä¸€ä¸ªç®€å•çš„ViTä½œä¸ºä¸‹æ¸¸äººè„¸å®‰å…¨ä»»åŠ¡çš„é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼šè·¨æ•°æ®é›†æ·±åº¦ä¼ªé€ æ£€æµ‹ã€è·¨åŸŸé¢éƒ¨é˜²ä¼ªã€æœªè§æ‰©æ•£é¢éƒ¨ä¼ªé€ æ£€æµ‹ã€‚åœ¨1Hä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿ç§»æ•ˆæœä¼˜äºç›‘ç£é¢„è®­ç»ƒã€è§†è§‰å’Œé¢éƒ¨è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œç”šè‡³è¶…è¶Šäº†ä»»åŠ¡ä¸“ä¸šåŒ–çš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12032v1">PDF</a> 21 pages, 11 figures, project page: <a target="_blank" rel="noopener" href="https://fsfm-3c.github.io/">https://fsfm-3c.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„çœŸå®é¢éƒ¨å›¾åƒè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ ç¨³å¥ä¸”å¯è¿ç§»çš„é¢éƒ¨è¡¨ç¤ºã€‚æå‡ºä¸€ç§è‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶FSFMï¼Œç»“åˆæ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰å’Œå®ä¾‹åˆ¤åˆ«ï¼ˆIDï¼‰æ¥å­¦ä¹ çœŸå®é¢éƒ¨å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºã€‚æ¢ç´¢äº†å¤šç§é¢éƒ¨æ©è†œç­–ç•¥ï¼Œå¹¶æå‡ºç®€å•è€Œå¼ºå¤§çš„CRFR-Pæ©è†œï¼Œä»¥æ˜ç¡®æ•æ‰åŒºåŸŸå†…çš„ä¸€è‡´æ€§ä»¥åŠåŒºåŸŸé—´çš„è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸MIMè‡ªç„¶ç»“åˆçš„IDç½‘ç»œï¼Œé€šè¿‡å®šåˆ¶çš„è‡ªæˆ‘è’¸é¦å»ºç«‹å±€éƒ¨åˆ°å…¨å±€çš„å¯¹åº”å…³ç³»ã€‚è¿™ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡ï¼Œå³3Cï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç¼–ç çœŸå®é¢éƒ¨çš„å±€éƒ¨ç‰¹å¾å’Œå…¨å±€è¯­ä¹‰ã€‚é¢„è®­ç»ƒåï¼Œå¯ä½œä¸ºä¸‹æ¸¸é¢éƒ¨å®‰å…¨ä»»åŠ¡çš„é€šç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¦‚è·¨æ•°æ®é›†æ·±åº¦ä¼ªé€ æ£€æµ‹ã€è·¨åŸŸé¢éƒ¨é˜²ä¼ªå’Œæœªè§æ‰©æ•£é¢éƒ¨ä¼ªé€ æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ—¨åœ¨åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„çœŸå®é¢éƒ¨å›¾åƒè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå­¦ä¹ ç¨³å¥ä¸”å¯è¿ç§»çš„é¢éƒ¨è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè‡ªç›‘ç£é¢„è®­ç»ƒæ¡†æ¶FSFMï¼Œç»“åˆMIMå’ŒIDæ¥å­¦ä¹ é¢éƒ¨å›¾åƒçš„åŸºæœ¬è¡¨ç¤ºã€‚</li>
<li>ä»‹ç»äº†CRFR-Pæ©è†œç­–ç•¥ï¼Œå¼ºè°ƒæ¨¡å‹å¯¹åŒºåŸŸå†…å’ŒåŒºåŸŸé—´çš„é¢éƒ¨ç‰¹å¾çš„æ•æ‰ã€‚</li>
<li>é€šè¿‡å®šåˆ¶çš„è‡ªæˆ‘è’¸é¦å»ºç«‹å±€éƒ¨åˆ°å…¨å±€çš„å¯¹åº”å…³ç³»ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>ä¸‰ä¸ªå­¦ä¹ ç›®æ ‡â€”â€”3Cï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç¼–ç çœŸå®é¢éƒ¨çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šäº†ç›‘ç£é¢„è®­ç»ƒå’Œå…¶ä»–é¢éƒ¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”šè‡³è¶…è¿‡äº†ä»»åŠ¡ä¸“ä¸šåŒ–çš„æœ€æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51fe725ed0bfc17a4137564afb63213d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6fe8794954004c0d53cb83f71153bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ed3ef88cb703ce660e6cf558c18a841.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbdd69074a50d3e26d3aeab3f98d8cfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45dd4ef520c7cc87cd57e415e4c9b7f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SAMIC-Segment-Anything-with-In-Context-Spatial-Prompt-Engineering"><a href="#SAMIC-Segment-Anything-with-In-Context-Spatial-Prompt-Engineering" class="headerlink" title="SAMIC: Segment Anything with In-Context Spatial Prompt Engineering"></a>SAMIC: Segment Anything with In-Context Spatial Prompt Engineering</h2><p><strong>Authors:Savinay Nagendra, Kashif Rashid, Chaopeng Shen, Daniel Kifer</strong></p>
<p>Few-shot segmentation is the problem of learning to identify specific types of objects (e.g., airplanes) in images from a small set of labeled reference images. The current state of the art is driven by resource-intensive construction of models for every new domain-specific application. Such models must be trained on enormous labeled datasets of unrelated objects (e.g., cars, trains, animals) so that their &#96;&#96;knowledgeâ€™â€™ can be transferred to new types of objects. In this paper, we show how to leverage existing vision foundation models (VFMs) to reduce the incremental cost of creating few-shot segmentation models for new domains. Specifically, we introduce SAMIC, a small network that learns how to prompt VFMs in order to segment new types of objects in domain-specific applications. SAMIC enables any task to be approached as a few-shot learning problem. At 2.6 million parameters, it is 94% smaller than the leading models (e.g., having ResNet 101 backbone with 45+ million parameters). Even using 1&#x2F;5th of the training data provided by one-shot benchmarks, SAMIC is competitive with, or sets the state of the art, on a variety of few-shot and semantic segmentation datasets including COCO-$20^i$, Pascal-$5^i$, PerSeg, FSS-1000, and NWPU VHR-10. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬åˆ†å‰²ï¼ˆFew-shot segmentationï¼‰æ˜¯æŒ‡ä»å°‘é‡æ ‡è®°å‚è€ƒå›¾åƒä¸­å­¦ä¹ è¯†åˆ«ç‰¹å®šç±»å‹çš„å¯¹è±¡ï¼ˆä¾‹å¦‚é£æœºï¼‰çš„é—®é¢˜ã€‚ç›®å‰çš„æŠ€æœ¯çŠ¶æ€ä¾èµ–äºé’ˆå¯¹æ¯ä¸ªæ–°çš„ç‰¹å®šé¢†åŸŸåº”ç”¨æ„å»ºèµ„æºå¯†é›†å‹çš„æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å¿…é¡»åœ¨å¤§é‡ä¸ç›¸å…³å¯¹è±¡ï¼ˆä¾‹å¦‚æ±½è½¦ã€ç«è½¦ã€åŠ¨ç‰©ï¼‰çš„æ ‡è®°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ä¾¿å°†å®ƒä»¬çš„â€œçŸ¥è¯†â€è½¬ç§»åˆ°æ–°å‹å¯¹è±¡ä¸Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰æ¥é™ä½åˆ›å»ºç”¨äºæ–°é¢†åŸŸçš„å°‘é‡æ ·æœ¬åˆ†å‰²æ¨¡å‹çš„å¢é‡æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†SAMICï¼Œè¿™æ˜¯ä¸€ä¸ªå°å‹ç½‘ç»œï¼Œå®ƒå­¦ä¹ å¦‚ä½•æç¤ºVFMä»¥å¯¹ç‰¹å®šé¢†åŸŸåº”ç”¨ä¸­çš„æ–°å‹å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚SAMICä½¿ä»»ä½•ä»»åŠ¡éƒ½å¯ä»¥ä½œä¸ºå°‘é‡æ ·æœ¬å­¦ä¹ é—®é¢˜æ¥å¤„ç†ã€‚å®ƒåªæœ‰260ä¸‡ä¸ªå‚æ•°ï¼Œæ¯”é¢†å…ˆçš„æ¨¡å‹å°94%ï¼ˆä¾‹å¦‚å…·æœ‰è¶…è¿‡4500ä¸‡ä¸ªå‚æ•°çš„ResNet 101ä¸»å¹²ç½‘ï¼‰ã€‚å³ä½¿åªä½¿ç”¨å•æ¬¡åŸºå‡†æµ‹è¯•æä¾›çš„äº”åˆ†ä¹‹ä¸€è®­ç»ƒæ•°æ®ï¼ŒSAMICåœ¨å„ç§å°‘é‡æ ·æœ¬å’Œè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šä¹Ÿå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬COCO-20iã€Pascal-5iã€PerSegã€FSS-1000å’ŒNWPU VHR-10ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11998v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•åˆ©ç”¨ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰é™ä½ä¸ºæ–°é¢†åŸŸåˆ›å»ºå°æ ·æœ¬åˆ†å‰²æ¨¡å‹çš„å¢é‡æˆæœ¬ã€‚é€šè¿‡å¼•å…¥SAMICç½‘ç»œï¼Œå­¦ä¹ å¦‚ä½•æç¤ºVFMså¯¹æ–°çš„å¯¹è±¡ç±»å‹è¿›è¡Œåˆ†å‰²ï¼Œä½¿å¾—ä»»ä½•ä»»åŠ¡éƒ½å¯ä»¥ä½œä¸ºå°æ ·æœ¬å­¦ä¹ é—®é¢˜æ¥è§£å†³ã€‚SAMICç½‘ç»œä»…æœ‰260ä¸‡ä¸ªå‚æ•°ï¼Œæ¯”é¢†å…ˆçš„æ¨¡å‹å°94%ï¼Œå³ä½¿ä½¿ç”¨ç°æœ‰åŸºå‡†æµ‹è¯•é›†çš„1&#x2F;5çš„è®­ç»ƒæ•°æ®ï¼Œå…¶åœ¨å¤šç§å°æ ·æœ¬å’Œè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¹Ÿæå…·ç«äº‰åŠ›ï¼Œç”šè‡³è¾¾åˆ°æˆ–è¶…è¶Šäº†å½“å‰æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»å°æ ·æœ¬åˆ†å‰²é—®é¢˜ï¼Œå³åˆ©ç”¨å°‘é‡æ ‡è®°å‚è€ƒå›¾åƒè¯†åˆ«å›¾åƒä¸­çš„ç‰¹å®šå¯¹è±¡ç±»å‹ã€‚</li>
<li>å½“å‰æŠ€æœ¯ä¾èµ–ä¸ºæ¯ä¸ªæ–°é¢†åŸŸç‰¹å®šåº”ç”¨æ„å»ºèµ„æºå¯†é›†å‹çš„æ¨¡å‹ã€‚</li>
<li>æå‡ºåˆ©ç”¨ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰é™ä½åˆ›å»ºæ–°é¢†åŸŸå°æ ·æœ¬åˆ†å‰²æ¨¡å‹çš„å¢é‡æˆæœ¬ã€‚</li>
<li>å¼•å…¥SAMICç½‘ç»œï¼Œå­¦ä¹ å¦‚ä½•æç¤ºVFMsè¿›è¡Œæ–°ç±»å‹å¯¹è±¡çš„åˆ†å‰²ã€‚</li>
<li>SAMICç½‘ç»œæ›´å°ã€æ›´é«˜æ•ˆï¼Œåªæœ‰260ä¸‡ä¸ªå‚æ•°ï¼Œç›¸æ¯”é¢†å…ˆçš„æ¨¡å‹å‡å°äº†94%ã€‚</li>
<li>SAMICåœ¨å¤šç§å°æ ·æœ¬å’Œè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°æå…·ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬COCO-20iã€Pascal-5iã€PerSegã€FSS-1000å’ŒNWPU VHR-10ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5dcccc8f7409bf3883ba5c7e65c85ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-502a1c29ea1e0438d60bdaffe992094a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e78564c07f0503bef0497aafa42a166b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0b8fd980b80bf2a5de9554fcd0b4eee.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="From-2D-CAD-Drawings-to-3D-Parametric-Models-A-Vision-Language-Approach"><a href="#From-2D-CAD-Drawings-to-3D-Parametric-Models-A-Vision-Language-Approach" class="headerlink" title="From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach"></a>From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach</h2><p><strong>Authors:Xilin Wang, Jia Zheng, Yuanchao Hu, Hao Zhu, Qian Yu, Zihan Zhou</strong></p>
<p>In this paper, we present CAD2Program, a new method for reconstructing 3D parametric models from 2D CAD drawings. Our proposed method is inspired by recent successes in vision-language models (VLMs), and departs from traditional methods which rely on task-specific data representations and&#x2F;or algorithms. Specifically, on the input side, we simply treat the 2D CAD drawing as a raster image, regardless of its original format, and encode the image with a standard ViT model. We show that such an encoding scheme achieves competitive performance against existing methods that operate on vector-graphics inputs, while imposing substantially fewer restrictions on the 2D drawings. On the output side, our method auto-regressively predicts a general-purpose language describing 3D parametric models in text form. Compared to other sequence modeling methods for CAD which use domain-specific sequence representations with fixed-size slots, our text-based representation is more flexible, and can be easily extended to arbitrary geometric entities and semantic or functional properties. Experimental results on a large-scale dataset of cabinet models demonstrate the effectiveness of our method. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†CAD2Programï¼Œè¿™æ˜¯ä¸€ç§ä»2D CADå›¾çº¸é‡å»º3Då‚æ•°æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å—åˆ°æœ€è¿‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æˆåŠŸçš„å¯å‘ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºç‰¹å®šçš„ä»»åŠ¡æ•°æ®è¡¨ç¤ºå’Œ&#x2F;æˆ–ç®—æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è¾“å…¥æ–¹é¢ï¼Œæˆ‘ä»¬ç®€å•åœ°å°†2D CADå›¾çº¸è§†ä¸ºæ …æ ¼å›¾åƒï¼Œè€Œä¸è€ƒè™‘å…¶åŸå§‹æ ¼å¼ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†çš„ViTæ¨¡å‹å¯¹å›¾åƒè¿›è¡Œç¼–ç ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ç§ç¼–ç æ–¹æ¡ˆåœ¨å®ç°ä¸é’ˆå¯¹çŸ¢é‡å›¾å½¢è¾“å…¥çš„ç°æœ‰æ–¹æ³•ç›¸å½“çš„æ€§èƒ½çš„åŒæ—¶ï¼Œå¯¹2Då›¾çº¸çš„é™åˆ¶å¤§å¤§å‡å°‘ã€‚åœ¨è¾“å‡ºæ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è‡ªåŠ¨å›å½’é¢„æµ‹æè¿°æ–‡æœ¬å½¢å¼çš„3Då‚æ•°æ¨¡å‹çš„ä¸€èˆ¬è¯­è¨€ã€‚ä¸å…¶ä»–ä½¿ç”¨å›ºå®šå¤§å°æ’æ§½çš„ç‰¹å®šé¢†åŸŸåºåˆ—è¡¨ç¤ºçš„CADåºåˆ—å»ºæ¨¡æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åŸºäºæ–‡æœ¬çš„è¡¨ç¤ºæ›´åŠ çµæ´»ï¼Œå¯ä»¥è½»æ¾åœ°æ‰©å±•åˆ°ä»»æ„å‡ ä½•å®ä½“å’Œè¯­ä¹‰æˆ–åŠŸèƒ½å±æ€§ã€‚åœ¨å¤§å‹æ©±æŸœæ¨¡å‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11892v2">PDF</a> To Appear in AAAI 2025. The project page is at   <a target="_blank" rel="noopener" href="https://manycore-research.github.io/CAD2Program">https://manycore-research.github.io/CAD2Program</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†CAD2Programæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä»2D CADç»˜å›¾é‡å»º3Då‚æ•°æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å—åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿‘æœŸæˆåŠŸçš„å¯å‘ï¼Œä¸åŒäºä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ•°æ®è¡¨ç¤ºå’Œ&#x2F;æˆ–ç®—æ³•ã€‚è¾“å…¥æ–¹é¢ï¼Œæˆ‘ä»¬å°†2D CADç»˜å›¾è§†ä¸ºé€šç”¨å›¾åƒï¼Œä½¿ç”¨æ ‡å‡†ViTæ¨¡å‹è¿›è¡Œç¼–ç ï¼Œå±•ç¤ºå‡ºäº†ä¸æ“ä½œçŸ¢é‡å›¾å½¢è¾“å…¥ç°æœ‰æ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ï¼ŒåŒæ—¶å¯¹2Dç»˜å›¾å®è´¨ä¸Šæ²¡æœ‰é™åˆ¶ã€‚è¾“å‡ºæ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è‡ªå›å½’åœ°é¢„æµ‹æè¿°æ–‡æœ¬å½¢å¼çš„3Då‚æ•°æ¨¡å‹çš„ä¸€èˆ¬è¯­è¨€ã€‚ç›¸æ¯”äºä½¿ç”¨å›ºå®šå¤§å°æ’æ§½çš„ç‰¹å®šé¢†åŸŸåºåˆ—è¡¨ç¤ºçš„å…¶ä»–åºåˆ—å»ºæ¨¡æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–‡æœ¬è¡¨ç¤ºæ›´åŠ çµæ´»ï¼Œå¯è½»æ¾æ‰©å±•åˆ°ä»»æ„å‡ ä½•å®ä½“å’Œè¯­ä¹‰æˆ–åŠŸèƒ½å±æ€§ã€‚åœ¨å¤§å‹æ©±æŸœæ¨¡å‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD2Programæ˜¯ä¸€ç§ä»2D CADç»˜å›¾é‡å»º3Då‚æ•°æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å—åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æˆåŠŸçš„å¯å‘ã€‚</li>
<li>è¾“å…¥æ–¹é¢ï¼Œä½¿ç”¨æ ‡å‡†ViTæ¨¡å‹å¯¹2D CADç»˜å›¾è¿›è¡Œç¼–ç ï¼Œæ— éœ€ç‰¹å®šæ ¼å¼ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸æ“ä½œçŸ¢é‡å›¾å½¢è¾“å…¥çš„ç°æœ‰æ–¹æ³•ç«äº‰åŠ›ç›¸å½“ï¼Œå¯¹2Dç»˜å›¾é™åˆ¶å°‘ã€‚</li>
<li>è¾“å‡ºæ–¹é¢ï¼Œæ–¹æ³•è‡ªå›å½’åœ°é¢„æµ‹æè¿°3Då‚æ•°æ¨¡å‹çš„æ–‡æœ¬ã€‚</li>
<li>ç›¸æ¯”å…¶ä»–åºåˆ—å»ºæ¨¡æ–¹æ³•ï¼Œæ–‡æœ¬è¡¨ç¤ºæ›´åŠ çµæ´»ï¼Œæ˜“äºæ‰©å±•åˆ°ä¸åŒå‡ ä½•å®ä½“å’Œå±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-699808d85c570298cd34317940a6a711.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ad0ee65a0f5e345198656bf70a4bf6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0e2952627374f1b1221a5d4afa429f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14f3dc0055e7abf63827e934fdf0f150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83c9578f6b88c8a88bc39d0b4f104175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b46a0f8d039c10dab7a6b6ba501dc5b5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Text-and-Image-Are-Mutually-Beneficial-Enhancing-Training-Free-Few-Shot-Classification-with-CLIP"><a href="#Text-and-Image-Are-Mutually-Beneficial-Enhancing-Training-Free-Few-Shot-Classification-with-CLIP" class="headerlink" title="Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot   Classification with CLIP"></a>Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot   Classification with CLIP</h2><p><strong>Authors:Yayuan Li, Jintao Guo, Lei Qi, Wenbin Li, Yinghuan Shi</strong></p>
<p>Contrastive Language-Image Pretraining (CLIP) has been widely used in vision tasks. Notably, CLIP has demonstrated promising performance in few-shot learning (FSL). However, existing CLIP-based methods in training-free FSL (i.e., without the requirement of additional training) mainly learn different modalities independently, leading to two essential issues: 1) severe anomalous match in image modality; 2) varying quality of generated text prompts. To address these issues, we build a mutual guidance mechanism, that introduces an Image-Guided-Text (IGT) component to rectify varying quality of text prompts through image representations, and a Text-Guided-Image (TGI) component to mitigate the anomalous match of image modality through text representations. By integrating IGT and TGI, we adopt a perspective of Text-Image Mutual guidance Optimization, proposing TIMO. Extensive experiments show that TIMO significantly outperforms the state-of-the-art (SOTA) training-free method. Additionally, by exploring the extent of mutual guidance, we propose an enhanced variant, TIMO-S, which even surpasses the best training-required methods by 0.33% with approximately 100 times less time cost. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/lyymuwu/TIMO">https://github.com/lyymuwu/TIMO</a>. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒCLIPåœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºCLIPçš„æ— è®­ç»ƒFSLæ–¹æ³•ï¼ˆå³æ— éœ€é¢å¤–è®­ç»ƒï¼‰ä¸»è¦ç‹¬ç«‹å­¦ä¹ ä¸åŒçš„æ¨¡å¼ï¼Œå¯¼è‡´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š1ï¼‰å›¾åƒæ¨¡å¼ä¸­çš„ä¸¥é‡å¼‚å¸¸åŒ¹é…ï¼›2ï¼‰ç”Ÿæˆçš„æ–‡æœ¬æç¤ºçš„è´¨é‡ä¸ä¸€ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ç§äº’å¼•å¯¼æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¼•å…¥äº†ä¸€ä¸ªå›¾åƒå¼•å¯¼æ–‡æœ¬ï¼ˆIGTï¼‰ç»„ä»¶ï¼Œé€šè¿‡å›¾åƒè¡¨ç¤ºæ¥çº æ­£æ–‡æœ¬æç¤ºçš„è´¨é‡ä¸ä¸€é—®é¢˜ï¼Œä»¥åŠä¸€ä¸ªæ–‡æœ¬å¼•å¯¼å›¾åƒï¼ˆTGIï¼‰ç»„ä»¶ï¼Œé€šè¿‡æ–‡æœ¬è¡¨ç¤ºæ¥ç¼“è§£å›¾åƒæ¨¡å¼çš„å¼‚å¸¸åŒ¹é…é—®é¢˜ã€‚é€šè¿‡ç»“åˆIGTå’ŒTGIï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ–‡æœ¬å›¾åƒäº’å¼•å¯¼ä¼˜åŒ–çš„è§‚ç‚¹ï¼Œæå‡ºäº†TIMOã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTIMOæ˜¾è‘—ä¼˜äºæœ€æ–°çš„æ— è®­ç»ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ¢ç´¢äº’å¼•å¯¼çš„ç¨‹åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¢å¼ºå‹å˜ä½“TIMO-Sï¼Œå®ƒç”šè‡³åœ¨æ—¶é—´æˆæœ¬å¤§çº¦å‡å°‘100å€çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†æœ€ä½³éœ€è®­ç»ƒæ–¹æ³•è¾¾0.33%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lyymuwu/TIMO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lyymuwu/TIMOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11375v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong><br>åŸºäºCLIPæ¨¡å‹çš„æ–‡æœ¬å›¾åƒé¢„è®­ç»ƒæŠ€æœ¯åœ¨è§†è§‰ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨æ¨¡æ€ç‹¬ç«‹å­¦ä¹ å¯¼è‡´çš„å›¾åƒæ¨¡æ€å¼‚å¸¸åŒ¹é…å’Œæ–‡æœ¬æç¤ºè´¨é‡ä¸ä¸€çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºæ–‡æœ¬å›¾åƒç›¸äº’å¼•å¯¼ä¼˜åŒ–ï¼ˆTIMOï¼‰æ–¹æ³•ï¼Œé€šè¿‡å›¾åƒå¼•å¯¼æ–‡æœ¬ï¼ˆIGTï¼‰ç»„ä»¶çº æ­£æ–‡æœ¬æç¤ºè´¨é‡ï¼Œä»¥åŠæ–‡æœ¬å¼•å¯¼å›¾åƒï¼ˆTGIï¼‰ç»„ä»¶å‡è½»å›¾åƒæ¨¡æ€å¼‚å¸¸åŒ¹é…é—®é¢˜ã€‚å®éªŒæ˜¾ç¤ºï¼ŒTIMOæ˜¾è‘—ä¼˜äºç°æœ‰æ— è®­ç»ƒéœ€æ±‚çš„æ–¹æ³•ï¼Œå…¶å¢å¼ºç‰ˆTIMO-Sç”šè‡³è¶…è¶Šæœ€ä½³è®­ç»ƒéœ€æ±‚æ–¹æ³•ï¼Œä¸”æ—¶é—´æˆæœ¬ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸Šã€‚</li>
<li>ç°æœ‰CLIPæ–¹æ³•å­˜åœ¨æ¨¡æ€ç‹¬ç«‹å­¦ä¹ å¯¼è‡´çš„å›¾åƒæ¨¡æ€å¼‚å¸¸åŒ¹é…å’Œæ–‡æœ¬æç¤ºè´¨é‡é—®é¢˜ã€‚</li>
<li>æå‡ºTIMOæ–¹æ³•ï¼Œé€šè¿‡æ–‡æœ¬å›¾åƒç›¸äº’å¼•å¯¼æœºåˆ¶è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>IGTç»„ä»¶é€šè¿‡å›¾åƒè¡¨ç¤ºçº æ­£æ–‡æœ¬æç¤ºè´¨é‡ã€‚</li>
<li>TGIç»„ä»¶é€šè¿‡æ–‡æœ¬è¡¨ç¤ºå‡è½»å›¾åƒæ¨¡æ€å¼‚å¸¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>TIMOæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ— è®­ç»ƒéœ€æ±‚æ–¹æ³•ã€‚</li>
<li>TIMOçš„å¢å¼ºç‰ˆTIMO-Såœ¨æ—¶é—´æˆæœ¬ä½çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¶…è¶Šæœ€ä½³è®­ç»ƒéœ€æ±‚æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7404aaa6248977ee707d7e09d45d9dcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d3b4683ab5343196b2b3b6315d020a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a18a74487ca58be48aefc640927e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24d004e40c9a3694a95bbf969bf36ba6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9a096878db179f4a39019eb30b179ae.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation"><a href="#MoRe-Class-Patch-Attention-Needs-Regularization-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation"></a>MoRe: Class Patch Attention Needs Regularization for Weakly Supervised   Semantic Segmentation</h2><p><strong>Authors:Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels typically uses Class Activation Maps (CAM) to achieve dense predictions. Recently, Vision Transformer (ViT) has provided an alternative to generate localization maps from class-patch attention. However, due to insufficient constraints on modeling such attention, we observe that the Localization Attention Maps (LAM) often struggle with the artifact issue, i.e., patch regions with minimal semantic relevance are falsely activated by class tokens. In this work, we propose MoRe to address this issue and further explore the potential of LAM. Our findings suggest that imposing additional regularization on class-patch attention is necessary. To this end, we first view the attention as a novel directed graph and propose the Graph Category Representation module to implicitly regularize the interaction among class-patch entities. It ensures that class tokens dynamically condense the related patch information and suppress unrelated artifacts at a graph level. Second, motivated by the observation that CAM from classification weights maintains smooth localization of objects, we devise the Localization-informed Regularization module to explicitly regularize the class-patch attention. It directly mines the token relations from CAM and further supervises the consistency between class and patch tokens in a learnable manner. Extensive experiments are conducted on PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact issue and achieves state-of-the-art performance, surpassing recent single-stage and even multi-stage methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe">https://github.com/zwyang6/MoRe</a>. </p>
<blockquote>
<p>åŸºäºå›¾åƒçº§åˆ«çš„æ ‡ç­¾è¿›è¡Œå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰é€šå¸¸ä½¿ç”¨ç±»åˆ«æ¿€æ´»å›¾ï¼ˆCAMï¼‰æ¥å®ç°å¯†é›†é¢„æµ‹ã€‚æœ€è¿‘ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æä¾›äº†ä¸€ç§ç”Ÿæˆå®šä½æ³¨æ„åŠ›å›¾ï¼ˆLAMï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ç±»åˆ«è¡¥ä¸æ³¨æ„åŠ›æ¥ç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºå¯¹è¿™ç±»æ³¨æ„åŠ›çš„å»ºæ¨¡çº¦æŸä¸è¶³ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å®šä½æ³¨æ„åŠ›å›¾ï¼ˆLAMï¼‰å¸¸å¸¸å­˜åœ¨ä¼ªå½±é—®é¢˜ï¼Œå³ä¸è¯­ä¹‰ç›¸å…³æ€§å¾ˆå°çš„è¡¥ä¸åŒºåŸŸè¢«ç±»ä»¤ç‰Œé”™è¯¯æ¿€æ´»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºMoReæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶LAMçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹ç±»åˆ«è¡¥ä¸æ³¨æ„åŠ›æ–½åŠ é¢å¤–çš„æ­£åˆ™åŒ–æ˜¯å¿…è¦çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ³¨æ„åŠ›è§†ä¸ºä¸€ç§æ–°å‹çš„æœ‰å‘å›¾ï¼Œå¹¶æå‡ºå›¾ç±»åˆ«è¡¨ç¤ºæ¨¡å—æ¥éšå«åœ°æ­£åˆ™åŒ–ç±»åˆ«è¡¥ä¸å®ä½“ä¹‹é—´çš„äº¤äº’ã€‚å®ƒç¡®ä¿ç±»ä»¤ç‰Œèƒ½å¤ŸåŠ¨æ€åœ°å‡èšç›¸å…³çš„è¡¥ä¸ä¿¡æ¯ï¼Œå¹¶åœ¨å›¾çº§åˆ«æŠ‘åˆ¶ä¸ç›¸å…³çš„ä¼ªå½±ã€‚å…¶æ¬¡ï¼Œå—åˆ†ç±»æƒé‡ä¸­çš„CAMèƒ½å¤Ÿä¿æŒå¯¹è±¡å®šä½å¹³æ»‘çš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå®šä½æ„ŸçŸ¥æ­£åˆ™åŒ–æ¨¡å—æ¥æ˜¾å¼åœ°æ­£åˆ™åŒ–ç±»åˆ«è¡¥ä¸æ³¨æ„åŠ›ã€‚å®ƒç›´æ¥ä»CAMæŒ–æ˜ä»¤ç‰Œå…³ç³»ï¼Œå¹¶ä»¥å¯å­¦ä¹ çš„æ–¹å¼ç›‘ç£ç±»åˆ«å’Œè¡¥ä¸ä»¤ç‰Œä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨PASCAL VOCå’ŒMS COCOä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†MoReæœ‰æ•ˆåœ°è§£å†³äº†ä¼ªå½±é—®é¢˜ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æœ€è¿‘çš„å•é˜¶æ®µç”šè‡³å¤šé˜¶æ®µæ–¹æ³•ã€‚ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/zwyang6/MoRe%E3%80%82">https://github.com/zwyang6/MoReã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11076v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾è¿›è¡Œå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ—¶ï¼ŒåŸºäºVision Transformerï¼ˆViTï¼‰çš„å®šä½æ³¨æ„åŠ›å›¾ï¼ˆLAMï¼‰æ‰€é¢ä¸´çš„ä¼ªæ¿€æ´»é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜å¹¶æ¢ç´¢LAMçš„æ½œåŠ›ï¼Œæœ¬æ–‡æå‡ºäº†MoReæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢å¤–çš„æ­£åˆ™åŒ–å¯¹ç±»-è¡¥ä¸æ³¨æ„åŠ›è¿›è¡Œçº¦æŸï¼Œé€šè¿‡å¼•å…¥å›¾ç±»åˆ«è¡¨ç¤ºæ¨¡å—å’Œå®šä½ä¿¡æ¯æ­£åˆ™åŒ–æ¨¡å—ï¼Œå®ç°äº†å¯¹ç±»-è¡¥ä¸å®ä½“é—´äº¤äº’çš„éšå¼å’Œæ˜¾å¼æ­£åˆ™åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoReæ–¹æ³•èƒ½æœ‰æ•ˆè§£å†³ä¼ªæ¿€æ´»é—®é¢˜ï¼Œå¹¶åœ¨PASCAL VOCå’ŒMS COCOæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformerï¼ˆViTï¼‰å¯ä»¥ç”¨äºç”Ÿæˆå®šä½æ³¨æ„åŠ›å›¾ï¼ˆLAMï¼‰ã€‚</li>
<li>LAMé¢ä¸´ä¼ªæ¿€æ´»é—®é¢˜ï¼Œå³ä¸ç›¸å…³çš„è¡¥ä¸åŒºåŸŸä¼šè¢«ç±»æ ‡è®°é”™è¯¯åœ°æ¿€æ´»ã€‚</li>
<li>MoReæ–¹æ³•é€šè¿‡å¼•å…¥å›¾ç±»åˆ«è¡¨ç¤ºæ¨¡å—å’Œå®šä½ä¿¡æ¯æ­£åˆ™åŒ–æ¨¡å—æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å›¾ç±»åˆ«è¡¨ç¤ºæ¨¡å—å°†æ³¨æ„åŠ›è§†ä¸ºæ–°å‹æœ‰å‘å›¾ï¼Œä»¥éšå¼æ–¹å¼æ­£åˆ™åŒ–ç±»-è¡¥ä¸å®ä½“é—´çš„äº¤äº’ã€‚</li>
<li>å®šä½ä¿¡æ¯æ­£åˆ™åŒ–æ¨¡å—åˆ©ç”¨åˆ†ç±»æƒé‡çš„CAMä¿¡æ¯ï¼Œæ˜¾å¼åœ°æ­£åˆ™åŒ–ç±»-è¡¥ä¸æ³¨æ„åŠ›ï¼Œå¹¶ä¿ƒè¿›ç±»æ ‡è®°ä¸è¡¥ä¸æ ‡è®°ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚</li>
<li>MoReæ–¹æ³•åœ¨PASCAL VOCå’ŒMS COCOæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6510c136895fa0abdc11ee0a807819f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-948b191c21d0a7790db8f2c48fcfc40e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3e4750dfaf5c1d3fd23143f2d50fec0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-699cfbb8aa515db4cbb28229318109c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RapidNet-Multi-Level-Dilated-Convolution-Based-Mobile-Backbone"><a href="#RapidNet-Multi-Level-Dilated-Convolution-Based-Mobile-Backbone" class="headerlink" title="RapidNet: Multi-Level Dilated Convolution Based Mobile Backbone"></a>RapidNet: Multi-Level Dilated Convolution Based Mobile Backbone</h2><p><strong>Authors:Mustafa Munir, Md Mostafijur Rahman, Radu Marculescu</strong></p>
<p>Vision transformers (ViTs) have dominated computer vision in recent years. However, ViTs are computationally expensive and not well suited for mobile devices; this led to the prevalence of convolutional neural network (CNN) and ViT-based hybrid models for mobile vision applications. Recently, Vision GNN (ViG) and CNN hybrid models have also been proposed for mobile vision tasks. However, all of these methods remain slower compared to pure CNN-based models. In this work, we propose Multi-Level Dilated Convolutions to devise a purely CNN-based mobile backbone. Using Multi-Level Dilated Convolutions allows for a larger theoretical receptive field than standard convolutions. Different levels of dilation also allow for interactions between the short-range and long-range features in an image. Experiments show that our proposed model outperforms state-of-the-art (SOTA) mobile CNN, ViT, ViG, and hybrid architectures in terms of accuracy and&#x2F;or speed on image classification, object detection, instance segmentation, and semantic segmentation. Our fastest model, RapidNet-Ti, achieves 76.3% top-1 accuracy on ImageNet-1K with 0.9 ms inference latency on an iPhone 13 mini NPU, which is faster and more accurate than MobileNetV2x1.4 (74.7% top-1 with 1.0 ms latency). Our work shows that pure CNN architectures can beat SOTA hybrid and ViT models in terms of accuracy and speed when designed properly. </p>
<blockquote>
<p>è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰åœ¨è¿‘å¹´æ¥ä¸»å®°äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚ç„¶è€Œï¼ŒViTsè®¡ç®—é‡å¤§ï¼Œä¸é€‚åˆç”¨äºç§»åŠ¨è®¾å¤‡ï¼Œè¿™å¯¼è‡´äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŸºäºViTçš„æ··åˆæ¨¡å‹åœ¨æ‰‹æœºè§†è§‰åº”ç”¨çš„ç››è¡Œã€‚æœ€è¿‘ï¼Œé’ˆå¯¹ç§»åŠ¨è§†è§‰ä»»åŠ¡ï¼Œè¿˜æå‡ºäº†Vision GNNï¼ˆViGï¼‰å’ŒCNNæ··åˆæ¨¡å‹ã€‚ç„¶è€Œï¼Œæ‰€æœ‰è¿™äº›æ–¹æ³•ç›¸æ¯”äºçº¯CNNæ¨¡å‹ä»ç„¶è¾ƒæ…¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºå¤šçº§è†¨èƒ€å·ç§¯æ¥å¼€å‘çº¯CNNçš„ç§»åŠ¨ç«¯ä¸»å¹²ç½‘ã€‚ä½¿ç”¨å¤šçº§è†¨èƒ€å·ç§¯å¯ä»¥äº§ç”Ÿæ¯”æ ‡å‡†å·ç§¯æ›´å¤§çš„ç†è®ºæ„Ÿå—é‡ã€‚ä¸åŒçº§åˆ«çš„è†¨èƒ€ä¹Ÿå…è®¸å›¾åƒä¸­çŸ­ç¨‹å’Œè¿œç¨‹ç‰¹å¾ä¹‹é—´çš„äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢ï¼Œå‡†ç¡®åº¦å’Œ&#x2F;æˆ–é€Ÿåº¦è¶…è¿‡äº†æœ€å…ˆè¿›çš„ç§»åŠ¨CNNã€ViTã€ViGä»¥åŠæ··åˆæ¶æ„ã€‚æˆ‘ä»¬çš„æœ€å¿«æ¨¡å‹RapidNet-Tiåœ¨iPhone 13 mini NPUä¸Šå®ç°äº†ImageNet-1Kçš„76.3% top-1å‡†ç¡®ç‡ï¼Œå¹¶ä¸”æ¨ç†å»¶è¿Ÿæ—¶é—´ä¸º0.9æ¯«ç§’ï¼Œæ¯”MobileNetV2x1.4æ›´å¿«ï¼ˆMobileNetV2x1.4çš„top-1å‡†ç¡®ç‡ä¸º74.7%ï¼Œå»¶è¿Ÿæ—¶é—´ä¸º1.0æ¯«ç§’ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œå½“è®¾è®¡å¾—å½“çš„æ—¶å€™ï¼Œçº¯CNNæ¶æ„å¯ä»¥åœ¨å‡†ç¡®åº¦å’Œé€Ÿåº¦ä¸Šå‡»è´¥æœ€å…ˆè¿›çš„æ··åˆå’ŒViTæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10995v1">PDF</a> Accepted in 2025 IEEE&#x2F;CVF Winter Conference on Applications of   Computer Vision (WACV 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œå°½ç®¡Vision Transformersï¼ˆViTsï¼‰è¿‘å¹´æ¥å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†ç”±äºè®¡ç®—æˆæœ¬é«˜ä¸”ä¸é€‚åˆç§»åŠ¨è®¾å¤‡ï¼Œä¿ƒä½¿äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒViTæ··åˆæ¨¡å‹åœ¨ç§»åŠ¨è§†è§‰åº”ç”¨ä¸­çš„æ™®åŠã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºå¤šå±‚æ¬¡è†¨èƒ€å·ç§¯çš„çº¯CNNç§»åŠ¨éª¨å¹²ç½‘æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ç†è®ºä¸Šæ¥å—æ›´å¤§çš„æ„Ÿå—é‡å¹¶å®ç°é•¿çŸ­è·ç¦»ç‰¹å¾çš„äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ˆSOTAï¼‰çš„ç§»åŠ¨CNNã€ViTã€ViGåŠæ··åˆæ¶æ„ã€‚å°¤å…¶æ˜¯RapidNet-Tiæ¨¡å‹ï¼Œåœ¨iPhone 13 mini NPUä¸Šçš„æ¨ç†å»¶è¿Ÿæ—¶é—´ä»…ä¸º0.9æ¯«ç§’ï¼Œå®ç°äº†ImageNet-1Kä¸Šçš„76.3% top-1å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºMobileNetV2x1.4æ›´ä¸ºå¿«é€Ÿå’Œå‡†ç¡®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision transformers (ViTs) è™½ç„¶ä¸»å¯¼è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œä½†ç”±äºè®¡ç®—æˆæœ¬é«˜åŠç§»åŠ¨è®¾å¤‡å…¼å®¹æ€§å·®ï¼Œä»éœ€è¦æ”¹è¿›ã€‚</li>
<li>æ··åˆæ¨¡å‹å’Œçº¯CNNæ¨¡å‹åœ¨ç§»åŠ¨è§†è§‰ä»»åŠ¡ä¸­å—åˆ°å…³æ³¨ï¼Œå…¶ä¸­çº¯CNNæ¨¡å‹å…·æœ‰è®¡ç®—æ•ˆç‡é«˜çš„ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºçš„å¤šå±‚æ¬¡è†¨èƒ€å·ç§¯æ¨¡å‹èƒ½å¤Ÿæ‰©å¤§ç†è®ºä¸Šçš„æ„Ÿå—é‡å¹¶ä¿ƒè¿›é•¿çŸ­è·ç¦»ç‰¹å¾çš„äº¤äº’ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šé¡¹è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼ˆSOTAï¼‰çš„CNNã€ViTå’Œæ··åˆæ¨¡å‹ã€‚</li>
<li>RapidNet-Tiæ¨¡å‹åœ¨ImageNet-1Kä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†76.3%ï¼Œå¹¶ä¸”åœ¨iPhone 13 mini NPUä¸Šçš„æ¨ç†å»¶è¿Ÿæä½ï¼Œä¸º0.9æ¯«ç§’ã€‚</li>
<li>è¯¥ç ”ç©¶è¯æ˜äº†è®¾è®¡å¾—å½“çš„çº¯CNNæ¨¡å‹å¯ä»¥åœ¨å‡†ç¡®ç‡å’Œé€Ÿåº¦ä¸Šè¶…è¶Šæ··åˆæ¨¡å‹å’ŒViTæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58cfe60764003e0b7bf1bc3556efffff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9774c3bdbd9afb82ec497d3b24010b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0acc1b46c7b00d2aed999096691faab4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4396e274ef1155f1e23e9763ddbde29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3825fba4d3c2e6d4e6806a5c0b2eca16.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Learning-Semantic-Aware-Representation-in-Visual-Language-Models-for-Multi-Label-Recognition-with-Partial-Labels"><a href="#Learning-Semantic-Aware-Representation-in-Visual-Language-Models-for-Multi-Label-Recognition-with-Partial-Labels" class="headerlink" title="Learning Semantic-Aware Representation in Visual-Language Models for   Multi-Label Recognition with Partial Labels"></a>Learning Semantic-Aware Representation in Visual-Language Models for   Multi-Label Recognition with Partial Labels</h2><p><strong>Authors:Haoxian Ruan, Zhihua Xu, Zhijing Yang, Yongyi Lu, Jinghui Qin, Tianshui Chen</strong></p>
<p>Multi-label recognition with partial labels (MLR-PL), in which only some labels are known while others are unknown for each image, is a practical task in computer vision, since collecting large-scale and complete multi-label datasets is difficult in real application scenarios. Recently, vision language models (e.g. CLIP) have demonstrated impressive transferability to downstream tasks in data limited or label limited settings. However, current CLIP-based methods suffer from semantic confusion in MLR task due to the lack of fine-grained information in the single global visual and textual representation for all categories. In this work, we address this problem by introducing a semantic decoupling module and a category-specific prompt optimization method in CLIP-based framework. Specifically, the semantic decoupling module following the visual encoder learns category-specific feature maps by utilizing the semantic-guided spatial attention mechanism. Moreover, the category-specific prompt optimization method is introduced to learn text representations aligned with category semantics. Therefore, the prediction of each category is independent, which alleviate the semantic confusion problem. Extensive experiments on Microsoft COCO 2014 and Pascal VOC 2007 datasets demonstrate that the proposed framework significantly outperforms current state-of-art methods with a simpler model structure. Additionally, visual analysis shows that our method effectively separates information from different categories and achieves better performance compared to CLIP-based baseline method. </p>
<blockquote>
<p>å¤šæ ‡ç­¾è¯†åˆ«ï¼ˆMLRï¼‰æ˜¯ä¸€é¡¹å®é™…åº”ç”¨ä¸­çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨éƒ¨åˆ†æ ‡ç­¾æƒ…å†µä¸‹ï¼ˆMLR-PLï¼‰ï¼Œå¯¹äºæ¯å¼ å›¾åƒåªæœ‰éƒ¨åˆ†æ ‡ç­¾æ˜¯å·²çŸ¥çš„ï¼Œè€Œå…¶ä»–æ ‡ç­¾æ˜¯æœªçŸ¥çš„ã€‚åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­ï¼Œæ”¶é›†å¤§è§„æ¨¡ä¸”å®Œæ•´çš„å¤šæ ‡ç­¾æ•°æ®é›†æ˜¯éå¸¸å›°éš¾çš„ã€‚æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰åœ¨æ•°æ®æœ‰é™æˆ–æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹å·²ç»æ˜¾ç¤ºå‡ºå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æƒŠäººå¯è¿ç§»æ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„CLIPæ–¹æ³•åœ¨å¤šæ ‡ç­¾è¯†åˆ«ä»»åŠ¡ä¸­ä¼šé‡åˆ°è¯­ä¹‰æ··æ·†çš„é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹æ‰€æœ‰ç±»åˆ«çš„ç²¾ç»†ç²’åº¦ä¿¡æ¯åœ¨å•ä¸€å…¨å±€è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥è¯­ä¹‰è§£è€¦æ¨¡å—å’Œç±»åˆ«ç‰¹å®šæç¤ºä¼˜åŒ–æ–¹æ³•åœ¨CLIPæ¡†æ¶å†…è§£å†³æ­¤é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œç´§éšè§†è§‰ç¼–ç å™¨çš„è¯­ä¹‰è§£è€¦æ¨¡å—é€šè¿‡åˆ©ç”¨è¯­ä¹‰å¼•å¯¼çš„ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ ç‰¹å®šç±»åˆ«çš„ç‰¹å¾æ˜ å°„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç±»åˆ«ç‰¹å®šçš„æç¤ºä¼˜åŒ–æ–¹æ³•æ¥å­¦ä¹ ä¸ç±»åˆ«è¯­ä¹‰å¯¹é½çš„æ–‡æœ¬è¡¨ç¤ºã€‚å› æ­¤ï¼Œæ¯ä¸ªç±»åˆ«çš„é¢„æµ‹éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œè¿™å‡è½»äº†è¯­ä¹‰æ··æ·†çš„é—®é¢˜ã€‚åœ¨Microsoft COCO 2014å’ŒPascal VOC 2007æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶ä»¥æ›´ç®€å•çš„æ¨¡å‹ç»“æ„æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè§†è§‰åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°åˆ†ç¦»äº†ä¸åŒç±»åˆ«çš„ä¿¡æ¯ï¼Œå¹¶ä¸”ç›¸è¾ƒäºCLIPåŸºå‡†æ–¹æ³•å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10843v1">PDF</a> ACM Transactions on Multimedia Computing Communications and   Applications</p>
<p><strong>Summary</strong><br>å¤šæ ‡ç­¾è¯†åˆ«ä»»åŠ¡åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®æ”¶é›†å’Œæ ‡ç­¾æ”¶é›†çš„å›°éš¾ã€‚æœ¬ç ”ç©¶é’ˆå¯¹CLIPæ¨¡å‹åœ¨MLRä»»åŠ¡ä¸­çš„è¯­ä¹‰æ··æ·†é—®é¢˜ï¼Œæå‡ºäº†è¯­ä¹‰è§£è€¦æ¨¡å—å’Œç±»åˆ«ç‰¹å®šæç¤ºä¼˜åŒ–æ–¹æ³•ã€‚é€šè¿‡è¯­ä¹‰è§£è€¦æ¨¡å—å’Œç±»åˆ«ç‰¹å®šæç¤ºå­¦ä¹ ï¼Œé¢„æµ‹æ¯ä¸ªç±»åˆ«çš„ç‹¬ç«‹æ€§å¾—ä»¥å¢å¼ºï¼Œç¼“è§£äº†è¯­ä¹‰æ··æ·†é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Microsoft COCOå’ŒPascal VOCæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ¨¡å‹ç»“æ„æ›´ç®€å•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ ‡ç­¾è¯†åˆ«ä»»åŠ¡åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºéš¾ä»¥æ”¶é›†å¤§è§„æ¨¡ä¸”å®Œæ•´çš„å¤šæ ‡ç­¾æ•°æ®é›†ã€‚</li>
<li>CLIPæ¨¡å‹åœ¨MLRä»»åŠ¡ä¸­å­˜åœ¨è¯­ä¹‰æ··æ·†é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†è¯­ä¹‰è§£è€¦æ¨¡å—æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œè¯¥æ¨¡å—åˆ©ç”¨è¯­ä¹‰å¼•å¯¼çš„ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ ç‰¹å®šç±»åˆ«çš„ç‰¹å¾å›¾ã€‚</li>
<li>é€šè¿‡å¼•å…¥ç±»åˆ«ç‰¹å®šæç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œä½¿æ–‡æœ¬è¡¨ç¤ºä¸ç±»åˆ«è¯­ä¹‰å¯¹é½ã€‚</li>
<li>æ–¹æ³•é€šè¿‡é¢„æµ‹æ¯ä¸ªç±»åˆ«çš„ç‹¬ç«‹æ€§æ¥ç¼“è§£è¯­ä¹‰æ··æ·†é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Microsoft COCOå’ŒPascal VOCæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41659fc4797ac13184c7fd65304540df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5781bfd9151219a4701b413edba38301.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ac745c73131793e6e1d8c60f51f8814.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba131a1cb794d37d5fa9d1a72aefb5a8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Boosting-ViT-based-MRI-Reconstruction-from-the-Perspectives-of-Frequency-Modulation-Spatial-Purification-and-Scale-Diversification"><a href="#Boosting-ViT-based-MRI-Reconstruction-from-the-Perspectives-of-Frequency-Modulation-Spatial-Purification-and-Scale-Diversification" class="headerlink" title="Boosting ViT-based MRI Reconstruction from the Perspectives of Frequency   Modulation, Spatial Purification, and Scale Diversification"></a>Boosting ViT-based MRI Reconstruction from the Perspectives of Frequency   Modulation, Spatial Purification, and Scale Diversification</h2><p><strong>Authors:Yucong Meng, Zhiwei Yang, Yonghong Shi, Zhijian Song</strong></p>
<p>The accelerated MRI reconstruction process presents a challenging ill-posed inverse problem due to the extensive under-sampling in k-space. Recently, Vision Transformers (ViTs) have become the mainstream for this task, demonstrating substantial performance improvements. However, there are still three significant issues remain unaddressed: (1) ViTs struggle to capture high-frequency components of images, limiting their ability to detect local textures and edge information, thereby impeding MRI restoration; (2) Previous methods calculate multi-head self-attention (MSA) among both related and unrelated tokens in content, introducing noise and significantly increasing computational burden; (3) The naive feed-forward network in ViTs cannot model the multi-scale information that is important for image restoration. In this paper, we propose FPS-Former, a powerful ViT-based framework, to address these issues from the perspectives of frequency modulation, spatial purification, and scale diversification. Specifically, for issue (1), we introduce a frequency modulation attention module to enhance the self-attention map by adaptively re-calibrating the frequency information in a Laplacian pyramid. For issue (2), we customize a spatial purification attention module to capture interactions among closely related tokens, thereby reducing redundant or irrelevant feature representations. For issue (3), we propose an efficient feed-forward network based on a hybrid-scale fusion strategy. Comprehensive experiments conducted on three public datasets show that our FPS-Former outperforms state-of-the-art methods while requiring lower computational costs. </p>
<blockquote>
<p>åŠ é€ŸMRIé‡å»ºè¿‡ç¨‹å‘ˆç°äº†ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„ä¸é€‚å®šé€†é—®é¢˜ï¼Œè¿™æ˜¯ç”±äºkç©ºé—´ä¸­çš„å¤§é‡æ¬ é‡‡æ ·é€ æˆçš„ã€‚æœ€è¿‘ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰å·²æˆä¸ºè¿™é¡¹ä»»åŠ¡çš„ä¸»æµï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸‰ä¸ªé‡å¤§é—®é¢˜éœ€è¦è§£å†³ï¼šï¼ˆ1ï¼‰ViTåœ¨æ•è·å›¾åƒçš„é«˜é¢‘æˆåˆ†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ£€æµ‹å±€éƒ¨çº¹ç†å’Œè¾¹ç¼˜ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä»è€Œé˜»ç¢äº†MRIçš„é‡å»ºï¼›ï¼ˆ2ï¼‰ä¹‹å‰çš„æ–¹æ³•åœ¨è®¡ç®—å†…å®¹å’Œæ— å…³æ ‡è®°ä¹‹é—´çš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMSAï¼‰æ—¶å¼•å…¥äº†å™ªå£°ï¼Œå¹¶æ˜¾è‘—å¢åŠ äº†è®¡ç®—è´Ÿæ‹…ï¼›ï¼ˆ3ï¼‰ViTä¸­çš„ç®€å•å‰é¦ˆç½‘ç»œæ— æ³•å¯¹å›¾åƒé‡å»ºè‡³å…³é‡è¦çš„å¤šå°ºåº¦ä¿¡æ¯è¿›è¡Œå»ºæ¨¡ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†FPS-Formerï¼Œä¸€ä¸ªåŸºäºViTçš„å¼ºå¤§æ¡†æ¶ï¼Œä»é¢‘ç‡è°ƒåˆ¶ã€ç©ºé—´å‡€åŒ–å’Œå°ºåº¦å¤šæ ·åŒ–çš„è§’åº¦æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºé—®é¢˜ï¼ˆ1ï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢‘ç‡è°ƒåˆ¶æ³¨æ„åŠ›æ¨¡å—ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°é‡æ–°æ ¡å‡†æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”ä¸­çš„é¢‘ç‡ä¿¡æ¯æ¥å¢å¼ºè‡ªæ³¨æ„åŠ›å›¾ã€‚å¯¹äºé—®é¢˜ï¼ˆ2ï¼‰ï¼Œæˆ‘ä»¬å®šåˆ¶äº†ä¸€ä¸ªç©ºé—´å‡€åŒ–æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æ•æ‰å¯†åˆ‡ç›¸å…³æ ‡è®°ä¹‹é—´çš„äº¤äº’ï¼Œä»è€Œå‡å°‘å†—ä½™æˆ–æ— å…³çš„ç‰¹å¾è¡¨ç¤ºã€‚å¯¹äºé—®é¢˜ï¼ˆ3ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ··åˆå°ºåº¦èåˆç­–ç•¥çš„æœ‰æ•ˆå‰é¦ˆç½‘ç»œã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„FPS-Formeråœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†æœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬æ›´ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10776v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡é’ˆå¯¹MRIé‡å»ºè¿‡ç¨‹ä¸­é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„FPS-Formeræ¡†æ¶ï¼Œé€šè¿‡é¢‘ç‡è°ƒåˆ¶ã€ç©ºé—´å‡€åŒ–å’Œå°ºåº¦å¤šæ ·åŒ–æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å¼•å…¥é¢‘ç‡è°ƒåˆ¶æ³¨æ„åŠ›æ¨¡å—å¢å¼ºè‡ªæ³¨æ„åŠ›å›¾ï¼Œå®šåˆ¶ç©ºé—´å‡€åŒ–æ³¨æ„åŠ›æ¨¡å—æ•æ‰ç›¸å…³æ ‡è®°é—´çš„äº¤äº’ï¼Œå¹¶æå‡ºåŸºäºæ··åˆå°ºåº¦èåˆç­–ç•¥çš„é«˜æ•ˆå‰é¦ˆç½‘ç»œã€‚å®éªŒè¯æ˜ï¼ŒFPS-Formeråœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬æ›´ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨MRIé‡å»ºä¸­é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šéš¾ä»¥æ•æ‰é«˜é¢‘æˆåˆ†ã€å¤šå¤´è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­çš„å™ªå£°å’Œå†—ä½™ä»¥åŠç¼ºä¹å¤šå°ºåº¦ä¿¡æ¯å»ºæ¨¡ã€‚</li>
<li>FPS-Formeræ¡†æ¶é€šè¿‡é¢‘ç‡è°ƒåˆ¶ã€ç©ºé—´å‡€åŒ–å’Œå°ºåº¦å¤šæ ·åŒ–æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>FPS-Formerå¼•å…¥é¢‘ç‡è°ƒåˆ¶æ³¨æ„åŠ›æ¨¡å—ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´Laplaciané‡‘å­—å¡”ä¸­çš„é¢‘ç‡ä¿¡æ¯æ¥å¢å¼ºè‡ªæ³¨æ„åŠ›å›¾ã€‚</li>
<li>ç©ºé—´å‡€åŒ–æ³¨æ„åŠ›æ¨¡å—ç”¨äºæ•æ‰ç´§å¯†ç›¸å…³æ ‡è®°é—´çš„äº¤äº’ï¼Œå‡å°‘å†—ä½™æˆ–æ— å…³çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæ··åˆå°ºåº¦èåˆç­–ç•¥çš„é«˜æ•ˆå‰é¦ˆç½‘ç»œã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFPS-Formeråœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬æ›´ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-37a9b4c26f3945969592400998dcff4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9493827051c50138b484e2b9959a36d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54888ab17e8de044cefa162933d4c986.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a4be539896a6010a16c30dfcd5ebe5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="One-Pixel-is-All-I-Need"><a href="#One-Pixel-is-All-I-Need" class="headerlink" title="One Pixel is All I Need"></a>One Pixel is All I Need</h2><p><strong>Authors:Deng Siqin, Zhou Xiaoyi</strong></p>
<p>Vision Transformers (ViTs) have achieved record-breaking performance in various visual tasks. However, concerns about their robustness against backdoor attacks have grown. Backdoor attacks involve associating a specific trigger with a target label, causing the model to predict the attacker-specified label when the trigger is present, while correctly identifying clean images.We found that ViTs exhibit higher attack success rates for quasi-triggers(patterns different from but similar to the original training triggers)compared to CNNs. Moreover, some backdoor features in clean samples can suppress the original trigger, making quasi-triggers more effective.To better understand and exploit these vulnerabilities, we developed a tool called the Perturbation Sensitivity Distribution Map (PSDM). PSDM computes and sums gradients over many inputs to show how sensitive the model is to small changes in the input. In ViTs, PSDM reveals a patch-like pattern where central pixels are more sensitive than edges. We use PSDM to guide the creation of quasi-triggers.Based on these findings, we designed â€œWorstVIT,â€ a simple yet effective data poisoning backdoor for ViT models. This attack requires an extremely low poisoning rate, trains for just one epoch, and modifies a single pixel to successfully attack all validation images. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œäººä»¬å¯¹å®ƒä»¬å¯¹æŠ—åé—¨æ”»å‡»çš„ç¨³å¥æ€§è¶Šæ¥è¶Šæ‹…å¿§ã€‚åé—¨æ”»å‡»æ¶‰åŠå°†ç‰¹å®šè§¦å‘å› ç´ ä¸ç›®æ ‡æ ‡ç­¾ç›¸å…³è”ï¼Œå½“å­˜åœ¨è§¦å‘å› ç´ æ—¶ï¼Œå¯¼è‡´æ¨¡å‹é¢„æµ‹æ”»å‡»è€…æŒ‡å®šçš„æ ‡ç­¾ï¼ŒåŒæ—¶æ­£ç¡®è¯†åˆ«å¹²å‡€å›¾åƒã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸CNNç›¸æ¯”ï¼ŒViTå¯¹ç±»ä¼¼ä½†ä¸åŒäºåŸå§‹è®­ç»ƒè§¦å‘çš„å‡†è§¦å‘ï¼ˆæ¨¡å¼ï¼‰è¡¨ç°å‡ºæ›´é«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œå¹²å‡€æ ·æœ¬ä¸­çš„ä¸€äº›åé—¨ç‰¹å¾å¯ä»¥æŠ‘åˆ¶åŸå§‹è§¦å‘ï¼Œä½¿å‡†è§¦å‘æ›´åŠ æœ‰æ•ˆã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨è¿™äº›æ¼æ´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºæ‰°åŠ¨æ•æ„Ÿæ€§åˆ†å¸ƒå›¾ï¼ˆPSDMï¼‰çš„å·¥å…·ã€‚PSDMè®¡ç®—å¹¶æ±‡æ€»å¤šä¸ªè¾“å…¥çš„æ¢¯åº¦ï¼Œä»¥æ˜¾ç¤ºæ¨¡å‹å¯¹è¾“å…¥å¾®å°å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚åœ¨ViTsä¸­ï¼ŒPSDMæ­ç¤ºäº†ä¸€ç§ç±»ä¼¼è¡¥ä¸çš„æ¨¡å¼ï¼Œå…¶ä¸­ä¸­å¤®åƒç´ æ¯”è¾¹ç¼˜æ›´æ•æ„Ÿã€‚æˆ‘ä»¬ä½¿ç”¨PSDMæ¥æŒ‡å¯¼å‡†è§¦å‘çš„åˆ›å»ºã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬è®¾è®¡äº†â€œWorstVITâ€ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ViTæ¨¡å‹çš„ç®€å•æœ‰æ•ˆçš„æ•°æ®ä¸­æ¯’åé—¨ã€‚è¿™ç§æ”»å‡»éœ€è¦æä½çš„ä¸­æ¯’ç‡ï¼Œåªéœ€ä¸€ä¸ªè®­ç»ƒå‘¨æœŸï¼Œä¿®æ”¹ä¸€ä¸ªåƒç´ å³å¯æˆåŠŸæ”»å‡»æ‰€æœ‰éªŒè¯å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10681v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ViTæ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶å¯¹åé—¨æ”»å‡»çš„é²æ£’æ€§å¼•å‘å…³æ³¨ã€‚ç ”ç©¶å‘ç°ï¼ŒViTå¯¹ç±»è§¦å‘å¼æ”»å‡»ï¼ˆquasi-triggersï¼‰çš„é˜²å¾¡è¾ƒå¼±ï¼Œä¸”æ¸…æ´æ ·æœ¬ä¸­çš„åé—¨ç‰¹å¾å¯èƒ½å¢å¼ºè¿™ç±»æ”»å‡»æ•ˆæœã€‚ä¸ºæ¢ç©¶è¿™äº›æ¼æ´ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†æ‰°åŠ¨æ•æ„Ÿåº¦åˆ†å¸ƒå›¾ï¼ˆPSDMï¼‰ï¼Œæ­ç¤ºViTæ¨¡å‹ä¸­å¤®åƒç´ å¯¹å¾®å°å˜åŠ¨çš„æ•æ„Ÿåº¦é«˜äºè¾¹ç¼˜ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œå›¢é˜Ÿä¸ºViTæ¨¡å‹è®¾è®¡äº†åä¸ºâ€œWorstVITâ€çš„ç®€å•é«˜æ•ˆæ•°æ®æ¯’è¯åé—¨æ”»å‡»æ–¹æ³•ï¼Œæ”»å‡»åªéœ€æä½çš„æ„ŸæŸ“ç‡ï¼Œè®­ç»ƒå‘¨æœŸçŸ­ï¼Œä»…ä¿®æ”¹å•ä¸ªåƒç´ å³å¯æˆåŠŸæ”»å‡»æ‰€æœ‰éªŒè¯å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViTæ¨¡å‹å¯¹åé—¨æ”»å‡»ï¼ˆç‰¹åˆ«æ˜¯ç±»è§¦å‘å¼æ”»å‡»ï¼‰çš„é²æ£’æ€§è¾ƒä½ã€‚</li>
<li>æ¸…æ´æ ·æœ¬ä¸­çš„åé—¨ç‰¹å¾å¯èƒ½å¢å¼ºç±»è§¦å‘å¼æ”»å‡»çš„æ•ˆæœã€‚</li>
<li>PSDMå·¥å…·å¯æ­ç¤ºæ¨¡å‹å¯¹è¾“å…¥å¾®å°å˜åŠ¨çš„æ•æ„Ÿåº¦ï¼Œä¸ºè®¾è®¡ç±»è§¦å‘å¼æ”»å‡»æä¾›æŒ‡å¯¼ã€‚</li>
<li>ViTæ¨¡å‹çš„ä¸­å¤®åƒç´ å¯¹æ‰°åŠ¨æ›´ä¸ºæ•æ„Ÿã€‚</li>
<li>â€œWorstVITâ€æ”»å‡»æ–¹æ³•é’ˆå¯¹ViTæ¨¡å‹ï¼Œå…·æœ‰ä½æ„ŸæŸ“ç‡ã€çŸ­è®­ç»ƒå‘¨æœŸåŠå•ä¸€åƒç´ ä¿®æ”¹çš„ç‰¹ç‚¹ã€‚</li>
<li>â€œWorstVITâ€æ”»å‡»èƒ½æˆåŠŸå½±å“æ‰€æœ‰éªŒè¯å›¾åƒã€‚</li>
<li>ç ”ç©¶ä¸ºå¢å¼ºViTæ¨¡å‹å¯¹åé—¨æ”»å‡»çš„é²æ£’æ€§æä¾›äº†é‡è¦å¯ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5153f7f2ad351e907ebb0bb43e7c5aba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e855b35d44796c6ae9e9bdb67d5dec33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-440c0045876578cc6df3da529437a247.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62f324c018542f0ffae822417fba4cd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51f67ab764441a7691b8d474c1632ea6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="UCDR-Adapter-Exploring-Adaptation-of-Pre-Trained-Vision-Language-Models-for-Universal-Cross-Domain-Retrieval"><a href="#UCDR-Adapter-Exploring-Adaptation-of-Pre-Trained-Vision-Language-Models-for-Universal-Cross-Domain-Retrieval" class="headerlink" title="UCDR-Adapter: Exploring Adaptation of Pre-Trained Vision-Language Models   for Universal Cross-Domain Retrieval"></a>UCDR-Adapter: Exploring Adaptation of Pre-Trained Vision-Language Models   for Universal Cross-Domain Retrieval</h2><p><strong>Authors:Haoyu Jiang, Zhi-Qi Cheng, Gabriel Moreira, Jiawen Zhu, Jingdong Sun, Bukun Ren, Jun-Yan He, Qi Dai, Xian-Sheng Hua</strong></p>
<p>Universal Cross-Domain Retrieval (UCDR) retrieves relevant images from unseen domains and classes without semantic labels, ensuring robust generalization. Existing methods commonly employ prompt tuning with pre-trained vision-language models but are inherently limited by static prompts, reducing adaptability. We propose UCDR-Adapter, which enhances pre-trained models with adapters and dynamic prompt generation through a two-phase training strategy. First, Source Adapter Learning integrates class semantics with domain-specific visual knowledge using a Learnable Textual Semantic Template and optimizes Class and Domain Prompts via momentum updates and dual loss functions for robust alignment. Second, Target Prompt Generation creates dynamic prompts by attending to masked source prompts, enabling seamless adaptation to unseen domains and classes. Unlike prior approaches, UCDR-Adapter dynamically adapts to evolving data distributions, enhancing both flexibility and generalization. During inference, only the image branch and generated prompts are used, eliminating reliance on textual inputs for highly efficient retrieval. Extensive benchmark experiments show that UCDR-Adapter consistently outperforms ProS in most cases and other state-of-the-art methods on UCDR, U(c)CDR, and U(d)CDR settings. </p>
<blockquote>
<p>é€šç”¨è·¨åŸŸæ£€ç´¢ï¼ˆUCDRï¼‰èƒ½å¤Ÿä»æœªè§è¿‡çš„é¢†åŸŸå’Œç±»åˆ«ä¸­æ£€ç´¢å‡ºç›¸å…³çš„å›¾åƒï¼Œè€Œæ— éœ€è¯­ä¹‰æ ‡ç­¾ï¼Œç¡®ä¿äº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨åŸºäºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æç¤ºè°ƒæ•´ï¼ˆprompt tuningï¼‰ï¼Œä½†ç”±äºé™æ€æç¤ºçš„å›ºæœ‰å±€é™æ€§ï¼Œå…¶é€‚åº”æ€§è¾ƒå·®ã€‚æˆ‘ä»¬æå‡ºäº†UCDR-Adapterï¼Œå®ƒé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å¢å¼ºé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å€ŸåŠ©é€‚é…å™¨ï¼ˆadaptersï¼‰å’ŒåŠ¨æ€æç¤ºç”Ÿæˆæ¥æé«˜é€‚åº”æ€§ã€‚é¦–å…ˆï¼Œæºé€‚é…å™¨å­¦ä¹ ï¼ˆSource Adapter Learningï¼‰ä½¿ç”¨å¯å­¦ä¹ çš„æ–‡æœ¬è¯­ä¹‰æ¨¡æ¿å°†ç±»åˆ«è¯­ä¹‰ä¸ç‰¹å®šé¢†åŸŸçš„è§†è§‰çŸ¥è¯†ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡åŠ¨é‡æ›´æ–°å’ŒåŒæŸå¤±å‡½æ•°ä¼˜åŒ–ç±»åˆ«å’Œé¢†åŸŸæç¤ºï¼Œä»¥å®ç°ç¨³å¥çš„å¯¹é½ã€‚å…¶æ¬¡ï¼Œç›®æ ‡æç¤ºç”Ÿæˆï¼ˆTarget Prompt Generationï¼‰é€šè¿‡å…³æ³¨è¢«é®è”½çš„æºæç¤ºæ¥åˆ›å»ºåŠ¨æ€æç¤ºï¼Œä»è€Œå®ç°æ— ç¼é€‚åº”æœªè§è¿‡çš„é¢†åŸŸå’Œç±»åˆ«ã€‚ä¸å…ˆå‰çš„æ–¹æ³•ä¸åŒï¼ŒUCDR-Adapterèƒ½å¤ŸåŠ¨æ€é€‚åº”ä¸æ–­å˜åŒ–çš„æ•°æ®åˆ†å¸ƒï¼Œæé«˜äº†çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä»…ä½¿ç”¨å›¾åƒåˆ†æ”¯å’Œç”Ÿæˆçš„æç¤ºï¼Œæ¶ˆé™¤äº†å¯¹æ–‡æœ¬è¾“å…¥çš„ä¾èµ–ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ£€ç´¢ã€‚å¤§é‡çš„åŸºå‡†å®éªŒè¡¨æ˜ï¼ŒUCDR-Adapteråœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å‡ä¼˜äºProSå’Œå…¶ä»–åœ¨UCDRã€U(c)CDRå’ŒU(d)CDRè®¾ç½®ä¸‹çš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10680v1">PDF</a> Accepted to WACV 2025. Project link:   <a target="_blank" rel="noopener" href="https://github.com/fine68/UCDR2024">https://github.com/fine68/UCDR2024</a></p>
<p><strong>Summary</strong></p>
<p>UCDRæŠ€æœ¯é€šè¿‡åŠ¨æ€è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ç°äº†åœ¨ä¸åŒé¢†åŸŸå’Œç±»åˆ«ä¸­æ£€ç´¢ç›¸å…³å›¾åƒçš„ç›®æ ‡ï¼Œä¸”æ— éœ€è¯­ä¹‰æ ‡ç­¾å³å¯å®ç°ç¨³å¥çš„æ³›åŒ–ã€‚è¯¥ç ”ç©¶æå‡ºäº†UCDR-Adapteræ–¹æ³•ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å¢å¼ºé¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬æºé€‚é…å™¨å­¦ä¹ ï¼ˆé›†æˆç±»åˆ«è¯­ä¹‰å’Œç‰¹å®šé¢†åŸŸçš„è§†è§‰çŸ¥è¯†ï¼‰å’Œç›®æ ‡æç¤ºç”Ÿæˆï¼ˆé€šè¿‡å…³æ³¨æ©ç æºæç¤ºæ¥ç”ŸæˆåŠ¨æ€æç¤ºï¼‰ã€‚è¿™ç§æ–¹æ³•èƒ½åŠ¨æ€é€‚åº”ä¸æ–­å˜åŒ–çš„æ•°æ®åˆ†å¸ƒï¼Œæé«˜äº†çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¨æ–­æ—¶ï¼Œä»…ä½¿ç”¨å›¾åƒåˆ†æ”¯å’Œç”Ÿæˆçš„æç¤ºï¼Œå®ç°äº†é«˜æ•ˆæ£€ç´¢ã€‚å®éªŒè¡¨æ˜ï¼ŒUCDR-Adapteråœ¨UCDRã€U(c)CDRå’ŒU(d)CDRè®¾ç½®ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>UCDRæŠ€æœ¯èƒ½å®ç°åœ¨ä¸åŒé¢†åŸŸå’Œç±»åˆ«ä¸­æ£€ç´¢ç›¸å…³å›¾åƒï¼Œä¸”æ— éœ€è¯­ä¹‰æ ‡ç­¾å³å¯å®ç°ç¨³å¥çš„æ³›åŒ–ã€‚</li>
<li>UCDR-Adapteræ–¹æ³•é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å¢å¼ºé¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬æºé€‚é…å™¨å­¦ä¹ å’Œç›®æ ‡æç¤ºç”Ÿæˆã€‚</li>
<li>æºé€‚é…å™¨å­¦ä¹ é›†æˆç±»åˆ«è¯­ä¹‰å’Œç‰¹å®šé¢†åŸŸçš„è§†è§‰çŸ¥è¯†ï¼Œé€šè¿‡ä¼˜åŒ–ç±»æç¤ºå’ŒåŸŸæç¤ºæ¥æé«˜æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>ç›®æ ‡æç¤ºç”Ÿæˆé€šè¿‡å…³æ³¨æ©ç æºæç¤ºæ¥ç”ŸæˆåŠ¨æ€æç¤ºï¼Œæé«˜æ¨¡å‹çš„çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>UCDR-Adapterèƒ½åŠ¨æ€é€‚åº”ä¸æ–­å˜åŒ–çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>åœ¨æ¨æ–­æ—¶ï¼ŒUCDR-Adapterä»…ä½¿ç”¨å›¾åƒåˆ†æ”¯å’Œç”Ÿæˆçš„æç¤ºï¼Œæé«˜äº†æ£€ç´¢æ•ˆç‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒUCDR-Adapteråœ¨å¤šç§è®¾ç½®ä¸Šå‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11feb1eba7e2260a36625669dd907357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34e307cc107be45b1c754dd055c7000a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6bf43e45e9cd5f8324d83da23387755.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cdee099e6f672b350feb3d15e89ac57.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RemDet-Rethinking-Efficient-Model-Design-for-UAV-Object-Detection"><a href="#RemDet-Rethinking-Efficient-Model-Design-for-UAV-Object-Detection" class="headerlink" title="RemDet: Rethinking Efficient Model Design for UAV Object Detection"></a>RemDet: Rethinking Efficient Model Design for UAV Object Detection</h2><p><strong>Authors:Chen Li, Rui Zhao, Zeyu Wang, Huiying Xu, Xinzhong Zhu</strong></p>
<p>Object detection in Unmanned Aerial Vehicle (UAV) images has emerged as a focal area of research, which presents two significant challenges: i) objects are typically small and dense within vast images; ii) computational resource constraints render most models unsuitable for real-time deployment. Current real-time object detectors are not optimized for UAV images, and complex methods designed for small object detection often lack real-time capabilities. To address these challenges, we propose a novel detector, RemDet (Reparameter efficient multiplication Detector). Our contributions are as follows: 1) Rethinking the challenges of existing detectors for small and dense UAV images, and proposing information loss as a design guideline for efficient models. 2) We introduce the ChannelC2f module to enhance small object detection performance, demonstrating that high-dimensional representations can effectively mitigate information loss. 3) We design the GatedFFN module to provide not only strong performance but also low latency, effectively addressing the challenges of real-time detection. Our research reveals that GatedFFN, through the use of multiplication, is more cost-effective than feed-forward networks for high-dimensional representation. 4) We propose the CED module, which combines the advantages of ViT and CNN downsampling to effectively reduce information loss. It specifically enhances context information for small and dense objects. Extensive experiments on large UAV datasets, Visdrone and UAVDT, validate the real-time efficiency and superior performance of our methods. On the challenging UAV dataset VisDrone, our methods not only provided state-of-the-art results, improving detection by more than 3.4%, but also achieve 110 FPS on a single 4090. </p>
<blockquote>
<p>æ— äººæœºå›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹å·²æˆä¸ºç ”ç©¶çš„æ ¸å¿ƒé¢†åŸŸï¼Œå®ƒé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€ã€åœ¨å¤§å‹å›¾åƒä¸­ï¼Œç›®æ ‡é€šå¸¸è¾ƒå°ä¸”å¯†é›†ï¼›äºŒã€è®¡ç®—èµ„æºé™åˆ¶ä½¿å¾—å¤§å¤šæ•°æ¨¡å‹ä¸é€‚åˆå®æ—¶éƒ¨ç½²ã€‚å½“å‰çš„å®æ—¶ç›®æ ‡æ£€æµ‹å™¨å¹¶æœªé’ˆå¯¹æ— äººæœºå›¾åƒè¿›è¡Œä¼˜åŒ–ï¼Œè€Œä¸ºå°å‹ç›®æ ‡æ£€æµ‹è®¾è®¡çš„å¤æ‚æ–¹æ³•å¾€å¾€ç¼ºä¹å®æ—¶åŠŸèƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ£€æµ‹å™¨â€”â€”RemDetï¼ˆé‡å‚æ•°é«˜æ•ˆä¹˜æ³•æ£€æµ‹å™¨ï¼‰ã€‚æˆ‘ä»¬çš„è´¡çŒ®å¦‚ä¸‹ï¼š</p>
</blockquote>
<p>ä¸€ã€é‡æ–°æ€è€ƒç°æœ‰æ£€æµ‹å™¨åœ¨å°å‹ä¸”å¯†é›†çš„æ— äººæœºå›¾åƒä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºä¿¡æ¯æŸå¤±ä½œä¸ºè®¾è®¡é«˜æ•ˆæ¨¡å‹çš„é‡è¦æŒ‡å¯¼åŸåˆ™ã€‚</p>
<p>äºŒã€æˆ‘ä»¬å¼•å…¥äº†ChannelC2fæ¨¡å—ï¼Œä»¥æé«˜å°å‹ç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œè¯æ˜é«˜ç»´è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆåœ°å‡è½»ä¿¡æ¯æŸå¤±ã€‚</p>
<p>ä¸‰ã€æˆ‘ä»¬è®¾è®¡äº†GatedFFNæ¨¡å—ï¼Œä¸ä»…æ€§èƒ½å¼ºå¤§ï¼Œè€Œä¸”å»¶è¿Ÿä½ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å®æ—¶æ£€æµ‹çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä¹˜æ³•ä½¿ç”¨ï¼ŒGatedFFNçš„æˆæœ¬æ•ˆç›Šé«˜äºå‰é¦ˆç½‘ç»œç”¨äºé«˜ç»´è¡¨ç¤ºã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10040v2">PDF</a> Accepted to AAAI25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ— äººæœºå›¾åƒç›®æ ‡æ£€æµ‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ£€æµ‹å™¨RemDetã€‚è¯¥æ£€æµ‹å™¨é€šè¿‡ä¼˜åŒ–æ¨¡å‹è®¾è®¡ä»¥å‡å°ä¿¡æ¯æŸå¤±ï¼Œå¼•å…¥ChannelC2fæ¨¡å—å¢å¼ºå°ç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œè®¾è®¡GatedFFNæ¨¡å—å®ç°å¼ºæ€§èƒ½ä¸ä½å»¶è¿Ÿçš„å®æ—¶æ£€æµ‹ï¼Œå¹¶ç»“åˆViTå’ŒCNNä¸‹é‡‡æ ·çš„ä¼˜åŠ¿æå‡ºCEDæ¨¡å—æ¥å‡å°‘ä¿¡æ¯æŸå¤±å¹¶å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨å¤§å‹æ— äººæœºæ•°æ®é›†Visdroneå’ŒUAVDTä¸Šçš„å®éªŒéªŒè¯äº†å…¶æ–¹æ³•çš„å®æ—¶æ•ˆç‡å’Œå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ— äººæœºå›¾åƒç›®æ ‡æ£€æµ‹é¢ä¸´å°ç›®æ ‡å’Œè®¡ç®—èµ„æºé™åˆ¶ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºæ–°å‹æ£€æµ‹å™¨RemDetï¼Œé’ˆå¯¹æ— äººæœºå›¾åƒç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å¼•å…¥ChannelC2fæ¨¡å—å¢å¼ºå°ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>è®¾è®¡GatedFFNæ¨¡å—å®ç°å®æ—¶é«˜æ€§èƒ½æ£€æµ‹ï¼Œå¹¶é€šè¿‡ä¹˜æ³•é™ä½æˆæœ¬ã€‚</li>
<li>ç»“åˆViTå’ŒCNNä¸‹é‡‡æ ·çš„ä¼˜ç‚¹ï¼Œæå‡ºCEDæ¨¡å—å‡å°‘ä¿¡æ¯æŸå¤±å¹¶å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åœ¨å¤§å‹æ— äººæœºæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æ–¹æ³•çš„å®æ—¶æ•ˆç‡å’Œä¼˜è¶Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88a68a5e31cfcadad3f08bd2ab577688.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29a85ecb9005bb56ffb63b86687b3f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c3363759e74e24decb4f1687e52c969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58071f781b9a5fce03689e840084f268.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b5ba8dac80b6baac5d6eaed57c25a88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d380bf947769afb5f51c667662926b21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aacbd78ceb8ee4d8723af562fca5cc7a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="EOV-Seg-Efficient-Open-Vocabulary-Panoptic-Segmentation"><a href="#EOV-Seg-Efficient-Open-Vocabulary-Panoptic-Segmentation" class="headerlink" title="EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation"></a>EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation</h2><p><strong>Authors:Hongwei Niu, Jie Hu, Jianghang Lin, Guannan Jiang, Shengchuan Zhang</strong></p>
<p>Open-vocabulary panoptic segmentation aims to segment and classify everything in diverse scenes across an unbounded vocabulary. Existing methods typically employ two-stage or single-stage framework. The two-stage framework involves cropping the image multiple times using masks generated by a mask generator, followed by feature extraction, while the single-stage framework relies on a heavyweight mask decoder to make up for the lack of spatial position information through self-attention and cross-attention in multiple stacked Transformer blocks. Both methods incur substantial computational overhead, thereby hindering the efficiency of model inference. To fill the gap in efficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and spatialaware framework designed for open-vocabulary panoptic segmentation. Specifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware Selection (VAS) module is proposed to improve the semantic comprehension of visual aggregated features and alleviate the feature interaction burden on the mask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE), which efficiently utilizes the spatial awareness capabilities of ViT-based CLIP backbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary panoptic segmentation framework towards efficiency, which runs faster and achieves competitive performance compared with state-of-the-art methods. Specifically, with COCO training only, EOV-Seg achieves 24.5 PQ, 32.1 mIoU, and 11.6 FPS on the ADE20K dataset and the inference time of EOV-Seg is 4-19 times faster than state-of-theart methods. Especially, equipped with ResNet50 backbone, EOV-Seg runs 23.8 FPS with only 71M parameters on a single RTX 3090 GPU. Code is available at <a target="_blank" rel="noopener" href="https://github.com/nhw649/EOV-Seg">https://github.com/nhw649/EOV-Seg</a>. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²æ—¨åœ¨åˆ†å‰²å¹¶åˆ†ç±»æ— é™è¯æ±‡è¡¨ä¸­ä¸åŒåœºæ™¯ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæˆ–å•é˜¶æ®µæ¡†æ¶ã€‚ä¸¤é˜¶æ®µæ¡†æ¶æ¶‰åŠä½¿ç”¨ç”±æ©è†œç”Ÿæˆå™¨ç”Ÿæˆçš„æ©è†œå¤šæ¬¡è£å‰ªå›¾åƒï¼Œç„¶åè¿›è¡Œç‰¹å¾æå–ï¼Œè€Œå•é˜¶æ®µæ¡†æ¶åˆ™ä¾èµ–äºé‡é‡çº§çš„æ©è†œè§£ç å™¨ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›åœ¨å¤šä¸ªå †å çš„Transformerå—ä¸­è¿›è¡Œç©ºé—´ä½ç½®ä¿¡æ¯çš„è¡¥å¿ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½ä¼šäº§ç”Ÿå¤§é‡çš„è®¡ç®—å¼€é”€ï¼Œä»è€Œé˜»ç¢äº†æ¨¡å‹æ¨ç†çš„æ•ˆç‡ã€‚ä¸ºäº†å¼¥è¡¥æ•ˆç‡ä¸Šçš„å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†EOV-Segï¼Œè¿™æ˜¯ä¸€ç§ä¸ºå¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²è®¾è®¡çš„æ–°å‹å•é˜¶æ®µã€å…±äº«ã€é«˜æ•ˆä¸”ç©ºé—´æ„ŸçŸ¥çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒEOV-Segåœ¨ä¸¤ä¸ªæ–¹é¢è¿›è¡Œäº†åˆ›æ–°ã€‚é¦–å…ˆï¼Œæå‡ºäº†è¯æ±‡æ„ŸçŸ¥é€‰æ‹©ï¼ˆVASï¼‰æ¨¡å—ï¼Œä»¥æé«˜è§†è§‰èšåˆç‰¹å¾è¯­ä¹‰ç†è§£èƒ½åŠ›å¹¶å‡è½»æ©è†œè§£ç å™¨ä¸Šçš„ç‰¹å¾äº¤äº’è´Ÿæ‹…ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒå‘åŠ¨æ€åµŒå…¥ä¸“å®¶ï¼ˆTDEEï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°åˆ©ç”¨äº†åŸºäºViTçš„CLIPéª¨å¹²ç½‘çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒEOV-Segæ˜¯é¢å‘æ•ˆç‡çš„ç¬¬ä¸€ä¸ªå¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²æ¡†æ¶ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒè¿è¡Œæ›´å¿«å¹¶å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œä»…åœ¨COCOè®­ç»ƒé›†ä¸Šï¼ŒEOV-Segåœ¨ADE20Kæ•°æ®é›†ä¸Šå®ç°äº†24.5çš„PQï¼Œ32.1çš„mIoUå’Œ11.6çš„FPSï¼Œå¹¶ä¸”EOV-Segçš„æ¨ç†æ—¶é—´æ˜¯ç°æœ‰å…ˆè¿›æ–¹æ³•çš„4-19å€ã€‚ç‰¹åˆ«æ˜¯ï¼Œé…å¤‡ResNet50éª¨å¹²ç½‘æ—¶ï¼ŒEOV-Segåœ¨å•ä¸ªRTX 3090 GPUä¸Šä»…ä½¿ç”¨71Må‚æ•°å³å¯è¿è¡Œ23.8 FPSã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nhw649/EOV-Seg%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nhw649/EOV-Segæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08628v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²çš„æŒ‘æˆ˜åŠç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºEOV-Segçš„æ–°å‹å•é˜¶æ®µã€å…±äº«ã€é«˜æ•ˆçš„ç©ºé—´æ„ŸçŸ¥æ¡†æ¶ã€‚EOV-Segé€šè¿‡VASæ¨¡å—æ”¹è¿›è§†è§‰èšåˆç‰¹å¾è¯­ä¹‰ç†è§£ï¼Œå¹¶å¼•å…¥TDEEæ¨¡å—æœ‰æ•ˆåˆ©ç”¨ViT-based CLIPä¸»å¹²çš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ã€‚ç›¸æ¯”å…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ŒEOV-Segè¿è¡Œæ›´å¿«ï¼Œæ€§èƒ½æ›´å…·ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²æ—¨åœ¨åˆ†å‰²å’Œåˆ†ç±»å¤šæ ·åœºæ™¯ä¸­çš„æ‰€æœ‰å†…å®¹ï¼Œé¢å¯¹æ— é™è¯æ±‡çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæˆ–å•é˜¶æ®µæ¡†æ¶ï¼Œå­˜åœ¨è®¡ç®—å¼€é”€å¤§ã€æ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚</li>
<li>EOV-Segæ˜¯ä¸€ä¸ªæ–°å‹çš„å•é˜¶æ®µã€å…±äº«ã€é«˜æ•ˆçš„ç©ºé—´æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºå¼€æ”¾è¯æ±‡å…¨æ™¯åˆ†å‰²ã€‚</li>
<li>EOV-Segé€šè¿‡VASæ¨¡å—æ”¹è¿›äº†è¯­ä¹‰ç†è§£çš„èšåˆç‰¹å¾ï¼Œå‡è½»äº†maskè§£ç å™¨çš„ç‰¹å¾äº¤äº’è´Ÿæ‹…ã€‚</li>
<li>TDEEæ¨¡å—çš„å¼•å…¥å®ç°äº†ViT-based CLIPä¸»å¹²çš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›çš„æœ‰æ•ˆåˆ©ç”¨ã€‚</li>
<li>EOV-Segç›¸æ¯”å…¶ä»–å…ˆè¿›æ–¹æ³•è¿è¡Œæ›´å¿«ï¼Œæ€§èƒ½æ›´å…·ç«äº‰åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ADE20Kæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc6d9f31be2522f5313c77a18a439c9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2669f37c296ca6de2d1d9995fe24cf02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fa686874ec075dd0c58195b69988acf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f97df02ee140bdeec354671394d1592d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe2db18003e53fff7e214cfbe2139b2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43be22103a9bf47683c784798e951b64.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision"><a href="#Can-Graph-Neural-Networks-Learn-Language-with-Extremely-Weak-Text-Supervision" class="headerlink" title="Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?"></a>Can Graph Neural Networks Learn Language with Extremely Weak Text   Supervision?</h2><p><strong>Authors:Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, Jingrui He, Jiawei Han</strong></p>
<p>While great success has been achieved in building vision models with Contrastive Language-Image Pre-training (CLIP) over Internet-scale image-text pairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is challenging because of three fundamental issues: the scarcity of labeled data and text supervision, different levels of downstream tasks, and the conceptual gaps between domains. In this work, to address these issues, we leverage multi-modal prompt learning to effectively adapt pre-trained GNN to downstream tasks and data, given only a few semantically labeled samples, each with extremely weak text supervision. Our new paradigm embeds the graphs directly in the same space as the Large Language Models (LLMs) by learning both graph prompts and text prompts simultaneously. To accomplish this, we improve state-of-the-art graph prompt method, and then propose the first graph-language multi-modal prompt learning approach for exploiting the knowledge in pre-trained models. Notably, due to the insufficient supervision for fine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen, so the learnable parameters are much fewer than fine-tuning any pre-trained model. Through extensive experiments on real-world datasets, we demonstrate the superior performance of our paradigm in few-shot, multi-task-level, and cross-domain settings. Moreover, we build the first CLIP-style zero-shot classification prototype that can generalize GNNs to unseen classes with extremely weak text supervision. </p>
<blockquote>
<p>åœ¨åˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨äº’è”ç½‘è§„æ¨¡çš„å›¾åƒæ–‡æœ¬å¯¹ä¸Šæ„å»ºè§†è§‰æ¨¡å‹å–å¾—å·¨å¤§æˆåŠŸçš„åŒæ—¶ï¼Œä½¿ç”¨CLIPç®¡é“æ„å»ºå¯è¿ç§»çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ã€ä¸‹æ¸¸ä»»åŠ¡çº§åˆ«ä¸åŒä»¥åŠé¢†åŸŸé—´çš„æ¦‚å¿µå·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¥æœ‰æ•ˆåœ°é€‚åº”é¢„è®­ç»ƒGNNåˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®ï¼Œä»…ä½¿ç”¨å°‘é‡è¯­ä¹‰æ ‡æ³¨æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½å¸¦æœ‰æå¼±çš„æ–‡æœ¬ç›‘ç£ã€‚æˆ‘ä»¬çš„æ–°èŒƒå¼é€šè¿‡å°†å›¾ç›´æ¥åµŒå…¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸åŒçš„ç©ºé—´ï¼ŒåŒæ—¶å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºã€‚ä¸ºäº†å®Œæˆè¿™é¡¹å·¥ä½œï¼Œæˆ‘ä»¬æ”¹è¿›äº†æœ€å…ˆè¿›çš„å›¾æç¤ºæ–¹æ³•ï¼Œç„¶åæå‡ºäº†ç¬¬ä¸€ä¸ªåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†çš„å›¾-è¯­è¨€å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºç²¾ç»†è°ƒæ•´çš„ç›‘ç£ä¸è¶³ï¼Œåœ¨æˆ‘ä»¬çš„èŒƒå¼ä¸­ï¼Œé¢„è®­ç»ƒçš„GNNå’ŒLLMä¿æŒä¸å˜ï¼Œå› æ­¤å¯å­¦ä¹ å‚æ•°è¿œè¿œå°‘äºå¯¹ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹çš„ç²¾ç»†è°ƒæ•´ã€‚é€šè¿‡åœ¨å®é™…æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„èŒƒå¼åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸­çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œå¯ä»¥å°†GNNæ¨å¹¿åˆ°æœªè§è¿‡çš„ç±»åˆ«ï¼Œå¸¦æœ‰æå¼±çš„æ–‡æœ¬ç›‘ç£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08174v2">PDF</a> Preprint, 25 pages</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶é’ˆå¯¹CLIPç®¡é“åœ¨æ„å»ºå¯è¿ç§»çš„å›¾å½¢ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œé€šè¿‡å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®çš„é¢„è®­ç»ƒGNNã€‚è¯¥ç ”ç©¶è§£å†³äº†æ•°æ®æ ‡ç­¾ç¨€ç¼ºã€æ–‡æœ¬ç›‘ç£ä¸è¶³ã€ä¸‹æ¸¸ä»»åŠ¡çº§åˆ«ä¸åŒä»¥åŠé¢†åŸŸé—´æ¦‚å¿µå·®è·ç­‰é—®é¢˜ã€‚é€šè¿‡æ”¹è¿›æœ€å…ˆè¿›çš„å›¾å½¢æç¤ºæ–¹æ³•ï¼Œè¯¥ç ”ç©¶æå‡ºäº†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†çš„é¦–ä¸ªå›¾å½¢è¯­è¨€å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å»ºç«‹äº†é¦–ä¸ªCLIPé£æ ¼çš„é›¶æ ·æœ¬åˆ†ç±»åŸå‹ï¼Œå¯ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„ç±»åˆ«ï¼Œåªéœ€æå¼±çš„æ–‡æœ¬ç›‘ç£å³å¯ã€‚è¿™é¡¹ç ”ç©¶åœ¨ä¿è¯è¶³å¤Ÿçš„æ ·æœ¬é‡çš„å‰æä¸‹è§£å†³äº†å„ç§åœºæ™¯ä¸‹çš„é—®é¢˜ï¼Œå¹¶å±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>CLIPæ¨¡å‹åœ¨äº’è”ç½‘è§„æ¨¡çš„å›¾åƒæ–‡æœ¬å¯¹ä¸Šæ„å»ºè§†è§‰æ¨¡å‹å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨æ„å»ºå¯è¿ç§»çš„å›¾å½¢ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æŒ‘æˆ˜åŒ…æ‹¬ç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ–‡æœ¬ç›‘ç£ã€ä¸‹æ¸¸ä»»åŠ¡çº§åˆ«ä¸åŒä»¥åŠé¢†åŸŸé—´çš„æ¦‚å¿µå·®è·ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä½¿å¾—é¢„è®­ç»ƒçš„GNNèƒ½å¤Ÿé€‚åº”ä¸‹æ¸¸ä»»åŠ¡å’Œåªæœ‰å°‘é‡è¯­ä¹‰æ ‡æ³¨æ ·æœ¬çš„æ•°æ®ã€‚è¯¥ç ”ç©¶æ–¹æ³•æ”¹è¿›äº†æœ€æ–°çš„å›¾å½¢æç¤ºæ–¹æ³•å¹¶æå‡ºäº†é¦–ä¸ªå›¾å½¢è¯­è¨€å¤šæ¨¡æ€æç¤ºå­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­åµŒå…¥å›¾æ¥å­¦ä¹ å›¾æç¤ºå’Œæ–‡æœ¬æç¤ºï¼Œå°†å›¾ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç½®äºåŒä¸€ç©ºé—´ã€‚ç”±äºç›‘ç£ä¸è¶³ï¼Œè¯¥ç ”ç©¶åœ¨æ¨¡å¼ä¸­ä¿æŒé¢„è®­ç»ƒçš„GNNå’ŒLLMå†»ç»“çŠ¶æ€ï¼Œå‡å°‘äº†å­¦ä¹ å‚æ•°çš„æ•°é‡ã€‚è¿™ä¸€æ–¹æ³•åœ¨å°‘æ ·æœ¬ã€å¤šä»»åŠ¡çº§åˆ«å’Œè·¨åŸŸè®¾ç½®ä¸Šå±•ç¤ºäº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7662b2e53ceb8c6a215b77a82e72ace.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-606197ccb8d5d60f8ae6d5732352d062.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-609c40c2133b921df69a34108402a294.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning"><a href="#Critic-V-VLM-Critics-Help-Catch-VLM-Errors-in-Multimodal-Reasoning" class="headerlink" title="Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning"></a>Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning</h2><p><strong>Authors:Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou</strong></p>
<p>Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasonerâ€™s capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç”±äºè¯¸å¦‚è™šæ„çš„å›¾åƒç†è§£æˆ–ç²—ç³™çš„æ¨ç†è·¯å¾„ç­‰é—®é¢˜ï¼Œå®ƒä»¬ä»ç„¶ç»å¸¸äº§ç”Ÿä¸å‡†ç¡®æˆ–ä¸ç›¸å…³çš„ååº”ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Critic-Vï¼Œè¿™æ˜¯ä¸€ä¸ªå—Actor-CriticèŒƒå¼å¯å‘çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æå‡VLMsçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶æ¥è§£è€¦æ¨ç†è¿‡ç¨‹å’Œæ‰¹è¯„è¿‡ç¨‹ï¼šReasonerï¼Œå®ƒæ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼›ä»¥åŠCriticï¼Œå®ƒæä¾›å»ºè®¾æ€§æ‰¹è¯„ä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚åœ¨æ­¤æ–¹æ³•ä¸­ï¼ŒReasoneræ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ¨ç†ååº”ï¼Œè¿™äº›ååº”å¯ä»¥åŸºäºæ¥è‡ªCriticçš„åé¦ˆè€Œè¿­ä»£åœ°å‘å±•ä¸ºç­–ç•¥ã€‚è¿™ä¸€äº¤äº’è¿‡ç¨‹æ˜¯ç”±å¼ºåŒ–å­¦ä¹ æ¡†æ¶é©±åŠ¨çš„ï¼Œå…¶ä¸­Criticæä¾›è‡ªç„¶è¯­è¨€æ‰¹è¯„è€Œä¸æ˜¯æ ‡é‡å¥–åŠ±ï¼Œä»è€Œæä¾›æ›´å¾®å¦™çš„åé¦ˆï¼Œæå‡Reasoneråœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚Criticæ¨¡å‹ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆRBRï¼‰æ’åçš„è¯„è®ºåå¥½æ•°æ®é›†æ¥å¢å¼ºå…¶æ‰¹è¯„èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCritic-Væ¡†æ¶åœ¨5ä¸ªæµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆåŒ…æ‹¬GPT-4Vï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚ç»“åˆReasonerçš„åŠ¨æ€æ–‡æœ¬ç­–ç•¥åå¥½ä¼˜åŒ–çš„Criticçš„å»ºè®¾æ€§åé¦ˆï¼Œèƒ½å¤Ÿå®ç°æ›´å¯é å’Œä¸Šä¸‹æ–‡æ•æ„Ÿçš„å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæå‡VLMsçš„å¯é æ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æœ‰æœ›æ”¹å–„å…¶åœ¨ç°å®ä¸–ç•Œæ¨ç†å¯†é›†å‹å¤šæ¨¡æ€åº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½ä½“ï¼‰ä¸­çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18203v3">PDF</a> 16 pages, 11 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆä¸å‡†ç¡®æˆ–æ— å…³å›åº”çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Critic-Væ¡†æ¶ï¼Œè¯¥æ¡†æ¶å—Actor-CriticèŒƒå¼çš„å¯å‘ï¼Œæ—¨åœ¨æå‡VLMçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶â€”â€”Reasonerå’ŒCriticï¼Œå®ç°äº†æ¨ç†è¿‡ç¨‹å’Œæ‰¹è¯„è¿‡ç¨‹çš„è§£è€¦ã€‚Reasoneræ ¹æ®è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œè€ŒCriticæä¾›å»ºè®¾æ€§æ‰¹è¯„ä»¥ä¼˜åŒ–è¿™äº›è·¯å¾„ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼ŒReasoneræ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæ¨ç†å›åº”ï¼Œå¹¶å¯æ ¹æ®æ¥è‡ªCriticçš„åé¦ˆè¿­ä»£åœ°æ¼”å˜ç­–ç•¥ã€‚è¿™ç§äº¤äº’è¿‡ç¨‹ç”±å¼ºåŒ–å­¦ä¹ æ¡†æ¶é©±åŠ¨ï¼ŒCriticæä¾›è‡ªç„¶è¯­è¨€æ‰¹è¯„è€Œéæ ‡é‡å¥–åŠ±ï¼Œä¸ºReasoneråœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„èƒ½åŠ›æå‡æä¾›æ›´å¾®å¦™çš„åé¦ˆã€‚Criticæ¨¡å‹ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ç”±è§„åˆ™å¥–åŠ±ï¼ˆRBRï¼‰æ’åçš„æ‰¹è¯„åå¥½æ•°æ®é›†å¢å¼ºå…¶æ‰¹è¯„èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒCritic-Væ¡†æ¶åœ¨5é¡¹åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆåŒ…æ‹¬GPT-4Vï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚ç»“åˆåŸºäºæ–‡æœ¬ç­–ç•¥çš„ReasoneråŠ¨æ€æ€§å’Œæ¥è‡ªåå¥½ä¼˜åŒ–Criticçš„å»ºè®¾æ€§åé¦ˆï¼Œå®ç°æ›´å¯é å’Œè¯­å¢ƒæ•æ„Ÿçš„å¤šæ¨¡æ€æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæé«˜VLMçš„å¯é æ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¯æ”¹å–„å…¶åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½ä½“ç°ç­‰ç°å®ä¸–ç•Œæ¨ç†å¯†é›†å‹å¤šæ¨¡æ€åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VLMåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºè¿›æ­¥ï¼Œä½†å­˜åœ¨ç”Ÿæˆä¸å‡†ç¡®æˆ–æ— å…³å›åº”çš„æŒ‘æˆ˜ã€‚</li>
<li>Critic-Væ¡†æ¶å—Actor-CriticèŒƒå¼çš„å¯å‘ï¼Œæ—¨åœ¨æå‡VLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶åŒ…å«Reasonerå’ŒCriticä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶ï¼Œåˆ†åˆ«è´Ÿè´£ç”Ÿæˆå’Œä¼˜åŒ–æ¨ç†è·¯å¾„ã€‚</li>
<li>Reasoneræ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›åº”ï¼Œå¹¶å¯æ ¹æ®æ¥è‡ªCriticçš„åé¦ˆè¿­ä»£è°ƒæ•´ç­–ç•¥ã€‚</li>
<li>Criticæ¨¡å‹ä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨æ‰¹è¯„åå¥½æ•°æ®é›†å¢å¼ºæ€§èƒ½ã€‚</li>
<li>Critic-Væ¡†æ¶åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8083afbcd16705a9d8afba4ea8e890bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6837c2c367cd1a4e7ee3ad4ec2bd8ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269d187768ea4ab5aa83a7d89fb133bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2278620f94584191480f74c9cf8598c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bf82c8a9b7c59eb8562c67e28067af8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="QCS-Feature-Refining-from-Quadruplet-Cross-Similarity-for-Facial-Expression-Recognition"><a href="#QCS-Feature-Refining-from-Quadruplet-Cross-Similarity-for-Facial-Expression-Recognition" class="headerlink" title="QCS:Feature Refining from Quadruplet Cross Similarity for Facial   Expression Recognition"></a>QCS:Feature Refining from Quadruplet Cross Similarity for Facial   Expression Recognition</h2><p><strong>Authors:Chengpeng Wang, Li Chen, Lili Wang, Zhaofan Li, Xuebin Lv</strong></p>
<p>Facial expression recognition faces challenges where labeled significant features in datasets are mixed with unlabeled redundant ones. In this paper, we introduce Cross Similarity Attention (CSA) to mine richer intrinsic information from image pairs, overcoming a limitation when the Scaled Dot-Product Attention of ViT is directly applied to calculate the similarity between two different images. Based on CSA, we simultaneously minimize intra-class differences and maximize inter-class differences at the fine-grained feature level through interactions among multiple branches. Contrastive residual distillation is utilized to transfer the information learned in the cross module back to the base network. We ingeniously design a four-branch centrally symmetric network, named Quadruplet Cross Similarity (QCS), which alleviates gradient conflicts arising from the cross module and achieves balanced and stable training. It can adaptively extract discriminative features while isolating redundant ones. The cross-attention modules exist during training, and only one base branch is retained during inference, resulting in no increase in inference time. Our proposed method achieves state-of-the-art performance on several FER datasets. </p>
<blockquote>
<p>é¢éƒ¨è¡¨æƒ…è¯†åˆ«é¢ä¸´ç€æ•°æ®é›†ä¸­æœ‰æ ‡ç­¾çš„é‡è¦ç‰¹å¾ä¸æ— æ ‡ç­¾çš„å†—ä½™ç‰¹å¾æ··åˆçš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨ç›¸ä¼¼åº¦æ³¨æ„åŠ›ï¼ˆCSAï¼‰æ¥ä»å›¾åƒå¯¹ä¸­æŒ–æ˜æ›´ä¸°å¯Œçš„å†…åœ¨ä¿¡æ¯ï¼Œå…‹æœäº†å½“ç›´æ¥ä½¿ç”¨ViTçš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—ä¸¤ä¸ªä¸åŒå›¾åƒä¹‹é—´çš„ç›¸ä¼¼åº¦æ—¶å­˜åœ¨çš„å±€é™æ€§ã€‚åŸºäºCSAï¼Œæˆ‘ä»¬é€šè¿‡å¤šä¸ªåˆ†æ”¯ä¹‹é—´çš„äº¤äº’ï¼Œåœ¨ç»†ç²’åº¦ç‰¹å¾å±‚é¢åŒæ—¶å‡å°ç±»å†…å·®å¼‚å¹¶å¢å¤§ç±»é—´å·®å¼‚ã€‚åˆ©ç”¨å¯¹æ¯”æ®‹å·®è’¸é¦å°†è·¨æ¨¡å—ä¸­å­¦ä¹ åˆ°çš„ä¿¡æ¯è½¬ç§»å›åŸºç¡€ç½‘ç»œã€‚æˆ‘ä»¬å·§å¦™åœ°è®¾è®¡äº†ä¸€ä¸ªå››åˆ†æ”¯ä¸­å¿ƒå¯¹ç§°ç½‘ç»œï¼Œåä¸ºå››å…ƒç»„äº¤å‰ç›¸ä¼¼åº¦ï¼ˆQCSï¼‰ï¼Œè¯¥ç½‘ç»œç¼“è§£äº†ç”±è·¨æ¨¡å—å¼•èµ·çš„æ¢¯åº¦å†²çªï¼Œå®ç°äº†å¹³è¡¡ç¨³å®šçš„è®­ç»ƒã€‚å®ƒå¯ä»¥è‡ªé€‚åº”åœ°æå–åˆ¤åˆ«ç‰¹å¾ï¼ŒåŒæ—¶éš”ç¦»å†—ä½™ç‰¹å¾ã€‚è·¨æ³¨æ„åŠ›æ¨¡å—å­˜åœ¨äºè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè€Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…ä¿ç•™ä¸€ä¸ªåŸºç¡€åˆ†æ”¯ï¼Œå› æ­¤ä¸ä¼šå¢åŠ æ¨ç†æ—¶é—´ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å‡ ä¸ªé¢éƒ¨è¡¨æƒ…è¯†åˆ«æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01988v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä½¿ç”¨Cross Similarity Attentionï¼ˆCSAï¼‰æ¥æŒ–æ˜å›¾åƒå¯¹ä¸­çš„æ›´ä¸°å¯Œå†…åœ¨ä¿¡æ¯ï¼Œè§£å†³é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä¸­çš„æŒ‘æˆ˜ã€‚å½“ç›´æ¥å°†Scaled Dot-Product Attentionåº”ç”¨äºè®¡ç®—ä¸¤ä¸ªä¸åŒå›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§æ—¶ï¼Œå­˜åœ¨å±€é™æ€§ã€‚åŸºäºCSAï¼Œé€šè¿‡å¤šä¸ªåˆ†æ”¯ä¹‹é—´çš„äº¤äº’ï¼ŒåŒæ—¶æœ€å°åŒ–ç±»å†…å·®å¼‚å¹¶æœ€å¤§åŒ–ç±»é—´å·®å¼‚ï¼Œå®ç°ç²¾ç»†ç‰¹å¾çº§åˆ«çš„è¯†åˆ«ã€‚åˆ©ç”¨å¯¹æ¯”æ®‹å·®è’¸é¦å°†è·¨æ¨¡å—å­¦ä¹ çš„ä¿¡æ¯è½¬å›åŸºç¡€ç½‘ç»œã€‚è®¾è®¡äº†ä¸€ä¸ªå››åˆ†æ”¯ä¸­å¿ƒå¯¹ç§°ç½‘ç»œQuadruplet Cross Similarityï¼ˆQCSï¼‰ï¼Œç¼“è§£è·¨æ¨¡å—å¼•èµ·çš„æ¢¯åº¦å†²çªï¼Œå®ç°å¹³è¡¡ç¨³å®šçš„è®­ç»ƒï¼Œå¯è‡ªé€‚åº”æå–åˆ¤åˆ«ç‰¹å¾å¹¶éš”ç¦»å†—ä½™ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºä½¿ç”¨Cross Similarity Attention (CSA) æ¥å¤„ç†é¢éƒ¨è¡¨æƒ…è¯†åˆ«ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“æ•°æ®é›†ä¸­æ ‡ç­¾çš„é‡è¦ç‰¹å¾ä¸æœªæ ‡ç­¾çš„å†—ä½™ç‰¹å¾æ··åˆæ—¶ã€‚</li>
<li>CSAæœ‰åŠ©äºæŒ–æ˜å›¾åƒå¯¹ä¸­çš„æ›´ä¸°å¯Œå†…åœ¨ä¿¡æ¯ï¼Œè§£å†³ç›´æ¥å°†Scaled Dot-Product Attentionåº”ç”¨äºé¢éƒ¨è¡¨æƒ…è¯†åˆ«æ—¶çš„å±€é™æ€§ã€‚</li>
<li>é€šè¿‡å¤šä¸ªåˆ†æ”¯ä¹‹é—´çš„äº¤äº’ï¼ŒåŒæ—¶æœ€å°åŒ–ç±»å†…å·®å¼‚å¹¶æœ€å¤§åŒ–ç±»é—´å·®å¼‚ï¼Œå®ç°æ›´ç²¾ç»†çš„ç‰¹å¾è¯†åˆ«ã€‚</li>
<li>åˆ©ç”¨å¯¹æ¯”æ®‹å·®è’¸é¦å°†è·¨æ¨¡å—ä¿¡æ¯è½¬å›åŸºç¡€ç½‘ç»œï¼Œæå‡ç½‘ç»œæ€§èƒ½ã€‚</li>
<li>è®¾è®¡çš„å››åˆ†æ”¯ä¸­å¿ƒå¯¹ç§°ç½‘ç»œQuadruplet Cross Similarity (QCS) èƒ½ç¼“è§£æ¢¯åº¦å†²çªï¼Œå®ç°æ›´å¹³è¡¡å’Œç¨³å®šçš„è®­ç»ƒã€‚</li>
<li>QCSç½‘ç»œå¯è‡ªé€‚åº”æå–åˆ¤åˆ«ç‰¹å¾ï¼ŒåŒæ—¶éš”ç¦»å†—ä½™ç‰¹å¾ï¼Œæé«˜é¢éƒ¨è¡¨æƒ…è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>è·¨æ³¨æ„æ¨¡å—ä»…å­˜åœ¨äºè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨ç†é˜¶æ®µåªä¿ç•™ä¸€ä¸ªåŸºç¡€åˆ†æ”¯ï¼Œä¸ä¼šå¢åŠ æ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.01988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8aecbe3df0813de5ed7f49807ae38017.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71ac5c063508a291c6323f1a5afa110b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81d12a148171cfd8365e1a17b4b9abe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bce87e743b437d21a989bac5ef6e0463.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2791047a86b7eed9940c74bc242c20b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b0e2d78499baa26ba4092d845e5ed25.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="THOR2-Topological-Analysis-for-3D-Shape-and-Color-Based-Human-Inspired-Object-Recognition-in-Unseen-Environments"><a href="#THOR2-Topological-Analysis-for-3D-Shape-and-Color-Based-Human-Inspired-Object-Recognition-in-Unseen-Environments" class="headerlink" title="THOR2: Topological Analysis for 3D Shape and Color-Based Human-Inspired   Object Recognition in Unseen Environments"></a>THOR2: Topological Analysis for 3D Shape and Color-Based Human-Inspired   Object Recognition in Unseen Environments</h2><p><strong>Authors:Ekta U. Samani, Ashis G. Banerjee</strong></p>
<p>Visual object recognition in unseen and cluttered indoor environments is a challenging problem for mobile robots. This study presents a 3D shape and color-based descriptor, TOPS2, for point clouds generated from RGB-D images and an accompanying recognition framework, THOR2. The TOPS2 descriptor embodies object unity, a human cognition mechanism, by retaining the slicing-based topological representation of 3D shape from the TOPS descriptor while capturing object color information through slicing-based color embeddings computed using a network of coarse color regions. These color regions, analogous to the MacAdam ellipses identified in human color perception, are obtained using the Mapper algorithm, a topological soft-clustering technique. THOR2, trained using synthetic data, demonstrates markedly improved recognition accuracy compared to THOR, its 3D shape-based predecessor, on two benchmark real-world datasets: the OCID dataset capturing cluttered scenes from different viewpoints and the UW-IS Occluded dataset reflecting different environmental conditions and degrees of object occlusion recorded using commodity hardware. THOR2 also outperforms baseline deep learning networks, and a widely-used Vision Transformer (ViT) adapted for RGB-D inputs trained using synthetic and limited real-world data on both the datasets. Therefore, THOR2 is a promising step toward achieving robust recognition in low-cost robots. </p>
<blockquote>
<p>å¯¹äºç§»åŠ¨æœºå™¨äººæ¥è¯´ï¼Œåœ¨æœªè§è¿‡çš„æ‚ä¹±å®¤å†…ç¯å¢ƒä¸­è¿›è¡Œè§†è§‰ç‰©ä½“è¯†åˆ«æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸‰ç»´å½¢çŠ¶å’Œé¢œè‰²çš„æè¿°ç¬¦TOPS2ï¼Œç”¨äºä»RGB-Då›¾åƒç”Ÿæˆç‚¹äº‘ï¼Œä»¥åŠä¸€ä¸ªé…å¥—çš„è¯†åˆ«æ¡†æ¶THOR2ã€‚TOPS2æè¿°ç¬¦é€šè¿‡ä¿ç•™TOPSæè¿°ç¬¦çš„åŸºäºåˆ‡ç‰‡çš„æ‹“æ‰‘è¡¨ç¤ºæ¥ä½“ç°ç‰©ä½“çš„æ•´ä½“ç»Ÿä¸€æ€§ï¼ˆè¿™æ˜¯ä¸€ç§äººç±»è®¤çŸ¥æœºåˆ¶ï¼‰ï¼ŒåŒæ—¶é€šè¿‡åˆ©ç”¨ç²—é¢œè‰²åŒºåŸŸç½‘ç»œè®¡ç®—åŸºäºåˆ‡ç‰‡çš„é¢œè‰²åµŒå…¥æ¥æ•è·ç‰©ä½“é¢œè‰²ä¿¡æ¯ã€‚è¿™äº›é¢œè‰²åŒºåŸŸç±»ä¼¼äºäººç±»é¢œè‰²æ„ŸçŸ¥ä¸­è¯†åˆ«å‡ºçš„MacAdamæ¤­åœ†ï¼Œæ˜¯é€šè¿‡æ‹“æ‰‘è½¯èšç±»æŠ€æœ¯Mapperç®—æ³•è·å¾—çš„ã€‚THOR2ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¸åŸºäºä¸‰ç»´å½¢çŠ¶çš„THORç›¸æ¯”ï¼Œåœ¨ä¸¤ä¸ªåŸºå‡†çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—æé«˜çš„è¯†åˆ«ç²¾åº¦ï¼šOCIDæ•°æ®é›†æ•æ‰æ¥è‡ªä¸åŒè§†è§’çš„æ‚ä¹±åœºæ™¯ï¼ŒUW-IS Occludedæ•°æ®é›†åæ˜ ä½¿ç”¨å•†å“ç¡¬ä»¶è®°å½•çš„ä¸åŒç¯å¢ƒæ¡ä»¶å’Œä¸åŒç¨‹åº¦çš„ç‰©ä½“é®æŒ¡ã€‚THOR2è¿˜ä¼˜äºåŸºçº¿æ·±åº¦å­¦ä¹ ç½‘ç»œï¼Œä»¥åŠé€‚åº”RGB-Dè¾“å…¥çš„å¹¿æ³›ä½¿ç”¨çš„Vision Transformerï¼ˆViTï¼‰ï¼Œåè€…ä½¿ç”¨åˆæˆå’Œæœ‰é™çš„çœŸå®ä¸–ç•Œæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œåœ¨ä¸¤ç»„æ•°æ®é›†ä¸Šçš„è¡¨ç°éƒ½ä¸å¦‚THOR2ã€‚å› æ­¤ï¼ŒTHOR2æ˜¯åœ¨ä½æˆæœ¬æœºå™¨äººä¸­å®ç°ç¨³å¥è¯†åˆ«çš„æœ‰å‰é€”çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01579v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ç§»åŠ¨æœºå™¨äººåœ¨æœªè§è¿‡çš„æ‚ä¹±å®¤å†…ç¯å¢ƒä¸­è¿›è¡Œè§†è§‰ç›®æ ‡è¯†åˆ«çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸‰ç»´å½¢çŠ¶å’Œé¢œè‰²çš„æè¿°ç¬¦TOPS2ï¼Œä»¥åŠä¸€ä¸ªé…å¥—çš„ç›®æ ‡è¯†åˆ«æ¡†æ¶THOR2ã€‚TOPS2æè¿°ç¬¦é€šè¿‡ä¿ç•™TOPSæè¿°ç¬¦çš„åŸºäºåˆ‡ç‰‡çš„ä¸‰ç»´å½¢çŠ¶è¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶ç»“åˆé€šè¿‡åˆ‡ç‰‡åµŒå…¥æ³•è®¡ç®—å¾—åˆ°çš„å¯¹è±¡é¢œè‰²ä¿¡æ¯ï¼Œä»è€Œä½“ç°äº†å¯¹è±¡æ•´ä½“æ€§çš„è®¤çŸ¥æœºåˆ¶ã€‚è¯¥ç ”ç©¶ä½¿ç”¨äº†æ˜ å°„ç®—æ³•ç­‰æ‹“æ‰‘è½¯èšç±»æŠ€æœ¯æ¥è·å¾—æ¨¡æ‹Ÿäººç±»é¢œè‰²æ„ŸçŸ¥çš„MacAdamæ¤­åœ†çš„é¢œè‰²åŒºåŸŸã€‚ç›¸è¾ƒäºTHORçš„å‰ç½®ä¸‰ç»´å½¢çŠ¶ä¸ºåŸºç¡€çš„ç³»ç»Ÿä»¥åŠå¸¸ç”¨çš„æ·±åº¦å­¦ä¹ ç½‘ç»œï¼ŒTHOR2åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯†åˆ«ç²¾åº¦æœ‰æ˜æ˜¾æå‡ã€‚ç‰¹åˆ«æ˜¯åœ¨OCIDæ•°æ®é›†å’Œåæ˜ ä¸åŒç¯å¢ƒæ¡ä»¶å’Œé®æŒ¡ç¨‹åº¦çš„UW-IS Occludedæ•°æ®é›†ä¸­ï¼ŒTHOR2çš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚å› æ­¤ï¼ŒTHOR2æ˜¯æœç€ä½æˆæœ¬æœºå™¨äººå®ç°ç¨³å¥è¯†åˆ«çš„æ–¹å‘è¿ˆå‡ºçš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºä¸‰ç»´å½¢çŠ¶å’Œé¢œè‰²çš„æè¿°ç¬¦TOPS2ï¼Œé€‚ç”¨äºRGB-Då›¾åƒç”Ÿæˆçš„ç‚¹äº‘æ•°æ®ã€‚</li>
<li>TOPS2æè¿°ç¬¦ç»“åˆåˆ‡ç‰‡æŠ€æœ¯ä½“ç°å¯¹è±¡æ•´ä½“æ€§çš„è®¤çŸ¥æœºåˆ¶ï¼ŒåŒæ—¶é€šè¿‡åµŒå…¥æ³•æ•è·å¯¹è±¡é¢œè‰²ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨æ˜ å°„ç®—æ³•ç­‰æ‹“æ‰‘è½¯èšç±»æŠ€æœ¯è·å¾—é¢œè‰²åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸç±»ä¼¼äºäººç±»é¢œè‰²æ„ŸçŸ¥ä¸­çš„MacAdamæ¤­åœ†ã€‚</li>
<li>THOR2åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶å‰èº«THORå’Œå¸¸è§çš„æ·±åº¦å­¦ä¹ ç½‘ç»œã€‚</li>
<li>THOR2åœ¨OCIDå’ŒUW-IS Occludedæ•°æ®é›†ä¸Šçš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œè¿™ä¸¤ä¸ªæ•°æ®é›†åˆ†åˆ«æ¨¡æ‹Ÿäº†ä¸åŒè§†è§’çš„æ‚ä¹±åœºæ™¯å’Œä¸åŒç¯å¢ƒæ¡ä»¶ä¸‹çš„ç›®æ ‡é®æŒ¡æƒ…å†µã€‚</li>
<li>THOR2é€šè¿‡ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ˜¾ç¤ºå‡ºåœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-64227f67575906f8f6db51b6793fdafd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebea8bf78bcbffbe73f07f7fe99632b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62648c7ee5ae36ea0bd91c4dff17f5b0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1><h2 id="ESOD-Efficient-Small-Object-Detection-on-High-Resolution-Images"><a href="#ESOD-Efficient-Small-Object-Detection-on-High-Resolution-Images" class="headerlink" title="ESOD: Efficient Small Object Detection on High-Resolution Images"></a>ESOD: Efficient Small Object Detection on High-Resolution Images</h2><p><strong>Authors:Kai Liu, Zhihang Fu, Sheng Jin, Ze Chen, Fan Zhou, Rongxin Jiang, Yaowu Chen, Jieping Ye</strong></p>
<p>Enlarging input images is a straightforward and effective approach to promote small object detection. However, simple image enlargement is significantly expensive on both computations and GPU memory. In fact, small objects are usually sparsely distributed and locally clustered. Therefore, massive feature extraction computations are wasted on the non-target background area of images. Recent works have tried to pick out target-containing regions using an extra network and perform conventional object detection, but the newly introduced computation limits their final performance. In this paper, we propose to reuse the detectorâ€™s backbone to conduct feature-level object-seeking and patch-slicing, which can avoid redundant feature extraction and reduce the computation cost. Incorporating a sparse detection head, we are able to detect small objects on high-resolution inputs (e.g., 1080P or larger) for superior performance. The resulting Efficient Small Object Detection (ESOD) approach is a generic framework, which can be applied to both CNN- and ViT-based detectors to save the computation and GPU memory costs. Extensive experiments demonstrate the efficacy and efficiency of our method. In particular, our method consistently surpasses the SOTA detectors by a large margin (e.g., 8% gains on AP) on the representative VisDrone, UAVDT, and TinyPerson datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/alibaba/esod">https://github.com/alibaba/esod</a>. </p>
<blockquote>
<p>æ”¾å¤§è¾“å…¥å›¾åƒæ˜¯ä¿ƒè¿›å°ç›®æ ‡æ£€æµ‹çš„ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç®€å•çš„å›¾åƒæ”¾å¤§åœ¨è®¡ç®—å’ŒGPUå†…å­˜æ–¹é¢æˆæœ¬é«˜æ˜‚ã€‚å®é™…ä¸Šï¼Œå°ç›®æ ‡é€šå¸¸ç¨€ç–åˆ†å¸ƒå¹¶å±€éƒ¨èšé›†ã€‚å› æ­¤ï¼Œå¤§é‡çš„ç‰¹å¾æå–è®¡ç®—è¢«æµªè´¹åœ¨éç›®æ ‡èƒŒæ™¯åŒºåŸŸä¸Šã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œè¯•å›¾ä½¿ç”¨é¢å¤–çš„ç½‘ç»œæŒ‘é€‰å‡ºåŒ…å«ç›®æ ‡çš„åŒºåŸŸï¼Œå¹¶è¿›è¡Œå¸¸è§„çš„ç›®æ ‡æ£€æµ‹ï¼Œä½†æ–°å¼•å…¥çš„è®¡ç®—é™åˆ¶äº†å…¶æœ€ç»ˆæ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºé‡ç”¨æ£€æµ‹å™¨çš„éª¨å¹²ç½‘è¿›è¡Œç‰¹å¾çº§çš„ç›®æ ‡æœç´¢å’Œè¡¥ä¸åˆ‡ç‰‡ï¼Œè¿™å¯ä»¥é¿å…å†—ä½™çš„ç‰¹å¾æå–å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚é€šè¿‡ç»“åˆç¨€ç–æ£€æµ‹å¤´ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸Šæ£€æµ‹å°ç›®æ ‡ï¼ˆä¾‹å¦‚1080Pæˆ–æ›´å¤§ï¼‰ä»¥å®ç°å“è¶Šæ€§èƒ½ã€‚æ‰€å¾—çš„Efficient Small Object Detectionï¼ˆESODï¼‰æ–¹æ³•æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯åº”ç”¨äºåŸºäºCNNå’ŒViTçš„æ£€æµ‹å™¨ï¼Œä»¥èŠ‚çœè®¡ç®—å’ŒGPUå†…å­˜æˆæœ¬ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€æµçš„VisDroneã€UAVDTå’ŒTinyPersonæ•°æ®é›†ä¸Šçš„è¡¨ç°å§‹ç»ˆè¶…è¿‡äº†æœ€å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹å™¨ä¸€å¤§æˆªï¼ˆä¾‹å¦‚åœ¨å¹³å‡ç²¾åº¦ä¸Šæé«˜äº†8%ï¼‰ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/alibaba/esod%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alibaba/esodæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.16424v2">PDF</a> This paper has been recerived by IEEE TIP 2024. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/alibaba/esod">https://github.com/alibaba/esod</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨æ£€æµ‹å™¨ä¸»å¹²è¿›è¡Œç‰¹å¾çº§ç›®æ ‡æœç´¢å’Œè¡¥ä¸åˆ‡å‰²çš„æ–¹æ³•ï¼Œæœ‰æ•ˆé¿å…äº†å†—ä½™ç‰¹å¾æå–ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚é€šè¿‡ç»“åˆç¨€ç–æ£€æµ‹å¤´ï¼Œèƒ½åœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸Šæ£€æµ‹å°ç›®æ ‡ï¼Œæå‡æ£€æµ‹æ€§èƒ½ã€‚æå‡ºçš„Efficient Small Object Detectionï¼ˆESODï¼‰æ–¹æ³•é€šç”¨æ€§å¼ºï¼Œå¯åº”ç”¨äºCNNå’ŒViTæ£€æµ‹å™¨ï¼Œæœ‰æ•ˆé™ä½è®¡ç®—æˆæœ¬å’ŒGPUå†…å­˜æ¶ˆè€—ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨VisDroneã€UAVDTå’ŒTinyPersonæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¶…è¿‡ç°æœ‰å…ˆè¿›æ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å°ç›®æ ‡æ£€æµ‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡ç‰¹å¾çº§ç›®æ ‡æœç´¢å’Œè¡¥ä¸åˆ‡å‰²é¿å…å†—ä½™ç‰¹å¾æå–ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>åˆ©ç”¨ç¨€ç–æ£€æµ‹å¤´ï¼Œèƒ½åœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸Šå®ç°å°ç›®æ ‡æ£€æµ‹ï¼Œä»è€Œæå‡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>Efficient Small Object Detectionï¼ˆESODï¼‰æ–¹æ³•æ˜¯ä¸€ç§é€šç”¨æ¡†æ¶ï¼Œå¯åº”ç”¨äºCNNå’ŒViTæ£€æµ‹å™¨ã€‚</li>
<li>ESODæ–¹æ³•æœ‰æ•ˆé™ä½è®¡ç®—æˆæœ¬å’ŒGPUå†…å­˜æ¶ˆè€—ã€‚</li>
<li>åœ¨VisDroneã€UAVDTå’ŒTinyPersonæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒESODæ–¹æ³•è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>ESODæ–¹æ³•é€šè¿‡é‡ç”¨æ£€æµ‹å™¨çš„ä¸»å¹²ç½‘ç»œæ¥å®ç°é«˜æ•ˆçš„å°ç›®æ ‡æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.16424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a60b0ed01ade625070c9fbb5ca055696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-908cfa3c8ea1627021acccf93083ae4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f18cabe98c00361a831a740bf95fe599.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8186842f6822de735cd898a182db38f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3b5163b34289362ef664ddbe47b8969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-599a605ca6f67107234a2245b9185bd9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14a846a868c7bef42a48d51aa53ccb18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db9aeac240970425782a8a29a641ec72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54c3ffda3a833c90d4323d0c084dc698.jpg" align="middle">
</details>


<h1 id="-20"><a href="#-20" class="headerlink" title=""></a></h1><h2 id="Instruct-IPT-All-in-One-Image-Processing-Transformer-via-Weight-Modulation"><a href="#Instruct-IPT-All-in-One-Image-Processing-Transformer-via-Weight-Modulation" class="headerlink" title="Instruct-IPT: All-in-One Image Processing Transformer via Weight   Modulation"></a>Instruct-IPT: All-in-One Image Processing Transformer via Weight   Modulation</h2><p><strong>Authors:Yuchuan Tian, Jianhong Han, Hanting Chen, Yuanyuan Xi, Ning Ding, Jie Hu, Chao Xu, Yunhe Wang</strong></p>
<p>Due to the unaffordable size and intensive computation costs of low-level vision models, All-in-One models that are designed to address a handful of low-level vision tasks simultaneously have been popular. However, existing All-in-One models are limited in terms of the range of tasks and performance. To overcome these limitations, we propose Instruct-IPT â€“ an All-in-One Image Processing Transformer (IPT) that could effectively address manifold image restoration tasks with large inter-task gaps, such as denoising, deblurring, deraining, dehazing, and desnowing. While most research propose feature adaptation methods, we reveal their failure in addressing highly distinct tasks, and suggest weight modulation that adapts weights to specific tasks. Firstly, we search for task-sensitive weights and introduce task-specific biases on top of them. Secondly, we conduct rank analysis for a good compression strategy and perform low-rank decomposition on the biases. Thirdly, we propose synchronous training that updates the task-general backbone model and the task-specific biases simultaneously. In this way, the model is instructed to learn both general and task-specific knowledge. Via our simple yet effective method that instructs the IPT to be task experts, Instruct-IPT could better cooperate between tasks with distinct characteristics at humble costs. As an additional feature, we enable Instruct-IPT to receive human prompts. We have conducted experiments on Instruct-IPT to demonstrate the effectiveness of our method on manifold tasks, and we have effectively extended our method to diffusion denoisers as well. The code is available at <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/Pretrained-IPT">https://github.com/huawei-noah/Pretrained-IPT</a>. </p>
<blockquote>
<p>ç”±äºä½çº§åˆ«è§†è§‰æ¨¡å‹çš„å¤§å°å’Œè®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œæ—¨åœ¨åŒæ—¶è§£å†³å¤šä¸ªä½çº§åˆ«è§†è§‰ä»»åŠ¡çš„å…¨èƒ½æ¨¡å‹å·²ç»å˜å¾—éå¸¸å—æ¬¢è¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…¨èƒ½æ¨¡å‹åœ¨ä»»åŠ¡èŒƒå›´å’Œæ€§èƒ½æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Instruct-IPTâ€”â€”ä¸€ç§å…¨èƒ½å›¾åƒå¤„ç†å˜å‹å™¨ï¼ˆIPTï¼‰ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å…·æœ‰å¤§ä»»åŠ¡é—´å·®è·çš„å¤šç§å›¾åƒæ¢å¤ä»»åŠ¡ï¼Œä¾‹å¦‚å»å™ªã€å»æ¨¡ç³Šã€å»é›¨ã€å»é›¾å’Œå»é›ªã€‚å¤§å¤šæ•°ç ”ç©¶æå‡ºç‰¹å¾è‡ªé€‚åº”æ–¹æ³•ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒä»¬æ— æ³•è§£å†³é«˜åº¦ä¸åŒçš„ä»»åŠ¡ï¼Œå¹¶å»ºè®®é€šè¿‡æƒé‡è°ƒåˆ¶æ¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„æƒé‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœç´¢ä»»åŠ¡æ•æ„Ÿæƒé‡å¹¶åœ¨å®ƒä»¬çš„åŸºç¡€ä¸Šå¼•å…¥ä»»åŠ¡ç‰¹å®šåè§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¿›è¡Œæ’ååˆ†æä»¥åˆ¶å®šè‰¯å¥½çš„å‹ç¼©ç­–ç•¥ï¼Œå¹¶å¯¹åè§è¿›è¡Œä½ç§©åˆ†è§£ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºåŒæ­¥è®­ç»ƒï¼ŒåŒæ—¶æ›´æ–°ä»»åŠ¡é€šç”¨èƒŒæ™¯æ¨¡å‹å’Œä»»åŠ¡ç‰¹å®šåè§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹è¢«æŒ‡å¯¼å­¦ä¹ é€šç”¨å’Œç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ã€‚é€šè¿‡æˆ‘ä»¬ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æŒ‡å¯¼IPTæˆä¸ºä»»åŠ¡ä¸“å®¶ï¼ŒInstruct-IPTå¯ä»¥åœ¨å¾®è–„çš„æˆæœ¬ä¸‹æ›´å¥½åœ°åœ¨å…·æœ‰ä¸åŒç‰¹å¾çš„ä»»åŠ¡ä¹‹é—´è¿›è¡Œåä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿Instruct-IPTèƒ½å¤Ÿæ¥æ”¶äººç±»æç¤ºã€‚æˆ‘ä»¬å·²ç»å¯¹Instruct-IPTè¿›è¡Œäº†å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è¿˜æˆåŠŸåœ°å°†æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•åˆ°æ‰©æ•£å»å™ªå™¨ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/huawei-noah/Pretrained-IPT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/huawei-noah/Pretrained-IPTä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.00676v2">PDF</a> 14 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„å…¨èƒ½å›¾åƒå¤„ç†æ¨¡å‹â€”â€”Instruct-IPTï¼Œè¯¥æ¨¡å‹åŸºäºTransformerç»“æ„ï¼Œæ—¨åœ¨åŒæ—¶å¤„ç†å¤šç§å›¾åƒæ¢å¤ä»»åŠ¡ï¼Œå¦‚å»å™ªã€å»æ¨¡ç³Šã€å»é›¨ã€å»é›¾å’Œé™¤é›ªç­‰ã€‚è¯¥æ¨¡å‹é€šè¿‡æƒé‡è°ƒåˆ¶æŠ€æœ¯é€‚åº”äº†ä¸åŒä»»åŠ¡çš„éœ€æ±‚ï¼Œå¹¶å¼•å…¥ä»»åŠ¡ç‰¹å®šåå·®ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚é€šè¿‡åŒæ­¥è®­ç»ƒï¼Œæ¨¡å‹èƒ½åŒæ—¶å­¦ä¹ é€šç”¨å’Œç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒInstruct-IPTè¿˜èƒ½æ¥æ”¶äººç±»æç¤ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”å·²æœ‰æ•ˆæ‰©å±•åˆ°æ‰©æ•£å»å™ªå™¨ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Instruct-IPTæ˜¯ä¸€ç§å…¨æ–°çš„All-in-Oneå›¾åƒå¤„ç†æ¨¡å‹ï¼Œèƒ½åŒæ—¶å¤„ç†å¤šç§ä½å±‚æ¬¡è§†è§‰ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æƒé‡è°ƒåˆ¶æŠ€æœ¯é€‚åº”ä¸åŒä»»åŠ¡çš„éœ€æ±‚ï¼Œå¼•å…¥ä»»åŠ¡ç‰¹å®šåå·®ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åŒæ­¥è®­ç»ƒï¼ŒInstruct-IPTèƒ½åŒæ—¶å­¦ä¹ é€šç”¨å’Œç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ã€‚</li>
<li>Instruct-IPTèƒ½å¤Ÿæ¥æ”¶äººç±»æç¤ºï¼Œæä¾›é¢å¤–çš„åŠŸèƒ½ã€‚</li>
<li>å®éªŒè¯æ˜Instruct-IPTåœ¨å¤šç§å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ¨¡å‹å·²æœ‰æ•ˆæ‰©å±•åˆ°æ‰©æ•£å»å™ªå™¨ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.00676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3abf9bb250ec904653143ec77e63fd74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be7dfc9c5529e32af3e3f1dce213ceab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7346740a3aedb13f4f0a3e9dc753f4b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c906bfb5e2ec7ab1b2cc1271e1805449.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-784dc08e58c22838a772153af25120fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4ba15257b87bb46f3d778aa342604a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fc6a0fabe5ffbe69ece38ec9f3ce012.jpg" align="middle">
</details>


<h1 id="-21"><a href="#-21" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-19/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-341bda71b5849f8db8e8fbfcf65e57fa.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Prompt Categories Cluster for Weakly Supervised Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3cd5b11aadac42b7c5cdf1fbae433acc.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  FocusChat Text-guided Long Video Understanding via Spatiotemporal   Information Filtering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19017.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
