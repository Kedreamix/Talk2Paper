<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  GeRe Towards Efficient Anti-Forgetting in Continual Learning of LLM via   General Samples Replay">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-71fc6b270c5b319f2b5cb5d4c1013b21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-08-æ›´æ–°"><a href="#2025-08-08-æ›´æ–°" class="headerlink" title="2025-08-08 æ›´æ–°"></a>2025-08-08 æ›´æ–°</h1><h2 id="GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay"><a href="#GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay" class="headerlink" title="GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via   General Samples Replay"></a>GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via   General Samples Replay</h2><p><strong>Authors:Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen</strong></p>
<p>The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concernsâ€“retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1&#x2F;L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Qznan/GeRe">https://github.com/Qznan/GeRe</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒç»­å­¦ä¹ èƒ½åŠ›å¯¹äºæ¨è¿›äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨ä¸åŒé¢†åŸŸè¿›è¡ŒLLMçš„æŒç»­å¾®è°ƒå¸¸å¸¸ä¼šé‡åˆ°ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼Œå…¶ç‰¹å¾è¡¨ç°ä¸ºï¼š1ï¼‰é€šç”¨èƒ½åŠ›çš„æ˜¾è‘—é—å¿˜ï¼›2ï¼‰å·²å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚ä¸ºäº†ä»¥ä¸€ç§ç®€å•è€Œç¨³å®šçš„æ–¹å¼åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨æ ·æœ¬å›æ”¾ï¼ˆGeReï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å¸¸è§„çš„é¢„è®­ç»ƒæ–‡æœ¬è¿›è¡Œé«˜æ•ˆçš„æŠ—é—å¿˜ã€‚é™¤äº†å›é¡¾GeReä¸‹æœ€å¸¸è§çš„å›æ”¾å®è·µä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥åˆ©ç”¨ç¥ç»çŠ¶æ€ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºé˜ˆå€¼è¾¹ç•Œï¼ˆTMï¼‰æŸå¤±çš„æ¿€æ´»çŠ¶æ€çº¦æŸä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å›æ”¾å­¦ä¹ è¿‡ç¨‹ä¸­ä¿æŒæ¿€æ´»çŠ¶æ€ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æ˜¯é¦–æ¬¡éªŒè¯ï¼Œä¸€å°æ‰¹é¢„å…ˆæ”¶é›†çš„é€šç”¨å›æ”¾æ ·æœ¬è¶³ä»¥è§£å†³è¿™ä¸¤æ–¹é¢çš„é—®é¢˜â€”â€”åœ¨ä¿ç•™é€šç”¨èƒ½åŠ›çš„åŒæ—¶ï¼Œä¿ƒè¿›å„é¡¹ä»»åŠ¡çš„æ€»ä½“æ€§èƒ½ã€‚å®é™…ä¸Šï¼Œå‰è€…å¯ä»¥å†…åœ¨ä¿ƒè¿›åè€…ã€‚é€šè¿‡æ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬åœ¨GeReæ¡†æ¶ä¸‹å°†TMä¸ä¸åŒçš„å›æ”¾ç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿæ¯”è¾ƒï¼ŒåŒ…æ‹¬æ™®é€šæ ‡ç­¾æ‹Ÿåˆã€é€šè¿‡KLæ•£åº¦å®ç°é€»è¾‘æ¨¡ä»¿ä»¥åŠé€šè¿‡L1&#x2F;L2æŸå¤±å®ç°ç‰¹å¾æ¨¡ä»¿ã€‚ç»“æœè¡¨æ˜ï¼ŒTMåœ¨æ€§èƒ½ä¸Šè¡¨ç°æ›´ç¨³å®šä¸”æ›´æŒä¹…ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºLLMçš„æœ‰æ•ˆå›æ”¾é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Qznan/GeRe%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Qznan/GeReæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04676v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒç»­å­¦ä¹ èƒ½åŠ›å¯¹æ¨è¿›äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè·¨åŸŸæŒç»­å¾®è°ƒLLMæ—¶å¸¸é­é‡ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œè¡¨ç°ä¸ºä¸€èˆ¬èƒ½åŠ›çš„æ˜¾è‘—é—å¿˜ä»¥åŠå…ˆå‰å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç®€å•ç¨³å®šçš„é€šç”¨æ ·æœ¬å›æ”¾ï¼ˆGeReï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨å¸¸è§„é¢„è®­ç»ƒæ–‡æœ¬è¿›è¡Œæœ‰æ•ˆæŠ—é—å¿˜ã€‚é™¤å›é¡¾GeReä¸‹çš„æœ€å¸¸è§å›æ”¾å®è·µå¤–ï¼Œæˆ‘ä»¬è¿˜å€ŸåŠ©ç¥ç»çŠ¶æ€ï¼Œå¼•å…¥ä½¿ç”¨é˜ˆå€¼è¾¹ç•Œï¼ˆTMï¼‰æŸå¤±çš„æ¿€æ´»çŠ¶æ€çº¦æŸä¼˜åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬æ˜¯é¦–æ‰¹éªŒè¯è€…ï¼Œè¯æ˜ä¸€å°éƒ¨åˆ†é¢„å…ˆæ”¶é›†çš„é€šç”¨å›æ”¾æ ·æœ¬è¶³ä»¥è§£å†³è¿™ä¸¤ä¸ªå…³æ³¨ç‚¹â€”â€”ä¿ç•™ä¸€èˆ¬èƒ½åŠ›çš„åŒæ—¶ï¼Œä¿ƒè¿›è·¨åºè´¯ä»»åŠ¡çš„æ€»ä½“æ€§èƒ½ã€‚å®é™…ä¸Šï¼Œå‰è€…å¯ä»¥å›ºæœ‰åœ°ä¿ƒè¿›åè€…ã€‚é€šè¿‡å—æ§å®éªŒï¼Œæˆ‘ä»¬åœ¨GeReæ¡†æ¶ä¸‹ç³»ç»Ÿåœ°æ¯”è¾ƒäº†TMä¸ä¸åŒçš„å›æ”¾ç­–ç•¥ï¼ŒåŒ…æ‹¬æ™®é€šæ ‡ç­¾æ‹Ÿåˆã€é€šè¿‡KLæ•£åº¦å®ç°é€»è¾‘æ¨¡ä»¿ä»¥åŠé€šè¿‡L1&#x2F;L2æŸå¤±å®ç°ç‰¹å¾æ¨¡ä»¿ã€‚ç»“æœè¡¨æ˜ï¼ŒTMåœ¨æ€§èƒ½å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæœªæ¥LLMçš„æœ‰æ•ˆå›æ”¾é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒç»­å­¦ä¹ èƒ½åŠ›å¯¹æ¨è¿›äººå·¥æ™ºèƒ½è‡³å…³é‡è¦ã€‚</li>
<li>LLMåœ¨è·¨åŸŸæŒç»­å¾®è°ƒä¸­é¢ä¸´ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œè¡¨ç°ä¸ºä¸€èˆ¬èƒ½åŠ›çš„é—å¿˜å’Œå…ˆå‰ä»»åŠ¡æ€§èƒ½çš„ä¸‹é™ã€‚</li>
<li>é€šç”¨æ ·æœ¬å›æ”¾ï¼ˆGeReï¼‰æ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œåˆ©ç”¨å¸¸è§„é¢„è®­ç»ƒæ–‡æœ¬è¿›è¡ŒæŠ—é—å¿˜ã€‚</li>
<li>GeReæ¡†æ¶ç»“åˆäº†å›æ”¾ç­–ç•¥å’Œç¥ç»çŠ¶æ€ï¼Œå¼•å…¥ä½¿ç”¨é˜ˆå€¼è¾¹ç•Œï¼ˆTMï¼‰æŸå¤±çš„æ¿€æ´»çŠ¶æ€çº¦æŸä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>é¢„å…ˆæ”¶é›†çš„é€šç”¨å›æ”¾æ ·æœ¬å¯æœ‰æ•ˆè§£å†³é—å¿˜é—®é¢˜ï¼ŒåŒæ—¶ä¿ƒè¿›è·¨åºè´¯ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼ŒTMæŸå¤±åœ¨å›æ”¾ç­–ç•¥ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ec4a9a057b6518d5d14a0e7a1c0f78d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca73db749c6591ab3e02866dff9ae474.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e8431597659c7f46175cec5d0184751.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-390dfe10d7aef5bbe6632a19dff9ce64.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="X-SAM-From-Segment-Anything-to-Any-Segmentation"><a href="#X-SAM-From-Segment-Anything-to-Any-Segmentation" class="headerlink" title="X-SAM: From Segment Anything to Any Segmentation"></a>X-SAM: From Segment Anything to Any Segmentation</h2><p><strong>Authors:Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, Xiaodan Liang</strong></p>
<p>Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \textit{segment anything} to \textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wanghao9610/X-SAM">https://github.com/wanghao9610/X-SAM</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹¿æ³›çš„çŸ¥è¯†è¡¨ç¤ºæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç„¶è€Œå®ƒä»¬åœ¨åƒç´ çº§åˆ«çš„æ„ŸçŸ¥ç†è§£æ–¹é¢å­˜åœ¨å›ºæœ‰ç¼ºé™·ã€‚å°½ç®¡Segment Anything Modelï¼ˆSAMï¼‰åœ¨è§†è§‰æç¤ºé©±åŠ¨çš„å›¾åƒåˆ†å‰²æ–¹é¢ä»£è¡¨äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å¤šæ©è†œé¢„æµ‹å’Œç‰¹å®šç±»åˆ«åˆ†å‰²ä»»åŠ¡æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ï¼Œå¹¶ä¸”å®ƒæ— æ³•åœ¨ç»Ÿä¸€çš„æ¨¡å‹æ¶æ„ä¸­é›†æˆæ‰€æœ‰çš„åˆ†å‰²ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†X-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œå®ƒå°†åˆ†å‰²èŒƒå¼ä»â€œåˆ†å‰²ä»»ä½•äº‹ç‰©â€æ‰©å±•åˆ°â€œä»»ä½•åˆ†å‰²â€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ç»Ÿä¸€æ¡†æ¶ï¼Œä½¿MLLMå…·å¤‡æ›´å…ˆè¿›çš„åƒç´ çº§åˆ«æ„ŸçŸ¥ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²ä»»åŠ¡ï¼Œç§°ä¸ºVisual GrounDedï¼ˆVGDï¼‰åˆ†å‰²ï¼Œå®ƒé€šè¿‡äº¤äº’å¼çš„è§†è§‰æç¤ºå¯¹æ‰€æœ‰å®ä¾‹å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œå¹¶ä¸ºMLLMæä¾›è§†è§‰åŸºç¡€ã€åƒç´ çº§çš„è§£é‡Šèƒ½åŠ›ã€‚ä¸ºäº†åœ¨ä¸åŒçš„æ•°æ®æºä¸Šè¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¯æŒè·¨å¤šä¸ªæ•°æ®é›†è¿›è¡Œè”åˆè®­ç»ƒçš„ç»Ÿä¸€è®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-SAMåœ¨å¹¿æ³›çš„å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œçªæ˜¾äº†å…¶åœ¨å¤šæ¨¡æ€ã€åƒç´ çº§åˆ«çš„è§†è§‰ç†è§£æ–¹é¢çš„æ•ˆç‡ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/wanghao9610/X-SAM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wanghao9610/X-SAMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04655v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å¹¿æ³›çš„çŸ¥è¯†è¡¨ç¤ºèƒ½åŠ›ï¼Œä½†åœ¨åƒç´ çº§åˆ«çš„æ„ŸçŸ¥ç†è§£ä¸Šå­˜åœ¨å›ºæœ‰ç¼ºé™·ã€‚Segment Anything Modelï¼ˆSAMï¼‰åœ¨è§†è§‰æç¤ºé©±åŠ¨çš„å›¾åƒåˆ†å‰²ä¸Šå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨å¤šæ©è†œé¢„æµ‹å’Œç‰¹å®šç±»åˆ«åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜æ˜¾å±€é™ï¼Œä¸”æ— æ³•å°†æ‰€æœ‰åˆ†å‰²ä»»åŠ¡æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¶æ„ä¸­ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†X-SAMï¼Œä¸€ä¸ªç®€åŒ–çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œå°†åˆ†å‰²èŒƒå¼ä»â€œåˆ†å‰²ä»»ä½•ä¸œè¥¿â€æ‰©å±•åˆ°â€œä»»ä½•åˆ†å‰²â€ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ç»Ÿä¸€æ¡†æ¶ï¼Œä¸ºMLLMæä¾›æ›´å…ˆè¿›çš„åƒç´ çº§åˆ«æ„ŸçŸ¥ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²ä»»åŠ¡ï¼Œç§°ä¸ºVisual GrounDedï¼ˆVGDï¼‰åˆ†å‰²ï¼Œé€šè¿‡äº¤äº’è§†è§‰æç¤ºå¯¹å®ä¾‹å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œå¹¶ä¸ºMLLMæä¾›è§†è§‰æ¥åœ°ã€åƒç´ çº§çš„è§£é‡Šèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨å¤šä¸ªæ•°æ®é›†çš„è”åˆè®­ç»ƒç­–ç•¥ï¼Œä»¥å®ç°ä¸åŒæ•°æ®æºçš„æœ‰æ•ˆè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-SAMåœ¨å¹¿æ³›çš„å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨å¤šæ¨¡æ€ã€åƒç´ çº§åˆ«çš„è§†è§‰ç†è§£ä¸Šçš„æ•ˆç‡ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsè™½åœ¨çŸ¥è¯†è¡¨ç¤ºæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨åƒç´ çº§åˆ«çš„æ„ŸçŸ¥ç†è§£ä¸Šå­˜åœ¨å±€é™ã€‚</li>
<li>Segment Anything Model (SAM)åœ¨è§†è§‰æç¤ºé©±åŠ¨çš„å›¾åƒåˆ†å‰²ä¸Šæœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨å¤šæ©è†œé¢„æµ‹å’Œç‰¹å®šç±»åˆ«åˆ†å‰²çš„å±€é™ã€‚</li>
<li>X-SAMæ˜¯ä¸€ä¸ªç®€åŒ–çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œæ‰©å±•äº†åˆ†å‰²èŒƒå¼ï¼Œå¼•å…¥äº†æ›´å…ˆè¿›çš„åƒç´ çº§åˆ«æ„ŸçŸ¥ç†è§£ã€‚</li>
<li>X-SAMé€šè¿‡äº¤äº’è§†è§‰æç¤ºè¿›è¡Œå®ä¾‹å¯¹è±¡åˆ†å‰²ï¼Œå¹¶èµ‹äºˆMLLMè§†è§‰æ¥åœ°ã€åƒç´ çº§çš„è§£é‡Šèƒ½åŠ›ã€‚</li>
<li>X-SAMé‡‡ç”¨ç»Ÿä¸€æ¡†æ¶æ•´åˆå¤šç§åˆ†å‰²ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²ä»»åŠ¡â€”â€”Visual GrounDed (VGD) åˆ†å‰²ã€‚</li>
<li>X-SAMé€šè¿‡è·¨å¤šä¸ªæ•°æ®é›†çš„è”åˆè®­ç»ƒç­–ç•¥å®ç°æœ‰æ•ˆè®­ç»ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aa03d7c941ccccbae5e6a17bb0b5fa64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e2bf61ea193368bfa3459854119e662.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82c70a87a82b701346d90ed1089e4ca9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b85043867b29ab1939f1d20e4cb4cca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-723b17fc21da469ca52a23900c55c91d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RoboTron-Sim-Improving-Real-World-Driving-via-Simulated-Hard-Case"><a href="#RoboTron-Sim-Improving-Real-World-Driving-via-Simulated-Hard-Case" class="headerlink" title="RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case"></a>RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case</h2><p><strong>Authors:Baihui Xiao, Chengjian Feng, Zhijian Huang, Feng yan, Yujie Zhong, Lin Ma</strong></p>
<p>Collecting real-world data for rare high-risk scenarios, long-tailed driving events, and complex interactions remains challenging, leading to poor performance of existing autonomous driving systems in these critical situations. In this paper, we propose RoboTron-Sim that improves real-world driving in critical situations by utilizing simulated hard cases. First, we develop a simulated dataset called Hard-case Augmented Synthetic Scenarios (HASS), which covers 13 high-risk edge-case categories, as well as balanced environmental conditions such as day&#x2F;night and sunny&#x2F;rainy. Second, we introduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder (I2E Encoder) to enable multimodal large language models to effectively learn real-world challenging driving skills from HASS, via adapting to environmental deviations and hardware differences between real-world and simulated scenarios. Extensive experiments on nuScenes show that RoboTron-Sim improves driving performance in challenging scenarios by around 50%, achieving state-of-the-art results in real-world open-loop planning. Qualitative results further demonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk driving scenarios. Project page: <a target="_blank" rel="noopener" href="https://stars79689.github.io/RoboTron-Sim/">https://stars79689.github.io/RoboTron-Sim/</a> </p>
<blockquote>
<p>æ”¶é›†çœŸå®ä¸–ç•Œæ•°æ®å¯¹äºç½•è§é«˜é£é™©åœºæ™¯ã€é•¿å°¾é©¾é©¶äº‹ä»¶å’Œå¤æ‚äº¤äº’ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯¼è‡´ç°æœ‰è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨è¿™äº›å…³é”®æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºRoboTron-Simï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ¨¡æ‹Ÿçš„å›°éš¾æ¡ˆä¾‹æ¥æ”¹å–„å…³é”®æƒ…å†µä¸‹çš„ç°å®ä¸–ç•Œé©¾é©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºHard-case Augmented Synthetic Scenariosï¼ˆHASSï¼‰çš„æ¨¡æ‹Ÿæ•°æ®é›†ï¼Œæ¶µç›–äº†13ç±»é«˜é£é™©è¾¹ç¼˜æ¡ˆä¾‹ï¼Œä»¥åŠå¦‚æ—¥å¤œã€æ™´é›¨ç­‰å¹³è¡¡çš„ç¯å¢ƒæ¡ä»¶ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†æƒ…æ™¯æ„ŸçŸ¥æç¤ºå·¥ç¨‹ï¼ˆSPEï¼‰å’Œå›¾åƒåˆ°è‡ªæˆ‘ç¼–ç å™¨ï¼ˆI2E Encoderï¼‰ï¼Œä»¥ä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é€‚åº”ç¯å¢ƒåå·®å’Œç°å®ä¸–ç•Œä¸æ¨¡æ‹Ÿåœºæ™¯ä¹‹é—´çš„ç¡¬ä»¶å·®å¼‚ï¼Œä»HASSæœ‰æ•ˆå­¦ä¹ ç°å®ä¸–ç•Œä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„é©¾é©¶æŠ€èƒ½ã€‚åœ¨nuScenesä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRoboTron-Simå°†æŒ‘æˆ˜åœºæ™¯ä¸­çš„é©¾é©¶æ€§èƒ½æé«˜äº†çº¦50%ï¼Œåœ¨ç°å®ä¸–ç•Œå¼€æ”¾å¾ªç¯è§„åˆ’ä¸­å®ç°äº†æœ€æ–°ç»“æœã€‚å®šæ€§ç»“æœè¿›ä¸€æ­¥è¯æ˜äº†RoboTron-Simåœ¨æ›´å¥½åœ°ç®¡ç†ç½•è§é«˜é£é™©é©¾é©¶åœºæ™¯æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://stars79689.github.io/RoboTron-Sim/">https://stars79689.github.io/RoboTron-Sim/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04642v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æ¨¡æ‹Ÿæ•°æ®é›†RoboTron-Simçš„æå‡ºï¼Œè§£å†³äº†çœŸå®ä¸–ç•Œé«˜é£é™©åœºæ™¯æ•°æ®æ”¶é›†å›°éš¾çš„é—®é¢˜ï¼Œé€šè¿‡åˆ›å»ºé«˜é£é™©çš„æ¨¡æ‹Ÿåœºæ™¯ï¼ˆå¦‚è¾¹ç¼˜æƒ…å†µï¼‰å¹¶å¼•å…¥åœºæ™¯æ„ŸçŸ¥æç¤ºå·¥ç¨‹å’Œå›¾åƒåˆ°è‡ªæˆ‘ç¼–ç å™¨æŠ€æœ¯ï¼Œä½¿å¾—è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨é¢ä¸´çœŸå®ä¸–ç•Œçš„å¤æ‚æŒ‘æˆ˜æ—¶è¡¨ç°æ›´ä¼˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æŠ€æœ¯èƒ½æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸­çš„æ€§èƒ½çº¦50%ï¼Œè¾¾åˆ°ç°å®å¼€æ”¾ç¯å¢ƒä¸­è§„åˆ’çš„æœ€ä¼˜æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡æ‹Ÿæ•°æ®é›†RoboTron-Simèƒ½å¤Ÿè¦†ç›–å¤šç§é«˜é£é™©è¾¹ç¼˜åœºæ™¯ï¼Œå¹¶æ¨¡æ‹Ÿä¸åŒç¯å¢ƒæ¡ä»¶å¦‚æ—¥å¤œäº¤æ›¿å’Œå¤©æ°”å˜åŒ–ã€‚</li>
<li>åˆ©ç”¨åœºæ™¯æ„ŸçŸ¥æç¤ºå·¥ç¨‹ï¼ˆSPEï¼‰å’Œå›¾åƒåˆ°è‡ªæˆ‘ç¼–ç å™¨ï¼ˆI2E Encoderï¼‰æŠ€æœ¯ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¤„ç†æ¨¡æ‹Ÿçš„é«˜é£é™©é©¾é©¶åœºæ™¯ã€‚</li>
<li>è¯¥æŠ€æœ¯é€šè¿‡é€‚åº”ç¯å¢ƒå˜åŒ–å’Œç¡¬ä»¶å·®å¼‚ï¼Œä½¿å¾—è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤„ç†çœŸå®ä¸–ç•Œçš„å¤æ‚é©¾é©¶æŠ€èƒ½æ—¶æ€§èƒ½æ›´ä¼˜ã€‚</li>
<li>åœ¨nuScenesä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRoboTron-SimæŠ€æœ¯æé«˜äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸­çš„æ€§èƒ½çº¦50%ã€‚</li>
<li>è¯¥æŠ€æœ¯è¾¾åˆ°äº†ç°å®å¼€æ”¾ç¯å¢ƒä¸­è§„åˆ’çš„æœ€ä¼˜æ°´å¹³ï¼Œå®ç°äº†å®šæ€§ç»“æœçš„æœ‰æ•ˆå±•ç¤ºã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†è¯¦ç»†çš„èƒŒæ™¯ä¿¡æ¯å’Œç ”ç©¶æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-16785dba3ee35fdc553b8c57abe2e77c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b5538b275d2c6109cb848eb5b65060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56bfbd5a71ee5bc111a607831b8c6ceb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6da1d48f89ce58296d644a6102442fa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eabf5055c745e2abb6124cd0fbd77ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0bf57fc4dba3b1dd065efb3a7fab418.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="P-Aligner-Enabling-Pre-Alignment-of-Language-Models-via-Principled-Instruction-Synthesis"><a href="#P-Aligner-Enabling-Pre-Alignment-of-Language-Models-via-Principled-Instruction-Synthesis" class="headerlink" title="P-Aligner: Enabling Pre-Alignment of Language Models via Principled   Instruction Synthesis"></a>P-Aligner: Enabling Pre-Alignment of Language Models via Principled   Instruction Synthesis</h2><p><strong>Authors:Feifan Song, Bofei Gao, Yifan Song, Yi Liu, Weimin Xiong, Yuyang Song, Tianyu Liu, Guoyin Wang, Houfeng Wang</strong></p>
<p>Large Language Models (LLMs) are expected to produce safe, helpful, and honest content during interaction with human users, but they frequently fail to align with such values when given flawed instructions, e.g., missing context, ambiguous directives, or inappropriate tone, leaving substantial room for improvement along multiple dimensions. A cost-effective yet high-impact way is to pre-align instructions before the model begins decoding. Existing approaches either rely on prohibitive test-time search costs or end-to-end model rewrite, which is powered by a customized training corpus with unclear objectives. In this work, we demonstrate that the goal of efficient and effective preference alignment can be achieved by P-Aligner, a lightweight module generating instructions that preserve the original intents while being expressed in a more human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree Search, which systematically explores the space of candidate instructions that are closely tied to human preference. Experiments across different methods show that P-Aligner generally outperforms strong baselines across various models and benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness and efficiency through multiple perspectives, including data quality, search strategies, iterative deployment, and time overhead. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸äººç±»ç”¨æˆ·äº¤äº’æ—¶ï¼Œé¢„æœŸä¼šäº§ç”Ÿå®‰å…¨ã€æœ‰ç”¨ã€è¯šå®çš„å†…å®¹ã€‚ç„¶è€Œï¼Œå½“ç»™äºˆæœ‰ç¼ºé™·çš„æŒ‡ä»¤æ—¶ï¼Œä¾‹å¦‚ç¼ºå°‘ä¸Šä¸‹æ–‡ã€æŒ‡ä»¤æ¨¡ç³Šæˆ–è¯­æ°”ä¸å½“ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•ä¸è¿™äº›ä»·å€¼å¯¹é½ï¼Œè¿™åœ¨å¤šä¸ªç»´åº¦ä¸Šä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜ä¸”å½±å“å¤§çš„æ–¹æ³•æ˜¯ï¼Œåœ¨æ¨¡å‹å¼€å§‹è§£ç ä¹‹å‰é¢„å…ˆå¯¹é½æŒ‡ä»¤ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆä¾èµ–äºç¦æ­¢çš„æµ‹è¯•æ—¶é—´æœç´¢æˆæœ¬ï¼Œè¦ä¹ˆä¾èµ–äºç«¯åˆ°ç«¯çš„æ¨¡å‹é‡å†™ï¼Œè¿™ç”±å¸¦æœ‰ä¸æ˜ç¡®ç›®æ ‡çš„å®šåˆ¶è®­ç»ƒè¯­æ–™åº“æä¾›æ”¯æŒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†P-Alignerå¯ä»¥å®ç°é«˜æ•ˆå’Œæœ‰æ•ˆçš„åå¥½å¯¹é½çš„ç›®æ ‡ã€‚P-Aligneræ˜¯ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œèƒ½å¤Ÿç”Ÿæˆåœ¨ä¿ç•™åŸå§‹æ„å›¾çš„åŒæ—¶ä»¥äººç±»æ›´åå¥½çš„å½¢å¼è¡¨è¾¾çš„æŒ‡ä»¤ã€‚P-Aligneræ˜¯åœ¨UltraPromptä¸Šè®­ç»ƒçš„ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æå‡ºçš„åŸåˆ™å¼•å¯¼ç®¡é“åˆæˆçš„æ–°æ•°æ®é›†ï¼Œè¯¥ç®¡é“ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ç³»ç»Ÿåœ°æ¢ç´¢ä¸äººç±»åå¥½ç´§å¯†ç›¸å…³çš„å€™é€‰æŒ‡ä»¤ç©ºé—´ã€‚åœ¨ä¸åŒæ–¹æ³•å’Œæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒP-Aligneré€šå¸¸ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬åœ¨GPT-4-turboå’ŒGemma-2-SimPOä¸Šçš„å¹³å‡èƒœç‡åˆ†åˆ«æé«˜äº†28.35%å’Œ8.69%ã€‚è¿›ä¸€æ­¥çš„åˆ†æä»æ•°æ®è´¨é‡ã€æœç´¢ç­–ç•¥ã€è¿­ä»£éƒ¨ç½²å’Œæ—¶é—´å¼€é”€ç­‰å¤šä¸ªè§’åº¦éªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04626v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨å¤„ç†ä¸ç”¨æˆ·äº¤äº’æ—¶çš„å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§æ–¹é¢å­˜åœ¨é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬åœ¨å¤„ç†å…·æœ‰ç¼ºé™·çš„æŒ‡ä»¤ï¼ˆå¦‚ç¼ºä¹ä¸Šä¸‹æ–‡ã€æŒ‡ä»¤æ¨¡ç³Šæˆ–ä¸æ°å½“çš„è¯­æ°”ï¼‰æ—¶æ— æ³•å¯¹é½ç›¸åº”çš„ä»·å€¼è§‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„åå¥½å¯¹é½æ–¹æ³•â€”â€”P-Alignerï¼Œå®ƒæ˜¯ä¸€ç§è½»é‡çº§æ¨¡å—ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¤šç¬¦åˆäººç±»è¡¨è¾¾ä¹ æƒ¯çš„æŒ‡ä»¤ã€‚è¯¥æ¨¡å—åœ¨UltraPromptæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æå‡ºçš„åŸç†å¼•å¯¼ç®¡é“åˆæˆçš„æ•°æ®é›†ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ç³»ç»Ÿåœ°æ¢ç´¢ä¸äººç±»åå¥½ç´§å¯†ç›¸å…³çš„å€™é€‰æŒ‡ä»¤ç©ºé—´ã€‚å®éªŒè¡¨æ˜ï¼ŒP-Aligneråœ¨å„ç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬åœ¨GPT-4-turboå’ŒGemma-2-SimPOä¸Šçš„å¹³å‡èƒœç‡åˆ†åˆ«æé«˜äº†28.35%å’Œ8.69%ã€‚è¿›ä¸€æ­¥çš„åˆ†æä»æ•°æ®è´¨é‡ã€æœç´¢ç­–ç•¥ã€è¿­ä»£éƒ¨ç½²å’Œæ—¶é—´å¼€é”€ç­‰å¤šä¸ªè§’åº¦éªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†å…·æœ‰ç¼ºé™·çš„æŒ‡ä»¤æ—¶å­˜åœ¨å¯¹é½é—®é¢˜ã€‚</li>
<li>P-Aligneræ˜¯ä¸€ç§è½»é‡çº§æ¨¡å—ï¼Œæ—¨åœ¨ç”Ÿæˆç¬¦åˆäººç±»è¡¨è¾¾ä¹ æƒ¯çš„æŒ‡ä»¤ä»¥æé«˜å®‰å…¨æ€§ã€æœ‰ç”¨æ€§å’Œè¯šå®æ€§ã€‚</li>
<li>P-Aligneråœ¨UltraPromptæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢åˆæˆçš„æ•°æ®é›†ã€‚</li>
<li>P-Aligneråœ¨å„ç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
<li>P-Aligneråœ¨GPT-4-turboå’ŒGemma-2-SimPOä¸Šçš„å¹³å‡èƒœç‡åˆ†åˆ«æé«˜äº†28.35%å’Œ8.69%ã€‚</li>
<li>P-Alignerçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡é€šè¿‡æ•°æ®è´¨é‡ã€æœç´¢ç­–ç•¥ã€è¿­ä»£éƒ¨ç½²å’Œæ—¶é—´å¼€é”€ç­‰å¤šä¸ªè§’åº¦å¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-071d0c8bf0276d02acfc5c64a4d4af4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f70059cf575e318044097e9670123f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34342131ac3dbe40d727bef43d84410a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4307f5d7cac61b6ce2f64738a5ccedb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eabae18dc8fe52b32797d51799083707.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d765631d61e497acff5a46394b31937e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FinMMR-Make-Financial-Numerical-Reasoning-More-Multimodal-Comprehensive-and-Challenging"><a href="#FinMMR-Make-Financial-Numerical-Reasoning-More-Multimodal-Comprehensive-and-Challenging" class="headerlink" title="FinMMR: Make Financial Numerical Reasoning More Multimodal,   Comprehensive, and Challenging"></a>FinMMR: Make Financial Numerical Reasoning More Multimodal,   Comprehensive, and Challenging</h2><p><strong>Authors:Zichen Tang, Haihong E, Jiacheng Liu, Zhongjun Yang, Rongjin Li, Zihua Rong, Haoyang He, Zhuodi Hao, Xinyang Hu, Kun Ji, Ziyan Ma, Mengyuan Ji, Jun Zhang, Chenghao Ma, Qianhe Zheng, Yang Liu, Yiling Huang, Xinyi Hu, Qing Huang, Zijian Xie, Shiyao Peng</strong></p>
<p>We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†FinMMRï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èæ•°å€¼æ¨ç†ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¯„ä¼°è€Œé‡èº«å®šåˆ¶çš„æ–°å‹åŒè¯­å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†ä¸‰ä¸ªé‡è¦è¿›å±•ã€‚(1) å¤šæ¨¡æ€ï¼šæˆ‘ä»¬ç²¾å¿ƒæ”¹é€ äº†ç°æœ‰çš„é‡‘èæ¨ç†åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ ¹æ®æœ€æ–°çš„ä¸­æ–‡é‡‘èç ”ç©¶æŠ¥å‘Šæ„å»ºäº†æ–°é—®é¢˜ã€‚FinMMRåŒ…å«4.3Kä¸ªé—®é¢˜å’Œ8.7Kä¸ªå›¾åƒï¼Œæ¶µç›–14ä¸ªç±»åˆ«ï¼ŒåŒ…æ‹¬è¡¨æ ¼ã€æ¡å½¢å›¾å’Œæ‰€æœ‰æƒç»“æ„å›¾ç­‰ã€‚(2) å…¨é¢æ€§ï¼šFinMMRæ¶µç›–äº†åŒ…æ‹¬ä¼ä¸šè´¢åŠ¡ã€é“¶è¡Œä¸šå’Œäº§ä¸šåˆ†æç­‰åœ¨å†…çš„14ä¸ªé‡‘èå­é¢†åŸŸï¼Œåœ¨è´¢åŠ¡é¢†åŸŸçŸ¥è¯†çš„å¹¿åº¦ä¸Šæ˜¾è‘—è¶…è¿‡äº†ç°æœ‰åŸºå‡†æµ‹è¯•ã€‚(3) æŒ‘æˆ˜æ€§ï¼šæ¨¡å‹éœ€è¦é€šè¿‡æ•´åˆé‡‘èçŸ¥è¯†ä»¥åŠå¯¹å¤æ‚çš„é‡‘èå›¾åƒå’Œæ–‡æœ¬çš„ç†è§£ï¼Œæ¥æ‰§è¡Œå¤šæ­¥éª¤ç²¾ç¡®æ•°å€¼æ¨ç†ã€‚åœ¨éš¾é¢˜ä¸Šï¼Œè¡¨ç°æœ€ä½³çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å‡†ç¡®ç‡ä»…ä¸º53.0%ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒFinMMRå°†æ¨åŠ¨åœ¨æé«˜ç°å®ä¸–ç•Œåœºæ™¯ä¸­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04625v1">PDF</a> Accepted by ICCV 2025. arXiv admin note: text overlap with   arXiv:2311.06602 by other authors</p>
<p><strong>Summary</strong></p>
<p>FinMMRæ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èæ•°å€¼æ¨ç†ä»»åŠ¡çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸è¾ƒäºç°æœ‰åŸºå‡†æµ‹è¯•ï¼ŒFinMMRæœ‰ä¸‰é¡¹æ˜¾è‘—è¿›æ­¥ï¼šåŒ…æ‹¬å¤šæ¨¡æ€ç‰¹æ€§ã€æ¶µç›–å¹¿æ³›çš„é‡‘èå­é¢†åŸŸä»¥åŠæŒ‘æˆ˜æ€§ã€‚å®ƒè¦æ±‚æ¨¡å‹ç»“åˆé‡‘èçŸ¥è¯†å¯¹å¤æ‚é‡‘èå›¾åƒå’Œæ–‡æœ¬è¿›è¡Œå¤šæ­¥éª¤ç²¾ç¡®æ•°å€¼æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FinMMRæ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èæ•°å€¼æ¨ç†çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œä¸“ä¸ºè¯„ä¼°MLLMsçš„æ¨ç†èƒ½åŠ›è€Œè®¾è®¡ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä¸‰é¡¹æ˜¾è‘—ç‰¹ç‚¹ï¼šå¤šæ¨¡æ€ç‰¹æ€§ã€æ¶µç›–å¹¿æ³›çš„é‡‘èå­é¢†åŸŸä»¥åŠæŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚</li>
<li>FinMMRåŒ…å«4.3Kä¸ªé—®é¢˜å’Œ8.7Kä¸ªå›¾åƒï¼Œæ¶‰åŠè¡¨æ ¼ã€æ¡å½¢å›¾ç­‰14ç§ç±»å‹ã€‚</li>
<li>å®ƒæ¶µç›–äº†ä»ä¼ä¸šè´¢åŠ¡ã€é“¶è¡Œä¸šåˆ°è¡Œä¸šåˆ†æç­‰14ä¸ªé‡‘èå­é¢†åŸŸã€‚</li>
<li>æ¨¡å‹éœ€è¦å¯¹å¤æ‚çš„é‡‘èå›¾åƒå’Œæ–‡æœ¬è¿›è¡Œå¤šæ­¥éª¤ç²¾ç¡®æ•°å€¼æ¨ç†ï¼Œé›†æˆé‡‘èçŸ¥è¯†ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„MLLMåœ¨å›°éš¾é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º53.0%ï¼Œè¡¨æ˜è¯¥é¢†åŸŸä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-170b1f8fe579deb2ce81991482f373ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bed2b9d3bafd237a7c5fde25d76c5f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6b4fa7c14550cff5e4515fde9263a73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-404b97ad576b91a611ebfe20c8d0806d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71fc6b270c5b319f2b5cb5d4c1013b21.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Lightweight-Transformers-for-Zero-Shot-and-Fine-Tuned-Text-to-SQL-Generation-Using-Spider"><a href="#Lightweight-Transformers-for-Zero-Shot-and-Fine-Tuned-Text-to-SQL-Generation-Using-Spider" class="headerlink" title="Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL   Generation Using Spider"></a>Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL   Generation Using Spider</h2><p><strong>Authors:Chirag Seth, Utkarsh Singh</strong></p>
<p>Text-to-SQL translation enables non-expert users to query relational databases using natural language, with applications in education and business intelligence. This study evaluates three lightweight transformer models - T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on low-resource settings. We developed a reusable, model-agnostic pipeline that tailors schema formatting to each modelâ€™s architecture, training them across 1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2 (20.1%), highlighting encoder-decoder modelsâ€™ superiority in schema-aware SQL generation. Despite resource constraints limiting performance, our pipelineâ€™s modularity supports future enhancements, such as advanced schema linking or alternative base models. This work underscores the potential of compact transformers for accessible text-to-SQL solutions in resource-scarce environments. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLçš„ç¿»è¯‘æŠ€æœ¯ä½¿å¾—éä¸“ä¸šç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢å…³ç³»æ•°æ®åº“ï¼Œåœ¨æ•™è‚²ã€å•†ä¸šæ™ºèƒ½ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚æœ¬ç ”ç©¶åœ¨Spideræ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸‰ç§è½»é‡çº§è½¬æ¢å™¨æ¨¡å‹â€”â€”T5-Smallã€BART-Smallå’ŒGPT-2ï¼Œé‡ç‚¹æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸‹è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯é‡ç”¨ã€æ¨¡å‹æ— å…³çš„ç®¡é“ï¼Œæ ¹æ®æ¯ä¸ªæ¨¡å‹çš„æ¶æ„å®šåˆ¶æ¨¡å¼æ ¼å¼åŒ–ï¼Œåœ¨1000åˆ°5000æ¬¡è¿­ä»£ä¹‹é—´å¯¹å®ƒä»¬è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨é€»è¾‘å½¢å¼å‡†ç¡®æ€§ï¼ˆLFAccï¼‰ã€BLEUå’Œç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰ç­‰æŒ‡æ ‡åœ¨1000ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ç»è¿‡å¾®è°ƒï¼ŒT5-Smallçš„LFAccæœ€é«˜ï¼ˆ27.8%ï¼‰ï¼Œä¼˜äºBART-Smallï¼ˆ23.98%ï¼‰å’ŒGPT-2ï¼ˆ20.1%ï¼‰ï¼Œçªæ˜¾å‡ºåœ¨æ¨¡å¼æ„ŸçŸ¥SQLç”Ÿæˆä¸­ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚å°½ç®¡èµ„æºçº¦æŸé™åˆ¶äº†æ€§èƒ½ï¼Œä½†æˆ‘ä»¬çš„ç®¡é“æ¨¡å—æ”¯æŒæœªæ¥å¢å¼ºåŠŸèƒ½ï¼Œå¦‚é«˜çº§æ¨¡å¼é“¾æ¥æˆ–æ›¿ä»£åŸºç¡€æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†ç´§å‡‘è½¬æ¢å™¨åœ¨èµ„æºç¨€ç¼ºç¯å¢ƒä¸­ç”¨äºå¯è®¿é—®æ–‡æœ¬åˆ°SQLè§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04623v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºTransformerçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ•°æ®åº“æ–¹æ³•åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒåœ¨Spideræ•°æ®é›†ä¸Šè¿›è¡Œï¼Œå¯¹æ¯”äº†T5-Smallã€BART-Smallå’ŒGPT-2ä¸‰ä¸ªè½»é‡çº§æ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„T5-Smallæ¨¡å‹åœ¨é€»è¾‘å½¢å¼å‡†ç¡®ç‡ï¼ˆLFAccï¼‰ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°27.8%ï¼Œä¼˜äºBART-Smallå’ŒGPT-2ã€‚è¿™è¡¨æ˜åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œç¼–ç è§£ç å™¨æ¨¡å‹åœ¨æ¨¡å¼æ„ŸçŸ¥SQLç”Ÿæˆæ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚æœ¬æ–‡çš„å·¥ä½œçªæ˜¾äº†ç´§å‡‘çš„Transformeræ¨¡å‹åœ¨èµ„æºç¨€ç¼ºç¯å¢ƒä¸­å®ç°å¯è®¿é—®çš„æ–‡æœ¬åˆ°SQLè§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ•°æ®åº“æ–¹æ³•åœ¨ä½èµ„æºç¯å¢ƒä¸‹å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚</li>
<li>å®éªŒå¯¹æ¯”äº†ä¸‰ç§è½»é‡çº§Transformeræ¨¡å‹ï¼ˆT5-Smallã€BART-Smallå’ŒGPT-2ï¼‰åœ¨æ–‡æœ¬åˆ°SQLè½¬æ¢æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>T5-Smallæ¨¡å‹ç»è¿‡ç²¾ç»†è°ƒæ•´åè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é€»è¾‘å½¢å¼å‡†ç¡®ç‡ï¼ˆLFAccï¼‰æ–¹é¢ã€‚</li>
<li>ç»“æœçªæ˜¾äº†ç¼–ç è§£ç å™¨æ¨¡å‹åœ¨æ¨¡å¼æ„ŸçŸ¥SQLç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>è™½ç„¶èµ„æºçº¦æŸé™åˆ¶äº†æ€§èƒ½ï¼Œä½†æ‰€ä½¿ç”¨çš„ç®¡é“å¯æ¨¡å—åŒ–æ”¯æŒæœªæ¥æ”¹è¿›ï¼Œå¦‚é«˜çº§æ¨¡å¼é“¾æ¥æˆ–æ›¿ä»£åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†ä¸€ä¸ªé‡è¦çš„è§†è§’ï¼Œå³åœ¨èµ„æºç¨€ç¼ºçš„ç¯å¢ƒä¸­å¦‚ä½•å®ç°å¯è®¿é—®çš„æ–‡æœ¬åˆ°SQLè§£å†³æ–¹æ¡ˆã€‚å¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œæ•°æ®åº“äº¤äº’æŠ€æœ¯æä¾›äº†é‡è¦å¯ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9356cd8e35cc0d20d557f1f8741ea1b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32ed226a8e9a04b942e9b3eeb2fe277c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f563dac47c5cd918cac26ce0f0c225d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49f1dd4be617089d7eccc032db679b93.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ConfProBench-A-Confidence-Evaluation-Benchmark-for-MLLM-Based-Process-Judges"><a href="#ConfProBench-A-Confidence-Evaluation-Benchmark-for-MLLM-Based-Process-Judges" class="headerlink" title="ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process   Judges"></a>ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process   Judges</h2><p><strong>Authors:Yue Zhou, Yi Chang, Yuan Wu</strong></p>
<p>Reasoning is a critical capability of multimodal large language models (MLLMs) for solving complex multimodal tasks, and judging the correctness of reasoning steps is crucial for improving this capability. Recently, MLLM-based process judges (MPJs) have been widely used to assess the correctness of reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important for identifying their limitations and guiding future improvements. However, existing benchmarks for MPJs mainly focus on tasks such as step correctness classification and reasoning process search, while overlooking a key aspect: whether the confidence scores produced by MPJs at the step level are reliable. To address this gap, we propose ConfProBench, the first comprehensive benchmark designed to systematically evaluate the reliability of step-level confidence scores generated by MPJs. Our benchmark constructs three types of adversarially perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and Image Perturbation, to test the robustness of MPJ confidence under perturbations. In addition, we introduce three novel evaluation metrics: Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including both proprietary and open-source models. Experiments reveal limitations in current MPJsâ€™ confidence performance and offer competitive baselines to support future research. </p>
<blockquote>
<p>æ¨ç†æ˜¯è§£å†³å¤æ‚å¤šæ¨¡å¼ä»»åŠ¡çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…³é”®èƒ½åŠ›ï¼Œè€Œåˆ¤æ–­æ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§å¯¹äºæé«˜è¿™ç§èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼ŒåŸºäºMLLMçš„è¿‡ç¨‹åˆ¤æ–­ï¼ˆMPJï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºè¯„ä¼°å¤šæ¨¡å¼ä»»åŠ¡ä¸­æ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚å› æ­¤ï¼Œè¯„ä¼°MPJå¯¹äºå‘ç°å…¶å±€é™æ€§å¹¶æŒ‡å¯¼æœªæ¥æ”¹è¿›éå¸¸é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MPJåŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å¦‚æ­¥éª¤æ­£ç¡®æ€§åˆ†ç±»å’Œæ¨ç†è¿‡ç¨‹æœç´¢ç­‰ä»»åŠ¡ï¼Œè€Œå¿½è§†äº†ä¸€ä¸ªå…³é”®æ–¹é¢ï¼šMPJåœ¨æ­¥éª¤å±‚é¢äº§ç”Ÿçš„ç½®ä¿¡åº¦åˆ†æ•°æ˜¯å¦å¯é ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ConfProBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°MPJäº§ç”Ÿçš„æ­¥éª¤çº§ç½®ä¿¡åº¦åˆ†æ•°å¯é æ€§çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ„å»ºäº†ä¸‰ç§å¯¹æŠ—æ€§æ‰°åŠ¨æ¨ç†æ­¥éª¤ï¼šåŒä¹‰è¯æ›¿æ¢ã€å¥æ³•è½¬æ¢å’Œå›¾åƒæ‰°åŠ¨ï¼Œä»¥æµ‹è¯•MPJç½®ä¿¡åº¦åœ¨æ‰°åŠ¨ä¸‹çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼šç½®ä¿¡ç¨³å¥æ€§å¾—åˆ†ï¼ˆCRSï¼‰ã€ç½®ä¿¡æ•æ„Ÿæ€§å¾—åˆ†ï¼ˆCSSï¼‰å’Œç½®ä¿¡æ ¡å‡†å¾—åˆ†ï¼ˆCCSï¼‰ï¼Œåˆ†åˆ«è¯„ä¼°ç¨³å¥æ€§ã€æ•æ„Ÿæ€§å’Œæ ¡å‡†ã€‚æˆ‘ä»¬è¯„ä¼°äº†14ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸“æœ‰å’Œå¼€æºæ¨¡å‹ã€‚å®éªŒæ­ç¤ºäº†å½“å‰MPJåœ¨ç½®ä¿¡åº¦è¡¨ç°æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæ”¯æŒæœªæ¥ç ”ç©¶æä¾›äº†ç«äº‰åŸºå‡†çº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04576v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œæ¨ç†æ˜¯åº”å¯¹å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡çš„å…³é”®èƒ½åŠ›ï¼Œä¸”å¯¹åˆ¤æ–­æ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§å¯¹æå‡æ­¤èƒ½åŠ›è‡³å…³é‡è¦ã€‚å½“å‰å¹¿æ³›ä½¿ç”¨çš„åŸºäºMLLMçš„è¿‡ç¨‹åˆ¤æ–­ï¼ˆMPJsï¼‰ä¸»è¦å…³æ³¨æ­¥éª¤æ­£ç¡®æ€§åˆ†ç±»å’Œæ¨ç†è¿‡ç¨‹æœç´¢ç­‰ä»»åŠ¡ï¼Œä½†å¿½ç•¥äº†å…¶å…³é”®æ–¹é¢â€”â€”MPJsç”Ÿæˆçš„æ­¥éª¤çº§åˆ«ç½®ä¿¡åº¦çš„å¯é æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºConfProBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°MPJsåœ¨æ­¥éª¤çº§åˆ«ä¸Šç”Ÿæˆçš„ç½®ä¿¡åº¦çš„å¯é æ€§ã€‚é€šè¿‡æ„å»ºä¸‰ç§å¯¹æŠ—æ€§æ‰°åŠ¨æ¨ç†æ­¥éª¤è¿›è¡Œæµ‹è¯•ï¼Œå¹¶å¼•å…¥ä¸‰ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°ç½®ä¿¡åº¦ã€‚æœ¬æ–‡è¯„ä¼°äº†å¤šä¸ªå…ˆè¿›çš„MLLMsæ¨¡å‹ï¼Œæ­ç¤ºäº†å½“å‰MPJsåœ¨ç½®ä¿¡åº¦æ€§èƒ½æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å¯¹äºè§£å†³å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>è¿‡ç¨‹åˆ¤æ–­ï¼ˆMPJsï¼‰å¯¹äºè¯„ä¼°MLLMsçš„æ¨ç†æ­¥éª¤æ­£ç¡®æ€§ååˆ†é‡è¦ã€‚</li>
<li>å½“å‰MPJè¯„ä¼°ä¸»è¦å…³æ³¨æ­¥éª¤æ­£ç¡®æ€§åˆ†ç±»å’Œæ¨ç†è¿‡ç¨‹æœç´¢ï¼Œä½†å¿½ç•¥äº†å…¶ç”Ÿæˆçš„æ­¥éª¤çº§åˆ«ç½®ä¿¡åº¦çš„å¯é æ€§ã€‚</li>
<li>ConfProBenchåŸºå‡†æµ‹è¯•å¹³å°è¢«æå‡ºä»¥ç³»ç»Ÿåœ°è¯„ä¼°MPJsåœ¨æ­¥éª¤çº§åˆ«ä¸Šç”Ÿæˆçš„ç½®ä¿¡åº¦çš„å¯é æ€§ã€‚</li>
<li>é€šè¿‡æ„å»ºä¸‰ç§å¯¹æŠ—æ€§æ‰°åŠ¨æ¨ç†æ­¥éª¤è¿›è¡Œæµ‹è¯•ï¼ŒåŒ…æ‹¬åŒä¹‰è¯æ›¿æ¢ã€å¥æ³•è½¬æ¢å’Œå›¾åƒæ‰°åŠ¨ã€‚</li>
<li>å¼•å…¥ä¸‰ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼šä¿¡å¿ƒç¨³å¥æ€§å¾—åˆ†ï¼ˆCRSï¼‰ã€ä¿¡å¿ƒæ•æ„Ÿæ€§å¾—åˆ†ï¼ˆCSSï¼‰å’Œä¿¡å¿ƒæ ¡å‡†å¾—åˆ†ï¼ˆCCSï¼‰ï¼Œä»¥å…¨é¢è¯„ä»·MPJsçš„ç½®ä¿¡åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-72efb63ce264a833694f5b44fc29b36d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d203b43cff8b22558d51d6b0797e7ba1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cc3afd3efd164903f8aab3764f55043.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1656042ae359a3db55e0e3e8396b639a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f265f48cc935594dc88c9c69b47e1063.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="StyliTruth-Unlocking-Stylized-yet-Truthful-LLM-Generation-via-Disentangled-Steering"><a href="#StyliTruth-Unlocking-Stylized-yet-Truthful-LLM-Generation-via-Disentangled-Steering" class="headerlink" title="StyliTruth : Unlocking Stylized yet Truthful LLM Generation via   Disentangled Steering"></a>StyliTruth : Unlocking Stylized yet Truthful LLM Generation via   Disentangled Steering</h2><p><strong>Authors:Chenglei Shen, Zhongxiang Sun, Teng Shi, Xiao Zhang, Jun Xu</strong></p>
<p>Generating stylized large language model (LLM) responses via representation editing is a promising way for fine-grained output control. However, there exists an inherent trade-off: imposing a distinctive style often degrades truthfulness. Existing representation editing methods, by naively injecting style signals, overlook this collateral impact and frequently contaminate the modelâ€™s core truthfulness representations, resulting in reduced answer correctness. We term this phenomenon stylization-induced truthfulness collapse. We attribute this issue to latent coupling between style and truth directions in certain key attention heads, and propose StyliTruth, a mechanism that preserves stylization while keeping truthfulness intact. StyliTruth separates the style-relevant and truth-relevant subspaces in the modelâ€™s representation space via an orthogonal deflation process. This decomposition enables independent control of style and truth in their own subspaces, minimizing interference. By designing adaptive, token-level steering vectors within each subspace, we dynamically and precisely control the generation process to maintain both stylistic fidelity and truthfulness. We validate our method on multiple styles and languages. Extensive experiments and analyses show that StyliTruth significantly reduces stylization-induced truthfulness collapse and outperforms existing inference-time intervention methods in balancing style adherence with truthfulness. </p>
<blockquote>
<p>é€šè¿‡è¡¨ç¤ºç¼–è¾‘ç”Ÿæˆé£æ ¼åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å“åº”æ˜¯æ§åˆ¶ç²¾ç»†è¾“å‡ºçš„ä¸€ç§æœ‰å‰é€”çš„æ–¹å¼ã€‚ç„¶è€Œï¼Œå­˜åœ¨ä¸€ç§å›ºæœ‰çš„æƒè¡¡ï¼šæ–½åŠ ä¸€ç§ç‹¬ç‰¹çš„é£æ ¼å¾€å¾€ä¼šé™ä½çœŸå®æ€§ã€‚ç°æœ‰çš„è¡¨ç¤ºç¼–è¾‘æ–¹æ³•ï¼Œé€šè¿‡ç®€å•åœ°æ³¨å…¥é£æ ¼ä¿¡å·ï¼Œå¿½è§†äº†è¿™ç§é™„å¸¦å½±å“ï¼Œå¹¶ç»å¸¸æ±¡æŸ“æ¨¡å‹çš„æ ¸å¿ƒçœŸå®æ€§è¡¨ç¤ºï¼Œå¯¼è‡´ç­”æ¡ˆçš„æ­£ç¡®æ€§é™ä½ã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºé£æ ¼åŒ–å¼•èµ·çš„çœŸå®æ€§å´©æºƒã€‚æˆ‘ä»¬å°†è¿™ä¸€é—®é¢˜å½’å› äºæŸäº›å…³é”®æ³¨æ„åŠ›å¤´ä¸­é£æ ¼ä¸çœŸå®æ–¹å‘ä¹‹é—´çš„æ½œåœ¨è€¦åˆï¼Œå¹¶æå‡ºäº†StyliTruthæœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨ä¿æŒé£æ ¼åŒ–çš„åŒæ—¶ä¿æŒçœŸå®æ€§ã€‚StyliTruthé€šè¿‡æ­£äº¤è†¨èƒ€è¿‡ç¨‹ï¼Œåœ¨æ¨¡å‹çš„è¡¨ç¤ºç©ºé—´ä¸­åˆ†ç¦»å‡ºä¸é£æ ¼å’ŒçœŸå®ç›¸å…³çš„å­ç©ºé—´ã€‚è¿™ç§åˆ†è§£èƒ½å¤Ÿåœ¨å„è‡ªçš„å­ç©ºé—´ä¸­ç‹¬ç«‹æ§åˆ¶é£æ ¼å’ŒçœŸå®ï¼Œæœ€å°åŒ–å¹²æ‰°ã€‚é€šè¿‡åœ¨æ¯ä¸ªå­ç©ºé—´å†…è®¾è®¡è‡ªé€‚åº”çš„ã€ä»¤ç‰Œçº§çš„å¼•å¯¼å‘é‡ï¼Œæˆ‘ä»¬èƒ½å¤ŸåŠ¨æ€å’Œç²¾ç¡®åœ°æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥ç»´æŒé£æ ¼ä¸Šçš„å¿ å®æ€§å’ŒçœŸå®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§é£æ ¼å’Œè¯­è¨€ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å¤§é‡çš„å®éªŒå’Œåˆ†æè¡¨æ˜ï¼ŒStyliTruthæ˜¾è‘—å‡å°‘äº†ç”±é£æ ¼åŒ–å¼•èµ·çš„çœŸå®æ€§å´©æºƒï¼Œå¹¶åœ¨å¹³è¡¡é£æ ¼åšæŒä¸çœŸå®æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æ¨ç†æ—¶é—´å¹²é¢„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04530v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ååº”ä¸­ï¼Œé€šè¿‡è¡¨ç°ç¼–è¾‘ç”Ÿæˆé£æ ¼åŒ–çš„å›å¤å±•ç°å‡ºäº†ç²¾ç»†æ§åˆ¶è¾“å‡ºçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œé£æ ¼åŒ–å¾€å¾€ä¼´éšç€çœŸå®æ€§çš„æŸå¤±ï¼Œå­˜åœ¨å†…åœ¨æƒè¡¡ã€‚ç°æœ‰çš„è¡¨ç°ç¼–è¾‘æ–¹æ³•è¿‡äºç®€å•æ³¨å…¥é£æ ¼ä¿¡å·ï¼Œå¿½è§†äº†è¿™ç§é™„å¸¦å½±å“ï¼Œä¼šå¹²æ‰°æ¨¡å‹çš„æ ¸å¿ƒçœŸå®æ€§è¡¨ç¤ºï¼Œé™ä½äº†å›ç­”çš„å‡†ç¡®åº¦ã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºé£æ ¼åŒ–å¼•èµ·çš„çœŸå®æ€§å´©æºƒã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å½’å› äºå…³é”®æ³¨æ„åŠ›å¤´éƒ¨çš„é£æ ¼å’ŒçœŸç†æ–¹å‘çš„æ½œåœ¨è€¦åˆï¼Œå¹¶æå‡ºäº†StyliTruthæœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé£æ ¼åŒ–çš„åŒæ—¶ä¿æŒçœŸå®æ€§ã€‚StyliTruthé€šè¿‡æ­£äº¤å‹ç¼©è¿‡ç¨‹åˆ†ç¦»äº†ä¸é£æ ¼å’ŒçœŸå®ç›¸å…³çš„å­ç©ºé—´ã€‚è¿™ç§åˆ†è§£ä½¿å¾—é£æ ¼å’ŒçœŸå®åœ¨å…¶å„è‡ªçš„å­ç©ºé—´å†…ç‹¬ç«‹æ§åˆ¶ï¼Œæœ€å°åŒ–å¹²æ‰°ã€‚é€šè¿‡è®¾è®¡æ¯ä¸ªå­ç©ºé—´å†…çš„è‡ªé€‚åº”ã€ä»¤ç‰Œçº§å¼•å¯¼å‘é‡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€ç²¾ç¡®åœ°æ§åˆ¶é£æ ¼ä¿æŒå’ŒçœŸå®æ€§ç»´æŠ¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šé£æ ¼å’Œå¤šç§è¯­è¨€ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒStyliTruthæ˜¾è‘—å‡å°‘äº†é£æ ¼åŒ–å¼•èµ·çš„çœŸå®æ€§å´©æºƒï¼Œå¹¶ä¸”åœ¨å¹³è¡¡é£æ ¼éµå®ˆå’ŒçœŸå®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æ¨ç†æ—¶é—´å¹²é¢„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡è¡¨ç°ç¼–è¾‘ç”Ÿæˆé£æ ¼åŒ–çš„LLMå›å¤å…·æœ‰ç²¾ç»†æ§åˆ¶è¾“å‡ºçš„æ½œåŠ›ã€‚</li>
<li>é£æ ¼åŒ–å¤§å‹è¯­è¨€æ¨¡å‹å›å¤æ—¶å­˜åœ¨çœŸå®æ€§çš„æŸå¤±ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¿‡äºç®€å•æ³¨å…¥é£æ ¼ä¿¡å·ï¼Œå¯¼è‡´æ¨¡å‹çš„æ ¸å¿ƒçœŸå®æ€§è¡¨ç¤ºå—åˆ°å¹²æ‰°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºStyliTruthçš„æœºåˆ¶æ¥è§£å†³é£æ ¼åŒ–å¼•èµ·çš„çœŸå®æ€§å´©æºƒé—®é¢˜ã€‚</li>
<li>StyliTruthé€šè¿‡åˆ†ç¦»é£æ ¼å’ŒçœŸå®æ€§çš„å­ç©ºé—´æ¥å¹³è¡¡é£æ ¼å’ŒçœŸå®æ€§çš„æ§åˆ¶ã€‚</li>
<li>StyliTruthé€šè¿‡è‡ªé€‚åº”ä»¤ç‰Œçº§å¼•å¯¼å‘é‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€æ§åˆ¶é£æ ¼å’ŒçœŸå®æ€§çš„ç»´æŠ¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b0c8d21f301b1f2744107bc3fb0ad41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eb3f1517b0eb2fa5843c21a65b72e5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6d2cada4b7dc2bec9092e4547342bd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13386a024849c01f0787fdbe0b56188e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b34be37cc36b5502340c1bfb0909a5ba.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Boosting-Visual-Knowledge-Intensive-Training-for-LVLMs-Through-Causality-Driven-Visual-Object-Completion"><a href="#Boosting-Visual-Knowledge-Intensive-Training-for-LVLMs-Through-Causality-Driven-Visual-Object-Completion" class="headerlink" title="Boosting Visual Knowledge-Intensive Training for LVLMs Through   Causality-Driven Visual Object Completion"></a>Boosting Visual Knowledge-Intensive Training for LVLMs Through   Causality-Driven Visual Object Completion</h2><p><strong>Authors:Qingguo Hu, Ante Wang, Jia Song, Delai Qiu, Qingsong Liu, Jinsong Su</strong></p>
<p>Large Vision-Language Models (LVLMs) have experienced significant advancements in recent years. However, their performance still falls short in tasks requiring deep visual perception, such as identifying subtle differences between images. A potential cause is the scarcity of visual knowledge in popular instruction-tuning corpora, resulting in inadequate visual perception and reasoning capabilities. To address this challenge, we introduce a self-improvement framework grounded in a novel visual knowledge-intensive task, \underline{C}ausality-driven \underline{V}isual object \underline{C}ompletion (CVC). This task requires LVLMs to infer the masked object in an image based on its \textit{causal} relationships with the other visible information. We first obtain rich examples cheaply through our automated instance construction pipeline, without relying on sophisticated LVLMs (\textit{e.g.}, GPT-4V) or human assistance. Then, LVLMs effectively self-improve through trial and error learning using these created instances. Our experiments demonstrate substantial gains across four challenging specialized tasks and four widely-used comprehensive benchmarks. Especially on specialized tasks, our method achieves an average improvement of 5.4% and 4.0% compared to the corresponding baselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code is available at <a target="_blank" rel="noopener" href="https://github.com/XMUDeepLIT/CVC">https://github.com/XMUDeepLIT/CVC</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨éœ€è¦æ·±åº¦è§†è§‰æ„ŸçŸ¥çš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»ç„¶ä¸è¶³ï¼Œå¦‚è¯†åˆ«å›¾åƒä¹‹é—´çš„ç»†å¾®å·®å¼‚ã€‚é€ æˆè¿™ç§æƒ…å†µçš„ä¸€ä¸ªæ½œåœ¨åŸå› æ˜¯æµè¡ŒæŒ‡ä»¤å¾®è°ƒè¯­æ–™åº“ä¸­è§†è§‰çŸ¥è¯†çš„åŒ®ä¹ï¼Œå¯¼è‡´è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ä¸è¶³ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åŸºäºæ–°å‹è§†è§‰çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼Œå¼•å…¥äº†ä¸€ç§è‡ªæˆ‘æ”¹è¿›æ¡†æ¶â€”â€”å› æœé©±åŠ¨å‹è§†è§‰å¯¹è±¡è¡¥å…¨ï¼ˆCVCï¼‰ã€‚è¯¥ä»»åŠ¡è¦æ±‚LVLMsåŸºäºä¸å…¶ä»–å¯è§ä¿¡æ¯çš„å› æœå…³ç³»æ¨æ–­å›¾åƒä¸­çš„é®æŒ¡å¯¹è±¡ã€‚æˆ‘ä»¬é¦–å¯ä»¥é€šè¿‡è‡ªåŠ¨åŒ–å®ä¾‹æ„å»ºç®¡é“ä½æˆæœ¬åœ°è·å–ä¸°å¯Œçš„æ ·æœ¬ï¼Œæ— éœ€ä¾èµ–å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4Vï¼‰æˆ–äººå·¥ååŠ©ã€‚éšåï¼Œè¿™äº›ç”Ÿæˆçš„å®ä¾‹é€šè¿‡è¯•é”™å­¦ä¹ ï¼Œä½¿LVLMså®ç°äº†æœ‰æ•ˆçš„è‡ªæˆ‘æå‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å››é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸“é¡¹ä»»åŠ¡å’Œå››é¡¹å¹¿æ³›ä½¿ç”¨çš„ç»¼åˆåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡å–å¾—äº†æ˜¾è‘—çš„æå‡æ•ˆæœã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸“é¡¹ä»»åŠ¡ä¸Šï¼Œä¸ç›¸åº”çš„åŸºçº¿ç›¸æ¯”ï¼Œä½¿ç”¨LLaVA-1.5-7Bå’ŒLLaVA-1.5-13Bæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¹³å‡æå‡äº†5.4%å’Œ4.0%ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/XMUDeepLIT/CVC%E4%B8%8A%E3%80%82">https://github.com/XMUDeepLIT/CVCä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04453v1">PDF</a> Accepted by IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿‘å¹´æ¥å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨éœ€è¦æ·±åº¦è§†è§‰æ„ŸçŸ¥çš„ä»»åŠ¡ä¸­è¡¨ç°ä»æœ‰æ‰€ä¸è¶³ï¼Œå¦‚å›¾åƒé—´çš„ç»†å¾®å·®å¼‚è¯†åˆ«ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå› æœé©±åŠ¨è§†è§‰å¯¹è±¡è¡¥å…¨ï¼ˆCVCï¼‰ä»»åŠ¡è¿›è¡Œè‡ªæˆ‘æå‡çš„æ–°æ¡†æ¶ï¼Œä»¥è§£å†³æ­¤æŒ‘æˆ˜ã€‚è¯¥ä»»åŠ¡è¦æ±‚LVLMsæ ¹æ®å›¾åƒä¸­å¯¹è±¡ä¹‹é—´çš„å› æœå…³ç³»æ¨æ–­å‡ºè¢«æ©ç›–çš„å¯¹è±¡ã€‚æœ¬æ–‡é€šè¿‡è‡ªåŠ¨åŒ–å®ä¾‹æ„å»ºæµç¨‹è·å–ä¸°å¯Œçš„ä¾‹å­ï¼Œæ— éœ€ä¾èµ–é«˜çº§LVLMsæˆ–äººå·¥è¾…åŠ©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸“é¡¹ä»»åŠ¡å’Œå››ä¸ªå¹¿æ³›ä½¿ç”¨çš„ç»¼åˆåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸“é¡¹ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºå¯¹åº”çš„åŸºçº¿æ–¹æ³•å¹³å‡æå‡äº†5.4%å’Œ4.0%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LVLMsåœ¨æ·±åº¦è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­ä»å­˜åœ¨æ€§èƒ½çŸ­æ¿ã€‚</li>
<li>æå‡ºäº†åŸºäºå› æœé©±åŠ¨è§†è§‰å¯¹è±¡è¡¥å…¨ï¼ˆCVCï¼‰ä»»åŠ¡çš„æ–°æ¡†æ¶ä»¥æ”¹å–„LVLMsçš„è¡¨ç°ã€‚</li>
<li>CVCä»»åŠ¡è¦æ±‚LVLMsæ¨æ–­å›¾åƒä¸­è¢«æ©ç›–çš„å¯¹è±¡ï¼ŒåŸºäºä¸å…¶ä»–å¯è§ä¿¡æ¯çš„å› æœå…³ç³»ã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨åŒ–å®ä¾‹æ„å»ºæµç¨‹è·å–ä¸°å¯Œä¾‹å­ï¼Œæ— éœ€é«˜çº§LVLMsæˆ–äººå·¥è¾…åŠ©ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æå‡5.4%å’Œ4.0%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8987ec0c727a131f94f77e738d1852aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b59cbb84f76df60ef12a13150a5c4035.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abaab2bb10b59372f0212fc28085a37a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3139bfbd77e773f8ae3150a2041de246.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e18a472fad4911bdf0f019e47bd4e3a7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Think-Before-You-Segment-An-Object-aware-Reasoning-Agent-for-Referring-Audio-Visual-Segmentation"><a href="#Think-Before-You-Segment-An-Object-aware-Reasoning-Agent-for-Referring-Audio-Visual-Segmentation" class="headerlink" title="Think Before You Segment: An Object-aware Reasoning Agent for Referring   Audio-Visual Segmentation"></a>Think Before You Segment: An Object-aware Reasoning Agent for Referring   Audio-Visual Segmentation</h2><p><strong>Authors:Jinxing Zhou, Yanghao Zhou, Mingfei Han, Tong Wang, Xiaojun Chang, Hisham Cholakkal, Rao Muhammad Anwer</strong></p>
<p>Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects in audible videos based on given reference expressions. Prior works typically rely on learning latent embeddings via multimodal fusion to prompt a tunable SAM&#x2F;SAM2 decoder for segmentation, which requires strong pixel-level supervision and lacks interpretability. From a novel perspective of explicit reference understanding, we propose TGS-Agent, which decomposes the task into a Think-Ground-Segment process, mimicking the human reasoning procedure by first identifying the referred object through multimodal analysis, followed by coarse-grained grounding and precise segmentation. To this end, we first propose Ref-Thinker, a multimodal language model capable of reasoning over textual, visual, and auditory cues. We construct an instruction-tuning dataset with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The object description inferred by Ref-Thinker is used as an explicit prompt for Grounding-DINO and SAM2, which perform grounding and segmentation without relying on pixel-level supervision. Additionally, we introduce R\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and reasoning-intensive references for better evaluating model generalization. Our approach achieves state-of-the-art results on both standard Ref-AVSBench and proposed R\textsuperscript{2}-AVSBench. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/jasongief/TGS-Agent">https://github.com/jasongief/TGS-Agent</a>. </p>
<blockquote>
<p>å¼•ç”¨éŸ³é¢‘è§†è§‰åˆ†å‰²ï¼ˆRef-AVSï¼‰æ—¨åœ¨æ ¹æ®ç»™å®šçš„å‚è€ƒè¡¨è¾¾å¼å¯¹å¯å¬è§†é¢‘ä¸­çš„ç›®æ ‡å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚æ—©æœŸçš„å·¥ä½œé€šå¸¸ä¾èµ–äºé€šè¿‡å¤šæ¨¡æ€èåˆå­¦ä¹ æ½œåœ¨åµŒå…¥ï¼Œä»¥æç¤ºå¯è°ƒSAM&#x2F;SAM2è§£ç å™¨è¿›è¡Œåˆ†å‰²ï¼Œè¿™éœ€è¦å¼ºçƒˆçš„åƒç´ çº§ç›‘ç£å¹¶ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚ä»æ˜¾å¼å‚è€ƒç†è§£çš„æ–°è§’åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†TGS-Agentï¼Œå®ƒå°†ä»»åŠ¡åˆ†è§£ä¸ºThink-Ground-Segmentè¿‡ç¨‹ï¼Œé€šè¿‡å¤šæ¨¡æ€åˆ†æé¦–å…ˆè¯†åˆ«æ‰€æŒ‡çš„ç‰©ä½“ï¼Œç„¶åè¿›è¡Œç²—ç²’åº¦å®šä½å’Œç²¾ç¡®åˆ†å‰²ï¼Œæ¨¡ä»¿äººç±»çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†Ref-Thinkerï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½å¤Ÿæ¨ç†æ–‡æœ¬ã€è§†è§‰å’Œå¬è§‰çº¿ç´¢çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç”¨äºRef-Thinkerç²¾ç»†è°ƒæ•´çš„å…·æœ‰æ˜ç¡®å¯¹è±¡æ„ŸçŸ¥çš„think-answeré“¾ã€‚Ref-Thinkeræ¨æ–­çš„å¯¹è±¡æè¿°è¢«ç”¨ä½œå¯¹Grounding-DINOå’ŒSAM__çš„æ˜ç¡®æç¤ºï¼Œè¿›è¡Œå®šä½å’Œåˆ†å‰²ï¼Œæ— éœ€ä¾èµ–åƒç´ çº§ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†RÂ²AVSBenchæ–°åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…å«è¯­è¨€å¤šæ ·åŒ–å’Œæ¨ç†å¯†é›†å‹çš„å‚è€ƒç‰©ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†çš„Ref-AVSBenchå’Œæå‡ºçš„RÂ²AVSBenchä¸Šéƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jasongief/TGS-Agent%E4%B8%8A%E6%8F%AD%E5%AE%9E%E3%80%82">https://github.com/jasongief/TGS-Agentä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04418v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/jasongief/TGS-Agent">https://github.com/jasongief/TGS-Agent</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºTGS-Agentçš„æ–°æ–¹æ³•ï¼Œç”¨äºéŸ³é¢‘è§†é¢‘åˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ–¹æ³•ä»ç†è§£æ˜ç¡®å‚è€ƒçš„æ–°è§’åº¦å…¥æ‰‹ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºæ€è€ƒã€å®šä½å’Œåˆ†å‰²ä¸‰ä¸ªè¿‡ç¨‹ï¼Œå¹¶æ¨¡ä»¿äººç±»æ¨ç†è¿‡ç¨‹ã€‚é¦–å…ˆé€šè¿‡å¤šæ¨¡æ€åˆ†æè¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œç„¶åè¿›è¡Œç²—ç•¥å®šä½ï¼Œæœ€åç²¾ç¡®åˆ†å‰²ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Ref-Thinkerå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¯¹æ–‡æœ¬ã€è§†è§‰å’Œå¬è§‰çº¿ç´¢è¿›è¡Œæ¨ç†ã€‚é€šè¿‡æ„å»ºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œå¯¹Ref-Thinkerè¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨å…¶æ¨æ–­çš„å¯¹è±¡æè¿°ä½œä¸ºå¯¹Grounding-DINOå’ŒSAM2çš„æ˜ç¡®æç¤ºï¼Œè¿›è¡Œå®šä½å’Œåˆ†å‰²ä»»åŠ¡ï¼Œæ— éœ€åƒç´ çº§ç›‘ç£ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†æ–°çš„R\textsuperscript{2}-AVSBenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚TGS-Agentåœ¨æ ‡å‡†çš„Ref-AVSBenchå’Œæ–°çš„R\textsuperscript{2}-AVSBenchä¸Šå‡è¾¾åˆ°äº†æœ€ä½³æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TGS-Agentæ–¹æ³•é€šè¿‡åˆ†è§£ä»»åŠ¡ä¸ºæ€è€ƒã€å®šä½å’Œåˆ†å‰²è¿‡ç¨‹ï¼Œæ¨¡ä»¿äººç±»æ¨ç†è¿‡ç¨‹è¿›è¡ŒéŸ³é¢‘è§†é¢‘åˆ†å‰²ã€‚</li>
<li>æå‡ºäº†Ref-Thinkerå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œèƒ½ç»“åˆæ–‡æœ¬ã€è§†è§‰å’Œå¬è§‰çº¿ç´¢è¿›è¡Œæ¨ç†ã€‚</li>
<li>é€šè¿‡æ„å»ºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†å¯¹Ref-Thinkerè¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨Ref-Thinkeræ¨æ–­çš„å¯¹è±¡æè¿°ä½œä¸ºå¯¹Grounding-DINOå’ŒSAM2çš„æ˜ç¡®æç¤ºï¼Œè¿›è¡Œå®šä½å’Œåˆ†å‰²ä»»åŠ¡ï¼Œå‡å°‘å¯¹åƒç´ çº§ç›‘ç£çš„ä¾èµ–ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„R\textsuperscript{2}-AVSBenchåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>TGS-Agentåœ¨Ref-AVSBenchå’ŒR\textsuperscript{2}-AVSBenchä¸Šå–å¾—äº†æœ€ä½³æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c52c96e0c2875228103089bee9237c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98991b6c2c4c6e21564bc657de0cb844.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77b4efdfd8c77575c393a2a6cc9f759f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15dc138b6bc925bf9acfb1a0227d974f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e93abc6269c861370f852bd98f60aeb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Length-Matters-Length-Aware-Transformer-for-Temporal-Sentence-Grounding"><a href="#Length-Matters-Length-Aware-Transformer-for-Temporal-Sentence-Grounding" class="headerlink" title="Length Matters: Length-Aware Transformer for Temporal Sentence Grounding"></a>Length Matters: Length-Aware Transformer for Temporal Sentence Grounding</h2><p><strong>Authors:Yifan Wang, Ziyi Liu, Xiaolong Sun, Jiawei Wang, Hongmin Liu</strong></p>
<p>Temporal sentence grounding (TSG) is a highly challenging task aiming to localize the temporal segment within an untrimmed video corresponding to a given natural language description. Benefiting from the design of learnable queries, the DETR-based models have achieved substantial advancements in the TSG task. However, the absence of explicit supervision often causes the learned queries to overlap in roles, leading to redundant predictions. Therefore, we propose to improve TSG by making each query fulfill its designated role, leveraging the length priors of the video-description pairs. In this paper, we introduce the Length-Aware Transformer (LATR) for TSG, which assigns different queries to handle predictions based on varying temporal lengths. Specifically, we divide all queries into three groups, responsible for segments with short, middle, and long temporal durations, respectively. During training, an additional length classification task is introduced. Predictions from queries with mismatched lengths are suppressed, guiding each query to specialize in its designated function. Extensive experiments demonstrate the effectiveness of our LATR, achieving state-of-the-art performance on three public benchmarks. Furthermore, the ablation studies validate the contribution of each component of our method and the critical role of incorporating length priors into the TSG task. </p>
<blockquote>
<p>æ—¶åºå¥å­å®šä½ï¼ˆTSGï¼‰æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨å®šä½æœªå‰ªè¾‘è§†é¢‘ä¸­ä¸ç»™å®šè‡ªç„¶è¯­è¨€æè¿°ç›¸å¯¹åº”çš„æ—¶é—´æ®µã€‚å¾—ç›Šäºå¯å­¦ä¹ æŸ¥è¯¢çš„è®¾è®¡ï¼ŒåŸºäºDETRçš„æ¨¡å‹åœ¨TSGä»»åŠ¡ä¸­å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ˜ç¡®çš„ç›‘ç£ï¼Œé€šå¸¸ä¼šå¯¼è‡´å­¦ä¹ åˆ°çš„æŸ¥è¯¢åœ¨è§’è‰²ä¸Šé‡å ï¼Œä»è€Œäº§ç”Ÿå†—ä½™é¢„æµ‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡ä½¿æ¯ä¸ªæŸ¥è¯¢å±¥è¡Œå…¶æŒ‡å®šè§’è‰²æ¥æ”¹å–„TSGä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨è§†é¢‘æè¿°å¯¹çš„æ—¶é—´é•¿åº¦å…ˆéªŒä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºTSGå¼•å…¥äº†é•¿åº¦æ„ŸçŸ¥è½¬æ¢å™¨ï¼ˆLATRï¼‰ï¼Œæ ¹æ®æ—¶é—´é•¿åº¦å·®å¼‚åˆ†é…ä¸åŒçš„æŸ¥è¯¢æ¥å¤„ç†é¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰æŸ¥è¯¢åˆ†ä¸ºä¸‰ç»„ï¼Œåˆ†åˆ«è´Ÿè´£å¤„ç†çŸ­ã€ä¸­å’Œé•¿æ—¶é—´æ®µçš„ç‰‡æ®µã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥äº†é¢å¤–çš„é•¿åº¦åˆ†ç±»ä»»åŠ¡ã€‚æ¥è‡ªä¸åŒ¹é…é•¿åº¦çš„æŸ¥è¯¢é¢„æµ‹ä¼šè¢«æŠ‘åˆ¶ï¼Œä»è€Œå¼•å¯¼æ¯ä¸ªæŸ¥è¯¢ä¸“é—¨æ‰§è¡Œå…¶æŒ‡å®šåŠŸèƒ½ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„LATRçš„æœ‰æ•ˆæ€§ï¼Œåœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸­æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ä»¥åŠå°†é•¿åº¦å…ˆéªŒçŸ¥è¯†çº³å…¥TSGä»»åŠ¡çš„å…³é”®ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04299v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹æ—¶åºå¥å­å®šä½ï¼ˆTSGï¼‰ä»»åŠ¡çš„æ–°æ–¹æ³•ï¼Œå³é•¿åº¦æ„ŸçŸ¥è½¬æ¢å™¨ï¼ˆLATRï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¯å­¦ä¹ æŸ¥è¯¢çš„ä¼˜åŠ¿ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ä¸æè¿°ä¹‹é—´æ—¶åºæ®µå®šä½çš„é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹å› ç¼ºä¹æ˜ç¡®ç›‘ç£å¯¼è‡´çš„æŸ¥è¯¢è§’è‰²é‡å é—®é¢˜ï¼ŒLATRé€šè¿‡åˆ©ç”¨è§†é¢‘æè¿°å¯¹çš„é•¿åº¦å…ˆéªŒä¿¡æ¯ï¼Œä½¿æ¯ä¸ªæŸ¥è¯¢å±¥è¡Œå…¶æŒ‡å®šè§’è‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æŸ¥è¯¢åˆ†ä¸ºä¸‰ç»„ï¼Œåˆ†åˆ«è´Ÿè´£å¤„ç†ä¸åŒæ—¶é•¿ï¼ˆçŸ­ã€ä¸­ã€é•¿ï¼‰çš„ç‰‡æ®µã€‚è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†é¢å¤–çš„é•¿åº¦åˆ†ç±»ä»»åŠ¡ï¼ŒæŠ‘åˆ¶äº†ä¸åŒ¹é…é•¿åº¦çš„æŸ¥è¯¢é¢„æµ‹ï¼Œä½¿æ¯ä¸ªæŸ¥è¯¢ä¸“æ³¨äºå…¶ç‰¹å®šåŠŸèƒ½ã€‚åœ¨ä¸‰ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLATRæ–¹æ³•å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶ä¹ŸéªŒè¯äº†è¯¥æ–¹æ³•æ¯ä¸ªç»„ä»¶çš„ä½œç”¨ä»¥åŠå°†é•¿åº¦å…ˆéªŒä¿¡æ¯èå…¥TSGä»»åŠ¡çš„é‡è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>TSGä»»åŠ¡æ—¨åœ¨å®šä½ç»™å®šè‡ªç„¶è¯­è¨€æè¿°åœ¨æœªç»å‰ªè¾‘è§†é¢‘ä¸­çš„æ—¶é—´ç‰‡æ®µï¼Œæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>åŸºäºDETRçš„æ¨¡å‹é€šè¿‡è®¾è®¡å¯å­¦ä¹ æŸ¥è¯¢å®ç°äº†TSGä»»åŠ¡çš„æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç¼ºä¹æ˜ç¡®ç›‘ç£ä¼šå¯¼è‡´æŸ¥è¯¢è§’è‰²é‡å å’Œå†—ä½™é¢„æµ‹ã€‚</li>
<li>LATRæ–¹æ³•é€šè¿‡åˆ©ç”¨è§†é¢‘æè¿°å¯¹çš„é•¿åº¦å…ˆéªŒä¿¡æ¯ï¼Œä½¿æ¯ä¸ªæŸ¥è¯¢å±¥è¡Œå…¶æŒ‡å®šè§’è‰²æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>LATRå°†æŸ¥è¯¢åˆ†ä¸ºä¸‰ç»„ï¼Œåˆ†åˆ«å¤„ç†ä¸åŒæ—¶é•¿çš„é¢„æµ‹ã€‚</li>
<li>é¢å¤–çš„é•¿åº¦åˆ†ç±»ä»»åŠ¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢«å¼•å…¥ï¼Œä»¥æŠ‘åˆ¶ä¸åŒ¹é…é•¿åº¦çš„æŸ¥è¯¢é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-95c8ab955ab00fb83c228582cd0771a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ee63cdf9484b2fbfc670dcca47fa9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-018abaee4e75606e30c8ade2847ee5a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-002323844bfa4552a30f79061cec83ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8a40e42d8ea20f241e590887df9b367.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2af9ed00379260d1a2a3ad627b0480c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25e0882560a9c735442242b8a9908241.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech"><a href="#Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech" class="headerlink" title="Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech"></a>Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech</h2><p><strong>Authors:Jingyuan Xing, Zhipeng Li, Jialong Mai, Xiaofen Xing, Xiangmin Xu</strong></p>
<p>Advances in speech representation and large language models have enhanced zero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS models face challenges in capturing the complex correlations between acoustic and semantic features, resulting in a lack of expressiveness and similarity. The primary reason lies in the complex relationship between semantic and acoustic features, which manifests independent and interdependent aspects.This paper introduces a TTS framework that combines both autoregressive (AR) and non-autoregressive (NAR) modules to harmonize the independence and interdependence of acoustic and semantic information. The AR model leverages the proposed Parallel Tokenizer to synthesize the top semantic and acoustic tokens simultaneously. In contrast, considering the interdependence, the Coupled NAR model predicts detailed tokens based on the general AR modelâ€™s output. Parallel GPT, built on this architecture, is designed to improve zero-shot text-to-speech synthesis through its parallel structure. Experiments on English and Chinese datasets demonstrate that the proposed model significantly outperforms the quality and efficiency of the synthesis of existing zero-shot TTS models. Speech demos are available at <a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/">https://t1235-ch.github.io/pgpt/</a>. </p>
<blockquote>
<p>è¯­éŸ³è¡¨ç¤ºçš„è¿›å±•å’Œå¤§è¯­è¨€æ¨¡å‹çš„è¿›æ­¥å·²ç»æé«˜äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›¶æ ·æœ¬TTSæ¨¡å‹åœ¨æ•è·å£°å­¦å’Œè¯­ä¹‰ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³è”æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ç¼ºä¹è¡¨è¾¾æ€§å’Œç›¸ä¼¼æ€§ã€‚ä¸»è¦åŸå› åœ¨äºè¯­ä¹‰å’Œå£°å­¦ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œè¡¨ç°ä¸ºç‹¬ç«‹å’Œç›¸äº’ä¾èµ–çš„æ–¹é¢ã€‚</p>
</blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç»“åˆè‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å—çš„TTSæ¡†æ¶ï¼Œä»¥åè°ƒå£°å­¦å’Œè¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ã€‚ARæ¨¡å‹åˆ©ç”¨æå‡ºçš„å¹¶è¡Œåˆ†è¯å™¨åŒæ—¶åˆæˆé¡¶çº§è¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ã€‚ç›¸åï¼Œè€ƒè™‘åˆ°ç›¸äº’ä¾èµ–æ€§ï¼Œè€¦åˆçš„NARæ¨¡å‹åŸºäºé€šç”¨ARæ¨¡å‹çš„è¾“å‡ºè¿›è¡Œè¯¦ç»†çš„æ ‡è®°é¢„æµ‹ã€‚åŸºäºæ­¤æ¶æ„æ„å»ºçš„å¹¶è¡ŒGPTæ—¨åœ¨é€šè¿‡å…¶å¹¶è¡Œç»“æ„æ”¹è¿›é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆã€‚åœ¨è‹±è¯­å’Œä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨åˆæˆè´¨é‡å’Œæ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚è¯­éŸ³æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/">https://t1235-ch.github.io/pgpt/</a> ä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04141v1">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing (TASLP)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»“åˆè‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å—çš„æ–°å‹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è°ƒå’Œå£°å­¦è¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ï¼Œé€šè¿‡å¹¶è¡Œä»¤ç‰ŒåŒ–å™¨ï¼ˆParallel Tokenizerï¼‰å’Œè€¦åˆNARæ¨¡å‹ç­‰æŠ€æœ¯ï¼Œå¢å¼ºäº†é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³çš„æ€§èƒ½å’Œè¡¨è¾¾æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ä¸Šçš„åˆæˆè´¨é‡å’Œæ•ˆç‡å‡æ˜¾è‘—ä¼˜äºç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›çš„è¯­éŸ³è¡¨ç¤ºå’Œå¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºäº†é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹åœ¨æ•è·å£°å­¦è¯­ä¹‰ç‰¹å¾çš„å¤æ‚ç›¸å…³æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´ç¼ºä¹è¡¨è¾¾æ€§å’Œç›¸ä¼¼æ€§ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ç»“åˆARå’ŒNARæ¨¡å—çš„TTSæ¡†æ¶ï¼Œä»¥è°ƒå’Œå£°å­¦è¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ã€‚</li>
<li>ARæ¨¡å‹ä½¿ç”¨å¹¶è¡Œä»¤ç‰ŒåŒ–å™¨ï¼ˆParallel Tokenizerï¼‰åŒæ—¶åˆæˆé¡¶çº§è¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ã€‚</li>
<li>è€¦åˆNARæ¨¡å‹åŸºäºARæ¨¡å‹çš„è¾“å‡ºé¢„æµ‹è¯¦ç»†æ ‡è®°ã€‚</li>
<li>åŸºäºæ­¤æ¶æ„æ„å»ºçš„å¹¶è¡ŒGPTæ—¨åœ¨é€šè¿‡å…¶å¹¶è¡Œç»“æ„æ”¹è¿›é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³åˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-12f0de09a0900c1d7a9800be441359cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa37142f69f7e05c7ee1f1ccc92eb754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92c8ad1224a54c6c9f7d2d2a40d2c8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d0677a13b72211796ef8e9ac8a1254d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Fine-tuning-for-Better-Few-Shot-Prompting-An-Empirical-Comparison-for-Short-Answer-Grading"><a href="#Fine-tuning-for-Better-Few-Shot-Prompting-An-Empirical-Comparison-for-Short-Answer-Grading" class="headerlink" title="Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for   Short Answer Grading"></a>Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for   Short Answer Grading</h2><p><strong>Authors:Joel Walsh, Siddarth Mamidanna, Benjamin Nye, Mark Core, Daniel Auerbach</strong></p>
<p>Research to improve Automated Short Answer Grading has recently focused on Large Language Models (LLMs) with prompt engineering and no- or few-shot prompting to achieve best results. This is in contrast to the fine-tuning approach, which has historically required large-scale compute clusters inaccessible to most users. New closed-model approaches such as OpenAIâ€™s fine-tuning service promise results with as few as 100 examples, while methods using open weights such as quantized low-rank adaptive (QLORA) can be used to fine-tune models on consumer GPUs. We evaluate both of these fine-tuning methods, measuring their interaction with few-shot prompting for automated short answer grading (ASAG) with structured (JSON) outputs. Our results show that finetuning with small amounts of data has limited utility for Llama open-weight models, but that fine-tuning methods can outperform few-shot baseline instruction-tuned LLMs for OpenAIâ€™s closed models. While our evaluation set is limited, we find some evidence that the observed benefits of finetuning may be impacted by the domain subject matter. Lastly, we observed dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by seeding the initial training examples with a significant amount of cheaply generated synthetic training data. </p>
<blockquote>
<p>è¿‘æœŸå…³äºæé«˜è‡ªåŠ¨ç®€ç­”è¯„åˆ†çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæç¤ºå·¥ç¨‹å’Œæ— æˆ–å°‘é‡æ ·æœ¬æç¤ºï¼Œä»¥å–å¾—æœ€ä½³ç»“æœã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å½¢æˆå¯¹æ¯”ï¼Œåè€…åœ¨å†å²ä¸Šéœ€è¦å¤§è§„æ¨¡è®¡ç®—é›†ç¾¤ï¼Œå¤§å¤šæ•°ç”¨æˆ·æ— æ³•æ¥è§¦ã€‚æ–°çš„å°é—­æ¨¡å‹æ–¹æ³•ï¼Œå¦‚OpenAIçš„å¾®è°ƒæœåŠ¡ï¼Œä»…ç”¨100ä¸ªæ ·æœ¬å³å¯è·å¾—ç»“æœï¼Œè€Œä½¿ç”¨å…¬å¼€æƒé‡çš„æ–¹æ³•ï¼Œå¦‚é‡åŒ–ä½ç§©è‡ªé€‚åº”ï¼ˆQLORAï¼‰ï¼Œå¯åœ¨æ¶ˆè´¹è€…GPUä¸Šè¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚æˆ‘ä»¬è¯„ä¼°äº†è¿™ä¸¤ç§å¾®è°ƒæ–¹æ³•ï¼Œæµ‹é‡å®ƒä»¬åœ¨å°‘é‡æç¤ºä¸è‡ªåŠ¨ç®€ç­”è¯„åˆ†ï¼ˆASAGï¼‰çš„äº¤äº’ä½œç”¨ä¸‹çš„ç»“æ„åŒ–ï¼ˆJSONï¼‰è¾“å‡ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œå¾®è°ƒå¯¹äºLlamaå…¬å¼€æƒé‡æ¨¡å‹çš„æ•ˆç”¨æœ‰é™ï¼Œä½†å¯¹äºOpenAIçš„å°é—­æ¨¡å‹ï¼Œå¾®è°ƒæ–¹æ³•å¯ä»¥åœ¨æŒ‡ä»¤è°ƒä¼˜çš„LLMä¸Šè¶…è¶Šå°‘é‡æ ·æœ¬åŸºçº¿ã€‚è™½ç„¶æˆ‘ä»¬çš„è¯„ä¼°é›†æœ‰é™ï¼Œä½†æˆ‘ä»¬å‘ç°è§‚å¯Ÿåˆ°çš„å¾®è°ƒæ•ˆç›Šå¯èƒ½å—åˆ°é¢†åŸŸä¸»é¢˜çš„å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œé€šè¿‡åœ¨åˆå§‹è®­ç»ƒç¤ºä¾‹ä¸­å¼•å…¥å¤§é‡ä½æˆæœ¬ç”Ÿæˆçš„åˆæˆè®­ç»ƒæ•°æ®ï¼ŒLLama 3.1 8B-Instructå…¬å¼€æƒé‡æ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04063v1">PDF</a> Proceedings of the Second Workshop on Automated Evaluation of   Learning and Assessment Content co-located with 26th International Conference   on Artificial Intelligence in Education (AIED 2025)</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶é›†ä¸­åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨ç®€ç­”è¯„åˆ†ä»»åŠ¡çš„æ”¹è¿›ä¸Šï¼Œé€šè¿‡æç¤ºå·¥ç¨‹å’Œæ— &#x2F;å°‘æ ·æœ¬æç¤ºè¾¾åˆ°æœ€ä½³æ•ˆæœã€‚ç›¸è¾ƒäºè¿‡å»éœ€è¦å¤§é‡è®¡ç®—é›†ç¾¤çš„å¾®è°ƒæ–¹æ³•ï¼Œæ–°çš„å°é—­æ¨¡å‹æ–¹æ³•å¦‚OpenAIçš„å¾®è°ƒæœåŠ¡ä»…éœ€å°‘é‡æ ·æœ¬å³å¯è·å¾—ç»“æœï¼ŒåŒæ—¶ä½¿ç”¨å…¬å¼€æƒé‡çš„æ–¹æ³•å¦‚é‡åŒ–ä½ç§©è‡ªé€‚åº”ï¼ˆQLORAï¼‰å¯ä»¥åœ¨æ¶ˆè´¹è€…GPUä¸Šè¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œå¯¹äºLlamaå…¬å¼€æƒé‡æ¨¡å‹ï¼Œå°æ•°æ®é‡çš„å¾®è°ƒæ•ˆç”¨æœ‰é™ï¼›ä½†å¯¹äºOpenAIçš„å°é—­æ¨¡å‹ï¼Œå¾®è°ƒæ–¹æ³•ä¼˜äºå°‘æ ·æœ¬åŸºçº¿æŒ‡ä»¤è°ƒä¼˜çš„LLMã€‚è¯„ä»·é›†è™½æœ‰é™ï¼Œä½†è§‚å¯Ÿåˆ°å¾®è°ƒçš„å¥½å¤„å¯èƒ½å—é¢†åŸŸä¸»é¢˜å½±å“ã€‚æ­¤å¤–ï¼Œå¯¹Llama 3.1 8B-Instructå…¬å¼€æƒé‡æ¨¡å‹ï¼Œé€šè¿‡åˆå§‹è®­ç»ƒç¤ºä¾‹å¼•å…¥å¤§é‡å»‰ä»·ç”Ÿæˆçš„åˆæˆè®­ç»ƒæ•°æ®ï¼Œå¯è§‚å¯Ÿåˆ°æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ç®€ç­”è¯„åˆ†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹å’Œæ— &#x2F;å°‘æ ·æœ¬æç¤ºè¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</li>
<li>ä¸ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæ–°çš„å°é—­æ¨¡å‹æ–¹æ³•å’Œå…¬å¼€æƒé‡æ–¹æ³•å¦‚QLORAå¯åœ¨æœ‰é™èµ„æºä¸‹å®ç°æ¨¡å‹å¾®è°ƒã€‚</li>
<li>å¯¹äºæŸäº›æ¨¡å‹ï¼ˆå¦‚Llamaå…¬å¼€æƒé‡æ¨¡å‹ï¼‰ï¼Œå°æ•°æ®é‡çš„å¾®è°ƒæ•ˆæœæœ‰é™ã€‚</li>
<li>OpenAIçš„å°é—­æ¨¡å‹åœ¨å¾®è°ƒæ–¹æ³•ä¸Šè¡¨ç°å‡ºä¼˜äºå°‘æ ·æœ¬åŸºçº¿æŒ‡ä»¤è°ƒä¼˜çš„LLMçš„æ•ˆæœã€‚</li>
<li>è¯„ä¼°ç»“æœå¯èƒ½å—é¢†åŸŸä¸»é¢˜å½±å“ã€‚</li>
<li>åˆå§‹è®­ç»ƒç¤ºä¾‹ä¸­å¼•å…¥å¤§é‡åˆæˆè®­ç»ƒæ•°æ®å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b06cc3fe78f1f99b6d6f6ee45f023072.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1b697fa05c5fcecc91d2d2873efe39e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5bbe84da1a24dc6693da5c891038a24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-793f0fdee84c333ec5f6d0a048cfdd2b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Globally-Predictable-k-Space-Interpolation-A-White-box-Transformer-Approach"><a href="#Towards-Globally-Predictable-k-Space-Interpolation-A-White-box-Transformer-Approach" class="headerlink" title="Towards Globally Predictable k-Space Interpolation: A White-box   Transformer Approach"></a>Towards Globally Predictable k-Space Interpolation: A White-box   Transformer Approach</h2><p><strong>Authors:Chen Luo, Qiyu Jin, Taofeng Xie, Xuemei Wang, Huayu Wang, Congcong Liu, Liming Tang, Guoqing Chen, Zhuo-Xu Cui, Dong Liang</strong></p>
<p>Interpolating missing data in k-space is essential for accelerating imaging. However, existing methods, including convolutional neural network-based deep learning, primarily exploit local predictability while overlooking the inherent global dependencies in k-space. Recently, Transformers have demonstrated remarkable success in natural language processing and image analysis due to their ability to capture long-range dependencies. This inspires the use of Transformers for k-space interpolation to better exploit its global structure. However, their lack of interpretability raises concerns regarding the reliability of interpolated data. To address this limitation, we propose GPI-WT, a white-box Transformer framework based on Globally Predictable Interpolation (GPI) for k-space. Specifically, we formulate GPI from the perspective of annihilation as a novel k-space structured low-rank (SLR) model. The global annihilation filters in the SLR model are treated as learnable parameters, and the subgradients of the SLR model naturally induce a learnable attention mechanism. By unfolding the subgradient-based optimization algorithm of SLR into a cascaded network, we construct the first white-box Transformer specifically designed for accelerated MRI. Experimental results demonstrate that the proposed method significantly outperforms state-of-the-art approaches in k-space interpolation accuracy while providing superior interpretability. </p>
<blockquote>
<p>åœ¨æˆåƒä¸­ï¼Œå¯¹kç©ºé—´ä¸­çš„ç¼ºå¤±æ•°æ®è¿›è¡Œæ’å€¼æ˜¯è‡³å…³é‡è¦çš„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ·±åº¦å­¦ä¹ ï¼Œä¸»è¦åˆ©ç”¨å±€éƒ¨é¢„æµ‹æ€§è€Œå¿½ç•¥äº†kç©ºé—´ä¸­å›ºæœ‰çš„å…¨å±€ä¾èµ–æ€§ã€‚æœ€è¿‘ï¼ŒTransformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒåˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œè¿™å½’åŠŸäºå…¶æ•æ‰é•¿æœŸä¾èµ–æ€§çš„èƒ½åŠ›ã€‚è¿™å¯å‘æˆ‘ä»¬å°†Transformerç”¨äºkç©ºé—´æ’å€¼ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨å…¶å…¨å±€ç»“æ„ã€‚ç„¶è€Œï¼Œå…¶ç¼ºä¹å¯è§£é‡Šæ€§å¼•å‘äº†äººä»¬å¯¹æ’å€¼æ•°æ®å¯é æ€§çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GPI-WTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå…¨å±€å¯é¢„æµ‹æ’å€¼ï¼ˆGPIï¼‰çš„white-box Transformeræ¡†æ¶ï¼Œç”¨äºkç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»æ¶ˆé™¤çš„è§’åº¦åˆ¶å®šGPIä½œä¸ºä¸€ç§æ–°å‹çš„kç©ºé—´ç»“æ„åŒ–ä½ç§©ï¼ˆSLRï¼‰æ¨¡å‹ã€‚SLRæ¨¡å‹ä¸­çš„å…¨å±€æ¶ˆé™¤æ»¤æ³¢å™¨è¢«è§†ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼ŒSLRæ¨¡å‹çš„å­æ¢¯åº¦è‡ªç„¶åœ°å¼•å‘äº†ä¸€ç§å¯å­¦ä¹ çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚é€šè¿‡å°†åŸºäºå­æ¢¯åº¦çš„SLRä¼˜åŒ–ç®—æ³•å±•å¼€ä¸ºçº§è”ç½‘ç»œï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªä¸“é—¨ç”¨äºåŠ é€ŸMRIçš„white-box Transformerã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨kç©ºé—´æ’å€¼ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼ŒåŒæ—¶æä¾›äº†å‡ºè‰²çš„å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04051v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå…¨å±€å¯é¢„æµ‹æ’å€¼ï¼ˆGPIï¼‰çš„ç™½è‰²ç›’å­Transformeræ¡†æ¶GPI-WTï¼Œç”¨äºåŠ é€ŸMRIæˆåƒä¸­çš„kç©ºé—´æ’å€¼ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»ç­è¿¹è§’åº¦æ„å»ºGPIï¼Œå½¢æˆæ–°å‹kç©ºé—´ç»“æ„åŒ–ä½ç§©ï¼ˆSLRï¼‰æ¨¡å‹ï¼Œå¹¶é€šè¿‡å­¦ä¹ å¯è°ƒæ•´å…¨å±€ç­è¿¹è¿‡æ»¤å™¨ã€‚å…¶åˆ©ç”¨SLRæ¨¡å‹çš„å­æ¢¯åº¦è‡ªç„¶è¯±å¯¼å‡ºå¯å­¦ä¹ çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶å°†åŸºäºå­æ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•å±•å¼€ä¸ºçº§è”ç½‘ç»œï¼Œæ„å»ºä¸“é—¨é’ˆå¯¹åŠ é€ŸMRIçš„ç™½è‰²ç›’å­Transformerã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨kç©ºé—´æ’å€¼ç²¾åº¦ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒåŒæ—¶æä¾›ä¼˜è¶Šçš„è§£è¯»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ’å€¼åœ¨åŠ é€Ÿæˆåƒä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦åˆ©ç”¨å±€éƒ¨å¯é¢„æµ‹æ€§ï¼Œå¿½ç•¥äº†kç©ºé—´çš„å›ºæœ‰å…¨å±€ä¾èµ–æ€§ã€‚</li>
<li>Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒåˆ†æä¸­çš„æˆåŠŸï¼Œæ¿€å‘äº†å…¶åœ¨kç©ºé—´æ’å€¼ä¸­çš„åº”ç”¨ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨å…¨å±€ç»“æ„ã€‚</li>
<li>ç¼ºä¹è§£é‡Šæ€§ä»æ˜¯Transformeråº”ç”¨çš„ä¸»è¦é¡¾è™‘ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç™½è‰²ç›’å­Transformeræ¡†æ¶GPI-WTã€‚</li>
<li>GPI-WTåŸºäºå…¨å±€å¯é¢„æµ‹æ’å€¼ï¼ˆGPIï¼‰å’Œkç©ºé—´ç»“æ„åŒ–ä½ç§©ï¼ˆSLRï¼‰æ¨¡å‹ï¼Œä»ç­è¿¹è§’åº¦æ„å»ºã€‚</li>
<li>SLRæ¨¡å‹ä¸­çš„å…¨å±€ç­è¿¹è¿‡æ»¤å™¨è¢«è§†ä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œå…¶å­æ¢¯åº¦è‡ªç„¶å¼•å¯¼å¯å­¦ä¹ çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>é€šè¿‡å°†SLRæ¨¡å‹çš„å­æ¢¯åº¦ä¼˜åŒ–ç®—æ³•å±•å¼€ä¸ºçº§è”ç½‘ç»œï¼Œåˆ›å»ºäº†ä¸“é—¨é’ˆå¯¹åŠ é€ŸMRIçš„ç™½è‰²ç›’å­Transformerã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-776ecf7060c4154e5502df4c0902f13f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40cb0f8f4345f13c25ab9a577621f78c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VisualTrans-A-Benchmark-for-Real-World-Visual-Transformation-Reasoning"><a href="#VisualTrans-A-Benchmark-for-Real-World-Visual-Transformation-Reasoning" class="headerlink" title="VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning"></a>VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning</h2><p><strong>Authors:Yuheng Ji, Yipu Wang, Yuyang Liu, Xiaoshuai Hao, Yue Liu, Yuting Zhao, Huaihai Lyu, Xiaolong Zheng</strong></p>
<p>Visual transformation reasoning (VTR) is a vital cognitive capability that empowers intelligent agents to understand dynamic scenes, model causal relationships, and predict future states, and thereby guiding actions and laying the foundation for advanced intelligent systems. However, existing benchmarks suffer from a sim-to-real gap, limited task complexity, and incomplete reasoning coverage, limiting their practical use in real-world scenarios. To address these limitations, we introduce VisualTrans, the first comprehensive benchmark specifically designed for VTR in real-world human-object interaction scenarios. VisualTrans encompasses 12 semantically diverse manipulation tasks and systematically evaluates three essential reasoning dimensions - spatial, procedural, and quantitative - through 6 well-defined subtask types. The benchmark features 472 high-quality question-answer pairs in various formats, including multiple-choice, open-ended counting, and target enumeration. We introduce a scalable data construction pipeline built upon first-person manipulation videos, which integrates task selection, image pair extraction, automated metadata annotation with large multimodal models, and structured question generation. Human verification ensures the final benchmark is both high-quality and interpretable. Evaluations of various state-of-the-art vision-language models show strong performance in static spatial tasks. However, they reveal notable shortcomings in dynamic, multi-step reasoning scenarios, particularly in areas like intermediate state recognition and transformation sequence planning. These findings highlight fundamental weaknesses in temporal modeling and causal reasoning, providing clear directions for future research aimed at developing more capable and generalizable VTR systems. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/WangYipu2002/VisualTrans">https://github.com/WangYipu2002/VisualTrans</a>. </p>
<blockquote>
<p>è§†è§‰è½¬æ¢æ¨ç†ï¼ˆVTRï¼‰æ˜¯ä¸€ç§é‡è¦çš„è®¤çŸ¥èƒ½åŠ›ï¼Œèµ‹äºˆæ™ºèƒ½ä¸»ä½“ç†è§£åŠ¨æ€åœºæ™¯ã€æ¨¡æ‹Ÿå› æœå…³ç³»ä»¥åŠé¢„æµ‹æœªæ¥çŠ¶æ€çš„èƒ½åŠ›ï¼Œä»è€Œä¸ºè¡ŒåŠ¨æä¾›æŒ‡å¯¼ï¼Œå¹¶ä¸ºå…ˆè¿›çš„æ™ºèƒ½ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨æ¨¡æ‹Ÿåˆ°ç°å®çš„å·®è·ã€ä»»åŠ¡å¤æ‚æ€§æœ‰é™ä»¥åŠæ¨ç†è¦†ç›–ä¸å®Œæ•´ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VisualTransï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºç°å®ä¸–ç•Œä¸­çš„äººæœºäº¤äº’åœºæ™¯ä¸­çš„VTRè®¾è®¡çš„é¦–ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p>VisualTransæ¶µç›–äº†12ç§è¯­ä¹‰å¤šæ ·çš„æ“ä½œä»»åŠ¡ï¼Œå¹¶é€šè¿‡6ç§å®šä¹‰è‰¯å¥½çš„å­ä»»åŠ¡ç±»å‹ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†ç©ºé—´ã€ç¨‹åºå’Œå®šé‡ä¸‰ä¸ªé‡è¦çš„æ¨ç†ç»´åº¦ã€‚åŸºå‡†æµ‹è¯•åŒ…å«472ç»„é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œå½¢å¼å¤šæ ·ï¼ŒåŒ…æ‹¬é€‰æ‹©é¢˜ã€å¼€æ”¾å¼è®¡æ•°å’Œç›®æ ‡æšä¸¾ç­‰ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºç¬¬ä¸€äººç§°æ“ä½œè§†é¢‘çš„å¯æ‰©å±•æ•°æ®æ„å»ºæµç¨‹ï¼Œè¯¥æµç¨‹åŒ…æ‹¬ä»»åŠ¡é€‰æ‹©ã€å›¾åƒå¯¹æå–ã€ä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œè‡ªåŠ¨åŒ–å…ƒæ•°æ®æ³¨é‡Šä»¥åŠç»“æ„åŒ–é—®é¢˜ç”Ÿæˆã€‚äººä¸ºéªŒè¯ç¡®ä¿äº†æœ€ç»ˆåŸºå‡†æµ‹è¯•æ—¢é«˜è´¨é‡åˆæ˜“äºè§£é‡Šã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04043v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰è½¬æ¢æ¨ç†ï¼ˆVTRï¼‰æ˜¯æ™ºèƒ½ä¸»ä½“ç†è§£åŠ¨æ€åœºæ™¯ã€æ¨¡æ‹Ÿå› æœå…³ç³»å¹¶é¢„æµ‹æœªæ¥çŠ¶æ€çš„é‡è¦è®¤çŸ¥èƒ½åŠ›ï¼Œå®ƒä¸ºè¡ŒåŠ¨æŒ‡å¯¼å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºå…ˆè¿›æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æ”¯æ’‘ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨æ¨¡æ‹Ÿåˆ°ç°å®çš„å·®è·ã€ä»»åŠ¡å¤æ‚æ€§æœ‰é™å’Œæ¨ç†è¦†ç›–ä¸å…¨é¢ç­‰å±€é™æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VisualTransï¼Œè¿™æ˜¯ä¸“ä¸ºç°å®ä¸–ç•Œä¸­çš„äººæœºäº¤äº’åœºæ™¯ä¸­çš„VTRè®¾è®¡çš„é¦–ä¸ªå…¨é¢åŸºå‡†æµ‹è¯•ã€‚VisualTransåŒ…å«12ä¸ªè¯­ä¹‰å¤šæ ·çš„æ“ä½œä»»åŠ¡ï¼Œå¹¶é€šè¿‡6ä¸ªå®šä¹‰æ˜ç¡®çš„ä»»åŠ¡ç±»å‹ç³»ç»Ÿåœ°è¯„ä¼°ç©ºé—´ã€ç¨‹åºå’Œå®šé‡ä¸‰ä¸ªåŸºæœ¬æ¨ç†ç»´åº¦ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«472ç»„é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼ŒåŒ…æ‹¬å¤šç§å½¢å¼ï¼Œå¦‚é€‰æ‹©é¢˜ã€å¼€æ”¾å¼è®¡æ•°å’Œç›®æ ‡æšä¸¾ç­‰ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®æ„å»ºç®¡é“ï¼Œè¯¥ç®¡é“å»ºç«‹åœ¨ç¬¬ä¸€äººç§°æ“ä½œè§†é¢‘ä¹‹ä¸Šï¼Œé›†æˆäº†ä»»åŠ¡é€‰æ‹©ã€å›¾åƒå¯¹æå–ã€ä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è‡ªåŠ¨åŒ–å…ƒæ•°æ®æ³¨é‡Šå’Œç»“æ„åŒ–é—®é¢˜ç”Ÿæˆã€‚äººç±»çš„éªŒè¯ç¡®ä¿äº†æœ€ç»ˆåŸºå‡†æµ‹è¯•æ—¢é«˜è´¨é‡åˆæ˜“äºè§£é‡Šã€‚å¯¹å„ç§æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°è¡¨æ˜ï¼Œå®ƒä»¬åœ¨é™æ€ç©ºé—´ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨åŠ¨æ€å¤šæ­¥éª¤æ¨ç†åœºæ™¯ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­é—´çŠ¶æ€è¯†åˆ«å’Œè½¬æ¢åºåˆ—è§„åˆ’æ–¹é¢ï¼Œå®ƒä»¬è¡¨ç°å‡ºæ˜æ˜¾çš„ä¸è¶³ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æ—¶é—´å»ºæ¨¡å’Œå› æœå…³ç³»æ¨ç†ä¸­çš„åŸºæœ¬å¼±ç‚¹ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ˜ç¡®æ–¹å‘ï¼Œæ—¨åœ¨å¼€å‘æ›´å…·èƒ½åŠ›å’Œé€šç”¨æ€§çš„VTRç³»ç»Ÿã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WangYipu2002/VisualTrans%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/WangYipu2002/VisualTransè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†è§‰è½¬æ¢æ¨ç†ï¼ˆVTRï¼‰æ˜¯æ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›ï¼Œæ¶‰åŠç†è§£åŠ¨æ€åœºæ™¯ã€æ¨¡æ‹Ÿå› æœå…³ç³»å’Œé¢„æµ‹æœªæ¥çŠ¶æ€ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨æ¨¡æ‹Ÿåˆ°ç°å®çš„å·®è·ã€ä»»åŠ¡å¤æ‚æ€§æœ‰é™å’Œæ¨ç†è¦†ç›–ä¸å…¨é¢ç­‰å±€é™æ€§ã€‚</li>
<li>VisualTransåŸºå‡†æµ‹è¯•æ˜¯ä¸“ä¸ºç°å®ä¸–ç•Œä¸­çš„äººæœºäº¤äº’åœºæ™¯ä¸­çš„VTRè®¾è®¡çš„é¦–ä¸ªå…¨é¢åŸºå‡†æµ‹è¯•ã€‚</li>
<li>VisualTransåŒ…å«12ä¸ªè¯­ä¹‰å¤šæ ·çš„æ“ä½œä»»åŠ¡å’Œ6ç§ä»»åŠ¡ç±»å‹ï¼Œè¯„ä¼°ç©ºé—´ã€ç¨‹åºå’Œå®šé‡ä¸‰ä¸ªåŸºæœ¬æ¨ç†ç»´åº¦ã€‚</li>
<li>åŸºå‡†æµ‹è¯•åŒ…å«é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼ŒåŒ…æ‹¬å¤šç§å½¢å¼ï¼Œå¦‚é€‰æ‹©é¢˜ã€å¼€æ”¾å¼è®¡æ•°å’Œç›®æ ‡æšä¸¾ç­‰ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨é™æ€ç©ºé—´ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŠ¨æ€å¤šæ­¥éª¤æ¨ç†åœºæ™¯ä¸­å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­é—´çŠ¶æ€è¯†åˆ«å’Œè½¬æ¢åºåˆ—è§„åˆ’æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ea52f74b5c7a0cdfb0789c1d13fa357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14c231dbe1f362a9b289522c68b71eae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57c1ea0ae9574b837d3e202ea1b641e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63408dd1056ab254fe45dc38b20e9174.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14e12d50bcb50bc707e32093acffba8f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Probing-the-Gaps-in-ChatGPT-Live-Video-Chat-for-Real-World-Assistance-for-People-who-are-Blind-or-Visually-Impaired"><a href="#Probing-the-Gaps-in-ChatGPT-Live-Video-Chat-for-Real-World-Assistance-for-People-who-are-Blind-or-Visually-Impaired" class="headerlink" title="Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance   for People who are Blind or Visually Impaired"></a>Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance   for People who are Blind or Visually Impaired</h2><p><strong>Authors:Ruei-Che Chang, Rosiana Natalie, Wenqian Xu, Jovan Zheng Feng Yap, Anhong Guo</strong></p>
<p>Recent advancements in large multimodal models have provided blind or visually impaired (BVI) individuals with new capabilities to interpret and engage with the real world through interactive systems that utilize live video feeds. However, the potential benefits and challenges of such capabilities to support diverse real-world assistive tasks remain unclear. In this paper, we present findings from an exploratory study with eight BVI participants. Participants used ChatGPTâ€™s Advanced Voice with Video, a state-of-the-art live video AI released in late 2024, in various real-world scenarios, from locating objects to recognizing visual landmarks, across unfamiliar indoor and outdoor environments. Our findings indicate that current live video AI effectively provides guidance and answers for static visual scenes but falls short in delivering essential live descriptions required in dynamic situations. Despite inaccuracies in spatial and distance information, participants leveraged the provided visual information to supplement their mobility strategies. Although the system was perceived as human-like due to high-quality voice interactions, assumptions about usersâ€™ visual abilities, hallucinations, generic responses, and a tendency towards sycophancy led to confusion, distrust, and potential risks for BVI users. Based on the results, we discuss implications for assistive video AI agents, including incorporating additional sensing capabilities for real-world use, determining appropriate intervention timing beyond turn-taking interactions, and addressing ecological and safety concerns. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è¿›æ­¥ä¸ºç›²äººæˆ–è§†åŠ›å—æŸï¼ˆBVIï¼‰ä¸ªä½“æä¾›äº†é€šè¿‡åˆ©ç”¨å®æ—¶è§†é¢‘åé¦ˆçš„äº’åŠ¨ç³»ç»Ÿæ¥è§£è¯»å’Œä¸ç°å®ä¸–ç•Œäº’åŠ¨çš„æ–°èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ­¤ç±»èƒ½åŠ›åœ¨æ”¯æŒå¤šæ ·åŒ–çš„ç°å®ä¸–ç•Œè¾…åŠ©ä»»åŠ¡æ–¹é¢çš„æ½œåœ¨ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ä»ä¸æ˜ç¡®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸å…«åBVIå‚ä¸è€…è¿›è¡Œçš„æ¢ç´¢æ€§ç ”ç©¶çš„å‘ç°ã€‚å‚ä¸è€…åœ¨å„ç§ç°å®åœºæ™¯ä¸­ä½¿ç”¨äº†ChatGPTçš„å…ˆè¿›è¯­éŸ³å’Œè§†é¢‘åŠŸèƒ½ï¼Œè¿™æ˜¯ä¸€é¡¹äº2024å¹´æœ«å‘å¸ƒçš„æœ€æ–°å®æ—¶è§†é¢‘äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œä»å¯»æ‰¾ç‰©ä½“åˆ°è¯†åˆ«è§†è§‰åœ°æ ‡ï¼Œæ¶‰åŠä¸ç†Ÿæ‚‰çš„å®¤å†…å’Œå®¤å¤–ç¯å¢ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å®æ—¶è§†é¢‘AIåœ¨æä¾›é™æ€è§†è§‰åœºæ™¯çš„æŒ‡å¯¼å’Œç­”æ¡ˆæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†åœ¨æä¾›åŠ¨æ€æƒ…å†µä¸‹æ‰€éœ€çš„å…³é”®å®æ—¶æè¿°æ–¹é¢å´ç›¸å½¢è§ç»Œã€‚å°½ç®¡åœ¨ç©ºé—´å’Œè·ç¦»ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸å‡†ç¡®ä¹‹å¤„ï¼Œå‚ä¸è€…è¿˜æ˜¯åˆ©ç”¨æ‰€æä¾›çš„è§†è§‰ä¿¡æ¯æ¥è¡¥å……ä»–ä»¬çš„è¡ŒåŠ¨ç­–ç•¥ã€‚å°½ç®¡ç”±äºé«˜è´¨é‡çš„å£°éŸ³äº’åŠ¨ï¼Œç³»ç»Ÿè¢«æ„ŸçŸ¥ä¸ºäººç±»èˆ¬çš„å­˜åœ¨ï¼Œä½†å¯¹äºç”¨æˆ·çš„è§†è§‰èƒ½åŠ›çš„å‡è®¾ã€å¹»è§‰ã€é€šç”¨å›åº”å’Œå¥‰æ‰¿çš„å€¾å‘å¯¼è‡´äº†æ··æ·†ã€ä¸ä¿¡ä»»å’Œç›²äººç”¨æˆ·çš„æ½œåœ¨é£é™©ã€‚åŸºäºç ”ç©¶ç»“æœï¼Œæˆ‘ä»¬è®¨è®ºäº†è¾…åŠ©è§†é¢‘AIä»£ç†çš„å¯ç¤ºï¼ŒåŒ…æ‹¬å¢åŠ ç°å®ä¸–ç•Œä½¿ç”¨çš„é¢å¤–æ„ŸçŸ¥èƒ½åŠ›ã€ç¡®å®šè¶…è¶Šè½®æµäº’åŠ¨çš„é€‚å½“å¹²é¢„æ—¶é—´ä»¥åŠè§£å†³ç”Ÿæ€å’Œå®‰å…¨é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03651v1">PDF</a> ACM ASSETS 2025</p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºç›²äººæˆ–è§†è§‰éšœç¢ï¼ˆBVIï¼‰ä¸ªä½“æä¾›äº†é€šè¿‡äº¤äº’å¼ç³»ç»Ÿè§£è¯»å¹¶å‚ä¸åˆ°ç°å®ä¸–ç•Œçš„æ–°èƒ½åŠ›ï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨å®æ—¶è§†é¢‘æµã€‚æœ¬æ–‡é€šè¿‡ä¸€é¡¹åŒ…å«å…«åBVIå‚ä¸è€…çš„æ¢ç´¢æ€§ç ”ç©¶æ¥æ¢è®¨è¿™ä¸€æŠ€æœ¯åœ¨ç°å®ä¸–ç•Œä¸­æ‰€é¢ä¸´çš„æ½œåŠ›å’ŒæŒ‘æˆ˜ã€‚å°½ç®¡åœ¨é™æ€è§†è§‰åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨æ€åœºæ™¯ä¸­æä¾›å¿…è¦çš„å®æ—¶æè¿°æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚å‚ä¸è€…è™½ç„¶èƒ½å¤Ÿåˆ©ç”¨æä¾›çš„è§†è§‰ä¿¡æ¯è¾…åŠ©å…¶ç§»åŠ¨ç­–ç•¥ï¼Œä½†å¯¹ç”¨æˆ·çš„è§†è§‰èƒ½åŠ›åšå‡ºå‡è®¾ã€å¹»è±¡ã€é€šç”¨å“åº”ä»¥åŠè¿åˆå€¾å‘å¯èƒ½å¯¼è‡´æ··æ·†ã€ä¸ä¿¡ä»»ä»¥åŠå¯¹BVIç”¨æˆ·å­˜åœ¨æ½œåœ¨é£é™©ã€‚éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›AIåŠ©æ‰‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€å¹²é¢„æ—¶æœºå’Œç”Ÿæ€å®‰å…¨é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸ºç›²äººæˆ–è§†è§‰éšœç¢è€…æä¾›äº†é€šè¿‡å®æ—¶è§†é¢‘æµè§£è¯»ç°å®ä¸–ç•Œçš„äº¤äº’ç³»ç»Ÿæ–°èƒ½åŠ›ã€‚</li>
<li>å®æ—¶è§†é¢‘AIåœ¨é™æ€è§†è§‰åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŠ¨æ€åœºæ™¯ä¸­æä¾›å¿…è¦çš„å®æ—¶æè¿°æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å°½ç®¡ç©ºé—´åŠè·ç¦»ä¿¡æ¯å­˜åœ¨ä¸å‡†ç¡®çš„æƒ…å†µï¼Œä½†å‚ä¸è€…ä»èƒ½åˆ©ç”¨æä¾›çš„è§†è§‰ä¿¡æ¯è¾…åŠ©ç§»åŠ¨ç­–ç•¥ã€‚</li>
<li>ç³»ç»Ÿåœ¨äººæ€§åŒ–æ–¹é¢åšå¾—è¾ƒå¥½ï¼Œä½†ä»å­˜åœ¨å‡è®¾ç”¨æˆ·è§†è§‰èƒ½åŠ›ã€å¹»è±¡ã€é€šç”¨å“åº”å’Œè¿åˆå€¾å‘ç­‰é—®é¢˜ã€‚</li>
<li>å¯¹è§†é¢‘AIåŠ©æ‰‹çš„æ”¹è¿›éœ€è¦å¢åŠ æ„ŸçŸ¥èƒ½åŠ›ä»¥é€‚åº”ç°å®ä¸–ç•Œçš„å¤æ‚ç¯å¢ƒã€‚</li>
<li>éœ€è¦ç¡®å®šé€‚å½“çš„å¹²é¢„æ—¶æœºï¼Œè¶…è¶Šå¯¹è¯ä¸­çš„è½®æµäº¤äº’æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-000d0d15e19365a31233e2b710aee7b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a0c2c5b65f8c587aa0e8e5024512ebf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf28b6d848eedef4694642c88ff98a9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa331315d12ed278fea76594516dc144.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Tackling-Distribution-Shift-in-LLM-via-KILO-Knowledge-Instructed-Learning-for-Continual-Adaptation"><a href="#Tackling-Distribution-Shift-in-LLM-via-KILO-Knowledge-Instructed-Learning-for-Continual-Adaptation" class="headerlink" title="Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed   Learning for Continual Adaptation"></a>Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed   Learning for Continual Adaptation</h2><p><strong>Authors:Iing Muttakhiroh, Thomas Fevens</strong></p>
<p>Large Language Models (LLMs) often suffer from performance degradation when faced with domain shifts, primarily due to catastrophic forgetting. In this work, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation), a novel continual learning framework that integrates dynamic knowledge graphs with instruction tuning. By leveraging retrieved domain-specific knowledge as guidance during training, KILO enhances both adaptability to new domains and retention of previously acquired knowledge. We pretrain our model on WikiText-103 and evaluate sequential adaptation across four diverse target domains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that KILO consistently outperforms strong baselines, including continual fine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward transfer, F1 score, retention rate, and training efficiency. These results highlight the effectiveness of combining structured knowledge retrieval and instruction prompting to overcome domain shift challenges in continual learning scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢ä¸´é¢†åŸŸè¿ç§»æ—¶å¸¸å¸¸ä¼šå‡ºç°æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¾éš¾æ€§é—å¿˜é€ æˆçš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†KILOï¼ˆç”¨äºæŒç»­é€‚åº”çš„çŸ¥è¯†æŒ‡å¯¼å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œå®ƒç»“åˆäº†åŠ¨æ€çŸ¥è¯†å›¾è°±å’ŒæŒ‡ä»¤å¾®è°ƒã€‚é€šè¿‡åˆ©ç”¨æ£€ç´¢åˆ°çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡å¯¼ï¼ŒKILOæé«˜äº†å¯¹æ–°é¢†åŸŸçš„é€‚åº”èƒ½åŠ›å’Œå¯¹å…ˆå‰è·å–çŸ¥è¯†çš„ä¿æŒèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨WikiText-103ä¸Šé¢„è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶åœ¨å››ä¸ªä¸åŒçš„ç›®æ ‡é¢†åŸŸï¼ˆBioASQã€SciQã€TweetEvalå’ŒMINDï¼‰è¿›è¡Œé¡ºåºé€‚åº”è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒKILOåœ¨å‘åè¿ç§»ã€å‘å‰è¿ç§»ã€F1åˆ†æ•°ã€ä¿æŒç‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢å‡ä¼˜äºæŒç»­å¾®è°ƒã€ERNIE 2.0å’ŒCPTç­‰å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚è¿™äº›ç»“æœçªå‡ºäº†åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸­ç»“åˆç»“æ„åŒ–çŸ¥è¯†æ£€ç´¢å’ŒæŒ‡ä»¤æç¤ºä»¥å…‹æœé¢†åŸŸè¿ç§»æŒ‘æˆ˜çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03571v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºKILOçš„æ–°å‹æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢ä¸´é¢†åŸŸè¿ç§»æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚KILOé€šè¿‡ç»“åˆåŠ¨æ€çŸ¥è¯†å›¾è°±å’ŒæŒ‡ä»¤è°ƒæ•´ï¼Œåˆ©ç”¨æ£€ç´¢åˆ°çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡å¯¼ï¼Œä»è€Œæé«˜å¯¹æ–°é¢†åŸŸçš„é€‚åº”èƒ½åŠ›å’Œå¯¹å…ˆå‰è·å–çŸ¥è¯†çš„ä¿ç•™èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒKILOåœ¨å¤šä¸ªç›®æ ‡é¢†åŸŸä¸Šçš„è¡¨ç°å‡ä¼˜äºå¼ºåŸºçº¿ï¼ŒåŒ…æ‹¬è¿ç»­å¾®è°ƒã€ERNIE 2.0å’ŒCPTï¼Œåœ¨åå‘ä¼ é€’ã€æ­£å‘ä¼ é€’ã€F1åˆ†æ•°ã€ä¿æŒç‡å’Œè®­ç»ƒæ•ˆç‡ç­‰æ–¹é¢å‡æœ‰æ‰€æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KILOæ˜¯ä¸€ç§æ–°å‹çš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³LLMåœ¨é¢ä¸´é¢†åŸŸè¿ç§»æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>KILOç»“åˆäº†åŠ¨æ€çŸ¥è¯†å›¾è°±å’ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æé«˜æ¨¡å‹å¯¹æ–°é¢†åŸŸçš„é€‚åº”èƒ½åŠ›å’Œä¿ç•™å…ˆå‰çŸ¥è¯†çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒKILOåœ¨å¤šä¸ªç›®æ ‡é¢†åŸŸä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>KILOçš„ä¼˜åŠ¿ä½“ç°åœ¨åå‘ä¼ é€’ã€æ­£å‘ä¼ é€’ã€F1åˆ†æ•°ã€ä¿æŒç‡å’Œè®­ç»ƒæ•ˆç‡ç­‰æ–¹é¢ã€‚</li>
<li>KILOåˆ©ç”¨æ£€ç´¢åˆ°çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡å¯¼ã€‚</li>
<li>KILOæ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°å…‹æœé¢†åŸŸè¿ç§»æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a57c38161ca33a864eedfe644d397dc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-105ebeff4333fd56aa7d6b39d030e82c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8000db1f2a3481832b6159971fad64fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bef6c41cdd4d55d2fd225f9dc6d6411.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MoKA-Mixture-of-Kronecker-Adapters"><a href="#MoKA-Mixture-of-Kronecker-Adapters" class="headerlink" title="MoKA: Mixture of Kronecker Adapters"></a>MoKA: Mixture of Kronecker Adapters</h2><p><strong>Authors:Mohammadreza Sadeghi, Mahsa Ghazvini Nejad, MirHamed Jafarzadeh Asl, Yu Gu, Yuanhao Yu, Masoud Asgharian, Vahid Partovi Nia</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) is essential for reducing the computational overhead of large language models (LLMs). Low-rank family adapters are commonly used to control the parameter size efficiently while maintaining the generative power of LLMs. However, their limited expressiveness due to the rank constraint often restricts their performance on complex tasks. We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker adapters that addresses this limitation by modeling weight updates as a mixture of Kronecker products. Our proposed adapter leverages a gating mechanism that measures the importance of each Kronecker factor, enabling more expressive adaptation. Moreover, MoKA enables a rank flexibility that provides a better trade-off between parameter efficiency and accuracy. To ensure hardware efficiency, we reformulate Kronecker computations using standard matrix operations, allowing seamless deployment on GPU-optimized hardware. We conduct extensive experiments on instruction-tuning and commonsense reasoning tasks using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not only outperforms PEFT baselines, but also reduces the number of trainable parameters up to 27x, achieving state-of-the-art trade-offs between performance and parameter efficiency. </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å¯¹äºå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—å¼€é”€è‡³å…³é‡è¦ã€‚ä½ç§©å®¶æ—é€‚é…å™¨é€šå¸¸ç”¨äºæœ‰æ•ˆåœ°æ§åˆ¶å‚æ•°å¤§å°ï¼ŒåŒæ—¶ä¿æŒLLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç§©çº¦æŸå¯¼è‡´çš„æœ‰é™è¡¨è¾¾èƒ½åŠ›å¾€å¾€é™åˆ¶äº†å®ƒä»¬åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†æ··åˆå…‹ç½—å†…å…‹é€‚é…å™¨ï¼ˆMoKAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°ä¸€ä»£çš„å…‹ç½—å†…å…‹é€‚é…å™¨ï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿæƒé‡æ›´æ–°ä½œä¸ºå…‹ç½—å†…å…‹äº§å“çš„æ··åˆæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬æå‡ºçš„é€‚é…å™¨åˆ©ç”¨é—¨æ§æœºåˆ¶æ¥è¡¡é‡æ¯ä¸ªå…‹ç½—å†…å…‹å› å­çš„é‡è¦æ€§ï¼Œä»è€Œå®ç°æ›´å…·è¡¨ç°åŠ›çš„é€‚åº”ã€‚æ­¤å¤–ï¼ŒMoKAæä¾›äº†ç§©çµæ´»æ€§ï¼Œåœ¨å‚æ•°æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´æä¾›äº†æ›´å¥½çš„æƒè¡¡ã€‚ä¸ºäº†ä¿è¯ç¡¬ä»¶æ•ˆç‡ï¼Œæˆ‘ä»¬åˆ©ç”¨æ ‡å‡†çŸ©é˜µæ“ä½œé‡æ–°åˆ¶å®šäº†å…‹ç½—å†…å…‹è®¡ç®—ï¼Œå¯ä»¥æ— ç¼åœ°éƒ¨ç½²åœ¨GPUä¼˜åŒ–çš„ç¡¬ä»¶ä¸Šã€‚æˆ‘ä»¬åœ¨æŒ‡ä»¤è°ƒæ•´å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œé‡‡ç”¨äº†LLaMAçš„ä½æ¯”ç‰¹é‡åŒ–ç‰ˆæœ¬ï¼ˆåŒ…æ‹¬LLaMAçš„ç²¾ç®€ç‰ˆæœ¬å’Œè½»é‡çº§ç‰ˆæœ¬ï¼‰ã€‚MoKAä¸ä»…ä¼˜äºPEFTåŸºçº¿ï¼Œè¿˜å°†è®­ç»ƒå‚æ•°æ•°é‡å‡å°‘äº†é«˜è¾¾27å€ï¼Œåœ¨æ€§èƒ½å’Œå‚æ•°æ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03527v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å¯¹äºå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—å¼€é”€è‡³å…³é‡è¦ã€‚ä½ç§©å®¶æ—é€‚é…å™¨åœ¨æ§åˆ¶å‚æ•°å¤§å°çš„åŒæ—¶ä¿æŒäº†LLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç§©çš„é™åˆ¶ï¼Œå…¶è¡¨è¾¾èƒ½åŠ›çš„æœ‰é™æ€§å¸¸å¸¸é™åˆ¶äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºæ··åˆå…‹ç½—å†…å…‹é€‚é…å™¨ï¼ˆMoKAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å…‹ç½—å†…å…‹é€‚é…å™¨ï¼Œé€šè¿‡æ¨¡æ‹Ÿæƒé‡æ›´æ–°ä½œä¸ºå…‹ç½—å†…å…‹ç§¯çš„æ··åˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚MoKAåˆ©ç”¨é—¨æ§æœºåˆ¶æ¥è¡¡é‡æ¯ä¸ªå…‹ç½—å†…å…‹å› å­çš„é‡è¦æ€§ï¼Œä»è€Œå®ç°æ›´çµæ´»çš„é€‚åº”è¡¨è¾¾ã€‚æ­¤å¤–ï¼ŒMoKAèƒ½å¤Ÿåœ¨å‚æ•°æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å®ç°æ›´å¥½çš„æƒè¡¡ã€‚ä¸ºç¡®ä¿ç¡¬ä»¶æ•ˆç‡ï¼Œæœ¬æ–‡ä½¿ç”¨æ ‡å‡†çŸ©é˜µæ“ä½œé‡æ–°åˆ¶å®šäº†å…‹ç½—å†…å…‹è®¡ç®—ï¼Œå¯åœ¨GPUä¼˜åŒ–ç¡¬ä»¶ä¸Šè¿›è¡Œæ— ç¼éƒ¨ç½²ã€‚åœ¨æŒ‡ä»¤è°ƒæ•´å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨ä½ç²¾åº¦é‡åŒ–çš„LLaMA2-7Bå’ŒLLaMA3-8Bæ¨¡å‹è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMoKAä¸ä»…ä¼˜äºPEFTåŸºçº¿ï¼Œè€Œä¸”å¯å°†è®­ç»ƒå‚æ•°å‡å°‘è‡³åŸæ¥çš„27å€ï¼Œåœ¨æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ä¹‹é—´å–å¾—äº†æœ€å…ˆè¿›çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å¯¹å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¡ç®—å¼€é”€å¾ˆé‡è¦ã€‚</li>
<li>ä½ç§©å®¶æ—é€‚é…å™¨åœ¨æ§åˆ¶å‚æ•°å¤§å°å’Œä¿æŒLLMç”Ÿæˆèƒ½åŠ›ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œä½†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°å—é™ã€‚</li>
<li>MoKAä½œä¸ºä¸€ç§æ–°å‹çš„å…‹ç½—å†…å…‹é€‚é…å™¨ï¼Œé€šè¿‡æ¨¡æ‹Ÿæƒé‡æ›´æ–°ä½œä¸ºå…‹ç½—å†…å…‹ç§¯çš„æ··åˆï¼Œè§£å†³äº†ä½ç§©å®¶æ—é€‚é…å™¨çš„é™åˆ¶ã€‚</li>
<li>MoKAåˆ©ç”¨é—¨æ§æœºåˆ¶ï¼Œæé«˜é€‚é…å™¨çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶åœ¨å‚æ•°æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å®ç°æ›´å¥½çš„æƒè¡¡ã€‚</li>
<li>MoKAé€šè¿‡æ ‡å‡†çŸ©é˜µæ“ä½œè¿›è¡Œå…‹ç½—å†…å…‹è®¡ç®—ï¼Œæé«˜äº†ç¡¬ä»¶æ•ˆç‡ï¼Œå¯åœ¨GPUä¸Šæ— ç¼éƒ¨ç½²ã€‚</li>
<li>åœ¨æŒ‡ä»¤è°ƒæ•´å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šï¼ŒMoKAä¼˜äºPEFTåŸºçº¿ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†è®­ç»ƒå‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4443f751a71a452718be124dbfef1d9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-630e237ca6f6c10aa0615ef71d011781.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d55f986e80ffc090350d0401166bf4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd277e69c722ab5f5e72e17a26b1caa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22ac4a556347ba46b204e5e5c6249821.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="On-the-Evaluation-of-Large-Language-Models-in-Multilingual-Vulnerability-Repair"><a href="#On-the-Evaluation-of-Large-Language-Models-in-Multilingual-Vulnerability-Repair" class="headerlink" title="On the Evaluation of Large Language Models in Multilingual Vulnerability   Repair"></a>On the Evaluation of Large Language Models in Multilingual Vulnerability   Repair</h2><p><strong>Authors:Dong wang, Junji Yu, Honglin Shu, Michael Fu, Chakkrit Tantithamthavorn, Yasutaka Kamei, Junjie Chen</strong></p>
<p>Various Deep Learning-based approaches with pre-trained language models have been proposed for automatically repairing software vulnerabilities. However, these approaches are limited to a specific programming language (C&#x2F;C++). Recent advances in large language models (LLMs) offer language-agnostic capabilities and strong semantic understanding, exhibiting potential to overcome multilingual vulnerability limitations. Although some work has begun to explore LLMsâ€™ repair performance, their effectiveness is unsatisfactory. To address these limitations, we conducted a large-scale empirical study to investigate the performance of automated vulnerability repair approaches and state-of-the-art LLMs across seven programming languages. Results show GPT-4o, instruction-tuned with few-shot prompting, performs competitively against the leading approach, VulMaster. Additionally, the LLM-based approach shows superior performance in repairing unique vulnerabilities and is more likely to repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o demonstrates strong generalization on vulnerabilities in previously unseen language, outperforming existing approaches. Analysis shows Go consistently achieves the highest effectiveness across all model types, while C&#x2F;C++ performs the worst. Based on findings, we discuss the promise of LLM on multilingual vulnerability repair and the reasons behind LLMâ€™s failed cases. This work takes the first look at repair approaches and LLMs across multiple languages, highlighting the promising future of adopting LLMs for multilingual vulnerability repair. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¤šç§æ–¹æ³•å·²è¢«æå‡ºï¼Œç”¨äºè‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…é™äºç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æä¾›äº†ä¸è¯­è¨€æ— å…³çš„èƒ½åŠ›å’Œå¼ºå¤§çš„è¯­ä¹‰ç†è§£ï¼Œæ˜¾ç¤ºå‡ºå…‹æœå¤šè¯­è¨€æ¼æ´é™åˆ¶çš„æ½œåŠ›ã€‚å°½ç®¡å·²ç»å¼€å§‹æ¢ç´¢LLMçš„ä¿®å¤æ€§èƒ½ï¼Œä½†å…¶æœ‰æ•ˆæ€§å°šä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œè°ƒæŸ¥äº†è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹æ³•å’Œæœ€æ–°LLMåœ¨ä¸ƒç§ç¼–ç¨‹è¯­è¨€ä¸­çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œç»è¿‡æŒ‡ä»¤å¾®è°ƒã€é€šè¿‡å‡ æ¬¡æç¤ºè¿›è¡Œçš„GPT-4oï¼Œä¸é¢†å…ˆçš„VulMasteræ–¹æ³•ç›¸æ¯”è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨ä¿®å¤ç‹¬ç‰¹æ¼æ´æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”æ›´æœ‰å¯èƒ½ä¿®å¤æœ€å±é™©çš„æ¼æ´ã€‚ç»è¿‡æŒ‡ä»¤è°ƒæ ¡çš„GPT-4oåœ¨ä»¥å‰æœªè§è¯­è¨€çš„æ¼æ´ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼ŒGoåœ¨æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­å§‹ç»ˆè·å¾—æœ€é«˜çš„æœ‰æ•ˆæ€§ï¼Œè€ŒC&#x2F;C++çš„è¡¨ç°æœ€å·®ã€‚åŸºäºç ”ç©¶ç»“æœï¼Œæˆ‘ä»¬è®¨è®ºäº†LLMåœ¨å¤šè¯­è¨€æ¼æ´ä¿®å¤æ–¹é¢çš„å‰æ™¯ä»¥åŠLLMå¤±è´¥æ¡ˆä¾‹èƒŒåçš„åŸå› ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡è·¨å¤šç§è¯­è¨€ç ”ç©¶ä¿®å¤æ–¹æ³•å’ŒLLMï¼Œçªæ˜¾äº†é‡‡ç”¨LLMè¿›è¡Œå¤šè¯­è¨€æ¼æ´ä¿®å¤çš„å¹¿é˜”å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03470v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´æ–¹æ³•å·²ç»æå‡ºï¼Œä½†ä»…é™äºç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æä¾›äº†è·¨è¯­è¨€çš„é€šç”¨èƒ½åŠ›å’Œå¼ºå¤§çš„è¯­ä¹‰ç†è§£ï¼Œå…·æœ‰å…‹æœå¤šè¯­è¨€æ¼æ´é™åˆ¶çš„æ½œåŠ›ã€‚å°½ç®¡å·²ç»å¼€å§‹æ¢ç´¢LLMåœ¨æ¼æ´ä¿®å¤æ–¹é¢çš„æ€§èƒ½ï¼Œä½†å…¶æœ‰æ•ˆæ€§å°šä¸æ»¡æ„ã€‚æœ¬ç ”ç©¶è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œæ¢è®¨äº†è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹æ³•å’Œæœ€å…ˆè¿›çš„LLMåœ¨ä¸ƒç§ç¼–ç¨‹è¯­è¨€ä¸­çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡æŒ‡ä»¤å¾®è°ƒä¸”ä½¿ç”¨å°‘é‡æç¤ºçš„GPT-4oä¸é¢†å…ˆçš„VulMasteræ–¹æ³•ç«äº‰è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨ä¿®å¤ç‹¬ç‰¹æ¼æ´æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶æ›´æœ‰å¯èƒ½ä¿®å¤æœ€å±é™©çš„æ¼æ´ã€‚ç»è¿‡æŒ‡ä»¤è°ƒæ ¡çš„GPT-4oåœ¨ä¹‹å‰æœªè§è¿‡çš„è¯­è¨€ä¸­çš„æ¼æ´ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚åˆ†ææ˜¾ç¤ºï¼ŒGoåœ¨æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­å§‹ç»ˆè·å¾—æœ€é«˜çš„æœ‰æ•ˆæ€§ï¼Œè€ŒC&#x2F;C++è¡¨ç°æœ€å·®ã€‚æœ¬ç ”ç©¶é¦–æ¬¡è·¨å¤šç§è¯­è¨€ç ”ç©¶ä¿®å¤æ–¹æ³•å’ŒLLMï¼Œçªæ˜¾äº†é‡‡ç”¨LLMè¿›è¡Œå¤šè¯­è¨€æ¼æ´ä¿®å¤çš„å¹¿é˜”å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰è·¨è¯­è¨€çš„é€šç”¨èƒ½åŠ›å’Œå¼ºå¤§çš„è¯­ä¹‰ç†è§£ï¼Œæœ‰åŠ©äºå…‹æœå¤šè¯­è¨€æ¼æ´ä¿®å¤çš„å±€é™æ€§ã€‚</li>
<li>ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼ŒLLMåœ¨è‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´æ–¹é¢å±•ç°å‡ºç«äº‰åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç‹¬ç‰¹å’Œæœ€å±é™©çš„æ¼æ´æ—¶ã€‚</li>
<li>GPT-4oç»è¿‡æŒ‡ä»¤å¾®è°ƒåï¼Œåœ¨è·¨è¯­è¨€çš„æ¼æ´ä¿®å¤ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸”èƒ½å¤Ÿæ³›åŒ–åˆ°ä¹‹å‰æœªè§è¿‡çš„è¯­è¨€ã€‚</li>
<li>åœ¨ç ”ç©¶çš„ä¸ƒç§ç¼–ç¨‹è¯­è¨€ä¸­ï¼ŒGoåœ¨æ¼æ´ä¿®å¤æ–¹é¢çš„æœ‰æ•ˆæ€§æœ€é«˜ï¼Œè€ŒC&#x2F;C++è¡¨ç°ç›¸å¯¹è¾ƒå·®ã€‚</li>
<li>LLMåœ¨ä¿®å¤æŸäº›æ¼æ´æ—¶ä»å­˜åœ¨å¤±è´¥æƒ…å†µï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œä¼˜åŒ–ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å…¨é¢è¯„ä¼°äº†LLMåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æ¼æ´ä¿®å¤èƒ½åŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0645a16447deda0f74b821db44e52506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4100e5b2b0e7f651825c8d1450308a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f21042fafdcf88c85716f9cf596b5a57.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Evaluation-and-Analysis-of-Deep-Neural-Transformers-and-Convolutional-Neural-Networks-on-Modern-Remote-Sensing-Datasets"><a href="#Evaluation-and-Analysis-of-Deep-Neural-Transformers-and-Convolutional-Neural-Networks-on-Modern-Remote-Sensing-Datasets" class="headerlink" title="Evaluation and Analysis of Deep Neural Transformers and Convolutional   Neural Networks on Modern Remote Sensing Datasets"></a>Evaluation and Analysis of Deep Neural Transformers and Convolutional   Neural Networks on Modern Remote Sensing Datasets</h2><p><strong>Authors:J. Alex Hurt, Trevor M. Bajkowski, Grant J. Scott, Curt H. Davis</strong></p>
<p>In 2012, AlexNet established deep convolutional neural networks (DCNNs) as the state-of-the-art in CV, as these networks soon led in visual tasks for many domains, including remote sensing. With the publication of Visual Transformers, we are witnessing the second modern leap in computational vision, and as such, it is imperative to understand how various transformer-based neural networks perform on satellite imagery. While transformers have shown high levels of performance in natural language processing and CV applications, they have yet to be compared on a large scale to modern remote sensing data. In this paper, we explore the use of transformer-based neural networks for object detection in high-resolution electro-optical satellite imagery, demonstrating state-of-the-art performance on a variety of publicly available benchmark data sets. We compare eleven distinct bounding-box detection and localization algorithms in this study, of which seven were published since 2020, and all eleven since 2015. The performance of five transformer-based architectures is compared with six convolutional networks on three state-of-the-art opensource high-resolution remote sensing imagery datasets ranging in size and complexity. Following the training and evaluation of thirty-three deep neural models, we then discuss and analyze model performance across various feature extraction methodologies and detection algorithms. </p>
<blockquote>
<p>åœ¨2012å¹´ï¼ŒAlexNetç¡®ç«‹äº†æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆDCNNï¼‰åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰é¢†åŸŸçš„é¢†å…ˆåœ°ä½ï¼Œå› ä¸ºè¿™äº›ç½‘ç»œå¾ˆå¿«åœ¨è®¸å¤šé¢†åŸŸï¼ˆåŒ…æ‹¬é¥æ„Ÿï¼‰çš„è§†è§‰ä»»åŠ¡ä¸­å æ®ä¸»å¯¼åœ°ä½ã€‚éšç€è§†è§‰Transformerçš„å‘å¸ƒï¼Œæˆ‘ä»¬è§è¯äº†è®¡ç®—è§†è§‰çš„ç¬¬äºŒæ¬¡ç°ä»£é£è·ƒï¼Œå› æ­¤äº†è§£å„ç§åŸºäºTransformerçš„ç¥ç»ç½‘ç»œåœ¨å«æ˜Ÿå›¾åƒä¸Šçš„è¡¨ç°è‡³å…³é‡è¦ã€‚è™½ç„¶Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å°šæœªåœ¨å¤§è§„æ¨¡ç°ä»£é¥æ„Ÿæ•°æ®ä¸Šå¾—åˆ°å¹¿æ³›æ¯”è¾ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†åŸºäºTransformerçš„ç¥ç»ç½‘ç»œåœ¨é«˜åˆ†è¾¨ç‡å…‰ç”µå«æ˜Ÿå›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹åº”ç”¨ï¼Œå¹¶åœ¨å„ç§å…¬å¼€çš„åŸºå‡†æ•°æ®é›†ä¸Šå±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†11ç§ä¸åŒçš„è¾¹ç•Œæ¡†æ£€æµ‹å’Œå®šä½ç®—æ³•ï¼Œå…¶ä¸­7ç§è‡ª2020å¹´ä»¥æ¥å‘è¡¨ï¼Œæ‰€æœ‰11ç§è‡ª2015å¹´ä»¥æ¥å‡æœ‰æ‰€å‘å±•ã€‚åœ¨äº”ç§åŸºäºTransformerçš„æ¶æ„ä¸å…­ç§å·ç§¯ç½‘ç»œä¹‹é—´è¿›è¡Œäº†æ¯”è¾ƒï¼Œæ¶‰åŠä¸‰ä¸ªå¼€æºçš„å…ˆè¿›é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒæ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†åœ¨è§„æ¨¡å’Œå¤æ‚æ€§æ–¹é¢å„ä¸ç›¸åŒã€‚åœ¨å¯¹33ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºå¹¶åˆ†æäº†å„ç§ç‰¹å¾æå–æ–¹æ³•å’Œæ£€æµ‹ç®—æ³•ä¹‹é—´çš„æ¨¡å‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02871v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºtransformerçš„ç¥ç»ç½‘ç»œåœ¨é«˜åˆ†è¾¨ç‡å…‰ç”µå«æ˜Ÿå›¾åƒä¸­çš„ç›®æ ‡æ£€æµ‹åº”ç”¨ï¼Œå¯¹æ¯”äº†å¤šä¸ªå…¬å¼€çš„åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶å¯¹æ¯”äº†å¤šç§æ£€æµ‹ç®—æ³•ï¼ŒåŒ…æ‹¬ä¸ƒä¸ªè‡ª2020å¹´ä»¥æ¥å‘å¸ƒçš„ç®—æ³•å’Œæ‰€æœ‰è‡ª2015å¹´ä»¥æ¥å‘å¸ƒçš„åä¸€ä¸ªç®—æ³•ã€‚è®ºæ–‡å¯¹æ¯”äº†äº”ç§åŸºäºtransformerçš„æ¶æ„å’Œå…­ç§å·ç§¯ç½‘ç»œåœ¨ä¸‰ç§ä¸åŒè§„æ¨¡å’Œå¤æ‚åº¦çš„å¼€æºé«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶é€šè¿‡å¯¹ä¸‰åä¸‰ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œåˆ†æå’Œè®¨è®ºäº†æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºtransformerçš„ç¥ç»ç½‘ç»œåœ¨é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒçš„ç›®æ ‡æ£€æµ‹ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹æ¯”äº†å¤šä¸ªå…¬å¼€çš„åŸºå‡†æ•°æ®é›†ä¸Šçš„ç›®æ ‡æ£€æµ‹ç®—æ³•ã€‚</li>
<li>å¯¹æ¯”äº†å¤šç§æ£€æµ‹ç®—æ³•ï¼ŒåŒ…æ‹¬è¿‘å¹´æ¥æ–°å‘å¸ƒçš„ç®—æ³•ã€‚</li>
<li>å¯¹æ¯”äº†äº”ç§åŸºäºtransformerçš„æ¶æ„å’Œå…­ç§å·ç§¯ç½‘ç»œåœ¨é¥æ„Ÿå›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡å¯¹ä¸‰åä¸‰ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œå…¨é¢åˆ†æäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åŸºäºtransformerçš„ç¥ç»ç½‘ç»œåœ¨é¥æ„Ÿå›¾åƒç›®æ ‡æ£€æµ‹é¢†åŸŸå…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-822ea6f09fddaa38d65075ebd24d4758.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76b74f55566b73ac13417e63a52b4bd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-259aac26c9572a30e76899cffdca6bb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-976343bac01bb794579b0bf756aebb8e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-66775d0a15f700bc915885216720c027.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  From MAS to MARS Coordination Failures and Reasoning Trade-offs in   Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-23a46cc0e9ff42796efe41c51ffd474d.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  From MAS to MARS Coordination Failures and Reasoning Trade-offs in   Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
