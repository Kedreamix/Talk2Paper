<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-08-08  GeRe Towards Efficient Anti-Forgetting in Continual Learning of LLM via   General Samples Replay">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-71fc6b270c5b319f2b5cb5d4c1013b21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    85 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-08-更新"><a href="#2025-08-08-更新" class="headerlink" title="2025-08-08 更新"></a>2025-08-08 更新</h1><h2 id="GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay"><a href="#GeRe-Towards-Efficient-Anti-Forgetting-in-Continual-Learning-of-LLM-via-General-Samples-Replay" class="headerlink" title="GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via   General Samples Replay"></a>GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via   General Samples Replay</h2><p><strong>Authors:Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen</strong></p>
<p>The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns–retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1&#x2F;L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Qznan/GeRe">https://github.com/Qznan/GeRe</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的持续学习能力对于推进人工智能通用智能至关重要。然而，在不同领域进行LLM的持续微调常常会遇到灾难性遗忘的问题，其特征表现为：1）通用能力的显著遗忘；2）已学习任务的性能急剧下降。为了以一种简单而稳定的方式同时解决这两个问题，我们提出了通用样本回放（GeRe）框架，该框架使用常规的预训练文本进行高效的抗遗忘。除了回顾GeRe下最常见的回放实践之外，我们还进一步利用神经状态，引入了一种基于阈值边界（TM）损失的激活状态约束优化方法，该方法在回放学习过程中保持激活状态一致性。我们是首次验证，一小批预先收集的通用回放样本足以解决这两方面的问题——在保留通用能力的同时，促进各项任务的总体性能。实际上，前者可以内在促进后者。通过控制实验，我们在GeRe框架下将TM与不同的回放策略进行了系统比较，包括普通标签拟合、通过KL散度实现逻辑模仿以及通过L1&#x2F;L2损失实现特征模仿。结果表明，TM在性能上表现更稳定且更持久。我们的工作为LLM的有效回放铺平了道路。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/Qznan/GeRe%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Qznan/GeRe找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04676v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的持续学习能力对推进人工智能通用智能至关重要。然而，跨域持续微调LLM时常遭遇灾难性遗忘问题，表现为一般能力的显著遗忘以及先前学习任务的性能急剧下降。为解决这两个问题，我们提出简单稳定的通用样本回放（GeRe）框架，利用常规预训练文本进行有效抗遗忘。除回顾GeRe下的最常见回放实践外，我们还借助神经状态，引入使用阈值边界（TM）损失的激活状态约束优化方法。我们是首批验证者，证明一小部分预先收集的通用回放样本足以解决这两个关注点——保留一般能力的同时，促进跨序贯任务的总体性能。实际上，前者可以固有地促进后者。通过受控实验，我们在GeRe框架下系统地比较了TM与不同的回放策略，包括普通标签拟合、通过KL散度实现逻辑模仿以及通过L1&#x2F;L2损失实现特征模仿。结果表明，TM在性能和稳健性方面表现更优秀。我们的工作为未来LLM的有效回放铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的持续学习能力对推进人工智能至关重要。</li>
<li>LLM在跨域持续微调中面临灾难性遗忘问题，表现为一般能力的遗忘和先前任务性能的下降。</li>
<li>通用样本回放（GeRe）框架旨在解决这一问题，利用常规预训练文本进行抗遗忘。</li>
<li>GeRe框架结合了回放策略和神经状态，引入使用阈值边界（TM）损失的激活状态约束优化方法。</li>
<li>预先收集的通用回放样本可有效解决遗忘问题，同时促进跨序贯任务的性能。</li>
<li>通过实验验证，TM损失在回放策略中表现优异，具有更好的性能和稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04676">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9ec4a9a057b6518d5d14a0e7a1c0f78d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca73db749c6591ab3e02866dff9ae474.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e8431597659c7f46175cec5d0184751.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-390dfe10d7aef5bbe6632a19dff9ce64.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="X-SAM-From-Segment-Anything-to-Any-Segmentation"><a href="#X-SAM-From-Segment-Anything-to-Any-Segmentation" class="headerlink" title="X-SAM: From Segment Anything to Any Segmentation"></a>X-SAM: From Segment Anything to Any Segmentation</h2><p><strong>Authors:Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, Xiaodan Liang</strong></p>
<p>Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \textit{segment anything} to \textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wanghao9610/X-SAM">https://github.com/wanghao9610/X-SAM</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在广泛的知识表示方面表现出强大的能力，然而它们在像素级别的感知理解方面存在固有缺陷。尽管Segment Anything Model（SAM）在视觉提示驱动的图像分割方面代表了显著的进步，但在多掩膜预测和特定类别分割任务方面存在明显的局限性，并且它无法在统一的模型架构中集成所有的分割任务。为了解决这些局限性，我们提出了X-SAM，这是一个简化的多模态大型语言模型（MLLM）框架，它将分割范式从“分割任何事物”扩展到“任何分割”。具体来说，我们引入了一种新型统一框架，使MLLM具备更先进的像素级别感知理解能力。此外，我们提出了一种新的分割任务，称为Visual GrounDed（VGD）分割，它通过交互式的视觉提示对所有实例对象进行分割，并为MLLM提供视觉基础、像素级的解释能力。为了在不同的数据源上进行有效的训练，我们提出了一种支持跨多个数据集进行联合训练的统一训练策略。实验结果表明，X-SAM在广泛的图像分割基准测试中达到了最新性能水平，突显了其在多模态、像素级别的视觉理解方面的效率。代码可通过<a target="_blank" rel="noopener" href="https://github.com/wanghao9610/X-SAM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wanghao9610/X-SAM获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04655v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong><br>大型语言模型（LLMs）具有广泛的知识表示能力，但在像素级别的感知理解上存在固有缺陷。Segment Anything Model（SAM）在视觉提示驱动的图像分割上取得了重大进展，但在多掩膜预测和特定类别分割任务上表现出明显局限，且无法将所有分割任务整合到一个统一的模型架构中。为解决这些问题，我们提出了X-SAM，一个简化的多模态大型语言模型（MLLM）框架，将分割范式从“分割任何东西”扩展到“任何分割”。我们引入了一种新型统一框架，为MLLM提供更先进的像素级别感知理解。此外，我们提出了一种新的分割任务，称为Visual GrounDed（VGD）分割，通过交互视觉提示对实例对象进行分割，并为MLLM提供视觉接地、像素级的解释能力。我们提出了一种跨多个数据集的联合训练策略，以实现不同数据源的有效训练。实验结果表明，X-SAM在广泛的图像分割基准测试中达到了最先进的性能，突显了其在多模态、像素级别的视觉理解上的效率。代码可在GitHub上找到。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs虽在知识表示方面表现出强大的能力，但在像素级别的感知理解上存在局限。</li>
<li>Segment Anything Model (SAM)在视觉提示驱动的图像分割上有显著进展，但存在多掩膜预测和特定类别分割的局限。</li>
<li>X-SAM是一个简化的多模态大型语言模型（MLLM）框架，扩展了分割范式，引入了更先进的像素级别感知理解。</li>
<li>X-SAM通过交互视觉提示进行实例对象分割，并赋予MLLM视觉接地、像素级的解释能力。</li>
<li>X-SAM采用统一框架整合多种分割任务，并提出了一种新的分割任务——Visual GrounDed (VGD) 分割。</li>
<li>X-SAM通过跨多个数据集的联合训练策略实现有效训练。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-aa03d7c941ccccbae5e6a17bb0b5fa64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e2bf61ea193368bfa3459854119e662.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82c70a87a82b701346d90ed1089e4ca9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b85043867b29ab1939f1d20e4cb4cca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-723b17fc21da469ca52a23900c55c91d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RoboTron-Sim-Improving-Real-World-Driving-via-Simulated-Hard-Case"><a href="#RoboTron-Sim-Improving-Real-World-Driving-via-Simulated-Hard-Case" class="headerlink" title="RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case"></a>RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case</h2><p><strong>Authors:Baihui Xiao, Chengjian Feng, Zhijian Huang, Feng yan, Yujie Zhong, Lin Ma</strong></p>
<p>Collecting real-world data for rare high-risk scenarios, long-tailed driving events, and complex interactions remains challenging, leading to poor performance of existing autonomous driving systems in these critical situations. In this paper, we propose RoboTron-Sim that improves real-world driving in critical situations by utilizing simulated hard cases. First, we develop a simulated dataset called Hard-case Augmented Synthetic Scenarios (HASS), which covers 13 high-risk edge-case categories, as well as balanced environmental conditions such as day&#x2F;night and sunny&#x2F;rainy. Second, we introduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder (I2E Encoder) to enable multimodal large language models to effectively learn real-world challenging driving skills from HASS, via adapting to environmental deviations and hardware differences between real-world and simulated scenarios. Extensive experiments on nuScenes show that RoboTron-Sim improves driving performance in challenging scenarios by around 50%, achieving state-of-the-art results in real-world open-loop planning. Qualitative results further demonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk driving scenarios. Project page: <a target="_blank" rel="noopener" href="https://stars79689.github.io/RoboTron-Sim/">https://stars79689.github.io/RoboTron-Sim/</a> </p>
<blockquote>
<p>收集真实世界数据对于罕见高风险场景、长尾驾驶事件和复杂交互仍然具有挑战性，导致现有自动驾驶系统在这些关键情况下表现不佳。在本文中，我们提出RoboTron-Sim，它通过利用模拟的困难案例来改善关键情况下的现实世界驾驶。首先，我们开发了一个名为Hard-case Augmented Synthetic Scenarios（HASS）的模拟数据集，涵盖了13类高风险边缘案例，以及如日夜、晴雨等平衡的环境条件。其次，我们引入了情景感知提示工程（SPE）和图像到自我编码器（I2E Encoder），以使多模态大型语言模型能够通过适应环境偏差和现实世界与模拟场景之间的硬件差异，从HASS有效学习现实世界中具有挑战性的驾驶技能。在nuScenes上的广泛实验表明，RoboTron-Sim将挑战场景中的驾驶性能提高了约50%，在现实世界开放循环规划中实现了最新结果。定性结果进一步证明了RoboTron-Sim在更好地管理罕见高风险驾驶场景方面的有效性。项目页面：<a target="_blank" rel="noopener" href="https://stars79689.github.io/RoboTron-Sim/">https://stars79689.github.io/RoboTron-Sim/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04642v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>模拟数据集RoboTron-Sim的提出，解决了真实世界高风险场景数据收集困难的问题，通过创建高风险的模拟场景（如边缘情况）并引入场景感知提示工程和图像到自我编码器技术，使得自动驾驶系统在面临真实世界的复杂挑战时表现更优。实验证明，该技术能提高自动驾驶系统在复杂场景中的性能约50%，达到现实开放环境中规划的最优水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模拟数据集RoboTron-Sim能够覆盖多种高风险边缘场景，并模拟不同环境条件如日夜交替和天气变化。</li>
<li>利用场景感知提示工程（SPE）和图像到自我编码器（I2E Encoder）技术，使大型语言模型能够学习处理模拟的高风险驾驶场景。</li>
<li>该技术通过适应环境变化和硬件差异，使得自动驾驶系统在处理真实世界的复杂驾驶技能时性能更优。</li>
<li>在nuScenes上的实验表明，RoboTron-Sim技术提高了自动驾驶系统在复杂场景中的性能约50%。</li>
<li>该技术达到了现实开放环境中规划的最优水平，实现了定性结果的有效展示。</li>
<li>项目页面提供了详细的背景信息和研究成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04642">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-16785dba3ee35fdc553b8c57abe2e77c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b5538b275d2c6109cb848eb5b65060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56bfbd5a71ee5bc111a607831b8c6ceb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6da1d48f89ce58296d644a6102442fa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eabf5055c745e2abb6124cd0fbd77ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0bf57fc4dba3b1dd065efb3a7fab418.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="P-Aligner-Enabling-Pre-Alignment-of-Language-Models-via-Principled-Instruction-Synthesis"><a href="#P-Aligner-Enabling-Pre-Alignment-of-Language-Models-via-Principled-Instruction-Synthesis" class="headerlink" title="P-Aligner: Enabling Pre-Alignment of Language Models via Principled   Instruction Synthesis"></a>P-Aligner: Enabling Pre-Alignment of Language Models via Principled   Instruction Synthesis</h2><p><strong>Authors:Feifan Song, Bofei Gao, Yifan Song, Yi Liu, Weimin Xiong, Yuyang Song, Tianyu Liu, Guoyin Wang, Houfeng Wang</strong></p>
<p>Large Language Models (LLMs) are expected to produce safe, helpful, and honest content during interaction with human users, but they frequently fail to align with such values when given flawed instructions, e.g., missing context, ambiguous directives, or inappropriate tone, leaving substantial room for improvement along multiple dimensions. A cost-effective yet high-impact way is to pre-align instructions before the model begins decoding. Existing approaches either rely on prohibitive test-time search costs or end-to-end model rewrite, which is powered by a customized training corpus with unclear objectives. In this work, we demonstrate that the goal of efficient and effective preference alignment can be achieved by P-Aligner, a lightweight module generating instructions that preserve the original intents while being expressed in a more human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree Search, which systematically explores the space of candidate instructions that are closely tied to human preference. Experiments across different methods show that P-Aligner generally outperforms strong baselines across various models and benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness and efficiency through multiple perspectives, including data quality, search strategies, iterative deployment, and time overhead. </p>
<blockquote>
<p>大型语言模型（LLM）在与人类用户交互时，预期会产生安全、有用、诚实的内容。然而，当给予有缺陷的指令时，例如缺少上下文、指令模糊或语气不当，它们往往无法与这些价值对齐，这在多个维度上仍有很大的改进空间。一种成本效益高且影响大的方法是，在模型开始解码之前预先对齐指令。现有方法要么依赖于禁止的测试时间搜索成本，要么依赖于端到端的模型重写，这由带有不明确目标的定制训练语料库提供支持。在这项工作中，我们展示了P-Aligner可以实现高效和有效的偏好对齐的目标。P-Aligner是一个轻量级模块，能够生成在保留原始意图的同时以人类更偏好的形式表达的指令。P-Aligner是在UltraPrompt上训练的，这是一个通过提出的原则引导管道合成的新数据集，该管道使用蒙特卡洛树搜索系统地探索与人类偏好紧密相关的候选指令空间。在不同方法和模型上的实验表明，P-Aligner通常优于强大的基线模型，并在各种基准测试上表现出色，包括在GPT-4-turbo和Gemma-2-SimPO上的平均胜率分别提高了28.35%和8.69%。进一步的分析从数据质量、搜索策略、迭代部署和时间开销等多个角度验证了其有效性和效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04626v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMs在处理与用户交互时的安全性和有用性方面存在问题，这主要是因为它们在处理具有缺陷的指令（如缺乏上下文、指令模糊或不恰当的语气）时无法对齐相应的价值观。为解决这一问题，本文提出了一种高效且有效的偏好对齐方法——P-Aligner，它是一种轻量级模块，能够生成更多符合人类表达习惯的指令。该模块在UltraPrompt数据集上进行训练，这是一个通过提出的原理引导管道合成的数据集，通过蒙特卡洛树搜索系统地探索与人类偏好紧密相关的候选指令空间。实验表明，P-Aligner在各种模型和基准测试上的表现均优于强大的基线模型，包括在GPT-4-turbo和Gemma-2-SimPO上的平均胜率分别提高了28.35%和8.69%。进一步的分析从数据质量、搜索策略、迭代部署和时间开销等多个角度验证了其有效性和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在处理具有缺陷的指令时存在对齐问题。</li>
<li>P-Aligner是一种轻量级模块，旨在生成符合人类表达习惯的指令以提高安全性、有用性和诚实性。</li>
<li>P-Aligner在UltraPrompt数据集上进行训练，这是一个通过蒙特卡洛树搜索合成的数据集。</li>
<li>P-Aligner在各种模型和基准测试上的表现优于基线模型。</li>
<li>P-Aligner在GPT-4-turbo和Gemma-2-SimPO上的平均胜率分别提高了28.35%和8.69%。</li>
<li>P-Aligner的有效性和效率通过数据质量、搜索策略、迭代部署和时间开销等多个角度得到了验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-071d0c8bf0276d02acfc5c64a4d4af4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f70059cf575e318044097e9670123f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34342131ac3dbe40d727bef43d84410a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4307f5d7cac61b6ce2f64738a5ccedb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eabae18dc8fe52b32797d51799083707.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d765631d61e497acff5a46394b31937e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FinMMR-Make-Financial-Numerical-Reasoning-More-Multimodal-Comprehensive-and-Challenging"><a href="#FinMMR-Make-Financial-Numerical-Reasoning-More-Multimodal-Comprehensive-and-Challenging" class="headerlink" title="FinMMR: Make Financial Numerical Reasoning More Multimodal,   Comprehensive, and Challenging"></a>FinMMR: Make Financial Numerical Reasoning More Multimodal,   Comprehensive, and Challenging</h2><p><strong>Authors:Zichen Tang, Haihong E, Jiacheng Liu, Zhongjun Yang, Rongjin Li, Zihua Rong, Haoyang He, Zhuodi Hao, Xinyang Hu, Kun Ji, Ziyan Ma, Mengyuan Ji, Jun Zhang, Chenghao Ma, Qianhe Zheng, Yang Liu, Yiling Huang, Xinyi Hu, Qing Huang, Zijian Xie, Shiyao Peng</strong></p>
<p>We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios. </p>
<blockquote>
<p>我们推出了FinMMR，这是一个针对金融数值推理任务的多模态大语言模型的推理能力评估而量身定制的新型双语多模态基准测试。与现有基准测试相比，我们的工作引入了三个重要进展。(1) 多模态：我们精心改造了现有的金融推理基准测试，并根据最新的中文金融研究报告构建了新问题。FinMMR包含4.3K个问题和8.7K个图像，涵盖14个类别，包括表格、条形图和所有权结构图等。(2) 全面性：FinMMR涵盖了包括企业财务、银行业和产业分析等在内的14个金融子领域，在财务领域知识的广度上显著超过了现有基准测试。(3) 挑战性：模型需要通过整合金融知识以及对复杂的金融图像和文本的理解，来执行多步骤精确数值推理。在难题上，表现最佳的多模态大语言模型的准确率仅为53.0%。我们相信，FinMMR将推动在提高现实世界场景中多模态大语言模型的推理能力方面的进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04625v1">PDF</a> Accepted by ICCV 2025. arXiv admin note: text overlap with   arXiv:2311.06602 by other authors</p>
<p><strong>Summary</strong></p>
<p>FinMMR是一个针对金融数值推理任务的多模态基准测试，旨在评估多模态大型语言模型（MLLMs）的推理能力。相较于现有基准测试，FinMMR有三项显著进步：包括多模态特性、涵盖广泛的金融子领域以及挑战性。它要求模型结合金融知识对复杂金融图像和文本进行多步骤精确数值推理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FinMMR是一个针对金融数值推理的多模态基准测试，专为评估MLLMs的推理能力而设计。</li>
<li>该基准测试包含三项显著特点：多模态特性、涵盖广泛的金融子领域以及挑战性任务。</li>
<li>FinMMR包含4.3K个问题和8.7K个图像，涉及表格、条形图等14种类型。</li>
<li>它涵盖了从企业财务、银行业到行业分析等14个金融子领域。</li>
<li>模型需要对复杂的金融图像和文本进行多步骤精确数值推理，集成金融知识。</li>
<li>最佳性能的MLLM在困难问题上的准确率仅为53.0%，表明该领域仍存在挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04625">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-170b1f8fe579deb2ce81991482f373ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bed2b9d3bafd237a7c5fde25d76c5f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6b4fa7c14550cff5e4515fde9263a73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-404b97ad576b91a611ebfe20c8d0806d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71fc6b270c5b319f2b5cb5d4c1013b21.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Lightweight-Transformers-for-Zero-Shot-and-Fine-Tuned-Text-to-SQL-Generation-Using-Spider"><a href="#Lightweight-Transformers-for-Zero-Shot-and-Fine-Tuned-Text-to-SQL-Generation-Using-Spider" class="headerlink" title="Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL   Generation Using Spider"></a>Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL   Generation Using Spider</h2><p><strong>Authors:Chirag Seth, Utkarsh Singh</strong></p>
<p>Text-to-SQL translation enables non-expert users to query relational databases using natural language, with applications in education and business intelligence. This study evaluates three lightweight transformer models - T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on low-resource settings. We developed a reusable, model-agnostic pipeline that tailors schema formatting to each model’s architecture, training them across 1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2 (20.1%), highlighting encoder-decoder models’ superiority in schema-aware SQL generation. Despite resource constraints limiting performance, our pipeline’s modularity supports future enhancements, such as advanced schema linking or alternative base models. This work underscores the potential of compact transformers for accessible text-to-SQL solutions in resource-scarce environments. </p>
<blockquote>
<p>文本到SQL的翻译技术使得非专业用户能够通过自然语言查询关系数据库，在教育、商业智能等领域有广泛应用。本研究在Spider数据集上评估了三种轻量级转换器模型——T5-Small、BART-Small和GPT-2，重点是在资源有限的环境下进行研究。我们开发了一个可重用、模型无关的管道，根据每个模型的架构定制模式格式化，在1000到5000次迭代之间对它们进行训练，使用逻辑形式准确性（LFAcc）、BLEU和精确匹配（EM）等指标在1000个测试样本上进行评估。经过微调，T5-Small的LFAcc最高（27.8%），优于BART-Small（23.98%）和GPT-2（20.1%），突显出在模式感知SQL生成中编码器-解码器模型的优越性。尽管资源约束限制了性能，但我们的管道模块支持未来增强功能，如高级模式链接或替代基础模型。这项工作突出了紧凑转换器在资源稀缺环境中用于可访问文本到SQL解决方案的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04623v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了基于Transformer的自然语言查询数据库方法在低资源环境下的性能表现。实验在Spider数据集上进行，对比了T5-Small、BART-Small和GPT-2三个轻量级模型。结果显示，经过精细调整的T5-Small模型在逻辑形式准确率（LFAcc）上表现最佳，达到27.8%，优于BART-Small和GPT-2。这表明在资源受限的环境中，编码解码器模型在模式感知SQL生成方面具有优势。本文的工作突显了紧凑的Transformer模型在资源稀缺环境中实现可访问的文本到SQL解决方案的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自然语言查询数据库方法在低资源环境下具有重要的应用价值。</li>
<li>实验对比了三种轻量级Transformer模型（T5-Small、BART-Small和GPT-2）在文本到SQL转换方面的性能。</li>
<li>T5-Small模型经过精细调整后表现出最佳性能，特别是在逻辑形式准确率（LFAcc）方面。</li>
<li>结果突显了编码解码器模型在模式感知SQL生成方面的优势。</li>
<li>虽然资源约束限制了性能，但所使用的管道可模块化支持未来改进，如高级模式链接或替代基础模型。</li>
<li>本文提供了一个重要的视角，即在资源稀缺的环境中如何实现可访问的文本到SQL解决方案。对于自然语言处理和数据库交互技术提供了重要启示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04623">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9356cd8e35cc0d20d557f1f8741ea1b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32ed226a8e9a04b942e9b3eeb2fe277c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f563dac47c5cd918cac26ce0f0c225d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49f1dd4be617089d7eccc032db679b93.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ConfProBench-A-Confidence-Evaluation-Benchmark-for-MLLM-Based-Process-Judges"><a href="#ConfProBench-A-Confidence-Evaluation-Benchmark-for-MLLM-Based-Process-Judges" class="headerlink" title="ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process   Judges"></a>ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process   Judges</h2><p><strong>Authors:Yue Zhou, Yi Chang, Yuan Wu</strong></p>
<p>Reasoning is a critical capability of multimodal large language models (MLLMs) for solving complex multimodal tasks, and judging the correctness of reasoning steps is crucial for improving this capability. Recently, MLLM-based process judges (MPJs) have been widely used to assess the correctness of reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important for identifying their limitations and guiding future improvements. However, existing benchmarks for MPJs mainly focus on tasks such as step correctness classification and reasoning process search, while overlooking a key aspect: whether the confidence scores produced by MPJs at the step level are reliable. To address this gap, we propose ConfProBench, the first comprehensive benchmark designed to systematically evaluate the reliability of step-level confidence scores generated by MPJs. Our benchmark constructs three types of adversarially perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and Image Perturbation, to test the robustness of MPJ confidence under perturbations. In addition, we introduce three novel evaluation metrics: Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including both proprietary and open-source models. Experiments reveal limitations in current MPJs’ confidence performance and offer competitive baselines to support future research. </p>
<blockquote>
<p>推理是解决复杂多模式任务的多模式大型语言模型（MLLMs）的关键能力，而判断推理步骤的正确性对于提高这种能力至关重要。最近，基于MLLM的过程判断（MPJ）已被广泛应用于评估多模式任务中推理步骤的正确性。因此，评估MPJ对于发现其局限性并指导未来改进非常重要。然而，现有的MPJ基准测试主要关注如步骤正确性分类和推理过程搜索等任务，而忽视了一个关键方面：MPJ在步骤层面产生的置信度分数是否可靠。为了弥补这一空白，我们提出了ConfProBench，这是第一个旨在系统评估MPJ产生的步骤级置信度分数可靠性的全面基准测试。我们的基准测试构建了三种对抗性扰动推理步骤：同义词替换、句法转换和图像扰动，以测试MPJ置信度在扰动下的稳健性。此外，我们引入了三个新的评估指标：置信稳健性得分（CRS）、置信敏感性得分（CSS）和置信校准得分（CCS），分别评估稳健性、敏感性和校准。我们评估了14个最先进的多模式大型语言模型，包括专有和开源模型。实验揭示了当前MPJ在置信度表现方面的局限性，并为支持未来研究提供了竞争基准线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04576v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于多模态大型语言模型（MLLMs）进行推理是应对复杂多模态任务的关键能力，且对判断推理步骤的正确性对提升此能力至关重要。当前广泛使用的基于MLLM的过程判断（MPJs）主要关注步骤正确性分类和推理过程搜索等任务，但忽略了其关键方面——MPJs生成的步骤级别置信度的可靠性。为解决此问题，本文提出ConfProBench基准测试平台，旨在系统地评估MPJs在步骤级别上生成的置信度的可靠性。通过构建三种对抗性扰动推理步骤进行测试，并引入三个新的评估指标来评估置信度。本文评估了多个先进的MLLMs模型，揭示了当前MPJs在置信度性能方面的局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）的推理能力对于解决复杂的多模态任务至关重要。</li>
<li>过程判断（MPJs）对于评估MLLMs的推理步骤正确性十分重要。</li>
<li>当前MPJ评估主要关注步骤正确性分类和推理过程搜索，但忽略了其生成的步骤级别置信度的可靠性。</li>
<li>ConfProBench基准测试平台被提出以系统地评估MPJs在步骤级别上生成的置信度的可靠性。</li>
<li>通过构建三种对抗性扰动推理步骤进行测试，包括同义词替换、句法转换和图像扰动。</li>
<li>引入三个新的评估指标：信心稳健性得分（CRS）、信心敏感性得分（CSS）和信心校准得分（CCS），以全面评价MPJs的置信度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-72efb63ce264a833694f5b44fc29b36d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d203b43cff8b22558d51d6b0797e7ba1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cc3afd3efd164903f8aab3764f55043.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1656042ae359a3db55e0e3e8396b639a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f265f48cc935594dc88c9c69b47e1063.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="StyliTruth-Unlocking-Stylized-yet-Truthful-LLM-Generation-via-Disentangled-Steering"><a href="#StyliTruth-Unlocking-Stylized-yet-Truthful-LLM-Generation-via-Disentangled-Steering" class="headerlink" title="StyliTruth : Unlocking Stylized yet Truthful LLM Generation via   Disentangled Steering"></a>StyliTruth : Unlocking Stylized yet Truthful LLM Generation via   Disentangled Steering</h2><p><strong>Authors:Chenglei Shen, Zhongxiang Sun, Teng Shi, Xiao Zhang, Jun Xu</strong></p>
<p>Generating stylized large language model (LLM) responses via representation editing is a promising way for fine-grained output control. However, there exists an inherent trade-off: imposing a distinctive style often degrades truthfulness. Existing representation editing methods, by naively injecting style signals, overlook this collateral impact and frequently contaminate the model’s core truthfulness representations, resulting in reduced answer correctness. We term this phenomenon stylization-induced truthfulness collapse. We attribute this issue to latent coupling between style and truth directions in certain key attention heads, and propose StyliTruth, a mechanism that preserves stylization while keeping truthfulness intact. StyliTruth separates the style-relevant and truth-relevant subspaces in the model’s representation space via an orthogonal deflation process. This decomposition enables independent control of style and truth in their own subspaces, minimizing interference. By designing adaptive, token-level steering vectors within each subspace, we dynamically and precisely control the generation process to maintain both stylistic fidelity and truthfulness. We validate our method on multiple styles and languages. Extensive experiments and analyses show that StyliTruth significantly reduces stylization-induced truthfulness collapse and outperforms existing inference-time intervention methods in balancing style adherence with truthfulness. </p>
<blockquote>
<p>通过表示编辑生成风格化的大型语言模型（LLM）响应是控制精细输出的一种有前途的方式。然而，存在一种固有的权衡：施加一种独特的风格往往会降低真实性。现有的表示编辑方法，通过简单地注入风格信号，忽视了这种附带影响，并经常污染模型的核心真实性表示，导致答案的正确性降低。我们将这种现象称为风格化引起的真实性崩溃。我们将这一问题归因于某些关键注意力头中风格与真实方向之间的潜在耦合，并提出了StyliTruth机制，该机制在保持风格化的同时保持真实性。StyliTruth通过正交膨胀过程，在模型的表示空间中分离出与风格和真实相关的子空间。这种分解能够在各自的子空间中独立控制风格和真实，最小化干扰。通过在每个子空间内设计自适应的、令牌级的引导向量，我们能够动态和精确地控制生成过程，以维持风格上的忠实性和真实性。我们的方法在多种风格和语言上进行了验证。大量的实验和分析表明，StyliTruth显著减少了由风格化引起的真实性崩溃，并在平衡风格坚持与真实性方面超越了现有的推理时间干预方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04530v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在大型语言模型（LLM）的反应中，通过表现编辑生成风格化的回复展现出了精细控制输出的潜力。然而，风格化往往伴随着真实性的损失，存在内在权衡。现有的表现编辑方法过于简单注入风格信号，忽视了这种附带影响，会干扰模型的核心真实性表示，降低了回答的准确度。我们将这种现象称为风格化引起的真实性崩溃。针对此问题，我们归因于关键注意力头部的风格和真理方向的潜在耦合，并提出了StyliTruth机制，能够在保持风格化的同时保持真实性。StyliTruth通过正交压缩过程分离了与风格和真实相关的子空间。这种分解使得风格和真实在其各自的子空间内独立控制，最小化干扰。通过设计每个子空间内的自适应、令牌级引导向量，我们能够在生成过程中动态精确地控制风格保持和真实性维护。我们的方法在多风格和多种语言上的验证表明，StyliTruth显著减少了风格化引起的真实性崩溃，并且在平衡风格遵守和真实性方面优于现有的推理时间干预方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通过表现编辑生成风格化的LLM回复具有精细控制输出的潜力。</li>
<li>风格化大型语言模型回复时存在真实性的损失。</li>
<li>现有方法过于简单注入风格信号，导致模型的核心真实性表示受到干扰。</li>
<li>提出了一种名为StyliTruth的机制来解决风格化引起的真实性崩溃问题。</li>
<li>StyliTruth通过分离风格和真实性的子空间来平衡风格和真实性的控制。</li>
<li>StyliTruth通过自适应令牌级引导向量在生成过程中动态控制风格和真实性的维护。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04530">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5b0c8d21f301b1f2744107bc3fb0ad41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eb3f1517b0eb2fa5843c21a65b72e5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6d2cada4b7dc2bec9092e4547342bd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13386a024849c01f0787fdbe0b56188e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b34be37cc36b5502340c1bfb0909a5ba.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Boosting-Visual-Knowledge-Intensive-Training-for-LVLMs-Through-Causality-Driven-Visual-Object-Completion"><a href="#Boosting-Visual-Knowledge-Intensive-Training-for-LVLMs-Through-Causality-Driven-Visual-Object-Completion" class="headerlink" title="Boosting Visual Knowledge-Intensive Training for LVLMs Through   Causality-Driven Visual Object Completion"></a>Boosting Visual Knowledge-Intensive Training for LVLMs Through   Causality-Driven Visual Object Completion</h2><p><strong>Authors:Qingguo Hu, Ante Wang, Jia Song, Delai Qiu, Qingsong Liu, Jinsong Su</strong></p>
<p>Large Vision-Language Models (LVLMs) have experienced significant advancements in recent years. However, their performance still falls short in tasks requiring deep visual perception, such as identifying subtle differences between images. A potential cause is the scarcity of visual knowledge in popular instruction-tuning corpora, resulting in inadequate visual perception and reasoning capabilities. To address this challenge, we introduce a self-improvement framework grounded in a novel visual knowledge-intensive task, \underline{C}ausality-driven \underline{V}isual object \underline{C}ompletion (CVC). This task requires LVLMs to infer the masked object in an image based on its \textit{causal} relationships with the other visible information. We first obtain rich examples cheaply through our automated instance construction pipeline, without relying on sophisticated LVLMs (\textit{e.g.}, GPT-4V) or human assistance. Then, LVLMs effectively self-improve through trial and error learning using these created instances. Our experiments demonstrate substantial gains across four challenging specialized tasks and four widely-used comprehensive benchmarks. Especially on specialized tasks, our method achieves an average improvement of 5.4% and 4.0% compared to the corresponding baselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code is available at <a target="_blank" rel="noopener" href="https://github.com/XMUDeepLIT/CVC">https://github.com/XMUDeepLIT/CVC</a>. </p>
<blockquote>
<p>大型视觉语言模型（LVLMs）近年来取得了显著进展。然而，它们在需要深度视觉感知的任务上的表现仍然不足，如识别图像之间的细微差异。造成这种情况的一个潜在原因是流行指令微调语料库中视觉知识的匮乏，导致视觉感知和推理能力不足以应对这些挑战。为了应对这一挑战，我们基于新型视觉知识密集型任务，引入了一种自我改进框架——因果驱动型视觉对象补全（CVC）。该任务要求LVLMs基于与其他可见信息的因果关系推断图像中的遮挡对象。我们首可以通过自动化实例构建管道低成本地获取丰富的样本，无需依赖先进的大型视觉语言模型（如GPT-4V）或人工协助。随后，这些生成的实例通过试错学习，使LVLMs实现了有效的自我提升。我们的实验表明，在四项具有挑战性的专项任务和四项广泛使用的综合基准测试中，我们的方法均取得了显著的提升效果。特别是在专项任务上，与相应的基线相比，使用LLaVA-1.5-7B和LLaVA-1.5-13B时，我们的方法平均提升了5.4%和4.0%。代码已公开在<a target="_blank" rel="noopener" href="https://github.com/XMUDeepLIT/CVC%E4%B8%8A%E3%80%82">https://github.com/XMUDeepLIT/CVC上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04453v1">PDF</a> Accepted by IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>大型视觉语言模型（LVLMs）近年来取得显著进展，但在需要深度视觉感知的任务中表现仍有所不足，如图像间的细微差异识别。本文提出了一种基于因果驱动视觉对象补全（CVC）任务进行自我提升的新框架，以解决此挑战。该任务要求LVLMs根据图像中对象之间的因果关系推断出被掩盖的对象。本文通过自动化实例构建流程获取丰富的例子，无需依赖高级LVLMs或人工辅助。实验结果显示，该方法在四个具有挑战性的专项任务和四个广泛使用的综合基准测试中均取得了显著的提升，特别是在专项任务上，相较于对应的基线方法平均提升了5.4%和4.0%。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LVLMs在深度视觉感知任务中仍存在性能短板。</li>
<li>提出了基于因果驱动视觉对象补全（CVC）任务的新框架以改善LVLMs的表现。</li>
<li>CVC任务要求LVLMs推断图像中被掩盖的对象，基于与其他可见信息的因果关系。</li>
<li>通过自动化实例构建流程获取丰富例子，无需高级LVLMs或人工辅助。</li>
<li>方法在多个挑战性任务和基准测试中表现优异，平均提升5.4%和4.0%。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04453">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8987ec0c727a131f94f77e738d1852aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b59cbb84f76df60ef12a13150a5c4035.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abaab2bb10b59372f0212fc28085a37a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3139bfbd77e773f8ae3150a2041de246.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e18a472fad4911bdf0f019e47bd4e3a7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Think-Before-You-Segment-An-Object-aware-Reasoning-Agent-for-Referring-Audio-Visual-Segmentation"><a href="#Think-Before-You-Segment-An-Object-aware-Reasoning-Agent-for-Referring-Audio-Visual-Segmentation" class="headerlink" title="Think Before You Segment: An Object-aware Reasoning Agent for Referring   Audio-Visual Segmentation"></a>Think Before You Segment: An Object-aware Reasoning Agent for Referring   Audio-Visual Segmentation</h2><p><strong>Authors:Jinxing Zhou, Yanghao Zhou, Mingfei Han, Tong Wang, Xiaojun Chang, Hisham Cholakkal, Rao Muhammad Anwer</strong></p>
<p>Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects in audible videos based on given reference expressions. Prior works typically rely on learning latent embeddings via multimodal fusion to prompt a tunable SAM&#x2F;SAM2 decoder for segmentation, which requires strong pixel-level supervision and lacks interpretability. From a novel perspective of explicit reference understanding, we propose TGS-Agent, which decomposes the task into a Think-Ground-Segment process, mimicking the human reasoning procedure by first identifying the referred object through multimodal analysis, followed by coarse-grained grounding and precise segmentation. To this end, we first propose Ref-Thinker, a multimodal language model capable of reasoning over textual, visual, and auditory cues. We construct an instruction-tuning dataset with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The object description inferred by Ref-Thinker is used as an explicit prompt for Grounding-DINO and SAM2, which perform grounding and segmentation without relying on pixel-level supervision. Additionally, we introduce R\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and reasoning-intensive references for better evaluating model generalization. Our approach achieves state-of-the-art results on both standard Ref-AVSBench and proposed R\textsuperscript{2}-AVSBench. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/jasongief/TGS-Agent">https://github.com/jasongief/TGS-Agent</a>. </p>
<blockquote>
<p>引用音频视觉分割（Ref-AVS）旨在根据给定的参考表达式对可听视频中的目标对象进行分割。早期的工作通常依赖于通过多模态融合学习潜在嵌入，以提示可调SAM&#x2F;SAM2解码器进行分割，这需要强烈的像素级监督并且缺乏可解释性。从显式参考理解的新角度，我们提出了TGS-Agent，它将任务分解为Think-Ground-Segment过程，通过多模态分析首先识别所指的物体，然后进行粗粒度定位和精确分割，模仿人类的推理过程。为此，我们首先提出了Ref-Thinker，这是一个能够推理文本、视觉和听觉线索的多模态语言模型。我们构建了一个指令微调数据集，其中包含用于Ref-Thinker精细调整的具有明确对象感知的think-answer链。Ref-Thinker推断的对象描述被用作对Grounding-DINO和SAM__的明确提示，进行定位和分割，无需依赖像素级监督。此外，我们介绍了R²AVSBench新基准测试，其中包含语言多样化和推理密集型的参考物，以更好地评估模型的泛化能力。我们的方法在标准的Ref-AVSBench和提出的R²AVSBench上都达到了最新水平。代码将在<a target="_blank" rel="noopener" href="https://github.com/jasongief/TGS-Agent%E4%B8%8A%E6%8F%AD%E5%AE%9E%E3%80%82">https://github.com/jasongief/TGS-Agent上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04418v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/jasongief/TGS-Agent">https://github.com/jasongief/TGS-Agent</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为TGS-Agent的新方法，用于音频视频分割任务。该方法从理解明确参考的新角度入手，将任务分解为思考、定位和分割三个过程，并模仿人类推理过程。首先通过多模态分析识别目标对象，然后进行粗略定位，最后精确分割。为此，本文提出了Ref-Thinker多模态语言模型，能够对文本、视觉和听觉线索进行推理。通过构建指令微调数据集，对Ref-Thinker进行微调，使用其推断的对象描述作为对Grounding-DINO和SAM2的明确提示，进行定位和分割任务，无需像素级监督。此外，还介绍了新的R\textsuperscript{2}-AVSBench基准测试，以评估模型的泛化能力。TGS-Agent在标准的Ref-AVSBench和新的R\textsuperscript{2}-AVSBench上均达到了最佳效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TGS-Agent方法通过分解任务为思考、定位和分割过程，模仿人类推理过程进行音频视频分割。</li>
<li>提出了Ref-Thinker多模态语言模型，能结合文本、视觉和听觉线索进行推理。</li>
<li>通过构建指令微调数据集对Ref-Thinker进行训练，提高了模型的推理能力。</li>
<li>使用Ref-Thinker推断的对象描述作为对Grounding-DINO和SAM2的明确提示，进行定位和分割任务，减少对像素级监督的依赖。</li>
<li>引入了新的R\textsuperscript{2}-AVSBench基准测试，以评估模型的泛化能力。</li>
<li>TGS-Agent在Ref-AVSBench和R\textsuperscript{2}-AVSBench上取得了最佳效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04418">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5c52c96e0c2875228103089bee9237c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98991b6c2c4c6e21564bc657de0cb844.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77b4efdfd8c77575c393a2a6cc9f759f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15dc138b6bc925bf9acfb1a0227d974f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e93abc6269c861370f852bd98f60aeb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Length-Matters-Length-Aware-Transformer-for-Temporal-Sentence-Grounding"><a href="#Length-Matters-Length-Aware-Transformer-for-Temporal-Sentence-Grounding" class="headerlink" title="Length Matters: Length-Aware Transformer for Temporal Sentence Grounding"></a>Length Matters: Length-Aware Transformer for Temporal Sentence Grounding</h2><p><strong>Authors:Yifan Wang, Ziyi Liu, Xiaolong Sun, Jiawei Wang, Hongmin Liu</strong></p>
<p>Temporal sentence grounding (TSG) is a highly challenging task aiming to localize the temporal segment within an untrimmed video corresponding to a given natural language description. Benefiting from the design of learnable queries, the DETR-based models have achieved substantial advancements in the TSG task. However, the absence of explicit supervision often causes the learned queries to overlap in roles, leading to redundant predictions. Therefore, we propose to improve TSG by making each query fulfill its designated role, leveraging the length priors of the video-description pairs. In this paper, we introduce the Length-Aware Transformer (LATR) for TSG, which assigns different queries to handle predictions based on varying temporal lengths. Specifically, we divide all queries into three groups, responsible for segments with short, middle, and long temporal durations, respectively. During training, an additional length classification task is introduced. Predictions from queries with mismatched lengths are suppressed, guiding each query to specialize in its designated function. Extensive experiments demonstrate the effectiveness of our LATR, achieving state-of-the-art performance on three public benchmarks. Furthermore, the ablation studies validate the contribution of each component of our method and the critical role of incorporating length priors into the TSG task. </p>
<blockquote>
<p>时序句子定位（TSG）是一项极具挑战性的任务，旨在定位未剪辑视频中与给定自然语言描述相对应的时间段。得益于可学习查询的设计，基于DETR的模型在TSG任务中取得了重大进展。然而，由于缺乏明确的监督，通常会导致学习到的查询在角色上重叠，从而产生冗余预测。因此，我们提出通过使每个查询履行其指定角色来改善TSG任务，并利用视频描述对的时间长度先验信息。在本文中，我们为TSG引入了长度感知转换器（LATR），根据时间长度差异分配不同的查询来处理预测。具体来说，我们将所有查询分为三组，分别负责处理短、中和长时间段的片段。在训练过程中，引入了额外的长度分类任务。来自不匹配长度的查询预测会被抑制，从而引导每个查询专门执行其指定功能。大量实验证明了我们的LATR的有效性，在三个公共基准测试上达到了最新性能水平。此外，消融研究验证了我们的方法中每个组件的贡献以及将长度先验知识纳入TSG任务的关键作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04299v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出了一个针对时序句子定位（TSG）任务的新方法，即长度感知转换器（LATR）。该方法利用可学习查询的优势，旨在解决视频与描述之间时序段定位的问题。针对现有模型因缺乏明确监督导致的查询角色重叠问题，LATR通过利用视频描述对的长度先验信息，使每个查询履行其指定角色。具体来说，我们将查询分为三组，分别负责处理不同时长（短、中、长）的片段。训练过程中引入了额外的长度分类任务，抑制了不匹配长度的查询预测，使每个查询专注于其特定功能。在三个公共基准测试上的大量实验表明，LATR方法取得了卓越的性能。此外，消融研究也验证了该方法每个组件的作用以及将长度先验信息融入TSG任务的重要性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>TSG任务旨在定位给定自然语言描述在未经剪辑视频中的时间片段，是一个具有挑战性的任务。</li>
<li>基于DETR的模型通过设计可学习查询实现了TSG任务的显著进展。</li>
<li>缺乏明确监督会导致查询角色重叠和冗余预测。</li>
<li>LATR方法通过利用视频描述对的长度先验信息，使每个查询履行其指定角色来解决这个问题。</li>
<li>LATR将查询分为三组，分别处理不同时长的预测。</li>
<li>额外的长度分类任务在训练过程中被引入，以抑制不匹配长度的查询预测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04299">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-95c8ab955ab00fb83c228582cd0771a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ee63cdf9484b2fbfc670dcca47fa9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-018abaee4e75606e30c8ade2847ee5a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-002323844bfa4552a30f79061cec83ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8a40e42d8ea20f241e590887df9b367.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2af9ed00379260d1a2a3ad627b0480c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25e0882560a9c735442242b8a9908241.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech"><a href="#Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech" class="headerlink" title="Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech"></a>Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech</h2><p><strong>Authors:Jingyuan Xing, Zhipeng Li, Jialong Mai, Xiaofen Xing, Xiangmin Xu</strong></p>
<p>Advances in speech representation and large language models have enhanced zero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS models face challenges in capturing the complex correlations between acoustic and semantic features, resulting in a lack of expressiveness and similarity. The primary reason lies in the complex relationship between semantic and acoustic features, which manifests independent and interdependent aspects.This paper introduces a TTS framework that combines both autoregressive (AR) and non-autoregressive (NAR) modules to harmonize the independence and interdependence of acoustic and semantic information. The AR model leverages the proposed Parallel Tokenizer to synthesize the top semantic and acoustic tokens simultaneously. In contrast, considering the interdependence, the Coupled NAR model predicts detailed tokens based on the general AR model’s output. Parallel GPT, built on this architecture, is designed to improve zero-shot text-to-speech synthesis through its parallel structure. Experiments on English and Chinese datasets demonstrate that the proposed model significantly outperforms the quality and efficiency of the synthesis of existing zero-shot TTS models. Speech demos are available at <a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/">https://t1235-ch.github.io/pgpt/</a>. </p>
<blockquote>
<p>语音表示的进展和大语言模型的进步已经提高了零样本文本到语音（TTS）的性能。然而，现有的零样本TTS模型在捕获声学和语义特征之间的复杂关联方面面临挑战，导致缺乏表达性和相似性。主要原因在于语义和声学特征之间的复杂关系，表现为独立和相互依赖的方面。</p>
</blockquote>
<p>本文介绍了一个结合自回归（AR）和非自回归（NAR）模块的TTS框架，以协调声学和语义信息的独立性和相互依赖性。AR模型利用提出的并行分词器同时合成顶级语义和声音标记。相反，考虑到相互依赖性，耦合的NAR模型基于通用AR模型的输出进行详细的标记预测。基于此架构构建的并行GPT旨在通过其并行结构改进零样本文本到语音合成。在英语和中文数据集上的实验表明，所提出的模型在合成质量和效率上显著优于现有的零样本TTS模型。语音演示可在<a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/">https://t1235-ch.github.io/pgpt/</a> 上找到。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04141v1">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing (TASLP)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了结合自回归（AR）和非自回归（NAR）模块的新型文本转语音（TTS）框架。该框架旨在调和声学语义信息的独立性和相互依赖性，通过并行令牌化器（Parallel Tokenizer）和耦合NAR模型等技术，增强了零样本文本转语音的性能和表达性。实验证明，该模型在英文和中文数据集上的合成质量和效率均显著优于现有零样本TTS模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>先进的语音表示和大型语言模型增强了零样本文本转语音（TTS）的性能。</li>
<li>现有零样本TTS模型在捕获声学语义特征的复杂相关性方面存在挑战，导致缺乏表达性和相似性。</li>
<li>本文介绍了结合AR和NAR模块的TTS框架，以调和声学语义信息的独立性和相互依赖性。</li>
<li>AR模型使用并行令牌化器（Parallel Tokenizer）同时合成顶级语义和声音标记。</li>
<li>耦合NAR模型基于AR模型的输出预测详细标记。</li>
<li>基于此架构构建的并行GPT旨在通过其并行结构改进零样本文本转语音合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04141">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-12f0de09a0900c1d7a9800be441359cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa37142f69f7e05c7ee1f1ccc92eb754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92c8ad1224a54c6c9f7d2d2a40d2c8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d0677a13b72211796ef8e9ac8a1254d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Fine-tuning-for-Better-Few-Shot-Prompting-An-Empirical-Comparison-for-Short-Answer-Grading"><a href="#Fine-tuning-for-Better-Few-Shot-Prompting-An-Empirical-Comparison-for-Short-Answer-Grading" class="headerlink" title="Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for   Short Answer Grading"></a>Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for   Short Answer Grading</h2><p><strong>Authors:Joel Walsh, Siddarth Mamidanna, Benjamin Nye, Mark Core, Daniel Auerbach</strong></p>
<p>Research to improve Automated Short Answer Grading has recently focused on Large Language Models (LLMs) with prompt engineering and no- or few-shot prompting to achieve best results. This is in contrast to the fine-tuning approach, which has historically required large-scale compute clusters inaccessible to most users. New closed-model approaches such as OpenAI’s fine-tuning service promise results with as few as 100 examples, while methods using open weights such as quantized low-rank adaptive (QLORA) can be used to fine-tune models on consumer GPUs. We evaluate both of these fine-tuning methods, measuring their interaction with few-shot prompting for automated short answer grading (ASAG) with structured (JSON) outputs. Our results show that finetuning with small amounts of data has limited utility for Llama open-weight models, but that fine-tuning methods can outperform few-shot baseline instruction-tuned LLMs for OpenAI’s closed models. While our evaluation set is limited, we find some evidence that the observed benefits of finetuning may be impacted by the domain subject matter. Lastly, we observed dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by seeding the initial training examples with a significant amount of cheaply generated synthetic training data. </p>
<blockquote>
<p>近期关于提高自动简答评分的研究主要集中在利用大型语言模型（LLM）进行提示工程和无或少量样本提示，以取得最佳结果。这与传统的微调方法形成对比，后者在历史上需要大规模计算集群，大多数用户无法接触。新的封闭模型方法，如OpenAI的微调服务，仅用100个样本即可获得结果，而使用公开权重的方法，如量化低秩自适应（QLORA），可在消费者GPU上进行模型微调。我们评估了这两种微调方法，测量它们在少量提示与自动简答评分（ASAG）的交互作用下的结构化（JSON）输出。我们的结果表明，使用少量数据进行微调对于Llama公开权重模型的效用有限，但对于OpenAI的封闭模型，微调方法可以在指令调优的LLM上超越少量样本基线。虽然我们的评估集有限，但我们发现观察到的微调效益可能受到领域主题的影响。最后，我们观察到，通过在初始训练示例中引入大量低成本生成的合成训练数据，LLama 3.1 8B-Instruct公开权重模型的性能得到了显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04063v1">PDF</a> Proceedings of the Second Workshop on Automated Evaluation of   Learning and Assessment Content co-located with 26th International Conference   on Artificial Intelligence in Education (AIED 2025)</p>
<p><strong>Summary</strong></p>
<p>近期研究集中在利用大型语言模型（LLM）进行自动简答评分任务的改进上，通过提示工程和无&#x2F;少样本提示达到最佳效果。相较于过去需要大量计算集群的微调方法，新的封闭模型方法如OpenAI的微调服务仅需少量样本即可获得结果，同时使用公开权重的方法如量化低秩自适应（QLORA）可以在消费者GPU上进行模型微调。评估显示，对于Llama公开权重模型，小数据量的微调效用有限；但对于OpenAI的封闭模型，微调方法优于少样本基线指令调优的LLM。评价集虽有限，但观察到微调的好处可能受领域主题影响。此外，对Llama 3.1 8B-Instruct公开权重模型，通过初始训练示例引入大量廉价生成的合成训练数据，可观察到显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在自动简答评分任务中表现优异，通过提示工程和无&#x2F;少样本提示达到最佳效果。</li>
<li>与传统微调方法相比，新的封闭模型方法和公开权重方法如QLORA可在有限资源下实现模型微调。</li>
<li>对于某些模型（如Llama公开权重模型），小数据量的微调效果有限。</li>
<li>OpenAI的封闭模型在微调方法上表现出优于少样本基线指令调优的LLM的效果。</li>
<li>评估结果可能受领域主题影响。</li>
<li>初始训练示例中引入大量合成训练数据可以显著提升模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04063">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b06cc3fe78f1f99b6d6f6ee45f023072.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1b697fa05c5fcecc91d2d2873efe39e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5bbe84da1a24dc6693da5c891038a24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-793f0fdee84c333ec5f6d0a048cfdd2b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Globally-Predictable-k-Space-Interpolation-A-White-box-Transformer-Approach"><a href="#Towards-Globally-Predictable-k-Space-Interpolation-A-White-box-Transformer-Approach" class="headerlink" title="Towards Globally Predictable k-Space Interpolation: A White-box   Transformer Approach"></a>Towards Globally Predictable k-Space Interpolation: A White-box   Transformer Approach</h2><p><strong>Authors:Chen Luo, Qiyu Jin, Taofeng Xie, Xuemei Wang, Huayu Wang, Congcong Liu, Liming Tang, Guoqing Chen, Zhuo-Xu Cui, Dong Liang</strong></p>
<p>Interpolating missing data in k-space is essential for accelerating imaging. However, existing methods, including convolutional neural network-based deep learning, primarily exploit local predictability while overlooking the inherent global dependencies in k-space. Recently, Transformers have demonstrated remarkable success in natural language processing and image analysis due to their ability to capture long-range dependencies. This inspires the use of Transformers for k-space interpolation to better exploit its global structure. However, their lack of interpretability raises concerns regarding the reliability of interpolated data. To address this limitation, we propose GPI-WT, a white-box Transformer framework based on Globally Predictable Interpolation (GPI) for k-space. Specifically, we formulate GPI from the perspective of annihilation as a novel k-space structured low-rank (SLR) model. The global annihilation filters in the SLR model are treated as learnable parameters, and the subgradients of the SLR model naturally induce a learnable attention mechanism. By unfolding the subgradient-based optimization algorithm of SLR into a cascaded network, we construct the first white-box Transformer specifically designed for accelerated MRI. Experimental results demonstrate that the proposed method significantly outperforms state-of-the-art approaches in k-space interpolation accuracy while providing superior interpretability. </p>
<blockquote>
<p>在成像中，对k空间中的缺失数据进行插值是至关重要的。然而，现有的方法，包括基于卷积神经网络（CNN）的深度学习，主要利用局部预测性而忽略了k空间中固有的全局依赖性。最近，Transformer在自然语言处理和图像分析方面取得了显著的成功，这归功于其捕捉长期依赖性的能力。这启发我们将Transformer用于k空间插值，以更好地利用其全局结构。然而，其缺乏可解释性引发了人们对插值数据可靠性的担忧。为了解决这一局限性，我们提出了GPI-WT，这是一种基于全局可预测插值（GPI）的white-box Transformer框架，用于k空间。具体来说，我们从消除的角度制定GPI作为一种新型的k空间结构化低秩（SLR）模型。SLR模型中的全局消除滤波器被视为可学习的参数，SLR模型的子梯度自然地引发了一种可学习的注意力机制。通过将基于子梯度的SLR优化算法展开为级联网络，我们构建了第一个专门用于加速MRI的white-box Transformer。实验结果表明，该方法在k空间插值精度上显著优于最新技术，同时提供了出色的可解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04051v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于全局可预测插值（GPI）的白色盒子Transformer框架GPI-WT，用于加速MRI成像中的k空间插值。该方法通过从灭迹角度构建GPI，形成新型k空间结构化低秩（SLR）模型，并通过学习可调整全局灭迹过滤器。其利用SLR模型的子梯度自然诱导出可学习的注意力机制，并将基于子梯度的优化算法展开为级联网络，构建专门针对加速MRI的白色盒子Transformer。实验表明，该方法在k空间插值精度上显著优于现有技术，同时提供优越的解读性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>插值在加速成像中至关重要，但现有方法主要利用局部可预测性，忽略了k空间的固有全局依赖性。</li>
<li>Transformer在自然语言处理和图像分析中的成功，激发了其在k空间插值中的应用，以更好地利用全局结构。</li>
<li>缺乏解释性仍是Transformer应用的主要顾虑，本研究提出了一种白色盒子Transformer框架GPI-WT。</li>
<li>GPI-WT基于全局可预测插值（GPI）和k空间结构化低秩（SLR）模型，从灭迹角度构建。</li>
<li>SLR模型中的全局灭迹过滤器被视为可学习参数，其子梯度自然引导可学习的注意力机制。</li>
<li>通过将SLR模型的子梯度优化算法展开为级联网络，创建了专门针对加速MRI的白色盒子Transformer。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04051">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-776ecf7060c4154e5502df4c0902f13f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40cb0f8f4345f13c25ab9a577621f78c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VisualTrans-A-Benchmark-for-Real-World-Visual-Transformation-Reasoning"><a href="#VisualTrans-A-Benchmark-for-Real-World-Visual-Transformation-Reasoning" class="headerlink" title="VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning"></a>VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning</h2><p><strong>Authors:Yuheng Ji, Yipu Wang, Yuyang Liu, Xiaoshuai Hao, Yue Liu, Yuting Zhao, Huaihai Lyu, Xiaolong Zheng</strong></p>
<p>Visual transformation reasoning (VTR) is a vital cognitive capability that empowers intelligent agents to understand dynamic scenes, model causal relationships, and predict future states, and thereby guiding actions and laying the foundation for advanced intelligent systems. However, existing benchmarks suffer from a sim-to-real gap, limited task complexity, and incomplete reasoning coverage, limiting their practical use in real-world scenarios. To address these limitations, we introduce VisualTrans, the first comprehensive benchmark specifically designed for VTR in real-world human-object interaction scenarios. VisualTrans encompasses 12 semantically diverse manipulation tasks and systematically evaluates three essential reasoning dimensions - spatial, procedural, and quantitative - through 6 well-defined subtask types. The benchmark features 472 high-quality question-answer pairs in various formats, including multiple-choice, open-ended counting, and target enumeration. We introduce a scalable data construction pipeline built upon first-person manipulation videos, which integrates task selection, image pair extraction, automated metadata annotation with large multimodal models, and structured question generation. Human verification ensures the final benchmark is both high-quality and interpretable. Evaluations of various state-of-the-art vision-language models show strong performance in static spatial tasks. However, they reveal notable shortcomings in dynamic, multi-step reasoning scenarios, particularly in areas like intermediate state recognition and transformation sequence planning. These findings highlight fundamental weaknesses in temporal modeling and causal reasoning, providing clear directions for future research aimed at developing more capable and generalizable VTR systems. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/WangYipu2002/VisualTrans">https://github.com/WangYipu2002/VisualTrans</a>. </p>
<blockquote>
<p>视觉转换推理（VTR）是一种重要的认知能力，赋予智能主体理解动态场景、模拟因果关系以及预测未来状态的能力，从而为行动提供指导，并为先进的智能系统奠定基础。然而，现有的基准测试存在模拟到现实的差距、任务复杂性有限以及推理覆盖不完整等问题，限制了它们在现实场景中的实际应用。为了解决这些局限性，我们推出了VisualTrans，这是专门为现实世界中的人机交互场景中的VTR设计的首个综合基准测试。</p>
</blockquote>
<p>VisualTrans涵盖了12种语义多样的操作任务，并通过6种定义良好的子任务类型，系统地评估了空间、程序和定量三个重要的推理维度。基准测试包含472组高质量的问题答案对，形式多样，包括选择题、开放式计数和目标枚举等。我们引入了一个基于第一人称操作视频的可扩展数据构建流程，该流程包括任务选择、图像对提取、使用大型多模态模型进行自动化元数据注释以及结构化问题生成。人为验证确保了最终基准测试既高质量又易于解释。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04043v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>视觉转换推理（VTR）是智能主体理解动态场景、模拟因果关系并预测未来状态的重要认知能力，它为行动指导奠定了基础，并为先进智能系统提供了支撑。然而，现有基准测试存在模拟到现实的差距、任务复杂性有限和推理覆盖不全面等局限性，限制了它们在现实场景中的实际应用。为解决这些问题，我们推出了VisualTrans，这是专为现实世界中的人机交互场景中的VTR设计的首个全面基准测试。VisualTrans包含12个语义多样的操作任务，并通过6个定义明确的任务类型系统地评估空间、程序和定量三个基本推理维度。该基准测试包含472组高质量的问题答案对，包括多种形式，如选择题、开放式计数和目标枚举等。我们建立了一个可扩展的数据构建管道，该管道建立在第一人称操作视频之上，集成了任务选择、图像对提取、使用大型多模态模型的自动化元数据注释和结构化问题生成。人类的验证确保了最终基准测试既高质量又易于解释。对各种最先进的视觉语言模型的评估表明，它们在静态空间任务中表现出强大的性能。然而，在动态多步骤推理场景中，特别是在中间状态识别和转换序列规划方面，它们表现出明显的不足。这些发现突显了时间建模和因果关系推理中的基本弱点，为未来研究提供了明确方向，旨在开发更具能力和通用性的VTR系统。数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/WangYipu2002/VisualTrans%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/WangYipu2002/VisualTrans获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>视觉转换推理（VTR）是智能系统的核心认知能力，涉及理解动态场景、模拟因果关系和预测未来状态。</li>
<li>现有基准测试存在模拟到现实的差距、任务复杂性有限和推理覆盖不全面等局限性。</li>
<li>VisualTrans基准测试是专为现实世界中的人机交互场景中的VTR设计的首个全面基准测试。</li>
<li>VisualTrans包含12个语义多样的操作任务和6种任务类型，评估空间、程序和定量三个基本推理维度。</li>
<li>基准测试包含高质量的问题答案对，包括多种形式，如选择题、开放式计数和目标枚举等。</li>
<li>评估显示，现有模型在静态空间任务中表现良好，但在动态多步骤推理场景中存在不足，特别是在中间状态识别和转换序列规划方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04043">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9ea52f74b5c7a0cdfb0789c1d13fa357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14c231dbe1f362a9b289522c68b71eae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57c1ea0ae9574b837d3e202ea1b641e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63408dd1056ab254fe45dc38b20e9174.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14e12d50bcb50bc707e32093acffba8f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Probing-the-Gaps-in-ChatGPT-Live-Video-Chat-for-Real-World-Assistance-for-People-who-are-Blind-or-Visually-Impaired"><a href="#Probing-the-Gaps-in-ChatGPT-Live-Video-Chat-for-Real-World-Assistance-for-People-who-are-Blind-or-Visually-Impaired" class="headerlink" title="Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance   for People who are Blind or Visually Impaired"></a>Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance   for People who are Blind or Visually Impaired</h2><p><strong>Authors:Ruei-Che Chang, Rosiana Natalie, Wenqian Xu, Jovan Zheng Feng Yap, Anhong Guo</strong></p>
<p>Recent advancements in large multimodal models have provided blind or visually impaired (BVI) individuals with new capabilities to interpret and engage with the real world through interactive systems that utilize live video feeds. However, the potential benefits and challenges of such capabilities to support diverse real-world assistive tasks remain unclear. In this paper, we present findings from an exploratory study with eight BVI participants. Participants used ChatGPT’s Advanced Voice with Video, a state-of-the-art live video AI released in late 2024, in various real-world scenarios, from locating objects to recognizing visual landmarks, across unfamiliar indoor and outdoor environments. Our findings indicate that current live video AI effectively provides guidance and answers for static visual scenes but falls short in delivering essential live descriptions required in dynamic situations. Despite inaccuracies in spatial and distance information, participants leveraged the provided visual information to supplement their mobility strategies. Although the system was perceived as human-like due to high-quality voice interactions, assumptions about users’ visual abilities, hallucinations, generic responses, and a tendency towards sycophancy led to confusion, distrust, and potential risks for BVI users. Based on the results, we discuss implications for assistive video AI agents, including incorporating additional sensing capabilities for real-world use, determining appropriate intervention timing beyond turn-taking interactions, and addressing ecological and safety concerns. </p>
<blockquote>
<p>最近的大型多模态模型的进步为盲人或视力受损（BVI）个体提供了通过利用实时视频反馈的互动系统来解读和与现实世界互动的新能力。然而，此类能力在支持多样化的现实世界辅助任务方面的潜在优势和挑战仍不明确。在本文中，我们展示了与八名BVI参与者进行的探索性研究的发现。参与者在各种现实场景中使用了ChatGPT的先进语音和视频功能，这是一项于2024年末发布的最新实时视频人工智能技术，从寻找物体到识别视觉地标，涉及不熟悉的室内和室外环境。我们的研究结果表明，当前的实时视频AI在提供静态视觉场景的指导和答案方面非常有效，但在提供动态情况下所需的关键实时描述方面却相形见绌。尽管在空间和距离信息方面存在不准确之处，参与者还是利用所提供的视觉信息来补充他们的行动策略。尽管由于高质量的声音互动，系统被感知为人类般的存在，但对于用户的视觉能力的假设、幻觉、通用回应和奉承的倾向导致了混淆、不信任和盲人用户的潜在风险。基于研究结果，我们讨论了辅助视频AI代理的启示，包括增加现实世界使用的额外感知能力、确定超越轮流互动的适当干预时间以及解决生态和安全问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03651v1">PDF</a> ACM ASSETS 2025</p>
<p><strong>Summary</strong>：<br>大型多模态模型的最新进展为盲人或视觉障碍（BVI）个体提供了通过交互式系统解读并参与到现实世界的新能力，该系统采用实时视频流。本文通过一项包含八名BVI参与者的探索性研究来探讨这一技术在现实世界中所面临的潜力和挑战。尽管在静态视觉场景方面表现出色，但在动态场景中提供必要的实时描述方面存在不足。参与者虽然能够利用提供的视觉信息辅助其移动策略，但对用户的视觉能力做出假设、幻象、通用响应以及迎合倾向可能导致混淆、不信任以及对BVI用户存在潜在风险。需要进一步改进AI助手的感知能力、干预时机和生态安全问题。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型多模态模型为盲人或视觉障碍者提供了通过实时视频流解读现实世界的交互系统新能力。</li>
<li>实时视频AI在静态视觉场景方面表现出色，但在动态场景中提供必要的实时描述方面存在不足。</li>
<li>尽管空间及距离信息存在不准确的情况，但参与者仍能利用提供的视觉信息辅助移动策略。</li>
<li>系统在人性化方面做得较好，但仍存在假设用户视觉能力、幻象、通用响应和迎合倾向等问题。</li>
<li>对视频AI助手的改进需要增加感知能力以适应现实世界的复杂环境。</li>
<li>需要确定适当的干预时机，超越对话中的轮流交互模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03651">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-000d0d15e19365a31233e2b710aee7b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a0c2c5b65f8c587aa0e8e5024512ebf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf28b6d848eedef4694642c88ff98a9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa331315d12ed278fea76594516dc144.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Tackling-Distribution-Shift-in-LLM-via-KILO-Knowledge-Instructed-Learning-for-Continual-Adaptation"><a href="#Tackling-Distribution-Shift-in-LLM-via-KILO-Knowledge-Instructed-Learning-for-Continual-Adaptation" class="headerlink" title="Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed   Learning for Continual Adaptation"></a>Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed   Learning for Continual Adaptation</h2><p><strong>Authors:Iing Muttakhiroh, Thomas Fevens</strong></p>
<p>Large Language Models (LLMs) often suffer from performance degradation when faced with domain shifts, primarily due to catastrophic forgetting. In this work, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation), a novel continual learning framework that integrates dynamic knowledge graphs with instruction tuning. By leveraging retrieved domain-specific knowledge as guidance during training, KILO enhances both adaptability to new domains and retention of previously acquired knowledge. We pretrain our model on WikiText-103 and evaluate sequential adaptation across four diverse target domains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that KILO consistently outperforms strong baselines, including continual fine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward transfer, F1 score, retention rate, and training efficiency. These results highlight the effectiveness of combining structured knowledge retrieval and instruction prompting to overcome domain shift challenges in continual learning scenarios. </p>
<blockquote>
<p>大型语言模型（LLM）在面临领域迁移时常常会出现性能下降的问题，这主要是由于灾难性遗忘造成的。在这项工作中，我们提出了KILO（用于持续适应的知识指导学习），这是一种新型持续学习框架，它结合了动态知识图谱和指令微调。通过利用检索到的领域特定知识作为训练过程中的指导，KILO提高了对新领域的适应能力和对先前获取知识的保持能力。我们在WikiText-103上预训练我们的模型，并在四个不同的目标领域（BioASQ、SciQ、TweetEval和MIND）进行顺序适应评估。实验表明，KILO在向后迁移、向前迁移、F1分数、保持率和训练效率方面均优于持续微调、ERNIE 2.0和CPT等强大的基线模型。这些结果突出了在持续学习场景中结合结构化知识检索和指令提示以克服领域迁移挑战的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03571v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为KILO的新型持续学习框架，旨在解决大型语言模型（LLM）在面临领域迁移时的性能下降问题。KILO通过结合动态知识图谱和指令调整，利用检索到的领域特定知识作为训练过程中的指导，从而提高对新领域的适应能力和对先前获取知识的保留能力。实验表明，KILO在多个目标领域上的表现均优于强基线，包括连续微调、ERNIE 2.0和CPT，在反向传递、正向传递、F1分数、保持率和训练效率等方面均有所改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KILO是一种新型的持续学习框架，旨在解决LLM在面临领域迁移时的性能下降问题。</li>
<li>KILO结合了动态知识图谱和指令调整，以提高模型对新领域的适应能力和保留先前知识的能力。</li>
<li>实验表明，KILO在多个目标领域上的表现均优于其他方法。</li>
<li>KILO的优势体现在反向传递、正向传递、F1分数、保持率和训练效率等方面。</li>
<li>KILO利用检索到的领域特定知识作为训练过程中的指导。</li>
<li>KILO框架可以有效地克服领域迁移挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03571">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a57c38161ca33a864eedfe644d397dc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-105ebeff4333fd56aa7d6b39d030e82c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8000db1f2a3481832b6159971fad64fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bef6c41cdd4d55d2fd225f9dc6d6411.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MoKA-Mixture-of-Kronecker-Adapters"><a href="#MoKA-Mixture-of-Kronecker-Adapters" class="headerlink" title="MoKA: Mixture of Kronecker Adapters"></a>MoKA: Mixture of Kronecker Adapters</h2><p><strong>Authors:Mohammadreza Sadeghi, Mahsa Ghazvini Nejad, MirHamed Jafarzadeh Asl, Yu Gu, Yuanhao Yu, Masoud Asgharian, Vahid Partovi Nia</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) is essential for reducing the computational overhead of large language models (LLMs). Low-rank family adapters are commonly used to control the parameter size efficiently while maintaining the generative power of LLMs. However, their limited expressiveness due to the rank constraint often restricts their performance on complex tasks. We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker adapters that addresses this limitation by modeling weight updates as a mixture of Kronecker products. Our proposed adapter leverages a gating mechanism that measures the importance of each Kronecker factor, enabling more expressive adaptation. Moreover, MoKA enables a rank flexibility that provides a better trade-off between parameter efficiency and accuracy. To ensure hardware efficiency, we reformulate Kronecker computations using standard matrix operations, allowing seamless deployment on GPU-optimized hardware. We conduct extensive experiments on instruction-tuning and commonsense reasoning tasks using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not only outperforms PEFT baselines, but also reduces the number of trainable parameters up to 27x, achieving state-of-the-art trade-offs between performance and parameter efficiency. </p>
<blockquote>
<p>参数高效微调（PEFT）对于减少大型语言模型（LLM）的计算开销至关重要。低秩家族适配器通常用于有效地控制参数大小，同时保持LLM的生成能力。然而，由于秩约束导致的有限表达能力往往限制了它们在复杂任务上的性能。我们提出了混合克罗内克适配器（MoKA），这是一种新一代的克罗内克适配器，它通过模拟权重更新作为克罗内克产品的混合来解决这个问题。我们提出的适配器利用门控机制来衡量每个克罗内克因子的重要性，从而实现更具表现力的适应。此外，MoKA提供了秩灵活性，在参数效率和准确性之间提供了更好的权衡。为了保证硬件效率，我们利用标准矩阵操作重新制定了克罗内克计算，可以无缝地部署在GPU优化的硬件上。我们在指令调整和常识推理任务上进行了大量实验，采用了LLaMA的低比特量化版本（包括LLaMA的精简版本和轻量级版本）。MoKA不仅优于PEFT基线，还将训练参数数量减少了高达27倍，在性能和参数效率方面达到了最先进的权衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03527v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>参数高效微调（PEFT）对于减少大型语言模型（LLM）的计算开销至关重要。低秩家族适配器在控制参数大小的同时保持了LLM的生成能力。然而，由于秩的限制，其表达能力的有限性常常限制了其在复杂任务上的性能。本文提出混合克罗内克适配器（MoKA），这是一种新型的克罗内克适配器，通过模拟权重更新作为克罗内克积的混合来解决这一问题。MoKA利用门控机制来衡量每个克罗内克因子的重要性，从而实现更灵活的适应表达。此外，MoKA能够在参数效率和准确性之间实现更好的权衡。为确保硬件效率，本文使用标准矩阵操作重新制定了克罗内克计算，可在GPU优化硬件上进行无缝部署。在指令调整和常识推理任务上，使用低精度量化的LLaMA2-7B和LLaMA3-8B模型进行的广泛实验表明，MoKA不仅优于PEFT基线，而且可将训练参数减少至原来的27倍，在性能和参数效率之间取得了最先进的权衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>参数高效微调（PEFT）对减少大型语言模型的计算开销很重要。</li>
<li>低秩家族适配器在控制参数大小和保持LLM生成能力之间取得平衡，但其在复杂任务上的表现受限。</li>
<li>MoKA作为一种新型的克罗内克适配器，通过模拟权重更新作为克罗内克积的混合，解决了低秩家族适配器的限制。</li>
<li>MoKA利用门控机制，提高适配器的表达能力，并在参数效率和准确性之间实现更好的权衡。</li>
<li>MoKA通过标准矩阵操作进行克罗内克计算，提高了硬件效率，可在GPU上无缝部署。</li>
<li>在指令调整和常识推理任务上，MoKA优于PEFT基线，并显著减少了训练参数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03527">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4443f751a71a452718be124dbfef1d9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-630e237ca6f6c10aa0615ef71d011781.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d55f986e80ffc090350d0401166bf4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd277e69c722ab5f5e72e17a26b1caa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22ac4a556347ba46b204e5e5c6249821.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="On-the-Evaluation-of-Large-Language-Models-in-Multilingual-Vulnerability-Repair"><a href="#On-the-Evaluation-of-Large-Language-Models-in-Multilingual-Vulnerability-Repair" class="headerlink" title="On the Evaluation of Large Language Models in Multilingual Vulnerability   Repair"></a>On the Evaluation of Large Language Models in Multilingual Vulnerability   Repair</h2><p><strong>Authors:Dong wang, Junji Yu, Honglin Shu, Michael Fu, Chakkrit Tantithamthavorn, Yasutaka Kamei, Junjie Chen</strong></p>
<p>Various Deep Learning-based approaches with pre-trained language models have been proposed for automatically repairing software vulnerabilities. However, these approaches are limited to a specific programming language (C&#x2F;C++). Recent advances in large language models (LLMs) offer language-agnostic capabilities and strong semantic understanding, exhibiting potential to overcome multilingual vulnerability limitations. Although some work has begun to explore LLMs’ repair performance, their effectiveness is unsatisfactory. To address these limitations, we conducted a large-scale empirical study to investigate the performance of automated vulnerability repair approaches and state-of-the-art LLMs across seven programming languages. Results show GPT-4o, instruction-tuned with few-shot prompting, performs competitively against the leading approach, VulMaster. Additionally, the LLM-based approach shows superior performance in repairing unique vulnerabilities and is more likely to repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o demonstrates strong generalization on vulnerabilities in previously unseen language, outperforming existing approaches. Analysis shows Go consistently achieves the highest effectiveness across all model types, while C&#x2F;C++ performs the worst. Based on findings, we discuss the promise of LLM on multilingual vulnerability repair and the reasons behind LLM’s failed cases. This work takes the first look at repair approaches and LLMs across multiple languages, highlighting the promising future of adopting LLMs for multilingual vulnerability repair. </p>
<blockquote>
<p>基于深度学习和预训练语言模型的多种方法已被提出，用于自动修复软件漏洞。然而，这些方法仅限于特定的编程语言（如C&#x2F;C++）。最近大型语言模型（LLM）的进展提供了与语言无关的能力和强大的语义理解，显示出克服多语言漏洞限制的潜力。尽管已经开始探索LLM的修复性能，但其有效性尚不理想。为了解决这个问题，我们进行了一项大规模实证研究，调查了自动化漏洞修复方法和最新LLM在七种编程语言中的表现。结果表明，经过指令微调、通过几次提示进行的GPT-4o，与领先的VulMaster方法相比表现良好。此外，基于LLM的方法在修复独特漏洞时表现出卓越的性能，并且更有可能修复最危险的漏洞。经过指令调校的GPT-4o在以前未见语言的漏洞上表现出强大的泛化能力，超过了现有方法。分析表明，Go在所有模型类型中始终获得最高的有效性，而C&#x2F;C++的表现最差。基于研究结果，我们讨论了LLM在多语言漏洞修复方面的前景以及LLM失败案例背后的原因。这项工作首次跨多种语言研究修复方法和LLM，突显了采用LLM进行多语言漏洞修复的广阔前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03470v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于深度学习和预训练语言模型的自动修复软件漏洞方法已经提出，但仅限于特定编程语言（如C&#x2F;C++）。最近的大型语言模型（LLM）的进展提供了跨语言的通用能力和强大的语义理解，具有克服多语言漏洞限制的潜力。尽管已经开始探索LLM在漏洞修复方面的性能，但其有效性尚不满意。本研究进行了大规模实证研究，探讨了自动化漏洞修复方法和最先进的LLM在七种编程语言中的表现。结果显示，经过指令微调且使用少量提示的GPT-4o与领先的VulMaster方法竞争表现良好。此外，基于LLM的方法在修复独特漏洞时表现出卓越的性能，并更有可能修复最危险的漏洞。经过指令调校的GPT-4o在之前未见过的语言中的漏洞上表现出强大的泛化能力。分析显示，Go在所有模型类型中始终获得最高的有效性，而C&#x2F;C++表现最差。本研究首次跨多种语言研究修复方法和LLM，突显了采用LLM进行多语言漏洞修复的广阔前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）具有跨语言的通用能力和强大的语义理解，有助于克服多语言漏洞修复的局限性。</li>
<li>相比传统方法，LLM在自动修复软件漏洞方面展现出竞争力，特别是在处理独特和最危险的漏洞时。</li>
<li>GPT-4o经过指令微调后，在跨语言的漏洞修复上表现出优异的性能，且能够泛化到之前未见过的语言。</li>
<li>在研究的七种编程语言中，Go在漏洞修复方面的有效性最高，而C&#x2F;C++表现相对较差。</li>
<li>LLM在修复某些漏洞时仍存在失败情况，需要进一步研究和优化。</li>
<li>本研究首次全面评估了LLM在多语言环境下的漏洞修复能力，为未来的研究提供了有价值的参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03470">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0645a16447deda0f74b821db44e52506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4100e5b2b0e7f651825c8d1450308a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f21042fafdcf88c85716f9cf596b5a57.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Evaluation-and-Analysis-of-Deep-Neural-Transformers-and-Convolutional-Neural-Networks-on-Modern-Remote-Sensing-Datasets"><a href="#Evaluation-and-Analysis-of-Deep-Neural-Transformers-and-Convolutional-Neural-Networks-on-Modern-Remote-Sensing-Datasets" class="headerlink" title="Evaluation and Analysis of Deep Neural Transformers and Convolutional   Neural Networks on Modern Remote Sensing Datasets"></a>Evaluation and Analysis of Deep Neural Transformers and Convolutional   Neural Networks on Modern Remote Sensing Datasets</h2><p><strong>Authors:J. Alex Hurt, Trevor M. Bajkowski, Grant J. Scott, Curt H. Davis</strong></p>
<p>In 2012, AlexNet established deep convolutional neural networks (DCNNs) as the state-of-the-art in CV, as these networks soon led in visual tasks for many domains, including remote sensing. With the publication of Visual Transformers, we are witnessing the second modern leap in computational vision, and as such, it is imperative to understand how various transformer-based neural networks perform on satellite imagery. While transformers have shown high levels of performance in natural language processing and CV applications, they have yet to be compared on a large scale to modern remote sensing data. In this paper, we explore the use of transformer-based neural networks for object detection in high-resolution electro-optical satellite imagery, demonstrating state-of-the-art performance on a variety of publicly available benchmark data sets. We compare eleven distinct bounding-box detection and localization algorithms in this study, of which seven were published since 2020, and all eleven since 2015. The performance of five transformer-based architectures is compared with six convolutional networks on three state-of-the-art opensource high-resolution remote sensing imagery datasets ranging in size and complexity. Following the training and evaluation of thirty-three deep neural models, we then discuss and analyze model performance across various feature extraction methodologies and detection algorithms. </p>
<blockquote>
<p>在2012年，AlexNet确立了深度卷积神经网络（DCNN）在计算机视觉（CV）领域的领先地位，因为这些网络很快在许多领域（包括遥感）的视觉任务中占据主导地位。随着视觉Transformer的发布，我们见证了计算视觉的第二次现代飞跃，因此了解各种基于Transformer的神经网络在卫星图像上的表现至关重要。虽然Transformer在自然语言处理和计算机视觉应用中表现出卓越的性能，但它们尚未在大规模现代遥感数据上得到广泛比较。在本文中，我们探讨了基于Transformer的神经网络在高分辨率光电卫星图像中的目标检测应用，并在各种公开的基准数据集上展示了其卓越的性能。在本研究中，我们比较了11种不同的边界框检测和定位算法，其中7种自2020年以来发表，所有11种自2015年以来均有所发展。在五种基于Transformer的架构与六种卷积网络之间进行了比较，涉及三个开源的先进高分辨率遥感图像数据集，这些数据集在规模和复杂性方面各不相同。在对33个深度神经网络进行训练和评估后，我们进一步讨论并分析了各种特征提取方法和检测算法之间的模型性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02871v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇论文探讨了基于transformer的神经网络在高分辨率光电卫星图像中的目标检测应用，对比了多个公开的基准数据集上的性能。该研究对比了多种检测算法，包括七个自2020年以来发布的算法和所有自2015年以来发布的十一个算法。论文对比了五种基于transformer的架构和六种卷积网络在三种不同规模和复杂度的开源高分辨率遥感图像数据集上的表现。研究通过对三十三个深度神经网络模型进行训练和评估，分析和讨论了模型性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于transformer的神经网络在高分辨率遥感图像的目标检测中表现出卓越性能。</li>
<li>该研究对比了多个公开的基准数据集上的目标检测算法。</li>
<li>对比了多种检测算法，包括近年来新发布的算法。</li>
<li>对比了五种基于transformer的架构和六种卷积网络在遥感图像数据集上的表现。</li>
<li>通过对三十三个深度神经网络模型的训练和评估，全面分析了模型性能。</li>
<li>基于transformer的神经网络在遥感图像目标检测领域具有潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02871">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-822ea6f09fddaa38d65075ebd24d4758.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76b74f55566b73ac13417e63a52b4bd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-259aac26c9572a30e76899cffdca6bb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-976343bac01bb794579b0bf756aebb8e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-66775d0a15f700bc915885216720c027.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-08  From MAS to MARS Coordination Failures and Reasoning Trade-offs in   Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-23a46cc0e9ff42796efe41c51ffd474d.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-08  From MAS to MARS Coordination Failures and Reasoning Trade-offs in   Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
