<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-08-08  DS$^2$Net Detail-Semantic Deep Supervision Network for Medical Image   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-0665bc2b177017d95aa2701c2d6c943a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-08-更新"><a href="#2025-08-08-更新" class="headerlink" title="2025-08-08 更新"></a>2025-08-08 更新</h1><h2 id="DS-2-Net-Detail-Semantic-Deep-Supervision-Network-for-Medical-Image-Segmentation"><a href="#DS-2-Net-Detail-Semantic-Deep-Supervision-Network-for-Medical-Image-Segmentation" class="headerlink" title="DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image   Segmentation"></a>DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image   Segmentation</h2><p><strong>Authors:Zhaohong Huang, Yuxin Zhang, Mingbao Lin, Taojian Zhou, Guorong Cai, Rongrong Ji</strong></p>
<p>Deep Supervision Networks exhibit significant efficacy for the medical imaging community. Nevertheless, existing work merely supervises either the coarse-grained semantic features or fine-grained detailed features in isolation, which compromises the fact that these two types of features hold vital relationships in medical image analysis. We advocate the powers of complementary feature supervision for medical image segmentation, by proposing a Detail-Semantic Deep Supervision Network (DS$^2$Net). DS$^2$Net navigates both low-level detailed and high-level semantic feature supervision through Detail Enhance Module (DEM) and Semantic Enhance Module (SEM). DEM and SEM respectively harness low-level and high-level feature maps to create detail and semantic masks for enhancing feature supervision. This is a novel shift from single-view deep supervision to multi-view deep supervision. DS$^2$Net is also equipped with a novel uncertainty-based supervision loss that adaptively assigns the supervision strength of features within distinct scales based on their uncertainty, thus circumventing the sub-optimal heuristic design that typifies previous works. Through extensive experiments on six benchmarks captured under either colonoscopy, ultrasound and microscope, we demonstrate that DS$^2$Net consistently outperforms state-of-the-art methods for medical image analysis. </p>
<blockquote>
<p>深度监督网络对医学影像界具有显著效果。然而，现有工作仅孤立地监督粗粒度语义特征或细粒度详细特征，这忽略了这两种特征在医学图像分析中具有重要关联的事实。我们提出通过细节语义深度监督网络（DS$^2$Net）进行医学图像分割的互补特征监督。DS$^2$Net通过细节增强模块（DEM）和语义增强模块（SEM）导航低级别的详细特征和高级别的语义特征监督。DEM和SEM分别利用低级别和高级别特征图创建细节和语义掩膜，以增强特征监督。这是从单视图深度监督到多视图深度监督的新转变。DS$^2$Net还配备了一种新型的不确定性监督损失，根据特征的不确定性自适应地分配不同尺度的特征监督强度，从而避免了以前工作中典型的次优启发式设计。通过在结肠镜、超声和显微镜下的六个基准测试集上进行广泛实验，我们证明了DS$^2$Net在医学图像分析方面始终优于最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04131v1">PDF</a> </p>
<p><strong>Summary</strong>：深度监督网络对医学影像领域具有显著效果。然而，现有工作仅孤立地监督粗粒度语义特征或细粒度详细特征，忽略了两者在医学图像分析中的紧密关系。我们提出了一个细节语义深度监督网络（DS^2Net），通过细节增强模块（DEM）和语义增强模块（SEM）进行互补特征监督。DEM和SEM分别利用低层次和高层次特征图创建细节和语义掩膜，以增强特征监督，实现了从单视角深度监督到多视角深度监督的新转变。此外，DS^2Net还引入了一种基于不确定性监督的损失函数，根据特征的不确定性自适应地分配不同尺度的特征监督强度，避免了以往工作中的次优启发式设计。在六个不同医学成像模式下的基准测试上，DS^2Net表现出卓越的性能，超越了现有的医学图像分析方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>深度监督网络在医学图像分析中具有显著效果。</li>
<li>现有工作孤立地监督粗粒度语义特征和细粒度详细特征，忽略了两者之间的关系。</li>
<li>提出了细节语义深度监督网络（DS^2Net），通过细节增强模块（DEM）和语义增强模块（SEM）进行互补特征监督。</li>
<li>DS^2Net实现了从单视角到多视角的深度监督转变。</li>
<li>DS^2Net引入了基于不确定性监督的损失函数，自适应地分配不同尺度的特征监督强度。</li>
<li>该损失函数克服了以往工作中的次优启发式设计。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04131">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-12038c37e305b1174df01b7db703d42a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e64252daba2f7f13a44d3d247a7bae63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bf20dc993e4ab1425912cef9bf68fde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58c30226eecd9c1721e6b22cf9e8d8bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c008540d8e5195615f8fdd26c5d65594.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48ea483f9e8b117e5de121c814510cbe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Conditional-Latent-Diffusion-Models-for-Zero-Shot-Instance-Segmentation"><a href="#Conditional-Latent-Diffusion-Models-for-Zero-Shot-Instance-Segmentation" class="headerlink" title="Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation"></a>Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation</h2><p><strong>Authors:Maximilian Ulmer, Wout Boerdijk, Rudolph Triebel, Maximilian Durner</strong></p>
<p>This paper presents OC-DiT, a novel class of diffusion models designed for object-centric prediction, and applies it to zero-shot instance segmentation. We propose a conditional latent diffusion framework that generates instance masks by conditioning the generative process on object templates and image features within the diffusion model’s latent space. This allows our model to effectively disentangle object instances through the diffusion process, which is guided by visual object descriptors and localized image cues. Specifically, we introduce two model variants: a coarse model for generating initial object instance proposals, and a refinement model that refines all proposals in parallel. We train these models on a newly created, large-scale synthetic dataset comprising thousands of high-quality object meshes. Remarkably, our model achieves state-of-the-art performance on multiple challenging real-world benchmarks, without requiring any retraining on target data. Through comprehensive ablation studies, we demonstrate the potential of diffusion models for instance segmentation tasks. </p>
<blockquote>
<p>本文介绍了OC-DiT，这是一种专为对象中心预测设计的新型扩散模型，并应用于零样本实例分割。我们提出了一个条件潜在扩散框架，该框架通过在扩散模型的潜在空间内对生成过程进行对象模板和图像特征的调节，生成实例掩码。这允许我们的模型通过扩散过程有效地分离对象实例，该过程由视觉对象描述符和局部图像线索指导。具体来说，我们引入了两种模型变体：一种用于生成初始对象实例提议的粗略模型，以及一种并行优化所有提议的精细模型。我们在新创建的大规模合成数据集上训练这些模型，该数据集包含数千个高质量的对象网格。值得注意的是，我们的模型在多个具有挑战性的真实世界基准测试上达到了最先进的性能，而无需对目标数据进行任何重新训练。通过全面的消融研究，我们展示了扩散模型在实例分割任务中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04122v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong><br>新一代对象中心预测扩散模型OC-DiT介绍。该模型应用于零实例分割，提出条件潜在扩散框架，生成实例掩膜，以对象模板和图像特征为条件在扩散模型的潜在空间中。通过视觉对象描述符和局部图像线索引导扩散过程，有效分离对象实例。包括粗模型用于生成初始对象实例提案和细化模型，用于并行细化所有提案。该模型在新创建的大规模合成数据集上进行训练，包含数千个高质量对象网格。在多个具有挑战性的真实世界基准测试中实现了卓越性能，无需在目标数据上重新训练。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了OC-DiT模型：这是一种新型扩散模型，设计用于对象中心预测，并应用于零实例分割。</li>
<li>提出了条件潜在扩散框架：该框架通过以对象模板和图像特征为条件来生成实例掩膜。</li>
<li>通过视觉对象描述符和局部图像线索引导扩散过程，实现了对象实例的有效分离。</li>
<li>OC-DiT包含两个模型变体：一个粗模型用于生成初始对象实例提案，一个细化模型用于并行细化所有提案。</li>
<li>模型在新创建的大规模合成数据集上进行训练，该数据集包含数千个高质量对象网格。</li>
<li>OC-DiT在多个挑战性的真实世界基准测试中实现了卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c462ffca81b2cb20970a7710bac35572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-085b0f4e751a53eb8c84d1b212c699ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e00b0fa043894966a47869a4473db1e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fc62820befebe46042d3de9c5d3466d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b4607949a551949a5999930d987846e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UNISELF-A-Unified-Network-with-Instance-Normalization-and-Self-Ensembled-Lesion-Fusion-for-Multiple-Sclerosis-Lesion-Segmentation"><a href="#UNISELF-A-Unified-Network-with-Instance-Normalization-and-Self-Ensembled-Lesion-Fusion-for-Multiple-Sclerosis-Lesion-Segmentation" class="headerlink" title="UNISELF: A Unified Network with Instance Normalization and   Self-Ensembled Lesion Fusion for Multiple Sclerosis Lesion Segmentation"></a>UNISELF: A Unified Network with Instance Normalization and   Self-Ensembled Lesion Fusion for Multiple Sclerosis Lesion Segmentation</h2><p><strong>Authors:Jinwei Zhang, Lianrui Zuo, Blake E. Dewey, Samuel W. Remedios, Yihao Liu, Savannah P. Hays, Dzung L. Pham, Ellen M. Mowry, Scott D. Newsome, Peter A. Calabresi, Aaron Carass, Jerry L. Prince</strong></p>
<p>Automated segmentation of multiple sclerosis (MS) lesions using multicontrast magnetic resonance (MR) images improves efficiency and reproducibility compared to manual delineation, with deep learning (DL) methods achieving state-of-the-art performance. However, these DL-based methods have yet to simultaneously optimize in-domain accuracy and out-of-domain generalization when trained on a single source with limited data, or their performance has been unsatisfactory. To fill this gap, we propose a method called UNISELF, which achieves high accuracy within a single training domain while demonstrating strong generalizability across multiple out-of-domain test datasets. UNISELF employs a novel test-time self-ensembled lesion fusion to improve segmentation accuracy, and leverages test-time instance normalization (TTIN) of latent features to address domain shifts and missing input contrasts. Trained on the ISBI 2015 longitudinal MS segmentation challenge training dataset, UNISELF ranks among the best-performing methods on the challenge test dataset. Additionally, UNISELF outperforms all benchmark methods trained on the same ISBI training data across diverse out-of-domain test datasets with domain shifts and missing contrasts, including the public MICCAI 2016 and UMCL datasets, as well as a private multisite dataset. These test datasets exhibit domain shifts and&#x2F;or missing contrasts caused by variations in acquisition protocols, scanner types, and imaging artifacts arising from imperfect acquisition. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/uponacceptance">https://github.com/uponacceptance</a>. </p>
<blockquote>
<p>采用多对比度磁共振（MR）图像自动分割多发性硬化症（MS）病变，相较于手动描记，提高了效率和可重复性，而深度学习（DL）方法已经达到了最新技术水平。然而，这些基于DL的方法在单一数据源、有限数据上训练时，尚未同时优化领域内精度和领域外泛化能力，或其表现并不令人满意。为了填补这一空白，我们提出了一种名为UNISELF的方法，该方法在单个训练领域内实现了高精度，同时在多个领域外的测试数据集上表现出了强大的泛化能力。UNISELF采用了一种新型测试时自集成病变融合方法，以提高分割精度，并利用测试时实例标准化（TTIN）潜在特征来解决领域变化和缺失输入对比度问题。在ISBI 2015纵向MS分割挑战训练数据集上训练的UNISELF，在挑战测试数据集上排名最佳方法之一。此外，UNISELF在具有领域变化和缺失对比度的各种领域外的测试数据集上的表现优于所有使用相同ISBI训练数据的基准方法，包括公共的MICCAI 2016和UMCL数据集以及一个私有跨站点数据集。这些测试数据集由于采集协议、扫描仪类型和成像伪影的不完美而导致领域变化和&#x2F;或缺失对比度。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/uponacceptance%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/uponacceptance找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03982v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了使用多对比度磁共振图像自动化分割多发性硬化症病灶的方法。虽然深度学习在单一训练域内实现了较高的准确性，但在跨域测试中其性能尚待提高。本文提出了一种名为UNISELF的方法，在单一训练域内实现了高准确性，同时在多个跨域测试数据集上表现出强大的泛化能力。UNISELF使用了一种新颖的测试时自集成病灶融合技术，并借助测试时实例归一化（TTIN）解决领域变化和缺失输入对比度问题。在ISBI 2015纵向MS分割挑战训练数据集上训练的UNISELF，在挑战测试数据集上表现优异，并在具有领域变化和缺失对比度的不同跨域测试数据集上优于使用相同ISBI训练数据的基准方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动化分割多发性硬化症病灶可以提高效率和重现性。</li>
<li>深度学习在病灶分割方面表现优异，但在单一训练域内的泛化能力有待提高。</li>
<li>提出了一种名为UNISELF的方法，实现了在单一训练域内的高准确性和跨多个域的强泛化能力。</li>
<li>UNISELF采用测试时自集成病灶融合技术提高分割准确性。</li>
<li>测试时实例归一化（TTIN）用于解决领域变化和缺失输入对比度问题。</li>
<li>UNISELF在ISBI 2015挑战测试数据集上表现优秀。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03982">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dfc1553a61b65bef82354a470bb71b65.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Domain-Adaptive-Semantic-Segmentation-by-Synthetic-Data-Generation-and-Progressive-Adaptation"><a href="#Zero-Shot-Domain-Adaptive-Semantic-Segmentation-by-Synthetic-Data-Generation-and-Progressive-Adaptation" class="headerlink" title="Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data   Generation and Progressive Adaptation"></a>Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data   Generation and Progressive Adaptation</h2><p><strong>Authors:Jun Luo, Zijing Zhao, Yang Liu</strong></p>
<p>Deep learning-based semantic segmentation models achieve impressive results yet remain limited in handling distribution shifts between training and test data. In this paper, we present SDGPA (Synthetic Data Generation and Progressive Adaptation), a novel method that tackles zero-shot domain adaptive semantic segmentation, in which no target images are available, but only a text description of the target domain’s style is provided. To compensate for the lack of target domain training data, we utilize a pretrained off-the-shelf text-to-image diffusion model, which generates training images by transferring source domain images to target style. Directly editing source domain images introduces noise that harms segmentation because the layout of source images cannot be precisely maintained. To address inaccurate layouts in synthetic data, we propose a method that crops the source image, edits small patches individually, and then merges them back together, which helps improve spatial precision. Recognizing the large domain gap, SDGPA constructs an augmented intermediate domain, leveraging easier adaptation subtasks to enable more stable model adaptation to the target domain. Additionally, to mitigate the impact of noise in synthetic data, we design a progressive adaptation strategy, ensuring robust learning throughout the training process. Extensive experiments demonstrate that our method achieves state-of-the-art performance in zero-shot semantic segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ROUJINN/SDGPA">https://github.com/ROUJINN/SDGPA</a> </p>
<blockquote>
<p>基于深度学习的语义分割模型取得了令人印象深刻的结果，但在处理训练和测试数据之间的分布转移时仍存在一定的局限性。本文中，我们提出了SDGPA（合成数据生成与逐步适应）方法，这是一种解决零样本域自适应语义分割的新方法，其中不提供目标图像，但只提供了目标域风格的文本描述。为了弥补目标域训练数据的缺乏，我们利用预训练的通用文本到图像扩散模型，通过转移源域图像到目标风格来生成训练图像。直接编辑源域图像会引入噪声，损害分割效果，因为源图像的布局无法精确保持。为了解决合成数据中的布局不准确问题，我们提出了一种方法，对源图像进行裁剪，单独编辑小块区域，然后将它们合并在一起，这有助于提高空间精度。认识到域之间的差距较大，SDGPA构建了一个增强的中间域，利用更容易的适应子任务来使模型更稳定地适应目标域。此外，为了减轻合成数据中的噪声影响，我们设计了一种逐步适应策略，确保在整个训练过程中的稳健学习。大量实验表明，我们的方法在零样本语义分割中达到了最先进的性能。代码可在[<a target="_blank" rel="noopener" href="https://github.com/ROUJINN/SDGPA%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/ROUJINN/SDGPA找到。]</a>(<a target="_blank" rel="noopener" href="https://github.com/ROUJINN/SDGPA%E6%89%BE%E5">https://github.com/ROUJINN/SDGPA%E6%89%BE%E5</a> 知乎专栏。)</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03300v1">PDF</a> Accepted to IROS 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为SDGPA（合成数据生成与渐进适应）的新方法，用于解决无目标图像仅提供目标域文本描述的零样本域自适应语义分割问题。通过利用预训练的文本到图像的扩散模型，生成目标域的训练图像，并引入图像编辑技术以提高合成数据的空间精度。SDGPA通过构建辅助中间域并利用渐进适应策略，实现了模型在目标域的稳健学习，从而达到零样本语义分割的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SDGPA是一种解决零样本域自适应语义分割问题的方法。</li>
<li>利用文本到图像的扩散模型生成目标域的训练图像。</li>
<li>通过图像编辑技术提高合成数据的空间精度。</li>
<li>构建辅助中间域以缩小目标域与源域之间的差距。</li>
<li>采用渐进适应策略，确保模型在训练过程中的稳健学习。</li>
<li>方法的性能在实验中达到了零样本语义分割的最佳状态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03300">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ef94222775646ce252dd5c461370433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a19aa5c29314dc271d5f3b3cb2d6142e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5018f196db889b1a11d55b91ac58e43f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0665bc2b177017d95aa2701c2d6c943a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d07a02bfaaec053d04f02bdd83f99768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32e09cadd01e0f08f554c7f643f67a82.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Adversarial-Attention-Perturbations-for-Large-Object-Detection-Transformers"><a href="#Adversarial-Attention-Perturbations-for-Large-Object-Detection-Transformers" class="headerlink" title="Adversarial Attention Perturbations for Large Object Detection   Transformers"></a>Adversarial Attention Perturbations for Large Object Detection   Transformers</h2><p><strong>Authors:Zachary Yahn, Selim Furkan Tekin, Fatih Ilhan, Sihao Hu, Tiansheng Huang, Yichang Xu, Margaret Loper, Ling Liu</strong></p>
<p>Adversarial perturbations are useful tools for exposing vulnerabilities in neural networks. Existing adversarial perturbation methods for object detection are either limited to attacking CNN-based detectors or weak against transformer-based detectors. This paper presents an Attention-Focused Offensive Gradient (AFOG) attack against object detection transformers. By design, AFOG is neural-architecture agnostic and effective for attacking both large transformer-based object detectors and conventional CNN-based detectors with a unified adversarial attention framework. This paper makes three original contributions. First, AFOG utilizes a learnable attention mechanism that focuses perturbations on vulnerable image regions in multi-box detection tasks, increasing performance over non-attention baselines by up to 30.6%. Second, AFOG’s attack loss is formulated by integrating two types of feature loss through learnable attention updates with iterative injection of adversarial perturbations. Finally, AFOG is an efficient and stealthy adversarial perturbation method. It probes the weak spots of detection transformers by adding strategically generated and visually imperceptible perturbations which can cause well-trained object detection models to fail. Extensive experiments conducted with twelve large detection transformers on COCO demonstrate the efficacy of AFOG. Our empirical results also show that AFOG outperforms existing attacks on transformer-based and CNN-based object detectors by up to 83% with superior speed and imperceptibility. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zacharyyahn/AFOG">https://github.com/zacharyyahn/AFOG</a>. </p>
<blockquote>
<p>对抗性扰动是揭示神经网络脆弱性的有用工具。现有的针对物体检测的对抗性扰动方法要么仅限于攻击基于CNN的检测器，要么对基于变压器的检测器的攻击效果较弱。本文提出了一种针对物体检测变压器的Attention-Focused Offensive Gradient (AFOG)攻击方法。AFOG设计用于神经架构无关性，能有效攻击大型基于变压器的物体检测器和传统的基于CNN的检测器，采用统一的对抗性关注框架。本文有三个原创贡献。首先，AFOG利用可学习的关注机制，将扰动集中在多框检测任务的脆弱图像区域，与非关注基线相比，性能提升高达30.6%。其次，AFOG的攻击损失是通过集成两种特征损失，通过可学习的关注更新和对抗性扰动的迭代注入来形成的。最后，AFOG是一种高效且隐蔽的对抗性扰动方法。它通过添加战略生成且视觉上几乎无法察觉的扰动来探测检测变压器的弱点，这些扰动可以导致训练良好的物体检测模型失效。在COCO数据集上对十二个大型检测变压器进行的广泛实验证明了AFOG的有效性。我们的实验结果还表明，AFOG在基于变压器和基于CNN的物体检测器上的攻击效果比现有攻击高出83%，同时速度和隐蔽性更优越。代码可在<a target="_blank" rel="noopener" href="https://github.com/zacharyyahn/AFOG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zacharyyahn/AFOG找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02987v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了一种针对目标检测变压器的注意力聚焦进攻梯度（AFOG）攻击方法。AFOG利用可学习的注意力机制，在多框检测任务中将扰动集中在脆弱图像区域，提高了对非注意力基准的性能。通过整合两种特征损失和通过可学习注意力更新进行迭代注入对抗性扰动，形成了AFOG的攻击损失。AFOG是一种高效且隐蔽的对抗性扰动方法，通过添加战略生成和视觉上不明显的扰动来探测检测变压器的弱点，导致训练良好的目标检测模型失败。在COCO上进行的实验表明，AFOG对基于变压器和CNN的目标检测器均有效，且性能优于现有攻击方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>AFOG是一种针对目标检测变压器的对抗性攻击方法，对神经网络架构具有通用性。</li>
<li>AFOG利用可学习的注意力机制，将扰动集中在图像的多框检测脆弱区域。</li>
<li>通过整合特征损失和迭代注入对抗性扰动，形成了AFOG的攻击损失。</li>
<li>AFOG在目标检测模型中的性能提升显著，最高可提高30.6%。</li>
<li>AFOG攻击能有效导致训练良好的目标检测模型失败。</li>
<li>AFOG在COCO数据集上的实验表明其性能优于现有攻击方法，对基于变压器和CNN的目标检测器均有效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02987">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0c83363b5b34c3ed484fb6a2841ac6b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5087f7e3953341077ce8cc7d8ed92582.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daa2eeedd688391e56e5a4ea031b501c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-508bf18c2b8091d2a2fbce2a38bc4958.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edb2c23e712a9a441cda9e358ce2f1c3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ESA-Annotation-Efficient-Active-Learning-for-Semantic-Segmentation"><a href="#ESA-Annotation-Efficient-Active-Learning-for-Semantic-Segmentation" class="headerlink" title="ESA: Annotation-Efficient Active Learning for Semantic Segmentation"></a>ESA: Annotation-Efficient Active Learning for Semantic Segmentation</h2><p><strong>Authors:Jinchao Ge, Zeyu Zhang, Minh Hieu Phan, Bowen Zhang, Akide Liu, Yang Zhao, Shuwen Zhao</strong></p>
<p>Active learning enhances annotation efficiency by selecting the most revealing samples for labeling, thereby reducing reliance on extensive human input. Previous methods in semantic segmentation have centered on individual pixels or small areas, neglecting the rich patterns in natural images and the power of advanced pre-trained models. To address these challenges, we propose three key contributions: Firstly, we introduce Entity-Superpixel Annotation (ESA), an innovative and efficient active learning strategy which utilizes a class-agnostic mask proposal network coupled with super-pixel grouping to capture local structural cues. Additionally, our method selects a subset of entities within each image of the target domain, prioritizing superpixels with high entropy to ensure comprehensive representation. Simultaneously, it focuses on a limited number of key entities, thereby optimizing for efficiency. By utilizing an annotator-friendly design that capitalizes on the inherent structure of images, our approach significantly outperforms existing pixel-based methods, achieving superior results with minimal queries, specifically reducing click cost by 98% and enhancing performance by 1.71%. For instance, our technique requires a mere 40 clicks for annotation, a stark contrast to the 5000 clicks demanded by conventional methods. </p>
<blockquote>
<p>主动学习通过选择最具代表性的样本进行标注，提高了标注效率，从而减少了大量人工输入的依赖。在语义分割方面的先前方法主要集中在单个像素或小区域上，忽略了自然图像中的丰富模式以及先进预训练模型的能力。针对这些挑战，我们提出了三项关键贡献：首先，我们引入了实体超像素标注（ESA），这是一种创新且高效的活动学习策略，它利用类无关掩模提案网络结合超像素分组来捕捉局部结构线索。此外，我们的方法选择目标域中每张图像的子集实体，优先处理高熵超像素以确保全面表示。同时，它专注于有限数量的关键实体，从而优化效率。通过利用图像内在结构的注释器友好设计，我们的方法显著优于现有的基于像素的方法，在少量查询的情况下取得了优越的结果，特别是减少了点击成本98%，提高了性能1.71%。例如，我们的技术只需要40次点击进行注释，与传统方法要求的5000次点击形成鲜明对比。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13491v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于实体超像素标注（ESA）的主动学习方法，该方法利用类别无关掩膜提案网络结合超像素分组，捕捉局部结构线索，并选择目标域图像中的实体子集进行优化标注效率。该方法通过减少点击成本和提高性能，显著优于传统的像素级方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>主动学习方法通过选择最具代表性的样本进行标注，提高了语义分割的标注效率，并降低了对大量人工输入的依赖。</li>
<li>传统方法主要关注单个像素或小区域，忽略了自然图像中的丰富模式和预训练模型的力量。</li>
<li>提出了Entity-Superpixel Annotation (ESA)策略，这是一种结合类别无关掩膜提案网络和超像素分组的创新方法。</li>
<li>ESA通过选择目标域图像中的实体子集进行优化标注效率，优先考虑具有高熵的超像素来确保全面的代表性。</li>
<li>ESA采用一种针对注释者的友好设计，利用图像的内在结构来提高标注效率。</li>
<li>ESA方法显著优于现有的像素级方法，实现了卓越的结果与最少的查询次数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13491">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49d50540fc710d2ce7efcc11138d0240.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-972f957e93372d4db7d82c8163613860.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b67b6ffc5ec3d73cd8383c5f40ffc592.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7b7c1f511af472060dfdb9561c3eb649.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-08-08  BEVCon Advancing Bird's Eye View Perception with Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a3a06165e09e33733fc4d9c82c339559.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-08  Visual Bias and Interpretability in Deep Learning for Dermatological   Image Analysis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
