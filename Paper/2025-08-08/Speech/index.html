<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  A Multi-stage Low-latency Enhancement System for Hearing Aids">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-338bf2387308d86c1854a7f157755e15.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-08-æ›´æ–°"><a href="#2025-08-08-æ›´æ–°" class="headerlink" title="2025-08-08 æ›´æ–°"></a>2025-08-08 æ›´æ–°</h1><h2 id="A-Multi-stage-Low-latency-Enhancement-System-for-Hearing-Aids"><a href="#A-Multi-stage-Low-latency-Enhancement-System-for-Hearing-Aids" class="headerlink" title="A Multi-stage Low-latency Enhancement System for Hearing Aids"></a>A Multi-stage Low-latency Enhancement System for Hearing Aids</h2><p><strong>Authors:Chengwei Ouyang, Kexin Fei, Haoshuai Zhou, Congxi Lu, Linkai Li</strong></p>
<p>This paper proposes an end-to-end system for the ICASSP 2023 Clarity Challenge. In this work, we introduce four major novelties: (1) a novel multi-stage system in both the magnitude and complex domains to better utilize phase information; (2) an asymmetric window pair to achieve higher frequency resolution with the 5ms latency constraint; (3) the integration of head rotation information and the mixture signals to achieve better enhancement; (4) a post-processing module that achieves higher hearing aid speech perception index (HASPI) scores with the hearing aid amplification stage provided by the baseline system. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨ä¸ºICASSP 2023æ¸…æ™°åº¦æŒ‘æˆ˜èµ›æå‡ºä¸€ç§ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å››ä¸ªä¸»è¦æ–°é¢–ä¹‹å¤„ï¼šï¼ˆ1ï¼‰åœ¨å¹…åº¦å’Œå¤æ‚åŸŸä¸­å¼•å…¥æ–°å‹çš„å¤šé˜¶æ®µç³»ç»Ÿï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨ç›¸ä½ä¿¡æ¯ï¼›ï¼ˆ2ï¼‰é‡‡ç”¨ä¸å¯¹ç§°çª—å£å¯¹ï¼Œåœ¨5æ¯«ç§’å»¶è¿Ÿçº¦æŸä¸‹å®ç°æ›´é«˜çš„é¢‘ç‡åˆ†è¾¨ç‡ï¼›ï¼ˆ3ï¼‰å°†å¤´éƒ¨æ—‹è½¬ä¿¡æ¯å’Œæ··åˆä¿¡å·ç›¸ç»“åˆï¼Œä»¥å®ç°æ›´å¥½çš„å¢å¼ºæ•ˆæœï¼›ï¼ˆ4ï¼‰åå¤„ç†æ¨¡å—çš„å®ç°ï¼Œè¯¥æ¨¡å—å€ŸåŠ©åŸºçº¿ç³»ç»Ÿæä¾›çš„åŠ©å¬å™¨æ”¾å¤§é˜¶æ®µï¼Œå®ç°äº†æ›´é«˜çš„åŠ©å¬è¯­éŸ³æ„ŸçŸ¥æŒ‡æ•°ï¼ˆHASPIï¼‰åˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04283v1">PDF</a> 2 pages, 1 figure, 1 table. accepted to ICASSP 2023</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ä¸ªé¢å‘ICASSP 2023 Clarity Challengeçš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œå¹¶å¼•å…¥å››é¡¹é‡å¤§åˆ›æ–°ï¼šä¸€æ˜¯åœ¨å¹…åº¦å’Œå¤æ•°åŸŸçš„å¤šé˜¶æ®µç³»ç»Ÿä»¥æ›´å¥½åœ°åˆ©ç”¨ç›¸ä½ä¿¡æ¯ï¼›äºŒæ˜¯å¯¹ç§°çª—å£å¯¹å®ç°é«˜é¢‘åˆ†è¾¨ç‡åŒæ—¶æ»¡è¶³5æ¯«ç§’å»¶è¿Ÿé™åˆ¶ï¼›ä¸‰æ˜¯æ•´åˆå¤´éƒ¨æ—‹è½¬ä¿¡æ¯å’Œæ··åˆä¿¡å·å®ç°æ›´ä½³å¢å¼ºæ•ˆæœï¼›å››æ˜¯åå¤„ç†æ¨¡å—æé«˜å¬åŠ›è¾…åŠ©è£…ç½®çš„è¯­éŸ³æ„ŸçŸ¥æŒ‡æ•°ï¼ˆHASPIï¼‰åˆ†æ•°ï¼ŒåŸºäºåŸºçº¿ç³»ç»Ÿæä¾›çš„å¬åŠ›è¾…åŠ©æ”¾å¤§é˜¶æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼•å…¥å¤šé˜¶æ®µç³»ç»Ÿåˆ©ç”¨ç›¸ä½ä¿¡æ¯ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨ä¸å¯¹ç§°çª—å£å¯¹æ»¡è¶³å»¶è¿Ÿçº¦æŸå¹¶æé«˜é¢‘ç‡åˆ†è¾¨ç‡ã€‚</li>
<li>æ•´åˆå¤´éƒ¨æ—‹è½¬ä¿¡æ¯å’Œæ··åˆä¿¡å·ä»¥å®ç°æ›´å¥½çš„å£°éŸ³å¢å¼ºã€‚</li>
<li>åå¤„ç†æ¨¡å—æé«˜å¬åŠ›è¾…åŠ©è£…ç½®çš„è¯­éŸ³æ„ŸçŸ¥æŒ‡æ•°ï¼ˆHASPIï¼‰åˆ†æ•°ã€‚</li>
<li>ç³»ç»Ÿé¢å‘ICASSP 2023 Clarity Challengeè®¾è®¡ã€‚</li>
<li>è¯¥ç³»ç»Ÿæ”¹è¿›äº†åŸºçº¿ç³»ç»Ÿçš„å¬åŠ›è¾…åŠ©æ”¾å¤§é˜¶æ®µã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-683278c725a9b350848009efa077b6dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb5090ef2a8224faec2775955ee2f96e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations"><a href="#NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations" class="headerlink" title="NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech   Modeling with Paralinguistic Vocalizations"></a>NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech   Modeling with Paralinguistic Vocalizations</h2><p><strong>Authors:Huan Liao, Qinke Ni, Yuancheng Wang, Yiheng Lu, Haoyue Zhan, Pengyuan Xie, Qiang Zhang, Zhizheng Wu</strong></p>
<p>Paralinguistic vocalizations-including non-verbal sounds like laughter and breathing, as well as lexicalized interjections such as â€œuhmâ€ and â€œohâ€-are integral to natural spoken communication. Despite their importance in conveying affect, intent, and interactional cues, such cues remain largely overlooked in conventional automatic speech recognition (ASR) and text-to-speech (TTS) systems. We present NVSpeech, an integrated and scalable pipeline that bridges the recognition and synthesis of paralinguistic vocalizations, encompassing dataset construction, ASR modeling, and controllable TTS. (1) We introduce a manually annotated dataset of 48,430 human-spoken utterances with 18 word-level paralinguistic categories. (2) We develop the paralinguistic-aware ASR model, which treats paralinguistic cues as inline decodable tokens (e.g., â€œYouâ€™re so funny [Laughter]â€), enabling joint lexical and non-verbal transcription. This model is then used to automatically annotate a large corpus, the first large-scale Chinese dataset of 174,179 utterances (573 hours) with word-level alignment and paralingustic cues. (3) We finetune zero-shot TTS models on both human- and auto-labeled data to enable explicit control over paralinguistic vocalizations, allowing context-aware insertion at arbitrary token positions for human-like speech synthesis. By unifying the recognition and generation of paralinguistic vocalizations, NVSpeech offers the first open, large-scale, word-level annotated pipeline for expressive speech modeling in Mandarin, integrating recognition and synthesis in a scalable and controllable manner. Dataset and audio demos are available at <a target="_blank" rel="noopener" href="https://nvspeech170k.github.io/">https://nvspeech170k.github.io/</a>. </p>
<blockquote>
<p>å‰¯è¯­è¨€å‘å£°â€”â€”åŒ…æ‹¬éè¯­è¨€æ€§çš„å£°éŸ³ï¼Œå¦‚ç¬‘å£°å’Œå‘¼å¸å£°ï¼Œä»¥åŠè¯æ±‡åŒ–çš„æ„Ÿå¹è¯ï¼Œå¦‚â€œå‘ƒâ€å’Œâ€œå“¦â€â€”â€”æ˜¯è‡ªç„¶å£è¯­äº¤æµçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡å®ƒä»¬åœ¨ä¼ è¾¾æƒ…æ„Ÿã€æ„å›¾å’Œäº¤äº’çº¿ç´¢æ–¹é¢éå¸¸é‡è¦ï¼Œä½†è¿™äº›çº¿ç´¢åœ¨ä¼ ç»Ÿçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­ä»ç„¶è¢«å¿½è§†ã€‚æˆ‘ä»¬æå‡ºäº†NVSpeechï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆä¸”å¯æ‰©å±•çš„ç®¡é“ï¼Œèƒ½å¤Ÿè¯†åˆ«å¹¶åˆæˆå‰¯è¯­è¨€å‘å£°ï¼ŒåŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ASRå»ºæ¨¡å’Œå¯æ§TTSã€‚</p>
</blockquote>
<p>ï¼ˆ1ï¼‰æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªåŒ…å«48430ä¸ªäººç±»å£è¯­å‘å£°ã€åˆ†ä¸º18ä¸ªè¯æ±‡çº§å‰¯è¯­è¨€ç±»åˆ«çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†ã€‚<br>ï¼ˆ2ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå‰¯è¯­è¨€æ„ŸçŸ¥ASRæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†å‰¯è¯­è¨€çº¿ç´¢è§†ä¸ºå†…è”å¯è§£ç çš„ä»¤ç‰Œï¼ˆä¾‹å¦‚ï¼Œâ€œä½ å¾ˆæœ‰è¶£[ç¬‘å£°]â€ï¼‰ï¼Œä»è€Œå®ç°è¯æ±‡å’Œéè¨€è¯­çš„è”åˆè½¬å½•ã€‚ç„¶åï¼Œè¯¥æ¨¡å‹è¢«ç”¨æ¥è‡ªåŠ¨æ³¨é‡Šå¤§è§„æ¨¡è¯­æ–™åº“ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡æ•°æ®é›†ï¼ŒåŒ…å«174179ä¸ªå‘å£°ï¼ˆ573å°æ—¶ï¼‰ï¼Œå…·æœ‰è¯æ±‡çº§å¯¹é½å’Œå‰¯è¯­è¨€çº¿ç´¢ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04195v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªç„¶å£è¯­äº¤æµä¸­çš„é‡è¦æ€§éè¨€è¯­å£°éŸ³å’Œè¯æ±‡åŒ–æ’å…¥è¯­ï¼Œå®ƒä»¬èƒ½å¤Ÿä¼ é€’æƒ…æ„Ÿã€æ„å›¾å’Œäº¤äº’çº¿ç´¢ã€‚é’ˆå¯¹ç›®å‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå¯¹æ­¤ç±»çº¿ç´¢çš„å¿½è§†ï¼Œæœ¬æ–‡æå‡ºäº†NVSpeechï¼Œä¸€ä¸ªèƒ½å¤Ÿè¯†åˆ«å’Œåˆæˆéè¨€è¯­è¯­éŸ³çš„é›†æˆå’Œå¯æ‰©å±•ç®¡é“ï¼ŒåŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ASRå»ºæ¨¡å’Œå¯æ§TTSã€‚NVSpeechä¸ºæ™®é€šè¯çš„è¡¨è¾¾æ€§è¯­éŸ³å»ºæ¨¡æä¾›äº†é¦–ä¸ªå¼€æ”¾çš„å¤§è§„æ¨¡è¯çº§æ³¨é‡Šç®¡é“ï¼Œä»¥å¯æ‰©å±•å’Œå¯æ§çš„æ–¹å¼ç»Ÿä¸€è¯†åˆ«å’Œç”Ÿæˆéè¨€è¯­è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè¨€è¯­å£°éŸ³å’Œè¯æ±‡åŒ–æ’å…¥è¯­å¯¹è‡ªç„¶å£è¯­äº¤æµè‡³å…³é‡è¦ï¼Œèƒ½ä¼ é€’æƒ…æ„Ÿã€æ„å›¾å’Œäº¤äº’çº¿ç´¢ã€‚</li>
<li>NVSpeechæ˜¯ä¸€ä¸ªé›†æˆå’Œå¯æ‰©å±•çš„ç®¡é“ï¼Œèƒ½å¤Ÿè¯†åˆ«å’Œåˆæˆéè¨€è¯­è¯­éŸ³ï¼ŒåŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ASRå»ºæ¨¡å’Œå¯æ§TTSã€‚</li>
<li>å¼•å…¥äº†åŒ…å«18ä¸ªè¯çº§éè¨€è¯­ç±»åˆ«çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†ï¼ŒåŒ…å«48,430ä¸ªäººç±»è¯­éŸ³ç‰‡æ®µã€‚</li>
<li>å¼€å‘äº†å°†éè¨€è¯­çº¿ç´¢è§†ä¸ºå†…è”å¯è§£ç æ ‡è®°çš„ASRæ¨¡å‹ï¼Œå®ç°äº†è¯æ±‡å’Œéè¨€è¯­çš„è”åˆè½¬å½•ã€‚</li>
<li>ä½¿ç”¨æ­¤æ¨¡å‹è‡ªåŠ¨æ³¨é‡Šäº†ä¸€ä¸ªå¤§è§„æ¨¡è¯­æ–™åº“ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡ä¸­æ–‡è¯­æ–™åº“ï¼ŒåŒ…å«174,179ä¸ªè¯­éŸ³ç‰‡æ®µï¼ˆ573å°æ—¶ï¼‰ï¼Œå…·æœ‰è¯çº§å¯¹é½å’Œéè¨€è¯­çº¿ç´¢ã€‚</li>
<li>å¯¹é›¶æ ·æœ¬TTSæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨äººç±»å’Œè‡ªåŠ¨æ ‡è®°çš„æ•°æ®ä¸Šæ˜¾å¼æ§åˆ¶éè¨€è¯­è¯­éŸ³ï¼Œå…è®¸åœ¨ä»»æ„æ ‡è®°ä½ç½®æ’å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¿¡æ¯ä»¥å®ç°äººç±»èˆ¬çš„è¯­éŸ³åˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04195">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b5d36064ac6cd9d0f22d443d02403bd8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-715dae7b676ec31fe5febb2466293386.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6ec6b76a0c3ac06fdba5842721656b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c259daee3f593ec5aece40872f04a80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08c54201151545ffe5c64f25541de239.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1627b0be13dd133e5ec182a259f41a1d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech"><a href="#Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech" class="headerlink" title="Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech"></a>Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech</h2><p><strong>Authors:Jingyuan Xing, Zhipeng Li, Jialong Mai, Xiaofen Xing, Xiangmin Xu</strong></p>
<p>Advances in speech representation and large language models have enhanced zero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS models face challenges in capturing the complex correlations between acoustic and semantic features, resulting in a lack of expressiveness and similarity. The primary reason lies in the complex relationship between semantic and acoustic features, which manifests independent and interdependent aspects.This paper introduces a TTS framework that combines both autoregressive (AR) and non-autoregressive (NAR) modules to harmonize the independence and interdependence of acoustic and semantic information. The AR model leverages the proposed Parallel Tokenizer to synthesize the top semantic and acoustic tokens simultaneously. In contrast, considering the interdependence, the Coupled NAR model predicts detailed tokens based on the general AR modelâ€™s output. Parallel GPT, built on this architecture, is designed to improve zero-shot text-to-speech synthesis through its parallel structure. Experiments on English and Chinese datasets demonstrate that the proposed model significantly outperforms the quality and efficiency of the synthesis of existing zero-shot TTS models. Speech demos are available at <a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/">https://t1235-ch.github.io/pgpt/</a>. </p>
<blockquote>
<p>éšç€è¯­éŸ³è¡¨å¾å’Œå¤§è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›¶æ ·æœ¬TTSæ¨¡å‹åœ¨æ•æ‰å£°å­¦ç‰¹å¾å’Œè¯­ä¹‰ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³è”æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´è¡¨ç°åŠ›ä¸è¶³å’Œç›¸ä¼¼æ€§ä¸è¶³ã€‚ä¸»è¦åŸå› åœ¨äºè¯­ä¹‰å’Œå£°å­¦ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œè¿™ç§å…³ç³»è¡¨ç°å‡ºç‹¬ç«‹å’Œç›¸äº’ä¾å­˜çš„æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04141v1">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing (TASLP)</p>
<p><strong>Summary</strong></p>
<p>éšç€è¯­éŸ³è¡¨å¾å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ€§èƒ½æœ‰æ‰€æå‡ã€‚ä½†ç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹åœ¨æ•æ‰å£°å­¦è¯­ä¹‰ç‰¹å¾å¤æ‚å…³è”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´è¡¨ç°åŠ›ä¸è¶³å’Œç›¸ä¼¼æ€§ç¼ºå¤±ã€‚æœ¬æ–‡å¼•å…¥ä¸€ç§ç»“åˆè‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å—çš„TTSæ¡†æ¶ï¼Œä»¥åè°ƒå£°å­¦è¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ã€‚ARæ¨¡å‹åˆ©ç”¨å¹¶è¡Œåˆ†è¯å™¨åˆæˆé¡¶çº§è¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ï¼Œè€Œè€¦åˆçš„NARæ¨¡å‹åŸºäºARæ¨¡å‹çš„è¾“å‡ºé¢„æµ‹è¯¦ç»†çš„æ ‡è®°ã€‚åŸºäºè¿™ç§æ¶æ„æ„å»ºçš„å¹¶è¡ŒGPTæ—¨åœ¨é€šè¿‡å…¶å¹¶è¡Œç»“æ„æ”¹è¿›é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åˆæˆè´¨é‡å’Œæ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³è¡¨å¾å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æå‡äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰TTSæ¨¡å‹åœ¨æ•æ‰å£°å­¦è¯­ä¹‰ç‰¹å¾å¤æ‚å…³è”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹è¡¨è¾¾åŠ›å’Œç›¸ä¼¼æ€§ã€‚</li>
<li>æœ¬æ–‡ç»“åˆARå’ŒNARæ¨¡å—åˆ›å»ºäº†ä¸€ä¸ªTTSæ¡†æ¶ï¼Œä»¥åè°ƒå£°å­¦è¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹å’Œç›¸äº’ä¾èµ–å…³ç³»ã€‚</li>
<li>ARæ¨¡å‹åˆ©ç”¨å¹¶è¡Œåˆ†è¯å™¨åŒæ—¶åˆæˆè¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ã€‚</li>
<li>NARæ¨¡å‹åŸºäºARæ¨¡å‹çš„è¾“å‡ºé¢„æµ‹è¯¦ç»†çš„æ ‡è®°ï¼Œä»¥è€ƒè™‘ç‰¹å¾é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚</li>
<li>å¹¶è¡ŒGPTçš„æ¶æ„æ”¹è¿›äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆè´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-12f0de09a0900c1d7a9800be441359cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa37142f69f7e05c7ee1f1ccc92eb754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92c8ad1224a54c6c9f7d2d2a40d2c8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d0677a13b72211796ef8e9ac8a1254d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Efficient-Scaling-for-LLM-based-ASR"><a href="#Efficient-Scaling-for-LLM-based-ASR" class="headerlink" title="Efficient Scaling for LLM-based ASR"></a>Efficient Scaling for LLM-based ASR</h2><p><strong>Authors:Bingshen Mu, Yiwen Shao, Kun Wei, Dong Yu, Lei Xie</strong></p>
<p>Large language model (LLM)-based automatic speech recognition (ASR) achieves strong performance but often incurs high computational costs. This work investigates how to obtain the best LLM-ASR performance efficiently. Through comprehensive and controlled experiments, we find that pretraining the speech encoder before integrating it with the LLM leads to significantly better scaling efficiency than the standard practice of joint post-training of LLM-ASR. Based on this insight, we propose a new multi-stage LLM-ASR training strategy, EFIN: Encoder First Integration. Among all training strategies evaluated, EFIN consistently delivers better performance (relative to 21.1% CERR) with significantly lower computation budgets (49.9% FLOPs). Furthermore, we derive a scaling law that approximates ASR error rates as a computation function, providing practical guidance for LLM-ASR scaling. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å¾€å¾€è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨ä»¥é«˜æ•ˆçš„æ–¹å¼è·å¾—æœ€ä½³çš„LLM-ASRæ€§èƒ½ã€‚é€šè¿‡å…¨é¢ä¸”å—æ§çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨å°†å…¶ä¸LLMé›†æˆä¹‹å‰å¯¹è¯­éŸ³ç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œä¸LLM-ASRçš„è”åˆåè®­ç»ƒçš„æ ‡å‡†å®è·µç›¸æ¯”ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ‰©å±•æ•ˆç‡ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šé˜¶æ®µLLM-ASRè®­ç»ƒç­–ç•¥EFINï¼ˆEncoder First Integrationï¼‰ã€‚åœ¨æ‰€æœ‰è¯„ä¼°çš„è®­ç»ƒç­–ç•¥ä¸­ï¼ŒEFINå§‹ç»ˆåœ¨æ€§èƒ½ä¸Šè¡¨ç°æ›´å¥½ï¼ˆç›¸å¯¹äº21.1%çš„CERRï¼‰ï¼ŒåŒæ—¶è®¡ç®—é¢„ç®—æ˜¾è‘—é™ä½ï¼ˆ49.9%çš„FLOPsï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¾—å‡ºäº†è¿‘ä¼¼ASRé”™è¯¯ç‡ä½œä¸ºè®¡ç®—å‡½æ•°çš„æ‰©å±•å®šå¾‹ï¼Œä¸ºLLM-ASRçš„æ‰©å±•æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04096v1">PDF</a> Accepted by ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½å‡ºè‰²ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ä»¥é«˜æ•ˆçš„æ–¹å¼è·å¾—æœ€ä½³çš„LLM-ASRæ€§èƒ½ã€‚é€šè¿‡å…¨é¢ä¸”å—æ§çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°é¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨å†å°†å…¶ä¸LLMé›†æˆï¼Œç›¸è¾ƒäºæ ‡å‡†è”åˆåè®­ç»ƒæ–¹å¼ï¼Œèƒ½æ˜¾è‘—æé«˜æ‰©å±•æ•ˆç‡ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šé˜¶æ®µLLM-ASRè®­ç»ƒç­–ç•¥â€”â€”EFINï¼ˆç¼–ç å™¨ä¼˜å…ˆé›†æˆï¼‰ã€‚åœ¨è¯„ä¼°çš„æ‰€æœ‰è®­ç»ƒç­–ç•¥ä¸­ï¼ŒEFINåœ¨æ€§èƒ½ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼Œç›¸å¯¹è¯¯å·®ç‡é™ä½21.1%ï¼Œè®¡ç®—é¢„ç®—æ˜¾è‘—é™ä½ï¼ˆFLOPså‡å°‘49.9%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å¯¼å‡ºä¸€ä¸ªè¿‘ä¼¼ASRé”™è¯¯ç‡ä¸è®¡ç®—é‡çš„å…³ç³»å…¬å¼ï¼Œä¸ºLLM-ASRçš„æ‰©å±•æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨å†ä¸LLMé›†æˆï¼Œèƒ½æé«˜LLM-ASRçš„æ‰©å±•æ•ˆç‡ã€‚</li>
<li>EFINè®­ç»ƒç­–ç•¥åœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸å¯¹è¯¯å·®ç‡é™ä½21.1%ï¼Œè®¡ç®—é¢„ç®—å‡å°‘49.9%ã€‚</li>
<li>EFINç­–ç•¥æ˜¯ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ç¼–ç å™¨ä¼˜å…ˆé›†æˆã€‚</li>
<li>å…¨é¢çš„å®éªŒéªŒè¯äº†EFINç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¨å¯¼å‡ºä¸€ä¸ªè¿‘ä¼¼ASRé”™è¯¯ç‡ä¸è®¡ç®—é‡çš„å…³ç³»å…¬å¼ï¼Œä¸ºLLM-ASRçš„æ‰©å±•æä¾›æŒ‡å¯¼ã€‚</li>
<li>LLM-ASRåœ¨è®¡ç®—æˆæœ¬æ–¹é¢ä»æœ‰å¾…ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cbceec27e32c2ca398719677ee57b0a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bb4f1179f230a1708d49d7f139906fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfcf5c5a98794138da3488eac827025c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1341714686d3065b11842e9a3e60b180.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ae230c0bb08bca628b523c238a334b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e8122e550a5cfa013b0e18b4ff14222.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80a3e9b4fa0a68d9a764c311f3698377.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1766ff7eebb4a1441d71a250fafc9940.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions"><a href="#MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions" class="headerlink" title="MiDashengLM: Efficient Audio Understanding with General Audio Captions"></a>MiDashengLM: Efficient Audio Understanding with General Audio Captions</h2><p><strong>Authors:Heinrich Dinkel, Gang Li, Jizhong Liu, Jian Luan, Yadong Niu, Xingwei Sun, Tianzi Wang, Qiyang Xiao, Junbo Zhang, Jiahao Zhou</strong></p>
<p>Current approaches for large audio language models (LALMs) often rely on closed data sources or proprietary models, limiting their generalization and accessibility. This paper introduces MiDashengLM, a novel open audio-language model designed for efficient and comprehensive audio understanding through the use of general audio captions using our novel ACAVCaps training dataset. MiDashengLM exclusively relies on publicly available pretraining and supervised fine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At its core, MiDashengLM integrates Dasheng, an open-source audio encoder, specifically engineered to process diverse auditory information effectively. Unlike previous works primarily focused on Automatic Speech Recognition (ASR) based audio-text alignment, our strategy centers on general audio captions, fusing speech, sound and music information into one textual representation, enabling a holistic textual representation of complex audio scenes. Lastly, MiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT) and up to 20x higher throughput than comparable models. Checkpoints are available online at <a target="_blank" rel="noopener" href="https://huggingface.co/mispeech/midashenglm-7b">https://huggingface.co/mispeech/midashenglm-7b</a> and <a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/dasheng-lm">https://github.com/xiaomi-research/dasheng-lm</a>. </p>
<blockquote>
<p>å½“å‰çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰æ–¹æ³•å¸¸å¸¸ä¾èµ–äºå°é—­æ•°æ®æºæˆ–ä¸“æœ‰æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†MiDashengLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼€æ”¾éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨æˆ‘ä»¬æ–°å‹ACAVCapsè®­ç»ƒæ•°æ®é›†çš„ä¸€èˆ¬éŸ³é¢‘å­—å¹•ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆä¸”å…¨é¢çš„éŸ³é¢‘ç†è§£ã€‚MiDashengLMå®Œå…¨ä¾èµ–äºå…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œç¡®ä¿å®Œå…¨é€æ˜å’Œå¯é‡å¤æ€§ã€‚å…¶æ ¸å¿ƒèåˆäº†Dashengè¿™ä¸€å¼€æºéŸ³é¢‘ç¼–ç å™¨ï¼Œä¸“é—¨ç”¨äºæœ‰æ•ˆå¤„ç†å„ç§å¬è§‰ä¿¡æ¯ã€‚ä¸ä¹‹å‰ä¸»è¦å…³æ³¨åŸºäºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„éŸ³é¢‘æ–‡æœ¬å¯¹é½çš„ç­–ç•¥ä¸åŒï¼Œæˆ‘ä»¬çš„ç­–ç•¥ä¾§é‡äºä¸€èˆ¬éŸ³é¢‘å­—å¹•ï¼Œå°†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯èåˆåˆ°ä¸€ç§æ–‡æœ¬è¡¨ç¤ºä¸­ï¼Œå®ç°å¯¹å¤æ‚éŸ³é¢‘åœºæ™¯çš„æ•´ä½“æ–‡æœ¬è¡¨ç¤ºã€‚æœ€åï¼ŒMiDashengLMåœ¨é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰æ–¹é¢æä¾›äº†é«˜è¾¾4å€çš„åŠ é€Ÿï¼Œå¹¶ä¸”ä¸åŒç±»æ¨¡å‹ç›¸æ¯”ï¼Œååé‡é«˜è¾¾20å€ã€‚æ£€æŸ¥ç‚¹æ•°æ®å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/mispeech/midashenglm-7b%E5%92%8Chttps://github.com/xiaomi-research/dasheng-lm%E5%9C%A8%E7%BA%BF%E8%8E%B7%E5%BE%97%E3%80%82">https://huggingface.co/mispeech/midashenglm-7bå’Œhttps://github.com/xiaomi-research/dasheng-lmåœ¨çº¿è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MiDashengLMæ˜¯ä¸€ä¸ªåŸºäºå…¬å¼€é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®é›†çš„æ–°å‹å¼€æ”¾éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡é€šç”¨éŸ³é¢‘å­—å¹•ACAVCapsè®­ç»ƒæ•°æ®é›†å®ç°é«˜æ•ˆå…¨é¢çš„éŸ³é¢‘ç†è§£ã€‚å®ƒèåˆäº†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯ï¼Œæä¾›å…¨é¢çš„éŸ³é¢‘åœºæ™¯æ–‡æœ¬è¡¨ç¤ºï¼Œä¸ä¸“æ³¨äºè¯­éŸ³è¯†åˆ«éŸ³é¢‘æ–‡æœ¬å¯¹é½çš„å…ˆå‰æ–¹æ³•ä¸åŒã€‚æ­¤å¤–ï¼ŒMiDashengLMçš„æ¨¡å‹è¿è¡Œé€Ÿåº¦æ¯”åŒç±»æ¨¡å‹å¿«ï¼Œæä¾›æ›´å¿«çš„å“åº”æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MiDashengLMæ˜¯ä¸€ä¸ªç”¨äºéŸ³é¢‘ç†è§£çš„å¼€æ”¾éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºå…¬å¼€æ•°æ®é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®é›†ã€‚</li>
<li>å®ƒé€šè¿‡ACAVCapsè®­ç»ƒæ•°æ®é›†å®ç°é«˜æ•ˆå…¨é¢çš„éŸ³é¢‘ç†è§£ã€‚</li>
<li>MiDashengLMèåˆäº†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯ï¼Œæä¾›å…¨é¢çš„éŸ³é¢‘åœºæ™¯æ–‡æœ¬è¡¨ç¤ºã€‚</li>
<li>ä¸ä¸“æ³¨äºè¯­éŸ³è¯†åˆ«éŸ³é¢‘æ–‡æœ¬å¯¹é½çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒMiDashengLMçš„ç­–ç•¥ä¾§é‡äºé€šç”¨éŸ³é¢‘å­—å¹•ã€‚</li>
<li>MiDashengLMæ¨¡å‹è¿è¡Œé€Ÿåº¦æ›´å¿«ï¼Œæä¾›æ›´å¿«çš„å“åº”æ—¶é—´ï¼Œä¸åŒç±»æ¨¡å‹ç›¸æ¯”ï¼Œæ—¶é—´è‡³ç¬¬ä¸€ä»¤ç‰Œï¼ˆTTFTï¼‰é€Ÿåº¦æé«˜äº†æœ€å¤š4å€ï¼Œååé‡æé«˜äº†é«˜è¾¾20å€ã€‚</li>
<li>MiDashengLMä½¿ç”¨Dashengè¿™ä¸€å¼€æºéŸ³é¢‘ç¼–ç å™¨å¤„ç†å„ç§å¬è§‰ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c8a8481836b0bb9f7c0fb5dcaded80f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c0e11fb8976c04eb43489994f04d7c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ef6a5f5d7ceb7f73e56b094400723bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0dc06c5f2be19ac784c4dd6086ec922.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5c1921345c64b85390c1096caba949c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4db33b1d59e2ab45c1c29d909f9b7e01.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LCS-CTC-Leveraging-Soft-Alignments-to-Enhance-Phonetic-Transcription-Robustness"><a href="#LCS-CTC-Leveraging-Soft-Alignments-to-Enhance-Phonetic-Transcription-Robustness" class="headerlink" title="LCS-CTC: Leveraging Soft Alignments to Enhance Phonetic Transcription   Robustness"></a>LCS-CTC: Leveraging Soft Alignments to Enhance Phonetic Transcription   Robustness</h2><p><strong>Authors:Zongli Ye, Jiachen Lian, Akshaj Gupta, Xuanru Zhou, Krish Patel, Haodong Li, Hwi Joo Park, Chenxu Guo, Shuhe Li, Sam Wang, Cheol Jun Cho, Zoe Ezzes, Jet M. J. Vonk, Brittany T. Morin, Rian Bogley, Lisa Wauters, Zachary A. Miller, Maria Luisa Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Phonetic speech transcription is crucial for fine-grained linguistic analysis and downstream speech applications. While Connectionist Temporal Classification (CTC) is a widely used approach for such tasks due to its efficiency, it often falls short in recognition performance, especially under unclear and nonfluent speech. In this work, we propose LCS-CTC, a two-stage framework for phoneme-level speech recognition that combines a similarity-aware local alignment algorithm with a constrained CTC training objective. By predicting fine-grained frame-phoneme cost matrices and applying a modified Longest Common Subsequence (LCS) algorithm, our method identifies high-confidence alignment zones which are used to constrain the CTC decoding path space, thereby reducing overfitting and improving generalization ability, which enables both robust recognition and text-free forced alignment. Experiments on both LibriSpeech and PPA demonstrate that LCS-CTC consistently outperforms vanilla CTC baselines, suggesting its potential to unify phoneme modeling across fluent and non-fluent speech. </p>
<blockquote>
<p>è¯­éŸ³å‘éŸ³è½¬å½•å¯¹äºç²¾ç»†çš„è¯­è¨€åˆ†æå’Œä¸‹æ¸¸è¯­éŸ³åº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰ç”±äºå…¶æ•ˆç‡è€Œå¹¿æ³›åº”ç”¨äºæ­¤ç±»ä»»åŠ¡ï¼Œä½†åœ¨è¯†åˆ«æ€§èƒ½ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸æ¸…æ™°å’Œéæµåˆ©è¯­éŸ³çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LCS-CTCï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„éŸ³ç´ çº§è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œå®ƒå°†ç›¸ä¼¼æ€§æ„ŸçŸ¥å±€éƒ¨å¯¹é½ç®—æ³•ä¸å—çº¦æŸçš„CTCè®­ç»ƒç›®æ ‡ç›¸ç»“åˆã€‚é€šè¿‡é¢„æµ‹ç²¾ç»†çš„å¸§-éŸ³ç´ æˆæœ¬çŸ©é˜µå¹¶åº”ç”¨æ”¹è¿›çš„æœ€é•¿å…¬å…±å­åºåˆ—ï¼ˆLCSï¼‰ç®—æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç¡®å®šé«˜ç½®ä¿¡åº¦çš„å¯¹é½åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸç”¨äºçº¦æŸCTCè§£ç è·¯å¾„ç©ºé—´ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆç°è±¡å¹¶å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå®ç°ç¨³å¥çš„è¯†åˆ«å’Œæ–‡æœ¬æ— å…³çš„å¼ºåˆ¶å¯¹é½ã€‚åœ¨LibriSpeechå’ŒPPAä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLCS-CTCæŒç»­ä¼˜äºä¼ ç»Ÿçš„CTCåŸºå‡†æµ‹è¯•ï¼Œè¿™è¡¨æ˜å…¶åœ¨æµåˆ©å’Œéæµåˆ©è¯­éŸ³çš„éŸ³ç´ å»ºæ¨¡ä¸Šå…·æœ‰ç»Ÿä¸€æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03937v1">PDF</a> 2025 ASRU</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬æå‡ºäº†LCS-CTCï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„éŸ³ç´ çº§è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œç»“åˆäº†ç›¸ä¼¼æ€§æ„ŸçŸ¥å±€éƒ¨å¯¹é½ç®—æ³•å’Œçº¦æŸæ€§CTCè®­ç»ƒç›®æ ‡ã€‚é€šè¿‡é¢„æµ‹ç²¾ç»†çš„å¸§-éŸ³ç´ æˆæœ¬çŸ©é˜µå¹¶åº”ç”¨æ”¹è¿›çš„æœ€é•¿å…¬å…±å­åºåˆ—ç®—æ³•ï¼ŒLCS-CTCèƒ½å¤Ÿç¡®å®šé«˜ç½®ä¿¡åº¦çš„å¯¹é½åŒºåŸŸï¼Œç”¨äºçº¦æŸCTCè§£ç è·¯å¾„ç©ºé—´ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆç°è±¡å¹¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä½¿å¾—LCS-CTCåœ¨é²æ£’è¯†åˆ«å’Œæ–‡æœ¬è‡ªç”±å¼ºåˆ¶å¯¹é½æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒè¡¨æ˜ï¼ŒLCS-CTCåœ¨LibriSpeechå’ŒPPAä¸Šçš„è¡¨ç°å‡ä¼˜äºä¼ ç»ŸCTCåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æµåˆ©å’Œéæµåˆ©è¯­éŸ³ä¸­çš„éŸ³ç´ å»ºæ¨¡æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Phonetic speech transcriptionå¯¹äºç²¾ç»†çš„è¯­è¨€å­¦åˆ†æå’Œä¸‹æ¸¸è¯­éŸ³åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>CTCï¼ˆè¿æ¥æ—¶åºåˆ†ç±»ï¼‰åœ¨è¯­éŸ³ä»»åŠ¡ä¸­ç”±äºæ•ˆç‡é«˜è€Œå¹¿æ³›ä½¿ç”¨ï¼Œä½†åœ¨æ¨¡ç³Šå’Œéæµåˆ©è¯­éŸ³ä¸‹çš„è¯†åˆ«æ€§èƒ½è¾ƒå·®ã€‚</li>
<li>LCS-CTCæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„éŸ³ç´ çº§è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œç»“åˆäº†ç›¸ä¼¼æ€§æ„ŸçŸ¥å±€éƒ¨å¯¹é½ç®—æ³•å’Œçº¦æŸCTCè®­ç»ƒç›®æ ‡ã€‚</li>
<li>LCS-CTCé€šè¿‡é¢„æµ‹ç²¾ç»†çš„å¸§-éŸ³ç´ æˆæœ¬çŸ©é˜µå¹¶åº”ç”¨LCSç®—æ³•ç¡®å®šé«˜ç½®ä¿¡åº¦å¯¹é½åŒºåŸŸã€‚</li>
<li>LCS-CTCèƒ½çº¦æŸCTCè§£ç è·¯å¾„ç©ºé—´ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆå’Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜LCS-CTCåœ¨LibriSpeechå’ŒPPAä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»ŸCTCæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b97cf8c3a4a7c76df0c109cc195b915.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d04a2b8a7a06f428928e45829b1eeabd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a68307006417f08e871d8414af16cb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4dfcb60b06b77f5ef633f528750a09f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab9017cb0df79168f54f9cc98243a8e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bfcff87a3af8418c91d948beef00f2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2238a37849a2d08ae1df450599f9973c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Taggus-An-Automated-Pipeline-for-the-Extraction-of-Charactersâ€™-Social-Networks-from-Portuguese-Fiction-Literature"><a href="#Taggus-An-Automated-Pipeline-for-the-Extraction-of-Charactersâ€™-Social-Networks-from-Portuguese-Fiction-Literature" class="headerlink" title="Taggus: An Automated Pipeline for the Extraction of Charactersâ€™ Social   Networks from Portuguese Fiction Literature"></a>Taggus: An Automated Pipeline for the Extraction of Charactersâ€™ Social   Networks from Portuguese Fiction Literature</h2><p><strong>Authors:Tiago G CanÃ¡rio, Catarina Duarte, FlÃ¡vio L. Pinheiro, JoÃ£o L. M. Pereira</strong></p>
<p>Automatically identifying characters and their interactions from fiction books is, arguably, a complex task that requires pipelines that leverage multiple Natural Language Processing (NLP) methods, such as Named Entity Recognition (NER) and Part-of-speech (POS) tagging. However, these methods are not optimized for the task that leads to the construction of Social Networks of Characters. Indeed, the currently available methods tend to underperform, especially in less-represented languages, due to a lack of manually annotated data for training. Here, we propose a pipeline, which we call Taggus, to extract social networks from literary fiction works in Portuguese. Our results show that compared to readily available State-of-the-Art tools â€“ off-the-shelf NER tools and Large Language Models (ChatGPT) â€“ the resulting pipeline, which uses POS tagging and a combination of heuristics, achieves satisfying results with an average F1-Score of $94.1%$ in the task of identifying characters and solving for co-reference and $75.9%$ in interaction detection. These represent, respectively, an increase of $50.7%$ and $22.3%$ on results achieved by the readily available State-of-the-Art tools. Further steps to improve results are outlined, such as solutions for detecting relationships between characters. Limitations on the size and scope of our testing samples are acknowledged. The Taggus pipeline is publicly available to encourage development in this field for the Portuguese language.2 </p>
<blockquote>
<p>è‡ªåŠ¨ä»è™šæ„ä½œå“ä¸­è¯†åˆ«å­—ç¬¦åŠå…¶äº¤äº’æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦åˆ©ç”¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ–¹æ³•çš„ç®¡é“ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œè¯æ€§ï¼ˆPOSï¼‰æ ‡æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¹¶æœªé’ˆå¯¹æ„å»ºè§’è‰²ç¤¾äº¤ç½‘ç»œçš„ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚å®é™…ä¸Šï¼Œç›®å‰å¯ç”¨çš„æ–¹æ³•å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨è¡¨ç°è¾ƒå°‘çš„è¯­è¨€ä¸­ï¼Œç”±äºç¼ºä¹æ‰‹åŠ¨æ³¨é‡Šçš„æ•°æ®æ¥è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œTaggusâ€çš„ç®¡é“ï¼Œç”¨äºä»è‘¡è„ç‰™è¯­æ–‡å­¦è™šæ„ä½œå“ä¸­æå–ç¤¾äº¤ç½‘ç»œã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸ç°æˆçš„å‰æ²¿å·¥å…·ï¼ˆç°æˆçš„NERå·¥å…·å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆChatGPTï¼‰ï¼‰ç›¸æ¯”ï¼Œä½¿ç”¨POSæ ‡æ³¨å’Œå¯å‘å¼æ–¹æ³•ç›¸ç»“åˆçš„ç®¡é“åœ¨è¯†åˆ«è§’è‰²å’Œè§£å†³å…±å¼•ç”¨ä»»åŠ¡ä¸­å–å¾—äº†å¹³å‡F1åˆ†æ•°ä¸º94.1%çš„ä»¤äººæ»¡æ„çš„ç»“æœï¼Œåœ¨äº¤äº’æ£€æµ‹ä¸­çš„å¹³å‡F1åˆ†æ•°ä¸º75.9%ã€‚è¿™åˆ†åˆ«æ¯”ç°æˆçš„å‰æ²¿å·¥å…·å–å¾—çš„æˆæœæé«˜äº†50.7%å’Œ22.3%ã€‚è¿˜æ¦‚è¿°äº†æé«˜ç»“æœçš„è¿›ä¸€æ­¥æ­¥éª¤ï¼Œä¾‹å¦‚æ£€æµ‹è§’è‰²ä¹‹é—´å…³ç³»çš„æ–¹æ³•ã€‚æ‰¿è®¤æµ‹è¯•æ ·æœ¬çš„å¤§å°å’ŒèŒƒå›´å­˜åœ¨å±€é™æ€§ã€‚Taggusç®¡é“ç°å…¬å¼€å¯ç”¨ï¼Œä»¥é¼“åŠ±è‘¡è„ç‰™è¯­é¢†åŸŸçš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03358v1">PDF</a> 24 pages, 5 Figures, 4 Tables</p>
<p><strong>Summary</strong>ï¼š<br>æå‡ºä¸€ç§åä¸ºTaggusçš„ç®¡é“ï¼Œç”¨äºä»è‘¡è„ç‰™è¯­æ–‡å­¦å°è¯´ä¸­æå–ç¤¾ä¼šç½‘ç»œã€‚è¯¥ç®¡é“ç»“åˆå¤šç§è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«å’Œè¯æ€§æ ‡æ³¨ï¼Œä»¥è¯†åˆ«è§’è‰²åŠå…¶äº¤äº’ã€‚ä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„å·¥å…·å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç®¡é“åœ¨è§’è‰²è¯†åˆ«ã€è§£å†³å…±å‚è€ƒå’Œäº¤äº’æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººæ»¡æ„çš„å¹³å‡F1åˆ†æ•°åˆ†åˆ«ä¸º94.1%å’Œ75.9%ï¼Œåˆ†åˆ«æé«˜äº†50.7%å’Œ22.3%ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨é™åˆ¶å’Œè¿›ä¸€æ­¥æ”¹è¿›çš„ç©ºé—´ã€‚Taggusç®¡é“å…¬å¼€å¯ç”¨ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Taggusç®¡é“ç”¨äºä»è‘¡è„ç‰™è¯­æ–‡å­¦å°è¯´ä¸­æå–ç¤¾ä¼šç½‘ç»œã€‚</li>
<li>ç®¡é“ç»“åˆå¤šç§è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«å’Œè¯æ€§æ ‡æ³¨ã€‚</li>
<li>ä¸ç°æœ‰å·¥å…·ç›¸æ¯”ï¼ŒTaggusåœ¨è§’è‰²è¯†åˆ«ã€è§£å†³å…±å‚è€ƒå’Œäº¤äº’æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Taggusçš„å¹³å‡F1åˆ†æ•°åœ¨è§’è‰²è¯†åˆ«å’Œäº¤äº’æ£€æµ‹ä»»åŠ¡ä¸Šåˆ†åˆ«ä¸º94.1%å’Œ75.9%ã€‚</li>
<li>Taggusçš„æå‡ºæ—¨åœ¨é¼“åŠ±è‘¡è„ç‰™è¯­é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
<li>ç›®å‰ä»å­˜åœ¨é™åˆ¶å’Œè¿›ä¸€æ­¥æ”¹è¿›çš„ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ac9131ec386ffc4a3716dfa644d5ba5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RooseBERT-A-New-Deal-For-Political-Language-Modelling"><a href="#RooseBERT-A-New-Deal-For-Political-Language-Modelling" class="headerlink" title="RooseBERT: A New Deal For Political Language Modelling"></a>RooseBERT: A New Deal For Political Language Modelling</h2><p><strong>Authors:Deborah Dore, Elena Cabrio, Serena Villata</strong></p>
<p>The increasing amount of political debates and politics-related discussions calls for the definition of novel computational methods to automatically analyse such content with the final goal of lightening up political deliberation to citizens. However, the specificity of the political language and the argumentative form of these debates (employing hidden communication strategies and leveraging implicit arguments) make this task very challenging, even for current general-purpose pre-trained Language Models. To address this issue, we introduce a novel pre-trained Language Model for political discourse language called RooseBERT. Pre-training a language model on a specialised domain presents different technical and linguistic challenges, requiring extensive computational resources and large-scale data. RooseBERT has been trained on large political debate and speech corpora (8K debates, each composed of several sub-debates on different topics) in English. To evaluate its performances, we fine-tuned it on four downstream tasks related to political debate analysis, i.e., named entity recognition, sentiment analysis, argument component detection and classification, and argument relation prediction and classification. Our results demonstrate significant improvements over general-purpose Language Models on these four tasks, highlighting how domain-specific pre-training enhances performance in political debate analysis. We release the RooseBERT language model for the research community. </p>
<blockquote>
<p>éšç€æ”¿æ²»è¾©è®ºå’Œæ”¿æ²»ç›¸å…³è®¨è®ºçš„æ—¥ç›Šå¢å¤šï¼Œéœ€è¦å®šä¹‰æ–°çš„è®¡ç®—æ–¹æ³•ï¼Œè‡ªåŠ¨åˆ†æè¿™äº›å†…å®¹ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯ä¸ºå…¬æ°‘æä¾›æ”¿æ²»å®¡è®®çš„ä¾¿åˆ©ã€‚ç„¶è€Œï¼Œæ”¿æ²»è¯­è¨€çš„ç‰¹æ®Šæ€§å’Œè¿™äº›è¾©è®ºçš„è®ºè¯å½¢å¼ï¼ˆé‡‡ç”¨éšè”½çš„æ²Ÿé€šç­–ç•¥å’Œåˆ©ç”¨éšå«çš„è®ºæ®ï¼‰ä½¿å¾—è¿™ä¸€ä»»åŠ¡éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå³ä½¿å¯¹äºå½“å‰çš„é€šç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºRooseBERTçš„æ”¿æ²»è¯è¯­è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ç‰¹å®šé¢†åŸŸä¸Šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹é¢ä¸´ç€ä¸åŒçš„æŠ€æœ¯å’Œè¯­è¨€æŒ‘æˆ˜ï¼Œéœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºå’Œå¤§è§„æ¨¡çš„æ•°æ®ã€‚RooseBERTå·²åœ¨å¤§é‡è‹±è¯­æ”¿æ²»è¾©è®ºå’Œæ¼”è®²è¯­æ–™åº“ï¼ˆåŒ…å«8Kè¾©è®ºï¼Œæ¯ä¸ªè¾©è®ºåŒ…å«ä¸åŒä¸»é¢˜çš„å‡ ä¸ªå­è¾©è®ºï¼‰ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°å…¶æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ä¸æ”¿æ²»è¾©è®ºåˆ†æç›¸å…³çš„å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡Œäº†å¾®è°ƒï¼Œå³å‘½åå®ä½“è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€è®ºè¯æˆåˆ†æ£€æµ‹å’Œåˆ†ç±»ä»¥åŠè®ºè¯å…³ç³»é¢„æµ‹å’Œåˆ†ç±»ã€‚æˆ‘ä»¬çš„ç»“æœåœ¨è¿™å››ä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æ”¹è¿›äº†é€šç”¨è¯­è¨€æ¨¡å‹ï¼Œè¯æ˜äº†é¢†åŸŸç‰¹å®šé¢„è®­ç»ƒåœ¨æ”¿æ²»è¾©è®ºåˆ†æä¸­çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬å‘ç ”ç©¶ç•Œå‘å¸ƒRooseBERTè¯­è¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03250v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ”¿æ²»è¾©è®ºå’Œè®¨è®ºçš„å¢åŠ ï¼Œå‘¼åå®šä¹‰æ–°çš„è®¡ç®—æ–¹æ³•æ¥è‡ªåŠ¨åˆ†æè¿™äº›å†…å®¹ï¼Œä»¥å‡è½»å…¬æ°‘çš„æ”¿æ²»è®¨è®ºè´Ÿæ‹…ã€‚é’ˆå¯¹æ”¿æ²»è¯­è¨€çš„ç‰¹æ®Šæ€§å’Œè¾©è®ºä¸­çš„è®ºè¯å½¢å¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹æ”¿æ²»è¯è¯­è¯­è¨€çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹RooseBERTã€‚å®ƒåœ¨è‹±è¯­çš„å¤§å‹æ”¿æ²»è¾©è®ºå’Œæ¼”è®²è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ç”¨äºè§£å†³å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬å‘½åå®ä½“è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æã€è®ºè¯æˆåˆ†æ£€æµ‹å’Œåˆ†ç±»ä»¥åŠè®ºè¯å…³ç³»é¢„æµ‹å’Œåˆ†ç±»ã€‚ç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºé€šç”¨è¯­è¨€æ¨¡å‹ï¼ŒRooseBERTåœ¨è¿™å››ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œçªæ˜¾äº†é¢†åŸŸç‰¹å®šé¢„è®­ç»ƒåœ¨æ”¿æ²»è¾©è®ºåˆ†æä¸­çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬å‘ç ”ç©¶ç¤¾åŒºå‘å¸ƒRooseBERTè¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¿æ²»è¾©è®ºå’Œè®¨è®ºçš„å¢é•¿éœ€æ±‚è‡ªåŠ¨åˆ†æçš„æ”¿æ²»å†…å®¹è®¡ç®—æ–¹æ³•çš„å®šä¹‰ã€‚</li>
<li>RooseBERTæ˜¯ä¸€ç§é’ˆå¯¹æ”¿æ²»è¯è¯­è¯­è¨€çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚</li>
<li>RooseBERTåœ¨æ”¿æ²»è¾©è®ºçš„å››ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>RooseBERTçš„è®­ç»ƒåŸºäºå¤§è§„æ¨¡çš„æ”¿æ²»è¾©è®ºå’Œæ¼”è®²è¯­æ–™åº“ã€‚</li>
<li>ä¸é€šç”¨è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒRooseBERTçš„ç»©æ•ˆæ˜¾è‘—æå‡ã€‚</li>
<li>é¢„è®­ç»ƒç‰¹å®šçš„è¯­è¨€æ¨¡å‹åœ¨æŠ€æœ¯é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å¹¿æ³›çš„è®¡ç®—èµ„æºå’Œå¤§è§„æ¨¡æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22647cec24211b1f60e2978b81a12485.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a20c87002d51ee535afcaa0db4e23970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6041de5effa5f979c734fe6ff63ce674.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-203ea2698a0088348428fa29f3d19f02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bff8b375fb24882f4e54ee7a3df71279.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TF-MLPNet-Tiny-Real-Time-Neural-Speech-Separation"><a href="#TF-MLPNet-Tiny-Real-Time-Neural-Speech-Separation" class="headerlink" title="TF-MLPNet: Tiny Real-Time Neural Speech Separation"></a>TF-MLPNet: Tiny Real-Time Neural Speech Separation</h2><p><strong>Authors:Malek Itani, Tuochao Chen, Shyamnath Gollakota</strong></p>
<p>Speech separation on hearable devices can enable transformative augmented and enhanced hearing capabilities. However, state-of-the-art speech separation networks cannot run in real-time on tiny, low-power neural accelerators designed for hearables, due to their limited compute capabilities. We present TF-MLPNet, the first speech separation network capable of running in real-time on such low-power accelerators while outperforming existing streaming models for blind speech separation and target speech extraction. Our network operates in the time-frequency domain, processing frequency sequences with stacks of fully connected layers that alternate along the channel and frequency dimensions, and independently processing the time sequence at each frequency bin using convolutional layers. Results show that our mixed-precision quantization-aware trained (QAT) model can process 6 ms audio chunks in real-time on the GAP9 processor, achieving a 3.5-4x runtime reduction compared to prior speech separation models. </p>
<blockquote>
<p>è¯­éŸ³åˆ†ç¦»åœ¨å¯å¬è®¾å¤‡ä¸Šçš„åº”ç”¨å¯ä»¥å¸¦æ¥å˜é©æ€§çš„å¢å¼ºå’Œæ‰©å±•å¬åŠ›èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºæœ€å…ˆè¿›çš„è¯­éŸ³åˆ†ç¦»ç½‘ç»œè®¡ç®—èƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•åœ¨é’ˆå¯¹å¯å¬è®¾å¤‡è®¾è®¡çš„å°å‹ã€ä½åŠŸè€—ç¥ç»ç½‘ç»œåŠ é€Ÿå™¨ä¸Šå®æ—¶è¿è¡Œã€‚æˆ‘ä»¬æ¨å‡ºäº†TF-MLPNetï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿåœ¨è¿™ç§ä½åŠŸè€—åŠ é€Ÿå™¨ä¸Šå®æ—¶è¿è¡Œä¸”è¡¨ç°ä¼˜äºç°æœ‰æµå¼æ¨¡å‹çš„è¯­éŸ³åˆ†ç¦»ç½‘ç»œï¼Œç”¨äºç›²è¯­éŸ³åˆ†ç¦»å’Œç›®æ ‡è¯­éŸ³æå–ã€‚æˆ‘ä»¬çš„ç½‘ç»œåœ¨æ—¶é¢‘åŸŸè¿è¡Œï¼Œä½¿ç”¨å…¨è¿æ¥å±‚å¤„ç†é¢‘ç‡åºåˆ—ï¼Œè¿™äº›å±‚åœ¨é€šé“å’Œé¢‘ç‡ç»´åº¦ä¸Šäº¤æ›¿å †å ï¼Œå¹¶ä½¿ç”¨å·ç§¯å±‚ç‹¬ç«‹å¤„ç†æ¯ä¸ªé¢‘ç‡çš„æ—¶é—´åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬ç»è¿‡æ··åˆç²¾åº¦é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰çš„æ¨¡å‹å¯ä»¥åœ¨GAP9å¤„ç†å™¨ä¸Šå®æ—¶å¤„ç†é•¿åº¦ä¸º6æ¯«ç§’çš„éŸ³é¢‘ç‰‡æ®µï¼Œç›¸è¾ƒäºå…ˆå‰çš„è¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œå®ç°äº†çº¦3.5è‡³4å€çš„è¿è¡Œæ—¶ç¼©å‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03047v1">PDF</a> The 6th Clarity Workshop on Improving Speech-in-Noise for Hearing   Devices (Clarity 2025)</p>
<p><strong>Summary</strong>ï¼š<br>å®æ—¶è¯­éŸ³åˆ†ç¦»ç½‘ç»œèƒ½åœ¨ä½åŠŸè€—åŠ é€Ÿå™¨ä¸Šå®ç°å®æ—¶è¿è¡Œï¼Œæé«˜å¬åŠ›å’Œå¢å¼ºå¬åŠ›è®¾å¤‡çš„æ€§èƒ½ã€‚ä½†ç”±äºä½åŠŸè€—åŠ é€Ÿå™¨çš„è®¡ç®—èƒ½åŠ›æœ‰é™ï¼Œå½“å‰å…ˆè¿›çš„è¯­éŸ³åˆ†ç¦»ç½‘ç»œæ— æ³•è¿è¡Œã€‚æœ¬æ–‡æå‡ºTF-MLPNetï¼Œé¦–ä¸ªèƒ½åœ¨è¿™ç§ä½åŠŸè€—åŠ é€Ÿå™¨ä¸Šå®æ—¶è¿è¡Œçš„è¯­éŸ³åˆ†ç¦»ç½‘ç»œï¼Œå¹¶ä¸”åœ¨ç›²è¯­éŸ³åˆ†ç¦»å’Œç›®æ ‡è¯­éŸ³æå–æ–¹é¢ä¼˜äºç°æœ‰æµå¼æ¨¡å‹ã€‚è¯¥ç½‘ç»œåœ¨æ—¶é¢‘åŸŸè¿è¡Œï¼Œå¤„ç†é¢‘ç‡åºåˆ—ï¼Œå¹¶ç‹¬ç«‹å¤„ç†æ¯ä¸ªé¢‘ç‡åˆ†åŒºçš„éŸ³é¢‘æ—¶é—´åºï¼Œä»è€Œè¾¾åˆ°ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚æ¨¡å‹åœ¨é€šè¿‡æ··åˆç²¾åº¦é‡åŒ–è®­ç»ƒåèƒ½å¤Ÿåœ¨æœ€çŸ­6æ¯«ç§’çš„æ—¶é—´å†…å¯¹éŸ³é¢‘è¿›è¡Œé‡åŒ–å’Œåˆ†æï¼Œå°†è¿è¡Œé€Ÿåº¦æå‡ä¸ºä¹‹å‰çš„è¯­éŸ³åˆ†ç¦»æ¨¡å‹çš„3.5è‡³4å€ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>TF-MLPNetç½‘ç»œå®ç°äº†å®æ—¶è¯­éŸ³åˆ†ç¦»æŠ€æœ¯åœ¨ä½åŠŸè€—åŠ é€Ÿå™¨ä¸Šçš„åº”ç”¨ï¼Œæ˜¾è‘—æå‡å¬åŠ›è®¾å¤‡çš„æ€§èƒ½å’Œå¢å¼ºä½“éªŒã€‚</li>
<li>ç”±äºå—åˆ°ç¡¬ä»¶èƒ½åŠ›çš„é™åˆ¶ï¼Œå¤§å¤šæ•°å½“å‰æµè¡Œçš„è¯­éŸ³åˆ†ç¦»æ¨¡å‹æ— æ³•åœ¨æä½åŠŸè€—åŠ é€Ÿå™¨ä¸­å®æ–½å®æ—¶æ“ä½œã€‚ç„¶è€Œï¼Œæœ¬æ–‡å…‹æœäº†è¿™ä¸€é™åˆ¶ï¼Œå¼€åˆ›äº†æ–°é¢†åŸŸçš„åˆ›æ–°ç ”ç©¶æœºä¼šã€‚è¿™ä¸€é¢†åŸŸè¿˜åŒ…æ‹¬æ‰©å¤§è¿è¡Œæ¨¡å‹çš„è§„æ¨¡ä»¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯å’Œç”¨æˆ·éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13fe7ca4b4e120ab0fffff9f07d85994.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-378967c16627292c5008a91ae52a3c0f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d863e6757156d7400b67befa02e1ffda.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Real-time-speech-enhancement-in-noise-for-throat-microphone-using-neural-audio-codec-as-foundation-model"><a href="#Real-time-speech-enhancement-in-noise-for-throat-microphone-using-neural-audio-codec-as-foundation-model" class="headerlink" title="Real-time speech enhancement in noise for throat microphone using neural   audio codec as foundation model"></a>Real-time speech enhancement in noise for throat microphone using neural   audio codec as foundation model</h2><p><strong>Authors:Julien Hauret, Thomas Joubaud, Ã‰ric Bavu</strong></p>
<p>We present a real-time speech enhancement demo using speech captured with a throat microphone. This demo aims to showcase the complete pipeline, from recording to deep learning-based post-processing, for speech captured in noisy environments with a body-conducted microphone. The throat microphone records skin vibrations, which naturally attenuate external noise, but this robustness comes at the cost of reduced audio bandwidth. To address this challenge, we fine-tune Kyutaiâ€™s Mimiâ€“a neural audio codec supporting real-time inferenceâ€“on Vibravox, a dataset containing paired air-conducted and throat microphone recordings. We compare this enhancement strategy against state-of-the-art models and demonstrate its superior performance. The inference runs in an interactive interface that allows users to toggle enhancement, visualize spectrograms, and monitor processing latency. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªä½¿ç”¨å–‰å¤´éº¦å…‹é£æ•æ‰è¯­éŸ³çš„å®æ—¶è¯­éŸ³å¢å¼ºæ¼”ç¤ºã€‚è¿™ä¸ªæ¼”ç¤ºæ—¨åœ¨å±•ç¤ºä»å½•åˆ¶åˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„åå¤„ç†çš„å®Œæ•´æµç¨‹ï¼Œç”¨äºåœ¨å˜ˆæ‚ç¯å¢ƒä¸­ä½¿ç”¨èº«ä½“ä¼ å¯¼éº¦å…‹é£æ•æ‰è¯­éŸ³ã€‚å–‰å¤´éº¦å…‹é£è®°å½•çš®è‚¤æŒ¯åŠ¨ï¼Œè‡ªç„¶åœ°è¡°å‡å¤–éƒ¨å™ªéŸ³ï¼Œä½†è¿™ç§ç¨³å¥æ€§æ˜¯ä»¥é™ä½éŸ³é¢‘å¸¦å®½ä¸ºä»£ä»·çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¯¹Kyutaiçš„Mimiè¿›è¡Œäº†å¾®è°ƒâ€”â€”è¿™æ˜¯ä¸€ç§æ”¯æŒå®æ—¶æ¨ç†çš„ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–è§£ç å™¨â€”â€”åœ¨Vibravoxæ•°æ®é›†ä¸Šï¼Œè¯¥æ•°æ®é›†åŒ…å«æ°”ä¼ å’Œå–‰å¤´éº¦å…‹é£å½•éŸ³çš„é…å¯¹è®°å½•ã€‚æˆ‘ä»¬å°†è¿™ç§å¢å¼ºç­–ç•¥ä¸æœ€æ–°æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å±•ç¤ºäº†å…¶å“è¶Šæ€§èƒ½ã€‚æ¨ç†åœ¨ä¸€ä¸ªäº¤äº’å¼ç•Œé¢ä¸­è¿è¡Œï¼Œå…è®¸ç”¨æˆ·åˆ‡æ¢å¢å¼ºåŠŸèƒ½ã€å¯è§†åŒ–é¢‘è°±å›¾å¹¶ç›‘æ§å¤„ç†å»¶è¿Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02974v1">PDF</a> 2 pages, 2 figures</p>
<p><strong>Summary</strong><br>ï¼šè¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªä½¿ç”¨å–‰éƒ¨éº¦å…‹é£è¿›è¡Œè¯­éŸ³é‡‡é›†çš„å®æ—¶è¯­éŸ³å¢å¼ºæ¼”ç¤ºã€‚æ¼”ç¤ºçš„ç›®çš„æ˜¯å±•ç¤ºä»å½•éŸ³åˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„åå¤„ç†çš„å®Œæ•´æµç¨‹ï¼Œé’ˆå¯¹åœ¨å˜ˆæ‚ç¯å¢ƒä¸­ä½¿ç”¨èº«ä½“ä¼ å¯¼éº¦å…‹é£é‡‡é›†çš„è¯­éŸ³ã€‚å–‰éƒ¨éº¦å…‹é£è®°å½•çš®è‚¤æŒ¯åŠ¨ï¼Œè‡ªç„¶è¡°å‡å¤–éƒ¨å™ªéŸ³ï¼Œä½†è¿™ä¸€ç¨³å¥æ€§æ˜¯ä»¥é™ä½éŸ³é¢‘å¸¦å®½ä¸ºä»£ä»·çš„ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…åœ¨Vibravoxæ•°æ®é›†ä¸Šå¾®è°ƒäº†Kyutaiçš„Mimiâ€”â€”ä¸€ä¸ªæ”¯æŒå®æ—¶æ¨æ–­çš„ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç©ºæ°”ä¼ å¯¼å’Œå–‰éƒ¨éº¦å…‹é£å½•éŸ³çš„é…å¯¹è®°å½•ã€‚ä½œè€…å°†è¿™ä¸€å¢å¼ºç­–ç•¥ä¸æœ€æ–°æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶å±•ç¤ºäº†å…¶å“è¶Šæ€§èƒ½ã€‚æ¨ç†åœ¨ä¸€ä¸ªäº¤äº’å¼ç•Œé¢ä¸­è¿è¡Œï¼Œå…è®¸ç”¨æˆ·åˆ‡æ¢å¢å¼ºåŠŸèƒ½ã€å¯è§†åŒ–é¢‘è°±å›¾å¹¶ç›‘æ§å¤„ç†å»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªä½¿ç”¨å–‰éƒ¨éº¦å…‹é£è¿›è¡Œè¯­éŸ³é‡‡é›†çš„å®æ—¶è¯­éŸ³å¢å¼ºæ¼”ç¤ºã€‚</li>
<li>æ¼”ç¤ºçš„ç›®çš„æ˜¯å±•ç¤ºå®Œæ•´çš„è¯­éŸ³å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬ä»å½•éŸ³åˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„åå¤„ç†ã€‚</li>
<li>å–‰éƒ¨éº¦å…‹é£èƒ½å¤Ÿè®°å½•çš®è‚¤æŒ¯åŠ¨å¹¶è‡ªç„¶è¡°å‡å¤–éƒ¨å™ªéŸ³ï¼Œä½†ä¼šé™ä½éŸ³é¢‘å¸¦å®½ã€‚</li>
<li>ä½œè€…ä½¿ç”¨Vibravoxæ•°æ®é›†å¯¹Kyutaiçš„Mimiç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨è¿›è¡Œäº†å¾®è°ƒã€‚</li>
<li>ä½œè€…å°†æå‡ºçš„è¯­éŸ³å¢å¼ºç­–ç•¥ä¸ç°æœ‰å…ˆè¿›æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5508020bdd893135808308ebf02fcdfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10e2d7e43d54a42af6ec79850fae14c5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adaptive-Knowledge-Distillation-for-Device-Directed-Speech-Detection"><a href="#Adaptive-Knowledge-Distillation-for-Device-Directed-Speech-Detection" class="headerlink" title="Adaptive Knowledge Distillation for Device-Directed Speech Detection"></a>Adaptive Knowledge Distillation for Device-Directed Speech Detection</h2><p><strong>Authors:Hyung Gun Chi, Florian Pesce, Wonil Chang, Oggi Rudovic, Arturo Argueta, Stefan Braun, Vineet Garg, Ahmed Hussen Abdelaziz</strong></p>
<p>Device-directed speech detection (DDSD) is a binary classification task that separates the userâ€™s queries to a voice assistant (VA) from background speech or side conversations. This is important for achieving naturalistic user experience. To this end, we propose knowledge distillation (KD) to enhance DDSD accuracy while ensuring efficient deployment. Specifically, we introduce a novel adaptive KD method that transfers knowledge from general representations of an ASR large pre-trained acoustic encoder (teacher). We apply task-specific adapters, on top of the (frozen) teacher encoder, trained jointly with the student model on DDSD. We demonstrate that the proposed adaptive KD outperforms the student model without distillation in the keyword and keyword-free (follow-up) invocations, with an improvement of +26% and +19% in terms of Equal Error Rate, respectively. We also show that this approach generalizes across the transformer and conformer-based model architectures. </p>
<blockquote>
<p>è®¾å¤‡å®šå‘è¯­éŸ³æ£€æµ‹ï¼ˆDDSDï¼‰æ˜¯ä¸€é¡¹äºŒåˆ†ç±»ä»»åŠ¡ï¼Œå®ƒå°†ç”¨æˆ·çš„æŸ¥è¯¢è¯­éŸ³åŠ©æ‰‹ï¼ˆVAï¼‰ä¸èƒŒæ™¯è¯­éŸ³æˆ–ä¾§é¢å¯¹è¯åŒºåˆ†å¼€æ¥ã€‚è¿™å¯¹äºå®ç°è‡ªç„¶ä¸»ä¹‰ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜DDSDçš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ç¡®ä¿é«˜æ•ˆéƒ¨ç½²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹è‡ªé€‚åº”KDæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ä»ä¸€ä¸ªå¤§å‹é¢„è®­ç»ƒå£°å­¦ç¼–ç å™¨çš„é€šç”¨è¡¨ç¤ºï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰ä¸­è½¬ç§»çŸ¥è¯†ã€‚æˆ‘ä»¬åœ¨ï¼ˆå†»ç»“çš„ï¼‰æ•™å¸ˆç¼–ç å™¨ä¹‹ä¸Šåº”ç”¨äº†ä»»åŠ¡ç‰¹å®šé€‚é…å™¨ï¼Œå¹¶ä¸DDSDä¸Šçš„å­¦ç”Ÿæ¨¡å‹è”åˆè®­ç»ƒã€‚æˆ‘ä»¬è¯æ˜ï¼Œä¸æœªç»è’¸é¦çš„å­¦ç”Ÿæ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºè‡ªé€‚åº”KDåœ¨å…³é”®è¯å’Œå…³é”®è¯æ— å…³ï¼ˆåç»­ï¼‰è°ƒç”¨ä¸­è¡¨ç°æ›´ä¼˜ï¼Œåœ¨ç­‰ä»·é”™è¯¯ç‡æ–¹é¢åˆ†åˆ«æé«˜äº†+26%å’Œ+19%ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯åº”ç”¨äºåŸºäºå˜å‹å™¨å’Œå·ç§¯å˜å‹å™¨æ¨¡å‹æ¶æ„çš„æ¨¡å‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02801v1">PDF</a> 5 pages, 2 figures, Interspeech accepted</p>
<p><strong>Summary</strong><br>è®¾å¤‡å®šå‘è¯­éŸ³æ£€æµ‹ï¼ˆDDSDï¼‰æ˜¯åŒºåˆ†ç”¨æˆ·æŸ¥è¯¢è¯­éŸ³åŠ©æ‰‹ï¼ˆVAï¼‰çš„è¯­éŸ³ä¸èƒŒæ™¯è¯­éŸ³æˆ–æ—è¾¹å¯¹è¯çš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡ï¼Œå¯¹å®ç°è‡ªç„¶ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é‡‡ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯æé«˜DDSDçš„å‡†ç¡®æ€§å¹¶ä¿éšœéƒ¨ç½²æ•ˆç‡ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œä»å¤§å‹é¢„è®­ç»ƒå£°å­¦ç¼–ç å™¨çš„é€šç”¨è¡¨ç¤ºä¸­è½¬ç§»çŸ¥è¯†ï¼ˆæ•™å¸ˆï¼‰ã€‚åœ¨å†»ç»“çš„æ•™å¸ˆç¼–ç å™¨ä¹‹ä¸Šåº”ç”¨ä»»åŠ¡ç‰¹å®šé€‚é…å™¨ï¼Œå¹¶ä¸DDSDçš„å­¦ç”Ÿæ¨¡å‹è”åˆè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œè‡ªé€‚åº”çŸ¥è¯†è’¸é¦åœ¨å…³é”®è¯å’Œå…³é”®è¯è‡ªç”±ï¼ˆè·Ÿè¿›ï¼‰è°ƒç”¨ä¸­å‡ä¼˜äºæœªç»è’¸é¦çš„å­¦ç”Ÿæ¨¡å‹ï¼Œåœ¨åŒç­‰é”™è¯¯ç‡ä¸Šåˆ†åˆ«æé«˜äº†26%å’Œæé«˜äº†æ”¹å–„ï¼Œå¹¶ä¸”åœ¨è½¬æ¢å™¨å’Œå·ç§¯å˜å‹å™¨æ¨¡å‹æ¶æ„ä¸Šè¿›è¡Œäº†æ¦‚æ‹¬åº”ç”¨ã€‚æ€»ä½“ä¸Šå¢å¼ºäº†è®¾å¤‡çš„è¯­è¨€å¤„ç†èƒ½åŠ›å¹¶ä¼˜åŒ–äº†ç”¨æˆ·ä½“éªŒã€‚è¯¥æ–¹æ³•å¯ä»¥çœ‹ä½œæ˜¯åŸºäºäººå·¥æ™ºèƒ½ä¸å£°å­¦å»ºæ¨¡çš„æ–°åº”ç”¨å·¥å…·çš„ä¸€æ¬¡æå‡ä¸åˆ›æ–°æ€§å¼€å‘ã€‚<strong>å…³é”®è¦ç‚¹åˆ—ä¸¾å¦‚ä¸‹</strong>ï¼šï¼šåˆ—å‡ºäº†è¿™ç¯‡è®ºæ–‡ä¸­æ¶‰åŠçš„å‡ ä¸ªæ–¹é¢å…³é”®ç ”ç©¶å†…å®¹ä¸è¦ç‚¹åˆ†æçš„å…³é”®è§‚ç‚¹å’Œä¿¡æ¯ä½œä¸ºä¾¿äºè¯»è€…å›é¡¾æ€»ç»“å¹¶å¯ä»¥å®¹æ˜“å¸æ”¶æ€»ç»“ä»¥åŠä¸ºç†è®ºç ”ç©¶åç»­å¯èƒ½çš„æ“ä½œä½œä¸ºå‚ç…§ä¸»è¦é›†ä¸­äºè‡ªæˆ‘ç ”ç©¶æ–¹å‘çš„æè¿°å’Œé‡è¦æ€æƒ³æ–¹æ³•è®ºè¿™äº›æ–¹å‘ä»æ¡†æ¶å»ºè®¾æ€æƒ³çš„é€‚ç”¨æ€§å…·ä½“åº”ç”¨é€»è¾‘å¯ä»¥è½åœ°çš„è¯¸å¤šç°å®ç¯å¢ƒä¸­æ‰€æå‡ºçš„ä¼˜ç§€åº”ç”¨è·¯å¾„æ¦‚è¿°ä¸‹æ¥æ€»å…±ä¸è¶…è¿‡ä¸ƒç‚¹æ˜¯å¸®åŠ©ç®¡ç†è€…åˆ†æå¸ˆåˆ†æå¸ˆä¸“ä¸šåˆ†æåçš„äº†è§£å¹¶åœ¨æ—¥åçš„å®é™…å·¥ä½œä¸­å­¦ä¹ æŒ‡å¯¼å­¦ä¹ å®è·µä»¥åŠå¯¹äºæ•´ä¸ªè¡Œä¸šæœªæ¥çš„å‘å±•è¶‹åŠ¿é¢„æµ‹èµ·åˆ°é‡è¦çš„å‚è€ƒä½œç”¨ï¼š</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16506d8de3ed8a5c1dd0d2cc22b90b03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc59b579daae6feb1248d0bd185260af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-347e1462864fbbe82da41c6b569a4063.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation"><a href="#AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation" class="headerlink" title="AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation"></a>AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation</h2><p><strong>Authors:Le Wang, Jun Wang, Feng Deng, Chen Zhang, Di Zhang, Kun Gai</strong></p>
<p>We present AudioGen-Omni - a unified approach based on multimodal diffusion transformers (MMDit), capable of generating high-fidelity audio, speech, and songs coherently synchronized with the input video. AudioGen-Omni introduces a novel joint training paradigm that seamlessly integrates large-scale video-text-audio corpora, enabling a model capable of generating semantically rich, acoustically diverse audio conditioned on multimodal inputs and adaptable to a wide range of audio generation tasks. AudioGen-Omni employs a unified lyrics-transcription encoder that encodes graphemes and phonemes from both sung and spoken inputs into dense frame-level representations. Dense frame-level representations are fused using an AdaLN-based joint attention mechanism enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein RoPE is selectively applied to temporally structured modalities to ensure precise and robust cross-modal alignment. By unfreezing all modalities and masking missing inputs, AudioGen-Omni mitigates the semantic constraints of text-frozen paradigms, enabling effective cross-modal conditioning. This joint training approach enhances audio quality, semantic alignment, and lip-sync accuracy, while also achieving state-of-the-art results on Text-to-Audio&#x2F;Speech&#x2F;Song tasks. With an inference time of 1.91 seconds for 8 seconds of audio, it offers substantial improvements in both efficiency and generality. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†AudioGen-Omniâ€”â€”ä¸€ç§åŸºäºå¤šæ¨¡å¼æ‰©æ•£å˜å‹å™¨ï¼ˆMMDitï¼‰çš„ç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸è¾“å…¥è§†é¢‘åŒæ­¥çš„é«˜ä¿çœŸéŸ³é¢‘ã€è¯­éŸ³å’Œæ­Œæ›²ã€‚AudioGen-Omniå¼•å…¥äº†ä¸€ç§æ–°çš„è”åˆè®­ç»ƒèŒƒå¼ï¼Œæ— ç¼é›†æˆäº†å¤§è§„æ¨¡çš„è§†é¢‘-æ–‡æœ¬-éŸ³é¢‘è¯­æ–™åº“ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸°å¯Œè¯­ä¹‰ã€å£°éŸ³å¤šæ ·çš„éŸ³é¢‘ï¼Œèƒ½æ ¹æ®å¤šæ¨¡å¼è¾“å…¥è¿›è¡Œè°ƒæ•´ï¼Œå¹¶é€‚åº”å¹¿æ³›çš„éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ã€‚AudioGen-Omnié‡‡ç”¨ç»Ÿä¸€çš„æ­Œè¯è½¬å½•ç¼–ç å™¨ï¼Œå°†æ­Œå”±å’Œå£è¯­è¾“å…¥ä¸­çš„å­—æ¯å’ŒéŸ³ç´ ç¼–ç æˆå¯†é›†çš„å¸§çº§è¡¨ç¤ºã€‚å¯†é›†çš„å¸§çº§è¡¨ç¤ºé€šè¿‡ä½¿ç”¨åŸºäºAdaLNçš„è”åˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèåˆï¼Œå¢å¼ºäº†ç›¸ä½å¯¹é½çš„å¼‚æ„ä½ç½®æ³¨å…¥ï¼ˆPAAPIï¼‰ï¼Œå…¶ä¸­RoPEè¢«é€‰æ‹©æ€§åœ°åº”ç”¨äºæ—¶é—´ç»“æ„æ¨¡æ€ï¼Œä»¥ç¡®ä¿ç²¾ç¡®å’Œç¨³å®šçš„è·¨æ¨¡æ€å¯¹é½ã€‚é€šè¿‡è§£å†»æ‰€æœ‰æ¨¡æ€å¹¶æ©ç›–ç¼ºå¤±çš„è¾“å…¥ï¼ŒAudioGen-Omniå‡è½»äº†æ–‡æœ¬å†»ç»“èŒƒå¼çš„è¯­ä¹‰çº¦æŸï¼Œå®ç°äº†æœ‰æ•ˆçš„è·¨æ¨¡æ€æ¡ä»¶ã€‚è¿™ç§è”åˆè®­ç»ƒæ–¹æ³•æé«˜äº†éŸ³é¢‘è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œå”‡åŒæ­¥ç²¾åº¦ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬åˆ°éŸ³é¢‘&#x2F;è¯­éŸ³&#x2F;æ­Œæ›²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚å…¶æ¨ç†æ—¶é—´ä¸º1.91ç§’å¯ç”Ÿæˆ8ç§’çš„éŸ³é¢‘ï¼Œåœ¨æ•ˆç‡å’Œé€šç”¨æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00733v3">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆMMDitï¼‰çš„AudioGen-Omniç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸è¾“å…¥è§†é¢‘åŒæ­¥çš„é«˜ä¿çœŸéŸ³é¢‘ã€è¯­éŸ³å’Œæ­Œæ›²ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°å‹è”åˆè®­ç»ƒèŒƒå¼ï¼Œæ— ç¼é›†æˆå¤§è§„æ¨¡è§†é¢‘-æ–‡æœ¬-éŸ³é¢‘è¯­æ–™åº“ï¼Œå¯ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œã€å£°éŸ³å¤šæ ·çš„éŸ³é¢‘ï¼Œå¹¶æ ¹æ®å¤šæ¨¡æ€è¾“å…¥è¿›è¡Œé€‚åº”ï¼Œé€‚ç”¨äºå¹¿æ³›çš„éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AudioGen-Omniæ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆMMDitï¼‰çš„ç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸéŸ³é¢‘ã€è¯­éŸ³å’Œæ­Œæ›²ï¼Œå¹¶ä¸è¾“å…¥è§†é¢‘åŒæ­¥ã€‚</li>
<li>å¼•å…¥æ–°å‹è”åˆè®­ç»ƒèŒƒå¼ï¼Œé›†æˆå¤§è§„æ¨¡è§†é¢‘-æ–‡æœ¬-éŸ³é¢‘è¯­æ–™åº“ã€‚</li>
<li>ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œã€å£°éŸ³å¤šæ ·çš„éŸ³é¢‘ï¼Œé€‚åº”å¤šæ¨¡æ€è¾“å…¥ã€‚</li>
<li>é‡‡ç”¨ç»Ÿä¸€æ­Œè¯-è½¬å½•ç¼–ç å™¨ï¼Œå¯¹æ­Œå”±å’Œå£è¯­è¾“å…¥è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå¯†é›†å¸§çº§è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨åŸºäºAdaLNçš„è”åˆæ³¨æ„æœºåˆ¶èåˆå¯†é›†å¸§çº§è¡¨ç¤ºï¼Œå¢å¼ºç›¸ä½å¯¹é½çš„å¼‚æ„ä½ç½®æ³¨å…¥ï¼ˆPAAPIï¼‰ã€‚</li>
<li>é€šè¿‡è§£å†»æ‰€æœ‰æ¨¡æ€å¹¶æ©ç›–ç¼ºå¤±è¾“å…¥ï¼Œç¼“è§£æ–‡æœ¬å†»ç»“èŒƒå¼çš„è¯­ä¹‰çº¦æŸï¼Œå®ç°æœ‰æ•ˆçš„è·¨æ¨¡æ€æ¡ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3378fa5fdd9d423d0ef6419df2ee2d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38e20067b86f9682aa8b7b2428f93191.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a86b4f217474a0d03ffcfc10bb25eabd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b67d41d4c49685f7d28ce294f9f9f4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation"><a href="#UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation" class="headerlink" title="UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation"></a>UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation</h2><p><strong>Authors:Jinting Wang, Shan Yang, Chenxing Li, Dong Yu, Li Liu</strong></p>
<p>Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics. </p>
<blockquote>
<p>æç¤ºæ€§è¨€è¯­ï¼ˆCued Speechï¼Œç®€ç§°CSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œæä¾›è§†è§‰éŸ³ç´ çº¿ç´¢ï¼Œæ”¯æŒå¬åŠ›å—æŸè€…ç²¾ç¡®æ„ŸçŸ¥è¯­éŸ³ã€‚CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰çš„ä»»åŠ¡æ—¨åœ¨å°†CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚ç›®å‰å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨CSè¯†åˆ«ï¼ˆCSRï¼‰ä¸Šï¼Œå³å°†è§†é¢‘å†…å®¹è½¬å½•ä¸ºæ–‡æœ¬ã€‚å› æ­¤ï¼ŒCSV2Sçš„å¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯å°†CSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç›¸ç»“åˆã€‚ç„¶è€Œï¼Œæ­¤æµç¨‹ä¾èµ–äºæ–‡æœ¬ä½œä¸ºä¸­é—´åª’ä»‹ï¼Œå¯èƒ½å¯¼è‡´è¯¯å·®ä¼ æ’­ä»¥åŠè¯­éŸ³å’ŒCSè§†é¢‘åŠ¨æ€ä¹‹é—´çš„æ—¶é—´ä¸å¯¹é½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›´æ¥ä»CSè§†é¢‘ç”ŸæˆéŸ³é¢‘è¯­éŸ³ï¼ˆç›´æ¥CSV2Sï¼‰å¸¸å¸¸é¢ä¸´å›ºæœ‰çš„å¤šæ¨¡å¼å¤æ‚æ€§å’Œæœ‰é™çš„CSæ•°æ®å¯ç”¨æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniCUEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºCSV2Sçš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚UniCUEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ•´åˆäº†ç†è§£ä»»åŠ¡ï¼ˆCSRï¼‰ï¼Œæä¾›ç²¾ç»†çš„CSè§†è§‰è¯­ä¹‰çº¿ç´¢æ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒUniCUEèå…¥äº†å§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€è¯­ä¹‰å¯¹é½æ± ï¼Œå®ç°äº†ç²¾ç¡®è§†è§‰è¯­ä¹‰æ˜ å°„ï¼Œä»¥åŠVisioPhoneticé€‚é…å™¨ï¼Œå¯åœ¨ç»Ÿä¸€æ¶æ„å†…æ­å»ºç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚ä¸ºäº†æ”¯æŒè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†UniCUE-HIï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡CSæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª14åæ‰“æ‰‹åŠ¿è€…çš„11282ä¸ªè§†é¢‘ï¼Œå…¶ä¸­åŒ…æ‹¬å¬éšœäººå£«å’Œå¬åŠ›æ­£å¸¸çš„äººã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniCUEåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04134v3">PDF</a> 8 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Cued Speechï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯­é˜…è¯»ï¼Œä¸ºå¬åŠ›å—æŸè€…æä¾›è§†è§‰è¯­éŸ³çº¿ç´¢ä»¥æ”¯æŒç²¾ç¡®è¯­éŸ³æ„ŸçŸ¥ã€‚CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰çš„ä»»åŠ¡æ—¨åœ¨å°†CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶ä¸“æ³¨äºCSè¯†åˆ«ï¼ˆCSRï¼‰ï¼Œå°†è§†é¢‘å†…å®¹è½¬å½•ä¸ºæ–‡æœ¬ã€‚å› æ­¤ï¼ŒCSV2Sçš„å¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯å°†CSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç»“åˆã€‚ç„¶è€Œï¼Œæ­¤ç®¡é“ä¾èµ–äºä½œä¸ºä¸­é—´åª’ä»‹çš„æ–‡æœ¬ï¼Œå¯èƒ½å¯¼è‡´è¯¯å·®ä¼ æ’­å’Œè¯­éŸ³ä¸CSè§†é¢‘åŠ¨æ€ä¹‹é—´çš„æ—¶é—´ä¸åŒ¹é…ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›´æ¥ä»CSè§†é¢‘ç”ŸæˆéŸ³é¢‘è¯­éŸ³ï¼ˆç›´æ¥CSV2Sï¼‰å¸¸å¸¸é¢ä¸´å†…åœ¨çš„å¤šæ¨¡å¼å¤æ‚æ€§å’ŒCSæ•°æ®æœ‰é™çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniCUEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºCSV2Sçš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¯ç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚UniCUEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ•´åˆäº†ç†è§£ä»»åŠ¡ï¼ˆCSRï¼‰ï¼Œæä¾›ç²¾ç»†çš„CSè§†è§‰è¯­ä¹‰çº¿ç´¢æ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒUniCUEç»“åˆäº†å§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€è¯­ä¹‰å¯¹é½æ± ï¼ˆå®ç°ç²¾ç¡®è§†è§‰è¯­ä¹‰æ˜ å°„ï¼‰å’ŒVisioPhoneticé€‚é…å™¨ï¼ˆåœ¨ç»Ÿä¸€æ¶æ„å†…æ¡¥æ¥ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼‰ã€‚ä¸ºæ”¯æŒæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«11282ä¸ªè§†é¢‘çš„å¤§å‹æ™®é€šè¯CSæ•°æ®é›†UniCUE-HIï¼Œè¿™äº›è§†é¢‘æ¥è‡ª14åcuersï¼ŒåŒ…æ‹¬å¬éšœå’Œæ­£å¸¸å¬åŠ›çš„äººã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniCUEåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Cued Speechï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯­é˜…è¯»ï¼Œä¸ºå¬åŠ›å—æŸè€…æä¾›è§†è§‰è¯­éŸ³çº¿ç´¢ã€‚</li>
<li>CSV2Sçš„ç›®æ ‡æ˜¯å°†CSè§†é¢‘è½¬åŒ–ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ï¼Œç°æœ‰ç ”ç©¶å¤šèšç„¦äºCSè¯†åˆ«ï¼ˆCSRï¼‰ã€‚</li>
<li>å°†CSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç»“åˆæ˜¯CSV2Sçš„å¸¸è§è§£å†³æ–¹æ¡ˆï¼Œä½†å­˜åœ¨è¯¯å·®ä¼ æ’­å’Œæ—¶é—´ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>ç›´æ¥ä»CSè§†é¢‘ç”ŸæˆéŸ³é¢‘è¯­éŸ³é¢ä¸´å¤šæ¨¡å¼å¤æ‚æ€§å’ŒCSæ•°æ®æœ‰é™æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>UniCUEæ˜¯é¦–ä¸ªç”¨äºCSV2Sçš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¯ç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚</li>
<li>UniCUEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ•´åˆç†è§£ä»»åŠ¡ï¼ˆCSRï¼‰æ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆï¼Œå…·æœ‰å§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€è¯­ä¹‰å¯¹é½æ± å’ŒVisioPhoneticé€‚é…å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-07845cec9a726ba2f15b028773000f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b9c1341630a0b49e215451149672980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb4e16a4a612ba253281afbde879cd2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbe9a9aeae12c0d60360803d6e2bad96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1357283a1bc69b5e7e50281d19c85a3b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Adaptive-Audio-Visual-Speech-Recognition-via-Matryoshka-Based-Multimodal-LLMs"><a href="#Adaptive-Audio-Visual-Speech-Recognition-via-Matryoshka-Based-Multimodal-LLMs" class="headerlink" title="Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal   LLMs"></a>Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal   LLMs</h2><p><strong>Authors:Umberto Cappellazzo, Minsu Kim, Stavros Petridis</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) leverages audio and visual modalities to improve robustness in noisy environments. Recent advances in Large Language Models (LLMs) show strong performance in speech recognition, including AVSR. However, the long speech representations lead to high computational costs for LLMs. Prior methods compress inputs before feeding them to LLMs, but high compression often harms accuracy. To address this, we propose Llama-MTSK, the first Matryoshka-based Multimodal LLM for AVSR, which flexibly adapts audio-visual token allocation under varying compute constraints. Inspired by Matryoshka Representation Learning, our model encodes representations at multiple granularities with a single architecture, avoiding the need for separate models. For efficient fine-tuning, we introduce three LoRA-based strategies using global and scale-specific modules. Evaluations on major AVSR datasets show Llama-MTSK matches or outperforms models trained at fixed compression levels. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åˆ©ç”¨éŸ³é¢‘å’Œè§†è§‰æ¨¡å¼æ¥æé«˜å™ªå£°ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢çš„è¿›å±•ï¼ŒåŒ…æ‹¬AVSRåœ¨å†…çš„è¡¨ç°éƒ½ç›¸å½“å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¾ƒé•¿çš„è¯­éŸ³è¡¨ç¤ºç»™LLMå¸¦æ¥äº†è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚ä¹‹å‰çš„æ–¹æ³•åœ¨å°†è¾“å…¥é€å…¥LLMä¹‹å‰å…ˆè¿›è¡Œå‹ç¼©ï¼Œä½†é«˜å‹ç¼©å¾€å¾€ä¼šæŸå®³å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Llama-MTSKï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºMatryoshkaçš„AVSRå¤šæ¨¡æ€LLMï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸åŒçš„è®¡ç®—çº¦æŸä¸‹çµæ´»åœ°é€‚åº”è§†å¬ä»¤ç‰Œåˆ†é…ã€‚æˆ‘ä»¬çš„æ¨¡å‹å—åˆ°Matryoshkaè¡¨ç¤ºå­¦ä¹ çš„å¯å‘ï¼Œä½¿ç”¨å•ä¸€æ¶æ„å¯¹å¤šä¸ªç²’åº¦çš„è¡¨ç¤ºè¿›è¡Œç¼–ç ï¼Œé¿å…äº†éœ€è¦ä½¿ç”¨å•ç‹¬æ¨¡å‹çš„éœ€è¦ã€‚ä¸ºäº†å®ç°é«˜æ•ˆçš„å¾®è°ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§åŸºäºLoRAçš„ç­–ç•¥ï¼Œä½¿ç”¨å…¨å±€å’Œè§„æ¨¡ç‰¹å®šçš„æ¨¡å—ã€‚åœ¨ä¸»è¦çš„AVSRæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLlama-MTSKçš„è¡¨ç°ä¸åœ¨å›ºå®šå‹ç¼©çº§åˆ«ä¸‹è®­ç»ƒçš„æ¨¡å‹ç›¸åŒ¹é…æˆ–æ›´èƒœä¸€ç­¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06362v2">PDF</a> Accepted to IEEE ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºéŸ³é¢‘è§†è§‰å¤šæ¨¡æ€èåˆçš„è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æŠ€æœ¯èƒ½å¤Ÿæé«˜åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬AVSRã€‚ç„¶è€Œï¼Œç”±äºé•¿è¯­éŸ³è¡¨ç¤ºï¼ŒLLMçš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMatryoshkaçš„å¤šæ¨¡æ€LLMâ€”â€”Llama-MTSKï¼Œå®ƒå¯ä»¥åœ¨ä¸åŒçš„è®¡ç®—çº¦æŸä¸‹çµæ´»åœ°é€‚åº”éŸ³è§†é¢‘ä»¤ç‰Œåˆ†é…ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å•ä¸€æ¶æ„å¯¹å¤šä¸ªç²’åº¦è¿›è¡Œç¼–ç è¡¨ç¤ºï¼Œé¿å…äº†éœ€è¦å•ç‹¬æ¨¡å‹çš„éœ€æ±‚ã€‚é€šè¿‡å¼•å…¥ä¸‰ç§åŸºäºLoRAçš„ç­–ç•¥å¹¶ä½¿ç”¨å…¨å±€å’Œè§„æ¨¡ç‰¹å®šçš„æ¨¡å—ï¼Œæˆ‘ä»¬å®ç°äº†é«˜æ•ˆçš„å¾®è°ƒã€‚åœ¨ä¸»è¦çš„AVSRæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLlama-MTSKçš„è¡¨ç°ä¸åœ¨å›ºå®šå‹ç¼©çº§åˆ«ä¸‹è®­ç»ƒçš„æ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰å¤šæ¨¡æ€èåˆï¼ˆAVSRï¼‰èƒ½æé«˜å™ªå£°ç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«ç¨³å¥æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬AVSRã€‚</li>
<li>LLMåœ¨å¤„ç†é•¿è¯­éŸ³è¡¨ç¤ºæ—¶å­˜åœ¨é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>Llama-MTSKæ˜¯é¦–ä¸ªåŸºäºMatryoshkaçš„å¤šæ¨¡æ€LLMï¼Œå¯çµæ´»é€‚åº”ä¸åŒè®¡ç®—çº¦æŸä¸‹çš„éŸ³è§†é¢‘ä»¤ç‰Œåˆ†é…ã€‚</li>
<li>Llama-MTSKé€šè¿‡å•ä¸€æ¶æ„ç¼–ç å¤šä¸ªç²’åº¦è¡¨ç¤ºï¼Œæé«˜äº†æ•ˆç‡å¹¶é¿å…äº†å•ç‹¬æ¨¡å‹çš„éœ€æ±‚ã€‚</li>
<li>é€šè¿‡å¼•å…¥ä¸‰ç§åŸºäºLoRAçš„ç­–ç•¥ï¼Œå®ç°äº†é«˜æ•ˆçš„å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06362">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-943998b688ac51716496368d048f9076.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d87cff102d74195b8e8eb67089af3c61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-693739709bf001491feadd13640928f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd410f20da38ee5809cf4da3e3f3446b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-338bf2387308d86c1854a7f157755e15.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6a0bdfdf63cac948d1a54a7ecf6398fa.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7b7c1f511af472060dfdb9561c3eb649.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  BEVCon Advancing Bird's Eye View Perception with Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
