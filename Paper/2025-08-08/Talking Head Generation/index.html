<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-08-08  READ Real-time and Efficient Asynchronous Diffusion for Audio-driven   Talking Head Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ab74f3bbf577e8232663cb2b58f69c86.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-08-更新"><a href="#2025-08-08-更新" class="headerlink" title="2025-08-08 更新"></a>2025-08-08 更新</h1><h2 id="READ-Real-time-and-Efficient-Asynchronous-Diffusion-for-Audio-driven-Talking-Head-Generation"><a href="#READ-Real-time-and-Efficient-Asynchronous-Diffusion-for-Audio-driven-Talking-Head-Generation" class="headerlink" title="READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven   Talking Head Generation"></a>READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven   Talking Head Generation</h2><p><strong>Authors:Haotian Wang, Yuzhe Weng, Jun Du, Haoran Xu, Xiaoyan Wu, Shan He, Bing Yin, Cong Liu, Jianqing Gao, Qingfeng Liu</strong></p>
<p>The introduction of diffusion models has brought significant advances to the field of audio-driven talking head generation. However, the extremely slow inference speed severely limits the practical implementation of diffusion-based talking head generation models. In this study, we propose READ, the first real-time diffusion-transformer-based talking head generation framework. Our approach first learns a spatiotemporal highly compressed video latent space via a temporal VAE, significantly reducing the token count to accelerate generation. To achieve better audio-visual alignment within this compressed latent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to generate temporally compressed speech latent codes corresponding to the video latent space. These latent representations are then modeled by a carefully designed Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient talking head synthesis. Furthermore, to ensure temporal consistency and accelerated inference in extended generation, we propose a novel asynchronous noise scheduler (ANS) for both the training and inference process of our framework. The ANS leverages asynchronous add-noise and asynchronous motion-guided generation in the latent space, ensuring consistency in generated video clips. Experimental results demonstrate that READ outperforms state-of-the-art methods by generating competitive talking head videos with significantly reduced runtime, achieving an optimal balance between quality and speed while maintaining robust metric stability in long-time generation. </p>
<blockquote>
<p>扩散模型的引入为音频驱动说话人头部生成领域带来了显著进展。然而，极慢的推理速度严重限制了基于扩散的说话人头部生成模型的实际应用。在本研究中，我们提出了READ，这是第一个基于实时扩散变压器的说话人头部生成框架。我们的方法首先通过时间VAE学习一个时空高度压缩的视频潜在空间，显著减少令牌计数以加速生成。为了在这个压缩的潜在空间内实现更好的音频视觉对齐，我们提出了一个预训练的语音自动编码器（SpeechAE）来生成与视频潜在空间对应的时空压缩语音潜在代码。然后，这些潜在表示由一个精心设计的音频到视频扩散变压器（A2V-DiT）主干进行建模，以实现高效的说话人头部合成。此外，为了确保扩展生成的时序一致性和加速推理，我们为框架的训练和推理过程提出了新型异步噪声调度器（ANS）。ANS利用潜在空间中的异步添加噪声和异步运动引导生成，确保生成视频剪辑的一致性。实验结果表明，READ通过生成竞争性的说话人头部视频并显著减少运行时间，在质量和速度之间实现了最佳平衡，同时在长时间生成中保持了稳健的指标稳定性，从而优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03457v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://readportrait.github.io/READ/">https://readportrait.github.io/READ/</a></p>
<p><strong>摘要</strong><br>扩散模型的引入为音频驱动说话人头部生成领域带来了显著进展。然而，其极慢的推理速度严重限制了扩散式说话人头部生成模型的实际应用。本研究提出了READ，首个基于实时扩散转换器的说话人头部生成框架。该方法首先通过学习时空高度压缩的视频潜在空间，显著减少令牌计数以加速生成。为了在这个压缩的潜在空间内实现更好的音视频对齐，提出了预训练的语音自编码器（SpeechAE）来生成与视频潜在空间对应的时空压缩语音潜在代码。这些潜在表示由一个精心设计的音频到视频扩散转换器（A2V-DiT）主干进行建模，用于高效说话人头部合成。此外，为了确保扩展生成的时序一致性和加速推理，我们为框架的训练和推理过程提出了新型异步噪声调度器（ANS）。ANS利用潜在空间中的异步添加噪声和异步运动引导生成，确保生成视频剪辑的一致性。实验结果表明，READ优于现有最先进的方法，生成的说话人头部视频具有竞争力，运行时显著减少，在长时间生成的同时实现了质量与速度的平衡。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在音频驱动说话头生成领域有重大进展，但推理速度较慢。</li>
<li>READ框架是基于实时扩散转换器的首个说话头生成框架。</li>
<li>通过学习高度压缩的视频潜在空间来加速生成过程。</li>
<li>引入预训练的语音自编码器（SpeechAE）实现音视频对齐。</li>
<li>使用A2V-DiT主干进行音频到视频的潜在表示建模。</li>
<li>提出了新型的异步噪声调度器（ANS）以确保时序一致性和加速推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03457">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cea4ac58943d1469b0b0b971a253f758.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55eaad030f25cb408e70921fe126a57d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b0702d6c094573d77b34ada55ada9a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-856f85964694729e9e93554176648883.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43091295a367d13c9770e1f2688cc55a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-human-Interactive-Talking-Dataset"><a href="#Multi-human-Interactive-Talking-Dataset" class="headerlink" title="Multi-human Interactive Talking Dataset"></a>Multi-human Interactive Talking Dataset</h2><p><strong>Authors:Zeyu Zhu, Weijia Wu, Mike Zheng Shou</strong></p>
<p>Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: <a target="_blank" rel="noopener" href="https://github.com/showlab/Multi-human-Talking-Video-Dataset">https://github.com/showlab/Multi-human-Talking-Video-Dataset</a>. </p>
<blockquote>
<p>关于对话视频生成的研究目前主要集中在单人独白或孤立的面部动画上，这限制了其在现实多人互动中的应用。为了填补这一空白，我们推出了MIT（大规模数据集），它是专门为多人对话视频生成设计的。为此，我们开发了一个自动管道，用于收集和标注多人对话视频。该数据集包含12小时的高分辨率视频片段，每段视频都有两到四名发言人，详细标注了身体姿势和语音交互。它捕捉了多人场景中自然的对话动态，为研究交互视觉行为提供了丰富的资源。为了展示MIT的潜力，我们进一步提出了CovOG作为这一新任务的基线模型。它集成了多人姿态编码器（MPE），通过聚合个体姿态嵌入来处理不同数量的发言人，以及交互式音频驱动器（IAD），根据发言人特定的音频特征调整头部动态。这些组件共同展示了生成逼真多人对话视频的可行性和挑战，使MIT成为未来研究的重要基准。代码可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/showlab/Multi-human-Talking-Video-Dataset%E3%80%82">https://github.com/showlab/Multi-human-Talking-Video-Dataset。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03050v1">PDF</a> 9 pages, 4 figures, 4 tables</p>
<p><strong>Summary</strong><br>本文主要介绍了一个针对多人物对话视频生成的大型数据集MIT及其相关研究成果。该数据集包含高清晰度、多人物对话的视频片段，并对人物动作和语音交互进行了精细标注。为应对不同数量说话者的挑战，研究者提出了一种多人物姿态编码器和交互音频驱动模块的方法，为生成真实的多人物对话视频提供了可能。MIT数据集为未来的研究提供了宝贵的基准测试资源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了针对多人物对话视频生成的大型数据集MIT。</li>
<li>数据集包含高清晰度、多人物对话的视频片段，并对人物动作和语音交互进行了精细标注。</li>
<li>提出了一种多人物姿态编码器（MPE）的方法，能够处理不同数量的说话者。</li>
<li>通过聚合个体姿态嵌入，MPE能够处理多人物场景中的复杂动态。</li>
<li>介绍了交互音频驱动（IAD）模块，能够根据说话者的音频特征调节头部动态。</li>
<li>MIT数据集为未来研究提供了宝贵的基准测试资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4573199751ba0df6d0ba72f284a46cc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eec616d154c63c5759cec3ef9203e1b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-459352b704e9f57ef1b5aeaae6384006.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97327832595601e4f5c9b335b14a22b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-547279b2920d03e6eb99a2c0237d9ccc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="X-Actor-Emotional-and-Expressive-Long-Range-Portrait-Acting-from-Audio"><a href="#X-Actor-Emotional-and-Expressive-Long-Range-Portrait-Acting-from-Audio" class="headerlink" title="X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio"></a>X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio</h2><p><strong>Authors:Chenxu Zhang, Zenan Li, Hongyi Xu, You Xie, Xiaochen Zhao, Tianpei Gu, Guoxian Song, Xin Chen, Chao Liang, Jianwen Jiang, Linjie Luo</strong></p>
<p>We present X-Actor, a novel audio-driven portrait animation framework that generates lifelike, emotionally expressive talking head videos from a single reference image and an input audio clip. Unlike prior methods that emphasize lip synchronization and short-range visual fidelity in constrained speaking scenarios, X-Actor enables actor-quality, long-form portrait performance capturing nuanced, dynamically evolving emotions that flow coherently with the rhythm and content of speech. Central to our approach is a two-stage decoupled generation pipeline: an audio-conditioned autoregressive diffusion model that predicts expressive yet identity-agnostic facial motion latent tokens within a long temporal context window, followed by a diffusion-based video synthesis module that translates these motions into high-fidelity video animations. By operating in a compact facial motion latent space decoupled from visual and identity cues, our autoregressive diffusion model effectively captures long-range correlations between audio and facial dynamics through a diffusion-forcing training paradigm, enabling infinite-length emotionally-rich motion prediction without error accumulation. Extensive experiments demonstrate that X-Actor produces compelling, cinematic-style performances that go beyond standard talking head animations and achieves state-of-the-art results in long-range, audio-driven emotional portrait acting. </p>
<blockquote>
<p>我们提出了X-Actor，这是一种新型音频驱动肖像动画框架，它可以从单张参考图像和输入音频片段生成逼真、情感丰富的说话人头视频。不同于以往强调唇同步和有限场景下的短期视觉保真度的方法，X-Actor能够实现演员级别的长时间肖像表现，捕捉微妙、动态变化的情绪，与演讲的节奏和内容流畅地结合。我们的方法的核心是一个两阶段的解耦生成管道：一个受音频调节的自回归扩散模型，该模型在长时间上下文窗口中预测表情丰富的面部运动潜在令牌，这些令牌与身份无关；其次是基于扩散的视频合成模块，将这些运动转化为高保真视频动画。我们的自回归扩散模型在独立于视觉和身份线索的紧凑面部运动潜在空间中进行操作，通过扩散强制训练范式有效地捕捉音频和面部动态之间的长期相关性，实现无误差累积的无限长情感丰富运动预测。大量实验表明，X-Actor产生的表演引人入胜、具有电影风格，超越了标准说话人头动画，并在长程、音频驱动的情感肖像表演方面取得了最新结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02944v1">PDF</a> Project Page at <a target="_blank" rel="noopener" href="https://byteaigc.github.io/X-Actor/">https://byteaigc.github.io/X-Actor/</a></p>
<p><strong>Summary</strong></p>
<p>X-Actor是一种新型音频驱动肖像动画框架，可从单张参考图像和输入音频片段生成生动、情感表达流畅的说话人头视频。与以往侧重于同步和短距离视觉保真度的约束演讲场景不同，X-Actor能够捕捉演员级的长期肖像表现，展示与语音节奏和内容流畅变化的微妙情绪。其核心是一个两阶段解耦生成管道：一个音频调节的自回归扩散模型，在一个长期时间窗口内预测表情丰富的面部运动潜在令牌，随后是一个基于扩散的视频合成模块，将这些运动转化为高保真视频动画。该自回归扩散模型在独立于视觉和身份线索的紧凑面部运动潜在空间内运行，通过扩散强迫训练模式有效地捕获音频和面部动力之间的长期关联，实现了无误差累积的情感丰富运动预测。<strong>Key Takeaways</strong>:</p>
<ol>
<li>X-Actor是一个新的音频驱动肖像动画框架。</li>
<li>可以从单个参考图像和音频片段生成长时间动态的说话人头视频。</li>
<li>采用两阶段解耦生成管道：自回归扩散模型预测面部运动潜在令牌，然后是视频合成模块。</li>
<li>自回归扩散模型在独立于视觉和身份线索的面部运动潜在空间内运行。</li>
<li>通过扩散强迫训练模式实现音频与面部动力之间的长期关联。</li>
<li>X-Actor实现了情感丰富的运动预测，不会出现误差累积。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02944">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3039b4de951300cf37e03da1a2687dfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46975910ad76bd76841db680b9cfca52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f2eb6b10d04a3139ae5cddcdc6ed665.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab74f3bbf577e8232663cb2b58f69c86.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UITron-Speech-Towards-Automated-GUI-Agents-Based-on-Speech-Instructions"><a href="#UITron-Speech-Towards-Automated-GUI-Agents-Based-on-Speech-Instructions" class="headerlink" title="UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions"></a>UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions</h2><p><strong>Authors:Wenkang Han, Zhixiong Zeng, Jing Huang, Shu Jiang, Liming Zheng, Haibo Qiu, Chang Yao, Jingyuan Chen, Lin Ma</strong></p>
<p>Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing human-computer interaction, yet their reliance on text-based instructions imposes limitations on accessibility and convenience, particularly in hands-free scenarios. To address this issue, we propose replacing text with speech as the instruction input modality for GUI agents, and introduce UITron-Speech, which is the first end-to-end GUI agent capable of directly processing speech instructions and on-device screenshots to predict user actions. To tackle the problem of data scarcity, we synthesize high-quality speech instruction datasets using a random-speaker text-to-speech model. Additionally, we design a mixed-modality training strategy to mitigate the inherent modality imbalance in pre-trained foundation models. Furthermore, we conduct a statistical analysis of the distribution of GUI grounding prediction errors and propose a training-free two-step grounding refinement method to alleviate minor localization deviations. Extensive experiments on multiple benchmarks demonstrate that UITron-Speech achieves robust performance and superior adaptability, underscoring the feasibility and potential of speech-driven GUI agents for more accessible and intelligent human-computer interaction. Our code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/UITron-hub/UITron-Speech">https://github.com/UITron-hub/UITron-Speech</a>. </p>
<blockquote>
<p>图形用户界面（GUI）的自主代理正在彻底改变人机交互的方式，然而它们对文本指令的依赖限制了可访问性和便利性，特别是在免提场景中。为了解决这一问题，我们提出用语音替换文本作为GUI代理的指令输入模式，并引入了UITron-Speech。UITron-Speech是第一个能够直接处理语音指令和设备截图以预测用户操作的端到端GUI代理。为了解决数据稀缺的问题，我们使用随机说话人文本到语音模型合成高质量语音指令数据集。此外，我们设计了一种混合模式训练策略，以缓解预训练基础模型中固有的模式不平衡问题。此外，我们对GUI接地预测误差的分布进行了统计分析，并提出了一种无训练的两步接地细化方法，以缓解轻微的定位偏差。在多个基准测试上的广泛实验表明，UITron-Speech实现了稳健的性能和卓越的适应性，突出了语音驱动GUI代理在更可访问和智能的人机交互中的可行性和潜力。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/UITron-hub/UITron-Speech%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/UITron-hub/UITron-Speech获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11127v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于图形用户界面（GUI）的自主代理正在彻底改变人机交互方式，但它们依赖文本指令的方式对无障碍使用和便捷性造成了限制，特别是在免提场景中。为解决这一问题，我们提出用语音替代文本作为GUI代理的指令输入模式，并引入了UITron-Speech，这是首个能够直接处理语音指令和设备截图以预测用户行为的端到端GUI代理。我们利用随机说话人文本到语音模型合成高质量语音指令数据集以解决数据稀缺问题，并设计了一种混合模态训练策略来缓解预训练基础模型中的固有模态不平衡问题。此外，我们对GUI定位预测错误的分布进行了统计分析，并提出了无需训练的两步定位优化方法，以减轻轻微定位偏差。在多个基准测试上的实验表明，UITron-Speech具有强大的性能和卓越的适应性，突显了语音驱动GUI代理在实现更无障碍和智能人机交互方面的可行性潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>自主代理在GUI中改变了人机交互方式，但仍依赖文本指令，存在无障碍使用和便捷性的限制。</li>
<li>提出了UITron-Speech，能处理语音指令和设备截图以预测用户行为，是首个端到端的GUI代理。</li>
<li>通过合成高质量语音指令数据集和混合模态训练策略解决数据稀缺和模态不平衡问题。</li>
<li>分析了GUI定位预测错误的分布，并提出无需训练的两步定位优化方法减轻定位偏差。<br>*UITron-Speech在多个基准测试上表现出强大的性能和适应性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11127">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9d925f7e627b1850827fb794a2e767bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7ae03e93249d62cfd6e46704e8c5fb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-429bae2058c3eecbf568076778d782f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc2b64e1065e18c85952e6246c912b08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e80976aff71e49a4f1f3191236e77c6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d99ea0b0e91ec0093aa13abe7e7f6791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb9188fbea236ebf9918eb8aabff5c62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a80e0d36462396c20cbb75d4fc5c25d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Disentangle-Identity-Cooperate-Emotion-Correlation-Aware-Emotional-Talking-Portrait-Generation"><a href="#Disentangle-Identity-Cooperate-Emotion-Correlation-Aware-Emotional-Talking-Portrait-Generation" class="headerlink" title="Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional   Talking Portrait Generation"></a>Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional   Talking Portrait Generation</h2><p><strong>Authors:Weipeng Tan, Chuming Lin, Chengming Xu, FeiFan Xu, Xiaobin Hu, Xiaozhong Ji, Junwei Zhu, Chengjie Wang, Yanwei Fu</strong></p>
<p>Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audio’s inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose a novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop a disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identity-agnostic Gaussian distributions. Second, we introduce a correlation-enhanced emotion conditioning module with learnable Emotion Banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our method’s superiority, outperforming state-of-the-art approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our method’s ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities. </p>
<blockquote>
<p>在Talking Head Generation（THG）的最新进展中，通过扩散模型实现了令人印象深刻的唇同步和视觉质量。然而，现有方法在生成情感表达肖像时很难同时保留说话者的身份。我们确定了当前情感对话头像生成中的三个关键局限性：未能充分利用音频的内在情感线索、情感表示中的身份泄露以及情感关联的独立学习。为了应对这些挑战，我们提出了一个名为DICE-Talk的新型框架，它的理念是解开身份与情感的关系，然后与具有类似特征的情感进行合作。首先，我们开发了一个解开的情感嵌入器，它通过跨模态注意力联合建模音频-视觉情感线索，将情感表示为与身份无关的高斯分布。其次，我们引入了一个增强相关性的情感调节模块，该模块具有可学习的情感库，通过向量量化和基于注意力的特征聚合显式捕获相互间的情感关系。第三，我们设计了一个情感辨别目标，通过在潜在空间分类来强化扩散过程中的情感一致性。在MEAD和HDTF数据集上的大量实验证明了我们方法的优越性，在情感准确性方面超过了最先进的方法，同时保持了有竞争力的唇同步性能。定性和用户研究进一步证实了我们方法生成具有丰富、相关情感表达的、保留身份的肖像的能力，这些肖像能够自然地适应未见过的身份。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18087v2">PDF</a> Accepted by ACM MM’25. arXiv admin note: text overlap with   arXiv:2409.03270</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在Talking Head Generation（THG）领域的最新进展，指出了现有方法面临的挑战，如未能充分利用音频中的情感线索、身份和情感表示之间的混淆以及情感关联学习的孤立性。为解决这些问题，提出了一种名为DICE-Talk的新型框架，通过解耦身份和情感，然后合作具有相似特征的情感。该框架包括开发一个解耦情感嵌入器，一个增强情感调节的模块和一个情感鉴别目标。实验证明，该方法在情感准确性方面优于现有技术，同时保持竞争性的唇同步性能。定性结果和用户研究进一步证实了该方法在生成具有丰富情感表达和身份一致性的肖像方面的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前Talking Head Generation（THG）在情感表达方面存在挑战，如未能充分利用音频中的情感线索和身份与情感的混淆。</li>
<li>提出了一种新型框架DICE-Talk，旨在解决这些问题，通过解耦身份和情感，并合作具有相似特征的情感。</li>
<li>DICE-Talk包括一个解耦情感嵌入器，该嵌入器通过跨模态注意力联合建模音频视觉情感线索，并将情感表示为身份无关的高斯分布。</li>
<li>引入了一个增强情感调节的模块，通过向量量化和基于注意力的特征聚合显式捕获情感之间的关系。</li>
<li>设计了一个情感鉴别目标，通过潜在空间分类在扩散过程中强制执行情感一致性。</li>
<li>实验证明，DICE-Talk在情感准确性方面优于现有技术，同时保持竞争性的唇同步性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18087">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f737045a099641927a718cc4062e3830.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e90538fbd017b83e32a72a781b4fafdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-625c3fc30fc20fd72b0ce7bedb270bc3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Talking-to-DINO-Bridging-Self-Supervised-Vision-Backbones-with-Language-for-Open-Vocabulary-Segmentation"><a href="#Talking-to-DINO-Bridging-Self-Supervised-Vision-Backbones-with-Language-for-Open-Vocabulary-Segmentation" class="headerlink" title="Talking to DINO: Bridging Self-Supervised Vision Backbones with Language   for Open-Vocabulary Segmentation"></a>Talking to DINO: Bridging Self-Supervised Vision Backbones with Language   for Open-Vocabulary Segmentation</h2><p><strong>Authors:Luca Barsellotti, Lorenzo Bianchi, Nicola Messina, Fabio Carrara, Marcella Cornia, Lorenzo Baraldi, Fabrizio Falchi, Rita Cucchiara</strong></p>
<p>Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form textual concepts without predefined training classes. While existing vision-language models such as CLIP can generate segmentation masks by leveraging coarse spatial information from Vision Transformers, they face challenges in spatial localization due to their global alignment of image and text features. Conversely, self-supervised visual models like DINO excel in fine-grained visual encoding but lack integration with language. To bridge this gap, we present Talk2DINO, a novel hybrid approach that combines the spatial accuracy of DINOv2 with the language understanding of CLIP. Our approach aligns the textual embeddings of CLIP to the patch-level features of DINOv2 through a learned mapping function without the need to fine-tune the underlying backbones. At training time, we exploit the attention maps of DINOv2 to selectively align local visual patches with textual embeddings. We show that the powerful semantic and localization abilities of Talk2DINO can enhance the segmentation process, resulting in more natural and less noisy segmentations, and that our approach can also effectively distinguish foreground objects from the background. Experimental results demonstrate that Talk2DINO achieves state-of-the-art performance across several unsupervised OVS benchmarks. Source code and models are publicly available at: <a target="_blank" rel="noopener" href="https://lorebianchi98.github.io/Talk2DINO/">https://lorebianchi98.github.io/Talk2DINO/</a>. </p>
<blockquote>
<p>开放词汇分割（OVS）旨在从自由的文本概念中对图像进行分割，而无需预先定义的训练类别。虽然现有的视觉语言模型（如CLIP）可以利用视觉变压器的粗略空间信息生成分割掩码，但由于图像和文本特征的全局对齐，它们在空间定位方面面临挑战。相反，自监督的视觉模型（如DINO）在精细的视觉编码方面表现出色，但与语言的结合能力有待提高。为了弥补这一差距，我们提出了Talk2DINO，这是一种新型混合方法，结合了DINOv2的空间精度与CLIP的语言理解能力。我们的方法通过将CLIP的文本嵌入与DINOv2的补丁级别特征通过学习的映射函数对齐，而无需微调底层骨干网。在训练过程中，我们利用DINOv2的注意力图有选择地将局部视觉补丁与文本嵌入对齐。我们证明了Talk2DINO的强大语义和定位能力可以加强分割过程，产生更自然、更少噪声的分割结果，并且我们的方法还可以有效地区分前景物体和背景。实验结果表明，Talk2DINO在多个无监督OVS基准测试中达到了最先进的性能。源代码和模型可在：<a target="_blank" rel="noopener" href="https://lorebianchi98.github.io/Talk2DINO/%E5%85%AC%E5%BC%B9%E8%AE%BF%E9%A2%84%E3%80%82">https://lorebianchi98.github.io/Talk2DINO/公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19331v2">PDF</a> </p>
<p><strong>摘要</strong><br>    结合CLIP的语言理解和DINOv2的空间准确性，通过学得映射函数对齐文本嵌入与局部视觉特征，提出一种新颖的混合方法Talk2DINO，用于开放式词汇分割任务，提高语义理解和定位能力，实现更自然、少噪声的分割效果，并有效区分前景和背景物体。</p>
<p><strong>要点提炼</strong></p>
<ol>
<li>开放词汇分割（OVS）的目标是从自由形式的文本概念中对图像进行分割，无需预定义训练类别。</li>
<li>现有视觉语言模型如CLIP可利用粗略的空间信息生成分割掩膜，但面临空间定位的挑战。</li>
<li>自我监督的视觉模型如DINO擅长精细的视觉编码，但缺乏与语言的整合。</li>
<li>Talk2DINO结合了CLIP的语言理解与DINOv2的空间准确性，提高了语义理解和定位能力。</li>
<li>通过学得映射函数对齐CLIP的文本嵌入与DINOv2的局部视觉特征，无需微调底层架构。</li>
<li>利用DINOv2的注意力图选择性地对齐局部视觉块与文本嵌入。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19331">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-64e563a880dae7048765058f9cb1c31a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff399f8e80f16d58fbd6d321be7540cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-064c2bf9dcd7e6bd8c2c2f1ea8fb0eca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f1489e7c7078465d3c8141164f5232b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-23a46cc0e9ff42796efe41c51ffd474d.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-08  From MAS to MARS Coordination Failures and Reasoning Trade-offs in   Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-58aedc0d155305a9a8aed35a1556368d.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-08-06  ReMoMask Retrieval-Augmented Masked Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
