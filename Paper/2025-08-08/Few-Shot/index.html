<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  GraphProp Training the Graph Foundation Models using Graph Properties">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2c5a6b0010dcec3d6ae49bd08f7e4a51.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-08-æ›´æ–°"><a href="#2025-08-08-æ›´æ–°" class="headerlink" title="2025-08-08 æ›´æ–°"></a>2025-08-08 æ›´æ–°</h1><h2 id="GraphProp-Training-the-Graph-Foundation-Models-using-Graph-Properties"><a href="#GraphProp-Training-the-Graph-Foundation-Models-using-Graph-Properties" class="headerlink" title="GraphProp: Training the Graph Foundation Models using Graph Properties"></a>GraphProp: Training the Graph Foundation Models using Graph Properties</h2><p><strong>Authors:Ziheng Sun, Qi Feng, Lehao Lin, Chris Ding, Jicong Fan</strong></p>
<p>This work focuses on training graph foundation models (GFMs) that have strong generalization ability in graph-level tasks such as graph classification. Effective GFM training requires capturing information consistent across different domains. We discover that graph structures provide more consistent cross-domain information compared to node features and graph labels. However, traditional GFMs primarily focus on transferring node features from various domains into a unified representation space but often lack structural cross-domain generalization. To address this, we introduce GraphProp, which emphasizes structural generalization. The training process of GraphProp consists of two main phases. First, we train a structural GFM by predicting graph invariants. Since graph invariants are properties of graphs that depend only on the abstract structure, not on particular labellings or drawings of the graph, this structural GFM has a strong ability to capture the abstract structural information and provide discriminative graph representations comparable across diverse domains. In the second phase, we use the representations given by the structural GFM as positional encodings to train a comprehensive GFM. This phase utilizes domain-specific node attributes and graph labels to further improve cross-domain node feature generalization. Our experiments demonstrate that GraphProp significantly outperforms the competitors in supervised learning and few-shot learning, especially in handling graphs without node attributes. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹ç ”ç©¶å›¾åŸºç¡€æ¨¡å‹ï¼ˆGFMï¼‰çš„è®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨å›¾çº§ä»»åŠ¡ï¼ˆå¦‚å›¾åˆ†ç±»ï¼‰ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ‰æ•ˆçš„GFMè®­ç»ƒéœ€è¦æ•è·ä¸åŒé¢†åŸŸé—´ä¸€è‡´çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸èŠ‚ç‚¹ç‰¹å¾å’Œå›¾æ ‡ç­¾ç›¸æ¯”ï¼Œå›¾ç»“æ„æä¾›äº†æ›´ä¸€è‡´çš„çš„è·¨åŸŸä¿¡æ¯ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„GFMä¸»è¦å…³æ³¨å°†ä¸åŒé¢†åŸŸçš„èŠ‚ç‚¹ç‰¹å¾è½¬ç§»åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ï¼Œä½†å¾€å¾€ç¼ºä¹ç»“æ„åŒ–çš„è·¨åŸŸæ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GraphPropï¼Œå®ƒå¼ºè°ƒç»“æ„åŒ–çš„æ³›åŒ–ã€‚GraphPropçš„è®­ç»ƒè¿‡ç¨‹ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡é¢„æµ‹å›¾ä¸å˜é‡æ¥è®­ç»ƒä¸€ä¸ªç»“æ„åŒ–çš„GFMã€‚ç”±äºå›¾ä¸å˜é‡æ˜¯ä»…ä¾èµ–äºæŠ½è±¡ç»“æ„ï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šçš„æ ‡æ³¨æˆ–å›¾çš„ç»˜åˆ¶çš„å›¾çš„å±æ€§ï¼Œå› æ­¤è¿™ç§ç»“æ„åŒ–çš„GFMå…·æœ‰å¾ˆå¼ºçš„æ•è·æŠ½è±¡ç»“æ„ä¿¡æ¯çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæä¾›è·¨ä¸åŒé¢†åŸŸçš„å¯æ¯”è¾ƒçš„å›¾è¡¨ç¤ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ç»“æ„åŒ–GFMç»™å‡ºçš„è¡¨ç¤ºä½œä¸ºä½ç½®ç¼–ç æ¥è®­ç»ƒä¸€ä¸ªå…¨é¢çš„GFMã€‚è¿™ä¸€é˜¶æ®µåˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„èŠ‚ç‚¹å±æ€§å’Œå›¾æ ‡ç­¾æ¥è¿›ä¸€æ­¥æé«˜è·¨åŸŸèŠ‚ç‚¹ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒGraphPropåœ¨ç›‘ç£å­¦ä¹ å’Œå°æ ·æœ¬å­¦ä¹ ä¸­éƒ½æ˜¾è‘—ä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ— èŠ‚ç‚¹å±æ€§çš„å›¾æ—¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04594v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬å…³æ³¨äºè®­ç»ƒå…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„å›¾åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰ï¼Œç”¨äºæ‰§è¡Œå¦‚å›¾åˆ†ç±»ç­‰å›¾çº§ä»»åŠ¡ã€‚æ–‡ä¸­æŒ‡å‡ºï¼Œä¼ ç»ŸGFMsä¸»è¦å…³æ³¨å°†èŠ‚ç‚¹ç‰¹å¾ä»å„ä¸ªé¢†åŸŸè½¬ç§»åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ï¼Œä½†å¾€å¾€ç¼ºä¹ç»“æ„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ä¸­å¼•å…¥äº†GraphPropï¼Œå¼ºè°ƒç»“æ„æ³›åŒ–ã€‚GraphPropçš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆé€šè¿‡é¢„æµ‹å›¾çš„ä¸å˜é‡æ¥è®­ç»ƒç»“æ„GFMï¼Œä»¥æ•è·æŠ½è±¡ç»“æ„ä¿¡æ¯ï¼›ç„¶åä½¿ç”¨ç»“æ„GFMæä¾›çš„è¡¨ç¤ºä½œä¸ºä½ç½®ç¼–ç æ¥è®­ç»ƒå…¨é¢çš„GFMï¼Œè¿›ä¸€æ­¥æé«˜è·¨åŸŸèŠ‚ç‚¹ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒGraphPropåœ¨ç›‘ç£å­¦ä¹ å’Œå°æ ·æœ¬å­¦ä¹ ä¸­æ˜¾è‘—ä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œå°¤å…¶åœ¨å¤„ç†æ— èŠ‚ç‚¹å±æ€§çš„å›¾æ—¶è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä½œé‡ç‚¹ï¼šè®­ç»ƒå…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„å›¾åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰ï¼Œç”¨äºå›¾çº§ä»»åŠ¡å¦‚å›¾åˆ†ç±»ã€‚</li>
<li>ä¼ ç»ŸGFMså­˜åœ¨çš„é—®é¢˜ï¼šä¸»è¦å…³æ³¨èŠ‚ç‚¹ç‰¹å¾çš„è·¨åŸŸè½¬ç§»ï¼Œä½†ç¼ºä¹ç»“æ„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>GraphPropçš„å¼•å…¥ï¼šå¼ºè°ƒç»“æ„æ³›åŒ–ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µï¼šé€šè¿‡é¢„æµ‹å›¾çš„ä¸å˜é‡æ¥è®­ç»ƒç»“æ„GFMï¼Œä»¥æ•è·æŠ½è±¡ç»“æ„ä¿¡æ¯ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ç»“æ„GFMæä¾›çš„è¡¨ç¤ºä½œä¸ºä½ç½®ç¼–ç æ¥è®­ç»ƒå…¨é¢çš„GFMï¼Œè¿›ä¸€æ­¥æé«˜è·¨åŸŸèŠ‚ç‚¹ç‰¹å¾çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœï¼šGraphPropåœ¨ç›‘ç£å­¦ä¹ å’Œå°æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ— èŠ‚ç‚¹å±æ€§çš„å›¾æ—¶æ•ˆæœçªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6caa164cc633ca8d81f211058f4d7811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-310193917190728c2cc507f9064f46d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3295dd4d14996270e11ef4ac8e646439.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ProtoN-Prototype-Node-Graph-Neural-Network-for-Unconstrained-Multi-Impression-Ear-Recognition"><a href="#ProtoN-Prototype-Node-Graph-Neural-Network-for-Unconstrained-Multi-Impression-Ear-Recognition" class="headerlink" title="ProtoN: Prototype Node Graph Neural Network for Unconstrained   Multi-Impression Ear Recognition"></a>ProtoN: Prototype Node Graph Neural Network for Unconstrained   Multi-Impression Ear Recognition</h2><p><strong>Authors:Santhoshkumar Peddi, Sadhvik Bathini, Arun Balasubramanian, Monalisa Sarma, Debasis Samanta</strong></p>
<p>Ear biometrics offer a stable and contactless modality for identity recognition, yet their effectiveness remains limited by the scarcity of annotated data and significant intra-class variability. Existing methods typically extract identity features from individual impressions in isolation, restricting their ability to capture consistent and discriminative representations. To overcome these limitations, a few-shot learning framework, ProtoN, is proposed to jointly process multiple impressions of an identity using a graph-based approach. Each impression is represented as a node in a class-specific graph, alongside a learnable prototype node that encodes identity-level information. This graph is processed by a Prototype Graph Neural Network (PGNN) layer, specifically designed to refine both impression and prototype representations through a dual-path message-passing mechanism. To further enhance discriminative power, the PGNN incorporates a cross-graph prototype alignment strategy that improves class separability by enforcing intra-class compactness while maintaining inter-class distinction. Additionally, a hybrid loss function is employed to balance episodic and global classification objectives, thereby improving the overall structure of the embedding space. Extensive experiments on five benchmark ear datasets demonstrate that ProtoN achieves state-of-the-art performance, with Rank-1 identification accuracy of up to 99.60% and an Equal Error Rate (EER) as low as 0.025, showing the effectiveness for few-shot ear recognition under limited data conditions. </p>
<blockquote>
<p>è€³éƒ¨ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯æä¾›äº†ä¸€ç§ç¨³å®šä¸”æ— æ¥è§¦çš„èº«ä»½è¯†åˆ«æ–¹å¼ï¼Œç„¶è€Œï¼Œå…¶æœ‰æ•ˆæ€§ä»ç„¶å—åˆ°æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œç±»å†…å˜åŒ–æ˜¾è‘—çš„å½±å“ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å•ç‹¬ä»ä¸ªåˆ«å°è±¡ä¸­æå–èº«ä»½ç‰¹å¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰ä¸€è‡´å’Œåˆ¤åˆ«æ€§è¡¨å¾çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§å°æ ·å­¦ä¹ æ¡†æ¶ProtoNï¼Œé‡‡ç”¨åŸºäºå›¾çš„æ–¹æ³•è”åˆå¤„ç†èº«ä»½çš„å¤šé‡å°è±¡ã€‚æ¯ä¸ªå°è±¡è¢«è¡¨ç¤ºä¸ºç‰¹å®šç±»çš„å›¾ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä»¥åŠä¸€ä¸ªå¯å­¦ä¹ çš„åŸå‹èŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹ç¼–ç èº«ä»½çº§ä¿¡æ¯ã€‚è¿™ä¸ªå›¾é€šè¿‡ä¸“é—¨è®¾è®¡çš„åŸå‹å›¾ç¥ç»ç½‘ç»œï¼ˆPGNNï¼‰å±‚è¿›è¡Œå¤„ç†ï¼Œé€šè¿‡åŒè·¯å¾„æ¶ˆæ¯ä¼ é€’æœºåˆ¶æ¥ä¼˜åŒ–å°è±¡å’ŒåŸå‹è¡¨å¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åˆ¤åˆ«åŠ›ï¼ŒPGNNé‡‡ç”¨è·¨å›¾åŸå‹å¯¹é½ç­–ç•¥ï¼Œé€šè¿‡å¼ºåˆ¶ç±»å†…ç´§å‡‘æ€§å’Œä¿æŒç±»é—´åŒºåˆ«æ¥æé«˜ç±»å¯åˆ†æ€§ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†ä¸€ç§æ··åˆæŸå¤±å‡½æ•°æ¥å¹³è¡¡æƒ…æ™¯å’Œå…¨å±€åˆ†ç±»ç›®æ ‡ï¼Œä»è€Œæ”¹å–„åµŒå…¥ç©ºé—´çš„æ•´ä½“ç»“æ„ã€‚åœ¨äº”ä¸ªåŸºå‡†è€³éƒ¨æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒProtoNè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒRank-1è¯†åˆ«ç‡é«˜è¾¾99.60%ï¼Œç­‰è¯¯ç‡ï¼ˆEERï¼‰ä½è‡³0.025%ï¼Œåœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹è¿›è¡Œå°‘æ ·æœ¬è€³éƒ¨è¯†åˆ«æ—¶æ•ˆæœæ˜¾è‘—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04381v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œçš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ProtoNï¼Œç”¨äºè€³éƒ¨ç”Ÿç‰©è¯†åˆ«ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè”åˆå¤„ç†åŒä¸€èº«ä»½çš„å¤šæ¡å°è±¡æ•°æ®ï¼Œé€šè¿‡æ„å»ºç±»ç‰¹å®šå›¾ï¼Œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå±‚ä¼˜åŒ–å°è±¡å’ŒåŸå‹è¡¨ç¤ºã€‚åŒæ—¶é‡‡ç”¨è·¨å›¾åŸå‹å¯¹é½ç­–ç•¥ï¼Œæé«˜ç±»å†…ç´§å‡‘æ€§å’Œç±»é—´åŒºåˆ†åº¦ã€‚åœ¨äº”ä¸ªåŸºå‡†è€³éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒProtoNå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒRank-1è¯†åˆ«å‡†ç¡®ç‡é«˜è¾¾99.6%ï¼Œæœ€ä½ç­‰è¯¯ç‡ï¼ˆEERï¼‰è¾¾åˆ°0.025%ï¼Œåœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹å®ç°äº†æœ‰æ•ˆçš„å°‘æ ·æœ¬è€³éƒ¨è¯†åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ProtoNæ˜¯ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œçš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè€³éƒ¨ç”Ÿç‰©è¯†åˆ«ã€‚</li>
<li>å®ƒé€šè¿‡æ„å»ºç±»ç‰¹å®šå›¾ï¼Œè”åˆå¤„ç†åŒä¸€èº«ä»½çš„å¤šæ¡å°è±¡æ•°æ®ã€‚</li>
<li>ProtoNä½¿ç”¨å›¾ç¥ç»ç½‘ç»œå±‚ä¼˜åŒ–å°è±¡å’ŒåŸå‹è¡¨ç¤ºï¼Œæé«˜è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>è·¨å›¾åŸå‹å¯¹é½ç­–ç•¥ç”¨äºæé«˜ç±»å†…ç´§å‡‘æ€§å’Œç±»é—´åŒºåˆ†åº¦ã€‚</li>
<li>åœ¨äº”ä¸ªåŸºå‡†è€³éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒProtoNå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ProtoNçš„Rank-1è¯†åˆ«å‡†ç¡®ç‡é«˜è¾¾99.6%ï¼Œæœ€ä½ç­‰è¯¯ç‡ï¼ˆEERï¼‰è¾¾åˆ°0.025%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c91c8e039507fd8f0727551fbfa10976.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30ba326a5fedd3ca2075813d1d42d30d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e86017ebbecba87a21eaee0abfcff20a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9bb99520a788c46345c13c7c9e298d3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="T3Time-Tri-Modal-Time-Series-Forecasting-via-Adaptive-Multi-Head-Alignment-and-Residual-Fusion"><a href="#T3Time-Tri-Modal-Time-Series-Forecasting-via-Adaptive-Multi-Head-Alignment-and-Residual-Fusion" class="headerlink" title="T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head   Alignment and Residual Fusion"></a>T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head   Alignment and Residual Fusion</h2><p><strong>Authors:Abdul Monaf Chowdhury, Rabeya Akter, Safaeid Hossain Arib</strong></p>
<p>Multivariate time series forecasting (MTSF) seeks to model temporal dynamics among variables to predict future trends. Transformer-based models and large language models (LLMs) have shown promise due to their ability to capture long-range dependencies and patterns. However, current methods often rely on rigid inductive biases, ignore intervariable interactions, or apply static fusion strategies that limit adaptability across forecast horizons. These limitations create bottlenecks in capturing nuanced, horizon-specific relationships in time-series data. To solve this problem, we propose T3Time, a novel trimodal framework consisting of time, spectral, and prompt branches, where the dedicated frequency encoding branch captures the periodic structures along with a gating mechanism that learns prioritization between temporal and spectral features based on the prediction horizon. We also proposed a mechanism which adaptively aggregates multiple cross-modal alignment heads by dynamically weighting the importance of each head based on the features. Extensive experiments on benchmark datasets demonstrate that our model consistently outperforms state-of-the-art baselines, achieving an average reduction of 3.28% in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in few-shot learning settings: with 5% training data, we see a reduction in MSE and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98% on average. Code - <a target="_blank" rel="noopener" href="https://github.com/monaf-chowdhury/T3Time/">https://github.com/monaf-chowdhury/T3Time/</a> </p>
<blockquote>
<p>å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆMTSFï¼‰æ—¨åœ¨å»ºç«‹å˜é‡ä¹‹é—´çš„æ—¶é—´åŠ¨æ€æ¨¡å‹ï¼Œä»¥é¢„æµ‹æœªæ¥è¶‹åŠ¿ã€‚åŸºäºTransformerçš„æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶æ•æ‰é•¿æœŸä¾èµ–å’Œæ¨¡å¼çš„èƒ½åŠ›è€Œæ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ç»å¸¸ä¾èµ–åƒµåŒ–çš„å½’çº³åè§ï¼Œå¿½ç•¥å˜é‡é—´çš„ç›¸äº’ä½œç”¨ï¼Œæˆ–é‡‡ç”¨é™æ€èåˆç­–ç•¥ï¼Œè¿™é™åˆ¶äº†åœ¨ä¸åŒé¢„æµ‹æœŸé™å†…çš„é€‚åº”æ€§ã€‚è¿™äº›å±€é™æ€§å¯¼è‡´åœ¨æ•æ‰æ—¶é—´åºåˆ—æ•°æ®ä¸­ç»†å¾®ã€ç‰¹å®šæ—¶æœŸçš„å¤æ‚å…³ç³»æ—¶å­˜åœ¨ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†T3Timeï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ä¸‰æ¨¡æ€æ¡†æ¶ï¼ŒåŒ…æ‹¬æ—¶é—´ã€å…‰è°±å’Œæç¤ºåˆ†æ”¯ï¼Œå…¶ä¸­ä¸“ç”¨çš„é¢‘ç‡ç¼–ç åˆ†æ”¯æ•æ‰å‘¨æœŸæ€§ç»“æ„ï¼Œä»¥åŠä¸€ä¸ªåŸºäºé¢„æµ‹æœŸé™å­¦ä¹ æ—¶é—´å’Œå…‰è°±ç‰¹å¾ä¹‹é—´ä¼˜å…ˆçº§çš„é—¨æ§æœºåˆ¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€æƒè¡¡æ¯ä¸ªå¤´çš„é‡è¦æ€§æ¥è‡ªé€‚åº”åœ°èšåˆå¤šä¸ªè·¨æ¨¡æ€å¯¹é½å¤´ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œåœ¨MSEå’ŒMAEä¸Šå¹³å‡é™ä½äº†3.28%å’Œ2.29%ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å°æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼šä½¿ç”¨5%çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬çœ‹åˆ°MSEå’ŒMAEåˆ†åˆ«å¹³å‡é™ä½äº†4.13%å’Œ1.91%ï¼›ä½¿ç”¨10%çš„æ•°æ®ï¼Œé™ä½äº†3.62%å’Œ1.98%ã€‚ä»£ç åœ°å€ï¼š[<a target="_blank" rel="noopener" href="https://github.com/monaf-chowdhury/T3Time/]">https://github.com/monaf-chowdhury/T3Time/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04251v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆMTSFï¼‰é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°å‹çš„ä¸‰æ¨¡æ€æ¡†æ¶T3Timeã€‚è¯¥æ¡†æ¶é€šè¿‡æ—¶é—´ã€å…‰è°±å’Œæç¤ºåˆ†æ”¯æ•æ‰æ—¶é—´åºåˆ—æ•°æ®çš„å‘¨æœŸæ€§ç»“æ„å’Œæ—¶é—´åŠ¨æ€ï¼Œå¹¶è‡ªé€‚åº”åœ°èšåˆè·¨æ¨¡æ€å¯¹é½å¤´ã€‚å®éªŒè¡¨æ˜ï¼ŒT3Timeåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—å‡å°‘äº†å¹³å‡è¯¯å·®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T3Timeæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆMTSFï¼‰çš„ä¸‰æ¨¡æ€æ¡†æ¶ï¼ŒåŒ…å«æ—¶é—´ã€å…‰è°±å’Œæç¤ºåˆ†æ”¯ã€‚</li>
<li>T3Timeé€šè¿‡é¢‘ç‡ç¼–ç åˆ†æ”¯æ•æ‰æ•°æ®çš„å‘¨æœŸæ€§ç»“æ„ï¼Œå¹¶æœ‰ä¸€ä¸ªé—¨æ§æœºåˆ¶æ ¹æ®é¢„æµ‹è§†é‡å­¦ä¹ æ—¶é—´ç‰¹å¾å’Œå…‰è°±ç‰¹å¾ä¹‹é—´çš„ä¼˜å…ˆçº§ã€‚</li>
<li>T3Timeè‡ªé€‚åº”åœ°èšåˆå¤šä¸ªè·¨æ¨¡æ€å¯¹é½å¤´ï¼Œæ ¹æ®ç‰¹å¾åŠ¨æ€è°ƒæ•´æ¯ä¸ªå¤´çš„é‡è¦æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒT3Timeåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œç›¸è¾ƒäºå…¶ä»–æœ€æ–°æŠ€æœ¯åŸºçº¿ï¼Œå¹³å‡å‡å°‘äº†MSEå’ŒMAEã€‚</li>
<li>T3Timeåœ¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿ç”¨5%å’Œ10%çš„æ•°æ®åˆ†åˆ«é™ä½äº†MSEå’ŒMAEã€‚</li>
<li>T3Timeçš„ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ–°çš„ã€æœ‰æ•ˆçš„æ¨¡å‹å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53d5972e9ee36b6626497e28e5bbfe5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33af578bcc30513e206fcf9d726b43c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39b354488e228a83782f06e77d16ca98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90a5c48188b6c2b93ac8d96023dee2d3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UniFGVC-Universal-Training-Free-Few-Shot-Fine-Grained-Vision-Classification-via-Attribute-Aware-Multimodal-Retrieval"><a href="#UniFGVC-Universal-Training-Free-Few-Shot-Fine-Grained-Vision-Classification-via-Attribute-Aware-Multimodal-Retrieval" class="headerlink" title="UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision   Classification via Attribute-Aware Multimodal Retrieval"></a>UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision   Classification via Attribute-Aware Multimodal Retrieval</h2><p><strong>Authors:Hongyu Guo, Kuan Zhu, Xiangzhao Hao, Haiyun Guo, Ming Tang, Jinqiao Wang</strong></p>
<p>Few-shot fine-grained visual classification (FGVC) aims to leverage limited data to enable models to discriminate subtly distinct categories. Recent works mostly finetuned the pre-trained visual language models to achieve performance gain, yet suffering from overfitting and weak generalization. To deal with this, we introduce UniFGVC, a universal training-free framework that reformulates few-shot FGVC as multimodal retrieval. First, we propose the Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the open-world knowledge of multimodal large language models (MLLMs) to generate a structured text description that captures the fine-grained attribute features distinguishing closely related classes. CDV-Captioner uses chain-of-thought prompting and visually similar reference images to reduce hallucination and enhance discrimination of generated captions. Using it we can convert each image into an image-description pair, enabling more comprehensive feature representation, and construct the multimodal category templates using few-shot samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and text encoders embed query and template pairs, and FGVC is accomplished by retrieving the nearest template in the joint space. UniFGVC ensures broad compatibility with diverse MLLMs and encoders, offering reliable generalization and adaptability across few-shot FGVC scenarios. Extensive experiments on 12 FGVC benchmarks demonstrate its consistent superiority over prior few-shot CLIP-based methods and even several fully-supervised MLLMs-based approaches. </p>
<blockquote>
<p>å°‘é‡ç²¾ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰æ—¨åœ¨åˆ©ç”¨æœ‰é™æ•°æ®ä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†ç»†å¾®ä¸åŒçš„ç±»åˆ«ã€‚è¿‘æœŸçš„å·¥ä½œå¤§å¤šå¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°æ€§èƒ½æå‡ï¼Œä½†ä»å­˜åœ¨è¿‡æ‹Ÿåˆå’Œæ³›åŒ–èƒ½åŠ›å¼±çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniFGVCï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œå®ƒå°†å°‘é‡ç²¾ç»†ç²’åº¦è§†è§‰åˆ†ç±»é‡æ–°å®šä¹‰ä¸ºå¤šæ¨¡æ€æ£€ç´¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ç±»åˆ«åˆ¤åˆ«è§†è§‰æè¿°å™¨ï¼ˆCDV-Captionerï¼‰ï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾ä¸–ç•ŒçŸ¥è¯†ï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–æ–‡æœ¬æè¿°ï¼Œè¯¥æè¿°æ•æ‰äº†åŒºåˆ†å¯†åˆ‡ç›¸å…³ç±»åˆ«çš„ç²¾ç»†ç²’åº¦å±æ€§ç‰¹å¾ã€‚CDV-Captionerä½¿ç”¨æ€ç»´é“¾æç¤ºå’Œè§†è§‰ç›¸ä¼¼çš„å‚è€ƒå›¾åƒï¼Œä»¥å‡å°‘å¹»è§‰ï¼Œå¢å¼ºç”Ÿæˆæè¿°çš„è¾¨åˆ«èƒ½åŠ›ã€‚ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¯å¼ å›¾åƒè½¬æ¢ä¸ºå›¾åƒæè¿°å¯¹ï¼Œå®ç°æ›´å…¨é¢çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å°‘é‡æ ·æœ¬æ„å»ºå¤šæ¨¡æ€ç±»åˆ«æ¨¡æ¿ï¼Œç”¨äºåç»­æ£€ç´¢æµç¨‹ã€‚ç„¶åï¼Œç°æˆçš„è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨åµŒå…¥æŸ¥è¯¢å’Œæ¨¡æ¿å¯¹ï¼Œé€šè¿‡æ£€ç´¢è”åˆç©ºé—´ä¸­çš„æœ€è¿‘æ¨¡æ¿æ¥å®ŒæˆFGVCã€‚UniFGVCç¡®ä¿ä¸å¤šç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œç¼–ç å™¨å¹¿æ³›å…¼å®¹ï¼Œåœ¨å°‘é‡ç²¾ç»†ç²’åº¦è§†è§‰åˆ†ç±»åœºæ™¯ä¸­æä¾›å¯é çš„ä¸€èˆ¬åŒ–å’Œé€‚åº”æ€§ã€‚åœ¨12ä¸ªFGVCåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨åŸºäºCLIPçš„å°‘é‡æ–¹æ³•ä»¥åŠåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®Œå…¨ç›‘ç£æ–¹æ³•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04136v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å°‘æ ·æœ¬ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰é—®é¢˜çš„UniFGVCæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥CDV-Captionerï¼Œå°†å›¾åƒè½¬æ¢ä¸ºå›¾åƒæè¿°å¯¹ï¼Œæ„å»ºæ¨¡æ€ç±»åˆ«æ¨¡æ¿ï¼Œæé«˜æ¨¡å‹å¯¹ç»†å¾®å·®åˆ«ç±»åˆ«çš„è¾¨åˆ«èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒUniFGVCåœ¨å¤šç§ç»†ç²’åº¦è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºåŸºäºCLIPçš„å°‘æ ·æœ¬æ–¹æ³•å’ŒæŸäº›å®Œå…¨ç›‘ç£çš„MLLMsæ–¹æ³•çš„ä¸€è‡´æ€§èƒ½ã€‚æ¡†æ¶è®¾è®¡å…·æœ‰å¹¿æ³›å…¼å®¹æ€§å’Œé€‚åº”æ€§ï¼Œèƒ½åœ¨ä¸åŒçš„å°‘æ ·æœ¬ç»†ç²’åº¦è§†è§‰åˆ†ç±»åœºæ™¯ä¸­å¯é åœ°æ¨å¹¿å’Œåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬ä¸­çš„å…³é”®è§è§£è¦ç‚¹ï¼š</p>
<ul>
<li>UniFGVCæ˜¯ä¸€ä¸ªé€šç”¨çš„è®­ç»ƒå…è´¹æ¡†æ¶ï¼Œç”¨äºè§£å†³å°‘æ ·æœ¬ç»†ç²’åº¦è§†è§‰åˆ†ç±»é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥CDV-Captionerï¼Œåˆ©ç”¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾ä¸–ç•ŒçŸ¥è¯†ç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬æè¿°ï¼Œæ•æ‰ç»†å¾®å·®åˆ«ç±»åˆ«çš„ç‰¹å¾ã€‚</li>
<li>CDV-Captionerä½¿ç”¨é“¾å¼æ€ç»´æç¤ºå’Œè§†è§‰ç›¸ä¼¼å‚è€ƒå›¾åƒæ¥å‡å°‘è™šæ„ç°è±¡å¹¶å¢å¼ºç”Ÿæˆçš„æ ‡é¢˜çš„è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>UniFGVCå°†å›¾åƒè½¬æ¢ä¸ºå›¾åƒæè¿°å¯¹ï¼Œæ„å»ºæ¨¡æ€ç±»åˆ«æ¨¡æ¿ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨ç°æˆçš„è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨åµŒå…¥æŸ¥è¯¢å’Œæ¨¡æ¿å¯¹ï¼Œé€šè¿‡æ£€ç´¢æœ€è¿‘çš„æ¨¡æ¿æ¥å®Œæˆç»†ç²’åº¦è§†è§‰åˆ†ç±»ã€‚</li>
<li>UniFGVCæ¡†æ¶å…·æœ‰å¹¿æ³›çš„å…¼å®¹æ€§å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„å°‘æ ·æœ¬ç»†ç²’åº¦è§†è§‰åˆ†ç±»åœºæ™¯ä¸­å¯é åœ°æ¨å¹¿å’Œåº”ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8773a27a857f023282ba5d2ab92e40c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a059333dfb8e9398fa4c476634b07c41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1599e93f44a1ebbbd957e371d9ec1739.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Fine-tuning-for-Better-Few-Shot-Prompting-An-Empirical-Comparison-for-Short-Answer-Grading"><a href="#Fine-tuning-for-Better-Few-Shot-Prompting-An-Empirical-Comparison-for-Short-Answer-Grading" class="headerlink" title="Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for   Short Answer Grading"></a>Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for   Short Answer Grading</h2><p><strong>Authors:Joel Walsh, Siddarth Mamidanna, Benjamin Nye, Mark Core, Daniel Auerbach</strong></p>
<p>Research to improve Automated Short Answer Grading has recently focused on Large Language Models (LLMs) with prompt engineering and no- or few-shot prompting to achieve best results. This is in contrast to the fine-tuning approach, which has historically required large-scale compute clusters inaccessible to most users. New closed-model approaches such as OpenAIâ€™s fine-tuning service promise results with as few as 100 examples, while methods using open weights such as quantized low-rank adaptive (QLORA) can be used to fine-tune models on consumer GPUs. We evaluate both of these fine-tuning methods, measuring their interaction with few-shot prompting for automated short answer grading (ASAG) with structured (JSON) outputs. Our results show that finetuning with small amounts of data has limited utility for Llama open-weight models, but that fine-tuning methods can outperform few-shot baseline instruction-tuned LLMs for OpenAIâ€™s closed models. While our evaluation set is limited, we find some evidence that the observed benefits of finetuning may be impacted by the domain subject matter. Lastly, we observed dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by seeding the initial training examples with a significant amount of cheaply generated synthetic training data. </p>
<blockquote>
<p>å…³äºæé«˜è‡ªåŠ¨ç®€ç­”é¢˜è¯„åˆ†çš„ç ”ç©¶æœ€è¿‘ä¸»è¦é›†ä¸­åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šï¼Œé€šè¿‡æç¤ºå·¥ç¨‹å’Œé›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æç¤ºæ¥è·å¾—æœ€ä½³ç»“æœã€‚è¿™ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å½¢æˆå¯¹æ¯”ï¼Œå¾®è°ƒæ–¹æ³•åœ¨è¿‡å»éœ€è¦å¤§è§„æ¨¡çš„è®¡ç®—é›†ç¾¤ï¼Œå¤§å¤šæ•°ç”¨æˆ·æ— æ³•ä½¿ç”¨ã€‚æ–°çš„å°é—­æ¨¡å‹æ–¹æ³•ï¼Œå¦‚OpenAIçš„å¾®è°ƒæœåŠ¡ï¼Œåªéœ€ä½¿ç”¨100ä¸ªç¤ºä¾‹å³å¯è·å¾—ç»“æœï¼Œè€Œä½¿ç”¨å…¬å¼€æƒé‡çš„æ–¹æ³•ï¼Œå¦‚é‡åŒ–ä½ç§©è‡ªé€‚åº”ï¼ˆQLORAï¼‰ï¼Œåˆ™å¯ä»¥åœ¨æ¶ˆè´¹è€…GPUä¸Šè¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚æˆ‘ä»¬å¯¹è¿™ä¸¤ç§å¾®è°ƒæ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œæµ‹é‡äº†å®ƒä»¬åœ¨ç»“æ„åŒ–ï¼ˆJSONï¼‰è¾“å‡ºçš„è‡ªåŠ¨ç®€ç­”é¢˜è¯„åˆ†ï¼ˆASAGï¼‰ä»»åŠ¡ä¸­ä¸å°‘æ ·æœ¬æç¤ºçš„äº¤äº’ä½œç”¨ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯¹äºLlamaå…¬å¼€æƒé‡æ¨¡å‹ï¼Œä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œå¾®è°ƒçš„æ•ˆç”¨æœ‰é™ï¼Œä½†å¯¹äºOpenAIçš„å°é—­æ¨¡å‹ï¼Œå¾®è°ƒæ–¹æ³•å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šè¶…è¶Šä»¥æŒ‡ä»¤ä¸ºåŸºå‡†è¿›è¡Œè®­ç»ƒçš„å°‘æ ·æœ¬LLMçš„è¡¨ç°ã€‚è™½ç„¶æˆ‘ä»¬çš„è¯„ä¼°é›†æœ‰é™ï¼Œä½†æˆ‘ä»¬å‘ç°è§‚å¯Ÿåˆ°çš„å¾®è°ƒçš„å¥½å¤„å¯èƒ½ä¼šå—åˆ°é¢†åŸŸä¸»é¢˜çš„å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é€šè¿‡åœ¨åˆå§‹è®­ç»ƒç¤ºä¾‹ä¸­æ·»åŠ å¤§é‡å»‰ä»·ç”Ÿæˆçš„åˆæˆè®­ç»ƒæ•°æ®ï¼ŒLLama 3.1 8B-Instructå…¬å¼€æƒé‡æ¨¡å‹å¯ä»¥æ˜¾è‘—æ”¹å–„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04063v1">PDF</a> Proceedings of the Second Workshop on Automated Evaluation of   Learning and Assessment Content co-located with 26th International Conference   on Artificial Intelligence in Education (AIED 2025)</p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨è‡ªåŠ¨çŸ­ç­”æ¡ˆè¯„åˆ†ä¸­çš„åº”ç”¨è¿‘æ¥å—åˆ°å…³æ³¨ï¼Œç ”ç©¶è€…é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆæç¤ºå·¥ç¨‹æŠ€æœ¯æˆ–æ— &#x2F;å°‘æç¤ºæ¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•éœ€è¦å¤§é‡è®¡ç®—é›†ç¾¤ï¼Œæ–°çš„é—­å¼æ¨¡å‹æ–¹æ³•å¦‚OpenAIçš„å¾®è°ƒæœåŠ¡åªéœ€å°‘é‡ç¤ºä¾‹å³å¯å®ç°ç»“æœã€‚ç ”ç©¶è¯„ä¼°äº†è¿™ä¸¤ç§å¾®è°ƒæ–¹æ³•ï¼Œå¹¶æµ‹é‡å®ƒä»¬åœ¨ç»“æ„åŒ–è¾“å‡ºä¸Šä¸å°‘æç¤ºç›¸ç»“åˆçš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œå¯¹äºæŸäº›æ¨¡å‹ï¼Œå¾®è°ƒå¯¹å°‘é‡æ•°æ®çš„æ•ˆç”¨æœ‰é™ï¼Œä½†å¯¹äºæŸäº›æ¨¡å‹ï¼Œå¾®è°ƒæ•ˆæœä¼˜äºåŸºäºæŒ‡ä»¤çš„å°‘é‡ç¤ºä¾‹LLMsã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå¾®è°ƒçš„ä¼˜åŠ¿å¯èƒ½å—ä¸»é¢˜é¢†åŸŸå½±å“ã€‚ä¸ºLLama 3.1 8B-Instructå¼€æ”¾æƒé‡æ¨¡å‹æä¾›å¤§é‡å»‰ä»·ç”Ÿæˆçš„åˆæˆè®­ç»ƒæ•°æ®å¯å®ç°æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨çŸ­ç­”æ¡ˆè¯„åˆ†ï¼ˆASAGï¼‰æ–¹é¢å–å¾—äº†æœ€æ–°ç ”ç©¶å…³æ³¨ã€‚</li>
<li>ç ”ç©¶äººå‘˜é€šè¿‡é‡‡ç”¨æç¤ºå·¥ç¨‹æŠ€æœ¯å®ç°æœ€ä½³ç»“æœï¼Œä½¿ç”¨æ— æˆ–å°‘æç¤ºæ–¹æ³•ã€‚</li>
<li>ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•éœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œè€Œæ–°çš„é—­å¼æ¨¡å‹æ–¹æ³•å¦‚OpenAIçš„å¾®è°ƒæœåŠ¡ä»…éœ€å°‘é‡ç¤ºä¾‹å³å¯å®ç°ç»“æœã€‚</li>
<li>å¯¹å¾®è°ƒæ–¹æ³•å’Œå°‘æç¤ºç›¸ç»“åˆçš„æ•ˆæœè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å¯¹ç‰¹å®šæ¨¡å‹çš„æ•ˆç”¨æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b06cc3fe78f1f99b6d6f6ee45f023072.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1b697fa05c5fcecc91d2d2873efe39e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5bbe84da1a24dc6693da5c891038a24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-793f0fdee84c333ec5f6d0a048cfdd2b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dynamic-User-controllable-Privacy-preserving-Few-shot-Sensing-Framework"><a href="#Dynamic-User-controllable-Privacy-preserving-Few-shot-Sensing-Framework" class="headerlink" title="Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework"></a>Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework</h2><p><strong>Authors:Ajesh Koyatan Chathoth, Shuhao Yu, Stephen Lee</strong></p>
<p>User-controllable privacy is important in modern sensing systems, as privacy preferences can vary significantly from person to person and may evolve over time. This is especially relevant in devices equipped with Inertial Measurement Unit (IMU) sensors, such as smartphones and wearables, which continuously collect rich time-series data that can inadvertently expose sensitive user behaviors. While prior work has proposed privacy-preserving methods for sensor data, most rely on static, predefined privacy labels or require large quantities of private training data, limiting their adaptability and user agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable, few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify and modify their privacy preferences by categorizing activities as sensitive (black-listed), non-sensitive (white-listed), or neutral (gray-listed). Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU sensor data with natural language activity descriptions in a shared embedding space, enabling few-shot detection of sensitive activities. When a privacy-sensitive activity is identified, the system uses a language-guided activity sanitizer and a motion generation module (IMU-GPT) to transform the original data into a privacy-compliant version that semantically resembles a non-sensitive activity. We evaluate PrivCLIP on multiple human activity recognition datasets and demonstrate that it significantly outperforms baseline methods in terms of both privacy protection and data utility. </p>
<blockquote>
<p>åœ¨ç°ä»£æ„ŸçŸ¥ç³»ç»Ÿä¸­ï¼Œç”¨æˆ·å¯æ§çš„éšç§è‡³å…³é‡è¦ï¼Œå› ä¸ºæ¯ä¸ªäººçš„éšç§åå¥½å¯èƒ½å¤§ç›¸å¾„åº­ï¼Œå¹¶å¯èƒ½éšæ—¶é—´æ¼”å˜ã€‚è¿™åœ¨é…å¤‡æœ‰æƒ¯æ€§æµ‹é‡å•å…ƒï¼ˆIMUï¼‰ä¼ æ„Ÿå™¨çš„è®¾å¤‡ä¸­å°¤å…¶é‡è¦ï¼Œä¾‹å¦‚æ™ºèƒ½æ‰‹æœºå’Œå¯ç©¿æˆ´è®¾å¤‡ï¼Œè¿™äº›è®¾å¤‡ä¼šæŒç»­æ”¶é›†ä¸°å¯Œçš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œè¿™äº›æ•°æ®å¯èƒ½æ— æ„ä¸­æš´éœ²ç”¨æˆ·çš„æ•æ„Ÿè¡Œä¸ºã€‚å°½ç®¡å…ˆå‰çš„å·¥ä½œå·²ç»æå‡ºäº†ä¿æŠ¤ä¼ æ„Ÿå™¨æ•°æ®éšç§çš„æ–¹æ³•ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºé™æ€çš„é¢„å®šä¹‰éšç§æ ‡ç­¾æˆ–éœ€è¦å¤§é‡ç§æœ‰è®­ç»ƒæ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶é€‚åº”æ€§å’Œç”¨æˆ·æ§åˆ¶æƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PrivCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€ã€ç”¨æˆ·å¯æ§çš„ã€å…·æœ‰éšç§ä¿æŠ¤åŠŸèƒ½çš„å°‘æ ·æœ¬æ„ŸçŸ¥æ¡†æ¶ã€‚PrivCLIPå…è®¸ç”¨æˆ·é€šè¿‡åˆ†ç±»æ´»åŠ¨ä¸ºæ•æ„Ÿï¼ˆé»‘åå•ï¼‰ã€éæ•æ„Ÿï¼ˆç™½åå•ï¼‰æˆ–ä¸­æ€§ï¼ˆç°åå•ï¼‰æ¥æŒ‡å®šå’Œä¿®æ”¹å…¶éšç§åå¥½ã€‚åˆ©ç”¨å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ŒPrivCLIPå°†IMUä¼ æ„Ÿå™¨æ•°æ®ä¸è‡ªç„¶è¯­è¨€æ´»åŠ¨æè¿°å¯¹é½åˆ°ä¸€ä¸ªå…±äº«åµŒå…¥ç©ºé—´ä¸­ï¼Œå®ç°äº†å¯¹æ•æ„Ÿæ´»åŠ¨çš„å°‘æ ·æœ¬æ£€æµ‹ã€‚å½“æ£€æµ‹åˆ°éšç§æ•æ„Ÿçš„æ´»åŠ¨æ—¶ï¼Œç³»ç»Ÿä¼šä½¿ç”¨è¯­è¨€æŒ‡å¯¼çš„æ´»åŠ¨å‡€åŒ–å™¨å’Œè¿åŠ¨ç”Ÿæˆæ¨¡å—ï¼ˆIMU-GPTï¼‰å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºç¬¦åˆéšç§çš„ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬åœ¨è¯­ä¹‰ä¸Šç±»ä¼¼äºéæ•æ„Ÿæ´»åŠ¨ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªäººç±»æ´»åŠ¨è¯†åˆ«æ•°æ®é›†ä¸Šè¯„ä¼°äº†PrivCLIPçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜å®ƒåœ¨éšç§ä¿æŠ¤å’Œæ•°æ®å®ç”¨æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03989v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ç°ä»£æ„Ÿåº”ç³»ç»Ÿä¸­ç”¨æˆ·å¯æ§éšç§çš„é‡è¦æ€§ã€‚é’ˆå¯¹é…å¤‡æœ‰æƒ¯æ€§æµ‹é‡å•å…ƒï¼ˆIMUï¼‰ä¼ æ„Ÿå™¨çš„è®¾å¤‡ï¼ˆå¦‚æ™ºèƒ½æ‰‹æœºå’Œå¯ç©¿æˆ´è®¾å¤‡ï¼‰ï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€ã€ç”¨æˆ·å¯æ§çš„å°‘é‡éšç§ä¿æŠ¤æ„Ÿåº”æ¡†æ¶â€”â€”PrivCLIPã€‚è¯¥æ¡†æ¶å…è®¸ç”¨æˆ·é€šè¿‡åˆ†ç±»æ´»åŠ¨ï¼ˆæ•æ„Ÿæ´»åŠ¨ã€éæ•æ„Ÿæ´»åŠ¨å’Œä¸­æ€§æ´»åŠ¨ï¼‰æ¥æŒ‡å®šå’Œä¿®æ”¹éšç§åå¥½ã€‚å€ŸåŠ©å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ŒPrivCLIPåœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å®ç°äº†IMUä¼ æ„Ÿå™¨æ•°æ®ä¸è‡ªç„¶è¯­è¨€æ´»åŠ¨æè¿°çš„åŒ¹é…ï¼Œèƒ½å¤Ÿå®ç°å¯¹æ•æ„Ÿæ´»åŠ¨çš„å°‘é‡æ£€æµ‹ã€‚å½“æ£€æµ‹åˆ°éšç§æ•æ„Ÿæ´»åŠ¨æ—¶ï¼Œç³»ç»Ÿä¼šä½¿ç”¨è¯­è¨€å¼•å¯¼çš„æ´»åŠ¨å‡€åŒ–å™¨å’Œè¿åŠ¨ç”Ÿæˆæ¨¡å—ï¼ˆIMU-GPTï¼‰å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºç¬¦åˆéšç§çš„ç‰ˆæœ¬ï¼ŒåŒæ—¶ä¿æŒä¸éæ•æ„Ÿæ´»åŠ¨çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚å¯¹PrivCLIPåœ¨å¤šä¸ªäººç±»æ´»åŠ¨è¯†åˆ«æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œå®ƒåœ¨éšç§ä¿æŠ¤å’Œæ•°æ®æ•ˆç”¨æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”¨æˆ·å¯æ§éšç§åœ¨ç°ä»£æ„Ÿåº”ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºæ¯ä¸ªäººçš„éšç§åå¥½å¯èƒ½ä¸åŒä¸”éšæ—¶é—´å˜åŒ–ã€‚</li>
<li>åœ¨é…å¤‡æœ‰IMUä¼ æ„Ÿå™¨çš„è®¾å¤‡ä¸­ï¼Œå¦‚æ™ºèƒ½æ‰‹æœºå’Œå¯ç©¿æˆ´è®¾å¤‡ï¼Œå­˜åœ¨éšç§æ³„éœ²é£é™©ï¼Œéœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§ã€‚</li>
<li>ç°æœ‰éšç§ä¿æŠ¤æ–¹æ³•å¤§å¤šä¾èµ–é™æ€çš„é¢„å®šä¹‰éšç§æ ‡ç­¾æˆ–éœ€è¦å¤§é‡ç§æœ‰è®­ç»ƒæ•°æ®ï¼Œé™åˆ¶äº†å…¶é€‚åº”æ€§å’Œç”¨æˆ·æ§åˆ¶æƒã€‚</li>
<li>PrivCLIPæ˜¯ä¸€ç§åŠ¨æ€ã€ç”¨æˆ·å¯æ§çš„å°‘é‡éšç§ä¿æŠ¤æ„Ÿåº”æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡åˆ†ç±»æ´»åŠ¨æ¥è°ƒæ•´éšç§åå¥½ã€‚</li>
<li>PrivCLIPåˆ©ç”¨å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼Œå®ç°äº†IMUä¼ æ„Ÿå™¨æ•°æ®ä¸è‡ªç„¶è¯­è¨€æ´»åŠ¨æè¿°çš„åŒ¹é…ï¼Œèƒ½å°‘é‡æ£€æµ‹æ•æ„Ÿæ´»åŠ¨ã€‚</li>
<li>å½“æ£€æµ‹åˆ°æ•æ„Ÿæ´»åŠ¨æ—¶ï¼ŒPrivCLIPèƒ½å¤Ÿè½¬æ¢åŸå§‹æ•°æ®ï¼Œç”Ÿæˆç¬¦åˆéšç§çš„ç‰ˆæœ¬ï¼ŒåŒæ—¶ä¿æŒä¸éæ•æ„Ÿæ´»åŠ¨çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d5e8823e8ecab22c7c5dc0477e09db4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4cb6b3b2b8f94e0db44502d13ea0a04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba88bc96acc69642c454137183ab8859.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66e86667db3d7c15d6c2ef04eb794d5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b493bce03f602be3ffc9b7062b41a56c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aae9dd69e8d7fa8210c46c85ac7c8145.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Trokens-Semantic-Aware-Relational-Trajectory-Tokens-for-Few-Shot-Action-Recognition"><a href="#Trokens-Semantic-Aware-Relational-Trajectory-Tokens-for-Few-Shot-Action-Recognition" class="headerlink" title="Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action   Recognition"></a>Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action   Recognition</h2><p><strong>Authors:Pulkit Kumar, Shuaiyi Huang, Matthew Walmer, Sai Saketh Rambhatla, Abhinav Shrivastava</strong></p>
<p>Video understanding requires effective modeling of both motion and appearance information, particularly for few-shot action recognition. While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist: selecting informative points to track and effectively modeling their motion patterns. We present Trokens, a novel approach that transforms trajectory points into semantic-aware relational tokens for action recognition. First, we introduce a semantic-aware sampling strategy to adaptively distribute tracking points based on object scale and semantic relevance. Second, we develop a motion modeling framework that captures both intra-trajectory dynamics through the Histogram of Oriented Displacements (HoD) and inter-trajectory relationships to model complex action patterns. Our approach effectively combines these trajectory tokens with semantic features to enhance appearance features with motion information, achieving state-of-the-art performance across six diverse few-shot action recognition benchmarks: Something-Something-V2 (both full and small splits), Kinetics, UCF101, HMDB51, and FineGym. For project page see <a target="_blank" rel="noopener" href="https://trokens-iccv25.github.io/">https://trokens-iccv25.github.io</a> </p>
<blockquote>
<p>è§†é¢‘ç†è§£éœ€è¦æœ‰æ•ˆå»ºæ¨¡è¿åŠ¨ä¿¡æ¯å’Œå¤–è§‚ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬åŠ¨ä½œè¯†åˆ«ä¸­ã€‚è™½ç„¶æœ€è¿‘çš„ç‚¹è·Ÿè¸ªæŠ€æœ¯å·²ç»æ˜¾ç¤ºå‡ºèƒ½æé«˜å°æ ·æœ¬åŠ¨ä½œè¯†åˆ«çš„èƒ½åŠ›ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šé€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„ç‚¹è¿›è¡Œè·Ÿè¸ªå’Œæœ‰æ•ˆåœ°å»ºæ¨¡å…¶è¿åŠ¨æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†Trokensï¼Œè¿™æ˜¯ä¸€ç§å°†è½¨è¿¹ç‚¹è½¬æ¢ä¸ºè¯­ä¹‰æ„ŸçŸ¥å…³ç³»ä»¤ç‰Œè¿›è¡ŒåŠ¨ä½œè¯†åˆ«çš„æ–°æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯­ä¹‰æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥ï¼ŒåŸºäºç›®æ ‡å°ºåº¦å’Œè¯­ä¹‰ç›¸å…³æ€§è‡ªé€‚åº”åœ°åˆ†å¸ƒè·Ÿè¸ªç‚¹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè¿åŠ¨å»ºæ¨¡æ¡†æ¶ï¼Œé€šè¿‡æ–¹å‘ä½ç§»ç›´æ–¹å›¾ï¼ˆHoDï¼‰æ•æ‰è½¨è¿¹å†…éƒ¨åŠ¨æ€ï¼Œå¹¶æ•æ‰è½¨è¿¹ä¹‹é—´çš„å…³ç³»ä»¥æ¨¡æ‹Ÿå¤æ‚çš„åŠ¨ä½œæ¨¡å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ç»“åˆäº†è¿™äº›è½¨è¿¹ä»¤ç‰Œå’Œè¯­ä¹‰ç‰¹å¾ï¼Œé€šè¿‡è¿åŠ¨ä¿¡æ¯å¢å¼ºå¤–è§‚ç‰¹å¾ï¼Œåœ¨å…­ä¸ªä¸åŒçš„å°æ ·æœ¬åŠ¨ä½œè¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼šåŒ…æ‹¬Something-Something-V2ï¼ˆå…¨åˆ†å‰²å’Œå°åˆ†å‰²ï¼‰ã€Kineticsã€UCF101ã€HMDB51å’ŒFineGymã€‚æœ‰å…³é¡¹ç›®é¡µé¢ï¼Œè¯·å‚è§<a target="_blank" rel="noopener" href="https://trokens-iccv25.github.io./">https://trokens-iccv25.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03695v1">PDF</a> Accepted at ICCV 2025; First two authors contributed equally</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«çš„æ–°å‹æ–¹æ³•Trokensã€‚è¯¥æ–¹æ³•å°†è½¨è¿¹ç‚¹è½¬æ¢ä¸ºè¯­ä¹‰æ„ŸçŸ¥çš„å…³ç³»ä»¤ç‰Œï¼Œé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥è‡ªé€‚åº”åˆ†å¸ƒè·Ÿè¸ªç‚¹ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè¿åŠ¨å»ºæ¨¡æ¡†æ¶ï¼Œç»“åˆç›´æ–¹ä½ç§»ï¼ˆHoDï¼‰å’Œè½¨è¿¹é—´çš„å…³ç³»æ¥æ¨¡æ‹Ÿå¤æ‚çš„åŠ¨ä½œæ¨¡å¼ã€‚è¯¥æ–¹æ³•å®ç°äº†åœ¨å¤šä¸ªå°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Trokensæ–¹æ³•å°†è½¨è¿¹ç‚¹è½¬æ¢ä¸ºè¯­ä¹‰æ„ŸçŸ¥çš„å…³ç³»ä»¤ç‰Œï¼Œç”¨äºåŠ¨ä½œè¯†åˆ«ã€‚</li>
<li>å¼•å…¥è¯­ä¹‰æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®ç›®æ ‡å°ºåº¦å’Œè¯­ä¹‰ç›¸å…³æ€§è‡ªé€‚åº”åˆ†å¸ƒè·Ÿè¸ªç‚¹ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªè¿åŠ¨å»ºæ¨¡æ¡†æ¶ï¼Œç»“åˆäº†ç›´æ–¹ä½ç§»ï¼ˆHoDï¼‰å’Œè½¨è¿¹é—´çš„å…³ç³»ã€‚</li>
<li>Trokenså®ç°äº†åœ¨å¤šä¸ªå°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å°†è½¨è¿¹ä»¤ç‰Œä¸è¯­ä¹‰ç‰¹å¾ç›¸ç»“åˆï¼Œå¢å¼ºäº†å¤–è§‚ç‰¹å¾çš„è¿åŠ¨ä¿¡æ¯ã€‚</li>
<li>é¢ä¸´é€‰æ‹©ä¿¡æ¯ç‚¹è¿›è¡Œè·Ÿè¸ªå’Œæœ‰æ•ˆæ¨¡æ‹Ÿå…¶è¿åŠ¨æ¨¡å¼çš„ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03695">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-023a19e00c395d19686014f0b4b0bdff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d9661ab57a575f71779d65aaa12cae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-078db757178752d8ed3ac31435ff75cc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MAUP-Training-free-Multi-center-Adaptive-Uncertainty-aware-Prompting-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#MAUP-Training-free-Multi-center-Adaptive-Uncertainty-aware-Prompting-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting   for Cross-domain Few-shot Medical Image Segmentation"></a>MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting   for Cross-domain Few-shot Medical Image Segmentation</h2><p><strong>Authors:Yazhou Zhu, Haofeng Zhang</strong></p>
<p>Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential solution for segmenting medical images with limited annotation using knowledge from other domains. The significant performance of current CD-FSMIS models relies on the heavily training procedure over other source medical domains, which degrades the universality and ease of model deployment. With the development of large visual models of natural images, we propose a training-free CD-FSMIS model that introduces the Multi-center Adaptive Uncertainty-aware Prompting (MAUP) strategy for adapting the foundation model Segment Anything Model (SAM), which is trained with natural images, into the CD-FSMIS task. To be specific, MAUP consists of three key innovations: (1) K-means clustering based multi-center prompts generation for comprehensive spatial coverage, (2) uncertainty-aware prompts selection that focuses on the challenging regions, and (3) adaptive prompt optimization that can dynamically adjust according to the target region complexity. With the pre-trained DINOv2 feature encoder, MAUP achieves precise segmentation results across three medical datasets without any additional training compared with several conventional CD-FSMIS models and training-free FSMIS model. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/MAUP">https://github.com/YazhouZhu19/MAUP</a>. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰æ˜¯åˆ©ç”¨å…¶ä»–é¢†åŸŸçš„çŸ¥è¯†å¯¹åŒ»å­¦å›¾åƒè¿›è¡Œæœ‰é™æ ‡æ³¨åˆ†å‰²çš„ä¸€ç§æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚å½“å‰CD-FSMISæ¨¡å‹çš„é«˜æ€§èƒ½è¡¨ç°ä¾èµ–äºåœ¨å…¶ä»–æºåŒ»å­¦é¢†åŸŸçš„å¤§é‡è®­ç»ƒè¿‡ç¨‹ï¼Œè¿™é™ä½äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œéƒ¨ç½²çš„ä¾¿æ·æ€§ã€‚éšç€å¤§å‹è‡ªç„¶å›¾åƒè§†è§‰æ¨¡å‹çš„å‘å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„CD-FSMISæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†å¤šä¸­å¿ƒè‡ªé€‚åº”ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºï¼ˆMAUPï¼‰ç­–ç•¥ï¼Œä»¥é€‚åº”åŸºäºè‡ªç„¶å›¾åƒè®­ç»ƒçš„åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰åˆ°CD-FSMISä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒMAUPåŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰åŸºäºK-meansèšç±»çš„å¤šä¸­å¿ƒæç¤ºç”Ÿæˆï¼Œä»¥å®ç°å…¨é¢çš„ç©ºé—´è¦†ç›–ï¼›ï¼ˆ2ï¼‰å…³æ³¨æŒ‘æˆ˜åŒºåŸŸçš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºé€‰æ‹©ï¼›ï¼ˆ3ï¼‰å¯æ ¹æ®ç›®æ ‡åŒºåŸŸå¤æ‚æ€§åŠ¨æ€è°ƒæ•´çš„è‡ªé€‚åº”æç¤ºä¼˜åŒ–ã€‚ä¸å‡ ä¸ªä¼ ç»Ÿçš„CD-FSMISæ¨¡å‹å’Œæ— éœ€è®­ç»ƒçš„FSMISæ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„DINOv2ç‰¹å¾ç¼–ç å™¨ï¼ŒMAUPåœ¨ä¸‰ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/MAUP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YazhouZhu19/MAUPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03511v1">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè·¨åŸŸå°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰æ–¹æ³•åœ¨å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²æ—¶é¢ä¸´è®­ç»ƒè¿‡ç¨‹å¤æ‚çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹éƒ¨ç½²çš„é€šç”¨æ€§å’Œä¾¿æ·æ€§é™ä½ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„CD-FSMISæ¨¡å‹ï¼Œé‡‡ç”¨åŸºäºè‡ªç„¶å›¾åƒçš„å¤§å‹è§†è§‰æ¨¡å‹è¿›è¡Œè®­ç»ƒå¹¶å¼•å…¥Multi-center Adaptive Uncertainty-aware Promptingï¼ˆMAUPï¼‰ç­–ç•¥è¿›è¡Œé€‚åº”ã€‚MAUPç­–ç•¥åŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šåŸºäºK-meansèšç±»çš„å¤šä¸­å¿ƒæç¤ºç”Ÿæˆã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºé€‰æ‹©å’Œè‡ªé€‚åº”æç¤ºä¼˜åŒ–ã€‚ä¸å¸¸è§„CD-FSMISæ¨¡å‹å’Œæ— éœ€è®­ç»ƒçš„FSMISæ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„DINOv2ç‰¹å¾ç¼–ç å™¨ï¼ŒMAUPåœ¨ä¸‰ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CD-FSMISæ–¹æ³•åœ¨å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²æ—¶é¢ä¸´è®­ç»ƒå¤æ‚çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„CD-FSMISæ¨¡å‹ï¼Œä½¿ç”¨å¤§å‹è§†è§‰æ¨¡å‹å’ŒMAUPç­–ç•¥è¿›è¡Œé€‚åº”ã€‚</li>
<li>MAUPç­–ç•¥åŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šå¤šä¸­å¿ƒæç¤ºç”Ÿæˆã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºé€‰æ‹©å’Œè‡ªé€‚åº”æç¤ºä¼˜åŒ–ã€‚</li>
<li>MAUPç­–ç•¥åœ¨ä¸‰ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚</li>
<li>è¯¥æ¨¡å‹çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
<li>è¯¥æ–¹æ³•ä¾èµ–äºé¢„è®­ç»ƒçš„DINOv2ç‰¹å¾ç¼–ç å™¨ä»¥å®ç°ç²¾ç¡®çš„åˆ†å‰²ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4ce8753932600e79a64a29fef59d5b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88d91333fa6c10dbe48a67f52ca48a5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9d4ca7caf0a1ae93150454786c425bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0dd224fe8792e5aafd4a68b1dc59b640.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="On-the-Evaluation-of-Large-Language-Models-in-Multilingual-Vulnerability-Repair"><a href="#On-the-Evaluation-of-Large-Language-Models-in-Multilingual-Vulnerability-Repair" class="headerlink" title="On the Evaluation of Large Language Models in Multilingual Vulnerability   Repair"></a>On the Evaluation of Large Language Models in Multilingual Vulnerability   Repair</h2><p><strong>Authors:Dong wang, Junji Yu, Honglin Shu, Michael Fu, Chakkrit Tantithamthavorn, Yasutaka Kamei, Junjie Chen</strong></p>
<p>Various Deep Learning-based approaches with pre-trained language models have been proposed for automatically repairing software vulnerabilities. However, these approaches are limited to a specific programming language (C&#x2F;C++). Recent advances in large language models (LLMs) offer language-agnostic capabilities and strong semantic understanding, exhibiting potential to overcome multilingual vulnerability limitations. Although some work has begun to explore LLMsâ€™ repair performance, their effectiveness is unsatisfactory. To address these limitations, we conducted a large-scale empirical study to investigate the performance of automated vulnerability repair approaches and state-of-the-art LLMs across seven programming languages. Results show GPT-4o, instruction-tuned with few-shot prompting, performs competitively against the leading approach, VulMaster. Additionally, the LLM-based approach shows superior performance in repairing unique vulnerabilities and is more likely to repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o demonstrates strong generalization on vulnerabilities in previously unseen language, outperforming existing approaches. Analysis shows Go consistently achieves the highest effectiveness across all model types, while C&#x2F;C++ performs the worst. Based on findings, we discuss the promise of LLM on multilingual vulnerability repair and the reasons behind LLMâ€™s failed cases. This work takes the first look at repair approaches and LLMs across multiple languages, highlighting the promising future of adopting LLMs for multilingual vulnerability repair. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¤šç§æ–¹æ³•å·²è¢«æå‡ºç”¨äºè‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…é™äºç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æä¾›äº†è·¨è¯­è¨€çš„èƒ½åŠ›å’Œå¼ºå¤§çš„è¯­ä¹‰ç†è§£ï¼Œæ˜¾ç¤ºå‡ºå…‹æœå¤šç§è¯­è¨€æ¼æ´é™åˆ¶çš„æ½œåŠ›ã€‚å°½ç®¡å·²ç»å¼€å§‹æ¢ç´¢LLMåœ¨ä¿®å¤æ€§èƒ½æ–¹é¢çš„åº”ç”¨ï¼Œä½†å…¶æœ‰æ•ˆæ€§å°šä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œè°ƒæŸ¥äº†è·¨ä¸ƒç§ç¼–ç¨‹è¯­è¨€çš„è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹æ³•å’Œæœ€æ–°LLMçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å°‘é‡æç¤ºè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´çš„GPT-4oä¸é¢†å…ˆçš„VulMasteræ–¹æ³•ç«äº‰è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨ä¿®å¤ç‹¬ç‰¹æ¼æ´æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”æ›´æœ‰å¯èƒ½ä¿®å¤æœ€å±é™©çš„æ¼æ´ã€‚é€šè¿‡æŒ‡ä»¤è°ƒæ•´çš„GPT-4oåœ¨ä¹‹å‰æœªè§çš„è¯­è¨€ä¸­çš„æ¼æ´ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼ŒGoåœ¨æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­å§‹ç»ˆå®ç°æœ€é«˜çš„æœ‰æ•ˆæ€§ï¼Œè€ŒC&#x2F;C++çš„è¡¨ç°æœ€å·®ã€‚åŸºäºç ”ç©¶ç»“æœï¼Œæˆ‘ä»¬è®¨è®ºäº†LLMåœ¨å¤šè¯­è¨€æ¼æ´ä¿®å¤æ–¹é¢çš„å‰æ™¯ä»¥åŠLLMå¤±è´¥æ¡ˆä¾‹èƒŒåçš„åŸå› ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹ç ”ç©¶äº†ä¿®å¤æ–¹æ³•å’ŒLLMï¼Œçªæ˜¾äº†é‡‡ç”¨LLMè¿›è¡Œå¤šè¯­è¨€æ¼æ´ä¿®å¤çš„å¹¿é˜”å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03470v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´çš„å„ç§æ·±åº¦å­¦ä¹ æ–¹æ³•åŸºäºé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å·²ç»æå‡ºï¼Œä½†å®ƒä»¬é€šå¸¸å±€é™äºç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›è·¨è¯­è¨€çš„é€šç”¨èƒ½åŠ›å’Œå¼ºå¤§çš„è¯­ä¹‰ç†è§£ï¼Œæ˜¾ç¤ºå‡ºå…‹æœå¤šç§è¯­è¨€æ¼æ´é™åˆ¶çš„æ½œåŠ›ã€‚å°½ç®¡å·²ç»å¼€å§‹æ¢ç´¢LLMsåœ¨æ¼æ´ä¿®å¤æ–¹é¢çš„æ€§èƒ½ï¼Œä½†å…¶æœ‰æ•ˆæ€§å°šä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œè°ƒæŸ¥è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹æ³•å’Œæœ€æ–°LLMsåœ¨ä¸ƒç§ç¼–ç¨‹è¯­è¨€ä¸­çš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡å°‘é‡æç¤ºè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´çš„GPT-4oä¸é¢†å…ˆçš„VulMasteræ–¹æ³•ç«äº‰è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨ä¿®å¤ç‹¬ç‰¹æ¼æ´æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”æ›´æœ‰å¯èƒ½ä¿®å¤æœ€å±é™©çš„æ¼æ´ã€‚åœ¨ä»¥å‰æœªè§è¿‡çš„è¯­è¨€ä¸­çš„æ¼æ´æ–¹é¢ï¼ŒæŒ‡ä»¤è°ƒæ•´çš„GPT-4oå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼ŒGoåœ¨æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­å§‹ç»ˆå®ç°æœ€é«˜çš„æœ‰æ•ˆæ€§ï¼Œè€ŒC&#x2F;C++çš„è¡¨ç°æœ€å·®ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬è®¨è®ºäº†LLMåœ¨å¤šè¯­è¨€æ¼æ´ä¿®å¤æ–¹é¢çš„å‰æ™¯ä»¥åŠLLMå¤±è´¥çš„åŸå› ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡è·¨å¤šç§è¯­è¨€è°ƒæŸ¥ä¿®å¤æ–¹æ³•å’ŒLLMsï¼Œçªæ˜¾äº†é‡‡ç”¨LLMsè¿›è¡Œå¤šè¯­è¨€æ¼æ´ä¿®å¤çš„å¹¿é˜”å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•ç»“åˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´é¢†åŸŸå·²æœ‰åº”ç”¨ï¼Œä½†å±€é™äºç‰¹å®šç¼–ç¨‹è¯­è¨€ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰è·¨è¯­è¨€çš„é€šç”¨èƒ½åŠ›å’Œå¼ºå¤§çš„è¯­ä¹‰ç†è§£ï¼Œä¸ºå…‹æœå¤šè¯­è¨€æ¼æ´ä¿®å¤çš„é™åˆ¶æä¾›äº†æ½œåŠ›ã€‚</li>
<li>GPT-4oåœ¨æŒ‡ä»¤å¾®è°ƒåï¼Œé€šè¿‡å°‘é‡æç¤ºä¾¿èƒ½å±•ç°å‡ºå¼ºå¤§çš„æ¼æ´ä¿®å¤èƒ½åŠ›ï¼Œä¸é¢†å…ˆçš„VulMasteræ–¹æ³•ç«äº‰ã€‚</li>
<li>LLMsåœ¨ä¿®å¤ç‹¬ç‰¹å’Œå±é™©æ¼æ´æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨æœªè§è¿‡çš„è¯­è¨€ä¸­çš„æ¼æ´æ–¹é¢ï¼ŒGPT-4oå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Goè¯­è¨€åœ¨è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤ä¸­çš„æœ‰æ•ˆæ€§æœ€é«˜ï¼Œè€ŒC&#x2F;C++è¡¨ç°æœ€å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0645a16447deda0f74b821db44e52506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4100e5b2b0e7f651825c8d1450308a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f21042fafdcf88c85716f9cf596b5a57.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Thinking-with-Nothinking-Calibration-A-New-In-Context-Learning-Paradigm-in-Reasoning-Large-Language-Models"><a href="#Thinking-with-Nothinking-Calibration-A-New-In-Context-Learning-Paradigm-in-Reasoning-Large-Language-Models" class="headerlink" title="Thinking with Nothinking Calibration: A New In-Context Learning Paradigm   in Reasoning Large Language Models"></a>Thinking with Nothinking Calibration: A New In-Context Learning Paradigm   in Reasoning Large Language Models</h2><p><strong>Authors:Haotian Wu, Bo Xu, Yao Shu, Menglin Yang, Chengwei Qin</strong></p>
<p>Reasoning large language models (RLLMs) have recently demonstrated remarkable capabilities through structured and multi-step reasoning. While prior research has primarily focused on improving their training and inference strategies, their potential for in-context learning (ICL) remains largely underexplored. To fill this gap, we propose Thinking with Nothinking Calibration (JointThinking), a new ICL paradigm that leverages the structured difference between two reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy. Specifically, our method prompts the model to generate two answers in parallel: one in Thinking mode and the other in Nothinking mode. A second round of Thinking is triggered only when the two initial responses are inconsistent, using a single prompt that incorporates the original question and both candidate answers. Since such disagreement occurs infrequently (e.g., only 6% in GSM8K), our method performs just one round of reasoning in most cases, resulting in minimal latency overhead. Extensive experiments across multiple reasoning benchmarks demonstrate that JointThinking significantly outperforms few-shot chain-of-thought (CoT) and majority voting with improved answer robustness. Moreover, It achieves comparable in-distribution performance to training-based SOTA method, while substantially outperforming on out-of-distribution tasks. We further conduct a systematic analysis of the calibration mechanism, showing that leveraging different reasoning modes consistently lowers the error rate and highlights the value of structural thinking diversity. Additionally, we observe that the performance gap between actual and ideal reasoning narrows as model size increases in the second round of thinking, indicating the strong scalability of our approach. Finally, we discuss current limitations and outline promising directions for future ICL research in RLLMs. </p>
<blockquote>
<p>æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼‰æœ€è¿‘é€šè¿‡ç»“æ„åŒ–åŠå¤šæ­¥éª¤æ¨ç†å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚å°½ç®¡å…ˆå‰çš„ç ”ç©¶ä¸»è¦ä¾§é‡äºæ”¹è¿›å…¶è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼Œä½†å®ƒä»¬åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹é¢çš„æ½œåŠ›å´é²œæœ‰ç ”ç©¶ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ— æ€è€ƒæ ¡å‡†æ€è€ƒâ€ï¼ˆJointThinkingï¼‰è¿™ä¸€æ–°çš„ICLèŒƒå¼ï¼Œå®ƒåˆ©ç”¨ä¸¤ç§æ¨ç†æ¨¡å¼ä¹‹é—´çš„ç»“æ„åŒ–å·®å¼‚ï¼Œå³â€œæ€è€ƒâ€å’Œâ€œæ— æ€è€ƒâ€ï¼Œæ¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æç¤ºæ¨¡å‹å¹¶è¡Œç”Ÿæˆä¸¤ä¸ªç­”æ¡ˆï¼šä¸€ä¸ªåœ¨â€œæ€è€ƒâ€æ¨¡å¼ä¸‹ï¼Œå¦ä¸€ä¸ªåœ¨â€œæ— æ€è€ƒâ€æ¨¡å¼ä¸‹ã€‚å½“ä¸¤ä¸ªåˆæ­¥å›ç­”ä¸ä¸€è‡´æ—¶ï¼Œæ‰ä¼šè§¦å‘ç¬¬äºŒè½®â€œæ€è€ƒâ€ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªç»“åˆäº†åŸå§‹é—®é¢˜å’Œä¸¤ä¸ªå€™é€‰ç­”æ¡ˆçš„å•ä¸€æç¤ºã€‚ç”±äºè¿™ç§åˆ†æ­§å¾ˆå°‘å‘ç”Ÿï¼ˆä¾‹å¦‚åœ¨GSM8Kä¸­ä»…å 6%ï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤šæ•°æƒ…å†µä¸‹åªè¿›è¡Œä¸€è½®æ¨ç†ï¼Œå› æ­¤å»¶è¿Ÿå¼€é”€å¾ˆå°ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒJointThinkingæ˜¾è‘—ä¼˜äºå°‘é•œå¤´æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œå¤šæ•°æŠ•ç¥¨æ³•ï¼Œæé«˜äº†ç­”æ¡ˆçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨åŸºäºè®­ç»ƒçš„æœ€æ–°æŠ€æœ¯æ–¹æ³•ä¸Šå®ç°äº†ç›¸å½“çš„åˆ†å†…åˆ†å¸ƒæ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶åœ¨è·¨åˆ†å¸ƒä»»åŠ¡ä¸Šå¤§å¹…è¶…è¶Šã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹æ ¡å‡†æœºåˆ¶è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œç»“æœè¡¨æ˜åˆ©ç”¨ä¸åŒçš„æ¨ç†æ¨¡å¼å¯ä»¥æŒç»­é™ä½é”™è¯¯ç‡ï¼Œå¹¶çªæ˜¾äº†ç»“æ„åŒ–æ€ç»´å¤šæ ·æ€§çš„ä»·å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨ç¬¬äºŒè½®æ€è€ƒä¸­ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œå®é™…ä¸ç†æƒ³æ¨ç†ä¹‹é—´çš„æ€§èƒ½å·®è·ç¼©å°ï¼Œè¿™è¡¨æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„å¯æ‰©å±•æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å½“å‰çš„å±€é™æ€§ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥RLLMsä¸­ICLç ”ç©¶çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03363v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§è¯­è¨€æ¨¡å‹é€šè¿‡ç»“æ„åŒ–å’Œå¤šæ­¥éª¤æ¨ç†å±•ç°äº†æƒŠäººçš„èƒ½åŠ›ã€‚è™½ç„¶å…ˆå‰çš„ç ”ç©¶ä¸»è¦èšç„¦äºæ”¹è¿›å…¶è®­ç»ƒå’Œæ¨ç†ç­–ç•¥ï¼Œä½†å®ƒä»¬å¯¹äºå†…éšå­¦ä¹ ï¼ˆICLï¼‰çš„æ½œåŠ›å´è¢«å¤§å¤§å¿½è§†äº†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ— éœ€æ€è€ƒæ ¡å‡†çš„æ€è€ƒâ€ï¼ˆJointThinkingï¼‰è¿™ä¸€æ–°çš„å†…éšå­¦ä¹ èŒƒå¼ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä¸¤ç§æ¨ç†æ¨¡å¼ä¹‹é—´çš„ç»“æ„åŒ–å·®å¼‚ï¼ˆå³æ€è€ƒä¸ä¸æ€è€ƒï¼‰ï¼Œæ¥æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿ƒä½¿æ¨¡å‹å¹¶è¡Œç”Ÿæˆä¸¤ä¸ªç­”æ¡ˆï¼šä¸€ä¸ªä¸ºæ€è€ƒæ¨¡å¼ï¼Œå¦ä¸€ä¸ªä¸ºä¸æ€è€ƒæ¨¡å¼ã€‚å½“ä¸¤ä¸ªåˆæ­¥å›ç­”ä¸ä¸€è‡´æ—¶ï¼Œä¼šä½¿ç”¨åŒ…å«åŸå§‹é—®é¢˜å’Œä¸¤ä¸ªå€™é€‰ç­”æ¡ˆçš„å•ä¸€æç¤ºæ¥è§¦å‘ç¬¬äºŒè½®æ€è€ƒã€‚ç”±äºè¿™ç§åˆ†æ­§å¾ˆå°‘å‘ç”Ÿï¼ˆä¾‹å¦‚åœ¨GSM8Kä¸­ä»…å 6%ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬çš„æ–¹æ³•å¤§å¤šæ•°æƒ…å†µä¸‹åªè¿›è¡Œä¸€æ¬¡æ¨ç†ï¼Œå‡ ä¹ä¸ä¼šé€ æˆå»¶è¿Ÿå¼€é”€ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å°‘æ•°æ€è€ƒé“¾å’Œå¤šæ•°æŠ•ç¥¨ç›¸æ¯”ï¼ŒJointThinkingæ˜¾è‘—æé«˜äº†è§£ç­”çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å†…éƒ¨åˆ†å¸ƒä»»åŠ¡ä¸­çš„è¡¨ç°ä¸åŸºäºè®­ç»ƒçš„æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“ï¼Œè€Œåœ¨å¤–éƒ¨åˆ†å¸ƒä»»åŠ¡ä¸­çš„è¡¨ç°åˆ™å¤§å¤§ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹æ ¡å‡†æœºåˆ¶è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œç»“æœè¡¨æ˜åˆ©ç”¨ä¸åŒçš„æ¨ç†æ¨¡å¼å¯ä»¥æŒç»­é™ä½é”™è¯¯ç‡ï¼Œå¹¶çªæ˜¾ç»“æ„æ€è€ƒå¤šæ ·æ€§çš„ä»·å€¼ã€‚æ­¤å¤–ï¼Œéšç€ç¬¬äºŒè½®æ€è€ƒä¸­æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œå®é™…æ¨ç†ä¸ç†æƒ³æ¨ç†ä¹‹é—´çš„æ€§èƒ½å·®è·ç¼©å°ï¼Œè¡¨æ˜æˆ‘ä»¬æ–¹æ³•çš„å¼ºå¤§å¯æ‰©å±•æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å½“å‰çš„å±€é™æ€§å¹¶ä¸ºæœªæ¥çš„ICLç ”ç©¶æå‡ºäº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>æå‡ºæ–°çš„å†…éšå­¦ä¹ èŒƒå¼â€”â€”æ— éœ€æ€è€ƒæ ¡å‡†çš„æ€è€ƒï¼ˆJointThinkingï¼‰ï¼Œåˆ©ç”¨ä¸¤ç§æ¨ç†æ¨¡å¼ï¼ˆæ€è€ƒä¸ä¸æ€è€ƒï¼‰ä¹‹é—´çš„å·®å¼‚æ¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆä¸¤ä¸ªä¸åŒæ¨¡å¼çš„ç­”æ¡ˆå¹¶è¿›è¡Œæ¯”è¾ƒï¼Œè§¦å‘ç¬¬äºŒè½®æ€è€ƒï¼Œå¤§å¤šæ•°æƒ…å†µåªéœ€ä¸€æ¬¡æ¨ç†ï¼Œå»¶è¿Ÿå¼€é”€æå°ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå°‘æ•°æ€è€ƒé“¾å’Œå¤šæ•°æŠ•ç¥¨æ–¹æ³•ï¼Œæé«˜è§£ç­”ç¨³å¥æ€§ã€‚</li>
<li>åœ¨å†…ã€å¤–éƒ¨åˆ†å¸ƒä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯å¤–éƒ¨ä»»åŠ¡ä¸­çš„è¡¨ç°çªå‡ºã€‚</li>
<li>ç³»ç»Ÿåˆ†ææ˜¾ç¤ºä¸åŒæ¨ç†æ¨¡å¼ç»“åˆèƒ½é™ä½é”™è¯¯ç‡ï¼Œå¹¶å¼ºè°ƒç»“æ„æ€è€ƒå¤šæ ·æ€§çš„é‡è¦æ€§ã€‚</li>
<li>ç¬¬äºŒè½®æ€è€ƒä¸­æ¨¡å‹è§„æ¨¡å¢åŠ æ—¶ï¼Œå®é™…ä¸ç†æƒ³æ¨ç†é—´çš„æ€§èƒ½å·®è·ç¼©å°ï¼Œè¡¨æ˜æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3aff91cd5ba749b198c95509c272438c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7f5421f57f2e2a0546b77a21ef4a6d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e7f8b5a5cc0f08da597be12edd712e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f03691d2ae7d7bc45bbf7827c2e6345f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35f590071393e32839d24130747a48e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d57fc4e618eba57f1d9b3c488597b76c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16db9f51865eab51ee930af2bef54ff8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Causal-Disentanglement-and-Cross-Modal-Alignment-for-Enhanced-Few-Shot-Learning"><a href="#Causal-Disentanglement-and-Cross-Modal-Alignment-for-Enhanced-Few-Shot-Learning" class="headerlink" title="Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot   Learning"></a>Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot   Learning</h2><p><strong>Authors:Tianjiao Jiang, Zhen Zhang, Yuhang Liu, Javen Qinfeng Shi</strong></p>
<p>Few-shot learning (FSL) often requires effective adaptation of models using limited labeled data. However, most existing FSL methods rely on entangled representations, requiring the model to implicitly recover the unmixing process to obtain disentangled representations using only limited supervision, which hinders effective adaptation. Recent theoretical studies show that multimodal contrastive learning methods, such as CLIP, can disentangle latent representations up to linear transformations. In light of this, we propose the Causal CLIP Adapter (CCA), a novel framework that explicitly disentangles visual features extracted from CLIP using unsupervised Independent Component Analysis (ICA). This removes the need to learn the unmixing process from the labeled data, thereby reducing the number of trainable parameters and mitigating overfitting. Taking a step further, while ICA can obtain visual disentangled representations, it may also disrupt CLIPâ€™s intra- and inter-modal alignment. To counteract this, CCA further leverages CLIPâ€™s inherent cross-modal alignment by enhancing it in two ways: unidirectionally, through fine-tuning a CLIP-based text classifier, and bidirectionally, via a cross-attention mechanism that enriches visual and textual representations through mutual interaction. Both unimodal and cross-modal classification outputs can be effectively combined linearly to improve classification accuracy. Extensive experiments on 11 benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches in terms of few-shot performance and robustness to distributional shifts, while maintaining computational efficiency. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/tianjiao-j/CCA">https://github.com/tianjiao-j/CCA</a>. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰é€šå¸¸éœ€è¦åˆ©ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®æœ‰æ•ˆåœ°é€‚åº”æ¨¡å‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„FSLæ–¹æ³•ä¾èµ–äºçº ç¼ è¡¨ç¤ºï¼Œéœ€è¦æ¨¡å‹ä»…é€šè¿‡æœ‰é™çš„ç›‘ç£éšå¼æ¢å¤æ··åˆè¿‡ç¨‹ä»¥è·å¾—è§£çº ç¼ è¡¨ç¤ºï¼Œè¿™é˜»ç¢äº†æœ‰æ•ˆçš„é€‚åº”ã€‚æœ€è¿‘çš„ç†è®ºç ”ç©¶è¡¨æ˜ï¼Œå¦‚CLIPä¹‹ç±»çš„å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¯ä»¥è§£å¼€æ½œåœ¨è¡¨ç¤ºï¼Œç›´åˆ°çº¿æ€§å˜æ¢ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å› æœCLIPé€‚é…å™¨ï¼ˆCCAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒä½¿ç”¨æ— ç›‘ç£çš„ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰æ˜¾å¼åœ°è§£å¼€ä»CLIPæå–çš„è§†è§‰ç‰¹å¾ã€‚è¿™æ¶ˆé™¤äº†ä»æ ‡è®°æ•°æ®ä¸­å­¦ä¹ æ··åˆè¿‡ç¨‹çš„éœ€æ±‚ï¼Œä»è€Œå‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å¹¶å‡è½»äº†è¿‡æ‹Ÿåˆã€‚æ›´è¿›ä¸€æ­¥çš„æ˜¯ï¼Œè™½ç„¶ICAå¯ä»¥è·å¾—è§†è§‰è§£çº ç¼ è¡¨ç¤ºï¼Œä½†å®ƒä¹Ÿå¯èƒ½ç ´åCLIPçš„è·¨æ¨¡æ€å†…å’Œå¯¹é½ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼ŒCCAè¿›ä¸€æ­¥åˆ©ç”¨CLIPçš„å›ºæœ‰è·¨æ¨¡æ€å¯¹é½ï¼Œé€šè¿‡ä¸¤ç§æ–¹å¼å¢å¼ºå®ƒï¼šå•å‘åœ°ï¼Œé€šè¿‡å¾®è°ƒåŸºäºCLIPçš„æ–‡æœ¬åˆ†ç±»å™¨ï¼›åŒå‘åœ°ï¼Œé€šè¿‡ä¸€ä¸ªä¸°å¯Œè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºçš„äº¤å‰æ³¨æ„æœºåˆ¶è¿›è¡Œç›¸äº’äº¤äº’ã€‚å•æ¨¡æ€å’Œè·¨æ¨¡æ€åˆ†ç±»è¾“å‡ºå¯ä»¥æœ‰æ•ˆåœ°çº¿æ€§ç»„åˆä»¥æé«˜åˆ†ç±»ç²¾åº¦ã€‚åœ¨11ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å°‘é‡æ ·æœ¬æ€§èƒ½å’Œåˆ†å¸ƒè½¬ç§»ç¨³å¥æ€§æ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tianjiao-j/CCA%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/tianjiao-j/CCAä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03102v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºCLIPçš„å› æœCLIPé€‚é…å™¨ï¼ˆCCAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ— ç›‘ç£çš„ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰æ˜¾å¼åœ°è§£çº ç¼ CLIPæå–çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é€‚åº”èƒ½åŠ›å¹¶é™ä½å¯¹æ ‡ç­¾æ•°æ®çš„ä¾èµ–æ€§ã€‚é€šè¿‡ç»“åˆCLIPæœ¬èº«çš„è·¨æ¨¡æ€å¯¹é½æŠ€æœ¯ï¼Œè¯¥æ¡†æ¶ä»¥ä¸¤ç§æ–¹å¼è¿›è¡Œæ”¹è¿›ï¼šå•å‘å¢å¼ºï¼Œé€šè¿‡å¾®è°ƒCLIPçš„æ–‡æœ¬åˆ†ç±»å™¨æ¥å®ç°ï¼Œä»¥åŠåŒå‘å¢å¼ºï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶ä¸°å¯Œè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºä»¥å®ç°åŒå‘äº’åŠ¨ã€‚ç»“åˆå•æ¨¡æ€å’Œè·¨æ¨¡æ€åˆ†ç±»è¾“å‡ºå¯ä»¥æé«˜åˆ†ç±»ç²¾åº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ•°æ ·æœ¬æ€§èƒ½ã€åˆ†å¸ƒè½¬ç§»é²æ£’æ€§å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å°‘æ•°å­¦ä¹ ï¼ˆFSLï¼‰éœ€è¦åˆ©ç”¨æœ‰é™çš„æ ‡ç­¾æ•°æ®æœ‰æ•ˆé€‚åº”æ¨¡å‹ã€‚</li>
<li>ç°æœ‰FSLæ–¹æ³•ä¾èµ–äºçº ç¼ è¡¨ç¤ºï¼Œéœ€è¦æ¨¡å‹éšå¼æ¢å¤æœªæ··åˆè¿‡ç¨‹ä»¥è·å¾—è§£çº ç¼ è¡¨ç¤ºï¼Œè¿™é˜»ç¢äº†æœ‰æ•ˆé€‚åº”ã€‚</li>
<li>å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚CLIPï¼‰å¯ä»¥è§£å¼€æ½œåœ¨è¡¨ç¤ºçš„çº¿æ€§å˜æ¢ã€‚</li>
<li>æå‡ºçš„Causal CLIP Adapterï¼ˆCCAï¼‰æ¡†æ¶åˆ©ç”¨ICAæ˜¾å¼è§£çº ç¼ CLIPçš„è§†è§‰ç‰¹å¾ï¼Œæ¶ˆé™¤å­¦ä¹ æœªæ··åˆè¿‡ç¨‹çš„éœ€è¦ï¼Œå‡å°‘è®­ç»ƒå‚æ•°å¹¶å‡è½»è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>CCAåˆ©ç”¨CLIPçš„è·¨æ¨¡æ€å¯¹é½ç‰¹æ€§è¿›è¡Œå•å‘å’ŒåŒå‘å¢å¼ºï¼Œé€šè¿‡å¾®è°ƒCLIPæ–‡æœ¬åˆ†ç±»å™¨å’Œè·¨æ³¨æ„åŠ›æœºåˆ¶å®ç°ã€‚</li>
<li>ç»“åˆå•æ¨¡æ€å’Œè·¨æ¨¡æ€åˆ†ç±»è¾“å‡ºå¯çº¿æ€§åœ°æé«˜åˆ†ç±»ç²¾åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bfc4b4d99be64a915872324975d640e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-543625355961b67ab831e473db16c5ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f21b9eda9d47d7253b9f805d06aa9033.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="KAN-or-MLP-Point-Cloud-Shows-the-Way-Forward"><a href="#KAN-or-MLP-Point-Cloud-Shows-the-Way-Forward" class="headerlink" title="KAN or MLP? Point Cloud Shows the Way Forward"></a>KAN or MLP? Point Cloud Shows the Way Forward</h2><p><strong>Authors:Yan Shi, Qingdong He, Yijun Liu, Xiaoyu Liu, Jingyong Su</strong></p>
<p>Multi-Layer Perceptrons (MLPs) have become one of the fundamental architectural component in point cloud analysis due to its effective feature learning mechanism. However, when processing complex geometric structures in point clouds, MLPsâ€™ fixed activation functions struggle to efficiently capture local geometric features, while suffering from poor parameter efficiency and high model redundancy. In this paper, we propose PointKAN, which applies Kolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate their efficacy in hierarchical feature representation. First, we introduce a Geometric Affine Module (GAM) to transform local features, improving the modelâ€™s robustness to geometric variations. Next, in the Local Feature Processing (LFP), a parallel structure extracts both group-level features and global context, providing a rich representation of both fine details and overall structure. Finally, these features are combined and processed in the Global Feature Processing (GFP). By repeating these operations, the receptive field gradually expands, enabling the model to capture complete geometric information of the point cloud. To overcome the high parameter counts and computational inefficiency of standard KANs, we develop Efficient-KANs in the PointKAN-elite variant, which significantly reduces parameters while maintaining accuracy. Experimental results demonstrate that PointKAN outperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN, and ShapeNetPart, with particularly strong performance in Few-shot Learning task. Additionally, PointKAN achieves substantial reductions in parameter counts and computational complexity (FLOPs). This work highlights the potential of KANs-based architectures in 3D vision and opens new avenues for research in point cloud understanding. </p>
<blockquote>
<p>å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰ç”±äºå…¶æœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ æœºåˆ¶ï¼Œå·²æˆä¸ºç‚¹äº‘åˆ†æä¸­çš„åŸºæœ¬æ¶æ„ç»„ä»¶ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†ç‚¹äº‘ä¸­çš„å¤æ‚å‡ ä½•ç»“æ„æ—¶ï¼ŒMLPsçš„å›ºå®šæ¿€æ´»å‡½æ•°åœ¨æœ‰æ•ˆåœ°æ•è·å±€éƒ¨å‡ ä½•ç‰¹å¾æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼ŒåŒæ—¶è¿˜å­˜åœ¨å‚æ•°æ•ˆç‡ä½ä¸‹å’Œæ¨¡å‹å†—ä½™çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PointKANï¼Œå®ƒå°†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰åº”ç”¨äºç‚¹äº‘åˆ†æä»»åŠ¡ï¼Œä»¥ç ”ç©¶å…¶åœ¨åˆ†å±‚ç‰¹å¾è¡¨ç¤ºä¸­çš„æœ‰æ•ˆæ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå‡ ä½•ä»¿å°„æ¨¡å—ï¼ˆGAMï¼‰æ¥è½¬æ¢å±€éƒ¨ç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹å¯¹å‡ ä½•å˜åŒ–çš„ç¨³å¥æ€§ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨å±€éƒ¨ç‰¹å¾å¤„ç†ï¼ˆLFPï¼‰ä¸­ï¼Œä¸€ä¸ªå¹¶è¡Œç»“æ„æå–äº†ç»„çº§ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œä¸ºç²¾ç»†ç»†èŠ‚å’Œæ•´ä½“ç»“æ„æä¾›äº†ä¸°å¯Œçš„è¡¨ç¤ºã€‚æœ€åï¼Œè¿™äº›ç‰¹å¾åœ¨å…¨å±€ç‰¹å¾å¤„ç†ï¼ˆGFPï¼‰ä¸­è¿›è¡Œç»„åˆå’Œå¤„ç†ã€‚é€šè¿‡é‡å¤è¿™äº›æ“ä½œï¼Œæ„Ÿå—é‡é€æ¸æ‰©å¤§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·ç‚¹äº‘çš„å®Œæ•´å‡ ä½•ä¿¡æ¯ã€‚ä¸ºäº†å…‹æœæ ‡å‡†KANså‚æ•°è¿‡å¤šå’Œè®¡ç®—æ•ˆç‡ä¸é«˜çš„ç¼ºç‚¹ï¼Œæˆ‘ä»¬åœ¨PointKAN-eliteå˜ç§ä¸­å¼€å‘äº†Efficient-KANsï¼Œå®ƒåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPointKANåœ¨ModelNet40ã€ScanObjectNNå’ŒShapeNetPartç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºPointMLPï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æ­¤å¤–ï¼ŒPointKANåœ¨å‚æ•°æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼ˆFLOPsï¼‰æ–¹é¢å®ç°äº†æ˜¾è‘—å‡å°‘ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†åŸºäºKANsçš„æ¶æ„åœ¨3Dè§†è§‰ä¸­çš„æ½œåŠ›ï¼Œä¸ºç‚¹äº‘ç†è§£ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13593v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºPointKANï¼Œåº”ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰äºç‚¹äº‘åˆ†æä»»åŠ¡ï¼Œä»¥ç ”ç©¶å…¶åœ¨åˆ†å±‚ç‰¹å¾è¡¨ç¤ºä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¼•å…¥å‡ ä½•ä»¿å°„æ¨¡å—ï¼ˆGAMï¼‰å’Œæœ¬åœ°ç‰¹å¾å¤„ç†ï¼ˆLFPï¼‰ï¼ŒPointKANèƒ½æœ‰æ•ˆè½¬æ¢å±€éƒ¨ç‰¹å¾å¹¶ä¸°å¯Œè¡¨ç¤ºå½¢å¼ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡å¤æ“ä½œï¼Œæ¨¡å‹çš„æ„Ÿå—é‡é€æ¸æ‰©å¤§ï¼Œèƒ½æ•æ‰ç‚¹äº‘çš„å®Œæ•´å‡ ä½•ä¿¡æ¯ã€‚é’ˆå¯¹æ ‡å‡†KANså‚æ•°å¤šã€è®¡ç®—æ•ˆç‡ä½çš„é—®é¢˜ï¼Œå¼€å‘å‡ºé«˜æ•ˆçš„PointKAN-eliteå˜ä½“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPointKANåœ¨ModelNet40ã€ScanObjectNNå’ŒShapeNetPartç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºPointMLPï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MLPåœ¨ç‚¹äº‘åˆ†æä¸­æ˜¯åŸºæœ¬çš„æ¶æ„ç»„ä»¶ï¼Œä½†åœ¨å¤„ç†å¤æ‚å‡ ä½•ç»“æ„æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>PointKANä½¿ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰è¿›è¡Œç‚¹äº‘åˆ†æï¼Œæ—¨åœ¨æé«˜ç‰¹å¾è¡¨ç¤ºçš„å±‚æ¬¡æ€§ã€‚</li>
<li>å¼•å…¥å‡ ä½•ä»¿å°„æ¨¡å—ï¼ˆGAMï¼‰æ”¹å–„æ¨¡å‹å¯¹å‡ ä½•å˜åŒ–çš„ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡æœ¬åœ°ç‰¹å¾å¤„ç†ï¼ˆLFPï¼‰å’Œå…¨å±€ç‰¹å¾å¤„ç†ï¼ˆGFPï¼‰ï¼Œæ¨¡å‹èƒ½æ•æ‰ç‚¹äº‘çš„ç²¾ç»†ç»†èŠ‚å’Œæ•´ä½“ç»“æ„ã€‚</li>
<li>PointKAN-eliteçš„å¼€å‘æé«˜äº†å‚æ•°æ•ˆç‡å¹¶ç»´æŒäº†å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºPointKANåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºPointMLPï¼Œå°¤å…¶åœ¨å°æ ·å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</li>
<li>è¿™é¡¹å·¥ä½œçªæ˜¾äº†KANsåœ¨3Dè§†è§‰ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä¸ºç‚¹äº‘ç†è§£ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6c083d1944909db58632da22de723a7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f5f4d6deb2eae82ddd8c1e23714ce74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc35a0c67d1b9e3940ba8a47f1442636.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ddb489054b707668a09f4cbdf80426ef.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MultiADS-Defect-aware-Supervision-for-Multi-type-Anomaly-Detection-and-Segmentation-in-Zero-Shot-Learning"><a href="#MultiADS-Defect-aware-Supervision-for-Multi-type-Anomaly-Detection-and-Segmentation-in-Zero-Shot-Learning" class="headerlink" title="MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and   Segmentation in Zero-Shot Learning"></a>MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and   Segmentation in Zero-Shot Learning</h2><p><strong>Authors:Ylli Sadikaj, Hongkuan Zhou, Lavdim Halilaj, Stefan Schmid, Steffen Staab, Claudia Plant</strong></p>
<p>Precise optical inspection in industrial applications is crucial for minimizing scrap rates and reducing the associated costs. Besides merely detecting if a product is anomalous or not, it is crucial to know the distinct type of defect, such as a bent, cut, or scratch. The ability to recognize the â€œexactâ€ defect type enables automated treatments of the anomalies in modern production lines. Current methods are limited to solely detecting whether a product is defective or not without providing any insights on the defect type, nevertheless detecting and identifying multiple defects. We propose MultiADS, a zero-shot learning approach, able to perform Multi-type Anomaly Detection and Segmentation. The architecture of MultiADS comprises CLIP and extra linear layers to align the visual- and textual representation in a joint feature space. To the best of our knowledge, our proposal, is the first approach to perform a multi-type anomaly segmentation task in zero-shot learning. Contrary to the other baselines, our approach i) generates specific anomaly masks for each distinct defect type, ii) learns to distinguish defect types, and iii) simultaneously identifies multiple defect types present in an anomalous product. Additionally, our approach outperforms zero&#x2F;few-shot learning SoTA methods on image-level and pixel-level anomaly detection and segmentation tasks on five commonly used datasets: MVTec-AD, Visa, MPDD, MAD and Real-IAD. </p>
<blockquote>
<p>åœ¨å·¥ä¸šåº”ç”¨ä¸­ï¼Œç²¾ç¡®çš„å…‰å­¦æ£€æµ‹å¯¹äºæœ€å°åŒ–åºŸå“ç‡å’Œé™ä½ç›¸å…³æˆæœ¬è‡³å…³é‡è¦ã€‚é™¤äº†æ£€æµ‹äº§å“æ˜¯å¦å¼‚å¸¸ä¹‹å¤–ï¼Œäº†è§£ç¼ºé™·çš„ç‰¹å®šç±»å‹ï¼Œå¦‚å¼¯æ›²ã€åˆ‡å‰²æˆ–åˆ’ç—•ï¼Œä¹Ÿéå¸¸å…³é”®ã€‚è¯†åˆ«â€œç²¾ç¡®â€ç¼ºé™·ç±»å‹çš„èƒ½åŠ›ï¼Œä½¿ç°ä»£ç”Ÿäº§çº¿èƒ½å¤Ÿå¯¹å¼‚å¸¸æƒ…å†µè¿›è¡Œè‡ªåŠ¨åŒ–å¤„ç†ã€‚å½“å‰çš„æ–¹æ³•ä»…é™äºæ£€æµ‹äº§å“æ˜¯å¦ç¼ºé™·ï¼Œè€Œæ— æ³•æä¾›å…³äºç¼ºé™·ç±»å‹çš„ä»»ä½•è§è§£ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œå®ƒä»¬å¯ä»¥æ£€æµ‹å’Œè¯†åˆ«å¤šç§ç¼ºé™·ã€‚æˆ‘ä»¬æå‡ºäº†MultiADSï¼Œè¿™æ˜¯ä¸€ç§é›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤šç±»å‹å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²ã€‚MultiADSçš„æ¶æ„åŒ…æ‹¬CLIPå’Œé¢å¤–çš„çº¿æ€§å±‚ï¼Œä»¥åœ¨è”åˆç‰¹å¾ç©ºé—´ä¸­å¯¹é½è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æè®®æ˜¯ç¬¬ä¸€ä¸ªåœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­è¿›è¡Œå¤šç±»å‹å¼‚å¸¸åˆ†å‰²ä»»åŠ¡çš„æ–¹æ³•ã€‚ä¸å…¶ä»–åŸºçº¿æ–¹æ³•ç›¸åï¼Œæˆ‘ä»¬çš„æ–¹æ³•iï¼‰ä¸ºæ¯ç§ä¸åŒçš„ç¼ºé™·ç±»å‹ç”Ÿæˆç‰¹å®šçš„å¼‚å¸¸æ©ç ï¼Œiiï¼‰å­¦ä¹ åŒºåˆ†ç¼ºé™·ç±»å‹ï¼Œå¹¶iiiï¼‰åŒæ—¶è¯†åˆ«å¼‚å¸¸äº§å“ä¸­å­˜åœ¨çš„å¤šç§ç¼ºé™·ç±»å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šï¼Œå¯¹å›¾åƒçº§å’Œåƒç´ çº§çš„å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ï¼Œå‡è¶…è¶Šäº†é›¶æ ·æœ¬&#x2F;å°æ ·å­¦ä¹ é¢†åŸŸçš„æœ€æ–°æ–¹æ³•ï¼Œè¿™äº›æ•°æ®é›†åŒ…æ‹¬MVTec-ADã€Visaã€MPDDã€MADå’ŒReal-IADã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06740v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å·¥ä¸šåº”ç”¨ä¸­ç²¾ç¡®å…‰å­¦æ£€æµ‹çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰æ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé›¶æ ·æœ¬å­¦ä¹ çš„æ–°å‹å¤šç±»å‹å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²æ–¹æ³•MultiADSã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆç‰¹å®šå¼‚å¸¸æ©è†œï¼ŒåŒºåˆ†å¹¶è¯†åˆ«äº§å“ä¸­çš„å¤šç§å¼‚å¸¸ç±»å‹ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨äº”ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šçš„å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„é›¶æ ·æœ¬&#x2F;å°æ ·å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç²¾ç¡®å…‰å­¦æ£€æµ‹å¯¹å·¥ä¸šåº”ç”¨è‡³å…³é‡è¦ï¼Œèƒ½é™ä½åºŸå“ç‡å’Œç›¸å…³æˆæœ¬ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦æ£€æµ‹äº§å“æ˜¯å¦å¼‚å¸¸ï¼Œä½†æ— æ³•è¯†åˆ«å…·ä½“çš„ç¼ºé™·ç±»å‹ã€‚</li>
<li>MultiADSæ˜¯ä¸€ç§é›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤šç±»å‹å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²ã€‚</li>
<li>MultiADSé‡‡ç”¨CLIPç»“æ„å¹¶å¢åŠ é¢å¤–çº¿æ€§å±‚ï¼Œå®ç°è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºçš„è”åˆç‰¹å¾ç©ºé—´å¯¹é½ã€‚</li>
<li>MultiADSèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹æ¯ç§ç‰¹å®šç¼ºé™·çš„å¼‚å¸¸æ©è†œã€‚</li>
<li>MultiADSèƒ½åŒºåˆ†å¹¶åŒæ—¶è¯†åˆ«äº§å“ä¸­çš„å¤šç§å¼‚å¸¸ç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-410fa5bb46ed4c6e270b8c5345daae99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c85883e3bf943ba70ae8400224e0a231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd32f8f6a09cb077774387878841eedd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DivCon-NeRF-Diverse-and-Consistent-Ray-Augmentation-for-Few-Shot-NeRF"><a href="#DivCon-NeRF-Diverse-and-Consistent-Ray-Augmentation-for-Few-Shot-NeRF" class="headerlink" title="DivCon-NeRF: Diverse and Consistent Ray Augmentation for Few-Shot NeRF"></a>DivCon-NeRF: Diverse and Consistent Ray Augmentation for Few-Shot NeRF</h2><p><strong>Authors:Ingyun Lee, Jae Won Jang, Seunghyeon Seo, Nojun Kwak</strong></p>
<p>Neural Radiance Field (NeRF) has shown remarkable performance in novel view synthesis but requires numerous multi-view images, limiting its practicality in few-shot scenarios. Ray augmentation has been proposed to alleviate overfitting caused by sparse training data by generating additional rays. However, existing methods, which generate augmented rays only near the original rays, exhibit pronounced floaters and appearance distortions due to limited viewpoints and inconsistent rays obstructed by nearby obstacles and complex surfaces. To address these problems, we propose DivCon-NeRF, which introduces novel sphere-based ray augmentations to significantly enhance both diversity and consistency. By employing a virtual sphere centered at the predicted surface point, our method generates diverse augmented rays from all 360-degree directions, facilitated by our consistency mask that effectively filters out inconsistent rays. We introduce tailored loss functions that leverage these augmentations, effectively reducing floaters and visual distortions. Consequently, our method outperforms existing few-shot NeRF approaches on the Blender, LLFF, and DTU datasets. Furthermore, DivCon-NeRF demonstrates strong generalizability by effectively integrating with both regularization- and framework-based few-shot NeRFs. Our code will be made publicly available. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†éœ€è¦å¤§é‡å¤šè§†å›¾å›¾åƒï¼Œè¿™åœ¨å°‘é•œå¤´åœºæ™¯ä¸­é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚å°„çº¿å¢å¼ºæ³•è¢«æå‡ºé€šè¿‡ç”Ÿæˆé¢å¤–çš„å°„çº¿æ¥ç¼“è§£ç¨€ç–è®­ç»ƒæ•°æ®å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»…åœ¨åŸå§‹å°„çº¿é™„è¿‘ç”Ÿæˆå¢å¼ºå°„çº¿ï¼Œç”±äºæœ‰é™çš„è§‚ç‚¹ã€é™„è¿‘éšœç¢å’Œå¤æ‚è¡¨é¢çš„å½±å“ï¼Œä¼šå‡ºç°æ˜æ˜¾çš„æµ®ç‚¹å’Œå¤–è§‚å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DivCon-NeRFï¼Œå®ƒå¼•å…¥äº†åŸºäºæ–°å‹çƒä½“çš„å°„çº¿å¢å¼ºæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡ä»¥é¢„æµ‹çš„è¡¨é¢ç‚¹ä¸ºä¸­å¿ƒæ„å»ºä¸€ä¸ªè™šæ‹Ÿçƒä½“ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»æ‰€æœ‰360åº¦çš„æ–¹å‘ç”Ÿæˆå¤šæ ·åŒ–çš„å¢å¼ºå°„çº¿ï¼Œå¹¶é€šè¿‡æˆ‘ä»¬çš„ä¸€è‡´æ€§æ©è†œæœ‰æ•ˆåœ°è¿‡æ»¤å‡ºä¸ä¸€è‡´çš„å°„çº¿ã€‚æˆ‘ä»¬å¼•å…¥äº†åˆ©ç”¨è¿™äº›å¢å¼ºçš„å®šåˆ¶æŸå¤±å‡½æ•°ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†æµ®ç‚¹å’Œè§†è§‰å¤±çœŸã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Blenderã€LLFFå’ŒDTUæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å°‘é•œå¤´NeRFæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒDivCon-NeRFå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½æœ‰æ•ˆåœ°ä¸åŸºäºæ­£åˆ™åŒ–å’Œæ¡†æ¶çš„å°‘é•œå¤´NeRFç›¸ç»“åˆã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12947v2">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>NeRFåœ¨æ–°å‹è§†å›¾åˆæˆä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†éœ€å¤§é‡å¤šè§†å›¾å›¾åƒï¼Œè¿™åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºç¼“è§£ç¨€ç–è®­ç»ƒæ•°æ®å¼•èµ·çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæå‡ºäº†å°„çº¿å¢å¼ºæ–¹æ³•ç”Ÿæˆé¢å¤–å°„çº¿ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»…åœ¨æ¥è¿‘åŸå§‹å°„çº¿æ—¶ç”Ÿæˆå¢å¼ºå°„çº¿ï¼Œå› æœ‰é™çš„è§†è§’å’Œä¸ä¸€è‡´çš„å°„çº¿è¢«é™„è¿‘éšœç¢ç‰©å’Œå¤æ‚è¡¨é¢é˜»æŒ¡ï¼Œå¯¼è‡´å‡ºç°æ˜æ˜¾çš„æµ®ä½“å’Œå¤–è§‚å¤±çœŸã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºDivCon-NeRFï¼Œé€šè¿‡åŸºäºçƒä½“çš„æ–°å‹å°„çº¿å¢å¼ºæ–¹æ³•ï¼Œæ˜¾è‘—å¢å¼ºå¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚ä»¥é¢„æµ‹è¡¨é¢ç‚¹ä¸ºä¸­å¿ƒçš„è™šæ‹Ÿçƒä½“ç”Ÿæˆæ¥è‡ªæ‰€æœ‰360åº¦æ–¹å‘çš„å¤šæ ·åŒ–å¢å¼ºå°„çº¿ï¼Œç”±æˆ‘ä»¬çš„ä¸€è‡´æ€§æ©è†œæœ‰æ•ˆè¿‡æ»¤å‡ºä¸ä¸€è‡´çš„å°„çº¿ã€‚å¼•å…¥å®šåˆ¶çš„æŸè€—å‡½æ•°ï¼Œåˆ©ç”¨è¿™äº›å¢å¼ºæ–¹æ³•ï¼Œæœ‰æ•ˆé™ä½æµ®ä½“å’Œè§†è§‰å¤±çœŸã€‚å› æ­¤ï¼ŒDivCon-NeRFåœ¨Blenderã€LLFFå’ŒDTUæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„å°‘æ ·æœ¬NeRFæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒDivCon-NeRFé€šè¿‡ä¸æ­£åˆ™åŒ–å’Œæ¡†æ¶åŸºç¡€ä¸Šçš„å°‘æ ·æœ¬NeRFè¿›è¡Œæœ‰æ•ˆæ•´åˆï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFåœ¨è§†å›¾åˆæˆä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­å› éœ€è¦å¤šè§†å›¾å›¾åƒè€Œå—åˆ°é™åˆ¶ã€‚</li>
<li>ç°æœ‰å°„çº¿å¢å¼ºæ–¹æ³•ä»…åœ¨æ¥è¿‘åŸå§‹å°„çº¿æ—¶ç”Ÿæˆå¢å¼ºå°„çº¿ï¼Œå­˜åœ¨æµ®ä½“å’Œå¤–è§‚å¤±çœŸé—®é¢˜ã€‚</li>
<li>DivCon-NeRFé€šè¿‡åŸºäºçƒä½“çš„æ–°å‹å°„çº¿å¢å¼ºæ–¹æ³•ï¼Œå¢å¼ºå¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>DivCon-NeRFé‡‡ç”¨è™šæ‹Ÿçƒä½“ç”Ÿæˆå¤šæ ·åŒ–å¢å¼ºå°„çº¿ï¼Œå¹¶è¿‡æ»¤å‡ºä¸ä¸€è‡´çš„å°„çº¿ã€‚</li>
<li>å®šåˆ¶æŸè€—å‡½æ•°åˆ©ç”¨å¢å¼ºæ–¹æ³•ï¼Œé™ä½æµ®ä½“å’Œè§†è§‰å¤±çœŸã€‚</li>
<li>DivCon-NeRFåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰å°‘æ ·æœ¬NeRFæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1fc9044fe906fb7e967040ba4fe4a47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4e082a672359d7707a671c92865aaaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2856282c7c5ddae0d763d90b19ed5355.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa2e1c850c9e2d627a552569db3e3636.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="End-to-End-Protocol-for-High-Quality-QAOA-Parameters-with-Few-Shots"><a href="#End-to-End-Protocol-for-High-Quality-QAOA-Parameters-with-Few-Shots" class="headerlink" title="End-to-End Protocol for High-Quality QAOA Parameters with Few Shots"></a>End-to-End Protocol for High-Quality QAOA Parameters with Few Shots</h2><p><strong>Authors:Tianyi Hao, Zichang He, Ruslan Shaydulin, Jeffrey Larson, Marco Pistoia</strong></p>
<p>The quantum approximate optimization algorithm (QAOA) is a quantum heuristic for combinatorial optimization that has been demonstrated to scale better than state-of-the-art classical solvers for some problems. For a given problem instance, QAOA performance depends crucially on the choice of the parameters. While average-case optimal parameters are available in many cases, meaningful performance gains can be obtained by fine-tuning these parameters for a given instance. This task is especially challenging, however, when the number of circuit executions (shots) is limited. In this work, we develop an end-to-end protocol that combines multiple parameter settings and fine-tuning techniques. We use large-scale numerical experiments to optimize the protocol for the shot-limited setting and observe that optimizers with the simplest internal model (linear) perform best. We implement the optimized pipeline on a trapped-ion processor using up to 32 qubits and 5 QAOA layers, and we demonstrate that the pipeline is robust to small amounts of hardware noise. To the best of our knowledge, these are the largest demonstrations of QAOA parameter fine-tuning on a trapped-ion processor in terms of 2-qubit gate count. </p>
<blockquote>
<p>é‡å­è¿‘ä¼¼ä¼˜åŒ–ç®—æ³•ï¼ˆQAOAï¼‰æ˜¯ä¸€ç§ç»„åˆä¼˜åŒ–çš„é‡å­å¯å‘å¼ç®—æ³•ï¼Œå¯¹äºæŸäº›é—®é¢˜ï¼Œå®ƒçš„æ‰©å±•æ€§å·²ç»è¯æ˜ä¼˜äºæœ€å…ˆè¿›çš„ç»å…¸æ±‚è§£å™¨ã€‚å¯¹äºç»™å®šçš„é—®é¢˜å®ä¾‹ï¼ŒQAOAçš„æ€§èƒ½å…³é”®åœ¨äºå‚æ•°çš„é€‰æ‹©ã€‚è™½ç„¶åœ¨è®¸å¤šæƒ…å†µä¸‹å­˜åœ¨å¹³å‡æƒ…å†µä¸‹çš„æœ€ä¼˜å‚æ•°ï¼Œä½†é€šè¿‡é’ˆå¯¹ç»™å®šå®ä¾‹å¾®è°ƒè¿™äº›å‚æ•°å¯ä»¥è·å¾—æœ‰æ„ä¹‰çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå½“ç”µè·¯æ‰§è¡Œæ¬¡æ•°ï¼ˆshotsï¼‰æœ‰é™æ—¶ï¼Œè¿™é¡¹ä»»åŠ¡å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç«¯åˆ°ç«¯çš„åè®®ï¼Œè¯¥åè®®ç»“åˆäº†å¤šç§å‚æ•°è®¾ç½®å’Œå¾®è°ƒæŠ€æœ¯ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§è§„æ¨¡æ•°å€¼å®éªŒæ¥ä¼˜åŒ–é€‚ç”¨äºæœ‰é™æ‰§è¡Œæ¬¡æ•°è®¾ç½®çš„åè®®ï¼Œå¹¶è§‚å¯Ÿåˆ°å…·æœ‰æœ€ç®€å•å†…éƒ¨æ¨¡å‹ï¼ˆçº¿æ€§ï¼‰çš„ä¼˜åŒ–å™¨è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬åœ¨ä½¿ç”¨æœ€å¤š32ä¸ªé‡å­æ¯”ç‰¹å’Œ5ä¸ªQAOAå±‚çš„ç¦»å­é˜±å¤„ç†å™¨ä¸Šå®ç°äº†ä¼˜åŒ–åçš„ç®¡é“ï¼Œå¹¶è¯æ˜è¯¥ç®¡é“å¯¹å°‘é‡çš„ç¡¬ä»¶å™ªå£°å…·æœ‰é²æ£’æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯åŸºäºç¦»å­é˜±å¤„ç†å™¨è¿›è¡ŒQAOAå‚æ•°ç»†è°ƒçš„æœ€å¤§çš„å±•ç¤ºï¼ŒæŒ‰ä¸¤é‡å­ä½é—¨è®¡æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00557v4">PDF</a> 13+2 pages, 11+3 figures, accepted by Physical Review Research</p>
<p><strong>Summary</strong><br>     é‡å­è¿‘ä¼¼ä¼˜åŒ–ç®—æ³•ï¼ˆQAOAï¼‰æ˜¯ä¸€ç§ç»„åˆä¼˜åŒ–çš„é‡å­å¯å‘å¼ç®—æ³•ï¼Œé’ˆå¯¹æŸäº›é—®é¢˜ï¼Œå…¶æ‰©å±•æ€§ä¼˜äºç°æœ‰ç»å…¸æ±‚è§£å™¨ã€‚å¯¹äºç»™å®šçš„é—®é¢˜å®ä¾‹ï¼ŒQAOAçš„æ€§èƒ½å–å†³äºå‚æ•°çš„é€‰æ‹©ã€‚è™½ç„¶å¹³å‡æƒ…å†µä¸‹çš„æœ€ä¼˜å‚æ•°åœ¨è®¸å¤šæƒ…å†µä¸‹æ˜¯å¯ç”¨çš„ï¼Œä½†é€šè¿‡é’ˆå¯¹ç»™å®šå®ä¾‹å¾®è°ƒè¿™äº›å‚æ•°å¯ä»¥è·å¾—æœ‰æ„ä¹‰çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå½“ç”µè·¯æ‰§è¡Œæ¬¡æ•°ï¼ˆshotsï¼‰æœ‰é™æ—¶ï¼Œè¿™é¡¹ä»»åŠ¡å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§ç»“åˆå¤šç§å‚æ•°è®¾ç½®å’Œå¾®è°ƒæŠ€æœ¯çš„ç«¯åˆ°ç«¯åè®®ã€‚é€šè¿‡å¤§è§„æ¨¡æ•°å€¼å®éªŒä¼˜åŒ–é€‚ç”¨äºæœ‰é™æ‰§è¡Œæ¬¡æ•°è®¾ç½®çš„åè®®ï¼Œå¹¶è§‚å¯Ÿåˆ°å…·æœ‰æœ€ç®€å•å†…éƒ¨æ¨¡å‹ï¼ˆçº¿æ€§ï¼‰çš„ä¼˜åŒ–å™¨è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬åœ¨ç¦»å­é˜±å¤„ç†å™¨ä¸Šå®ç°äº†ä¼˜åŒ–åçš„ç®¡é“ï¼Œä½¿ç”¨æœ€å¤š32ä¸ªé‡å­æ¯”ç‰¹å’Œ5ä¸ªQAOAå±‚ï¼Œå¹¶è¯æ˜è¯¥ç®¡é“å¯¹å°‘é‡çš„ç¡¬ä»¶å™ªå£°å…·æœ‰é²æ£’æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯åŸºäºç¦»å­é˜±å¤„ç†å™¨ä¸ŠQAOAå‚æ•°å¾®è°ƒçš„æœ€å¤§æ¼”ç¤ºï¼ŒæŒ‰ä¸¤é‡å­ä½é—¨è®¡æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>QAOAæ˜¯ä¸€ç§ç”¨äºç»„åˆä¼˜åŒ–çš„é‡å­å¯å‘å¼ç®—æ³•ï¼Œé’ˆå¯¹æŸäº›é—®é¢˜å…·æœ‰ä¼˜å¼‚çš„æ‰©å±•æ€§ã€‚</li>
<li>QAOAæ€§èƒ½å–å†³äºå‚æ•°é€‰æ‹©ï¼Œå¯ä»¥é€šè¿‡å¾®è°ƒå‚æ•°æ¥æå‡æ€§èƒ½ã€‚</li>
<li>å½“ç”µè·¯æ‰§è¡Œæ¬¡æ•°æœ‰é™æ—¶ï¼Œå‚æ•°å¾®è°ƒå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§ç»“åˆå¤šç§å‚æ•°è®¾ç½®å’Œå¾®è°ƒæŠ€æœ¯çš„ç«¯åˆ°ç«¯åè®®ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡æ•°å€¼å®éªŒä¼˜åŒ–åè®®ï¼Œå‘ç°å…·æœ‰ç®€å•å†…éƒ¨æ¨¡å‹çš„ä¼˜åŒ–å™¨è¡¨ç°æœ€ä½³ã€‚</li>
<li>åœ¨ç¦»å­é˜±å¤„ç†å™¨ä¸Šå®ç°äº†ä¼˜åŒ–åçš„ç®¡é“ï¼Œå±•ç¤ºäº†å…¶å¯¹ç¡¬ä»¶å™ªå£°çš„é²æ£’æ€§ã€‚</li>
<li>è¿™æ˜¯è¿„ä»Šä¸ºæ­¢åœ¨ç¦»å­é˜±å¤„ç†å™¨ä¸Šè¿›è¡Œçš„æœ€å¤§è§„æ¨¡çš„QAOAå‚æ•°å¾®è°ƒæ¼”ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7944c7e173eb066f7f0a914f123a0ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c615914977e4026cccd26def9335c2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c139b615b3c48921d00d4760158a35ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-807d3121defabc471a1221305249badf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c5a6b0010dcec3d6ae49bd08f7e4a51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bcf426aec09d155cbe9c057b8c38c6b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e878d35c5a7feb357c5c95e20a3f75cf.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  GanitBench A bi-lingual benchmark for evaluating mathematical reasoning   in Vision Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-66775d0a15f700bc915885216720c027.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  From MAS to MARS Coordination Failures and Reasoning Trade-offs in   Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
