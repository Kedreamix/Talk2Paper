<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  HierarchicalPrune Position-Aware Compression for Large-Scale Diffusion   Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7b911a887b1630bae4ef42b3788f6ba8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-08-æ›´æ–°"><a href="#2025-08-08-æ›´æ–°" class="headerlink" title="2025-08-08 æ›´æ–°"></a>2025-08-08 æ›´æ–°</h1><h2 id="HierarchicalPrune-Position-Aware-Compression-for-Large-Scale-Diffusion-Models"><a href="#HierarchicalPrune-Position-Aware-Compression-for-Large-Scale-Diffusion-Models" class="headerlink" title="HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion   Models"></a>HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion   Models</h2><p><strong>Authors:Young D. Kwon, Rui Li, Sijia Li, Da Li, Sourav Bhattacharya, Stylianos I. Venieris</strong></p>
<p>State-of-the-art text-to-image diffusion models (DMs) achieve remarkable quality, yet their massive parameter scale (8-11B) poses significant challenges for inferences on resource-constrained devices. In this paper, we present HierarchicalPrune, a novel compression framework grounded in a key observation: DM blocks exhibit distinct functional hierarchies, where early blocks establish semantic structures while later blocks handle texture refinements. HierarchicalPrune synergistically combines three techniques: (1) Hierarchical Position Pruning, which identifies and removes less essential later blocks based on position hierarchy; (2) Positional Weight Preservation, which systematically protects early model portions that are essential for semantic structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts knowledge-transfer intensity based on our discovery of block-wise sensitivity variations. As a result, our framework brings billion-scale diffusion models into a range more suitable for on-device inference, while preserving the quality of the output images. Specifically, when combined with INT4 weight quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction (e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score and 7% in HPSv2 score compared to the original model. Last but not least, our comprehensive user study with 85 participants demonstrates that HierarchicalPrune maintains perceptual quality comparable to the original model while significantly outperforming prior works. </p>
<blockquote>
<p>å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è¾¾åˆ°äº†æ˜¾è‘—çš„è´¨é‡ï¼Œç„¶è€Œå…¶å·¨å¤§çš„å‚æ•°è§„æ¨¡ï¼ˆ8-11Bï¼‰å¯¹èµ„æºå—é™è®¾å¤‡ä¸Šçš„æ¨ç†æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HierarchicalPruneï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å‹ç¼©æ¡†æ¶ï¼Œå®ƒåŸºäºä¸€ä¸ªå…³é”®è§‚å¯Ÿï¼šDMå—è¡¨ç°å‡ºä¸åŒçš„åŠŸèƒ½å±‚æ¬¡ï¼Œæ—©æœŸå—å»ºç«‹è¯­ä¹‰ç»“æ„ï¼Œè€ŒåæœŸå—å¤„ç†çº¹ç†ç»†åŒ–ã€‚HierarchicalPruneååŒç»“åˆäº†ä¸‰ç§æŠ€æœ¯ï¼šï¼ˆ1ï¼‰å±‚æ¬¡ä½ç½®å‰ªæï¼Œå®ƒåŸºäºä½ç½®å±‚æ¬¡è¯†åˆ«å¹¶ç§»é™¤è¾ƒä¸é‡è¦çš„åæœŸå—ï¼›ï¼ˆ2ï¼‰ä½ç½®æƒé‡ä¿ç•™ï¼Œå®ƒç³»ç»Ÿåœ°ä¿æŠ¤æ—©æœŸæ¨¡å‹éƒ¨åˆ†ï¼Œå¯¹äºè¯­ä¹‰ç»“æ„å®Œæ•´æ€§è‡³å…³é‡è¦ï¼›ï¼ˆ3ï¼‰æ•æ„Ÿåº¦å¼•å¯¼è’¸é¦ï¼Œå®ƒæ ¹æ®æˆ‘ä»¬å‘ç°çš„å—çŠ¶æ•æ„Ÿåº¦å˜åŒ–æ¥è°ƒæ•´çŸ¥è¯†è½¬ç§»å¼ºåº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å°†æ•°åäº¿è§„æ¨¡çš„æ‰©æ•£æ¨¡å‹å¼•å…¥åˆ°æ›´é€‚åˆäºè®¾å¤‡ç«¯æ¨ç†çš„èŒƒå›´ï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºå›¾åƒçš„è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œå½“ä¸INT4æƒé‡é‡åŒ–ç»“åˆæ—¶ï¼ŒHierarchicalPruneå®ç°äº†77.5-80.4%çš„å†…å­˜å ç”¨å‡å°‘ï¼ˆä¾‹å¦‚ï¼Œä»15.8 GBå‡å°‘åˆ°3.2 GBï¼‰ï¼Œå¹¶åœ¨æœåŠ¡å™¨å’Œæ¶ˆè´¹çº§GPUä¸Šå®ç°äº†27.9-38.0%çš„å»¶è¿Ÿå‡å°‘ï¼Œä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼ŒGenEvalå¾—åˆ†æœ€ä½ä¸‹é™2.6%ï¼ŒHPSv2å¾—åˆ†ä¸‹é™7%ã€‚æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç»¼åˆç”¨æˆ·ç ”ç©¶æœ‰85åå‚ä¸è€…è¯æ˜ï¼ŒHierarchicalPruneåœ¨ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„å¯æ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04663v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„å‹ç¼©æ¡†æ¶â€”â€”HierarchicalPruneã€‚è¯¥æ¡†æ¶è§‚å¯Ÿåˆ°DMå—å…·æœ‰ä¸åŒçš„åŠŸèƒ½å±‚æ¬¡ï¼Œæ—©æœŸå—å»ºç«‹è¯­ä¹‰ç»“æ„ï¼ŒåæœŸå—å¤„ç†çº¹ç†ç»†åŒ–ã€‚é€šè¿‡ç»“åˆä¸‰ç§æŠ€æœ¯ï¼šå±‚æ¬¡ä½ç½®å‰ªæã€ä½ç½®æƒé‡ä¿ç•™å’Œæ•æ„Ÿæ€§å¼•å¯¼è’¸é¦ï¼ŒHierarchicalPruneæˆåŠŸåœ°å°†åäº¿çº§åˆ«çš„æ‰©æ•£æ¨¡å‹é€‚åº”äºè®¾å¤‡ç«¯æ¨ç†ï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºå›¾åƒçš„è´¨é‡ã€‚ä¸INT4æƒé‡é‡åŒ–ç»“åˆï¼ŒHierarchicalPruneå®ç°äº†åœ¨æœåŠ¡å™¨å’Œæ¶ˆè´¹è€…çº§GPUä¸Š77.5%~80.4%çš„å†…å­˜å ç”¨å‡å°‘ï¼ˆä¾‹å¦‚ä»15.8 GBé™è‡³3.2 GBï¼‰å’Œ27.9%~38.0%çš„å»¶è¿Ÿå‡å°‘ï¼ŒåŒæ—¶ç›¸è¾ƒäºåŸå§‹æ¨¡å‹ï¼ŒGenEvalå¾—åˆ†ä»…é™ä½æœ€ä½2.6%ï¼ŒHPSv2å¾—åˆ†é™ä½7%ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹85åå‚ä¸è€…çš„ç»¼åˆç”¨æˆ·ç ”ç©¶è¯æ˜ï¼ŒHierarchicalPruneåœ¨ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„å¯æ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰ä½œå“ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹é¢ä¸´åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šæ¨ç†çš„æŒ‘æˆ˜ï¼Œå…¶å‚æ•°è§„æ¨¡å·¨å¤§ï¼ˆ8~11Bï¼‰ã€‚</li>
<li>HierarchicalPruneæ˜¯ä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„å‹ç¼©æ¡†æ¶ï¼Œå®ƒè§‚å¯Ÿåˆ°DMå—çš„åŠŸèƒ½å±‚æ¬¡å·®å¼‚ã€‚</li>
<li>HierarchicalPruneé€šè¿‡ç»“åˆå±‚æ¬¡ä½ç½®å‰ªæã€ä½ç½®æƒé‡ä¿ç•™å’Œæ•æ„Ÿæ€§å¼•å¯¼è’¸é¦ç­‰æŠ€æœ¯å®ç°äº†å†…å­˜å ç”¨å’Œå»¶è¿Ÿçš„æœ‰æ•ˆå‡å°‘ã€‚</li>
<li>ä¸INT4æƒé‡é‡åŒ–ç»“åˆä½¿ç”¨ï¼ŒHierarchicalPruneå®ç°äº†æ˜¾è‘—çš„å†…å­˜å ç”¨å’Œå»¶è¿Ÿä¼˜åŒ–ã€‚</li>
<li>HierarchicalPruneèƒ½åœ¨ä¿æŒè¾ƒé«˜å›¾åƒè´¨é‡çš„åŒæ—¶å¤§å¹…é™ä½æ¨¡å‹çš„å¤§å°å’Œæ¨ç†æ—¶é—´ã€‚</li>
<li>ç»¼åˆç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒHierarchicalPruneåœ¨ç»´æŒå¯æ„ŸçŸ¥è´¨é‡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e7faf004e352768a1ada429f557c511c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72799118fcba3ceac69746478f5c701d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f03a59ed69eb2d041f93866617cb8be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1af42dd48bb89b76ea5817fa39df4801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90ea0c80967975aae3d21f7c02e978ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96c42bb99a33201bcd707cff59216415.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DDTracking-A-Deep-Generative-Framework-for-Diffusion-MRI-Tractography-with-Streamline-Local-Global-Spatiotemporal-Modeling"><a href="#DDTracking-A-Deep-Generative-Framework-for-Diffusion-MRI-Tractography-with-Streamline-Local-Global-Spatiotemporal-Modeling" class="headerlink" title="DDTracking: A Deep Generative Framework for Diffusion MRI Tractography   with Streamline Local-Global Spatiotemporal Modeling"></a>DDTracking: A Deep Generative Framework for Diffusion MRI Tractography   with Streamline Local-Global Spatiotemporal Modeling</h2><p><strong>Authors:Yijie Li, Wei Zhang, Xi Zhu, Ye Wu, Yogesh Rathi, Lauren J. Oâ€™Donnell, Fan Zhang</strong></p>
<p>This paper presents DDTracking, a novel deep generative framework for diffusion MRI tractography that formulates streamline propagation as a conditional denoising diffusion process. In DDTracking, we introduce a dual-pathway encoding network that jointly models local spatial encoding (capturing fine-scale structural details at each streamline point) and global temporal dependencies (ensuring long-range consistency across the entire streamline). Furthermore, we design a conditional diffusion model module, which leverages the learned local and global embeddings to predict streamline propagation orientations for tractography in an end-to-end trainable manner. We conduct a comprehensive evaluation across diverse, independently acquired dMRI datasets, including both synthetic and clinical data. Experiments on two well-established benchmarks with ground truth (ISMRM Challenge and TractoInferno) demonstrate that DDTracking largely outperforms current state-of-the-art tractography methods. Furthermore, our results highlight DDTrackingâ€™s strong generalizability across heterogeneous datasets, spanning varying health conditions, age groups, imaging protocols, and scanner types. Collectively, DDTracking offers anatomically plausible and robust tractography, presenting a scalable, adaptable, and end-to-end learnable solution for broad dMRI applications. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/yishengpoxiao/DDtracking.git">https://github.com/yishengpoxiao/DDtracking.git</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DDTrackingï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£MRIè¿½è¸ªçš„æ–°å‹æ·±åº¦ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒå°†æµçº¿ä¼ æ’­è¡¨è¿°ä¸ºæ¡ä»¶å»å™ªæ‰©æ•£è¿‡ç¨‹ã€‚åœ¨DDTrackingä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒè·¯å¾„ç¼–ç ç½‘ç»œï¼Œè¯¥ç½‘ç»œåŒæ—¶å¯¹å±€éƒ¨ç©ºé—´ç¼–ç è¿›è¡Œå»ºæ¨¡ï¼ˆæ•è·æ¯æ¡æµçº¿ç‚¹ä¸Šçš„ç²¾ç»†ç»“æ„ç»†èŠ‚ï¼‰å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§ï¼ˆç¡®ä¿æ•´æ¡æµçº¿ä¸Šçš„è¿œç¨‹ä¸€è‡´æ€§ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨å­¦ä¹ åˆ°çš„å±€éƒ¨å’Œå…¨å±€åµŒå…¥æ¥é¢„æµ‹æµçº¿ä¼ æ’­æ–¹å‘ï¼Œä»¥è¿›è¡Œç«¯åˆ°ç«¯çš„è¿½è¸ªã€‚æˆ‘ä»¬åœ¨å¤šç§ç‹¬ç«‹è·å–çš„dMRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬åˆæˆæ•°æ®å’Œä¸´åºŠæ•°æ®ã€‚åœ¨å…·æœ‰çœŸå®å€¼çš„ä¸¤ä¸ªå…¬è®¤åŸºå‡†æµ‹è¯•ï¼ˆISMRMæŒ‘æˆ˜å’ŒTractoInfernoï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDDTrackingåœ¨å½“å‰çš„æœ€æ–°è¿½è¸ªæ–¹æ³•ä¸­æœ‰å¾ˆå¤§çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†DDTrackingåœ¨ä¸åŒæ•°æ®é›†ï¼ˆåŒ…æ‹¬å„ç§å¥åº·çŠ¶å†µã€å¹´é¾„ç»„ã€æˆåƒåè®®å’Œæ‰«æä»ªç±»å‹ï¼‰ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼ŒDDTrackingæä¾›äº†è§£å‰–ä¸Šåˆç†ä¸”ç¨³å¥çš„è¿½è¸ªï¼Œä¸ºå¹¿æ³›çš„dMRIåº”ç”¨æä¾›äº†å¯ä¼¸ç¼©ã€å¯é€‚åº”å’Œç«¯åˆ°ç«¯çš„å¯å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/yishengpoxiao/DDtracking.git">https://github.com/yishengpoxiao/DDtracking.git</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04568v1">PDF</a> Preprint version. The content may be updated in the future</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DDTrackingï¼Œä¸€ç§ç”¨äºæ‰©æ•£MRIè¿½è¸ªçš„æ–°å‹æ·±åº¦ç”Ÿæˆæ¡†æ¶ã€‚å®ƒå°†æµçº¿ä¼ æ’­åˆ¶å®šä¸ºæ¡ä»¶å»å™ªæ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥åŒè·¯å¾„ç¼–ç ç½‘ç»œï¼Œè”åˆå»ºæ¨¡å±€éƒ¨ç©ºé—´ç¼–ç å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§ã€‚é€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¨¡å—ï¼Œåˆ©ç”¨å­¦ä¹ å’Œå…¨å±€åµŒå…¥é¢„æµ‹æµçº¿ä¼ æ’­æ–¹å‘ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å¯è®­ç»ƒæ–¹å¼ã€‚åœ¨å¤šç§ç‹¬ç«‹é‡‡é›†çš„dMRIæ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒDDTrackingåœ¨ç°æœ‰æœ€å…ˆè¿›çš„è¿½è¸ªæ–¹æ³•ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸”åœ¨ä¸åŒæ•°æ®é›†ä¸Šå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDTrackingæ˜¯ä¸€ä¸ªç”¨äºæ‰©æ•£MRIè¿½è¸ªçš„æ·±åº¦ç”Ÿæˆæ¡†æ¶ï¼Œå°†æµçº¿ä¼ æ’­å®šä¹‰ä¸ºæ¡ä»¶å»å™ªæ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥åŒè·¯å¾„ç¼–ç ç½‘ç»œï¼ŒåŒæ—¶å¤„ç†å±€éƒ¨ç©ºé—´ç¼–ç å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>è®¾è®¡äº†æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¨¡å—ï¼Œç”¨äºé¢„æµ‹æµçº¿ä¼ æ’­æ–¹å‘ï¼Œå®ç°ç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚</li>
<li>åœ¨å¤šä¸ªdMRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬åˆæˆå’Œä¸´åºŠæ•°æ®ã€‚</li>
<li>åœ¨ä¸¤ä¸ªæ‹¥æœ‰åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ˆISMRM Challengeå’ŒTractoInfernoï¼‰çš„è¯„ä¼°ä¸­ï¼ŒDDTrackingæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>DDTrackingå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„å¥åº·çŠ¶æ€ã€å¹´é¾„ç»„ã€æˆåƒåè®®å’Œæ‰«æä»ªç±»å‹çš„æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bad5ca3ffb26bcf49ee7307b77308bb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1067a71654cf0c57db98a75f145334c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22436357bcbe02497999258277c39f17.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3bc53771ebff9e5dc00cfc658bd894a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c827ae95e58298e3a678f5bdb295de8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DocVCE-Diffusion-based-Visual-Counterfactual-Explanations-for-Document-Image-Classification"><a href="#DocVCE-Diffusion-based-Visual-Counterfactual-Explanations-for-Document-Image-Classification" class="headerlink" title="DocVCE: Diffusion-based Visual Counterfactual Explanations for Document   Image Classification"></a>DocVCE: Diffusion-based Visual Counterfactual Explanations for Document   Image Classification</h2><p><strong>Authors:Saifullah Saifullah, Stefan Agne, Andreas Dengel, Sheraz Ahmed</strong></p>
<p>As black-box AI-driven decision-making systems become increasingly widespread in modern document processing workflows, improving their transparency and reliability has become critical, especially in high-stakes applications where biases or spurious correlations in decision-making could lead to serious consequences. One vital component often found in such document processing workflows is document image classification, which, despite its widespread use, remains difficult to explain. While some recent works have attempted to explain the decisions of document image classification models through feature-importance maps, these maps are often difficult to interpret and fail to provide insights into the global features learned by the model. In this paper, we aim to bridge this research gap by introducing generative document counterfactuals that provide meaningful insights into the modelâ€™s decision-making through actionable explanations. In particular, we propose DocVCE, a novel approach that leverages latent diffusion models in combination with classifier guidance to first generate plausible in-distribution visual counterfactual explanations, and then performs hierarchical patch-wise refinement to search for a refined counterfactual that is closest to the target factual image. We demonstrate the effectiveness of our approach through a rigorous qualitative and quantitative assessment on 3 different document classification datasets â€“ RVL-CDIP, Tobacco3482, and DocLayNet â€“ and 3 different models â€“ ResNet, ConvNeXt, and DiT â€“ using well-established evaluation criteria such as validity, closeness, and realism. To the best of the authorsâ€™ knowledge, this is the first work to explore generative counterfactual explanations in document image analysis. </p>
<blockquote>
<p>éšç€ä»¥äººå·¥æ™ºèƒ½é©±åŠ¨çš„å†³ç­–ç³»ç»Ÿåœ¨ç°ä»£æ–‡æ¡£å¤„ç†æµç¨‹ä¸­çš„æ—¥ç›Šæ™®åŠï¼Œæé«˜å…¶é€æ˜åº¦å’Œå¯é æ€§å˜å¾—è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©åº”ç”¨ä¸­ï¼Œå†³ç­–ä¸­çš„åè§æˆ–è™šå‡å…³è”å¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡åæœã€‚åœ¨æ–‡æ¡£å¤„ç†æµç¨‹ä¸­ç»å¸¸å¯ä»¥å‘ç°ä¸€ä¸ªé‡è¦çš„ç»„ä»¶æ˜¯æ–‡æ¡£å›¾åƒåˆ†ç±»ï¼Œå°½ç®¡å…¶åº”ç”¨å¹¿æ³›ï¼Œä½†å…¶è§£é‡Šæ€§ä»ç„¶å¾ˆéš¾ã€‚è™½ç„¶æœ€è¿‘çš„ä¸€äº›ç ”ç©¶å°è¯•é€šè¿‡ç‰¹å¾é‡è¦æ€§å›¾æ¥è§£é‡Šæ–‡æ¡£å›¾åƒåˆ†ç±»æ¨¡å‹çš„å†³ç­–ï¼Œä½†è¿™äº›å›¾é€šå¸¸å¾ˆéš¾è§£é‡Šï¼Œå¹¶ä¸”æ— æ³•æä¾›å¯¹æ¨¡å‹æ‰€å­¦ä¹ å…¨å±€ç‰¹å¾çš„è§è§£ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ç”Ÿæˆæ–‡æ¡£åäº‹å®æ¥å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œè¿™äº›åäº‹å®æä¾›äº†æœ‰å…³æ¨¡å‹å†³ç­–åˆ¶å®šçš„æœ‰æ„ä¹‰è§è§£ï¼Œå¹¶é€šè¿‡å¯æ“ä½œçš„è§£é‡Šæ¥æä¾›æœ‰æ„ä¹‰çš„è§è§£ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºDocVCEçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å¹¶ç»“åˆåˆ†ç±»å™¨æŒ‡å¯¼ï¼Œé¦–å…ˆç”Ÿæˆåˆç†çš„åˆ†å¸ƒå†…è§†è§‰åäº‹å®è§£é‡Šï¼Œç„¶åè¿›è¡Œåˆ†å±‚è¡¥ä¸å¼ç»†åŒ–ï¼Œä»¥å¯»æ‰¾æœ€æ¥è¿‘ç›®æ ‡å®é™…å›¾åƒçš„ç²¾ç»†åäº‹å®ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒçš„æ–‡æ¡£åˆ†ç±»æ•°æ®é›†ï¼ˆRVL-CDIPã€Tobacco3482å’ŒDocLayNetï¼‰å’Œä¸‰ä¸ªä¸åŒçš„æ¨¡å‹ï¼ˆResNetã€ConvNeXtå’ŒDiTï¼‰ä¸Šé€šè¿‡ä¸¥æ ¼çš„å®šæ€§å’Œå®šé‡è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯„ä¼°æ ‡å‡†åŒ…æ‹¬æœ‰æ•ˆæ€§ã€æ¥è¿‘æ€§å’Œç°å®æ€§ã€‚æ®ä½œè€…æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨æ–‡æ¡£å›¾åƒåˆ†æä¸­æ¢ç´¢ç”Ÿæˆåäº‹å®è§£é‡Šçš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04233v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€AIé©±åŠ¨çš„å†³ç­–ç³»ç»Ÿåœ¨ç°ä»£æ–‡æ¡£å¤„ç†æµç¨‹ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæé«˜é€æ˜åº¦å’Œå¯é æ€§å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•DocVCEï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç»“åˆåˆ†ç±»å™¨æŒ‡å¯¼ï¼Œç”Ÿæˆåˆç†çš„è§†è§‰åäº‹å®è§£é‡Šï¼Œä¸ºæ–‡æ¡£å›¾åƒåˆ†ç±»æ¨¡å‹çš„å†³ç­–æä¾›æ›´å¯æ“ä½œçš„è§£é‡Šã€‚è¯¥æ–¹æ³•é€šè¿‡å±‚æ¬¡åŒ–çš„è¡¥ä¸çº§ä¼˜åŒ–ï¼Œå¯»æ‰¾æœ€æ¥è¿‘ç›®æ ‡å®é™…å›¾åƒçš„åäº‹å®è§£é‡Šã€‚é€šè¿‡åœ¨ä¸åŒæ•°æ®é›†å’Œæ¨¡å‹ä¸Šçš„ä¸¥æ ¼å®šæ€§å’Œå®šé‡åˆ†æï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™æ˜¯é¦–æ¬¡åœ¨æ–‡æ¡£å›¾åƒåˆ†æä¸­æ¢ç´¢ç”Ÿæˆåäº‹å®è§£é‡Šçš„å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIé©±åŠ¨çš„å†³ç­–ç³»ç»Ÿåœ¨ç°ä»£æ–‡æ¡£å¤„ç†æµç¨‹ä¸­çš„é‡è¦æ€§åŠå…¶é€æ˜åº¦å’Œå¯é æ€§çš„éœ€æ±‚ã€‚</li>
<li>æ–‡æ¡£å›¾åƒåˆ†ç±»åœ¨AIå†³ç­–ä¸­çš„å…³é”®ä½œç”¨åŠå…¶è§£é‡Šéš¾åº¦ã€‚</li>
<li>ç°æœ‰ç‰¹å¾é‡è¦æ€§å›¾è§£é‡Šçš„ä¸è¶³ï¼Œéš¾ä»¥æä¾›æ¨¡å‹å…¨å±€ç‰¹å¾çš„æ´å¯Ÿã€‚</li>
<li>å¼•å…¥ç”Ÿæˆæ–‡æ¡£åäº‹å®è§£é‡Šçš„æ„ä¹‰ï¼Œä¸ºæ¨¡å‹å†³ç­–æä¾›æ›´å¯æ“ä½œçš„è§£é‡Šã€‚</li>
<li>DocVCEæ–¹æ³•åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œåˆ†ç±»å™¨æŒ‡å¯¼ç”Ÿæˆè§†è§‰åäº‹å®è§£é‡Šã€‚</li>
<li>DocVCEæ–¹æ³•é€šè¿‡å±‚æ¬¡åŒ–çš„è¡¥ä¸çº§ä¼˜åŒ–å¯»æ‰¾æœ€æ¥è¿‘ç›®æ ‡å®é™…å›¾åƒçš„åäº‹å®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f3fc3f6d3331f1277dcdb26820dde3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6a151ecfc4e30b6110519902e708ee9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="IDCNet-Guided-Video-Diffusion-for-Metric-Consistent-RGBD-Scene-Generation-with-Precise-Camera-Control"><a href="#IDCNet-Guided-Video-Diffusion-for-Metric-Consistent-RGBD-Scene-Generation-with-Precise-Camera-Control" class="headerlink" title="IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene   Generation with Precise Camera Control"></a>IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene   Generation with Precise Camera Control</h2><p><strong>Authors:Lijuan Liu, Wenfa Li, Dongbo Zhang, Shuo Wang, Shaohui Jiao</strong></p>
<p>We present IDC-Net (Image-Depth Consistency Network), a novel framework designed to generate RGB-D video sequences under explicit camera trajectory control. Unlike approaches that treat RGB and depth generation separately, IDC-Net jointly synthesizes both RGB images and corresponding depth maps within a unified geometry-aware diffusion model. The joint learning framework strengthens spatial and geometric alignment across frames, enabling more precise camera control in the generated sequences. To support the training of this camera-conditioned model and ensure high geometric fidelity, we construct a camera-image-depth consistent dataset with metric-aligned RGB videos, depth maps, and accurate camera poses, which provides precise geometric supervision with notably improved inter-frame geometric consistency. Moreover, we introduce a geometry-aware transformer block that enables fine-grained camera control, enhancing control over the generated sequences. Extensive experiments show that IDC-Net achieves improvements over state-of-the-art approaches in both visual quality and geometric consistency of generated scene sequences. Notably, the generated RGB-D sequences can be directly feed for downstream 3D Scene reconstruction tasks without extra post-processing steps, showcasing the practical benefits of our joint learning framework. See more at <a target="_blank" rel="noopener" href="https://idcnet-scene.github.io/">https://idcnet-scene.github.io</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†IDC-Netï¼ˆå›¾åƒæ·±åº¦ä¸€è‡´æ€§ç½‘ç»œï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨æ˜ç¡®çš„ç›¸æœºè½¨è¿¹æ§åˆ¶ä¸‹ç”ŸæˆRGB-Dè§†é¢‘åºåˆ—ã€‚ä¸åŒäºåˆ†åˆ«å¤„ç†RGBå’Œæ·±åº¦ç”Ÿæˆçš„æ–¹æ³•ï¼ŒIDC-Netåœ¨ä¸€ä¸ªç»Ÿä¸€çš„å‡ ä½•æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ä¸­è”åˆåˆæˆRGBå›¾åƒå’Œå¯¹åº”çš„æ·±åº¦å›¾ã€‚è”åˆå­¦ä¹ æ¡†æ¶åŠ å¼ºäº†å¸§ä¹‹é—´çš„ç©ºé—´å’Œå‡ ä½•å¯¹é½ï¼Œä»è€Œåœ¨ç”Ÿæˆçš„åºåˆ—ä¸­å®ç°äº†æ›´ç²¾ç¡®çš„ç›¸æœºæ§åˆ¶ã€‚ä¸ºäº†æ”¯æŒè¿™ç§ç›¸æœºæ¡ä»¶æ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶ç¡®ä¿é«˜å‡ ä½•ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç›¸æœºå›¾åƒæ·±åº¦ä¸€è‡´çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åº¦é‡å¯¹é½çš„RGBè§†é¢‘ã€æ·±åº¦å›¾å’Œç²¾ç¡®çš„ç›¸æœºå§¿æ€ï¼Œè¿™æä¾›äº†ç²¾ç¡®çš„å‡ ä½•ç›‘ç£ï¼Œæ˜¾è‘—æé«˜äº†å¸§é—´å‡ ä½•ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå‡ ä½•æ„ŸçŸ¥çš„å˜å‹å™¨å—ï¼Œå®ç°äº†ç²¾ç»†çš„ç›¸æœºæ§åˆ¶ï¼Œå¢å¼ºäº†ç”Ÿæˆåºåˆ—çš„æ§åˆ¶èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIDC-Netåœ¨è§†è§‰è´¨é‡å’Œç”Ÿæˆåœºæ™¯åºåˆ—çš„å‡ ä½•ä¸€è‡´æ€§æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”Ÿæˆçš„RGB-Dåºåˆ—å¯ä»¥ç›´æ¥ç”¨äºä¸‹æ¸¸çš„3Dåœºæ™¯é‡å»ºä»»åŠ¡ï¼Œè€Œæ— éœ€é¢å¤–çš„åå¤„ç†æ­¥éª¤ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„è”åˆå­¦ä¹ æ¡†æ¶çš„å®é™…æ•ˆç›Šã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§<a target="_blank" rel="noopener" href="https://idcnet-scene.github.io./">https://idcnet-scene.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04147v1">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>IDC-Netæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºåœ¨æ˜ç¡®çš„ç›¸æœºè½¨è¿¹æ§åˆ¶ä¸‹ç”ŸæˆRGB-Dè§†é¢‘åºåˆ—ã€‚è¯¥æ¡†æ¶åœ¨ç»Ÿä¸€çš„ç©ºé—´å‡ ä½•æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ä¸­è”åˆåˆæˆRGBå›¾åƒå’Œå¯¹åº”çš„æ·±åº¦å›¾ï¼ŒåŠ å¼ºäº†å¸§é—´çš„ç©ºé—´å‡ ä½•å¯¹é½ï¼Œä½¿ç”Ÿæˆçš„åºåˆ—å…·æœ‰æ›´ç²¾ç¡®çš„ç›¸æœºæ§åˆ¶ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†ä¸€ä¸ªæ”¯æŒè¯¥ç›¸æœºæ¡ä»¶æ¨¡å‹çš„è®­ç»ƒé›†ï¼Œé€šè¿‡å¼•å…¥å‡ ä½•æ„ŸçŸ¥è½¬æ¢å™¨å—å®ç°ç²¾ç»†çš„ç›¸æœºæ§åˆ¶ã€‚IDC-Netæé«˜äº†è§†è§‰è´¨é‡å’Œåœºæ™¯åºåˆ—çš„å‡ ä½•ä¸€è‡´æ€§ï¼Œç”Ÿæˆçš„RGB-Dåºåˆ—å¯ç›´æ¥ç”¨äºä¸‹æ¸¸çš„3Dåœºæ™¯é‡å»ºä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>IDC-Netæ˜¯ä¸€ä¸ªç”¨äºç”ŸæˆRGB-Dè§†é¢‘åºåˆ—çš„ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è”åˆå­¦ä¹ å¼ºåŒ–ç©ºé—´å‡ ä½•å¯¹é½ï¼Œæé«˜å¸§é—´ä¸€è‡´æ€§ã€‚</li>
<li>æ„å»ºäº†æ”¯æŒç›¸æœºæ¡ä»¶æ¨¡å‹çš„è®­ç»ƒé›†ï¼Œæä¾›ç²¾ç¡®çš„å‡ ä½•ç›‘ç£ã€‚</li>
<li>å¼•å…¥å‡ ä½•æ„ŸçŸ¥è½¬æ¢å™¨å—å®ç°ç²¾ç»†çš„ç›¸æœºæ§åˆ¶ã€‚</li>
<li>IDC-Netåœ¨è§†è§‰è´¨é‡å’Œå‡ ä½•ä¸€è‡´æ€§ä¸Šå®ç°äº†å¯¹å…ˆè¿›æ–¹æ³•çš„æ”¹è¿›ã€‚</li>
<li>ç”Ÿæˆçš„RGB-Dåºåˆ—å¯ç›´æ¥ç”¨äºä¸‹æ¸¸çš„3Dåœºæ™¯é‡å»ºä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c6a770d3de3360f03298620631f603ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45f3eabf2fbbdee1a34a6f0ae300c063.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78ef95a9420fa2dbd5d0b3de0fd1c5d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50d61ea6cc5ed793ec64e003f8b01cfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d6bff1d2beda6091c08ffe84585fb81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dbc22b1cb15532bfeba7eba58c67ec1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Conditional-Latent-Diffusion-Models-for-Zero-Shot-Instance-Segmentation"><a href="#Conditional-Latent-Diffusion-Models-for-Zero-Shot-Instance-Segmentation" class="headerlink" title="Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation"></a>Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation</h2><p><strong>Authors:Maximilian Ulmer, Wout Boerdijk, Rudolph Triebel, Maximilian Durner</strong></p>
<p>This paper presents OC-DiT, a novel class of diffusion models designed for object-centric prediction, and applies it to zero-shot instance segmentation. We propose a conditional latent diffusion framework that generates instance masks by conditioning the generative process on object templates and image features within the diffusion modelâ€™s latent space. This allows our model to effectively disentangle object instances through the diffusion process, which is guided by visual object descriptors and localized image cues. Specifically, we introduce two model variants: a coarse model for generating initial object instance proposals, and a refinement model that refines all proposals in parallel. We train these models on a newly created, large-scale synthetic dataset comprising thousands of high-quality object meshes. Remarkably, our model achieves state-of-the-art performance on multiple challenging real-world benchmarks, without requiring any retraining on target data. Through comprehensive ablation studies, we demonstrate the potential of diffusion models for instance segmentation tasks. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†OC-DiTï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¯¹è±¡ä¸­å¿ƒé¢„æµ‹è®¾è®¡çš„æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºé›¶æ ·æœ¬å®ä¾‹åˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­ä»¥å¯¹è±¡æ¨¡æ¿å’Œå›¾åƒç‰¹å¾ä¸ºæ¡ä»¶ç”Ÿæˆå®ä¾‹æ©æ¨¡ã€‚è¿™ä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ç”±è§†è§‰å¯¹è±¡æè¿°ç¬¦å’Œå±€éƒ¨å›¾åƒçº¿ç´¢å¼•å¯¼çš„æ‰©æ•£è¿‡ç¨‹æœ‰æ•ˆåœ°åˆ†ç¦»å¯¹è±¡å®ä¾‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§æ¨¡å‹å˜ä½“ï¼šä¸€ç§ç”¨äºç”Ÿæˆåˆå§‹å¯¹è±¡å®ä¾‹æè®®çš„ç²—ç•¥æ¨¡å‹ï¼Œä»¥åŠä¸€ç§ç”¨äºå¹¶è¡Œç»†åŒ–æ‰€æœ‰æè®®çš„ç²¾ç»†æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨æ–°åˆ›å»ºçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ•°åƒä¸ªé«˜è´¨é‡å¯¹è±¡ç½‘æ ¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œæ— éœ€å¯¹ç›®æ ‡æ•°æ®è¿›è¡Œä»»ä½•é‡æ–°è®­ç»ƒã€‚é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04122v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†OC-DiTï¼Œä¸€ç§ç”¨äºå¯¹è±¡ä¸­å¿ƒé¢„æµ‹çš„æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åº”ç”¨äºé›¶æ ·æœ¬å®ä¾‹åˆ†å‰²ã€‚é€šè¿‡æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œç”Ÿæˆå¯¹è±¡æ©è†œï¼Œè¯¥æ¡†æ¶åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­ä»¥å¯¹è±¡æ¨¡æ¿å’Œå›¾åƒç‰¹å¾ä¸ºæ¡ä»¶ç”Ÿæˆå¯¹è±¡æ©è†œã€‚æ­¤æ¨¡å‹èƒ½æœ‰æ•ˆé€šè¿‡æ‰©æ•£è¿‡ç¨‹åˆ†ç¦»å¯¹è±¡å®ä¾‹ï¼Œç”±è§†è§‰å¯¹è±¡æè¿°å™¨å’Œå±€éƒ¨å›¾åƒçº¿ç´¢å¼•å¯¼ã€‚åŒ…æ‹¬ç²—æ¨¡å‹ç”¨äºç”Ÿæˆåˆå§‹å¯¹è±¡å®ä¾‹ææ¡ˆå’Œç»†åŒ–æ¨¡å‹ï¼Œåè€…å¹¶è¡Œç»†åŒ–æ‰€æœ‰ææ¡ˆã€‚æ¨¡å‹åœ¨æ–°åˆ›çš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«æ•°åƒä¸ªé«˜è´¨é‡å¯¹è±¡ç½‘æ ¼ã€‚åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨¡å‹å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ— éœ€å¯¹ç›®æ ‡æ•°æ®è¿›è¡Œä»»ä½•é‡æ–°è®­ç»ƒã€‚é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OC-DiTæ˜¯ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå¯¹è±¡ä¸­å¿ƒé¢„æµ‹ï¼Œç‰¹åˆ«æ˜¯é›¶æ ·æœ¬å®ä¾‹åˆ†å‰²ã€‚</li>
<li>æå‡ºäº†æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­ä»¥å¯¹è±¡æ¨¡æ¿å’Œå›¾åƒç‰¹å¾ä¸ºæ¡ä»¶ç”Ÿæˆå®ä¾‹æ©è†œã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ‰©æ•£è¿‡ç¨‹æœ‰æ•ˆåœ°åˆ†ç¦»å¯¹è±¡å®ä¾‹ï¼Œè¿™ä¸€è¿‡ç¨‹ç”±è§†è§‰å¯¹è±¡æè¿°å™¨å’Œå±€éƒ¨å›¾åƒçº¿ç´¢å¼•å¯¼ã€‚</li>
<li>æ¨¡å‹åŒ…æ‹¬ç²—æ¨¡å‹å’Œç»†åŒ–æ¨¡å‹ä¸¤ä¸ªç‰ˆæœ¬ï¼Œåˆ†åˆ«ç”¨äºç”Ÿæˆåˆå§‹å¯¹è±¡å®ä¾‹ææ¡ˆå’Œç»†åŒ–æ‰€æœ‰ææ¡ˆã€‚</li>
<li>æ¨¡å‹åœ¨æ–°åˆ›å»ºçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«æ•°åƒä¸ªé«˜è´¨é‡å¯¹è±¡ç½‘æ ¼ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”æ— éœ€åœ¨ç›®æ ‡æ•°æ®ä¸Šè¿›è¡Œä»»ä½•é‡æ–°è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c462ffca81b2cb20970a7710bac35572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-085b0f4e751a53eb8c84d1b212c699ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e00b0fa043894966a47869a4473db1e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fc62820befebe46042d3de9c5d3466d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b4607949a551949a5999930d987846e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CADD-Context-aware-disease-deviations-via-restoration-of-brain-images-using-normative-conditional-diffusion-models"><a href="#CADD-Context-aware-disease-deviations-via-restoration-of-brain-images-using-normative-conditional-diffusion-models" class="headerlink" title="CADD: Context aware disease deviations via restoration of brain images   using normative conditional diffusion models"></a>CADD: Context aware disease deviations via restoration of brain images   using normative conditional diffusion models</h2><p><strong>Authors:Ana Lawry Aguila, Ayodeji Ijishakin, Juan Eugenio Iglesias, Tomomi Takenaga, Yukihiro Nomura, Takeharu Yoshikawa, Osamu Abe, Shouhei Hanaoka</strong></p>
<p>Applying machine learning to real-world medical data, e.g. from hospital archives, has the potential to revolutionize disease detection in brain images. However, detecting pathology in such heterogeneous cohorts is a difficult challenge. Normative modeling, a form of unsupervised anomaly detection, offers a promising approach to studying such cohorts where the &#96;&#96;normalâ€™â€™ behavior is modeled and can be used at subject level to detect deviations relating to disease pathology. Diffusion models have emerged as powerful tools for anomaly detection due to their ability to capture complex data distributions and generate high-quality images. Their performance relies on image restoration; differences between the original and restored images highlight potential abnormalities. However, unlike normative models, these diffusion model approaches do not incorporate clinical information which provides important context to guide the disease detection process. Furthermore, standard approaches often poorly restore healthy regions, resulting in poor reconstructions and suboptimal detection performance. We present CADD, the first conditional diffusion model for normative modeling in 3D images. To guide the healthy restoration process, we propose a novel inference inpainting strategy which balances anomaly removal with retention of subject-specific features. Evaluated on three challenging datasets, including clinical scans, which may have lower contrast, thicker slices, and motion artifacts, CADD achieves state-of-the-art performance in detecting neurological abnormalities in heterogeneous cohorts. </p>
<blockquote>
<p>å°†æœºå™¨å­¦ä¹ åº”ç”¨äºåŒ»é™¢æ¡£æ¡ˆç­‰çœŸå®ä¸–ç•ŒåŒ»ç–—æ•°æ®ï¼Œæœ‰å¯èƒ½é©æ–°è„‘å›¾åƒçš„ç–¾ç—…æ£€æµ‹ã€‚ç„¶è€Œï¼Œåœ¨å¦‚æ­¤å¤šæ ·åŒ–çš„ç¾¤ä½“ä¸­æ£€æµ‹ç—…ç†å­¦æ˜¯ä¸€é¡¹è‰°å·¨çš„æŒ‘æˆ˜ã€‚è§„èŒƒå»ºæ¨¡æ˜¯ä¸€ç§æ— ç›‘ç£çš„å¼‚å¸¸æ£€æµ‹æ³•ï¼Œåœ¨ç ”ç©¶æ­¤ç±»ç¾¤ä½“æ—¶å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ï¼Œå¯å¯¹â€œæ­£å¸¸â€è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä»¥å—è¯•è€…æ°´å¹³æ£€æµ‹ä¸ç–¾ç—…ç—…ç†å­¦ç›¸å…³çš„åå·®ã€‚æ‰©æ•£æ¨¡å‹å› èƒ½å¤Ÿæ•æ‰å¤æ‚çš„æ•°æ®åˆ†å¸ƒå¹¶ç”Ÿæˆé«˜è´¨é‡å›¾åƒè€Œæˆä¸ºå¼‚å¸¸æ£€æµ‹çš„æœ‰åŠ›å·¥å…·ã€‚å…¶æ€§èƒ½ä¾èµ–äºå›¾åƒä¿®å¤ï¼›åŸå§‹å›¾åƒå’Œä¿®å¤å›¾åƒä¹‹é—´çš„å·®å¼‚çªå‡ºäº†æ½œåœ¨çš„å¼‚å¸¸ã€‚ç„¶è€Œï¼Œä¸è§„èŒƒæ¨¡å‹ä¸åŒï¼Œè¿™äº›æ‰©æ•£æ¨¡å‹æ–¹æ³•å¹¶æœªèå…¥ä¸´åºŠä¿¡æ¯ï¼Œè€Œåè€…ä¸ºå¼•å¯¼ç–¾ç—…æ£€æµ‹è¿‡ç¨‹æä¾›äº†é‡è¦çš„èƒŒæ™¯ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæ ‡å‡†æ–¹æ³•å¾€å¾€ä¸èƒ½å¾ˆå¥½åœ°ä¿®å¤å¥åº·åŒºåŸŸï¼Œå¯¼è‡´é‡å»ºè´¨é‡å·®å’Œæ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚æˆ‘ä»¬é¦–æ¬¡æ¨å‡ºç”¨äºä¸‰ç»´å›¾åƒè§„èŒƒå»ºæ¨¡çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹CADDã€‚ä¸ºäº†å¼•å¯¼å¥åº·çš„ä¿®å¤è¿‡ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¨ç†è¡¥å…¨ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨æ¶ˆé™¤å¼‚å¸¸ä¸ä¿ç•™å—è¯•è€…ç‰¹å®šç‰¹å¾ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚åœ¨åŒ…æ‹¬å¯èƒ½å­˜åœ¨ä½å¯¹æ¯”åº¦ã€åšåˆ‡ç‰‡å’Œè¿åŠ¨ä¼ªå½±çš„ä¸´åºŠæ‰«æåœ¨å†…çš„ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒCADDåœ¨æ£€æµ‹å¼‚è´¨äººç¾¤ä¸­ç¥ç»å¼‚å¸¸æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03594v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åº”ç”¨æœºå™¨å­¦ä¹ äºçœŸå®ä¸–ç•ŒåŒ»ç–—æ•°æ®ï¼Œå¦‚åŒ»é™¢æ¡£æ¡ˆï¼Œæœ‰æ½œåŠ›é©æ–°è„‘å›¾åƒç–¾ç—…æ£€æµ‹ã€‚ç„¶è€Œï¼Œåœ¨å¦‚æ­¤å¤šæ ·åŒ–çš„ç¾¤ä½“ä¸­æ£€æµ‹ç—…ç†æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è§„èŒƒå»ºæ¨¡æä¾›äº†ä¸€ç§ç ”ç©¶æ­¤ç±»ç¾¤ä½“çš„æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥æ¨¡æ‹Ÿâ€œæ­£å¸¸â€è¡Œä¸ºå¹¶åœ¨ä¸»ä½“å±‚é¢æ£€æµ‹ä¸ç–¾ç—…ç—…ç†ç›¸å…³çš„åå·®ã€‚æ‰©æ•£æ¨¡å‹å› èƒ½å¤Ÿæ•æ‰å¤æ‚æ•°æ®åˆ†å¸ƒå¹¶ç”Ÿæˆé«˜è´¨é‡å›¾åƒè€Œæˆä¸ºä¸€ç§å¼ºå¤§çš„å¼‚å¸¸æ£€æµ‹å·¥å…·ã€‚å…¶æ€§èƒ½ä¾èµ–äºå›¾åƒä¿®å¤ï¼›åŸå§‹å’Œä¿®å¤å›¾åƒä¹‹é—´çš„å·®å¼‚çªå‡ºäº†æ½œåœ¨çš„å¼‚å¸¸ã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹æ–¹æ³•å¹¶æœªèå…¥ä¸´åºŠä¿¡æ¯ï¼Œè¿™å¯¹å¼•å¯¼ç–¾ç—…æ£€æµ‹è¿‡ç¨‹è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæ ‡å‡†æ–¹æ³•å¸¸å¸¸ä¸èƒ½å¾ˆå¥½åœ°ä¿®å¤å¥åº·åŒºåŸŸï¼Œå¯¼è‡´é‡å»ºè´¨é‡å·®å’Œæ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªç”¨äº3Då›¾åƒè§„èŒƒå»ºæ¨¡çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹CADDã€‚ä¸ºäº†å¼•å¯¼å¥åº·çš„ä¿®å¤è¿‡ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨ç†è¡¥å…¨ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨æ¶ˆé™¤å¼‚å¸¸ä¸ä¿ç•™ä¸»ä½“ç‰¹å®šç‰¹å¾ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚åœ¨åŒ…æ‹¬å¯èƒ½å­˜åœ¨ä½å¯¹æ¯”åº¦ã€åšåˆ‡ç‰‡å’Œè¿åŠ¨ä¼ªå½±çš„ä¸´åºŠæ‰«æåœ¨å†…çš„ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCADDåœ¨æ£€æµ‹å¼‚è´¨äººç¾¤ä¸­çš„ç¥ç»å¼‚å¸¸æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ æ–¹æ³•åº”ç”¨äºåŒ»ç–—æ•°æ®æœ‰æ½œåŠ›é©æ–°ç–¾ç—…æ£€æµ‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¯æ•æ‰å¤æ‚æ•°æ®åˆ†å¸ƒå¹¶ç”¨äºå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨è„‘å›¾åƒç–¾ç—…æ£€æµ‹ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®å¼‚è´¨æ€§ã€å›¾åƒä¿®å¤è´¨é‡ç­‰ã€‚</li>
<li>CADDæ˜¯é¦–ä¸ªç”¨äº3Då›¾åƒè§„èŒƒå»ºæ¨¡çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>CADDç»“åˆäº†ä¸€ç§æ–°çš„æ¨ç†è¡¥å…¨ç­–ç•¥ï¼Œä»¥å¹³è¡¡å¼‚å¸¸æ¶ˆé™¤å’Œä¸»ä½“ç‰¹å¾ä¿ç•™ã€‚</li>
<li>CADDåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç¥ç»å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d4f588dccd6c4bf9339115ad7c75050d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0c5a4a2928bb81fb89cf2319de769d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-835b02c41b52f9986f9ee14d60d1e592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47d68ba4cfe959b891ea1fafa7482490.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CoEmoGen-Towards-Semantically-Coherent-and-Scalable-Emotional-Image-Content-Generation"><a href="#CoEmoGen-Towards-Semantically-Coherent-and-Scalable-Emotional-Image-Content-Generation" class="headerlink" title="CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image   Content Generation"></a>CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image   Content Generation</h2><p><strong>Authors:Kaishen Yuan, Yuting Zhang, Shang Gao, Yijie Zhu, Wenshuo Chen, Yutao Yue</strong></p>
<p>Emotional Image Content Generation (EICG) aims to generate semantically clear and emotionally faithful images based on given emotion categories, with broad application prospects. While recent text-to-image diffusion models excel at generating concrete concepts, they struggle with the complexity of abstract emotions. There have also emerged methods specifically designed for EICG, but they excessively rely on word-level attribute labels for guidance, which suffer from semantic incoherence, ambiguity, and limited scalability. To address these challenges, we propose CoEmoGen, a novel pipeline notable for its semantic coherence and high scalability. Specifically, leveraging multimodal large language models (MLLMs), we construct high-quality captions focused on emotion-triggering content for context-rich semantic guidance. Furthermore, inspired by psychological insights, we design a Hierarchical Low-Rank Adaptation (HiLoRA) module to cohesively model both polarity-shared low-level features and emotion-specific high-level semantics. Extensive experiments demonstrate CoEmoGenâ€™s superiority in emotional faithfulness and semantic coherence from quantitative, qualitative, and user study perspectives. To intuitively showcase scalability, we curate EmoArt, a large-scale dataset of emotionally evocative artistic images, providing endless inspiration for emotion-driven artistic creation. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/yuankaishen2001/CoEmoGen">https://github.com/yuankaishen2001/CoEmoGen</a>. </p>
<blockquote>
<p>æƒ…æ„Ÿå›¾åƒå†…å®¹ç”Ÿæˆï¼ˆEICGï¼‰æ—¨åœ¨æ ¹æ®ç»™å®šçš„æƒ…æ„Ÿç±»åˆ«ç”Ÿæˆè¯­ä¹‰æ¸…æ™°ã€æƒ…æ„ŸçœŸå®çš„å›¾åƒï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚è™½ç„¶æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå…·ä½“æ¦‚å¿µæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬éš¾ä»¥å¤„ç†æŠ½è±¡æƒ…æ„Ÿçš„å¤æ‚æ€§ã€‚ä¹Ÿæœ‰ä¸€äº›ä¸“é—¨ä¸ºEICGè®¾è®¡çš„æ–¹æ³•å‡ºç°ï¼Œä½†å®ƒä»¬è¿‡äºä¾èµ–è¯çº§å±æ€§æ ‡ç­¾ä½œä¸ºæŒ‡å¯¼ï¼Œè¿™ä¼šå¯¼è‡´è¯­ä¹‰ä¸ä¸€è‡´ã€æ¨¡ç³Šå’Œæœ‰é™çš„æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoEmoGenï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç®¡é“ï¼Œä»¥å…¶è¯­ä¹‰è¿è´¯æ€§å’Œé«˜å¯æ‰©å±•æ€§è€Œè‘—ç§°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„å»ºé«˜è´¨é‡çš„ä¸“æ³¨äºæƒ…æ„Ÿè§¦å‘å†…å®¹çš„æ ‡é¢˜ï¼Œä»¥æä¾›ä¸°å¯Œçš„è¯­ä¹‰æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å—åˆ°å¿ƒç†æ´å¯Ÿçš„å¯å‘ï¼Œè®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚ä½ç§©é€‚åº”ï¼ˆHiLoRAï¼‰æ¨¡å—ï¼Œä»¥åè°ƒåœ°å»ºæ¨¡å…±äº«ææ€§çš„ä½çº§åˆ«ç‰¹å¾å’Œæƒ…æ„Ÿç‰¹å®šçš„é«˜çº§è¯­ä¹‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä»å®šé‡ã€å®šæ€§å’Œç”¨æˆ·ç ”ç©¶çš„è§’åº¦ï¼ŒCoEmoGenåœ¨æƒ…æ„ŸçœŸå®æ€§å’Œè¯­ä¹‰è¿è´¯æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¸ºäº†ç›´è§‚åœ°å±•ç¤ºå¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬ç­–åˆ’äº†EmoArtï¼Œè¿™æ˜¯ä¸€ä¸ªæƒ…æ„Ÿæ¿€å‘è‰ºæœ¯å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¸ºæƒ…æ„Ÿé©±åŠ¨çš„è‰ºæœ¯åˆ›ä½œæä¾›äº†æ— å°½çš„çµæ„Ÿã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yuankaishen200">https://github.com/yuankaishen200</a> ç»“å°¾å¤„æ‰¾åˆ°ã€‚è¿™é‡Œéœ€è¦æ‚¨è‡ªå·±æä¾›ç½‘ç«™åœ°å€é“¾æ¥æˆ–è€…ç½‘ç«™å…¨ç§°ç­‰ä¿¡æ¯è¿›è¡Œå®Œæ•´è¡¨è¿°ã€‚è¿™ä¸ªé¡¹ç›®çš„ä»‹ç»åŠä»£ç ç­‰ç›¸å…³èµ„æºå¯ä¾›æŸ¥è¯¢å’Œä½¿ç”¨ã€‚åœ¨è¿™é‡Œå¼ºçƒˆæ¨èçš„è‹±æ–‡ç½‘å€ä¸å®é™…åº”ç”¨ä¸­çš„æœç´¢ç»“æœä»…ä¾›å‚è€ƒä½œç”¨å“¦ï¼Œè¿˜éœ€è¦æ‚¨å¯¹æœ€ç»ˆç½‘ç«™èµ„æºçš„åº”ç”¨é£é™©åšæå‰çš„é¢„æµ‹ä¸é¢„é˜²å‡†å¤‡ã€‚æˆ‘ä»¬åœ¨é“¾æ¥ä¸­åŒ…å«æœ‰å…³æ¨¡å‹çš„è®­ç»ƒå’Œå®ç°çš„è¯¦ç»†æŒ‡å¯¼æ–‡æ¡£ã€å¤§å‹æƒ…æ„Ÿå›¾åƒæ•°æ®é›†ä»¥åŠç›¸å…³å·¥å…·åŒ…çš„ä¸‹è½½æ–¹å¼ç­‰ä¿¡æ¯ï¼Œä¸ºé‚£äº›å¸Œæœ›åœ¨å›¾åƒå†…å®¹ç”Ÿæˆæ–¹é¢å±•å¼€ç ”ç©¶çš„äººå£«æä¾›äº†å·¨å¤§çš„å¸®åŠ©å’Œæ”¯æŒã€‚å½“ç„¶æˆ‘ä»¬ä¼šéµå¾ªç‰¹å®šçš„ç¤¾ä¼šæœŸæœ›å’Œè¡Œä¸šè§„åˆ™æ¥åšåç»­çš„æ¨è¿›å“¦ï¼å¦‚æœæœ‰å…³äºæ•°æ®çš„æ ‡æ³¨åŠç†è§£çš„éœ€æ±‚ä¹Ÿå¯å‘æˆ‘åé¦ˆçš„å“ˆã€‚æ­¤å¤–åœ¨ä½¿ç”¨ä»£ç çš„æ—¶å€™å¯èƒ½ä¼šé‡åˆ°ä¸€äº›å°é—®é¢˜ç­‰éš¾ä»¥é¢„æ–™çš„æ„å¤–çŠ¶å†µã€‚ä¸ºäº†æ›´å¥½åœ°ä¼˜åŒ–æµç¨‹å»ºè®®æ‚¨åœ¨æ‰§è¡Œå¤æ‚æ“ä½œæ—¶å®‰æ’ä¸€åç›¸å…³é¢†åŸŸçš„ä¸“ä¸šäººå‘˜äºˆä»¥é™ªåŒåä½œæŒ‡å¯¼ï¼Œä¾¿äºåšå‡ºå¿…è¦å“åº”ç­‰å¹¶åŠæ—¶ä¸ºæ‚¨ç­”ç–‘è§£æƒ‘å’Œå¦¥å–„åº”å¯¹éš¾é¢˜ä»¥ç¡®ä¿é¡¹ç›®å®æ–½è¿‡ç¨‹ä¸­çš„é¡ºåˆ©æ¨è¿›å“¦ï¼å¸Œæœ›ä»¥ä¸Šä¿¡æ¯èƒ½å¤Ÿä¸ºæ‚¨æä¾›æœ‰ä»·å€¼çš„å‚è€ƒå’Œå¸®åŠ©ï¼æœŸå¾…æ‚¨çš„è¿›ä¸€æ­¥åˆä½œå’Œäº¤æµï¼</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03535v1">PDF</a> 10 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCoEmoGençš„æ–°å‹æƒ…æ„Ÿå›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆè¯­ä¹‰æ¸…æ™°ã€æƒ…æ„ŸçœŸå®çš„å›¾åƒã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºé«˜è´¨é‡çš„æƒ…æ„Ÿè§¦å‘å†…å®¹æè¿°ï¼Œå®ç°ä¸°å¯Œçš„è¯­ä¹‰æŒ‡å¯¼ã€‚åŒæ—¶ï¼Œç»“åˆå¿ƒç†å­¦åŸç†è®¾è®¡çš„åˆ†å±‚ä½ç§©é€‚åº”æ¨¡å—ï¼Œèƒ½åŒæ—¶å»ºæ¨¡å…±äº«ææ€§ä½å±‚æ¬¡ç‰¹å¾å’Œæƒ…æ„Ÿç‰¹å®šé«˜å±‚æ¬¡è¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼ŒCoEmoGenåœ¨æƒ…æ„ŸçœŸå®æ€§å’Œè¯­ä¹‰è¿è´¯æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†ä¸€ä¸ªå¤§å‹æƒ…æ„Ÿè‰ºæœ¯å›¾åƒæ•°æ®é›†EmoArtï¼Œä¸ºæƒ…æ„Ÿé©±åŠ¨çš„è‰ºæœ¯åˆ›ä½œæä¾›æ— å°½çµæ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Emotional Image Content Generation (EICG)çš„ç›®æ ‡æ˜¯ç”ŸæˆåŸºäºç»™å®šæƒ…æ„Ÿç±»åˆ«çš„è¯­ä¹‰æ¸…æ™°ã€æƒ…æ„ŸçœŸå®çš„å›¾åƒã€‚</li>
<li>ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨æŠ½è±¡æƒ…æ„Ÿå¤æ‚æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>CoEmoGenæ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºé«˜è´¨é‡çš„æƒ…æ„Ÿè§¦å‘å†…å®¹æè¿°ï¼Œå®ç°ä¸°å¯Œçš„è¯­ä¹‰æŒ‡å¯¼ã€‚</li>
<li>CoEmoGenè®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚ä½ç§©é€‚åº”æ¨¡å—ï¼Œä»¥å»ºæ¨¡å…±äº«ææ€§ä½å±‚æ¬¡ç‰¹å¾å’Œæƒ…æ„Ÿç‰¹å®šé«˜å±‚æ¬¡è¯­ä¹‰ã€‚</li>
<li>å®éªŒè¯æ˜CoEmoGenåœ¨æƒ…æ„ŸçœŸå®æ€§å’Œè¯­ä¹‰è¿è´¯æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>CoEmoGenå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¹¶æ¨å‡ºäº†ä¸€ä¸ªå¤§å‹æƒ…æ„Ÿè‰ºæœ¯å›¾åƒæ•°æ®é›†EmoArtã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b4ab93296ed708dee695fa4e7ece025.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-311a95fffd057342bc4db868fd3bb739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63a0f5ccf0f858ff8bc4cd1dfc3084ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a26f929f9320d10862b93c51446b2741.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40ba0a449ff11ced682bf529556f8a81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d47bd6c5c064c091451f592ba7e6b245.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diffusion-Once-and-Done-Degradation-Aware-LoRA-for-Efficient-All-in-One-Image-Restoration"><a href="#Diffusion-Once-and-Done-Degradation-Aware-LoRA-for-Efficient-All-in-One-Image-Restoration" class="headerlink" title="Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One   Image Restoration"></a>Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One   Image Restoration</h2><p><strong>Authors:Ni Tang, Xiaotong Luo, Zihan Cheng, Liangtai Zhou, Dongxiao Zhang, Yanyun Qu</strong></p>
<p>Diffusion models have revealed powerful potential in all-in-one image restoration (AiOIR), which is talented in generating abundant texture details. The existing AiOIR methods either retrain a diffusion model or fine-tune the pretrained diffusion model with extra conditional guidance. However, they often suffer from high inference costs and limited adaptability to diverse degradation types. In this paper, we propose an efficient AiOIR method, Diffusion Once and Done (DOD), which aims to achieve superior restoration performance with only one-step sampling of Stable Diffusion (SD) models. Specifically, multi-degradation feature modulation is first introduced to capture different degradation prompts with a pretrained diffusion model. Then, parameter-efficient conditional low-rank adaptation integrates the prompts to enable the fine-tuning of the SD model for adapting to different degradation types. Besides, a high-fidelity detail enhancement module is integrated into the decoder of SD to improve structural and textural details. Experiments demonstrate that our method outperforms existing diffusion-based restoration approaches in both visual quality and inference efficiency. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å…¨èƒ½å›¾åƒä¿®å¤ï¼ˆAiOIRï¼‰ä¸­å±•ç°äº†å¼ºå¤§çš„æ½œåŠ›ï¼Œè¯¥æŠ€æœ¯åœ¨ç”Ÿæˆä¸°å¯Œçº¹ç†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç°æœ‰çš„AiOIRæ–¹æ³•è¦ä¹ˆé‡æ–°è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œè¦ä¹ˆä½¿ç”¨é¢å¤–çš„æ¡ä»¶æŒ‡å¯¼å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œå®ƒä»¬å¸¸å¸¸é¢ä¸´æ¨ç†æˆæœ¬é«˜å’Œå¯¹å„ç§é€€åŒ–ç±»å‹é€‚åº”æ€§æœ‰é™çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„AiOIRæ–¹æ³•ï¼Œå³â€œä¸€æ¬¡æ‰©æ•£å®Œæˆâ€ï¼ˆDODï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰æ¨¡å‹çš„ä¸€æ¬¡æ­¥é‡‡æ ·å®ç°å‡ºè‰²çš„ä¿®å¤æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆå¼•å…¥å¤šé€€åŒ–ç‰¹å¾è°ƒåˆ¶ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ•æ‰ä¸åŒçš„é€€åŒ–æç¤ºã€‚ç„¶åï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆçš„æ¡ä»¶ä½ç§©é€‚é…ï¼Œå°†æç¤ºé›†æˆåœ¨ä¸€èµ·ï¼Œä½¿SDæ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒçš„é€€åŒ–ç±»å‹ã€‚æ­¤å¤–ï¼Œè¿˜å°†é«˜ä¿çœŸç»†èŠ‚å¢å¼ºæ¨¡å—é›†æˆåˆ°SDçš„è§£ç å™¨ä¸­ï¼Œä»¥æé«˜ç»“æ„å’Œçº¹ç†ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ¨ç†æ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„åŸºäºæ‰©æ•£çš„ä¿®å¤æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03373v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å…¨èƒ½å›¾åƒæ¢å¤ï¼ˆAiOIRï¼‰ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸°å¯Œçš„çº¹ç†ç»†èŠ‚ã€‚ç°æœ‰AiOIRæ–¹æ³•è¦ä¹ˆé‡æ–°è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œè¦ä¹ˆä½¿ç”¨é¢å¤–çš„æ¡ä»¶æŒ‡å¯¼å¯¹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œå®ƒä»¬å¸¸å¸¸é¢ä¸´é«˜æ¨ç†æˆæœ¬å’Œå¯¹å„ç§é€€åŒ–ç±»å‹é€‚åº”æ€§æœ‰é™çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„AiOIRæ–¹æ³•â€”â€”ä¸€æ¬¡å®Œæˆæ‰©æ•£ï¼ˆDODï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰æ¨¡å‹çš„ä¸€æ¬¡é‡‡æ ·æ­¥éª¤å®ç°å‡ºè‰²çš„æ¢å¤æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆå¼•å…¥å¤šé€€åŒ–ç‰¹å¾è°ƒåˆ¶æ¥æ•æ‰ä¸åŒçš„é€€åŒ–æç¤ºå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚ç„¶åï¼Œé€šè¿‡å‚æ•°æœ‰æ•ˆçš„æ¡ä»¶ä½ç§©é€‚åº”æ¥æ•´åˆè¿™äº›æç¤ºï¼Œä½¿SDæ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸åŒçš„é€€åŒ–ç±»å‹ã€‚æ­¤å¤–ï¼Œå°†é«˜ä¿çœŸç»†èŠ‚å¢å¼ºæ¨¡å—é›†æˆåˆ°SDçš„è§£ç å™¨ä¸­ï¼Œä»¥æé«˜ç»“æ„å’Œçº¹ç†ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ¨ç†æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰çš„æ‰©æ•£æ¢å¤æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å…¨èƒ½å›¾åƒæ¢å¤ï¼ˆAiOIRï¼‰ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸°å¯Œçš„çº¹ç†ç»†èŠ‚ã€‚</li>
<li>ç°æœ‰AiOIRæ–¹æ³•é¢ä¸´é«˜æ¨ç†æˆæœ¬å’Œå¯¹å¤šç§é€€åŒ–ç±»å‹é€‚åº”æ€§æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„AiOIRæ–¹æ³•â€”â€”ä¸€æ¬¡å®Œæˆæ‰©æ•£ï¼ˆDODï¼‰ï¼Œé€šè¿‡ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰æ¨¡å‹çš„ä¸€æ¬¡é‡‡æ ·æ­¥éª¤å®ç°å‡ºè‰²çš„æ¢å¤æ€§èƒ½ã€‚</li>
<li>å¤šé€€åŒ–ç‰¹å¾è°ƒåˆ¶ç”¨äºæ•æ‰ä¸åŒçš„é€€åŒ–æç¤ºå¹¶ä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å‚æ•°æœ‰æ•ˆçš„æ¡ä»¶ä½ç§©é€‚åº”èƒ½å¤Ÿæ•´åˆæç¤ºå¹¶é€‚åº”ä¸åŒçš„é€€åŒ–ç±»å‹ã€‚</li>
<li>å°†é«˜ä¿çœŸç»†èŠ‚å¢å¼ºæ¨¡å—é›†æˆåˆ°SDè§£ç å™¨ä¸­ï¼Œæé«˜ç»“æ„å’Œçº¹ç†ç»†èŠ‚çš„æ¢å¤ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDODæ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ¨ç†æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ‰©æ•£æ¢å¤æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b911a887b1630bae4ef42b3788f6ba8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a5a60d8546c6f0c69d619c45aaf406b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2e582719e05a97e8fb71b6a7c46b350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-168e7bfa8ac3198556b4428838eeb289.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83db922fb41c53eefa73ebfaba5ec93d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8375ad107c0d98f2f7c690b55ee5c618.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GL-LCM-Global-Local-Latent-Consistency-Models-for-Fast-High-Resolution-Bone-Suppression-in-Chest-X-Ray-Images"><a href="#GL-LCM-Global-Local-Latent-Consistency-Models-for-Fast-High-Resolution-Bone-Suppression-in-Chest-X-Ray-Images" class="headerlink" title="GL-LCM: Global-Local Latent Consistency Models for Fast High-Resolution   Bone Suppression in Chest X-Ray Images"></a>GL-LCM: Global-Local Latent Consistency Models for Fast High-Resolution   Bone Suppression in Chest X-Ray Images</h2><p><strong>Authors:Yifei Sun, Zhanghao Chen, Hao Zheng, Yuqing Lu, Lixin Duan, Fenglei Fan, Ahmed Elazab, Xiang Wan, Changmiao Wang, Ruiquan Ge</strong></p>
<p>Chest X-Ray (CXR) imaging for pulmonary diagnosis raises significant challenges, primarily because bone structures can obscure critical details necessary for accurate diagnosis. Recent advances in deep learning, particularly with diffusion models, offer significant promise for effectively minimizing the visibility of bone structures in CXR images, thereby improving clarity and diagnostic accuracy. Nevertheless, existing diffusion-based methods for bone suppression in CXR imaging struggle to balance the complete suppression of bones with preserving local texture details. Additionally, their high computational demand and extended processing time hinder their practical use in clinical settings. To address these limitations, we introduce a Global-Local Latent Consistency Model (GL-LCM) architecture. This model combines lung segmentation, dual-path sampling, and global-local fusion, enabling fast high-resolution bone suppression in CXR images. To tackle potential boundary artifacts and detail blurring in local-path sampling, we further propose Local-Enhanced Guidance, which addresses these issues without additional training. Comprehensive experiments on a self-collected dataset SZCH-X-Rays, and the public dataset JSRT, reveal that our GL-LCM delivers superior bone suppression and remarkable computational efficiency, significantly outperforming several competitive methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/diaoquesang/GL-LCM">https://github.com/diaoquesang/GL-LCM</a>. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰å½±åƒåœ¨è‚ºéƒ¨è¯Šæ–­ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºéª¨éª¼ç»“æ„å¯èƒ½ä¼šæ©ç›–å‡†ç¡®è¯Šæ–­æ‰€éœ€çš„å…³é”®ç»†èŠ‚ã€‚æœ€è¿‘æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œä¸ºæœ‰æ•ˆå‡å°‘CXRå›¾åƒä¸­éª¨éª¼ç»“æ„çš„å¯è§æ€§æä¾›äº†å·¨å¤§æ½œåŠ›ï¼Œä»è€Œæé«˜äº†æ¸…æ™°åº¦å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ‰©æ•£çš„CXRæˆåƒä¸­éª¨éª¼æŠ‘åˆ¶æ–¹æ³•å¾ˆéš¾åœ¨å®Œå…¨æŠ‘åˆ¶éª¨éª¼ä¸ä¿ç•™å±€éƒ¨çº¹ç†ç»†èŠ‚ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„é«˜è®¡ç®—éœ€æ±‚å’Œè¾ƒé•¿çš„å¤„ç†æ—¶é—´é˜»ç¢äº†å…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨çƒå±€éƒ¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆGL-LCMï¼‰æ¶æ„ã€‚è¯¥æ¨¡å‹ç»“åˆäº†è‚ºéƒ¨åˆ†å‰²ã€åŒè·¯å¾„é‡‡æ ·å’Œå…¨å±€æœ¬åœ°èåˆï¼Œèƒ½å¤Ÿåœ¨CXRå›¾åƒä¸­å®ç°å¿«é€Ÿé«˜åˆ†è¾¨ç‡çš„éª¨éª¼æŠ‘åˆ¶ã€‚ä¸ºäº†è§£å†³å±€éƒ¨è·¯å¾„é‡‡æ ·ä¸­å¯èƒ½å‡ºç°çš„è¾¹ç•Œä¼ªå½±å’Œç»†èŠ‚æ¨¡ç³Šé—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å±€éƒ¨å¢å¼ºæŒ‡å¯¼ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯è§£å†³è¿™äº›é—®é¢˜ã€‚åœ¨è‡ªæ”¶é›†çš„SZCH-Xå°„çº¿æ•°æ®é›†å’Œå…¬å…±JSRTæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GL-LCMåœ¨éª¨éª¼æŠ‘åˆ¶æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶ä¸”è®¡ç®—æ•ˆç‡æé«˜ï¼Œæ˜¾è‘—ä¼˜äºå‡ ç§ç«äº‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/diaoquesang/GL-LCM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/diaoquesang/GL-LCMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03357v1">PDF</a> 11 pages, 3 figures, accepted by MICCAI 2025</p>
<p><strong>Summary</strong><br>     ä½¿ç”¨æ·±åº¦å­¦ä¹ æ‰©æ•£æ¨¡å‹è§£å†³èƒ¸éƒ¨Xå…‰å½±åƒè¯Šæ–­ä¸­çš„éª¨éª¼ç»“æ„é®æŒ¡é—®é¢˜ï¼Œæå‡ºä¸€ç§ç»“åˆè‚ºéƒ¨åˆ†å‰²ã€åŒè·¯å¾„é‡‡æ ·å’Œå…¨å±€-å±€éƒ¨èåˆçš„å…¨å±€å±€éƒ¨æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆGL-LCMï¼‰ã€‚è¯¥æ¨¡å‹å¯å®ç°å¿«é€Ÿé«˜åˆ†è¾¨ç‡çš„éª¨éª¼æŠ‘åˆ¶ï¼Œä¸”è®¡ç®—æ•ˆç‡é«˜ï¼Œåœ¨è‡ªæˆ‘æ”¶é›†çš„SZCH-X-Rayså’Œå…¬å…±JSRTæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒ¸éƒ¨Xå…‰å½±åƒè¯Šæ–­ä¸­ï¼Œéª¨éª¼ç»“æ„é®æŒ¡å…³é”®ç»†èŠ‚ï¼Œå½±å“è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ‰©æ•£æ¨¡å‹åœ¨å‡å°‘éª¨éª¼ç»“æ„å¯è§æ€§æ–¹é¢å±•ç°æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨éª¨éª¼æŠ‘åˆ¶å’Œä¿ç•™å±€éƒ¨çº¹ç†ç»†èŠ‚ä¹‹é—´éš¾ä»¥å¹³è¡¡ã€‚</li>
<li>æå‡ºçš„GL-LCMæ¨¡å‹ç»“åˆè‚ºéƒ¨åˆ†å‰²ã€åŒè·¯å¾„é‡‡æ ·å’Œå…¨å±€-å±€éƒ¨èåˆï¼Œå®ç°å¿«é€Ÿé«˜åˆ†è¾¨ç‡çš„éª¨éª¼æŠ‘åˆ¶ã€‚</li>
<li>GL-LCMæ¨¡å‹é€šè¿‡å±€éƒ¨å¢å¼ºå¼•å¯¼è§£å†³è¾¹ç•Œä¼ªå½±å’Œç»†èŠ‚æ¨¡ç³Šé—®é¢˜ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>åœ¨SZCH-X-Rayså’ŒJSRTæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºGL-LCMæ¨¡å‹åœ¨éª¨éª¼æŠ‘åˆ¶å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>GL-LCMæ¨¡å‹çš„ä»£ç å·²å…¬å¼€å¯è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6cc49446300db61e5998d776ae770c82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a5bde9e379c69637dc22f2bf807d34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a96dcdad929f404e90044df8dece21d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Domain-Adaptive-Semantic-Segmentation-by-Synthetic-Data-Generation-and-Progressive-Adaptation"><a href="#Zero-Shot-Domain-Adaptive-Semantic-Segmentation-by-Synthetic-Data-Generation-and-Progressive-Adaptation" class="headerlink" title="Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data   Generation and Progressive Adaptation"></a>Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data   Generation and Progressive Adaptation</h2><p><strong>Authors:Jun Luo, Zijing Zhao, Yang Liu</strong></p>
<p>Deep learning-based semantic segmentation models achieve impressive results yet remain limited in handling distribution shifts between training and test data. In this paper, we present SDGPA (Synthetic Data Generation and Progressive Adaptation), a novel method that tackles zero-shot domain adaptive semantic segmentation, in which no target images are available, but only a text description of the target domainâ€™s style is provided. To compensate for the lack of target domain training data, we utilize a pretrained off-the-shelf text-to-image diffusion model, which generates training images by transferring source domain images to target style. Directly editing source domain images introduces noise that harms segmentation because the layout of source images cannot be precisely maintained. To address inaccurate layouts in synthetic data, we propose a method that crops the source image, edits small patches individually, and then merges them back together, which helps improve spatial precision. Recognizing the large domain gap, SDGPA constructs an augmented intermediate domain, leveraging easier adaptation subtasks to enable more stable model adaptation to the target domain. Additionally, to mitigate the impact of noise in synthetic data, we design a progressive adaptation strategy, ensuring robust learning throughout the training process. Extensive experiments demonstrate that our method achieves state-of-the-art performance in zero-shot semantic segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ROUJINN/SDGPA">https://github.com/ROUJINN/SDGPA</a> </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹è™½ç„¶å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†åœ¨å¤„ç†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒè½¬ç§»æ—¶ä»å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SDGPAï¼ˆåˆæˆæ•°æ®ç”Ÿæˆå’Œé€æ­¥é€‚åº”ï¼ŒSynthetic Data Generation and Progressive Adaptationï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§£å†³é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²çš„æ–°æ–¹æ³•ï¼Œå…¶ä¸­ä¸æä¾›ç›®æ ‡å›¾åƒï¼Œä½†æä¾›äº†ç›®æ ‡åŸŸé£æ ¼çš„æ–‡æœ¬æè¿°ã€‚ä¸ºäº†å¼¥è¡¥ç›®æ ‡åŸŸè®­ç»ƒæ•°æ®çš„ç¼ºä¹ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„å³ç”¨å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è½¬ç§»æºåŸŸå›¾åƒåˆ°ç›®æ ‡é£æ ¼æ¥ç”Ÿæˆè®­ç»ƒå›¾åƒã€‚ç›´æ¥ç¼–è¾‘æºåŸŸå›¾åƒä¼šäº§ç”ŸæŸå®³åˆ†å‰²çš„å™ªå£°ï¼Œå› ä¸ºæºå›¾åƒçš„å¸ƒå±€æ— æ³•ç²¾ç¡®ä¿æŒã€‚ä¸ºäº†è§£å†³åˆæˆæ•°æ®ä¸­çš„å¸ƒå±€ä¸å‡†ç¡®é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¹æºå›¾åƒè¿›è¡Œè£å‰ªï¼Œåˆ†åˆ«ç¼–è¾‘å°å—åŒºåŸŸï¼Œç„¶åå°†å®ƒä»¬åˆå¹¶åœ¨ä¸€èµ·ï¼Œè¿™æœ‰åŠ©äºæé«˜ç©ºé—´ç²¾åº¦ã€‚è®¤è¯†åˆ°å·¨å¤§çš„é¢†åŸŸå·®è·åï¼ŒSDGPAæ„å»ºäº†ä¸€ä¸ªå¢å¼ºçš„ä¸­é—´é¢†åŸŸï¼Œåˆ©ç”¨æ›´å®¹æ˜“çš„é€‚åº”å­ä»»åŠ¡æ¥ä½¿æ¨¡å‹æ›´ç¨³å®šåœ°é€‚åº”ç›®æ ‡é¢†åŸŸã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»åˆæˆæ•°æ®ä¸­çš„å™ªå£°å½±å“ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é€æ­¥é€‚åº”ç­–ç•¥ï¼Œç¡®ä¿åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å¥å­¦ä¹ ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä¸­è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/ROUJINN/SDGPA">https://github.com/ROUJINN/SDGPA</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03300v1">PDF</a> Accepted to IROS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSDGPAçš„æ–°æ–¹æ³•ï¼Œç”¨äºå¤„ç†é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå°†æºåŸŸå›¾åƒè½¬æ¢ä¸ºç›®æ ‡é£æ ¼æ¥ç”Ÿæˆè®­ç»ƒå›¾åƒã€‚ä¸ºæé«˜ç©ºé—´ç²¾åº¦ï¼Œæå‡ºäº†åŸºäºå›¾åƒåˆ†å—ç¼–è¾‘å’Œåˆå¹¶çš„ç­–ç•¥ã€‚åŒæ—¶æ„å»ºä¸­é—´åŸŸå¹¶åˆ©ç”¨æ¸è¿›é€‚åº”ç­–ç•¥ï¼Œå®ç°æ¨¡å‹å¯¹ç›®æ ‡åŸŸçš„ç¨³å¥é€‚åº”ã€‚è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºSDGPAæ–¹æ³•ç”¨äºå¤„ç†é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å°†æºåŸŸå›¾åƒè½¬æ¢ä¸ºç›®æ ‡é£æ ¼æ¥ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚</li>
<li>åº”å¯¹ç”Ÿæˆæ•°æ®ä¸­çš„å¸ƒå±€ä¸å‡†ç¡®é—®é¢˜ï¼Œé‡‡ç”¨å›¾åƒåˆ†å—ç¼–è¾‘å’Œåˆå¹¶çš„ç­–ç•¥ã€‚</li>
<li>æ„å»ºä¸­é—´åŸŸä»¥ç¼©å°æ¨¡å‹ä¸ç›®æ ‡åŸŸä¹‹é—´çš„å·®è·ã€‚</li>
<li>åˆ©ç”¨æ¸è¿›é€‚åº”ç­–ç•¥æ¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨é›¶æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ef94222775646ce252dd5c461370433.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a19aa5c29314dc271d5f3b3cb2d6142e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5018f196db889b1a11d55b91ac58e43f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0665bc2b177017d95aa2701c2d6c943a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d07a02bfaaec053d04f02bdd83f99768.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32e09cadd01e0f08f554c7f643f67a82.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BadBlocks-Low-Cost-and-Stealthy-Backdoor-Attacks-Tailored-for-Text-to-Image-Diffusion-Models"><a href="#BadBlocks-Low-Cost-and-Stealthy-Backdoor-Attacks-Tailored-for-Text-to-Image-Diffusion-Models" class="headerlink" title="BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for   Text-to-Image Diffusion Models"></a>BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for   Text-to-Image Diffusion Models</h2><p><strong>Authors:Yu Pan, Jiahao Chen, Lin Wang, Bingrong Dai, Yi Du</strong></p>
<p>In recent years,Diffusion models have achieved remarkable progress in the field of image generation.However,recent studies have shown that diffusion models are susceptible to backdoor attacks,in which attackers can manipulate the output by injecting covert triggers such as specific visual patterns or textual phrases into the training dataset.Fortunately,with the continuous advancement of defense techniques,defenders have become increasingly capable of identifying and mitigating most backdoor attacks using visual inspection and neural network-based detection methods.However,in this paper,we identify a novel type of backdoor threat that is more lightweight and covert than existing approaches,which we name BadBlocks,requires only about 30% of the computational resources and 20% GPU time typically needed by previous backdoor attacks,yet it successfully injects backdoors and evades the most advanced defense frameworks.BadBlocks enables attackers to selectively contaminate specific blocks within the UNet architecture of diffusion models while maintaining normal functionality in the remaining components.Experimental results demonstrate that BadBlocks achieves a high attack success rate (ASR) and low perceptual quality loss (as measured by FID Score),even under extremely constrained computational resources and GPU time.Moreover,BadBlocks is able to bypass existing defense frameworks,especially the attention-based backdoor detection method, highlighting it as a novel and noteworthy threat.Ablation studies further demonstrate that effective backdoor injection does not require fine-tuning the entire network and highlight the pivotal role of certain neural network layers in backdoor mapping.Overall,BadBlocks significantly reduces the barrier to conducting backdoor attacks in all aspects.It enables attackers to inject backdoors into large-scale diffusion models even using consumer-grade GPUs. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡å‘è®­ç»ƒæ•°æ®é›†ä¸­æ³¨å…¥éšè”½çš„è§¦å‘å› ç´ ï¼ˆå¦‚ç‰¹å®šçš„è§†è§‰æ¨¡å¼æˆ–æ–‡æœ¬çŸ­è¯­ï¼‰æ¥æ“çºµè¾“å‡ºã€‚å¹¸è¿çš„æ˜¯ï¼Œéšç€é˜²å¾¡æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œé˜²å¾¡è€…è¶Šæ¥è¶Šèƒ½å¤Ÿä½¿ç”¨è§†è§‰æ£€æŸ¥å’ŒåŸºäºç¥ç»ç½‘ç»œçš„æ£€æµ‹æ–¹æ³•æ¥è¯†åˆ«å’Œç¼“è§£å¤§å¤šæ•°åé—¨æ”»å‡»ã€‚ç„¶è€Œï¼Œæœ¬æ–‡ä¸­å‘ç°äº†ä¸€ç§æ–°å‹åé—¨å¨èƒâ€”â€”BadBlocksã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒBadBlocksæ›´ä¸ºè½»å·§å’Œéšè”½ï¼Œä»…éœ€å¤§çº¦30%çš„è®¡ç®—èµ„æºå’Œ20%çš„GPUæ—¶é—´ï¼Œå³å¯æˆåŠŸæ³¨å…¥åé—¨å¹¶ç»•è¿‡æœ€å…ˆè¿›çš„é˜²å¾¡æ¡†æ¶ã€‚BadBlocksèƒ½å¤Ÿä½¿æ”»å‡»è€…é€‰æ‹©æ€§æ±¡æŸ“æ‰©æ•£æ¨¡å‹çš„UNetæ¶æ„ä¸­çš„ç‰¹å®šå—ï¼ŒåŒæ—¶ä¿æŒå…¶ä½™ç»„ä»¶çš„æ­£å¸¸åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨è®¡ç®—èµ„æºå’ŒGPUæ—¶é—´æåº¦å—é™çš„æƒ…å†µä¸‹ï¼ŒBadBlocksä»èƒ½å®ç°è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰å’Œè¾ƒä½çš„å¯æ„ŸçŸ¥è´¨é‡æŸå¤±ï¼ˆä»¥FIDåˆ†æ•°è¡¡é‡ï¼‰ã€‚æ­¤å¤–ï¼ŒBadBlocksèƒ½å¤Ÿç»•è¿‡ç°æœ‰çš„é˜²å¾¡æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ³¨æ„åŠ›çš„åé—¨æ£€æµ‹æ–¹æ³•ï¼Œå‡¸æ˜¾å‡ºå®ƒæ˜¯ä¸€ç§æ–°å‹ä¸”å€¼å¾—å…³æ³¨çš„å¨èƒã€‚è¿›ä¸€æ­¥çš„ç ”ç©¶è¡¨æ˜ï¼Œæœ‰æ•ˆçš„åé—¨æ³¨å…¥ä¸éœ€è¦å¯¹æ•´ä¸ªç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œå¹¶å¼ºè°ƒäº†æŸäº›ç¥ç»ç½‘ç»œå±‚åœ¨åé—¨æ˜ å°„ä¸­çš„å…³é”®ä½œç”¨ã€‚æ€»ä½“è€Œè¨€ï¼ŒBadBlocksåœ¨å„ä¸ªæ–¹é¢å¤§å¤§é™ä½äº†è¿›è¡Œåé—¨æ”»å‡»çš„éš¾åº¦ï¼Œç”šè‡³ä½¿ç”¨æ¶ˆè´¹çº§GPUä¹Ÿèƒ½å¤Ÿå®ç°å¯¹å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„åé—¨æ³¨å…¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜æ‰©æ•£æ¨¡å‹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œæ”»å‡»è€…é€šè¿‡åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ³¨å…¥éšè”½è§¦å‘å™¨ï¼ˆå¦‚ç‰¹å®šè§†è§‰æ¨¡å¼æˆ–æ–‡æœ¬çŸ­è¯­ï¼‰æ¥æ“çºµè¾“å‡ºã€‚å¹¸è¿çš„æ˜¯ï¼Œéšç€é˜²å¾¡æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œå¤§å¤šæ•°åé—¨æ”»å‡»å¯ä»¥é€šè¿‡è§†è§‰æ£€æŸ¥å’ŒåŸºäºç¥ç»ç½‘ç»œçš„æ£€æµ‹æ–¹æ³•è¿›è¡Œè¯†åˆ«å’Œç¼“è§£ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æ­ç¤ºäº†ä¸€ç§æ–°å‹åé—¨å¨èƒâ€”â€”BadBlocksã€‚BadBlocksç›¸è¾ƒäºç°æœ‰æ–¹æ³•æ›´ä¸ºè½»é‡çº§å’Œéšè”½ï¼Œä»…éœ€çº¦30%çš„è®¡ç®—èµ„æºå’Œ20%çš„GPUæ—¶é—´å³å¯æˆåŠŸæ³¨å…¥åé—¨å¹¶ç»•è¿‡æœ€å…ˆè¿›çš„é˜²å¾¡æ¡†æ¶ã€‚BadBlocksé€šè¿‡é€‰æ‹©æ€§æ±¡æŸ“æ‰©æ•£æ¨¡å‹çš„UNetæ¶æ„ä¸­çš„ç‰¹å®šå—ï¼ŒåŒæ—¶ä¿æŒå…¶ä½™ç»„ä»¶çš„æ­£å¸¸åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBadBlockså…·æœ‰é«˜æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰å’Œä½æ„ŸçŸ¥è´¨é‡æŸå¤±ï¼ˆä»¥FIDåˆ†æ•°è¡¡é‡ï¼‰ï¼Œå³ä½¿åœ¨æå…¶æœ‰é™çš„è®¡ç®—èµ„æºå’ŒGPUæ—¶é—´ä¸‹ä¹Ÿèƒ½å®ç°æœ‰æ•ˆçš„æ”»å‡»ã€‚æ­¤å¤–ï¼ŒBadBlocksèƒ½å¤Ÿç»•è¿‡ç°æœ‰çš„é˜²å¾¡æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ³¨æ„åŠ›çš„åé—¨æ£€æµ‹æ–¹æ³•ï¼Œå±•ç°ä¸ºä¸€ç§æ–°å‹ä¸”å€¼å¾—å…³æ³¨çš„å¨èƒã€‚æ€»ä½“è€Œè¨€ï¼ŒBadBlockså¤§å¤§é™ä½äº†è¿›è¡Œåé—¨æ”»å‡»çš„éš¾åº¦ï¼Œå³ä½¿åœ¨æ¶ˆè´¹çº§GPUä¸Šä¹Ÿèƒ½å°†åé—¨æ³¨å…¥å¤§å‹æ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ã€‚</li>
<li>æ–°å‹åé—¨å¨èƒBadBlocksæ›´ä¸ºè½»é‡çº§å’Œéšè”½ï¼Œèƒ½æˆåŠŸæ³¨å…¥åé—¨å¹¶ç»•è¿‡æœ€å…ˆè¿›çš„é˜²å¾¡æ¡†æ¶ã€‚</li>
<li>BadBlocksä»…éœ€çº¦30%çš„è®¡ç®—èµ„æºå’Œ20%çš„GPUæ—¶é—´ï¼Œå®ç°é«˜æ”»å‡»æˆåŠŸç‡å’Œä½æ„ŸçŸ¥è´¨é‡æŸå¤±ã€‚</li>
<li>BadBlocksé€šè¿‡é€‰æ‹©æ€§æ±¡æŸ“æ‰©æ•£æ¨¡å‹çš„ç‰¹å®šå—è¿›è¡Œæ”»å‡»ï¼Œç»´æŒæ¨¡å‹å…¶ä½™éƒ¨åˆ†æ­£å¸¸åŠŸèƒ½ã€‚</li>
<li>BadBlocksèƒ½å¤Ÿç»•è¿‡ç°æœ‰çš„é˜²å¾¡æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ³¨æ„åŠ›çš„åé—¨æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜BadBlocksåœ¨å—é™è®¡ç®—èµ„æºå’ŒGPUæ—¶é—´ä¸‹ä¾ç„¶èƒ½æœ‰æ•ˆå®æ–½æ”»å‡»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f6f1c64236044b2d92f524c767861c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9f685408054dc342919d6a74e05e4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75091792281a2f4eaa2ab61b326e672f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a1a57d1e9d54d805dc013b84b38cb5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-954264da8793b3152041455699afc95a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Zero-shot-Segmentation-of-Skin-Conditions-Erythema-with-Edit-Friendly-Inversion"><a href="#Zero-shot-Segmentation-of-Skin-Conditions-Erythema-with-Edit-Friendly-Inversion" class="headerlink" title="Zero-shot Segmentation of Skin Conditions: Erythema with Edit-Friendly   Inversion"></a>Zero-shot Segmentation of Skin Conditions: Erythema with Edit-Friendly   Inversion</h2><p><strong>Authors:Konstantinos Moutselos, Ilias Maglogiannis</strong></p>
<p>This study proposes a zero-shot image segmentation framework for detecting erythema (redness of the skin) using edit-friendly inversion in diffusion models. The method synthesizes reference images of the same patient that are free from erythema via generative editing and then accurately aligns these references with the original images. Color-space analysis is performed with minimal user intervention to identify erythematous regions. This approach significantly reduces the reliance on labeled dermatological datasets while providing a scalable and flexible diagnostic support tool by avoiding the need for any annotated training masks. In our initial qualitative experiments, the pipeline successfully isolated facial erythema in diverse cases, demonstrating performance improvements over baseline threshold-based techniques. These results highlight the potential of combining generative diffusion models and statistical color segmentation for computer-aided dermatology, enabling efficient erythema detection without prior training data. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹çº¢æ–‘ï¼ˆçš®è‚¤å‘çº¢ï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆç¼–è¾‘åœ¨æ‰©æ•£æ¨¡å‹ä¸­åˆæˆåŒä¸€æ‚£è€…çš„æ— çº¢æ–‘å‚è€ƒå›¾åƒï¼Œç„¶åå‡†ç¡®åœ°å°†è¿™äº›å‚è€ƒå›¾åƒä¸åŸå§‹å›¾åƒå¯¹é½ã€‚è¿›è¡Œé¢œè‰²ç©ºé—´åˆ†æï¼Œå°½é‡å‡å°‘ç”¨æˆ·å¹²é¢„ï¼Œä»¥è¯†åˆ«çº¢æ–‘åŒºåŸŸã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å¯¹é¢éƒ¨çš®è‚¤ç—…æ•°æ®é›†æ ‡æ³¨çš„ä¾èµ–ï¼ŒåŒæ—¶æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”çµæ´»çš„è¯Šæ–­æ”¯æŒå·¥å…·ï¼Œé¿å…äº†ä»»ä½•æ ‡æ³¨è®­ç»ƒæ©è†œçš„éœ€æ±‚ã€‚åœ¨æˆ‘ä»¬çš„åˆæ­¥å®šæ€§å®éªŒä¸­ï¼Œè¯¥æµç¨‹æˆåŠŸåœ°åœ¨å¤šç§æƒ…å†µä¸‹å®ç°äº†é¢éƒ¨çº¢æ–‘çš„éš”ç¦»ï¼Œæ˜¾ç¤ºå‡ºä¸åŸºäºé˜ˆå€¼çš„åŸºçº¿æŠ€æœ¯ç›¸æ¯”çš„æ€§èƒ½æ”¹è¿›ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ç»“åˆç”Ÿæˆæ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡é¢œè‰²åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©çš®è‚¤ç§‘ä¸­çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿå®ç°æ— éœ€é¢„å…ˆè®­ç»ƒæ•°æ®çš„æœ‰æ•ˆçº¢æ–‘æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01334v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹çš®è‚¤çº¢æ–‘ï¼ˆerythemaï¼‰ã€‚è¯¥ç ”ç©¶é€šè¿‡ç”Ÿæˆç¼–è¾‘åˆæˆæ— çº¢æ–‘çš„å‚è€ƒå›¾åƒï¼Œç„¶åå°†å…¶ä¸åŸå§‹å›¾åƒå‡†ç¡®å¯¹é½ã€‚é€šè¿‡è‰²å½©ç©ºé—´åˆ†æï¼Œæœ€å°é™åº¦åœ°å‡å°‘ç”¨æˆ·å¹²é¢„ï¼Œè¯†åˆ«çº¢æ–‘åŒºåŸŸã€‚æ­¤æ–¹æ³•æ˜¾è‘—å‡å°‘å¯¹æ ‡æ³¨çš®è‚¤ç—…æ•°æ®é›†çš„ä¾èµ–ï¼Œå¹¶æä¾›å¯ä¼¸ç¼©ã€çµæ´»çš„è¯Šæ–­æ”¯æŒå·¥å…·ï¼Œæ— éœ€ä»»ä½•æ³¨é‡Šè®­ç»ƒæ©è†œã€‚åˆæ­¥å®šæ€§å®éªŒæˆåŠŸåœ°åœ¨å¤šç§æƒ…å†µä¸‹åˆ†ç¦»é¢éƒ¨çº¢æ–‘ï¼Œæ˜¾ç¤ºå‡ºæ¯”åŸºäºé˜ˆå€¼çš„æŠ€æœ¯æ›´å¥½çš„æ€§èƒ½æ”¹è¿›ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ç»“åˆç”Ÿæˆæ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡è‰²å½©åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©çš®è‚¤ç—…å­¦ä¸­çš„æ½œåŠ›ï¼Œå¯å®ç°æœ‰æ•ˆçš„çº¢æ–‘æ£€æµ‹è€Œæ— éœ€äº‹å…ˆçš„è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬å›¾åƒåˆ†å‰²æ¡†æ¶ç”¨äºçš®è‚¤çº¢æ–‘æ£€æµ‹ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆç¼–è¾‘åˆæˆæ— çº¢æ–‘çš„å‚è€ƒå›¾åƒï¼Œç„¶åå°†å…¶ä¸åŸå§‹å›¾åƒå¯¹é½ã€‚</li>
<li>è‰²å½©ç©ºé—´åˆ†æç”¨äºè¯†åˆ«çº¢æ–‘åŒºåŸŸï¼Œå‡å°‘ç”¨æˆ·å¹²é¢„ã€‚</li>
<li>è¯¥æ–¹æ³•å‡å°‘å¯¹æ ‡æ³¨çš®è‚¤ç—…æ•°æ®é›†çš„ä¾èµ–ï¼Œä¸ºè¯Šæ–­æä¾›å¯ä¼¸ç¼©ã€çµæ´»çš„å·¥å…·ã€‚</li>
<li>åˆæ­¥å®éªŒæˆåŠŸåˆ†ç¦»é¢éƒ¨çº¢æ–‘ï¼Œæ€§èƒ½ä¼˜äºåŸºäºé˜ˆå€¼çš„æŠ€æœ¯ã€‚</li>
<li>ç»“åˆç”Ÿæˆæ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡è‰²å½©åˆ†å‰²å¯æœ‰æ•ˆè¿›è¡Œçº¢æ–‘æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d6c2dc90597c98c4d873179390849f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df4801a1914556e8c8a06e5fd98b4b3a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LeakyCLIP-Extracting-Training-Data-from-CLIP"><a href="#LeakyCLIP-Extracting-Training-Data-from-CLIP" class="headerlink" title="LeakyCLIP: Extracting Training Data from CLIP"></a>LeakyCLIP: Extracting Training Data from CLIP</h2><p><strong>Authors:Yunhao Chen, Shujie Wang, Xin Wang, Xingjun Ma</strong></p>
<p>Understanding the memorization and privacy leakage risks in Contrastive Languageâ€“Image Pretraining (CLIP) is critical for ensuring the security of multimodal models. Recent studies have demonstrated the feasibility of extracting sensitive training examples from diffusion models, with conditional diffusion models exhibiting a stronger tendency to memorize and leak information. In this work, we investigate data memorization and extraction risks in CLIP through the lens of CLIP inversion, a process that aims to reconstruct training images from text prompts. To this end, we introduce \textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality, semantically accurate image reconstruction from CLIP embeddings. We identify three key challenges in CLIP inversion: 1) non-robust features, 2) limited visual semantics in text embeddings, and 3) low reconstruction fidelity. To address these challenges, LeakyCLIP employs 1) adversarial fine-tuning to enhance optimization smoothness, 2) linear transformation-based embedding alignment, and 3) Stable Diffusion-based refinement to improve fidelity. Empirical results demonstrate the superiority of LeakyCLIP, achieving over 358% improvement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared to baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive leakage risk, showing that training data membership can even be successfully inferred from the metrics of low-fidelity reconstructions. Our work introduces a practical method for CLIP inversion while offering novel insights into the nature and scope of privacy risks in multimodal models. </p>
<blockquote>
<p>ç†è§£å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸­çš„è®°å¿†å’Œéšç§æ³„éœ²é£é™©å¯¹äºç¡®ä¿å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»è¯æ˜äº†ä»æ‰©æ•£æ¨¡å‹ä¸­æå–æ•æ„Ÿè®­ç»ƒæ ·æœ¬çš„å¯è¡Œæ€§ï¼Œæ¡ä»¶æ‰©æ•£æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„è®°å¿†å’Œæ³„éœ²ä¿¡æ¯çš„è¶‹åŠ¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡CLIPåè½¬çš„è§†è§’æ¥ç ”ç©¶CLIPä¸­çš„æ•°æ®è®°å¿†å’Œæå–é£é™©ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»æ–‡æœ¬æç¤ºé‡å»ºè®­ç»ƒå›¾åƒçš„è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textbf{LeakyCLIP}ï¼Œä¸€ç§æ–°å‹æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä»CLIPåµŒå…¥ä¸­è¿›è¡Œé«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒé‡å»ºã€‚æˆ‘ä»¬å‘ç°CLIPåè½¬é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ç‰¹å¾ä¸ç¨³å¥ï¼Œ2ï¼‰æ–‡æœ¬åµŒå…¥ä¸­çš„è§†è§‰è¯­ä¹‰æœ‰é™ï¼Œä»¥åŠ3ï¼‰é‡å»ºä¿çœŸåº¦ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒLeakyCLIPé‡‡ç”¨1ï¼‰å¯¹æŠ—æ€§å¾®è°ƒä»¥å¢å¼ºä¼˜åŒ–å¹³æ»‘åº¦ï¼Œ2ï¼‰åŸºäºçº¿æ€§å˜æ¢çš„åµŒå…¥å¯¹é½ï¼Œä»¥åŠ3ï¼‰åŸºäºç¨³å®šæ‰©æ•£çš„ç»†åŒ–ä»¥æé«˜ä¿çœŸåº¦ã€‚å®è¯ç»“æœè¡¨æ˜LeakyCLIPçš„ä¼˜åŠ¿ï¼Œåœ¨LAION-2Bå­é›†ä¸Šä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒViT-B-16çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰æé«˜äº†358%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæ™®éå­˜åœ¨çš„æ³„éœ²é£é™©ï¼Œå³å³ä½¿ä»ä½ä¿çœŸé‡å»ºçš„æŒ‡æ ‡ä¸­ä¹Ÿå¯ä»¥æˆåŠŸæ¨æ–­å‡ºè®­ç»ƒæ•°æ®æˆå‘˜ã€‚æˆ‘ä»¬çš„å·¥ä½œä»‹ç»äº†ä¸€ç§å®ç”¨çš„CLIPåè½¬æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†å…³äºå¤šæ¨¡æ€æ¨¡å‹ä¸­éšç§é£é™©æ€§è´¨å’ŒèŒƒå›´çš„æ–°è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00756v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆCLIPï¼‰ä¸­çš„è®°å¿†å’Œéšç§æ³„éœ²é£é™©é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡CLIPåæ¼”è§’åº¦å±•å¼€ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºLeakyCLIPçš„æ–°å‹æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨ä»CLIPåµŒå…¥ä¸­å®ç°é«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒé‡å»ºã€‚ç ”ç©¶é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜å¹¶ç›¸åº”æå‡ºäº†è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¢å¼ºä¼˜åŒ–å¹³æ»‘åº¦ã€åŸºäºçº¿æ€§å˜æ¢çš„åµŒå…¥å¯¹é½ä»¥åŠæé«˜ä¿çœŸåº¦çš„Stable DiffusionåŸºç¡€ä¼˜åŒ–ã€‚è¯¥ç ”ç©¶ä¸ä»…åœ¨å›¾åƒé‡å»ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¿˜æ­ç¤ºäº†æ™®éå­˜åœ¨çš„æ³„éœ²é£é™©ï¼Œç”šè‡³å¯ä»¥ä»ä½ä¿çœŸé‡å»ºçš„åº¦é‡æŒ‡æ ‡ä¸­æˆåŠŸæ¨æ–­å‡ºè®­ç»ƒæ•°æ®æˆå‘˜èº«ä»½ã€‚è¿™ä¸ºCLIPåæ¼”æä¾›äº†å®ç”¨æ–¹æ³•ï¼Œå¹¶å¯¹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„éšç§é£é™©æœ¬è´¨å’ŒèŒƒå›´æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆCLIPï¼‰å­˜åœ¨è®°å¿†å’Œéšç§æ³„éœ²é£é™©ï¼Œç‰¹åˆ«æ˜¯æ¡ä»¶æ‰©æ•£æ¨¡å‹æ›´å®¹æ˜“å‡ºç°è¿™äº›é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºLeakyCLIPçš„æ–°å‹æ”»å‡»æ¡†æ¶ï¼Œç”¨äºä»CLIPåµŒå…¥ä¸­é‡å»ºé«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒã€‚</li>
<li>LeakyCLIPè§£å†³äº†CLIPåæ¼”ä¸­çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šéç¨³å¥ç‰¹å¾ã€æ–‡æœ¬åµŒå…¥ä¸­æœ‰é™çš„è§†è§‰è¯­ä¹‰å’Œä½é‡å»ºä¿çœŸåº¦ã€‚</li>
<li>LeakyCLIPé€šè¿‡å¢å¼ºä¼˜åŒ–å¹³æ»‘åº¦ã€åµŒå…¥å¯¹é½å’Œæé«˜ä¿çœŸåº¦çš„æŠ€æœ¯ï¼Œå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œå³ä½¿å¯¹äºä½è´¨é‡çš„å›¾åƒé‡å»ºï¼Œä¹Ÿèƒ½æˆåŠŸæ¨æ–­è®­ç»ƒæ•°æ®æˆå‘˜èº«ä»½ï¼Œè¿™è¡¨æ˜å­˜åœ¨æ™®éçš„æ³„éœ²é£é™©ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºCLIPåæ¼”æä¾›äº†å®ç”¨æ–¹æ³•ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„éšç§é£é™©æä¾›äº†æ–°çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-873c3e286480775b2043c532ce4b4380.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64a0eefcf7f259bd66e4c30339e3e1ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbb890dd60799f5e4d90b79ecab16db4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a56def90e3a3794c4e19cc07cad4730.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Near-Boundary-OOD-Samples-for-Out-of-Distribution-Detection"><a href="#Synthesizing-Near-Boundary-OOD-Samples-for-Out-of-Distribution-Detection" class="headerlink" title="Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection"></a>Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection</h2><p><strong>Authors:Jinglun Li, Kaixun Jiang, Zhaoyu Chen, Bo Lin, Yao Tang, Weifeng Ge, Wenqiang Zhang</strong></p>
<p>Pre-trained vision-language models have exhibited remarkable abilities in detecting out-of-distribution (OOD) samples. However, some challenging OOD samples, which lie close to in-distribution (InD) data in image feature space, can still lead to misclassification. The emergence of foundation models like diffusion models and multimodal large language models (MLLMs) offers a potential solution to this issue. In this work, we propose SynOOD, a novel approach that harnesses foundation models to generate synthetic, challenging OOD data for fine-tuning CLIP models, thereby enhancing boundary-level discrimination between InD and OOD samples. Our method uses an iterative in-painting process guided by contextual prompts from MLLMs to produce nuanced, boundary-aligned OOD samples. These samples are refined through noise adjustments based on gradients from OOD scores like the energy score, effectively sampling from the InD&#x2F;OOD boundary. With these carefully synthesized images, we fine-tune the CLIP image encoder and negative label features derived from the text encoder to strengthen connections between near-boundary OOD samples and a set of negative labels. Finally, SynOOD achieves state-of-the-art performance on the large-scale ImageNet benchmark, with minimal increases in parameters and runtime. Our approach significantly surpasses existing methods, and the code is available at <a target="_blank" rel="noopener" href="https://github.com/Jarvisgivemeasuit/SynOOD">https://github.com/Jarvisgivemeasuit/SynOOD</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹åœ¨æ£€æµ‹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ ·æœ¬æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€äº›åœ¨å›¾åƒç‰¹å¾ç©ºé—´ä¸Šä¸åˆ†å¸ƒå†…ï¼ˆInDï¼‰æ•°æ®ç›¸è¿‘çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„OODæ ·æœ¬ï¼Œä»å¯èƒ½å¯¼è‡´è¯¯åˆ†ç±»ã€‚æ‰©æ•£æ¨¡å‹å’Œå¤šåª’ä½“è¯­è¨€æ¨¡å‹ç­‰åŸºç¡€æ¨¡å‹çš„æ¶Œç°ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SynOODï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆåˆæˆã€å…·æœ‰æŒ‘æˆ˜æ€§çš„OODæ•°æ®ï¼Œå¯¹CLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ–°æ–¹æ³•ï¼Œä»è€Œå¢å¼ºInDå’ŒOODæ ·æœ¬ä¹‹é—´è¾¹ç•Œçº§åˆ«çš„è¾¨åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ç”±å¤šåª’ä½“è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æç¤ºå¼•å¯¼çš„è¿­ä»£å¡«å……è¿‡ç¨‹ï¼Œäº§ç”Ÿç»†å¾®ã€ä¸è¾¹ç•Œå¯¹é½çš„OODæ ·æœ¬ã€‚è¿™äº›æ ·æœ¬é€šè¿‡åŸºäºOODåˆ†æ•°ï¼ˆå¦‚èƒ½é‡åˆ†æ•°ï¼‰çš„æ¢¯åº¦è¿›è¡Œå™ªå£°è°ƒæ•´æ¥åŠ ä»¥æ”¹è¿›ï¼Œæœ‰æ•ˆåœ°ä»InD&#x2F;OODè¾¹ç•Œè¿›è¡Œé‡‡æ ·ã€‚ä½¿ç”¨è¿™äº›ç²¾å¿ƒåˆæˆçš„å›¾åƒï¼Œæˆ‘ä»¬å¾®è°ƒäº†CLIPå›¾åƒç¼–ç å™¨å’Œæ¥è‡ªæ–‡æœ¬ç¼–ç å™¨çš„è´Ÿæ ‡ç­¾ç‰¹å¾ï¼Œä»¥åŠ å¼ºè¿‘è¾¹ç•ŒOODæ ·æœ¬ä¸ä¸€ç»„è´Ÿæ ‡ç­¾ä¹‹é—´çš„è”ç³»ã€‚æœ€ç»ˆï¼ŒSynOODåœ¨å¤§è§„æ¨¡ImageNetåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‚æ•°å’Œè¿è¡Œæ—¶å¢åŠ æœ€å°‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jarvisgivemeasuit/SynOOD%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Jarvisgivemeasuit/SynOODä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10225v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹æ£€æµ‹å¼‚å¸¸æ ·æœ¬æ—¶ä»å­˜åœ¨è¯¯åˆ¤é£é™©ï¼Œç‰¹åˆ«æ˜¯ä¸å¸¸è§„æ ·æœ¬åœ¨å›¾åƒç‰¹å¾ç©ºé—´ç›¸è¿‘çš„æ ·æœ¬ã€‚æ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶æå‡ºSynOODæ–¹æ³•ï¼Œç»“åˆåŸºç¡€æ¨¡å‹ç”Ÿæˆåˆæˆå¼‚å¸¸æ ·æœ¬æ•°æ®ï¼Œç”¨äºå¾®è°ƒCLIPæ¨¡å‹ï¼Œæé«˜å¸¸è§„æ ·æœ¬ä¸å¼‚å¸¸æ ·æœ¬ä¹‹é—´çš„è¾¹ç•Œçº§åˆ«é‰´åˆ«èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£å¡«å……è¿‡ç¨‹ï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æç¤ºï¼Œç”Ÿæˆç²¾ç»†ã€è¾¹ç•Œå¯¹é½çš„å¼‚å¸¸æ ·æœ¬ã€‚è¿™äº›æ ·æœ¬é€šè¿‡åŸºäºå¼‚å¸¸åˆ†æ•°ï¼ˆå¦‚èƒ½é‡åˆ†æ•°ï¼‰çš„æ¢¯åº¦è¿›è¡Œå™ªå£°è°ƒæ•´ï¼Œæœ‰æ•ˆé‡‡æ ·å¸¸è§„æ ·æœ¬&#x2F;å¼‚å¸¸æ ·æœ¬è¾¹ç•Œã€‚é€šè¿‡ç²¾å¿ƒåˆæˆçš„å›¾åƒï¼Œæˆ‘ä»¬å¾®è°ƒCLIPå›¾åƒç¼–ç å™¨å’Œä»æ–‡æœ¬ç¼–ç å™¨æ´¾ç”Ÿçš„è´Ÿæ ‡ç­¾ç‰¹å¾ï¼ŒåŠ å¼ºè¿‘è¾¹ç•Œå¼‚å¸¸æ ·æœ¬ä¸ä¸€ç³»åˆ—è´Ÿæ ‡ç­¾ä¹‹é—´çš„å…³è”ã€‚æœ€ç»ˆï¼ŒSynOODåœ¨å¤§å‹ImageNetåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œä¸”å‚æ•°å’Œè¿è¡Œæ—¶é—´å¢åŠ å¹…åº¦è¾ƒå°ã€‚è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹æŸäº›ä¸å¸¸è§„æ ·æœ¬ç›¸è¿‘çš„å¼‚å¸¸æ ·æœ¬æ—¶å­˜åœ¨è¯¯åˆ¤é£é™©ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæ”¹å–„è¿™ä¸€çŠ¶å†µæä¾›äº†æ½œåŠ›ã€‚</li>
<li>æå‡ºSynOODæ–¹æ³•ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆåˆæˆå¼‚å¸¸æ ·æœ¬æ•°æ®ï¼Œæé«˜CLIPæ¨¡å‹çš„è¾¹ç•Œé‰´åˆ«èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è¿­ä»£å¡«å……è¿‡ç¨‹å’Œä¸Šä¸‹æ–‡æç¤ºç”Ÿæˆç²¾ç»†ã€è¾¹ç•Œå¯¹é½çš„å¼‚å¸¸æ ·æœ¬ã€‚</li>
<li>é€šè¿‡åŸºäºå¼‚å¸¸åˆ†æ•°çš„æ¢¯åº¦è¿›è¡Œå™ªå£°è°ƒæ•´ï¼Œæœ‰æ•ˆé‡‡æ ·å¸¸è§„æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬çš„è¾¹ç•Œã€‚</li>
<li>é€šè¿‡å¾®è°ƒCLIPå›¾åƒç¼–ç å™¨å’Œè´Ÿæ ‡ç­¾ç‰¹å¾ï¼ŒåŠ å¼ºè¿‘è¾¹ç•Œå¼‚å¸¸æ ·æœ¬ä¸è´Ÿæ ‡ç­¾ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>SynOODåœ¨å¤§å‹ImageNetåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”å‚æ•°å’Œè¿è¡Œæ—¶å¢åŠ è¾ƒå°ï¼Œä»£ç å¯å…¬å¼€è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c62be2f65ab748f52fd66a8e093696ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9eff2f1034c6fe8cfa8da20ff80c095.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-014152f40cd33b577bc10e2c7878cd59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-681933d97e047dcacb3d67817f780db4.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Causally-Steered-Diffusion-for-Automated-Video-Counterfactual-Generation"><a href="#Causally-Steered-Diffusion-for-Automated-Video-Counterfactual-Generation" class="headerlink" title="Causally Steered Diffusion for Automated Video Counterfactual Generation"></a>Causally Steered Diffusion for Automated Video Counterfactual Generation</h2><p><strong>Authors:Nikos Spyrou, Athanasios Vlontzos, Paraskevas Pegios, Thomas Melistas, Nefeli Gkouti, Yannis Panagakis, Giorgos Papanastasiou, Sotirios A. Tsaftaris</strong></p>
<p>Adapting text-to-image (T2I) latent diffusion models (LDMs) to video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships inherent to the video data generating process. Edits affecting causally dependent attributes often generate unrealistic or misleading outcomes if these relationships are ignored. In this work, we introduce a causally faithful framework for counterfactual video generation, formulated as an Out-of-Distribution (OOD) prediction problem. We embed prior causal knowledge by encoding the relationships specified in a causal graph into text prompts and guide the generation process by optimizing these prompts using a vision-language model (VLM)-based textual loss. This loss encourages the latent space of the LDMs to capture OOD variations in the form of counterfactuals, effectively steering generation toward causally meaningful alternatives. The proposed framework, dubbed CSVC, is agnostic to the underlying video editing system and does not require access to its internal mechanisms or fine-tuning. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Experimental results show that CSVC generates causally faithful video counterfactuals within the LDM distribution via prompt-based causal steering, achieving state-of-the-art causal effectiveness without compromising temporal consistency or visual quality on real-world facial videos. Due to its compatibility with any black-box video editing system, our framework has significant potential to generate realistic â€˜what ifâ€™ hypothetical video scenarios in diverse areas such as digital media and healthcare. </p>
<blockquote>
<p>é€‚åº”æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è¿›è¡Œè§†é¢‘ç¼–è¾‘å·²ç»è¡¨ç°å‡ºäº†å¼ºå¤§çš„è§†è§‰ä¿çœŸåº¦å’Œå¯æ§æ€§ï¼Œä½†åœ¨ä¿æŒè§†é¢‘æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸­å›ºæœ‰çš„å› æœå…³ç³»æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚å½±å“å› æœä¾èµ–å±æ€§çš„ç¼–è¾‘å¦‚æœå¿½ç•¥è¿™äº›å…³ç³»ï¼Œé€šå¸¸ä¼šç”Ÿæˆä¸ç°å®æˆ–è¯¯å¯¼æ€§çš„ç»“æœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå› æœå¿ å®æ¡†æ¶ï¼Œç”¨äºåäº‹å®è§†é¢‘ç”Ÿæˆï¼Œè¯¥æ¡†æ¶è¢«åˆ¶å®šä¸ºä¸€ä¸ªç¦»ç¾¤é¢„æµ‹é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å°†å› æœå›¾ä¸­æŒ‡å®šçš„å…³ç³»åµŒå…¥æ–‡æœ¬æç¤ºä¸­ï¼Œæ¥åµŒå…¥å…ˆéªŒå› æœå…³ç³»çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ä½¿ç”¨åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–‡æœ¬æŸå¤±æ¥ä¼˜åŒ–è¿™äº›æç¤ºæ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚è¿™ç§æŸå¤±é¼“åŠ±LDMçš„æ½œåœ¨ç©ºé—´ä»¥åäº‹å®çš„å½¢å¼æ•è·OODå˜åŒ–ï¼Œæœ‰æ•ˆåœ°å°†ç”Ÿæˆå¼•å¯¼å‘å…·æœ‰å› æœæ„ä¹‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚æ‰€æå‡ºçš„æ¡†æ¶è¢«ç§°ä¸ºCSVCï¼Œå®ƒå¯¹åº•å±‚è§†é¢‘ç¼–è¾‘ç³»ç»ŸæŒä¸­ç«‹æ€åº¦ï¼Œæ— éœ€è®¿é—®å…¶å†…éƒ¨æœºåˆ¶æˆ–è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†è§†é¢‘è´¨é‡æŒ‡æ ‡å’Œåäº‹å®ç‰¹å®šæ ‡å‡†ï¼ˆå¦‚å› æœæœ‰æ•ˆæ€§å’Œæœ€å°æ€§ï¼‰æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCSVCé€šè¿‡åŸºäºæç¤ºçš„å› æœå¼•å¯¼åœ¨LDMåˆ†å¸ƒå†…ç”Ÿæˆäº†å› æœå¿ å®çš„è§†é¢‘åäº‹å®ï¼Œåœ¨ç°å®ä¸–ç•Œé¢éƒ¨è§†é¢‘ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å› æœæœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¸æŸå®³æ—¶é—´ä¸€è‡´æ€§æˆ–è§†è§‰è´¨é‡ã€‚ç”±äºå…¶ä¸ä»»ä½•é»‘ç®±è§†é¢‘ç¼–è¾‘ç³»ç»Ÿçš„å…¼å®¹æ€§ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ•°å­—åª’ä½“å’ŒåŒ»ç–—ä¿å¥ç­‰å„ä¸ªé¢†åŸŸç”Ÿæˆç°å®çš„â€œå¦‚æœâ€å‡è®¾æ€§è§†é¢‘åœºæ™¯æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14404v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•å°†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰é€‚é…äºè§†é¢‘ç¼–è¾‘é¢†åŸŸã€‚æ–‡ç« æŒ‡å‡ºäº†ä¿æŒå› æœå…³ç³»çš„é‡è¦æ€§ï¼Œå› ä¸ºåœ¨è§†é¢‘æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå¿½ç•¥è¿™äº›å…³ç³»å¯èƒ½å¯¼è‡´ç¼–è¾‘åå‡ºç°ä¸çœŸå®æˆ–è¯¯å¯¼æ€§çš„ç»“æœã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªå› æœå¿ å®çš„æ¡†æ¶æ¥è¿›è¡Œåäº‹å®è§†é¢‘ç”Ÿæˆï¼Œå°†å…¶è¡¨è¿°ä¸ºè¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰é¢„æµ‹é—®é¢˜ã€‚è¯¥æ¡†æ¶åµŒå…¥å…ˆéªŒå› æœå…³ç³»çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–æ–‡æœ¬æç¤ºæ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸æŸå®³æ—¶é—´è¿è´¯æ€§æˆ–è§†è§‰è´¨é‡çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆå…·æœ‰å› æœå…³ç³»çš„è§†é¢‘åäº‹å®åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨è§†é¢‘ç¼–è¾‘ä¸­å±•ç°å‡ºå¼ºå¤§çš„è§†è§‰ä¿çœŸåº¦å’Œå¯æ§æ€§ã€‚</li>
<li>åœ¨è§†é¢‘æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒå› æœå…³ç³»è‡³å…³é‡è¦ï¼Œå¿½ç•¥è¿™äº›å…³ç³»å¯èƒ½å¯¼è‡´ç¼–è¾‘ç»“æœä¸çœŸå®æˆ–è¯¯å¯¼ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå› æœå¿ å®çš„æ¡†æ¶æ¥è¿›è¡Œåäº‹å®è§†é¢‘ç”Ÿæˆï¼Œè¡¨è¿°ä¸ºè¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰é¢„æµ‹é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡åµŒå…¥å…ˆéªŒå› æœå…³ç³»çŸ¥è¯†å’Œä¼˜åŒ–æ–‡æœ¬æç¤ºæ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶ï¼ˆCSVCï¼‰å¯¹åº•å±‚è§†é¢‘ç¼–è¾‘ç³»ç»Ÿå…·æœ‰é€šç”¨æ€§ï¼Œæ— éœ€è®¿é—®å…¶å†…éƒ¨æœºåˆ¶æˆ–å¾®è°ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCSVCåœ¨ç”Ÿæˆå…·æœ‰å› æœå…³ç³»çš„è§†é¢‘åäº‹å®åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†æ—¶é—´è¿è´¯æ€§å’Œè§†è§‰è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-954be0592a07880feb0819e52b64cfe5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a47b07f250a63a438b368c506578e63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d19ab1e19ca6645830c2016a1b59b61.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Robust-Photo-Realistic-Hand-Gesture-Generation-from-Single-View-to-Multiple-View"><a href="#Robust-Photo-Realistic-Hand-Gesture-Generation-from-Single-View-to-Multiple-View" class="headerlink" title="Robust Photo-Realistic Hand Gesture Generation: from Single View to   Multiple View"></a>Robust Photo-Realistic Hand Gesture Generation: from Single View to   Multiple View</h2><p><strong>Authors:Qifan Fu, Xu Chen, Muhammad Asad, Shanxin Yuan, Changjae Oh, Gregory Slabaugh</strong></p>
<p>High-fidelity hand gesture generation represents a significant challenge in human-centric generation tasks. Existing methods typically employ a single-view mesh-rendered image prior to enhancing gesture generation quality. However, the spatial complexity of hand gestures and the inherent limitations of single-view rendering make it difficult to capture complete gesture information, particularly when fingers are occluded. The fundamental contradiction lies in the loss of 3D topological relationships through 2D projection and the incomplete spatial coverage inherent to single-view representations. Diverging from single-view prior approaches, we propose a multi-view prior framework, named Multi-Modal UNet-based Feature Encoder (MUFEN), to guide diffusion models in learning comprehensive 3D hand information. Specifically, we extend conventional front-view rendering to include rear, left, right, top, and bottom perspectives, selecting the most information-rich view combination as training priors to address occlusion. This multi-view prior with a dedicated dual stream encoder significantly improves the modelâ€™s understanding of complete hand features. Furthermore, we design a bounding box feature fusion module, which can fuse the gesture localization features and multi-modal features to enhance the location-awareness of the MUFEN features to the gesture-related features. Experiments demonstrate that our method achieves state-of-the-art performance in both quantitative metrics and qualitative evaluations. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/fuqifan/MUFEN">https://github.com/fuqifan/MUFEN</a>. </p>
<blockquote>
<p>é«˜ç²¾åº¦æ‰‹åŠ¿ç”Ÿæˆæ˜¯é¢å‘äººç±»ç”Ÿæˆä»»åŠ¡ä¸­çš„ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å•è§†å›¾ç½‘æ ¼æ¸²æŸ“å›¾åƒæ¥æé«˜æ‰‹åŠ¿ç”Ÿæˆè´¨é‡ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¿çš„ç©ºé—´å¤æ‚æ€§ä»¥åŠå•è§†å›¾æ¸²æŸ“çš„å†…åœ¨å±€é™æ€§ï¼Œä½¿å¾—éš¾ä»¥æ•æ‰å®Œæ•´çš„æ‰‹åŠ¿ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å½“æ‰‹æŒ‡è¢«é®æŒ¡æ—¶ã€‚æ ¹æœ¬çŸ›ç›¾åœ¨äºé€šè¿‡äºŒç»´æŠ•å½±æ‰€ä¸¢å¤±çš„ä¸‰ç»´æ‹“æ‰‘å…³ç³»ä»¥åŠå•è§†å›¾è¡¨ç¤ºæ‰€å›ºæœ‰çš„ç©ºé—´è¦†ç›–ä¸å®Œæ•´ã€‚ä¸å•è§†å›¾å…ˆéªŒæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€UNetçš„ç‰¹å¾ç¼–ç å™¨ï¼ˆMUFENï¼‰çš„å¤šè§†å›¾å…ˆéªŒæ¡†æ¶ï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹å­¦ä¹ å…¨é¢çš„ä¸‰ç»´æ‰‹åŠ¿ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä¼ ç»Ÿçš„å‰è§†æ¸²æŸ“æ‰©å±•åˆ°åŒ…æ‹¬åè§†ã€å·¦è§†ã€å³è§†ã€é¡¶è§†å’Œåº•è§†çš„è§’åº¦ï¼Œé€‰æ‹©ä¿¡æ¯æœ€ä¸°å¯Œçš„è§†å›¾ç»„åˆä½œä¸ºè®­ç»ƒå…ˆéªŒæ¥è§£å†³é®æŒ¡é—®é¢˜ã€‚è¿™ç§å¤šè§†å›¾å…ˆéªŒä¸ä¸“ç”¨çš„åŒæµç¼–ç å™¨ç›¸ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹å®Œæ•´æ‰‹åŠ¿ç‰¹å¾çš„ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¾¹ç•Œæ¡†ç‰¹å¾èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥èåˆæ‰‹åŠ¿å®šä½ç‰¹å¾å’Œå¤šæ¨¡æ€ç‰¹å¾ï¼Œä»¥å¢å¼ºMUFENç‰¹å¾å¯¹æ‰‹åŠ¿ç›¸å…³ç‰¹å¾çš„å®šä½æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fuqifan/MUFEN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/fuqifan/MUFENæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10576v2">PDF</a> This nine pages paper has been accepted for publication in   Proceedings of the 33rd ACM International Conference on Multimedia (ACM MM   2025). This is the authorâ€™s version which has not been fully edited and   content may change prior to final publication. Citation information: DOI   <a target="_blank" rel="noopener" href="https://doi.org/10.1145/3746027.3755828">https://doi.org/10.1145/3746027.3755828</a></p>
<p><strong>Summary</strong></p>
<p>é«˜ä¿çœŸæ‰‹åŠ¿ç”Ÿæˆæ˜¯é¢å‘äººç±»ç”Ÿæˆä»»åŠ¡çš„ä¸€å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å•è§†å›¾ç½‘æ ¼æ¸²æŸ“å›¾åƒæ¥æå‡æ‰‹åŠ¿ç”Ÿæˆè´¨é‡ï¼Œä½†æ‰‹åŠ¿çš„ç©ºé—´å¤æ‚æ€§ä»¥åŠå•è§†å›¾æ¸²æŸ“çš„å›ºæœ‰å±€é™æ€§ï¼Œä½¿å¾—åœ¨æ‰‹æŒ‡è¢«é®æŒ¡æ—¶éš¾ä»¥æ•è·å®Œæ•´çš„æ‰‹åŠ¿ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMUFENçš„å¤šè§†è§’å…ˆéªŒæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€UNetåŸºç¡€ç‰¹å¾ç¼–ç å™¨å¼•å¯¼æ‰©æ•£æ¨¡å‹å­¦ä¹ å…¨é¢çš„3Dæ‰‹åŠ¿ä¿¡æ¯ã€‚é€šè¿‡æ‰©å±•å¸¸è§„çš„å‰è§†å›¾æ¸²æŸ“ï¼ŒåŒ…æ‹¬åè§†ã€å·¦è§†ã€å³è§†ã€ä¸Šè§†å’Œä¸‹è§†è§’åº¦ï¼Œé€‰æ‹©ä¿¡æ¯æœ€ä¸°å¯Œçš„è§†å›¾ç»„åˆä½œä¸ºè®­ç»ƒå…ˆéªŒï¼Œä»¥è§£å†³é®æŒ¡é—®é¢˜ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªè¾¹ç•Œæ¡†ç‰¹å¾èåˆæ¨¡å—ï¼Œå¯ä»¥èåˆæ‰‹åŠ¿å®šä½ç‰¹å¾å’Œå¤šæ¨¡æ€ç‰¹å¾ï¼Œæé«˜MUFENç‰¹å¾å¯¹æ‰‹åŠ¿ç›¸å…³ç‰¹å¾çš„å®šä½æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä»·ä¸Šå‡è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜ä¿çœŸæ‰‹åŠ¿ç”Ÿæˆæ˜¯é¢å‘äººç±»ç”Ÿæˆä»»åŠ¡çš„é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨å•è§†å›¾ç½‘æ ¼æ¸²æŸ“å›¾åƒï¼Œä½†å­˜åœ¨ç©ºé—´å¤æ‚æ€§å’Œé®æŒ¡é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºMUFENçš„å¤šè§†è§’å…ˆéªŒæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€UNetåŸºç¡€ç‰¹å¾ç¼–ç å™¨å­¦ä¹ å…¨é¢çš„3Dæ‰‹åŠ¿ä¿¡æ¯ã€‚</li>
<li>æ‰©å±•äº†å¸¸è§„å‰è§†å›¾æ¸²æŸ“ï¼ŒåŒ…æ‹¬å¤šä¸ªè§†è§’ï¼Œå¹¶é€‰æ‹©ä¿¡æ¯æœ€ä¸°å¯Œçš„è§†å›¾ä½œä¸ºè®­ç»ƒå…ˆéªŒã€‚</li>
<li>è®¾è®¡çš„è¾¹ç•Œæ¡†ç‰¹å¾èåˆæ¨¡å—èƒ½èåˆæ‰‹åŠ¿å®šä½å’Œå¤šæ¨¡æ€ç‰¹å¾ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä»·ä¸Šå‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe2da62c28c4265f6919b39e38c70f75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47c0494eef7df45c42c6b6f37b7a54c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5ad114b8ee0a2272807b733f3efccd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8177b8f6d9013f7a14a7ed0c87db3e0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48f778efc68d9fbfd1002a9946367479.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FLUX-Text-A-Simple-and-Advanced-Diffusion-Transformer-Baseline-for-Scene-Text-Editing"><a href="#FLUX-Text-A-Simple-and-Advanced-Diffusion-Transformer-Baseline-for-Scene-Text-Editing" class="headerlink" title="FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for   Scene Text Editing"></a>FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for   Scene Text Editing</h2><p><strong>Authors:Rui Lan, Yancheng Bai, Xu Duan, Mingxing Li, Dongyang Jin, Ryan Xu, Lei Sun, Xiangxiang Chu</strong></p>
<p>Scene text editing aims to modify or add texts on images while ensuring text fidelity and overall visual quality consistent with the background. Recent methods are primarily built on UNet-based diffusion models, which have improved scene text editing results, but still struggle with complex glyph structures, especially for non-Latin ones (\eg, Chinese, Korean, Japanese). To address these issues, we present \textbf{FLUX-Text}, a simple and advanced multilingual scene text editing DiT method. Specifically, our FLUX-Text enhances glyph understanding and generation through lightweight Visual and Text Embedding Modules, while preserving the original generative capability of FLUX. We further propose a Regional Text Perceptual Loss tailored for text regions, along with a matching two-stage training strategy to better balance text editing and overall image quality. Benefiting from the DiT-based architecture and lightweight feature injection modules, FLUX-Text can be trained with only $0.1$M training examples, a \textbf{97%} reduction compared to $2.9$M required by popular methods. Extensive experiments on multiple public datasets, including English and Chinese benchmarks, demonstrate that our method surpasses other methods in visual quality and text fidelity. All the code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/FluxText">https://github.com/AMAP-ML/FluxText</a>. </p>
<blockquote>
<p>åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ—¨åœ¨åœ¨å›¾åƒä¸Šä¿®æ”¹æˆ–æ·»åŠ æ–‡æœ¬ï¼ŒåŒæ—¶ç¡®ä¿æ–‡æœ¬å¿ è¯šåº¦å’Œä¸èƒŒæ™¯ä¸€è‡´çš„æ•´ä½“è§†è§‰è´¨é‡ã€‚æœ€è¿‘çš„æ–¹æ³•ä¸»è¦å»ºç«‹åœ¨åŸºäºUNetçš„æ‰©æ•£æ¨¡å‹ä¸Šï¼Œå·²ç»æ”¹å–„äº†åœºæ™¯æ–‡æœ¬ç¼–è¾‘çš„ç»“æœï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„å­—å½¢ç»“æ„æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯éæ‹‰ä¸è¯­ç³»ï¼ˆä¾‹å¦‚ä¸­æ–‡ã€éŸ©è¯­ã€æ—¥è¯­ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<strong>FLUX-Text</strong>ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•å…ˆè¿›çš„å¤šå…ƒåœºæ™¯æ–‡æœ¬ç¼–è¾‘DiTæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„FLUX-Texté€šè¿‡è½»é‡çº§çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥æ¨¡å—ï¼Œå¢å¼ºäº†å­—å½¢ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†FLUXçš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬åŒºåŸŸçš„åŒºåŸŸæ–‡æœ¬æ„ŸçŸ¥æŸå¤±ï¼Œä»¥åŠä¸€ä¸ªé…å¥—çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡æ–‡æœ¬ç¼–è¾‘å’Œæ•´ä½“å›¾åƒè´¨é‡ã€‚å—ç›ŠäºDiTæ¶æ„å’Œè½»é‡çº§ç‰¹å¾æ³¨å…¥æ¨¡å—ï¼ŒFLUX-Textä»…éœ€0.1Mè®­ç»ƒæ ·æœ¬å³å¯è¿›è¡Œè®­ç»ƒï¼Œä¸æµè¡Œæ–¹æ³•æ‰€éœ€çš„2.9Mç›¸æ¯”ï¼Œå‡å°‘äº†97%ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬è‹±è¯­å’Œä¸­æ–‡åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ–‡æœ¬å¿ è¯šåº¦æ–¹é¢è¶…è¿‡äº†å…¶ä»–æ–¹æ³•ã€‚æ‰€æœ‰ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/FluxText%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AMAP-ML/FluxTextè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03329v2">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºUNetçš„æ‰©æ•£æ¨¡å‹åœ¨åœºæ™¯æ–‡æœ¬ç¼–è¾‘ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä»é¢ä¸´å¤„ç†å¤æ‚å­—å½¢ç»“æ„å°¤å…¶æ˜¯éæ‹‰ä¸å­—æ¯çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FLUX-Textï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„å¤šè¯­è¨€åœºæ™¯æ–‡æœ¬ç¼–è¾‘DiTæ–¹æ³•ã€‚å®ƒé€šè¿‡è½»é‡çº§çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥æ¨¡å—æå‡å­—å½¢ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒFLUXçš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºæ–‡æœ¬åŒºåŸŸé‡èº«å®šåˆ¶äº†åŒºåŸŸæ–‡æœ¬æ„ŸçŸ¥æŸå¤±ï¼Œå¹¶é…åˆä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥æ›´å¥½åœ°å¹³è¡¡æ–‡æœ¬ç¼–è¾‘å’Œæ•´ä½“å›¾åƒè´¨é‡ã€‚FLUX-Textå€ŸåŠ©DiTæ¶æ„å’Œè½»é‡çº§ç‰¹å¾æ³¨å…¥æ¨¡å—ï¼Œä»…ä½¿ç”¨0.1Mè®­ç»ƒæ ·æœ¬å³å¯è®­ç»ƒï¼Œç›¸è¾ƒäºæµè¡Œæ–¹æ³•æ‰€éœ€çš„2.9Mæ ·æœ¬ï¼Œå‡å°‘äº†97%ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ–‡æœ¬ä¿çœŸåº¦ä¸Šè¶…è¶Šäº†å…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœºæ™¯æ–‡æœ¬ç¼–è¾‘çš„ç›®æ ‡æ˜¯åœ¨ç¡®ä¿æ–‡æœ¬ä¿çœŸåº¦å’Œä¸èƒŒæ™¯ä¸€è‡´çš„æ•´ä½“è§†è§‰è´¨é‡çš„å‰æä¸‹ï¼Œå¯¹å›¾åƒä¸Šçš„æ–‡æœ¬è¿›è¡Œä¿®æ”¹æˆ–æ·»åŠ ã€‚</li>
<li>è¿‘æœŸçš„æ–¹æ³•ä¸»è¦åŸºäºUNetæ‰©æ•£æ¨¡å‹ï¼Œåœ¨åœºæ™¯æ–‡æœ¬ç¼–è¾‘ä¸­å–å¾—äº†æˆæœï¼Œä½†åœ¨å¤„ç†å¤æ‚å­—å½¢ç»“æ„ï¼ˆç‰¹åˆ«æ˜¯éæ‹‰ä¸å­—æ¯ï¼‰æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„FLUX-Textæ˜¯ä¸€ç§å¤šè¯­è¨€åœºæ™¯æ–‡æœ¬ç¼–è¾‘çš„DiTæ–¹æ³•ï¼Œé€šè¿‡è½»é‡çº§çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥æ¨¡å—å¢å¼ºå­—å½¢ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>FLUX-Texté‡‡ç”¨é’ˆå¯¹æ–‡æœ¬åŒºåŸŸçš„åŒºåŸŸæ–‡æœ¬æ„ŸçŸ¥æŸå¤±å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥å¹³è¡¡æ–‡æœ¬ç¼–è¾‘å’Œæ•´ä½“å›¾åƒè´¨é‡ã€‚</li>
<li>FLUX-Textä»…éœ€0.1Mè®­ç»ƒæ ·æœ¬å³å¯è®­ç»ƒï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å¤§å¤§å‡å°‘äº†æ‰€éœ€æ ·æœ¬æ•°é‡ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFLUX-Textåœ¨è§†è§‰è´¨é‡å’Œæ–‡æœ¬ä¿çœŸåº¦æ–¹é¢è¡¨ç°è¶…è¶Šå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-258ecdcdd5d99ad957699e895583a08a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e91eb2b201091db265d584cb6ad251d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8d5280844e91632a6a4677afec08994.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1368a212715e29a72b28910a429cb3aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-46fe98b625663acfea9aa6119d54877a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3ce41b3c4de87e1325f0f800a29a89e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="IntroStyle-Training-Free-Introspective-Style-Attribution-using-Diffusion-Features"><a href="#IntroStyle-Training-Free-Introspective-Style-Attribution-using-Diffusion-Features" class="headerlink" title="IntroStyle: Training-Free Introspective Style Attribution using   Diffusion Features"></a>IntroStyle: Training-Free Introspective Style Attribution using   Diffusion Features</h2><p><strong>Authors:Anand Kumar, Jiteng Mu, Nuno Vasconcelos</strong></p>
<p>Text-to-image (T2I) models have recently gained widespread adoption. This has spurred concerns about safeguarding intellectual property rights and an increasing demand for mechanisms that prevent the generation of specific artistic styles. Existing methods for style extraction typically necessitate the collection of custom datasets and the training of specialized models. This, however, is resource-intensive, time-consuming, and often impractical for real-time applications. We present a novel, training-free framework to solve the style attribution problem, using the features produced by a diffusion model alone, without any external modules or retraining. This is denoted as Introspective Style attribution (IntroStyle) and is shown to have superior performance to state-of-the-art models for style attribution. We also introduce a synthetic dataset of Artistic Style Split (ArtSplit) to isolate artistic style and evaluate fine-grained style attribution performance. Our experimental results on WikiArt and DomainNet datasets show that \ours is robust to the dynamic nature of artistic styles, outperforming existing methods by a wide margin. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹æœ€è¿‘å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚è¿™å¼•å‘äº†å…³äºä¿æŠ¤çŸ¥è¯†äº§æƒçš„æ‹…å¿§ï¼Œä»¥åŠå¯¹é˜²æ­¢ç‰¹å®šè‰ºæœ¯é£æ ¼ç”Ÿæˆçš„æœºåˆ¶çš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚ç°æœ‰çš„é£æ ¼æå–æ–¹æ³•é€šå¸¸éœ€è¦æ”¶é›†è‡ªå®šä¹‰æ•°æ®é›†å’Œè®­ç»ƒä¸“ç”¨æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™æ ·åšèµ„æºå¯†é›†ã€è€—æ—¶ï¼Œå¯¹äºå®æ—¶åº”ç”¨é€šå¸¸ä¸åˆ‡å®é™…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„é£æ ¼å½’å±é—®é¢˜è§£å†³æ–¹æ¡ˆæ¡†æ¶ï¼Œä»…ä½¿ç”¨æ‰©æ•£æ¨¡å‹äº§ç”Ÿçš„ç‰¹å¾ï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨æ¨¡å—æˆ–é‡æ–°è®­ç»ƒã€‚è¿™è¢«ç§°ä¸ºå†…çœé£æ ¼å½’å±ï¼ˆIntroStyleï¼‰ï¼Œå¹¶è¡¨ç°å‡ºä¼˜äºå½“å‰æœ€å…ˆè¿›çš„é£æ ¼å½’å±æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè‰ºæœ¯é£æ ¼åˆ†å‰²ï¼ˆArtSplitï¼‰çš„åˆæˆæ•°æ®é›†ï¼Œä»¥éš”ç¦»è‰ºæœ¯é£æ ¼å¹¶è¯„ä¼°ç²¾ç»†é£æ ¼å½’å±æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨WikiArtå’ŒDomainNetæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹è‰ºæœ¯é£æ ¼çš„åŠ¨æ€ç‰¹æ€§å…·æœ‰ç¨³å¥æ€§ï¼Œå¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14432v2">PDF</a> 17 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ–°æå‡ºçš„è®­ç»ƒæœ‰ç´ çš„æ¡†æ¶â€”â€”Introspective Style attributionï¼ˆIntroStyleï¼‰ï¼Œç”¨äºè§£å†³é£æ ¼å½’å±é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾ï¼Œæ— éœ€å¤–éƒ¨æ¨¡å—æˆ–é‡æ–°è®­ç»ƒï¼Œå³å¯å®ç°ä¼˜è¶Šçš„é£æ ¼å½’å±æ€§èƒ½ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ä¸ªåˆæˆæ•°æ®é›†Artistic Style Splitï¼ˆArtSplitï¼‰æ¥è¯„ä¼°ç²¾ç»†é£æ ¼å½’å±æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨WikiArtå’ŒDomainNetæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ¨¡å‹å¹¿æ³›é‡‡ç”¨å¼•å‘äº†å…³äºä¿æŠ¤çŸ¥è¯†äº§æƒçš„æ‹…å¿§å’Œéœ€æ±‚æœºåˆ¶é˜²æ­¢ç‰¹å®šè‰ºæœ¯é£æ ¼çš„ç”Ÿæˆã€‚</li>
<li>ç°æœ‰é£æ ¼æå–æ–¹æ³•é€šå¸¸éœ€è¦æ”¶é›†è‡ªå®šä¹‰æ•°æ®é›†å’Œè®­ç»ƒä¸“ç”¨æ¨¡å‹ï¼Œè¿™æ—¢è€—æ—¶åˆè€—èµ„æºï¼Œä¸”å¯¹å®æ—¶åº”ç”¨æ¥è¯´ä¸åˆ‡å®é™…ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒå¤–æ¡†æ¶æ¥è§£å†³é£æ ¼å½’å±é—®é¢˜ï¼Œç§°ä¸ºIntrospective Style attributionï¼ˆIntroStyleï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶ä»…åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾ï¼Œæ— éœ€å¤–éƒ¨æ¨¡å—æˆ–é‡æ–°è®­ç»ƒã€‚</li>
<li>å¼•å…¥äº†åˆæˆæ•°æ®é›†Artistic Style Splitï¼ˆArtSplitï¼‰ä»¥è¯„ä¼°ç²¾ç»†é£æ ¼å½’å±æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨WikiArtå’ŒDomainNetæ•°æ®é›†ä¸Šï¼ŒIntroStyleä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å…¶å¯¹åŠ¨æ€è‰ºæœ¯é£æ ¼çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14432">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f48d39d3ee954d4bb752ffbaa0d8635c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-398a3629b4d91774ccb853da96075cc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-819bad4e175a0f7dae5d2f08ef02acd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b84e7220029e1784c96c89329916ea5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b25fea3f88af60a755a73157622c65de.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CreatiLayout-Siamese-Multimodal-Diffusion-Transformer-for-Creative-Layout-to-Image-Generation"><a href="#CreatiLayout-Siamese-Multimodal-Diffusion-Transformer-for-Creative-Layout-to-Image-Generation" class="headerlink" title="CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative   Layout-to-Image Generation"></a>CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative   Layout-to-Image Generation</h2><p><strong>Authors:Hui Zhang, Dexiang Hong, Yitong Wang, Jie Shao, Xinglong Wu, Zuxuan Wu, Yu-Gang Jiang</strong></p>
<p>Diffusion models have been recognized for their ability to generate images that are not only visually appealing but also of high artistic quality. As a result, Layout-to-Image (L2I) generation has been proposed to leverage region-specific positions and descriptions to enable more precise and controllable generation. However, previous methods primarily focus on UNet-based models (\eg SD1.5 and SDXL), and limited effort has explored Multimodal Diffusion Transformers (MM-DiTs), which have demonstrated powerful image generation capabilities. Enabling MM-DiT for layout-to-image generation seems straightforward but is challenging due to the complexity of how layout is introduced, integrated, and balanced among multiple modalities. To this end, we explore various network variants to efficiently incorporate layout guidance into MM-DiT, and ultimately present SiamLayout. To inherit the advantages of MM-DiT, we use a separate set of network weights to process the layout, treating it as equally important as the image and text modalities. Meanwhile, to alleviate the competition among modalities, we decouple the image-layout interaction into a siamese branch alongside the image-text one and fuse them in the later stage. Moreover, we contribute a large-scale layout dataset, named LayoutSAM, which includes 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a bounding box and a detailed description. We further construct the LayoutSAM-Eval benchmark as a comprehensive tool for evaluating the L2I generation quality. Finally, we introduce the Layout Designer, which taps into the potential of large language models in layout planning, transforming them into experts in layout generation and optimization. These components form CreatiLayout â€“ a systematic solution that integrates the layout model, dataset, and planner for creative layout-to-image generation. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶ç”Ÿæˆå›¾åƒçš„èƒ½åŠ›è€Œå—åˆ°è®¤å¯ï¼Œè¿™äº›å›¾åƒä¸ä»…è§†è§‰ä¸Šå¸å¼•äººï¼Œè€Œä¸”å…·æœ‰é«˜åº¦çš„è‰ºæœ¯è´¨é‡ã€‚å› æ­¤ï¼Œæå‡ºäº†Layout-to-Imageï¼ˆL2Iï¼‰ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨ç‰¹å®šåŒºåŸŸçš„ä½ç½®å’Œæè¿°æ¥å®ç°æ›´ç²¾ç¡®å’Œå¯æ§çš„ç”Ÿæˆã€‚ç„¶è€Œï¼Œä»¥å¾€çš„æ–¹æ³•ä¸»è¦å…³æ³¨åŸºäºUNetçš„æ¨¡å‹ï¼ˆä¾‹å¦‚SD1.5å’ŒSDXLï¼‰ï¼Œå¯¹å¤šæ¨¡æ€æ‰©æ•£Transformerï¼ˆMM-DiTï¼‰çš„æ¢ç´¢æœ‰é™ï¼Œåè€…å·²æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚è™½ç„¶ä½¿MM-DiTç”¨äºå¸ƒå±€åˆ°å›¾åƒç”Ÿæˆçœ‹ä¼¼ç®€å•ï¼Œä½†ç”±äºå¼•å…¥ã€é›†æˆå’Œå¹³è¡¡å¸ƒå±€äºå¤šç§æ¨¡æ€ä¹‹é—´çš„å¤æ‚æ€§ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å„ç§ç½‘ç»œå˜ä½“ï¼Œä»¥æœ‰æ•ˆåœ°å°†å¸ƒå±€æŒ‡å¯¼èå…¥MM-DiTï¼Œå¹¶æœ€ç»ˆæ¨å‡ºSiamLayoutã€‚ä¸ºäº†ç»§æ‰¿MM-DiTçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç»„ç‹¬ç«‹çš„ç½‘ç»œæƒé‡æ¥å¤„ç†å¸ƒå±€ï¼Œå°†å…¶è§†ä¸ºä¸å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€åŒæ ·é‡è¦ã€‚åŒæ—¶ï¼Œä¸ºäº†å‡è½»æ¨¡æ€ä¹‹é—´çš„ç«äº‰ï¼Œæˆ‘ä»¬å°†å›¾åƒå¸ƒå±€äº¤äº’ä»å›¾åƒæ–‡æœ¬åˆ†æ”¯ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå¹¶åœ¨åæœŸè¿›è¡Œèåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªå¤§è§„æ¨¡å¸ƒå±€æ•°æ®é›†ï¼Œåä¸ºLayoutSAMï¼Œå…¶ä¸­åŒ…æ‹¬270ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹å’Œ1070ä¸‡ä¸ªå®ä½“ã€‚æ¯ä¸ªå®ä½“éƒ½å¸¦æœ‰è¾¹ç•Œæ¡†å’Œè¯¦ç»†æè¿°ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†LayoutSAM-EvalåŸºå‡†æµ‹è¯•ï¼Œä½œä¸ºè¯„ä¼°L2Iç”Ÿæˆè´¨é‡çš„ç»¼åˆå·¥å…·ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¸ƒå±€è®¾è®¡å¸ˆï¼ˆLayout Designerï¼‰ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¸ƒå±€è§„åˆ’ä¸­çš„æ½œåŠ›ï¼Œå°†å…¶è½¬åŒ–ä¸ºå¸ƒå±€ç”Ÿæˆå’Œä¼˜åŒ–çš„ä¸“å®¶ã€‚è¿™äº›ç»„ä»¶å…±åŒæ„æˆäº†CreatiLayoutâ€”â€”ä¸€ä¸ªç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼Œé›†æˆäº†å¸ƒå±€æ¨¡å‹ã€æ•°æ®é›†å’Œè§„åˆ’å™¨ï¼Œç”¨äºåˆ›æ„å¸ƒå±€åˆ°å›¾åƒçš„ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03859v3">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Diffusionæ¨¡å‹åœ¨ç”Ÿæˆå…·æœ‰å¸ƒå±€çš„å›¾åƒæ–¹é¢çš„èƒ½åŠ›ã€‚ä»¥å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŸºäºUNetæ¨¡å‹çš„å¸ƒå±€ç”Ÿæˆæ–¹æ³•ä¸Šï¼Œä½†å¯¹äºMultimodal Diffusion Transformers (MM-DiT)çš„ç ”ç©¶æœ‰é™ã€‚ä¸ºäº†å°†MM-DiTåº”ç”¨äºå¸ƒå±€åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œæœ¬æ–‡æ¢ç´¢äº†å¤šç§ç½‘ç»œå˜ä½“ä»¥æœ‰æ•ˆåœ°å°†å¸ƒå±€æŒ‡å¯¼èå…¥MM-DiTï¼Œå¹¶æå‡ºäº†SiamLayoutæ–¹æ³•ã€‚åŒæ—¶ï¼Œä¸ºäº†å‡è½»æ¨¡æ€ä¹‹é—´çš„ç«äº‰ï¼Œå°†å›¾åƒä¸å¸ƒå±€çš„äº¤äº’è§£è€¦ä¸ºä¸å›¾åƒæ–‡æœ¬å¹¶è¡Œçš„å­ªç”Ÿåˆ†æ”¯ï¼Œå¹¶åœ¨åæœŸè¿›è¡Œèåˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è´¡çŒ®äº†ä¸€ä¸ªå¤§è§„æ¨¡å¸ƒå±€æ•°æ®é›†LayoutSAMï¼ŒåŒ…å«å›¾åƒæ–‡æœ¬å¯¹å’Œå®ä½“æ ‡æ³¨çš„è¾¹ç•Œæ¡†å’Œè¯¦ç»†æè¿°ã€‚æœ€åï¼Œå¼•å…¥äº†Layout Designerä½œä¸ºåˆ›æ„å¸ƒå±€è§„åˆ’çš„å·¥å…·ï¼Œå½¢æˆäº†é›†å¸ƒå±€æ¨¡å‹ã€æ•°æ®é›†å’Œè§„åˆ’å™¨äºä¸€ä½“çš„CreatiLayoutç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusionæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>Layout-to-Imageï¼ˆL2Iï¼‰ç”Ÿæˆåˆ©ç”¨åŒºåŸŸç‰¹å®šä½ç½®å’Œæè¿°æ¥å®ç°æ›´ç²¾ç¡®å’Œå¯æ§çš„ç”Ÿæˆã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨UNet-basedæ¨¡å‹ï¼Œä½†å¯¹Multimodal Diffusion Transformers (MM-DiT)çš„ç ”ç©¶æœ‰é™ã€‚</li>
<li>å°†MM-DiTç”¨äºL2Iç”Ÿæˆé¢ä¸´å¤æ‚æ€§ï¼Œéœ€è¦æ¢ç´¢å¦‚ä½•æœ‰æ•ˆèå…¥å¸ƒå±€æŒ‡å¯¼ã€‚</li>
<li>SiamLayoutæ–¹æ³•æ—¨åœ¨ç»§æ‰¿MM-DiTçš„ä¼˜ç‚¹ï¼Œä½¿ç”¨å•ç‹¬çš„ç½‘ç»œæƒé‡å¤„ç†å¸ƒå±€ã€‚</li>
<li>ä¸ºäº†å‡è½»æ¨¡æ€ç«äº‰ï¼Œå›¾åƒä¸å¸ƒå±€çš„äº¤äº’è¢«è§£è€¦ä¸ºå­ªç”Ÿåˆ†æ”¯å¹¶åœ¨åæœŸèåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-801f5bce18b516eeb9427fd70bf3a4c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26fbd8de78e813b98b0216047d6b9996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d0b86c52feca87286e9d69698727f63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8671d9d7259b61533ae357f5325deca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ec96ebca697e6cdfdab43f8bdb08c33.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Individual-Content-and-Motion-Dynamics-Preserved-Pruning-for-Video-Diffusion-Models"><a href="#Individual-Content-and-Motion-Dynamics-Preserved-Pruning-for-Video-Diffusion-Models" class="headerlink" title="Individual Content and Motion Dynamics Preserved Pruning for Video   Diffusion Models"></a>Individual Content and Motion Dynamics Preserved Pruning for Video   Diffusion Models</h2><p><strong>Authors:Yiming Wu, Zhenghao Chen, Huan Wang, Dong Xu</strong></p>
<p>The high computational cost and slow inference time are major obstacles to deploying Video Diffusion Models (VDMs). To overcome this, we introduce a new Video Diffusion Model Compression approach using individual content and motion dynamics preserved pruning and consistency loss. First, we empirically observe that deeper VDM layers are crucial for maintaining the quality of \textbf{motion dynamics} (\textit{e.g.,} coherence of the entire video), while shallower layers are more focused on \textbf{individual content} (\textit{e.g.,} individual frames). Therefore, we prune redundant blocks from the shallower layers while preserving more of the deeper layers, resulting in a lightweight VDM variant called VDMini. Moreover, we propose an \textbf{Individual Content and Motion Dynamics (ICMD)} Consistency Loss to gain comparable generation performance as larger VDM to VDMini. In particular, we first use the Individual Content Distillation (ICD) Loss to preserve the consistency in the features of each generated frame between the teacher and student models. Next, we introduce a Multi-frame Content Adversarial (MCA) Loss to enhance the motion dynamics across the generated video as a whole. This method significantly accelerates inference time while maintaining high-quality video generation. Extensive experiments demonstrate the effectiveness of our VDMini on two important video generation tasks, Text-to-Video (T2V) and Image-to-Video (I2V), where we respectively achieve an average 2.5 $\times$, 1.4 $\times$, and 1.25 $\times$ speed up for the I2V method SF-V, the T2V method T2V-Turbo-v2, and the T2V method HunyuanVideo, while maintaining the quality of the generated videos on several benchmarks including UCF101, VBench-T2V, and VBench-I2V. </p>
<blockquote>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰çš„é«˜è®¡ç®—æˆæœ¬å’Œç¼“æ…¢çš„æ¨ç†æ—¶é—´æ˜¯å…¶éƒ¨ç½²çš„ä¸»è¦éšœç¢ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¿ç•™ä¸ªäººå†…å®¹å’Œè¿åŠ¨åŠ¨åŠ›å­¦çš„ä¿®å‰ªå’Œä¸€è‡´æ€§æŸå¤±æ¥å®ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒè§‚å¯Ÿå‘ç°ï¼Œè¾ƒæ·±çš„VDMå±‚å¯¹äºç»´æŒè¿åŠ¨åŠ¨åŠ›å­¦çš„è´¨é‡ï¼ˆä¾‹å¦‚æ•´ä¸ªè§†é¢‘çš„ä¸€è‡´æ€§ï¼‰è‡³å…³é‡è¦ï¼Œè€Œè¾ƒæµ…çš„å±‚åˆ™æ›´ä¸“æ³¨äºä¸ªäººå†…å®¹ï¼ˆä¾‹å¦‚å•ä¸ªå¸§ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»è¾ƒæµ…çš„å±‚ä¸­ä¿®å‰ªæ‰å†—ä½™çš„å—ï¼ŒåŒæ—¶ä¿ç•™æ›´å¤šçš„æ·±å±‚ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªè½»é‡çº§çš„VDMå˜ä½“ï¼Œç§°ä¸ºVDMiniã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ªäººå†…å®¹å’Œè¿åŠ¨åŠ¨åŠ›å­¦ï¼ˆICMDï¼‰ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥åœ¨è¾ƒå¤§çš„VDMå’ŒVDMiniä¹‹é—´è·å¾—ç›¸å½“çš„ç”Ÿäº§æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä¸ªäººå†…å®¹è’¸é¦ï¼ˆICDï¼‰æŸå¤±æ¥ä¿æŒæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´æ¯ä¸ªç”Ÿæˆå¸§çš„ç‰¹å¾ä¸€è‡´æ€§ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šå¸§å†…å®¹å¯¹æŠ—ï¼ˆMCAï¼‰æŸå¤±ï¼Œä»¥æé«˜æ•´ä¸ªç”Ÿæˆè§†é¢‘çš„è¿åŠ¨åŠ¨åŠ›å­¦ã€‚è¯¥æ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„VDMiniåœ¨ä¸¤é¡¹é‡è¦çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡â€”â€”æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ä¸Šéå¸¸æœ‰æ•ˆã€‚åœ¨I2Væ–¹æ³•SF-Vã€T2Væ–¹æ³•T2V-Turbo-v2å’ŒT2Væ–¹æ³•HunyuanVideoä¸Šï¼Œæˆ‘ä»¬åˆ†åˆ«å®ç°äº†å¹³å‡2.5å€ã€1.4å€å’Œ1.25å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨å‡ ä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¿æŒäº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ï¼ŒåŒ…æ‹¬UCF101ã€VBench-T2Vå’ŒVBench-I2Vã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18375v3">PDF</a> ACM MM 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰åœ¨è®¡ç®—æˆæœ¬é«˜å’Œæ¨ç†é€Ÿåº¦æ…¢æ–¹é¢çš„ä¸»è¦æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†ä¸ªäººå†…å®¹å’Œè¿åŠ¨åŠ¨åŠ›å­¦çš„ä¿®å‰ªå’Œä¸€è‡´æ€§æŸå¤±ã€‚è§‚å¯Ÿåˆ°è¾ƒæ·±çš„VDMå±‚å¯¹äºä¿æŒè¿åŠ¨åŠ¨åŠ›å­¦è´¨é‡è‡³å…³é‡è¦ï¼Œè€Œè¾ƒæµ…çš„å±‚åˆ™æ›´ä¸“æ³¨äºä¸ªäººå†…å®¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»è¾ƒæµ…çš„å±‚ä¸­åˆ é™¤å†—ä½™å—ï¼ŒåŒæ—¶ä¿ç•™æ›´å¤šçš„æ·±å±‚ï¼Œä»è€Œäº§ç”Ÿäº†è½»é‡çº§çš„VDMå˜ä½“VDMiniã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸ªäººå†…å®¹å’Œè¿åŠ¨åŠ¨åŠ›å­¦ï¼ˆICMDï¼‰ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥åœ¨è¾ƒå¤§çš„VDMå’ŒVDMiniä¹‹é—´è·å¾—ç›¸å½“çš„ç”Ÿäº§æ€§èƒ½ã€‚é€šè¿‡ä¸ªä½“å†…å®¹è’¸é¦ï¼ˆICDï¼‰æŸå¤±ä¿æŒæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´æ¯ä¸ªç”Ÿæˆå¸§çš„ç‰¹å¾ä¸€è‡´æ€§ã€‚æ¥ä¸‹æ¥ï¼Œå¼•å…¥å¤šå¸§å†…å®¹å¯¹æŠ—ï¼ˆMCAï¼‰æŸå¤±ï¼Œä»¥å¢å¼ºæ•´ä¸ªç”Ÿæˆè§†é¢‘çš„è¿åŠ¨åŠ¨åŠ›å­¦ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜è´¨é‡è§†é¢‘ç”Ÿæˆçš„åŒæ—¶ï¼Œæ˜¾è‘—åŠ é€Ÿäº†æ¨ç†æ—¶é—´ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒVDMiniåœ¨æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ä¸¤ä¸ªé‡è¦çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­æ•ˆæœæ˜¾è‘—ï¼Œåœ¨I2Væ–¹æ³•SF-Vã€T2Væ–¹æ³•T2V-Turbo-v2å’ŒT2Væ–¹æ³•HunyuanVideoä¸Šåˆ†åˆ«å®ç°äº†å¹³å‡2.5å€ã€1.4å€å’Œ1.25å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨UCF101ã€VBench-T2Vå’ŒVBench-I2Vç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¿æŒäº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œæ…¢æ¨ç†æ—¶é—´çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†VDMiniï¼šä¸€ç§è½»é‡çº§çš„VDMå˜ä½“ï¼Œé€šè¿‡ä¿ç•™æ·±å±‚ç»“æ„å¹¶ä¿®å‰ªè¾ƒæµ…å±‚çš„å†—ä½™å—æ¥å®ç°ã€‚</li>
<li>å¼•å…¥äº†ä¸ªä½“å†…å®¹å’Œè¿åŠ¨åŠ¨åŠ›å­¦ï¼ˆICMDï¼‰ä¸€è‡´æ€§æŸå¤±ï¼Œç¡®ä¿æ¨¡å‹å‹ç¼©åæ€§èƒ½ç›¸å½“ã€‚</li>
<li>ä½¿ç”¨ä¸ªä½“å†…å®¹è’¸é¦ï¼ˆICDï¼‰æŸå¤±å’Œå¤šå¸§å†…å®¹å¯¹æŠ—ï¼ˆMCAï¼‰æŸå¤±æ¥å¢å¼ºè§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>VDMiniåœ¨æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—åŠ é€Ÿã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒVDMiniä¿æŒäº†é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>æ–¹æ³•åœ¨ä¿æŒè§†é¢‘è¿è´¯æ€§å’ŒåŠ¨æ€æ•ˆæœæ–¹é¢ç‰¹åˆ«æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bcba5122d76108ed691a8245c0e363f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39099762fc947c51230a54f797664409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30d2b9d1b66601b800572b268202e5ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-191d67e34a36365ada83c07d08399370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bcd9fe6a7e5d62e56b37d42c3f6d47e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6db1c5897da7f283087bd2e51ff304c7.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  X-SAM From Segment Anything to Any Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ce4d29d5056acc8ebd1f2d4bbcbb6ca1.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  MuGS Multi-Baseline Generalizable Gaussian Splatting Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
