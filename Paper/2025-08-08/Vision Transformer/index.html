<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  Visual Bias and Interpretability in Deep Learning for Dermatological   Image Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a3a06165e09e33733fc4d9c82c339559.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    47 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-08-æ›´æ–°"><a href="#2025-08-08-æ›´æ–°" class="headerlink" title="2025-08-08 æ›´æ–°"></a>2025-08-08 æ›´æ–°</h1><h2 id="Visual-Bias-and-Interpretability-in-Deep-Learning-for-Dermatological-Image-Analysis"><a href="#Visual-Bias-and-Interpretability-in-Deep-Learning-for-Dermatological-Image-Analysis" class="headerlink" title="Visual Bias and Interpretability in Deep Learning for Dermatological   Image Analysis"></a>Visual Bias and Interpretability in Deep Learning for Dermatological   Image Analysis</h2><p><strong>Authors:Enam Ahmed Taufik, Abdullah Khondoker, Antara Firoz Parsa, Seraj Al Mahmud Mostafa</strong></p>
<p>Accurate skin disease classification is a critical yet challenging task due to high inter-class similarity, intra-class variability, and complex lesion textures. While deep learning-based computer-aided diagnosis (CAD) systems have shown promise in automating dermatological assessments, their performance is highly dependent on image pre-processing and model architecture. This study proposes a deep learning framework for multi-class skin disease classification, systematically evaluating three image pre-processing techniques: standard RGB, CMY color space transformation, and Contrast Limited Adaptive Histogram Equalization (CLAHE). We benchmark the performance of pre-trained convolutional neural networks (DenseNet201, Efficient-NetB5) and transformer-based models (ViT, Swin Transformer, DinoV2 Large) using accuracy and F1-score as evaluation metrics. Results show that DinoV2 with RGB pre-processing achieves the highest accuracy (up to 93%) and F1-scores across all variants. Grad-CAM visualizations applied to RGB inputs further reveal precise lesion localization, enhancing interpretability. These findings underscore the importance of effective pre-processing and model choice in building robust and explainable CAD systems for dermatology. </p>
<blockquote>
<p>ç²¾ç¡®çš„çš®è‚¤ç–¾ç—…åˆ†ç±»æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå­˜åœ¨ç±»é—´ç›¸ä¼¼æ€§é«˜ã€ç±»å†…å˜åŒ–å¤§ä»¥åŠç—…å˜çº¹ç†å¤æ‚ç­‰é—®é¢˜ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿåœ¨è‡ªåŠ¨åŒ–çš®è‚¤ç§‘è¯„ä¼°æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½é«˜åº¦ä¾èµ–äºå›¾åƒé¢„å¤„ç†å’Œæ¨¡å‹æ¶æ„ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¤šç±»çš®è‚¤ç–¾ç—…åˆ†ç±»ï¼Œç³»ç»Ÿè¯„ä¼°ä¸‰ç§å›¾åƒé¢„å¤„ç†æŠ€æœ¯ï¼šæ ‡å‡†RGBã€CMYè‰²å½©ç©ºé—´è½¬æ¢å’Œå¯¹æ¯”åº¦æœ‰é™çš„è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼ˆCLAHEï¼‰ã€‚æˆ‘ä»¬ä»¥å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰å’ŒF1åˆ†æ•°ä½œä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œè¯„ä¼°äº†é¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œï¼ˆDenseNet201ã€Efficient-NetB5ï¼‰å’ŒåŸºäºtransformerçš„æ¨¡å‹ï¼ˆViTã€Swin Transformerã€DinoV2 Largeï¼‰çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨RGBé¢„å¤„ç†çš„DinoV2æ¨¡å‹åœ¨å‡†ç¡®ç‡å’ŒF1åˆ†æ•°æ–¹é¢å‡è¾¾åˆ°æœ€é«˜ï¼ˆé«˜è¾¾93%ï¼‰ã€‚åº”ç”¨äºRGBè¾“å…¥çš„Grad-CAMå¯è§†åŒ–è¿›ä¸€æ­¥æ­ç¤ºäº†ç²¾ç¡®çš„ç—…å˜å®šä½ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æœ‰æ•ˆé¢„å¤„ç†å’Œæ¨¡å‹é€‰æ‹©å¯¹äºæ„å»ºç¨³å¥ä¸”å¯è§£é‡Šçš„çš®è‚¤ç§‘CADç³»ç»Ÿçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04573v1">PDF</a> This paper has been accepted in the 4th IEEE International Conference   on Image Processing and Media Computing (ICIPMC) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ·±åº¦å­¦ä¹ åœ¨å¤šç±»çš®è‚¤ç—…åˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œæ¢è®¨äº†ä¸åŒçš„å›¾åƒé¢„å¤„ç†æ–¹æ³•å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨DinoV2æ¨¡å‹ä¸RGBé¢„å¤„ç†æŠ€æœ¯ç›¸ç»“åˆï¼Œå¯è¾¾åˆ°æœ€é«˜åˆ†ç±»å‡†ç¡®ç‡ï¼ˆé«˜è¾¾93%ï¼‰å’ŒF1åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒGrad-CAMå¯è§†åŒ–æŠ€æœ¯èƒ½ç²¾ç¡®æ˜¾ç¤ºç—…å˜éƒ¨ä½ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨çš®è‚¤ç—…åˆ†ç±»ä¸­é¢ä¸´é«˜ç±»é—´ç›¸ä¼¼åº¦ã€ç±»å†…å·®å¼‚å¤§å’Œç—…å˜çº¹ç†å¤æ‚ç­‰æŒ‘æˆ˜ã€‚</li>
<li>å›¾åƒé¢„å¤„ç†å’Œæ¨¡å‹æ¶æ„å¯¹æ·±åº¦å­¦ä¹ åœ¨çš®è‚¤ç—…åˆ†ç±»ä¸­çš„æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†å¤šç§å›¾åƒé¢„å¤„ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬æ ‡å‡†RGBã€CMYé¢œè‰²ç©ºé—´è½¬æ¢å’ŒCLAHEã€‚</li>
<li>é‡‡ç”¨äº†é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆDenseNet201ï¼ŒEfficient-NetB5ï¼‰å’ŒåŸºäºTransformerçš„æ¨¡å‹ï¼ˆViTï¼ŒSwin Transformerï¼ŒDinoV2 Largeï¼‰è¿›è¡Œå®éªŒã€‚</li>
<li>DinoV2æ¨¡å‹ç»“åˆRGBé¢„å¤„ç†æŠ€æœ¯è·å¾—æœ€é«˜åˆ†ç±»å‡†ç¡®ç‡ï¼ˆé«˜è¾¾93%ï¼‰å’ŒF1åˆ†æ•°ã€‚</li>
<li>Grad-CAMå¯è§†åŒ–æŠ€æœ¯èƒ½ç²¾ç¡®æ˜¾ç¤ºç—…å˜éƒ¨ä½ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æœ‰æ•ˆé¢„å¤„ç†å’Œæ¨¡å‹é€‰æ‹©å¯¹äºæ„å»ºç¨³å¥å’Œå¯è§£é‡Šçš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-898b881ab4cf44ec0e48ef80256c2ae4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3a06165e09e33733fc4d9c82c339559.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d2b2a7741d15b20a43af9ab898af8b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8249217769c9061c068ca74d74b55e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae78d32cd29f3ca9e545a9b7a804acd2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dual-Prompt-Learning-for-Adapting-Vision-Language-Models-to-Downstream-Image-Text-Retrieval"><a href="#Dual-Prompt-Learning-for-Adapting-Vision-Language-Models-to-Downstream-Image-Text-Retrieval" class="headerlink" title="Dual Prompt Learning for Adapting Vision-Language Models to Downstream   Image-Text Retrieval"></a>Dual Prompt Learning for Adapting Vision-Language Models to Downstream   Image-Text Retrieval</h2><p><strong>Authors:Yifan Wang, Tao Wang, Chenwei Tang, Caiyang Yu, Zhengqing Zang, Mengmi Zhang, Shudong Huang, Jiancheng Lv</strong></p>
<p>Recently, prompt learning has demonstrated remarkable success in adapting pre-trained Vision-Language Models (VLMs) to various downstream tasks such as image classification. However, its application to the downstream Image-Text Retrieval (ITR) task is more challenging. We find that the challenge lies in discriminating both fine-grained attributes and similar subcategories of the downstream data. To address this challenge, we propose Dual prompt Learning with Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learning framework to achieve precise image-text matching. The framework dynamically adjusts prompt vectors from both semantic and visual dimensions to improve the performance of CLIP on the downstream ITR task. Based on the prompt paradigm, DCAR jointly optimizes attribute and class features to enhance fine-grained representation learning. Specifically, (1) at the attribute level, it dynamically updates the weights of attribute descriptions based on text-image mutual information correlation; (2) at the category level, it introduces negative samples from multiple perspectives with category-matching weighting to learn subcategory distinctions. To validate our method, we construct the Fine-class Described Retrieval Dataset (FDRD), which serves as a challenging benchmark for ITR in downstream data domains. It covers over 1,500 downstream fine categories and 230,000 image-caption pairs with detailed attribute annotations. Extensive experiments on FDRD demonstrate that DCAR achieves state-of-the-art performance over existing baselines. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæç¤ºå­¦ä¹ åœ¨å°†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€‚åº”äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå…¶åœ¨ä¸‹æ¸¸çš„å›¾åƒæ–‡æœ¬æ£€ç´¢ï¼ˆITRï¼‰ä»»åŠ¡ä¸­çš„åº”ç”¨æ›´å…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼ŒæŒ‘æˆ˜åœ¨äºåŒºåˆ†ä¸‹æ¸¸æ•°æ®çš„ç²¾ç»†å±æ€§ä»¥åŠç›¸ä¼¼çš„å­ç±»åˆ«ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè”åˆç±»åˆ«å±æ€§å†æƒé‡çš„åŒé‡æç¤ºå­¦ä¹ ï¼ˆDCARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒæç¤ºå­¦ä¹ æ¡†æ¶ï¼Œå¯å®ç°ç²¾ç¡®çš„å›¾ç”Ÿæ–‡æœ¬åŒ¹é…ã€‚è¯¥æ¡†æ¶æ ¹æ®è¯­ä¹‰å’Œè§†è§‰ç»´åº¦åŠ¨æ€è°ƒæ•´æç¤ºå‘é‡ï¼Œä»¥æé«˜CLIPåœ¨ä¸‹æ¸¸ITRä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åŸºäºæç¤ºèŒƒå¼ï¼ŒDCARè”åˆä¼˜åŒ–å±æ€§å’Œç±»åˆ«ç‰¹å¾ï¼Œä»¥å¢å¼ºç²¾ç»†ç²’åº¦è¡¨ç¤ºå­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰åœ¨å±æ€§å±‚é¢ï¼Œå®ƒæ ¹æ®æ–‡æœ¬å›¾åƒäº’ä¿¡æ¯ç›¸å…³æ€§åŠ¨æ€æ›´æ–°å±æ€§æè¿°æƒé‡ï¼›ï¼ˆ2ï¼‰åœ¨ç±»åˆ«å±‚é¢ï¼Œå®ƒä»å¤šä¸ªè§’åº¦å¼•å…¥å¸¦ç±»åˆ«åŒ¹é…çš„è´Ÿæ ·æœ¬æ¥å­¦ä¹ å­ç±»åˆ«çš„åŒºåˆ«ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ç²¾ç»†åˆ†ç±»æè¿°æ£€ç´¢æ•°æ®é›†ï¼ˆFDRDï¼‰ï¼Œä½œä¸ºä¸‹æ¸¸æ•°æ®åŸŸçš„ITRæŒ‘æˆ˜åŸºå‡†æ•°æ®é›†ã€‚å®ƒæ¶µç›–äº†è¶…è¿‡1500ä¸ªä¸‹æ¸¸ç²¾ç»†ç±»åˆ«å’Œå¸¦æœ‰è¯¦ç»†å±æ€§æ³¨é‡Šçš„23ä¸‡ä¸ªå›¾åƒæ ‡é¢˜å¯¹ã€‚åœ¨FDRDä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDCARåœ¨ç°æœ‰åŸºçº¿æŠ€æœ¯ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04028v1">PDF</a> 10 pages, 7figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºåŒæç¤ºå­¦ä¹ æ¡†æ¶çš„è”åˆç±»åˆ«å±æ€§å†æƒé‡æ–¹æ³•ï¼ˆDCARï¼‰ï¼Œä»¥åº”å¯¹ä¸‹æ¸¸å›¾åƒæ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­çš„ç²¾ç»†åˆ†ç±»æŒ‘æˆ˜ã€‚DCARåŠ¨æ€è°ƒæ•´è¯­ä¹‰å’Œè§†è§‰ç»´åº¦çš„æç¤ºå‘é‡ï¼Œæé«˜CLIPåœ¨ä¸‹æ¸¸å›¾åƒæ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åŒæ—¶ï¼ŒDCARä¼˜åŒ–äº†å±æ€§çº§ç‰¹å¾åˆ†ç±»çº§ç‰¹å¾ï¼Œå¢å¼ºç²¾ç»†ç²’åº¦è¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡åŠ¨æ€æ›´æ–°å±æ€§æè¿°çš„æƒé‡å’Œå¼•å…¥å¤šè§†è§’çš„è´Ÿæ ·æœ¬åŠç±»åˆ«åŒ¹é…æƒé‡ï¼ŒDCARè§£å†³äº†ä¸‹æ¸¸æ•°æ®çš„ç²¾ç»†åˆ†ç±»å’Œç›¸ä¼¼å­ç±»åˆ«åŒºåˆ†é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æç¤ºå­¦ä¹ åœ¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”ä¸‹æ¸¸ä»»åŠ¡å¦‚å›¾åƒåˆ†ç±»ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸­æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>æŒ‘æˆ˜åœ¨äºåŒºåˆ†ä¸‹æ¸¸æ•°æ®çš„ç²¾ç»†ç²’åº¦å’Œç›¸ä¼¼å­ç±»åˆ«ã€‚</li>
<li>DCARæ˜¯ä¸€ç§æ–°é¢–çš„åŒæç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è¯­ä¹‰å’Œè§†è§‰ç»´åº¦çš„æç¤ºå‘é‡ï¼Œå®ç°ç²¾ç¡®å›¾åƒæ–‡æœ¬åŒ¹é…ï¼Œæé«˜CLIPåœ¨ä¸‹æ¸¸å›¾åƒæ–‡æœ¬æ£€ç´¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>DCARåŒæ—¶ä¼˜åŒ–å±æ€§çº§å’Œåˆ†ç±»çº§ç‰¹å¾ï¼Œä»¥å¢å¼ºç²¾ç»†ç²’åº¦è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>åœ¨å±æ€§å±‚é¢ï¼ŒDCARæ ¹æ®æ–‡æœ¬å›¾åƒäº’ä¿¡æ¯ç›¸å…³æ€§åŠ¨æ€æ›´æ–°å±æ€§æè¿°çš„æƒé‡ã€‚</li>
<li>åœ¨ç±»åˆ«å±‚é¢ï¼ŒDCARé€šè¿‡å¼•å…¥å¤šè§†è§’çš„è´Ÿæ ·æœ¬å’Œç±»åˆ«åŒ¹é…æƒé‡ï¼Œå­¦ä¹ å­ç±»åˆ«é—´çš„å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8c448955d844b8688ffd3083391fa911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d93ce4c03307e8dd510624942418aad5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88040e53ab4ca153777c24f0c348e33f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ffc88f126e19d843cf31b66afa6123b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25cc9f6aed08c2018fab4ceebf97ad2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcf3a74e34c66382074851f085bbaaaa.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Investigating-the-Impact-of-Large-Scale-Pre-training-on-Nutritional-Content-Estimation-from-2D-Images"><a href="#Investigating-the-Impact-of-Large-Scale-Pre-training-on-Nutritional-Content-Estimation-from-2D-Images" class="headerlink" title="Investigating the Impact of Large-Scale Pre-training on Nutritional   Content Estimation from 2D Images"></a>Investigating the Impact of Large-Scale Pre-training on Nutritional   Content Estimation from 2D Images</h2><p><strong>Authors:Michele Andrade, Guilherme A. L. Silva, ValÃ©ria Santos, Gladston Moreira, Eduardo Luz</strong></p>
<p>Estimating the nutritional content of food from images is a critical task with significant implications for health and dietary monitoring. This is challenging, especially when relying solely on 2D images, due to the variability in food presentation, lighting, and the inherent difficulty in inferring volume and mass without depth information. Furthermore, reproducibility in this domain is hampered by the reliance of state-of-the-art methods on proprietary datasets for large-scale pre-training. In this paper, we investigate the impact of large-scale pre-training datasets on the performance of deep learning models for nutritional estimation using only 2D images. We fine-tune and evaluate Vision Transformer (ViT) models pre-trained on two large public datasets, ImageNet and COYO, comparing their performance against baseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art method pre-trained on the proprietary JFT-300M dataset. We conduct extensive experiments on the Nutrition5k dataset, a large-scale collection of real-world food plates with high-precision nutritional annotations. Our evaluation using Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) reveals that models pre-trained on JFT-300M significantly outperform those pre-trained on public datasets. Unexpectedly, the model pre-trained on the massive COYO dataset performs worse than the model pre-trained on ImageNet for this specific regression task, refuting our initial hypothesis. Our analysis provides quantitative evidence highlighting the critical role of pre-training dataset characteristics, including scale, domain relevance, and curation quality, for effective transfer learning in 2D nutritional estimation. </p>
<blockquote>
<p>ä»å›¾åƒä¼°ç®—é£Ÿå“çš„è¥å…»æˆåˆ†æ˜¯å¥åº·ä¸é¥®é£Ÿç›‘æµ‹ä¸­å…·æœ‰é‡è¦æ„ä¹‰çš„ä»»åŠ¡ã€‚è¿™æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä»…ä¾èµ–2Då›¾åƒæ—¶ï¼Œç”±äºé£Ÿå“å±•ç¤ºã€å…‰ç…§çš„å·®å¼‚æ€§ä»¥åŠç¼ºä¹æ·±åº¦ä¿¡æ¯å¯¼è‡´æ¨æ–­ä½“ç§¯å’Œè´¨é‡å›ºæœ‰çš„å›°éš¾ã€‚æ­¤å¤–ï¼Œè¯¥é¢†åŸŸçš„å¯é‡å¤æ€§å—åˆ°äº†é˜»ç¢ï¼Œå› ä¸ºæœ€å…ˆè¿›çš„æ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡é¢„è®­ç»ƒçš„ä¸“æœ‰æ•°æ®é›†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†å¯¹ä»…ä½¿ç”¨2Då›¾åƒè¿›è¡Œè¥å…»ä¼°ç®—çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬å¾®è°ƒå¹¶è¯„ä¼°äº†åœ¨ImageNetå’ŒCOYOä¸¤ä¸ªå¤§å‹å…¬å…±æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ï¼Œå°†å…¶æ€§èƒ½ä¸åŸºçº¿CNNæ¨¡å‹ï¼ˆInceptionV2å’ŒResNet-50ï¼‰ä»¥åŠé¢„è®­ç»ƒåœ¨ä¸“æœ‰JFT-300Mæ•°æ®é›†ä¸Šçš„æœ€æ–°æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨Nutrition5kæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯¥æ•°æ®é›†æ˜¯åŒ…å«é«˜ç²¾åº¦è¥å…»æ³¨é‡Šçš„å¤§è§„æ¨¡ç°å®ä¸–ç•Œé£Ÿå“ç›˜ç‚¹é›†åˆã€‚æˆ‘ä»¬ä½¿ç”¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å’Œå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼ˆMAE%ï¼‰è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºé¢„è®­ç»ƒåœ¨JFT-300Mä¸Šçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºåœ¨å…¬å…±æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚å‡ºä¹æ„æ–™çš„æ˜¯ï¼Œåœ¨å¤§é‡COYOæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨æ­¤ç‰¹å®šå›å½’ä»»åŠ¡ä¸Šçš„è¡¨ç°æ¯”é¢„è®­ç»ƒåœ¨ImageNetä¸Šçš„æ¨¡å‹å·®ï¼Œè¿™åé©³äº†æˆ‘ä»¬æœ€åˆçš„å‡è®¾ã€‚æˆ‘ä»¬çš„åˆ†ææä¾›äº†å®šé‡è¯æ®ï¼Œçªå‡ºæ˜¾ç¤ºäº†é¢„è®­ç»ƒæ•°æ®é›†ç‰¹æ€§ï¼ˆåŒ…æ‹¬è§„æ¨¡ã€é¢†åŸŸç›¸å…³æ€§å’Œç¼–çº‚è´¨é‡ï¼‰åœ¨äºŒç»´è¥å…»ä¼°ç®—ä¸­çš„æœ‰æ•ˆè¿ç§»å­¦ä¹ ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03996v1">PDF</a> 12 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†å¯¹ä»…ä½¿ç”¨2Då›¾åƒè¿›è¡Œè¥å…»ä¼°ç®—çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚é€šè¿‡å¾®è°ƒå¹¶åœ¨Nutrition5kæ•°æ®é›†ä¸Šè¯„ä¼°é¢„è®­ç»ƒäºImageNetå’ŒCOYOçš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹ï¼Œä¸åŸºå‡†CNNæ¨¡å‹ï¼ˆInceptionV2å’ŒResNet-50ï¼‰ä»¥åŠé¢„è®­ç»ƒäºä¸“æœ‰JFT-300Mæ•°æ®é›†çš„æœ€å…ˆè¿›æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒäºJFT-300Mçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºé¢„è®­ç»ƒäºå…¬å¼€æ•°æ®é›†çš„æ¨¡å‹ã€‚æ„å¤–çš„æ˜¯ï¼Œé¢„è®­ç»ƒäºå·¨å¤§COYOæ•°æ®é›†çš„æ¨¡å‹åœ¨æ­¤ç‰¹å®šå›å½’ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸å¦‚é¢„è®­ç»ƒäºImageNetçš„æ¨¡å‹ï¼Œè¿™åé©³äº†æˆ‘ä»¬çš„åˆæ­¥å‡è®¾ã€‚åˆ†ææä¾›äº†å®šé‡è¯æ®ï¼Œå¼ºè°ƒé¢„è®­ç»ƒæ•°æ®é›†ç‰¹æ€§ï¼ˆåŒ…æ‹¬è§„æ¨¡ã€é¢†åŸŸç›¸å…³æ€§å’Œç­›é€‰è´¨é‡ï¼‰åœ¨2Dè¥å…»ä¼°ç®—ä¸­çš„æœ‰æ•ˆè¿ç§»å­¦ä¹ ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¼°ç®—é£Ÿç‰©è¥å…»æˆåˆ†çš„å›¾åƒè¯†åˆ«æ˜¯ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œå¯¹å¥åº·å’Œé¥®é£Ÿç›‘æµ‹æœ‰é‡å¤§å½±å“ã€‚</li>
<li>ä»…ä½¿ç”¨2Då›¾åƒè¿›è¡Œæ­¤ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¼ºä¹æ·±åº¦ä¿¡æ¯ï¼Œä¸”é£Ÿç‰©å‘ˆç°ã€å…‰ç…§å˜åŒ–å¤§ã€‚</li>
<li>ç°æœ‰å…ˆè¿›æ–¹æ³•ä¾èµ–äºä¸“æœ‰å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œé˜»ç¢äº†è¯¥é¢†åŸŸçš„å¯é‡å¤æ€§ã€‚</li>
<li>ç ”ç©¶äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†å¯¹è¥å…»ä¼°ç®—æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>é¢„è®­ç»ƒäºJFT-300Mæ•°æ®é›†çš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºé¢„è®­ç»ƒäºå…¬å¼€æ•°æ®é›†çš„æ¨¡å‹ã€‚</li>
<li>COYOæ•°æ®é›†åœ¨ç‰¹å®šå›å½’ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸å¦‚ImageNetï¼Œè¿™åé©³äº†å…³äºæ•°æ®é›†è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´ç›´æ¥å…³ç³»çš„å‡è®¾ã€‚</li>
<li>åˆ†æå¼ºè°ƒäº†é¢„è®­ç»ƒæ•°æ®é›†ç‰¹æ€§ï¼ˆè§„æ¨¡ã€é¢†åŸŸç›¸å…³æ€§å’Œç­›é€‰è´¨é‡ï¼‰åœ¨è¿ç§»å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-471ff06c146d74e7bab9e815feb26ca6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae5c5926e15005c3e7ab5d92565821e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e406cfab08800a8ef480c7fcf190fed3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45b2e201361ae2b8b5eacbb0dc35c8cc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RAVID-Retrieval-Augmented-Visual-Detection-A-Knowledge-Driven-Approach-for-AI-Generated-Image-Identification"><a href="#RAVID-Retrieval-Augmented-Visual-Detection-A-Knowledge-Driven-Approach-for-AI-Generated-Image-Identification" class="headerlink" title="RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach   for AI-Generated Image Identification"></a>RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach   for AI-Generated Image Identification</h2><p><strong>Authors:Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, Abdenour Hadid</strong></p>
<p>In this paper, we introduce RAVID, the first framework for AI-generated image detection that leverages visual retrieval-augmented generation (RAG). While RAG methods have shown promise in mitigating factual inaccuracies in foundation models, they have primarily focused on text, leaving visual knowledge underexplored. Meanwhile, existing detection methods, which struggle with generalization and robustness, often rely on low-level artifacts and model-specific features, limiting their adaptability. To address this, RAVID dynamically retrieves relevant images to enhance detection. Our approach utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with category-related prompts to improve representation learning. We further integrate a vision-language model (VLM) to fuse retrieved images with the query, enriching the input and improving accuracy. Given a query image, RAVID generates an embedding using RAVID CLIP, retrieves the most relevant images from a database, and combines these with the query image to form an enriched input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the UniversalFakeDetect benchmark, which covers 19 generative models, show that RAVID achieves state-of-the-art performance with an average accuracy of 93.85%. RAVID also outperforms traditional methods in terms of robustness, maintaining high accuracy even under image degradations such as Gaussian blur and JPEG compression. Specifically, RAVID achieves an average accuracy of 80.27% under degradation conditions, compared to 63.44% for the state-of-the-art model C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG compression scenarios. The code will be publicly available upon acceptance. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†RAVIDï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨è§†è§‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¿›è¡ŒAIç”Ÿæˆå›¾åƒæ£€æµ‹çš„æ¡†æ¶ã€‚è™½ç„¶RAGæ–¹æ³•åœ¨ç¼“è§£åŸºç¡€æ¨¡å‹ä¸­çš„äº‹å®é”™è¯¯æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬ä¸Šï¼Œå¿½è§†äº†è§†è§‰çŸ¥è¯†çš„æ¢ç´¢ã€‚ä¸æ­¤åŒæ—¶ï¼Œç°æœ‰çš„æ£€æµ‹æ–¹æ³•åœ¨é€šç”¨æ€§å’Œç¨³å¥æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå®ƒä»¬å¾€å¾€ä¾èµ–äºä½çº§çš„ä¼ªç‰¹å¾å’Œæ¨¡å‹ç‰¹å®šçš„ç‰¹å¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒRAVIDåŠ¨æ€æ£€ç´¢ç›¸å…³å›¾åƒä»¥å¢å¼ºæ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç»è¿‡å¾®è°ƒCLIPå›¾åƒç¼–ç å™¨RAVID CLIPï¼Œå¹¶é€šè¿‡ä¸ç±»åˆ«ç›¸å…³çš„æç¤ºæ¥æ”¹å–„è¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é›†æˆäº†ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥èåˆæ£€ç´¢åˆ°çš„å›¾åƒä¸æŸ¥è¯¢ï¼Œä¸°å¯Œè¾“å…¥å¹¶æé«˜å‡†ç¡®æ€§ã€‚å¯¹äºç»™å®šçš„æŸ¥è¯¢å›¾åƒï¼ŒRAVIDä½¿ç”¨RAVID CLIPç”ŸæˆåµŒå…¥ï¼Œä»æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„å›¾åƒï¼Œå¹¶å°†è¿™äº›å›¾åƒä¸æŸ¥è¯¢å›¾åƒç»“åˆï¼Œå½¢æˆä¸€ä¸ªä¸°å¯Œçš„è¾“å…¥ç”¨äºVLMï¼ˆä¾‹å¦‚Qwen-VLæˆ–Openflamingoï¼‰ã€‚åœ¨æ¶µç›–1ç¬¬åä¹ç« ç”Ÿæˆæ¨¡å‹çš„UniversalFakeDetectåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRAVIDè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º93.85%ã€‚RAVIDåœ¨ç¨³å¥æ€§æ–¹é¢ä¹Ÿä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå³ä½¿åœ¨å›¾åƒé€€åŒ–ï¼ˆå¦‚é«˜æ–¯æ¨¡ç³Šå’ŒJPEGå‹ç¼©ï¼‰çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒé«˜å‡†ç¡®ç‡ã€‚å…·ä½“è€Œè¨€ï¼ŒRAVIDåœ¨é€€åŒ–æ¡ä»¶ä¸‹çš„å¹³å‡å‡†ç¡®ç‡ä¸º80.27%ï¼Œè€Œç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹C2P-CLIPçš„å‡†ç¡®ç‡ä¸º63.44%ï¼Œåœ¨Gaussianæ¨¡ç³Šå’ŒJPEGå‹ç¼©åœºæ™¯ä¸­å‡è¡¨ç°å‡ºæŒç»­ä¸€è‡´çš„æ”¹è¿›ã€‚ä»£ç åœ¨æ¥å—åå°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03967v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†RAVIDï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨è§†è§‰æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼ˆRAGï¼‰è¿›è¡ŒAIå›¾åƒæ£€æµ‹ã€‚å®ƒé€šè¿‡ç»“åˆç»†è°ƒçš„CLIPå›¾åƒç¼–ç å™¨ä¸ç±»åˆ«ç›¸å…³çš„æç¤ºæ¥æ”¹å–„è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶é€šè¿‡é›†æˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥ä¸°å¯Œæ£€ç´¢å›¾åƒä¸æŸ¥è¯¢å†…å®¹ï¼Œä»è€Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚åœ¨UniversalFakeDetectåŸºå‡†æµ‹è¯•ä¸Šï¼ŒRAVIDå®ç°äº†å¹³å‡å‡†ç¡®ç‡é«˜è¾¾93.85%ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œåœ¨å„ç§å›¾åƒå¤±çœŸæ¡ä»¶ä¸‹ï¼ŒRAVIDçš„è¡¨ç°ä¹Ÿæ¯”ç°æœ‰æ–¹æ³•æ›´åŠ ä¼˜ç§€ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜æä¾›ä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å°†åœ¨æ¥å—åå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAVIDæ˜¯é¦–ä¸ªåˆ©ç”¨è§†è§‰æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼ˆRAGï¼‰è¿›è¡ŒAIå›¾åƒæ£€æµ‹çš„æ¡†æ¶ã€‚</li>
<li>RAGæŠ€æœ¯ç”¨äºå‡è½»åŸºç¡€æ¨¡å‹ä¸­çš„äº‹å®ä¸å‡†ç¡®é—®é¢˜ï¼Œå¹¶é¦–æ¬¡å°†è¯¥æ–¹æ³•åº”ç”¨äºå›¾åƒæ£€æµ‹é¢†åŸŸã€‚</li>
<li>RAVIDé€šè¿‡ç»“åˆCLIPå›¾åƒç¼–ç å™¨ä¸ç±»åˆ«ç›¸å…³çš„æç¤ºæ¥æ”¹å–„è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>RAVIDé›†æˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå°†æ£€ç´¢åˆ°çš„å›¾åƒä¸æŸ¥è¯¢ç›¸ç»“åˆï¼Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨UniversalFakeDetectåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAVIDå®ç°äº†é«˜è¾¾93.85%çš„å¹³å‡å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½å’Œç¨³å¥æ€§ã€‚</li>
<li>RAVIDåœ¨å„ç§å›¾åƒå¤±çœŸæ¡ä»¶ä¸‹è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¾‹å¦‚é«˜æ–¯æ¨¡ç³Šå’ŒJPEGå‹ç¼©ã€‚ç›¸æ¯”å…¶ä»–æ–¹æ³•å¦‚C2P-CLIPæ¨¡å‹ï¼Œå…·æœ‰æ›´å¥½çš„è¡¨ç°åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-227a5b213661ed76e9738bdb2dc965e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b399d4e24279672db57d09159a9e5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f730abfed9cfd215e2eb95282ffe235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c36767b25d84e886b2f5ad02c522859.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Closed-Circuit-Television-Data-as-an-Emergent-Data-Source-for-Urban-Rail-Platform-Crowding-Estimation"><a href="#Closed-Circuit-Television-Data-as-an-Emergent-Data-Source-for-Urban-Rail-Platform-Crowding-Estimation" class="headerlink" title="Closed-Circuit Television Data as an Emergent Data Source for Urban Rail   Platform Crowding Estimation"></a>Closed-Circuit Television Data as an Emergent Data Source for Urban Rail   Platform Crowding Estimation</h2><p><strong>Authors:Riccardo Fiorista, Awad Abdelhalim, Anson F. Stewart, Gabriel L. Pincus, Ian Thistle, Jinhua Zhao</strong></p>
<p>Accurately estimating urban rail platform occupancy can enhance transit agenciesâ€™ ability to make informed operational decisions, thereby improving safety, operational efficiency, and customer experience, particularly in the context of crowding. However, sensing real-time crowding remains challenging and often depends on indirect proxies such as automatic fare collection data or staff observations. Recently, Closed-Circuit Television (CCTV) footage has emerged as a promising data source with the potential to yield accurate, real-time occupancy estimates. The presented study investigates this potential by comparing three state-of-the-art computer vision approaches for extracting crowd-related features from platform CCTV imagery: (a) object detection and counting using YOLOv11, RT-DETRv2, and APGCC; (b) crowd-level classification via a custom-trained Vision Transformer, Crowd-ViT; and (c) semantic segmentation using DeepLabV3. Additionally, we present a novel, highly efficient linear-optimization-based approach to extract counts from the generated segmentation maps while accounting for image object depth and, thus, for passenger dispersion along a platform. Tested on a privacy-preserving dataset created in collaboration with the Washington Metropolitan Area Transit Authority (WMATA) that encompasses more than 600 hours of video material, our results demonstrate that computer vision approaches can provide substantive value for crowd estimation. This work demonstrates that CCTV image data, independent of other data sources available to a transit agency, can enable more precise real-time crowding estimation and, eventually, timely operational responses for platform crowding mitigation. </p>
<blockquote>
<p>å‡†ç¡®ä¼°è®¡åŸå¸‚é“è·¯æœˆå°ä¹˜å®¢æ•°é‡å¯ä»¥æå‡äº¤é€šæœºæ„çš„å†³ç­–èƒ½åŠ›ï¼Œä»è€Œæé«˜å®‰å…¨æ€§ã€æ“ä½œæ•ˆç‡å’Œå®¢æˆ·ä½“éªŒï¼Œç‰¹åˆ«æ˜¯åœ¨æ‹¥æŒ¤çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œæ„ŸçŸ¥å®æ—¶æ‹¥æŒ¤ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œé€šå¸¸ä¾èµ–äºè‡ªåŠ¨å”®ç¥¨æ•°æ®æˆ–å‘˜å·¥è§‚å¯Ÿç­‰é—´æ¥ä»£ç†ã€‚æœ€è¿‘ï¼Œé—­è·¯ç”µè§†ï¼ˆCCTVï¼‰å½±åƒä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ•°æ®æºï¼Œå…·æœ‰äº§ç”Ÿå‡†ç¡®ã€å®æ—¶å ç”¨ä¼°è®¡çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶é€šè¿‡æ¯”è¾ƒä¸‰ç§æœ€å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰æ–¹æ³•æ¥æ¢ç©¶è¿™ä¸€æ½œåŠ›ï¼Œè¿™äº›æ–¹æ³•å¯ä»æœˆå°CCTVå½±åƒä¸­æå–äººç¾¤ç›¸å…³ç‰¹å¾ï¼šï¼ˆaï¼‰ä½¿ç”¨YOLOv11ã€RT-DETRv2å’ŒAPGCCè¿›è¡Œç›®æ ‡æ£€æµ‹å’Œè®¡æ•°ï¼›ï¼ˆbï¼‰é€šè¿‡è‡ªå®šä¹‰è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨Crowd-ViTè¿›è¡Œäººç¾¤çº§åˆ«åˆ†ç±»ï¼›ï¼ˆcï¼‰ä½¿ç”¨DeepLabV3è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–ã€é«˜æ•ˆåŸºäºçº¿æ€§ä¼˜åŒ–çš„æ–¹æ³•ï¼Œä»ç”Ÿæˆçš„åˆ†å‰²å›¾ä¸­æå–è®¡æ•°ï¼ŒåŒæ—¶è€ƒè™‘å›¾åƒå¯¹è±¡æ·±åº¦å’Œæœˆå°ä¸Šçš„ä¹˜å®¢åˆ†æ•£æƒ…å†µã€‚åœ¨ä¸åç››é¡¿åœ°åŒºäº¤é€šå±€ï¼ˆWMATAï¼‰åˆä½œåˆ›å»ºçš„éšç§ä¿æŠ¤æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡600å°æ—¶çš„è§†é¢‘ææ–™ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè®¡ç®—æœºè§†è§‰æ–¹æ³•åœ¨äººç¾¤ä¼°è®¡ä¸­å…·æœ‰å®è´¨æ€§ä»·å€¼ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œç‹¬ç«‹äºäº¤é€šæœºæ„å¯ç”¨çš„å…¶ä»–æ•°æ®æºä¹‹å¤–çš„CCTVå›¾åƒæ•°æ®ï¼Œå¯ä»¥å®ç°æ›´ç²¾ç¡®çš„å®æ—¶æ‹¥æŒ¤ä¼°è®¡ï¼Œå¹¶æœ€ç»ˆå¯¹æœˆå°æ‹¥æŒ¤ç¼“è§£åšå‡ºåŠæ—¶çš„è¿è¥å“åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03749v1">PDF</a> 26 pages, 17 figures, 4 tables</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬ç ”ç©¶åˆ©ç”¨è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼Œé€šè¿‡é—­è·¯ç”µè§†ï¼ˆCCTVï¼‰å½±åƒåˆ†æï¼Œå¯¹åŸå¸‚é“è·¯å¹³å°ä¹˜å®¢æ‹¥æŒ¤æƒ…å†µè¿›è¡Œå®æ—¶å‡†ç¡®ä¼°ç®—ã€‚ç ”ç©¶æ¯”è¾ƒäº†ä¸‰ç§å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰æ–¹æ³•ï¼ŒåŒ…æ‹¬ç›®æ ‡æ£€æµ‹ä¸è®¡æ•°ã€äººç¾¤çº§åˆ«åˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ï¼Œå¹¶æå‡ºä¸€ç§æ–°å‹çº¿æ€§ä¼˜åŒ–æ–¹æ³•ä»¥ä»åˆ†å‰²åœ°å›¾ä¸­æå–è®¡æ•°ï¼ŒåŒæ—¶è€ƒè™‘å›¾åƒå¯¹è±¡æ·±åº¦åŠä¹˜å®¢åœ¨å¹³å°ä¸Šçš„åˆ†æ•£æƒ…å†µã€‚åœ¨éšç§ä¿æŠ¤çš„æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œç»“æœè¡¨æ˜è®¡ç®—æœºè§†è§‰æ–¹æ³•èƒ½ä¸ºäººç¾¤ä¼°ç®—æä¾›å®è´¨æ€§ä»·å€¼ï¼ŒCCTVå›¾åƒæ•°æ®å¯ç‹¬ç«‹äºå…¶ä»–æ•°æ®æºï¼Œä¸ºäº¤é€šæœºæ„æä¾›æ›´ç²¾ç¡®çš„å®æ—¶æ‹¥æŒ¤ä¼°ç®—å’ŒåŠæ—¶çš„å¹³å°æ‹¥æŒ¤ç¼“è§£æ“ä½œå“åº”ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åˆ©ç”¨CCTVå½±åƒè¿›è¡ŒåŸå¸‚é“è·¯å¹³å°å ç”¨æƒ…å†µå®æ—¶ä¼°ç®—å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>æ¯”è¾ƒäº†ä¸‰ç§å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰æ–¹æ³•ï¼šYOLOv11ã€RT-DETRv2ã€APGCCè¿›è¡Œç›®æ ‡æ£€æµ‹ä¸è®¡æ•°ï¼ŒCrowd-ViTè¿›è¡Œäººç¾¤çº§åˆ«åˆ†ç±»ä»¥åŠDeepLabV3è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çº¿æ€§ä¼˜åŒ–æ–¹æ³•ï¼Œå¯ä»åˆ†å‰²åœ°å›¾ä¸­æå–è®¡æ•°ï¼Œå¹¶è€ƒè™‘å›¾åƒå¯¹è±¡çš„æ·±åº¦åŠä¹˜å®¢åˆ†æ•£æƒ…å†µã€‚</li>
<li>åœ¨éšç§ä¿æŠ¤æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¯æ˜è®¡ç®—æœºè§†è§‰æ–¹æ³•åœ¨äººç¾¤ä¼°ç®—æ–¹é¢çš„ä»·å€¼ã€‚</li>
<li>CCTVå›¾åƒæ•°æ®å¯ç‹¬ç«‹äºå…¶ä»–æ•°æ®æºï¼Œæä¾›ç²¾ç¡®çš„å®æ—¶æ‹¥æŒ¤ä¼°ç®—ã€‚</li>
<li>è¯¥æŠ€æœ¯æœ‰åŠ©äºæé«˜äº¤é€šæœºæ„çš„æ“ä½œæ•ˆç‡ã€å®‰å…¨æ€§å’Œå®¢æˆ·ä½“éªŒï¼Œå°¤å…¶åœ¨æ‹¥æŒ¤æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6c26446c28aa6afb9d89a41c0b40844.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f95d28f2e5af46036a8408a76350b297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf1ccdec8c7b55cc98f80e97ad4f22d5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a81b4fbd16423cc1754c72123aa4283.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48d100729e7890ef78634950d5eb141e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CoPS-Conditional-Prompt-Synthesis-for-Zero-Shot-Anomaly-Detection"><a href="#CoPS-Conditional-Prompt-Synthesis-for-Zero-Shot-Anomaly-Detection" class="headerlink" title="CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection"></a>CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection</h2><p><strong>Authors:Qiyu Chen, Zhen Qu, Wei Luo, Haiming Yao, Yunkang Cao, Yuxin Jiang, Yinan Duan, Huiyuan Luo, Chengkan Lv, Zhengtao Zhang</strong></p>
<p>Recently, large pre-trained vision-language models have shown remarkable performance in zero-shot anomaly detection (ZSAD). With fine-tuning on a single auxiliary dataset, the model enables cross-category anomaly detection on diverse datasets covering industrial defects and medical lesions. Compared to manually designed prompts, prompt learning eliminates the need for expert knowledge and trial-and-error. However, it still faces the following challenges: (i) static learnable tokens struggle to capture the continuous and diverse patterns of normal and anomalous states, limiting generalization to unseen categories; (ii) fixed textual labels provide overly sparse category information, making the model prone to overfitting to a specific semantic subspace. To address these issues, we propose Conditional Prompt Synthesis (CoPS), a novel framework that synthesizes dynamic prompts conditioned on visual features to enhance ZSAD performance. Specifically, we extract representative normal and anomaly prototypes from fine-grained patch features and explicitly inject them into prompts, enabling adaptive state modeling. Given the sparsity of class labels, we leverage a variational autoencoder to model semantic image features and implicitly fuse varied class tokens into prompts. Additionally, integrated with our spatially-aware alignment mechanism, extensive experiments demonstrate that CoPS surpasses state-of-the-art methods by 2.5% AUROC in both classification and segmentation across 13 industrial and medical datasets. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/cqylunlun/CoPS">https://github.com/cqylunlun/CoPS</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚é€šè¿‡åœ¨ä¸€ä¸ªè¾…åŠ©æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ¶µç›–å·¥ä¸šç¼ºé™·å’ŒåŒ»ç–—ç—…å˜çš„å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè·¨ç±»åˆ«å¼‚å¸¸æ£€æµ‹ã€‚ä¸æ‰‹åŠ¨è®¾è®¡çš„æç¤ºç›¸æ¯”ï¼Œæç¤ºå­¦ä¹ æ— éœ€ä¸“å®¶çŸ¥è¯†å’Œè¯•é”™ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼šï¼ˆiï¼‰é™æ€çš„å¯å­¦ä¹ ä»¤ç‰Œåœ¨æ•è·æ­£å¸¸å’Œå¼‚å¸¸çŠ¶æ€çš„è¿ç»­å’Œå¤šæ ·æ¨¡å¼æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æœªè§ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼›ï¼ˆiiï¼‰å›ºå®šçš„æ–‡æœ¬æ ‡ç­¾æä¾›äº†è¿‡äºç¨€ç–çš„ç±»åˆ«ä¿¡æ¯ï¼Œä½¿æ¨¡å‹å®¹æ˜“è¿‡åº¦é€‚åº”ç‰¹å®šçš„è¯­ä¹‰å­ç©ºé—´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¡ä»¶æç¤ºåˆæˆï¼ˆCoPSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒæ ¹æ®è§†è§‰ç‰¹å¾åˆæˆåŠ¨æ€æç¤ºï¼Œä»¥æé«˜ZSADçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»ç²¾ç»†çš„è¡¥ä¸ç‰¹å¾ä¸­æå–ä»£è¡¨æ€§çš„æ­£å¸¸å’Œå¼‚å¸¸åŸå‹ï¼Œå¹¶å°†å…¶æ˜¾å¼åœ°æ³¨å…¥æç¤ºä¸­ï¼Œä»¥å®ç°è‡ªé€‚åº”çŠ¶æ€å»ºæ¨¡ã€‚é‰´äºç±»åˆ«æ ‡ç­¾çš„ç¨€ç–æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å¯¹è¯­ä¹‰å›¾åƒç‰¹å¾è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°†å„ç§ç±»åˆ«ä»¤ç‰Œéšå¼åœ°èåˆåˆ°æç¤ºä¸­ã€‚æ­¤å¤–ï¼Œç»“åˆæˆ‘ä»¬çš„ç©ºé—´æ„ŸçŸ¥å¯¹é½æœºåˆ¶ï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼ŒCoPSåœ¨13ä¸ªå·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†çš„åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œè¶…å‡ºæœ€æ–°æ–¹æ³•2.5%çš„AUROCã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/cqylunlun/CoPS">https://github.com/cqylunlun/CoPS</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03447v1">PDF</a> 19 pages, 33 figures, 14 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ä¸­ï¼Œå¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„è¡¨ç°åŠé¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°å‹æ¡†æ¶â€”â€”æ¡ä»¶æç¤ºåˆæˆï¼ˆCoPSï¼‰ï¼Œé€šè¿‡åŠ¨æ€æç¤ºåˆæˆå¢å¼ºZSADæ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒCoPSåœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œåœ¨13ä¸ªå·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„å¹³å‡å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰æé«˜2.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä¸­è¡¨ç°å“è¶Šï¼Œèƒ½é€šè¿‡å¾®è°ƒå•ä¸€è¾…åŠ©æ•°æ®é›†å®ç°è·¨ç±»åˆ«å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>ä¼ ç»Ÿæç¤ºå­¦ä¹ éœ€ä¾èµ–ä¸“å®¶çŸ¥è¯†å’Œåå¤è¯•éªŒï¼Œè€ŒCoPSæ¡†æ¶é€šè¿‡åŠ¨æ€æç¤ºåˆæˆå¢å¼ºæ€§èƒ½ã€‚</li>
<li>é™æ€å­¦ä¹ ä»¤ç‰Œéš¾ä»¥æ•æ‰æ­£å¸¸å’Œå¼‚å¸¸çŠ¶æ€çš„è¿ç»­å’Œå¤šæ ·æ¨¡å¼ï¼Œé™åˆ¶äº†å…¶åœ¨æœªè§ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å›ºå®šæ–‡æœ¬æ ‡ç­¾æä¾›çš„ç±»åˆ«ä¿¡æ¯è¿‡äºç¨€ç–ï¼Œä½¿æ¨¡å‹æ˜“äºè¿‡åº¦é€‚åº”ç‰¹å®šè¯­ä¹‰å­ç©ºé—´ã€‚</li>
<li>CoPSä»ç»†ç²’åº¦è¡¥ä¸ç‰¹å¾ä¸­æå–æ­£å¸¸å’Œå¼‚å¸¸åŸå‹ï¼Œå¹¶æ˜¾å¼æ³¨å…¥æç¤ºï¼Œå®ç°è‡ªé€‚åº”çŠ¶æ€å»ºæ¨¡ã€‚</li>
<li>é’ˆå¯¹ç±»åˆ«æ ‡ç­¾çš„ç¨€ç–æ€§ï¼ŒCoPSåˆ©ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å¯¹è¯­ä¹‰å›¾åƒç‰¹å¾è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°†å„ç§ç±»åˆ«ä»¤ç‰Œéšå¼èåˆåˆ°æç¤ºä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a26fde243f1f26545ed9c498e56405d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd0dcb3916dc9427dd511aba48e89a00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5fdf8c729ab1502be58086df7b9e0c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e8d4b101ae66c87f2ed45f24742aa0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4421a675906d5e0dac6fca91bb35c99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac8c2c57feee2fe4b53092567aa41fa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DeepAP-Deep-Learning-based-Aperture-Photometry-Feasibility-Assessment-and-Aperture-Size-Prediction"><a href="#DeepAP-Deep-Learning-based-Aperture-Photometry-Feasibility-Assessment-and-Aperture-Size-Prediction" class="headerlink" title="DeepAP: Deep Learning-based Aperture Photometry Feasibility Assessment   and Aperture Size Prediction"></a>DeepAP: Deep Learning-based Aperture Photometry Feasibility Assessment   and Aperture Size Prediction</h2><p><strong>Authors:Zheng-Jun Du, Qing-Quan Li, Yi-Cheng Rui, Yu-Li Liu, Yu-Ting Wu, Dong Li, Bing-Feng Seng, Yi-Fan Xuan, Fa-Bo Feng</strong></p>
<p>Aperture photometry is a fundamental technique widely used to obtain high-precision light curves in optical survey projects like Tianyu. However, its effectiveness is limited in crowded fields, and the choice of aperture size critically impacts photometric precision. To address these challenges, we propose DeepAP, an efficient and accurate two-stage deep learning framework for aperture photometry. Specifically, for a given source, we first train a Vision Transformer (ViT) model to assess its feasibility of aperture photometry. We then train the Residual Neural Network (ResNet) to predict its optimal aperture size. For aperture photometry feasibility assessment, the ViT model yields an ROC AUC value of 0.96, and achieves a precision of 0.974, a recall of 0.930, and an F1 score of 0.952 on the test set. For aperture size prediction, the ResNet model effectively mitigates biases inherent in classical growth curve methods by adaptively selecting apertures appropriate for sources of varying brightness, thereby enhancing the signal-to-noise ratio (SNR) across a wide range of targets. Meanwhile, some samples in the test set have a higher SNR than those obtained by exhaustive aperture size enumeration because of the finer granularity of aperture size estimation. By integrating ResNet with the ViT network, the DeepAP framework achieves a median total processing time of 18 milliseconds for a batch of 10 images, representing a speed-up of approximately 59000 times compared to exhaustive aperture size enumeration. This work paves the way for the automatic application of aperture photometry in future high-precision surveys such as Tianyu and LSST. The source code and model are available at <a target="_blank" rel="noopener" href="https://github.com/ruiyicheng/DeepAP">https://github.com/ruiyicheng/DeepAP</a>. </p>
<blockquote>
<p>å­”å¾„å…‰åº¦æ³•æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºå…‰å­¦è§‚æµ‹é¡¹ç›®ï¼ˆå¦‚å¤©å®‡ï¼‰çš„é«˜ç²¾åº¦å…‰åº¦æ›²çº¿è·å–çš„åŸºæœ¬æŠ€æœ¯ã€‚ç„¶è€Œï¼Œåœ¨æ‹¥æŒ¤çš„é¢†åŸŸé‡Œï¼Œå®ƒçš„æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ï¼Œå­”å¾„å¤§å°çš„é€‰æ‹©å¯¹å…‰åº¦ç²¾åº¦æœ‰é‡è¦å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepAPï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„å­”å¾„å…‰åº¦æ³•çš„ä¸¤é˜¶æ®µæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºç»™å®šçš„æºï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹æ¥è¯„ä¼°å…¶è¿›è¡Œå­”å¾„å…‰åº¦æ³•çš„å¯è¡Œæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒæ®‹å·®ç¥ç»ç½‘ç»œï¼ˆResNetï¼‰æ¥é¢„æµ‹æœ€ä½³å­”å¾„å¤§å°ã€‚å¯¹äºå­”å¾„å…‰åº¦å¯è¡Œæ€§è¯„ä¼°ï¼ŒViTæ¨¡å‹çš„ROC AUCå€¼ä¸º0.96ï¼Œæµ‹è¯•é›†ä¸Šçš„ç²¾åº¦ä¸º0.974ï¼Œå¬å›ç‡ä¸º0.930ï¼ŒF1åˆ†æ•°ä¸º0.952ã€‚å¯¹äºå­”å¾„å¤§å°é¢„æµ‹ï¼ŒResNetæ¨¡å‹é€šè¿‡è‡ªé€‚åº”é€‰æ‹©é€‚åˆä¸åŒäº®åº¦æºçš„å­”å¾„ï¼Œæœ‰æ•ˆç¼“è§£äº†ç»å…¸å¢é•¿æ›²çº¿æ–¹æ³•ä¸­çš„å›ºæœ‰åè§ï¼Œä»è€Œæé«˜äº†ç›®æ ‡çš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€‚åŒæ—¶ï¼Œç”±äºå­”å¾„å¤§å°ä¼°è®¡çš„ç²’åº¦æ›´ç²¾ç»†ï¼Œæµ‹è¯•é›†ä¸­ä¸€äº›æ ·æœ¬çš„SNRé«˜äºé€šè¿‡è¯¦å°½çš„å­”å¾„å¤§å°æšä¸¾æ‰€è·å¾—çš„SNRã€‚é€šè¿‡æ•´åˆResNetä¸ViTç½‘ç»œï¼ŒDeepAPæ¡†æ¶åœ¨æ‰¹å¤„ç†10å¼ å›¾åƒæ—¶è¾¾åˆ°ä¸­ä½æ•°æ€»å¤„ç†æ—¶é—´ä¸º18æ¯«ç§’ï¼Œä¸è¯¦å°½çš„å­”å¾„å¤§å°æšä¸¾ç›¸æ¯”ï¼Œå®ç°äº†çº¦59000å€çš„åŠ é€Ÿã€‚è¿™é¡¹å·¥ä½œä¸ºå­”å¾„å…‰åº¦æ³•åœ¨å¤©å®‡å’ŒLSSTç­‰æœªæ¥é«˜ç²¾åº¦è§‚æµ‹é¡¹ç›®ä¸­çš„è‡ªåŠ¨åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚æºä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ruiyicheng/DeepAP%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ruiyicheng/DeepAPè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03400v1">PDF</a> 16 pages,12 figures</p>
<p><strong>Summary</strong><br>å¤©èª‰é¡¹ç›®ä¸­å¹¿æ³›åº”ç”¨çš„å­”å¾„å…‰åº¦æ³•é¢ä¸´æ‹¥æŒ¤åŒºåŸŸæœ‰æ•ˆæ€§å’Œå­”å¾„å¤§å°é€‰æ‹©å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºDeepAPæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šå…ˆç”¨Vision Transformeræ¨¡å‹è¯„ä¼°å­”å¾„å…‰åº¦æ³•çš„å¯è¡Œæ€§ï¼Œå†ç”¨Residual Neural Networké¢„æµ‹æœ€ä½³å­”å¾„å¤§å°ã€‚è¯¥æ¡†æ¶æé«˜äº†ä¿¡å·å™ªå£°æ¯”ï¼Œç¼©çŸ­äº†å¤„ç†æ—¶é—´ï¼Œä¸ºæœªæ¥çš„é«˜ç²¾åº¦è°ƒæŸ¥å¦‚å¤©èª‰å’ŒLSSTè‡ªåŠ¨åº”ç”¨å­”å¾„å…‰åº¦æ³•é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepAPæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæé«˜å­”å¾„å…‰åº¦æ³•åœ¨æ‹¥æŒ¤åŒºåŸŸçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Vision Transformeræ¨¡å‹ç”¨äºè¯„ä¼°å­”å¾„å…‰åº¦æ³•çš„å¯è¡Œæ€§ï¼Œå¹¶è¾¾åˆ°é«˜è¯„ä¼°ç²¾åº¦ã€‚</li>
<li>Residual Neural Networkç”¨äºé¢„æµ‹æœ€ä½³å­”å¾„å¤§å°ï¼Œæœ‰æ•ˆæé«˜ä¿¡å·å™ªå£°æ¯”ã€‚</li>
<li>DeepAPæ¡†æ¶æé«˜äº†å¤„ç†é€Ÿåº¦ï¼Œç›¸è¾ƒäºå…¨é¢çš„å­”å¾„å¤§å°æšä¸¾ï¼ŒåŠ é€Ÿäº†çº¦59000å€ã€‚</li>
<li>è¯¥æ¡†æ¶é€‚ç”¨äºæœªæ¥é«˜ç²¾åº¦è°ƒæŸ¥ï¼Œå¦‚å¤©èª‰å’ŒLSSTã€‚</li>
<li>DeepAPçš„æºä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
<li>æ¡†æ¶å¯¹äºå¤æ‚æ•°æ®å¤„ç†å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿã€å‡†ç¡®å¤„ç†å¤§é‡å›¾åƒæ•°æ®çš„åœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d58986f4e3faf6203afceb37eb69bfe6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fca97125edda7c842395beaf26f6afd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ce76b539cf3396e515108ee477f9bf6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Glioblastoma-Overall-Survival-Prediction-With-Vision-Transformers"><a href="#Glioblastoma-Overall-Survival-Prediction-With-Vision-Transformers" class="headerlink" title="Glioblastoma Overall Survival Prediction With Vision Transformers"></a>Glioblastoma Overall Survival Prediction With Vision Transformers</h2><p><strong>Authors:Yin Lin, Riccardo Barbieri, Domenico Aquino, Giuseppe Lauria, Marina Grisoli, Elena De Momi, Alberto Redaelli, Simona Ferrante</strong></p>
<p>Glioblastoma is one of the most aggressive and common brain tumors, with a median survival of 10-15 months. Predicting Overall Survival (OS) is critical for personalizing treatment strategies and aligning clinical decisions with patient outcomes. In this study, we propose a novel Artificial Intelligence (AI) approach for OS prediction using Magnetic Resonance Imaging (MRI) images, exploiting Vision Transformers (ViTs) to extract hidden features directly from MRI images, eliminating the need of tumor segmentation. Unlike traditional approaches, our method simplifies the workflow and reduces computational resource requirements.   The proposed model was evaluated on the BRATS dataset, reaching an accuracy of 62.5% on the test set, comparable to the top-performing methods. Additionally, it demonstrated balanced performance across precision, recall, and F1 score, overcoming the best model in these metrics. The dataset size limits the generalization of the ViT which typically requires larger datasets compared to convolutional neural networks. This limitation in generalization is observed across all the cited studies. This work highlights the applicability of ViTs for downsampled medical imaging tasks and establishes a foundation for OS prediction models that are computationally efficient and do not rely on segmentation. </p>
<blockquote>
<p>èƒ¶è´¨æ¯ç»†èƒç˜¤æ˜¯æœ€å…·ä¾µè¢­æ€§å’Œæœ€å¸¸è§çš„è„‘è‚¿ç˜¤ä¹‹ä¸€ï¼Œä¸­ä½ç”Ÿå­˜æœŸä¸º10-15ä¸ªæœˆã€‚é¢„æµ‹æ€»ä½“ç”Ÿå­˜æœŸï¼ˆOSï¼‰å¯¹äºä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆå’Œä½¿ä¸´åºŠå†³ç­–ä¸ç—…äººç»“æœç›¸ç¬¦è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒè¿›è¡Œæ€»ä½“ç”Ÿå­˜æœŸé¢„æµ‹çš„æ–°å‹äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ç›´æ¥ä»MRIå›¾åƒä¸­æå–éšè—ç‰¹å¾ï¼Œæ— éœ€è¿›è¡Œè‚¿ç˜¤åˆ†å‰²ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç®€åŒ–äº†å·¥ä½œæµç¨‹ï¼Œé™ä½äº†è®¡ç®—èµ„æºè¦æ±‚ã€‚æ‰€æå‡ºæ¨¡å‹åœ¨BRATSæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†62.5%çš„å‡†ç¡®ç‡ï¼Œä¸è¡¨ç°æœ€ä½³çš„æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢è¡¨ç°å‡ºå¹³è¡¡çš„æ€§èƒ½ï¼Œåœ¨è¿™äº›æŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€ä½³æ¨¡å‹ã€‚æ•°æ®é›†çš„å¤§å°é™åˆ¶äº†ViTçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸å·ç§¯ç¥ç»ç½‘ç»œç›¸æ¯”ï¼ŒViTé€šå¸¸éœ€è¦æ›´å¤§çš„æ•°æ®é›†ã€‚æ‰€æœ‰å¼•ç”¨çš„ç ”ç©¶éƒ½è§‚å¯Ÿåˆ°è¿™ç§æ³›åŒ–é™åˆ¶ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ViTåœ¨ä¸‹é‡‡æ ·åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ï¼Œå¹¶ä¸ºè®¡ç®—æ•ˆç‡é«˜ã€ä¸ä¾èµ–åˆ†å‰²çš„æ€»ä½“ç”Ÿå­˜æœŸé¢„æµ‹æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02439v2">PDF</a> 4 pages, 4 figures, EMBC2025</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒè¿›è¡Œæ€»ä½“ç”Ÿå­˜æœŸï¼ˆOSï¼‰é¢„æµ‹çš„äººå·¥æ™ºèƒ½æ–°æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ç›´æ¥ä»MRIå›¾åƒä¸­æå–éšè—ç‰¹å¾ï¼Œæ— éœ€è¿›è¡Œè‚¿ç˜¤åˆ†å‰²ã€‚è¯¥æ–¹æ³•ç®€åŒ–äº†å·¥ä½œæµç¨‹å¹¶é™ä½äº†è®¡ç®—èµ„æºéœ€æ±‚ï¼Œåœ¨BRATSæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæµ‹è¯•é›†å‡†ç¡®ç‡è¾¾åˆ°äº†62.5%ï¼Œä¸”åœ¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢è¡¨ç°å‡è¡¡ã€‚è™½ç„¶æ•°æ®é›†è§„æ¨¡é™åˆ¶äº†ViTçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†æ­¤ç ”ç©¶çªæ˜¾äº†ViTsåœ¨åŒ»å­¦å›¾åƒå¤„ç†ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ï¼Œå¹¶ä¸ºè®¡ç®—æ•ˆç‡é«˜ä¸”æ— éœ€ä¾èµ–åˆ†å‰²çš„OSé¢„æµ‹æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶ä½¿ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰è¿›è¡ŒMRIå›¾åƒçš„æ€»ä½“ç”Ÿå­˜æœŸï¼ˆOSï¼‰é¢„æµ‹ã€‚</li>
<li>ViTså¯ä»¥ç›´æ¥ä»MRIå›¾åƒä¸­æå–éšè—ç‰¹å¾ï¼Œæ— éœ€è‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>æ–¹æ³•ç®€åŒ–äº†å·¥ä½œæµç¨‹å¹¶é™ä½äº†è®¡ç®—èµ„æºéœ€æ±‚ã€‚</li>
<li>åœ¨BRATSæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†62.5%ï¼Œè¡¨ç°å‡è¡¡çš„ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚</li>
<li>æ•°æ®é›†è§„æ¨¡é™åˆ¶äº†ViTçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ­¤ç ”ç©¶çªæ˜¾äº†ViTsåœ¨åŒ»å­¦å›¾åƒå¤„ç†ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b304478f43fd8cdbbc2381f62257effa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-754873117e9cc61f3abb3b0d1771972e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HiPrune-Training-Free-Visual-Token-Pruning-via-Hierarchical-Attention-in-Vision-Language-Models"><a href="#HiPrune-Training-Free-Visual-Token-Pruning-via-Hierarchical-Attention-in-Vision-Language-Models" class="headerlink" title="HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention   in Vision-Language Models"></a>HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention   in Vision-Language Models</h2><p><strong>Authors:Jizhihui Liu, Feiyi Du, Guangdao Zhu, Niu Lian, Jun Li, Bin Chen</strong></p>
<p>Vision-Language Models (VLMs) encode images into lengthy sequences of visual tokens, leading to excessive computational overhead and limited inference efficiency. While prior efforts prune or merge tokens to address this issue, they often rely on special tokens (e.g., CLS) or require task-specific training, hindering scalability across architectures. In this paper, we propose HiPrune, a training-free and model-agnostic token Pruning framework that exploits the Hierarchical attention structure within vision encoders. We identify that middle layers attend to object-centric regions, while deep layers capture global contextual features. Based on this observation, HiPrune selects three types of informative tokens: (1) Anchor tokens with high attention in object-centric layers, (2) Buffer tokens adjacent to anchors for spatial continuity, and (3) Register tokens with strong attention in deep layers for global summarization. Our method requires no retraining and integrates seamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5, LLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art pruning performance, preserving up to 99.3% task accuracy with only 33.3% tokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it reduces inference FLOPs and latency by up to 9$\times$, showcasing strong generalization across models and tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Danielement321/HiPrune">https://github.com/Danielement321/HiPrune</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å°†å›¾åƒç¼–ç ä¸ºå†—é•¿çš„è§†è§‰ä»¤ç‰Œåºåˆ—ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€è¿‡å¤§å’Œæ¨ç†æ•ˆç‡ä½ä¸‹ã€‚è™½ç„¶ä¹‹å‰çš„åŠªåŠ›é€šè¿‡åˆ é™¤æˆ–åˆå¹¶ä»¤ç‰Œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç‰¹æ®Šä»¤ç‰Œï¼ˆä¾‹å¦‚CLSï¼‰ï¼Œæˆ–éœ€è¦è¿›è¡Œç‰¹å®šä»»åŠ¡çš„è®­ç»ƒï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨æ¶æ„ä¹‹é—´çš„å¯æ‰©å±•æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HiPruneï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„ä»¤ç‰Œåˆ é™¤æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è§†è§‰ç¼–ç å™¨ä¸­çš„åˆ†å±‚æ³¨æ„åŠ›ç»“æ„ã€‚æˆ‘ä»¬å‘ç°ä¸­å±‚å…³æ³¨å¯¹è±¡ä¸ºä¸­å¿ƒçš„åŒºåŸŸï¼Œè€Œæ·±å±‚æ•è·å…¨å±€ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼ŒHiPruneé€‰æ‹©äº†ä¸‰ç§ç±»å‹çš„æ ‡è®°ä¿¡æ¯ï¼šï¼ˆ1ï¼‰åœ¨å¯¹è±¡ä¸­å¿ƒå±‚ä¸­å…·æœ‰é«˜æ³¨æ„åŠ›çš„é”šç‚¹ä»¤ç‰Œï¼Œï¼ˆ2ï¼‰ä¸é”šç‚¹ç›¸é‚»çš„ç¼“å†²åŒºä»¤ç‰Œä»¥å®ç°ç©ºé—´è¿ç»­æ€§ï¼Œï¼ˆ3ï¼‰åœ¨æ·±å±‚ä¸­å…·æœ‰å¼ºçƒˆæ³¨æ„åŠ›çš„å¯„å­˜å™¨ä»¤ç‰Œä»¥è¿›è¡Œå…¨å±€æ‘˜è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€é‡æ–°è®­ç»ƒï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•åŸºäºViTçš„VLMä¸­ã€‚åœ¨LLaVA-1.5ã€LLaVA-NeXTå’ŒQwen2.5-VLä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHiPruneè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ é™¤æ€§èƒ½ï¼Œä¿ç•™äº†é«˜è¾¾99.3%çš„ä»»åŠ¡å‡†ç¡®æ€§å¹¶ä¸”åªä½¿ç”¨33.3%çš„ä»¤ç‰Œï¼Œå¹¶èƒ½åœ¨ä»…ä½¿ç”¨11.1%çš„ä»¤ç‰Œæ—¶ä¿æŒ99.5%çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œå®ƒå‡å°‘äº†é«˜è¾¾9å€çš„æ¨ç†æµ®ç‚¹è¿ç®—é‡å’Œå»¶è¿Ÿæ—¶é—´ï¼Œåœ¨æ¨¡å‹å’Œä»»åŠ¡ä¹‹é—´è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Danielement321/HiPrune%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Danielement321/HiPruneè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00553v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æå‡ºä¸€ç§åä¸ºHiPruneçš„æ— è®­ç»ƒã€æ¨¡å‹æ— å…³çš„tokenè£å‰ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰ç¼–ç å™¨çš„å±‚æ¬¡åŒ–æ³¨æ„åŠ›ç»“æ„ï¼Œé€šè¿‡è¯†åˆ«å¹¶é€‰æ‹©ä¸‰ç§ç±»å‹çš„æ ‡è®°æ€§tokenè¿›è¡Œè£å‰ªã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æ— ç¼é›†æˆä»»ä½•åŸºäºViTçš„VLMï¼Œå¹¶å®ç°å‡ºè‰²çš„è£å‰ªæ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§å‡å°‘è®¡ç®—å¼€é”€å’Œæ¨ç†æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMså°†å›¾åƒç¼–ç ä¸ºå†—é•¿çš„è§†è§‰tokenåºåˆ—ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ä¸”æ¨ç†æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡è£å‰ªæˆ–åˆå¹¶tokenæ¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†é€šå¸¸éœ€è¦ç‰¹å®šä»»åŠ¡è®­ç»ƒæˆ–ç‰¹æ®Šæ ‡è®°ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸åŒæ¶æ„ä¸­çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>HiPruneæ˜¯ä¸€ä¸ªæ— è®­ç»ƒã€æ¨¡å‹æ— å…³çš„tokenè£å‰ªæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰ç¼–ç å™¨çš„å±‚æ¬¡åŒ–æ³¨æ„åŠ›ç»“æ„ã€‚</li>
<li>HiPruneé€šè¿‡è¯†åˆ«å¹¶é€‰æ‹©ä¸‰ç§ç±»å‹çš„æ ‡è®°æ€§tokenï¼ˆé”šç‚¹tokenã€ç¼“å†²åŒºtokenå’Œå¯„å­˜å™¨tokenï¼‰æ¥è¿›è¡Œè£å‰ªã€‚</li>
<li>HiPruneåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆä»»ä½•åŸºäºViTçš„VLMï¼Œå¹¶å®ç°å“è¶Šçš„è£å‰ªæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒHiPruneåœ¨å¤šä¸ªæ¨¡å‹å’Œä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è£å‰ªæ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™é«˜è¾¾99.3%çš„ä»»åŠ¡å‡†ç¡®æ€§çš„åŒæ—¶ä»…ä½¿ç”¨33.3%çš„tokenï¼Œå¹¶åœ¨ä»…ä½¿ç”¨11.1%çš„tokenæ—¶ä¿æŒ99.5%çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86ff97e309218f8a54e8debf43a3820d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29dc76e0d1d76631d0e62ec18dc8ac5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f59b1adac7bfe2e05986ed8cf7dbda8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48a9779e23583aaded78475aa5f0cfc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2637f02a04debaa150d96c7c7f825670.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Automatic-Synthesis-of-High-Quality-Triplet-Data-for-Composed-Image-Retrieval"><a href="#Automatic-Synthesis-of-High-Quality-Triplet-Data-for-Composed-Image-Retrieval" class="headerlink" title="Automatic Synthesis of High-Quality Triplet Data for Composed Image   Retrieval"></a>Automatic Synthesis of High-Quality Triplet Data for Composed Image   Retrieval</h2><p><strong>Authors:Haiwen Li, Delong Liu, Zhaohui Hou, Zhicheng Zhao, Fei Su</strong></p>
<p>As a challenging vision-language (VL) task, Composed Image Retrieval (CIR) aims to retrieve target images using multimodal (image+text) queries. Although many existing CIR methods have attained promising performance, their reliance on costly, manually labeled triplets hinders scalability and zero-shot capability. To address this issue, we propose a scalable pipeline for automatic triplet generation, along with a fully synthetic dataset named Composed Image Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a large language model (LLM) to generate diverse prompts, controlling a text-to-image generative model to produce image pairs with identical elements in each pair, which are then filtered and reorganized to form the CIRHS dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a novel CIR framework, which can accomplish global alignment and local reasoning within a broader context, enabling the model to learn more robust and informative representations. By utilizing the synthetic CIRHS dataset, CoAlign achieves outstanding zero-shot performance on three commonly used benchmarks, demonstrating for the first time the feasibility of training CIR models on a fully synthetic dataset. Furthermore, under supervised training, our method outperforms all the state-of-the-art supervised CIR approaches, validating the effectiveness of our proposed retrieval framework. The code and the CIRHS dataset will be released soon. </p>
<blockquote>
<p>ä½œä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰è¯­è¨€ï¼ˆVLï¼‰ä»»åŠ¡ï¼Œç»„åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰æ—¨åœ¨ä½¿ç”¨å¤šæ¨¡æ€ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰æŸ¥è¯¢æ¥æ£€ç´¢ç›®æ ‡å›¾åƒã€‚å°½ç®¡è®¸å¤šç°æœ‰çš„CIRæ–¹æ³•å·²ç»å–å¾—äº†æœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¯¹æˆæœ¬é«˜æ˜‚çš„æ‰‹åŠ¨æ ‡æ³¨ä¸‰å…ƒç»„çš„ä¾èµ–é˜»ç¢äº†å…¶å¯æ‰©å±•æ€§å’Œé›¶æ ·æœ¬èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºè‡ªåŠ¨ä¸‰å…ƒç»„ç”Ÿæˆçš„å¯æ‰©å±•ç®¡é“ï¼Œä»¥åŠä¸€ä¸ªåä¸ºâ€œåŸºäºé«˜è´¨é‡åˆæˆä¸‰å…ƒç»„çš„ç»„åˆå›¾åƒæ£€ç´¢ï¼ˆCIRHSï¼‰â€çš„å®Œå…¨åˆæˆæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç®¡é“åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå„ç§æç¤ºï¼Œæ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥äº§ç”Ÿå…·æœ‰ç›¸åŒå…ƒç´ çš„å›¾åƒå¯¹ï¼Œç„¶åå¯¹å®ƒä»¬è¿›è¡Œè¿‡æ»¤å’Œé‡ç»„ä»¥å½¢æˆCIRHSæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ··åˆä¸Šä¸‹æ–‡å¯¹é½ï¼ˆCoAlignï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„CIRæ¡†æ¶ï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„èƒŒæ™¯ä¸‹å®Œæˆå…¨å±€å¯¹é½å’Œå±€éƒ¨æ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´ç¨³å¥å’Œæ›´å…·ä¿¡æ¯é‡çš„è¡¨ç¤ºã€‚é€šè¿‡åˆ©ç”¨åˆæˆçš„CIRHSæ•°æ®é›†ï¼ŒCoAlignåœ¨ä¸‰ä¸ªå¸¸ç”¨åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œé¦–æ¬¡è¯æ˜äº†åœ¨å®Œå…¨åˆæˆçš„æ•°æ®é›†ä¸Šè®­ç»ƒCIRæ¨¡å‹çš„å¯è¡Œæ€§ã€‚æ­¤å¤–ï¼Œåœ¨ç›‘ç£è®­ç»ƒä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æ‰€æœ‰æœ€æ–°çš„ç›‘ç£CIRæ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€æå‡ºçš„æ£€ç´¢æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’ŒCIRHSæ•°æ®é›†å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05970v2">PDF</a> This paper was originally submitted to ACM MM 2025 on April 12, 2025</p>
<p><strong>Summary</strong><br>åŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç”Ÿæˆæç¤ºï¼Œæ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œç”Ÿæˆå…·æœ‰ç›¸åŒå…ƒç´ çš„å›¾åƒå¯¹ï¼Œæ„å»ºåˆæˆæ•°æ®é›†CIRHSï¼Œç”¨äºè§£å†³å›¾åƒæ£€ç´¢ä»»åŠ¡ã€‚å¼•å…¥Hybrid Contextual Alignmentæ¡†æ¶ï¼Œå®ç°å…¨å±€å¯¹é½å’Œå±€éƒ¨æ¨ç†ï¼Œåˆ©ç”¨åˆæˆæ•°æ®é›†è®­ç»ƒæ¨¡å‹å¹¶è¾¾åˆ°å‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä½¿ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç”Ÿæˆæç¤ºä»¥æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæ„å»ºåˆæˆæ•°æ®é›†CIRHSã€‚</li>
<li>CIRHSæ•°æ®é›†ç”¨äºè§£å†³å›¾åƒæ£€ç´¢ä»»åŠ¡ï¼Œå¯å®ç°è‡ªåŠ¨ä¸‰å…ƒç»„ç”Ÿæˆã€‚</li>
<li>å¼•å…¥Hybrid Contextual Alignmentæ¡†æ¶ï¼Œå®ç°å…¨å±€å¯¹é½å’Œå±€éƒ¨æ¨ç†ã€‚</li>
<li>åˆ©ç”¨åˆæˆæ•°æ®é›†CIRHSè®­ç»ƒæ¨¡å‹ï¼Œè¾¾åˆ°å‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå¸¸ç”¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCoAlignçš„é›¶æ ·æœ¬æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†ä½¿ç”¨åˆæˆæ•°æ®é›†è®­ç»ƒCIRæ¨¡å‹çš„å¯è¡Œæ€§ã€‚</li>
<li>åœ¨ç›‘ç£è®­ç»ƒä¸‹ï¼Œè¯¥æ–¹æ³•ä¼˜äºæ‰€æœ‰ç°æœ‰å…ˆè¿›ç›‘ç£CIRæ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05970">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db88577ef31799f41b4c6be4df3ec11c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d114aa7a3573b1b0d22e83a3ebc762a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4eec6f12a8893ed495a6dbea1bc00a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d4c99b18c5a9b84031877c5aafdc6b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="COBRA-A-Continual-Learning-Approach-to-Vision-Brain-Understanding"><a href="#COBRA-A-Continual-Learning-Approach-to-Vision-Brain-Understanding" class="headerlink" title="COBRA: A Continual Learning Approach to Vision-Brain Understanding"></a>COBRA: A Continual Learning Approach to Vision-Brain Understanding</h2><p><strong>Authors:Xuan-Bac Nguyen, Manuel Serna-Aguilera, Arabinda Kumar Choudhary, Pawan Sinha, Xin Li, Khoa Luu</strong></p>
<p>Vision-Brain Understanding (VBU) aims to extract visual information perceived by humans from brain activity recorded through functional Magnetic Resonance Imaging (fMRI). Despite notable advancements in recent years, existing studies in VBU continue to face the challenge of catastrophic forgetting, where models lose knowledge from prior subjects as they adapt to new ones. Addressing continual learning in this field is, therefore, essential. This paper introduces a novel framework called Continual Learning for Vision-Brain (COBRA) to address continual learning in VBU. Our approach includes three novel modules: a Subject Commonality (SC) module, a Prompt-based Subject Specific (PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer module. The SC module captures shared vision-brain patterns across subjects, preserving this knowledge as the model encounters new subjects, thereby reducing the impact of catastrophic forgetting. On the other hand, the PSS module learns unique vision-brain patterns specific to each subject. Finally, the MRIFormer module contains a transformer encoder and decoder that learns the fMRI features for VBU from common and specific patterns. In a continual learning setup, COBRA is trained in new PSS and MRIFormer modules for new subjects, leaving the modules of previous subjects unaffected. As a result, COBRA effectively addresses catastrophic forgetting and achieves state-of-the-art performance in both continual learning and vision-brain reconstruction tasks, surpassing previous methods. </p>
<blockquote>
<p>è§†è§‰å¤§è„‘ç†è§£ï¼ˆVBUï¼‰æ—¨åœ¨ä»é€šè¿‡åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰è®°å½•çš„å¤§è„‘æ´»åŠ¨ä¸­æå–äººç±»æ„ŸçŸ¥åˆ°çš„è§†è§‰ä¿¡æ¯ã€‚å°½ç®¡è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†VBUçš„ç°æœ‰ç ”ç©¶ä»ç„¶é¢ä¸´ç€ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ï¼Œå³æ¨¡å‹åœ¨é€‚åº”æ–°ä¸»é¢˜æ—¶ä¸¢å¤±äº†å…ˆå‰ä¸»é¢˜çš„çŸ¥è¯†ã€‚å› æ­¤ï¼Œè§£å†³è¯¥é¢†åŸŸçš„æŒç»­å­¦ä¹ è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCOBRAçš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œä»¥è§£å†³VBUä¸­çš„æŒç»­å­¦ä¹ é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ–°é¢–æ¨¡å—ï¼šä¸»é¢˜å…±æ€§ï¼ˆSCï¼‰æ¨¡å—ã€åŸºäºæç¤ºçš„ä¸»é¢˜ç‰¹å®šï¼ˆPSSï¼‰æ¨¡å—å’ŒåŸºäºå˜å‹å™¨çš„fMRIæ¨¡å—ï¼Œç§°ä¸ºMRIFormeræ¨¡å—ã€‚SCæ¨¡å—æ•è·è·¨ä¸»é¢˜çš„å…±åŒè§†è§‰å¤§è„‘æ¨¡å¼ï¼Œå¹¶éšç€æ¨¡å‹é‡åˆ°æ–°ä¸»é¢˜è€Œä¿ç•™è¿™äº›çŸ¥è¯†ï¼Œä»è€Œå‡å°‘ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚å¦ä¸€æ–¹é¢ï¼ŒPSSæ¨¡å—å­¦ä¹ æ¯ä¸ªä¸»é¢˜çš„ç‹¬ç‰¹è§†è§‰å¤§è„‘æ¨¡å¼ã€‚æœ€åï¼ŒMRIFormeræ¨¡å—åŒ…å«ä¸€ä¸ªå˜å‹å™¨ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œä»å…¬å…±å’Œç‰¹å®šæ¨¡å¼ä¸­å­¦ä¹ VBUçš„fMRIç‰¹å¾ã€‚åœ¨æŒç»­å­¦ä¹ è®¾ç½®ä¸­ï¼ŒCOBRAé’ˆå¯¹æ–°ä¸»é¢˜è¿›è¡ŒPSSå’ŒMRIFormeræ¨¡å—çš„æ›´æ–°è®­ç»ƒï¼Œè€Œä¹‹å‰ä¸»é¢˜çš„æ¨¡å—ä¸å—å½±å“ã€‚å› æ­¤ï¼ŒCOBRAæœ‰æ•ˆåœ°è§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶åœ¨æŒç»­å­¦ä¹ å’Œè§†è§‰å¤§è„‘é‡å»ºä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17475v3">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿™ç¯‡è®ºæ–‡æå‡ºäº†é’ˆå¯¹è§†è§‰è„‘ç†è§£ï¼ˆVBUï¼‰é¢†åŸŸçš„æŒç»­å­¦ä¹ é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ä¸ªåä¸ºCOBRAçš„æ–°æ¡†æ¶ï¼ŒåŒ…å«ä¸»ä½“å…±æ€§ï¼ˆSCï¼‰æ¨¡å—ã€åŸºäºæç¤ºçš„ä¸»ä½“ç‰¹å®šï¼ˆPSSï¼‰æ¨¡å—å’ŒåŸºäºå˜å‹å™¨çš„fMRIæ¨¡å—MRIFormerã€‚è¿™äº›æ¨¡å—èƒ½å¤Ÿæ•æ‰è·¨ä¸»ä½“çš„è§†è§‰è„‘æ¨¡å¼ï¼Œå­¦ä¹ ä¸»ä½“çš„ç‹¬ç‰¹æ¨¡å¼ï¼Œå¹¶é€šè¿‡æŒç»­è®­ç»ƒæ–°ä¸»ä½“æ¥å‡å°‘ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚COBRAåœ¨æŒç»­å­¦ä¹ å’Œè§†è§‰è„‘é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-Brain Understanding (VBU)æ—¨åœ¨ä»é€šè¿‡åŠŸèƒ½ç£å…±æŒ¯æˆåƒ(fMRI)è®°å½•çš„å¤§è„‘æ´»åŠ¨ä¸­æå–äººç±»æ„ŸçŸ¥çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰VBUç ”ç©¶é¢ä¸´ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ï¼Œå³æ¨¡å‹åœ¨é€‚åº”æ–°ä¸»ä½“æ—¶ä¸§å¤±å…ˆå‰çŸ¥è¯†ã€‚</li>
<li>COBRAæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ–°æ¨¡å—ï¼šä¸»ä½“å…±æ€§ï¼ˆSCï¼‰æ¨¡å—ã€åŸºäºæç¤ºçš„ä¸»ä½“ç‰¹å®šï¼ˆPSSï¼‰æ¨¡å—å’ŒåŸºäºå˜å‹å™¨çš„fMRIæ¨¡å—ï¼ˆMRIFormerï¼‰ã€‚</li>
<li>SCæ¨¡å—æ•æ‰è·¨ä¸»ä½“çš„å…±äº«è§†è§‰è„‘æ¨¡å¼ï¼Œä¿ç•™çŸ¥è¯†å¹¶å‡å°‘ç¾éš¾æ€§é—å¿˜çš„å½±å“ã€‚</li>
<li>PSSæ¨¡å—å­¦ä¹ æ¯ä¸ªä¸»ä½“çš„ç‹¬ç‰¹è§†è§‰è„‘æ¨¡å¼ã€‚</li>
<li>MRIFormeræ¨¡å—åŒ…å«å˜å‹å™¨ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œä»å…¬å…±å’Œç‰¹å®šæ¨¡å¼ä¸­å­¦ä¹ VBUçš„fMRIç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e8d776f3bdb14c81a4b70b06780c254.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c5f532088fddc155a445db0e4b57c99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5839214953af6eb1755521247df8894.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3067587925e9d343f544225d2cc72f96.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0665bc2b177017d95aa2701c2d6c943a.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  DS$^2$Net Detail-Semantic Deep Supervision Network for Medical Image   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0b1083293ab302780c041050b69efbdc.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  TSPO Temporal Sampling Policy Optimization for Long-form Video Language   Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
