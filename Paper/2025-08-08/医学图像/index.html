<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  X-SAM From Segment Anything to Any Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6db1c5897da7f283087bd2e51ff304c7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-08-æ›´æ–°"><a href="#2025-08-08-æ›´æ–°" class="headerlink" title="2025-08-08 æ›´æ–°"></a>2025-08-08 æ›´æ–°</h1><h2 id="X-SAM-From-Segment-Anything-to-Any-Segmentation"><a href="#X-SAM-From-Segment-Anything-to-Any-Segmentation" class="headerlink" title="X-SAM: From Segment Anything to Any Segmentation"></a>X-SAM: From Segment Anything to Any Segmentation</h2><p><strong>Authors:Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, Xiaodan Liang</strong></p>
<p>Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \textit{segment anything} to \textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wanghao9610/X-SAM">https://github.com/wanghao9610/X-SAM</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¹¿æ³›çš„çŸ¥è¯†è¡¨ç¤ºæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨åƒç´ çº§åˆ«çš„æ„ŸçŸ¥ç†è§£æ–¹é¢å­˜åœ¨å›ºæœ‰ç¼ºé™·ã€‚å°½ç®¡Segment Anything Modelï¼ˆSAMï¼‰åœ¨è§†è§‰æç¤ºé©±åŠ¨çš„å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨å¤šæ©è†œé¢„æµ‹å’Œç‰¹å®šç±»åˆ«åˆ†å‰²ä»»åŠ¡æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œä¸”æ— æ³•å°†æ‰€æœ‰åˆ†å‰²ä»»åŠ¡æ•´åˆåˆ°ç»Ÿä¸€çš„æ¨¡å‹æ¶æ„ä¸­ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†X-SAMï¼Œä¸€ä¸ªç®€åŒ–çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œå®ƒå°†åˆ†å‰²èŒƒå¼ä»â€œåˆ†å‰²ä»»ä½•äº‹ç‰©â€æ‰©å±•åˆ°â€œä»»ä½•åˆ†å‰²â€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ç»Ÿä¸€æ¡†æ¶ï¼Œä¸ºMLLMæä¾›æ›´å…ˆè¿›çš„åƒç´ çº§åˆ«æ„ŸçŸ¥ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†å‰²ä»»åŠ¡ï¼Œç§°ä¸ºVisual GrounDedï¼ˆVGDï¼‰åˆ†å‰²ï¼Œå®ƒé€šè¿‡äº¤äº’å¼çš„è§†è§‰æç¤ºå¯¹æ‰€æœ‰å®ä¾‹å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œå¹¶ä¸ºMLLMæä¾›è§†è§‰æ¥åœ°ã€åƒç´ çº§çš„è§£é‡Šèƒ½åŠ›ã€‚ä¸ºäº†èƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„æ•°æ®æºä¸Šè¿›è¡Œæœ‰æ•ˆè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¯æŒè·¨å¤šä¸ªæ•°æ®é›†è”åˆè®­ç»ƒçš„ç»Ÿä¸€è®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒX-SAMåœ¨å¹¿æ³›çš„å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨å¤šæ¨¡æ€ã€åƒç´ çº§åˆ«çš„è§†è§‰ç†è§£æ–¹é¢çš„æ•ˆç‡ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/wanghao9eav%E6%8A%80%E6%9C%AF%E8%81%94%E7%9B%9F%E7%BB%84%E7%BB%87%E9%95%BF%E7%9E%84%E5%87%86%E4%BD%8E%E6%88%90%E6%9C%AC%E5%A4%9A%E6%A8%A1%E5%BC%8F%E6%BC%94%E7%A4%BA%E6%96%B0%E6%8A%80%E6%9C%AF%E7%AB%AF%E7%A7%91%E7%A0%94%E6%96%B9%E5%90%91%E4%BB%A5%E4%BE%BF%E7%94%B3%E8%AF%B7%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE%E5%9F%BA%E9%87%91**%E7%AE%80%E6%98%8E%E6%89%BC%E8%A6%81%E6%91%98%E8%A6%81%E6%8F%90%E7%82%BC%E3%80%82**%E7%A0%94%E7%A9%B6%E4%BA%86%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E9%A2%86%E5%9F%9F%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7%EF%BC%8C%E5%B9%B6%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8C%96%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6X-SAM%E6%9D%A5%E8%A7%A3%E5%86%B3%E8%BF%99%E4%BA%9B%E9%97%AE%E9%A2%98%E3%80%82%E8%AF%A5%E6%A1%86%E6%9E%B6%E5%BC%95%E5%85%A5%E4%BA%86%E6%96%B0%E5%9E%8B%E7%BB%9F%E4%B8%80%E6%A1%86%E6%9E%B6%E5%92%8CVisual">https://github.com/wanghao9eavæŠ€æœ¯è”ç›Ÿç»„ç»‡é•¿ç„å‡†ä½æˆæœ¬å¤šæ¨¡å¼æ¼”ç¤ºæ–°æŠ€æœ¯ç«¯ç§‘ç ”æ–¹å‘ä»¥ä¾¿ç”³è¯·ç§‘ç ”é¡¹ç›®åŸºé‡‘**ç®€æ˜æ‰¼è¦æ‘˜è¦æç‚¼ã€‚**ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸçš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç®€åŒ–çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶X-SAMæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ–°å‹ç»Ÿä¸€æ¡†æ¶å’ŒVisual</a> GrounDedåˆ†å‰²ä»»åŠ¡æ¥æå‡æ¨¡å‹çš„åƒç´ çº§åˆ«æ„ŸçŸ¥ç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜äº†X-SAMçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”å¼ºè°ƒäº†å…¶åœ¨å¤šæ¨¡æ€è§†è§‰ç†è§£æ–¹é¢çš„æ½œåŠ›ã€‚ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04655v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å¹¿æ³›çš„çŸ¥è¯†è¡¨ç¤ºèƒ½åŠ›ï¼Œä½†åœ¨åƒç´ çº§æ„ŸçŸ¥ç†è§£æ–¹é¢å­˜åœ¨å›ºæœ‰ç¼ºé™·ã€‚Segment Anything Modelï¼ˆSAMï¼‰åœ¨è§†è§‰æç¤ºé©±åŠ¨å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤šæ©è†œé¢„æµ‹å’Œç‰¹å®šç±»åˆ«åˆ†å‰²ä»»åŠ¡æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸”æ— æ³•å°†æ‰€æœ‰åˆ†å‰²ä»»åŠ¡é›†æˆåˆ°ç»Ÿä¸€æ¨¡å‹æ¶æ„ä¸­ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†X-SAMï¼Œä¸€ä¸ªç®€åŒ–çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œå°†åˆ†å‰²èŒƒå¼ä»â€œåˆ†å‰²ä»»ä½•äº‹ç‰©â€æ‰©å±•åˆ°â€œä»»ä½•åˆ†å‰²â€ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„ç»Ÿä¸€æ¡†æ¶ï¼Œä¸ºMLLMæä¾›æ›´å…ˆè¿›çš„åƒç´ çº§æ„ŸçŸ¥ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºVisual GrounDedï¼ˆVGDï¼‰åˆ†å‰²çš„æ–°åˆ†å‰²ä»»åŠ¡ï¼Œé€šè¿‡äº¤äº’å¼è§†è§‰æç¤ºå¯¹æ‰€æœ‰å®ä¾‹å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œå¹¶ä¸ºMLLMæä¾›è§†è§‰åŸºç¡€ã€åƒç´ çº§çš„è§£é‡Šèƒ½åŠ›ã€‚ä¸ºåœ¨å¤šæ ·æ•°æ®æºä¸Šè¿›è¡Œæœ‰æ•ˆè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¯æŒè·¨å¤šä¸ªæ•°æ®é›†è”åˆè®­ç»ƒçš„ç»Ÿä¸€è®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-SAMåœ¨å¹¿æ³›å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼Œè¯æ˜å…¶åœ¨å¤šæ¨¡æ€ã€åƒç´ çº§è§†è§‰ç†è§£æ–¹é¢çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åƒç´ çº§æ„ŸçŸ¥ç†è§£æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰åœ¨è§†è§‰æç¤ºé©±åŠ¨å›¾åƒåˆ†å‰²æ–¹é¢æœ‰æ‰€çªç ´ï¼Œä½†ä»å­˜åœ¨å¤šæ©è†œé¢„æµ‹å’Œç‰¹å®šç±»åˆ«åˆ†å‰²çš„ä»»åŠ¡å±€é™æ€§ã€‚</li>
<li>X-SAMæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³SAMçš„å±€é™æ€§ï¼Œå®ç°ä»â€œåˆ†å‰²ä»»ä½•äº‹ç‰©â€åˆ°â€œä»»ä½•åˆ†å‰²â€çš„æ‰©å±•ã€‚</li>
<li>X-SAMå¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¢å¼ºäº†MLLMçš„åƒç´ çº§æ„ŸçŸ¥ç†è§£èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†Visual GrounDedï¼ˆVGDï¼‰åˆ†å‰²ä»»åŠ¡ï¼Œé€šè¿‡äº¤äº’å¼è§†è§‰æç¤ºå¯¹æ‰€æœ‰å®ä¾‹å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œå¢å¼ºMLLMçš„è§†è§‰åŸºç¡€ã€åƒç´ çº§è§£é‡Šèƒ½åŠ›ã€‚</li>
<li>X-SAMé‡‡ç”¨ç»Ÿä¸€è®­ç»ƒç­–ç•¥ï¼Œæ”¯æŒè·¨å¤šä¸ªæ•°æ®é›†çš„æœ‰æ•ˆè®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒX-SAMåœ¨å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·å¤‡å¤šæ¨¡æ€ã€åƒç´ çº§è§†è§‰ç†è§£çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aa03d7c941ccccbae5e6a17bb0b5fa64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e2bf61ea193368bfa3459854119e662.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82c70a87a82b701346d90ed1089e4ca9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b85043867b29ab1939f1d20e4cb4cca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-723b17fc21da469ca52a23900c55c91d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Bias-and-Interpretability-in-Deep-Learning-for-Dermatological-Image-Analysis"><a href="#Visual-Bias-and-Interpretability-in-Deep-Learning-for-Dermatological-Image-Analysis" class="headerlink" title="Visual Bias and Interpretability in Deep Learning for Dermatological   Image Analysis"></a>Visual Bias and Interpretability in Deep Learning for Dermatological   Image Analysis</h2><p><strong>Authors:Enam Ahmed Taufik, Abdullah Khondoker, Antara Firoz Parsa, Seraj Al Mahmud Mostafa</strong></p>
<p>Accurate skin disease classification is a critical yet challenging task due to high inter-class similarity, intra-class variability, and complex lesion textures. While deep learning-based computer-aided diagnosis (CAD) systems have shown promise in automating dermatological assessments, their performance is highly dependent on image pre-processing and model architecture. This study proposes a deep learning framework for multi-class skin disease classification, systematically evaluating three image pre-processing techniques: standard RGB, CMY color space transformation, and Contrast Limited Adaptive Histogram Equalization (CLAHE). We benchmark the performance of pre-trained convolutional neural networks (DenseNet201, Efficient-NetB5) and transformer-based models (ViT, Swin Transformer, DinoV2 Large) using accuracy and F1-score as evaluation metrics. Results show that DinoV2 with RGB pre-processing achieves the highest accuracy (up to 93%) and F1-scores across all variants. Grad-CAM visualizations applied to RGB inputs further reveal precise lesion localization, enhancing interpretability. These findings underscore the importance of effective pre-processing and model choice in building robust and explainable CAD systems for dermatology. </p>
<blockquote>
<p>ç²¾ç¡®çš„çš®è‚¤ç–¾ç—…åˆ†ç±»æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå­˜åœ¨ç±»é—´ç›¸ä¼¼æ€§é«˜ã€ç±»å†…å˜åŒ–å¤§ä»¥åŠç—…å˜çº¹ç†å¤æ‚çš„é—®é¢˜ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿåœ¨è‡ªåŠ¨åŒ–çš®è‚¤ç§‘è¯„ä¼°æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶æ€§èƒ½é«˜åº¦ä¾èµ–äºå›¾åƒé¢„å¤„ç†å’Œæ¨¡å‹æ¶æ„ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºå¤šç±»çš®è‚¤ç–¾ç—…åˆ†ç±»çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç³»ç»Ÿè¯„ä¼°äº†ä¸‰ç§å›¾åƒé¢„å¤„ç†æŠ€æœ¯ï¼šæ ‡å‡†RGBã€CMYé¢œè‰²ç©ºé—´è½¬æ¢å’Œå¯¹æ¯”åº¦å—é™è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼ˆCLAHEï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆDenseNet201ã€Efficient-NetB5ï¼‰å’ŒåŸºäºtransformerçš„æ¨¡å‹ï¼ˆViTã€Swin Transformerã€DinoV2 Largeï¼‰ï¼Œä»¥å‡†ç¡®ç‡å’ŒF1åˆ†æ•°ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œå¯¹å®ƒä»¬è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨RGBé¢„å¤„ç†çš„DinoV2è·å¾—äº†æœ€é«˜å‡†ç¡®ç‡ï¼ˆé«˜è¾¾93%ï¼‰å’ŒF1åˆ†æ•°ã€‚åº”ç”¨äºRGBè¾“å…¥çš„Grad-CAMå¯è§†åŒ–è¿›ä¸€æ­¥æ­ç¤ºäº†ç²¾ç¡®çš„ç—…å˜å®šä½ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æœ‰æ•ˆé¢„å¤„ç†å’Œæ¨¡å‹é€‰æ‹©åœ¨æ„å»ºç”¨äºçš®è‚¤ç—…çš„ç¨³å¥ä¸”å¯è§£é‡Šçš„CADç³»ç»Ÿæ–¹é¢çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04573v1">PDF</a> This paper has been accepted in the 4th IEEE International Conference   on Image Processing and Media Computing (ICIPMC) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šç±»çš®è‚¤ç—…åˆ†ç±»é—®é¢˜ï¼Œæ¢è®¨äº†ä¸åŒçš„å›¾åƒé¢„å¤„ç†æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ ‡å‡†RGBã€CMYè‰²å½©ç©ºé—´è½¬æ¢å’ŒContrast Limited Adaptive Histogram Equalizationï¼ˆCLAHEï¼‰ã€‚ç ”ç©¶ä½¿ç”¨äº†é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆDenseNet201ã€Efficient-NetB5ï¼‰å’ŒåŸºäºtransformerçš„æ¨¡å‹ï¼ˆViTã€Swin Transformerã€DinoV2 Largeï¼‰ï¼Œä»¥å‡†ç¡®åº¦å’ŒF1åˆ†æ•°ä¸ºè¯„ä»·æŒ‡æ ‡ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨RGBé¢„å¤„ç†çš„DinoV2æ¨¡å‹åœ¨å‡†ç¡®åº¦å’ŒF1åˆ†æ•°ä¸Šè¡¨ç°æœ€ä½³ï¼ˆé«˜è¾¾93%ï¼‰ã€‚Grad-CAMå¯è§†åŒ–è¿›ä¸€æ­¥æ­ç¤ºäº†ç²¾ç¡®çš„ç—…å˜å®šä½ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨çš®è‚¤ç—…åˆ†ç±»ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†é¢ä¸´ç±»é—´ç›¸ä¼¼åº¦é«˜ã€ç±»å†…å˜å¼‚å¤æ‚å’Œç—…å˜çº¹ç†å¤šæ ·ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æœ‰æ•ˆçš„å›¾åƒé¢„å¤„ç†æŠ€æœ¯å¯¹äºæé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬ç ”ç©¶æµ‹è¯•äº†å¤šç§å›¾åƒé¢„å¤„ç†æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ ‡å‡†RGBã€CMYè‰²å½©ç©ºé—´è½¬æ¢å’ŒCLAHEã€‚</li>
<li>é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œå’ŒåŸºäºtransformerçš„æ¨¡å‹åœ¨çš®è‚¤ç—…åˆ†ç±»ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>DinoV2æ¨¡å‹ä½¿ç”¨RGBé¢„å¤„ç†åœ¨å‡†ç¡®åº¦å’ŒF1åˆ†æ•°ä¸Šè¾¾åˆ°æœ€é«˜è¡¨ç°ã€‚</li>
<li>Grad-CAMå¯è§†åŒ–æœ‰åŠ©äºç²¾ç¡®ç—…å˜å®šä½ï¼Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-898b881ab4cf44ec0e48ef80256c2ae4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3a06165e09e33733fc4d9c82c339559.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d2b2a7741d15b20a43af9ab898af8b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8249217769c9061c068ca74d74b55e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae78d32cd29f3ca9e545a9b7a804acd2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DDTracking-A-Deep-Generative-Framework-for-Diffusion-MRI-Tractography-with-Streamline-Local-Global-Spatiotemporal-Modeling"><a href="#DDTracking-A-Deep-Generative-Framework-for-Diffusion-MRI-Tractography-with-Streamline-Local-Global-Spatiotemporal-Modeling" class="headerlink" title="DDTracking: A Deep Generative Framework for Diffusion MRI Tractography   with Streamline Local-Global Spatiotemporal Modeling"></a>DDTracking: A Deep Generative Framework for Diffusion MRI Tractography   with Streamline Local-Global Spatiotemporal Modeling</h2><p><strong>Authors:Yijie Li, Wei Zhang, Xi Zhu, Ye Wu, Yogesh Rathi, Lauren J. Oâ€™Donnell, Fan Zhang</strong></p>
<p>This paper presents DDTracking, a novel deep generative framework for diffusion MRI tractography that formulates streamline propagation as a conditional denoising diffusion process. In DDTracking, we introduce a dual-pathway encoding network that jointly models local spatial encoding (capturing fine-scale structural details at each streamline point) and global temporal dependencies (ensuring long-range consistency across the entire streamline). Furthermore, we design a conditional diffusion model module, which leverages the learned local and global embeddings to predict streamline propagation orientations for tractography in an end-to-end trainable manner. We conduct a comprehensive evaluation across diverse, independently acquired dMRI datasets, including both synthetic and clinical data. Experiments on two well-established benchmarks with ground truth (ISMRM Challenge and TractoInferno) demonstrate that DDTracking largely outperforms current state-of-the-art tractography methods. Furthermore, our results highlight DDTrackingâ€™s strong generalizability across heterogeneous datasets, spanning varying health conditions, age groups, imaging protocols, and scanner types. Collectively, DDTracking offers anatomically plausible and robust tractography, presenting a scalable, adaptable, and end-to-end learnable solution for broad dMRI applications. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/yishengpoxiao/DDtracking.git">https://github.com/yishengpoxiao/DDtracking.git</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DDTrackingï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ·±åº¦ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºå°†æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰çš„æµçº¿ä¼ æ’­å»ºæ¨¡ä¸ºæ¡ä»¶å»å™ªæ‰©æ•£è¿‡ç¨‹ã€‚åœ¨DDTrackingä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒè·¯å¾„ç¼–ç ç½‘ç»œï¼Œè¯¥ç½‘ç»œå¯¹å±€éƒ¨ç©ºé—´ç¼–ç è¿›è¡Œè”åˆå»ºæ¨¡ï¼ˆæ•è·æ¯æ¡æµçº¿ç‚¹çš„ç²¾ç»†ç»“æ„ç»†èŠ‚ï¼‰ï¼Œå¹¶å»ºæ¨¡å…¨å±€æ—¶é—´ä¾èµ–æ€§ï¼ˆç¡®ä¿æ•´ä¸ªæµçº¿çš„è¿œç¨‹ä¸€è‡´æ€§ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨å­¦ä¹ åˆ°çš„å±€éƒ¨å’Œå…¨å±€åµŒå…¥æ¥é¢„æµ‹æµçº¿ä¼ æ’­æ–¹å‘ï¼Œä»¥è¿›è¡Œç«¯åˆ°ç«¯çš„å¯è®­ç»ƒ tractographyã€‚æˆ‘ä»¬åœ¨å¤šç§ç‹¬ç«‹è·å–çš„dMRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬åˆæˆæ•°æ®å’Œä¸´åºŠæ•°æ®ã€‚åœ¨å…·æœ‰åŸºå‡†çœŸå®å€¼ï¼ˆISMRM Challengeå’ŒTractoInfernoï¼‰çš„ä¸¤ä¸ªå…¬è®¤åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDDTrackingåœ¨æ€»ä½“ä¸Šå¤§å¤§ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„tractographyæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒDDTrackingåœ¨ä¸åŒæ•°æ®é›†ä¸Šå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ¶µç›–å„ç§å¥åº·çŠ¶å†µã€å¹´é¾„ç»„ã€æˆåƒåè®®å’Œæ‰«æä»ªç±»å‹ã€‚æ€»ä¹‹ï¼ŒDDTrackingæä¾›äº†è§£å‰–ä¸Šåˆç†ä¸”ç¨³å¥çš„tractographyï¼Œä¸ºå¹¿æ³›çš„dMRIåº”ç”¨æä¾›äº†å¯æ‰©å±•ã€å¯é€‚åº”å’Œç«¯åˆ°ç«¯çš„å¯å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/yishengpoxiao/DDtracking.git">https://github.com/yishengpoxiao/DDtracking.git</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04568v1">PDF</a> Preprint version. The content may be updated in the future</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºDDTrackingï¼Œä¸€ç§ç”¨äºæ‰©æ•£MRIè½¨è¿¹è¿½è¸ªçš„æ–°å‹æ·±åº¦ç”Ÿæˆæ¡†æ¶ã€‚å®ƒå°†æµçº¿ä¼ æ’­è¡¨è¿°ä¸ºæ¡ä»¶å»å™ªæ‰©æ•£è¿‡ç¨‹ï¼Œå¼•å…¥åŒè·¯å¾„ç¼–ç ç½‘ç»œï¼ŒåŒæ—¶å»ºæ¨¡å±€éƒ¨ç©ºé—´ç¼–ç å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§ã€‚é€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¨¡å—ï¼Œåˆ©ç”¨å­¦ä¹ å’Œå…¨å±€åµŒå…¥é¢„æµ‹æµçº¿ä¼ æ’­æ–¹å‘ï¼Œå®ç°ç«¯å¯¹ç«¯è®­ç»ƒã€‚åœ¨å¤šç§ç‹¬ç«‹è·å–çš„dMRIæ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒDDTrackingæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„è½¨è¿¹è¿½è¸ªæ–¹æ³•ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå¹¿æ³›çš„dMRIåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDTrackingæ˜¯ä¸€ä¸ªç”¨äºæ‰©æ•£MRIè½¨è¿¹è¿½è¸ªçš„æ·±åº¦ç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>æµçº¿ä¼ æ’­è¢«è¡¨è¿°ä¸ºæ¡ä»¶å»å™ªæ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥åŒè·¯å¾„ç¼–ç ç½‘ç»œï¼ŒåŒæ—¶å»ºæ¨¡å±€éƒ¨ç©ºé—´ç¼–ç å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¨¡å—åˆ©ç”¨å­¦ä¹ å’Œå…¨å±€åµŒå…¥é¢„æµ‹æµçº¿ä¼ æ’­æ–¹å‘ã€‚</li>
<li>DDTrackingåœ¨å¤šç§dMRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†ç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬åˆæˆå’Œä¸´åºŠæ•°æ®ã€‚</li>
<li>DDTrackingæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„è½¨è¿¹è¿½è¸ªæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bad5ca3ffb26bcf49ee7307b77308bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1067a71654cf0c57db98a75f145334c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22436357bcbe02497999258277c39f17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3bc53771ebff9e5dc00cfc658bd894a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c827ae95e58298e3a678f5bdb295de8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Augmentation-based-Domain-Generalization-and-Joint-Training-from-Multiple-Source-Domains-for-Whole-Heart-Segmentation"><a href="#Augmentation-based-Domain-Generalization-and-Joint-Training-from-Multiple-Source-Domains-for-Whole-Heart-Segmentation" class="headerlink" title="Augmentation-based Domain Generalization and Joint Training from   Multiple Source Domains for Whole Heart Segmentation"></a>Augmentation-based Domain Generalization and Joint Training from   Multiple Source Domains for Whole Heart Segmentation</h2><p><strong>Authors:Franz Thaler, Darko Stern, Gernot Plank, Martin Urschler</strong></p>
<p>As the leading cause of death worldwide, cardiovascular diseases motivate the development of more sophisticated methods to analyze the heart and its substructures from medical images like Computed Tomography (CT) and Magnetic Resonance (MR). Semantic segmentations of important cardiac structures that represent the whole heart are useful to assess patient-specific cardiac morphology and pathology. Furthermore, accurate semantic segmentations can be used to generate cardiac digital twin models which allows e.g. electrophysiological simulation and personalized therapy planning. Even though deep learning-based methods for medical image segmentation achieved great advancements over the last decade, retaining good performance under domain shift â€“ i.e. when training and test data are sampled from different data distributions â€“ remains challenging. In order to perform well on domains known at training-time, we employ a (1) balanced joint training approach that utilizes CT and MR data in equal amounts from different source domains. Further, aiming to alleviate domain shift towards domains only encountered at test-time, we rely on (2) strong intensity and spatial augmentation techniques to greatly diversify the available training data. Our proposed whole heart segmentation method, a 5-fold ensemble with our contributions, achieves the best performance for MR data overall and a performance similar to the best performance for CT data when compared to a model trained solely on CT. With 93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR data, our method demonstrates great potential to efficiently obtain accurate semantic segmentations from which patient-specific cardiac twin models can be generated. </p>
<blockquote>
<p>ä½œä¸ºå…¨ä¸–ç•Œçš„ä¸»è¦æ­»äº¡åŸå› ï¼Œå¿ƒè¡€ç®¡ç–¾ç—…ä¿ƒä½¿äº†æ›´ç²¾ç»†çš„æ–¹æ³•çš„å‘å±•ï¼Œç”¨äºä»è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œç£å…±æŒ¯ï¼ˆMRï¼‰ç­‰åŒ»å­¦å›¾åƒä¸­åˆ†æå¿ƒè„åŠå…¶å­ç»“æ„ã€‚è¡¨ç¤ºæ•´ä¸ªå¿ƒè„çš„é‡è¦å¿ƒè„ç»“æ„çš„è¯­ä¹‰åˆ†å‰²å¯¹äºè¯„ä¼°æ‚£è€…ç‰¹å®šçš„å¿ƒè„å½¢æ€å’Œç—…ç†éå¸¸æœ‰ç”¨ã€‚æ­¤å¤–ï¼Œå‡†ç¡®çš„è¯­ä¹‰åˆ†å‰²å¯ç”¨äºç”Ÿæˆå¿ƒè„æ•°å­—å­ªç”Ÿæ¨¡å‹ï¼Œè¿™å¯ä»¥è¿›è¡Œä¾‹å¦‚ç”µç”Ÿç†æ¨¡æ‹Ÿå’Œä¸ªæ€§åŒ–æ²»ç–—è®¡åˆ’ã€‚å°½ç®¡åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•åœ¨è¿‡å»åå¹´ä¸­å–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œä½†åœ¨é¢†åŸŸè¿ç§»ï¼ˆå³å½“è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ¥è‡ªä¸åŒçš„æ•°æ®åˆ†å¸ƒæ—¶ï¼‰çš„æƒ…å†µä¸‹ä¿æŒè‰¯å¥½çš„æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†åœ¨å¯¹è®­ç»ƒæ—¶æœ‰åŸŸçŸ¥è¯†çš„é¢†åŸŸä¸Šè¡¨ç°è‰¯å¥½ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ï¼ˆ1ï¼‰å¹³è¡¡è”åˆè®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¹³ç­‰åœ°åˆ©ç”¨æ¥è‡ªä¸åŒæºåŸŸçš„CTå’ŒMRæ•°æ®ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£ä»…åœ¨æµ‹è¯•æ—¶é‡åˆ°çš„é¢†åŸŸè¿ç§»é—®é¢˜ï¼Œæˆ‘ä»¬ä¾èµ–äºï¼ˆ2ï¼‰å¼ºå¤§çš„å¼ºåº¦å’Œç©ºé—´å¢å¼ºæŠ€æœ¯æ¥æå¤§åœ°å¤šæ ·åŒ–å¯ç”¨çš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æå‡ºçš„æ•´ä¸ªå¿ƒè„åˆ†å‰²æ–¹æ³•æ˜¯ä¸€ä¸ªäº”é‡é›†æˆæ–¹æ³•ï¼Œå…¶ä¸­åŒ…å«æˆ‘ä»¬çš„è´¡çŒ®ï¼Œåœ¨MRæ•°æ®ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸ä»…ä½¿ç”¨CTæ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”æ—¶ï¼Œå¯¹äºCTæ•°æ®ä¹Ÿè¾¾åˆ°äº†ç±»ä¼¼æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨CTæ•°æ®ä¸Šè¾¾åˆ°äº†93.33%çš„DSCå’Œ0.8388æ¯«ç±³çš„ASSDï¼Œåœ¨MRæ•°æ®ä¸Šè¾¾åˆ°äº†89.30%çš„DSCå’Œ1.2411æ¯«ç±³çš„ASSDï¼Œæ˜¾ç¤ºå‡ºä»å‡†ç¡®çš„è¯­ä¹‰åˆ†å‰²ä¸­æœ‰æ•ˆè·å¾—æ‚£è€…ç‰¹å®šå¿ƒè„å­ªç”Ÿæ¨¡å‹çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04552v1">PDF</a> Accepted for the MICCAI Challenge on Comprehensive Analysis and   Computing of Real-World Medical Images 2024, 12 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¿ƒè¡€ç®¡ç–¾ç—…çš„åŒ»å­¦å›¾åƒåˆ†æï¼Œç‰¹åˆ«æ˜¯å¿ƒè„ç»“æ„çš„è¯­ä¹‰åˆ†å‰²ã€‚æ–‡ç« ä»‹ç»äº†ä½¿ç”¨å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç»“åˆè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œç£å…±æŒ¯ï¼ˆMRï¼‰æ•°æ®ï¼Œè¿›è¡Œå¿ƒè„ç»“æ„åˆ†å‰²çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶é€šè¿‡å¹³è¡¡è”åˆè®­ç»ƒæ–¹æ³•å’Œå¼ºåº¦ä¸ç©ºé—´å¢å¼ºçš„æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸæ•°æ®ä¸Šçš„è¡¨ç°ã€‚æå‡ºçš„å…¨å¿ƒè„åˆ†å‰²æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç£å…±æŒ¯æ•°æ®ä¸Šï¼Œå¹¶å±•ç¤ºäº†ç”Ÿæˆä¸ªæ€§åŒ–å¿ƒè„åŒèƒèƒæ¨¡å‹çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¿ƒè¡€ç®¡ç–¾ç—…æ˜¯å…¨çƒä¸»è¦æ­»äº¡åŸå› ï¼Œä¿ƒä½¿å¼€å‘ä»åŒ»å­¦å›¾åƒï¼ˆå¦‚CTå’ŒMRï¼‰åˆ†æå¿ƒè„åŠå…¶å­ç»“æ„çš„æ›´å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>å¿ƒè„ç»“æ„çš„è¯­ä¹‰åˆ†å‰²æœ‰åŠ©äºè¯„ä¼°æ‚£è€…ç‰¹å®šçš„å¿ƒè„å½¢æ€å’Œç—…ç†ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨é¢†åŸŸè½¬ç§»ï¼ˆå³è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ¥è‡ªä¸åŒåˆ†å¸ƒï¼‰çš„æƒ…å†µä¸‹ä¿æŒæ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¹³è¡¡è”åˆè®­ç»ƒæ–¹æ³•å’Œå¼ºåº¦ä¸ç©ºé—´å¢å¼ºæŠ€æœ¯ï¼Œè¯¥ç ”ç©¶æé«˜äº†æ¨¡å‹åœ¨å·²çŸ¥å’ŒæœªçŸ¥é¢†åŸŸæ•°æ®ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æå‡ºçš„å…¨å¿ƒè„åˆ†å‰²æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç£å…±æŒ¯æ•°æ®ä¸Šï¼Œæ€§èƒ½å“è¶Šï¼Œä¸ä»…ä½¿ç”¨CTæ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰ç”Ÿæˆæ‚£è€…ç‰¹å¼‚æ€§å¿ƒè„åŒèƒèƒæ¨¡å‹çš„æ½œåŠ›ï¼Œè¿™å¯¹äºä¸ªæ€§åŒ–æ²»ç–—è§„åˆ’å’Œç”µç”Ÿç†æ¨¡æ‹Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3627d21175e6d3443e815fec01b212b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c2d2a9219c4fe47c44c03fe60156bad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e44fb05bf1bc882860a823734edd136.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99a6c330905f920f8a1d446fa3c3da9a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Conditional-Fetal-Brain-Atlas-Learning-for-Automatic-Tissue-Segmentation"><a href="#Conditional-Fetal-Brain-Atlas-Learning-for-Automatic-Tissue-Segmentation" class="headerlink" title="Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation"></a>Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation</h2><p><strong>Authors:Johannes Tischer, Patric Kienast, Marlene StÃ¼mpflen, Gregor Kasprian, Georg Langs, Roxane Licandro</strong></p>
<p>Magnetic Resonance Imaging (MRI) of the fetal brain has become a key tool for studying brain development in vivo. Yet, its assessment remains challenging due to variability in brain maturation, imaging protocols, and uncertain estimates of Gestational Age (GA). To overcome these, brain atlases provide a standardized reference framework that facilitates objective evaluation and comparison across subjects by aligning the atlas and subjects in a common coordinate system. In this work, we introduce a novel deep-learning framework for generating continuous, age-specific fetal brain atlases for real-time fetal brain tissue segmentation. The framework combines a direct registration model with a conditional discriminator. Trained on a curated dataset of 219 neurotypical fetal MRIs spanning from 21 to 37 weeks of gestation. The method achieves high registration accuracy, captures dynamic anatomical changes with sharp structural detail, and robust segmentation performance with an average Dice Similarity Coefficient (DSC) of 86.3% across six brain tissues. Furthermore, volumetric analysis of the generated atlases reveals detailed neurotypical growth trajectories, providing valuable insights into the maturation of the fetal brain. This approach enables individualized developmental assessment with minimal pre-processing and real-time performance, supporting both research and clinical applications. The model code is available at <a target="_blank" rel="noopener" href="https://github.com/cirmuw/fetal-brain-atlas">https://github.com/cirmuw/fetal-brain-atlas</a> </p>
<blockquote>
<p>èƒå„¿è„‘éƒ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å·²æˆä¸ºç ”ç©¶è„‘å‘è‚²çš„å…³é”®å·¥å…·ã€‚ç„¶è€Œï¼Œç”±äºè„‘éƒ¨æˆç†Ÿçš„å·®å¼‚æ€§ã€æˆåƒåè®®çš„ä¸ç¡®å®šæ€§ä»¥åŠå¦Šå¨ æœŸå¹´é¾„ï¼ˆGAï¼‰çš„ä¼°ç®—ä¸ç¡®å®šï¼Œå…¶è¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œè„‘å›¾è°±æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„å‚è€ƒæ¡†æ¶ï¼Œé€šè¿‡åœ¨ä¸€ä¸ªå…±åŒçš„åæ ‡ç³»ä¸­å¯¹å›¾è°±å’Œå—è¯•è€…è¿›è¡Œå¯¹é½ï¼Œä»è€Œä¾¿äºè¿›è¡Œè·¨å—è¯•è€…ä¹‹é—´çš„å®¢è§‚è¯„ä¼°å’Œæ¯”è¾ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆè¿ç»­ã€å¹´é¾„ç‰¹å®šçš„èƒå„¿è„‘å›¾è°±ï¼Œä»¥è¿›è¡Œå®æ—¶èƒå„¿è„‘ç»„ç»‡åˆ†å‰²ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸€ä¸ªç›´æ¥æ³¨å†Œæ¨¡å‹å’Œä¸€ä¸ªæ¡ä»¶é‰´åˆ«å™¨ã€‚è¯¥æ¡†æ¶åœ¨ç”±21è‡³37å‘¨å¦Šå¨ çš„ç¥ç»å…¸å‹èƒå„¿MRIç»„æˆçš„ç²¾é€‰æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•å®ç°äº†é«˜æ³¨å†Œç²¾åº¦ï¼Œèƒ½å¤Ÿæ•æ‰åŠ¨æ€è§£å‰–ç»“æ„å˜åŒ–å¹¶æ˜¾ç¤ºå°–é”çš„ç»“æ„ç»†èŠ‚ï¼Œå¹¶ä¸”åœ¨å…­ç§è„‘ç»„ç»‡ä¸Šå®ç°äº†å¹³å‡Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸º86.3%çš„ç¨³å¥åˆ†å‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„å›¾è°±çš„ä½“ç§¯åˆ†ææ­ç¤ºäº†è¯¦ç»†çš„ç¥ç»å…¸å‹ç”Ÿé•¿è½¨è¿¹ï¼Œä¸ºèƒå„¿è„‘éƒ¨æˆç†Ÿæä¾›äº†å®è´µçš„è§è§£ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå®ç°ä¸ªæ€§åŒ–çš„å‘è‚²è¯„ä¼°ï¼Œå…·æœ‰æœ€å°çš„é¢„å¤„ç†å’Œå®æ—¶æ€§èƒ½ï¼Œæ—¢æ”¯æŒç ”ç©¶ä¹Ÿæ”¯æŒä¸´åºŠåº”ç”¨ã€‚æ¨¡å‹ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/cirmuw/fetal-brain-atlas">https://github.com/cirmuw/fetal-brain-atlas</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04522v1">PDF</a> 12 pages, 4 figures, MICCAI Workshop on Perinatal Imaging, Placental   and Preterm Image analysis</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ çš„æ–°å‹èƒå„¿è„‘å›¾è°±ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨ç”¨äºèƒå„¿è„‘ç»„ç»‡åˆ†å‰²å’Œä¸ªä½“åŒ–å‘è‚²è¯„ä¼°ã€‚æ­¤æ¡†æ¶ç»“åˆäº†ç›´æ¥æ³¨å†Œæ¨¡å‹å’Œæ¡ä»¶åˆ¤åˆ«å™¨ï¼Œå¯å®ç°é«˜æ•ˆå‡†ç¡®çš„æ³¨å†Œã€ç²¾ç»†çš„è§£å‰–ç»“æ„æ•æ‰ä»¥åŠç¨³å¥çš„åˆ†å‰²æ€§èƒ½ã€‚åŒæ—¶ï¼Œé€šè¿‡ç”Ÿæˆçš„å›¾è°±å¯è¯¦ç»†äº†è§£èƒå„¿ç¥ç»ç³»ç»Ÿçš„æ­£å¸¸å‘è‚²è½¨è¿¹ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å…·æœ‰ç®€æ´å¿«é€Ÿçš„å®æ—¶æ€§èƒ½ï¼Œå¯ç”¨äºä¸´åºŠå’Œç ”ç©¶åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>èƒå„¿è„‘MRIæˆåƒå·²æˆä¸ºç ”ç©¶è„‘å‘è‚²çš„å…³é”®å·¥å…·ï¼Œä½†è¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>è„‘å›¾è°±æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„å‚è€ƒæ¡†æ¶ï¼Œæœ‰åŠ©äºå®¢è§‚è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒå—è¯•è€…ä¹‹é—´çš„æ•°æ®ã€‚</li>
<li>æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ç»“åˆäº†ç›´æ¥æ³¨å†Œæ¨¡å‹å’Œæ¡ä»¶åˆ¤åˆ«å™¨ï¼Œç”¨äºç”Ÿæˆè¿ç»­ã€ç‰¹å®šå¹´é¾„çš„èƒå„¿è„‘å›¾è°±ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†é«˜æ³¨å†Œç²¾åº¦å’Œç²¾ç»†çš„ç»“æ„ç»†èŠ‚æ•æ‰ï¼Œå¹³å‡Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸º86.3%ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆçš„å›¾è°±ï¼Œå¯è¯¦ç»†äº†è§£èƒå„¿ç¥ç»ç³»ç»Ÿçš„æ­£å¸¸å‘è‚²è½¨è¿¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5131ab2baca157f1e0c122ee7fbb43b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bd2d1f6f491ee01f372ca9f987dacf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd46d5ba47755fedfa190eaf0fae0549.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c136f04b6706a9cdc6c8ca6e1a835096.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TotalRegistrator-Towards-a-Lightweight-Foundation-Model-for-CT-Image-Registration"><a href="#TotalRegistrator-Towards-a-Lightweight-Foundation-Model-for-CT-Image-Registration" class="headerlink" title="TotalRegistrator: Towards a Lightweight Foundation Model for CT Image   Registration"></a>TotalRegistrator: Towards a Lightweight Foundation Model for CT Image   Registration</h2><p><strong>Authors:Xuan Loc Pham, Gwendolyn Vuurberg, Marjan Doppen, Joey Roosen, Tip Stille, Thi Quynh Ha, Thuy Duong Quach, Quoc Vu Dang, Manh Ha Luu, Ewoud J. Smit, Hong Son Mai, Mattias Heinrich, Bram van Ginneken, Mathias Prokop, Alessa Hering</strong></p>
<p>Image registration is a fundamental technique in the analysis of longitudinal and multi-phase CT images within clinical practice. However, most existing methods are tailored for single-organ applications, limiting their generalizability to other anatomical regions. This work presents TotalRegistrator, an image registration framework capable of aligning multiple anatomical regions simultaneously using a standard UNet architecture and a novel field decomposition strategy. The model is lightweight, requiring only 11GB of GPU memory for training. To train and evaluate our method, we constructed a large-scale longitudinal dataset comprising 695 whole-body (thorax-abdomen-pelvic) paired CT scans from individual patients acquired at different time points. We benchmarked TotalRegistrator against a generic classical iterative algorithm and a recent foundation model for image registration. To further assess robustness and generalizability, we evaluated our model on three external datasets: the public thoracic and abdominal datasets from the Learn2Reg challenge, and a private multiphase abdominal dataset from a collaborating hospital. Experimental results on the in-house dataset show that the proposed approach generally surpasses baseline methods in multi-organ abdominal registration, with a slight drop in lung alignment performance. On out-of-distribution datasets, it achieved competitive results compared to leading single-organ models, despite not being fine-tuned for those tasks, demonstrating strong generalizability. The source code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/DIAGNijmegen/oncology_image_registration.git">https://github.com/DIAGNijmegen/oncology_image_registration.git</a>. </p>
<blockquote>
<p>å›¾åƒé…å‡†æ˜¯ä¸´åºŠå®è·µä¸­å¯¹çºµå‘å’Œå¤šé˜¶æ®µCTå›¾åƒè¿›è¡Œåˆ†æçš„åŸºæœ¬æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½æ˜¯é’ˆå¯¹å•å™¨å®˜åº”ç”¨è€Œå®šåˆ¶çš„ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å…¶å®ƒè§£å‰–åŒºåŸŸçš„åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†TotalRegistratorï¼Œè¿™æ˜¯ä¸€ç§å›¾åƒé…å‡†æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨æ ‡å‡†UNetæ¶æ„å’Œä¸€ç§æ–°çš„åœºåˆ†è§£ç­–ç•¥ï¼Œèƒ½å¤ŸåŒæ—¶å¯¹å‡†å¤šä¸ªè§£å‰–åŒºåŸŸã€‚è¯¥æ¨¡å‹è½»é‡çº§ï¼Œåªéœ€11GBçš„GPUå†…å­˜å³å¯è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çºµå‘æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªåŒä¸€æ‚£è€…åœ¨ä¸åŒæ—¶é—´ç‚¹é‡‡é›†çš„695ä¸ªå…¨èº«ï¼ˆèƒ¸è…¹ç›†ï¼‰é…å¯¹CTæ‰«æã€‚æˆ‘ä»¬å°†TotalRegistratorä¸ä¸€ç§é€šç”¨çš„ç»å…¸è¿­ä»£ç®—æ³•å’Œä¸€ç§æœ€æ–°çš„å›¾åƒé…å‡†åŸºç¡€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°å…¶é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªå¤–éƒ¨æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼šæ¥è‡ªLearn2RegæŒ‘æˆ˜çš„å…¬å…±èƒ¸éƒ¨å’Œè…¹éƒ¨æ•°æ®é›†ï¼Œä»¥åŠæ¥è‡ªåˆä½œåŒ»é™¢çš„ç§äººå¤šé˜¶æ®µè…¹éƒ¨æ•°æ®é›†ã€‚åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šæ•°å™¨å®˜è…¹éƒ¨é…å‡†æ–¹é¢ï¼Œæ‰€æå‡ºçš„æ–¹æ³•é€šå¸¸è¶…è¿‡äº†åŸºçº¿æ–¹æ³•ï¼Œä½†åœ¨è‚ºå¯¹é½æ–¹é¢çš„æ€§èƒ½ç•¥æœ‰ä¸‹é™ã€‚åœ¨è¶…å‡ºåˆ†å¸ƒçš„æ•°æ®é›†ä¸Šï¼Œå°½ç®¡æ²¡æœ‰è¿›è¡Œé’ˆå¯¹è¿™äº›ä»»åŠ¡çš„å¾®è°ƒï¼Œä½†å®ƒä»ç„¶å–å¾—äº†ä¸é¢†å…ˆçš„å•å™¨å®˜æ¨¡å‹ç«äº‰çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æºä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/DIAGNijmegen/oncology_image_registration.git">https://github.com/DIAGNijmegen/oncology_image_registration.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04450v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å›¾åƒé…å‡†æ˜¯ä¸´åºŠå®è·µä¸­åˆ†æçºµå‘å’Œå¤šé˜¶æ®µCTå›¾åƒçš„åŸºæœ¬æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½æ˜¯é’ˆå¯¹å•å™¨å®˜åº”ç”¨çš„ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å…¶ä»–è§£å‰–åŒºåŸŸçš„æ¨å¹¿ã€‚æœ¬ç ”ç©¶æå‡ºäº†TotalRegistratorï¼Œè¿™æ˜¯ä¸€ç§å›¾åƒé…å‡†æ¡†æ¶ï¼Œé‡‡ç”¨æ ‡å‡†UNetæ¶æ„å’Œæ–°å‹åœºåŸŸåˆ†è§£ç­–ç•¥ï¼Œèƒ½å¤ŸåŒæ—¶å¯¹å‡†å¤šä¸ªè§£å‰–åŒºåŸŸã€‚è¯¥æ¨¡å‹è½»é‡çº§ï¼Œä»…éœ€11GBçš„GPUå†…å­˜è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çºµå‘æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªä¸åŒæ—¶é—´ç‚¹é‡‡é›†çš„695ä¾‹å…¨èº«ï¼ˆèƒ¸è…¹ç›†ï¼‰é…å¯¹CTæ‰«æã€‚æˆ‘ä»¬å°†TotalRegistratorä¸é€šç”¨çš„ç»å…¸è¿­ä»£ç®—æ³•å’Œæœ€è¿‘çš„å›¾åƒé…å‡†åŸºç¡€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°å…¶ç¨³å¥æ€§å’Œé€šç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªå¤–éƒ¨æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼šæ¥è‡ªLearn2RegæŒ‘æˆ˜çš„å…¬å…±èƒ¸éƒ¨å’Œè…¹éƒ¨æ•°æ®é›†ï¼Œä»¥åŠæ¥è‡ªåˆä½œåŒ»é™¢çš„ç§äººå¤šé˜¶æ®µè…¹éƒ¨æ•°æ®é›†ã€‚åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šæ•°å™¨å®˜è…¹éƒ¨é…å‡†æ–¹é¢ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ€»ä½“ä¸Šè¶…è¿‡äº†åŸºçº¿æ–¹æ³•ï¼Œä½†åœ¨è‚ºå¯¹é½æ€§èƒ½æ–¹é¢ç•¥æœ‰ä¸‹é™ã€‚åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šï¼Œå°½ç®¡æœªé’ˆå¯¹è¿™äº›ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä½†ä¸é¢†å…ˆçš„å•ä¸€å™¨å®˜æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æºä»£ç å°†å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/DIAGNijmegen/oncology_image_registration.git%E3%80%82">https://github.com/DIAGNijmegen/oncology_image_registration.gitã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>TotalRegistratoræ˜¯ä¸€ç§å¤šè§£å‰–åŒºåŸŸå›¾åƒé…å‡†æ¡†æ¶ï¼ŒåŸºäºæ ‡å‡†UNetæ¶æ„å’Œæ–°é¢–åœºåŸŸåˆ†è§£ç­–ç•¥ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤ŸåŒæ—¶è¿›è¡Œå¤šåŒºåŸŸé…å‡†ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡çºµå‘æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼ŒåŒ…å«æ¥è‡ªä¸åŒæ—¶é—´ç‚¹çš„å…¨èº«CTæ‰«æã€‚</li>
<li>TotalRegistratoråœ¨å¤šæ•°å™¨å®˜è…¹éƒ¨é…å‡†æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>åœ¨æœªé’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>TotalRegistratoråœ¨è‚ºå¯¹é½æ€§èƒ½æ–¹é¢ç•¥æœ‰ä¸è¶³ï¼Œä½†ä»å…·æœ‰æ½œåŠ›è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚</li>
<li>æºä»£ç å°†å…¬å¼€ä¾›å…¬ä¼—è®¿é—®ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-36e65630087dbed35cd3b40038b4cfcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7720faa6a92639533a967f92b36c4437.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14b79bc28430aaf6b0a41bb4d77d6bb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="3D-Mapping-of-Static-Magnetic-Field-Magnitude-and-AxialComponents-around-a-total-body-3T-MRI-clinical-scanner"><a href="#3D-Mapping-of-Static-Magnetic-Field-Magnitude-and-AxialComponents-around-a-total-body-3T-MRI-clinical-scanner" class="headerlink" title="3D Mapping of Static Magnetic Field Magnitude and AxialComponents around   a total body 3T MRI clinical scanner"></a>3D Mapping of Static Magnetic Field Magnitude and AxialComponents around   a total body 3T MRI clinical scanner</h2><p><strong>Authors:Francesco Girardello, Maria Antonietta Dâ€™Avanzo, Massimo Mattozzi, Victorian Michele Ferro, Giuseppe Acri, Valentina Hartwig</strong></p>
<p>Objective. The technology employed in magnetic resonance imaging (MRI) systems has evolved continuously, resulting in MRI scanners with stronger static magnetic fields (SMF) B0, faster and stronger gradient magnetic fields, and more powerful radiofrequency transmission coils. The most well-known hazard associated with an MRI environment is the projectile effect due to Spatial Field Gradient (SFG). Furthermore, movement through the SFG generates a time-varying magnetic field, which in turn induces a voltage in body tissues. This has the potential to result in a range of physiological symptoms, including headache, nausea, vertigo, phosphenes, numbness, tingling, loss of proprioception, and balance disturbances.   Approach. The methodology outlined in this study provides a comprehensive and reliable approach to creating a 3D map of the SMF (fringe field) around a clinical MRI facility. The methodology involves measuring the unperturbed B field, including magnitude and axial components, in specific points and subsequently performing a mathematical procedure involving fitting and interpolation.   Main results. Fringe field magnitude and axial components 3D maps are presented for a 3T whole-body MRI scanner for clinical application located in a hospital facility.   Significance. The map obtained could be used for a number of purposes, including the evaluation of hazard. This could be achieved by using digital tools to create a simulation of all types of MRI workers movements within the facility. The map could also be used for the training and education of MRI operators, with a view to establishing best practices. The estimation of magnetic field axial components represents a valuable enhancement, as these data can be used to calculate induced electric fields during rotational movements, such as those of the head or torso. </p>
<blockquote>
<p><strong>ç›®æ ‡</strong>ï¼šç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç³»ç»Ÿæ‰€é‡‡ç”¨çš„æŠ€æœ¯ä¸æ–­å‘å±•ï¼Œå¯¼è‡´MRIæ‰«æä»ªçš„é™æ€ç£åœºï¼ˆSMFï¼‰B0æ›´å¼ºã€æ¢¯åº¦ç£åœºæ›´å¿«æ›´å¼ºï¼Œå°„é¢‘ä¼ è¾“çº¿åœˆçš„æ€§èƒ½ä¹Ÿæ›´å¼ºå¤§ã€‚ä¸MRIç¯å¢ƒç›¸å…³çš„æœ€çŸ¥åçš„å±å®³æ˜¯ç”±äºç©ºé—´åœºæ¢¯åº¦ï¼ˆSFGï¼‰å¼•èµ·çš„æŠ•å°„æ•ˆåº”ã€‚æ­¤å¤–ï¼Œé€šè¿‡SFGçš„ç§»åŠ¨ä¼šäº§ç”Ÿæ—¶å˜ç£åœºï¼Œè¿›è€Œåœ¨äººä½“ç»„ç»‡ä¸­æ„Ÿåº”å‡ºç”µå‹ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´ä¸€ç³»åˆ—ç”Ÿç†ç—‡çŠ¶ï¼ŒåŒ…æ‹¬å¤´ç—›ã€æ¶å¿ƒã€çœ©æ™•ã€çœ¼å†’é‡‘æ˜Ÿã€éº»æœ¨ã€åˆºç—›ã€å¤±å»ä½ç½®æ„Ÿå’Œå¹³è¡¡å¤±è°ƒã€‚</p>
</blockquote>
<p><strong>æ–¹æ³•</strong>ï¼šæœ¬ç ”ç©¶ä¸­æ¦‚è¿°çš„æ–¹æ³•æä¾›äº†ä¸€ä¸ªå…¨é¢å¯é çš„æ–¹æ³•æ¥åˆ›å»ºä¸´åºŠMRIè®¾æ–½å‘¨å›´SMFï¼ˆè¾¹ç¼˜åœºï¼‰çš„3Dåœ°å›¾ã€‚è¯¥æ–¹æ³•æ¶‰åŠæµ‹é‡ç‰¹å®šç‚¹çš„æœªå—å¹²æ‰°çš„Båœºï¼ŒåŒ…æ‹¬å¹…åº¦å’Œè½´å‘åˆ†é‡ï¼Œéšåè¿›è¡Œæ¶‰åŠæ‹Ÿåˆå’Œæ’å€¼çš„æ•°å­¦ç¨‹åºã€‚</p>
<p><strong>ä¸»è¦ç»“æœ</strong>ï¼šé’ˆå¯¹ä½äºåŒ»é™¢è®¾æ–½çš„ç”¨äºä¸´åºŠåº”ç”¨çš„3Tå…¨èº«MRIæ‰«æä»ªï¼Œå‘ˆç°äº†è¾¹ç¼˜åœºå¹…åº¦å’Œè½´å‘åˆ†é‡çš„3Dåœ°å›¾ã€‚</p>
<p><strong>æ„ä¹‰</strong>ï¼šæ‰€è·å¾—çš„åœ°å›¾å¯ç”¨äºå¤šç§ç”¨é€”ï¼ŒåŒ…æ‹¬å±é™©è¯„ä¼°ã€‚è¿™å¯ä»¥é€šè¿‡ä½¿ç”¨æ•°å­—å·¥å…·æ¨¡æ‹ŸMRIè®¾æ–½å†…æ‰€æœ‰ç±»å‹çš„å·¥äººç§»åŠ¨æ¥å®ç°ã€‚è¯¥åœ°å›¾ä¹Ÿå¯ç”¨äºMRIæ“ä½œå‘˜çš„åŸ¹è®­å’Œæ•™è‚²ï¼Œç›®çš„æ˜¯å»ºç«‹æœ€ä½³å®è·µã€‚ä¼°è®¡ç£åœºè½´å‘åˆ†é‡æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„å¢å¼ºï¼Œå› ä¸ºè¿™äº›æ•°æ®å¯ç”¨äºè®¡ç®—æ—‹è½¬è¿åŠ¨ï¼ˆå¦‚å¤´éƒ¨æˆ–èº¯å¹²ï¼‰æœŸé—´æ„Ÿåº”çš„ç”µåœºã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04432v1">PDF</a> 13 pages, 10 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç³»ç»Ÿçš„æŠ€æœ¯è¿›æ­¥ï¼ŒåŒ…æ‹¬æ›´å¼ºçš„é™æ€ç£åœºï¼ˆSMFï¼‰B0ã€æ›´å¿«çš„æ¢¯åº¦ç£åœºå’Œæ›´å¼ºå¤§çš„å°„é¢‘ä¼ è¾“çº¿åœˆçš„åº”ç”¨ã€‚é‡ç‚¹é˜è¿°äº†å› ç©ºé—´åœºæ¢¯åº¦ï¼ˆSFGï¼‰è€Œäº§ç”Ÿçš„é¡¹ç›®å°„æ•ˆåº”å’Œéšæ—¶é—´å˜åŒ–çš„ç£åœºå¯¹äººä½“çš„å½±å“ï¼Œå¹¶å¯èƒ½å¼•å‘ä¸€ç³»åˆ—ç”Ÿç†ç—‡çŠ¶ã€‚ç ”ç©¶äº†ä¸€ç§ä¸ºä¸´åºŠMRIè®¾æ–½å‘¨å›´åˆ›å»ºSMFï¼ˆè¾¹ç¼˜åœºï¼‰ä¸‰ç»´åœ°å›¾çš„å¯é æ–¹æ³•ï¼Œå¹¶å¯¹ç»“æœè¿›è¡Œäº†ä»‹ç»ã€‚æ­¤åœ°å›¾å¯åº”ç”¨äºè¯„ä¼°å±å®³ã€è®­ç»ƒå’Œæ•™è‚²MRIæ“ä½œäººå‘˜ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIæŠ€æœ¯ä¸æ–­è¿›æ­¥ï¼ŒåŒ…æ‹¬æ›´å¼ºçš„é™æ€ç£åœºï¼ˆSMFï¼‰ã€æ›´å¿«çš„æ¢¯åº¦ç£åœºå’Œæ›´å¼ºå¤§çš„å°„é¢‘ä¼ è¾“çº¿åœˆçš„åº”ç”¨ã€‚</li>
<li>ç©ºé—´åœºæ¢¯åº¦ï¼ˆSFGï¼‰å¯¼è‡´çš„é¡¹ç›®å°„æ•ˆåº”æ˜¯MRIç¯å¢ƒä¸­æœ€ä¸ºäººçŸ¥çš„é£é™©ã€‚</li>
<li>äººä½“é€šè¿‡SFGäº§ç”Ÿçš„éšæ—¶é—´å˜åŒ–çš„ç£åœºä¼šåœ¨ä½“å†…æ„Ÿåº”å‡ºç”µå‹ï¼Œå¯èƒ½å¯¼è‡´ä¸€ç³»åˆ—ç”Ÿç†ç—‡çŠ¶ã€‚</li>
<li>ç ”ç©¶æä¾›äº†ä¸€ç§ä¸ºä¸´åºŠMRIè®¾æ–½å‘¨å›´åˆ›å»ºSMFï¼ˆè¾¹ç¼˜åœºï¼‰ä¸‰ç»´åœ°å›¾çš„å¯é æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…æ‹¬æµ‹é‡æœªå—å¹²æ‰°çš„Båœºï¼ˆåŒ…æ‹¬å¤§å°å’Œè½´å‘åˆ†é‡ï¼‰åœ¨ç‰¹å®šç‚¹ï¼Œç„¶åè¿›è¡Œæ•°å­¦æ‹Ÿåˆå’Œæ’å€¼è¿‡ç¨‹ã€‚</li>
<li>è·å¾—äº†é€‚ç”¨äºåŒ»é™¢è®¾æ–½ä¸­ç”¨äºä¸´åºŠåº”ç”¨çš„3Tå…¨èº«MRIæ‰«æä»ªçš„è¾¹ç¼˜åœºå¤§å°å’Œè½´å‘åˆ†é‡çš„ä¸‰ç»´åœ°å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04432">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e8f6d26c6ec75e6a93379f452f89e1a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unmasking-Interstitial-Lung-Diseases-Leveraging-Masked-Autoencoders-for-Diagnosis"><a href="#Unmasking-Interstitial-Lung-Diseases-Leveraging-Masked-Autoencoders-for-Diagnosis" class="headerlink" title="Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for   Diagnosis"></a>Unmasking Interstitial Lung Diseases: Leveraging Masked Autoencoders for   Diagnosis</h2><p><strong>Authors:Ethan Dack, Lorenzo Brigato, Vasilis Dedousis, Janine Gote-Schniering,  Cheryl, Hanno Hoppe, Aristomenis Exadaktylos, Manuela Funke-Chambour, Thomas Geiser, Andreas Christe, Lukas Ebner, Stavroula Mougiakakou</strong></p>
<p>Masked autoencoders (MAEs) have emerged as a powerful approach for pre-training on unlabelled data, capable of learning robust and informative feature representations. This is particularly advantageous in diffused lung disease research, where annotated imaging datasets are scarce. To leverage this, we train an MAE on a curated collection of over 5,000 chest computed tomography (CT) scans, combining in-house data with publicly available scans from related conditions that exhibit similar radiological patterns, such as COVID-19 and bacterial pneumonia. The pretrained MAE is then fine-tuned on a downstream classification task for diffused lung disease diagnosis. Our findings demonstrate that MAEs can effectively extract clinically meaningful features and improve diagnostic performance, even in the absence of large-scale labelled datasets. The code and the models are available here: <a target="_blank" rel="noopener" href="https://github.com/eedack01/lung_masked_autoencoder">https://github.com/eedack01/lung_masked_autoencoder</a>. </p>
<blockquote>
<p>æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEsï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ— æ ‡ç­¾æ•°æ®é¢„è®­ç»ƒæ–¹æ³•å·²ç»å´­éœ²å¤´è§’ï¼Œå®ƒèƒ½å¤Ÿå­¦ä¹ ç¨³å¥ä¸”å¯Œæœ‰ä¿¡æ¯é‡çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™åœ¨ç½•è§è‚ºç—…ç ”ç©¶ä¸­å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œå› ä¸ºç›¸å…³çš„æ ‡æ³¨æˆåƒæ•°æ®é›†ç¨€ç¼ºã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€ä¼˜åŠ¿ï¼Œæˆ‘ä»¬åœ¨ç²¾é€‰çš„è¶…è¿‡5000ä¸ªèƒ¸éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ‰«ææ•°æ®é›†ä¸Šè®­ç»ƒMAEï¼Œç»“åˆå†…éƒ¨æ•°æ®ä¸å…¬å¼€å¯ç”¨çš„ã€å…·æœ‰ç›¸ä¼¼æ”¾å°„å­¦æ¨¡å¼çš„æ‰«ææ•°æ®ï¼Œå¦‚COVID-19å’Œç»†èŒæ€§è‚ºç‚ç­‰ã€‚ç„¶åï¼Œå¯¹é¢„è®­ç»ƒçš„MAEè¿›è¡Œå¾®è°ƒï¼Œç”¨äºé’ˆå¯¹ç½•è§è‚ºç—…çš„ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡è¿›è¡Œè¯Šæ–­ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒMAEså¯ä»¥æœ‰æ•ˆåœ°æå–ä¸´åºŠæœ‰æ„ä¹‰çš„ç‰¹å¾ï¼Œå³ä½¿åœ¨ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æé«˜è¯Šæ–­æ€§èƒ½ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/eedack01/lung_masked_autoencoder%E3%80%82">https://github.com/eedack01/lung_masked_autoencoderã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04429v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨Masked Autoencodersï¼ˆMAEsï¼‰åœ¨æœªç»æ ‡æ³¨çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚ºéƒ¨ç–¾ç—…ç ”ç©¶ä¸­çš„åº”ç”¨ã€‚é€šè¿‡åœ¨è¶…è¿‡5000ä»½èƒ¸éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ‰«ææ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ç»“åˆå†…éƒ¨æ•°æ®å’Œå…¬å¼€å¯ç”¨çš„ç±»ä¼¼ç–¾ç—…æ‰«ææ•°æ®ï¼Œå¦‚COVID-19å’Œç»†èŒæ€§è‚ºç‚ç­‰ï¼ŒMAEæ¨¡å‹è¢«ç”¨äºå¯¹ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡è¿›è¡Œå¾®è°ƒä»¥è¯Šæ–­è‚ºéƒ¨ç–¾ç—…ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ²¡æœ‰å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼ŒMAEsä¹Ÿèƒ½æœ‰æ•ˆåœ°æå–ä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„ç‰¹å¾å¹¶æ”¹å–„è¯Šæ–­æ€§èƒ½ã€‚ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒäºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Masked autoencoders (MAEs)æˆä¸ºæ— æ ‡ç­¾æ•°æ®é¢„è®­ç»ƒçš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>åœ¨è‚ºéƒ¨ç–¾ç—…ç ”ç©¶ä¸­ï¼ŒMAEså¯å­¦ä¹ ç¨³å¥ä¸”å…·ä¿¡æ¯é‡çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>åˆ©ç”¨è¶…è¿‡5000ä»½èƒ¸éƒ¨CTæ‰«ææ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç»“åˆå†…éƒ¨æ•°æ®å’Œå…¬å¼€å¯ç”¨æ•°æ®ã€‚</li>
<li>MAEæ¨¡å‹é€šè¿‡å¾®è°ƒç”¨äºè¯Šæ–­è‚ºéƒ¨ç–¾ç—…çš„ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>MAEsèƒ½åœ¨æ— å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„æƒ…å†µä¸‹æå–ä¸´åºŠæœ‰æ„ä¹‰ç‰¹å¾å¹¶æ”¹å–„è¯Šæ–­æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6bc754a974b9d0ed8256a29c8aae1f2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f73e636fa3fde34a27d24f878de765.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1675d535dfdf96bedd67a2f283db400.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-842d0c241c6a45977d98ceecdfd521d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4eae9ef7743ef4bd2b9d6e0139faff9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65e16b7a9327db046d7450c9872361d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6753d23108fb150aff4ebf652b9493f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Discriminating-Distal-Ischemic-Stroke-from-Seizure-Induced-Stroke-Mimics-Using-Dynamic-Susceptibility-Contrast-MRI"><a href="#Discriminating-Distal-Ischemic-Stroke-from-Seizure-Induced-Stroke-Mimics-Using-Dynamic-Susceptibility-Contrast-MRI" class="headerlink" title="Discriminating Distal Ischemic Stroke from Seizure-Induced Stroke Mimics   Using Dynamic Susceptibility Contrast MRI"></a>Discriminating Distal Ischemic Stroke from Seizure-Induced Stroke Mimics   Using Dynamic Susceptibility Contrast MRI</h2><p><strong>Authors:Marijn Borghouts, Richard McKinley, Josien Pluim, Manuel KÃ¶stner, Roland Wiest, Ruisheng Su</strong></p>
<p>Distinguishing acute ischemic strokes (AIS) from stroke mimics (SMs), particularly in cases involving medium and small vessel occlusions, remains a significant diagnostic challenge. While computed tomography (CT) based protocols are commonly used in emergency settings, their sensitivity for detecting distal occlusions is limited. This study explores the potential of magnetic resonance perfusion (MRP) imaging as a tool for differentiating distal AIS from epileptic seizures, a prevalent SM. Using a retrospective dataset of 162 patients (129 AIS, 33 seizures), we extracted region-wise perfusion map descriptors (PMDs) from dynamic susceptibility contrast (DSC) images. Statistical analyses identified several brain regions, located mainly in the temporal and occipital lobe, exhibiting significant group differences in certain PMDs. Hemispheric asymmetry analyses further highlighted these regions as discriminative. A logistic regression model trained on PMDs achieved an area under the receiver operating characteristic (AUROC) curve of 0.90, and an area under the precision recall curve (AUPRC) of 0.74, with a specificity of 92% and a sensitivity of 73%, suggesting strong performance in distinguishing distal AIS from seizures. These findings support further exploration of MRP-based PMDs as interpretable features for distinguishing true strokes from various mimics. The code is openly available at our GitHub <a target="_blank" rel="noopener" href="https://github.com/Marijn311/PMD_extraction_and_analysis%7Bgithub.com/Marijn311/PMD/_extraction/_and/_analysis">https://github.com/Marijn311/PMD_extraction_and_analysis{github.com/Marijn311/PMD\_extraction\_and\_analysis</a> </p>
<blockquote>
<p>æ€¥æ€§ç¼ºè¡€æ€§ä¸­é£ï¼ˆAISï¼‰ä¸ä¸­é£æ¨¡ä»¿è€…ï¼ˆSMsï¼‰ä¹‹é—´çš„åŒºåˆ†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠä¸­å‹å’Œè¡€ç®¡é—­å¡çš„æƒ…å†µä¸‹ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„è¯Šæ–­æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰çš„åè®®åœ¨ç´§æ€¥æƒ…å†µä¸‹å¸¸ç”¨ï¼Œä½†å…¶åœ¨æ£€æµ‹è¿œç«¯é—­å¡æ–¹é¢çš„æ•æ„Ÿæ€§æœ‰é™ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ç£å…±æŒ¯çŒæ³¨ï¼ˆMRPï¼‰æˆåƒåœ¨åŒºåˆ†è¿œç«¯AISä¸å¸¸è§çš„ä¸­é£æ¨¡ä»¿è€…ç™«ç—«å‘ä½œä¸­çš„æ½œåŠ›ã€‚é€šè¿‡å›é¡¾æ€§æ•°æ®é›†ï¼ˆåŒ…æ‹¬129ä¾‹AISå’Œ33ä¾‹ç™«ç—«å‘ä½œï¼‰çš„æ ·æœ¬ï¼Œæˆ‘ä»¬ä»åŠ¨æ€ç£æ•æ„Ÿå¯¹æ¯”ï¼ˆDSCï¼‰å›¾åƒä¸­æå–äº†åŒºåŸŸçŒæ³¨å›¾æè¿°ç¬¦ï¼ˆPMDsï¼‰ã€‚ç»Ÿè®¡åˆ†æç¡®å®šäº†å‡ ä¸ªå¤§è„‘åŒºåŸŸï¼Œä¸»è¦ä½äºé¢å¶å’Œæ•å¶ï¼Œåœ¨æŸäº›PMDsä¸Šå­˜åœ¨æ˜æ˜¾çš„ç»„é—´å·®å¼‚ã€‚åŠçƒä¸å¯¹ç§°åˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†è¿™äº›åŒºåŸŸçš„åŒºåˆ«ã€‚åŸºäºPMDsè®­ç»ƒçš„é€»è¾‘å›å½’æ¨¡å‹è¾¾åˆ°äº†æ¥æ”¶ç‰¹å¾æ›²çº¿ä¸‹çš„åŒºåŸŸï¼ˆAUROCï¼‰ä¸º0.90å’Œç²¾åº¦å¬å›æ›²çº¿ä¸‹çš„åŒºåŸŸï¼ˆAUPRCï¼‰ä¸º0.74ï¼Œç‰¹å¼‚æ€§ä¸º92%ï¼Œæ•æ„Ÿæ€§ä¸º73%ï¼Œåœ¨åŒºåˆ†è¿œç«¯AISä¸ç™«ç—«å‘ä½œæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°æ”¯æŒè¿›ä¸€æ­¥æ¢ç´¢åŸºäºMRPçš„PMDsä½œä¸ºåŒºåˆ†çœŸå®ä¸­é£å’Œå„ç§æ¨¡ä»¿è€…çš„å¯è§£é‡Šç‰¹å¾ã€‚ä»£ç å¯åœ¨æˆ‘ä»¬çš„GitHubä¸Šå…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Marijn311/PMD_extraction_and_analysis%E3%80%82">https://github.com/Marijn311/PMD_extraction_and_analysisã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04404v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ç£å…±æŒ¯çŒæ³¨ï¼ˆMRPï¼‰æˆåƒåœ¨åŒºåˆ†è¿œç«¯æ€¥æ€§ç¼ºè¡€æ€§è„‘å’ä¸­ï¼ˆAISï¼‰ä¸å¸¸è§çš„å’ä¸­æ¨¡ä»¿è€…ï¼ˆå¦‚ç™«ç—«ï¼‰æ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹åŠ¨æ€ç£æ•æ„Ÿå¯¹æ¯”ï¼ˆDSCï¼‰å›¾åƒçš„åŒºåŸŸçŒæ³¨å›¾æè¿°ç¬¦ï¼ˆPMDsï¼‰è¿›è¡Œåˆ†æï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºæŸäº›å¤§è„‘åŒºåŸŸï¼ˆä¸»è¦åœ¨é¢å¶å’Œæ•å¶ï¼‰åœ¨PMDsä¸Šæœ‰æ˜¾è‘—å·®å¼‚ï¼Œè¡¨æ˜è¿™äº›åŒºåŸŸåœ¨åŒºåˆ†AISå’Œç™«ç—«æ—¶å…·æœ‰é‰´åˆ«åŠ›ã€‚åˆ©ç”¨PMDsè®­ç»ƒçš„é€»è¾‘å›å½’æ¨¡å‹è¡¨ç°å‡ºè¾ƒé«˜çš„è¯Šæ–­æ€§èƒ½ï¼Œå¯¹åŒºåˆ†è¿œç«¯AISå’Œç™«ç—«å…·æœ‰è¾ƒå¼ºçš„æ½œåŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜MRPæˆåƒæŠ€æœ¯ä¸ºé‰´åˆ«çœŸæ­£å’ä¸­ä¸ä¸åŒæ¨¡ä»¿è€…çš„æä¾›äº†é‡è¦çš„å¯èƒ½æ€§ã€‚å¯è®¿é—®çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ€¥æ€§ç¼ºè¡€æ€§è„‘å’ä¸­ï¼ˆAISï¼‰ä¸å’ä¸­æ¨¡ä»¿è€…ï¼ˆSMsï¼‰çš„åŒºåˆ†ï¼Œå°¤å…¶åœ¨æ¶‰åŠä¸­å°è¡€ç®¡é—­å¡çš„æƒ…å†µä¸‹ï¼Œä»æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨æ£€æµ‹è¿œç«¯é—­å¡æ–¹é¢çš„çµæ•åº¦æœ‰é™ï¼Œè€Œç£å…±æŒ¯çŒæ³¨ï¼ˆMRPï¼‰æˆåƒæŠ€æœ¯å¯èƒ½æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ›¿ä»£å·¥å…·ã€‚</li>
<li>é€šè¿‡åˆ†æåŠ¨æ€ç£æ•æ„Ÿå¯¹æ¯”ï¼ˆDSCï¼‰å›¾åƒçš„åŒºåŸŸçŒæ³¨å›¾æè¿°ç¬¦ï¼ˆPMDsï¼‰ï¼Œç ”ç©¶å‘ç°æŸäº›å¤§è„‘åŒºåŸŸåœ¨PMDsä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›åŒºåŸŸä¸»è¦ä½äºé¢å¶å’Œæ•å¶ã€‚</li>
<li>é€šè¿‡PMDsè®­ç»ƒçš„é€»è¾‘å›å½’æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„è¯Šæ–­æ€§èƒ½ï¼Œä¸ºåŒºåˆ†è¿œç«¯AISå’Œç™«ç—«æä¾›äº†é‡è¦ä¾æ®ã€‚</li>
<li>è¯¥æ¨¡å‹çš„è¯Šæ–­è¡¨ç°å¯é€šè¿‡ä¸¤ä¸ªå…³é”®æŒ‡æ ‡æ¥è¡¡é‡ï¼šæ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ä¸º0.90ï¼Œç²¾ç¡®å¬å›æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUPRCï¼‰ä¸º0.74ï¼Œç‰¹å¼‚æ€§ä¸º92%ï¼Œæ•æ„Ÿæ€§ä¸º73%ã€‚è¿™äº›ç»“æœæ”¯æŒè¿›ä¸€æ­¥æ¢ç´¢ä½¿ç”¨MRPæˆåƒæŠ€æœ¯æ¥åŒºåˆ†çœŸå®å’ä¸­å’Œå„ç§æ¨¡ä»¿è€…ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†ç£å…±æŒ¯çŒæ³¨æˆåƒåœ¨å’ä¸­è¯Šæ–­ä¸­çš„æ½œåœ¨ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒºåˆ†çœŸæ­£å’ä¸­ä¸ä¸åŒæ¨¡ä»¿è€…æ–¹é¢ã€‚è¿™ä¸ºæœªæ¥çš„å’ä¸­è¯Šæ–­å’Œæ²»ç–—æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bef3e429334dd9dc568c060716baa802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d1c6246fb3146b872463e24b05abda4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b7c1f511af472060dfdb9561c3eb649.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01869cdbb757af0d4bbad30ea7447a30.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VisionTS-Cross-Modal-Time-Series-Foundation-Model-with-Continual-Pre-trained-Visual-Backbones"><a href="#VisionTS-Cross-Modal-Time-Series-Foundation-Model-with-Continual-Pre-trained-Visual-Backbones" class="headerlink" title="VisionTS++: Cross-Modal Time Series Foundation Model with Continual   Pre-trained Visual Backbones"></a>VisionTS++: Cross-Modal Time Series Foundation Model with Continual   Pre-trained Visual Backbones</h2><p><strong>Authors:Lefei Shen, Mouxiang Chen, Xu Liu, Han Fu, Xiaoxue Ren, Jianling Sun, Zhuo Li, Chenghao Liu</strong></p>
<p>Recent studies have revealed that vision models pre-trained on images can perform well in time series forecasting by reformulating forecasting as an image reconstruction task, suggesting their potential as universal time series foundation models. However, effective cross-modal transfer from vision to time series remains challenging due to three key discrepancies: (1) data-modality gap between structured, bounded image data and unbounded, heterogeneous time series; (2) multivariate-forecasting gap between standard RGB three-channel-based vision models and the need to model time series with arbitrary numbers of variates; and (3) probabilistic-forecasting gap between the deterministic output formats of most vision models and the requirement for uncertainty-aware probabilistic predictions. To bridge these gaps, we propose VisionTS++, a vision-model-based TSFM that performs continual pre-training on large-scale time series datasets, including 3 innovations: (1) a vision-model-based filtering mechanism to identify high-quality time series data, thereby mitigating modality gap and improving pre-training stability, (2) a colorized multivariate conversion method that transforms multivariate time series into multi-subfigure RGB images, capturing complex inter-variate dependencies; and (3) a multi-quantile forecasting approach using parallel reconstruction heads to generate forecasts of different quantile levels, thus more flexibly approximating arbitrary output distributions without restrictive prior distributional assumptions. Evaluated on both in-distribution and out-of-distribution TSF benchmarks, \model achieves SOTA results, outperforming specialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12 probabilistic forecasting settings. Our work establishes a new paradigm for cross-modal knowledge transfer, advancing the development of universal TSFMs. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡é‡æ–°åˆ¶å®šé¢„æµ‹ä¸ºå›¾åƒé‡å»ºä»»åŠ¡ï¼Œé¢„å…ˆåœ¨å›¾åƒä¸Šè®­ç»ƒè¿‡çš„è§†è§‰æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­è¡¨ç°è‰¯å¥½ï¼Œè¿™è¡¨æ˜äº†å®ƒä»¬ä½œä¸ºé€šç”¨æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºä¸‰ä¸ªä¸»è¦å·®å¼‚ï¼Œä»è§†è§‰åˆ°æ—¶é—´åºåˆ—çš„æœ‰æ•ˆè·¨æ¨¡æ€è½¬ç§»ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼šï¼ˆ1ï¼‰ç»“æ„åŒ–ã€æœ‰ç•Œå›¾åƒæ•°æ®ä¸æ— ç•Œã€å¼‚è´¨æ—¶é—´åºåˆ—ä¹‹é—´çš„æ•°æ®æ¨¡æ€å·®è·ï¼›ï¼ˆ2ï¼‰åŸºäºæ ‡å‡†RGBä¸‰é€šé“çš„è§†è§‰æ¨¡å‹ä¸éœ€è¦å¯¹ä»»æ„æ•°é‡çš„å˜é‡è¿›è¡Œæ—¶é—´åºåˆ—å»ºæ¨¡ä¹‹é—´çš„å¤šå…ƒé¢„æµ‹å·®è·ï¼›ï¼ˆ3ï¼‰å¤§å¤šæ•°è§†è§‰æ¨¡å‹çš„ç¡®å®šæ€§è¾“å‡ºæ ¼å¼ä¸éœ€è¦äº†è§£ä¸ç¡®å®šæ€§çš„æ¦‚ç‡é¢„æµ‹ä¹‹é—´çš„æ¦‚ç‡é¢„æµ‹å·®è·ã€‚ä¸ºäº†å¼¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†VisionTS++ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§†è§‰æ¨¡å‹çš„TSFMï¼Œå¯ä»¥åœ¨å¤§è§„æ¨¡æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬ä¸‰é¡¹åˆ›æ–°ï¼šï¼ˆ1ï¼‰åŸºäºè§†è§‰æ¨¡å‹çš„è¿‡æ»¤æœºåˆ¶ï¼Œç”¨äºè¯†åˆ«é«˜è´¨é‡çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œä»è€Œå‡è½»æ¨¡æ€å·®è·å¹¶æé«˜é¢„è®­ç»ƒç¨³å®šæ€§ï¼›ï¼ˆ2ï¼‰å½©è‰²å¤šå…ƒè½¬æ¢æ–¹æ³•ï¼Œå°†å¤šå…ƒæ—¶é—´åºåˆ—è½¬æ¢ä¸ºå¤šå­å›¾RGBå›¾åƒï¼Œæ•æ‰å¤æ‚çš„å˜é‡é—´ä¾èµ–å…³ç³»ï¼›ï¼ˆ3ï¼‰ä½¿ç”¨å¹¶è¡Œé‡å»ºå¤´è¿›è¡Œå¤šåˆ†ä½é¢„æµ‹ï¼Œç”Ÿæˆä¸åŒåˆ†ä½æ°´å¹³çš„é¢„æµ‹ï¼Œä»è€Œæ›´çµæ´»åœ°è¿‘ä¼¼ä»»æ„è¾“å‡ºåˆ†å¸ƒï¼Œè€Œæ²¡æœ‰é™åˆ¶æ€§çš„å…ˆéªŒåˆ†å¸ƒå‡è®¾ã€‚åœ¨å†…å¤–åˆ†å¸ƒçš„æ—¶é—´åºåˆ—é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œåœ¨å‡æ–¹è¯¯å·®å‡å°‘æ–¹é¢æ¯”ä¸“é—¨çš„æ—¶é—´åºåˆ—æ¨¡å‹é«˜å‡º6%~44%ï¼Œåœ¨12ä¸ªæ¦‚ç‡é¢„æµ‹è®¾ç½®ä¸­çš„9ä¸ªä¸­æ’åç¬¬ä¸€ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè·¨æ¨¡æ€çŸ¥è¯†è½¬ç§»å»ºç«‹äº†æ–°çš„èŒƒå¼ï¼Œæ¨åŠ¨äº†é€šç”¨TSFMsçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04379v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong><br>     æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒå›¾åƒè§†è§‰æ¨¡å‹é€šè¿‡é‡æ–°åˆ¶å®šæ—¶é—´åºé¢„æµ‹ä¸ºå›¾åƒé‡å»ºä»»åŠ¡ï¼Œåœ¨æ—¶åºé¢„æµ‹ä¸­è¡¨ç°è‰¯å¥½ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºé€šç”¨æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä»è§†è§‰åˆ°æ—¶é—´åºåˆ—çš„æœ‰æ•ˆè·¨æ¨¡æ€è¿ç§»é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ã€‚ä¸ºç¼©å°è¿™äº›å·®è·ï¼Œæå‡ºVisionTS++æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤§å‹æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬ä¸‰é¡¹åˆ›æ–°ï¼šåŸºäºè§†è§‰æ¨¡å‹çš„è¿‡æ»¤æœºåˆ¶ã€å½©è‰²å¤šå…ƒè½¬æ¢æ–¹æ³•å’Œå¤šåˆ†ä½é¢„æµ‹æ–¹æ³•ã€‚è¯¥æ¨¡å‹åœ¨æ—¶åºé¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œç›¸æ¯”ä¸“ä¸šæ—¶åºé¢„æµ‹æ¨¡å‹å‡å°‘äº†6%-44%çš„MSEè¯¯å·®ï¼Œå¹¶åœ¨æ¦‚ç‡é¢„æµ‹è®¾ç½®ä¸­æ’åç¬¬ä¸€ã€‚æœ¬ç ”ç©¶ä¸ºè·¨æ¨¡æ€çŸ¥è¯†è¿ç§»å»ºç«‹äº†æ–°èŒƒå¼ï¼Œæ¨åŠ¨äº†é€šç”¨æ—¶åºé¢„æµ‹æ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†è§‰æ¨¡å‹é€šè¿‡å›¾åƒé‡å»ºä»»åŠ¡åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>ä»è§†è§‰åˆ°æ—¶åºçš„è·¨æ¨¡æ€è¿ç§»å­˜åœ¨ä¸‰å¤§æŒ‘æˆ˜ï¼šæ•°æ®æ¨¡æ€å·®è·ã€å¤šå…ƒé¢„æµ‹å·®è·å’Œæ¦‚ç‡é¢„æµ‹å·®è·ã€‚</li>
<li>VisionTS++æ¨¡å‹é€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯ç¼©å°äº†è¿™äº›å·®è·ï¼šåŸºäºè§†è§‰æ¨¡å‹çš„è¿‡æ»¤æœºåˆ¶ã€å½©è‰²å¤šå…ƒè½¬æ¢æ–¹æ³•å’Œå¤šåˆ†ä½é¢„æµ‹æ–¹æ³•ã€‚</li>
<li>VisionTS++æ¨¡å‹åœ¨æ—¶åºé¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œç›¸æ¯”ä¸“ä¸šæ—¶åºé¢„æµ‹æ¨¡å‹æœ‰æ‰€æ”¹è¿›ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè·¨æ¨¡æ€çŸ¥è¯†è¿ç§»æä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc02e505da7383db3d617df2c9ccc9dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31a965528b5cb3660885022df771ba0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-454286041d1567d77885ed672d14f338.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Automated-ultrasound-doppler-angle-estimation-using-deep-learning"><a href="#Automated-ultrasound-doppler-angle-estimation-using-deep-learning" class="headerlink" title="Automated ultrasound doppler angle estimation using deep learning"></a>Automated ultrasound doppler angle estimation using deep learning</h2><p><strong>Authors:Nilesh Patil, Ajay Anand</strong></p>
<p>Angle estimation is an important step in the Doppler ultrasound clinical workflow to measure blood velocity. It is widely recognized that incorrect angle estimation is a leading cause of error in Doppler-based blood velocity measurements. In this paper, we propose a deep learning-based approach for automated Doppler angle estimation. The approach was developed using 2100 human carotid ultrasound images including image augmentation. Five pre-trained models were used to extract images features, and these features were passed to a custom shallow network for Doppler angle estimation. Independently, measurements were obtained by a human observer reviewing the images for comparison. The mean absolute error (MAE) between the automated and manual angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated. Furthermore, the MAE for the best performing model was less than the acceptable clinical Doppler angle error threshold thus avoiding misclassification of normal velocity values as a stenosis. The results demonstrate potential for applying a deep-learning based technique for automated ultrasound Doppler angle estimation. Such a technique could potentially be implemented within the imaging software on commercial ultrasound scanners. </p>
<blockquote>
<p>è§’åº¦ä¼°è®¡æ˜¯å¤šæ™®å‹’è¶…å£°è¡€æµé€Ÿåº¦æµ‹é‡ä¸­çš„å…³é”®æ­¥éª¤ã€‚äººä»¬æ™®éè®¤ä¸ºï¼Œè§’åº¦ä¼°è®¡ä¸æ­£ç¡®æ˜¯å¤šæ™®å‹’è¡€æµé€Ÿåº¦æµ‹é‡çš„ä¸»è¦è¯¯å·®æ¥æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–å¤šæ™®å‹’è§’åº¦ä¼°è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŒ…æ‹¬å›¾åƒå¢å¼ºåœ¨å†…çš„2100å¼ äººä½“é¢ˆåŠ¨è„‰è¶…å£°å›¾åƒè¿›è¡Œå¼€å‘ã€‚æˆ‘ä»¬ä½¿ç”¨äº”ä¸ªé¢„è®­ç»ƒæ¨¡å‹æå–å›¾åƒç‰¹å¾ï¼Œå¹¶å°†è¿™äº›ç‰¹å¾ä¼ é€’ç»™è‡ªå®šä¹‰çš„æµ…å±‚ç½‘ç»œè¿›è¡Œå¤šæ™®å‹’è§’åº¦ä¼°è®¡ã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡äººç±»è§‚å¯Ÿè€…ç‹¬ç«‹å®¡æŸ¥å›¾åƒè¿›è¡Œæ¯”è¾ƒæµ‹é‡ã€‚è¯„ä¼°æ¨¡å‹çš„è‡ªåŠ¨å’Œæ‰‹åŠ¨è§’åº¦ä¼°è®¡ä¹‹é—´çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰èŒƒå›´ä»3.9Â°åˆ°9.4Â°ã€‚æ­¤å¤–ï¼Œæœ€ä½³æ€§èƒ½æ¨¡å‹çš„MAEä½äºå¯æ¥å—çš„ä¸´åºŠå¤šæ™®å‹’è§’åº¦è¯¯å·®é˜ˆå€¼ï¼Œä»è€Œé¿å…äº†å°†æ­£å¸¸é€Ÿåº¦å€¼é”™è¯¯åˆ†ç±»ä¸ºç‹­çª„ã€‚ç»“æœè¡¨æ˜ï¼Œå°†åŸºäºæ·±åº¦å­¦ä¹ çš„æŠ€æœ¯åº”ç”¨äºè‡ªåŠ¨åŒ–è¶…å£°å¤šæ™®å‹’è§’åº¦ä¼°è®¡å…·æœ‰æ½œåŠ›ã€‚è¿™æ ·çš„æŠ€æœ¯æœ‰å¯èƒ½åœ¨å•†ç”¨è¶…å£°æ‰«æä»ªçš„æˆåƒè½¯ä»¶ä¸­å®æ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04243v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è¶…å£°å¤šæ™®å‹’è§’åº¦ä¼°è®¡æ˜¯æµ‹é‡è¡€æµé€Ÿåº¦çš„é‡è¦æ­¥éª¤ã€‚ä¸æ­£ç¡®çš„è§’åº¦ä¼°è®¡æ˜¯å¯¼è‡´å¤šæ™®å‹’è¡€æµé€Ÿåº¦æµ‹é‡è¯¯å·®çš„ä¸»è¦åŸå› ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–å¤šæ™®å‹’è§’åº¦ä¼°è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŒ…æ‹¬å›¾åƒå¢å¼ºåœ¨å†…çš„2100å¼ äººç±»é¢ˆåŠ¨è„‰è¶…å£°å›¾åƒè¿›è¡Œå¼€å‘ã€‚äº”ä¸ªé¢„è®­ç»ƒæ¨¡å‹ç”¨äºæå–å›¾åƒç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾è¢«ä¼ é€’ç»™ä¸€ä¸ªè‡ªå®šä¹‰æµ…å±‚ç½‘ç»œè¿›è¡Œå¤šæ™®å‹’è§’åº¦ä¼°è®¡ã€‚åŒæ—¶ï¼Œé€šè¿‡äººç±»è§‚å¯Ÿè€…å¯¹å›¾åƒè¿›è¡Œå›é¡¾æ€§è¯„ä¼°ä»¥è·å¾—å¯¹æ¯”æµ‹é‡å€¼ã€‚è‡ªåŠ¨å’Œæ‰‹åŠ¨è§’åº¦ä¼°è®¡ä¹‹é—´çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰åœ¨æ¨¡å‹è¯„ä¼°ä¸­çš„èŒƒå›´ä¸º$ 3.9^\circåˆ° 9.4^\circ $ä¹‹é—´ã€‚è€Œä¸”è¡¨ç°æœ€å¥½çš„æ¨¡å‹çš„MAEä½äºå¯æ¥å—çš„ä¸´åºŠå¤šæ™®å‹’è§’åº¦è¯¯å·®é˜ˆå€¼ï¼Œé¿å…äº†æ­£å¸¸é€Ÿåº¦å€¼è¢«è¯¯åˆ†ç±»ä¸ºç‹­çª„çš„å¯èƒ½æ€§ã€‚ç»“æœè¯æ˜äº†åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„è‡ªåŠ¨åŒ–è¶…å£°å¤šæ™®å‹’è§’åº¦ä¼°è®¡æ–¹æ³•çš„æ½œåŠ›ã€‚è¿™æ ·çš„æŠ€æœ¯æœ‰å¯èƒ½è¢«å®ç°åœ¨å•†ä¸šè¶…å£°æ‰«æä»ªçš„æˆåƒè½¯ä»¶ä¸­ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¶…å£°å¤šæ™®å‹’è§’åº¦ä¼°è®¡æ˜¯è¡€æµé€Ÿåº¦æµ‹é‡çš„å…³é”®æ­¥éª¤ï¼Œå…¶å‡†ç¡®æ€§å¯¹ç»“æœè‡³å…³é‡è¦ã€‚</li>
<li>ä¸æ­£ç¡®çš„è§’åº¦ä¼°è®¡æ˜¯å¯¼è‡´å¤šæ™®å‹’è¡€æµé€Ÿåº¦æµ‹é‡è¯¯å·®çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–å¤šæ™®å‹’è§’åº¦ä¼°è®¡æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹æå–å›¾åƒç‰¹å¾å¹¶è¿›è¡Œä¼°è®¡ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†å¤§é‡çš„äººç±»é¢ˆåŠ¨è„‰è¶…å£°å›¾åƒè¿›è¡Œæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ã€‚</li>
<li>ä¸æ‰‹åŠ¨æµ‹é‡ç›¸æ¯”ï¼Œè‡ªåŠ¨ä¼°è®¡çš„å¹³å‡ç»å¯¹è¯¯å·®åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„è¯¯å·®ä½äºä¸´åºŠå¯æ¥å—çš„è¯¯å·®é˜ˆå€¼ï¼Œå¯ä»¥é¿å…è¯¯åˆ¤æ­£å¸¸é€Ÿåº¦å€¼ä¸ºç‹­çª„æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7a4a2d4e6753ac89810478d02587ce2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63e7177cb4fbda7296191b13292504bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c8938030e266687bf67e8084faac4e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d48706d71c9ee5df3539e92c05f05949.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40387b6ba2f181f787ca569b21db231f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Small-Lesions-aware-Bidirectional-Multimodal-Multiscale-Fusion-Network-for-Lung-Disease-Classification"><a href="#Small-Lesions-aware-Bidirectional-Multimodal-Multiscale-Fusion-Network-for-Lung-Disease-Classification" class="headerlink" title="Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network   for Lung Disease Classification"></a>Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network   for Lung Disease Classification</h2><p><strong>Authors:Jianxun Yu, Ruiquan Ge, Zhipeng Wang, Cheng Yang, Chenyu Lin, Xianjun Fu, Jikui Liu, Ahmed Elazab, Changmiao Wang</strong></p>
<p>The diagnosis of medical diseases faces challenges such as the misdiagnosis of small lesions. Deep learning, particularly multimodal approaches, has shown great potential in the field of medical disease diagnosis. However, the differences in dimensionality between medical imaging and electronic health record data present challenges for effective alignment and fusion. To address these issues, we propose the Multimodal Multiscale Cross-Attention Fusion Network (MMCAF-Net). This model employs a feature pyramid structure combined with an efficient 3D multi-scale convolutional attention module to extract lesion-specific features from 3D medical images. To further enhance multimodal data integration, MMCAF-Net incorporates a multi-scale cross-attention module, which resolves dimensional inconsistencies, enabling more effective feature fusion. We evaluated MMCAF-Net on the Lung-PET-CT-Dx dataset, and the results showed a significant improvement in diagnostic accuracy, surpassing current state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/yjx1234/MMCAF-Net">https://github.com/yjx1234/MMCAF-Net</a> </p>
<blockquote>
<p>åŒ»å­¦ç–¾ç—…çš„è¯Šæ–­é¢ä¸´å¦‚å°ç—…ç¶è¯¯è¯Šç­‰æŒ‘æˆ˜ã€‚æ·±åº¦å­¦ä¹ ï¼Œå°¤å…¶æ˜¯å¤šæ¨¡æ€æ–¹æ³•ï¼Œåœ¨åŒ»å­¦ç–¾ç—…è¯Šæ–­é¢†åŸŸè¡¨ç°å‡ºäº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒåŒ»å­¦æˆåƒä¸ç”µå­å¥åº·è®°å½•æ•°æ®ä¹‹é—´çš„ç»´åº¦å·®å¼‚ç»™æœ‰æ•ˆå¯¹é½å’Œèåˆå¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€å¤šå°ºåº¦äº¤å‰æ³¨æ„èåˆç½‘ç»œï¼ˆMMCAF-Netï¼‰ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç‰¹å¾é‡‘å­—å¡”ç»“æ„ï¼Œç»“åˆé«˜æ•ˆçš„3Då¤šå°ºåº¦å·ç§¯æ³¨æ„æ¨¡å—ï¼Œä»3DåŒ»å­¦å›¾åƒä¸­æå–ç—…ç¶ç‰¹å®šç‰¹å¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›å¤šæ¨¡æ€æ•°æ®é›†æˆï¼ŒMMCAF-Netèå…¥äº†ä¸€ä¸ªå¤šå°ºåº¦äº¤å‰æ³¨æ„æ¨¡å—ï¼Œè¯¥æ¨¡å—è§£å†³äº†ç»´åº¦ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå®ç°äº†æ›´æœ‰æ•ˆçš„ç‰¹å¾èåˆã€‚æˆ‘ä»¬åœ¨Lung-PET-CT-Dxæ•°æ®é›†ä¸Šè¯„ä¼°äº†MMCAF-Netï¼Œç»“æœæ˜¾ç¤ºè¯Šæ–­å‡†ç¡®åº¦æ˜¾è‘—æé«˜ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/yjx1234/MMCAF-Net%E3%80%82">https://github.com/yjx1234/MMCAF-Netã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04205v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦ç–¾ç—…è¯Šæ–­é¢ä¸´å°ç—…ç¶è¯¯è¯Šç­‰æŒ‘æˆ˜ã€‚æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯å¤šæ¨¡æ€æ–¹æ³•ï¼Œåœ¨è¯¥é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼ŒåŒ»å­¦æˆåƒä¸ç”µå­å¥åº·è®°å½•æ•°æ®åœ¨ç»´åº¦ä¸Šçš„å·®å¼‚ç»™æœ‰æ•ˆå¯¹é½å’Œèåˆå¸¦æ¥æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºå¤šæ¨¡æ€å¤šå°ºåº¦äº¤å‰æ³¨æ„åŠ›èåˆç½‘ç»œï¼ˆMMCAF-Netï¼‰ï¼Œé‡‡ç”¨ç‰¹å¾é‡‘å­—å¡”ç»“æ„ç»“åˆé«˜æ•ˆçš„3Då¤šå°ºåº¦å·ç§¯æ³¨æ„åŠ›æ¨¡å—ï¼Œä»3DåŒ»å­¦å›¾åƒä¸­æå–ç—…ç¶ç‰¹å¾ã€‚ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–å¤šæ¨¡æ€æ•°æ®é›†æˆï¼ŒMMCAF-Netèå…¥å¤šå°ºåº¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œè§£å†³ç»´åº¦ä¸ä¸€è‡´é—®é¢˜ï¼Œå®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾èåˆã€‚åœ¨Lung-PET-CT-Dxæ•°æ®é›†ä¸Šè¯„ä¼°MMCAF-Netï¼Œç»“æœæ˜¾ç¤ºè¯Šæ–­å‡†ç¡®åº¦æ˜¾è‘—æé«˜ï¼Œè¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦ç–¾ç—…è¯Šæ–­é¢ä¸´å°ç—…ç¶è¯¯è¯Šçš„æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯å¤šæ¨¡æ€æ–¹æ³•ï¼Œåœ¨åŒ»å­¦è¯Šæ–­ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>åŒ»å­¦æˆåƒä¸ç”µå­å¥åº·è®°å½•æ•°æ®åœ¨ç»´åº¦ä¸Šçš„å·®å¼‚æ˜¯æ•°æ®èåˆçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„å¤šæ¨¡æ€å¤šå°ºåº¦äº¤å‰æ³¨æ„åŠ›èåˆç½‘ç»œï¼ˆMMCAF-Netï¼‰èƒ½æå–ç—…ç¶ç‰¹å¾å¹¶ä¼˜åŒ–æ•°æ®èåˆã€‚</li>
<li>MMCAF-Neté‡‡ç”¨ç‰¹å¾é‡‘å­—å¡”ç»“æ„å’Œ3Då¤šå°ºåº¦å·ç§¯æ³¨æ„åŠ›æ¨¡å—ã€‚</li>
<li>MMCAF-Netåœ¨Lung-PET-CT-Dxæ•°æ®é›†ä¸Šè¯Šæ–­å‡†ç¡®åº¦æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e206303908fe2032fa7d4a8255d93501.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c88102335447504799d7e630fbb3c0fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b324aecbd4bd5bc205b7a2ce129428a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dac8035a3bcc5b6fcc83af02692a344.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DS-2-Net-Detail-Semantic-Deep-Supervision-Network-for-Medical-Image-Segmentation"><a href="#DS-2-Net-Detail-Semantic-Deep-Supervision-Network-for-Medical-Image-Segmentation" class="headerlink" title="DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image   Segmentation"></a>DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image   Segmentation</h2><p><strong>Authors:Zhaohong Huang, Yuxin Zhang, Mingbao Lin, Taojian Zhou, Guorong Cai, Rongrong Ji</strong></p>
<p>Deep Supervision Networks exhibit significant efficacy for the medical imaging community. Nevertheless, existing work merely supervises either the coarse-grained semantic features or fine-grained detailed features in isolation, which compromises the fact that these two types of features hold vital relationships in medical image analysis. We advocate the powers of complementary feature supervision for medical image segmentation, by proposing a Detail-Semantic Deep Supervision Network (DS$^2$Net). DS$^2$Net navigates both low-level detailed and high-level semantic feature supervision through Detail Enhance Module (DEM) and Semantic Enhance Module (SEM). DEM and SEM respectively harness low-level and high-level feature maps to create detail and semantic masks for enhancing feature supervision. This is a novel shift from single-view deep supervision to multi-view deep supervision. DS$^2$Net is also equipped with a novel uncertainty-based supervision loss that adaptively assigns the supervision strength of features within distinct scales based on their uncertainty, thus circumventing the sub-optimal heuristic design that typifies previous works. Through extensive experiments on six benchmarks captured under either colonoscopy, ultrasound and microscope, we demonstrate that DS$^2$Net consistently outperforms state-of-the-art methods for medical image analysis. </p>
<blockquote>
<p>æ·±åº¦ç›‘ç£ç½‘ç»œå¯¹åŒ»å­¦å½±åƒç•Œå…·æœ‰æ˜¾è‘—æ•ˆæœã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œåªæ˜¯å­¤ç«‹åœ°ç›‘ç£ç²—ç²’åº¦çš„è¯­ä¹‰ç‰¹å¾æˆ–ç»†ç²’åº¦çš„è¯¦ç»†ç‰¹å¾ï¼Œå¿½ç•¥äº†è¿™ä¸¤ç§ç‰¹å¾åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰é‡è¦å…³ç³»çš„äº‹å®ã€‚æˆ‘ä»¬æå‡ºäº†ç»†èŠ‚è¯­ä¹‰æ·±åº¦ç›‘ç£ç½‘ç»œï¼ˆDS$^2$Netï¼‰ï¼Œæå€¡åŒ»å­¦å›¾åƒåˆ†å‰²çš„äº’è¡¥ç‰¹å¾ç›‘ç£ã€‚DS$^2$Neté€šè¿‡ç»†èŠ‚å¢å¼ºæ¨¡å—ï¼ˆDEMï¼‰å’Œè¯­ä¹‰å¢å¼ºæ¨¡å—ï¼ˆSEMï¼‰å¯¼èˆªä½çº§åˆ«çš„è¯¦ç»†ç‰¹å¾å’Œé«˜çº§åˆ«çš„è¯­ä¹‰ç‰¹å¾ç›‘ç£ã€‚DEMå’ŒSEMåˆ†åˆ«åˆ©ç”¨ä½çº§åˆ«å’Œé«˜çº§åˆ«çš„ç‰¹å¾å›¾åˆ›å»ºç»†èŠ‚å’Œè¯­ä¹‰æ©è†œï¼Œä»¥å¢å¼ºç‰¹å¾ç›‘ç£ã€‚è¿™æ˜¯ä¸€ç§ä»å•è§†å›¾æ·±åº¦ç›‘ç£åˆ°å¤šè§†å›¾æ·±åº¦ç›‘ç£çš„æ–°è½¬å˜ã€‚DS$^2$Netè¿˜é…å¤‡äº†ä¸€ç§åŸºäºæ–°å‹ä¸ç¡®å®šæ€§ç›‘ç£æŸå¤±ï¼Œæ ¹æ®ç‰¹å¾çš„ä¸ç¡®å®šæ€§è‡ªé€‚åº”åœ°åˆ†é…ä¸åŒå°ºåº¦ç‰¹å¾çš„ç›‘ç£å¼ºåº¦ï¼Œä»è€Œé¿å…äº†ä»¥å‰å·¥ä½œçš„æ¬¡ä¼˜å¯å‘å¼è®¾è®¡ã€‚é€šè¿‡åœ¨ç»“è‚ é•œã€è¶…å£°å’Œæ˜¾å¾®é•œä¸‹é‡‡é›†çš„å…­ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†DS$^2$Netåœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04131v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå­˜åœ¨ç›‘ç£é—®é¢˜ï¼Œé€šå¸¸ä»…å…³æ³¨ç²—ç²’åº¦è¯­ä¹‰ç‰¹å¾æˆ–ç»†ç²’åº¦è¯¦ç»†ç‰¹å¾å…¶ä¸­ä¹‹ä¸€ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§ç»†èŠ‚è¯­ä¹‰æ·±åº¦ç›‘ç£ç½‘ç»œï¼ˆDS$^2$Netï¼‰ï¼ŒåŒ…å«ç»†èŠ‚å¢å¼ºæ¨¡å—ï¼ˆDEMï¼‰å’Œè¯­ä¹‰å¢å¼ºæ¨¡å—ï¼ˆSEMï¼‰ï¼Œå®ç°ä½çº§åˆ«è¯¦ç»†ç‰¹å¾å’Œé«˜çº§åˆ«è¯­ä¹‰ç‰¹å¾çš„å…±åŒç›‘ç£ã€‚æ­¤å¤–ï¼Œå¼•å…¥åŸºäºä¸ç¡®å®šæ€§çš„ç›‘ç£æŸå¤±ï¼Œæ ¹æ®ç‰¹å¾çš„ä¸ç¡®å®šæ€§è‡ªé€‚åº”è°ƒæ•´ä¸åŒå°ºåº¦çš„ç‰¹å¾ç›‘ç£å¼ºåº¦ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDS$^2$Netåœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ç›‘ç£æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä»…å…³æ³¨å•ä¸€ç±»å‹çš„ç‰¹å¾ï¼ˆå¦‚è¯­ä¹‰æˆ–è¯¦ç»†ç‰¹å¾ï¼‰ã€‚</li>
<li>DS$^2$Netæå‡ºä¸€ç§ç»†èŠ‚è¯­ä¹‰æ·±åº¦ç›‘ç£ç½‘ç»œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç»“åˆäº†ç»†èŠ‚å¢å¼ºæ¨¡å—ï¼ˆDEMï¼‰å’Œè¯­ä¹‰å¢å¼ºæ¨¡å—ï¼ˆSEMï¼‰ã€‚</li>
<li>DS$^2$Netå®ç°äº†ä»å•è§†å›¾åˆ°å¤šè§†å›¾çš„æ·±åº¦ç›‘ç£è½¬å˜ã€‚</li>
<li>å¼•å…¥åŸºäºä¸ç¡®å®šæ€§çš„ç›‘ç£æŸå¤±ï¼Œèƒ½æ ¹æ®ç‰¹å¾çš„ä¸ç¡®å®šæ€§è‡ªé€‚åº”è°ƒæ•´ç›‘ç£å¼ºåº¦ã€‚</li>
<li>DS$^2$Netåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬ç»“è‚ é•œã€è¶…å£°å’Œæ˜¾å¾®é•œå›¾åƒåˆ†æã€‚</li>
<li>è¯¥ç½‘ç»œçš„è®¾è®¡æœ‰åŠ©äºæé«˜åŒ»å­¦å›¾åƒåˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-12038c37e305b1174df01b7db703d42a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e64252daba2f7f13a44d3d247a7bae63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bf20dc993e4ab1425912cef9bf68fde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58c30226eecd9c1721e6b22cf9e8d8bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c008540d8e5195615f8fdd26c5d65594.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48ea483f9e8b117e5de121c814510cbe.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PET2Rep-Towards-Vision-Language-Model-Drived-Automated-Radiology-Report-Generation-for-Positron-Emission-Tomography"><a href="#PET2Rep-Towards-Vision-Language-Model-Drived-Automated-Radiology-Report-Generation-for-Positron-Emission-Tomography" class="headerlink" title="PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report   Generation for Positron Emission Tomography"></a>PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report   Generation for Positron Emission Tomography</h2><p><strong>Authors:Yichi Zhang, Wenbo Zhang, Zehui Ling, Gang Feng, Sisi Peng, Deshu Chen, Yuchen Liu, Hongwei Zhang, Shuqi Wang, Lanlan Li, Limei Han, Yuan Cheng, Zixin Hu, Yuan Qi, Le Xue</strong></p>
<p>Positron emission tomography (PET) is a cornerstone of modern oncologic and neurologic imaging, distinguished by its unique ability to illuminate dynamic metabolic processes that transcend the anatomical focus of traditional imaging technologies. Radiology reports are essential for clinical decision making, yet their manual creation is labor-intensive and time-consuming. Recent advancements of vision-language models (VLMs) have shown strong potential in medical applications, presenting a promising avenue for automating report generation. However, existing applications of VLMs in the medical domain have predominantly focused on structural imaging modalities, while the unique characteristics of molecular PET imaging have largely been overlooked. To bridge the gap, we introduce PET2Rep, a large-scale comprehensive benchmark for evaluation of general and medical VLMs for radiology report generation for PET images. PET2Rep stands out as the first dedicated dataset for PET report generation with metabolic information, uniquely capturing whole-body image-report pairs that cover dozens of organs to fill the critical gap in existing benchmarks and mirror real-world clinical comprehensiveness. In addition to widely recognized natural language generation metrics, we introduce a series of clinical efficiency metrics to evaluate the quality of radiotracer uptake pattern description in key organs in generated reports. We conduct a head-to-head comparison of 30 cutting-edge general-purpose and medical-specialized VLMs. The results show that the current state-of-the-art VLMs perform poorly on PET report generation task, falling considerably short of fulfilling practical needs. Moreover, we identify several key insufficiency that need to be addressed to advance the development in medical applications. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æ˜¯ç°ä»£è‚¿ç˜¤å­¦å’Œç¥ç»æˆåƒçš„åŸºçŸ³ï¼Œä»¥å…¶ç‹¬ç‰¹çš„èƒ½å¤Ÿæ­ç¤ºè¶…è¶Šä¼ ç»ŸæˆåƒæŠ€æœ¯è§£å‰–é‡ç‚¹çš„åŠ¨æ€ä»£è°¢è¿‡ç¨‹çš„èƒ½åŠ›è€Œè„±é¢–è€Œå‡ºã€‚æ”¾å°„å­¦æŠ¥å‘Šå¯¹äºä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œä½†å…¶æ‰‹åŠ¨åˆ›å»ºè¿‡ç¨‹å´è€—æ—¶ä¸”åŠ³åŠ›å¯†é›†ã€‚æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›å±•åœ¨åŒ»å­¦åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä¸ºæŠ¥å‘Šç”Ÿæˆè‡ªåŠ¨åŒ–æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚ç„¶è€Œï¼ŒVLMsåœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä¸»è¦é›†ä¸­åœ¨ç»“æ„æ€§æˆåƒæ¨¡å¼ä¸Šï¼Œè€ŒPETåˆ†å­æˆåƒçš„ç‹¬ç‰¹ç‰¹æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½è§†äº†ã€‚ä¸ºäº†å¼¥å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PET2Repï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ç”¨äºPETå›¾åƒæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„é€šç”¨å’ŒåŒ»ç–—VLMsçš„è¯„ä¼°ã€‚PET2Repä»¥å¸¦æœ‰ä»£è°¢ä¿¡æ¯çš„PETæŠ¥å‘Šç”Ÿæˆæ•°æ®é›†çš„èº«ä»½è„±é¢–è€Œå‡ºï¼Œç‹¬ç‰¹åœ°æ•æ‰å…¨èº«å›¾åƒæŠ¥å‘Šå¯¹ï¼Œæ¶µç›–æ•°åä¸ªå™¨å®˜ï¼Œä»¥å¡«è¡¥ç°æœ‰åŸºå‡†æµ‹è¯•çš„ç©ºç™½å¹¶åæ˜ ç°å®ä¸–ç•Œä¸´åºŠçš„å…¨é¢æ€§ã€‚é™¤äº†å¹¿æ³›è®¤å¯çš„è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç³»åˆ—ä¸´åºŠæ•ˆç‡æŒ‡æ ‡æ¥è¯„ä¼°ç”ŸæˆæŠ¥å‘Šä¸­å…³é”®å™¨å®˜æ”¾å°„æ€§ç¤ºè¸ªå‰‚æ‘„å–æ¨¡å¼æè¿°çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¯¹30æ¬¾å…ˆè¿›é€šç”¨å’ŒåŒ»ç–—ä¸“ç”¨VLMè¿›è¡Œäº†ç›´é¢æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„VLMåœ¨PETæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œè¿œè¿œä¸èƒ½æ»¡è¶³å®é™…éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®å®šäº†å‡ ä¸ªå…³é”®ä¸è¶³ï¼Œéœ€è¦è§£å†³è¿™äº›ä¸è¶³ä»¥ä¿ƒè¿›åŒ»ç–—åº”ç”¨çš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04062v1">PDF</a> </p>
<p><strong>Summary</strong><br>     PETæˆåƒåœ¨ç°ä»£è‚¿ç˜¤å­¦å’Œç¥ç»æˆåƒä¸­æ˜¯æ ¸å¿ƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ­ç¤ºåŠ¨æ€ä»£è°¢è¿‡ç¨‹ï¼Œè¶…è¶Šä¼ ç»ŸæˆåƒæŠ€æœ¯çš„è§£å‰–é‡ç‚¹ã€‚å°½ç®¡æ”¾å°„å­¦æŠ¥å‘Šå¯¹ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œä½†å…¶æ‰‹åŠ¨ç¼–å†™è€—æ—¶ä¸”åŠ³åŠ›å¯†é›†ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä¸ºæŠ¥å‘Šç”Ÿæˆè‡ªåŠ¨åŒ–æä¾›æœ‰å‰é€”çš„é€”å¾„ã€‚ç„¶è€Œï¼ŒVLMsåœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä¸»è¦é›†ä¸­åœ¨ç»“æ„æ€§æˆåƒæ¨¡å¼ä¸Šï¼Œè€ŒPETæˆåƒçš„ç‹¬ç‰¹ç‰¹æ€§å´è¢«å¿½è§†ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºPET2Repï¼Œä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°ç”¨äºPETå›¾åƒæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„é€šç”¨å’ŒåŒ»ç–—VLMsã€‚PET2Repæ˜¯é¦–ä¸ªä¸“æ³¨äºPETæŠ¥å‘Šç”Ÿæˆçš„æ•°æ®é›†ï¼ŒåŒ…å«ä»£è°¢ä¿¡æ¯ï¼Œç‹¬ç‰¹åœ°æ•æ‰å…¨èº«å›¾åƒæŠ¥å‘Šå¯¹ï¼Œè¦†ç›–å¤šä¸ªå™¨å®˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å°–ç«¯é€šç”¨å’ŒåŒ»ç–—ä¸“ç”¨VLMsçš„å¯¹æ¯”ç ”ç©¶ï¼Œç»“æœæ˜¾ç¤ºå½“å‰æœ€å…ˆè¿›çš„VLMsåœ¨PETæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œéš¾ä»¥æ»¡è¶³å®é™…éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETæˆåƒæŠ€æœ¯åœ¨ç°ä»£è‚¿ç˜¤å­¦å’Œç¥ç»æˆåƒä¸­çš„æ ¸å¿ƒåœ°ä½åŠå…¶æ­ç¤ºåŠ¨æ€ä»£è°¢è¿‡ç¨‹çš„èƒ½åŠ›ã€‚</li>
<li>æ”¾å°„å­¦æŠ¥å‘Šåœ¨ä¸´åºŠå†³ç­–ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠæ‰‹åŠ¨ç¼–å†™æŠ¥å‘Šçš„è€—æ—¶å’ŒåŠ³åŠ›å¯†é›†é—®é¢˜ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„æ½œåŠ›åŠå…¶å¯¹æŠ¥å‘Šç”Ÿæˆè‡ªåŠ¨åŒ–çš„å‰æ™¯ã€‚</li>
<li>VLMsåœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä¸»è¦å…³æ³¨ç»“æ„æ€§æˆåƒæ¨¡å¼ï¼Œå¿½è§†äº†PETæˆåƒçš„ç‹¬ç‰¹ç‰¹æ€§ã€‚</li>
<li>PET2Repçš„æ¨å‡ºï¼Œä½œä¸ºé¦–ä¸ªä¸“æ³¨äºPETæŠ¥å‘Šç”Ÿæˆçš„æ•°æ®é›†ï¼ŒåŒ…å«ä»£è°¢ä¿¡æ¯ï¼Œè¦†ç›–å¤šä¸ªå™¨å®˜ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„VLMsåœ¨PETæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6db1c5897da7f283087bd2e51ff304c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ac6960fb57cf92ed15a1079fb353500.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96c42ed24a677976917a5ee60c3cf3ae.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TCSAFormer-Efficient-Vision-Transformer-with-Token-Compression-and-Sparse-Attention-for-Medical-Image-Segmentation"><a href="#TCSAFormer-Efficient-Vision-Transformer-with-Token-Compression-and-Sparse-Attention-for-Medical-Image-Segmentation" class="headerlink" title="TCSAFormer: Efficient Vision Transformer with Token Compression and   Sparse Attention for Medical Image Segmentation"></a>TCSAFormer: Efficient Vision Transformer with Token Compression and   Sparse Attention for Medical Image Segmentation</h2><p><strong>Authors:Zunhui Xia, Hongxing Li, Libin Lan</strong></p>
<p>In recent years, transformer-based methods have achieved remarkable progress in medical image segmentation due to their superior ability to capture long-range dependencies. However, these methods typically suffer from two major limitations. First, their computational complexity scales quadratically with the input sequences. Second, the feed-forward network (FFN) modules in vanilla Transformers typically rely on fully connected layers, which limits modelsâ€™ ability to capture local contextual information and multiscale features critical for precise semantic segmentation. To address these issues, we propose an efficient medical image segmentation network, named TCSAFormer. The proposed TCSAFormer adopts two key ideas. First, it incorporates a Compressed Attention (CA) module, which combines token compression and pixel-level sparse attention to dynamically focus on the most relevant key-value pairs for each query. This is achieved by pruning globally irrelevant tokens and merging redundant ones, significantly reducing computational complexity while enhancing the modelâ€™s ability to capture relationships between tokens. Second, it introduces a Dual-Branch Feed-Forward Network (DBFFN) module as a replacement for the standard FFN to capture local contextual features and multiscale information, thereby strengthening the modelâ€™s feature representation capability. We conduct extensive experiments on three publicly available medical image segmentation datasets: ISIC-2018, CVC-ClinicDB, and Synapse, to evaluate the segmentation performance of TCSAFormer. Experimental results demonstrate that TCSAFormer achieves superior performance compared to existing state-of-the-art (SOTA) methods, while maintaining lower computational overhead, thus achieving an optimal trade-off between efficiency and accuracy. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºè½¬æ¢å™¨çš„æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰æ•è·è¿œç¨‹ä¾èµ–å…³ç³»çš„å“è¶Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™æ€§ã€‚é¦–å…ˆï¼Œå®ƒä»¬çš„è®¡ç®—å¤æ‚åº¦ä¸è¾“å…¥åºåˆ—æˆäºŒæ¬¡æ–¹å…³ç³»ã€‚å…¶æ¬¡ï¼Œæ™®é€šTransformerä¸­çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰æ¨¡å—é€šå¸¸ä¾èµ–äºå…¨è¿æ¥å±‚ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹æ•è·å±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå¤šå°ºåº¦ç‰¹å¾çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºç²¾ç¡®çš„è¯­ä¹‰åˆ†å‰²è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œï¼Œåä¸ºTCSAFormerã€‚æ‰€æå‡ºçš„TCSAFormeré‡‡ç”¨äº†ä¸¤ä¸ªå…³é”®æ€æƒ³ã€‚é¦–å…ˆï¼Œå®ƒç»“åˆäº†å‹ç¼©æ³¨æ„åŠ›ï¼ˆCAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ç»“åˆäº†ä»¤ç‰Œå‹ç¼©å’Œåƒç´ çº§ç¨€ç–æ³¨æ„åŠ›ï¼Œä»¥åŠ¨æ€å…³æ³¨æ¯ä¸ªæŸ¥è¯¢çš„æœ€ç›¸å…³é”®å€¼å¯¹ã€‚è¿™æ˜¯é€šè¿‡åˆ é™¤å…¨å±€ä¸ç›¸å…³çš„ä»¤ç‰Œå¹¶åˆå¹¶å†—ä½™ä»¤ç‰Œæ¥å®ç°çš„ï¼Œè¿™æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶å¢å¼ºäº†æ¨¡å‹æ•è·ä»¤ç‰Œä¹‹é—´å…³ç³»çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œå®ƒå¼•å…¥äº†åŒåˆ†æ”¯å‰é¦ˆç½‘ç»œï¼ˆDBFFNï¼‰æ¨¡å—ï¼Œä½œä¸ºæ ‡å‡†FFNçš„æ›¿ä»£å“ï¼Œä»¥æ•è·å±€éƒ¨ä¸Šä¸‹æ–‡ç‰¹å¾å’Œå¤šå°ºåº¦ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å¼€çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ISIC-2018ã€CVC-ClinicDBå’ŒSynapseä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥è¯„ä¼°TCSAFormerçš„åˆ†å‰²æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTCSAFormerä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—å¼€é”€ï¼Œä»è€Œåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å®ç°äº†æœ€ä¼˜æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04058v1">PDF</a> 11 pages, 7 figures, 4 tables; The code is available at   <a target="_blank" rel="noopener" href="https://github.com/XiaZunhui/TCSAFormer">https://github.com/XiaZunhui/TCSAFormer</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²é¢†åŸŸï¼ŒåŸºäºTransformerçš„æ–¹æ³•å­˜åœ¨çš„ä¸¤å¤§å±€é™æ€§ï¼šè®¡ç®—å¤æ‚åº¦è¾ƒé«˜å’Œå¯¹å±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯åŠå¤šå°ºåº¦ç‰¹å¾æ•æ‰èƒ½åŠ›æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åŒ»ç–—å›¾åƒåˆ†å‰²ç½‘ç»œTCSAFormerã€‚å®ƒé€šè¿‡ç»“åˆå‹ç¼©æ³¨æ„åŠ›æ¨¡å—å’ŒåŒé‡åˆ†æ”¯å‰é¦ˆç½‘ç»œæ¨¡å—ï¼Œåœ¨é™ä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œå¢å¼ºäº†æ¨¡å‹æ•æ‰å…³é”®ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå®ç°äº†åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ–¹æ³•åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨è®¡ç®—å¤æ‚åº¦é«˜å’Œå¯¹å±€éƒ¨ä¸Šä¸‹æ–‡åŠå¤šå°ºåº¦ç‰¹å¾æ•æ‰èƒ½åŠ›æœ‰é™çš„å±€é™ã€‚</li>
<li>TCSAFormerç½‘ç»œé€šè¿‡å¼•å…¥å‹ç¼©æ³¨æ„åŠ›æ¨¡å—ï¼Œç»“åˆä»¤ç‰Œå‹ç¼©å’Œåƒç´ çº§ç¨€ç–æ³¨æ„åŠ›ï¼Œæé«˜äº†æ¨¡å‹å¯¹å…³é”®ä¿¡æ¯çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>TCSAFormeré‡‡ç”¨åŒé‡åˆ†æ”¯å‰é¦ˆç½‘ç»œæ¨¡å—æ›¿ä»£æ ‡å‡†å‰é¦ˆç½‘ç»œï¼Œä»¥æ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡ç‰¹å¾å’Œå¤šå°ºåº¦ä¿¡æ¯ï¼Œå¢å¼ºäº†ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>TCSAFormeråœ¨ä¸‰ä¸ªå…¬å¼€åŒ»ç–—å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—å¼€é”€è¾ƒä½ï¼Œå®ç°äº†æ•ˆç‡å’Œå‡†ç¡®æ€§çš„æœ€ä¼˜å¹³è¡¡ã€‚</li>
<li>TCSAFormerèƒ½å¤ŸåŠ¨æ€å…³æ³¨ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„é”®å€¼å¯¹ï¼Œé€šè¿‡å‰”é™¤å…¨å±€æ— å…³ä»¤ç‰Œå¹¶åˆå¹¶å†—ä½™ä»¤ç‰Œï¼Œæœ‰æ•ˆæé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>TCSAFormerç½‘ç»œçš„åˆ›æ–°è®¾è®¡å…‹æœäº†ä¼ ç»ŸTransformeræ–¹æ³•çš„ä¸¤å¤§å±€é™ï¼Œä¸ºåŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡æä¾›äº†æ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-da28b05a941c808d01a17fab2a7f54e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f2da11a7c22912ed99c19f1e9147333.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8921da024a9bedb5930d740e539f0dc5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Iterative-pseudo-labeling-based-adaptive-copy-paste-supervision-for-semi-supervised-tumor-segmentation"><a href="#Iterative-pseudo-labeling-based-adaptive-copy-paste-supervision-for-semi-supervised-tumor-segmentation" class="headerlink" title="Iterative pseudo-labeling based adaptive copy-paste supervision for   semi-supervised tumor segmentation"></a>Iterative pseudo-labeling based adaptive copy-paste supervision for   semi-supervised tumor segmentation</h2><p><strong>Authors:Qiangguo Jin, Hui Cui, Junbo Wang, Changming Sun, Yimiao He, Ping Xuan, Linlin Wang, Cong Cong, Leyi Wei, Ran Su</strong></p>
<p>Semi-supervised learning (SSL) has attracted considerable attention in medical image processing. The latest SSL methods use a combination of consistency regularization and pseudo-labeling to achieve remarkable success. However, most existing SSL studies focus on segmenting large organs, neglecting the challenging scenarios where there are numerous tumors or tumors of small volume. Furthermore, the extensive capabilities of data augmentation strategies, particularly in the context of both labeled and unlabeled data, have yet to be thoroughly investigated. To tackle these challenges, we introduce a straightforward yet effective approach, termed iterative pseudo-labeling based adaptive copy-paste supervision (IPA-CP), for tumor segmentation in CT scans. IPA-CP incorporates a two-way uncertainty based adaptive augmentation mechanism, aiming to inject tumor uncertainties present in the mean teacher architecture into adaptive augmentation. Additionally, IPA-CP employs an iterative pseudo-label transition strategy to generate more robust and informative pseudo labels for the unlabeled samples. Extensive experiments on both in-house and public datasets show that our framework outperforms state-of-the-art SSL methods in medical image segmentation. Ablation study results demonstrate the effectiveness of our technical contributions. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨åŒ»å­¦å›¾åƒå¤„ç†ä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æœ€æ–°çš„SSLæ–¹æ³•ç»“åˆäº†ä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œä¼ªæ ‡ç­¾æŠ€æœ¯ï¼Œå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„SSLç ”ç©¶é›†ä¸­åœ¨åˆ†å‰²å¤§å™¨å®˜ä¸Šï¼Œå¿½è§†äº†å­˜åœ¨å¤§é‡è‚¿ç˜¤æˆ–å°ä½“ç§¯è‚¿ç˜¤çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œæ•°æ®å¢å¼ºç­–ç•¥çš„å¹¿æ³›èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®çš„èƒŒæ™¯ä¸‹ï¼Œå°šæœªå¾—åˆ°å……åˆ†çš„è°ƒæŸ¥å’Œç ”ç©¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºè¿­ä»£ä¼ªæ ‡ç­¾çš„é€‚åº”æ€§æ‹·è´ç²˜è´´ç›‘ç£ï¼ˆIPA-CPï¼‰ï¼Œç”¨äºCTæ‰«æä¸­çš„è‚¿ç˜¤åˆ†å‰²ã€‚IPA-CPç»“åˆäº†åŸºäºåŒå‘ä¸ç¡®å®šæ€§çš„è‡ªé€‚åº”å¢å¼ºæœºåˆ¶ï¼Œæ—¨åœ¨å°†å­˜åœ¨äºå¹³å‡æ•™å¸ˆæ¶æ„ä¸­çš„è‚¿ç˜¤ä¸ç¡®å®šæ€§æ³¨å…¥è‡ªé€‚åº”å¢å¼ºä¸­ã€‚æ­¤å¤–ï¼ŒIPA-CPé‡‡ç”¨è¿­ä»£ä¼ªæ ‡ç­¾è½¬æ¢ç­–ç•¥ï¼Œä¸ºæ— æ ‡ç­¾æ ·æœ¬ç”Ÿæˆæ›´ç¨³å¥å’Œæ›´æœ‰ç”¨çš„ä¼ªæ ‡ç­¾ã€‚åœ¨å®¶åº­å†…éƒ¨å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„SSLæ–¹æ³•ã€‚æ¶ˆèç ”ç©¶ç»“æœè¡¨æ˜æˆ‘ä»¬çš„æŠ€æœ¯è´¡çŒ®æ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04044v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŠç›‘ç£å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒå¤„ç†é¢†åŸŸå¤‡å—å…³æ³¨ã€‚æœ€æ–°åŠç›‘ç£å­¦ä¹ æ–¹æ³•é€šè¿‡ä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œä¼ªæ ‡ç­¾æŠ€æœ¯å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶å…³æ³¨å¤§å™¨å®˜çš„åˆ†å‰²ï¼Œå¿½è§†äº†å­˜åœ¨å¤šä¸ªè‚¿ç˜¤æˆ–å°å‹ä½“ç§¯è‚¿ç˜¤çš„å¤æ‚åœºæ™¯çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæ•°æ®å¢å¼ºç­–ç•¥çš„å¹¿æ³›èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå°šæœªå¾—åˆ°æ·±å…¥ç ”ç©¶ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è¿­ä»£ä¼ªæ ‡ç­¾è‡ªé€‚åº”æ‹·è´ç²˜è´´ç›‘ç£æ–¹æ³•ï¼ˆIPA-CPï¼‰ï¼Œç”¨äºCTæ‰«æä¸­çš„è‚¿ç˜¤åˆ†å‰²ã€‚IPA-CPç»“åˆäº†åŸºäºåŒå‘ä¸ç¡®å®šæ€§çš„è‡ªé€‚åº”å¢å¼ºæœºåˆ¶ï¼Œæ—¨åœ¨å°†è‚¿ç˜¤çš„ä¸ç¡®å®šæ€§æ³¨å…¥åˆ°å¹³å‡æ•™å¸ˆæ¶æ„ä¸­è¿›è¡Œè‡ªé€‚åº”å¢å¼ºã€‚åŒæ—¶ï¼ŒIPA-CPä½¿ç”¨è¿­ä»£ä¼ªæ ‡ç­¾è½¬æ¢ç­–ç•¥ä¸ºæœªæ ‡è®°æ ·æœ¬ç”Ÿæˆæ›´ç¨³å¥å’Œæœ‰ç”¨çš„ä¼ªæ ‡ç­¾ã€‚åœ¨å†…éƒ¨å’Œå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸä¼˜äºæœ€æ–°çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚æ¶ˆèç ”ç©¶ç»“æœè¡¨æ˜æˆ‘ä»¬çš„æŠ€æœ¯è´¡çŒ®æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠç›‘ç£å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒå¤„ç†ä¸­å—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯ç”¨äºè‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>æœ€æ–°åŠç›‘ç£å­¦ä¹ æ–¹æ³•é€šè¿‡ä¸€è‡´æ€§æ­£åˆ™åŒ–å’Œä¼ªæ ‡ç­¾æŠ€æœ¯å–å¾—æ˜¾è‘—æˆæœã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šå…³æ³¨å¤§å™¨å®˜åˆ†å‰²ï¼Œå¿½è§†äº†å¤æ‚åœºæ™¯ä¸‹çš„è‚¿ç˜¤åˆ†å‰²æŒ‘æˆ˜ã€‚</li>
<li>IPA-CPæ–¹æ³•ç»“åˆäº†åŸºäºåŒå‘ä¸ç¡®å®šæ€§çš„è‡ªé€‚åº”å¢å¼ºæœºåˆ¶ï¼Œä»¥æé«˜è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>IPA-CPé‡‡ç”¨è¿­ä»£ä¼ªæ ‡ç­¾è½¬æ¢ç­–ç•¥ç”Ÿæˆæ›´ç¨³å¥å’Œæœ‰ç”¨çš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>åœ¨å†…éƒ¨å’Œå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIPA-CPåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3dd27d21624100b83a652f09a73f396a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-786319b84ad094a12c37d467ae9af657.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11b760932b8018c72131c8dbc91b0f3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00994c47bb741ccea6963c5ec13ad433.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Radar-Based-NLoS-Pedestrian-Localization-for-Darting-Out-Scenarios-Near-Parked-Vehicles-with-Camera-Assisted-Point-Cloud-Interpretation"><a href="#Radar-Based-NLoS-Pedestrian-Localization-for-Darting-Out-Scenarios-Near-Parked-Vehicles-with-Camera-Assisted-Point-Cloud-Interpretation" class="headerlink" title="Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near   Parked Vehicles with Camera-Assisted Point Cloud Interpretation"></a>Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near   Parked Vehicles with Camera-Assisted Point Cloud Interpretation</h2><p><strong>Authors:Hee-Yeun Kim, Byeonggyu Park, Byonghyok Choi, Hansang Cho, Byungkwan Kim, Soomok Lee, Mingu Jeon, Seung-Woo Seo, Seong-Woo Kim</strong></p>
<p>The presence of Non-Line-of-Sight (NLoS) blind spots resulting from roadside parking in urban environments poses a significant challenge to road safety, particularly due to the sudden emergence of pedestrians. mmWave technology leverages diffraction and reflection to observe NLoS regions, and recent studies have demonstrated its potential for detecting obscured objects. However, existing approaches predominantly rely on predefined spatial information or assume simple wall reflections, thereby limiting their generalizability and practical applicability. A particular challenge arises in scenarios where pedestrians suddenly appear from between parked vehicles, as these parked vehicles act as temporary spatial obstructions. Furthermore, since parked vehicles are dynamic and may relocate over time, spatial information obtained from satellite maps or other predefined sources may not accurately reflect real-time road conditions, leading to erroneous sensor interpretations. To address this limitation, we propose an NLoS pedestrian localization framework that integrates monocular camera image with 2D radar point cloud (PCD) data. The proposed method initially detects parked vehicles through image segmentation, estimates depth to infer approximate spatial characteristics, and subsequently refines this information using 2D radar PCD to achieve precise spatial inference. Experimental evaluations conducted in real-world urban road environments demonstrate that the proposed approach enhances early pedestrian detection and contributes to improved road safety. Supplementary materials are available at <a target="_blank" rel="noopener" href="https://hiyeun.github.io/NLoS/">https://hiyeun.github.io/NLoS/</a>. </p>
<blockquote>
<p>åœ¨åŸå¸‚ç¯å¢ƒä¸­ï¼Œè·¯è¾¹åœè½¦äº§ç”Ÿçš„éç›´è§†ï¼ˆNLoSï¼‰ç›²ç‚¹å¯¹é“è·¯å®‰å…¨æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è¡Œäººçªç„¶å‡ºç°çš„æƒ…å†ã€‚æ¯«ç±³æ³¢æŠ€æœ¯åˆ©ç”¨è¡å°„å’Œåå°„æ¥è§‚å¯ŸNLoSåŒºåŸŸï¼Œæœ€è¿‘çš„ç ”ç©¶å·²ç»è¯æ˜äº†å…¶åœ¨æ£€æµ‹éšè”½ç‰©ä½“æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé¢„å®šä¹‰çš„ç©ºé—´ä¿¡æ¯æˆ–å‡è®¾ç®€å•çš„å¢™å£åå°„ï¼Œä»è€Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå®é™…åº”ç”¨ã€‚ä¸€ä¸ªç‰¹æ®Šçš„æŒ‘æˆ˜å‡ºç°åœ¨è¡Œäººçªç„¶ä»åœè½¦è½¦è¾†ä¹‹é—´å‡ºç°çš„æƒ…å†µä¸‹ï¼Œè¿™äº›åœæ”¾çš„è½¦è¾†å……å½“äº†ä¸´æ—¶çš„ç©ºé—´éšœç¢ç‰©ã€‚æ­¤å¤–ï¼Œç”±äºåœæ”¾çš„è½¦è¾†æ˜¯åŠ¨æ€çš„å¹¶ä¸”å¯èƒ½ä¼šéšæ—¶é—´è€Œé‡æ–°å®šä½ï¼Œå› æ­¤ä»å«æ˜Ÿåœ°å›¾æˆ–å…¶ä»–é¢„å®šä¹‰æºè·å¾—çš„ç©ºé—´ä¿¡æ¯å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ å®æ—¶çš„é“è·¯çŠ¶å†µï¼Œä»è€Œå¯¼è‡´ä¼ æ„Ÿå™¨è§£é‡Šé”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§NLoSè¡Œäººå®šä½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å•ç›®ç›¸æœºå›¾åƒä¸2Dé›·è¾¾ç‚¹äº‘ï¼ˆPCDï¼‰æ•°æ®ç›¸ç»“åˆã€‚æ‰€æå‡ºçš„æ–¹æ³•é¦–å…ˆé€šè¿‡å›¾åƒåˆ†å‰²æ£€æµ‹åœæ”¾çš„è½¦è¾†ï¼Œä¼°è®¡æ·±åº¦ä»¥æ¨æ–­è¿‘ä¼¼ç©ºé—´ç‰¹å¾ï¼Œç„¶åä½¿ç”¨è¯¥ä¿¡æ¯ç»“åˆ2Dé›·è¾¾PCDè¿›è¡Œç²¾ç¡®çš„ç©ºé—´æ¨æ–­ã€‚åœ¨çœŸå®ä¸–ç•Œçš„åŸå¸‚é“è·¯ç¯å¢ƒä¸­æ‰€è¿›è¡Œçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†å¯¹è¡Œäººçš„æ—©æœŸæ£€æµ‹èƒ½åŠ›ï¼Œå¹¶æœ‰åŠ©äºæ”¹å–„é“è·¯å®‰å…¨ã€‚è¡¥å……ææ–™å¯è®¿é—®<a target="_blank" rel="noopener" href="https://hiyeun.github.io/NLoS/%E3%80%82">https://hiyeun.github.io/NLoS/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04033v1">PDF</a> Accepted to IEEE&#x2F;RSJ International Conference on Intelligent Robots   and Systems (IROS), 2025. 8 pages, 3 figures</p>
<p><strong>Summary</strong><br>     éè§†çº¿ç›²åŒºçš„å­˜åœ¨å¯¹é“è·¯å®‰å…¨æ„æˆæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è·¯è¾¹åœè½¦é€ æˆçš„ç›²åŒºä¸­çªç„¶å‡ºç°çš„è¡Œäººã€‚æ¯«ç±³æ³¢æŠ€æœ¯å¯é€šè¿‡è¡å°„å’Œåå°„è§‚å¯Ÿéè§†çº¿åŒºåŸŸï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–é¢„è®¾çš„ç©ºé—´ä¿¡æ¯æˆ–ç®€å•çš„å¢™åå°„å‡è®¾ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå®é™…åº”ç”¨èƒ½åŠ›ã€‚å› æ­¤ï¼Œæå‡ºä¸€ç§ç»“åˆå•ç›®ç›¸æœºå›¾åƒä¸é›·è¾¾ç‚¹äº‘æ•°æ®çš„éè§†çº¿è¡Œäººå®šä½æ¡†æ¶ï¼Œé€šè¿‡å›¾åƒåˆ†å‰²æ£€æµ‹åœè½¦è½¦è¾†ï¼Œä¼°è®¡æ·±åº¦è¿›è¡Œç©ºé—´ç‰¹å¾æ¨æ–­ï¼Œå¹¶ä½¿ç”¨é›·è¾¾ç‚¹äº‘æ•°æ®è¿›è¡Œç²¾ç¡®ç©ºé—´æ¨ç†ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æé«˜äº†æ—©æœŸè¡Œäººæ£€æµ‹èƒ½åŠ›ï¼Œæœ‰åŠ©äºæ”¹å–„é“è·¯å®‰å…¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè§†çº¿ç›²åŒºï¼ˆNLoSï¼‰å¯¹é“è·¯å®‰å…¨æ„æˆæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸå¸‚ç¯å¢ƒä¸­è·¯è¾¹åœè½¦é€ æˆçš„ç›²åŒºä¸­çªç„¶å‡ºç°çš„è¡Œäººã€‚</li>
<li>æ¯«ç±³æ³¢æŠ€æœ¯å¯é€šè¿‡è¡å°„å’Œåå°„è§‚å¯Ÿéè§†çº¿åŒºåŸŸï¼Œåœ¨æ£€æµ‹è¢«é®æŒ¡ç‰©ä½“æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–é¢„è®¾çš„ç©ºé—´ä¿¡æ¯æˆ–ç®€å•çš„å¢™åå°„å‡è®¾ï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„é€šç”¨æ€§å’Œåº”ç”¨ã€‚</li>
<li>åœè½¦è½¦è¾†ä½œä¸ºä¸´æ—¶ç©ºé—´éšœç¢ç‰©å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä»¥åŠå®ƒä»¬éšæ—¶å¯èƒ½æ”¹å˜ä½ç½®å¯¼è‡´çš„ç©ºé—´ä¿¡æ¯ä¸å‡†ç¡®é—®é¢˜ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶ç»“åˆäº†å•ç›®ç›¸æœºå›¾åƒå’Œé›·è¾¾ç‚¹äº‘æ•°æ®ï¼Œä»¥æ”¹è¿›éè§†çº¿è¡Œäººçš„å®šä½ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å›¾åƒåˆ†å‰²æ£€æµ‹åœè½¦è½¦è¾†ï¼Œå¹¶é€šè¿‡ä¼°è®¡æ·±åº¦è¿›è¡Œç©ºé—´ç‰¹å¾æ¨æ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-34b6d9a534422adcf0e9306f66bf3953.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa57d46f866bb482b46fb6496c692e59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-babf7d559083b74809d6e0a459a79a2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17f740d4e80d89d399cc9a0fc25481b5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Prototype-Driven-Structure-Synergy-Network-for-Remote-Sensing-Images-Segmentation"><a href="#Prototype-Driven-Structure-Synergy-Network-for-Remote-Sensing-Images-Segmentation" class="headerlink" title="Prototype-Driven Structure Synergy Network for Remote Sensing Images   Segmentation"></a>Prototype-Driven Structure Synergy Network for Remote Sensing Images   Segmentation</h2><p><strong>Authors:Junyi Wang, Jinjiang Li, Guodong Fan, Yakun Ju, Xiang Fang, Alex C. Kot</strong></p>
<p>In the semantic segmentation of remote sensing images, acquiring complete ground objects is critical for achieving precise analysis. However, this task is severely hindered by two major challenges: high intra-class variance and high inter-class similarity. Traditional methods often yield incomplete segmentation results due to their inability to effectively unify class representations and distinguish between similar features. Even emerging class-guided approaches are limited by coarse class prototype representations and a neglect of target structural information.   Therefore, this paper proposes a Prototype-Driven Structure Synergy Network (PDSSNet). The design of this network is based on a core concept, a complete ground object is jointly defined by its invariant class semantics and its variant spatial structure. To implement this, we have designed three key modules. First, the Adaptive Prototype Extraction Module (APEM) ensures semantic accuracy from the source by encoding the ground truth to extract unbiased class prototypes. Subsequently, the designed Semantic-Structure Coordination Module (SSCM) follows a hierarchical semantics-first, structure-second principle. This involves first establishing a global semantic cognition, then leveraging structural information to constrain and refine the semantic representation, thereby ensuring the integrity of class information. Finally, the Channel Similarity Adjustment Module (CSAM) employs a dynamic step-size adjustment mechanism to focus on discriminative features between classes.   Extensive experiments demonstrate that PDSSNet outperforms state-of-the-art methods. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/wangjunyi-1/PDSSNet">https://github.com/wangjunyi-1/PDSSNet</a>. </p>
<blockquote>
<p>åœ¨é¥æ„Ÿå›¾åƒçš„è¯­ä¹‰åˆ†å‰²ä¸­ï¼Œè·å–å®Œæ•´çš„åœ°é¢ç‰©ä½“å¯¹äºå®ç°ç²¾ç¡®åˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™é¡¹ä»»åŠ¡å—åˆ°ä¸¤å¤§æŒ‘æˆ˜çš„ä¸¥é‡é˜»ç¢ï¼šç±»å†…é«˜å˜å¼‚æ€§å’Œç±»é—´é«˜ç›¸ä¼¼æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€ç”±äºæ— æ³•æœ‰æ•ˆåœ°ç»Ÿä¸€ç±»è¡¨ç¤ºå’ŒåŒºåˆ†ç›¸ä¼¼ç‰¹å¾ï¼Œå¯¼è‡´åˆ†å‰²ç»“æœä¸å®Œæ•´ã€‚ç”šè‡³æ–°å…´çš„ç±»å¼•å¯¼æ–¹æ³•ä¹Ÿå—é™äºç²—ç³™çš„ç±»åŸå‹è¡¨ç¤ºå’Œç›®æ ‡ç»“æ„ä¿¡æ¯çš„å¿½è§†ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸå‹é©±åŠ¨çš„ç»“æ„ååŒç½‘ç»œï¼ˆPDSSNetï¼‰ã€‚è¯¥ç½‘ç»œçš„è®¾è®¡åŸºäºä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼šä¸€ä¸ªå®Œæ•´çš„åœ°é¢ç‰©ä½“ç”±å…¶ä¸å˜çš„ç±»è¯­ä¹‰å’Œå…¶å˜åŒ–çš„ç©ºé—´ç»“æ„å…±åŒå®šä¹‰ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ä¸ªå…³é”®æ¨¡å—ã€‚é¦–å…ˆï¼Œè‡ªé€‚åº”åŸå‹æå–æ¨¡å—ï¼ˆAPEMï¼‰é€šè¿‡ç¼–ç çœŸå®å€¼ç¡®ä¿æºçš„è¯­ä¹‰å‡†ç¡®æ€§ï¼Œä»¥æå–æ— åç±»åŸå‹ã€‚æ¥ç€ï¼Œè®¾è®¡çš„è¯­ä¹‰ç»“æ„åè°ƒæ¨¡å—ï¼ˆSSCMï¼‰éµå¾ªåˆ†å±‚è¯­ä¹‰ä¼˜å…ˆã€ç»“æ„å…¶æ¬¡çš„åŸåˆ™ã€‚è¿™åŒ…æ‹¬é¦–å…ˆå»ºç«‹å…¨å±€è¯­ä¹‰è®¤çŸ¥ï¼Œç„¶ååˆ©ç”¨ç»“æ„ä¿¡æ¯æ¥çº¦æŸå’Œç»†åŒ–è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œç¡®ä¿ç±»ä¿¡æ¯çš„å®Œæ•´æ€§ã€‚æœ€åï¼Œé€šé“ç›¸ä¼¼æ€§è°ƒæ•´æ¨¡å—ï¼ˆCSAMï¼‰é‡‡ç”¨åŠ¨æ€æ­¥é•¿è°ƒæ•´æœºåˆ¶ï¼Œå…³æ³¨ç±»ä¹‹é—´çš„åˆ¤åˆ«ç‰¹å¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPDSSNetä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wangjunyi-1/PDSSNet%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wangjunyi-1/PDSSNetä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04022v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç”¨äºé¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„Prototype-Driven Structure Synergy Networkï¼ˆPDSSNetï¼‰ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•æ— æ³•æœ‰æ•ˆç»Ÿä¸€ç±»è¡¨ç¤ºå’ŒåŒºåˆ†ç›¸ä¼¼ç‰¹å¾çš„é—®é¢˜ï¼Œè¯¥ç½‘ç»œé€šè¿‡è‡ªé€‚åº”åŸå‹æå–æ¨¡å—ï¼ˆAPEMï¼‰ç¡®ä¿è¯­ä¹‰å‡†ç¡®æ€§ï¼Œé€šè¿‡è¯­ä¹‰-ç»“æ„åè°ƒæ¨¡å—ï¼ˆSSCMï¼‰å®ç°å…¨å±€è¯­ä¹‰è®¤çŸ¥å¹¶åˆ©ç”¨ç»“æ„ä¿¡æ¯çº¦æŸå’Œç»†åŒ–è¯­ä¹‰è¡¨ç¤ºï¼Œç¡®ä¿ç±»ä¿¡æ¯çš„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç½‘ç»œè¿˜é€šè¿‡é€šé“ç›¸ä¼¼æ€§è°ƒæ•´æ¨¡å—ï¼ˆCSAMï¼‰é‡‡ç”¨åŠ¨æ€æ­¥é•¿è°ƒæ•´æœºåˆ¶ï¼Œå…³æ³¨ç±»é—´åˆ¤åˆ«ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒPDSSNetä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ä¸­ï¼Œè·å–å®Œæ•´åœ°é¢ç‰©ä½“å¯¹ç²¾ç¡®åˆ†æè‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•é¢ä¸´é«˜ç±»å†…æ–¹å·®å’Œé«˜ç±»é—´ç›¸ä¼¼æ€§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´ä¸å®Œå…¨åˆ†å‰²ç»“æœã€‚</li>
<li>PDSSNetç½‘ç»œè®¾è®¡åŸºäºä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼šå®Œæ•´åœ°é¢ç‰©ä½“ç”±å…¶ä¸å˜ç±»è¯­ä¹‰å’Œå¯å˜ç©ºé—´ç»“æ„å…±åŒå®šä¹‰ã€‚</li>
<li>ç½‘ç»œåŒ…å«ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šè‡ªé€‚åº”åŸå‹æå–æ¨¡å—ï¼ˆAPEMï¼‰ç¡®ä¿è¯­ä¹‰å‡†ç¡®æ€§ï¼Œè¯­ä¹‰-ç»“æ„åè°ƒæ¨¡å—ï¼ˆSSCMï¼‰å®ç°å…¨å±€è¯­ä¹‰è®¤çŸ¥å¹¶ç»†åŒ–è¯­ä¹‰è¡¨ç¤ºï¼Œé€šé“ç›¸ä¼¼æ€§è°ƒæ•´æ¨¡å—ï¼ˆCSAMï¼‰å…³æ³¨ç±»é—´åˆ¤åˆ«ç‰¹å¾ã€‚</li>
<li>PDSSNeté€šè¿‡åŠ¨æ€æ­¥é•¿è°ƒæ•´æœºåˆ¶ï¼Œæé«˜äº†å¯¹é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</li>
<li>PDSSNetåœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-abc83a7b2e57e131d2bee48de70429c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42da97de30bcdaa5b266a12152ca0d2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b070d115cd17ddc1867809ddd8bb0385.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-230887be1a115167857219f62e403861.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CAD-Judge-Toward-Efficient-Morphological-Grading-and-Verification-for-Text-to-CAD-Generation"><a href="#CAD-Judge-Toward-Efficient-Morphological-Grading-and-Verification-for-Text-to-CAD-Generation" class="headerlink" title="CAD-Judge: Toward Efficient Morphological Grading and Verification for   Text-to-CAD Generation"></a>CAD-Judge: Toward Efficient Morphological Grading and Verification for   Text-to-CAD Generation</h2><p><strong>Authors:Zheyuan Zhou, Jiayi Han, Liang Du, Naiyu Fang, Lemiao Qiu, Shuyou Zhang</strong></p>
<p>Computer-Aided Design (CAD) models are widely used across industrial design, simulation, and manufacturing processes. Text-to-CAD systems aim to generate editable, general-purpose CAD models from textual descriptions, significantly reducing the complexity and entry barrier associated with traditional CAD workflows. However, rendering CAD models can be slow, and deploying VLMs to review CAD models can be expensive and may introduce reward hacking that degrades the systems. To address these challenges, we propose CAD-Judge, a novel, verifiable reward system for efficient and effective CAD preference grading and grammatical validation. We adopt the Compiler-as-a-Judge Module (CJM) as a fast, direct reward signal, optimizing model alignment by maximizing generative utility through prospect theory. To further improve the robustness of Text-to-CAD in the testing phase, we introduce a simple yet effective agentic CAD generation approach and adopt the Compiler-as-a-Review Module (CRM), which efficiently verifies the generated CAD models, enabling the system to refine them accordingly. Extensive experiments on challenging CAD datasets demonstrate that our method achieves state-of-the-art performance while maintaining superior efficiency. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹åœ¨å·¥ä¸šè®¾è®¡ã€ä»¿çœŸå’Œåˆ¶é€ è¿‡ç¨‹ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚æ–‡æœ¬åˆ°CADç³»ç»Ÿæ—¨åœ¨ä»æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆå¯ç¼–è¾‘çš„é€šç”¨CADæ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—å‡å°‘ä¸ä¼ ç»ŸCADå·¥ä½œæµç¨‹ç›¸å…³çš„å¤æ‚æ€§å’Œå…¥é—¨å£å’ã€‚ç„¶è€Œï¼Œæ¸²æŸ“CADæ¨¡å‹å¯èƒ½ä¼šå¾ˆæ…¢ï¼Œä½¿ç”¨VLMsæ¥å®¡æŸ¥CADæ¨¡å‹å¯èƒ½ä¼šå¾ˆæ˜‚è´µï¼Œå¹¶å¯èƒ½å¼•å…¥å¥–åŠ±ç ´è§£ï¼Œä»è€Œç ´åç³»ç»Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CAD-Judgeï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé«˜æ•ˆæœ‰æ•ˆçš„CADåå¥½åˆ†çº§å’Œè¯­æ³•éªŒè¯çš„å¯éªŒè¯å¥–åŠ±ç³»ç»Ÿã€‚æˆ‘ä»¬é‡‡ç”¨ç¼–è¯‘å™¨ä½œä¸ºæ³•å®˜æ¨¡å—ï¼ˆCJMï¼‰ä½œä¸ºå¿«é€Ÿç›´æ¥çš„å¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡å‰æ™¯ç†è®ºä¼˜åŒ–ç”Ÿæˆå®ç”¨æ€§çš„æœ€å¤§åŒ–ï¼Œä»è€Œå®ç°æ¨¡å‹å¯¹é½çš„ä¼˜åŒ–ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ–‡æœ¬åˆ°CADåœ¨æµ‹è¯•é˜¶æ®µçš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä»£ç†CADç”Ÿæˆæ–¹æ³•ï¼Œå¹¶é‡‡ç”¨ç¼–è¯‘å™¨ä½œä¸ºè¯„å®¡æ¨¡å—ï¼ˆCRMï¼‰ï¼Œæœ‰æ•ˆåœ°éªŒè¯äº†ç”Ÿæˆçš„CADæ¨¡å‹ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œç›¸åº”çš„æ”¹è¿›ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„CADæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å“è¶Šçš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04002v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¸²æŸ“é€Ÿåº¦æ…¢å’ŒéªŒè¯æˆæœ¬é«˜çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†CAD-Judgeç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨Compiler-as-a-Judgeæ¨¡å—ä½œä¸ºå¿«é€Ÿç›´æ¥çš„å¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡å‰æ™¯ç†è®ºä¼˜åŒ–æ¨¡å‹å¯¹é½ï¼Œæé«˜CADåå¥½åˆ†çº§å’Œè¯­æ³•éªŒè¯çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†Compiler-as-a-Reviewæ¨¡å—ï¼Œç”¨äºåœ¨æµ‹è¯•é˜¶æ®µæé«˜æ–‡æœ¬åˆ°CADè½¬æ¢çš„ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ•ˆç‡çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADæ¨¡å‹å¹¿æ³›åº”ç”¨äºå·¥ä¸šè®¾è®¡ã€æ¨¡æ‹Ÿå’Œåˆ¶é€ è¿‡ç¨‹ã€‚</li>
<li>æ–‡æœ¬åˆ°CADç³»ç»Ÿæ—¨åœ¨ä»æ–‡æœ¬æè¿°ç”Ÿæˆå¯ç¼–è¾‘çš„é€šç”¨CADæ¨¡å‹ï¼Œé™ä½ä¼ ç»ŸCADå·¥ä½œæµçš„å¤æ‚æ€§å’Œå…¥é—¨é—¨æ§›ã€‚</li>
<li>CAD-Judgeç³»ç»Ÿæ˜¯ä¸€ç§ç”¨äºé«˜æ•ˆã€æœ‰æ•ˆçš„CADåå¥½åˆ†çº§å’Œè¯­æ³•éªŒè¯çš„å¯éªŒè¯å¥–åŠ±ç³»ç»Ÿã€‚</li>
<li>Compiler-as-a-Judgeæ¨¡å—ä½œä¸ºå¿«é€Ÿç›´æ¥çš„å¥–åŠ±ä¿¡å·ï¼Œä¼˜åŒ–æ¨¡å‹å¯¹é½ï¼Œæé«˜ç”Ÿæˆå®ç”¨æ€§ã€‚</li>
<li>å¼•å…¥Compiler-as-a-Reviewæ¨¡å—ï¼Œæœ‰æ•ˆéªŒè¯ç”Ÿæˆçš„CADæ¨¡å‹ï¼Œæé«˜ç³»ç»Ÿç¨³å¥æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCAD-Judgeç³»ç»Ÿåœ¨ä¿æŒé«˜æ•ˆç‡çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-24fc74709b87e93fe15a32275c1c82cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7af8d56bfcfefc9fe8fd1d4ba79cb240.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-724fe6ba4cc4d3cd7c2aaa74e5684c89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44a2ea20fa9b1ff4a07455f001328abe.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="JanusNet-Hierarchical-Slice-Block-Shuffle-and-Displacement-for-Semi-Supervised-3D-Multi-Organ-Segmentation"><a href="#JanusNet-Hierarchical-Slice-Block-Shuffle-and-Displacement-for-Semi-Supervised-3D-Multi-Organ-Segmentation" class="headerlink" title="JanusNet: Hierarchical Slice-Block Shuffle and Displacement for   Semi-Supervised 3D Multi-Organ Segmentation"></a>JanusNet: Hierarchical Slice-Block Shuffle and Displacement for   Semi-Supervised 3D Multi-Organ Segmentation</h2><p><strong>Authors:Zheng Zhang, Tianzhuzi Tan, Guanchun Yin, Bo Zhang, Xiuzhuang Zhou</strong></p>
<p>Limited by the scarcity of training samples and annotations, weakly supervised medical image segmentation often employs data augmentation to increase data diversity, while randomly mixing volumetric blocks has demonstrated strong performance. However, this approach disrupts the inherent anatomical continuity of 3D medical images along orthogonal axes, leading to severe structural inconsistencies and insufficient training in challenging regions, such as small-sized organs, etc. To better comply with and utilize human anatomical information, we propose JanusNet}, a data augmentation framework for 3D medical data that globally models anatomical continuity while locally focusing on hard-to-segment regions. Specifically, our Slice-Block Shuffle step performs aligned shuffling of same-index slice blocks across volumes along a random axis, while preserving the anatomical context on planes perpendicular to the perturbation axis. Concurrently, the Confidence-Guided Displacement step uses prediction reliability to replace blocks within each slice, amplifying signals from difficult areas. This dual-stage, axis-aligned framework is plug-and-play, requiring minimal code changes for most teacher-student schemes. Extensive experiments on the Synapse and AMOS datasets demonstrate that JanusNet significantly surpasses state-of-the-art methods, achieving, for instance, a 4% DSC gain on the Synapse dataset with only 20% labeled data. </p>
<blockquote>
<p>å—é™äºè®­ç»ƒæ ·æœ¬å’Œæ ‡æ³¨çš„ç¨€ç¼ºæ€§ï¼Œå¼±ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²é€šå¸¸é‡‡ç”¨æ•°æ®å¢å¼ºæ¥å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œè€Œéšæœºæ··åˆä½“ç§¯å—çš„æ–¹æ³•å·²ç»è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šç ´å3DåŒ»å­¦å›¾åƒåœ¨æ­£äº¤è½´ä¸Šçš„å›ºæœ‰è§£å‰–è¿ç»­æ€§ï¼Œå¯¼è‡´ä¸¥é‡çš„ç»“æ„ä¸ä¸€è‡´æ€§å’Œåœ¨æŒ‘æˆ˜æ€§åŒºåŸŸï¼ˆä¾‹å¦‚å°å‹å™¨å®˜ç­‰ï¼‰çš„è®­ç»ƒä¸è¶³ã€‚ä¸ºäº†æ›´å¥½åœ°ç¬¦åˆå’Œåˆ©ç”¨äººä½“è§£å‰–ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†JanusNetï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹3DåŒ»å­¦æ•°æ®çš„æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œèƒ½å¤Ÿå…¨å±€åœ°å»ºæ¨¡è§£å‰–è¿ç»­æ€§ï¼ŒåŒæ—¶åœ¨å±€éƒ¨å…³æ³¨éš¾ä»¥åˆ†å‰²çš„åŒºåŸŸã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„åˆ‡ç‰‡å—æ‰“ä¹±æ­¥éª¤ä¼šåœ¨ä½“ç§¯å†…æ²¿éšæœºè½´æ‰§è¡Œç›¸åŒç´¢å¼•åˆ‡ç‰‡å—çš„å¯¹é½æ‰“ä¹±ï¼ŒåŒæ—¶ä¿ç•™å‚ç›´äºæ‰°åŠ¨è½´çš„å¹³é¢ä¸Šçš„è§£å‰–ä¸Šä¸‹æ–‡ã€‚åŒæ—¶ï¼Œç½®ä¿¡åº¦å¼•å¯¼ä½ç§»æ­¥éª¤ä½¿ç”¨é¢„æµ‹å¯é æ€§æ¥æ›¿æ¢æ¯ä¸ªåˆ‡ç‰‡å†…çš„å—ï¼Œä»è€Œæ”¾å¤§æ¥è‡ªå›°éš¾åŒºåŸŸçš„ä¿¡å·ã€‚è¿™ç§åŒé˜¶æ®µã€è½´å¯¹é½çš„æ¡†æ¶å³æ’å³ç”¨ï¼Œå¯¹äºå¤§å¤šæ•°å¸ˆå¾’æ–¹æ¡ˆæ¥è¯´ï¼Œå‡ ä¹ä¸éœ€è¦ä¿®æ”¹ä»£ç ã€‚åœ¨Synapseå’ŒAMOSæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒJanusNetæ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¾‹å¦‚ï¼Œåœ¨ä»…æœ‰20%æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨Synapseæ•°æ®é›†ä¸Šå®ç°äº†4%çš„DSCå¢ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03997v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹3DåŒ»å­¦æ•°æ®çš„æ–°å‹æ•°æ®å¢å¼ºæ¡†æ¶JanusNetï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å…¨å±€ä¸Šæ¨¡æ‹Ÿäººä½“è§£å‰–è¿ç»­æ€§ï¼ŒåŒæ—¶åœ¨å±€éƒ¨å…³æ³¨éš¾ä»¥åˆ†å‰²çš„åŒºåŸŸã€‚é€šè¿‡åˆ‡ç‰‡å—éšæœºè½´å‘ä¸Šçš„å¯¹é½æ‰“ä¹±æ“ä½œï¼Œä»¥åŠåˆ©ç”¨é¢„æµ‹å¯é æ€§æ›¿æ¢åˆ‡ç‰‡å†…çš„åŒºå—ï¼Œå®ç°äº†å¯¹è§£å‰–ç»“æ„ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨å’Œè¡¥å……ã€‚å®éªŒè¡¨æ˜ï¼ŒJanusNetåœ¨ä»…æœ‰å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»èƒ½åœ¨Synapseç­‰æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JanusNetæ˜¯ä¸€ä¸ªé’ˆå¯¹3DåŒ»å­¦å›¾åƒçš„æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è®­ç»ƒæ ·æœ¬å’Œæ ‡æ³¨ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éšæœºæ··åˆä½“ç§¯å—ï¼Œç ´åäº†3DåŒ»å­¦å›¾åƒçš„è§£å‰–è¿ç»­æ€§ã€‚</li>
<li>JanusNeté€šè¿‡å…¨å±€å»ºæ¨¡è§£å‰–è¿ç»­æ€§ï¼Œå¹¶åœ¨å±€éƒ¨å…³æ³¨éš¾ä»¥åˆ†å‰²åŒºåŸŸæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>JanusNetåŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼šSlice-Block Shuffleï¼ˆåˆ‡ç‰‡å—æ‰“ä¹±ï¼‰å’ŒConfidence-Guided Displacementï¼ˆç½®ä¿¡åº¦å¼•å¯¼ä½ç§»ï¼‰ã€‚</li>
<li>Slice-Block Shuffleæ­¥éª¤æ²¿ç€éšæœºè½´å¯¹ç›¸åŒç´¢å¼•çš„åˆ‡ç‰‡å—è¿›è¡Œå¯¹é½æ‰“ä¹±ï¼ŒåŒæ—¶ä¿æŒå‚ç›´äºæ‰°åŠ¨è½´çš„å¹³é¢ä¸Šçš„è§£å‰–ä¸Šä¸‹æ–‡ã€‚</li>
<li>Confidence-Guided Displacementæ­¥éª¤åˆ©ç”¨é¢„æµ‹å¯é æ€§æ¥æ›¿æ¢åˆ‡ç‰‡å†…çš„åŒºå—ï¼Œä»è€Œæ”¾å¤§å›°éš¾åŒºåŸŸçš„ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ac7f540adc13b8e64d1436b2f34c90b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d108220ec8e198c55e794d53a3fffa3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa99c1b749ba0f68766c561305f82969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a57ecba2856318e06c55079750c226b9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e80976aff71e49a4f1f3191236e77c6d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  NVSpeech An Integrated and Scalable Pipeline for Human-Like Speech   Modeling with Paralinguistic Vocalizations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7b911a887b1630bae4ef42b3788f6ba8.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  HierarchicalPrune Position-Aware Compression for Large-Scale Diffusion   Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
