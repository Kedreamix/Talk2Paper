<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2025-08-08  MuGS Multi-Baseline Generalizable Gaussian Splatting Reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ce4d29d5056acc8ebd1f2d4bbcbb6ca1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-08-更新"><a href="#2025-08-08-更新" class="headerlink" title="2025-08-08 更新"></a>2025-08-08 更新</h1><h2 id="MuGS-Multi-Baseline-Generalizable-Gaussian-Splatting-Reconstruction"><a href="#MuGS-Multi-Baseline-Generalizable-Gaussian-Splatting-Reconstruction" class="headerlink" title="MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction"></a>MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction</h2><p><strong>Authors:Yaopeng Lou, Liao Shen, Tianqi Liu, Jiaqi Li, Zihao Huang, Huiqiang Sun, Zhiguo Cao</strong></p>
<p>We present Multi-Baseline Gaussian Splatting (MuRF), a generalized feed-forward approach for novel view synthesis that effectively handles diverse baseline settings, including sparse input views with both small and large baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and Monocular Depth Estimation (MDE) to enhance feature representations for generalizable reconstruction. Next, We propose a projection-and-sampling mechanism for deep depth fusion, which constructs a fine probability volume to guide the regression of the feature map. Furthermore, We introduce a reference-view loss to improve geometry and optimization efficiency. We leverage 3D Gaussian representations to accelerate training and inference time while enhancing rendering quality. MuRF achieves state-of-the-art performance across multiple baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360 datasets. </p>
<blockquote>
<p>我们提出了多基线高斯平铺（MuRF），这是一种用于新颖视图合成的广义前馈方法，可以有效处理包括具有小和大基线的稀疏输入视图在内的多种基线设置。具体来说，我们整合了多视图立体（MVS）和单眼深度估计（MDE）的特征，以增强可推广重建的特征表示。接着，我们提出了一种用于深度深度融合的投影和采样机制，它构建了一个精细的概率体积来指导特征图的回归。此外，我们引入了一种参考视图损失，以提高几何和优化效率。我们利用3D高斯表示来加快训练和推理时间，同时提高渲染质量。MuRF在多种基线设置和从简单对象（DTU）到复杂室内和室外场景（RealEstate10K）的多种场景中实现了最新技术性能。我们在LLFF和Mip-NeRF 360数据集上展示了有前景的零样本性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04297v1">PDF</a> This work is accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>多基线高斯喷绘（MuRF）是一种广义的前馈方法，用于新颖视角合成，能有效处理包括稀疏输入视图在内的多种基线设置，涵盖小基线和大基线。通过整合多视点立体（MVS）和单眼深度估计（MDE）的特征，提升特征表示的可泛化重建能力。提出一种深度融合投影采样机制，构建精细概率体积以引导特征图的回归。引入参考视图损失提高几何和优化效率。利用三维高斯表示加速训练和推理时间，同时提高渲染质量。MuRF在多种基线设置和从简单对象（DTU）到复杂室内室外场景（RealEstate10K）的多样化场景中实现最先进的性能表现，并在LLFF和Mip-NeRF 360数据集上展示出有前景的零样本性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MuRF是一种用于新颖视角合成的方法，适用于多种基线设置，包括稀疏输入视图。</li>
<li>综合了多视点立体（MVS）和单眼深度估计（MDE）的特征，增强了特征表示的泛化能力。</li>
<li>提出了一种深度融合的投影采样机制，用于构建精细概率体积，引导特征图回归。</li>
<li>引入了参考视图损失，以提升几何还原和优化效率。</li>
<li>采用三维高斯表示加速训练和推理过程，提高渲染质量。</li>
<li>MuRF在多种场景中实现先进性能，包括简单对象和复杂室内室外场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04297">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6fb780814dde9d24d6a64d507567427f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9735e16b71d856246759368983a4741.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7de7e296fc51da47c17621dc4901829.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LiGen-GAN-Augmented-Spectral-Fingerprinting-for-Indoor-Positioning"><a href="#LiGen-GAN-Augmented-Spectral-Fingerprinting-for-Indoor-Positioning" class="headerlink" title="LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning"></a>LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning</h2><p><strong>Authors:Jie Lin, Hsun-Yu Lee, Ho-Ming Li, Fang-Jing Wu</strong></p>
<p>Accurate and robust indoor localization is critical for smart building applications, yet existing Wi-Fi-based systems are often vulnerable to environmental conditions. This work presents a novel indoor localization system, called LiGen, that leverages the spectral intensity patterns of ambient light as fingerprints, offering a more stable and infrastructure-free alternative to radio signals. To address the limited spectral data, we design a data augmentation framework based on generative adversarial networks (GANs), featuring two variants: PointGAN, which generates fingerprints conditioned on coordinates, and FreeGAN, which uses a weak localization model to label unconditioned samples. Our positioning model, leveraging a Multi-Layer Perceptron (MLP) architecture to train on synthesized data, achieves submeter-level accuracy, outperforming Wi-Fi-based baselines by over 50%. LiGen also demonstrates strong robustness in cluttered environments. To the best of our knowledge, this is the first system to combine spectral fingerprints with GAN-based data augmentation for indoor localization. </p>
<blockquote>
<p>精确而稳定的室内定位对于智能建筑应用至关重要，但现有的基于Wi-Fi的系统通常容易受到环境条件的干扰。这项工作提出了一种新型的室内定位系统，名为LiGen，它利用环境光的谱强度模式作为指纹，为无线电信号提供了一种更稳定且无基础设施的替代方案。为了解决有限的谱数据问题，我们设计了一个基于生成对抗网络（GANs）的数据增强框架，其中包括两个变体：PointGAN，它根据坐标生成指纹；FreeGAN，它使用一个弱定位模型来标记无条件的样本。我们的定位模型采用多层感知器（MLP）架构在合成数据上进行训练，实现了亚米级精度，比基于Wi-Fi的基线高出50%以上。LiGen在杂乱的环境中表现出强大的稳健性。据我们所知，这是第一个将谱指纹与基于GAN的数据增强相结合的室内定位系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03024v1">PDF</a> 6 pages, 10 figures</p>
<p><strong>Summary</strong><br>室内定位对于智能建筑应用至关重要，但现有Wi-Fi系统易受环境影响。本研究提出了一种新型室内定位系统LiGen，利用环境光的谱强度模式作为指纹，为无线电信号提供了一种更稳定且无基础设施的替代方案。为解决谱数据有限的问题，研究团队设计了一个基于生成对抗网络（GANs）的数据增强框架，包括PointGAN和FreeGAN两种变体。LiGen采用多层感知器（MLP）架构训练合成数据，实现了亚米级定位精度，较Wi-Fi基线性能高出超过50%，并在杂乱环境中表现出强鲁棒性。此为首个结合光谱指纹和GAN数据增强的室内定位系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>室内定位对智能建筑应用非常重要，但现有Wi-Fi系统存在稳定性问题。</li>
<li>LiGen系统利用环境光的谱强度模式作为指纹，提供稳定的室内定位。</li>
<li>为解决谱数据有限的问题，研究团队采用了基于生成对抗网络（GANs）的数据增强框架。</li>
<li>LiGen系统包括PointGAN和FreeGAN两种变体，分别用于生成条件和非条件样本的指纹。</li>
<li>LiGen采用多层感知器（MLP）架构，通过合成数据训练，实现亚米级定位精度。</li>
<li>LiGen较Wi-Fi基线性能高出超过50%，显示出强鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03024">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-daa671fc2bf71f14c6d773d87c1c080c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9c9e7d539b34c1dc762ab159830a38f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46c8cea43f5d58a221351f8f45c68f80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9117d1f74855e6bd88cc4801c22e83f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fc8e6c2ef1f3ceba7eae30751de464c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89fe912254db87f37bbf3ab849a4c685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1377b05b40282986cb26c847be5c0121.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing"><a href="#GENIE-Gaussian-Encoding-for-Neural-Radiance-Fields-Interactive-Editing" class="headerlink" title="GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing"></a>GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing</h2><p><strong>Authors:Mikołaj Zieliński, Krzysztof Byrski, Tomasz Szczepanik, Przemysław Spurek</strong></p>
<p>Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (<a target="_blank" rel="noopener" href="https://github.com/MikolajZielinski/genie">https://github.com/MikolajZielinski/genie</a>) </p>
<blockquote>
<p>神经辐射场（NeRF）和高斯拼贴（GS）最近已经改变了3D场景表示和渲染的方式。NeRF通过学习神经网络体积表示来实现高保真新型视图合成，但其隐式编码使得编辑和物理交互具有挑战性。相比之下，GS将场景表示为高斯原始数据的显式集合，可实现实时渲染、更快的训练和更直观的操控。这种显式结构使GS特别适合进行交互式编辑以及与基于物理的模拟进行集成。在本文中，我们介绍了GENIE（用于神经辐射场交互式编辑的高斯编码），这是一种结合了NeRF的光照现实渲染质量和GS的可编辑结构化表示的混合模型。我们没有使用球面谐波进行外观建模，而是为每个高斯分配可训练特征嵌入。这些嵌入被用于基于每个查询点的k个最近高斯来条件化NeRF网络。为了使这种条件设置有效，我们引入了基于修改后的光线追踪管道的快速最近高斯搜索——光线追踪高斯邻近搜索（RT-GPS）。我们还集成了多分辨率哈希网格来初始化和更新高斯特征。这些组件共同作用，可实现实时、局部感知的编辑：当高斯原始数据被重新定位或修改时，其插值影响会立即反映在渲染输出中。通过结合隐式和显式表示的优点，GENIE支持直观的场景操作、动态交互以及与物理模拟的兼容性，从而弥合了基于几何的编辑和神经渲染之间的差距。代码可在（<a target="_blank" rel="noopener" href="https://github.com/MikolajZielinski/genie%EF%BC%89%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MikolajZielinski/genie）找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02831v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>NeRF与Gaussian Splatting（GS）的结合实现了高质量的渲染与场景编辑。GENIE模型结合了NeRF的光照真实渲染与GS的明确结构化表示，通过为每个高斯分配可训练特征嵌入，实现了基于k近邻高斯点的NeRF网络条件化。引入的Ray-Traced Gaussian Proximity Search（RT-GPS）技术提高了效率。该模型支持实时、局部感知编辑，可直观操作场景、动态交互，并与物理模拟兼容。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF与GS的结合实现了高质量的渲染与场景编辑，GS的显式结构使其适合交互式编辑和物理模拟。</li>
<li>GENIE模型结合了NeRF的光照真实渲染与GS的明确结构化表示，通过为每个高斯分配特征嵌入，改善了场景编辑和渲染效果。</li>
<li>RT-GPS技术实现了高效的近邻高斯点搜索，提高了渲染速度。</li>
<li>多分辨率哈希网格用于初始化并更新高斯特征。</li>
<li>该模型支持实时、局部感知编辑，实现了直观的场景操作与动态交互。</li>
<li>该模型与物理模拟兼容，缩小了基于几何的编辑和神经渲染之间的差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02831">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e2c4c006deb2aa65e0c9dec5a5886ac6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-423ea2c8f8b707224bc611837cfea5a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bbec3f0559a96a124c722f82ec766bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8bb56f50690c907c2ae50fd9e18e3e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-993037dbe9087953a523c496c7debfd0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d224525486436d37cb3eb931a5d39138.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Imbalance-Robust-and-Sampling-Efficient-Continuous-Conditional-GANs-via-Adaptive-Vicinity-and-Auxiliary-Regularization"><a href="#Imbalance-Robust-and-Sampling-Efficient-Continuous-Conditional-GANs-via-Adaptive-Vicinity-and-Auxiliary-Regularization" class="headerlink" title="Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via   Adaptive Vicinity and Auxiliary Regularization"></a>Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via   Adaptive Vicinity and Auxiliary Regularization</h2><p><strong>Authors:Xin Ding, Yun Chen, Yongwei Wang, Kao Zhang, Sen Zhang, Peibei Cao, Xiangxue Wang</strong></p>
<p>Recent advances in conditional generative modeling have introduced Continuous conditional Generative Adversarial Network (CcGAN) and Continuous Conditional Diffusion Model (CCDM) for estimating high-dimensional data distributions conditioned on scalar, continuous regression labels (e.g., angles, ages, or temperatures). However, these approaches face fundamental limitations: CcGAN suffers from data imbalance due to fixed-size vicinity constraints, while CCDM requires computationally expensive iterative sampling. We present CcGAN-AVAR, an enhanced CcGAN framework that addresses both challenges: (1) leveraging the GAN framework’s native one-step generation to overcome CCDMs’ sampling bottleneck (achieving 300x-2000x faster inference), while (2) two novel components specifically target data imbalance - an adaptive vicinity mechanism that dynamically adjusts vicinity’s size, and a multi-task discriminator that constructs two regularization terms (through auxiliary regression and density ratio estimation) to significantly improve generator training. Extensive experiments on four benchmark datasets (64x64 to 192x192 resolution) across eight challenging imbalanced settings demonstrate that CcGAN-AVAR achieves state-of-the-art generation quality while maintaining sampling efficiency. </p>
<blockquote>
<p>近期条件生成模型的发展引入了连续条件生成对抗网络（CcGAN）和连续条件扩散模型（CCDM），用于估计基于标量、连续回归标签的高维数据分布（例如角度、年龄或温度）。然而，这些方法面临根本性局限：CcGAN受到固定大小邻近约束导致的数据不平衡的影响，而CCDM需要计算量大的迭代采样。我们提出了CcGAN-AVAR，这是一个增强的CcGAN框架，解决了这两个挑战：（1）利用GAN框架的一次性生成来克服CCDM的采样瓶颈（实现300倍至2000倍更快的推理速度），同时（2）两个新组件专门解决数据不平衡问题——一个自适应邻近机制，动态调整邻近大小，和一个多任务鉴别器，构建两个正则化项（通过辅助回归和密度比率估计），以显著改善生成器训练。在四个基准数据集（从64x64到192x192分辨率）上进行的八个具有挑战性的不平衡设置的大量实验表明，CcGAN-AVAR在保持采样效率的同时实现了最先进的生成质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01725v2">PDF</a> </p>
<p><strong>Summary</strong><br>基于连续条件生成对抗网络（CcGAN）和连续条件扩散模型（CCDM）的近期进展，文章提出了CcGAN-AVAR模型，该模型旨在解决在给定标量连续回归标签条件下估计高维数据分布的问题。该模型解决了CcGAN存在的数据不平衡问题和CCDM计算昂贵的迭代采样问题。通过利用GAN框架的一步生成特性，实现了快速的推理过程，并引入两个新组件来解决数据不平衡问题，包括自适应邻近机制和多任务鉴别器。实验结果表明，CcGAN-AVAR在生成质量和采样效率方面均达到了业界最佳水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CcGAN-AVAR解决了现有连续条件生成模型面临的数据不平衡和计算昂贵的迭代采样问题。</li>
<li>该模型利用GAN的一步生成特性，实现了快速推理。</li>
<li>CcGAN-AVAR通过自适应邻近机制动态调整邻近大小，解决数据不平衡问题。</li>
<li>多任务鉴别器通过辅助回归和密度比率估计构建两个正则化项，显著改善生成器训练。</li>
<li>CcGAN-AVAR在四个基准数据集上的实验结果表明，它在生成质量方面达到了业界最佳水平。</li>
<li>该模型适用于不同分辨率的数据集，从64x64到192x192。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01725">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b7c4deecf9ff689fe20c521a4b28d48f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ebd74a861aa4147ce065a319c90a782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01e865c2fedb2e0690efee02d7177957.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c82e9f34975b6c3da762ff12a1c61fe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f9b9795b6837a72275bcbd02e115493.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d79a15f7261ab4da6f7e0bec5f4eb5bc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UnMix-NeRF-Spectral-Unmixing-Meets-Neural-Radiance-Fields"><a href="#UnMix-NeRF-Spectral-Unmixing-Meets-Neural-Radiance-Fields" class="headerlink" title="UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields"></a>UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields</h2><p><strong>Authors:Fabian Perez, Sara Rojas, Carlos Hinojosa, Hoover Rueda-Chacón, Bernard Ghanem</strong></p>
<p>Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: <a target="_blank" rel="noopener" href="https://www.factral.co/UnMix-NeRF">https://www.factral.co/UnMix-NeRF</a>. </p>
<blockquote>
<p>基于神经辐射场（NeRF）的分割方法主要关注对象语义，并且仅依赖于RGB数据，缺乏内在材料属性。这一局限性限制了材料感知的准确性，对于机器人、增强现实、模拟和其他应用而言，这是至关重要的。我们引入了UnMix-NeRF框架，它将光谱混合技术集成到NeRF中，实现了联合高光谱新视角合成和无监督材料分割。我们的方法通过漫反射和镜面反射成分对光谱反射进行建模，其中通过全局端元学习字典表示纯材料特征，而每点的丰度则捕捉其分布。对于材料分割，我们使用沿学习端元的频谱特征预测，从而实现无监督材料聚类。此外，UnMix-NeRF还通过修改学习的端元字典，实现了场景的编辑，以便进行灵活的材料外观操纵。大量的实验验证了我们方法的有效性，表现出优越的光谱重建和材料分割性能。项目页面：<a target="_blank" rel="noopener" href="https://www.factral.co/UnMix-NeRF%E3%80%82">https://www.factral.co/UnMix-NeRF。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21884v2">PDF</a> Paper accepted at ICCV 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>基于神经辐射场（NeRF）的分割方法主要关注对象语义，并仅依赖于RGB数据，缺乏内在材料属性。本文引入UnMix-NeRF框架，将光谱混合技术集成到NeRF中，实现联合高光谱新视角合成和无监督材料分割。该方法通过漫反射和镜面反射成分模拟光谱反射，其中通过全局端元学习字典表示纯材料签名，每点的丰度捕捉其分布。对于材料分割，使用光谱签名预测和学习的端元进行无监督材料聚类。此外，UnMix-NeRF可通过修改学习的端元字典实现场景编辑，为基于材料的外观操纵提供灵活性。实验证明，该方法在光谱重建和材料分割方面优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UnMix-NeRF框架集成了光谱混合技术到NeRF中。</li>
<li>该方法实现了联合高光谱新视角合成和无监督材料分割。</li>
<li>UnMix-NeRF模拟光谱反射通过漫反射和镜面反射成分。</li>
<li>利用全局端元学习字典表示纯材料签名。</li>
<li>通过光谱签名预测和学习的端元进行无监督材料聚类。</li>
<li>UnMix-NeRF能实现场景编辑，具有灵活的材料外观操纵功能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21884">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9be234d48d1729d4bac210e2b42dea84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce4d29d5056acc8ebd1f2d4bbcbb6ca1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-447fdaa6bf12ed632ddae3bf47bba947.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Knowledge-Distillation-for-Underwater-Feature-Extraction-and-Matching-via-GAN-synthesized-Images"><a href="#Knowledge-Distillation-for-Underwater-Feature-Extraction-and-Matching-via-GAN-synthesized-Images" class="headerlink" title="Knowledge Distillation for Underwater Feature Extraction and Matching   via GAN-synthesized Images"></a>Knowledge Distillation for Underwater Feature Extraction and Matching   via GAN-synthesized Images</h2><p><strong>Authors:Jinghe Yang, Mingming Gong, Ye Pu</strong></p>
<p>Autonomous Underwater Vehicles (AUVs) play a crucial role in underwater exploration. Vision-based methods offer cost-effective solutions for localization and mapping in the absence of conventional sensors like GPS and LiDAR. However, underwater environments present significant challenges for feature extraction and matching due to image blurring and noise caused by attenuation, scattering, and the interference of \textit{marine snow}. In this paper, we aim to improve the robustness of the feature extraction and matching in the turbid underwater environment using the cross-modal knowledge distillation method that transfers the in-air feature extraction and matching models to underwater settings using synthetic underwater images as the medium. We first propose a novel adaptive GAN-synthesis method to estimate water parameters and underwater noise distribution, to generate environment-specific synthetic underwater images. We then introduce a general knowledge distillation framework compatible with different teacher models. The evaluation of GAN-based synthesis highlights the significance of the new components, i.e. GAN-synthesized noise and forward scattering, in the proposed model. Additionally, VSLAM, as a representative downstream application of feature extraction and matching, is employed on real underwater sequences to validate the effectiveness of the transferred model. Project page: <a target="_blank" rel="noopener" href="https://github.com/Jinghe-mel/UFEN-GAN">https://github.com/Jinghe-mel/UFEN-GAN</a>. </p>
<blockquote>
<p>自主水下车辆（AUVs）在水下探测中起着至关重要的作用。在没有GPS和激光雷达等传统传感器的情况下，基于视觉的方法为定位和地图绘制提供了经济高效的解决方案。然而，水下环境由于衰减、散射和“海洋雪”的干扰导致的图像模糊和噪声，给特征提取和匹配带来了重大挑战。在本文中，我们旨在利用跨模态知识蒸馏方法，通过合成水下图像作为媒介，将空气中的特征提取和匹配模型转移到水下环境，从而提高在浑浊水下环境中特征提取和匹配的稳健性。我们首先提出了一种新颖的自适应GAN合成方法，以估计水参数和水下噪声分布，生成特定环境的合成水下图像。然后，我们介绍了一个与不同教师模型兼容的通用知识蒸馏框架。基于GAN的合成评估突出了新组件，即GAN合成噪声和前向散射，在拟议模型中的重要性。此外，VSLAM作为特征提取和匹配的下游应用的代表，被用于真实的水下序列，以验证转移模型的有效性。项目页面：<a target="_blank" rel="noopener" href="https://github.com/Jinghe-mel/UFEN-GAN%E3%80%82">https://github.com/Jinghe-mel/UFEN-GAN。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08253v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文聚焦于在浑浊水下环境中提高特征提取与匹配的稳健性。采用跨模态知识蒸馏方法，借助合成水下图像，将空中特征提取与匹配模型转移至水下场景。创新性地提出自适应GAN合成方法，用于估算水参数和噪声分布，生成针对环境的水下图像。此外，还引入与不同教师模型兼容的一般知识蒸馏框架。通过基于GAN的合成评估验证了新方法的有效性。同时，采用视觉SLAM作为特征提取与匹配下游应用的代表，在真实水下序列上验证了转移模型的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AUVs在浑浊水下环境中面临特征提取和匹配的挑战。图像模糊和噪声影响特征提取与匹配效果。</li>
<li>采用跨模态知识蒸馏方法提高水下环境特征提取与匹配的稳健性。利用合成水下图像将空中模型转移至水下场景。</li>
<li>提出自适应GAN合成方法，用于估算水参数和噪声分布，生成环境特定的水下图像。此方法是提高知识蒸馏效率的关键步骤之一。</li>
<li>介绍通用知识蒸馏框架，兼容多种教师模型，提升模型的适应性和灵活性。</li>
<li>基于GAN的合成评估验证了新方法的有效性，其中GAN合成的噪声和前向散射等要素起到了重要作用。</li>
<li>采用视觉SLAM作为特征提取与匹配的下游应用代表，验证转移模型在实际水下场景中的效果。</li>
<li>项目页面提供详细信息和代码链接（<a target="_blank" rel="noopener" href="https://github.com/Jinghe-mel/UFEN-GAN%EF%BC%89%E3%80%82">https://github.com/Jinghe-mel/UFEN-GAN）。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-83974effe7915b4c8d143da80ed1bcc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a729ee8b0b158fd2956f043b67e22099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a841c791449bd355f7e24e8624f73d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b53efb9c6aba2c3b49fce78de2e5ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4726e3ddd0351772742a45c95780bfe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d696600b68bfe62b032e3b06e3c43d7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DivCon-NeRF-Diverse-and-Consistent-Ray-Augmentation-for-Few-Shot-NeRF"><a href="#DivCon-NeRF-Diverse-and-Consistent-Ray-Augmentation-for-Few-Shot-NeRF" class="headerlink" title="DivCon-NeRF: Diverse and Consistent Ray Augmentation for Few-Shot NeRF"></a>DivCon-NeRF: Diverse and Consistent Ray Augmentation for Few-Shot NeRF</h2><p><strong>Authors:Ingyun Lee, Jae Won Jang, Seunghyeon Seo, Nojun Kwak</strong></p>
<p>Neural Radiance Field (NeRF) has shown remarkable performance in novel view synthesis but requires numerous multi-view images, limiting its practicality in few-shot scenarios. Ray augmentation has been proposed to alleviate overfitting caused by sparse training data by generating additional rays. However, existing methods, which generate augmented rays only near the original rays, exhibit pronounced floaters and appearance distortions due to limited viewpoints and inconsistent rays obstructed by nearby obstacles and complex surfaces. To address these problems, we propose DivCon-NeRF, which introduces novel sphere-based ray augmentations to significantly enhance both diversity and consistency. By employing a virtual sphere centered at the predicted surface point, our method generates diverse augmented rays from all 360-degree directions, facilitated by our consistency mask that effectively filters out inconsistent rays. We introduce tailored loss functions that leverage these augmentations, effectively reducing floaters and visual distortions. Consequently, our method outperforms existing few-shot NeRF approaches on the Blender, LLFF, and DTU datasets. Furthermore, DivCon-NeRF demonstrates strong generalizability by effectively integrating with both regularization- and framework-based few-shot NeRFs. Our code will be made publicly available. </p>
<blockquote>
<p>神经辐射场（NeRF）在新型视图合成中展现出了显著的性能，但需要大量多视图图像，这在少量场景的情况下限制了其实用性。射线增强法被提出来通过生成额外的射线来缓解由稀疏训练数据引起的过拟合问题。然而，现有方法仅在原始射线附近生成增强射线，由于有限的观点、附近障碍物的阻挡以及复杂表面的不一致性，会出现明显的浮点和外观失真。为了解决这些问题，我们提出了DivCon-NeRF，它引入了基于球体的新型射线增强法，以显著提高多样性和一致性。通过以预测的表面点为中心构建一个虚拟球体，我们的方法从所有360度方向生成多样化的增强射线，这得益于我们的一致性掩码，它能有效地过滤出不一致的射线。我们引入了利用这些增强的定制损失函数，有效地减少了浮点和视觉失真。因此，我们的方法在Blender、LLFF和DTU数据集上的少量射击NeRF方法表现优越。此外，DivCon-NeRF通过有效地与基于正则化和框架的少量射击NeRF相结合，表现出强大的泛化能力。我们的代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12947v2">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>神经网络辐射场（NeRF）在新型视角合成方面展现出卓越性能，但需依赖大量多视角图像，限制了其在小样本场景中的应用。为解决因稀疏训练数据导致的过拟合问题，研究者提出了射线增强技术。然而，现有方法仅在接近原始射线时生成增强射线，在有限视角和复杂表面下的邻近障碍物的阻碍下，容易出现明显的浮动和外观失真。为此，本文提出DivCon-NeRF方法，引入基于球体的新型射线增强技术，显著提高了多样性和一致性。通过以预测表面点为中心的虚拟球体生成来自所有360度方向的多样化增强射线，并使用一致性掩膜有效过滤出不一致的射线。此外，本文还引入利用这些增强的定制损失函数，有效减少浮动和视觉失真。该方法在Blender、LLFF和DTU数据集上均优于现有小样本NeRF方法，并展示了强大的泛化能力，能有效整合正则化和框架式小样本NeRFs。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF技术在新型视角合成领域表现优异，但受限于需要大量多视角图像，对小样本场景实用性有限。</li>
<li>现有射线增强技术主要解决稀疏数据过拟合问题，但存在因视角和障碍物导致的浮动和外观失真问题。</li>
<li>DivCon-NeRF方法通过引入基于球体的新型射线增强技术，提高多样性和一致性。</li>
<li>采用虚拟球体生成增强射线，覆盖全部360度方向，使用一致性掩膜过滤不一致射线。</li>
<li>定制损失函数利用增强射线，有效减少浮动和视觉失真。</li>
<li>DivCon-NeRF在多个数据集上表现优于现有小样本NeRF方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12947">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a1fc9044fe906fb7e967040ba4fe4a47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4e082a672359d7707a671c92865aaaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2856282c7c5ddae0d763d90b19ed5355.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa2e1c850c9e2d627a552569db3e3636.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7b911a887b1630bae4ef42b3788f6ba8.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-08-08  HierarchicalPrune Position-Aware Compression for Large-Scale Diffusion   Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9f6783b7143ec4f25063d879b7cabcc9.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-08-08  Pseudo Depth Meets Gaussian A Feed-forward RGB SLAM Baseline
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
