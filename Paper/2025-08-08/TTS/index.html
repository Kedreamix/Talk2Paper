<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  NVSpeech An Integrated and Scalable Pipeline for Human-Like Speech   Modeling with Paralinguistic Vocalizations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e80976aff71e49a4f1f3191236e77c6d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-08-æ›´æ–°"><a href="#2025-08-08-æ›´æ–°" class="headerlink" title="2025-08-08 æ›´æ–°"></a>2025-08-08 æ›´æ–°</h1><h2 id="NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations"><a href="#NVSpeech-An-Integrated-and-Scalable-Pipeline-for-Human-Like-Speech-Modeling-with-Paralinguistic-Vocalizations" class="headerlink" title="NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech   Modeling with Paralinguistic Vocalizations"></a>NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech   Modeling with Paralinguistic Vocalizations</h2><p><strong>Authors:Huan Liao, Qinke Ni, Yuancheng Wang, Yiheng Lu, Haoyue Zhan, Pengyuan Xie, Qiang Zhang, Zhizheng Wu</strong></p>
<p>Paralinguistic vocalizations-including non-verbal sounds like laughter and breathing, as well as lexicalized interjections such as â€œuhmâ€ and â€œohâ€-are integral to natural spoken communication. Despite their importance in conveying affect, intent, and interactional cues, such cues remain largely overlooked in conventional automatic speech recognition (ASR) and text-to-speech (TTS) systems. We present NVSpeech, an integrated and scalable pipeline that bridges the recognition and synthesis of paralinguistic vocalizations, encompassing dataset construction, ASR modeling, and controllable TTS. (1) We introduce a manually annotated dataset of 48,430 human-spoken utterances with 18 word-level paralinguistic categories. (2) We develop the paralinguistic-aware ASR model, which treats paralinguistic cues as inline decodable tokens (e.g., â€œYouâ€™re so funny [Laughter]â€), enabling joint lexical and non-verbal transcription. This model is then used to automatically annotate a large corpus, the first large-scale Chinese dataset of 174,179 utterances (573 hours) with word-level alignment and paralingustic cues. (3) We finetune zero-shot TTS models on both human- and auto-labeled data to enable explicit control over paralinguistic vocalizations, allowing context-aware insertion at arbitrary token positions for human-like speech synthesis. By unifying the recognition and generation of paralinguistic vocalizations, NVSpeech offers the first open, large-scale, word-level annotated pipeline for expressive speech modeling in Mandarin, integrating recognition and synthesis in a scalable and controllable manner. Dataset and audio demos are available at <a target="_blank" rel="noopener" href="https://nvspeech170k.github.io/">https://nvspeech170k.github.io/</a>. </p>
<blockquote>
<p>å‰¯è¯­è¨€å‘å£°ï¼ŒåŒ…æ‹¬éè¯­è¨€å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå‘¼å¸å£°ï¼‰ï¼Œä»¥åŠè¯æ±‡åŒ–æ„Ÿå¹è¯ï¼ˆå¦‚â€œå‘ƒâ€å’Œâ€œå“¦â€ï¼‰ï¼Œæ˜¯è‡ªç„¶å£è¯­äº¤æµçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡å®ƒä»¬åœ¨ä¼ è¾¾æƒ…æ„Ÿã€æ„å›¾å’Œäº¤äº’çº¿ç´¢æ–¹é¢å¾ˆé‡è¦ï¼Œä½†è¿™äº›çº¿ç´¢åœ¨ä¼ ç»Ÿè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­ä»ç„¶è¢«å¿½è§†ã€‚æˆ‘ä»¬æå‡ºäº†NVSpeechï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆä¸”å¯æ‰©å±•çš„ç®¡é“ï¼Œèƒ½å¤Ÿè¯†åˆ«å¹¶åˆæˆå‰¯è¯­è¨€å‘å£°ï¼ŒåŒ…æ‹¬æ•°æ®é›†æ„å»ºã€ASRå»ºæ¨¡å’Œå¯æ§TTSã€‚</p>
</blockquote>
<p>ï¼ˆ1ï¼‰æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªåŒ…å«48430ä¸ªäººç±»å£è¯­å‘éŸ³çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«18ä¸ªå•è¯çº§åˆ«çš„å‰¯è¯­è¨€ç±»åˆ«ã€‚<br>ï¼ˆ2ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ„ŸçŸ¥å‰¯è¯­è¨€çš„ASRæ¨¡å‹ï¼Œå®ƒå°†å‰¯è¯­è¨€çº¿ç´¢è§†ä¸ºå†…è”å¯è§£ç ä»¤ç‰Œï¼ˆä¾‹å¦‚ï¼Œâ€œä½ å¾ˆæœ‰è¶£[ç¬‘å£°]â€ï¼‰ï¼Œä»è€Œå®ç°è¯æ±‡å’Œéè¨€è¯­çš„å…±åŒè½¬å½•ã€‚ç„¶åï¼Œè¯¥æ¨¡å‹è¢«ç”¨äºè‡ªåŠ¨æ³¨é‡Šå¤§è§„æ¨¡è¯­æ–™åº“ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡æ•°æ®é›†ï¼ŒåŒ…å«174179ä¸ªå‘éŸ³ï¼ˆå…±573å°æ—¶ï¼‰ï¼Œå…·æœ‰å•è¯çº§åˆ«çš„å¯¹é½å’Œå‰¯è¯­è¨€çº¿ç´¢ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04195v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªç„¶å£è¯­äº¤æµä¸­çš„é‡è¦æ€§éè¨€è¯­å£°éŸ³å’Œè¯æ±‡åŒ–æ’è¯ï¼Œè¿™äº›åœ¨å¸¸è§„è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆç³»ç»Ÿä¸­å¸¸è¢«å¿½è§†ã€‚æå‡ºäº†ä¸€ç§é›†è¯†åˆ«ä¸åˆæˆä¸ºä¸€ä½“çš„å¯æ‰©å±•ç®¡é“NVSpeechï¼Œè¯¥ç®¡é“åŒ…å«æ•°æ®é›†æ„å»ºã€è¯­éŸ³è¯†åˆ«å»ºæ¨¡å’Œå¯æ§è¯­éŸ³åˆæˆã€‚NVSpeechä¸ºè¡¨è¾¾æ€§è¯­éŸ³å»ºæ¨¡æä¾›äº†é¦–ä¸ªå¼€æ”¾çš„å¤§è§„æ¨¡è¯è¯­çº§æ³¨é‡Šç®¡é“ï¼Œä»¥å¯æ‰©å±•å’Œå¯æ§çš„æ–¹å¼ç»Ÿä¸€è¯†åˆ«å’Œç”Ÿæˆéè¨€è¯­å‘å£°ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè¨€è¯­å‘å£°ï¼ˆå¦‚ç¬‘å£°å’Œå‘¼å¸ï¼‰ä»¥åŠè¯æ±‡åŒ–æ’è¯ï¼ˆå¦‚â€œå‘ƒâ€å’Œâ€œå“¦â€ï¼‰å¯¹äºè‡ªç„¶å£è¯­äº¤æµè‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³åˆæˆï¼ˆTTSï¼‰ç³»ç»Ÿå¿½ç•¥äº†è¿™äº›è¡¨è¾¾æ€§å‘å£°çš„é‡è¦æ€§ã€‚</li>
<li>NVSpeechæ˜¯ä¸€ä¸ªé›†æˆäº†è¯†åˆ«å’Œåˆæˆçš„å¯æ‰©å±•ç®¡é“ï¼ŒåŒ…å«æ•°æ®é›†æ„å»ºã€ASRå»ºæ¨¡å’Œå¯æ§TTSã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŒ…å«18ä¸ªè¯çº§çš„éè¨€è¯­ç±»åˆ«æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§éè¨€è¯­æ„ŸçŸ¥çš„ASRæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå°†éè¨€è¯­å‘å£°è§†ä¸ºå†…è”è§£ç ä»¤ç‰Œï¼Œä»è€Œå®ç°è¯æ±‡å’Œéè¨€è¯­çš„è”åˆè½¬å½•ã€‚</li>
<li>ä½¿ç”¨è¯¥æ¨¡å‹è‡ªåŠ¨æ³¨é‡Šäº†ä¸€ä¸ªå¤§è§„æ¨¡è¯­æ–™åº“ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„ä¸­æ–‡è¯­æ–™åº“ï¼ŒåŒ…å«è¯çº§å¯¹é½å’Œéè¨€è¯­çº¿ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04195">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b5d36064ac6cd9d0f22d443d02403bd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-715dae7b676ec31fe5febb2466293386.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6ec6b76a0c3ac06fdba5842721656b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c259daee3f593ec5aece40872f04a80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08c54201151545ffe5c64f25541de239.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1627b0be13dd133e5ec182a259f41a1d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-State-Of-TTS-A-Case-Study-with-Human-Fooling-Rates"><a href="#The-State-Of-TTS-A-Case-Study-with-Human-Fooling-Rates" class="headerlink" title="The State Of TTS: A Case Study with Human Fooling Rates"></a>The State Of TTS: A Case Study with Human Fooling Rates</h2><p><strong>Authors:Praveen Srinivasa Varadhan, Sherry Thomas, Sai Teja M. S., Suvrat Bhooshan, Mitesh M. Khapra</strong></p>
<p>While subjective evaluations in recent years indicate rapid progress in TTS, can current TTS systems truly pass a human deception test in a Turing-like evaluation? We introduce Human Fooling Rate (HFR), a metric that directly measures how often machine-generated speech is mistaken for human. Our large-scale evaluation of open-source and commercial TTS models reveals critical insights: (i) CMOS-based claims of human parity often fail under deception testing, (ii) TTS progress should be benchmarked on datasets where human speech achieves high HFRs, as evaluating against monotonous or less expressive reference samples sets a low bar, (iii) Commercial models approach human deception in zero-shot settings, while open-source systems still struggle with natural conversational speech; (iv) Fine-tuning on high-quality data improves realism but does not fully bridge the gap. Our findings underscore the need for more realistic, human-centric evaluations alongside existing subjective tests. </p>
<blockquote>
<p>å°½ç®¡è¿‘å¹´æ¥çš„ä¸»è§‚è¯„ä¼°è¡¨æ˜æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯è¿…é€Ÿè¿›æ­¥ï¼Œä½†å½“å‰TTSç³»ç»Ÿæ˜¯å¦çœŸæ­£èƒ½é€šè¿‡å›¾çµæµ‹è¯•è¿™æ ·çš„äººç±»æ¬ºéª—æµ‹è¯•å‘¢ï¼Ÿæˆ‘ä»¬å¼•å…¥äº†â€œäººç±»æ¬ºéª—ç‡â€ï¼ˆHFRï¼‰è¿™ä¸€æŒ‡æ ‡ï¼Œå®ƒç›´æ¥è¡¡é‡æœºå™¨ç”Ÿæˆçš„è¯­éŸ³è¢«è¯¯è®¤ä¸ºäººç±»çš„é¢‘ç‡ã€‚æˆ‘ä»¬å¯¹å¼€æºå’Œå•†ä¸šTTSæ¨¡å‹çš„å¤§è§„æ¨¡è¯„ä¼°æ­ç¤ºäº†ä¸€äº›å…³é”®è§è§£ï¼šï¼ˆiï¼‰åŸºäºCMOSçš„äººç±»ç­‰åŒä¸»å¼ é€šå¸¸åœ¨æ¬ºéª—æµ‹è¯•ä¸­å¤±è´¥ï¼Œï¼ˆiiï¼‰TTSçš„è¿›æ­¥åº”è¯¥åœ¨äººç±»è¯­éŸ³å®ç°é«˜HFRçš„æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå› ä¸ºä¸å•è°ƒæˆ–ç¼ºä¹è¡¨ç°åŠ›çš„å‚è€ƒæ ·æœ¬è¿›è¡Œè¯„ä¼°ä¼šè®¾ç½®ä¸€ä¸ªä½æ ‡å‡†ï¼Œï¼ˆiiiï¼‰å•†ä¸šæ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­å¯ä»¥æ¥è¿‘äººç±»æ¬ºéª—ï¼Œè€Œå¼€æºç³»ç»Ÿä»ç„¶åœ¨è‡ªç„¶å¯¹è¯è¯­éŸ³æ–¹é¢æŒ£æ‰ï¼›ï¼ˆivï¼‰åœ¨é«˜è´¨é‡æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥æé«˜çœŸå®æ€§ï¼Œä½†å¹¶ä¸èƒ½å®Œå…¨å¼¥è¡¥å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†é™¤äº†ç°æœ‰çš„ä¸»è§‚æµ‹è¯•ä¹‹å¤–ï¼Œè¿˜éœ€è¦æ›´ç°å®ã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04179v1">PDF</a> Accepted at InterSpeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Human Fooling Rateï¼ˆHFRï¼‰è¿™ä¸€æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æœºå™¨ç”Ÿæˆçš„è¯­éŸ³è¢«è¯¯åˆ¤ä¸ºäººç±»è¯­éŸ³çš„é¢‘ç‡ï¼Œä»¥æ­¤è¯„ä¼°TTSç³»ç»Ÿçš„è¿›æ­¥ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡è¿‘å¹´æ¥ä¸»è§‚è¯„ä¼°æ˜¾ç¤ºTTSè¿…é€Ÿè¿›æ­¥ï¼Œä½†åœ¨æ¬ºéª—æµ‹è¯•ä¸­äººæœºéš¾è¾¨çœŸå‡çš„æƒ…å†µå¹¶ä¸æ™®éã€‚å¯¹å¼€æºå’Œå•†ä¸šTTSæ¨¡å‹çš„å¤§è§„æ¨¡è¯„ä¼°æ­ç¤ºäº†å…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Human Fooling Rateï¼ˆHFRï¼‰æ˜¯è¡¡é‡TTSç³»ç»Ÿè¿›æ­¥çš„ç›´æ¥æŒ‡æ ‡ï¼Œåæ˜ æœºå™¨ç”Ÿæˆè¯­éŸ³è¢«è¯¯åˆ¤ä¸ºäººç±»è¯­éŸ³çš„é¢‘ç‡ã€‚</li>
<li>ç°æœ‰çš„å£°ç§°ä¸äººç±»è¡¨ç°ç›¸å½“çš„CMOSåœ¨æ¬ºéª—æµ‹è¯•ä¸‹ç»å¸¸å¤±æ•ˆã€‚</li>
<li>TTSçš„è¿›å±•åº”ä»¥äººç±»è¯­éŸ³è¾¾åˆ°é«˜HFRçš„æ•°æ®é›†ä¸ºåŸºå‡†è¿›è¡Œè¯„ä¼°ï¼Œå› ä¸ºä¸å•è°ƒæˆ–ç¼ºä¹è¡¨è¾¾åŠ›çš„å‚ç…§æ ·æœ¬è¿›è¡Œè¯„ä¼°ä¼šè®¾å®šè¾ƒä½çš„æ ‡å‡†ã€‚</li>
<li>å•†ä¸šæ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹æ¥è¿‘äººç±»æ¬ºéª—æ°´å¹³ï¼Œè€Œå¼€æºç³»ç»Ÿåœ¨è‡ªç„¶ä¼šè¯è¯­éŸ³æ–¹é¢ä»æœ‰å›°éš¾ã€‚</li>
<li>é«˜è´¨é‡æ•°æ®çš„ç²¾ç»†è°ƒæ•´å¯ä»¥æé«˜çœŸå®æ€§ï¼Œä½†å¹¶ä¸èƒ½å®Œå…¨å¼¥è¡¥å·®è·ã€‚</li>
<li>éœ€è¦æ›´ç°å®ã€ä»¥äººä¸ºä¸­å¿ƒçš„è¯„ä»·æ–¹æ³•ï¼Œä¸ç°æœ‰çš„ä¸»è§‚æµ‹è¯•ç›¸ç»“åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cfaa5c0ea2199d49f30490fd125b0d6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0a9a044bec59a8a554c778a6418869.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3aca61df15ca4212728bd5bf2ad2331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ad46333fd8f1c3cd07dd3d398746ea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78bd42d0e03f29df6c0b0f09466e31c8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech"><a href="#Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech" class="headerlink" title="Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech"></a>Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech</h2><p><strong>Authors:Jingyuan Xing, Zhipeng Li, Jialong Mai, Xiaofen Xing, Xiangmin Xu</strong></p>
<p>Advances in speech representation and large language models have enhanced zero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS models face challenges in capturing the complex correlations between acoustic and semantic features, resulting in a lack of expressiveness and similarity. The primary reason lies in the complex relationship between semantic and acoustic features, which manifests independent and interdependent aspects.This paper introduces a TTS framework that combines both autoregressive (AR) and non-autoregressive (NAR) modules to harmonize the independence and interdependence of acoustic and semantic information. The AR model leverages the proposed Parallel Tokenizer to synthesize the top semantic and acoustic tokens simultaneously. In contrast, considering the interdependence, the Coupled NAR model predicts detailed tokens based on the general AR modelâ€™s output. Parallel GPT, built on this architecture, is designed to improve zero-shot text-to-speech synthesis through its parallel structure. Experiments on English and Chinese datasets demonstrate that the proposed model significantly outperforms the quality and efficiency of the synthesis of existing zero-shot TTS models. Speech demos are available at <a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/">https://t1235-ch.github.io/pgpt/</a>. </p>
<blockquote>
<p>è¯­éŸ³è¡¨ç¤ºçš„è¿›å±•å’Œå¤§è¯­è¨€æ¨¡å‹çš„è¿›æ­¥å·²ç»æé«˜äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›¶æ ·æœ¬TTSæ¨¡å‹åœ¨æ•æ‰å£°å­¦ç‰¹å¾å’Œè¯­ä¹‰ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³è”æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´è¡¨ç°åŠ›ä¸è¶³å’Œç›¸ä¼¼æ€§ä¸è¶³ã€‚ä¸»è¦åŸå› åœ¨äºè¯­ä¹‰å’Œå£°å­¦ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œè¿™è¡¨ç°å‡ºç‹¬ç«‹å’Œç›¸äº’ä¾å­˜çš„æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04141v1">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing (TASLP)</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ç»“åˆè‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å—çš„TTSæ¡†æ¶ï¼Œæ—¨åœ¨è°ƒå’Œå£°å­¦è¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ï¼Œä»è€Œæé«˜é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚è¯¥æ¡†æ¶åœ¨è‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡æ˜¾è‘—ä¼˜äºç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æåˆ°äº†ç°æœ‰çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³æ•æ‰å£°å­¦è¯­ä¹‰ç‰¹å¾çš„å¤æ‚å…³è”æ€§çš„å›°éš¾ï¼Œå¯¼è‡´ç¼ºä¹è¡¨è¾¾æ€§å’Œç›¸ä¼¼æ€§ã€‚</li>
<li>ä»‹ç»äº†æ–°çš„TTSæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å—ï¼Œæ—¨åœ¨è°ƒå’Œå£°å­¦è¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ã€‚</li>
<li>ARæ¨¡å‹åˆ©ç”¨æå‡ºçš„å¹¶è¡Œåˆ†è¯å™¨åŒæ—¶åˆæˆé¡¶çº§è¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ã€‚</li>
<li>è€ƒè™‘ç›¸äº’ä¾èµ–æ€§ï¼Œè€¦åˆçš„NARæ¨¡å‹åŸºäºARæ¨¡å‹çš„è¾“å‡ºé¢„æµ‹è¯¦ç»†çš„æ ‡è®°ã€‚</li>
<li>åŸºäºæ­¤æ¶æ„æ„å»ºçš„å¹¶è¡ŒGPTæ—¨åœ¨é€šè¿‡å…¶å¹¶è¡Œç»“æ„æ”¹è¿›é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆã€‚</li>
<li>åœ¨è‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åˆæˆè´¨é‡å’Œæ•ˆç‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12f0de09a0900c1d7a9800be441359cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa37142f69f7e05c7ee1f1ccc92eb754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92c8ad1224a54c6c9f7d2d2a40d2c8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d0677a13b72211796ef8e9ac8a1254d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TROOP-At-the-Roofline-Performance-for-Vector-Processors-on-Low-Operational-Intensity-Workloads"><a href="#TROOP-At-the-Roofline-Performance-for-Vector-Processors-on-Low-Operational-Intensity-Workloads" class="headerlink" title="TROOP: At-the-Roofline Performance for Vector Processors on Low   Operational Intensity Workloads"></a>TROOP: At-the-Roofline Performance for Vector Processors on Low   Operational Intensity Workloads</h2><p><strong>Authors:Navaneeth Kunhi Purayil, Diyou Shen, Matteo Perotti, Luca Benini</strong></p>
<p>The fast evolution of Machine Learning (ML) models requires flexible and efficient hardware solutions as hardwired accelerators face rapid obsolescence. Vector processors are fully programmable and achieve high energy efficiencies by exploiting data parallelism, amortizing instruction fetch and decoding costs. Hence, a promising design choice is to build accelerators based on shared L1-memory clusters of streamlined Vector Processing Elements (VPEs). However, current state-of-the-art VPEs are limited in L1 memory bandwidth and achieve high efficiency only for computational kernels with high data reuse in the Vector Register File (VRF), such as General Matrix Multiplication (GEMM). Performance is suboptimal for workloads with lower data reuse like General Matrix-Vector Multiplication (GEMV). To fully exploit available bandwidth at the L1 memory interface, the VPE micro-architecture must be optimized to achieve near-ideal utilization, i.e., to be as close as possible to the L1 memory roofline (at-the-roofline). In this work, we propose TROOP, a set of hardware optimizations that include decoupled load-store interfaces, improved vector chaining, shadow buffers to hide VRF conflicts, and address scrambling techniques to achieve at-the-roofline performance for VPEs without compromising their area and energy efficiency. We implement TROOP on an open-source streamlined vector processor in a 12nm FinFET technology. TROOP achieves significant speedups of 1.5x, 2.2x, and 2.6x, respectively, for key memory-intensive kernels such as GEMV, DOTP and AXPY, achieving at-the-roofline performance. Additionally, TROOP enhances the energy efficiency by up to 45%, reaching 38 DP-GFLOPs&#x2F;W (1 GHz, TT, 0.8V) for DOTP while maintaining a high energy efficiency of 61 DP-GFLOPs&#x2F;W for GEMMs, incurring only a minor area overhead of less than 7%. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹çš„å¿«é€Ÿè¿›åŒ–è¦æ±‚çµæ´»é«˜æ•ˆçš„ç¡¬ä»¶è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿå™¨é¢ä¸´è¿…é€Ÿæ·˜æ±°çš„é—®é¢˜ã€‚å‘é‡å¤„ç†å™¨å…·å¤‡å®Œå…¨å¯ç¼–ç¨‹æ€§ï¼Œé€šè¿‡åˆ©ç”¨æ•°æ®å¹¶è¡Œæ€§å¹¶æ‘Šé”€æŒ‡ä»¤è·å–å’Œè§£ç æˆæœ¬ï¼Œå®ç°äº†é«˜èƒ½æ•ˆã€‚å› æ­¤ï¼Œä¸€ä¸ªæœ‰å‰é€”çš„è®¾è®¡é€‰æ‹©æ˜¯æ„å»ºåŸºäºå…±äº«L1å†…å­˜é›†ç¾¤çš„æµçº¿å‹å‘é‡å¤„ç†å…ƒç´ ï¼ˆVPEï¼‰çš„åŠ é€Ÿå™¨ã€‚ç„¶è€Œï¼Œç›®å‰æœ€å…ˆè¿›çš„VPEåœ¨L1å†…å­˜å¸¦å®½æ–¹é¢å­˜åœ¨é™åˆ¶ï¼Œä»…å¯¹äºåœ¨å‘é‡å¯„å­˜å™¨æ–‡ä»¶ï¼ˆVRFï¼‰ä¸­æ•°æ®é‡ç”¨ç‡é«˜çš„è®¡ç®—å†…æ ¸ï¼ˆä¾‹å¦‚é€šç”¨çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰ï¼‰æ‰èƒ½å®ç°é«˜æ•ˆç‡ã€‚å¯¹äºå·¥ä½œé‡è¾ƒå°ã€æ•°æ®é‡ç”¨ç‡è¾ƒä½çš„ä»»åŠ¡ï¼ˆå¦‚é€šç”¨çŸ©é˜µå‘é‡ä¹˜æ³•ï¼ˆGEMVï¼‰ï¼‰è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨L1å†…å­˜æ¥å£å¯ç”¨çš„å¸¦å®½ï¼Œå¿…é¡»ä¼˜åŒ–VPEçš„å¾®è§‚ç»“æ„ä»¥å®ç°è¿‘ç†æƒ³çš„åˆ©ç”¨ç‡ï¼Œå³å°½å¯èƒ½æ¥è¿‘L1å†…å­˜é™åˆ¶çº¿ï¼ˆå±‹é¡¶çº¿ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TROOPï¼Œè¿™æ˜¯ä¸€ç»„ç¡¬ä»¶ä¼˜åŒ–æªæ–½ï¼ŒåŒ…æ‹¬è§£è€¦çš„åŠ è½½å­˜å‚¨æ¥å£ã€æ”¹è¿›çš„å‘é‡é“¾ã€éšè—VRFå†²çªçš„é˜´å½±ç¼“å†²åŒºä»¥åŠåœ°å€æ‰°ä¹±æŠ€æœ¯ï¼Œä»¥å®ç°VPEçš„ç†æƒ³æ€§èƒ½ï¼ŒåŒæ—¶ä¸æŸå®³å…¶é¢ç§¯å’Œèƒ½æ•ˆã€‚æˆ‘ä»¬åœ¨é‡‡ç”¨å¼€æ”¾æºä»£ç çš„æµçº¿å‹å‘é‡å¤„ç†å™¨å’Œ12çº³ç±³FinFETæŠ€æœ¯ä¸Šå®ç°äº†TROOPã€‚TROOPé’ˆå¯¹å…³é”®å†…å­˜å¯†é›†å‹å†…æ ¸ï¼ˆå¦‚GEMVã€DOTPå’ŒAXPYï¼‰å®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼Œåˆ†åˆ«ä¸ºåŸæ¥çš„1.5å€ã€2.2å€å’Œ2.6å€ï¼Œè¾¾åˆ°äº†ç†æƒ³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTROOPæé«˜äº†èƒ½æ•ˆé«˜è¾¾45%ï¼Œåœ¨è¿›è¡ŒDOTPæ“ä½œæ—¶è¾¾åˆ°æ¯ç“¦38 DP-GFLOPsï¼ˆåœ¨é¢‘ç‡ä¸ºæ¯ç§’åäº¿æ¬¡æ—¶ï¼‰ï¼ŒåŒæ—¶ä¿æŒå¯¹GEMMçš„é«˜èƒ½æ•ˆä¸ºæ¯ç“¦61 DP-GFLOPsã€‚è¯¥è®¾è®¡ä»…æœ‰å°äº7%çš„é¢ç§¯å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03900v1">PDF</a> To be published in IEEE International Conference on Computer Design   (ICCD) 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¹ç¡¬ä»¶è§£å†³æ–¹æ¡ˆçš„çµæ´»æ€§å’Œæ•ˆç‡æå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚å‘é‡å¤„ç†å™¨é€šè¿‡åˆ©ç”¨æ•°æ®å¹¶è¡Œæ€§å®ç°é«˜èƒ½æ•ˆï¼Œé’ˆå¯¹æ­¤ï¼Œä¸€ç§æœ‰å‰æ™¯çš„è®¾è®¡æ˜¯æ„å»ºåŸºäºå…±äº«L1å†…å­˜é›†ç¾¤çš„æµçº¿å‹å‘é‡å¤„ç†å…ƒç´ ï¼ˆVPEï¼‰ã€‚ä½†ç°æœ‰VPEåœ¨L1å†…å­˜å¸¦å®½ä¸Šå­˜åœ¨å±€é™ï¼Œå¯¹äºçŸ©é˜µå‘é‡ä¹˜æ³•ç­‰ä½æ•°æ®å¤ç”¨å·¥ä½œè´Ÿè½½æ€§èƒ½ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºTROOPç¡¬ä»¶ä¼˜åŒ–æ–¹æ¡ˆï¼ŒåŒ…æ‹¬è§£è€¦è´Ÿè½½å­˜å‚¨æ¥å£ã€æ”¹è¿›å‘é‡é“¾ã€éšè—VRFå†²çªçš„å½±å­ç¼“å†²åŒºä»¥åŠåœ°å€æ··æ·†æŠ€æœ¯ï¼Œæ—¨åœ¨å®ç°æ¥è¿‘L1å†…å­˜å¸¦å®½æé™çš„ç†æƒ³åˆ©ç”¨ï¼Œæé«˜VPEæ€§èƒ½ã€‚åœ¨12nm FinFETæŠ€æœ¯ä¸Šå®ç°çš„å¼€æºæµçº¿å‹å¤„ç†å™¨ä¸Šåº”ç”¨TROOPåï¼Œé’ˆå¯¹å…³é”®å†…å­˜å¯†é›†å‹æ ¸å¿ƒæ“ä½œï¼Œå¦‚GEMVã€DOTPå’ŒAXPYç­‰ï¼Œå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼Œå¹¶æé«˜äº†èƒ½æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¿«é€Ÿå‘å±•è¦æ±‚ç¡¬ä»¶è§£å†³æ–¹æ¡ˆå…·å¤‡çµæ´»æ€§å’Œé«˜æ•ˆç‡ã€‚</li>
<li>å‘é‡å¤„ç†å™¨é€šè¿‡æ•°æ®å¹¶è¡Œæ€§å®ç°é«˜èƒ½æ•ˆï¼Œæ˜¯ç¡¬ä»¶åŠ é€Ÿçš„ä¸€ç§æœ‰å‰é€”çš„è®¾è®¡ã€‚</li>
<li>å½“å‰å‘é‡å¤„ç†å™¨åœ¨L1å†…å­˜å¸¦å®½ä¸Šå­˜åœ¨å±€é™ï¼Œå¯¹äºæŸäº›å·¥ä½œè´Ÿè½½æ€§èƒ½ä¸ä½³ã€‚</li>
<li>TROOPæ˜¯ä¸€ç§ç¡¬ä»¶ä¼˜åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨æé«˜å‘é‡å¤„ç†å™¨çš„æ€§èƒ½å¹¶æ¥è¿‘L1å†…å­˜å¸¦å®½æé™ã€‚</li>
<li>TROOPåŒ…æ‹¬è§£è€¦è´Ÿè½½å­˜å‚¨æ¥å£ã€æ”¹è¿›å‘é‡é“¾ã€éšè—VRFå†²çªçš„å½±å­ç¼“å†²åŒºä»¥åŠåœ°å€æ··æ·†æŠ€æœ¯ã€‚</li>
<li>åœ¨å¼€æºæµçº¿å‹å¤„ç†å™¨ä¸Šåº”ç”¨TROOPåï¼Œå®ç°äº†å¯¹å…³é”®å†…å­˜å¯†é›†å‹æ ¸å¿ƒæ“ä½œçš„é€Ÿåº¦æå‡å’Œèƒ½æ•ˆæé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c416b282acc785a5d77d25d2b4b59037.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca0845191d62641cb6f0ada88ddf99c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d08e47d2827023fb55bd6395a056df02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b105233de75b467a32641230e91a3cc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5fde1e604009d8cd26394e681f3ae9b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EmoSteer-TTS-Fine-Grained-and-Training-Free-Emotion-Controllable-Text-to-Speech-via-Activation-Steering"><a href="#EmoSteer-TTS-Fine-Grained-and-Training-Free-Emotion-Controllable-Text-to-Speech-via-Activation-Steering" class="headerlink" title="EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable   Text-to-Speech via Activation Steering"></a>EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable   Text-to-Speech via Activation Steering</h2><p><strong>Authors:Tianxin Xie, Shan Yang, Chenxing Li, Dong Yu, Li Liu</strong></p>
<p>Text-to-speech (TTS) has shown great progress in recent years. However, most existing TTS systems offer only coarse and rigid emotion control, typically via discrete emotion labels or a carefully crafted and detailed emotional text prompt, making fine-grained emotion manipulation either inaccessible or unstable. These models also require extensive, high-quality datasets for training. To address these limitations, we propose EmoSteer-TTS, a novel training-free approach, to achieve fine-grained speech emotion control (conversion, interpolation, erasure) by activation steering. We first empirically observe that modifying a subset of the internal activations within a flow matching-based TTS model can effectively alter the emotional tone of synthesized speech. Building on this insight, we then develop a training-free and efficient algorithm, including activation extraction, emotional token searching, and inference-time steering, which can be seamlessly integrated into a wide range of pretrained models (e.g., F5-TTS, CosyVoice2, and E2-TTS). In addition, to derive effective steering vectors, we construct a curated emotional speech dataset with diverse speakers. Extensive experiments demonstrate that EmoSteer-TTS enables fine-grained, interpretable, and continuous control over speech emotion, outperforming the state-of-the-art (SOTA). To the best of our knowledge, this is the first method that achieves training-free and continuous fine-grained emotion control in TTS. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åœ¨æœ€è¿‘å‡ å¹´å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„TTSç³»ç»Ÿåªæä¾›ç²—ç³™ä¸”åƒµåŒ–çš„æƒ…æ„Ÿæ§åˆ¶ï¼Œé€šå¸¸æ˜¯é€šè¿‡ç¦»æ•£çš„æƒ…ç»ªæ ‡ç­¾æˆ–ç²¾å¿ƒè®¾è®¡å’Œè¯¦ç»†çš„æƒ…ç»ªæ–‡æœ¬æç¤ºæ¥å®ç°ï¼Œè¿™ä½¿å¾—ç²¾ç»†ç²’åº¦çš„æƒ…æ„Ÿæ“æ§å˜å¾—ä¸å¯è®¿é—®æˆ–ä¸ç¨³å®šã€‚è¿™äº›æ¨¡å‹è¿˜éœ€è¦å¤§é‡é«˜è´¨é‡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EmoSteer-TTSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ¿€æ´»æ§åˆ¶æ¥å®ç°ç²¾ç»†ç²’åº¦çš„è¯­éŸ³æƒ…æ„Ÿæ§åˆ¶ï¼ˆè½¬æ¢ã€æ’å€¼ã€æ“¦é™¤ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆä»å®è¯ä¸Šè§‚å¯Ÿåˆ°ï¼Œä¿®æ”¹åŸºäºæµåŒ¹é…çš„TTSæ¨¡å‹å†…éƒ¨æ¿€æ´»çš„ä¸€éƒ¨åˆ†å¯ä»¥æœ‰æ•ˆåœ°æ”¹å˜åˆæˆè¯­éŸ³çš„æƒ…æ„ŸåŸºè°ƒã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œç„¶åæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ— è®­ç»ƒä¸”é«˜æ•ˆçš„ç®—æ³•ï¼ŒåŒ…æ‹¬æ¿€æ´»æå–ã€æƒ…æ„Ÿæ ‡è®°æœç´¢å’Œæ¨ç†æ—¶é—´æ§åˆ¶ï¼Œè¯¥ç®—æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°å„ç§é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¾‹å¦‚F5-TTSã€CosyVoice2å’ŒE2-TTSï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¾—å‡ºæœ‰æ•ˆçš„æ§åˆ¶å‘é‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§è¯´è¯äººçš„ç²¾é€‰æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEmoSteer-TTSèƒ½å¤Ÿå¯¹è¯­éŸ³æƒ…æ„Ÿè¿›è¡Œç²¾ç»†ã€å¯è§£é‡Šå’Œè¿ç»­çš„æ§åˆ¶ï¼Œä¼˜äºå½“å‰æœ€ä½³æ°´å¹³ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯TTSä¸­å®ç°æ— è®­ç»ƒå’Œè¿ç»­ç²¾ç»†ç²’åº¦æƒ…æ„Ÿæ§åˆ¶çš„ç¬¬ä¸€ç§æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03543v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„æœ€æ–°ç ”ç©¶æˆæœã€‚é’ˆå¯¹ç°æœ‰TTSç³»ç»Ÿåœ¨æƒ…ç»ªæ§åˆ¶æ–¹é¢çš„å±€é™ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„TTSæƒ…æ„Ÿæ“æ§æ–°æ–¹æ³•â€”â€”EmoSteer-TTSã€‚é€šè¿‡æ¿€æ´»è°ƒæ§ï¼ŒEmoSteer-TTSå¯ä»¥å®ç°ç²¾ç»†åŒ–çš„è¯­éŸ³æƒ…æ„Ÿæ§åˆ¶ï¼ˆè½¬æ¢ã€æ’å€¼ã€æ¶ˆé™¤ï¼‰ã€‚è¯¥æ–¹æ³•åŸºäºè§‚å¯Ÿåˆ°ä¸€ä¸ªç°è±¡ï¼šé€šè¿‡è°ƒæ•´åŸºäºæµåŒ¹é…çš„TTSæ¨¡å‹å†…éƒ¨æ¿€æ´»çš„ä¸€éƒ¨åˆ†å¯ä»¥å½±å“åˆæˆè¯­éŸ³çš„æƒ…æ„Ÿè°ƒå­ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„é«˜æ•ˆç®—æ³•ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§é¢„è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç”Ÿæˆæœ‰æ•ˆçš„è°ƒæ§å‘é‡ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§è¯´è¯äººçš„ç²¾é€‰æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼ŒEmoSteer-TTSå¯å®ç°ç²¾ç»†ã€å¯è§£é‡Šã€è¿ç»­çš„æƒ…æ„Ÿæ§åˆ¶ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¿™æ˜¯é¦–ä¸ªå®ç°æ— éœ€è®­ç»ƒä¸”èƒ½è¿ç»­è¿›è¡Œç²¾ç»†åŒ–æƒ…æ„Ÿæ§åˆ¶çš„TTSæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EmoSteer-TTSæ˜¯ä¸€ç§æ–°å‹çš„æ— éœ€è®­ç»ƒçš„TTSæƒ…æ„Ÿæ“æ§æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç»†åŒ–çš„è¯­éŸ³æƒ…æ„Ÿæ§åˆ¶ã€‚</li>
<li>é€šè¿‡è°ƒæ•´åŸºäºæµåŒ¹é…çš„TTSæ¨¡å‹å†…éƒ¨æ¿€æ´»çš„ä¸€éƒ¨åˆ†ï¼Œå¯ä»¥å½±å“åˆæˆè¯­éŸ³çš„æƒ…æ„Ÿè°ƒå­ã€‚</li>
<li>EmoSteer-TTSåŒ…æ‹¬æ¿€æ´»æå–ã€æƒ…æ„Ÿæ ‡è®°æœç´¢å’Œæ¨ç†æ—¶è°ƒæ§ç­‰æ­¥éª¤ï¼Œå¯æ— ç¼é›†æˆåˆ°å„ç§é¢„è®­ç»ƒæ¨¡å‹ä¸­ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§è¯´è¯äººçš„ç²¾é€‰æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ï¼Œç”¨äºç”Ÿæˆæœ‰æ•ˆçš„è°ƒæ§å‘é‡ã€‚</li>
<li>EmoSteer-TTSå¯å®ç°ç²¾ç»†ã€å¯è§£é‡Šã€è¿ç»­çš„æƒ…æ„Ÿæ§åˆ¶ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>EmoSteer-TTSæ˜¯é¦–ä¸ªå®ç°æ— éœ€è®­ç»ƒä¸”èƒ½è¿ç»­è¿›è¡Œç²¾ç»†åŒ–æƒ…æ„Ÿæ§åˆ¶çš„TTSæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0025824b5206b8aa016e40b6e26c10e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-273c68aef975c56ee1c06bb7ca5e4370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-caad050d4f6b45d4ce083fb35846bfe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0261c4c0e807933a12768c0dc250e322.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b1535288e2a417a9986ac4fc6867f55.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CTTS-Collective-Test-Time-Scaling"><a href="#CTTS-Collective-Test-Time-Scaling" class="headerlink" title="CTTS: Collective Test-Time Scaling"></a>CTTS: Collective Test-Time Scaling</h2><p><strong>Authors:Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Tao Chen</strong></p>
<p>Test-time scaling (TTS) has emerged as a promising research field for enhancing the effectiveness of large language models (LLMs) without extra training. However, most existing approaches, e.g., Best-of-N and Self-Consistency rely on a single agent interacting with a reward model (SA-SR), constrained by limited capabilities of a single test-time scaling (STTS) paradigm. On the other hand, recent works demonstrate that collective-agent methods can break through the upper bound of single-agent systems by orchestrating diverse models. Thus, in this paper, we take a first step towards exploring Collective Test-Time Scaling (CTTS). Consider the different interaction types of single and multiple models, we design three primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent to multiple reward models (SA-MR); (2) multiple agents to single reward model (MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive experiments demonstrate that MA-MR consistently achieves the best performance. Based on this, we propose a novel framework named CTTS-MM that effectively leverages both multi-agent and multi-reward-model collaboration for enhanced inference. Specifically, for multi-agent collaboration, we propose an Agent Collaboration Search (ACS), which searches for the most effective combination of LLM agents from a large candidate pool; for multi-reward-model collaboration, we propose Mixture of Reword Models (MoR), which consists of a curated question pool and a Prior Reward model Ensemble Selection (PRES) to select the optimal combinations of reward models via Pair-wise Reward Ranking (PRR) metric. Experiments across seven mainstream benchmarks demonstrate that the proposed CTTS-MM consistently obtains superior performance. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/magent4aci/CTTS-MM">https://github.com/magent4aci/CTTS-MM</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ä½œä¸ºä¸€ä¸ªæœ‰å‰é€”çš„ç ”ç©¶é¢†åŸŸï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚æœ€ä½³Né€‰å’Œè‡ªä¸€è‡´æ€§ï¼Œéƒ½ä¾èµ–äºå•ä¸ªä»£ç†ä¸å¥–åŠ±æ¨¡å‹ï¼ˆSA-SRï¼‰è¿›è¡Œäº¤äº’ï¼Œå—é™äºå•ä¸€çš„æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆSTTSï¼‰æ¨¡å¼çš„æœ‰é™èƒ½åŠ›ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé›†ä½“ä»£ç†æ–¹æ³•å¯ä»¥é€šè¿‡åè°ƒä¸åŒçš„æ¨¡å‹çªç ´å•ä»£ç†ç³»ç»Ÿçš„ä¸Šé™ã€‚å› æ­¤ï¼Œæœ¬æ–‡æœç€æ¢ç´¢é›†ä½“æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆCTTSï¼‰è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚è€ƒè™‘åˆ°å•ä¸ªå’Œå¤šä¸ªæ¨¡å‹çš„ä¸åŒäº¤äº’ç±»å‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ç§ä¸»è¦èŒƒå¼æ¥ç ”ç©¶CTTSçš„æœ€ä½³èŒƒå¼ï¼šï¼ˆ1ï¼‰å•ä»£ç†åˆ°å¤šå¥–åŠ±æ¨¡å‹ï¼ˆSA-MRï¼‰ï¼›ï¼ˆ2ï¼‰å¤šä»£ç†åˆ°å•å¥–åŠ±æ¨¡å‹ï¼ˆMA-SRï¼‰ï¼›ä»¥åŠï¼ˆ3ï¼‰å¤šä»£ç†åˆ°å¤šå¥–åŠ±æ¨¡å‹ï¼ˆMA-MRï¼‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMA-MRæŒç»­å–å¾—æœ€ä½³æ€§èƒ½ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºCTTS-MMçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒæœ‰æ•ˆåœ°åˆ©ç”¨å¤šä»£ç†å’Œå¤šå¥–åŠ±æ¨¡å‹çš„åˆä½œè¿›è¡Œå¢å¼ºæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºå¤šä»£ç†åä½œï¼Œæˆ‘ä»¬æå‡ºäº†ä»£ç†åä½œæœç´¢ï¼ˆACSï¼‰ï¼Œå®ƒä»å¤§é‡å€™é€‰æ± ä¸­æœç´¢æœ€æœ‰æ•ˆçš„LLMä»£ç†ç»„åˆï¼›å¯¹äºå¤šå¥–åŠ±æ¨¡å‹åä½œï¼Œæˆ‘ä»¬æå‡ºäº†å¥–åŠ±æ¨¡å‹æ··åˆç‰©ï¼ˆMoRï¼‰ï¼Œå®ƒç”±ä¸€ä¸ªç²¾é€‰çš„é—®é¢˜æ± å’Œå…ˆéªŒå¥–åŠ±æ¨¡å‹é›†åˆé€‰æ‹©ï¼ˆPRESï¼‰ç»„æˆï¼Œé€šè¿‡é…å¯¹å¥–åŠ±æ’åï¼ˆPRRï¼‰æŒ‡æ ‡é€‰æ‹©æœ€ä½³çš„å¥–åŠ±æ¨¡å‹ç»„åˆã€‚åœ¨ä¸ƒä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„CTTS-MMæŒç»­è·å¾—ä¼˜è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/magent4aci/CTTS-MM%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/magent4aci/CTTS-MMå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03333v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest-Time Scalingï¼Œç®€ç§°TTSï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºä¼ ç»Ÿçš„TTSæ–¹æ³•å—é™äºå•ä¸€æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆSTTSï¼‰èŒƒå¼çš„èƒ½åŠ›ï¼Œè€Œé›†ä½“æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆCollective Test-Time Scalingï¼Œç®€ç§°CTTSï¼‰èƒ½çªç ´å•ä¸€ä»£ç†ç³»ç»Ÿçš„ä¸Šé™ã€‚æ–‡ç« é€šè¿‡æ¢ç´¢ä¸‰ç§ä¸»è¦çš„CTTSèŒƒå¼ï¼Œæå‡ºä¸€ä¸ªåä¸ºCTTS-MMçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåˆ©ç”¨å¤šä»£ç†å’Œå¤šå¥–åŠ±æ¨¡å‹åˆä½œè¿›è¡Œæ¨æ–­ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒCTTS-MMåœ¨ä¸ƒä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ˜¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ•ˆæœçš„ç ”ç©¶é¢†åŸŸã€‚</li>
<li>ä¼ ç»ŸTTSæ–¹æ³•å—é™äºå•ä¸€æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆSTTSï¼‰èŒƒå¼çš„èƒ½åŠ›ã€‚</li>
<li>é›†ä½“æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆCTTSï¼‰èƒ½çªç ´å•ä¸€ä»£ç†ç³»ç»Ÿçš„é™åˆ¶ã€‚</li>
<li>æ–‡ç« æ¢ç´¢äº†ä¸‰ç§ä¸»è¦çš„CTTSèŒƒå¼ï¼šSA-MRã€MA-SRå’ŒMA-MRã€‚</li>
<li>MA-MRèŒƒå¼åœ¨å®éªŒä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„CTTS-MMæ¡†æ¶ç»“åˆå¤šä»£ç†å’Œå¤šå¥–åŠ±æ¨¡å‹åˆä½œè¿›è¡Œæ¨æ–­ã€‚</li>
<li>CTTS-MMåœ¨ä¸ƒä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f179ee1b9d097b0929074fb8f240d40f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9723a5ea9c51a65cb5b134e8f8f76a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fea085440b22e8dfd15d836ce37a5518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c67e44d6a86e5ba68c096121a9e03f83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac8931e9c68dc3d08db7c7652550b08a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MiSTR-Multi-Modal-iEEG-to-Speech-Synthesis-with-Transformer-Based-Prosody-Prediction-and-Neural-Phase-Reconstruction"><a href="#MiSTR-Multi-Modal-iEEG-to-Speech-Synthesis-with-Transformer-Based-Prosody-Prediction-and-Neural-Phase-Reconstruction" class="headerlink" title="MiSTR: Multi-Modal iEEG-to-Speech Synthesis with Transformer-Based   Prosody Prediction and Neural Phase Reconstruction"></a>MiSTR: Multi-Modal iEEG-to-Speech Synthesis with Transformer-Based   Prosody Prediction and Neural Phase Reconstruction</h2><p><strong>Authors:Mohammed Salah Al-Radhi, GÃ©za NÃ©meth, Branislav Gerazov</strong></p>
<p>Speech synthesis from intracranial EEG (iEEG) signals offers a promising avenue for restoring communication in individuals with severe speech impairments. However, achieving intelligible and natural speech remains challenging due to limitations in feature representation, prosody modeling, and phase reconstruction. We introduce MiSTR, a deep-learning framework that integrates: 1) Wavelet-based feature extraction to capture fine-grained temporal, spectral, and neurophysiological representations of iEEG signals, 2) A Transformer-based decoder for prosody-aware spectrogram prediction, and 3) A neural phase vocoder enforcing harmonic consistency via adaptive spectral correction. Evaluated on a public iEEG dataset, MiSTR achieves state-of-the-art speech intelligibility, with a mean Pearson correlation of 0.91 between reconstructed and original Mel spectrograms, improving over existing neural speech synthesis baselines. </p>
<blockquote>
<p>ä»é¢…å†…è„‘ç”µå›¾ï¼ˆiEEGï¼‰ä¿¡å·è¿›è¡Œè¯­éŸ³åˆæˆï¼Œä¸ºæ‚£æœ‰ä¸¥é‡è¯­è¨€éšœç¢çš„äººæ¢å¤äº¤æµæä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç”±äºç‰¹å¾è¡¨ç¤ºã€éŸµå¾‹å»ºæ¨¡å’Œç›¸ä½é‡å»ºç­‰æ–¹é¢çš„å±€é™æ€§ï¼Œå®ç°å¯ç†è§£å’Œè‡ªç„¶çš„è¯­éŸ³ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MiSTRï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ï¼š1ï¼‰åŸºäºå°æ³¢çš„ç‰¹å¾æå–ï¼Œç”¨äºæ•è·iEEGä¿¡å·çš„ç»†ç²’åº¦æ—¶é—´ã€é¢‘è°±å’Œç¥ç»ç”Ÿç†è¡¨ç¤ºï¼›2ï¼‰åŸºäºå˜å‹å™¨çš„è§£ç å™¨ï¼Œç”¨äºå…·æœ‰éŸµå¾‹æ„ŸçŸ¥çš„é¢‘è°±å›¾é¢„æµ‹ï¼›3ï¼‰ç¥ç»ç›¸ä½ç¼–ç å™¨é€šè¿‡è‡ªé€‚åº”å…‰è°±æ ¡æ­£æ¥å¼ºåˆ¶æ‰§è¡Œè°æ³¢ä¸€è‡´æ€§ã€‚åœ¨å…¬å…±iEEGæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒMiSTRè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¯­éŸ³å¯æ‡‚åº¦ï¼Œé‡å»ºçš„Melé¢‘è°±å›¾ä¸åŸå§‹Melé¢‘è°±å›¾ä¹‹é—´çš„å¹³å‡Pearsonç›¸å…³æ€§ä¸º0.91ï¼Œè¶…è¿‡äº†ç°æœ‰çš„ç¥ç»è¯­éŸ³åˆæˆåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03166v1">PDF</a> 5 pages, 2 figures, 1 table. Accepted for presentation at Interspeech   2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºé¢…å†…è„‘ç”µå›¾ï¼ˆiEEGï¼‰ä¿¡å·çš„è¯­éŸ³åˆæˆå¯¹äºæ¢å¤ä¸¥é‡è¨€è¯­éšœç¢è€…çš„äº¤æµèƒ½åŠ›å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼Œç”±äºç‰¹å¾è¡¨ç¤ºã€è¯­è°ƒå»ºæ¨¡å’Œç›¸ä½é‡å»ºæ–¹é¢çš„å±€é™æ€§ï¼Œå®ç°å¯ç†è§£å’Œè‡ªç„¶çš„è¯­éŸ³ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶å¼•å…¥MiSTRæ·±åº¦æ¡†æ¶ï¼Œé›†æˆä»¥ä¸‹æŠ€æœ¯ï¼š1ï¼‰åŸºäºå°æ³¢çš„ç‰¹å¾æå–ï¼Œæ•æ‰iEEGä¿¡å·çš„ç»†ç²’åº¦æ—¶é—´ã€é¢‘è°±å’Œç¥ç»ç”Ÿç†å­¦è¡¨ç¤ºï¼›2ï¼‰åŸºäºTransformerçš„è§£ç å™¨ï¼Œç”¨äºè¯­è°ƒæ„ŸçŸ¥é¢‘è°±é¢„æµ‹ï¼›3ï¼‰ç¥ç»ç›¸ä½ç¼–ç å™¨ï¼Œé€šè¿‡è‡ªé€‚åº”è°±æ ¡æ­£å®ç°è°æ³¢ä¸€è‡´æ€§ã€‚åœ¨å…¬å…±iEEGæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMiSTRåœ¨è¯­éŸ³æ¸…æ™°åº¦æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œé‡å»ºçš„Melé¢‘è°±å›¾ä¸åŸå§‹Melé¢‘è°±å›¾ä¹‹é—´çš„å¹³å‡Pearsonç›¸å…³ç³»æ•°ä¸º0.91ï¼Œè¶…è¿‡äº†ç°æœ‰çš„ç¥ç»ç½‘ç»œè¯­éŸ³åˆæˆåŸºçº¿ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>é¢…å†…è„‘ç”µå›¾ï¼ˆiEEGï¼‰ä¿¡å·è¯­éŸ³åˆæˆå…·æœ‰æ¢å¤ä¸¥é‡è¨€è¯­éšœç¢è€…äº¤æµèƒ½åŠ›çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ç‰¹å¾è¡¨ç¤ºã€è¯­è°ƒå»ºæ¨¡å’Œç›¸ä½é‡å»ºã€‚</li>
<li>MiSTRæ¡†æ¶é›†æˆäº†åŸºäºå°æ³¢çš„ç‰¹å¾æå–ï¼Œæ•æ‰iEEGä¿¡å·çš„ç»†ç²’åº¦è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨åŸºäºTransformerçš„è§£ç å™¨è¿›è¡Œè¯­è°ƒæ„ŸçŸ¥é¢‘è°±é¢„æµ‹ã€‚</li>
<li>ç¥ç»ç›¸ä½ç¼–ç å™¨é€šè¿‡è‡ªé€‚åº”è°±æ ¡æ­£å®ç°è°æ³¢ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å…¬å…±iEEGæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºMiSTRè¾¾åˆ°æœ€æ–°è¯­éŸ³æ¸…æ™°åº¦æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62d4e1908a69f2bebc92f0de678f4fc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e5f5c565517e7ca8a4d99875304da03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50a49543a1266603068105eea40c6945.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-Text-to-Speech-Diffusion-Models-Using-Reinforcement-Learning-with-Human-Feedback"><a href="#Fine-Tuning-Text-to-Speech-Diffusion-Models-Using-Reinforcement-Learning-with-Human-Feedback" class="headerlink" title="Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning   with Human Feedback"></a>Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning   with Human Feedback</h2><p><strong>Authors:Jingyi Chen, Ju Seung Byun, Micha Elsner, Pichao Wang, Andrew Perrault</strong></p>
<p>Diffusion models produce high-fidelity speech but are inefficient for real-time use due to long denoising steps and challenges in modeling intonation and rhythm. To improve this, we propose Diffusion Loss-Guided Policy Optimization (DLPO), an RLHF framework for TTS diffusion models. DLPO integrates the original training loss into the reward function, preserving generative capabilities while reducing inefficiencies. Using naturalness scores as feedback, DLPO aligns reward optimization with the diffusion modelâ€™s structure, improving speech quality. We evaluate DLPO on WaveGrad 2, a non-autoregressive diffusion-based TTS model. Results show significant improvements in objective metrics (UTMOS 3.65, NISQA 4.02) and subjective evaluations, with DLPO audio preferred 67% of the time. These findings demonstrate DLPOâ€™s potential for efficient, high-quality diffusion TTS in real-time, resource-limited settings. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„è¯­éŸ³ï¼Œä½†ç”±äºå»å™ªæ­¥éª¤é•¿ä¸”å»ºæ¨¡éŸ³è°ƒå’ŒèŠ‚å¥å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› æ­¤åœ¨å®é™…ä½¿ç”¨åœºæ™¯ä¸­æ•ˆç‡è¾ƒä½ã€‚ä¸ºäº†æ”¹å–„è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æŸå¤±å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆDLPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºTTSæ‰©æ•£æ¨¡å‹çš„RLHFæ¡†æ¶ã€‚DLPOå°†åŸå§‹è®­ç»ƒæŸå¤±é›†æˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ï¼Œæ—¢ä¿ç•™äº†ç”Ÿæˆèƒ½åŠ›åˆæé«˜äº†æ•ˆç‡ã€‚é€šè¿‡ä½¿ç”¨è‡ªç„¶åº¦åˆ†æ•°ä½œä¸ºåé¦ˆï¼ŒDLPOä½¿å¥–åŠ±ä¼˜åŒ–ä¸æ‰©æ•£æ¨¡å‹çš„ç»“æ„ç›¸åŒ¹é…ï¼Œæé«˜äº†è¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬åœ¨WaveGrad 2è¿™ä¸ªéè‡ªå›å½’çš„åŸºäºæ‰©æ•£çš„TTSæ¨¡å‹ä¸Šè¯„ä¼°äº†DLPOã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨å®¢è§‚æŒ‡æ ‡ï¼ˆUTMOS 3.65ï¼ŒNISQA 4.02ï¼‰å’Œä¸»è§‚è¯„ä¼°ä¸Šï¼ŒDLPOéŸ³é¢‘çš„åå¥½ç‡é«˜è¾¾67%ã€‚è¿™äº›å‘ç°è¯æ˜äº†DLPOåœ¨å®æ—¶ã€èµ„æºæœ‰é™çš„åœºæ™¯ä¸­å®ç°é«˜æ•ˆã€é«˜è´¨é‡çš„æ‰©æ•£TTSçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03123v1">PDF</a> 4 pages, 1 figure, INTERSPEECH 2025. arXiv admin note: text overlap   with arXiv:2405.14632</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„è¯­éŸ³ï¼Œä½†ç”±äºå»å™ªæ­¥éª¤é•¿ä¸”éš¾ä»¥å¯¹è¯­è°ƒã€èŠ‚å¥è¿›è¡Œå»ºæ¨¡ï¼Œå› æ­¤ä¸é€‚ç”¨äºå®æ—¶ä½¿ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„äººç±»åé¦ˆæ¡†æ¶Diffusion Loss-Guided Policy Optimization (DLPO)ï¼Œç”¨äºæ”¹å–„TTSæ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚DLPOå°†åŸå§‹è®­ç»ƒæŸå¤±é›†æˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ï¼Œæ—¢ä¿ç•™äº†ç”Ÿæˆèƒ½åŠ›åˆæé«˜äº†æ•ˆç‡ã€‚åˆ©ç”¨è‡ªç„¶åº¦è¯„åˆ†ä½œä¸ºåé¦ˆï¼ŒDLPOä½¿å¥–åŠ±ä¼˜åŒ–ä¸æ‰©æ•£æ¨¡å‹ç»“æ„å¯¹é½ï¼Œæé«˜äº†è¯­éŸ³è´¨é‡ã€‚æˆ‘ä»¬åœ¨éè‡ªå›å½’æ‰©æ•£å¼TTSæ¨¡å‹WaveGrad 2ä¸Šè¯„ä¼°äº†DLPOçš„æ•ˆæœï¼Œç»“æœæ˜¾ç¤ºå®¢è§‚æŒ‡æ ‡ï¼ˆUTMOS 3.65ï¼ŒNISQA 4.02ï¼‰å’Œä¸»è§‚è¯„ä»·å‡æœ‰æ˜¾è‘—æé«˜ï¼ŒDLPOéŸ³é¢‘çš„ä¼˜é€‰ç‡ä¸º67%ã€‚è¿™è¡¨æ˜DLPOåœ¨å®æ—¶ã€èµ„æºæœ‰é™çš„è®¾ç½®ä¸­ï¼Œå…·æœ‰å®ç°é«˜æ•ˆã€é«˜è´¨é‡æ‰©æ•£TTSçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è™½èƒ½ç”Ÿæˆé«˜ä¿çœŸè¯­éŸ³ï¼Œä½†å­˜åœ¨å®æ—¶ä½¿ç”¨æ•ˆç‡ä½çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºå»å™ªæ­¥éª¤é•¿åŠå»ºæ¨¡è¯­è°ƒã€èŠ‚å¥çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆçš„DLPOæ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„TTSæ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>DLPOå°†åŸå§‹è®­ç»ƒæŸå¤±é›†æˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ï¼Œåœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¿ç•™ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨è‡ªç„¶åº¦è¯„åˆ†ä½œä¸ºåé¦ˆï¼Œä½¿å¥–åŠ±ä¼˜åŒ–ä¸æ‰©æ•£æ¨¡å‹ç»“æ„å¯¹é½ï¼Œæå‡è¯­éŸ³è´¨é‡ã€‚</li>
<li>åœ¨WaveGrad 2æ¨¡å‹ä¸Šè¯„ä¼°DLPOï¼Œå®¢è§‚æŒ‡æ ‡å’Œä¸»è§‚è¯„ä»·å‡æ˜¾ç¤ºæ˜¾è‘—æ”¹å–„ã€‚</li>
<li>DLPOéŸ³é¢‘çš„ä¼˜é€‰ç‡é«˜è¾¾67%ï¼Œè¡¨æ˜å…¶åœ¨å®æ—¶ã€èµ„æºæœ‰é™ç¯å¢ƒä¸‹çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe9672217b39f67aa0a13ead5d74d719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7ae9810f026cb2b11297dff5f9efb0f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69cd8346520a6fadb3aef595ca53b803.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Marco-Voice-Technical-Report"><a href="#Marco-Voice-Technical-Report" class="headerlink" title="Marco-Voice Technical Report"></a>Marco-Voice Technical Report</h2><p><strong>Authors:Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang</strong></p>
<p>This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/Marco-Voice">https://github.com/AIDC-AI/Marco-Voice</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS">https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS</a> respectively. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šåŠŸèƒ½è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…é›†æˆäº†è¯­éŸ³å…‹éš†å’Œæƒ…æ„Ÿæ§åˆ¶è¯­éŸ³åˆæˆã€‚æœ¬å·¥ä½œçš„ç›®æ ‡æ˜¯è§£å†³é•¿æœŸä»¥æ¥åœ¨å®ç°é«˜åº¦è¡¨è¾¾ã€å¯æ§å’Œè‡ªç„¶è¯­éŸ³ç”Ÿæˆæ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¿ å®åœ°åœ¨å„ç§è¯­è¨€å’Œæƒ…æ„ŸèƒŒæ™¯ä¸‹ä¿ç•™è¯´è¯è€…èº«ä»½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„è¯´è¯äººæƒ…æ„Ÿåˆ†ç¦»æœºåˆ¶ï¼Œé‡‡ç”¨æ‰¹é‡å¯¹æ¯”å­¦ä¹ ï¼Œå®ç°å¯¹è¯´è¯äººèº«ä»½å’Œæƒ…æ„Ÿé£æ ¼çš„ç‹¬ç«‹æ“ä½œï¼Œä»¥åŠç”¨äºå¹³æ»‘æƒ…æ„Ÿæ§åˆ¶çš„æ—‹è½¬æƒ…æ„ŸåµŒå…¥é›†æˆæ–¹æ³•ã€‚ä¸ºäº†æ”¯æŒå…¨é¢çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†CSEMOTIONSæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ï¼ŒåŒ…å«å…­åä¸“ä¸šè¯´è¯äºº10å°æ—¶çš„æ™®é€šè¯è¯­éŸ³ï¼Œè·¨è¶Šä¸ƒä¸ªæƒ…æ„Ÿç±»åˆ«ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Marco-Voiceç³»ç»Ÿåœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°å’Œåˆ†æï¼Œç»“æœè¡¨æ˜MarcoVoiceåœ¨è¯­éŸ³æ¸…æ™°åº¦å’Œæƒ…æ„Ÿä¸°å¯Œåº¦æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä»£è¡¨äº†ç¥ç»è¯­éŸ³åˆæˆé¢†åŸŸçš„é‡å¤§è¿›å±•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/Marco-Voice%E5%92%8Chttps://huggingface.co/datasets/AIDC-AI/CSEMOTIONS%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AIDC-AI/Marco-Voiceå’Œhttps://huggingface.co/datasets/AIDC-AI/CSEMOTIONSä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02038v2">PDF</a> Technical Report. Our code and dataset are publicly available at   <a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/Marco-Voice">https://github.com/AIDC-AI/Marco-Voice</a> and   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS">https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS</a> respectively</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šåŠŸèƒ½è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæ•´åˆäº†è¯­éŸ³å…‹éš†å’Œæƒ…æ„Ÿæ§åˆ¶è¯­éŸ³åˆæˆåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…ã€‚æ—¨åœ¨è§£å†³é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå®ç°é«˜åº¦è¡¨è¾¾ã€å¯æ§ã€è‡ªç„¶çš„è¯­éŸ³ç”Ÿæˆï¼Œå¿ å®ä¿ç•™è¯´è¯è€…èº«ä»½åœ¨ä¸åŒè¯­è¨€å’Œæƒ…æ„ŸèƒŒæ™¯ä¸­ã€‚é€šè¿‡å¼•å…¥æœ‰æ•ˆçš„è¯´è¯äººæƒ…æ„Ÿåˆ†ç¦»æœºåˆ¶å’Œæ—‹è½¬æƒ…æ„ŸåµŒå…¥é›†æˆæ–¹æ³•ï¼Œå®ç°å¯¹è¯´è¯äººèº«ä»½å’Œæƒ…æ„Ÿé£æ ¼çš„ç‹¬ç«‹æ“æ§ã€‚ä¸ºæ”¯æŒå…¨é¢çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œæ„å»ºäº†é«˜è´¨é‡çš„æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†CSEMOTIONSã€‚å®éªŒè¡¨æ˜ï¼ŒMarco-Voiceç³»ç»Ÿåœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å¤šåŠŸèƒ½è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œæ•´åˆè¯­éŸ³å…‹éš†å’Œæƒ…æ„Ÿæ§åˆ¶è¯­éŸ³åˆæˆã€‚</li>
<li>ç›®æ ‡å®ç°é«˜åº¦è¡¨è¾¾ã€å¯æ§ã€è‡ªç„¶çš„è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>å¼•å…¥æœ‰æ•ˆçš„è¯´è¯äººæƒ…æ„Ÿåˆ†ç¦»æœºåˆ¶ï¼Œå®ç°è¯´è¯äººèº«ä»½å’Œæƒ…æ„Ÿé£æ ¼çš„ç‹¬ç«‹æ“æ§ã€‚</li>
<li>é‡‡ç”¨æ—‹è½¬æƒ…æ„ŸåµŒå…¥é›†æˆæ–¹æ³•ï¼Œç”¨äºå¹³ç¨³æƒ…æ„Ÿæ§åˆ¶ã€‚</li>
<li>æ„å»ºäº†é«˜è´¨é‡çš„æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†CSEMOTIONSã€‚</li>
<li>Marco-Voiceç³»ç»Ÿåœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1c9a0bec9b782322b7b4fadfe01d9fe0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65fa35ed79a2045e61868e07682c1d5d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="UITron-Speech-Towards-Automated-GUI-Agents-Based-on-Speech-Instructions"><a href="#UITron-Speech-Towards-Automated-GUI-Agents-Based-on-Speech-Instructions" class="headerlink" title="UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions"></a>UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions</h2><p><strong>Authors:Wenkang Han, Zhixiong Zeng, Jing Huang, Shu Jiang, Liming Zheng, Haibo Qiu, Chang Yao, Jingyuan Chen, Lin Ma</strong></p>
<p>Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing human-computer interaction, yet their reliance on text-based instructions imposes limitations on accessibility and convenience, particularly in hands-free scenarios. To address this issue, we propose replacing text with speech as the instruction input modality for GUI agents, and introduce UITron-Speech, which is the first end-to-end GUI agent capable of directly processing speech instructions and on-device screenshots to predict user actions. To tackle the problem of data scarcity, we synthesize high-quality speech instruction datasets using a random-speaker text-to-speech model. Additionally, we design a mixed-modality training strategy to mitigate the inherent modality imbalance in pre-trained foundation models. Furthermore, we conduct a statistical analysis of the distribution of GUI grounding prediction errors and propose a training-free two-step grounding refinement method to alleviate minor localization deviations. Extensive experiments on multiple benchmarks demonstrate that UITron-Speech achieves robust performance and superior adaptability, underscoring the feasibility and potential of speech-driven GUI agents for more accessible and intelligent human-computer interaction. Our code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/UITron-hub/UITron-Speech">https://github.com/UITron-hub/UITron-Speech</a>. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„è‡ªä¸»ä»£ç†æ­£åœ¨å½»åº•æ”¹å˜äººæœºäº¤äº’çš„æ–¹å¼ï¼Œç„¶è€Œå®ƒä»¬å¯¹åŸºäºæ–‡æœ¬çš„æŒ‡ä»¤çš„ä¾èµ–ï¼Œå¯¹æ— éšœç¢å’Œä¾¿æ·æ€§é€ æˆäº†é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å…æåœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç”¨è¯­éŸ³æ›¿æ¢æ–‡æœ¬ä½œä¸ºGUIä»£ç†çš„æŒ‡ä»¤è¾“å…¥æ¨¡å¼ï¼Œå¹¶å¼•å…¥äº†UITron-Speechã€‚å®ƒæ˜¯é¦–æ¬¾èƒ½å¤Ÿç›´æ¥å¤„ç†è¯­éŸ³æŒ‡ä»¤å’Œè®¾å¤‡æˆªå›¾çš„ç«¯åˆ°ç«¯GUIä»£ç†ï¼Œä»¥é¢„æµ‹ç”¨æˆ·æ“ä½œã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨éšæœºè¯´è¯äººæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹åˆæˆé«˜è´¨é‡è¯­éŸ³æŒ‡ä»¤æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ··åˆæ¨¡å¼è®­ç»ƒç­–ç•¥ï¼Œä»¥ç¼“è§£é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä¸­çš„å›ºæœ‰æ¨¡å¼ä¸å¹³è¡¡é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¯¹GUIæ¥åœ°é¢„æµ‹è¯¯å·®çš„åˆ†å¸ƒè¿›è¡Œäº†ç»Ÿè®¡åˆ†æï¼Œå¹¶æå‡ºäº†æ— éœ€è®­ç»ƒçš„ä¸¤æ­¥æ¥åœ°ç»†åŒ–æ–¹æ³•ï¼Œä»¥å‡è½»è½»å¾®çš„å®šä½åå·®ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUITron-Speechå®ç°äº†ç¨³å¥çš„æ€§èƒ½å’Œä¼˜è¶Šçš„é€‚åº”æ€§ï¼Œå‡¸æ˜¾äº†è¯­éŸ³é©±åŠ¨GUIä»£ç†çš„å¯è¡Œæ€§å’Œæ½œåŠ›ï¼Œä¸ºæ›´æ— éšœç¢å’Œæ™ºèƒ½çš„äººæœºäº¤äº’æä¾›äº†å¯èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/UITron-hub/UITron-Speech%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/UITron-hub/UITron-Speechæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11127v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯­éŸ³é©±åŠ¨çš„GUIä»£ç†æŠ€æœ¯æ­£åœ¨æ”¹å˜äººæœºäº¤äº’æ–¹å¼ã€‚ä¸ºè§£å†³æ–‡æœ¬æŒ‡ä»¤å¸¦æ¥çš„è®¿é—®å’Œä¾¿åˆ©æ€§é™åˆ¶é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨è¯­éŸ³ä½œä¸ºGUIä»£ç†çš„æŒ‡ä»¤è¾“å…¥æ¨¡å¼ï¼Œå¹¶å¼•å…¥äº†UITron-Speechã€‚å®ƒæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿç›´æ¥å¤„ç†è¯­éŸ³æŒ‡ä»¤å’Œè®¾å¤‡æˆªå›¾çš„ç«¯åˆ°ç«¯GUIä»£ç†ï¼Œå¯é¢„æµ‹ç”¨æˆ·æ“ä½œã€‚é€šè¿‡åˆæˆé«˜è´¨é‡è¯­éŸ³æŒ‡ä»¤æ•°æ®é›†å’Œæ··åˆæ¨¡æ€è®­ç»ƒç­–ç•¥è§£å†³æ•°æ®ç¨€ç¼ºå’Œæ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚å¯¹GUIå®šä½é¢„æµ‹è¯¯å·®è¿›è¡Œç»Ÿè®¡åˆ†æï¼Œå¹¶æå‡ºä¸¤æ­¥å®šä½ç»†åŒ–æ–¹æ³•ä»¥å‡è½»è½»å¾®å®šä½åå·®ã€‚å®éªŒç»“æœè¯æ˜äº†UITron-Speechçš„ç¨³å¥æ€§èƒ½å’Œå“è¶Šé€‚åº”æ€§ï¼Œå±•ç°äº†è¯­éŸ³é©±åŠ¨GUIä»£ç†åœ¨å®ç°æ›´æ™ºèƒ½å’Œå¯è®¿é—®çš„äººæœºäº¤äº’ä¸­çš„æ½œåŠ›å’Œå¯è¡Œæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨â€‹â€‹ <a target="_blank" rel="noopener" href="https://github.com/UITron-hub/UITron-Speech">https://github.com/UITron-hub/UITron-Speech</a> è·å¾—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³é©±åŠ¨çš„GUIä»£ç†æŠ€æœ¯æ­£åœ¨æ”¹å˜äººæœºäº¤äº’æ–¹å¼ï¼Œæé«˜äº†è®¿é—®æ€§å’Œä¾¿åˆ©æ€§ã€‚</li>
<li>UITron-Speechæ˜¯é¦–ä¸ªèƒ½å¤Ÿç›´æ¥å¤„ç†è¯­éŸ³æŒ‡ä»¤å’Œè®¾å¤‡æˆªå›¾çš„ç«¯åˆ°ç«¯GUIä»£ç†ã€‚</li>
<li>é€šè¿‡åˆæˆé«˜è´¨é‡è¯­éŸ³æŒ‡ä»¤æ•°æ®é›†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>æå‡ºäº†æ··åˆæ¨¡æ€è®­ç»ƒç­–ç•¥ä»¥è§£å†³é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä¸­çš„æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>å¯¹GUIå®šä½é¢„æµ‹è¯¯å·®è¿›è¡Œç»Ÿè®¡åˆ†æï¼Œå¹¶æå‡ºä¸¤æ­¥å®šä½ç»†åŒ–æ–¹æ³•ä»¥å‡è½»å®šä½åå·®ã€‚</li>
<li>UITron-Speechå…·æœ‰ç¨³å¥æ€§èƒ½å’Œå“è¶Šé€‚åº”æ€§ï¼ŒéªŒè¯äº†è¯­éŸ³é©±åŠ¨GUIä»£ç†çš„æ½œåŠ›å’Œå¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d925f7e627b1850827fb794a2e767bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7ae03e93249d62cfd6e46704e8c5fb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-429bae2058c3eecbf568076778d782f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc2b64e1065e18c85952e6246c912b08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e80976aff71e49a4f1f3191236e77c6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d99ea0b0e91ec0093aa13abe7e7f6791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb9188fbea236ebf9918eb8aabff5c62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a80e0d36462396c20cbb75d4fc5c25d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation"><a href="#UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation" class="headerlink" title="UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation"></a>UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation</h2><p><strong>Authors:Jinting Wang, Shan Yang, Chenxing Li, Dong Yu, Li Liu</strong></p>
<p>Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics. </p>
<blockquote>
<p>å”‡éŸ³æç¤ºï¼ˆCued Speechï¼Œç®€ç§°CSï¼‰é€šè¿‡æ‰‹åŠ¨ç¼–ç æé«˜å”‡è¯­é˜…è¯»æ•ˆç‡ï¼Œæä¾›è§†è§‰è¯­éŸ³æç¤ºï¼Œæ”¯æŒå¬åŠ›éšœç¢è€…ç²¾ç¡®æ„ŸçŸ¥è¯­éŸ³ã€‚CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰çš„ä»»åŠ¡æ—¨åœ¨å°†CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚ç›®å‰å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨CSè¯†åˆ«ï¼ˆCSRï¼‰ä¸Šï¼Œå°†è§†é¢‘å†…å®¹è½¬å½•ä¸ºæ–‡æœ¬ã€‚å› æ­¤ï¼ŒCSV2Sçš„ä¸€ç§å¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯å°†CSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç»“åˆèµ·æ¥ã€‚ç„¶è€Œï¼Œæ­¤æµç¨‹ä¾èµ–äºæ–‡æœ¬ä½œä¸ºä¸­é—´åª’ä»‹ï¼Œå¯èƒ½å¯¼è‡´è¯¯å·®ä¼ æ’­ä»¥åŠè¯­éŸ³å’ŒCSè§†é¢‘åŠ¨æ€ä¹‹é—´çš„æ—¶é—´é”™ä½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›´æ¥ä»CSè§†é¢‘ç”ŸæˆéŸ³é¢‘è¯­éŸ³ï¼ˆç›´æ¥CSV2Sï¼‰å¾€å¾€å—åˆ°å›ºæœ‰çš„å¤šæ¨¡å¼å¤æ‚æ€§å’Œæœ‰é™çš„CSæ•°æ®å¯ç”¨æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniCUEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºCSV2Sçš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚UniCUEçš„æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºæ•´åˆäº†ç†è§£ä»»åŠ¡ï¼ˆCSRï¼‰ï¼Œæä¾›ç²¾ç»†çš„CSè§†è§‰è¯­ä¹‰æç¤ºæ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒUniCUEç»“åˆäº†å§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€è¯­ä¹‰å¯¹é½æ± ï¼ˆä½¿ç²¾ç¡®è§†è§‰è¯­ä¹‰æ˜ å°„æˆä¸ºå¯èƒ½ï¼‰å’ŒVisioPhoneticé€‚é…å™¨ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¶æ„å†…æ¡¥æ¥ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚ä¸ºäº†æ”¯æŒæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§å‹æ™®é€šè¯CSæ•°æ®é›†UniCUE-HIï¼ŒåŒ…å«æ¥è‡ª14åæ‰“æ‰‹åŠ¿è€…çš„11282ä¸ªè§†é¢‘ï¼Œå…¶ä¸­åŒ…æ‹¬å¬éšœäººå£«å’Œæ­£å¸¸å¬åŠ›äººå£«ã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniCUEåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†ä¸€æµçš„æ€§èƒ½è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04134v3">PDF</a> 8 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Cued Speechï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œä¸ºå¬åŠ›å—æŸè€…æä¾›è§†è§‰è¯­éŸ³çº¿ç´¢ä»¥æ”¯æŒç²¾ç¡®çš„è¯­è¨€æ„ŸçŸ¥ã€‚CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰çš„ä»»åŠ¡æ—¨åœ¨å°†CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶é›†ä¸­åœ¨CSè¯†åˆ«ï¼ˆCSRï¼‰ä¸Šï¼Œå°†è§†é¢‘å†…å®¹è½¬å½•ä¸ºæ–‡æœ¬ã€‚å› æ­¤ï¼ŒCSV2Sçš„å¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯å°†CSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç»“åˆã€‚ç„¶è€Œï¼Œè¿™ç§ç®¡é“ä¾èµ–äºä½œä¸ºä¸­é—´åª’ä»‹çš„æ–‡æœ¬ï¼Œå¯èƒ½å¯¼è‡´è¯¯å·®ä¼ æ’­å’Œè¯­éŸ³ä¸CSè§†é¢‘åŠ¨æ€ä¹‹é—´çš„æ—¶é—´ä¸å¯¹é½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›´æ¥ä»CSè§†é¢‘ç”ŸæˆéŸ³é¢‘è¯­éŸ³ï¼ˆç›´æ¥CSV2Sï¼‰å¸¸å¸¸é¢ä¸´å›ºæœ‰çš„å¤šæ¨¡å¼å¤æ‚æ€§å’ŒCSæ•°æ®æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniCUEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€çš„CSV2Sæ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼Œè€Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚UniCUEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ•´åˆäº†ç†è§£ä»»åŠ¡ï¼ˆCSRï¼‰ï¼Œæä¾›ç²¾ç»†çš„CSè§†è§‰è¯­ä¹‰çº¿ç´¢æ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒUniCUEç»“åˆäº†å§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€è¯­ä¹‰å¯¹é½æ± ï¼ˆä½¿ç²¾ç¡®è§†è§‰è¯­ä¹‰æ˜ å°„æˆä¸ºå¯èƒ½ï¼‰å’ŒVisioPhoneticé€‚é…å™¨ï¼ˆåœ¨ç†è§£ä»»åŠ¡å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´æ¶èµ·æ¡¥æ¢ï¼‰åœ¨ä¸€ä¸ªç»Ÿä¸€æ¶æ„ä¸­ã€‚ä¸ºäº†æ”¯æŒæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§å‹æ™®é€šè¯CSæ•°æ®é›†UniCUE-HIï¼ŒåŒ…å«æ¥è‡ª14åæ‰“æ‰‹åŠ¿è€…çš„11282ä¸ªè§†é¢‘ï¼ŒåŒ…æ‹¬å¬éšœäººå£«å’Œæ­£å¸¸å¬åŠ›äººå£«ã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUniCUEåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå–å¾—äº†åˆ›çºªå½•çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Cued Speechï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œä¸ºå¬åŠ›å—æŸè€…æä¾›è§†è§‰è¯­éŸ³çº¿ç´¢ã€‚</li>
<li>CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰çš„ä»»åŠ¡æ˜¯è½¬æ¢CSè§†é¢‘ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ï¼Œå­˜åœ¨å¤šç§æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ç ”ç©¶å¤šé›†ä¸­åœ¨CSè¯†åˆ«ï¼ˆCSRï¼‰ï¼Œå³å°†è§†é¢‘å†…å®¹è½¬å½•ä¸ºæ–‡æœ¬ï¼Œè€ŒCSV2Sçš„å¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯ç»“åˆCSRä¸TTSç³»ç»Ÿï¼Œä½†å­˜åœ¨è¯¯å·®ä¼ æ’­å’Œæ—¶é—´ä¸å¯¹é½é—®é¢˜ã€‚</li>
<li>ç›´æ¥ä»CSè§†é¢‘ç”ŸæˆéŸ³é¢‘è¯­éŸ³é¢ä¸´å¤šæ¨¡å¼å¤æ‚æ€§å’ŒCSæ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>UniCUEæ˜¯é¦–ä¸ªèƒ½ç›´æ¥ç”Ÿæˆè¯­éŸ³çš„CSV2Sç»Ÿä¸€æ¡†æ¶ï¼Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚</li>
<li>UniCUEç»“åˆå§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€è¯­ä¹‰å¯¹é½æ± å’ŒVisioPhoneticé€‚é…å™¨ï¼Œå®ç°ç²¾ç»†çš„CSè§†è§‰è¯­ä¹‰çº¿ç´¢æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-07845cec9a726ba2f15b028773000f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b9c1341630a0b49e215451149672980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb4e16a4a612ba253281afbde879cd2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbe9a9aeae12c0d60360803d6e2bad96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1357283a1bc69b5e7e50281d19c85a3b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Pseudo-Autoregressive-Neural-Codec-Language-Models-for-Efficient-Zero-Shot-Text-to-Speech-Synthesis"><a href="#Pseudo-Autoregressive-Neural-Codec-Language-Models-for-Efficient-Zero-Shot-Text-to-Speech-Synthesis" class="headerlink" title="Pseudo-Autoregressive Neural Codec Language Models for Efficient   Zero-Shot Text-to-Speech Synthesis"></a>Pseudo-Autoregressive Neural Codec Language Models for Efficient   Zero-Shot Text-to-Speech Synthesis</h2><p><strong>Authors:Yifan Yang, Shujie Liu, Jinyu Li, Yuxuan Hu, Haibin Wu, Hui Wang, Jianwei Yu, Lingwei Meng, Haiyang Sun, Yanqing Liu, Yan Lu, Kai Yu, Xie Chen</strong></p>
<p>Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at <a target="_blank" rel="noopener" href="https://microsoft.com/research/project/vall-e-x/palle">https://microsoft.com/research/project/vall-e-x/palle</a>. </p>
<blockquote>
<p>è¿‘æœŸé›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿé¢ä¸´ä¸€ä¸ªå…±åŒå›°å¢ƒï¼šè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹å­˜åœ¨ç”Ÿæˆé€Ÿåº¦æ…¢å’Œç¼ºä¹æŒç»­æ—¶é—´æ§åˆ¶çš„é—®é¢˜ï¼Œè€Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å‹åˆ™ç¼ºä¹æ—¶é—´å»ºæ¨¡å¹¶ä¸”é€šå¸¸éœ€è¦å¤æ‚çš„è®¾è®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹ä¼ªè‡ªå›å½’ï¼ˆPARï¼‰ç¼–ç è§£ç è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œå®ƒå°†ARå’ŒNARå»ºæ¨¡ç»Ÿä¸€èµ·æ¥ã€‚é€šè¿‡å°†ARçš„æ˜¾å¼æ—¶é—´å»ºæ¨¡ä¸NARçš„å¹¶è¡Œç”Ÿæˆç›¸ç»“åˆï¼ŒPARåœ¨å›ºå®šæ—¶é—´æ­¥é•¿ä¸Šç”ŸæˆåŠ¨æ€é•¿åº¦çš„è·¨åº¦ã€‚åŸºäºPARï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„TTSç³»ç»ŸPALLEï¼Œå®ƒé¦–å…ˆåˆ©ç”¨PARè¿›è¡Œåˆæ­¥ç”Ÿæˆï¼Œç„¶ååˆ©ç”¨NARè¿›è¡Œç»†åŒ–ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒPARæ²¿æ—¶é—´ç»´åº¦é€æ­¥ç”Ÿæˆè¯­éŸ³æ ‡è®°ï¼Œæ¯ä¸€æ­¥éƒ½å¹¶è¡Œé¢„æµ‹æ‰€æœ‰ä½ç½®ï¼Œä½†åªä¿ç•™æœ€å·¦è¾¹çš„è·¨åº¦ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶è¡Œè¿­ä»£ä¼˜åŒ–ä½ç½®ä¿¡åº¦çš„æ ‡è®°ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨LibriSpeechæµ‹è¯•æ¸…æ´é›†ä¸Šï¼ŒPALLEåœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæ¸…æ™°åº¦æ–¹é¢ä¼˜äºåœ¨å¤§å‹æ•°æ®ä¸Šè®­ç»ƒçš„æœ€æ–°ç³»ç»Ÿï¼ŒåŒ…æ‹¬F5-TTSã€E2-TTSå’ŒMaskGCTï¼ŒåŒæ—¶å®ç°äº†é«˜è¾¾åå€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://microsoft.com/research/project/vall-e-%20x/palle%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://microsoft.com/research/project/vall-e-x/palleä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10352v3">PDF</a> Accepted in ACMMM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¼ªè‡ªå›å½’ï¼ˆPARï¼‰ç¼–ç è§£ç è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰å»ºæ¨¡çš„ä¼˜ç‚¹ã€‚åŸºäºæ­¤æ–¹æ³•ï¼Œç ”å‘äº†åä¸ºPALLEçš„ä¸¤é˜¶æ®µæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚PALLEåœ¨åˆå§‹é˜¶æ®µä½¿ç”¨PARç”Ÿæˆè¯­éŸ³æ ‡è®°ï¼Œç„¶ååœ¨ç¬¬äºŒé˜¶æ®µä½¿ç”¨NARè¿›è¡Œç²¾ç»†ä¿®æ­£ã€‚å®éªŒè¡¨æ˜ï¼ŒPALLEåœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œæ¸…æ™°åº¦æ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›ç³»ç»Ÿï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æ›´å¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ªè‡ªå›å½’ï¼ˆPARï¼‰ç¼–ç è§£ç è¯­è¨€å»ºæ¨¡ç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚</li>
<li>PALLEç³»ç»Ÿé‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œåˆå§‹é˜¶æ®µä½¿ç”¨PARç”Ÿæˆè¯­éŸ³æ ‡è®°ï¼Œç„¶åè¿›è¡ŒNARç²¾ç»†ä¿®æ­£ã€‚</li>
<li>PARèƒ½å¤Ÿåœ¨å›ºå®šæ—¶é—´æ­¥é•¿å†…ç”ŸæˆåŠ¨æ€é•¿åº¦è·¨åº¦ã€‚</li>
<li>PALLEåœ¨LibriSpeechæµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„è¯­éŸ³è´¨é‡ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œæ¸…æ™°åº¦ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›ç³»ç»Ÿç›¸æ¯”ï¼ŒPALLEå®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://microsoft.com/research/project/vall-e-%x%e7%9c%8b%e3%80%82">https://microsoft.com/research/project/vall-e-x/palleæŸ¥çœ‹ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-30212e08992e3a7f1bab4b888590837e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efcdd90ef1e90e5ebc2e5574a8c03f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de7a7c6f41261fd3afcd24817e4b38c9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-08/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-91cda87ef49dd0b65e1dc134b7e96331.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  SID Benchmarking Guided Instruction Capabilities in STEM Education with   a Socratic Interdisciplinary Dialogues Dataset
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6db1c5897da7f283087bd2e51ff304c7.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-08  X-SAM From Segment Anything to Any Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
