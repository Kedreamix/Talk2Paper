<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-26  Zero-resource Speech Translation and Recognition with LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-43ae7c12668f3d8fae80249694d410de.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    57 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-26-æ›´æ–°"><a href="#2024-12-26-æ›´æ–°" class="headerlink" title="2024-12-26 æ›´æ–°"></a>2024-12-26 æ›´æ–°</h1><h2 id="Zero-resource-Speech-Translation-and-Recognition-with-LLMs"><a href="#Zero-resource-Speech-Translation-and-Recognition-with-LLMs" class="headerlink" title="Zero-resource Speech Translation and Recognition with LLMs"></a>Zero-resource Speech Translation and Recognition with LLMs</h2><p><strong>Authors:Karel Mundnich, Xing Niu, Prashant Mathur, Srikanth Ronanki, Brady Houston, Veera Raghavendra Elluru, Nilaksh Das, Zejiang Hou, Goeric Huybrechts, Anshu Bhatia, Daniel Garcia-Romero, Kyu J. Han, Katrin Kirchhoff</strong></p>
<p>Despite recent advancements in speech processing, zero-resource speech translation (ST) and automatic speech recognition (ASR) remain challenging problems. In this work, we propose to leverage a multilingual Large Language Model (LLM) to perform ST and ASR in languages for which the model has never seen paired audio-text data. We achieve this by using a pre-trained multilingual speech encoder, a multilingual LLM, and a lightweight adaptation module that maps the audio representations to the token embedding space of the LLM. We perform several experiments both in ST and ASR to understand how to best train the model and what data has the most impact on performance in previously unseen languages. In ST, our best model is capable to achieve BLEU scores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we achieve WERs of up to 28.2%. We finally show that the performance of our system is bounded by the ability of the LLM to output text in the desired language. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘è¯­éŸ³å¤„ç†é¢†åŸŸæœ‰æ‰€è¿›å±•ï¼Œä½†é›¶èµ„æºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ‰§è¡ŒSTå’ŒASRï¼Œé’ˆå¯¹æ¨¡å‹ä»æœªæ¥è§¦è¿‡çš„é…å¯¹è¯­éŸ³æ–‡æœ¬æ•°æ®çš„è¯­è¨€ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ã€å¤šè¯­è¨€LLMå’Œä¸€ä¸ªè½»é‡çº§é€‚é…æ¨¡å—ï¼ˆè¯¥æ¨¡å—å°†éŸ³é¢‘è¡¨ç¤ºæ˜ å°„åˆ°LLMçš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´ï¼‰æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬åœ¨STå’ŒASRä¸­è¿›è¡Œäº†å¤šæ¬¡å®éªŒï¼Œä»¥äº†è§£å¦‚ä½•æœ€å¥½åœ°è®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠå“ªäº›æ•°æ®åœ¨å¯¹æœªè§è¿‡çš„è¯­è¨€æ€§èƒ½å½±å“æ–¹é¢æœ€ä¸ºé‡è¦ã€‚åœ¨STä¸­ï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹èƒ½å¤Ÿåœ¨CoVoST2ä¸Šå®ç°è¶…è¿‡23çš„BLEUåˆ†æ•°ï¼Œç”¨äºä¸¤ç§å…ˆå‰æœªè§è¿‡çš„è¯­è¨€ï¼›è€Œåœ¨ASRä¸­ï¼Œæˆ‘ä»¬å®ç°é«˜è¾¾28.2%çš„WERã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬ç³»ç»Ÿçš„æ€§èƒ½å—åˆ°LLMè¾“å‡ºæ‰€éœ€è¯­è¨€æ–‡æœ¬çš„èƒ½åŠ›çš„é™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18566v1">PDF</a> ICASSP 2025, 5 pages, 2 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†é›¶èµ„æºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„éš¾é¢˜ï¼Œå¹¶æå‡ºåˆ©ç”¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»æœªæ¥è§¦è¿‡é…å¯¹éŸ³é¢‘æ–‡æœ¬æ•°æ®çš„è¯­è¨€ä¸­è¿›è¡ŒSTå’ŒASRã€‚é€šè¿‡é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ã€å¤šè¯­è¨€LLMå’Œè½»é‡çº§é€‚åº”æ¨¡å—ï¼Œå°†è¯­éŸ³è¡¨ç¤ºæ˜ å°„åˆ°LLMçš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´ã€‚åœ¨STå’ŒASRæ–¹é¢è¿›è¡Œäº†å¤šæ¬¡å®éªŒï¼Œä»¥äº†è§£å¦‚ä½•æœ€ä½³åœ°è®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠå“ªäº›æ•°æ®å¯¹åœ¨æœªè§è¿‡çš„è¯­è¨€ä¸­çš„æ€§èƒ½å½±å“æœ€å¤§ã€‚åœ¨STä¸­ï¼Œæœ€ä½³æ¨¡å‹çš„BLEUå¾—åˆ†åœ¨CoVoST2ä¸Šè¶…è¿‡23ï¼Œè€Œåœ¨ASRä¸­ï¼Œæˆ‘ä»¬å®ç°äº†æœ€é«˜28.2%çš„WERã€‚æœ€åï¼Œè¡¨æ˜ç³»ç»Ÿæ€§èƒ½å—é™äºLLMè¾“å‡ºæ‰€éœ€è¯­è¨€æ–‡æœ¬çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶èµ„æºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºåˆ©ç”¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœªè§è¿‡çš„è¯­è¨€ä¸­è¿›è¡ŒSTå’ŒASRã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ã€å¤šè¯­è¨€LLMå’Œè½»é‡çº§é€‚åº”æ¨¡å—å®ç°è¯­éŸ³åˆ°æ–‡æœ¬çš„è½¬æ¢ã€‚</li>
<li>åœ¨STå’ŒASRæ–¹é¢è¿›è¡Œäº†å¤šæ¬¡å®éªŒï¼Œä»¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒå¹¶äº†è§£æ•°æ®å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>åœ¨STæ–¹é¢ï¼Œæœ€ä½³æ¨¡å‹çš„BLEUå¾—åˆ†åœ¨CoVoST2æ•°æ®é›†ä¸Šè¶…è¿‡23ã€‚</li>
<li>åœ¨ASRæ–¹é¢ï¼Œå®ç°äº†æœ€é«˜28.2%çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5b357107fcd6a2cde520d689ab7b9bd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43ae7c12668f3d8fae80249694d410de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad0e1040b8bc8cb5a6123cd4919ddfcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2381c4b0d90bf7f46d411b0fe79095ff.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Text-Aware-Adapter-for-Few-Shot-Keyword-Spotting"><a href="#Text-Aware-Adapter-for-Few-Shot-Keyword-Spotting" class="headerlink" title="Text-Aware Adapter for Few-Shot Keyword Spotting"></a>Text-Aware Adapter for Few-Shot Keyword Spotting</h2><p><strong>Authors:Youngmoon Jung, Jinyoung Lee, Seungjin Lee, Myunghun Jung, Yong-Hyeok Lee, Hoon-Young Cho</strong></p>
<p>Recent advances in flexible keyword spotting (KWS) with text enrollment allow users to personalize keywords without uttering them during enrollment. However, there is still room for improvement in target keyword performance. In this work, we propose a novel few-shot transfer learning method, called text-aware adapter (TA-adapter), designed to enhance a pre-trained flexible KWS model for specific keywords with limited speech samples. To adapt the acoustic encoder, we leverage a jointly pre-trained text encoder to generate a text embedding that acts as a representative vector for the keyword. By fine-tuning only a small portion of the network while keeping the core componentsâ€™ weights intact, the TA-adapter proves highly efficient for few-shot KWS, enabling a seamless return to the original pre-trained model. In our experiments, the TA-adapter demonstrated significant performance improvements across 35 distinct keywords from the Google Speech Commands V2 dataset, with only a 0.14% increase in the total number of parameters. </p>
<blockquote>
<p>è¿‘æœŸï¼Œæ–‡æœ¬æ³¨å†Œåˆ¶çš„çµæ´»å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰æŠ€æœ¯å–å¾—äº†è¿›å±•ï¼Œå…è®¸ç”¨æˆ·ä¸ªæ€§åŒ–å…³é”®è¯ï¼Œè€Œæ— éœ€åœ¨æ³¨å†Œæ—¶å‘å‡ºå£°éŸ³ã€‚ç„¶è€Œï¼Œç›®æ ‡å…³é”®è¯çš„æ€§èƒ½ä»æœ‰æå‡ç©ºé—´ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºæ–‡æœ¬æ„ŸçŸ¥é€‚é…å™¨ï¼ˆTA-adapterï¼‰ï¼Œæ—¨åœ¨åˆ©ç”¨æœ‰é™çš„è¯­éŸ³æ ·æœ¬ï¼Œå¢å¼ºé¢„è®­ç»ƒçš„çµæ´»KWSæ¨¡å‹å¯¹ç‰¹å®šå…³é”®è¯çš„è¯†åˆ«ã€‚ä¸ºäº†è°ƒæ•´å£°å­¦ç¼–ç å™¨ï¼Œæˆ‘ä»¬åˆ©ç”¨è”åˆé¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼Œä½œä¸ºå…³é”®è¯çš„ä»£è¡¨å‘é‡ã€‚é€šè¿‡ä»…å¾®è°ƒç½‘ç»œçš„ä¸€éƒ¨åˆ†ï¼ŒåŒæ—¶ä¿æŒæ ¸å¿ƒç»„ä»¶çš„æƒé‡ä¸å˜ï¼ŒTA-adapteråœ¨å°‘æ ·æœ¬KWSä¸­è¡¨ç°å‡ºé«˜æ•ˆç‡ï¼Œå¯ä»¥æ— ç¼åœ°å›å½’åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒTA-adapteråœ¨Googleè¯­éŸ³å‘½ä»¤V2æ•°æ®é›†ä¸­çš„35ä¸ªä¸åŒå…³é”®è¯ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”æ€»å‚æ•°ä»…å¢åŠ äº†0.14%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18142v1">PDF</a> 5 pages, 3 figures, Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºæ–‡æœ¬æ„ŸçŸ¥é€‚é…å™¨ï¼ˆTA-adapterï¼‰çš„å°‘é‡æ ·æœ¬è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›é¢„è®­ç»ƒçš„çµæ´»å…³é”®è¯è¯†åˆ«æ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨è”åˆé¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡æ¥ä»£è¡¨å…³é”®è¯ï¼Œè¯¥æ–¹æ³•åœ¨å¾®è°ƒç½‘ç»œå°éƒ¨åˆ†çš„åŒæ—¶ä¿æŒæ ¸å¿ƒç»„ä»¶æƒé‡ä¸å˜ï¼Œå®ç°äº†é«˜æ•ˆçš„å°‘é‡æ ·æœ¬å…³é”®è¯è¯†åˆ«ã€‚åœ¨Google Speech Commands V2æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTA-adapteråœ¨35ä¸ªä¸åŒå…³é”®è¯ä¸Šçš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œæ€»å‚æ•°ä»…å¢åŠ 0.14%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ–‡æœ¬æ„ŸçŸ¥é€‚é…å™¨ï¼ˆTA-adapterï¼‰çš„è¿ç§»å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•è®¾è®¡ç”¨äºæ”¹è¿›é¢„è®­ç»ƒçš„çµæ´»å…³é”®è¯è¯†åˆ«æ¨¡å‹ä»¥è¯†åˆ«ç‰¹å®šå…³é”®è¯ã€‚</li>
<li>åˆ©ç”¨è”åˆé¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡æ¥ä»£è¡¨å…³é”®è¯ã€‚</li>
<li>é€šè¿‡å¾®è°ƒå°éƒ¨åˆ†ç½‘ç»œï¼ŒåŒæ—¶ä¿æŒæ ¸å¿ƒç»„ä»¶æƒé‡ä¸å˜ï¼Œå®ç°é«˜æ•ˆå°‘é‡çš„æ ·æœ¬å…³é”®è¯è¯†åˆ«ã€‚</li>
<li>å®éªŒåœ¨Google Speech Commands V2æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œæ¶‰åŠ35ä¸ªä¸åŒå…³é”®è¯ã€‚</li>
<li>TA-adapteræ˜¾è‘—æé«˜äº†å…³é”®è¯è¯†åˆ«æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-902c662f563eb67621aed26782be8251.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f086b14f5ef0d8ada080df1021e2db42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4372921412aecd7d9f3ab737a5739767.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d10111658b6d5b0fb82bea8fc21f6591.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Neural-Directed-Speech-Enhancement-with-Dual-Microphone-Array-in-High-Noise-Scenario"><a href="#Neural-Directed-Speech-Enhancement-with-Dual-Microphone-Array-in-High-Noise-Scenario" class="headerlink" title="Neural Directed Speech Enhancement with Dual Microphone Array in High   Noise Scenario"></a>Neural Directed Speech Enhancement with Dual Microphone Array in High   Noise Scenario</h2><p><strong>Authors:Wen Wen, Qiang Zhou, Yu Xi, Haoyu Li, Ziqi Gong, Kai Yu</strong></p>
<p>In multi-speaker scenarios, leveraging spatial features is essential for enhancing target speech. While with limited microphone arrays, developing a compact multi-channel speech enhancement system remains challenging, especially in extremely low signal-to-noise ratio (SNR) conditions. To tackle this issue, we propose a triple-steering spatial selection method, a flexible framework that uses three steering vectors to guide enhancement and determine the enhancement range. Specifically, we introduce a causal-directed U-Net (CDUNet) model, which takes raw multi-channel speech and the desired enhancement width as inputs. This enables dynamic adjustment of steering vectors based on the target direction and fine-tuning of the enhancement region according to the angular separation between the target and interference signals. Our model with only a dual microphone array, excels in both speech quality and downstream task performance. It operates in real-time with minimal parameters, making it ideal for low-latency, on-device streaming applications. </p>
<blockquote>
<p>åœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­ï¼Œåˆ©ç”¨ç©ºé—´ç‰¹å¾å¢å¼ºç›®æ ‡è¯­éŸ³è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨æœ‰é™çš„éº¦å…‹é£é˜µåˆ—ä¸‹ï¼Œå¼€å‘ç´§å‡‘çš„å¤šé€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ¡ä»¶ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰å¯¼å‘ç©ºé—´é€‰æ‹©æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œä½¿ç”¨ä¸‰ä¸ªå¯¼å‘å‘é‡æ¥æŒ‡å¯¼å¢å¼ºå¹¶ç¡®å®šå¢å¼ºèŒƒå›´ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å› æœå¯¼å‘U-Netï¼ˆCDUNetï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥åŸå§‹å¤šé€šé“è¯­éŸ³å’Œæ‰€éœ€çš„å¢å¼ºå®½åº¦ä¸ºè¾“å…¥ã€‚è¿™å¯ä»¥æ ¹æ®ç›®æ ‡æ–¹å‘å’Œå¹²æ‰°ä¿¡å·ä¹‹é—´çš„è§’åº¦é—´éš”åŠ¨æ€è°ƒæ•´å¯¼å‘å‘é‡ï¼Œå¹¶å¾®è°ƒå¢å¼ºåŒºåŸŸã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»…ä½¿ç”¨åŒéº¦å…‹é£é˜µåˆ—ï¼Œåœ¨è¯­éŸ³è´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚å®ƒä»¥å®æ—¶æ–¹å¼è¿è¡Œï¼Œå‚æ•°æœ€å°‘ï¼Œéå¸¸é€‚åˆä½å»¶è¿Ÿçš„åœ¨çº¿æµå¼åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18141v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤æ‚çš„å¤šè¯´è¯äººåœºæ™¯ä¸­ï¼Œåˆ©ç”¨ç©ºé—´ç‰¹å¾å¢å¼ºç›®æ ‡è¯­éŸ³è‡³å…³é‡è¦ã€‚é’ˆå¯¹æœ‰é™éº¦å…‹é£é˜µåˆ—åœ¨æä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹æ„å»ºç´§å‡‘çš„å¤šé€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„ä¸‰å‘ç©ºé—´é€‰æ‹©æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¸‰ä¸ªå¼•å¯¼å‘é‡è¿›è¡Œå¢å¼ºå¹¶ç¡®å®šå¢å¼ºèŒƒå›´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å› æœå¯¼å‘U-Netæ¨¡å‹ï¼ˆCDUNetï¼‰ï¼Œèƒ½å¤Ÿæ ¹æ®ç›®æ ‡æ–¹å‘å’Œè§’åº¦é—´éš”å¯¹å¹²æ‰°ä¿¡å·è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œä»è€Œå®ç°ç²¾ç»†åŒ–å¢å¼ºã€‚æ­¤æ¨¡å‹ä»…ç”¨åŒéº¦å…‹é£é˜µåˆ—å°±è¡¨ç°å‡ºè‰²ï¼Œé€‚ç”¨äºå®æ—¶ã€ä½å»¶è¿Ÿçš„åœ¨çº¿æµå¼åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­ï¼Œåˆ©ç”¨ç©ºé—´ç‰¹å¾å¢å¼ºç›®æ ‡è¯­éŸ³æ˜¯å¿…è¦çš„ã€‚</li>
<li>é’ˆå¯¹æœ‰é™éº¦å…‹é£é˜µåˆ—å’Œæä½ä¿¡å™ªæ¯”æ¡ä»¶ï¼Œæ„å»ºå¤šé€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§çµæ´»çš„ä¸‰å‘ç©ºé—´é€‰æ‹©æ–¹æ³•ï¼Œä½¿ç”¨ä¸‰ä¸ªå¼•å¯¼å‘é‡è¿›è¡Œè¯­éŸ³å¢å¼ºã€‚</li>
<li>å¼•å…¥äº†å› æœå¯¼å‘U-Netï¼ˆCDUNetï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç›®æ ‡æ–¹å‘å’Œè§’åº¦é—´éš”åŠ¨æ€è°ƒæ•´å¢å¼ºæ•ˆæœã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ä»…æœ‰åŒéº¦å…‹é£é˜µåˆ—çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ¨¡å‹åœ¨è¯­éŸ³è´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9e4b40551aef40ff2add3310f36ddd2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86e4be31bad6d3e8241d5204a21f28fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc14985cb9d94204b247b4ac6b002fe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09e77a76cac8a1564b7aa6c5cf970e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00a9eb67b6d38e11e50710ea46033a23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fed10ba26e571a57a58449496edd1e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8089373fb35fd890c18a705caaa0c4b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Time-Graph-Frequency-Representation-with-Singular-Value-Decomposition-for-Neural-Speech-Enhancement"><a href="#Time-Graph-Frequency-Representation-with-Singular-Value-Decomposition-for-Neural-Speech-Enhancement" class="headerlink" title="Time-Graph Frequency Representation with Singular Value Decomposition   for Neural Speech Enhancement"></a>Time-Graph Frequency Representation with Singular Value Decomposition   for Neural Speech Enhancement</h2><p><strong>Authors:Tingting Wang, Tianrui Wang, Meng Ge, Qiquan Zhang, Zirui Ge, Zhen Yang</strong></p>
<p>Time-frequency (T-F) domain methods for monaural speech enhancement have benefited from the success of deep learning. Recently, focus has been put on designing two-stream network models to predict amplitude mask and phase separately, or, coupling the amplitude and phase into Cartesian coordinates and constructing real and imaginary pairs. However, most methods suffer from the alignment modeling of amplitude and phase (real and imaginary pairs) in a two-stream network framework, which inevitably incurs performance restrictions. In this paper, we introduce a graph Fourier transform defined with the singular value decomposition (GFT-SVD), resulting in real-valued time-graph representation for neural speech enhancement. This real-valued representation-based GFT-SVD provides an ability to align the modeling of amplitude and phase, leading to avoiding recovering the target speech phase information. Our findings demonstrate the effects of real-valued time-graph representation based on GFT-SVD for neutral speech enhancement. The extensive speech enhancement experiments establish that the combination of GFT-SVD and DNN outperforms the combination of GFT with the eigenvector decomposition (GFT-EVD) and magnitude estimation UNet, and outperforms the short-time Fourier transform (STFT) and DNN, regarding objective intelligibility and perceptual quality. We release our source code at: <a target="_blank" rel="noopener" href="https://github.com/Wangfighting0015/GFT/_project">https://github.com/Wangfighting0015/GFT\_project</a>. </p>
<blockquote>
<p>é’ˆå¯¹å•å£°é“è¯­éŸ³å¢å¼ºçš„æ—¶é¢‘åŸŸæ–¹æ³•å—ç›Šäºæ·±åº¦å­¦ä¹ å–å¾—çš„æˆå°±ã€‚æœ€è¿‘ï¼Œäººä»¬ä¸“æ³¨äºè®¾è®¡åŒæµä¼ ç»œæ¨¡å‹ï¼Œä»¥ä¾¿åˆ†åˆ«é¢„æµ‹æŒ¯å¹…æ©æ¨¡å’Œç›¸ä½ï¼Œæˆ–è€…å°†æŒ¯å¹…å’Œç›¸ä½è€¦åˆåˆ°ç¬›å¡å°”åæ ‡ç³»ä¸­ï¼Œå¹¶æ„å»ºå®æ•°å’Œè™šæ•°å¯¹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•åœ¨åŒæµä¼ ç»œæ¡†æ¶ä¸­é­å—æŒ¯å¹…å’Œç›¸ä½ï¼ˆå®æ•°å’Œè™šæ•°å¯¹ï¼‰çš„å¯¹é½å»ºæ¨¡å›°æ‰°ï¼Œè¿™ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´æ€§èƒ½é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£å®šä¹‰çš„å›¾å‚…é‡Œå¶å˜æ¢ï¼ˆGFT-SVDï¼‰ï¼Œä»è€Œå¾—åˆ°ç”¨äºç¥ç»è¯­éŸ³å¢å¼ºçš„å®å€¼æ—¶é—´å›¾è¡¨ç¤ºã€‚è¿™ç§åŸºäºå®å€¼è¡¨ç¤ºçš„GFT-SVDæä¾›äº†å¯¹é½æŒ¯å¹…å’Œç›¸ä½å»ºæ¨¡çš„èƒ½åŠ›ï¼Œé¿å…äº†æ¢å¤ç›®æ ‡è¯­éŸ³ç›¸ä½ä¿¡æ¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºGFT-SVDçš„å®å€¼æ—¶é—´å›¾è¡¨ç¤ºå¯¹äºä¸­æ€§è¯­éŸ³å¢å¼ºå…·æœ‰å½±å“ã€‚å¹¿æ³›çš„è¯­éŸ³å¢å¼ºå®éªŒè¯å®ï¼ŒGFT-SVDä¸æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„ç»“åˆä¼˜äºGFTä¸ç‰¹å¾å‘é‡åˆ†è§£ï¼ˆGFT-EVDï¼‰çš„ç»“åˆä»¥åŠå¹…åº¦ä¼°è®¡UNetï¼Œå¹¶ä¸”åœ¨å®¢è§‚å¯ç†è§£æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºçŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰å’ŒDNNã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹é“¾æ¥å‘å¸ƒäº†æˆ‘ä»¬çš„æºä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Wangfighting0015/GFT_project%E3%80%82">https://github.com/Wangfighting0015/GFT_projectã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16823v2">PDF</a> 5 pages, 4 figures, Accepted by ICASSP2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨åŸºäºå¥‡å¼‚å€¼åˆ†è§£çš„å›¾å‚…é‡Œå¶å˜æ¢ï¼ˆGFT-SVDï¼‰å®ç°çœŸå®å€¼æ—¶é—´å›¾è¡¨ç¤ºä¸ºç¥ç»è¯­éŸ³å¢å¼ºæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆæ–¹æ³•ã€‚é€šè¿‡å¯¹æŒ¯å¹…å’Œç›¸ä½è¿›è¡Œå®å€¼æ—¶é—´å›¾è¡¨ç¤ºï¼Œé¿å…äº†ç›®æ ‡è¯­éŸ³ç›¸ä½ä¿¡æ¯çš„æ¢å¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå®å€¼æ—¶é—´å›¾è¡¨ç¤ºçš„GFT-SVDå¯¹äºä¸­æ€§è¯­éŸ³å¢å¼ºå…·æœ‰æ˜¾è‘—æ•ˆæœã€‚åœ¨å¹¿æ³›çš„è¯­éŸ³å¢å¼ºå®éªŒä¸­ï¼Œç»“åˆGFT-SVDå’Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„è¡¨ç°ä¼˜äºç»“åˆGFTä¸ç‰¹å¾å‘é‡åˆ†è§£ï¼ˆGFT-EVDï¼‰ä»¥åŠå¹…åº¦ä¼°è®¡UNetï¼Œå¹¶ä¸”åœ¨å®¢è§‚å¯ç†è§£æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºçŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰å’ŒDNNã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥åŸºäºå¥‡å¼‚å€¼åˆ†è§£çš„å›¾å‚…é‡Œå¶å˜æ¢ï¼ˆGFT-SVDï¼‰å®ç°çœŸå®å€¼æ—¶é—´å›¾è¡¨ç¤ºï¼Œä¸ºç¥ç»è¯­éŸ³å¢å¼ºæä¾›æ–°æ–¹æ³•ã€‚</li>
<li>å®å€¼æ—¶é—´å›¾è¡¨ç¤ºèƒ½å¤Ÿå®ç°å¯¹æŒ¯å¹…å’Œç›¸ä½çš„å¯¹é½å»ºæ¨¡ï¼Œé¿å…æ¢å¤ç›®æ ‡è¯­éŸ³çš„ç›¸ä½ä¿¡æ¯ã€‚</li>
<li>GFT-SVDç»“åˆDNNçš„æ–¹æ³•åœ¨è¯­éŸ³å¢å¼ºå®éªŒä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>GFT-SVDä¸STFTåŠDNNç­‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å®¢è§‚å¯ç†è§£æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢æœ‰æ›´ä¼˜è¡¨ç°ã€‚</li>
<li>æœ¬æ–‡æä¾›çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
<li>é€šè¿‡å®å€¼æ—¶é—´å›¾è¡¨ç¤ºæ–¹æ³•æé«˜äº†ç¥ç»è¯­éŸ³å¢å¼ºçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†å®å€¼æ—¶é—´å›¾è¡¨ç¤ºåœ¨å¤„ç†æŒ¯å¹…å’Œç›¸ä½å¯¹é½å»ºæ¨¡é—®é¢˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bdfaa15e3fad3d7f5270dbd003c22507.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffa49783ef187100a7c44402b5a13b30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43d52ce4c19efb35169d29f18f9fae5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d301f59d6408a6a46145b1b4ba9a69b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d5e6b6abebea4d5dd31b482a22ad596.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding"><a href="#Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding" class="headerlink" title="Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding"></a>Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding</h2><p><strong>Authors:Jiahui Zhao, Hao Shi, Chenrui Cui, Tianrui Wang, Hexin Liu, Zhaoheng Ni, Lingxuan Ye, Longbiao Wang</strong></p>
<p>Code-switching (CS) automatic speech recognition (ASR) faces challenges due to the language confusion resulting from accents, auditory similarity, and seamless language switches. Adaptation on the pre-trained multi-lingual model has shown promising performance for CS-ASR. In this paper, we adapt Whisper, which is a large-scale multilingual pre-trained speech recognition model, to CS from both encoder and decoder parts. First, we propose an encoder refiner to enhance the encoderâ€™s capacity of intra-sentence swithching. Second, we propose using two sets of language-aware adapters with different language prompt embeddings to achieve language-specific decoding information in each decoder layer. Then, a fusion module is added to fuse the language-aware decoding. The experimental results using the SEAME dataset show that, compared with the baseline model, the proposed approach achieves a relative MER reduction of 4.1% and 7.2% on the dev_man and dev_sge test sets, respectively, surpassing state-of-the-art methods. Through experiments, we found that the proposed method significantly improves the performance on non-native language in CS speech, indicating that our approach enables Whisper to better distinguish between the two languages. </p>
<blockquote>
<p>ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢ä¸´ç€ç”±äºå£éŸ³ã€å¬è§‰ç›¸ä¼¼æ€§å’Œæ— ç¼è¯­è¨€åˆ‡æ¢å¯¼è‡´çš„è¯­è¨€æ··æ·†æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é¢„è®­ç»ƒçš„å¤šå…ƒè¯­è¨€æ¨¡å‹çš„é€‚é…å·²ç»æ˜¾ç¤ºå‡ºå¯¹CS-ASRçš„æœ‰å‰é€”çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€‚é…äº†whisperè¿™æ ·ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­è¨€é¢„è®­ç»ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œä»¥é€‚åº”ç¼–ç å™¨å’Œè§£ç å™¨éƒ¨åˆ†çš„CSã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç¼–ç å™¨ç²¾ç‚¼å™¨ï¼Œä»¥æé«˜ç¼–ç å™¨åœ¨å¥å­å†…åˆ‡æ¢çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ä¸¤ç»„å¸¦æœ‰ä¸åŒè¯­è¨€æç¤ºåµŒå…¥çš„è¯­è¨€æ„ŸçŸ¥é€‚é…å™¨ï¼Œä»¥å®ç°æ¯ä¸ªè§£ç å™¨å±‚ä¸­çš„ç‰¹å®šè¯­è¨€è§£ç ä¿¡æ¯ã€‚ç„¶åï¼Œæ·»åŠ ä¸€ä¸ªèåˆæ¨¡å—æ¥èåˆè¯­è¨€æ„ŸçŸ¥è§£ç ã€‚ä½¿ç”¨SEAMEæ•°æ®é›†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨dev_manå’Œdev_sgeæµ‹è¯•é›†ä¸Šåˆ†åˆ«å®ç°äº†ç›¸å¯¹MERé™ä½4.1%å’Œ7.2%ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¯¥æ–¹æ³•åœ¨éæœ¬åœ°è¯­è¨€çš„CSè¯­éŸ³æ€§èƒ½ä¸Šæœ‰äº†æ˜¾è‘—æé«˜ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•èƒ½ä½¿whisperæ›´å¥½åœ°åŒºåˆ†ä¸¤ç§è¯­è¨€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16507v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹è¿›è¡Œä»£ç åˆ‡æ¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†æ”¹è¿›æ€§èƒ½ï¼Œå¯¹å¤§å‹é¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹Whisperè¿›è¡Œäº†é€‚åº”è°ƒæ•´ï¼Œæ¶‰åŠç¼–ç å™¨ç²¾ç‚¼å™¨å’Œè§£ç å™¨éƒ¨åˆ†çš„æ”¹è¿›ã€‚é€šè¿‡é‡‡ç”¨èåˆæ¨¡å—ï¼Œå®ç°è¯­è¨€ç‰¹å®šçš„è§£ç ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨SEAMEæ•°æ®é›†ä¸Šçš„å¹³å‡é”™è¯¯ç‡æœ‰æ‰€é™ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨éæ¯è¯­ä»£ç çš„åˆ‡æ¢è¯­éŸ³æ–¹é¢è¡¨ç°æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«é¢ä¸´è¯­è¨€æ··æ·†çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å£éŸ³ã€å¬è§‰ç›¸ä¼¼æ€§å’Œæ— ç¼è¯­è¨€åˆ‡æ¢ã€‚</li>
<li>é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹é€‚åº”äºä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ˜¾ç¤ºå‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>å¯¹ç¼–ç å™¨è¿›è¡Œç²¾ç‚¼ä»¥æé«˜å¥å­å†…éƒ¨çš„åˆ‡æ¢èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨ä¸¤ç»„è¯­è¨€æ„ŸçŸ¥é€‚é…å™¨ä¸ä¸åŒçš„è¯­è¨€æç¤ºåµŒå…¥å®ç°æ¯å±‚è§£ç å™¨çš„è¯­è¨€ç‰¹å®šè§£ç ä¿¡æ¯ã€‚</li>
<li>æ·»åŠ èåˆæ¨¡å—ä»¥èåˆè¯­è¨€æ„ŸçŸ¥è§£ç ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æµ‹è¯•é›†ä¸Šçš„å¹³å‡é”™è¯¯ç‡æœ‰æ‰€é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad2bffb15c91a8bf319a4b37ecf3c835.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a30302356c17d7ddd2d5b0cd9dad213.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82c3763f9c6385f3d6ee3a285235e05f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Streaming-Keyword-Spotting-Boosted-by-Cross-layer-Discrimination-Consistency"><a href="#Streaming-Keyword-Spotting-Boosted-by-Cross-layer-Discrimination-Consistency" class="headerlink" title="Streaming Keyword Spotting Boosted by Cross-layer Discrimination   Consistency"></a>Streaming Keyword Spotting Boosted by Cross-layer Discrimination   Consistency</h2><p><strong>Authors:Yu Xi, Haoyu Li, Xiaoyu Gu, Hao Li, Yidi Jiang, Kai Yu</strong></p>
<p>Connectionist Temporal Classification (CTC), a non-autoregressive training criterion, is widely used in online keyword spotting (KWS). However, existing CTC-based KWS decoding strategies either rely on Automatic Speech Recognition (ASR), which performs suboptimally due to its broad search over the acoustic space without keyword-specific optimization, or on KWS-specific decoding graphs, which are complex to implement and maintain. In this work, we propose a streaming decoding algorithm enhanced by Cross-layer Discrimination Consistency (CDC), tailored for CTC-based KWS. Specifically, we introduce a streamlined yet effective decoding algorithm capable of detecting the start of the keyword at any arbitrary position. Furthermore, we leverage discrimination consistency information across layers to better differentiate between positive and false alarm samples. Our experiments on both clean and noisy Hey Snips datasets show that the proposed streaming decoding strategy outperforms ASR-based and graph-based KWS baselines. The CDC-boosted decoding further improves performance, yielding an average absolute recall improvement of 6.8% and a 46.3% relative reduction in the miss rate compared to the graph-based KWS baseline, with a very low false alarm rate of 0.05 per hour. </p>
<blockquote>
<p>è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰æ˜¯ä¸€ç§éè‡ªå›å½’è®­ç»ƒå‡†åˆ™ï¼Œå¹¿æ³›åº”ç”¨äºåœ¨çº¿å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºCTCçš„KWSè§£ç ç­–ç•¥è¦ä¹ˆä¾èµ–äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼Œç”±äºå…¶åœ¨å£°å­¦ç©ºé—´è¿›è¡Œå¹¿æ³›æœç´¢è€Œæ²¡æœ‰é’ˆå¯¹å…³é”®è¯è¿›è¡Œç‰¹å®šä¼˜åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ï¼›è¦ä¹ˆä¾èµ–äºç‰¹å®šçš„KWSè§£ç å›¾ï¼Œè¿™äº›å›¾å¤æ‚ä¸”éš¾ä»¥å®æ–½å’Œç»´æŠ¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡è·¨å±‚é‰´åˆ«ä¸€è‡´æ€§ï¼ˆCDCï¼‰å¢å¼ºçš„æµå¼è§£ç ç®—æ³•ï¼Œé€‚ç”¨äºåŸºäºCTCçš„KWSã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€åŒ–ä½†æœ‰æ•ˆçš„è§£ç ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„ä½ç½®æ£€æµ‹å…³é”®è¯çš„å¼€å§‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨è·¨å±‚çš„é‰´åˆ«ä¸€è‡´æ€§ä¿¡æ¯æ¥æ›´å¥½åœ°åŒºåˆ†æ­£æ ·æœ¬å’Œè¯¯æŠ¥æ ·æœ¬ã€‚æˆ‘ä»¬åœ¨å¹²å‡€å’Œå˜ˆæ‚çš„Hey Snipsæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æµå¼è§£ç ç­–ç•¥ä¼˜äºåŸºäºASRå’ŒåŸºäºå›¾çš„KWSåŸºçº¿ã€‚é€šè¿‡CDCå¢å¼ºçš„è§£ç è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œä¸åŸºäºå›¾çš„KWSåŸºçº¿ç›¸æ¯”ï¼Œå¹³å‡ç»å¯¹å¬å›ç‡æé«˜äº†6.8%ï¼Œæ¼æŠ¥ç‡ç›¸å¯¹é™ä½äº†46.3%ï¼ŒåŒæ—¶æ¯å°æ—¶è¯¯æŠ¥ç‡éå¸¸ä½ï¼Œä¸º0.05æ¬¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12635v2">PDF</a> Accepted by ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰çš„åœ¨çº¿å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰çš„æ”¹è¿›è§£ç ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡è·¨å±‚åˆ¤åˆ«ä¸€è‡´æ€§ï¼ˆCDCï¼‰å¢å¼ºï¼Œå¼•å…¥äº†ä¸€ç§ç®€åŒ–çš„è§£ç ç®—æ³•ï¼Œèƒ½åœ¨ä»»æ„ä½ç½®æ£€æµ‹å…³é”®è¯çš„å¼€å§‹ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’ŒåŸºäºå›¾çš„KWSåŸºçº¿ç›¸æ¯”ï¼Œè¯¥è§£ç ç­–ç•¥å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚CDCçš„å¼•å…¥è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œå¹³å‡ç»å¯¹å¬å›ç‡æé«˜äº†6.8%ï¼Œç›¸å¯¹äºåŸºäºå›¾çš„KWSåŸºçº¿ï¼Œè¯¯æŠ¥ç‡é™ä½äº†46.3%ï¼Œæ¯å°æ—¶çš„è¯¯æŠ¥ç‡ä»…ä¸ºæ¯å°æ—¶0.05æ¬¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰ç”¨äºåœ¨çº¿å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ã€‚</li>
<li>ç°æœ‰CTC-based KWSè§£ç ç­–ç•¥ä¾èµ–äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–å¤æ‚çš„KWSè§£ç å›¾ï¼Œå­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>æå‡ºä¸€ç§æ”¹è¿›çš„è§£ç ç®—æ³•ï¼Œèƒ½åœ¨ä»»æ„ä½ç½®æ£€æµ‹å…³é”®è¯çš„å¼€å§‹ã€‚</li>
<li>é€šè¿‡è·¨å±‚åˆ¤åˆ«ä¸€è‡´æ€§ï¼ˆCDCï¼‰å¢å¼ºè§£ç ç®—æ³•ï¼Œæé«˜å…³é”®è¯è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥è§£ç ç­–ç•¥åœ¨æ¸…æ´å’Œå˜ˆæ‚çš„Hey Snipsæ•°æ®é›†ä¸Šå‡ä¼˜äºåŸºäºASRå’ŒåŸºäºå›¾çš„KWSåŸºçº¿ã€‚</li>
<li>CDCçš„å¼•å…¥è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œå¹³å‡ç»å¯¹å¬å›ç‡æé«˜ï¼Œè¯¯æŠ¥ç‡é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e8fb80ddcd2106a951b4eb89fca24b31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25becc6ca17512c6389bde9859232d21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-729e4fdd3eabb88bbcff0997af20c29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e24f5c79b89e9a15fd52b096f632537b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbe6980727f7156eb6a3ce658e9f476e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HDMoLE-Mixture-of-LoRA-Experts-with-Hierarchical-Routing-and-Dynamic-Thresholds-for-Fine-Tuning-LLM-based-ASR-Models"><a href="#HDMoLE-Mixture-of-LoRA-Experts-with-Hierarchical-Routing-and-Dynamic-Thresholds-for-Fine-Tuning-LLM-based-ASR-Models" class="headerlink" title="HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic   Thresholds for Fine-Tuning LLM-based ASR Models"></a>HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic   Thresholds for Fine-Tuning LLM-based ASR Models</h2><p><strong>Authors:Bingshen Mu, Kun Wei, Qijie Shao, Yong Xu, Lei Xie</strong></p>
<p>Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named \textit{HDMoLE}, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç»“åˆçš„è¿›å±•åœ¨é€šç”¨é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æ•ˆæœã€‚è™½ç„¶é€šå¸¸é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ‰€æœ‰æ¨¡å‹å‚æ•°æ¥é€‚åº”åŸºäºé¢„è®­ç»ƒLLMçš„ASRæ¨¡å‹åˆ°ç‰¹å®šé¢†åŸŸï¼Œä½†å®ƒå¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”ä¼šæ˜¾è‘—å‡å°‘å…¶åœ¨é€šç”¨é¢†åŸŸä¸­çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å‚æ•°é«˜æ•ˆå¤šé¢†åŸŸå¾®è°ƒæ–¹æ³•ï¼Œç”¨äºé€‚åº”åŸºäºé¢„è®­ç»ƒLLMçš„ASRæ¨¡å‹åˆ°å¤šå£éŸ³é¢†åŸŸï¼Œè¯¥æ–¹æ³•åœ¨å¤šå£éŸ³é¢†åŸŸä¸­æ— éœ€ç¾éš¾æ€§é—å¿˜å³å¯è¿›è¡Œé€‚åº”ï¼Œè¢«ç§°ä¸ºHDMoLEã€‚å®ƒç»“åˆäº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä¸ä¸“å®¶æ··åˆå™¨ï¼ˆMoEï¼‰çš„ä¼˜åŠ¿ï¼Œå¹¶åˆ©ç”¨å±‚æ¬¡è·¯ç”±å’ŒåŠ¨æ€é˜ˆå€¼ï¼Œå¯ä»¥æ¨å¹¿åˆ°ä»»ä½•çº¿æ€§å±‚ã€‚å±‚æ¬¡è·¯ç”±å»ºç«‹äº†LoRAä¸“å®¶å’Œå£éŸ³é¢†åŸŸä¹‹é—´çš„æ˜ç¡®å¯¹åº”å…³ç³»ï¼Œæé«˜äº†LoRAä¸“å®¶ä¹‹é—´çš„è·¨é¢†åŸŸåä½œã€‚ä¸é™æ€Top-Kç­–ç•¥æ¿€æ´»LoRAä¸“å®¶ä¸åŒï¼ŒåŠ¨æ€é˜ˆå€¼å¯ä»¥è‡ªé€‚åº”åœ°æ¿€æ´»MoEå±‚ä¸­ä¸åŒæ•°é‡çš„LoRAä¸“å®¶ã€‚åœ¨å¤šç§å£éŸ³å’Œæ ‡å‡†æ™®é€šè¯æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†HDMoLEçš„æœ‰æ•ˆæ€§ã€‚å°†HDMoLEåº”ç”¨äºåŸºäºLLMçš„ASRæ¨¡å‹çš„æŠ•å½±æ¨¡å—ï¼Œåœ¨ç›®æ ‡å¤šå£éŸ³é¢†åŸŸå–å¾—äº†ä¸å®Œå…¨å¾®è°ƒç›¸ä¼¼çš„æ€§èƒ½ï¼ŒåŒæ—¶ä»…ä½¿ç”¨å…¨å¾®è°ƒæ‰€éœ€çš„å¯è®­ç»ƒå‚æ•°çš„9.6%ï¼Œå¹¶ä¸”åœ¨æºé€šç”¨é¢†åŸŸçš„æ€§èƒ½å‡ ä¹æ²¡æœ‰ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19878v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯åœ¨é€šç”¨é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹æ‰€æœ‰æ¨¡å‹å‚æ•°è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»¥é€‚åº”ç‰¹å®šé¢†åŸŸæ—¶ï¼Œä¼šå¸¦æ¥é«˜è®¡ç®—æˆæœ¬å¹¶é™ä½å…¶åœ¨é€šç”¨é¢†åŸŸçš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å‚æ•°é«˜æ•ˆå¤šé¢†åŸŸå¾®è°ƒæ–¹æ³•ï¼Œåä¸ºHDMoLEï¼Œç”¨äºé€‚åº”å¤šå£éŸ³é¢†åŸŸçš„é¢„è®­ç»ƒLLM-based ASRæ¨¡å‹ï¼ŒåŒæ—¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä¸ä¸“å®¶æ··åˆå™¨ï¼ˆMoEï¼‰ï¼Œé‡‡ç”¨å±‚æ¬¡è·¯ç”±å’ŒåŠ¨æ€é˜ˆå€¼ï¼Œå¯ä»¥æ¨å¹¿åˆ°ä»»ä½•çº¿æ€§å±‚ã€‚å®éªŒè¡¨æ˜ï¼ŒHDMoLEåœ¨å¤šå£éŸ³åŠæ ‡å‡†æ™®é€šè¯æ•°æ®é›†ä¸Šè¡¨ç°æœ‰æ•ˆã€‚åº”ç”¨äºLLM-based ASRæ¨¡å‹çš„æŠ•å½±æ¨¡å—æ—¶ï¼ŒHDMoLEåœ¨ç›®æ ‡å¤šå£éŸ³é¢†åŸŸå®ç°äº†ä¸å…¨å¾®è°ƒç›¸ä¼¼çš„æ€§èƒ½ï¼ŒåŒæ—¶ä»…ä½¿ç”¨å…¨å¾®è°ƒæ‰€éœ€è®­ç»ƒå‚æ•°çš„9.6%ï¼Œå¹¶ä¸”åœ¨æºé€šç”¨é¢†åŸŸçš„æ€§èƒ½æŸå¤±æå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„é›†æˆåœ¨é€šç”¨é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è™½ç„¶èƒ½ä½¿LLM-based ASRæ¨¡å‹é€‚åº”ç‰¹å®šé¢†åŸŸï¼Œä½†è®¡ç®—æˆæœ¬é«˜å¹¶åœ¨é€šç”¨é¢†åŸŸæ€§èƒ½é™ä½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å‚æ•°é«˜æ•ˆå¤šé¢†åŸŸå¾®è°ƒæ–¹æ³•â€”â€”HDMoLEï¼Œé€‚ç”¨äºå¤šå£éŸ³é¢†åŸŸçš„é¢„è®­ç»ƒLLM-based ASRæ¨¡å‹ã€‚</li>
<li>HDMoLEç»“åˆäº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä¸ä¸“å®¶æ··åˆå™¨ï¼ˆMoEï¼‰ã€‚</li>
<li>å±‚æ¬¡è·¯ç”±å’ŒåŠ¨æ€é˜ˆå€¼æ–¹æ³•è¢«ç”¨äºHDMoLEï¼Œä»¥æé«˜è·¨é¢†åŸŸçš„åä½œèƒ½åŠ›å¹¶é€‚åº”ä¸åŒæ•°é‡çš„LoRAä¸“å®¶ã€‚</li>
<li>å®éªŒè¡¨æ˜HDMoLEåœ¨å¤šå£éŸ³åŠæ ‡å‡†æ™®é€šè¯æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aadcf52086991c24b2d998a5372495c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c21bce8956dc25aedc5af40374b09f41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c37cb7eb9add48fb1727b9ed5be64f8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NanoVoice-Efficient-Speaker-Adaptive-Text-to-Speech-for-Multiple-Speakers"><a href="#NanoVoice-Efficient-Speaker-Adaptive-Text-to-Speech-for-Multiple-Speakers" class="headerlink" title="NanoVoice: Efficient Speaker-Adaptive Text-to-Speech for Multiple   Speakers"></a>NanoVoice: Efficient Speaker-Adaptive Text-to-Speech for Multiple   Speakers</h2><p><strong>Authors:Nohil Park, Heeseung Kim, Che Hyun Lee, Jooyoung Choi, Jiheum Yeom, Sungroh Yoon</strong></p>
<p>We present NanoVoice, a personalized text-to-speech model that efficiently constructs voice adapters for multiple speakers simultaneously. NanoVoice introduces a batch-wise speaker adaptation technique capable of fine-tuning multiple references in parallel, significantly reducing training time. Beyond building separate adapters for each speaker, we also propose a parameter sharing technique that reduces the number of parameters used for speaker adaptation. By incorporating a novel trainable scale matrix, NanoVoice mitigates potential performance degradation during parameter sharing. NanoVoice achieves performance comparable to the baselines, while training 4 times faster and using 45 percent fewer parameters for speaker adaptation with 40 reference voices. Extensive ablation studies and analysis further validate the efficiency of our model. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†NanoVoiceï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸ªæ€§åŒ–çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°ä¸ºå¤šä¸ªå‘è¨€è€…åŒæ—¶æ„å»ºè¯­éŸ³é€‚é…å™¨ã€‚NanoVoiceå¼•å…¥äº†ä¸€ç§æ‰¹å¤„ç†è¯´è¯äººé€‚åº”æŠ€æœ¯ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¾®è°ƒå¤šä¸ªå‚è€ƒæ•°æ®ï¼Œä»è€Œå¤§å¤§ç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚é™¤äº†ä¸ºæ¯ä¸ªè¯´è¯äººæ„å»ºå•ç‹¬çš„é€‚é…å™¨å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å‚æ•°å…±äº«æŠ€æœ¯ï¼Œä»¥å‡å°‘ç”¨äºè¯´è¯äººé€‚åº”çš„å‚æ•°æ•°é‡ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„å¯è®­ç»ƒæ¯”ä¾‹çŸ©é˜µï¼ŒNanoVoiceåœ¨å‚æ•°å…±äº«æ—¶å‡è½»äº†æ€§èƒ½ä¸‹é™çš„æ½œåœ¨é£é™©ã€‚NanoVoiceçš„æ€§èƒ½ä¸åŸºçº¿ç›¸å½“ï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æé«˜äº†4å€ï¼Œç”¨äºè¯´è¯äººé€‚åº”çš„å‚æ•°å‡å°‘äº†45%ã€‚å¹¿æ³›çš„æ¶ˆèå®éªŒå’Œåˆ†æè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ¨¡å‹æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15760v2">PDF</a> IEEE International Conference on Acoustics, Speech, and Signal   Processing (ICASSP), 2025, Demo Page: <a target="_blank" rel="noopener" href="https://nanovoice.github.io/">https://nanovoice.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>NanoVoiceæ˜¯ä¸€ä¸ªä¸ªæ€§åŒ–çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶ä¸ºå¤šä¸ªè¯´è¯è€…æ„å»ºè¯­éŸ³é€‚é…å™¨ï¼Œå¹¶å¼•å…¥äº†æ‰¹å¤„ç†è¯´è¯è€…é€‚åº”æŠ€æœ¯ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¾®è°ƒå¤šä¸ªå‚è€ƒï¼Œæ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´ã€‚è¯¥æ¨¡å‹è¿˜æå‡ºäº†ä¸€ç§å‚æ•°å…±äº«æŠ€æœ¯ï¼Œå‡å°‘ç”¨äºè¯´è¯è€…é€‚åº”çš„å‚æ•°æ•°é‡ï¼Œå¹¶å€ŸåŠ©æ–°å‹å¯è®­ç»ƒæ¯”ä¾‹çŸ©é˜µç¼“è§£å‚æ•°å…±äº«æœŸé—´æ½œåœ¨çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚NanoVoiceçš„æ€§èƒ½ä¸åŸºçº¿ç›¸å½“ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜4å€ï¼Œä½¿ç”¨å‚æ•°è¿›è¡Œè¯´è¯è€…é€‚åº”æ—¶å‡å°‘äº†45%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NanoVoiceæ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ï¼Œèƒ½åŒæ—¶ä¸ºå¤šä¸ªè¯´è¯è€…æ„å»ºè¯­éŸ³é€‚é…å™¨ã€‚</li>
<li>å¼•å…¥æ‰¹å¤„ç†è¯´è¯è€…é€‚åº”æŠ€æœ¯ï¼Œèƒ½å¹¶è¡Œå¾®è°ƒå¤šä¸ªå‚è€ƒï¼Œæ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´ã€‚</li>
<li>æå‡ºäº†å‚æ•°å…±äº«æŠ€æœ¯ï¼Œå‡å°‘ç”¨äºè¯´è¯è€…é€‚åº”çš„å‚æ•°æ•°é‡ã€‚</li>
<li>é€šè¿‡æ–°å‹å¯è®­ç»ƒæ¯”ä¾‹çŸ©é˜µï¼Œç¼“è§£å‚æ•°å…±äº«æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>NanoVoiceçš„æ€§èƒ½ä¸åŸºçº¿ç›¸å½“ï¼Œè®­ç»ƒé€Ÿåº¦æå‡4å€ï¼Œå‚æ•°ä½¿ç”¨å‡å°‘45%ã€‚</li>
<li>è¿›è¡Œäº†å¹¿æ³›çš„åˆ†æå’Œæ¶ˆèç ”ç©¶ï¼ŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9785a5a98711f10b8167a1c69c0266a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6d1f62781447f565b27d2b15c775630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b58f850e7947ac5983ff7f0a0a990b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60eb50c57418bf7d0814c984ebe6bd4a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VoiceGuider-Enhancing-Out-of-Domain-Performance-in-Parameter-Efficient-Speaker-Adaptive-Text-to-Speech-via-Autoguidance"><a href="#VoiceGuider-Enhancing-Out-of-Domain-Performance-in-Parameter-Efficient-Speaker-Adaptive-Text-to-Speech-via-Autoguidance" class="headerlink" title="VoiceGuider: Enhancing Out-of-Domain Performance in Parameter-Efficient   Speaker-Adaptive Text-to-Speech via Autoguidance"></a>VoiceGuider: Enhancing Out-of-Domain Performance in Parameter-Efficient   Speaker-Adaptive Text-to-Speech via Autoguidance</h2><p><strong>Authors:Jiheum Yeom, Heeseung Kim, Jooyoung Choi, Che Hyun Lee, Nohil Park, Sungroh Yoon</strong></p>
<p>When applying parameter-efficient finetuning via LoRA onto speaker adaptive text-to-speech models, adaptation performance may decline compared to full-finetuned counterparts, especially for out-of-domain speakers. Here, we propose VoiceGuider, a parameter-efficient speaker adaptive text-to-speech system reinforced with autoguidance to enhance the speaker adaptation performance, reducing the gap against full-finetuned models. We carefully explore various ways of strengthening autoguidance, ultimately finding the optimal strategy. VoiceGuider as a result shows robust adaptation performance especially on extreme out-of-domain speech data. We provide audible samples in our demo page. </p>
<blockquote>
<p>å½“é€šè¿‡LoRAè¿›è¡Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ¥é€‚åº”è¯´è¯äººçš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹æ—¶ï¼Œä¸å®Œå…¨å¾®è°ƒè¿‡çš„æ¨¡å‹ç›¸æ¯”ï¼Œé€‚åº”æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéåŸŸå†…çš„è¯´è¯äººã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VoiceGuiderï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è‡ªåŠ¨å¼•å¯¼å¢å¼ºçš„å‚æ•°é«˜æ•ˆè¯´è¯äººè‡ªé€‚åº”æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿï¼Œä»¥æé«˜è¯´è¯äººé€‚åº”æ€§èƒ½ï¼Œç¼©å°ä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬ä»”ç»†æ¢ç´¢äº†åŠ å¼ºè‡ªåŠ¨å¼•å¯¼çš„å„ç§æ–¹æ³•ï¼Œå¹¶æœ€ç»ˆæ‰¾åˆ°äº†æœ€ä½³ç­–ç•¥ã€‚VoiceGuideråœ¨æç«¯éåŸŸè¯­éŸ³æ•°æ®ä¸Šè¡¨ç°å‡ºäº†ç¨³å¥çš„é€‚åº”æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨æ¼”ç¤ºé¡µé¢ä¸Šæä¾›äº†å¯å¬çš„æ ·æœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15759v2">PDF</a> IEEE International Conference on Acoustics, Speech, and Signal   Processing (ICASSP), 2025, Demo Page: <a target="_blank" rel="noopener" href="https://voiceguider.github.io/">https://voiceguider.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åœ¨æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ä¸­é‡‡ç”¨LoRAè¿›è¡Œå‚æ•°æœ‰æ•ˆå¾®è°ƒæ—¶ï¼Œç›¸æ¯”äºå…¨å¾®è°ƒçš„æ¨¡å‹ï¼Œå…¶é€‚åº”æ€§æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéåŸŸå†…çš„å‘è¨€äººæ›´ä¸ºæ˜æ˜¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VoiceGuiderç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¼ºåŒ–äº†å‚æ•°æœ‰æ•ˆçš„è¯­éŸ³è‡ªé€‚åº”åŠŸèƒ½å¹¶å¼•å…¥äº†è‡ªåŠ¨æŒ‡å¯¼æœºåˆ¶æ¥å¢å¼ºè¯­éŸ³é€‚åº”æ€§èƒ½ï¼Œç¼©å°äº†ä¸å…¨å¾®è°ƒæ¨¡å‹çš„å·®è·ã€‚ç»è¿‡å¯¹å¼ºåŒ–è‡ªåŠ¨æŒ‡å¯¼æœºåˆ¶çš„å¤šç§æ–¹æ³•çš„æ¢ç´¢ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†æœ€ä½³ç­–ç•¥ã€‚VoiceGuiderå¯¹äºæç«¯éåŸŸå†…çš„è¯­éŸ³æ•°æ®å±•ç°å‡ºäº†å¼ºå¤§çš„é€‚åº”èƒ½åŠ›ã€‚æˆ‘ä»¬å°†åœ¨æ¼”ç¤ºé¡µé¢ä¸­æä¾›å¯å¬çš„æ ·æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRAåœ¨åº”ç”¨äºè¯­éŸ³è‡ªé€‚åº”æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹æ—¶ï¼Œç›¸è¾ƒäºå…¨å¾®è°ƒæ¨¡å‹ï¼Œå…¶é€‚åº”æ€§æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚</li>
<li>VoiceGuiderç³»ç»Ÿæ˜¯ä¸€ç§å‚æ•°æœ‰æ•ˆçš„è¯­éŸ³è‡ªé€‚åº”ç³»ç»Ÿï¼Œé€šè¿‡å¼•å…¥è‡ªåŠ¨æŒ‡å¯¼æœºåˆ¶å¼ºåŒ–äº†æ€§èƒ½ã€‚</li>
<li>VoiceGuiderç³»ç»Ÿèƒ½æœ‰æ•ˆç¼©å°ä¸å…¨å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚</li>
<li>å¼ºåŒ–è‡ªåŠ¨æŒ‡å¯¼æœºåˆ¶çš„å¤šç§æ–¹æ³•è¢«æ¢ç´¢ï¼Œå¹¶æ‰¾åˆ°äº†æœ€ä½³ç­–ç•¥ã€‚</li>
<li>VoiceGuiderç³»ç»Ÿåœ¨å¤„ç†æç«¯éåŸŸå†…çš„è¯­éŸ³æ•°æ®æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>æ¼”ç¤ºé¡µé¢ä¸­æä¾›äº†å¯å¬çš„æ ·æœ¬ä»¥å±•ç¤ºVoiceGuiderç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9112fc85431af49e01e11faab161dbf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05889e4ac4ae9f23063e3d916d892b99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c7b2a9efa449e1ddaaad87e7454b04a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-016703487e861605fd3629e812d9d0fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5af3f8574589d06a6556533f9cd8bc9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Effective-Integration-of-KAN-for-Keyword-Spotting"><a href="#Effective-Integration-of-KAN-for-Keyword-Spotting" class="headerlink" title="Effective Integration of KAN for Keyword Spotting"></a>Effective Integration of KAN for Keyword Spotting</h2><p><strong>Authors:Anfeng Xu, Biqiao Zhang, Shuyu Kong, Yiteng Huang, Zhaojun Yang, Sangeeta Srivastava, Ming Sun</strong></p>
<p>Keyword spotting (KWS) is an important speech processing component for smart devices with voice assistance capability. In this paper, we investigate if Kolmogorov-Arnold Networks (KAN) can be used to enhance the performance of KWS. We explore various approaches to integrate KAN for a model architecture based on 1D Convolutional Neural Networks (CNN). We find that KAN is effective at modeling high-level features in lower-dimensional spaces, resulting in improved KWS performance when integrated appropriately. The findings shed light on understanding KAN for speech processing tasks and on other modalities for future researchers. </p>
<blockquote>
<p>å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰æ˜¯æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹è®¾å¤‡çš„é‡è¦è¯­éŸ³å¤„ç†ç»„ä»¶ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æ˜¯å¦èƒ½ç”¨äºæé«˜KWSçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ¢ç´¢äº†åŸºäºä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ¨¡å‹æ¶æ„é›†æˆKANçš„å„ç§æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨é€‚å½“é›†æˆæ—¶ï¼ŒKANåœ¨ä½ç»´ç©ºé—´ä¸­å»ºæ¨¡é«˜çº§ç‰¹å¾éå¸¸æœ‰æ•ˆï¼Œä»è€Œæé«˜äº†KWSçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæœªæ¥ç ”ç©¶äººå‘˜ç†è§£KANåœ¨è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶ä¸ºå…¶ä»–æ¨¡å¼æä¾›å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08605v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡æ¢è®¨äº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰åœ¨å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ä¸­çš„åº”ç”¨ï¼Œä»¥æ”¹è¿›æ™ºèƒ½è®¾å¤‡çš„è¯­éŸ³å¤„ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼ŒKANèƒ½å¤Ÿæœ‰æ•ˆåœ°åœ¨ä½ç»´ç©ºé—´ä¸­å»ºç«‹é«˜çº§ç‰¹å¾æ¨¡å‹ï¼Œå¹¶å¯ä»¥åœ¨é€‚å½“é›†æˆåæé«˜KWSçš„æ€§èƒ½ã€‚è¿™ä¸€å‘ç°æœ‰åŠ©äºæœªæ¥ç ”ç©¶äººå‘˜ç†è§£KANåœ¨è¯­éŸ³å¤„ç†ä»»åŠ¡åŠå…¶ä»–é¢†åŸŸçš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰è¢«åº”ç”¨äºå…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ä»¥æé«˜æ™ºèƒ½è®¾å¤‡çš„è¯­éŸ³å¤„ç†èƒ½åŠ›ã€‚</li>
<li>KANèƒ½å¤Ÿæœ‰æ•ˆåœ°åœ¨ä½ç»´ç©ºé—´ä¸­å»ºç«‹é«˜çº§ç‰¹å¾æ¨¡å‹ã€‚</li>
<li>KANçš„é€‚å½“é›†æˆå¯ä»¥æé«˜KWSçš„æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶åŸºäº1Då·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ¨¡å‹æ¶æ„æ¥é›†æˆKANã€‚</li>
<li>ç ”ç©¶ç»“æœæœ‰åŠ©äºæœªæ¥ç ”ç©¶äººå‘˜ç†è§£KANåœ¨è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå…¶ä»–é¢†åŸŸçš„ç ”ç©¶è€…æä¾›äº†å…³äºä½¿ç”¨KANçš„å¯ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-637d37df3b49f9d6185406224979ae04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7631d1be956723816b9e73b1d8c49538.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e344f0851469dcd0245c8658271b749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f9430e3210e6c45f652f57b8e97afef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4957e10ec79eab5bd7d2d2a1653d0846.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8b79f8332d90d0e13340664733a5ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6cf66c6ab5144f50ebe4fa60be1e49a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Full-text-Error-Correction-for-Chinese-Speech-Recognition-with-Large-Language-Model"><a href="#Full-text-Error-Correction-for-Chinese-Speech-Recognition-with-Large-Language-Model" class="headerlink" title="Full-text Error Correction for Chinese Speech Recognition with Large   Language Model"></a>Full-text Error Correction for Chinese Speech Recognition with Large   Language Model</h2><p><strong>Authors:Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang</strong></p>
<p>Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic Speech Recognition (ASR). However, most research focuses on utterances from short-duration speech recordings, which are the predominant form of speech data for supervised ASR training. This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer speech recordings, such as transcripts from podcasts, news broadcasts, and meetings. First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-speech synthesis, ASR, and error-correction pair extractor. This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive. Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction. Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. This establishes a promising baseline for further research. The dataset is available on the website. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„é”™è¯¯çº æ­£æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨æ¥è‡ªçŸ­æ—¶æ®µè¯­éŸ³è®°å½•çš„ç‰‡æ®µä¸Šï¼Œè¿™æ˜¯æœ‰ç›‘ç£ASRè®­ç»ƒçš„ä¸»è¦è¯­éŸ³æ•°æ®å½¢å¼ã€‚æœ¬æ–‡ç ”ç©¶äº†LLMåœ¨ç”±ASRç³»ç»Ÿä»è¾ƒé•¿è¯­éŸ³è®°å½•ï¼ˆå¦‚æ’­å®¢ã€æ–°é—»å¹¿æ’­å’Œä¼šè®®è®°å½•ï¼‰ç”Ÿæˆçš„å…¨æ–‡ä¸­çš„é”™è¯¯çº æ­£æ•ˆæœã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç”¨äºå…¨æ–‡é”™è¯¯çº æ­£çš„ä¸­æ–‡æ•°æ®é›†ï¼Œåä¸ºChFTï¼Œè¯¥æ•°æ®é›†é‡‡ç”¨äº†åŒ…æ‹¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆã€ASRå’Œé”™è¯¯æ ¡æ­£é…å¯¹æå–å™¨åœ¨å†…çš„ç®¡é“ã€‚è¯¥æ•°æ®é›†ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çº æ­£é”™è¯¯ï¼ŒåŒ…æ‹¬å…¨æ–‡å’Œç‰‡æ®µï¼Œå¹¶å¤„ç†æ›´å¹¿æ³›çš„é”™è¯¯ç±»å‹ï¼Œå¦‚æ ‡ç‚¹æ¢å¤å’Œé€†å‘æ–‡æœ¬è§„èŒƒåŒ–ï¼Œä»è€Œä½¿çº æ­£è¿‡ç¨‹æ›´åŠ å…¨é¢ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åœ¨æ„å»ºçš„æ•°æ®é›†ä¸Šå¯¹é¢„è®­ç»ƒçš„LLMè¿›è¡Œäº†å¾®è°ƒï¼Œä½¿ç”¨äº†å¤šç§æç¤ºå’Œç›®æ ‡æ ¼å¼æ¥è¯„ä¼°å…¶åœ¨å…¨æ–‡é”™è¯¯çº æ­£æ–¹é¢çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åŸºäºå…¨æ–‡å’Œç‰‡æ®µè®¾è®¡äº†æç¤ºï¼Œå¹¶è€ƒè™‘äº†å„ç§è¾“å‡ºæ ¼å¼ï¼Œå¦‚ç›´æ¥ä¿®æ­£çš„æ–‡æœ¬å’ŒåŸºäºJSONçš„é”™è¯¯æ ¡æ­£å¯¹ã€‚é€šè¿‡åŒ…æ‹¬åŒè´¨çš„ã€æœ€æ–°çš„å’Œå›°éš¾çš„æµ‹è¯•é›†åœ¨å†…çš„å„ç§æµ‹è¯•è®¾ç½®ï¼Œæˆ‘ä»¬å‘ç°ç»è¿‡å¾®è°ƒLLMåœ¨å…¨æ–‡è®¾ç½®ä¸­çš„è¡¨ç°è‰¯å¥½ï¼Œä¸åŒçš„æç¤ºå„æœ‰å…¶ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚è¿™ä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶å¥ å®šäº†æœ‰å¸Œæœ›çš„åŸºå‡†ã€‚æ•°æ®é›†å¯åœ¨ç½‘ç«™ä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07790v2">PDF</a> ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„é”™è¯¯æ ¡æ­£æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶ä¾§é‡äºçŸ­æœŸè¯­éŸ³å½•åˆ¶çš„å‘éŸ³ï¼Œè¿™æ˜¯ç›‘ç£ASRè®­ç»ƒçš„ä¸»è¦å½¢å¼ã€‚æœ¬æ–‡ç ”ç©¶äº†LLMåœ¨ç”±ASRç³»ç»Ÿä»é•¿è¯­éŸ³å½•åˆ¶ä¸­ç”Ÿæˆçš„å…¨æ–‡é”™è¯¯æ ¡æ­£æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¦‚Podcastã€æ–°é—»å¹¿æ’­å’Œä¼šè®®çš„è®°å½•ç¨¿ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç”¨äºå…¨æ–‡é”™è¯¯æ ¡æ­£çš„ä¸­æ–‡æ•°æ®é›†ChFTï¼Œè¯¥æ•°æ®é›†é€šè¿‡æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆã€ASRå’Œé”™è¯¯æ ¡æ­£å¯¹æå–å™¨æ„æˆçš„ç®¡é“å®ç°ã€‚è¯¥æ•°æ®é›†ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å„ç§æƒ…å¢ƒä¸‹çº æ­£é”™è¯¯ï¼ŒåŒ…æ‹¬å…¨æ–‡å’Œæ®µè½ï¼Œå¹¶å¤„ç†æ›´å¹¿æ³›çš„é”™è¯¯ç±»å‹ï¼Œå¦‚æ ‡ç‚¹æ¢å¤å’Œé€†æ–‡æœ¬è§„èŒƒåŒ–ï¼Œä»è€Œä½¿æ ¡æ­£è¿‡ç¨‹æ›´åŠ å…¨é¢ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åœ¨æ„å»ºçš„æ•°æ®é›†ä¸Šå¾®è°ƒäº†é¢„è®­ç»ƒçš„LLMï¼Œä½¿ç”¨å„ç§æç¤ºå’Œç›®æ ‡æ ¼å¼å¯¹å…¶æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬è®¾è®¡äº†åŸºäºå…¨æ–‡å’Œæ®µè½çš„æç¤ºï¼Œå¹¶è€ƒè™‘äº†å„ç§è¾“å‡ºæ ¼å¼ï¼Œå¦‚ç›´æ¥ä¿®æ­£æ–‡æœ¬å’ŒåŸºäºJSONçš„é”™è¯¯æ ¡æ­£å¯¹ã€‚é€šè¿‡å„ç§æµ‹è¯•è®¾ç½®ï¼ŒåŒ…æ‹¬åŒè´¨çš„ã€æœ€æ–°çš„å’Œå›°éš¾çš„æµ‹è¯•é›†ï¼Œæˆ‘ä»¬å‘ç°å¾®è°ƒåçš„LLMåœ¨å…¨æ–‡è®¾ç½®ä¸­çš„è¡¨ç°è‰¯å¥½ï¼Œæ¯ç§æç¤ºéƒ½æœ‰å…¶è‡ªèº«çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚è¿™ä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†æœ‰å¸Œæœ›çš„åŸºå‡†ã€‚æ•°æ®é›†å¯åœ¨ç½‘ç«™ä¸Šè·å¾—ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„é”™è¯¯æ ¡æ­£æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨çŸ­æœŸè¯­éŸ³å½•åˆ¶çš„å‘éŸ³ï¼Œä½†æœ¬æ–‡æ¢ç´¢äº†LLMåœ¨ç”±é•¿è¯­éŸ³å½•åˆ¶ç”Ÿæˆçš„å…¨æ–‡é”™è¯¯æ ¡æ­£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ä¸­æ–‡å…¨æ–‡é”™è¯¯æ ¡æ­£æ•°æ®é›†ChFTï¼Œç”¨äºæ›´å…¨é¢çš„é”™è¯¯æ ¡æ­£ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ä¸­çš„æ ‡ç‚¹æ¢å¤å’Œé€†æ–‡æœ¬è§„èŒƒåŒ–ç­‰é”™è¯¯ç±»å‹ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„LLMå¹¶åœ¨å¤šç§æµ‹è¯•è®¾ç½®ä¸‹è¯„ä¼°å…¶æ€§èƒ½ï¼Œå‘ç°LLMåœ¨å…¨æ–‡è®¾ç½®ä¸­çš„è¡¨ç°è‰¯å¥½ï¼Œä¸åŒæç¤ºå„æœ‰ä¼˜åŠ¿ä¸åŠ£åŠ¿ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„åŸºå‡†ã€‚</li>
<li>æ‰€æ„å»ºçš„æ•°æ®é›†ChFTå·²å…¬å¼€å‘å¸ƒï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07790">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2b65f4d824e1db9858fb68cb7a985861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26be500de3a31e2fa5336219bb82d568.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d53bac2df374ec0bdb946ce3369c3d6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6425d0d71f48f50bba28352e7a21696e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46660c58a4177a91d3aa3f203cdd12d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db5b0db0a3844add6c2b0a17f8e97838.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DCIM-AVSR-Efficient-Audio-Visual-Speech-Recognition-via-Dual-Conformer-Interaction-Module"><a href="#DCIM-AVSR-Efficient-Audio-Visual-Speech-Recognition-via-Dual-Conformer-Interaction-Module" class="headerlink" title="DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer   Interaction Module"></a>DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer   Interaction Module</h2><p><strong>Authors:Xinyu Wang, Haotian Jiang, Haolin Huang, Yu Fang, Mengjie Xua nd Qian Wang</strong></p>
<p>Speech recognition is the technology that enables machines to interpret and process human speech, converting spoken language into text or commands. This technology is essential for applications such as virtual assistants, transcription services, and communication tools. The Audio-Visual Speech Recognition (AVSR) model enhances traditional speech recognition, particularly in noisy environments, by incorporating visual modalities like lip movements and facial expressions. While traditional AVSR models trained on large-scale datasets with numerous parameters can achieve remarkable accuracy, often surpassing human performance, they also come with high training costs and deployment challenges. To address these issues, we introduce an efficient AVSR model that reduces the number of parameters through the integration of a Dual Conformer Interaction Module (DCIM). In addition, we propose a pre-training method that further optimizes model performance by selectively updating parameters, leading to significant improvements in efficiency. Unlike conventional models that require the system to independently learn the hierarchical relationship between audio and visual modalities, our approach incorporates this distinction directly into the model architecture. This design enhances both efficiency and performance, resulting in a more practical and effective solution for AVSR tasks. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿè§£é‡Šå’Œå¤„ç†äººç±»è¯­éŸ³çš„æŠ€æœ¯ï¼Œå®ƒå°†å£è¯­è½¬åŒ–ä¸ºæ–‡æœ¬æˆ–å‘½ä»¤ã€‚å¯¹äºè™šæ‹ŸåŠ©æ‰‹ã€è½¬å½•æœåŠ¡å’Œé€šä¿¡å·¥å…·ç­‰åº”ç”¨ç¨‹åºè€Œè¨€ï¼Œè¿™é¡¹æŠ€æœ¯è‡³å…³é‡è¦ã€‚è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ¨¡å‹é€šè¿‡èå…¥è¯¸å¦‚å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨è¡¨æƒ…ç­‰è§†è§‰æ¨¡å¼ï¼Œå¢å¼ºäº†ä¼ ç»Ÿè¯­éŸ³è¯†åˆ«ï¼Œç‰¹åˆ«æ˜¯åœ¨å˜ˆæ‚çš„ç¯å¢ƒä¸­ã€‚è™½ç„¶ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒçš„ä¼ ç»ŸAVSRæ¨¡å‹å…·æœ‰è®¸å¤šå‚æ•°ï¼Œå¯ä»¥å®ç°æƒŠäººçš„å‡†ç¡®æ€§ï¼Œæœ‰æ—¶ç”šè‡³è¶…è¿‡äººç±»çš„è¡¨ç°ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥äº†é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬å’Œéƒ¨ç½²æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„AVSRæ¨¡å‹ï¼Œé€šè¿‡é›†æˆåŒå·ç§¯äº¤äº’æ¨¡å—ï¼ˆDCIMï¼‰å‡å°‘äº†å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§æ›´æ–°å‚æ•°è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œä»è€Œåœ¨æ•ˆç‡ä¸Šå–å¾—äº†æ˜¾ç€æé«˜ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦ç³»ç»Ÿç‹¬ç«‹å­¦ä¹ éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€ä¹‹é—´å±‚æ¬¡å…³ç³»çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥å°†è¿™ç§åŒºåˆ«çº³å…¥æ¨¡å‹æ¶æ„ä¸­ã€‚è¿™ç§è®¾è®¡æé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ï¼Œä¸ºAVSRä»»åŠ¡æä¾›äº†æ›´å®ç”¨ã€æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00481v4">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>æ€»ç»“</strong><br>è¯­éŸ³è¯†åˆ«æŠ€æœ¯è®©æœºå™¨èƒ½å¤Ÿè§£è¯»å’Œå¤„ç†äººç±»è¯­éŸ³ï¼Œå°†å£è¯­è½¬åŒ–ä¸ºæ–‡å­—æˆ–å‘½ä»¤ã€‚å¯¹äºè™šæ‹ŸåŠ©ç†ã€è½¬å½•æœåŠ¡å’Œé€šä¿¡å·¥å…·ç­‰åº”ç”¨ï¼Œè¿™é¡¹æŠ€æœ¯è‡³å…³é‡è¦ã€‚éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ¨¡å‹é€šè¿‡åœ¨è§†è§‰æ¨¡å¼ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨è¡¨æƒ…ï¼‰çš„èå…¥ï¼Œå¢å¼ºäº†ä¼ ç»Ÿè¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å˜ˆæ‚ç¯å¢ƒä¸­ã€‚è™½ç„¶ä¼ ç»Ÿçš„å¤§å‹æ•°æ®é›†è®­ç»ƒAVSRæ¨¡å‹æœ‰è®¸å¤šå‚æ•°ï¼Œå¯ä»¥å®ç°æƒŠäººçš„å‡†ç¡®æ€§ï¼Œæœ‰æ—¶ç”šè‡³è¶…è¶Šäººç±»è¡¨ç°æ°´å¹³ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥äº†é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬å’Œéƒ¨ç½²æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é«˜æ•ˆçš„AVSRæ¨¡å‹ï¼Œé€šè¿‡é›†æˆåŒå˜å‹å™¨äº¤äº’æ¨¡å—ï¼ˆDCIMï¼‰å‡å°‘äº†å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é€‰æ‹©æ€§æ›´æ–°å‚æ•°çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œåœ¨æ•ˆç‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¸åŒäºä¼ ç»Ÿæ¨¡å‹éœ€è¦ç³»ç»Ÿç‹¬ç«‹å­¦ä¹ éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥å°†è¿™ç§åŒºåˆ«çº³å…¥æ¨¡å‹æ¶æ„ä¸­ã€‚è¿™ç§è®¾è®¡æé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ï¼Œä¸ºAVSRä»»åŠ¡æä¾›äº†æ›´å®ç”¨ã€æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³è¯†åˆ«æŠ€æœ¯èƒ½å°†äººç±»è¯­éŸ³è½¬åŒ–ä¸ºæ–‡å­—æˆ–å‘½ä»¤ï¼Œå¹¿æ³›åº”ç”¨äºè™šæ‹ŸåŠ©ç†ã€è½¬å½•æœåŠ¡å’Œé€šä¿¡å·¥å…·ç­‰é¢†åŸŸã€‚</li>
<li>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ¨¡å‹åœ¨å˜ˆæ‚ç¯å¢ƒä¸­è¡¨ç°æ›´ä¼˜ï¼Œé€šè¿‡èå…¥è§†è§‰æ¨¡å¼ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨è¡¨æƒ…ï¼‰æå‡è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»ŸAVSRæ¨¡å‹è™½ç„¶å‡†ç¡®åº¦é«˜ï¼Œä½†å­˜åœ¨è®­ç»ƒæˆæœ¬é«˜å’Œéƒ¨ç½²æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥é«˜æ•ˆAVSRæ¨¡å‹ï¼Œé€šè¿‡é›†æˆåŒå˜å‹å™¨äº¤äº’æ¨¡å—ï¼ˆDCIMï¼‰å‡å°‘å‚æ•°æ•°é‡ï¼Œæé«˜æ•ˆç‡ã€‚</li>
<li>æå‡ºé¢„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§æ›´æ–°å‚æ•°ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ¨¡å‹ä¸åŒï¼Œæ–°æ–¹æ³•å°†éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„åŒºåˆ«ç›´æ¥çº³å…¥æ¨¡å‹æ¶æ„ä¸­ï¼Œå¢å¼ºäº†æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>è¿™ç§è®¾è®¡æä¾›äº†æ›´å®ç”¨ã€æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºAVSRä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2adb1bf4996b1c649e625733571b7840.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c61bd461280aaa599e4e0945408115aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b83c3536ec993295c4b5cf18813237.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f51746486c27928c98b9d2520d32fd6b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Beyond-Silent-Letters-Amplifying-LLMs-in-Emotion-Recognition-with-Vocal-Nuances"><a href="#Beyond-Silent-Letters-Amplifying-LLMs-in-Emotion-Recognition-with-Vocal-Nuances" class="headerlink" title="Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal   Nuances"></a>Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal   Nuances</h2><p><strong>Authors:Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan Donbekci, Julia Hirschberg</strong></p>
<p>Emotion recognition in speech is a challenging multimodal task that requires understanding both verbal content and vocal nuances. This paper introduces a novel approach to emotion detection using Large Language Models (LLMs), which have demonstrated exceptional capabilities in natural language understanding. To overcome the inherent limitation of LLMs in processing audio inputs, we propose SpeechCueLLM, a method that translates speech characteristics into natural language descriptions, allowing LLMs to perform multimodal emotion analysis via text prompts without any architectural changes. Our method is minimal yet impactful, outperforming baseline models that require structural modifications. We evaluate SpeechCueLLM on two datasets: IEMOCAP and MELD, showing significant improvements in emotion recognition accuracy, particularly for high-quality audio data. We also explore the effectiveness of various feature representations and fine-tuning strategies for different LLMs. Our experiments demonstrate that incorporating speech descriptions yields a more than 2% increase in the average weighted F1 score on IEMOCAP (from 70.111% to 72.596%). </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡å¼ä»»åŠ¡ï¼Œéœ€è¦ç†è§£å£å¤´å†…å®¹å’Œè¯­éŸ³ç»†å¾®å·®åˆ«ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæƒ…æ„Ÿæ£€æµ‹çš„æ–°æ–¹æ³•ï¼ŒLLMåœ¨è‡ªç„¶è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœLLMåœ¨å¤„ç†éŸ³é¢‘è¾“å…¥æ–¹é¢çš„å›ºæœ‰å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SpeechCueLLMæ–¹æ³•ï¼Œå°†è¯­éŸ³ç‰¹å¾è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œå…è®¸LLMé€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œå¤šæ¨¡å¼æƒ…æ„Ÿåˆ†æï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•ç»“æ„æ›´æ”¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€å•è€Œæœ‰æ•ˆï¼Œè¶…è¶Šäº†éœ€è¦ç»“æ„ä¿®æ”¹çš„åŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨IEMOCAPå’ŒMELDä¸¤ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†SpeechCueLLMï¼Œåœ¨æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜è´¨é‡éŸ³é¢‘æ•°æ®ä¸Šã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†é’ˆå¯¹ä¸åŒLLMçš„å„ç§ç‰¹å¾è¡¨ç¤ºå’Œå¾®è°ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œèå…¥è¯­éŸ³æè¿°åï¼ŒIEMOCAPä¸Šçš„å¹³å‡åŠ æƒF1åˆ†æ•°æé«˜äº†2%ä»¥ä¸Šï¼ˆä»70.111%æé«˜åˆ°72.596%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21315v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæƒ…ç»ªæ£€æµ‹çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹LLMså¤„ç†éŸ³é¢‘è¾“å…¥æ—¶çš„å›ºæœ‰å±€é™æ€§ï¼Œæå‡ºSpeechCueLLMæ–¹æ³•ï¼Œå°†è¯­éŸ³ç‰¹å¾è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œä½¿LLMsèƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œå¤šæ¨¡å¼æƒ…ç»ªåˆ†æï¼Œæ— éœ€è¿›è¡Œä»»ä½•æ¶æ„æ›´æ”¹ã€‚è¯¥æ–¹æ³•åœ¨IEMOCAPå’ŒMELDä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾è‘—æé«˜äº†æƒ…ç»ªè¯†åˆ«å‡†ç¡®ç‡ï¼Œå°¤å…¶æ˜¯å¯¹é«˜è´¨é‡éŸ³é¢‘æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œç»“åˆè¯­éŸ³æè¿°ä½¿IEMOCAPçš„åŠ æƒå¹³å‡F1åˆ†æ•°æé«˜äº†2%ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæƒ…ç»ªè¯†åˆ«çš„æ–°æ–¹æ³•ã€‚</li>
<li>æå‡ºSpeechCueLLMæ–¹æ³•ï¼Œå°†è¯­éŸ³ç‰¹å¾è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œä»¥å…‹æœLLMså¤„ç†éŸ³é¢‘çš„å±€é™æ€§ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬æç¤ºï¼ŒLLMså¯è¿›è¡Œå¤šæ¨¡å¼æƒ…ç»ªåˆ†æï¼Œæ— éœ€æ›´æ”¹æ¶æ„ã€‚</li>
<li>åœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºè¾ƒé«˜çš„æƒ…ç»ªè¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
<li>é«˜è´¨é‡éŸ³é¢‘æ•°æ®çš„æƒ…ç»ªè¯†åˆ«æ•ˆæœå°¤ä¸ºæ˜¾è‘—ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç»“åˆè¯­éŸ³æè¿°å¯ä»¥æé«˜æƒ…ç»ªè¯†åˆ«çš„F1åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.21315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdca217a59f685272fbcf9e04a8e5e2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c4a568ab7f3db4e5559d5cb4d5aa0f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2076776f0626831987445e5658872e03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3942a9d4c303da930e68025ea9ed54f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Dynamic-Language-Group-Based-MoE-Enhancing-Code-Switching-Speech-Recognition-with-Hierarchical-Routing"><a href="#Dynamic-Language-Group-Based-MoE-Enhancing-Code-Switching-Speech-Recognition-with-Hierarchical-Routing" class="headerlink" title="Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech   Recognition with Hierarchical Routing"></a>Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech   Recognition with Hierarchical Routing</h2><p><strong>Authors:Hukai Huang, Shenghui Lu, Yahui Shan, He Qu, Fengrun Zhang, Wenhao Guan, Qingyang Hong, Lin Li</strong></p>
<p>The Mixture of Experts (MoE) model is a promising approach for handling code-switching speech recognition (CS-ASR) tasks. However, the existing CS-ASR work on MoE has yet to leverage the advantages of MoEâ€™s parameter scaling ability fully. This work proposes DLG-MoE, a Dynamic Language Group-based MoE, which can effectively handle the CS-ASR task and leverage the advantages of parameter scaling. DLG-MoE operates based on a hierarchical routing mechanism. First, the language router explicitly models the language attribute and dispatches the representations to the corresponding language expert groups. Subsequently, the unsupervised router within each language group implicitly models attributes beyond language and coordinates expert routing and collaboration. DLG-MoE outperforms the existing MoE methods on CS-ASR tasks while demonstrating great flexibility. It supports different top-$k$ inference and streaming capabilities and can also prune the model parameters flexibly to obtain a monolingual sub-model. The code has been released. </p>
<blockquote>
<p>ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¨¡å‹åœ¨å¤„ç†ä»£ç åˆ‡æ¢è¯­éŸ³è¯†åˆ«ï¼ˆCS-ASRï¼‰ä»»åŠ¡æ—¶å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…³äºMoEçš„CS-ASRç ”ç©¶å°šæœªå……åˆ†åˆ©ç”¨MoEçš„å‚æ•°ç¼©æ”¾èƒ½åŠ›ä¼˜åŠ¿ã€‚æœ¬æ–‡æå‡ºåŸºäºåŠ¨æ€è¯­è¨€åˆ†ç»„çš„DLG-MoEï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†CS-ASRä»»åŠ¡å¹¶å……åˆ†åˆ©ç”¨å‚æ•°ç¼©æ”¾çš„ä¼˜ç‚¹ã€‚DLG-MoEåŸºäºåˆ†å±‚è·¯ç”±æœºåˆ¶è¿è¡Œã€‚é¦–å…ˆï¼Œè¯­è¨€è·¯ç”±å™¨æ˜¾å¼åœ°å»ºæ¨¡è¯­è¨€å±æ€§å¹¶å°†è¡¨ç¤ºåˆ†æ´¾åˆ°ç›¸åº”çš„è¯­è¨€ä¸“å®¶ç»„ã€‚æ¥ç€ï¼Œæ¯ä¸ªè¯­è¨€ç»„å†…çš„æ— ç›‘ç£è·¯ç”±å™¨éšå¼åœ°å»ºæ¨¡è¯­è¨€ä»¥å¤–çš„å±æ€§ï¼Œå¹¶åè°ƒä¸“å®¶è·¯ç”±å’Œåä½œã€‚åœ¨CS-ASRä»»åŠ¡ä¸Šï¼ŒDLG-MoEçš„è¡¨ç°ä¼˜äºç°æœ‰çš„MoEæ–¹æ³•ï¼Œå±•ç°å‡ºæå¤§çš„çµæ´»æ€§ã€‚å®ƒæ”¯æŒä¸åŒçš„top-kæ¨ç†å’Œæµå¼ä¼ è¾“åŠŸèƒ½ï¼Œè¿˜å¯ä»¥çµæ´»åœ°ä¿®å‰ªæ¨¡å‹å‚æ•°ä»¥è·å¾—å•è¯­å­æ¨¡å‹ã€‚ä»£ç å·²ç»å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18581v4">PDF</a> Accepted by ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŠ¨æ€è¯­è¨€åˆ†ç»„çš„MoEæ¨¡å‹ï¼ˆDLG-MoEï¼‰åœ¨ä»£ç åˆ‡æ¢è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹åˆ©ç”¨åˆ†å±‚è·¯ç”±æœºåˆ¶ï¼Œé€šè¿‡è¯­è¨€è·¯ç”±å™¨æ˜ç¡®å»ºæ¨¡è¯­è¨€å±æ€§å¹¶è°ƒåº¦è¡¨ç¤ºåˆ°å¯¹åº”çš„è¯­è¨€ä¸“å®¶ç»„ï¼ŒåŒæ—¶å„ç»„å†…çš„æ— ç›‘ç£è·¯ç”±å™¨éšå¼å»ºæ¨¡è¶…è¶Šè¯­è¨€çš„å±æ€§å¹¶åè°ƒä¸“å®¶è·¯ç”±ä¸åä½œã€‚ç›¸è¾ƒäºç°æœ‰MoEæ–¹æ³•ï¼ŒDLG-MoEæ›´å…·çµæ´»æ€§ï¼Œæ”¯æŒä¸åŒçš„top-kæ¨ç†å’Œæµå¼å¤„ç†ï¼Œå¹¶èƒ½çµæ´»è°ƒæ•´æ¨¡å‹å‚æ•°ä»¥è·å–å•è¯­ç§å­æ¨¡å‹ã€‚ä»£ç å·²å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€è¯­è¨€åˆ†ç»„çš„MoEæ¨¡å‹ï¼ˆDLG-MoEï¼‰èƒ½æœ‰æ•ˆå¤„ç†ä»£ç åˆ‡æ¢è¯­éŸ³è¯†åˆ«ï¼ˆCS-ASRï¼‰ä»»åŠ¡ã€‚</li>
<li>DLG-MoEé€šè¿‡åˆ†å±‚è·¯ç”±æœºåˆ¶å®ç°è¯­è¨€å±æ€§çš„æ˜ç¡®å»ºæ¨¡å’Œè¡¨ç¤ºè°ƒåº¦ã€‚</li>
<li>è¯­è¨€è·¯ç”±å™¨å’Œæ— ç›‘ç£è·¯ç”±å™¨ååŒå·¥ä½œï¼Œåˆ†åˆ«è´Ÿè´£è¯­è¨€å±æ€§å’Œè¶…è¶Šè¯­è¨€çš„å±æ€§å»ºæ¨¡ã€‚</li>
<li>DLG-MoEç›¸è¾ƒäºç°æœ‰MoEæ–¹æ³•æ›´å…·çµæ´»æ€§ï¼Œæ”¯æŒå¤šç§æ¨ç†æ¨¡å¼å’Œå‚æ•°è°ƒæ•´ã€‚</li>
<li>DLG-MoEæ¨¡å‹å¯ä»¥ç”Ÿæˆå•è¯­ç§å­æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†å…¶åº”ç”¨åœºæ™¯ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²ç»å‘å¸ƒï¼Œä¾¿äºç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c888a89e24a18dde5a49f50cc080f161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba4f4a7c643702ed56d461826685a5c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eacfa030164e760f4f79259c7b2e0ee2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19bb7032f4c17180af10f7a60564ee6f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PI-Whisper-Designing-an-Adaptive-and-Incremental-Automatic-Speech-Recognition-System-for-Edge-Devices"><a href="#PI-Whisper-Designing-an-Adaptive-and-Incremental-Automatic-Speech-Recognition-System-for-Edge-Devices" class="headerlink" title="PI-Whisper: Designing an Adaptive and Incremental Automatic Speech   Recognition System for Edge Devices"></a>PI-Whisper: Designing an Adaptive and Incremental Automatic Speech   Recognition System for Edge Devices</h2><p><strong>Authors:Amir Nassereldine, Dancheng Liu, Chenhui Xu, Ruiyang Qin, Yiyu Shi, Jinjun Xiong</strong></p>
<p>Edge-based automatic speech recognition (ASR) technologies are increasingly prevalent in the development of intelligent and personalized assistants. However, resource-constrained ASR models face significant challenges in adaptivity, incrementality, and inclusivity when faced with a diverse population. To tackle those challenges, we propose PI-Whisper, a novel ASR system that adaptively enhances recognition capabilities by identifying speakersâ€™ characteristics in real-time. In this work, we show how the design of PI-Whisper allows for incremental adaptation of new characteristics without the need for repetitive retraining, enhances recognition capabilities, and improves equity and fairness across diverse speaker groups. PI-Whisper demonstrates these advantages by achieving state-of-the-art accuracy, reducing the word error rate (WER) by up to 13.7% relative to baselines while scaling linearly to computing resources. </p>
<blockquote>
<p>åŸºäºè¾¹ç¼˜çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯åœ¨æ™ºèƒ½å’Œä¸ªæ€§åŒ–åŠ©ç†çš„å¼€å‘ä¸­è¶Šæ¥è¶Šæ™®éã€‚ç„¶è€Œï¼Œèµ„æºæœ‰é™çš„ASRæ¨¡å‹åœ¨é¢å¯¹å¤šæ ·åŒ–äººç¾¤æ—¶ï¼Œåœ¨é€‚åº”æ€§ã€å¢é‡æ€§å’ŒåŒ…å®¹æ€§æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PI-Whisperï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ASRç³»ç»Ÿï¼Œé€šè¿‡å®æ—¶è¯†åˆ«è¯´è¯äººçš„ç‰¹ç‚¹ï¼Œè‡ªé€‚åº”åœ°å¢å¼ºè¯†åˆ«èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†PI-Whisperçš„è®¾è®¡å¦‚ä½•å…è®¸å¯¹æ–°ç‰¹æ€§è¿›è¡Œå¢é‡é€‚åº”ï¼Œæ— éœ€é‡å¤å†è®­ç»ƒï¼Œå¢å¼ºè¯†åˆ«èƒ½åŠ›ï¼Œå¹¶æ”¹å–„ä¸åŒè¯´è¯äººç¾¤ä½“ä¹‹é—´çš„å…¬å¹³æ€§å’Œå…¬æ­£æ€§ã€‚PI-Whisperé€šè¿‡å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œç›¸å¯¹äºåŸºå‡†çº¿é™ä½äº†é«˜è¾¾1.7%çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œå¹¶ä¸”éšç€è®¡ç®—èµ„æºçš„å¢åŠ è€Œçº¿æ€§æ‰©å±•ï¼Œä»è€Œè¯æ˜äº†è¿™äº›ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15668v2">PDF</a> in submission</p>
<p><strong>Summary</strong></p>
<p>è¾¹ç¼˜è®¡ç®—åŸºç¡€ä¸Šçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯åœ¨æ™ºèƒ½ä¸ªæ€§åŒ–åŠ©ç†çš„å¼€å‘ä¸­è¶Šæ¥è¶Šæ™®éã€‚ç„¶è€Œï¼Œèµ„æºæœ‰é™çš„ASRæ¨¡å‹åœ¨é€‚åº”å¤šæ ·åŒ–äººç¾¤æ—¶é¢ä¸´ç€é€‚åº”æ€§ã€å¢é‡æ€§å’ŒåŒ…å®¹æ€§çš„é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PI-Whisperè¿™ä¸€æ–°å‹ASRç³»ç»Ÿï¼Œå®ƒå¯ä»¥é€šè¿‡å®æ—¶è¯†åˆ«è¯´è¯äººçš„ç‰¹ç‚¹æ¥åŠ¨æ€æå‡è¯†åˆ«èƒ½åŠ›ã€‚æœ¬å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†PI-Whisperçš„è®¾è®¡å¯ä»¥æ”¯æŒå¯¹æ–°ç‰¹æ€§çš„å¢é‡é€‚åº”ï¼Œæ— éœ€é‡å¤è®­ç»ƒï¼Œæé«˜è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶æ”¹å–„å¯¹ä¸åŒè¯´è¯ç¾¤ä½“çš„å…¬å¹³æ€§å’Œå…¬æ­£æ€§ã€‚PI-Whisperçš„è¿™äº›ä¼˜åŠ¿ä½¿å…¶æˆä¸ºä¸šç•Œé¢†å…ˆçš„æŠ€æœ¯ï¼Œç›¸å¯¹äºåŸºçº¿æŠ€æœ¯ï¼Œå…¶å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†é«˜è¾¾13.7%ï¼Œå¹¶ä¸”éšç€è®¡ç®—èµ„æºçš„å¢åŠ è€Œçº¿æ€§æ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PI-Whisperæ˜¯ä¸€ç§æ–°å‹çš„ASRç³»ç»Ÿï¼Œå¯å®æ—¶è¯†åˆ«è¯´è¯äººçš„ç‰¹ç‚¹å¹¶åŠ¨æ€æå‡è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>PI-Whisperè®¾è®¡æ”¯æŒå¯¹æ–°ç‰¹æ€§çš„å¢é‡é€‚åº”ï¼Œæ— éœ€é‡å¤è®­ç»ƒã€‚</li>
<li>PI-Whisperæé«˜äº†å¯¹ä¸åŒè¯´è¯ç¾¤ä½“çš„å…¬å¹³æ€§å’Œå…¬æ­£æ€§ã€‚</li>
<li>PI-Whisperå®ç°äº†ä¸šç•Œé¢†å…ˆçš„å‡†ç¡®æ€§ï¼Œå•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç›¸å¯¹äºåŸºçº¿æŠ€æœ¯é™ä½äº†é«˜è¾¾13.7%ã€‚</li>
<li>PI-Whisperç³»ç»Ÿéšç€è®¡ç®—èµ„æºçš„å¢åŠ è€Œçº¿æ€§æ‰©å±•ã€‚</li>
<li>è¯¥ç³»ç»Ÿä¸»è¦é€‚ç”¨äºèµ„æºæœ‰é™çš„ASRæ¨¡å‹åœ¨é¢ä¸´å¤šæ ·åŒ–äººç¾¤æ—¶çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13b3e2362004ead459bc46222cd08cc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-419aea759583e3976d723613023e6598.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-502e9dc8005f2d526dc3d91a18d62321.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dcde70132e329c7b29342c971de3258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8029ab6c84a2ae39663f86c36a34a560.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db2b04ba3b7c41c376bb9c9a48f6edbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac08a5e69bf2a7870007e0fc8255bad2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="HeadStudio-Text-to-Animatable-Head-Avatars-with-3D-Gaussian-Splatting"><a href="#HeadStudio-Text-to-Animatable-Head-Avatars-with-3D-Gaussian-Splatting" class="headerlink" title="HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting"></a>HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting</h2><p><strong>Authors:Zhenglin Zhou, Fan Ma, Hehe Fan, Zongxin Yang, Yi Yang</strong></p>
<p>Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising results achieved with 2D diffusion priors, current methods struggle to create high-quality and consistent animated avatars efficiently. Previous animatable head models like FLAME have difficulty in accurately representing detailed texture and geometry. Additionally, high-quality 3D static representations face challenges in semantically driving with dynamic priors. In this paper, we introduce \textbf{HeadStudio}, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animatable avatars from text prompts. Firstly, we associate 3D Gaussians with animatable head prior model, facilitating semantic animation on high-quality 3D representations. To ensure consistent animation, we further enhance the optimization from initialization, distillation, and regularization to jointly learn the shape, texture, and animation. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting appealing appearances. The avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel views at a resolution of 1024. Moreover, These avatars can be smoothly driven by real-world speech and video. We hope that HeadStudio can enhance digital avatar creation and gain popularity in the community. Code is at: <a target="_blank" rel="noopener" href="https://github.com/ZhenglinZhou/HeadStudio">https://github.com/ZhenglinZhou/HeadStudio</a>. </p>
<blockquote>
<p>ä»æ–‡æœ¬æç¤ºåˆ›å»ºæ•°å­—åŒ–èº«ä¸€ç›´æ˜¯ä¸€é¡¹ä»¤äººå‘å¾€ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡åˆ©ç”¨äºŒç»´æ‰©æ•£å…ˆéªŒå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨é«˜æ•ˆåˆ›å»ºé«˜è´¨é‡ä¸”è¿è´¯çš„åŠ¨ç”»åŒ–èº«æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚åƒFLAMEè¿™æ ·çš„ä¹‹å‰å¯åŠ¨ç”»å¤´éƒ¨æ¨¡å‹åœ¨å‡†ç¡®è¡¨ç¤ºè¯¦ç»†çº¹ç†å’Œå‡ ä½•ç»“æ„æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æ­¤å¤–ï¼Œé«˜è´¨é‡çš„ä¸‰ç»´é™æ€è¡¨ç¤ºåœ¨è¯­ä¹‰é©±åŠ¨çš„åŠ¨æ€å…ˆéªŒæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ³¼æ–‘æŠ€æœ¯ç”Ÿæˆä»æ–‡æœ¬æç¤ºå‡ºå‘çš„çœŸå®ä¸”å¯åŠ¨ç”»çš„åŒ–èº«çš„å…¨æ–°æ¡†æ¶â€”â€”HeadStudioã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒä¸å¯åŠ¨ç”»å¤´éƒ¨å…ˆéªŒæ¨¡å‹å…³è”èµ·æ¥ï¼Œå®ç°åœ¨é«˜è´¨é‡ä¸‰ç»´è¡¨ç¤ºä¸Šçš„è¯­ä¹‰åŠ¨ç”»ã€‚ä¸ºç¡®ä¿è¿è´¯çš„åŠ¨ç”»æ•ˆæœï¼Œæˆ‘ä»¬ä»åˆå§‹åŒ–ã€è’¸é¦å’Œæ­£åˆ™åŒ–ä¸‰ä¸ªæ–¹é¢è¿›ä¸€æ­¥ä¼˜åŒ–äº†å½¢çŠ¶ã€çº¹ç†å’ŒåŠ¨ç”»çš„è”åˆå­¦ä¹ ã€‚å¤§é‡å®éªŒè¯æ˜äº†HeadStudioä»æ–‡æœ¬æç¤ºç”Ÿæˆå¯åŠ¨ç”»åŒ–èº«çš„æœ‰æ•ˆæ€§ï¼Œå±•ç°å‡ºå¸å¼•äººçš„å¤–è§‚ã€‚è¿™äº›åŒ–èº«èƒ½å¤Ÿä»¥é«˜äºæˆ–ç­‰äº40å¸§æ¯ç§’çš„é€Ÿåº¦åœ¨1024çš„åˆ†è¾¨ç‡ä¸‹å‘ˆç°é«˜è´¨é‡å®æ—¶æ–°è§†è§’ã€‚æ­¤å¤–ï¼Œè¿™äº›åŒ–èº«å¯ä»¥è¢«ç°å®ä¸–ç•Œä¸­çš„è¯­éŸ³å’Œè§†é¢‘æµç•…é©±åŠ¨ã€‚æˆ‘ä»¬å¸Œæœ›HeadStudioèƒ½å¢å¼ºæ•°å­—åŒ–èº«çš„åˆ›å»ºåŠŸèƒ½å¹¶åœ¨ç¤¾åŒºä¸­å¹¿å—æ¬¢è¿ã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/ZhenglinZhou/HeadStudio%E3%80%82">https://github.com/ZhenglinZhou/HeadStudioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06149v2">PDF</a> 26 pages, 18 figures, accepted by ECCV 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHeadStudioçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨3Dé«˜æ–¯æ¶‚é¸¦æŠ€æœ¯ï¼Œä»æ–‡æœ¬æç¤ºç”ŸæˆçœŸå®ä¸”å¯åŠ¨ç”»çš„å¤´åƒã€‚HeadStudioé€šè¿‡ç»“åˆåŠ¨ç”»å¤´åƒå…ˆéªŒæ¨¡å‹ä¸3Dé«˜æ–¯åˆ†å¸ƒï¼Œå®ç°é«˜è´¨é‡3Dè¡¨ç¤ºä¸Šçš„è¯­ä¹‰åŠ¨ç”»ã€‚é€šè¿‡ä¼˜åŒ–åˆå§‹åŒ–ã€è’¸é¦å’Œæ­£åˆ™åŒ–è¿‡ç¨‹ï¼Œè”åˆå­¦ä¹ å½¢çŠ¶ã€çº¹ç†å’ŒåŠ¨ç”»ï¼Œç¡®ä¿åŠ¨ç”»çš„ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒHeadStudioèƒ½é«˜æ•ˆåœ°ä»æ–‡æœ¬æç¤ºç”Ÿæˆå¯åŠ¨ç”»çš„å¤´åƒï¼Œå‘ˆç°å¸å¼•äººçš„å¤–è§‚ï¼Œå¹¶å¯å®æ—¶ï¼ˆâ‰¥40å¸§&#x2F;ç§’ï¼‰æ¸²æŸ“é«˜è´¨é‡å¤´åƒæ–°è§†è§’ï¼Œä¸”å¤´åƒå¯è¢«çœŸå®è¯­éŸ³å’Œè§†é¢‘æµç•…é©±åŠ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HeadStudioæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå¯ä»æ–‡æœ¬æç¤ºç”ŸæˆçœŸå®ä¸”å¯åŠ¨ç”»çš„å¤´åƒã€‚</li>
<li>åˆ©ç”¨3Dé«˜æ–¯æ¶‚é¸¦æŠ€æœ¯å®ç°é«˜è´¨é‡å’Œä¸€è‡´çš„åŠ¨ç”»å¤´åƒç”Ÿæˆã€‚</li>
<li>ç»“åˆåŠ¨ç”»å¤´åƒå…ˆéªŒæ¨¡å‹ä¸3Dé«˜æ–¯åˆ†å¸ƒï¼Œå®ç°è¯­ä¹‰åŠ¨ç”»ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–åˆå§‹åŒ–ã€è’¸é¦å’Œæ­£åˆ™åŒ–è¿‡ç¨‹ï¼Œç¡®ä¿åŠ¨ç”»ä¸€è‡´æ€§ã€‚</li>
<li>èƒ½é«˜æ•ˆç”Ÿæˆå¸å¼•äººçš„å¤´åƒï¼Œå®æ—¶æ¸²æŸ“é«˜è´¨é‡æ–°è§†è§’ã€‚</li>
<li>ç”Ÿæˆçš„å¤´åƒèƒ½è¢«çœŸå®è¯­éŸ³å’Œè§†é¢‘æµç•…é©±åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.06149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-215d6480ae84da5896c28fbfbfba36d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70eb1a7b18cd38c2ce7113fe7c708209.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfa619e455be609c31826d56564bb1b5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6ad198e2b18880b953fd90d912db4343.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-26  Hardware-aware Circuit Cutting and Distributed Qubit Mapping for   Connected Quantum Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-05b76fb015fc2d99bc60a863c31ee42a.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-26  ErasableMask A Robust and Erasable Privacy Protection Scheme against   Black-box Face Recognition Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
