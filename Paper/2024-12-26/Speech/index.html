<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2024-12-26  Zero-resource Speech Translation and Recognition with LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-43ae7c12668f3d8fae80249694d410de.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    57 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-26-更新"><a href="#2024-12-26-更新" class="headerlink" title="2024-12-26 更新"></a>2024-12-26 更新</h1><h2 id="Zero-resource-Speech-Translation-and-Recognition-with-LLMs"><a href="#Zero-resource-Speech-Translation-and-Recognition-with-LLMs" class="headerlink" title="Zero-resource Speech Translation and Recognition with LLMs"></a>Zero-resource Speech Translation and Recognition with LLMs</h2><p><strong>Authors:Karel Mundnich, Xing Niu, Prashant Mathur, Srikanth Ronanki, Brady Houston, Veera Raghavendra Elluru, Nilaksh Das, Zejiang Hou, Goeric Huybrechts, Anshu Bhatia, Daniel Garcia-Romero, Kyu J. Han, Katrin Kirchhoff</strong></p>
<p>Despite recent advancements in speech processing, zero-resource speech translation (ST) and automatic speech recognition (ASR) remain challenging problems. In this work, we propose to leverage a multilingual Large Language Model (LLM) to perform ST and ASR in languages for which the model has never seen paired audio-text data. We achieve this by using a pre-trained multilingual speech encoder, a multilingual LLM, and a lightweight adaptation module that maps the audio representations to the token embedding space of the LLM. We perform several experiments both in ST and ASR to understand how to best train the model and what data has the most impact on performance in previously unseen languages. In ST, our best model is capable to achieve BLEU scores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we achieve WERs of up to 28.2%. We finally show that the performance of our system is bounded by the ability of the LLM to output text in the desired language. </p>
<blockquote>
<p>尽管最近语音处理领域有所进展，但零资源语音翻译（ST）和自动语音识别（ASR）仍是具有挑战性的问题。在这项工作中，我们提出利用多语言大型语言模型（LLM）来执行ST和ASR，针对模型从未接触过的配对语音文本数据的语言。我们通过使用预训练的多语言语音编码器、多语言LLM和一个轻量级适配模块（该模块将音频表示映射到LLM的令牌嵌入空间）来实现这一点。我们在ST和ASR中进行了多次实验，以了解如何最好地训练模型，以及哪些数据在对未见过的语言性能影响方面最为重要。在ST中，我们最好的模型能够在CoVoST2上实现超过23的BLEU分数，用于两种先前未见过的语言；而在ASR中，我们实现高达28.2%的WER。最后，我们表明，我们系统的性能受到LLM输出所需语言文本的能力的限制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18566v1">PDF</a> ICASSP 2025, 5 pages, 2 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>本研究探讨了零资源语音翻译（ST）和自动语音识别（ASR）的难题，并提出利用多语言大型语言模型（LLM）在从未接触过配对音频文本数据的语言中进行ST和ASR。通过预训练的多语言语音编码器、多语言LLM和轻量级适应模块，将语音表示映射到LLM的令牌嵌入空间。在ST和ASR方面进行了多次实验，以了解如何最佳地训练模型，以及哪些数据对在未见过的语言中的性能影响最大。在ST中，最佳模型的BLEU得分在CoVoST2上超过23，而在ASR中，我们实现了最高28.2%的WER。最后，表明系统性能受限于LLM输出所需语言文本的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零资源语音翻译（ST）和自动语音识别（ASR）仍是具有挑战性的问题。</li>
<li>研究提出利用多语言大型语言模型（LLM）在未见过的语言中进行ST和ASR。</li>
<li>通过预训练的多语言语音编码器、多语言LLM和轻量级适应模块实现语音到文本的转换。</li>
<li>在ST和ASR方面进行了多次实验，以优化模型训练并了解数据对性能的影响。</li>
<li>在ST方面，最佳模型的BLEU得分在CoVoST2数据集上超过23。</li>
<li>在ASR方面，实现了最高28.2%的字错误率（WER）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5b357107fcd6a2cde520d689ab7b9bd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43ae7c12668f3d8fae80249694d410de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad0e1040b8bc8cb5a6123cd4919ddfcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2381c4b0d90bf7f46d411b0fe79095ff.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Text-Aware-Adapter-for-Few-Shot-Keyword-Spotting"><a href="#Text-Aware-Adapter-for-Few-Shot-Keyword-Spotting" class="headerlink" title="Text-Aware Adapter for Few-Shot Keyword Spotting"></a>Text-Aware Adapter for Few-Shot Keyword Spotting</h2><p><strong>Authors:Youngmoon Jung, Jinyoung Lee, Seungjin Lee, Myunghun Jung, Yong-Hyeok Lee, Hoon-Young Cho</strong></p>
<p>Recent advances in flexible keyword spotting (KWS) with text enrollment allow users to personalize keywords without uttering them during enrollment. However, there is still room for improvement in target keyword performance. In this work, we propose a novel few-shot transfer learning method, called text-aware adapter (TA-adapter), designed to enhance a pre-trained flexible KWS model for specific keywords with limited speech samples. To adapt the acoustic encoder, we leverage a jointly pre-trained text encoder to generate a text embedding that acts as a representative vector for the keyword. By fine-tuning only a small portion of the network while keeping the core components’ weights intact, the TA-adapter proves highly efficient for few-shot KWS, enabling a seamless return to the original pre-trained model. In our experiments, the TA-adapter demonstrated significant performance improvements across 35 distinct keywords from the Google Speech Commands V2 dataset, with only a 0.14% increase in the total number of parameters. </p>
<blockquote>
<p>近期，文本注册制的灵活关键词识别（KWS）技术取得了进展，允许用户个性化关键词，而无需在注册时发出声音。然而，目标关键词的性能仍有提升空间。在此工作中，我们提出了一种新的少样本迁移学习方法，称为文本感知适配器（TA-adapter），旨在利用有限的语音样本，增强预训练的灵活KWS模型对特定关键词的识别。为了调整声学编码器，我们利用联合预训练的文本编码器生成文本嵌入，作为关键词的代表向量。通过仅微调网络的一部分，同时保持核心组件的权重不变，TA-adapter在少样本KWS中表现出高效率，可以无缝地回归原始预训练模型。在我们的实验中，TA-adapter在Google语音命令V2数据集中的35个不同关键词上显示出显著的性能提升，且总参数仅增加了0.14%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18142v1">PDF</a> 5 pages, 3 figures, Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的基于文本感知适配器（TA-adapter）的少量样本迁移学习方法，用于改进预训练的灵活关键词识别模型。通过利用联合预训练的文本编码器生成文本嵌入向量来代表关键词，该方法在微调网络小部分的同时保持核心组件权重不变，实现了高效的少量样本关键词识别。在Google Speech Commands V2数据集上的实验表明，TA-adapter在35个不同关键词上的性能显著提升，总参数仅增加0.14%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新型的基于文本感知适配器（TA-adapter）的迁移学习方法。</li>
<li>方法设计用于改进预训练的灵活关键词识别模型以识别特定关键词。</li>
<li>利用联合预训练的文本编码器生成文本嵌入向量来代表关键词。</li>
<li>通过微调小部分网络，同时保持核心组件权重不变，实现高效少量的样本关键词识别。</li>
<li>实验在Google Speech Commands V2数据集上进行，涉及35个不同关键词。</li>
<li>TA-adapter显著提高了关键词识别性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18142">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-902c662f563eb67621aed26782be8251.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f086b14f5ef0d8ada080df1021e2db42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4372921412aecd7d9f3ab737a5739767.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d10111658b6d5b0fb82bea8fc21f6591.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Neural-Directed-Speech-Enhancement-with-Dual-Microphone-Array-in-High-Noise-Scenario"><a href="#Neural-Directed-Speech-Enhancement-with-Dual-Microphone-Array-in-High-Noise-Scenario" class="headerlink" title="Neural Directed Speech Enhancement with Dual Microphone Array in High   Noise Scenario"></a>Neural Directed Speech Enhancement with Dual Microphone Array in High   Noise Scenario</h2><p><strong>Authors:Wen Wen, Qiang Zhou, Yu Xi, Haoyu Li, Ziqi Gong, Kai Yu</strong></p>
<p>In multi-speaker scenarios, leveraging spatial features is essential for enhancing target speech. While with limited microphone arrays, developing a compact multi-channel speech enhancement system remains challenging, especially in extremely low signal-to-noise ratio (SNR) conditions. To tackle this issue, we propose a triple-steering spatial selection method, a flexible framework that uses three steering vectors to guide enhancement and determine the enhancement range. Specifically, we introduce a causal-directed U-Net (CDUNet) model, which takes raw multi-channel speech and the desired enhancement width as inputs. This enables dynamic adjustment of steering vectors based on the target direction and fine-tuning of the enhancement region according to the angular separation between the target and interference signals. Our model with only a dual microphone array, excels in both speech quality and downstream task performance. It operates in real-time with minimal parameters, making it ideal for low-latency, on-device streaming applications. </p>
<blockquote>
<p>在多说话人场景中，利用空间特征增强目标语音至关重要。然而，在有限的麦克风阵列下，开发紧凑的多通道语音增强系统仍然具有挑战性，特别是在极低信噪比（SNR）条件下。为了解决这一问题，我们提出了一种三导向空间选择方法，这是一个灵活的框架，使用三个导向向量来指导增强并确定增强范围。具体来说，我们引入了因果导向U-Net（CDUNet）模型，该模型以原始多通道语音和所需的增强宽度为输入。这可以根据目标方向和干扰信号之间的角度间隔动态调整导向向量，并微调增强区域。我们的模型仅使用双麦克风阵列，在语音质量和下游任务性能方面都表现出色。它以实时方式运行，参数最少，非常适合低延迟的在线流式应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18141v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>在复杂的多说话人场景中，利用空间特征增强目标语音至关重要。针对有限麦克风阵列在极低信噪比条件下构建紧凑的多通道语音增强系统的挑战，我们提出了一种灵活的三向空间选择方法。该方法使用三个引导向量进行增强并确定增强范围。此外，我们引入了因果导向U-Net模型（CDUNet），能够根据目标方向和角度间隔对干扰信号进行动态调整，从而实现精细化增强。此模型仅用双麦克风阵列就表现出色，适用于实时、低延迟的在线流式应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在多说话人场景中，利用空间特征增强目标语音是必要的。</li>
<li>针对有限麦克风阵列和极低信噪比条件，构建多通道语音增强系统具有挑战性。</li>
<li>提出了一种灵活的三向空间选择方法，使用三个引导向量进行语音增强。</li>
<li>引入了因果导向U-Net（CDUNet）模型，能够根据目标方向和角度间隔动态调整增强效果。</li>
<li>该模型在仅有双麦克风阵列的情况下表现出色。</li>
<li>模型在语音质量和下游任务性能上均表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18141">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b9e4b40551aef40ff2add3310f36ddd2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86e4be31bad6d3e8241d5204a21f28fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc14985cb9d94204b247b4ac6b002fe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09e77a76cac8a1564b7aa6c5cf970e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00a9eb67b6d38e11e50710ea46033a23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fed10ba26e571a57a58449496edd1e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8089373fb35fd890c18a705caaa0c4b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Time-Graph-Frequency-Representation-with-Singular-Value-Decomposition-for-Neural-Speech-Enhancement"><a href="#Time-Graph-Frequency-Representation-with-Singular-Value-Decomposition-for-Neural-Speech-Enhancement" class="headerlink" title="Time-Graph Frequency Representation with Singular Value Decomposition   for Neural Speech Enhancement"></a>Time-Graph Frequency Representation with Singular Value Decomposition   for Neural Speech Enhancement</h2><p><strong>Authors:Tingting Wang, Tianrui Wang, Meng Ge, Qiquan Zhang, Zirui Ge, Zhen Yang</strong></p>
<p>Time-frequency (T-F) domain methods for monaural speech enhancement have benefited from the success of deep learning. Recently, focus has been put on designing two-stream network models to predict amplitude mask and phase separately, or, coupling the amplitude and phase into Cartesian coordinates and constructing real and imaginary pairs. However, most methods suffer from the alignment modeling of amplitude and phase (real and imaginary pairs) in a two-stream network framework, which inevitably incurs performance restrictions. In this paper, we introduce a graph Fourier transform defined with the singular value decomposition (GFT-SVD), resulting in real-valued time-graph representation for neural speech enhancement. This real-valued representation-based GFT-SVD provides an ability to align the modeling of amplitude and phase, leading to avoiding recovering the target speech phase information. Our findings demonstrate the effects of real-valued time-graph representation based on GFT-SVD for neutral speech enhancement. The extensive speech enhancement experiments establish that the combination of GFT-SVD and DNN outperforms the combination of GFT with the eigenvector decomposition (GFT-EVD) and magnitude estimation UNet, and outperforms the short-time Fourier transform (STFT) and DNN, regarding objective intelligibility and perceptual quality. We release our source code at: <a target="_blank" rel="noopener" href="https://github.com/Wangfighting0015/GFT/_project">https://github.com/Wangfighting0015/GFT\_project</a>. </p>
<blockquote>
<p>针对单声道语音增强的时频域方法受益于深度学习取得的成就。最近，人们专注于设计双流传络模型，以便分别预测振幅掩模和相位，或者将振幅和相位耦合到笛卡尔坐标系中，并构建实数和虚数对。然而，大多数方法在双流传络框架中遭受振幅和相位（实数和虚数对）的对齐建模困扰，这不可避免地会导致性能限制。在本文中，我们引入了一种利用奇异值分解定义的图傅里叶变换（GFT-SVD），从而得到用于神经语音增强的实值时间图表示。这种基于实值表示的GFT-SVD提供了对齐振幅和相位建模的能力，避免了恢复目标语音相位信息。我们的研究结果表明，基于GFT-SVD的实值时间图表示对于中性语音增强具有影响。广泛的语音增强实验证实，GFT-SVD与深度神经网络（DNN）的结合优于GFT与特征向量分解（GFT-EVD）的结合以及幅度估计UNet，并且在客观可理解性和感知质量方面优于短时傅里叶变换（STFT）和DNN。我们在以下链接发布了我们的源代码：<a target="_blank" rel="noopener" href="https://github.com/Wangfighting0015/GFT_project%E3%80%82">https://github.com/Wangfighting0015/GFT_project。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16823v2">PDF</a> 5 pages, 4 figures, Accepted by ICASSP2025</p>
<p><strong>摘要</strong></p>
<p>利用基于奇异值分解的图傅里叶变换（GFT-SVD）实现真实值时间图表示为神经语音增强提供了一个有效方法。通过对振幅和相位进行实值时间图表示，避免了目标语音相位信息的恢复。实验结果表明，基于实值时间图表示的GFT-SVD对于中性语音增强具有显著效果。在广泛的语音增强实验中，结合GFT-SVD和深度神经网络（DNN）的表现优于结合GFT与特征向量分解（GFT-EVD）以及幅度估计UNet，并且在客观可理解性和感知质量方面优于短时傅里叶变换（STFT）和DNN。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>引入基于奇异值分解的图傅里叶变换（GFT-SVD）实现真实值时间图表示，为神经语音增强提供新方法。</li>
<li>实值时间图表示能够实现对振幅和相位的对齐建模，避免恢复目标语音的相位信息。</li>
<li>GFT-SVD结合DNN的方法在语音增强实验中表现出优越性能。</li>
<li>GFT-SVD与STFT及DNN等方法相比，在客观可理解性和感知质量方面有更优表现。</li>
<li>本文提供的源代码已公开发布在GitHub上。</li>
<li>通过实值时间图表示方法提高了神经语音增强的性能。</li>
<li>研究展示了实值时间图表示在处理振幅和相位对齐建模问题中的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16823">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bdfaa15e3fad3d7f5270dbd003c22507.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffa49783ef187100a7c44402b5a13b30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43d52ce4c19efb35169d29f18f9fae5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d301f59d6408a6a46145b1b4ba9a69b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d5e6b6abebea4d5dd31b482a22ad596.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding"><a href="#Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding" class="headerlink" title="Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding"></a>Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding</h2><p><strong>Authors:Jiahui Zhao, Hao Shi, Chenrui Cui, Tianrui Wang, Hexin Liu, Zhaoheng Ni, Lingxuan Ye, Longbiao Wang</strong></p>
<p>Code-switching (CS) automatic speech recognition (ASR) faces challenges due to the language confusion resulting from accents, auditory similarity, and seamless language switches. Adaptation on the pre-trained multi-lingual model has shown promising performance for CS-ASR. In this paper, we adapt Whisper, which is a large-scale multilingual pre-trained speech recognition model, to CS from both encoder and decoder parts. First, we propose an encoder refiner to enhance the encoder’s capacity of intra-sentence swithching. Second, we propose using two sets of language-aware adapters with different language prompt embeddings to achieve language-specific decoding information in each decoder layer. Then, a fusion module is added to fuse the language-aware decoding. The experimental results using the SEAME dataset show that, compared with the baseline model, the proposed approach achieves a relative MER reduction of 4.1% and 7.2% on the dev_man and dev_sge test sets, respectively, surpassing state-of-the-art methods. Through experiments, we found that the proposed method significantly improves the performance on non-native language in CS speech, indicating that our approach enables Whisper to better distinguish between the two languages. </p>
<blockquote>
<p>代码切换（CS）自动语音识别（ASR）面临着由于口音、听觉相似性和无缝语言切换导致的语言混淆所带来的挑战。预训练的多元语言模型的适配已经显示出对CS-ASR的有前途的性能。在本文中，我们适配了whisper这样一个大规模的多语言预训练语音识别模型，以适应编码器和解码器部分的CS。首先，我们提出了一个编码器精炼器，以提高编码器在句子内切换的能力。其次，我们建议使用两组带有不同语言提示嵌入的语言感知适配器，以实现每个解码器层中的特定语言解码信息。然后，添加一个融合模块来融合语言感知解码。使用SEAME数据集的实验结果表明，与基线模型相比，所提出的方法在dev_man和dev_sge测试集上分别实现了相对MER降低4.1%和7.2%，超过了最先进的方法。通过实验，我们发现该方法在非本地语言的CS语音性能上有了显著提高，这表明我们的方法能使whisper更好地区分两种语言。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16507v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了在自动语音识别中，使用预训练的多语言模型进行代码切换的挑战。为了改进性能，对大型预训练语音模型Whisper进行了适应调整，涉及编码器精炼器和解码器部分的改进。通过采用融合模块，实现语言特定的解码信息。实验结果表明，与基线模型相比，所提出的方法在SEAME数据集上的平均错误率有所降低，特别是在非母语代码的切换语音方面表现显著。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码切换自动语音识别面临语言混淆的挑战，包括口音、听觉相似性和无缝语言切换。</li>
<li>预训练的多语言模型适应于代码切换自动语音识别显示出良好性能。</li>
<li>对编码器进行精炼以提高句子内部的切换能力。</li>
<li>采用两组语言感知适配器与不同的语言提示嵌入实现每层解码器的语言特定解码信息。</li>
<li>添加融合模块以融合语言感知解码。</li>
<li>实验结果表明，与基线模型相比，所提出的方法在测试集上的平均错误率有所降低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ad2bffb15c91a8bf319a4b37ecf3c835.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a30302356c17d7ddd2d5b0cd9dad213.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82c3763f9c6385f3d6ee3a285235e05f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Streaming-Keyword-Spotting-Boosted-by-Cross-layer-Discrimination-Consistency"><a href="#Streaming-Keyword-Spotting-Boosted-by-Cross-layer-Discrimination-Consistency" class="headerlink" title="Streaming Keyword Spotting Boosted by Cross-layer Discrimination   Consistency"></a>Streaming Keyword Spotting Boosted by Cross-layer Discrimination   Consistency</h2><p><strong>Authors:Yu Xi, Haoyu Li, Xiaoyu Gu, Hao Li, Yidi Jiang, Kai Yu</strong></p>
<p>Connectionist Temporal Classification (CTC), a non-autoregressive training criterion, is widely used in online keyword spotting (KWS). However, existing CTC-based KWS decoding strategies either rely on Automatic Speech Recognition (ASR), which performs suboptimally due to its broad search over the acoustic space without keyword-specific optimization, or on KWS-specific decoding graphs, which are complex to implement and maintain. In this work, we propose a streaming decoding algorithm enhanced by Cross-layer Discrimination Consistency (CDC), tailored for CTC-based KWS. Specifically, we introduce a streamlined yet effective decoding algorithm capable of detecting the start of the keyword at any arbitrary position. Furthermore, we leverage discrimination consistency information across layers to better differentiate between positive and false alarm samples. Our experiments on both clean and noisy Hey Snips datasets show that the proposed streaming decoding strategy outperforms ASR-based and graph-based KWS baselines. The CDC-boosted decoding further improves performance, yielding an average absolute recall improvement of 6.8% and a 46.3% relative reduction in the miss rate compared to the graph-based KWS baseline, with a very low false alarm rate of 0.05 per hour. </p>
<blockquote>
<p>连接时序分类（CTC）是一种非自回归训练准则，广泛应用于在线关键词识别（KWS）。然而，现有的基于CTC的KWS解码策略要么依赖于自动语音识别（ASR），由于其在声学空间进行广泛搜索而没有针对关键词进行特定优化，导致性能不佳；要么依赖于特定的KWS解码图，这些图复杂且难以实施和维护。在这项工作中，我们提出了一种通过跨层鉴别一致性（CDC）增强的流式解码算法，适用于基于CTC的KWS。具体来说，我们引入了一种简化但有效的解码算法，能够在任意位置检测关键词的开始。此外，我们利用跨层的鉴别一致性信息来更好地区分正样本和误报样本。我们在干净和嘈杂的Hey Snips数据集上的实验表明，所提出的流式解码策略优于基于ASR和基于图的KWS基线。通过CDC增强的解码进一步提高了性能，与基于图的KWS基线相比，平均绝对召回率提高了6.8%，漏报率相对降低了46.3%，同时每小时误报率非常低，为0.05次。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12635v2">PDF</a> Accepted by ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对连接时序分类（CTC）的在线关键词识别（KWS）的改进解码算法。该算法通过跨层判别一致性（CDC）增强，引入了一种简化的解码算法，能在任意位置检测关键词的开始。实验表明，与基于自动语音识别（ASR）和基于图的KWS基线相比，该解码策略具有更好的性能。CDC的引入进一步提高了性能，平均绝对召回率提高了6.8%，相对于基于图的KWS基线，误报率降低了46.3%，每小时的误报率仅为每小时0.05次。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入连接时序分类（CTC）用于在线关键词识别（KWS）。</li>
<li>现有CTC-based KWS解码策略依赖于自动语音识别（ASR）或复杂的KWS解码图，存在性能瓶颈。</li>
<li>提出一种改进的解码算法，能在任意位置检测关键词的开始。</li>
<li>通过跨层判别一致性（CDC）增强解码算法，提高关键词识别的准确性。</li>
<li>实验表明，该解码策略在清洁和嘈杂的Hey Snips数据集上均优于基于ASR和基于图的KWS基线。</li>
<li>CDC的引入进一步提高了性能，平均绝对召回率提高，误报率降低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e8fb80ddcd2106a951b4eb89fca24b31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25becc6ca17512c6389bde9859232d21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-729e4fdd3eabb88bbcff0997af20c29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e24f5c79b89e9a15fd52b096f632537b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbe6980727f7156eb6a3ce658e9f476e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HDMoLE-Mixture-of-LoRA-Experts-with-Hierarchical-Routing-and-Dynamic-Thresholds-for-Fine-Tuning-LLM-based-ASR-Models"><a href="#HDMoLE-Mixture-of-LoRA-Experts-with-Hierarchical-Routing-and-Dynamic-Thresholds-for-Fine-Tuning-LLM-based-ASR-Models" class="headerlink" title="HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic   Thresholds for Fine-Tuning LLM-based ASR Models"></a>HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic   Thresholds for Fine-Tuning LLM-based ASR Models</h2><p><strong>Authors:Bingshen Mu, Kun Wei, Qijie Shao, Yong Xu, Lei Xie</strong></p>
<p>Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named \textit{HDMoLE}, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain. </p>
<blockquote>
<p>最近，将大型语言模型（LLM）与自动语音识别（ASR）结合的进展在通用领域取得了显著的效果。虽然通常通过监督微调（SFT）所有模型参数来适应基于预训练LLM的ASR模型到特定领域，但它带来了巨大的计算成本，并且会显著减少其在通用领域中的性能。在本文中，我们提出了一种新的参数高效多领域微调方法，用于适应基于预训练LLM的ASR模型到多口音领域，该方法在多口音领域中无需灾难性遗忘即可进行适应，被称为HDMoLE。它结合了低秩适应（LoRA）与专家混合器（MoE）的优势，并利用层次路由和动态阈值，可以推广到任何线性层。层次路由建立了LoRA专家和口音领域之间的明确对应关系，提高了LoRA专家之间的跨领域协作。与静态Top-K策略激活LoRA专家不同，动态阈值可以自适应地激活MoE层中不同数量的LoRA专家。在多种口音和标准普通话数据集上的实验证明了HDMoLE的有效性。将HDMoLE应用于基于LLM的ASR模型的投影模块，在目标多口音领域取得了与完全微调相似的性能，同时仅使用全微调所需的可训练参数的9.6%，并且在源通用领域的性能几乎没有下降。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19878v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>基于预训练的大型语言模型（LLM）的自动语音识别（ASR）技术在通用领域取得了显著进展。然而，对所有模型参数进行有监督微调（SFT）以适应特定领域时，会带来高计算成本并降低其在通用领域的性能。本文提出了一种新型的参数高效多领域微调方法，名为HDMoLE，用于适应多口音领域的预训练LLM-based ASR模型，同时避免灾难性遗忘。该方法结合了低秩适应（LoRA）与专家混合器（MoE），采用层次路由和动态阈值，可以推广到任何线性层。实验表明，HDMoLE在多口音及标准普通话数据集上表现有效。应用于LLM-based ASR模型的投影模块时，HDMoLE在目标多口音领域实现了与全微调相似的性能，同时仅使用全微调所需训练参数的9.6%，并且在源通用领域的性能损失极小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）与自动语音识别（ASR）的集成在通用领域取得了显著进展。</li>
<li>监督微调（SFT）虽然能使LLM-based ASR模型适应特定领域，但计算成本高并在通用领域性能降低。</li>
<li>提出了一种新型的参数高效多领域微调方法——HDMoLE，适用于多口音领域的预训练LLM-based ASR模型。</li>
<li>HDMoLE结合了低秩适应（LoRA）与专家混合器（MoE）。</li>
<li>层次路由和动态阈值方法被用于HDMoLE，以提高跨领域的协作能力并适应不同数量的LoRA专家。</li>
<li>实验表明HDMoLE在多口音及标准普通话数据集上的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aadcf52086991c24b2d998a5372495c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c21bce8956dc25aedc5af40374b09f41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c37cb7eb9add48fb1727b9ed5be64f8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NanoVoice-Efficient-Speaker-Adaptive-Text-to-Speech-for-Multiple-Speakers"><a href="#NanoVoice-Efficient-Speaker-Adaptive-Text-to-Speech-for-Multiple-Speakers" class="headerlink" title="NanoVoice: Efficient Speaker-Adaptive Text-to-Speech for Multiple   Speakers"></a>NanoVoice: Efficient Speaker-Adaptive Text-to-Speech for Multiple   Speakers</h2><p><strong>Authors:Nohil Park, Heeseung Kim, Che Hyun Lee, Jooyoung Choi, Jiheum Yeom, Sungroh Yoon</strong></p>
<p>We present NanoVoice, a personalized text-to-speech model that efficiently constructs voice adapters for multiple speakers simultaneously. NanoVoice introduces a batch-wise speaker adaptation technique capable of fine-tuning multiple references in parallel, significantly reducing training time. Beyond building separate adapters for each speaker, we also propose a parameter sharing technique that reduces the number of parameters used for speaker adaptation. By incorporating a novel trainable scale matrix, NanoVoice mitigates potential performance degradation during parameter sharing. NanoVoice achieves performance comparable to the baselines, while training 4 times faster and using 45 percent fewer parameters for speaker adaptation with 40 reference voices. Extensive ablation studies and analysis further validate the efficiency of our model. </p>
<blockquote>
<p>我们推出了NanoVoice，这是一款个性化的文本到语音模型，能够高效地为多个发言者同时构建语音适配器。NanoVoice引入了一种批处理说话人适应技术，能够并行微调多个参考数据，从而大大缩短训练时间。除了为每个说话人构建单独的适配器外，我们还提出了一种参数共享技术，以减少用于说话人适应的参数数量。通过引入一个新的可训练比例矩阵，NanoVoice在参数共享时减轻了性能下降的潜在风险。NanoVoice的性能与基线相当，同时训练速度提高了4倍，用于说话人适应的参数减少了45%。广泛的消融实验和分析进一步验证了我们的模型效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15760v2">PDF</a> IEEE International Conference on Acoustics, Speech, and Signal   Processing (ICASSP), 2025, Demo Page: <a target="_blank" rel="noopener" href="https://nanovoice.github.io/">https://nanovoice.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>NanoVoice是一个个性化的文本到语音模型，能够同时为多个说话者构建语音适配器，并引入了批处理说话者适应技术，能够并行微调多个参考，显著减少训练时间。该模型还提出了一种参数共享技术，减少用于说话者适应的参数数量，并借助新型可训练比例矩阵缓解参数共享期间潜在的性能下降问题。NanoVoice的性能与基线相当，训练速度提高4倍，使用参数进行说话者适应时减少了45%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NanoVoice是一个文本到语音模型，能同时为多个说话者构建语音适配器。</li>
<li>引入批处理说话者适应技术，能并行微调多个参考，显著减少训练时间。</li>
<li>提出了参数共享技术，减少用于说话者适应的参数数量。</li>
<li>通过新型可训练比例矩阵，缓解参数共享时的性能下降问题。</li>
<li>NanoVoice的性能与基线相当，训练速度提升4倍，参数使用减少45%。</li>
<li>进行了广泛的分析和消融研究，验证了模型的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15760">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b9785a5a98711f10b8167a1c69c0266a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6d1f62781447f565b27d2b15c775630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b58f850e7947ac5983ff7f0a0a990b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60eb50c57418bf7d0814c984ebe6bd4a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VoiceGuider-Enhancing-Out-of-Domain-Performance-in-Parameter-Efficient-Speaker-Adaptive-Text-to-Speech-via-Autoguidance"><a href="#VoiceGuider-Enhancing-Out-of-Domain-Performance-in-Parameter-Efficient-Speaker-Adaptive-Text-to-Speech-via-Autoguidance" class="headerlink" title="VoiceGuider: Enhancing Out-of-Domain Performance in Parameter-Efficient   Speaker-Adaptive Text-to-Speech via Autoguidance"></a>VoiceGuider: Enhancing Out-of-Domain Performance in Parameter-Efficient   Speaker-Adaptive Text-to-Speech via Autoguidance</h2><p><strong>Authors:Jiheum Yeom, Heeseung Kim, Jooyoung Choi, Che Hyun Lee, Nohil Park, Sungroh Yoon</strong></p>
<p>When applying parameter-efficient finetuning via LoRA onto speaker adaptive text-to-speech models, adaptation performance may decline compared to full-finetuned counterparts, especially for out-of-domain speakers. Here, we propose VoiceGuider, a parameter-efficient speaker adaptive text-to-speech system reinforced with autoguidance to enhance the speaker adaptation performance, reducing the gap against full-finetuned models. We carefully explore various ways of strengthening autoguidance, ultimately finding the optimal strategy. VoiceGuider as a result shows robust adaptation performance especially on extreme out-of-domain speech data. We provide audible samples in our demo page. </p>
<blockquote>
<p>当通过LoRA进行参数高效的微调来适应说话人的文本到语音模型时，与完全微调过的模型相比，适应性能可能会下降，特别是对于非域内的说话人。针对这一问题，我们提出了VoiceGuider，这是一个通过自动引导增强的参数高效说话人自适应文本到语音系统，以提高说话人适应性能，缩小与完全微调模型之间的差距。我们仔细探索了加强自动引导的各种方法，并最终找到了最佳策略。VoiceGuider在极端非域语音数据上表现出了稳健的适应性能。我们在演示页面上提供了可听的样本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15759v2">PDF</a> IEEE International Conference on Acoustics, Speech, and Signal   Processing (ICASSP), 2025, Demo Page: <a target="_blank" rel="noopener" href="https://voiceguider.github.io/">https://voiceguider.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>在文本转语音模型中采用LoRA进行参数有效微调时，相比于全微调的模型，其适应性性能可能会下降，特别是对于非域内的发言人更为明显。为解决这一问题，我们提出了VoiceGuider系统，该系统强化了参数有效的语音自适应功能并引入了自动指导机制来增强语音适应性能，缩小了与全微调模型的差距。经过对强化自动指导机制的多种方法的探索，我们找到了最佳策略。VoiceGuider对于极端非域内的语音数据展现出了强大的适应能力。我们将在演示页面中提供可听的样本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRA在应用于语音自适应文本转语音模型时，相较于全微调模型，其适应性性能可能会下降。</li>
<li>VoiceGuider系统是一种参数有效的语音自适应系统，通过引入自动指导机制强化了性能。</li>
<li>VoiceGuider系统能有效缩小与全微调模型的性能差距。</li>
<li>强化自动指导机制的多种方法被探索，并找到了最佳策略。</li>
<li>VoiceGuider系统在处理极端非域内的语音数据时表现出强大的适应能力。</li>
<li>演示页面中提供了可听的样本以展示VoiceGuider系统的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9112fc85431af49e01e11faab161dbf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05889e4ac4ae9f23063e3d916d892b99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c7b2a9efa449e1ddaaad87e7454b04a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-016703487e861605fd3629e812d9d0fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5af3f8574589d06a6556533f9cd8bc9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Effective-Integration-of-KAN-for-Keyword-Spotting"><a href="#Effective-Integration-of-KAN-for-Keyword-Spotting" class="headerlink" title="Effective Integration of KAN for Keyword Spotting"></a>Effective Integration of KAN for Keyword Spotting</h2><p><strong>Authors:Anfeng Xu, Biqiao Zhang, Shuyu Kong, Yiteng Huang, Zhaojun Yang, Sangeeta Srivastava, Ming Sun</strong></p>
<p>Keyword spotting (KWS) is an important speech processing component for smart devices with voice assistance capability. In this paper, we investigate if Kolmogorov-Arnold Networks (KAN) can be used to enhance the performance of KWS. We explore various approaches to integrate KAN for a model architecture based on 1D Convolutional Neural Networks (CNN). We find that KAN is effective at modeling high-level features in lower-dimensional spaces, resulting in improved KWS performance when integrated appropriately. The findings shed light on understanding KAN for speech processing tasks and on other modalities for future researchers. </p>
<blockquote>
<p>关键词识别（KWS）是智能语音助手设备的重要语音处理组件。本文旨在研究Kolmogorov-Arnold网络（KAN）是否能用于提高KWS的性能。我们探索了基于一维卷积神经网络（CNN）的模型架构集成KAN的各种方法。我们发现，在适当集成时，KAN在低维空间中建模高级特征非常有效，从而提高了KWS的性能。这些发现有助于未来研究人员理解KAN在语音处理任务中的应用，并为其他模式提供启示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08605v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong>：本文探讨了Kolmogorov-Arnold网络（KAN）在关键词识别（KWS）中的应用，以改进智能设备的语音处理能力。研究发现，KAN能够有效地在低维空间中建立高级特征模型，并可以在适当集成后提高KWS的性能。这一发现有助于未来研究人员理解KAN在语音处理任务及其他领域的应用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Kolmogorov-Arnold网络（KAN）被应用于关键词识别（KWS）以提高智能设备的语音处理能力。</li>
<li>KAN能够有效地在低维空间中建立高级特征模型。</li>
<li>KAN的适当集成可以提高KWS的性能。</li>
<li>本文研究基于1D卷积神经网络（CNN）的模型架构来集成KAN。</li>
<li>研究结果有助于未来研究人员理解KAN在语音处理任务中的应用。</li>
<li>该研究为其他领域的研究者提供了关于使用KAN的启示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08605">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-637d37df3b49f9d6185406224979ae04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7631d1be956723816b9e73b1d8c49538.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e344f0851469dcd0245c8658271b749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f9430e3210e6c45f652f57b8e97afef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4957e10ec79eab5bd7d2d2a1653d0846.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8b79f8332d90d0e13340664733a5ee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6cf66c6ab5144f50ebe4fa60be1e49a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Full-text-Error-Correction-for-Chinese-Speech-Recognition-with-Large-Language-Model"><a href="#Full-text-Error-Correction-for-Chinese-Speech-Recognition-with-Large-Language-Model" class="headerlink" title="Full-text Error Correction for Chinese Speech Recognition with Large   Language Model"></a>Full-text Error Correction for Chinese Speech Recognition with Large   Language Model</h2><p><strong>Authors:Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang</strong></p>
<p>Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic Speech Recognition (ASR). However, most research focuses on utterances from short-duration speech recordings, which are the predominant form of speech data for supervised ASR training. This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer speech recordings, such as transcripts from podcasts, news broadcasts, and meetings. First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-speech synthesis, ASR, and error-correction pair extractor. This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive. Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction. Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. This establishes a promising baseline for further research. The dataset is available on the website. </p>
<blockquote>
<p>大型语言模型（LLM）在自动语音识别（ASR）的错误纠正方面展现出了巨大的潜力。然而，大多数研究都集中在来自短时段语音记录的片段上，这是有监督ASR训练的主要语音数据形式。本文研究了LLM在由ASR系统从较长语音记录（如播客、新闻广播和会议记录）生成的全文中的错误纠正效果。首先，我们开发了一个用于全文错误纠正的中文数据集，名为ChFT，该数据集采用了包括文本到语音合成、ASR和错误校正配对提取器在内的管道。该数据集使我们能够在不同上下文中纠正错误，包括全文和片段，并处理更广泛的错误类型，如标点恢复和逆向文本规范化，从而使纠正过程更加全面。其次，我们在构建的数据集上对预训练的LLM进行了微调，使用了多种提示和目标格式来评估其在全文错误纠正方面的性能。具体来说，我们基于全文和片段设计了提示，并考虑了各种输出格式，如直接修正的文本和基于JSON的错误校正对。通过包括同质的、最新的和困难的测试集在内的各种测试设置，我们发现经过微调LLM在全文设置中的表现良好，不同的提示各有其优势和劣势。这为进一步的研究奠定了有希望的基准。数据集可在网站上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07790v2">PDF</a> ICASSP 2025</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）在自动语音识别（ASR）的错误校正方面展现出巨大潜力。然而，大多数研究侧重于短期语音录制的发音，这是监督ASR训练的主要形式。本文研究了LLM在由ASR系统从长语音录制中生成的全文错误校正方面的有效性，如Podcast、新闻广播和会议的记录稿。首先，我们开发了一个用于全文错误校正的中文数据集ChFT，该数据集通过文本到语音合成、ASR和错误校正对提取器构成的管道实现。该数据集使我们能够在各种情境下纠正错误，包括全文和段落，并处理更广泛的错误类型，如标点恢复和逆文本规范化，从而使校正过程更加全面。其次，我们在构建的数据集上微调了预训练的LLM，使用各种提示和目标格式对其性能进行了评估。我们设计了基于全文和段落的提示，并考虑了各种输出格式，如直接修正文本和基于JSON的错误校正对。通过各种测试设置，包括同质的、最新的和困难的测试集，我们发现微调后的LLM在全文设置中的表现良好，每种提示都有其自身的优势和劣势。这为未来的研究奠定了有希望的基准。数据集可在网站上获得。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）在自动语音识别（ASR）的错误校正方面具有显著潜力。</li>
<li>当前研究主要关注短期语音录制的发音，但本文探索了LLM在由长语音录制生成的全文错误校正方面的有效性。</li>
<li>引入了一个新的中文全文错误校正数据集ChFT，用于更全面的错误校正，包括上下文中的标点恢复和逆文本规范化等错误类型。</li>
<li>通过微调预训练的LLM并在多种测试设置下评估其性能，发现LLM在全文设置中的表现良好，不同提示各有优势与劣势。</li>
<li>该研究为未来的研究提供了一个有希望的基准。</li>
<li>所构建的数据集ChFT已公开发布，可供研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07790">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2b65f4d824e1db9858fb68cb7a985861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26be500de3a31e2fa5336219bb82d568.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d53bac2df374ec0bdb946ce3369c3d6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6425d0d71f48f50bba28352e7a21696e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46660c58a4177a91d3aa3f203cdd12d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db5b0db0a3844add6c2b0a17f8e97838.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DCIM-AVSR-Efficient-Audio-Visual-Speech-Recognition-via-Dual-Conformer-Interaction-Module"><a href="#DCIM-AVSR-Efficient-Audio-Visual-Speech-Recognition-via-Dual-Conformer-Interaction-Module" class="headerlink" title="DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer   Interaction Module"></a>DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer   Interaction Module</h2><p><strong>Authors:Xinyu Wang, Haotian Jiang, Haolin Huang, Yu Fang, Mengjie Xua nd Qian Wang</strong></p>
<p>Speech recognition is the technology that enables machines to interpret and process human speech, converting spoken language into text or commands. This technology is essential for applications such as virtual assistants, transcription services, and communication tools. The Audio-Visual Speech Recognition (AVSR) model enhances traditional speech recognition, particularly in noisy environments, by incorporating visual modalities like lip movements and facial expressions. While traditional AVSR models trained on large-scale datasets with numerous parameters can achieve remarkable accuracy, often surpassing human performance, they also come with high training costs and deployment challenges. To address these issues, we introduce an efficient AVSR model that reduces the number of parameters through the integration of a Dual Conformer Interaction Module (DCIM). In addition, we propose a pre-training method that further optimizes model performance by selectively updating parameters, leading to significant improvements in efficiency. Unlike conventional models that require the system to independently learn the hierarchical relationship between audio and visual modalities, our approach incorporates this distinction directly into the model architecture. This design enhances both efficiency and performance, resulting in a more practical and effective solution for AVSR tasks. </p>
<blockquote>
<p>语音识别是使机器能够解释和处理人类语音的技术，它将口语转化为文本或命令。对于虚拟助手、转录服务和通信工具等应用程序而言，这项技术至关重要。视听语音识别（AVSR）模型通过融入诸如嘴唇动作和面部表情等视觉模式，增强了传统语音识别，特别是在嘈杂的环境中。虽然经过大规模数据集训练的传统AVSR模型具有许多参数，可以实现惊人的准确性，有时甚至超过人类的表现，但它们也带来了高昂的训练成本和部署挑战。为了解决这些问题，我们引入了一种高效的AVSR模型，通过集成双卷积交互模块（DCIM）减少了参数数量。此外，我们还提出了一种预训练方法，通过选择性更新参数进一步优化模型性能，从而在效率上取得了显着提高。与传统的需要系统独立学习音频和视觉模态之间层次关系的方法不同，我们的方法直接将这种区别纳入模型架构中。这种设计提高了效率和性能，为AVSR任务提供了更实用、更有效的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00481v4">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>总结</strong><br>语音识别技术让机器能够解读和处理人类语音，将口语转化为文字或命令。对于虚拟助理、转录服务和通信工具等应用，这项技术至关重要。音频视觉语音识别（AVSR）模型通过在视觉模式（如嘴唇动作和面部表情）的融入，增强了传统语音识别能力，特别是在嘈杂环境中。虽然传统的大型数据集训练AVSR模型有许多参数，可以实现惊人的准确性，有时甚至超越人类表现水平，但它们也带来了高昂的训练成本和部署挑战。为解决这些问题，我们引入了高效的AVSR模型，通过集成双变压器交互模块（DCIM）减少了参数数量。此外，我们还提出了一种选择性更新参数的预训练方法，进一步优化模型性能，在效率上取得了显著改进。不同于传统模型需要系统独立学习音频和视觉模态之间的层次关系，我们的方法直接将这种区别纳入模型架构中。这种设计提高了效率和性能，为AVSR任务提供了更实用、有效的解决方案。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>语音识别技术能将人类语音转化为文字或命令，广泛应用于虚拟助理、转录服务和通信工具等领域。</li>
<li>音频视觉语音识别（AVSR）模型在嘈杂环境中表现更优，通过融入视觉模式（如嘴唇动作和面部表情）提升识别能力。</li>
<li>传统AVSR模型虽然准确度高，但存在训练成本高和部署挑战。</li>
<li>引入高效AVSR模型，通过集成双变压器交互模块（DCIM）减少参数数量，提高效率。</li>
<li>提出预训练方法，通过选择性更新参数优化模型性能。</li>
<li>与传统模型不同，新方法将音频和视觉模态的区别直接纳入模型架构中，增强了效率和性能。</li>
<li>这种设计提供了更实用、有效的解决方案，适用于AVSR任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2adb1bf4996b1c649e625733571b7840.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c61bd461280aaa599e4e0945408115aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b83c3536ec993295c4b5cf18813237.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f51746486c27928c98b9d2520d32fd6b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Beyond-Silent-Letters-Amplifying-LLMs-in-Emotion-Recognition-with-Vocal-Nuances"><a href="#Beyond-Silent-Letters-Amplifying-LLMs-in-Emotion-Recognition-with-Vocal-Nuances" class="headerlink" title="Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal   Nuances"></a>Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal   Nuances</h2><p><strong>Authors:Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan Donbekci, Julia Hirschberg</strong></p>
<p>Emotion recognition in speech is a challenging multimodal task that requires understanding both verbal content and vocal nuances. This paper introduces a novel approach to emotion detection using Large Language Models (LLMs), which have demonstrated exceptional capabilities in natural language understanding. To overcome the inherent limitation of LLMs in processing audio inputs, we propose SpeechCueLLM, a method that translates speech characteristics into natural language descriptions, allowing LLMs to perform multimodal emotion analysis via text prompts without any architectural changes. Our method is minimal yet impactful, outperforming baseline models that require structural modifications. We evaluate SpeechCueLLM on two datasets: IEMOCAP and MELD, showing significant improvements in emotion recognition accuracy, particularly for high-quality audio data. We also explore the effectiveness of various feature representations and fine-tuning strategies for different LLMs. Our experiments demonstrate that incorporating speech descriptions yields a more than 2% increase in the average weighted F1 score on IEMOCAP (from 70.111% to 72.596%). </p>
<blockquote>
<p>语音情感识别是一项具有挑战性的多模式任务，需要理解口头内容和语音细微差别。本文介绍了一种使用大型语言模型（LLM）进行情感检测的新方法，LLM在自然语言理解方面表现出了卓越的能力。为了克服LLM在处理音频输入方面的固有局限性，我们提出了SpeechCueLLM方法，将语音特征转化为自然语言描述，允许LLM通过文本提示进行多模式情感分析，而无需进行任何结构更改。我们的方法简单而有效，超越了需要结构修改的基线模型。我们在IEMOCAP和MELD两个数据集上评估了SpeechCueLLM，在情感识别准确率上取得了显著的提升，特别是在高质量音频数据上。我们还探索了针对不同LLM的各种特征表示和微调策略的有效性。我们的实验表明，融入语音描述后，IEMOCAP上的平均加权F1分数提高了2%以上（从70.111%提高到72.596%）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21315v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种利用大型语言模型（LLMs）进行情绪检测的新方法。针对LLMs处理音频输入时的固有局限性，提出SpeechCueLLM方法，将语音特征转换为自然语言描述，使LLMs能够通过文本提示进行多模式情绪分析，无需进行任何架构更改。该方法在IEMOCAP和MELD两个数据集上进行了评估，显著提高了情绪识别准确率，尤其是对高质量音频数据。实验表明，结合语音描述使IEMOCAP的加权平均F1分数提高了2%以上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种利用大型语言模型进行情绪识别的新方法。</li>
<li>提出SpeechCueLLM方法，将语音特征转换为自然语言描述，以克服LLMs处理音频的局限性。</li>
<li>通过文本提示，LLMs可进行多模式情绪分析，无需更改架构。</li>
<li>在IEMOCAP和MELD数据集上评估，显示出较高的情绪识别准确率。</li>
<li>高质量音频数据的情绪识别效果尤为显著。</li>
<li>实验表明，结合语音描述可以提高情绪识别的F1分数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.21315">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fdca217a59f685272fbcf9e04a8e5e2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c4a568ab7f3db4e5559d5cb4d5aa0f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2076776f0626831987445e5658872e03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3942a9d4c303da930e68025ea9ed54f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Dynamic-Language-Group-Based-MoE-Enhancing-Code-Switching-Speech-Recognition-with-Hierarchical-Routing"><a href="#Dynamic-Language-Group-Based-MoE-Enhancing-Code-Switching-Speech-Recognition-with-Hierarchical-Routing" class="headerlink" title="Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech   Recognition with Hierarchical Routing"></a>Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech   Recognition with Hierarchical Routing</h2><p><strong>Authors:Hukai Huang, Shenghui Lu, Yahui Shan, He Qu, Fengrun Zhang, Wenhao Guan, Qingyang Hong, Lin Li</strong></p>
<p>The Mixture of Experts (MoE) model is a promising approach for handling code-switching speech recognition (CS-ASR) tasks. However, the existing CS-ASR work on MoE has yet to leverage the advantages of MoE’s parameter scaling ability fully. This work proposes DLG-MoE, a Dynamic Language Group-based MoE, which can effectively handle the CS-ASR task and leverage the advantages of parameter scaling. DLG-MoE operates based on a hierarchical routing mechanism. First, the language router explicitly models the language attribute and dispatches the representations to the corresponding language expert groups. Subsequently, the unsupervised router within each language group implicitly models attributes beyond language and coordinates expert routing and collaboration. DLG-MoE outperforms the existing MoE methods on CS-ASR tasks while demonstrating great flexibility. It supports different top-$k$ inference and streaming capabilities and can also prune the model parameters flexibly to obtain a monolingual sub-model. The code has been released. </p>
<blockquote>
<p>专家混合（MoE）模型在处理代码切换语音识别（CS-ASR）任务时展现出巨大的潜力。然而，现有的关于MoE的CS-ASR研究尚未充分利用MoE的参数缩放能力优势。本文提出基于动态语言分组的DLG-MoE，它能够有效地处理CS-ASR任务并充分利用参数缩放的优点。DLG-MoE基于分层路由机制运行。首先，语言路由器显式地建模语言属性并将表示分派到相应的语言专家组。接着，每个语言组内的无监督路由器隐式地建模语言以外的属性，并协调专家路由和协作。在CS-ASR任务上，DLG-MoE的表现优于现有的MoE方法，展现出极大的灵活性。它支持不同的top-k推理和流式传输功能，还可以灵活地修剪模型参数以获得单语子模型。代码已经发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18581v4">PDF</a> Accepted by ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>基于动态语言分组的MoE模型（DLG-MoE）在代码切换语音识别任务中展现出强大的性能。该模型利用分层路由机制，通过语言路由器明确建模语言属性并调度表示到对应的语言专家组，同时各组内的无监督路由器隐式建模超越语言的属性并协调专家路由与协作。相较于现有MoE方法，DLG-MoE更具灵活性，支持不同的top-k推理和流式处理，并能灵活调整模型参数以获取单语种子模型。代码已发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动态语言分组的MoE模型（DLG-MoE）能有效处理代码切换语音识别（CS-ASR）任务。</li>
<li>DLG-MoE通过分层路由机制实现语言属性的明确建模和表示调度。</li>
<li>语言路由器和无监督路由器协同工作，分别负责语言属性和超越语言的属性建模。</li>
<li>DLG-MoE相较于现有MoE方法更具灵活性，支持多种推理模式和参数调整。</li>
<li>DLG-MoE模型可以生成单语种子模型，进一步扩展了其应用场景。</li>
<li>该模型的代码已经发布，便于研究和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18581">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c888a89e24a18dde5a49f50cc080f161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba4f4a7c643702ed56d461826685a5c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eacfa030164e760f4f79259c7b2e0ee2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19bb7032f4c17180af10f7a60564ee6f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PI-Whisper-Designing-an-Adaptive-and-Incremental-Automatic-Speech-Recognition-System-for-Edge-Devices"><a href="#PI-Whisper-Designing-an-Adaptive-and-Incremental-Automatic-Speech-Recognition-System-for-Edge-Devices" class="headerlink" title="PI-Whisper: Designing an Adaptive and Incremental Automatic Speech   Recognition System for Edge Devices"></a>PI-Whisper: Designing an Adaptive and Incremental Automatic Speech   Recognition System for Edge Devices</h2><p><strong>Authors:Amir Nassereldine, Dancheng Liu, Chenhui Xu, Ruiyang Qin, Yiyu Shi, Jinjun Xiong</strong></p>
<p>Edge-based automatic speech recognition (ASR) technologies are increasingly prevalent in the development of intelligent and personalized assistants. However, resource-constrained ASR models face significant challenges in adaptivity, incrementality, and inclusivity when faced with a diverse population. To tackle those challenges, we propose PI-Whisper, a novel ASR system that adaptively enhances recognition capabilities by identifying speakers’ characteristics in real-time. In this work, we show how the design of PI-Whisper allows for incremental adaptation of new characteristics without the need for repetitive retraining, enhances recognition capabilities, and improves equity and fairness across diverse speaker groups. PI-Whisper demonstrates these advantages by achieving state-of-the-art accuracy, reducing the word error rate (WER) by up to 13.7% relative to baselines while scaling linearly to computing resources. </p>
<blockquote>
<p>基于边缘的自动语音识别（ASR）技术在智能和个性化助理的开发中越来越普遍。然而，资源有限的ASR模型在面对多样化人群时，在适应性、增量性和包容性方面面临重大挑战。为了解决这些挑战，我们提出了PI-Whisper，这是一种新型ASR系统，通过实时识别说话人的特点，自适应地增强识别能力。在这项工作中，我们展示了PI-Whisper的设计如何允许对新特性进行增量适应，无需重复再训练，增强识别能力，并改善不同说话人群体之间的公平性和公正性。PI-Whisper通过实现最先进的准确性，相对于基准线降低了高达1.7%的单词错误率（WER），并且随着计算资源的增加而线性扩展，从而证明了这些优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15668v2">PDF</a> in submission</p>
<p><strong>Summary</strong></p>
<p>边缘计算基础上的自动语音识别（ASR）技术在智能个性化助理的开发中越来越普遍。然而，资源有限的ASR模型在适应多样化人群时面临着适应性、增量性和包容性的重大挑战。为解决这些挑战，我们提出了PI-Whisper这一新型ASR系统，它可以通过实时识别说话人的特点来动态提升识别能力。本工作中，我们展示了PI-Whisper的设计可以支持对新特性的增量适应，无需重复训练，提高识别能力，并改善对不同说话群体的公平性和公正性。PI-Whisper的这些优势使其成为业界领先的技术，相对于基线技术，其单词错误率（WER）降低了高达13.7%，并且随着计算资源的增加而线性扩展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PI-Whisper是一种新型的ASR系统，可实时识别说话人的特点并动态提升识别能力。</li>
<li>PI-Whisper设计支持对新特性的增量适应，无需重复训练。</li>
<li>PI-Whisper提高了对不同说话群体的公平性和公正性。</li>
<li>PI-Whisper实现了业界领先的准确性，单词错误率（WER）相对于基线技术降低了高达13.7%。</li>
<li>PI-Whisper系统随着计算资源的增加而线性扩展。</li>
<li>该系统主要适用于资源有限的ASR模型在面临多样化人群时的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15668">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-13b3e2362004ead459bc46222cd08cc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-419aea759583e3976d723613023e6598.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-502e9dc8005f2d526dc3d91a18d62321.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dcde70132e329c7b29342c971de3258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8029ab6c84a2ae39663f86c36a34a560.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db2b04ba3b7c41c376bb9c9a48f6edbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac08a5e69bf2a7870007e0fc8255bad2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="HeadStudio-Text-to-Animatable-Head-Avatars-with-3D-Gaussian-Splatting"><a href="#HeadStudio-Text-to-Animatable-Head-Avatars-with-3D-Gaussian-Splatting" class="headerlink" title="HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting"></a>HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting</h2><p><strong>Authors:Zhenglin Zhou, Fan Ma, Hehe Fan, Zongxin Yang, Yi Yang</strong></p>
<p>Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising results achieved with 2D diffusion priors, current methods struggle to create high-quality and consistent animated avatars efficiently. Previous animatable head models like FLAME have difficulty in accurately representing detailed texture and geometry. Additionally, high-quality 3D static representations face challenges in semantically driving with dynamic priors. In this paper, we introduce \textbf{HeadStudio}, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animatable avatars from text prompts. Firstly, we associate 3D Gaussians with animatable head prior model, facilitating semantic animation on high-quality 3D representations. To ensure consistent animation, we further enhance the optimization from initialization, distillation, and regularization to jointly learn the shape, texture, and animation. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting appealing appearances. The avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel views at a resolution of 1024. Moreover, These avatars can be smoothly driven by real-world speech and video. We hope that HeadStudio can enhance digital avatar creation and gain popularity in the community. Code is at: <a target="_blank" rel="noopener" href="https://github.com/ZhenglinZhou/HeadStudio">https://github.com/ZhenglinZhou/HeadStudio</a>. </p>
<blockquote>
<p>从文本提示创建数字化身一直是一项令人向往但具有挑战性的任务。尽管利用二维扩散先验取得了有前景的结果，但当前的方法在高效创建高质量且连贯的动画化身方面遇到了困难。像FLAME这样的之前可动画头部模型在准确表示详细纹理和几何结构方面存在困难。此外，高质量的三维静态表示在语义驱动的动态先验方面面临挑战。在本文中，我们介绍了利用三维高斯泼斑技术生成从文本提示出发的真实且可动画的化身的全新框架——HeadStudio。首先，我们将三维高斯分布与可动画头部先验模型关联起来，实现在高质量三维表示上的语义动画。为确保连贯的动画效果，我们从初始化、蒸馏和正则化三个方面进一步优化了形状、纹理和动画的联合学习。大量实验证明了HeadStudio从文本提示生成可动画化身的有效性，展现出吸引人的外观。这些化身能够以高于或等于40帧每秒的速度在1024的分辨率下呈现高质量实时新视角。此外，这些化身可以被现实世界中的语音和视频流畅驱动。我们希望HeadStudio能增强数字化身的创建功能并在社区中广受欢迎。代码地址为：<a target="_blank" rel="noopener" href="https://github.com/ZhenglinZhou/HeadStudio%E3%80%82">https://github.com/ZhenglinZhou/HeadStudio。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06149v2">PDF</a> 26 pages, 18 figures, accepted by ECCV 2024</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为HeadStudio的新型框架，该框架利用3D高斯涂鸦技术，从文本提示生成真实且可动画的头像。HeadStudio通过结合动画头像先验模型与3D高斯分布，实现高质量3D表示上的语义动画。通过优化初始化、蒸馏和正则化过程，联合学习形状、纹理和动画，确保动画的一致性。实验证明，HeadStudio能高效地从文本提示生成可动画的头像，呈现吸引人的外观，并可实时（≥40帧&#x2F;秒）渲染高质量头像新视角，且头像可被真实语音和视频流畅驱动。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HeadStudio是一种新型框架，可从文本提示生成真实且可动画的头像。</li>
<li>利用3D高斯涂鸦技术实现高质量和一致的动画头像生成。</li>
<li>结合动画头像先验模型与3D高斯分布，实现语义动画。</li>
<li>通过优化初始化、蒸馏和正则化过程，确保动画一致性。</li>
<li>能高效生成吸引人的头像，实时渲染高质量新视角。</li>
<li>生成的头像能被真实语音和视频流畅驱动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.06149">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-215d6480ae84da5896c28fbfbfba36d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70eb1a7b18cd38c2ce7113fe7c708209.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfa619e455be609c31826d56564bb1b5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6ad198e2b18880b953fd90d912db4343.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2024-12-26  Hardware-aware Circuit Cutting and Distributed Qubit Mapping for   Connected Quantum Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-05b76fb015fc2d99bc60a863c31ee42a.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2024-12-26  ErasableMask A Robust and Erasable Privacy Protection Scheme against   Black-box Face Recognition Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
