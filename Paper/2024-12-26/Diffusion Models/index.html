<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-26  PartGen Part-level 3D Generation and Reconstruction with Multi-View   Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-1ead218b7a67780339b0cc9755cc860e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-26-更新"><a href="#2024-12-26-更新" class="headerlink" title="2024-12-26 更新"></a>2024-12-26 更新</h1><h2 id="PartGen-Part-level-3D-Generation-and-Reconstruction-with-Multi-View-Diffusion-Models"><a href="#PartGen-Part-level-3D-Generation-and-Reconstruction-with-Multi-View-Diffusion-Models" class="headerlink" title="PartGen: Part-level 3D Generation and Reconstruction with Multi-View   Diffusion Models"></a>PartGen: Part-level 3D Generation and Reconstruction with Multi-View   Diffusion Models</h2><p><strong>Authors:Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, Andrea Vedaldi</strong></p>
<p>Text- or image-to-3D generators and 3D scanners can now produce 3D assets with high-quality shapes and textures. These assets typically consist of a single, fused representation, like an implicit neural field, a Gaussian mixture, or a mesh, without any useful structure. However, most applications and creative workflows require assets to be made of several meaningful parts that can be manipulated independently. To address this gap, we introduce PartGen, a novel approach that generates 3D objects composed of meaningful parts starting from text, an image, or an unstructured 3D object. First, given multiple views of a 3D object, generated or rendered, a multi-view diffusion model extracts a set of plausible and view-consistent part segmentations, dividing the object into parts. Then, a second multi-view diffusion model takes each part separately, fills in the occlusions, and uses those completed views for 3D reconstruction by feeding them to a 3D reconstruction network. This completion process considers the context of the entire object to ensure that the parts integrate cohesively. The generative completion model can make up for the information missing due to occlusions; in extreme cases, it can hallucinate entirely invisible parts based on the input 3D asset. We evaluate our method on generated and real 3D assets and show that it outperforms segmentation and part-extraction baselines by a large margin. We also showcase downstream applications such as 3D part editing. </p>
<blockquote>
<p>文本或图像到3D生成器和3D扫描仪现在可以生成具有高质量形状和纹理的3D资产。这些资产通常由单一融合表示组成，如隐式神经网络、高斯混合或网格，而没有有用的结构。然而，大多数应用程序和创意工作流程要求资产由可以独立操作的多个有意义的部分组成。为了解决这个问题，我们引入了PartGen，这是一种从文本、图像或无序的3D对象开始生成由有意义的部件组成的3D对象的新方法。首先，给定一个3D对象的多个视图，无论是生成的还是渲染的，多视图扩散模型会提取一组合理且视图一致的部分分割，将对象分割成各个部分。然后，第二个多视图扩散模型单独处理每个部分，填充遮挡物，并使用这些完成的视图进行3D重建，方法是将其输入到3D重建网络中。此完成过程会考虑整个对象的上下文，以确保各部分能够紧密集成。生成完成模型可以弥补因遮挡而缺失的信息；在极端情况下，它可以根据输入的3D资产完全虚构出不可见的部分。我们在生成的和真实的3D资产上评估了我们的方法，并显示出它大大优于分割和部分提取的基线。我们还展示了下游应用，如3D零件编辑。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18608v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://silent-chen.github.io/PartGen/">https://silent-chen.github.io/PartGen/</a></p>
<p><strong>Summary</strong></p>
<p>该文介绍了一种名为PartGen的新方法，该方法能够从文本、图像或无序的3D对象开始，生成由有意义的部件组成的3D物体。PartGen通过多视角扩散模型提取出可能的、视角一致的部件分割，然后对每一个部件进行填充和完善，最后通过3D重建网络进行三维重建。此方法能够弥补因遮挡而缺失的信息，甚至在极端情况下，可以基于输入的3D资产凭空生成完全不可见的部件。PartGen在生成和真实的3D资产上的表现均优于现有的分割和部件提取方法，并在3D部件编辑等下游应用展示了其潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PartGen能够生成由有意义部件组成的3D资产，从文本、图像或无序的3D对象开始。</li>
<li>PartGen使用多视角扩散模型提取3D物体的部件分割。</li>
<li>扩散模型能够填充和完善部件，并考虑整个物体的上下文来确保部件的整合。</li>
<li>PartGen能够弥补因遮挡而缺失的信息，甚至生成完全不可见的部件。</li>
<li>PartGen在生成和真实的3D资产上的表现均优于现有的方法。</li>
<li>PartGen具有广泛的应用潜力，如3D部件编辑等下游应用。</li>
<li>PartGen为创建和操作复杂的3D场景提供了一个有效的工具，有助于推动3D生成和编辑技术的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18608">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8a6eb0c9674fc20fcc197c698773b0c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a4484bed5c2f49df6684085dd8cb8c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a04b43ae9c468b0c9a10571b740f65a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec6164f2b484e5b17e41a37dff6fa50.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Resolution-Robust-3D-MRI-Reconstruction-with-2D-Diffusion-Priors-Diverse-Resolution-Training-Outperforms-Interpolation"><a href="#Resolution-Robust-3D-MRI-Reconstruction-with-2D-Diffusion-Priors-Diverse-Resolution-Training-Outperforms-Interpolation" class="headerlink" title="Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:   Diverse-Resolution Training Outperforms Interpolation"></a>Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:   Diverse-Resolution Training Outperforms Interpolation</h2><p><strong>Authors:Anselm Krainovic, Stefan Ruschke, Reinhard Heckel</strong></p>
<p>Deep learning-based 3D imaging, in particular magnetic resonance imaging (MRI), is challenging because of limited availability of 3D training data. Therefore, 2D diffusion models trained on 2D slices are starting to be leveraged for 3D MRI reconstruction. However, as we show in this paper, existing methods pertain to a fixed voxel size, and performance degrades when the voxel size is varied, as it is often the case in clinical practice. In this paper, we propose and study several approaches for resolution-robust 3D MRI reconstruction with 2D diffusion priors. As a result of this investigation, we obtain a simple resolution-robust variational 3D reconstruction approach based on diffusion-guided regularization of randomly sampled 2D slices. This method provides competitive reconstruction quality compared to posterior sampling baselines. Towards resolving the sensitivity to resolution-shifts, we investigate state-of-the-art model-based approaches including Gaussian splatting, neural representations, and infinite-dimensional diffusion models, as well as a simple data-centric approach of training the diffusion model on several resolutions. Our experiments demonstrate that the model-based approaches fail to close the performance gap in 3D MRI. In contrast, the data-centric approach of training the diffusion model on various resolutions effectively provides a resolution-robust method without compromising accuracy. </p>
<blockquote>
<p>基于深度学习的3D成像，特别是磁共振成像（MRI），由于3D训练数据的有限可用性而具有挑战性。因此，开始利用在2D切片上训练的2D扩散模型进行3D MRI重建。然而，我们在本文中展示，现有方法涉及固定的体素大小，当体素大小发生变化时，性能会下降，这在临床实践中是常见的情况。在本文中，我们针对具有2D扩散先验的分辨率稳健的3D MRI重建提出了几种方法并进行了研究。通过这项研究，我们获得了一种基于随机采样2D切片的扩散引导正则化的简单分辨率稳健变分3D重建方法。该方法与后采样基线相比，提供了具有竞争力的重建质量。为了解决对分辨率变化的敏感性，我们调查了基于模型的最新方法，包括高斯喷射、神经表示和无限维扩散模型，以及一种在多种分辨率上训练扩散模型的简单以数据为中心的方法。我们的实验表明，基于模型的方法未能缩小在3D MRI中的性能差距。相反，以数据为中心的在多种分辨率上训练扩散模型的方法有效地提供了一种分辨率稳健的方法，而不会牺牲准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18584v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于深度学习的三维成像技术（尤其是磁共振成像）所面临的挑战。因三维训练数据的局限性，开始利用二维扩散模型进行三维MRI重建。然而，现有方法受限于固定的体素大小，在临床实践中体素大小变化时性能会下降。本文提出并研究了几种具有二维扩散先验的分辨率鲁棒三维MRI重建方法。经过调查，得出一种基于扩散引导正则化的简单分辨率鲁棒三维重建方法，对随机采样的二维切片进行处理。与后采样基线相比，该方法具有竞争力的重建质量。为解决对分辨率变化的敏感性，本文研究了基于模型的方法，包括高斯溅射、神经表征和无限维扩散模型等，以及一种简单的以数据为中心的训练扩散模型的方法，即在多种分辨率上进行训练。实验表明，基于模型的方法在3D MRI中未能弥补性能差距。相反，以数据为中心的训练扩散模型的方法在各种分辨率上提供有效的分辨率鲁棒方法，且不损失准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在三维成像（尤其是MRI）中的应用面临缺乏足够的三维训练数据的挑战。</li>
<li>当前的方法受限于固定的体素大小，使得在临床实践中体素大小变化时性能下降。</li>
<li>提出了一种基于扩散引导正则化的分辨率鲁棒的三维MRI重建方法，该方法处理随机采样的二维切片。</li>
<li>与后采样基线相比，该方法具有竞争力的重建质量。</li>
<li>尝试多种模型方法以解决分辨率变化的问题，包括高斯溅射、神经表征和无限维扩散模型等，但效果并不理想。</li>
<li>一种以数据为中心的训练扩散模型的方法在各种分辨率上表现有效，提供分辨率鲁棒性且不损失准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18584">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4ab37b91bb3c25653117a3e6afd05dc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4277b872cd8a48906ad1edb80745a469.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3947428a182ab03c5f554d225b6327e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="3DEnhancer-Consistent-Multi-View-Diffusion-for-3D-Enhancement"><a href="#3DEnhancer-Consistent-Multi-View-Diffusion-for-3D-Enhancement" class="headerlink" title="3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement"></a>3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement</h2><p><strong>Authors:Yihang Luo, Shangchen Zhou, Yushi Lan, Xingang Pan, Chen Change Loy</strong></p>
<p>Despite advances in neural rendering, due to the scarcity of high-quality 3D datasets and the inherent limitations of multi-view diffusion models, view synthesis and 3D model generation are restricted to low resolutions with suboptimal multi-view consistency. In this study, we present a novel 3D enhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent diffusion model to enhance coarse 3D inputs while preserving multi-view consistency. Our method includes a pose-aware encoder and a diffusion-based denoiser to refine low-quality multi-view images, along with data augmentation and a multi-view attention module with epipolar aggregation to maintain consistent, high-quality 3D outputs across views. Unlike existing video-based approaches, our model supports seamless multi-view enhancement with improved coherence across diverse viewing angles. Extensive evaluations show that 3DEnhancer significantly outperforms existing methods, boosting both multi-view enhancement and per-instance 3D optimization tasks. </p>
<blockquote>
<p>尽管神经网络渲染有所进展，但由于高质量3D数据集稀缺以及多视角扩散模型本身的局限性，视图合成和3D模型生成仍然受限于低分辨率，并且多视角一致性不佳。在本研究中，我们提出了一种新型3D增强流程，名为“3DEnhancer”，它采用多视角潜在扩散模型，能够在保持多视角一致性的同时，增强粗糙的3D输入。我们的方法包括一个姿态感知编码器和一个基于扩散的去噪器，用于细化低质量的多视角图像，还包括数据增强和带有极线聚合的多视角注意力模块，以在多种视角之间保持一致、高质量输出。与现有的视频方法不同，我们的模型支持无缝的多视角增强，并且在不同的观看角度上都具备更强的连贯性。广泛评估表明，相较于现有方法，3DEnhancer在多视角增强和每个实例的3D优化任务上都表现出显著优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18565v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://yihangluo.com/projects/3DEnhancer">https://yihangluo.com/projects/3DEnhancer</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为3DEnhancer的新型3D增强管道，采用多视角潜在扩散模型，对粗糙的3D输入进行增强，同时保持多视角的一致性。通过姿态感知编码器、基于扩散的去噪器、数据增强和多视角注意力模块等多种技术，实现了低质量多视角图像的精细处理，并维持了跨视角的高品质3D输出一致性。与现有视频方法不同，该模型支持无缝多视角增强，在不同观看角度间具有更好的连贯性。评估表明，3DEnhancer显著优于现有方法，同时提升了多视角增强和每个实例的3D优化任务的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了新型的3D增强管道——3DEnhancer。</li>
<li>采用多视角潜在扩散模型进行低质量3D输入的增强。</li>
<li>通过姿态感知编码器和基于扩散的去噪器对低质量的多视角图像进行精细处理。</li>
<li>利用数据增强和多视角注意力模块维持跨视角的高品质输出一致性。</li>
<li>与现有视频方法不同，支持无缝多视角增强，提高不同观看角度间的连贯性。</li>
<li>评估结果显示，3DEnhancer在多项任务上显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18565">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f0a83df7bf7fc59916b692efdd3d7500.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8195456775649f2779d2720d6702a36b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887734334fd2d73c4119c506bc2c07c5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Fashionability-Enhancing-Outfit-Image-Editing-with-Conditional-Diffusion-Models"><a href="#Fashionability-Enhancing-Outfit-Image-Editing-with-Conditional-Diffusion-Models" class="headerlink" title="Fashionability-Enhancing Outfit Image Editing with Conditional Diffusion   Models"></a>Fashionability-Enhancing Outfit Image Editing with Conditional Diffusion   Models</h2><p><strong>Authors:Qice Qin, Yuki Hirakawa, Ryotaro Shimizu, Takuya Furusawa, Edgar Simo-Serra</strong></p>
<p>Image generation in the fashion domain has predominantly focused on preserving body characteristics or following input prompts, but little attention has been paid to improving the inherent fashionability of the output images. This paper presents a novel diffusion model-based approach that generates fashion images with improved fashionability while maintaining control over key attributes. Key components of our method include: 1) fashionability enhancement, which ensures that the generated images are more fashionable than the input; 2) preservation of body characteristics, encouraging the generated images to maintain the original shape and proportions of the input; and 3) automatic fashion optimization, which does not rely on manual input or external prompts. We also employ two methods to collect training data for guidance while generating and evaluating the images. In particular, we rate outfit images using fashionability scores annotated by multiple fashion experts through OpenSkill-based and five critical aspect-based pairwise comparisons. These methods provide complementary perspectives for assessing and improving the fashionability of the generated images. The experimental results show that our approach outperforms the baseline Fashion++ in generating images with superior fashionability, demonstrating its effectiveness in producing more stylish and appealing fashion images. </p>
<blockquote>
<p>时尚领域的图像生成主要聚焦于保留人体特征或遵循输入提示，但对提高输出图像的内在时尚性关注较少。本文提出了一种基于扩散模型的新方法，在保持关键属性控制的同时，生成具有改进时尚性的时尚图像。我们的方法的关键组成部分包括：1）时尚性增强，确保生成的图像比输入图像更时尚；2）保留人体特征，鼓励生成的图像保持输入的原始形状和比例；3）自动时尚优化，不依赖手动输入或外部提示。我们还采用两种方法收集训练数据，用于在生成和评估图像时进行指导。特别是，我们通过基于OpenSkill和五个关键方面的成对比较，由多名时尚专家对服装图像进行时尚度评分注释。这些方法为评估和提高生成图像的时尚性提供了互补的视角。实验结果表明，我们的方法在生成具有优越时尚性的图像方面优于基线Fashion++，证明其在生成更时尚和吸引人的时尚图像方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18421v1">PDF</a> 11 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于扩散模型的新方法，用于生成时尚图像。该方法可改善生成图像的时尚性，同时保持对关键属性的控制。该研究采用多种方法收集训练数据，并利用时尚专家标注的时尚评分来评估和改进生成图像的时尚性。实验结果表明，该方法在生成具有更高时尚性的图像方面优于基线Fashion++。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时尚领域的图像生成主要关注保留身体特征或遵循输入提示，但很少有人关注提高输出图像的内在时尚性。</li>
<li>本文提出了一种基于扩散模型的方法，能够生成具有改进时尚性的时尚图像，同时保持对关键属性的控制。</li>
<li>方法的关键组成部分包括：提高时尚性、保留身体特征以及自动时尚优化。</li>
<li>采用两种方法来收集训练数据，用于在生成和评估图像时提供指导。</li>
<li>通过基于时尚的评分和五个关键方面的成对比较，由多位时尚专家对服装图像进行评估。</li>
<li>这些方法提供了评估和改进生成图像时尚性的互补视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18421">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-322d04d054ae05a912e6574227b6116b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3368c2bdc1a1b7b9e7efb2546ef2b46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-410ef73b77e04c6991bdb5fcb6d53f38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95f8aa51dbcce4b118c6d338a7ea18e4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Expand-VSR-Benchmark-for-VLLM-to-Expertize-in-Spatial-Rules"><a href="#Expand-VSR-Benchmark-for-VLLM-to-Expertize-in-Spatial-Rules" class="headerlink" title="Expand VSR Benchmark for VLLM to Expertize in Spatial Rules"></a>Expand VSR Benchmark for VLLM to Expertize in Spatial Rules</h2><p><strong>Authors:Peijin Xie, Lin Sun, Bingquan Liu, Dexin Wang, Xiangzheng Zhang, Chengjie Sun, Jiajia Zhang</strong></p>
<p>Distinguishing spatial relations is a basic part of human cognition which requires fine-grained perception on cross-instance. Although benchmarks like MME, MMBench and SEED comprehensively have evaluated various capabilities which already include visual spatial reasoning(VSR). There is still a lack of sufficient quantity and quality evaluation and optimization datasets for Vision Large Language Models(VLLMs) specifically targeting visual positional reasoning. To handle this, we first diagnosed current VLLMs with the VSR dataset and proposed a unified test set. We found current VLLMs to exhibit a contradiction of over-sensitivity to language instructions and under-sensitivity to visual positional information. By expanding the original benchmark from two aspects of tunning data and model structure, we mitigated this phenomenon. To our knowledge, we expanded spatially positioned image data controllably using diffusion models for the first time and integrated original visual encoding(CLIP) with other 3 powerful visual encoders(SigLIP, SAM and DINO). After conducting combination experiments on scaling data and models, we obtained a VLLM VSR Expert(VSRE) that not only generalizes better to different instructions but also accurately distinguishes differences in visual positional information. VSRE achieved over a 27% increase in accuracy on the VSR test set. It becomes a performant VLLM on the position reasoning of both the VSR dataset and relevant subsets of other evaluation benchmarks. We open-sourced the expanded model with data and Appendix at \url{<a target="_blank" rel="noopener" href="https://github.com/peijin360/vsre%7D">https://github.com/peijin360/vsre}</a> and hope it will accelerate advancements in VLLM on VSR learning. </p>
<blockquote>
<p>区分空间关系是人类认知的基本组成部分，这需要跨实例的精细感知。虽然MME、MMBench和SEED等基准测试已经全面评估了各种能力，其中包括视觉空间推理（VSR）。然而，针对视觉大语言模型（VLLMs）专门用于视觉位置推理的充足数量和质量评价和优化的数据集仍然缺乏。为了解决这个问题，我们首先使用VSR数据集对当前的VLLMs进行了诊断，并提出了一个统一的测试集。我们发现当前的VLLMs表现出对语言指令过于敏感和对视觉位置信息不够敏感的矛盾。我们通过从调整数据和模型结构两个方面扩展基准测试，缓解了这一现象。据我们所知，我们首次使用扩散模型可控地扩展了空间定位图像数据，并将原始的视觉编码（CLIP）与其他三种强大的视觉编码器（SigLIP、SAM和DINO）进行了集成。在扩大数据和模型规模的组合实验后，我们获得了一个VLLM VSR专家（VSRE），它不仅能更好地适应不同的指令，还能准确区分视觉位置信息的差异。VSRE在VSR测试集上的准确率提高了27%以上。它成为在VSR数据集和其他相关子集的定位推理方面的性能良好的VLLM。我们已在<a target="_blank" rel="noopener" href="https://github.com/peijin360/vsre%E5%BC%80%E6%BA%90%E6%89%A9%E5%B1%95%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%99%84%E5%BD%95%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%B8%8C%E6%9C%9B%E5%AE%83%E8%83%BD%E5%8A%A0%E9%80%9FVLLM%E5%9C%A8VSR%E5%AD%A6%E4%B9%A0%E6%96%B9%E9%9D%A2%E7%9A%84%E8%BF%9B%E5%B1%95%E3%80%82">https://github.com/peijin360/vsre开源扩展的模型和附录数据，希望它能加速VLLM在VSR学习方面的进展。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18224v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>该文研究了人类认知中的空间关系区分能力，并指出当前针对视觉大语言模型（VLLMs）在视觉位置推理方面的数据集缺乏足够的数量和质量。文章通过诊断当前VLLMs与VSR数据集的问题，提出了一种统一的测试集，并发现当前模型对语言指令过于敏感而对视觉位置信息不够敏感的问题。通过从数据和模型结构两方面进行扩展，缓解了这一问题。此外，文章首次使用扩散模型可控地扩展了空间定位图像数据，并将原始视觉编码CLIP与其他三种强大的视觉编码器SigLIP、SAM和DINO相结合。通过数据和模型的规模扩展组合实验，获得了VLLM VSR Expert（VSRE），该模型不仅对不同指令的泛化能力更强，而且能准确区分视觉位置信息的差异。VSRE在VSR测试集上的准确率提高了超过27%，成为在VSR数据集和其他相关子集的位置推理方面的优秀VLLM。研究团队已公开扩展的模型和附录数据，以加速VLLM在VSR学习方面的进展。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>当前针对视觉大语言模型（VLLMs）在视觉位置推理方面的数据集缺乏足够的数量和质量评价和优化。</li>
<li>对当前VLLMs进行诊断，发现它们对语言指令过于敏感，对视觉位置信息不够敏感。</li>
<li>通过扩展数据和模型结构，提出了一种缓解上述现象的方法。</li>
<li>首次使用扩散模型可控地扩展空间定位图像数据，并将CLIP与其他三种视觉编码器结合。</li>
<li>通过组合实验，获得了名为VSRE的VLLM VSR Expert，该模型在VSR数据集上的准确率显著提高。</li>
<li>VSRE模型已在不同指令中表现出良好的泛化能力，并能准确区分视觉位置信息的差异。</li>
<li>研究团队已公开模型和附录数据，以加速VLLM在VSR学习方面的进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18224">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-757e45d22fad6bd5f6fd81e80d8f7ad7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5801014f6d501ca685538bf1d78406ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e79c67494a9a98161c00b97dbee34503.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3068dc504705f14123a918249b80a559.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d3300295e228f4a87990454572932db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535ccb900dbf7efa279a64a90db2a57e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dense-Face-Personalized-Face-Generation-Model-via-Dense-Annotation-Prediction"><a href="#Dense-Face-Personalized-Face-Generation-Model-via-Dense-Annotation-Prediction" class="headerlink" title="Dense-Face: Personalized Face Generation Model via Dense Annotation   Prediction"></a>Dense-Face: Personalized Face Generation Model via Dense Annotation   Prediction</h2><p><strong>Authors:Xiao Guo, Manh Tran, Jiaxin Cheng, Xiaoming Liu</strong></p>
<p>The text-to-image (T2I) personalization diffusion model can generate images of the novel concept based on the user input text caption. However, existing T2I personalized methods either require test-time fine-tuning or fail to generate images that align well with the given text caption. In this work, we propose a new T2I personalization diffusion model, Dense-Face, which can generate face images with a consistent identity as the given reference subject and align well with the text caption. Specifically, we introduce a pose-controllable adapter for the high-fidelity image generation while maintaining the text-based editing ability of the pre-trained stable diffusion (SD). Additionally, we use internal features of the SD UNet to predict dense face annotations, enabling the proposed method to gain domain knowledge in face generation. Empirically, our method achieves state-of-the-art or competitive generation performance in image-text alignment, identity preservation, and pose control. </p>
<blockquote>
<p>文本转图像（T2I）个性化扩散模型可以根据用户输入的文本描述生成新型概念的图像。然而，现有的T2I个性化方法要么需要测试时的微调，要么无法生成与给定文本描述相符的图像。在这项工作中，我们提出了一种新的T2I个性化扩散模型——Dense-Face，该模型能够生成与给定参考主体身份一致的面部图像，并与文本描述良好对齐。具体来说，我们在保持预训练稳定扩散（SD）的文本编辑能力的同时，引入了一个姿态可控的适配器，用于高保真图像生成。此外，我们还使用SD UNet的内部特征来预测密集面部注释，使所提出的方法能够获取面部生成的领域知识。从经验上看，我们的方法在图像文本对齐、身份保留和姿态控制方面达到了最先进的或具有竞争力的生成性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18149v1">PDF</a> 15 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型文本转图像个性化扩散模型——Dense-Face。该模型能够根据用户输入的文本描述生成新颖概念的图像，并且能生成与给定参考主体身份一致的人脸图像，同时与文本描述高度对齐。模型引入了姿态可控适配器，以在保持基于文本的编辑能力的同时，实现高保真图像生成。此外，通过使用预训练稳定扩散的内部特征来预测密集人脸注释，使该方法获得人脸生成领域的专业知识。实验表明，该方法在图像文本对齐、身份保留和姿态控制方面达到了领先水平或具有竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Dense-Face模型能够根据用户输入的文本描述生成新颖概念的图像。</li>
<li>该模型能够生成与给定参考主体身份一致的人脸图像。</li>
<li>Dense-Face模型引入了姿态可控适配器以实现高保真图像生成。</li>
<li>模型结合了预训练稳定扩散的内部特征进行密集人脸注释预测。</li>
<li>Dense-Face模型在图像文本对齐方面达到了领先水平或具有竞争力。</li>
<li>该模型在身份保留和姿态控制方面表现出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18149">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-45d73ef7bd4a55034e21586e69cc6368.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-151c9344f59c7460eeecd09816897c86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebc316c36204be8b4735e512921a66e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fcd302267d86d19a36bc5ae8bb0bec2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy"><a href="#Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy" class="headerlink" title="Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy"></a>Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy</h2><p><strong>Authors:Shaoyan Pan, Yikang Liu, Lin Zhao, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun</strong></p>
<p>The accurate segmentation of guidewires in interventional cardiac fluoroscopy videos is crucial for computer-aided navigation tasks. Although deep learning methods have demonstrated high accuracy and robustness in wire segmentation, they require substantial annotated datasets for generalizability, underscoring the need for extensive labeled data to enhance model performance. To address this challenge, we propose the Segmentation-guided Frame-consistency Video Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy videos, augmenting the training data for wire segmentation networks. SF-VD leverages videos with limited annotations by independently modeling scene distribution and motion distribution. It first samples the scene distribution by generating 2D fluoroscopy images with wires positioned according to a specified input mask, and then samples the motion distribution by progressively generating subsequent frames, ensuring frame-to-frame coherence through a frame-consistency strategy. A segmentation-guided mechanism further refines the process by adjusting wire contrast, ensuring a diverse range of visibility in the synthesized image. Evaluation on a fluoroscopy dataset confirms the superior quality of the generated videos and shows significant improvements in guidewire segmentation. </p>
<blockquote>
<p>在心脏介入手术的荧视视频中对导线的精确分割对于计算机辅助导航任务至关重要。虽然深度学习的方法已经在导线分割中显示出较高的准确性和稳健性，但它们需要大规模的标注数据集来实现泛化，这突显了对大量标注数据的需要以提高模型性能。为了应对这一挑战，我们提出了基于分割引导的帧一致性视频扩散模型（SF-VD），用于生成大量标注的荧视视频，增强导线分割网络的训练数据。SF-VD通过独立建模场景分布和运动分布来利用标注不足的视频。它首先通过根据指定的输入掩膜生成带有导线的二维荧视图像来采样场景分布，然后通过逐步生成后续帧来采样运动分布，并通过帧一致性策略确保帧与帧之间的连贯性。分割引导机制进一步调整了导线的对比度，确保合成图像的可见性范围多样化。在荧视数据集上的评估证实了所生成视频的高质量，并显示出在导线分割方面的显著改善。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16050v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>在心脏介入手术荧光透视视频中准确分割导丝对于计算机辅助导航任务至关重要。为解决深度学习模型在导丝分割中对大量标注数据的依赖问题，我们提出了基于分割引导的帧一致性视频扩散模型（SF-VD）。该模型能生成大量标注的荧光透视视频，增强导丝分割网络的训练数据。SF-VD通过独立建模场景分布和运动分布，利用有限标注的视频样本进行训练。它首先根据指定的输入掩膜生成二维荧光透视图像来采样场景分布，然后通过逐步生成后续帧来采样运动分布，确保帧间一致性。分割引导机制进一步调整导丝对比度，确保合成图像的可见性多样性。在荧光数据集上的评估证明，生成视频的质量上乘，导丝分割的改进显著。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>导丝在心脏介入手术荧光透视视频中的准确分割对计算机辅助导航至关重要。</li>
<li>深度学习在导丝分割中虽表现出高准确性与稳健性，但需大量标注数据来提升模型性能。</li>
<li>提出SF-VD模型以生成大量标注的荧光透视视频，增强导丝分割网络的训练数据。</li>
<li>SF-VD通过独立建模场景分布和运动分布来处理有限标注视频样本。</li>
<li>模型通过生成二维荧光透视图像采样场景分布，并逐步生成后续帧来采样运动分布。</li>
<li>分割引导机制调整导丝对比度，确保合成图像的可见性多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b893f5d2e52178f2901ce6712e4992d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9a2f0b88c0190b7511a9244cdd8f011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6dfead2b062aa6a8f15edfc59eb3429.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d47112dbe22818737ff4b9362916e1d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Adversarial-Score-identity-Distillation-Rapidly-Surpassing-the-Teacher-in-One-Step"><a href="#Adversarial-Score-identity-Distillation-Rapidly-Surpassing-the-Teacher-in-One-Step" class="headerlink" title="Adversarial Score identity Distillation: Rapidly Surpassing the Teacher   in One Step"></a>Adversarial Score identity Distillation: Rapidly Surpassing the Teacher   in One Step</h2><p><strong>Authors:Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, Hai Huang</strong></p>
<p>Score identity Distillation (SiD) is a data-free method that has achieved SOTA performance in image generation by leveraging only a pretrained diffusion model, without requiring any training data. However, its ultimate performance is constrained by how accurate the pretrained model captures the true data scores at different stages of the diffusion process. In this paper, we introduce SiDA (SiD with Adversarial Loss), which not only enhances generation quality but also improves distillation efficiency by incorporating real images and adversarial loss. SiDA utilizes the encoder from the generator’s score network as a discriminator, allowing it to distinguish between real images and those generated by SiD. The adversarial loss is batch-normalized within each GPU and then combined with the original SiD loss. This integration effectively incorporates the average “fakeness” per GPU batch into the pixel-based SiD loss, enabling SiDA to distill a single-step generator. SiDA converges significantly faster than its predecessor when distilled from scratch, and swiftly improves upon the original model’s performance during fine-tuning from a pre-distilled SiD generator. This one-step adversarial distillation method establishes new benchmarks in generation performance when distilling EDM diffusion models, achieving FID scores of 1.110 on ImageNet 64x64. When distilling EDM2 models trained on ImageNet 512x512, our SiDA method surpasses even the largest teacher model, EDM2-XXL, which achieved an FID of 1.81 using classifier-free guidance (CFG) and 63 generation steps. In contrast, SiDA achieves FID scores of 2.156 for size XS, 1.669 for S, 1.488 for M, 1.413 for L, 1.379 for XL, and 1.366 for XXL, all without CFG and in a single generation step. These results highlight substantial improvements across all model sizes. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/mingyuanzhou/SiD/tree/sida">https://github.com/mingyuanzhou/SiD/tree/sida</a>. </p>
<blockquote>
<p>Score identity Distillation（SiD）是一种无需数据的方法，仅利用预训练的扩散模型，无需任何训练数据，在图像生成方面达到了 state-of-the-art（SOTA）的性能。然而，其最终性能受限于预训练模型在扩散过程的不同阶段捕捉真实数据准确度的程度。在本文中，我们引入了带有对抗性损失（Adversarial Loss）的SiD（SiDA）。SiDA不仅提高了生成质量，而且通过结合真实图像和对抗性损失提高了蒸馏效率。SiDA使用生成器评分网络中的编码器作为鉴别器，能够区分真实图像和SiD生成的图像。对抗性损失在每个GPU内进行批量归一化，然后与原始SiD损失相结合。这种结合有效地将每个GPU批次的平均“虚假性”纳入基于像素的SiD损失，使SiDA能够蒸馏单步生成器。当从零开始蒸馏时，SiDA比其前身收敛得更快，并且在预蒸馏的SiD生成器上进行微调时，能迅速提高原始模型的性能。这种一步对抗性蒸馏方法在为EDM扩散模型进行蒸馏时，在ImageNet 64x64上实现了FID分数为1.110的新基准。当对ImageNet 512x512训练的EDM2模型进行蒸馏时，我们的SiDA方法甚至超越了最大的教师模型EDM2-XXL，该模型使用无分类器引导（CFG）和63个生成步骤实现了FID分数为1.81。相比之下，SiDA实现了FID分数为2.156（XS大小）、1.669（S大小）、1.488（M大小）、1.413（L大小）、1.379（XL大小）和1.366（XXL大小），所有这些都没有使用CFG且仅在一个生成步骤中完成。这些结果突显了所有模型尺寸的显著改进。我们的代码位于 <a target="_blank" rel="noopener" href="https://github.com/mingyuanzhou/SiD/tree/sida%E3%80%82">https://github.com/mingyuanzhou/SiD/tree/sida。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14919v4">PDF</a> 10 pages (main text), 34 figures, and 10 tables</p>
<p><strong>摘要</strong></p>
<p>SiDA（带有对抗损失的分数身份蒸馏法）是一种结合真实图像与对抗损失的技术，提高了图像生成的品质与蒸馏效率。它通过引入对抗损失并融入原有SiD（分数身份蒸馏法）损失，有效结合平均“假度”与像素级SiD损失，实现了单步生成器的蒸馏。SiDA相较于前代技术，从空白状态开始蒸馏时收敛速度更快，并且在预蒸馏SiD生成器上进行微调时迅速提升性能。这一单步对抗蒸馏方法打破了蒸馏扩散模型的生成性能记录，如在ImageNet 64x64上实现FID得分1.110。在蒸馏训练于ImageNet 512x512的EDM2模型时，SiDA方法甚至超越了使用无分类指导（CFG）和63步生成的最大的教师模型EDM2-XXL（FID 1.81）。相反，SiDA在所有模型尺寸上都实现了显著改进，无需CFG即可在单一生成步骤中达到FID得分：XS为2.156、S为1.669、M为1.488、L为1.413、XL为1.379以及XXL为1.366。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>SiDA是首个结合真实图像和对抗损失以提高图像生成质量和蒸馏效率的方法。</li>
<li>SiDA利用生成器分数网络中的编码器作为判别器，能够区分真实图像和由SiD生成的图像。</li>
<li>对抗损失与SiD损失的融合，有效融入了GPU批处理中的平均“假度”，实现了单步生成器的蒸馏。</li>
<li>SiDA在多种模型尺寸上实现了显著的性能提升，无需复杂的分类指导（CFG）和多个生成步骤。</li>
<li>SiDA在ImageNet 64x64上达到了FID得分1.110的新性能基准。</li>
<li>对于更大尺寸的ImageNet 512x512模型，SiDA超越了最先进模型的性能表现。</li>
<li>SiDA方法已经开源，并且具有广泛的应用前景，尤其是在提高图像生成质量和效率方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b44a083cfb44565b8cfda8cf62fd545.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18b516f3bbead8b8d06a21305cf6298e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="EraseDraw-Learning-to-Draw-Step-by-Step-via-Erasing-Objects-from-Images"><a href="#EraseDraw-Learning-to-Draw-Step-by-Step-via-Erasing-Objects-from-Images" class="headerlink" title="EraseDraw: Learning to Draw Step-by-Step via Erasing Objects from Images"></a>EraseDraw: Learning to Draw Step-by-Step via Erasing Objects from Images</h2><p><strong>Authors:Alper Canberk, Maksym Bondarenko, Ege Ozguroglu, Ruoshi Liu, Carl Vondrick</strong></p>
<p>Creative processes such as painting often involve creating different components of an image one by one. Can we build a computational model to perform this task? Prior works often fail by making global changes to the image, inserting objects in unrealistic spatial locations, and generating inaccurate lighting details. We observe that while state-of-the-art models perform poorly on object insertion, they can remove objects and erase the background in natural images very well. Inverting the direction of object removal, we obtain high-quality data for learning to insert objects that are spatially, physically, and optically consistent with the surroundings. With this scalable automatic data generation pipeline, we can create a dataset for learning object insertion, which is used to train our proposed text conditioned diffusion model. Qualitative and quantitative experiments have shown that our model achieves state-of-the-art results in object insertion, particularly for in-the-wild images. We show compelling results on diverse insertion prompts and images across various domains.In addition, we automate iterative insertion by combining our insertion model with beam search guided by CLIP. </p>
<blockquote>
<p>绘画等创造性过程通常涉及逐个创建图像的不同组成部分。我们可以建立一个计算模型来完成这项任务吗？先前的工作往往通过在图像上进行全局更改、在不现实的空间位置插入对象以及生成不准确的光照细节而失败。我们发现，虽然最先进的模型在对象插入方面表现不佳，但它们可以很好地从自然图像中移除对象和背景。通过反转对象移除的方向，我们可以获得高质量的数据来学习插入与周围环境在空间、物理和光学上一致的物体。通过这种可扩展的自动数据生成管道，我们可以创建一个用于学习对象插入的数据集，该数据集用于训练我们提出的文本条件扩散模型。定性和定量实验表明，我们的模型在对象插入方面达到了最新水平，特别是在野外图像方面。我们在各种插入提示和跨域图像上展示了令人信服的结果。此外，我们将插入模型与CLIP引导的束搜索相结合，实现了自动迭代插入。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00522v2">PDF</a> </p>
<p><strong>Summary</strong><br>创作过程如绘画需逐个构建图像的不同组成部分。能否建立计算模型完成此任务？先前的工作常因全局更改图像、插入不现实的物体位置及生成不准确的光照细节而失败。我们发现虽然最新模型在物体插入方面表现不佳，但在自然图像的背景去除方面非常出色。通过反转物体移除的方向，我们获得了高质量数据，学习插入与周围环境在空间、物理和光学上一致的物体。利用这一可扩展的自动数据生成管道，我们创建了用于训练文本条件扩散模型的数据集。实验证明，我们的模型在物体插入方面达到最新水平，特别是在野外图像方面。我们展示了各种插入提示和跨域图像的令人信服的结果。此外，我们将插入模型与CLIP引导的束搜索相结合，实现了自动迭代插入。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算模型可模拟创作过程如绘画中的物体逐个构建。</li>
<li>先前模型在插入物体时表现欠佳，存在空间位置不真实、光照细节不准确等问题。</li>
<li>最新模型在去除背景方面表现优秀，通过反转这一方向获得高质量数据用于学习物体插入。</li>
<li>建立了自动数据生成管道以训练文本条件扩散模型。</li>
<li>模型在物体插入方面达到最新水平，特别是在处理野外图像时效果显著。</li>
<li>模型可处理多种插入提示和跨域图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00522">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1ead218b7a67780339b0cc9755cc860e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57f3f41ffec48a1b0fdf51b016d3088b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89eb5e8f8981feaa95bb5d1a89261fbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba7f1d3af760a7d255ca253ba19981c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab4599751b95b360e59b67b15ec7e5f3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3ee184809f97812831c45a8673743f23.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-26  Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors   Diverse-Resolution Training Outperforms Interpolation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-718132e6f5bbf9d8ece72adb831f3ee7.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2024-12-26  Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs   Algorithms
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16042k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
