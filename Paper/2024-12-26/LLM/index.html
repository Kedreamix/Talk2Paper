<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-26  Decentralized Intelligence in GameFi Embodied AI Agents and the   Convergence of DeFi and Virtual Ecosystems">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-58bc6ec476c7e5ccd33ef106d5ae9e69.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-26-æ›´æ–°"><a href="#2024-12-26-æ›´æ–°" class="headerlink" title="2024-12-26 æ›´æ–°"></a>2024-12-26 æ›´æ–°</h1><h2 id="Decentralized-Intelligence-in-GameFi-Embodied-AI-Agents-and-the-Convergence-of-DeFi-and-Virtual-Ecosystems"><a href="#Decentralized-Intelligence-in-GameFi-Embodied-AI-Agents-and-the-Convergence-of-DeFi-and-Virtual-Ecosystems" class="headerlink" title="Decentralized Intelligence in GameFi: Embodied AI Agents and the   Convergence of DeFi and Virtual Ecosystems"></a>Decentralized Intelligence in GameFi: Embodied AI Agents and the   Convergence of DeFi and Virtual Ecosystems</h2><p><strong>Authors:Fernando Jia, Jade Zheng, Florence Li</strong></p>
<p>In the rapidly evolving landscape of GameFi, a fusion of gaming and decentralized finance (DeFi), there exists a critical need to enhance player engagement and economic interaction within gaming ecosystems. Our GameFi ecosystem aims to fundamentally transform this landscape by integrating advanced embodied AI agents into GameFi platforms. These AI agents, developed using cutting-edge large language models (LLMs), such as GPT-4 and Claude AI, are capable of proactive, adaptive, and contextually rich interactions with players. By going beyond traditional scripted responses, these agents become integral participants in the gameâ€™s narrative and economic systems, directly influencing player strategies and in-game economies. We address the limitations of current GameFi platforms, which often lack immersive AI interactions and mechanisms for community engagement or creator monetization. Through the deep integration of AI agents with blockchain technology, we establish a consensus-driven, decentralized GameFi ecosystem. This ecosystem empowers creators to monetize their contributions and fosters democratic collaboration among players and creators. Furthermore, by embedding DeFi mechanisms into the gaming experience, we enhance economic participation and provide new opportunities for financial interactions within the game. Our approach enhances player immersion and retention and advances the GameFi ecosystem by bridging traditional gaming with Web3 technologies. By integrating sophisticated AI and DeFi elements, we contribute to the development of more engaging, economically robust, and community-centric gaming environments. This project represents a significant advancement in the state-of-the-art in GameFi, offering insights and methodologies that can be applied throughout the gaming industry. </p>
<blockquote>
<p>åœ¨æ¸¸æˆä¸å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰çš„èåˆä½“â€”â€”GameFiè¿…é€Ÿå‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œå¢å¼ºç©å®¶åœ¨æ¸¸æˆç”Ÿæ€ç³»ç»Ÿä¸­çš„å‚ä¸åº¦å’Œç»æµäº’åŠ¨æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚æˆ‘ä»¬çš„GameFiç”Ÿæ€ç³»ç»Ÿæ—¨åœ¨é€šè¿‡æŠŠå…ˆè¿›çš„åµŒå…¥å¼äººå·¥æ™ºèƒ½ä»£ç†ï¼ˆAIï¼‰é›†æˆåˆ°GameFiå¹³å°ï¼Œä»æ ¹æœ¬ä¸Šæ”¹å˜è¿™ä¸€æ ¼å±€ã€‚è¿™äº›åˆ©ç”¨å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4å’ŒClaude AIï¼‰å¼€å‘çš„äººå·¥æ™ºèƒ½ä»£ç†ï¼Œå…·å¤‡ä¸ç©å®¶è¿›è¡Œä¸»åŠ¨ã€è‡ªé€‚åº”å’Œè¯­å¢ƒä¸°å¯Œçš„äº¤äº’çš„èƒ½åŠ›ã€‚è¿™äº›ä»£ç†è¶…è¶Šäº†ä¼ ç»Ÿçš„è„šæœ¬å“åº”ï¼Œæˆä¸ºæ¸¸æˆå™äº‹å’Œç»æµç³»ç»Ÿçš„æ ¸å¿ƒå‚ä¸è€…ï¼Œç›´æ¥å½±å“ç©å®¶çš„ç­–ç•¥å’Œæ¸¸æˆå†…ç»æµã€‚æˆ‘ä»¬è§£å†³äº†å½“å‰GameFiå¹³å°çš„å±€é™æ€§ï¼Œè¿™äº›å¹³å°é€šå¸¸ç¼ºä¹æ²‰æµ¸å¼çš„äººå·¥æ™ºèƒ½äº¤äº’å’Œç¤¾åŒºå‚ä¸æœºåˆ¶åˆ›ä½œè€…å˜ç°çš„æœºåˆ¶ã€‚é€šè¿‡äººå·¥æ™ºèƒ½ä»£ç†ä¸åŒºå—é“¾æŠ€æœ¯çš„æ·±åº¦èåˆï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå»ä¸­å¿ƒåŒ–çš„GameFiç”Ÿæ€ç³»ç»Ÿï¼Œä»¥å…±è¯†é©±åŠ¨ã€‚è¿™ä¸ªç”Ÿæ€ç³»ç»Ÿè®©åˆ›ä½œè€…èƒ½å¤Ÿå˜ç°ä»–ä»¬çš„è´¡çŒ®ï¼Œå¹¶ä¿ƒè¿›ç©å®¶å’Œåˆ›ä½œè€…ä¹‹é—´çš„æ°‘ä¸»åˆä½œã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†DeFiæœºåˆ¶åµŒå…¥æ¸¸æˆä½“éªŒï¼Œæˆ‘ä»¬å¢å¼ºäº†ç»æµå‚ä¸æ€§ï¼Œä¸ºæ¸¸æˆå†…çš„é‡‘èäº’åŠ¨æä¾›äº†æ–°çš„æœºä¼šã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ç©å®¶çš„æ²‰æµ¸æ„Ÿå’Œç•™å­˜ç‡ï¼Œå¹¶é€šè¿‡å¼¥åˆä¼ ç»Ÿæ¸¸æˆä¸Web3æŠ€æœ¯çš„é¸¿æ²Ÿï¼Œæ¨åŠ¨äº†GameFiç”Ÿæ€ç³»ç»Ÿçš„å‘å±•ã€‚é€šè¿‡é›†æˆå¤æ‚çš„äººå·¥æ™ºèƒ½å’ŒDeFiå…ƒç´ ï¼Œæˆ‘ä»¬è‡´åŠ›äºå¼€å‘æ›´å…·å¸å¼•åŠ›ã€ç»æµç¨³å¥å’Œä»¥ç¤¾åŒºä¸ºä¸­å¿ƒçš„æ¸¸æˆç¯å¢ƒã€‚è¿™ä¸ªé¡¹ç›®ä»£è¡¨äº†GameFié¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œæä¾›äº†å¯åº”ç”¨äºæ•´ä¸ªæ¸¸æˆè¡Œä¸šçš„è§è§£å’Œæ–¹æ³•è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18601v1">PDF</a> 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GameFiç”Ÿæ€ç³»ç»Ÿå¦‚ä½•é€šè¿‡é›†æˆå…ˆè¿›çš„AIä»£ç†æ¥æ ¹æœ¬æ€§åœ°æ”¹å˜æ¸¸æˆä¸å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰çš„èåˆç°çŠ¶ã€‚è¿™äº›AIä»£ç†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ï¼Œå¦‚GPT-4å’ŒClaude AIå¼€å‘ï¼Œèƒ½å¤Ÿä¸ç©å®¶è¿›è¡Œç§¯æä¸»åŠ¨ã€é€‚åº”æ€§å¼ºå’Œä¸Šä¸‹æ–‡ä¸°å¯Œçš„äº¤äº’ã€‚AIçš„æ·±åº¦é›†æˆå°†å¢å¼ºç©å®¶æ²‰æµ¸æ„Ÿå’Œç•™å­˜ç‡ï¼ŒåŒæ—¶é€šè¿‡åŒºå—é“¾æŠ€æœ¯å»ºç«‹å…±è¯†é©±åŠ¨çš„ç”Ÿæ€ç³»ç»Ÿï¼Œä¿ƒè¿›åˆ›ä½œè€…è´¡çŒ®è´§å¸åŒ–å’Œç©å®¶ä¸åˆ›ä½œè€…ä¹‹é—´çš„æ°‘ä¸»åˆä½œã€‚è¯¥é¡¹ç›®å°†ä¼ ç»Ÿæ¸¸æˆä¸Web3æŠ€æœ¯ç›¸ç»“åˆï¼Œä¸ºæ¸¸æˆè¡Œä¸šæä¾›äº†å…ˆè¿›çš„å‘å±•æ€è·¯å’Œä¸°å¯Œçš„å®è·µç»éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GameFiç”Ÿæ€ç³»ç»Ÿè‡´åŠ›äºé€šè¿‡é›†æˆå…ˆè¿›çš„AIä»£ç†æ¥æå‡æ¸¸æˆç©å®¶ä¸é‡‘èç³»ç»Ÿçš„äº’åŠ¨ä½“éªŒã€‚</li>
<li>AIä»£ç†é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œèƒ½è¿›è¡Œä¸°å¯Œã€è‡ªé€‚åº”çš„äº¤äº’ï¼Œå½±å“ç©å®¶ç­–ç•¥å’Œæ¸¸æˆç»æµã€‚</li>
<li>åŒºå—é“¾æŠ€æœ¯ä¸AIçš„æ·±åº¦é›†æˆå»ºç«‹äº†å»ä¸­å¿ƒåŒ–çš„GameFiç”Ÿæ€ç³»ç»Ÿï¼Œæ”¯æŒåˆ›ä½œè€…è´¡çŒ®è´§å¸åŒ–ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡åµŒå…¥DeFiæœºåˆ¶å¢å¼ºäº†ç»æµå‚ä¸åº¦ï¼Œä¸ºæ¸¸æˆå†…é‡‘èäº’åŠ¨æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>GameFié¡¹ç›®èåˆä¼ ç»Ÿæ¸¸æˆä¸Web3æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†ç©å®¶æ²‰æµ¸æ„Ÿå’Œç•™å­˜ç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65d0d7ef09145e79d90dc7652649cc5e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Zero-resource-Speech-Translation-and-Recognition-with-LLMs"><a href="#Zero-resource-Speech-Translation-and-Recognition-with-LLMs" class="headerlink" title="Zero-resource Speech Translation and Recognition with LLMs"></a>Zero-resource Speech Translation and Recognition with LLMs</h2><p><strong>Authors:Karel Mundnich, Xing Niu, Prashant Mathur, Srikanth Ronanki, Brady Houston, Veera Raghavendra Elluru, Nilaksh Das, Zejiang Hou, Goeric Huybrechts, Anshu Bhatia, Daniel Garcia-Romero, Kyu J. Han, Katrin Kirchhoff</strong></p>
<p>Despite recent advancements in speech processing, zero-resource speech translation (ST) and automatic speech recognition (ASR) remain challenging problems. In this work, we propose to leverage a multilingual Large Language Model (LLM) to perform ST and ASR in languages for which the model has never seen paired audio-text data. We achieve this by using a pre-trained multilingual speech encoder, a multilingual LLM, and a lightweight adaptation module that maps the audio representations to the token embedding space of the LLM. We perform several experiments both in ST and ASR to understand how to best train the model and what data has the most impact on performance in previously unseen languages. In ST, our best model is capable to achieve BLEU scores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we achieve WERs of up to 28.2%. We finally show that the performance of our system is bounded by the ability of the LLM to output text in the desired language. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘è¯­éŸ³å¤„ç†é¢†åŸŸå–å¾—äº†è¿›å±•ï¼Œä½†é›¶èµ„æºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»ç„¶å­˜åœ¨é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ‰§è¡ŒSTå’ŒASRï¼Œé’ˆå¯¹é‚£äº›æ¨¡å‹ä»æœªè§è¿‡é…å¯¹è¯­éŸ³æ–‡æœ¬æ•°æ®çš„è¯­è¨€ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ã€å¤šè¯­è¨€LLMå’Œä¸€ä¸ªè½»é‡çº§é€‚é…æ¨¡å—æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥æ¨¡å—å°†éŸ³é¢‘è¡¨ç¤ºæ˜ å°„åˆ°LLMçš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´ã€‚æˆ‘ä»¬åœ¨STå’ŒASRä¸­éƒ½è¿›è¡Œäº†å¤šæ¬¡å®éªŒï¼Œä»¥äº†è§£å¦‚ä½•æœ€ä½³åœ°è®­ç»ƒæ¨¡å‹ä»¥åŠå“ªäº›æ•°æ®åœ¨æœªè§è¿‡çš„è¯­è¨€ä¸­å¯¹æ€§èƒ½çš„å½±å“æœ€å¤§ã€‚åœ¨STä¸­ï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹èƒ½å¤Ÿåœ¨CoVoST2ä¸­è¾¾åˆ°è¶…è¿‡23çš„BLEUåˆ†æ•°ï¼Œç”¨äºä¸¤ç§ä¹‹å‰æœªè§è¿‡çš„è¯­è¨€ï¼›è€Œåœ¨ASRä¸­ï¼Œæˆ‘ä»¬è¾¾åˆ°äº†é«˜è¾¾28.2%çš„WERã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬ç³»ç»Ÿçš„æ€§èƒ½å—é™äºLLMè¾“å‡ºæ‰€éœ€è¯­è¨€æ–‡æœ¬çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18566v1">PDF</a> ICASSP 2025, 5 pages, 2 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé›¶èµ„æºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ã€å¤šè¯­è¨€LLMå’Œè½»é‡çº§é€‚é…æ¨¡å—ï¼Œå°†éŸ³é¢‘è¡¨ç¤ºæ˜ å°„åˆ°LLMçš„ä»¤ç‰ŒåµŒå…¥ç©ºé—´ï¼Œå®ç°åœ¨æœªè§é…å¯¹éŸ³é¢‘æ–‡æœ¬æ•°æ®çš„è¯­è¨€ä¸­è¿›è¡ŒSTå’ŒASRã€‚å®éªŒè¡¨æ˜ï¼Œæœ€ä½³æ¨¡å‹çš„ç¿»è¯‘æ€§èƒ½è¾¾åˆ°CoVoST2çš„BLEUåˆ†æ•°è¶…è¿‡23ï¼ŒASRçš„WERè¾¾åˆ°28.2%ã€‚ç³»ç»Ÿæ€§èƒ½å—é™äºLLMè¾“å‡ºæ‰€éœ€è¯­è¨€æ–‡æœ¬çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³é›¶èµ„æºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³ç¼–ç å™¨ã€å¤šè¯­è¨€LLMå’Œè½»é‡çº§é€‚é…æ¨¡å—å®ç°æœªè§é…å¯¹éŸ³é¢‘æ–‡æœ¬æ•°æ®çš„è¯­è¨€ä¸­çš„STå’ŒASRã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„ç¿»è¯‘æ€§èƒ½è¾¾åˆ°CoVoST2çš„BLEUåˆ†æ•°è¶…è¿‡23ã€‚</li>
<li>ASRçš„WERè¾¾åˆ°28.2%ã€‚</li>
<li>ç³»ç»Ÿæ€§èƒ½å—é™äºLLMè¾“å‡ºæ‰€éœ€è¯­è¨€æ–‡æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒæ¨¡å‹å’Œé€‰æ‹©æ•°æ®å¯¹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b357107fcd6a2cde520d689ab7b9bd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43ae7c12668f3d8fae80249694d410de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad0e1040b8bc8cb5a6123cd4919ddfcf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2381c4b0d90bf7f46d411b0fe79095ff.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Distilling-Fine-grained-Sentiment-Understanding-from-Large-Language-Models"><a href="#Distilling-Fine-grained-Sentiment-Understanding-from-Large-Language-Models" class="headerlink" title="Distilling Fine-grained Sentiment Understanding from Large Language   Models"></a>Distilling Fine-grained Sentiment Understanding from Large Language   Models</h2><p><strong>Authors:Yice Zhang, Guangyu Xie, Hongling Xu, Kaiheng Hou, Jianzhu Bao, Qianlong Wang, Shiwei Chen, Ruifeng Xu</strong></p>
<p>Fine-grained sentiment analysis (FSA) aims to extract and summarize user opinions from vast opinionated text. Recent studies demonstrate that large language models (LLMs) possess exceptional sentiment understanding capabilities. However, directly deploying LLMs for FSA applications incurs high inference costs. Therefore, this paper investigates the distillation of fine-grained sentiment understanding from LLMs into small language models (SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews and then utilize the generated content to pretrain SLMs. Additionally, we develop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive experiments on this benchmark reveal that: (1) distillation significantly enhances the performance of SLMs in FSA tasks, achieving a 6.00% improvement in $F_1$-score, and the distilled model can outperform Llama-2-7b with only 220M parameters; (2) distillation equips SLMs with excellent zero-shot sentiment classification capabilities, enabling them to match or even exceed their teacher models. These results suggest that distillation from LLMs is a highly promising direction for FSA. We will release our code, data, and pretrained model weights at \url{<a target="_blank" rel="noopener" href="https://github.com/HITSZ-HLT/FSA-Distillation%7D">https://github.com/HITSZ-HLT/FSA-Distillation}</a>. </p>
<blockquote>
<p>ç²¾ç»†ç²’åº¦æƒ…æ„Ÿåˆ†æï¼ˆFSAï¼‰æ—¨åœ¨ä»å¤§é‡å«æœ‰ä¸»è§‚æ„è§çš„æ–‡æœ¬ä¸­æå–å¹¶æ€»ç»“ç”¨æˆ·æ„è§ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å‡ºè‰²çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥å°†LLMç”¨äºFSAåº”ç”¨ä¼šäº§ç”Ÿè¾ƒé«˜çš„æ¨ç†æˆæœ¬ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ¢è®¨äº†ä»LLMä¸­æç‚¼ç²¾ç»†ç²’åº¦æƒ…æ„Ÿç†è§£å¹¶å°†å…¶è½¬ç§»åˆ°å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬æç¤ºLLMæ£€æŸ¥å¹¶è§£é‡Šç»™å®šè¯„è®ºçš„æƒ…æ„Ÿï¼Œç„¶ååˆ©ç”¨ç”Ÿæˆçš„å†…å®¹å¯¹SLMè¿›è¡Œé¢„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„FSAåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°SLMå’ŒLLMçš„æ€§èƒ½ã€‚åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼šï¼ˆ1ï¼‰è’¸é¦æŠ€æœ¯æ˜¾è‘—æé«˜äº†SLMåœ¨FSAä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼ŒF1åˆ†æ•°æé«˜äº†6.00%ï¼Œå¹¶ä¸”è’¸é¦åçš„æ¨¡å‹ä»…ä½¿ç”¨2.2äº¿ä¸ªå‚æ•°å°±èƒ½è¶…è¶ŠLlama-2-7bï¼›ï¼ˆ2ï¼‰è’¸é¦èµ‹äºˆäº†SLMå‡ºè‰²çš„é›¶æ ·æœ¬æƒ…æ„Ÿåˆ†ç±»èƒ½åŠ›ï¼Œä½¿å®ƒä»¬èƒ½å¤ŸåŒ¹é…ç”šè‡³è¶…è¶Šå…¶æ•™å¸ˆæ¨¡å‹ï¼ˆå³LLMï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä»LLMä¸­è¿›è¡Œè’¸é¦æ˜¯FSAçš„ä¸€ä¸ªéå¸¸æœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/HITSZ-HLT/FSA-Distillation">https://github.com/HITSZ-HLT/FSA-Distillation</a>å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18552v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ç ”ç©¶å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç²¾ç»†æƒ…æ„Ÿåˆ†æï¼ˆFSAï¼‰ä»»åŠ¡ä¸­çš„é«˜æˆæœ¬é—®é¢˜ï¼Œå¹¶æå‡ºé‡‡ç”¨çŸ¥è¯†è’¸é¦æ–¹æ³•ä»LLMä¸­æå–ç²¾ç»†æƒ…æ„Ÿç†è§£èƒ½åŠ›å¹¶å°†å…¶ä¼ é€’ç»™å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè’¸é¦æŠ€æœ¯åœ¨æé«˜SLMåœ¨FSAä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸Šæ•ˆæœæ˜¾è‘—ï¼Œå®ç°$F_1$åˆ†æ•°æå‡6%ï¼Œè’¸é¦åçš„æ¨¡å‹ä»¥ä»…22äº¿å‚æ•°ä¾¿èƒ½è¶…è¶ŠLlama-2-7bæ¨¡å‹ã€‚æ­¤å¤–ï¼Œè’¸é¦æŠ€æœ¯èµ‹äºˆSLMå‡ºè‰²çš„é›¶æ ·æœ¬æƒ…æ„Ÿåˆ†ç±»èƒ½åŠ›ï¼Œèƒ½å¤ŸåŒ¹é…ç”šè‡³è¶…è¶Šå…¶æ•™å¸ˆæ¨¡å‹ã€‚ç ”ç©¶è¯å®è’¸é¦æŠ€æœ¯åœ¨ç²¾ç»†æƒ…æ„Ÿåˆ†æé¢†åŸŸå…·æœ‰å¹¿é˜”å‰æ™¯ã€‚ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€äºæŒ‡å®šç½‘å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†è’¸é¦æŠ€æœ¯æ˜¾è‘—æé«˜äº†å°å‹è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œæé«˜äº†æ¨¡å‹å¤„ç†çš„æ•ˆç‡å’Œæ•ˆæœã€‚</li>
<li>é‡‡ç”¨è’¸é¦åçš„æ¨¡å‹èƒ½å¤§å¹…åº¦æå‡SLMæ¨¡å‹çš„é›¶æ ·æœ¬æƒ…æ„Ÿåˆ†ç±»èƒ½åŠ›ï¼Œæ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨å„ç§å¤æ‚æƒ…æ„Ÿåˆ†æåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼Œè’¸é¦åçš„æ¨¡å‹æ€§èƒ½è¶…è¶Šäº†è®¸å¤šç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯æ˜äº†å°å‹è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†æƒ…æ„Ÿåˆ†æé¢†åŸŸçš„æ½œåŠ›ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æŠ€æœ¯æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­çš„é«˜æˆæœ¬é—®é¢˜ï¼Œæœ‰åŠ©äºé™ä½å®é™…åº”ç”¨ä¸­çš„æˆæœ¬è´Ÿæ‹…ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ä»…æä¾›äº†å®éªŒç»“æœï¼Œè¿˜æä¾›äº†å¼€æºçš„ä»£ç å’Œæ•°æ®é›†ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†æ–¹ä¾¿ã€‚</li>
<li>è¯¥ç ”ç©¶æå‡ºçš„è’¸é¦æŠ€æœ¯å¯ä»¥åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¯¹å¤§è§„æ¨¡æ•°æ®è¿›è¡Œç²¾ç»†æƒ…æ„Ÿåˆ†æçš„åº”ç”¨åœºæ™¯ä¸­æœ‰ç€å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-78c4aeb298d138ee892a40b04e6d5f15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8c5f87485e5631b1b3afd56baba9b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02aeb35f8cd0d2696bd3e9515f385470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f5ed616376cd5d39c675f5e13f205fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de7ac02decc4caee76d506e0ef02d172.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b744f821d5c1322eeb979ad0f339b1d0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Token-Budget-Aware-LLM-Reasoning"><a href="#Token-Budget-Aware-LLM-Reasoning" class="headerlink" title="Token-Budget-Aware LLM Reasoning"></a>Token-Budget-Aware LLM Reasoning</h2><p><strong>Authors:Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, Zhenting Wang</strong></p>
<p>Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: <a target="_blank" rel="noopener" href="https://github.com/GeniusHTX/TALE">https://github.com/GeniusHTX/TALE</a>. </p>
<blockquote>
<p>æ¨ç†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹¿æ³›ä»»åŠ¡ä¸­çš„å“è¶Šè¡¨ç°è‡³å…³é‡è¦ã€‚è™½ç„¶åƒæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ç­‰æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥å¢å¼ºLLMçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬è¿˜ä¼šäº§ç”Ÿæ˜¾è‘—çš„ä»¤ç‰Œä½¿ç”¨å¼€é”€ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€‚æˆ‘ä»¬å‘ç°å½“å‰LLMçš„æ¨ç†è¿‡ç¨‹ä¸å¿…è¦åœ°å†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©åœ¨å®é™…å‹ç¼©æ•ˆæœä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä»¤ç‰Œé¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°è®¡ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°è®¡çš„ä»¤ç‰Œé¢„ç®—æ¥æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°é™ä½äº†æ€ç»´é“¾æ¨ç†ä¸­çš„ä»¤ç‰Œæˆæœ¬ï¼Œå¹¶ä¸”æ€§èƒ½ä¸‹é™å¹…åº¦è¾ƒå°ï¼Œä¸ºåœ¨LLMæ¨ç†ä¸­å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§æä¾›äº†ä¸€ç§å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚ç›¸å…³ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/GeniusHTX/TALE%E3%80%82">https://github.com/GeniusHTX/TALEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18547v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†è¿‡ç¨‹å¯¹å®Œæˆå„ç§ä»»åŠ¡è‡³å…³é‡è¦ã€‚è™½ç„¶é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç­‰æ¨ç†æ–¹æ³•é€šè¿‡åˆ†è§£é—®é¢˜ä¸ºä¸­é—´æ­¥éª¤å¢å¼ºäº†LLMçš„æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—ç¬¦å·ä½¿ç”¨è´Ÿæ‹…ï¼Œå¢åŠ äº†æˆæœ¬ã€‚å½“å‰LLMçš„æ¨ç†è¿‡ç¨‹å†—é•¿ä¸”ä¸å¿…è¦çš„é•¿åº¦ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…æ‹¬åˆç†çš„ç¬¦å·é¢„ç®—æ¥å‹ç¼©ï¼Œä½†ç¬¦å·é¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å› æ­¤ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¬¦å·é¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°è®¡ä¸åŒé—®é¢˜çš„ç¬¦å·é¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°è®¡çš„ç¬¦å·é¢„ç®—æ¥æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘CoTæ¨ç†ä¸­çš„ç¬¦å·æˆæœ¬æ—¶ï¼Œæ€§èƒ½æŸå¤±è¾ƒå°ï¼Œä¸ºå¹³è¡¡LLMæ¨ç†ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨ç†å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®Œæˆå„ç§ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰LLMä½¿ç”¨çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç­‰æ¨ç†æ–¹æ³•ä¼šå¢åŠ ç¬¦å·ä½¿ç”¨è´Ÿæ‹…å’Œæˆæœ¬ã€‚</li>
<li>å½“å‰LLMçš„æ¨ç†è¿‡ç¨‹å¯ä»¥å‹ç¼©ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ç¬¦å·é¢„ç®—æ¥å®ç°ã€‚</li>
<li>ç¬¦å·é¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºç¬¦å·é¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶èƒ½åŠ¨æ€ä¼°è®¡ä¸åŒé—®é¢˜çš„ç¬¦å·é¢„ç®—ï¼Œå¹¶æ®æ­¤æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å‡å°‘CoTæ¨ç†ä¸­çš„ç¬¦å·æˆæœ¬æ—¶ï¼Œæ€§èƒ½æŸå¤±è¾ƒå°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18547">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-654e27dd8b4204ff6a51676d0f005da0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-995d9f2b247126036c14104df3782a10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a9db941a98ecb6269087d9c612fd561.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53f9226ab9b4eaddb57819ef4c507f04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d639e74c25cb0f5089b1565646f32116.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00f5a01030fa8994fb71cb5e4aaee62c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bec4e66468d29547084736404f4c7f22.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PLD-Tree-Persistent-Laplacian-Decision-Tree-for-Protein-Protein-Binding-Free-Energy-Prediction"><a href="#PLD-Tree-Persistent-Laplacian-Decision-Tree-for-Protein-Protein-Binding-Free-Energy-Prediction" class="headerlink" title="PLD-Tree: Persistent Laplacian Decision Tree for Protein-Protein Binding   Free Energy Prediction"></a>PLD-Tree: Persistent Laplacian Decision Tree for Protein-Protein Binding   Free Energy Prediction</h2><p><strong>Authors:Xingjian Xu, Jiahui Chen, Chunmei Wang</strong></p>
<p>Recent advances in topology-based modeling have accelerated progress in physical modeling and molecular studies, including applications to protein-ligand binding affinity. In this work, we introduce the Persistent Laplacian Decision Tree (PLD-Tree), a novel method designed to address the challenging task of predicting protein-protein interaction (PPI) affinities. PLD-Tree focuses on protein chains at binding interfaces and employs the persistent Laplacian to capture topological invariants reflecting critical inter-protein interactions. These topological descriptors, derived from persistent homology, are further enhanced by incorporating evolutionary scale modeling (ESM) from a large language model to integrate sequence-based information. We validate PLD-Tree on two benchmark datasets-PDBbind V2020 and SKEMPI v2 demonstrating a correlation coefficient ($R_p$) of 0.83 under the sophisticated leave-out-protein-out cross-validation. Notably, our approach outperforms all reported state-of-the-art methods on these datasets. These results underscore the power of integrating machine learning techniques with topology-based descriptors for molecular docking and virtual screening, providing a robust and accurate framework for predicting protein-protein binding affinities. </p>
<blockquote>
<p>åŸºäºæ‹“æ‰‘å»ºæ¨¡çš„æœ€æ–°è¿›å±•åŠ é€Ÿäº†ç‰©ç†å»ºæ¨¡å’Œåˆ†å­ç ”ç©¶ï¼ˆåŒ…æ‹¬åœ¨è›‹ç™½è´¨-é…ä½“ç»“åˆäº²å’ŒåŠ›ä¸­çš„åº”ç”¨ï¼‰çš„è¿›æ­¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æŒä¹…æ‹‰æ™®æ‹‰æ–¯å†³ç­–æ ‘ï¼ˆPLD-Treeï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é¢„æµ‹è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰äº²å’ŒåŠ›è¿™ä¸€å…·æœ‰æŒ‘æˆ˜çš„ä»»åŠ¡è€Œè®¾è®¡çš„æ–°æ–¹æ³•ã€‚PLD-Treeå…³æ³¨ç»“åˆç•Œé¢ä¸Šçš„è›‹ç™½è´¨é“¾ï¼Œå¹¶åˆ©ç”¨æŒä¹…æ‹‰æ™®æ‹‰æ–¯æ¥æ•æ‰åæ˜ å…³é”®è›‹ç™½è´¨é—´ç›¸äº’ä½œç”¨çš„æ‹“æ‰‘ä¸å˜é‡ã€‚è¿™äº›æ‹“æ‰‘æè¿°ç¬¦æ¥æºäºæŒä¹…åŒæºæ€§ï¼Œé€šè¿‡èå…¥å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›åŒ–å°ºåº¦å»ºæ¨¡ï¼ˆESMï¼‰ä»¥å¢å¼ºåºåˆ—ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†PDBbind V2020å’ŒSKEMPI v2ä¸Šå¯¹PLD-Treeè¿›è¡Œäº†éªŒè¯ï¼Œå±•ç¤ºå‡ºåœ¨å¤æ‚çš„ç•™ä¸€è›‹ç™½è´¨äº¤å‰éªŒè¯ä¸‹çš„ç›¸å…³æ€§ç³»æ•°ï¼ˆRpï¼‰ä¸º0.83ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿™ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæ‰€æœ‰æŠ¥å‘Šçš„æœ€æ–°æ–¹æ³•ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å°†æœºå™¨å­¦ä¹ æ–¹æ³•ä¸æ‹“æ‰‘æè¿°ç¬¦ç»“åˆåœ¨åˆ†å­å¯¹æ¥å’Œè™šæ‹Ÿç­›é€‰ä¸­çš„åŠ›é‡ï¼Œä¸ºé¢„æµ‹è›‹ç™½è´¨-è›‹ç™½è´¨ç»“åˆäº²å’ŒåŠ›æä¾›äº†ç¨³å¥è€Œå‡†ç¡®çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18541v1">PDF</a> 19 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‹“æ‰‘çš„å»ºæ¨¡æŠ€æœ¯çš„æœ€æ–°è¿›å±•å·²åŠ é€Ÿç‰©ç†å»ºæ¨¡å’Œåˆ†å­ç ”ç©¶æ–¹é¢çš„è¿›æ­¥ï¼ŒåŒ…æ‹¬åœ¨è›‹ç™½è´¨-é…ä½“ç»“åˆäº²å’ŒåŠ›æ–¹é¢çš„åº”ç”¨ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æŒä¹…æ‹‰æ™®æ‹‰æ–¯å†³ç­–æ ‘ï¼ˆPLD-Treeï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é¢„æµ‹è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰äº²å’ŒåŠ›è€Œè®¾è®¡çš„æ–°æ–¹æ³•ã€‚PLD-Treeä¸“æ³¨äºè›‹ç™½è´¨ç»“åˆç•Œé¢ä¸Šçš„è›‹ç™½è´¨é“¾ï¼Œå¹¶åˆ©ç”¨æŒä¹…æ‹‰æ™®æ‹‰æ–¯ç®—å­æ•æ‰åæ˜ å…³é”®è›‹ç™½è´¨é—´ç›¸äº’ä½œç”¨çš„æ‹“æ‰‘ä¸å˜æ€§ã€‚è¿™äº›æ‹“æ‰‘æè¿°ç¬¦æ¥æºäºæŒä¹…åŒæºæ€§ï¼Œé€šè¿‡èå…¥å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è¿›åŒ–å°ºåº¦å»ºæ¨¡ï¼ˆESMï¼‰ä»¥å¢å¼ºåºåˆ—ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨PDBbind V2020å’ŒSKEMPI v2ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†PLD-Treeï¼Œåœ¨å‰”é™¤è›‹ç™½è´¨äº¤å‰éªŒè¯ä¸‹ï¼Œç›¸å…³ç³»æ•°ï¼ˆRpï¼‰è¾¾åˆ°0.83ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰è¿™äº›æ•°æ®é›†ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºæ‰€æœ‰æŠ¥é“çš„å…ˆè¿›æ–¹æ³•ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å°†æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸æ‹“æ‰‘æè¿°ç¬¦ç›¸ç»“åˆçš„åŠ›é‡ï¼Œä¸ºåˆ†å­å¯¹æ¥å’Œè™šæ‹Ÿç­›é€‰æä¾›äº†ç¨³å¥ä¸”å‡†ç¡®çš„é¢„æµ‹è›‹ç™½è´¨-è›‹ç™½è´¨ç»“åˆäº²å’ŒåŠ›çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‹“æ‰‘å»ºæ¨¡æŠ€æœ¯çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†ç‰©ç†å»ºæ¨¡å’Œåˆ†å­ç ”ç©¶çš„å‘å±•ã€‚</li>
<li>Persistent Laplacian Decision Tree (PLD-Tree)æ˜¯ä¸€ç§é¢„æµ‹è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰äº²å’ŒåŠ›çš„æ–°æ–¹æ³•ã€‚</li>
<li>PLD-Treeä¸“æ³¨äºè›‹ç™½è´¨ç»“åˆç•Œé¢ä¸Šçš„è›‹ç™½è´¨é“¾ï¼Œå¹¶æ•æ‰å…³é”®è›‹ç™½è´¨é—´ç›¸äº’ä½œç”¨çš„æ‹“æ‰‘ä¸å˜æ€§ã€‚</li>
<li>æ‹“æ‰‘æè¿°ç¬¦ä¸æ¥è‡ªå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è¿›åŒ–å°ºåº¦å»ºæ¨¡ï¼ˆESMï¼‰ç›¸ç»“åˆï¼Œå¢å¼ºäº†åºåˆ—ä¿¡æ¯çš„ä½œç”¨ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒPLD-Treeçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œç›¸å…³ç³»æ•°è¾¾åˆ°0.83ã€‚</li>
<li>é›†æˆæœºå™¨å­¦ä¹ æŠ€æœ¯å’Œæ‹“æ‰‘æè¿°ç¬¦çš„æ–¹æ³•ä¸ºåˆ†å­å¯¹æ¥å’Œè™šæ‹Ÿç­›é€‰æä¾›äº†ç¨³å¥ä¸”å‡†ç¡®çš„æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea09d29b943736b9d54eb0258310c15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-200b460a6309c143af2def5d7ccabe40.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Harnessing-Large-Language-Models-for-Knowledge-Graph-Question-Answering-via-Adaptive-Multi-Aspect-Retrieval-Augmentation"><a href="#Harnessing-Large-Language-Models-for-Knowledge-Graph-Question-Answering-via-Adaptive-Multi-Aspect-Retrieval-Augmentation" class="headerlink" title="Harnessing Large Language Models for Knowledge Graph Question Answering   via Adaptive Multi-Aspect Retrieval-Augmentation"></a>Harnessing Large Language Models for Knowledge Graph Question Answering   via Adaptive Multi-Aspect Retrieval-Augmentation</h2><p><strong>Authors:Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, Zhihong Zhu, Zhi Zheng, Xian Wu, Xiangyu Zhao, Tong Xu, Enhong Chen</strong></p>
<p>Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The Amar framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMsâ€™ output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a 1.9% improvement in accuracy over its best competitor and a 6.6% improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of Amar in improving the reasoning of LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†åœ¨é¢å¯¹å¤æ‚çŸ¥è¯†æ¨ç†ä»»åŠ¡æ—¶ï¼Œå®¹æ˜“å‡ºç°å¹»è§‰å’Œè¿‡æ—¶çŸ¥è¯†çš„é—®é¢˜ï¼Œå¯¼è‡´è¾“å‡ºäº‹å®é”™è¯¯ã€‚ä»¥å¾€çš„ç ”ç©¶è¯•å›¾é€šè¿‡ä»å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä¸­æ£€ç´¢äº‹å®çŸ¥è¯†æ¥è¾…åŠ©LLMè¿›è¡Œé€»è¾‘æ¨ç†å’Œç­”æ¡ˆé¢„æµ‹ï¼Œä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¸¸å¸¸ä¼šå¼•å…¥å™ªå£°å’Œæ— å…³æ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤šä¸ªçŸ¥è¯†æ–¹é¢çš„å¹¿æ³›ä¸Šä¸‹æ–‡æƒ…å†µä¸‹ã€‚è¿™æ ·ï¼ŒLLMçš„æ³¨æ„åŠ›å¯èƒ½ä¼šä»é—®é¢˜å’Œç›¸å…³ä¿¡æ¯ä¸­è¢«è¯¯å¯¼ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºçŸ¥è¯†å›¾è°±çš„è‡ªé€‚åº”å¤šæ–¹é¢æ£€ç´¢å¢å¼ºï¼ˆAmarï¼‰æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ£€ç´¢çŸ¥è¯†ï¼ŒåŒ…æ‹¬å®ä½“ã€å…³ç³»å’Œå­å›¾ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„æ¯æ®µæ–‡æœ¬è½¬æ¢ä¸ºæç¤ºåµŒå…¥ã€‚Amaræ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®å­ç»„ä»¶ï¼š1ï¼‰è‡ªå¯¹é½æ¨¡å—ï¼Œå¯¹é½å®ä½“ã€å…³ç³»å’Œå­å›¾ä¹‹é—´çš„å…±æ€§ï¼Œä»¥å¢å¼ºæ£€ç´¢åˆ°çš„æ–‡æœ¬ï¼Œä»è€Œå‡å°‘å™ªå£°å¹²æ‰°ï¼›2ï¼‰ç›¸å…³æ€§é—¨æ§æ¨¡å—ï¼Œé‡‡ç”¨è½¯é—¨æ§å­¦ä¹ é—®é¢˜ä¸å¤šæ–¹é¢æ£€ç´¢æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§å¾—åˆ†ï¼Œä»¥ç¡®å®šå“ªäº›ä¿¡æ¯åº”è¢«ç”¨æ¥å¢å¼ºLLMçš„è¾“å‡ºï¼Œç”šè‡³å¯ä»¥è¢«å®Œå…¨è¿‡æ»¤æ‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨WebQSPå’ŒCWQä¸¤ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å‡†ç¡®ç‡ä¸Šæ¯”æœ€ä½³ç«äº‰å¯¹æ‰‹æé«˜äº†1.9%ï¼Œåœ¨é€»è¾‘å½¢å¼ç”Ÿæˆä¸Šæ¯”ç›´æ¥ä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡æç¤ºçš„æ–¹æ³•æé«˜äº†6.6%ã€‚è¿™äº›ç»“æœè¯æ˜äº†Amaråœ¨æ”¹å–„LLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18537v1">PDF</a> Accepted by AAAIâ€™2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚çŸ¥è¯†æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå¦‚æ˜“å‡ºç°å¹»è§‰å’Œè¿‡æ—¶çŸ¥è¯†çš„é—®é¢˜ï¼Œå¯¼è‡´è¾“å‡ºäº‹å®é”™è¯¯ã€‚å…ˆå‰çš„ç ”ç©¶å°è¯•é€šè¿‡ä»å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä¸­æ£€ç´¢äº‹å®çŸ¥è¯†æ¥è¾…åŠ©LLMsè¿›è¡Œé€»è¾‘æ¨ç†å’Œç­”æ¡ˆé¢„æµ‹ï¼Œä½†è¿™ç§æ–¹æ³•å¸¸å¼•å…¥å™ªå£°å’Œæ— å…³æ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å¤šå±‚é¢çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºï¼ˆAmarï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬è‡ªæˆ‘å¯¹é½æ¨¡å—å’Œç›¸å…³æ€§é—¨æ§æ¨¡å—ï¼Œé€šè¿‡è½¬åŒ–æ£€ç´¢æ–‡æœ¬ä¸ºæç¤ºåµŒå…¥ï¼Œä»¥æé«˜LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨WebQSPå’ŒCWQä¸¤ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤æ‚çŸ¥è¯†æ¨ç†æ—¶å­˜åœ¨å±€é™ï¼Œæ˜“äº§ç”Ÿäº‹å®é”™è¯¯çš„è¾“å‡ºã€‚</li>
<li>å¼•å…¥çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰æ£€ç´¢ä»¥å¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›æ˜¯ä¸€ç§å¸¸è§ç­–ç•¥ï¼Œä½†å­˜åœ¨å¼•å…¥å™ªå£°å’Œæ— å…³æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„Amaræ¡†æ¶åŒ…æ‹¬è‡ªæˆ‘å¯¹é½æ¨¡å—ï¼Œèƒ½å¢å¼ºæ£€ç´¢æ–‡æœ¬ï¼Œå‡å°‘å™ªå£°å¹²æ‰°ã€‚</li>
<li>Amaræ¡†æ¶è¿˜åŒ…æ‹¬ç›¸å…³æ€§é—¨æ§æ¨¡å—ï¼Œèƒ½å­¦ä¹ é—®é¢˜ä¸å¤šå±‚é¢æ£€ç´¢æ•°æ®ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå†³å®šå“ªäº›ä¿¡æ¯åº”è¢«ç”¨äºå¢å¼ºLLMsçš„è¾“å‡ºã€‚</li>
<li>Amaræ¡†æ¶åœ¨WebQSPå’ŒCWQæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå‡†ç¡®æ€§æé«˜1.9%ï¼Œé€»è¾‘å½¢å¼ç”Ÿæˆæé«˜6.6%ã€‚</li>
<li>Amaræ¡†æ¶èƒ½æœ‰æ•ˆæé«˜LLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a3398859d7d040fe3428010b9f62d7a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0af29c52b5e0cbf14e7526b3c7aa85f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-070ed6566363250fe8aea785b550211f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89b33a52266cee5869c4795c06da2481.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-740097602bf75891a4c7ca18cb091ecd.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Automated-Code-Review-In-Practice"><a href="#Automated-Code-Review-In-Practice" class="headerlink" title="Automated Code Review In Practice"></a>Automated Code Review In Practice</h2><p><strong>Authors:Umut Cihan, Vahid Haratian, Arda Ä°Ã§Ã¶z, Mert Kaan GÃ¼l, Ã–mercan Devran, Emircan Furkan Bayendur, Baykal Mehmet UÃ§ar, Eray TÃ¼zÃ¼n</strong></p>
<p>Code review is a widespread practice to improve software quality and transfer knowledge. It is often seen as time-consuming due to the need for manual effort and potential delays. Several AI-assisted tools, such as Qodo, GitHub Copilot, and Coderabbit, provide automated reviews using large language models (LLMs). The effects of such tools in the industry are yet to be examined.   This study examines the impact of LLM-based automated code review tools in an industrial setting. The study was conducted within a software development environment that adopted an AI-assisted review tool (based on open-source Qodo PR Agent). Around 238 practitioners across ten projects had access to the tool. We focused on three projects with 4,335 pull requests, 1,568 of which underwent automated reviews. Data collection comprised three sources: (1) a quantitative analysis of pull request data, including comment labels indicating whether developers acted on the automated comments, (2) surveys sent to developers regarding their experience with reviews on individual pull requests, and (3) a broader survey of 22 practitioners capturing their general opinions on automated reviews.   73.8% of automated comments were resolved. However, the average pull request closure duration increased from five hours 52 minutes to eight hours 20 minutes, with varying trends across projects. Most practitioners reported a minor improvement in code quality due to automated reviews.   The LLM-based tool proved useful in software development, enhancing bug detection, increasing awareness of code quality, and promoting best practices. However, it also led to longer pull request closure times and introduced drawbacks like faulty reviews, unnecessary corrections, and irrelevant comments. </p>
<blockquote>
<p>ä»£ç å®¡æŸ¥æ˜¯ä¸€ç§æ™®éå®è·µï¼Œæ—¨åœ¨æé«˜è½¯ä»¶è´¨é‡å¹¶ä¼ é€’çŸ¥è¯†ã€‚ç”±äºéœ€è¦æ‰‹åŠ¨æ“ä½œå’Œå¯èƒ½çš„å»¶è¿Ÿï¼Œå®ƒé€šå¸¸è¢«è§†ä¸ºè€—æ—¶ã€‚ä¸€äº›AIè¾…åŠ©å·¥å…·ï¼Œå¦‚Qodoã€GitHub Copilotå’ŒCoderabbitï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›è‡ªåŠ¨åŒ–å®¡æŸ¥ã€‚è¿™ç±»å·¥å…·åœ¨è¡Œä¸šä¸­çš„æ•ˆæœè¿˜æœ‰å¾…æ£€éªŒã€‚æœ¬ç ”ç©¶æ—¨åœ¨åœ¨ä¸€ä¸ªå·¥ä¸šç¯å¢ƒä¸­è€ƒå¯ŸåŸºäºLLMçš„è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥å·¥å…·çš„å½±å“ã€‚ç ”ç©¶æ˜¯åœ¨ä¸€ä¸ªé‡‡ç”¨AIè¾…åŠ©å®¡æŸ¥å·¥å…·ï¼ˆåŸºäºå¼€æºQodo PR Agentï¼‰çš„è½¯ä»¶å¼€å‘ç¯å¢ƒä¸­è¿›è¡Œçš„ã€‚å¤§çº¦238åä»ä¸šè€…å‚ä¸äº†åä¸ªé¡¹ç›®çš„å·¥å…·ä½¿ç”¨ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨äº†ä¸‰ä¸ªé¡¹ç›®ï¼Œæ¶‰åŠ4335ä¸ªè¯·æ±‚ï¼Œå…¶ä¸­1568ä¸ªè¯·æ±‚è¿›è¡Œäº†è‡ªåŠ¨åŒ–å®¡æŸ¥ã€‚æ•°æ®æ”¶é›†åŒ…æ‹¬ä¸‰ä¸ªæ¥æºï¼šï¼ˆ1ï¼‰è¯·æ±‚æ•°æ®çš„å®šé‡åˆ†æï¼ŒåŒ…æ‹¬è¯„è®ºæ ‡ç­¾ï¼Œç”¨äºæŒ‡ç¤ºå¼€å‘äººå‘˜æ˜¯å¦å¯¹è‡ªåŠ¨åŒ–è¯„è®ºé‡‡å–è¡ŒåŠ¨ï¼›ï¼ˆ2ï¼‰å‘é€ç»™å¼€å‘äººå‘˜å…³äºä»–ä»¬å¯¹ä¸ªåˆ«è¯·æ±‚å®¡æŸ¥çš„ä¸ªäººä½“éªŒçš„è°ƒæŸ¥ï¼›ï¼ˆ3ï¼‰å¯¹22åä»ä¸šè€…çš„å¹¿æ³›è°ƒæŸ¥ï¼Œäº†è§£ä»–ä»¬å¯¹è‡ªåŠ¨åŒ–å®¡æŸ¥çš„æ€»ä½“çœ‹æ³•ã€‚73.8%çš„è‡ªåŠ¨åŒ–è¯„è®ºå¾—åˆ°è§£å†³ã€‚ç„¶è€Œï¼Œå¹³å‡è¯·æ±‚å…³é—­æ—¶é—´ä»5å°æ—¶52åˆ†é’Ÿå¢åŠ åˆ°8å°æ—¶20åˆ†é’Ÿï¼Œä¸åŒé¡¹ç›®çš„è¶‹åŠ¿å„ä¸ç›¸åŒã€‚å¤§å¤šæ•°ä»ä¸šè€…æŠ¥å‘Šè¯´ï¼Œç”±äºè‡ªåŠ¨åŒ–å®¡æŸ¥ï¼Œä»£ç è´¨é‡ç•¥æœ‰æé«˜ã€‚åŸºäºLLMçš„å·¥å…·åœ¨è½¯ä»¶å¼€å‘ä¸­è¯æ˜æ˜¯æœ‰ç”¨çš„ï¼Œå¯ä»¥æé«˜æ•…éšœæ£€æµ‹èƒ½åŠ›ï¼Œå¢åŠ å¯¹ä»£ç è´¨é‡çš„æ„è¯†ï¼Œå¹¶ä¿ƒè¿›æœ€ä½³å®è·µã€‚ç„¶è€Œï¼Œå®ƒä¹Ÿä¼šå¯¼è‡´è¯·æ±‚å…³é—­æ—¶é—´å»¶é•¿ï¼Œå¹¶å¸¦æ¥ä¸€äº›ç¼ºç‚¹ï¼Œå¦‚å®¡æŸ¥é”™è¯¯ã€ä¸å¿…è¦çš„æ›´æ­£å’Œæ— å…³è¯„è®ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18531v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥å·¥å…·åœ¨è½¯ä»¶è¡Œä¸šä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æœ¬ç ”ç©¶åœ¨ä¸€ä¸ªè½¯ä»¶å¼€å‘ç¯å¢ƒä¸­ï¼Œå¯¹ä¸€ä¸ªåŸºäºå¼€æºQodo PR Agentçš„AIè¾…åŠ©å®¡æŸ¥å·¥å…·çš„å½±å“è¿›è¡Œäº†ç ”ç©¶ã€‚ç»“æœæ˜¾ç¤ºï¼Œè‡ªåŠ¨åŒ–å®¡æŸ¥å·¥å…·åœ¨æé«˜ä»£ç è´¨é‡ã€å¢å¼ºå¯¹é”™è¯¯çš„æ„è¯†ä»¥åŠæ¨å¹¿æœ€ä½³å®è·µæ–¹é¢è¡¨ç°å‡ºä¸€å®šçš„æ•ˆæœï¼Œä½†åŒæ—¶ä¹Ÿå¯¼è‡´äº†æ›´é•¿çš„æ‹‰å–è¯·æ±‚å…³é—­æ—¶é—´ï¼Œå¹¶å¼•å…¥äº†ä¸€äº›ç¼ºé™·ï¼Œå¦‚é”™è¯¯çš„å®¡æŸ¥ã€ä¸å¿…è¦çš„ä¿®æ­£å’Œä¸ç›¸å…³çš„è¯„è®ºã€‚æ€»ä½“è€Œè¨€ï¼Œè™½ç„¶å­˜åœ¨ä¸€å®šçš„é—®é¢˜å’ŒæŒ‘æˆ˜ï¼Œä½†è¿™äº›å·¥å…·å¯¹è½¯ä»¶å¼€å‘å…·æœ‰ç§¯æå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç å®¡æŸ¥æ˜¯æå‡è½¯ä»¶è´¨é‡å’ŒçŸ¥è¯†ä¼ é€’çš„æ™®éå®è·µï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡æ‰‹åŠ¨æ“ä½œå’Œå¯èƒ½å»¶è¿Ÿï¼Œå› æ­¤è¢«è§†ä¸ºè€—æ—¶ã€‚</li>
<li>AIè¾…åŠ©å·¥å…·å¦‚Qodoã€GitHub Copilotå’ŒCoderabbitä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–å®¡æŸ¥ã€‚</li>
<li>åœ¨ä¸€ä¸ªè½¯ä»¶å¼€å‘ç¯å¢ƒä¸­è¿›è¡Œçš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºLLMçš„è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥å·¥å…·åœ¨æé«˜ä»£ç è´¨é‡ã€å¢å¼ºå¯¹é”™è¯¯çš„æ„è¯†å’Œæ¨å¹¿æœ€ä½³å®è·µæ–¹é¢å…·æœ‰ç§¯æå½±å“ã€‚</li>
<li>è‡ªåŠ¨åŒ–å®¡æŸ¥å·¥å…·å¯èƒ½å¯¼è‡´æ›´é•¿çš„æ‹‰å–è¯·æ±‚å…³é—­æ—¶é—´ã€‚</li>
<li>å°½ç®¡è‡ªåŠ¨åŒ–å®¡æŸ¥å·¥å…·åœ¨æŸäº›æƒ…å†µä¸‹èƒ½å¤Ÿæé«˜ä»£ç è´¨é‡ï¼Œä½†ä¹Ÿå¯èƒ½å¼•å…¥ä¸€äº›é—®é¢˜ï¼Œå¦‚é”™è¯¯çš„å®¡æŸ¥ã€ä¸å¿…è¦çš„ä¿®æ­£å’Œä¸ç›¸å…³çš„è¯„è®ºã€‚</li>
<li>å®è·µè€…æ™®éåæ˜ è‡ªåŠ¨åŒ–å®¡æŸ¥å·¥å…·åœ¨æé«˜ä»£ç è´¨é‡æ–¹é¢æœ‰ä¸€å®šçš„æ”¹å–„ï¼Œä½†æ•ˆæœå› é¡¹ç›®å’Œå›¢é˜Ÿè€Œå¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f9ec663e6d0854ae6ed92e6b5566c0a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c45862c17c93ee5417b4f56631194f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d1b95414aaf496f7ff7ded661d6fd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05fe827deaeb54c0d0643a9640950ade.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3302f773032e8ec1c5e8aaa1540db83d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31fbba8c38b2599a2dcbcc35e17733bc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-guided-Deep-Reinforcement-Learning-for-Decision-Making-in-Autonomous-Driving"><a href="#Large-Language-Model-guided-Deep-Reinforcement-Learning-for-Decision-Making-in-Autonomous-Driving" class="headerlink" title="Large Language Model guided Deep Reinforcement Learning for Decision   Making in Autonomous Driving"></a>Large Language Model guided Deep Reinforcement Learning for Decision   Making in Autonomous Driving</h2><p><strong>Authors:Hao Pang, Zhenpo Wang, Guoqiang Li</strong></p>
<p>Deep reinforcement learning (DRL) shows promising potential for autonomous driving decision-making. However, DRL demands extensive computational resources to achieve a qualified policy in complex driving scenarios due to its low learning efficiency. Moreover, leveraging expert guidance from human to enhance DRL performance incurs prohibitively high labor costs, which limits its practical application. In this study, we propose a novel large language model (LLM) guided deep reinforcement learning (LGDRL) framework for addressing the decision-making problem of autonomous vehicles. Within this framework, an LLM-based driving expert is integrated into the DRL to provide intelligent guidance for the learning process of DRL. Subsequently, in order to efficiently utilize the guidance of the LLM expert to enhance the performance of DRL decision-making policies, the learning and interaction process of DRL is enhanced through an innovative expert policy constrained algorithm and a novel LLM-intervened interaction mechanism. Experimental results demonstrate that our method not only achieves superior driving performance with a 90% task success rate but also significantly improves the learning efficiency and expert guidance utilization efficiency compared to state-of-the-art baseline algorithms. Moreover, the proposed method enables the DRL agent to maintain consistent and reliable performance in the absence of LLM expert guidance. The code and supplementary videos are available at <a target="_blank" rel="noopener" href="https://bitmobility.github.io/LGDRL/">https://bitmobility.github.io/LGDRL/</a>. </p>
<blockquote>
<p>æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰åœ¨è‡ªåŠ¨é©¾é©¶å†³ç­–åˆ¶å®šä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºå­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼ŒDRLåœ¨å¤æ‚çš„é©¾é©¶åœºæ™¯ä¸­éœ€è¦æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºæ‰èƒ½å®ç°åˆæ ¼çš„æ”¿ç­–ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨äººç±»ä¸“å®¶æŒ‡å¯¼å¢å¼ºDRLæ€§èƒ½å¯¼è‡´äº†é«˜æ˜‚çš„åŠ³åŠ¨åŠ›æˆæœ¬ï¼Œä»è€Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆLGDRLï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„å†³ç­–é—®é¢˜ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼ŒåŸºäºLLMçš„é©¾é©¶ä¸“å®¶è¢«æ•´åˆåˆ°DRLä¸­ï¼Œä¸ºDRLçš„å­¦ä¹ è¿‡ç¨‹æä¾›æ™ºèƒ½æŒ‡å¯¼ã€‚ä¸ºäº†æœ‰æ•ˆåˆ©ç”¨LLMä¸“å®¶çš„æŒ‡å¯¼ï¼Œæé«˜DRLå†³ç­–ç­–ç•¥çš„æ€§èƒ½ï¼Œé€šè¿‡åˆ›æ–°çš„ä¸“å®¶æ”¿ç­–çº¦æŸç®—æ³•å’Œæ–°å‹LLMå¹²é¢„äº¤äº’æœºåˆ¶ï¼Œå¢å¼ºäº†DRLçš„å­¦ä¹ å’Œäº¤äº’è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å®ç°äº†é«˜è¾¾90%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œè€Œä¸”åœ¨é©¾é©¶æ€§èƒ½ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºçº¿ç®—æ³•ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†å­¦ä¹ æ•ˆç‡å’Œå¯¹ä¸“å®¶æŒ‡å¯¼çš„åˆ©ç”¨æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä½¿DRLä»£ç†èƒ½å¤Ÿåœ¨æ²¡æœ‰LLMä¸“å®¶æŒ‡å¯¼çš„æƒ…å†µä¸‹ä¿æŒä¸€è‡´å’Œå¯é çš„æ€§èƒ½ã€‚ç›¸å…³ä»£ç å’Œè¡¥å……è§†é¢‘å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://bitmobility.github.io/LGDRL/%E8%AE%BF%E9%97%AE%E3%80%82">https://bitmobility.github.io/LGDRL/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18511v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨é©¾é©¶å†³ç­–åˆ¶å®šä¸­å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œä½†å…¶å­¦ä¹ æ•ˆç‡å’Œè®¡ç®—èµ„æºéœ€æ±‚é™åˆ¶äº†å®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆLGDRLï¼‰æ¡†æ¶ï¼Œæ•´åˆäº†è¯­è¨€æ¨¡å‹é©±åŠ¨çš„é©¾é©¶ä¸“å®¶æ™ºèƒ½æŒ‡å¯¼ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å¹¶ä¼˜åŒ–å†³ç­–åˆ¶å®šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¸ä»…æé«˜äº†é©¾é©¶æ€§èƒ½ï¼ˆä»»åŠ¡æˆåŠŸç‡è¾¾90%ï¼‰ï¼Œè€Œä¸”æ˜¾è‘—æå‡äº†å­¦ä¹ æ•ˆç‡å’Œä¸“å®¶æŒ‡å¯¼çš„åˆ©ç”¨æ•ˆç‡ã€‚åœ¨ç¼ºå°‘è¯­è¨€æ¨¡å‹ä¸“å®¶æŒ‡å¯¼çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ä¾ç„¶ç¨³å®šå¯é ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨é©¾é©¶ä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ï¼Œä½†å­˜åœ¨å­¦ä¹ æ•ˆç‡ä½å’Œè®¡ç®—èµ„æºéœ€æ±‚å¤§çš„é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆLGDRLï¼‰æ¡†æ¶ï¼Œç»“åˆäº†è¯­è¨€æ¨¡å‹é©±åŠ¨çš„é©¾é©¶ä¸“å®¶æ™ºèƒ½æŒ‡å¯¼ã€‚</li>
<li>LGDRLæ¡†æ¶é€šè¿‡åˆ›æ–°æ€§çš„ä¸“å®¶æ”¿ç­–çº¦æŸç®—æ³•å’Œè¯­è¨€æ¨¡å‹ä»‹å…¥çš„äº¤äº’æœºåˆ¶ï¼Œæé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œä¼˜åŒ–äº†å†³ç­–åˆ¶å®šã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLGDRLåœ¨é©¾é©¶æ€§èƒ½ã€å­¦ä¹ æ•ˆç‡å’Œä¸“å®¶æŒ‡å¯¼åˆ©ç”¨æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ç®—æ³•ã€‚</li>
<li>LGDRLæ¡†æ¶ä¸‹çš„è‡ªåŠ¨é©¾é©¶å†³ç­–åœ¨ç¼ºå°‘è¯­è¨€æ¨¡å‹ä¸“å®¶æŒ‡å¯¼æ—¶ä»èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å’Œè¡¥å……è§†é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://bitmobility.github.io/LGDRL/%E8%8E%B7%E5%8F%96%E3%80%82">https://bitmobility.github.io/LGDRL/è·å–ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f54d4d28ca0b9f3742b59aa86ac913c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97d9b4da90a74b424bf85abe423f9a7b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-713511db0474137832a67accfa39cb27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f51f409fb15953cff7b3f6333b4877c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Think-or-Remember-Detecting-and-Directing-LLMs-Towards-Memorization-or-Generalization"><a href="#Think-or-Remember-Detecting-and-Directing-LLMs-Towards-Memorization-or-Generalization" class="headerlink" title="Think or Remember? Detecting and Directing LLMs Towards Memorization or   Generalization"></a>Think or Remember? Detecting and Directing LLMs Towards Memorization or   Generalization</h2><p><strong>Authors:Yi-Fu Fu, Yu-Chieh Tu, Tzu-Ling Cheng, Cheng-Yu Lin, Yi-Ting Yang, Heng-Yi Liu, Keng-Te Liao, Da-Cheng Juan, Shou-De Lin</strong></p>
<p>In this paper, we explore the foundational mechanisms of memorization and generalization in Large Language Models (LLMs), inspired by the functional specialization observed in the human brain. Our investigation serves as a case study leveraging specially designed datasets and experimental-scale LLMs to lay the groundwork for understanding these behaviors. Specifically, we aim to first enable LLMs to exhibit both memorization and generalization by training with the designed dataset, then (a) examine whether LLMs exhibit neuron-level spatial differentiation for memorization and generalization, (b) predict these behaviors using model internal representations, and (c) steer the behaviors through inference-time interventions. Our findings reveal that neuron-wise differentiation of memorization and generalization is observable in LLMs, and targeted interventions can successfully direct their behavior. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å—äººç±»å¤§è„‘åŠŸèƒ½ç‰¹åŒ–çš„å¯å‘ï¼Œæ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è®°å¿†å’Œæ³›åŒ–çš„åŸºç¡€æœºåˆ¶ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥ä½œä¸ºä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶ï¼Œåˆ©ç”¨ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†å’Œå®éªŒè§„æ¨¡çš„LLMï¼Œä¸ºç†è§£è¿™äº›è¡Œä¸ºå¥ å®šåŸºç¡€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç›®æ ‡é¦–å…ˆæ˜¯é€šè¿‡è®­ç»ƒè®¾è®¡çš„æ•°æ®é›†ä½¿LLMèƒ½å¤Ÿè¡¨ç°å‡ºè®°å¿†å’Œæ³›åŒ–ï¼Œç„¶åï¼ˆaï¼‰æ£€æŸ¥LLMæ˜¯å¦åœ¨è®°å¿†å’Œæ³›åŒ–æ–¹é¢è¡¨ç°å‡ºç¥ç»å…ƒçº§çš„ç©ºé—´åˆ†åŒ–ï¼Œï¼ˆbï¼‰ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨è¡¨å¾æ¥é¢„æµ‹è¿™äº›è¡Œä¸ºï¼Œï¼ˆcï¼‰é€šè¿‡æ¨ç†æ—¶çš„å¹²é¢„æ¥å¼•å¯¼è¿™äº›è¡Œä¸ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMä¸­å¯ä»¥è§‚å¯Ÿåˆ°è®°å¿†å’Œæ³›åŒ–çš„ç¥ç»å…ƒçº§åˆ†åŒ–ï¼Œå¹¶ä¸”æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„å¯ä»¥æˆåŠŸæŒ‡å¯¼å…¶è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18497v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºäººç±»å¤§è„‘åŠŸèƒ½ç‰¹åŒ–ç°è±¡çš„å¯ç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®°å¿†å’Œæ³›åŒ–åŸºç¡€æœºåˆ¶ã€‚ç ”ç©¶åˆ©ç”¨ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†å’Œå®éªŒè§„æ¨¡çš„LLMè¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ï¼Œä¸ºç†è§£è¿™äº›è¡Œä¸ºå¥ å®šåŸºç¡€ã€‚ç ”ç©¶æ—¨åœ¨é€šè¿‡è®­ç»ƒLLMä½¿ç”¨è®¾è®¡æ•°æ®é›†æ¥å±•ç°è®°å¿†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œç„¶åï¼ˆaï¼‰æ£€æŸ¥LLMæ˜¯å¦åœ¨ç¥ç»å…ƒå±‚é¢å±•ç°è®°å¿†å’Œæ³›åŒ–çš„ç©ºé—´åˆ†åŒ–ï¼Œï¼ˆbï¼‰ä½¿ç”¨æ¨¡å‹å†…éƒ¨è¡¨å¾é¢„æµ‹è¿™äº›è¡Œä¸ºï¼Œï¼ˆcï¼‰é€šè¿‡æ¨ç†æ—¶é—´å¹²é¢„æ¥å¼•å¯¼è¿™äº›è¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼ŒLLMä¸­å¯è§‚å¯Ÿåˆ°ç¥ç»å…ƒå¯¹è®°å¿†å’Œæ³›åŒ–çš„åˆ†åŒ–ï¼Œä¸”é’ˆå¯¹æ€§å¹²é¢„å¯æˆåŠŸæŒ‡å¯¼å…¶è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡åŸºäºäººç±»å¤§è„‘åŠŸèƒ½ç‰¹åŒ–çš„å¯ç¤ºï¼Œæ¢è®¨äº†LLMçš„è®°å¿†å’Œæ³›åŒ–æœºåˆ¶ã€‚</li>
<li>é€šè¿‡ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†å’Œå®éªŒè§„æ¨¡LLMè¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡æ˜¯ä½¿LLMå±•ç°è®°å¿†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æ¢ç´¢å…¶ç¥ç»å…ƒå±‚é¢çš„ç©ºé—´åˆ†åŒ–ã€‚</li>
<li>ç ”ç©¶å‘ç°LLMä¸­ç¥ç»å…ƒå¯¹è®°å¿†å’Œæ³›åŒ–çš„åˆ†åŒ–æ˜¯å¯è§çš„ã€‚</li>
<li>å¯é€šè¿‡æ¨¡å‹å†…éƒ¨è¡¨å¾é¢„æµ‹LLMçš„è®°å¿†å’Œæ³›åŒ–è¡Œä¸ºã€‚</li>
<li>é€šè¿‡æ¨ç†æ—¶é—´å¹²é¢„ï¼Œå¯ä»¥æˆåŠŸå¼•å¯¼LLMçš„è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0005d3693a17ce154c949c6da0213d99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dbae9f263116bf2cc7c28ae31b9e8fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e70c4cb6fce5342514a85e67ba17fdb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Segment-Based-Attention-Masking-for-GPTs"><a href="#Segment-Based-Attention-Masking-for-GPTs" class="headerlink" title="Segment-Based Attention Masking for GPTs"></a>Segment-Based Attention Masking for GPTs</h2><p><strong>Authors:Shahar Katz, Liran Ringel, Yaniv Romano, Lior Wolf</strong></p>
<p>Modern Language Models (LMs) owe much of their success to masked causal attention, the backbone of Generative Pre-Trained Transformer (GPT) models. Although GPTs can process the entire user prompt at once, the causal masking is applied to all input tokens step-by-step, mimicking the generation process. This imposes an unnecessary constraint during the initial â€œprefillâ€ phase when the model processes the input prompt and generates the internal representations before producing any output tokens. In this work, attention is masked based on the known block structure at the prefill phase, followed by the conventional token-by-token autoregressive process after that. For example, in a typical chat prompt, the system prompt is treated as one block, and the user prompt as the next one. Each of these is treated as a unit for the purpose of masking, such that the first tokens in each block can access the subsequent tokens in a non-causal manner. Then, the model answer is generated in the conventional causal manner. This Segment-by-Segment scheme entails no additional computational overhead. When integrating it into models such as Llama and Qwen, state-of-the-art performance is consistently achieved. </p>
<blockquote>
<p>ç°ä»£è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½’åŠŸäºæ©ç›–å› æœæ³¨æ„åŠ›ï¼Œè¿™æ˜¯ç”Ÿæˆå¼é¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰æ¨¡å‹çš„æ ¸å¿ƒã€‚è™½ç„¶GPTå¯ä»¥ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªç”¨æˆ·æç¤ºï¼Œä½†å› æœæ©ç›–æ˜¯é€æ­¥åº”ç”¨äºæ‰€æœ‰è¾“å…¥æ ‡è®°çš„ï¼Œæ¨¡ä»¿ç”Ÿæˆè¿‡ç¨‹ã€‚è¿™åœ¨æ¨¡å‹å¤„ç†è¾“å…¥æç¤ºå¹¶äº§ç”Ÿä»»ä½•è¾“å‡ºæ ‡è®°ä¹‹å‰çš„åˆå§‹â€œé¢„å¡«å……â€é˜¶æ®µæ–½åŠ äº†ä¸€ä¸ªä¸å¿…è¦çš„çº¦æŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18487v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šç°ä»£è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šå½’åŠŸäºæ©ç å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™æ˜¯ç”Ÿæˆå¼é¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰æ¨¡å‹çš„æ ¸å¿ƒã€‚å°½ç®¡GPTèƒ½å¤Ÿä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªç”¨æˆ·æç¤ºï¼Œä½†åœ¨åˆå§‹â€œé¢„å¡«å……â€é˜¶æ®µï¼Œå› æœæ©ç ä¼šé€æ­¥åº”ç”¨äºæ‰€æœ‰è¾“å…¥ä»¤ç‰Œï¼Œæ¨¡ä»¿ç”Ÿæˆè¿‡ç¨‹ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ†æ®µæ©ç ç­–ç•¥ï¼Œåœ¨é¢„å¡«å……é˜¶æ®µåŸºäºå·²çŸ¥çš„åˆ†å—ç»“æ„è¿›è¡Œæ³¨æ„åŠ›æ©ç ï¼Œä¹‹åé‡‡ç”¨ä¼ ç»Ÿçš„é€ä»¤ç‰Œè‡ªå›å½’è¿‡ç¨‹ã€‚è¿™ç§ç­–ç•¥åœ¨å¤„ç†ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æç¤ºæ—¶ï¼Œå°†æ¯ä¸ªæç¤ºè§†ä¸ºä¸€ä¸ªå—å•ä½è¿›è¡Œæ©ç å¤„ç†ï¼Œå¹¶åœ¨å—å†…é‡‡ç”¨éå› æœæ–¹å¼è®¿é—®åç»­ä»¤ç‰Œã€‚æœ€ç»ˆæ¨¡å‹ç­”æ¡ˆä»¥å¸¸è§„å› æœæ–¹å¼ç”Ÿæˆã€‚è¿™ç§åˆ†æ®µå¤„ç†ç­–ç•¥ä¸ä¼šå¢åŠ é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œå¹¶èƒ½å®ç°ä¸å½“å‰å…ˆè¿›æŠ€æœ¯ç›¸ä¸€è‡´çš„é›†æˆæ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°ä»£è¯­è¨€æ¨¡å‹çš„æˆåŠŸå½’åŠŸäºæ©ç å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™æ˜¯GPTæ¨¡å‹çš„æ ¸å¿ƒã€‚</li>
<li>GPTæ¨¡å‹åœ¨é¢„å¡«å……é˜¶æ®µå¯¹è¾“å…¥è¿›è¡Œé€ä»¤ç‰Œçš„å› æœæ©ç å¤„ç†ï¼Œæ¨¡ä»¿ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>æå‡ºä¸€ç§åˆ†æ®µæ©ç ç­–ç•¥ï¼Œåœ¨é¢„å¡«å……é˜¶æ®µåŸºäºåˆ†å—ç»“æ„è¿›è¡Œæ³¨æ„åŠ›æ©ç ã€‚</li>
<li>åˆ†æ®µæ©ç ç­–ç•¥åœ¨å¤„ç†ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æç¤ºæ—¶ï¼Œå°†æ¯ä¸ªæç¤ºè§†ä¸ºä¸€ä¸ªå—å•ä½ã€‚</li>
<li>åˆ†æ®µæ©ç ç­–ç•¥å…è®¸å—å†…çš„ä»¤ç‰Œä»¥éå› æœæ–¹å¼è®¿é—®åç»­ä»¤ç‰Œã€‚</li>
<li>æ¨¡å‹ç­”æ¡ˆä»¥å¸¸è§„å› æœæ–¹å¼ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9024cc7df07aed4abdd268b80425d525.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e00050d87eb46389d2910305a7848fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70c3f2a404b2124a726681ea8d74f791.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="3DGraphLLM-Combining-Semantic-Graphs-and-Large-Language-Models-for-3D-Scene-Understanding"><a href="#3DGraphLLM-Combining-Semantic-Graphs-and-Large-Language-Models-for-3D-Scene-Understanding" class="headerlink" title="3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D   Scene Understanding"></a>3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D   Scene Understanding</h2><p><strong>Authors:Tatiana Zemskova, Dmitry Yudin</strong></p>
<p>A 3D scene graph represents a compact scene model, storing information about the objects and the semantic relationships between them, making its use promising for robotic tasks. When interacting with a user, an embodied intelligent agent should be capable of responding to various queries about the scene formulated in natural language. Large Language Models (LLMs) are beneficial solutions for user-robot interaction due to their natural language understanding and reasoning abilities. Recent methods for creating learnable representations of 3D scenes have demonstrated the potential to improve the quality of LLMs responses by adapting to the 3D world. However, the existing methods do not explicitly utilize information about the semantic relationships between objects, limiting themselves to information about their coordinates. In this work, we propose a method 3DGraphLLM for constructing a learnable representation of a 3D scene graph. The learnable representation is used as input for LLMs to perform 3D vision-language tasks. In our experiments on popular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap datasets, we demonstrate the advantage of this approach over baseline methods that do not use information about the semantic relationships between objects. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/CognitiveAISystems/3DGraphLLM">https://github.com/CognitiveAISystems/3DGraphLLM</a>. </p>
<blockquote>
<p>ä¸€ä¸ª3Dåœºæ™¯å›¾ä»£è¡¨äº†ä¸€ä¸ªç´§å‡‘çš„åœºæ™¯æ¨¡å‹ï¼Œå­˜å‚¨å…³äºç‰©ä½“ä»¥åŠå®ƒä»¬ä¹‹é—´è¯­ä¹‰å…³ç³»çš„ä¿¡æ¯ï¼Œä½¿å…¶åœ¨æœºå™¨äººä»»åŠ¡ä¸­çš„ä½¿ç”¨å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚åœ¨ä¸ç”¨æˆ·äº¤äº’æ—¶ï¼Œæ™ºèƒ½å®ä½“ä»£ç†åº”èƒ½å¤Ÿç”¨è‡ªç„¶è¯­è¨€å›ç­”å…³äºåœºæ™¯çš„å„ç§æŸ¥è¯¢ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…·å¤‡è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå› æ­¤æ˜¯ç”¨æˆ·ä¸æœºå™¨äººäº¤äº’çš„æœ‰ç›Šè§£å†³æ–¹æ¡ˆã€‚æœ€è¿‘åˆ›å»º3Dåœºæ™¯å›¾çš„å¯å­¦ä¹ è¡¨ç¤ºæ–¹æ³•æ˜¾ç¤ºå‡ºé€šè¿‡é€‚åº”3Dä¸–ç•Œæ¥æé«˜LLMå“åº”è´¨é‡çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¹¶æœªæ˜ç¡®åˆ©ç”¨å¯¹è±¡ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ä¿¡æ¯ï¼Œä»…é™äºå…¶åæ ‡ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º3DGraphLLMçš„æ–¹æ³•ï¼Œç”¨äºæ„å»º3Dåœºæ™¯å›¾çš„å¯å­¦ä¹ è¡¨ç¤ºã€‚è¯¥å¯å­¦ä¹ è¡¨ç¤ºç”¨ä½œLLMçš„è¾“å…¥ï¼Œä»¥æ‰§è¡Œ3Dè§†è§‰è¯­è¨€ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨æµè¡Œçš„ScanReferã€RIOReferã€Multi3DReferã€ScanQAã€Sqa3Då’ŒScan2capæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¸ä½¿ç”¨å¯¹è±¡ä¹‹é—´è¯­ä¹‰å…³ç³»ä¿¡æ¯çš„åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/CognitiveAISystems/3DGraphLLM">https://github.com/CognitiveAISystems/3DGraphLLM</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18450v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>3Dåœºæ™¯å›¾åœ¨æœºå™¨äººä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡å­˜å‚¨ç‰©ä½“ä¿¡æ¯å’Œå®ƒä»¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³»æ¥è¡¨ç¤ºç´§å‡‘çš„åœºæ™¯æ¨¡å‹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”¨æˆ·å’Œæœºå™¨äººçš„äº¤äº’ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œè€Œæœ€æ–°çš„åˆ›å»º3Dåœºæ™¯å­¦ä¹ è¡¨ç¤ºçš„æ–¹æ³•å±•ç¤ºäº†æé«˜LLMså“åº”è´¨é‡çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æœªå……åˆ†åˆ©ç”¨ç‰©ä½“é—´çš„è¯­ä¹‰å…³ç³»ä¿¡æ¯ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º3DGraphLLMçš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºåŒ…å«è¯­ä¹‰å…³ç³»çš„3Dåœºæ™¯å›¾çš„å­¦ä¹ è¡¨ç¤ºã€‚è¯¥å­¦ä¹ è¡¨ç¤ºç”¨äºLLMsæ‰§è¡Œ3Dè§†è§‰è¯­è¨€ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¸ä½¿ç”¨ç‰©ä½“é—´è¯­ä¹‰å…³ç³»ä¿¡æ¯çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dåœºæ™¯å›¾æ˜¯ä¸€ç§ç´§å‡‘çš„åœºæ™¯æ¨¡å‹ï¼ŒåŒ…å«ç‰©ä½“ä¿¡æ¯å’Œå®ƒä»¬ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”¨æˆ·å’Œæœºå™¨äººçš„äº¤äº’ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>ç°æœ‰åˆ›å»º3Dåœºæ™¯å­¦ä¹ è¡¨ç¤ºçš„æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ç‰©ä½“é—´çš„è¯­ä¹‰å…³ç³»ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†åä¸º3DGraphLLMçš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºåŒ…å«è¯­ä¹‰å…³ç³»çš„3Dåœºæ™¯å›¾çš„å­¦ä¹ è¡¨ç¤ºã€‚</li>
<li>è¯¥å­¦ä¹ è¡¨ç¤ºç”¨äºLLMsæ‰§è¡Œ3Dè§†è§‰è¯­è¨€ä»»åŠ¡ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œ3DGraphLLMæ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>ç›¸å…³ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-339d571b662e3503b64772ff0a4338e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98636b78c7f27557423f5c541ae6c43f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13a4a26461ad6efdd442256beb73c0cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dcc8aad3b429466c60eef570502dd8e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Explainable-Multi-Modal-Data-Exploration-in-Natural-Language-via-LLM-Agent"><a href="#Explainable-Multi-Modal-Data-Exploration-in-Natural-Language-via-LLM-Agent" class="headerlink" title="Explainable Multi-Modal Data Exploration in Natural Language via LLM   Agent"></a>Explainable Multi-Modal Data Exploration in Natural Language via LLM   Agent</h2><p><strong>Authors:Farhad Nooralahzadeh, Yi Zhang, Jonathan Furst, Kurt Stockinger</strong></p>
<p>International enterprises, organizations, or hospitals collect large amounts of multi-modal data stored in databases, text documents, images, and videos. While there has been recent progress in the separate fields of multi-modal data exploration as well as in database systems that automatically translate natural language questions to database query languages, the research challenge of querying database systems combined with other unstructured modalities such as images in natural language is widely unexplored.   In this paper, we propose XMODE - a system that enables explainable, multi-modal data exploration in natural language. Our approach is based on the following research contributions: (1) Our system is inspired by a real-world use case that enables users to explore multi-modal information systems. (2) XMODE leverages a LLM-based agentic AI framework to decompose a natural language question into subtasks such as text-to-SQL generation and image analysis. (3) Experimental results on multi-modal datasets over relational data and images demonstrate that our system outperforms state-of-the-art multi-modal exploration systems, excelling not only in accuracy but also in various performance metrics such as query latency, API costs, planning efficiency, and explanation quality, thanks to the more effective utilization of the reasoning capabilities of LLMs. </p>
<blockquote>
<p>å›½é™…ä¼ä¸šã€ç»„ç»‡æˆ–åŒ»é™¢æ”¶é›†å¤§é‡å­˜å‚¨åœ¨æ•°æ®åº“ã€æ–‡æœ¬æ–‡æ¡£ã€å›¾åƒå’Œè§†é¢‘ä¸­çš„å¤šæ¨¡å¼æ•°æ®ã€‚è™½ç„¶æœ€è¿‘åœ¨å¤šæ¨¡å¼æ•°æ®æ¢ç´¢é¢†åŸŸä»¥åŠèƒ½å¤Ÿè‡ªåŠ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆæ•°æ®åº“æŸ¥è¯¢è¯­è¨€çš„æ•°æ®åº“ç³»ç»Ÿæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å°†æ•°æ®åº“ç³»ç»Ÿä¸å›¾åƒç­‰æ— ç»“æ„æ¨¡å¼ç»“åˆä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡ŒæŸ¥è¯¢çš„ç ”ç©¶æŒ‘æˆ˜ä»è¢«å¹¿æ³›æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†XMODEç³»ç»Ÿï¼Œå®ƒæ”¯æŒç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œå¯è§£é‡Šçš„å¤šæ¨¡å¼æ•°æ®æ¢ç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä»¥ä¸‹ç ”ç©¶è´¡çŒ®ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬çš„ç³»ç»Ÿå—åˆ°ç°å®ä½¿ç”¨æ¡ˆä¾‹çš„å¯å‘ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ¢ç´¢å¤šæ¨¡å¼ä¿¡æ¯ç³»ç»Ÿã€‚ï¼ˆ2ï¼‰XMODEåˆ©ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œå°†è‡ªç„¶è¯­è¨€é—®é¢˜åˆ†è§£ä¸ºæ–‡æœ¬åˆ°SQLç”Ÿæˆå’Œå›¾åƒåˆ†æç­‰å­ä»»åŠ¡ã€‚ï¼ˆ3ï¼‰åœ¨å…³ç³»æ•°æ®å’Œå›¾åƒä¸Šçš„å¤šæ¨¡å¼æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨æœ€æ–°æœ€å…ˆè¿›çš„çš„å¤šæ¨¡å¼æ¢ç´¢ç³»ç»Ÿä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ä»…åœ¨å‡†ç¡®æ€§ä¸Šï¼Œè€Œä¸”åœ¨å„ç§æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æŸ¥è¯¢å»¶è¿Ÿã€APIæˆæœ¬ã€è§„åˆ’æ•ˆç‡å’Œè§£é‡Šè´¨é‡ï¼‰æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¿™è¦å½’åŠŸäºå¯¹LLMæ¨ç†èƒ½åŠ›çš„æ›´æœ‰æ•ˆåˆ©ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18428v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºXMODEçš„ç³»ç»Ÿï¼Œç”¨äºä»¥è‡ªç„¶è¯­è¨€è¿›è¡Œå¯è§£é‡Šçš„å¤šæ¨¡æ€æ•°æ®æ¢ç´¢ã€‚è¯¥ç³»ç»Ÿå¯å¤„ç†å›½é™…ä¼ä¸šã€ç»„ç»‡æˆ–åŒ»é™¢ç­‰å¤§å‹æ•°æ®åº“ä¸­å­˜å‚¨çš„æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®ã€‚XMODEç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„åˆ†é˜¶æ®µæ™ºèƒ½æ¡†æ¶ï¼Œå°†è‡ªç„¶è¯­è¨€é—®é¢˜åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°SQLçš„ç”Ÿæˆå’Œå›¾åƒåˆ†æç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šæ¨¡æ€æ•°æ®é›†ä¸Šï¼Œæœ¬ç³»ç»Ÿæ€§èƒ½ä¼˜äºæœ€æ–°çš„å¤šæ¨¡æ€æ¢ç´¢ç³»ç»Ÿï¼Œä¸ä»…ç²¾åº¦é«˜ï¼Œè€Œä¸”åœ¨æŸ¥è¯¢å»¶è¿Ÿã€APIæˆæœ¬ã€è§„åˆ’æ•ˆç‡å’Œè§£é‡Šè´¨é‡ç­‰å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šä¹Ÿæœ‰ä¼˜åŠ¿ã€‚è¿™å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆåˆ©ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>XMODEç³»ç»Ÿå¯å®ç°å¤šæ¨¡æ€æ•°æ®æ¢ç´¢çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚</li>
<li>ç³»ç»Ÿå¯ä»çœŸå®åº”ç”¨åœºæ™¯ä¸­è·å¾—çµæ„Ÿï¼Œå¹¶åº”ç”¨äºå¤§å‹å›½é™…ä¼ä¸šæˆ–ç»„ç»‡çš„æ•°æ®å¤„ç†ã€‚</li>
<li>XMODEåˆ©ç”¨LLMä¸ºåŸºç¡€çš„åˆ†é˜¶æ®µæ™ºèƒ½æ¡†æ¶åˆ†è§£è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚</li>
<li>XMODEç³»ç»Ÿå¯¹å¤šæ¨¡æ€æ•°æ®è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œæ¶‰åŠå…³ç³»æ•°æ®å’Œå›¾åƒå¤„ç†ã€‚</li>
<li>XMODEç³»ç»Ÿåœ¨å‡†ç¡®æ€§ã€æŸ¥è¯¢å»¶è¿Ÿã€APIæˆæœ¬ã€è§„åˆ’æ•ˆç‡å’Œè§£é‡Šè´¨é‡ç­‰å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šç°æœ‰ç³»ç»Ÿã€‚</li>
<li>è¯¥ç³»ç»Ÿçš„ä¼˜åŠ¿åœ¨äºæœ‰æ•ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f1e108390bc1ee23988768f626d93859.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c77a754ca24be677137e27354b98898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b06237b755dd4beaf20bf99d400ef12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89b06e497655c8eb54679f8398395942.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GUI-Testing-Arena-A-Unified-Benchmark-for-Advancing-Autonomous-GUI-Testing-Agent"><a href="#GUI-Testing-Arena-A-Unified-Benchmark-for-Advancing-Autonomous-GUI-Testing-Agent" class="headerlink" title="GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI   Testing Agent"></a>GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI   Testing Agent</h2><p><strong>Authors:Kangjia Zhao, Jiahui Song, Leigang Sha, Haozhan Shen, Zhi Chen, Tiancheng Zhao, Xiubo Liang, Jianwei Yin</strong></p>
<p>Nowadays, research on GUI agents is a hot topic in the AI community. However, current research focuses on GUI task automation, limiting the scope of applications in various GUI scenarios. In this paper, we propose a formalized and comprehensive environment to evaluate the entire process of automated GUI Testing (GTArena), offering a fair, standardized environment for consistent operation of diverse multimodal large language models. We divide the testing process into three key subtasks: test intention generation, test task execution, and GUI defect detection, and construct a benchmark dataset based on these to conduct a comprehensive evaluation. It evaluates the performance of different models using three data types: real mobile applications, mobile applications with artificially injected defects, and synthetic data, thoroughly assessing their capabilities in this relevant task. Additionally, we propose a method that helps researchers explore the correlation between the performance of multimodal language large models in specific scenarios and their general capabilities in standard benchmark tests. Experimental results indicate that even the most advanced models struggle to perform well across all sub-tasks of automated GUI Testing, highlighting a significant gap between the current capabilities of Autonomous GUI Testing and its practical, real-world applicability. This gap provides guidance for the future direction of GUI Agent development. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ZJU-ACES-ISE/ChatUITest">https://github.com/ZJU-ACES-ISE/ChatUITest</a>. </p>
<blockquote>
<p>å¦‚ä»Šï¼ŒGUIä»£ç†çš„ç ”ç©¶å·²æˆä¸ºäººå·¥æ™ºèƒ½ç¤¾åŒºçš„çƒ­é—¨è¯é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨GUIä»»åŠ¡è‡ªåŠ¨åŒ–ä¸Šï¼Œé™åˆ¶äº†å…¶åœ¨å„ç§GUIåœºæ™¯ä¸­çš„åº”ç”¨èŒƒå›´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå½¢å¼åŒ–ä¸”å…¨é¢çš„ç¯å¢ƒï¼Œä»¥è¯„ä¼°è‡ªåŠ¨åŒ–GUIæµ‹è¯•çš„å…¨è¿‡ç¨‹ï¼ˆGTArenaï¼‰ï¼Œä¸ºå„ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­è¿è¡Œæä¾›äº†ä¸€ä¸ªå…¬å¹³ã€æ ‡å‡†åŒ–çš„ç¯å¢ƒã€‚æˆ‘ä»¬å°†æµ‹è¯•è¿‡ç¨‹åˆ†ä¸ºä¸‰å¤§å…³é”®å­ä»»åŠ¡ï¼šæµ‹è¯•æ„å›¾ç”Ÿæˆã€æµ‹è¯•ä»»åŠ¡æ‰§è¡Œå’ŒGUIç¼ºé™·æ£€æµ‹ï¼Œå¹¶åŸºäºè¿™äº›æ„å»ºäº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†è¿›è¡Œç»¼åˆè¯„ä»·ã€‚å®ƒä½¿ç”¨ä¸‰ç§æ•°æ®ç±»å‹å¯¹ä¸åŒçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼šçœŸå®çš„ç§»åŠ¨åº”ç”¨ç¨‹åºã€äººå·¥æ³¨å…¥ç¼ºé™·çš„ç§»åŠ¨åº”ç”¨ç¨‹åºå’Œåˆæˆæ•°æ®ï¼Œå½»åº•è¯„ä¼°äº†å®ƒä»¬åœ¨ç›¸å…³ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ¢ç´¢å¤šæ¨¡æ€è¯­è¨€å¤§å‹æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸­çš„æ€§èƒ½ä¸å®ƒä»¬åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­çš„é€šç”¨èƒ½åŠ›ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æœ€å…ˆè¿›çš„æ¨¡å‹ä¸­ï¼Œä¹Ÿå¾ˆéš¾åœ¨æ‰€æœ‰è‡ªåŠ¨åŒ–GUIæµ‹è¯•çš„å­ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†è‡ªåŠ¨GUIæµ‹è¯•çš„å®é™…èƒ½åŠ›ä¸å®é™…åº”ç”¨ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¿™ä¸€å·®è·ä¸ºGUIä»£ç†æœªæ¥çš„å‘å±•æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZJU-ACES-ISE/ChatUITest%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZJU-ACES-ISE/ChatUITestä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18426v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªæ­£å¼ä¸”å…¨é¢çš„ç¯å¢ƒ â€”â€” GTArenaï¼Œç”¨äºè¯„ä¼°è‡ªåŠ¨åŒ–GUIæµ‹è¯•çš„å…¨è¿‡ç¨‹ã€‚è¯¥ç¯å¢ƒå°†æµ‹è¯•è¿‡ç¨‹åˆ†ä¸ºæµ‹è¯•æ„å›¾ç”Ÿæˆã€æµ‹è¯•ä»»åŠ¡æ‰§è¡Œå’ŒGUIç¼ºé™·æ£€æµ‹ä¸‰ä¸ªå…³é”®å­ä»»åŠ¡ï¼Œå¹¶åŸºäºè¿™äº›æ„å»ºäº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¸®åŠ©ç ”ç©¶äººå‘˜æ¢ç´¢ç‰¹å®šåœºæ™¯ä¸­å¤šæ¨¡æ€è¯­è¨€å¤§æ¨¡å‹çš„æ€§èƒ½ä¸å…¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­çš„é€šç”¨èƒ½åŠ›ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–GUIæµ‹è¯•çš„æ‰€æœ‰å­ä»»åŠ¡ä¸­éƒ½è¡¨ç°è‰¯å¥½ä»ç„¶å­˜åœ¨å·®è·ï¼Œå¼ºè°ƒäº†è‡ªä¸»GUIæµ‹è¯•çš„å®é™…ç°å®ä¸–ç•Œåº”ç”¨ä¸å®é™…åº”ç”¨ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œä¸ºæœªæ¥GUIä»£ç†å¼€å‘æä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰GUIä»£ç†çš„ç ”ç©¶é›†ä¸­åœ¨GUIä»»åŠ¡è‡ªåŠ¨åŒ–ä¸Šï¼Œä½†å…¶åº”ç”¨èŒƒå›´æœ‰é™ã€‚</li>
<li>GTArenaç¯å¢ƒç”¨äºè¯„ä¼°è‡ªåŠ¨åŒ–GUIæµ‹è¯•çš„å…¨è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æµ‹è¯•æ„å›¾ç”Ÿæˆã€æµ‹è¯•ä»»åŠ¡æ‰§è¡Œå’ŒGUIç¼ºé™·æ£€æµ‹ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºå…¨é¢çš„è¯„ä¼°æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–GUIæµ‹è¯•ä¸­çš„è¡¨ç°ã€‚</li>
<li>ä½¿ç”¨ä¸‰ç§æ•°æ®ç±»å‹è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼šçœŸå®ç§»åŠ¨åº”ç”¨ç¨‹åºã€äººå·¥æ³¨å…¥ç¼ºé™·çš„ç§»åŠ¨åº”ç”¨ç¨‹åºå’Œåˆæˆæ•°æ®ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–GUIæµ‹è¯•çš„æ‰€æœ‰å­ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰¯å¥½ä½†ä»æœ‰å·®è·ã€‚</li>
<li>å¤šæ¨¡æ€è¯­è¨€å¤§æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸­çš„æ€§èƒ½ä¸å…¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­çš„é€šç”¨èƒ½åŠ›ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-013c983b0f9b77146a0ba34bd9f4a4e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-783c4167b2fb226db1dd09d33e4072c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58bc6ec476c7e5ccd33ef106d5ae9e69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b602886ba19034f6485f229d0833b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c325a48d9f14de4aed9df708a080063.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Muse-A-Multimodal-Conversational-Recommendation-Dataset-with-Scenario-Grounded-User-Profiles"><a href="#Muse-A-Multimodal-Conversational-Recommendation-Dataset-with-Scenario-Grounded-User-Profiles" class="headerlink" title="Muse: A Multimodal Conversational Recommendation Dataset with   Scenario-Grounded User Profiles"></a>Muse: A Multimodal Conversational Recommendation Dataset with   Scenario-Grounded User Profiles</h2><p><strong>Authors:Zihan Wang, Xiaocui Yang, Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang</strong></p>
<p>Current conversational recommendation systems focus predominantly on text. However, real-world recommendation settings are generally multimodal, causing a significant gap between existing research and practical applications. To address this issue, we propose Muse, the first multimodal conversational recommendation dataset. Muse comprises 83,148 utterances from 7,000 conversations centered around the Clothing domain. Each conversation contains comprehensive multimodal interactions, rich elements, and natural dialogues. Data in Muse are automatically synthesized by a multi-agent framework powered by multimodal large language models (MLLMs). It innovatively derives user profiles from real-world scenarios rather than depending on manual design and history data for better scalability, and then it fulfills conversation simulation and optimization. Both human and LLM evaluations demonstrate the high quality of conversations in Muse. Additionally, fine-tuning experiments on three MLLMs demonstrate Museâ€™s learnable patterns for recommendations and responses, confirming its value for multimodal conversational recommendation. Our dataset and codes are available at \url{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Muse-0086%7D">https://anonymous.4open.science/r/Muse-0086}</a>. </p>
<blockquote>
<p>å½“å‰çš„å¯¹è°ˆæ¨èç³»ç»Ÿä¸»è¦èšç„¦äºæ–‡æœ¬ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æ¨èè®¾ç½®é€šå¸¸æ˜¯å¤šæ¨¡å¼çš„ï¼Œå¯¼è‡´ç°æœ‰ç ”ç©¶å’Œå®é™…åº”ç”¨ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Museï¼Œè¿™æ˜¯é¦–ä¸ªå¤šæ¨¡å¼å¯¹è°ˆæ¨èæ•°æ®é›†ã€‚Museç”±7000æ¬¡å›´ç»•æœè£…é¢†åŸŸçš„å¯¹è¯ä¸­çš„83,148æ¡è¨€è¯­ç»„æˆã€‚æ¯æ¡å¯¹è¯éƒ½åŒ…å«å…¨é¢çš„å¤šæ¨¡å¼äº’åŠ¨ã€ä¸°å¯Œçš„å…ƒç´ ä»¥åŠè‡ªç„¶å¯¹è¯ã€‚Museä¸­çš„æ•°æ®ç”±å¤šæ™ºèƒ½ä½“æ¡†æ¶é€šè¿‡å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è‡ªåŠ¨åˆæˆã€‚å®ƒä»ç°å®åœºæ™¯ä¸­åˆ›æ–°åœ°æ¨å¯¼ç”¨æˆ·ç”»åƒï¼Œè€Œä¸æ˜¯ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡å’Œå†å²æ•°æ®æ¥å®ç°æ›´å¥½çš„å¯æ‰©å±•æ€§ï¼Œç„¶åå®ƒå®ç°å¯¹è¯æ¨¡æ‹Ÿå’Œä¼˜åŒ–ã€‚äººç±»å’ŒLLMè¯„ä¼°å‡è¯æ˜äº†Museå¯¹è¯çš„é«˜è´¨é‡ã€‚æ­¤å¤–ï¼Œåœ¨ä¸‰ä¸ªMLLMä¸Šè¿›è¡Œå¾®è°ƒå®éªŒè¯æ˜äº†Museå¯¹æ¨èå’Œå›å¤çš„å¯å­¦ä¹ æ¨¡å¼ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡å¼å¯¹è°ˆæ¨èä¸­çš„ä»·å€¼ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Muse-0086%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/Muse-0086è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18416v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Museæ˜¯ç¬¬ä¸€å¥—å¤šæ¨¡æ€å¯¹è¯æ¨èæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨èç³»ç»Ÿä¸ç°å®éœ€æ±‚ä¹‹é—´çš„é¸¿æ²Ÿã€‚æ•°æ®é›†åŒ…å«å›´ç»•æœè£…é¢†åŸŸçš„7000ä¸ªå¯¹è¯ä¸­çš„83,148ä¸ªè¯­å¥ï¼Œæ¯ä¸ªå¯¹è¯éƒ½åŒ…å«å…¨é¢çš„å¤šæ¨¡æ€äº¤äº’ã€ä¸°å¯Œçš„å…ƒç´ å’Œè‡ªç„¶å¯¹è¯ã€‚æ•°æ®é›†ç”±å¤šä»£ç†æ¡†æ¶è‡ªåŠ¨åˆæˆï¼Œè¯¥æ¡†æ¶åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚Museé€šè¿‡ä»ç°å®åœºæ™¯ä¸­åˆ›æ–°åœ°è¡ç”Ÿç”¨æˆ·èµ„æ–™æ¥æé«˜å¯æ‰©å±•æ€§ï¼Œå®ç°äº†å¯¹è¯æ¨¡æ‹Ÿå’Œä¼˜åŒ–ã€‚ç»è¿‡äººç±»å’ŒLLMè¯„ä¼°ï¼Œè¯æ˜äº†Museä¸­å¯¹è¯çš„é«˜è´¨é‡ã€‚æ­¤å¤–ï¼Œå¯¹ä¸‰ä¸ªMLLMçš„å¾®è°ƒå®éªŒè¯æ˜äº†Museåœ¨æ¨èå’Œå“åº”æ–¹é¢çš„å­¦ä¹ æ¨¡å¼ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Museæ˜¯å¤šæ¨¡æ€å¯¹è¯æ¨èæ•°æ®é›†çš„é¦–ä¸ªå°è¯•ï¼Œæ—¨åœ¨ç¼©å°ç°æœ‰æ¨èç³»ç»Ÿä¸å®é™…åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–å¤šä¸ªå›´ç»•æœè£…é¢†åŸŸçš„å¯¹è¯ï¼Œæ¯ä¸ªå¯¹è¯éƒ½åŒ…å«å…¨é¢çš„å¤šæ¨¡æ€äº¤äº’å’Œä¸°å¯Œçš„å…ƒç´ ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡å¤šä»£ç†æ¡†æ¶è‡ªåŠ¨åˆæˆï¼ŒåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
<li>Museé€šè¿‡ä»ç°å®åœºæ™¯ä¸­è¡ç”Ÿç”¨æˆ·èµ„æ–™æ¥æé«˜å¯æ‰©å±•æ€§ï¼Œå¹¶å®ç°å¯¹è¯æ¨¡æ‹Ÿå’Œä¼˜åŒ–ã€‚</li>
<li>æ•°æ®é›†çš„é«˜è´¨é‡å¯¹è¯å¾—åˆ°äº†äººç±»å’ŒLLMè¯„ä¼°çš„éªŒè¯ã€‚</li>
<li>é€šè¿‡å¾®è°ƒå®éªŒè¯æ˜ï¼ŒMuseå¯¹æ¨èå’Œå“åº”çš„å­¦ä¹ æ¨¡å¼æœ‰ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a618b8839c7ee7a2ade2b336059872a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d396cf6abef98f818a485ab524562e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c0e72603bc747a7323f06de834b4b13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b7e1ab2096fc356d563cf585ec25d29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d15d24e3cbf86aebe98afbbeaa28d37c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cd258e3a0b1c4a51235cf7088c4e0b1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Multilingual-Mathematical-Reasoning-Advancing-Open-Source-LLMs-in-Hindi-and-English"><a href="#Multilingual-Mathematical-Reasoning-Advancing-Open-Source-LLMs-in-Hindi-and-English" class="headerlink" title="Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi   and English"></a>Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi   and English</h2><p><strong>Authors:Avinash Anand, Kritarth Prasad, Chhavi Kirtani, Ashwin R Nair, Manvendra Kumar Nema, Raj Jaiswal, Rajiv Ratn Shah</strong></p>
<p>Large Language Models (LLMs) excel in linguistic tasks but struggle with mathematical reasoning, particularly in non English languages like Hindi. This research aims to enhance the mathematical reasoning skills of smaller, resource efficient open-source LLMs in both Hindi and English. We evaluate models like OpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B, Gemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods, and supervised fine-tuning. Our approach incorporates curriculum learning, progressively training models on increasingly difficult problems, a novel Decomposition Strategy to simplify complex arithmetic operations, and a Structured Solution Design that divides solutions into phases. Our experiments result in notable performance enhancements. WizardMath 7B exceeds Geminiâ€™s accuracy on English datasets by +6% and matches Geminiâ€™s performance on Hindi datasets. Adopting a bilingual approach that combines English and Hindi samples achieves results comparable to individual language models, demonstrating the capability to learn mathematical reasoning in both languages. This research highlights the potential for improving mathematical reasoning in open-source LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿è¯­è¨€ä»»åŠ¡ï¼Œä½†åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨éè‹±è¯­è¯­ç§å¦‚å°åœ°è¯­æ–¹é¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å°å‹ã€èµ„æºé«˜æ•ˆçš„å¼€æºLLMåœ¨å°åœ°è¯­å’Œè‹±è¯­ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬æ€ç»´é“¾æ–¹æ³•è¯„ä¼°äº†OpenHathi 7Bã€LLaMA-2 7Bã€WizardMath 7Bã€Mistral 7Bã€LLeMMa 7Bã€MAmmoTH 7Bã€Gemini Proå’ŒGPT-4ç­‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬è¯¾ç¨‹å­¦ä¹ ï¼ˆé€æ¸åœ¨éš¾åº¦æ›´å¤§çš„é—®é¢˜ä¸Šè®­ç»ƒæ¨¡å‹ï¼‰ã€ä¸€ç§ç®€åŒ–çš„å¤æ‚ç®—æœ¯è¿ç®—çš„åˆ†è§£ç­–ç•¥ä»¥åŠåˆ†é˜¶æ®µçš„ç»“æ„åŒ–è§£å†³æ–¹æ¡ˆè®¾è®¡ã€‚æˆ‘ä»¬çš„å®éªŒå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚WizardMath 7Båœ¨è‹±è¯­æ•°æ®é›†ä¸Šçš„å‡†ç¡®åº¦è¶…è¿‡äº†Geminiçš„å‡†ç¡®åº¦+6%ï¼Œå¹¶åœ¨å°åœ°è¯­æ•°æ®é›†ä¸Šä¸Geminiæ€§èƒ½ç›¸åŒ¹é…ã€‚é‡‡ç”¨ç»“åˆè‹±è¯­å’Œå°åœ°è¯­æ ·æœ¬çš„åŒè¯­æ–¹æ³•å–å¾—äº†ä¸å•ä¸€è¯­è¨€æ¨¡å‹ç›¸å½“çš„ç»“æœï¼Œè¯æ˜äº†åœ¨ä¸¤ç§è¯­è¨€ä¸­å­¦ä¹ æ•°å­¦æ¨ç†çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†æ”¹è¿›å¼€æºLLMæ•°å­¦æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18415v1">PDF</a> Accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨æé«˜å°å‹å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°åœ°è¯­å’Œè‹±è¯­ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡é‡‡ç”¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬é“¾æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•å’Œç›‘ç£å¾®è°ƒï¼Œç»“åˆè¯¾ç¨‹å­¦ä¹ ã€åˆ†è§£ç­–ç•¥å’Œç»“æ„åŒ–è§£å†³æ–¹æ¡ˆç­‰æ–¹æ³•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹æ€§èƒ½å¾—åˆ°æ˜¾è‘—æé«˜ã€‚ä¾‹å¦‚ï¼ŒWizardMath 7Båœ¨è‹±è¯­æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æ¯”Geminié«˜å‡º+6%ï¼Œå¹¶ä¸”ä¸Geminiåœ¨å°åœ°è¯­æ•°æ®é›†ä¸Šçš„è¡¨ç°ç›¸åŒ¹é…ã€‚é‡‡ç”¨ç»“åˆè‹±è¯­å’Œå°åœ°è¯­æ ·æœ¬çš„åŒè¯­æ–¹æ³•å–å¾—äº†ä¸å•ä¸€è¯­è¨€æ¨¡å‹ç›¸å½“çš„ç»“æœï¼Œè¯æ˜äº†åœ¨ä¸¤ç§è¯­è¨€ä¸­å­¦ä¹ æ•°å­¦æ¨ç†çš„èƒ½åŠ›ã€‚è¿™é¡¹ç ”ç©¶çªå‡ºäº†æé«˜å¼€æºLLMæ•°å­¦æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‹±è¯­ä»¥å¤–çš„è¯­è¨€ï¼Œå¦‚å°åœ°è¯­ä¸­ï¼Œæ•°å­¦æ¨ç†èƒ½åŠ›å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡åœ¨äºæé«˜å°å‹ã€èµ„æºæ•ˆç‡é«˜çš„å¼€æºLLMåœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡é›¶æ ·æœ¬ã€å°‘æ ·æœ¬é“¾æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•å’Œç›‘ç£å¾®è°ƒæ¥è¯„ä¼°æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ ã€åˆ†è§£ç­–ç•¥å’Œç»“æ„åŒ–è§£å†³æ–¹æ¡ˆç­‰æ–¹æ³•æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>WizardMath 7Båœ¨è‹±è¯­æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºGeminiï¼Œå‡†ç¡®ç‡æé«˜+6%ã€‚</li>
<li>åœ¨å°åœ°è¯­æ•°æ®é›†ä¸Šï¼ŒWizardMath 7Bä¸Geminiè¡¨ç°ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-623daf068fb41d56ac955bce4d5d4eb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c70f6cfea30e16936ee64146ae5a157.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9983212f3d5eccfe5e23d60d601b1ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2e62d456b8d4f03a4d01323d42a5d91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e17f652cdf9743d638148e84b5707dc4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models"><a href="#Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models" class="headerlink" title="Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models"></a>Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models</h2><p><strong>Authors:Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu</strong></p>
<p>Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in <a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºé—®ç­”ã€æœºå™¨ç¿»è¯‘ç­‰å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡æ³¨æ•°æ®å’Œç”ŸåŒ–å±æ€§æ‰‹åŠ¨æ ‡æ³¨çš„å›°éš¾ï¼Œåˆ†å­ç”Ÿæˆä»»åŠ¡æ€§èƒ½ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠå¤šå±æ€§çº¦æŸçš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤æ­¥æ¡†æ¶PEITï¼ˆå±æ€§å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼‰ï¼Œä»¥æ”¹è¿›åˆ†å­ç›¸å…³ä»»åŠ¡çš„LLMã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æè¿°ã€SMILESå’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä½œä¸ºå¤šæ¨¡å¼è¾“å…¥ï¼Œé€šè¿‡å¯¹é½å¤šæ¨¡å¼è¡¨ç¤ºæ¥åˆæˆæŒ‡ä»¤æ•°æ®ï¼Œä»è€Œé¢„è®­ç»ƒä¸€ä¸ªåä¸ºPEIT-GENçš„æ¨¡å‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆæ•°æ®å¯¹ç°æœ‰çš„å¼€æºLLMè¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°çš„PEIT-LLMå¯ä»¥å¤„ç†åˆ†å­æè¿°ã€åŸºäºæ–‡æœ¬çš„åˆ†å­ç”Ÿæˆã€åˆ†å­å±æ€§é¢„æµ‹ä»¥åŠæˆ‘ä»¬æ–°æå‡ºçš„å¤šçº¦æŸåˆ†å­ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬é¢„è®­ç»ƒçš„PEIT-GENåœ¨åˆ†å­æè¿°æ–¹é¢ä¼˜äºMolT5å’ŒBioT5ï¼Œè¯æ˜äº†æ–‡æœ¬æè¿°ã€ç»“æ„å’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä¹‹é—´çš„æ¨¡æ€å¯¹é½è‰¯å¥½ã€‚æ­¤å¤–ï¼ŒPEIT-LLMåœ¨å¤šä»»åŠ¡åˆ†å­ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ”¹è¿›ï¼Œè¯æ˜äº†PEITæ¡†æ¶åœ¨å„ç§åˆ†å­ä»»åŠ¡ä¸­çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>å‘å¸ƒäº†ä»£ç ã€æ„å»ºæŒ‡ä»¤æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18084v1">PDF</a> </p>
<p><strong>Summary</strong><br>LLMåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡ç ”ç©¶å–å¾—äº†æ–°è¿›å±•ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPEITçš„ä¸¤æ­¥æ¡†æ¶ï¼Œç”¨äºå¢å¼ºæŒ‡ä»¤è°ƒä¼˜å¹¶åº”å¯¹å„ç§åˆ†å­ä»»åŠ¡ï¼Œå¦‚åˆ†å­æ ‡é¢˜æè¿°ã€åŸºäºæ–‡æœ¬åˆ†å­çš„ç”Ÿæˆå’Œé¢„æµ‹ç­‰ã€‚é¢„è®­ç»ƒçš„PEIT-GENè¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œåœ¨PEITæ¡†æ¶ä¸‹è¿›ä¸€æ­¥è°ƒæ•´äº†ç°æœ‰å¼€æºLLMã€‚æ­¤ç ”ç©¶è¯å®äº†æ¨¡æ€ä¹‹é—´çš„è‰¯å¥½å¯¹é½å’ŒPEITæ¡†æ¶çš„å¯æ‰©å±•æ€§ã€‚å…·ä½“ä¿¡æ¯å’Œç ”ç©¶æˆæœå‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¹¿æ³›åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä½†åœ¨å¤„ç†æ¶‰åŠç”Ÿç‰©åŒ–å­¦ç‰¹æ€§çš„ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>PEITæ¡†æ¶åˆ†ä¸ºä¸¤æ­¥ï¼šç¬¬ä¸€æ­¥æ˜¯é¢„è®­ç»ƒä¸€ä¸ªåä¸ºPEIT-GENçš„æ¨¡å‹ï¼Œä½¿ç”¨æ–‡æœ¬æè¿°ã€SMILESå’Œç”Ÿç‰©åŒ–å­¦ç‰¹æ€§ä½œä¸ºå¤šæ¨¡æ€è¾“å…¥ï¼›ç¬¬äºŒæ­¥æ˜¯å¯¹ç°æœ‰å¼€æºLLMè¿›è¡Œå¾®è°ƒï¼Œä»¥å¤„ç†åˆ†å­ç›¸å…³ä»»åŠ¡ã€‚</li>
<li>PEIT-GENæ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å­æ ‡é¢˜æè¿°æ–¹é¢ï¼ŒéªŒè¯äº†æ–‡æœ¬æè¿°ã€ç»“æ„å’Œç”Ÿç‰©åŒ–å­¦ç‰¹æ€§ä¹‹é—´çš„è‰¯å¥½å¯¹é½ã€‚</li>
<li>PEITæ¡†æ¶å±•ç¤ºäº†åœ¨å¤šä»»åŠ¡åˆ†å­ç”Ÿæˆæ–¹é¢çš„å·¨å¤§æ”¹è¿›ï¼Œè¡¨æ˜è¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡åœ¨GitHubä¸Šå‘å¸ƒä»£ç ã€æ„å»ºæŒ‡ä»¤æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹æ¥åˆ†äº«å…¶æˆæœã€‚</li>
<li>PEITæ¡†æ¶æœ‰åŠ©äºæé«˜LLMåœ¨å¤„ç†æ¶‰åŠç”Ÿç‰©åŒ–å­¦ç‰¹æ€§çš„ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œè¿™å¯¹äºè¯ç‰©å‘ç°ã€åŒ–å­¦ä¿¡æ¯å­¦ç­‰é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9facab36a4f10a26a987a098319d3e57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8bc599ee5f838396df94302610447b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82277305e4fd457a7cab55de7f25439e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-512e598551be4bc0458c6d92a671a4fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adc3722069c415319569254ce05c5147.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="YuLan-Mini-An-Open-Data-efficient-Language-Model"><a href="#YuLan-Mini-An-Open-Data-efficient-Language-Model" class="headerlink" title="YuLan-Mini: An Open Data-efficient Language Model"></a>YuLan-Mini: An Open Data-efficient Language Model</h2><p><strong>Authors:Yiwen Hu, Huatong Song, Jia Deng, Jiapeng Wang, Jie Chen, Kun Zhou, Yutao Zhu, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, Ji-Rong Wen</strong></p>
<p>Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: <a target="_blank" rel="noopener" href="https://github.com/RUC-GSAI/YuLan-Mini">https://github.com/RUC-GSAI/YuLan-Mini</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆé¢„è®­ç»ƒä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå…¶èµ„æºéœ€æ±‚å·¨å¤§ï¼Œæ¶‰åŠçš„æŠ€æœ¯è¿‡ç¨‹éå¸¸å¤æ‚ã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†YuLan-Miniçš„è¯¦ç»†æŠ€æœ¯æŠ¥å‘Šï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰2.42äº¿å‚æ•°çš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨åŒç±»å‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­è¾¾åˆ°äº†é¡¶çº§æ€§èƒ½ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒæ–¹æ³•é€šè¿‡ä¸‰ä¸ªå…³é”®æŠ€æœ¯è´¡çŒ®æ¥æé«˜è®­ç»ƒæ•ˆæœï¼šç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç®¡é“ç»“åˆäº†æ•°æ®æ¸…ç†å’Œæ•°æ®è°ƒåº¦ç­–ç•¥ï¼Œä¸€ç§ç¨³å¥çš„ä¼˜åŒ–æ–¹æ³•æ¥å‡è½»è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œä»¥åŠæœ‰æ•ˆçš„é€€ç«æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç›®æ ‡æ•°æ®é€‰æ‹©å’Œé•¿ä¸Šä¸‹æ–‡è®­ç»ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒYuLan-Miniåœ¨1.08ä¸‡äº¿æ ‡è®°çš„è®­ç»ƒä¸‹ï¼Œå®ç°äº†ä¸éœ€è¦å¤§é‡æ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ä¸ºäº†æ–¹ä¾¿å¤åˆ¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æ¯ä¸ªè®­ç»ƒé˜¶æ®µæ•°æ®ç»„åˆçš„å…¨éƒ¨ç»†èŠ‚ã€‚é¡¹ç›®è¯¦æƒ…å¯è®¿é—®ä»¥ä¸‹é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/RUC-GSAI/YuLan-Mini%E3%80%82">https://github.com/RUC-GSAI/YuLan-Miniã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17743v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>YuLan-Miniæ˜¯ä¸€ä¸ªæ‹¥æœ‰å¼ºå¤§æ€§èƒ½çš„åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰2.42äº¿å‚æ•°ï¼Œåœ¨ç±»ä¼¼å‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚å…¶é¢„è®­ç»ƒç­–ç•¥é€šè¿‡ä¸‰ä¸ªå…³é”®æŠ€æœ¯è´¡çŒ®æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼šç²¾ç»†çš„æ•°æ®ç®¡é“ç»“åˆäº†æ•°æ®æ¸…ç†å’Œæ•°æ®è°ƒåº¦ç­–ç•¥ï¼Œç¨³å¥çš„ä¼˜åŒ–æ–¹æ³•å‡è½»äº†è®­ç»ƒçš„ä¸ç¨³å®šæ€§ï¼Œä»¥åŠæœ‰æ•ˆçš„é€€ç«æ–¹æ³•ç»“åˆäº†ç›®æ ‡æ•°æ®é€‰æ‹©å’Œé•¿è¯­å¢ƒè®­ç»ƒã€‚åœ¨ä»…è®­ç»ƒäº†1.08ä¸‡äº¿ä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼ŒYuLan-Miniçš„è¡¨ç°ä¸éœ€è¦å¤§é‡æ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>YuLan-Miniæ˜¯ä¸€ä¸ªå…·æœ‰2.42äº¿å‚æ•°çš„åŸºç¡€æ¨¡å‹ï¼Œæ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>è¯¥æ¨¡å‹çš„é¢„è®­ç»ƒç­–ç•¥åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æŠ€æœ¯è´¡çŒ®ï¼Œåˆ†åˆ«æ˜¯æ•°æ®ç®¡é“ã€ä¼˜åŒ–æ–¹æ³•å’Œé€€ç«æ–¹æ³•ã€‚</li>
<li>æ•°æ®ç®¡é“ç»“åˆäº†æ•°æ®æ¸…ç†å’Œæ•°æ®è°ƒåº¦ç­–ç•¥ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ä¼˜åŒ–æ–¹æ³•æ—¨åœ¨å‡è½»è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šæ€§ã€‚</li>
<li>é€€ç«æ–¹æ³•ç»“åˆäº†ç›®æ ‡æ•°æ®é€‰æ‹©å’Œé•¿è¯­å¢ƒè®­ç»ƒï¼Œå¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>YuLan-Miniåœ¨ä»…è®­ç»ƒäº†1.08ä¸‡äº¿ä»¤ç‰Œçš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ï¼Œä¸éœ€è¦å¤§é‡æ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe971276f045e9d7029f52f57519b595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ee824cbdad627397c92a8bb620cc186.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc60d420c15664de124cc051aafab70f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-283b0d3aeaa210624fb80d95d900a582.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5bc71fc40e20950c01e7086683ecd2d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="System-2-Mathematical-Reasoning-via-Enriched-Instruction-Tuning"><a href="#System-2-Mathematical-Reasoning-via-Enriched-Instruction-Tuning" class="headerlink" title="System-2 Mathematical Reasoning via Enriched Instruction Tuning"></a>System-2 Mathematical Reasoning via Enriched Instruction Tuning</h2><p><strong>Authors:Huanqia Cai, Yijun Yang, Zhifeng Li</strong></p>
<p>Solving complex mathematical problems via system-2 reasoning is a natural human skill, yet it remains a significant challenge for current large language models (LLMs). We identify the scarcity of deliberate multi-step reasoning data as a primary limiting factor. To this end, we introduce Enriched Instruction Tuning (EIT), a method that enriches existing human-annotated mathematical datasets by synergizing human and AI feedback to create fine-grained reasoning trajectories. These datasets are then used to fine-tune open-source LLMs, enhancing their mathematical reasoning abilities without reliance on any symbolic verification program. Concretely, EIT is composed of two critical steps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step (ERS). The former generates a high-level plan that breaks down complex instructions into a sequence of simpler objectives, while ERS fills in reasoning contexts often overlooked by human annotators, creating a smoother reasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods that generate reasoning chains only depending on LLMâ€™s internal knowledge, our method leverages human-annotated initial answers as &#96;&#96;meta-knowledgeâ€™â€™ to help LLMs generate more detailed and precise reasoning processes, leading to a more trustworthy LLM expert for complex mathematical problems. In experiments, EIT achieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing state-of-the-art fine-tuning and prompting methods, and even matching the performance of tool-augmented methods. </p>
<blockquote>
<p>é€šè¿‡ç³»ç»Ÿ2ç†æ€§è§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜æ˜¯ä¸€é¡¹äººç±»è‡ªç„¶æŠ€èƒ½ï¼Œä½†ä»ç„¶æ˜¯å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç¡®å®šäº†ç¼ºä¹æœ‰æ„è¯†çš„å¤šæ­¥éª¤æ¨ç†æ•°æ®æ˜¯ä¸»è¦é™åˆ¶å› ç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸°å¯ŒæŒ‡ä»¤è®­ç»ƒï¼ˆEITï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ååŒäººç±»å’Œäººå·¥æ™ºèƒ½åé¦ˆæ¥ä¸°å¯Œç°æœ‰äººç±»æ³¨é‡Šçš„æ•°å­¦æ•°æ®é›†çš„æ–¹æ³•ï¼Œä»¥åˆ›å»ºç²¾ç»†çš„æ¨ç†è½¨è¿¹ã€‚è¿™äº›æ•°æ®é›†éšåç”¨äºå¾®è°ƒå¼€æºLLMï¼Œå¢å¼ºå®ƒä»¬çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–ä»»ä½•ç¬¦å·éªŒè¯ç¨‹åºã€‚å…·ä½“æ¥è¯´ï¼ŒEITç”±ä¸¤ä¸ªå…³é”®æ­¥éª¤ç»„æˆï¼šä¸°å¯Œæ¨ç†è®¡åˆ’ï¼ˆERPï¼‰å’Œä¸°å¯Œæ¨ç†æ­¥éª¤ï¼ˆERSï¼‰ã€‚å‰è€…ç”Ÿæˆä¸€ä¸ªé«˜çº§è®¡åˆ’ï¼Œå°†å¤æ‚çš„æŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç³»åˆ—ç®€å•çš„ç›®æ ‡ï¼Œè€ŒERSåˆ™å¡«è¡¥äº†äººç±»æ³¨é‡Šè€…ç»å¸¸å¿½ç•¥çš„æ¨ç†èƒŒæ™¯ï¼Œä¸ºLLMå¾®è°ƒåˆ›å»ºæ›´å¹³æ»‘çš„æ¨ç†è½¨è¿¹ã€‚ä¸ç°æœ‰çš„ä»…ä¾èµ–äºLLMå†…éƒ¨çŸ¥è¯†çš„CoTæç¤ºæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äººç±»æ³¨é‡Šçš„åˆå§‹ç­”æ¡ˆä½œä¸ºâ€œå…ƒçŸ¥è¯†â€ï¼Œå¸®åŠ©LLMç”Ÿæˆæ›´è¯¦ç»†ã€æ›´ç²¾ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œä½¿LLMæˆä¸ºè§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„æ›´å¯ä¿¡èµ–çš„ä¸“å®¶ã€‚åœ¨å®éªŒä¸­ï¼ŒEITåœ¨GSM8Kä¸Šè¾¾åˆ°äº†84.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨MATHä¸Šè¾¾åˆ°äº†32.5%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†æœ€æ–°çš„å¾®è°ƒæç¤ºæ–¹æ³•ï¼Œç”šè‡³ä¸å·¥å…·å¢å¼ºæ–¹æ³•çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16964v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é€šè¿‡ç³»ç»Ÿ2æ¨ç†è§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜æ˜¯ä¸€é¡¹äººç±»çš„è‡ªç„¶èƒ½åŠ›ï¼Œä½†å¯¹äºå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸»è¦é™åˆ¶å› ç´ ä¹‹ä¸€æ˜¯ç¼ºä¹æœ‰æ„è¯†çš„å¤šæ­¥éª¤æ¨ç†æ•°æ®ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥äº†å¯Œé›†æŒ‡ä»¤è°ƒä¼˜ï¼ˆEITï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ååŒäººç±»å’ŒAIåé¦ˆæ¥ä¸°å¯Œç°æœ‰çš„äººç±»æ³¨é‡Šæ•°å­¦æ•°æ®é›†ï¼Œåˆ›å»ºç²¾ç»†çš„æ¨ç†è½¨è¿¹ã€‚ç„¶åï¼Œè¿™äº›æ•°æ®é›†è¢«ç”¨æ¥å¾®è°ƒå¼€æºLLMï¼Œå¢å¼ºå®ƒä»¬çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–ä»»ä½•ç¬¦å·éªŒè¯ç¨‹åºã€‚å…·ä½“æ¥è¯´ï¼ŒEITåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šå¯Œé›†æ¨ç†è®¡åˆ’ï¼ˆERPï¼‰å’Œå¯Œé›†æ¨ç†æ­¥éª¤ï¼ˆERSï¼‰ã€‚ERPç”Ÿæˆé«˜çº§è®¡åˆ’ï¼Œå°†å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç³»åˆ—ç®€å•ç›®æ ‡ï¼Œè€ŒERSå¡«è¡¥äº†äººç±»æ³¨é‡Šè€…ç»å¸¸å¿½ç•¥çš„æ¨ç†èƒŒæ™¯ï¼Œä¸ºLLMå¾®è°ƒåˆ›å»ºæ›´å¹³æ»‘çš„æ¨ç†è½¨è¿¹ã€‚ä¸åŒäºä»…ä¾èµ–LLMå†…éƒ¨çŸ¥è¯†çš„ç°æœ‰CoTæç¤ºæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äººç±»æ³¨é‡Šçš„åˆå§‹ç­”æ¡ˆä½œä¸ºâ€œå…ƒçŸ¥è¯†â€æ¥å¸®åŠ©LLMç”Ÿæˆæ›´è¯¦ç»†ã€æ›´ç²¾ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œä½¿LLMæˆä¸ºè§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„æ›´å¯ä¿¡èµ–çš„ä¸“å®¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§£å†³å¤æ‚æ•°å­¦é—®é¢˜å¯¹äºLLMæ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç¼ºä¹å¤šæ­¥éª¤æ¨ç†æ•°æ®æ˜¯ä¸»è¦é™åˆ¶å› ç´ ã€‚</li>
<li>å¼•å…¥Enriched Instruction Tuning (EIT)æ–¹æ³•æ¥å¢å¼ºLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>EITé€šè¿‡ååŒäººç±»å’ŒAIåé¦ˆä¸°å¯Œç°æœ‰çš„äººç±»æ³¨é‡Šæ•°å­¦æ•°æ®é›†ã€‚</li>
<li>EITåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šç”Ÿæˆé«˜çº§è®¡åˆ’çš„ERPå’Œå¡«è¡¥æ¨ç†èƒŒæ™¯çš„ERSã€‚</li>
<li>EITåˆ©ç”¨äººç±»æ³¨é‡Šçš„åˆå§‹ç­”æ¡ˆä½œä¸ºâ€œå…ƒçŸ¥è¯†â€ï¼Œå¸®åŠ©LLMç”Ÿæˆæ›´è¯¦ç»†ã€ç²¾ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>EITåœ¨GSM8Kä¸Šè¾¾åˆ°84.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨MATHä¸Šè¾¾åˆ°32.5%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¾®è°ƒæ–¹æ³•å’Œæç¤ºæ–¹æ³•ï¼Œç”šè‡³ä¸å·¥å…·å¢å¼ºæ–¹æ³•çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</li>
<li>EITæ–¹æ³•ä¸ºLLMåœ¨å¤„ç†å¤æ‚æ•°å­¦é—®é¢˜æ–¹é¢æä¾›äº†æ›´å¯é çš„ä¸“å®¶çº§è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f51c02af3ee0d27e4960d4d566d3b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a688e3a315843d5e31c4e58083f4cb4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e19dcdf0880026d32a267ba6662fba8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b39d70f8382b8d3ed44c3fc72c9b7d55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07848e27733cfcda25b6d11d5f04cc9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-802dd9034951db089614c63021a86751.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="HashEvict-A-Pre-Attention-KV-Cache-Eviction-Strategy-using-Locality-Sensitive-Hashing"><a href="#HashEvict-A-Pre-Attention-KV-Cache-Eviction-Strategy-using-Locality-Sensitive-Hashing" class="headerlink" title="HashEvict: A Pre-Attention KV Cache Eviction Strategy using   Locality-Sensitive Hashing"></a>HashEvict: A Pre-Attention KV Cache Eviction Strategy using   Locality-Sensitive Hashing</h2><p><strong>Authors:Minghui Liu, Tahseen Rabbani, Tony Oâ€™Halloran, Ananth Sankaralingam, Mary-Anne Hartley, Brian Gravelle, Furong Huang, Cornelia FermÃ¼ller, Yiannis Aloimonos</strong></p>
<p>Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce HashEvict, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, HashEvict makes these decisions pre-attention, thereby reducing computational costs. Additionally, HashEvict is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that HashEvict can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜æ¥å­˜å‚¨è¿‡å»ä»¤ç‰Œçš„å…³é”®å­—å’Œå€¼åµŒå…¥ï¼Œä»è€Œæ˜¾è‘—åŠ é€Ÿæ¨ç†ã€‚ç„¶è€Œï¼Œè¿™ä¸ªç¼“å­˜ä¼šæ¶ˆè€—å¤§é‡çš„GPUå†…å­˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†HashEvictï¼Œä¸€ç§ä½¿ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰æ¥å‹ç¼©KVç¼“å­˜çš„ç®—æ³•ã€‚HashEvictèƒ½å¤Ÿå¿«é€Ÿå®šä½ç¼“å­˜ä¸­ä¸å½“å‰æŸ¥è¯¢ä»¤ç‰Œä½™å¼¦ä¸ç›¸ä¼¼çš„ä»¤ç‰Œã€‚è¿™æ˜¯é€šè¿‡è®¡ç®—å½“å‰ä»¤ç‰ŒæŸ¥è¯¢çš„äºŒè¿›åˆ¶åŒ–é«˜æ–¯æŠ•å½±ä¸ç¼“å­˜ä»¤ç‰Œé”®ä¹‹é—´çš„æ±‰æ˜è·ç¦»æ¥å®ç°çš„ï¼ŒæŠ•å½±é•¿åº¦è¿œå°äºåµŒå…¥ç»´åº¦ã€‚æˆ‘ä»¬åœ¨GPUå†…å­˜ä¸­ç»´æŠ¤ä¸€ä¸ªè½»é‡çº§çš„äºŒè¿›åˆ¶ç»“æ„ï¼Œä»¥ä¿ƒè¿›è¿™äº›è®¡ç®—ã€‚ä¸è®¡ç®—æ³¨æ„åŠ›æ¥ç¡®å®šä»¤ç‰Œä¿ç•™çš„ç°æœ‰å‹ç¼©ç­–ç•¥ä¸åŒï¼ŒHashEvictåœ¨æ³¨æ„åŠ›ä¹‹å‰åšå‡ºè¿™äº›å†³ç­–ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒHashEvictå…·æœ‰åŠ¨æ€æ€§â€”â€”åœ¨æ¯ä¸€æ­¥è§£ç æ—¶ï¼Œå½“å‰ä»¤ç‰Œçš„å…³é”®å­—å’Œå€¼ä¼šæ›¿æ¢é¢„è®¡äº§ç”Ÿæœ€ä½æ³¨æ„åŠ›å¾—åˆ†çš„ä»¤ç‰Œçš„åµŒå…¥ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒHashEvictå¯ä»¥å°†KVç¼“å­˜å‹ç¼©30%-70%ï¼ŒåŒæ—¶åœ¨æ¨ç†ã€å¤šé¡¹é€‰æ‹©ã€é•¿ä¸Šä¸‹æ–‡æ£€ç´¢å’Œæ‘˜è¦ä»»åŠ¡ä¸­ä¿æŒé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16187v2">PDF</a> 10 pages, 6 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œä½†ä¼šæ¶ˆè€—å¤§é‡GPUå†…å­˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHashEvictçš„ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰æ¥å‹ç¼©KVç¼“å­˜ã€‚HashEvicté€šè¿‡è®¡ç®—å½“å‰æŸ¥è¯¢ä»¤ç‰Œä¸ç¼“å­˜ä»¤ç‰Œé”®çš„é«˜æ–¯æŠ•å½±ä¹‹é—´çš„æ±‰æ˜è·ç¦»ï¼Œå¿«é€Ÿå®šä½ä¸æŸ¥è¯¢ä»¤ç‰Œä½™å¼¦ä¸ç›¸ä¼¼çš„ä»¤ç‰Œã€‚ä¸ä¾èµ–æ³¨æ„åŠ›è®¡ç®—æ¥å†³å®šä¿ç•™ä»¤ç‰Œçš„ç°æœ‰å‹ç¼©ç­–ç•¥ä¸åŒï¼ŒHashEvictåœ¨æ³¨æ„åŠ›è®¡ç®—ä¹‹å‰åšå‡ºå†³ç­–ï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒHashEvictå…·æœ‰åŠ¨æ€æ€§ï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸€æ­¥è§£ç æ—¶æ›¿æ¢é¢„æœŸäº§ç”Ÿæœ€ä½æ³¨æ„åŠ›å¾—åˆ†çš„ä»¤ç‰Œçš„é”®å’Œå€¼ã€‚å®éªŒè¡¨æ˜ï¼ŒHashEvictå¯ä»¥å°†KVç¼“å­˜å‹ç¼©30%-70%ï¼ŒåŒæ—¶ä¿æŒåœ¨å„ç§ä»»åŠ¡ä¸Šçš„é«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based LLMsåˆ©ç”¨é”®å€¼ç¼“å­˜åŠ é€Ÿæ¨ç†ï¼Œä½†æ¶ˆè€—å¤§é‡GPUå†…å­˜ã€‚</li>
<li>HashEvictç®—æ³•ä½¿ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰æ¥å‹ç¼©KVç¼“å­˜ã€‚</li>
<li>HashEvicté€šè¿‡è®¡ç®—å½“å‰æŸ¥è¯¢ä»¤ç‰Œä¸ç¼“å­˜ä»¤ç‰Œé”®çš„æ±‰æ˜è·ç¦»å¿«é€Ÿå®šä½ç›¸å…³ä»¤ç‰Œã€‚</li>
<li>HashEvictåœ¨æ³¨æ„åŠ›è®¡ç®—å‰åšå‡ºå†³ç­–ï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>HashEvictå…·æœ‰åŠ¨æ€æ€§ï¼Œèƒ½åœ¨è§£ç è¿‡ç¨‹ä¸­æ›¿æ¢é¢„æœŸäº§ç”Ÿä½æ³¨æ„åŠ›å¾—åˆ†çš„ä»¤ç‰Œçš„é”®å’Œå€¼ã€‚</li>
<li>HashEvictå¯ä»¥å°†KVç¼“å­˜å‹ç¼©30%-70%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16187">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dbaf6691811cc46137189159fa975f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24c727e8b9b73c9a91bfe3bbbd9e06ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2578112a0e8ab0ce42e60b61dd9c3f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d58bf72c6118f81896de5d5725da9116.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Obfuscate-Code-A-Systematic-Analysis-of-Large-Language-Models-into-Assembly-Code-Obfuscation"><a href="#Can-LLMs-Obfuscate-Code-A-Systematic-Analysis-of-Large-Language-Models-into-Assembly-Code-Obfuscation" class="headerlink" title="Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models   into Assembly Code Obfuscation"></a>Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models   into Assembly Code Obfuscation</h2><p><strong>Authors:Seyedreza Mohseni, Seyedali Mohammadi, Deepa Tilwani, Yash Saxena, Gerald Ndawula, Sriram Vema, Edward Raff, Manas Gaur</strong></p>
<p>Malware authors often employ code obfuscations to make their malware harder to detect. Existing tools for generating obfuscated code often require access to the original source code (e.g., C++ or Java), and adding new obfuscations is a non-trivial, labor-intensive process. In this study, we ask the following question: Can Large Language Models (LLMs) potentially generate a new obfuscated assembly code? If so, this poses a risk to anti-virus engines and potentially increases the flexibility of attackers to create new obfuscation patterns. We answer this in the affirmative by developing the MetamorphASM benchmark comprising MetamorphASM Dataset (MAD) along with three code obfuscation techniques: dead code, register substitution, and control flow change. The MetamorphASM systematically evaluates the ability of LLMs to generate and analyze obfuscated code using MAD, which contains 328,200 obfuscated assembly code samples. We release this dataset and analyze the success rate of various LLMs (e.g., GPT-3.5&#x2F;4, GPT-4o-mini, Starcoder, CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly code. The evaluation was performed using established information-theoretic metrics and manual human review to ensure correctness and provide the foundation for researchers to study and develop remediations to this risk. The source code can be found at the following GitHub link: <a target="_blank" rel="noopener" href="https://github.com/mohammadi-ali/MetamorphASM">https://github.com/mohammadi-ali/MetamorphASM</a>. </p>
<blockquote>
<p>æ¶æ„è½¯ä»¶ä½œè€…ç»å¸¸ä½¿ç”¨ä»£ç æ··æ·†æŠ€æœ¯æ¥ä½¿ä»–ä»¬çš„æ¶æ„è½¯ä»¶æ›´éš¾è¢«æ£€æµ‹ã€‚ç°æœ‰çš„ç”Ÿæˆæ··æ·†ä»£ç çš„å·¥å…·é€šå¸¸éœ€è¦è®¿é—®åŸå§‹æºä»£ç ï¼ˆä¾‹å¦‚C++æˆ–Javaï¼‰ï¼Œå¹¶ä¸”æ·»åŠ æ–°çš„æ··æ·†æ˜¯ä¸€é¡¹éå¸¸ç¹çä¸”åŠ³åŠ¨å¯†é›†å‹çš„æµç¨‹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥ä¸‹é—®é¢˜ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦èƒ½å¤Ÿç”Ÿæˆæ–°çš„æ··æ·†æ±‡ç¼–ä»£ç ï¼Ÿå¦‚æœæ˜¯è¿™æ ·ï¼Œè¿™å¯¹æŠ—ç—…æ¯’å¼•æ“æ„æˆé£é™©ï¼Œå¹¶å¯èƒ½æé«˜æ”»å‡»è€…åˆ›å»ºæ–°æ··æ·†æ¨¡å¼çš„çµæ´»æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘MetamorphASMåŸºå‡†æµ‹è¯•æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æµ‹è¯•åŒ…æ‹¬MetamorphASMæ•°æ®é›†ï¼ˆMADï¼‰ä»¥åŠä¸‰ç§ä»£ç æ··æ·†æŠ€æœ¯ï¼šæ­»ä»£ç ã€å¯„å­˜å™¨æ›¿æ¢å’Œæ§åˆ¶æµæ›´æ”¹ã€‚MetamorphASMä½¿ç”¨MADç³»ç»Ÿåœ°è¯„ä¼°LLMç”Ÿæˆå’Œåˆ†ææ··æ·†ä»£ç çš„èƒ½åŠ›ï¼ŒMADåŒ…å«328,200ä¸ªæ··æ·†çš„æ±‡ç¼–ä»£ç æ ·æœ¬ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ­¤æ•°æ®é›†ï¼Œå¹¶åˆ†æäº†å„ç§LLMï¼ˆå¦‚GPT-3.5&#x2F;4ã€GPT-4o-miniã€Starcoderã€CodeGemmaã€CodeLlamaã€CodeT5å’ŒLLaMA 3.1ï¼‰åœ¨ç”Ÿæˆæ··æ·†æ±‡ç¼–ä»£ç æ–¹é¢çš„æˆåŠŸç‡ã€‚è¯„ä¼°æ˜¯é€šè¿‡å»ºç«‹çš„ä¿¡æ¯ç†è®ºæŒ‡æ ‡å’Œäººå·¥å®¡æŸ¥æ¥å®Œæˆçš„ï¼Œä»¥ç¡®ä¿æ­£ç¡®æ€§å¹¶ä¸ºç ”ç©¶äººå‘˜ç ”ç©¶å’Œå¼€å‘å¯¹æ­¤é£é™©çš„è¡¥æ•‘æªæ–½æä¾›åŸºç¡€ã€‚æºä»£ç å¯åœ¨ä»¥ä¸‹GitHubé“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/mohammadi-ali/MetamorphASM%E3%80%82">https://github.com/mohammadi-ali/MetamorphASMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16135v2">PDF</a> To appear in AAAI 2025, Main Track</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆæ–°çš„æ··æ·†æ±‡ç¼–ä»£ç æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶æ„å»ºäº†MetamorphASMåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«MetamorphASMæ•°æ®é›†ï¼ˆMADï¼‰å’Œä¸‰é‡ä»£ç æ··æ·†æŠ€æœ¯ï¼šæ­»ä»£ç ã€å¯„å­˜å™¨æ›¿æ¢å’Œæ§åˆ¶æµå˜åŒ–ã€‚é€šè¿‡MADåŒ…å«çš„å¤§é‡æ··æ·†æ±‡ç¼–ä»£ç æ ·æœ¬å¯¹LLMè¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå„ç§LLMï¼ˆå¦‚GPT-3.5&#x2F;4ç­‰ï¼‰åœ¨ç”Ÿæˆæ··æ·†æ±‡ç¼–ä»£ç æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„æˆåŠŸç‡ã€‚è¿™ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ç ”ç©¶å¹¶åˆ¶å®šè¡¥æ•‘æªæ–½çš„åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºç”Ÿæˆæ··æ·†æ±‡ç¼–ä»£ç ï¼Œå¯¹æŠ—ç—…æ¯’å¼•æ“æ„æˆé£é™©ã€‚</li>
<li>ç ”ç©¶å¼€å‘äº†MetamorphASMåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMç”Ÿæˆå’Œåˆ†ææ··æ·†ä»£ç çš„èƒ½åŠ›ã€‚</li>
<li>MetamorphASMæ•°æ®é›†ï¼ˆMADï¼‰åŒ…å«å¤§é‡æ··æ·†æ±‡ç¼–ä»£ç æ ·æœ¬ï¼Œç”¨äºè¯„ä¼°LLMçš„æ€§èƒ½ã€‚</li>
<li>ä¸åŒLLMæ¨¡å‹åœ¨ç”Ÿæˆæ··æ·†æ±‡ç¼–ä»£ç æ–¹é¢çš„æˆåŠŸç‡æœ‰æ‰€å·®å¼‚ã€‚</li>
<li>ä¿¡æ¯ç†è®ºæŒ‡æ ‡å’Œäººå·¥å®¡æ ¸è¢«ç”¨äºç¡®ä¿è¯„ä¼°çš„æ­£ç¡®æ€§ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†æ·±å…¥ç ”ç©¶å¹¶åˆ¶å®šé’ˆå¯¹è¯¥é£é™©çš„è¡¥æ•‘æªæ–½çš„æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca88a90606757684fa7ed5942b8ad854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ced293bb3f1389fb5d4cd340734dc92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62b4eaafb7dd64bdc27bb1bfc4bbe3c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c2e81da3beb18b30cd3d740e3a9e3d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-050e4a5017475a9043a80f105b2d066b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0df203f8eb6629451477a7fc4ef9ab0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12fe5a6dbc44c6d6970043a2605e9094.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e1635733451991cd096e486b03d06fd.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e75ef3e55bc4d58fc7f7cbbf1bba323c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-26  Decentralized Intelligence in GameFi Embodied AI Agents and the   Convergence of DeFi and Virtual Ecosystems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-702c880f40660d5be0da75439badbd14.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  FADA Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG   Distillation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
