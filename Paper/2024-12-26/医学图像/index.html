<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-26  Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors   Diverse-Resolution Training Outperforms Interpolation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3ee184809f97812831c45a8673743f23.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    52 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-26-æ›´æ–°"><a href="#2024-12-26-æ›´æ–°" class="headerlink" title="2024-12-26 æ›´æ–°"></a>2024-12-26 æ›´æ–°</h1><h2 id="Resolution-Robust-3D-MRI-Reconstruction-with-2D-Diffusion-Priors-Diverse-Resolution-Training-Outperforms-Interpolation"><a href="#Resolution-Robust-3D-MRI-Reconstruction-with-2D-Diffusion-Priors-Diverse-Resolution-Training-Outperforms-Interpolation" class="headerlink" title="Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:   Diverse-Resolution Training Outperforms Interpolation"></a>Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:   Diverse-Resolution Training Outperforms Interpolation</h2><p><strong>Authors:Anselm Krainovic, Stefan Ruschke, Reinhard Heckel</strong></p>
<p>Deep learning-based 3D imaging, in particular magnetic resonance imaging (MRI), is challenging because of limited availability of 3D training data. Therefore, 2D diffusion models trained on 2D slices are starting to be leveraged for 3D MRI reconstruction. However, as we show in this paper, existing methods pertain to a fixed voxel size, and performance degrades when the voxel size is varied, as it is often the case in clinical practice. In this paper, we propose and study several approaches for resolution-robust 3D MRI reconstruction with 2D diffusion priors. As a result of this investigation, we obtain a simple resolution-robust variational 3D reconstruction approach based on diffusion-guided regularization of randomly sampled 2D slices. This method provides competitive reconstruction quality compared to posterior sampling baselines. Towards resolving the sensitivity to resolution-shifts, we investigate state-of-the-art model-based approaches including Gaussian splatting, neural representations, and infinite-dimensional diffusion models, as well as a simple data-centric approach of training the diffusion model on several resolutions. Our experiments demonstrate that the model-based approaches fail to close the performance gap in 3D MRI. In contrast, the data-centric approach of training the diffusion model on various resolutions effectively provides a resolution-robust method without compromising accuracy. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„3Dæˆåƒï¼Œç‰¹åˆ«æ˜¯ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºé«˜è´¨é‡çš„ä¸‰ç»´è®­ç»ƒæ•°æ®æœ‰é™ã€‚å› æ­¤ï¼Œäººä»¬å¼€å§‹åˆ©ç”¨äºŒç»´æ‰©æ•£æ¨¡å‹å¯¹äºŒç»´åˆ‡ç‰‡è¿›è¡Œè®­ç»ƒï¼Œç”¨äºä¸‰ç»´MRIé‡å»ºã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å±•ç¤ºï¼Œç°æœ‰æ–¹æ³•æ¶‰åŠå›ºå®šä½“ç´ å¤§å°ï¼Œå½“ä½“ç´ å¤§å°å‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ€§èƒ½ä¼šä¸‹é™ï¼Œè¿™åœ¨ä¸´åºŠå®è·µä¸­æ˜¯å¸¸è§çš„æƒ…å†µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹å…·æœ‰äºŒç»´æ‰©æ•£å…ˆéªŒçš„åˆ†è¾¨ç‡é²æ£’ä¸‰ç»´MRIé‡å»ºæå‡ºäº†å‡ ç§æ–¹æ³•å¹¶è¿›è¡Œç ”ç©¶ã€‚é€šè¿‡è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ç§åŸºäºéšæœºé‡‡æ ·äºŒç»´åˆ‡ç‰‡çš„æ‰©æ•£å¼•å¯¼æ­£åˆ™åŒ–çš„ç®€å•åˆ†è¾¨ç‡é²æ£’ä¸‰ç»´é‡å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸åé‡‡æ ·åŸºçº¿ç›¸æ¯”ï¼Œå¯æä¾›å…·æœ‰ç«äº‰åŠ›çš„é‡å»ºè´¨é‡ã€‚ä¸ºäº†è§£å†³å¯¹åˆ†è¾¨ç‡å˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†åŸºäºæ¨¡å‹çš„æœ€æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬é«˜æ–¯å±•ç‰‡æ³•ã€ç¥ç»è¡¨ç¤ºå’Œæ— é™ç»´æ‰©æ•£æ¨¡å‹ç­‰ï¼Œä»¥åŠä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç®€å•æ–¹æ³•ï¼Œå³åœ¨å¤šç§åˆ†è¾¨ç‡ä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºæ¨¡å‹çš„æœ€æ–°æ–¹æ³•æœªèƒ½ç¼©å°ä¸‰ç»´MRIçš„æ€§èƒ½å·®è·ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç›´æ¥åœ¨å¤šç§åˆ†è¾¨ç‡ä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•æœ‰æ•ˆåœ°æä¾›äº†ä¸€ç§åˆ†è¾¨ç‡é²æ£’çš„æ–¹æ³•ï¼Œä¸ä¼šæŸå®³å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18584v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨ä¸‰ç»´æˆåƒï¼Œç‰¹åˆ«æ˜¯ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºé«˜è´¨é‡çš„ä¸‰ç»´è®­ç»ƒæ•°æ®æœ‰é™ã€‚ç ”ç©¶è€…å¼€å§‹åˆ©ç”¨äºŒç»´æ‰©æ•£æ¨¡å‹å¯¹ä¸‰ç»´MRIè¿›è¡Œé‡å»ºã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å—é™äºå›ºå®šä½“ç´ å¤§å°ï¼Œåœ¨ä¸´åºŠå®è·µä¸­ä½“ç´ å¤§å°å˜åŒ–æ—¶æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºå¹¶ç ”ç©¶äº†å‡ ç§åˆ†è¾¨ç‡ç¨³å¥çš„ä¸‰ç»´MRIé‡å»ºæ–¹æ³•ï¼ŒåŸºäºäºŒç»´æ‰©æ•£å…ˆéªŒã€‚ç»è¿‡ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å¼•å¯¼æ­£åˆ™åŒ–çš„éšæœºé‡‡æ ·äºŒç»´åˆ‡ç‰‡åˆ†è¾¨ç‡ç¨³å¥çš„ä¸‰ç»´é‡å»ºæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¯¹åŸºäºæ¨¡å‹çš„æœ€æ–°æ–¹æ³•å¦‚é«˜æ–¯æ¶‚æŠ¹ã€ç¥ç»è¡¨ç¤ºå’Œæ— é™ç»´æ‰©æ•£æ¨¡å‹ä»¥åŠä¸€ç§ç®€å•çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•è¿›è¡Œäº†å®éªŒå¯¹æ¯”ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•åœ¨3D MRIä¸Šæ— æ³•ç¼©å°æ€§èƒ½å·®è·ã€‚ç›¸åï¼Œä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨å„ç§åˆ†è¾¨ç‡ä¸Šçš„æ–¹æ³•æœ‰æ•ˆåœ°æä¾›äº†åˆ†è¾¨ç‡ç¨³å¥ä¸”å‡†ç¡®çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨ä¸‰ç»´ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­é¢ä¸´é«˜è´¨é‡è®­ç»ƒæ•°æ®æœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>äºŒç»´æ‰©æ•£æ¨¡å‹è¢«ç”¨äºä¸‰ç»´MRIé‡å»ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºå›ºå®šä½“ç´ å¤§å°ï¼Œä½“ç´ å¤§å°å˜åŒ–æ—¶æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºäº†å‡ ç§åˆ†è¾¨ç‡ç¨³å¥çš„ä¸‰ç»´MRIé‡å»ºæ–¹æ³•ï¼ŒåŸºäºäºŒç»´æ‰©æ•£å…ˆéªŒã€‚</li>
<li>ä¸€ç§åŸºäºæ‰©æ•£å¼•å¯¼æ­£åˆ™åŒ–çš„éšæœºé‡‡æ ·äºŒç»´åˆ‡ç‰‡æ–¹æ³•æä¾›ç«äº‰åŠ›ã€‚</li>
<li>åŸºäºæ¨¡å‹çš„æ–¹æ³•åœ¨è§£å†³åˆ†è¾¨ç‡å˜åŒ–é—®é¢˜ä¸Šæ•ˆæœæœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ab37b91bb3c25653117a3e6afd05dc0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4277b872cd8a48906ad1edb80745a469.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3947428a182ab03c5f554d225b6327e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FedVCK-Non-IID-Robust-and-Communication-Efficient-Federated-Learning-via-Valuable-Condensed-Knowledge-for-Medical-Image-Analysis"><a href="#FedVCK-Non-IID-Robust-and-Communication-Efficient-Federated-Learning-via-Valuable-Condensed-Knowledge-for-Medical-Image-Analysis" class="headerlink" title="FedVCK: Non-IID Robust and Communication-Efficient Federated Learning   via Valuable Condensed Knowledge for Medical Image Analysis"></a>FedVCK: Non-IID Robust and Communication-Efficient Federated Learning   via Valuable Condensed Knowledge for Medical Image Analysis</h2><p><strong>Authors:Guochen Yan, Luyuan Xie, Xinyi Gao, Wentao Zhang, Qingni Shen, Yuejian Fang, Zhonghai Wu</strong></p>
<p>Federated learning has become a promising solution for collaboration among medical institutions. However, data owned by each institution would be highly heterogeneous and the distribution is always non-independent and identical distribution (non-IID), resulting in client drift and unsatisfactory performance. Despite existing federated learning methods attempting to solve the non-IID problems, they still show marginal advantages but rely on frequent communication which would incur high costs and privacy concerns. In this paper, we propose a novel federated learning method: \textbf{Fed}erated learning via \textbf{V}aluable \textbf{C}ondensed \textbf{K}nowledge (FedVCK). We enhance the quality of condensed knowledge and select the most necessary knowledge guided by models, to tackle the non-IID problem within limited communication budgets effectively. Specifically, on the client side, we condense the knowledge of each client into a small dataset and further enhance the condensation procedure with latent distribution constraints, facilitating the effective capture of high-quality knowledge. During each round, we specifically target and condense knowledge that has not been assimilated by the current model, thereby preventing unnecessary repetition of homogeneous knowledge and minimizing the frequency of communications required. On the server side, we propose relational supervised contrastive learning to provide more supervision signals to aid the global model updating. Comprehensive experiments across various medical tasks show that FedVCK can outperform state-of-the-art methods, demonstrating that itâ€™s non-IID robust and communication-efficient. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ å·²æˆä¸ºåŒ»ç–—æœºæ„é—´åä½œçš„å…·æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ¯ä¸ªæœºæ„æ‹¥æœ‰çš„æ•°æ®å°†æ˜¯é«˜åº¦å¼‚æ„çš„ï¼Œä¸”åˆ†å¸ƒé€šå¸¸æ˜¯éç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼ˆnon-IIDï¼‰ï¼Œå¯¼è‡´å®¢æˆ·ç«¯åç§»å’Œæ€§èƒ½ä¸ä½³ã€‚å°½ç®¡ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•è¯•å›¾è§£å†³éIIDé—®é¢˜ï¼Œä½†å®ƒä»¬ä»æ˜¾ç¤ºå‡ºæœ‰é™çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”ä¾èµ–äºé¢‘ç¹çš„é€šä¿¡ï¼Œè¿™ä¼šäº§ç”Ÿé«˜æ˜‚çš„æˆæœ¬å’Œéšç§æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è”é‚¦å­¦ä¹ æ–¹æ³•ï¼šé€šè¿‡æœ‰ä»·å€¼çš„æµ“ç¼©çŸ¥è¯†ï¼ˆFedVCKï¼‰è¿›è¡Œè”é‚¦å­¦ä¹ ã€‚æˆ‘ä»¬æé«˜äº†æµ“ç¼©çŸ¥è¯†çš„è´¨é‡ï¼Œå¹¶åœ¨æ¨¡å‹çš„æŒ‡å¯¼ä¸‹é€‰æ‹©æœ€å¿…è¦çš„çŸ¥è¯†ï¼Œä»¥åœ¨æœ‰é™çš„é€šä¿¡é¢„ç®—å†…æœ‰æ•ˆè§£å†³éIIDé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å®¢æˆ·ç«¯ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå®¢æˆ·ç«¯çš„çŸ¥è¯†æµ“ç¼©æˆä¸€ä¸ªå°æ•°æ®é›†ï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡æ½œåœ¨åˆ†å¸ƒçº¦æŸå¢å¼ºæµ“ç¼©è¿‡ç¨‹ï¼Œä»è€Œæœ‰æ•ˆåœ°æ•è·é«˜è´¨é‡çš„çŸ¥è¯†ã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œæˆ‘ä»¬ç‰¹åˆ«é’ˆå¯¹å½“å‰æ¨¡å‹å°šæœªå¸æ”¶çš„çŸ¥è¯†è¿›è¡Œæµ“ç¼©ï¼Œä»è€Œé˜²æ­¢ä¸å¿…è¦åœ°é‡å¤åŒè´¨çŸ¥è¯†å¹¶æœ€å°åŒ–æ‰€éœ€çš„é€šä¿¡é¢‘ç‡ã€‚åœ¨æœåŠ¡å™¨ç«¯ï¼Œæˆ‘ä»¬æå‡ºäº†å…³ç³»ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œä¸ºå…¨å±€æ¨¡å‹æ›´æ–°æä¾›æ›´å¤šçš„ç›‘ç£ä¿¡å·ã€‚åœ¨å¤šç§åŒ»å­¦ä»»åŠ¡ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒFedVCKå¯ä»¥è¶…è¶Šæœ€æ–°æ–¹æ³•ï¼Œè¡¨æ˜å…¶åœ¨éIIDæƒ…å†µä¸‹çš„ç¨³å¥æ€§å’Œé€šä¿¡æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18557v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è”é‚¦å­¦ä¹ æ–¹æ³•â€”â€”FedVCKï¼Œé€šè¿‡å‡ç»“æœ‰ä»·å€¼çš„çŸ¥è¯†æ¥è§£å†³åŒ»ç–—æœºæ„é—´æ•°æ®é«˜åº¦å¼‚æ„å’Œéç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰çš„é—®é¢˜ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å®¢æˆ·ç«¯é€šè¿‡å‡ç»“çŸ¥è¯†å¹¶æ–½åŠ æ½œåœ¨åˆ†å¸ƒçº¦æŸæ¥æ•è·é«˜è´¨é‡çŸ¥è¯†ï¼Œå‡å°‘é€šä¿¡é¢‘ç‡ï¼›åœ¨æœåŠ¡å™¨ç«¯é‡‡ç”¨å…³ç³»ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œä¸ºå…¨å±€æ¨¡å‹æ›´æ–°æä¾›æ›´å¤šç›‘ç£ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒFedVCKåœ¨åŒ»ç–—ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå…·æœ‰non-IIDé²æ£’æ€§å’Œé€šä¿¡æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ å·²æˆä¸ºåŒ»ç–—åä½œé¢†åŸŸçš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åŒ»ç–—æœºæ„æ•°æ®å­˜åœ¨é«˜åº¦å¼‚æ„æ€§å’Œéç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸ä½³ã€‚</li>
<li>ç°æœ‰è”é‚¦å­¦ä¹ æ–¹æ³•è™½èƒ½è§£å†³non-IIDé—®é¢˜ï¼Œä½†é€šä¿¡æˆæœ¬é«˜æ˜‚å¹¶å­˜åœ¨éšç§æ‹…å¿§ã€‚</li>
<li>FedVCKæ–¹æ³•é€šè¿‡å‡ç»“å’Œé€‰æ‹©å¿…è¦çš„çŸ¥è¯†æ¥è§£å†³non-IIDé—®é¢˜ï¼Œå¹¶åœ¨æœ‰é™é€šä¿¡é¢„ç®—å†…æœ‰æ•ˆåº”å¯¹ã€‚</li>
<li>FedVCKåœ¨å®¢æˆ·ç«¯å®æ–½çŸ¥è¯†å‡ç»“ï¼Œå¹¶æ–½åŠ æ½œåœ¨åˆ†å¸ƒçº¦æŸä»¥æ•è·é«˜è´¨é‡çŸ¥è¯†ã€‚</li>
<li>åœ¨æœåŠ¡å™¨ç«¯ï¼ŒFedVCKé‡‡ç”¨å…³ç³»ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œä¸ºå…¨å±€æ¨¡å‹æ›´æ–°æä¾›æ›´å¤šç›‘ç£ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4416cf665ef92011ca7d34881861b068.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19535be5ab764dcc4bc8139dc0e7e1af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd0c22a702b56d6c776d70233153126b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5df0b189373f5d5b7c40f1935073998a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Advancing-Deformable-Medical-Image-Registration-with-Multi-axis-Cross-covariance-Attention"><a href="#Advancing-Deformable-Medical-Image-Registration-with-Multi-axis-Cross-covariance-Attention" class="headerlink" title="Advancing Deformable Medical Image Registration with Multi-axis   Cross-covariance Attention"></a>Advancing Deformable Medical Image Registration with Multi-axis   Cross-covariance Attention</h2><p><strong>Authors:Mingyuan Meng, Michael Fulham, Lei Bi, Jinman Kim</strong></p>
<p>Deformable image registration is a fundamental requirement for medical image analysis. Recently, transformers have been widely used in deep learning-based registration methods for their ability to capture long-range dependency via self-attention (SA). However, the high computation and memory loads of SA (growing quadratically with the spatial resolution) hinder transformers from processing subtle textural information in high-resolution image features, e.g., at the full and half image resolutions. This limits deformable registration as the high-resolution textural information is crucial for finding precise pixel-wise correspondence between subtle anatomical structures. Cross-covariance Attention (XCA), as a â€œtransposedâ€ version of SA that operates across feature channels, has complexity growing linearly with the spatial resolution, providing the feasibility of capturing long-range dependency among high-resolution image features. However, existing XCA-based transformers merely capture coarse global long-range dependency, which are unsuitable for deformable image registration relying primarily on fine-grained local correspondence. In this study, we propose to improve existing deep learning-based registration methods by embedding a new XCA mechanism. To this end, we design an XCA-based transformer block optimized for deformable medical image registration, named Multi-Axis XCA (MAXCA). Our MAXCA serves as a general network block that can be embedded into various registration network architectures. It can capture both global and local long-range dependency among high-resolution image features by applying regional and dilated XCA in parallel via a multi-axis design. Extensive experiments on two well-benchmarked inter-&#x2F;intra-patient registration tasks with seven public medical datasets demonstrate that our MAXCA block enables state-of-the-art registration performance. </p>
<blockquote>
<p>å¯å˜å›¾åƒé…å‡†ï¼ˆDeformable Image Registrationï¼‰æ˜¯åŒ»å­¦å›¾åƒåˆ†æçš„åŸºæœ¬éœ€æ±‚ã€‚è¿‘æœŸï¼Œç”±äºå…¶é€šè¿‡è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attention, SAï¼‰æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»çš„èƒ½åŠ›ï¼Œtransformeråœ¨å„ç§åŸºäºæ·±åº¦å­¦ä¹ çš„é…å‡†æ–¹æ³•ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œè‡ªæ³¨æ„åŠ›çš„è®¡ç®—ä¸å†…å­˜è´Ÿè½½è¾ƒé«˜ï¼ˆéšç€ç©ºé—´åˆ†è¾¨ç‡çš„äºŒæ¬¡æ–¹å¢é•¿ï¼‰ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å¤„ç†é«˜æ¸…æ™°åº¦å›¾åƒç‰¹å¾ä¸­çš„å¾®å¦™çº¹ç†ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ï¼Œæ¯”å¦‚åœ¨å…¨å›¾å’ŒåŠå›¾åˆ†è¾¨ç‡æ—¶ã€‚è¿™å¯¹äºå¯å˜é…å‡†æ˜¯ä¸€ä¸ªé™åˆ¶ï¼Œå› ä¸ºé«˜æ¸…æ™°åº¦çš„çº¹ç†ä¿¡æ¯å¯¹äºåœ¨å¾®å¦™çš„è§£å‰–ç»“æ„ä¹‹é—´æ‰¾åˆ°ç²¾ç¡®çš„åƒç´ å¯¹åº”è‡³å…³é‡è¦ã€‚ä½œä¸ºåœ¨ç‰¹å¾é€šé“ä¹‹é—´è¿›è¡Œæ“ä½œçš„è·¨åæ–¹å·®æ³¨æ„åŠ›ï¼ˆCross-covariance Attention, XCAï¼‰æ˜¯ä¸€ç§è‡ªæ³¨æ„åŠ›çš„â€œè½¬ç½®â€ç‰ˆæœ¬ï¼Œå…¶å¤æ‚æ€§éšç©ºé—´åˆ†è¾¨ç‡çº¿æ€§å¢é•¿ï¼Œä¸ºæ•æ‰é«˜æ¸…æ™°åº¦å›¾åƒç‰¹å¾ä¹‹é—´çš„é•¿è·ç¦»ä¾èµ–å…³ç³»æä¾›äº†å¯è¡Œæ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºXCAçš„transformerä»…æ•æ‰ç²—ç³™çš„å…¨å±€é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œå¯¹äºä¸»è¦ä¾èµ–äºç²¾ç»†å±€éƒ¨å¯¹åº”å…³ç³»çš„å¯å˜å›¾åƒé…å‡†æ¥è¯´å¹¶ä¸é€‚ç”¨ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡åµŒå…¥æ–°çš„XCAæœºåˆ¶æ”¹è¿›ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„é…å‡†æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºXCAçš„ä¼˜åŒ–å—ç”¨äºå¯å˜åŒ»å­¦å›¾åƒé…å‡†ï¼Œç§°ä¸ºå¤šè½´XCAï¼ˆMAXCAï¼‰ã€‚æˆ‘ä»¬çš„MAXCAä½œä¸ºä¸€ä¸ªé€šç”¨ç½‘ç»œå—ï¼Œå¯ä»¥åµŒå…¥åˆ°å„ç§é…å‡†ç½‘ç»œæ¶æ„ä¸­ã€‚å®ƒé€šè¿‡å¹¶è¡Œåº”ç”¨åŒºåŸŸå’Œè†¨èƒ€XCAçš„å¤šè½´è®¾è®¡æ¥æ•æ‰é«˜åˆ†è¾¨ç‡å›¾åƒç‰¹å¾ä¹‹é—´çš„å…¨å±€å’Œå±€éƒ¨é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚åœ¨ä¸¤ä¸ªå¹¿æ³›è®¤å¯çš„æ‚£è€…é—´å’Œæ‚£è€…å†…é…å‡†ä»»åŠ¡ä¸Šçš„ä¸ƒä¸ªå…¬å¼€åŒ»å­¦æ•°æ®é›†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MAXCAå—å¯å®ç°æœ€å…ˆè¿›çš„é…å‡†æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18545v1">PDF</a> Under Review</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¯å˜å½¢å›¾åƒé…å‡†é—®é¢˜ï¼Œæå‡ºäº†åŸºäºäº¤å‰åæ–¹å·®æ³¨æ„åŠ›ï¼ˆXCAï¼‰æœºåˆ¶çš„æ”¹è¿›æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚é€šè¿‡è®¾è®¡ä¸€ç§åä¸ºMulti-Axis XCAï¼ˆMAXCAï¼‰çš„XCAåŸºç¡€transformerå—ï¼Œèƒ½æ•è·é«˜åˆ†è¾¨å›¾åƒç‰¹å¾ä¸­çš„å…¨å±€å’Œå±€éƒ¨è¿œç¨‹ä¾èµ–å…³ç³»ï¼Œå®ç°äº†å¯¹ç°æœ‰æ·±åº¦å­¦ä¹ é…å‡†æ–¹æ³•çš„æ”¹è¿›ã€‚è¯¥MAXCAå—å¯ä½œä¸ºé€šç”¨ç½‘ç»œå—åµŒå…¥å„ç§é…å‡†ç½‘ç»œæ¶æ„ä¸­ã€‚åœ¨è·¨æ‚£è€…å’ŒåŒä¸€æ‚£è€…çš„é…å‡†ä»»åŠ¡ä»¥åŠä¸ƒä¸ªå…¬å¼€åŒ»å­¦æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMAXCAå—å®ç°äº†æœ€å…ˆè¿›çš„é…å‡†æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯å˜å½¢å›¾åƒé…å‡†åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å˜å‹å™¨å› è‡ªæˆ‘æ³¨æ„åŠ›ï¼ˆSAï¼‰æœºåˆ¶è€Œå¹¿æ³›åº”ç”¨äºæ·±åº¦å­¦ä¹ ä¸­ï¼Œä½†å…¶åœ¨å¤„ç†é«˜åˆ†è¾¨å›¾åƒç‰¹å¾æ—¶çš„è®¡ç®—å’Œå†…å­˜è´Ÿè½½è¾ƒé«˜ã€‚</li>
<li>äº¤å‰åæ–¹å·®æ³¨æ„åŠ›ï¼ˆXCAï¼‰ä½œä¸ºä¸€ç§â€œè½¬ç½®â€çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½åœ¨é«˜åˆ†è¾¨å›¾åƒç‰¹å¾ä¸Šæ•è·è¿œç¨‹ä¾èµ–æ€§ï¼Œå¤æ‚åº¦éšç©ºé—´åˆ†è¾¨ç‡å‘ˆçº¿æ€§å¢é•¿ã€‚</li>
<li>ç°æœ‰åŸºäºXCAçš„å˜å‹å™¨ä»…æ•è·ç²—ç³™çš„å…¨å±€é•¿ç¨‹ä¾èµ–æ€§ï¼Œä¸é€‚ç”¨äºä¾èµ–ç²¾ç»†å±€éƒ¨å¯¹åº”çš„å¯å˜å½¢å›¾åƒé…å‡†ã€‚</li>
<li>ç ”ç©¶ä¸­æå‡ºäº†Multi-Axis XCAï¼ˆMAXCAï¼‰å—ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¯å˜å½¢åŒ»å­¦å›¾åƒé…å‡†çš„XCAåŸºç¡€å˜å‹å™¨å—ã€‚</li>
<li>MAXCAå—é€šè¿‡å¹¶è¡Œåº”ç”¨åŒºåŸŸå’Œè†¨èƒ€XCAçš„å¤šè½´è®¾è®¡ï¼Œèƒ½å¤Ÿæ•è·é«˜åˆ†è¾¨å›¾åƒç‰¹å¾ä¸­çš„å…¨å±€å’Œå±€éƒ¨è¿œç¨‹ä¾èµ–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b411daf4bbab786cf04acb38f2fdac58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0b3be03bd730b4c5ae9251df4b5f79d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a18c6e27ae7e189573bcb5c6fe228fd5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis"><a href="#VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis" class="headerlink" title="VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis"></a>VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis</h2><p><strong>Authors:Shicheng Yin, Kaixuan Yin, Weixing Chen, Enbo Huang, Yang Liu</strong></p>
<p>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are two dominant models for image analysis. While CNNs excel at extracting multi-scale features and ViTs effectively capture global dependencies, both suffer from high computational costs, particularly when processing high-resolution images. Recently, state-space models (SSMs) and recurrent neural networks (RNNs) have attracted attention due to their efficiency. However, their performance in image classification tasks remains limited. To address these challenges, this paper introduces VisionGRU, a novel RNN-based architecture designed for efficient image classification. VisionGRU leverages a simplified Gated Recurrent Unit (minGRU) to process large-scale image features with linear complexity. It divides images into smaller patches and progressively reduces the sequence length while increasing the channel depth, thus facilitating multi-scale feature extraction. A hierarchical 2DGRU module with bidirectional scanning captures both local and global contexts, improving long-range dependency modeling, particularly for tasks like semantic segmentation. Experimental results on the ImageNet and ADE20K datasets demonstrate that VisionGRU outperforms ViTs, significantly reducing memory usage and computational costs, especially for high-resolution images. These findings underscore the potential of RNN-based approaches for developing efficient and scalable computer vision solutions. Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a>. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ˜¯å›¾åƒåˆ†æçš„ä¸¤ç§ä¸»å¯¼æ¨¡å‹ã€‚è™½ç„¶CNNæ“…é•¿æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œè€ŒViTèƒ½æœ‰æ•ˆåœ°æ•æ‰å…¨å±€ä¾èµ–æ€§ï¼Œä½†å®ƒä»¬éƒ½å­˜åœ¨è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ã€‚è¿‘å¹´æ¥ï¼Œç”±äºæ•ˆç‡è¾ƒé«˜ï¼ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºRNNçš„å›¾åƒåˆ†ç±»æ¶æ„â€”â€”VisionGRUã€‚VisionGRUåˆ©ç”¨ç®€åŒ–çš„é—¨æ§å¾ªç¯å•å…ƒï¼ˆminGRUï¼‰ä»¥çº¿æ€§å¤æ‚åº¦å¤„ç†å¤§è§„æ¨¡å›¾åƒç‰¹å¾ã€‚å®ƒå°†å›¾åƒåˆ†æˆè¾ƒå°çš„æ–‘å—ï¼Œé€æ­¥å‡å°‘åºåˆ—é•¿åº¦ï¼ŒåŒæ—¶å¢åŠ é€šé“æ·±åº¦ï¼Œä»è€Œä¾¿äºå¤šå°ºåº¦ç‰¹å¾æå–ã€‚å…·æœ‰åŒå‘æ‰«æçš„åˆ†å±‚2DGRUæ¨¡å—èƒ½å¤Ÿæ•è·å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ”¹è¿›äº†é•¿è·ç¦»ä¾èµ–å»ºæ¨¡ï¼Œå°¤å…¶åœ¨è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚åœ¨ImageNetå’ŒADE20Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒVisionGRUçš„æ€§èƒ½ä¼˜äºViTï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ï¼Œå°¤å…¶é€‚ç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒã€‚è¿™äº›å‘ç°çªæ˜¾äº†åŸºäºRNNçš„æ–¹æ³•åœ¨å¼€å‘é«˜æ•ˆä¸”å¯æ‰©å±•çš„è®¡ç®—æœºè§†è§‰è§£å†³æ–¹æ¡ˆæ–¹é¢çš„æ½œåŠ›ã€‚ç›¸å…³ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/YangLiu9208/VisionGRUä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18178v1">PDF</a> Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºä¸€ç§åŸºäºRNNçš„æ–°å‹å›¾åƒåˆ†ç±»æ¶æ„â€”â€”VisionGRUï¼Œå®ƒåˆ©ç”¨ç®€åŒ–çš„é—¨æ§å¾ªç¯å•å…ƒï¼ˆminGRUï¼‰å¤„ç†å¤§è§„æ¨¡å›¾åƒç‰¹å¾ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚VisionGRUå°†å›¾åƒåˆ†æˆè¾ƒå°çš„æ–‘å—ï¼Œåœ¨å¢åŠ é€šé“æ·±åº¦çš„åŒæ—¶é€æ­¥å‡å°‘åºåˆ—é•¿åº¦ï¼Œä¿ƒè¿›å¤šå°ºåº¦ç‰¹å¾æå–ã€‚å…¶å±‚æ¬¡åŒ–çš„2DGRUæ¨¡å—å…·æœ‰åŒå‘æ‰«æåŠŸèƒ½ï¼Œèƒ½æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸­ï¼Œé•¿æœŸä¾èµ–å»ºæ¨¡è¡¨ç°ä¼˜å¼‚ã€‚åœ¨ImageNetå’ŒADE20Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVisionGRUåœ¨é™ä½å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¼˜äºViTsï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶è¡¨ç°æ›´å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisionGRUæ˜¯ä¸€ç§åŸºäºRNNçš„æ–°å‹å›¾åƒåˆ†ç±»æ¶æ„ã€‚</li>
<li>VisionGRUåˆ©ç”¨ç®€åŒ–çš„Gated Recurrent Unitï¼ˆminGRUï¼‰å¤„ç†å›¾åƒç‰¹å¾ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚</li>
<li>VisionGRUå°†å›¾åƒåˆ†æˆè¾ƒå°çš„æ–‘å—ï¼Œé€æ­¥å¤„ç†ï¼Œä¿ƒè¿›å¤šå°ºåº¦ç‰¹å¾æå–ã€‚</li>
<li>å±‚æ¬¡åŒ–çš„2DGRUæ¨¡å—å…·æœ‰åŒå‘æ‰«æï¼Œèƒ½æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚</li>
<li>VisionGRUåœ¨è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸­ï¼Œé•¿æœŸä¾èµ–å»ºæ¨¡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨ImageNetå’ŒADE20Kæ•°æ®é›†ä¸Šï¼ŒVisionGRUè¡¨ç°ä¼˜äºViTsï¼Œå°¤å…¶æ˜¯å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ee184809f97812831c45a8673743f23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49746cf59be5692774d012e2aab7bab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73664fb72c773b1bf490d6a20e930639.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f8b7955c0062d89f3422d901b00d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32fecfa3a65b425b6a4fd26d38b4a8f5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Optimization-of-Convolutional-Neural-Network-Hyperparameter-for-Medical-Image-Diagnosis-using-Metaheuristic-Algorithms-A-short-Recent-Review-2019-2022"><a href="#Optimization-of-Convolutional-Neural-Network-Hyperparameter-for-Medical-Image-Diagnosis-using-Metaheuristic-Algorithms-A-short-Recent-Review-2019-2022" class="headerlink" title="Optimization of Convolutional Neural Network Hyperparameter for Medical   Image Diagnosis using Metaheuristic Algorithms: A short Recent Review   (2019-2022)"></a>Optimization of Convolutional Neural Network Hyperparameter for Medical   Image Diagnosis using Metaheuristic Algorithms: A short Recent Review   (2019-2022)</h2><p><strong>Authors:Qusay Shihab Hamad, Hussein Samma, Shahrel Azmin Suandi</strong></p>
<p>Convolutional Neural Networks (CNNs) have been successfully utilized in the medical diagnosis of many illnesses. Nevertheless, identifying the optimal architecture and hyperparameters among the available possibilities might be a substantial challenge. Typically, CNN hyperparameter selection is performed manually. Nonetheless, this is a computationally costly procedure, as numerous rounds of hyperparameter settings must be evaluated to determine which produces the best results. Choosing the proper hyperparameter settings has always been a crucial and challenging task, as it depends on the researcherâ€™s knowledge and experience. This study will present work done in recent years on the usage of metaheuristic optimization algorithms in the CNN optimization process. It looks at a number of recent studies that focus on the use of optimization methods to optimize hyperparameters in order to find high-performing CNNs. This helps researchers figure out how to set hyperparameters efficiently. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å·²åœ¨å¤šç§ç–¾ç—…çš„åŒ»å­¦è¯Šæ–­ä¸­æˆåŠŸåº”ç”¨ã€‚ç„¶è€Œï¼Œåœ¨ä¼—å¤šçš„å¯èƒ½æ¶æ„å’Œè¶…å‚æ•°ä¸­è¯†åˆ«å‡ºæœ€ä¼˜çš„æ¶æ„å’Œè¶…å‚æ•°å¯èƒ½æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚é€šå¸¸ï¼ŒCNNè¶…å‚æ•°çš„é€‰æ‹©æ˜¯æ‰‹åŠ¨å®Œæˆçš„ã€‚ç„¶è€Œï¼Œè¿™æ˜¯ä¸€ä¸ªè®¡ç®—æˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ï¼Œå› ä¸ºå¿…é¡»è¯„ä¼°å¤šè½®è¶…å‚æ•°è®¾ç½®ï¼Œä»¥ç¡®å®šå“ªç§è®¾ç½®èƒ½äº§ç”Ÿæœ€ä½³ç»“æœã€‚é€‰æ‹©åˆé€‚çš„è¶…å‚æ•°è®¾ç½®ä¸€ç›´æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä¸”å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒå–å†³äºç ”ç©¶äººå‘˜çš„çŸ¥è¯†å’Œç»éªŒã€‚æœ¬ç ”ç©¶å°†ä»‹ç»è¿‘å¹´æ¥åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¼˜åŒ–è¿‡ç¨‹ä¸­ä½¿ç”¨å…ƒå¯å‘å¼ä¼˜åŒ–ç®—æ³•çš„å·¥ä½œã€‚å®ƒå…³æ³¨äº†ä¸€äº›è¿‘æœŸçš„ç ”ç©¶ï¼Œè¿™äº›ç ”ç©¶ä¸“æ³¨äºä½¿ç”¨ä¼˜åŒ–æ–¹æ³•æ¥ä¼˜åŒ–è¶…å‚æ•°ï¼Œä»¥æ‰¾åˆ°é«˜æ€§èƒ½çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚è¿™æœ‰åŠ©äºç ”ç©¶äººå‘˜æœ‰æ•ˆåœ°è®¾ç½®è¶…å‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17956v1">PDF</a> its a conference paper; the full proceeding is available online at   <a target="_blank" rel="noopener" href="https://icogoia.utem.edu.my/proceedings.html">https://icogoia.utem.edu.my/proceedings.html</a></p>
<p><strong>Summary</strong></p>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨åŒ»å­¦è¯Šæ–­ä¸­å·²å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œåœ¨ä¼—å¤šçš„ç½‘ç»œæ¶æ„ä¸è¶…å‚æ•°ä¸­ï¼Œå¦‚ä½•é€‰æ‹©æœ€ä¼˜çš„æ¶æ„ä¸è¶…å‚æ•°æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æ‰‹åŠ¨é€‰æ‹©CNNè¶…å‚æ•°æ˜¯ä¸€ä¸ªè®¡ç®—æˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ï¼Œéœ€è¦è¯„ä¼°å¤šç§è¶…å‚æ•°è®¾ç½®ä»¥æ‰¾å‡ºæœ€ä½³ç»“æœã€‚æœ¬ç ”ç©¶å°†ä»‹ç»è¿‘å¹´æ¥ä½¿ç”¨å…ƒå¯å‘å¼ä¼˜åŒ–ç®—æ³•åœ¨CNNä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶å…³æ³¨ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•æ¥ä¼˜åŒ–è¶…å‚æ•°ï¼Œä»¥æ‰¾åˆ°æ€§èƒ½é«˜çš„CNNï¼Œæœ‰åŠ©äºç ”ç©¶äººå‘˜é«˜æ•ˆè®¾ç½®è¶…å‚æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CNNå·²æˆåŠŸåº”ç”¨äºåŒ»å­¦è¯Šæ–­ã€‚</li>
<li>é€‰æ‹©CNNçš„æœ€ä¼˜æ¶æ„å’Œè¶…å‚æ•°æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æ‰‹åŠ¨é€‰æ‹©CNNè¶…å‚æ•°æ˜¯ä¸€ä¸ªè®¡ç®—æˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ã€‚</li>
<li>å…ƒå¯å‘å¼ä¼˜åŒ–ç®—æ³•è¢«ç”¨äºCNNçš„ä¼˜åŒ–è¿‡ç¨‹ä¸­ã€‚</li>
<li>ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•æ¥ä¼˜åŒ–è¶…å‚æ•°ï¼Œä»¥æ‰¾åˆ°é«˜æ€§èƒ½çš„CNNã€‚</li>
<li>è¿™äº›æ–¹æ³•æœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´æœ‰æ•ˆåœ°è®¾ç½®CNNçš„è¶…å‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b77ebfb0123fea1622a8c9c02cab7194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6171fcc0a7f6de117a916b81474eca3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eb18e3d1bb0caa4892f5a16140e4bbc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cadb74f4223e3192088b541585c5d69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a957bf543f4bee29aa7f8039e531f28b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b499514baa263269a17130565d9e1fa6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hyperbolic-Chamfer-Distance-for-Point-Cloud-Completion-and-Beyond"><a href="#Hyperbolic-Chamfer-Distance-for-Point-Cloud-Completion-and-Beyond" class="headerlink" title="Hyperbolic Chamfer Distance for Point Cloud Completion and Beyond"></a>Hyperbolic Chamfer Distance for Point Cloud Completion and Beyond</h2><p><strong>Authors:Fangzhou Lin, Songlin Hou, Haotian Liu, Shang Gao, Kazunori D Yamada, Haichong K. Zhang, Ziming Zhang</strong></p>
<p>Chamfer Distance (CD) is widely used as a metric to quantify difference between two point clouds. In point cloud completion, Chamfer Distance (CD) is typically used as a loss function in deep learning frameworks. However, it is generally acknowledged within the field that Chamfer Distance (CD) is vulnerable to the presence of outliers, which can consequently lead to the convergence on suboptimal models. In divergence from the existing literature, which largely concentrates on resolving such concerns in the realm of Euclidean space, we put forth a notably uncomplicated yet potent metric specifically designed for point cloud completion tasks: {Hyperbolic Chamfer Distance (HyperCD)}. This metric conducts Chamfer Distance computations within the parameters of hyperbolic space. During the backpropagation process, HyperCD systematically allocates greater weight to matched point pairs exhibiting reduced Euclidean distances. This mechanism facilitates the preservation of accurate point pair matches while permitting the incremental adjustment of suboptimal matches, thereby contributing to enhanced point cloud completion outcomes. Moreover, measure the shape dissimilarity is not solely work for point cloud completion task, we further explore its applications in other generative related tasks, including single image reconstruction from point cloud, and upsampling. We demonstrate state-of-the-art performance on the point cloud completion benchmark datasets, PCN, ShapeNet-55, and ShapeNet-34, and show from visualization that HyperCD can significantly improve the surface smoothness, we also provide the provide experimental results beyond completion task. </p>
<blockquote>
<p>Chamfer Distanceï¼ˆCDï¼‰è¢«å¹¿æ³›ç”¨ä½œè¡¡é‡ä¸¤ä¸ªç‚¹äº‘ä¹‹é—´å·®å¼‚çš„æŒ‡æ ‡ã€‚åœ¨ç‚¹äº‘è¡¥å…¨ä¸­ï¼ŒChamfer Distanceï¼ˆCDï¼‰é€šå¸¸ä½œä¸ºæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„æŸå¤±å‡½æ•°ã€‚ç„¶è€Œï¼Œä¸šå†…æ™®éè®¤ä¸ºChamfer Distanceï¼ˆCDï¼‰å®¹æ˜“å—åˆ°å¼‚å¸¸å€¼çš„å½±å“ï¼Œè¿™å¯èƒ½å¯¼è‡´æ”¶æ•›åˆ°éæœ€ä¼˜æ¨¡å‹ã€‚ä¸ç°æœ‰æ–‡çŒ®å¤§å¤šé›†ä¸­åœ¨è§£å†³æ¬§å‡ é‡Œå¾—ç©ºé—´é¢†åŸŸçš„æ­¤ç±»é—®é¢˜ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨ä¸ºç‚¹äº‘è¡¥å…¨ä»»åŠ¡è®¾è®¡çš„ç®€æ´è€Œå¼ºå¤§çš„æŒ‡æ ‡ï¼šHyperbolic Chamfer Distanceï¼ˆHyperCDï¼‰ã€‚è¯¥æŒ‡æ ‡åœ¨åŒæ›²ç©ºé—´å‚æ•°å†…è¿›è¡ŒChamfer Distanceè®¡ç®—ã€‚åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼ŒHyperCDç³»ç»Ÿåœ°ç»™è¡¨ç°å‡ºè¾ƒå°æ¬§å‡ é‡Œå¾—è·ç¦»çš„åŒ¹é…ç‚¹åˆ†é…æ›´å¤§çš„æƒé‡ã€‚è¿™ç§æœºåˆ¶æœ‰åŠ©äºä¿ç•™å‡†ç¡®çš„ç‚¹å¯¹åŒ¹é…ï¼ŒåŒæ—¶å…è®¸å¯¹æ¬¡ä¼˜åŒ¹é…è¿›è¡Œå¢é‡è°ƒæ•´ï¼Œä»è€Œæé«˜äº†ç‚¹äº‘è¡¥å…¨çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæµ‹é‡å½¢çŠ¶å·®å¼‚ä¸ä»…ä»…é€‚ç”¨äºç‚¹äº‘è¡¥å…¨ä»»åŠ¡ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ¢ç´¢äº†å…¶åœ¨å…¶ä»–ç”Ÿæˆç›¸å…³ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ä»ç‚¹äº‘é‡å»ºå•å¼ å›¾åƒå’Œä¸Šé‡‡æ ·ã€‚æˆ‘ä»¬åœ¨ç‚¹äº‘è¡¥å…¨åŸºå‡†æ•°æ®é›†PCNã€ShapeNet-55å’ŒShapeNet-34ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡å¯è§†åŒ–å±•ç¤ºHyperCDå¯ä»¥æ˜¾è‘—æé«˜è¡¨é¢å¹³æ»‘åº¦ã€‚æˆ‘ä»¬è¿˜æä¾›äº†è¶…å‡ºè¡¥å…¨ä»»åŠ¡å®éªŒçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17951v1">PDF</a> 13 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºä¸€ç§é’ˆå¯¹ç‚¹äº‘å®Œæˆä»»åŠ¡çš„æ–°å‹åº¦é‡æ ‡å‡†â€”â€”Hyperbolic Chamfer Distance (HyperCD)ã€‚è¯¥åº¦é‡åœ¨åŒæ›²ç©ºé—´ä¸­è¿›è¡ŒChamfer Distanceè®¡ç®—ï¼Œä¸ºåŒ¹é…ç‚¹åˆ†é…æ›´å¤§çš„æƒé‡ï¼Œæœ‰åŠ©äºä¿ç•™å‡†ç¡®çš„ç‚¹åŒ¹é…å¹¶è°ƒæ•´æ¬¡ä¼˜åŒ¹é…ï¼Œä»è€Œæé«˜ç‚¹äº‘å®Œæˆç»“æœã€‚é™¤äº†ç‚¹äº‘å®Œæˆä»»åŠ¡å¤–ï¼Œè¿˜æ¢ç´¢äº†å…¶åœ¨å…¶ä»–ç”Ÿæˆç›¸å…³ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ç‚¹äº‘åˆ°å•å›¾åƒçš„é‡å»ºå’Œé‡‡æ ·ç­‰ã€‚åœ¨PCNã€ShapeNet-55å’ŒShapeNet-34ç­‰ç‚¹äº‘å®ŒæˆåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç‚¹äº‘å®Œæˆåº¦é‡æ ‡å‡†â€”â€”Hyperbolic Chamfer Distance (HyperCD)ã€‚</li>
<li>HyperCDåœ¨åŒæ›²ç©ºé—´ä¸­è¿›è¡ŒChamfer Distanceè®¡ç®—ï¼Œæé«˜äº†ç‚¹åŒ¹é…çš„å‡†ç¡®æ€§å¹¶å…è®¸è°ƒæ•´æ¬¡ä¼˜åŒ¹é…ã€‚</li>
<li>HyperCDèƒ½æ˜¾è‘—æ”¹å–„ç‚¹äº‘å®Œæˆçš„è¡¨é¢å¹³æ»‘åº¦ã€‚</li>
<li>é™¤äº†ç‚¹äº‘å®Œæˆä»»åŠ¡å¤–ï¼ŒHyperCDè¿˜åº”ç”¨äºå…¶ä»–ç”Ÿæˆç›¸å…³ä»»åŠ¡ï¼Œå¦‚ç‚¹äº‘åˆ°å•å›¾åƒçš„é‡å»ºå’Œé‡‡æ ·ç­‰ã€‚</li>
<li>åœ¨å¤šä¸ªç‚¹äº‘å®ŒæˆåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒHyperCDå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>HyperCDä¸ä»…æœ‰åŠ©äºè§£å†³ç”±äºå­˜åœ¨ç¦»ç¾¤å€¼è€Œå¯¼è‡´çš„Chamfer Distanceçš„å±€é™æ€§ï¼Œè¿˜ä¸ºæé«˜ç‚¹äº‘å®Œæˆè´¨é‡æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf9d89b978a99da865d9a650f21e2780.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90f1e3834d8c3de7f1276a7f6b0354fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4ae3ead5f39ac11998b18a9e59ec4a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd5d44432f8ce2fabe2eeabafc8712ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63ae53104bc01ec83f0a56dcd88c002.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Parkinson-Disease-Detection-Based-on-In-air-Dynamics-Feature-Extraction-and-Selection-Using-Machine-Learning"><a href="#Parkinson-Disease-Detection-Based-on-In-air-Dynamics-Feature-Extraction-and-Selection-Using-Machine-Learning" class="headerlink" title="Parkinson Disease Detection Based on In-air Dynamics Feature Extraction   and Selection Using Machine Learning"></a>Parkinson Disease Detection Based on In-air Dynamics Feature Extraction   and Selection Using Machine Learning</h2><p><strong>Authors:Jungpil Shin, Abu Saleh Musa Miah, Koki Hirooka, Md. Al Mehedi Hasan, Md. Maniruzzaman</strong></p>
<p>Parkinsonâ€™s disease (PD) is a progressive neurological disorder that impairs movement control, leading to symptoms such as tremors, stiffness, and bradykinesia. Many researchers analyzing handwriting data for PD detection typically rely on computing statistical features over the entirety of the handwriting task. While this method can capture broad patterns, it has several limitations, including a lack of focus on dynamic change, oversimplified feature representation, lack of directional information, and missing micro-movements or subtle variations. Consequently, these systems face challenges in achieving good performance accuracy, robustness, and sensitivity. To overcome this problem, we proposed an optimized PD detection methodology that incorporates newly developed dynamic kinematic features and machine learning (ML)-based techniques to capture movement dynamics during handwriting tasks. In the procedure, we first extracted 65 newly developed kinematic features from the first and last 10% phases of the handwriting task rather than using the entire task. Alongside this, we also reused 23 existing kinematic features, resulting in a comprehensive new feature set. Next, we enhanced the kinematic features by applying statistical formulas to compute hierarchical features from the handwriting data. This approach allows us to capture subtle movement variations that distinguish PD patients from healthy controls. To further optimize the feature set, we applied the Sequential Forward Floating Selection method to select the most relevant features, reducing dimensionality and computational complexity. Finally, we employed an ML-based approach based on ensemble voting across top-performing tasks, achieving an impressive 96.99% accuracy on task-wise classification and 99.98% accuracy on task ensembles, surpassing the existing state-of-the-art model by 2% for the PaHaW dataset. </p>
<blockquote>
<p>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ˜¯ä¸€ç§è¿›è¡Œæ€§ç¥ç»ç–¾ç—…ï¼Œä¼šæŸå®³è¿åŠ¨æ§åˆ¶ï¼Œå¯¼è‡´éœ‡é¢¤ã€åƒµç¡¬å’Œè¡ŒåŠ¨è¿Ÿç¼“ç­‰ç—‡çŠ¶ã€‚è®¸å¤šç ”ç©¶äººå‘˜åœ¨åˆ†æå¸•é‡‘æ£®ç—…çš„æ‰‹å†™æ•°æ®æ£€æµ‹æ—¶ï¼Œé€šå¸¸ä¾èµ–äºå¯¹æ•´ä¸ªæ‰‹å†™ä»»åŠ¡è¿›è¡Œç»Ÿè®¡ç‰¹å¾çš„è®¡ç®—ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¯ä»¥æ•æ‰å¤§è‡´çš„æ¨¡å¼ï¼Œä½†å®ƒå­˜åœ¨ä¸€äº›å±€é™æ€§ï¼ŒåŒ…æ‹¬ç¼ºä¹åŠ¨æ€å˜åŒ–çš„å…³æ³¨ã€ç‰¹å¾è¡¨ç¤ºè¿‡äºç®€åŒ–ã€ç¼ºä¹æ–¹å‘ä¿¡æ¯å’Œç¼ºå¤±çš„å¾®åŠ¨ä½œæˆ–ç»†å¾®å˜åŒ–ã€‚å› æ­¤ï¼Œè¿™äº›ç³»ç»Ÿåœ¨å®ç°æ€§èƒ½å‡†ç¡®æ€§ã€ç¨³å¥æ€§å’Œæ•æ„Ÿæ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¼˜åŒ–çš„å¸•é‡‘æ£®ç—…æ£€æµ‹æ–¹æ³•è®ºï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ–°å¼€å‘çš„åŠ¨æ€è¿åŠ¨ç‰¹å¾ä»¥åŠåŸºäºæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰çš„æŠ€æœ¯ï¼Œä»¥æ•æ‰æ‰‹å†™ä»»åŠ¡è¿‡ç¨‹ä¸­çš„è¿åŠ¨åŠ¨æ€ã€‚åœ¨è¯¥è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»æ‰‹å†™ä»»åŠ¡çš„å¼€å§‹å’Œç»“æŸ10%çš„é˜¶æ®µä¸­æå–äº†65ä¸ªæ–°å¼€å‘çš„è¿åŠ¨ç‰¹å¾ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ•´ä¸ªä»»åŠ¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜é‡ç”¨äº†23ä¸ªç°æœ‰çš„è¿åŠ¨ç‰¹å¾ï¼Œå½¢æˆäº†ä¸€ä¸ªå…¨é¢çš„æ–°ç‰¹å¾é›†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡åº”ç”¨ç»Ÿè®¡å…¬å¼æ¥è®¡ç®—æ‰‹å†™æ•°æ®çš„åˆ†å±‚ç‰¹å¾æ¥å¢å¼ºè¿åŠ¨ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•å…è®¸æˆ‘ä»¬æ•æ‰åˆ°ç»†å¾®çš„è¿åŠ¨å˜åŒ–ï¼Œä»¥åŒºåˆ†å¸•é‡‘æ£®ç—…æ‚£è€…å’Œå¥åº·å¯¹ç…§è€…ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–ç‰¹å¾é›†ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åºè´¯å‰å‘æµ®åŠ¨é€‰æ‹©æ–¹æ³•ï¼Œä»¥é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œé™ä½ç»´åº¦å’Œè®¡ç®—å¤æ‚æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬åŸºäºä»»åŠ¡æ€§èƒ½æœ€ä½³çš„é›†åˆæŠ•ç¥¨æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„ç­–ç•¥ï¼Œåœ¨ä»»åŠ¡çº§åˆ†ç±»ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„96.99%çš„å‡†ç¡®ç‡ï¼Œåœ¨ä»»åŠ¡é›†åˆä¸Šè¾¾åˆ°äº†æƒŠäººçš„99.98%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†PaHaWæ•°æ®é›†ä¸Šçš„ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17849v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ˜¯ä¸€ç§å½±å“è¿åŠ¨æ§åˆ¶çš„ç¥ç»æ€§ç–¾ç—…ã€‚ä¼ ç»Ÿçš„æ‰‹å†™æ•°æ®åˆ†ææ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å¿½è§†åŠ¨æ€å˜åŒ–ã€ç‰¹å¾è¡¨ç¤ºè¿‡äºç®€åŒ–ç­‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¼˜åŒ–çš„PDæ£€æµ‹æ³•ï¼Œç»“åˆæ–°çš„åŠ¨æ€è¿åŠ¨ç‰¹å¾å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œä»¥æ•æ‰æ‰‹å†™è¿‡ç¨‹ä¸­çš„è¿åŠ¨åŠ¨æ€ã€‚è¯¥ç ”ç©¶ä»ä¹¦å†™ä»»åŠ¡çš„å¼€å§‹å’Œç»“æŸé˜¶æ®µæå–ç‰¹å¾ï¼ŒåŒæ—¶åˆ©ç”¨ç»Ÿè®¡å…¬å¼è®¡ç®—å±‚æ¬¡ç‰¹å¾ï¼Œä»¥åŒºåˆ†PDæ‚£è€…å’Œå¥åº·äººã€‚é€šè¿‡ç‰¹å¾é€‰æ‹©æ–¹æ³•ä¼˜åŒ–ç‰¹å¾é›†ï¼Œå¹¶é‡‡ç”¨åŸºäºé›†æˆæŠ•ç¥¨çš„æœºå™¨å­¦ä¹ æ³•ï¼Œå–å¾—äº†é«˜å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¸•é‡‘æ£®ç—…æ˜¯ä¸€ç§å½±å“è¿åŠ¨æ§åˆ¶çš„ç¥ç»æ€§ç–¾ç—…ï¼Œå…·æœ‰æ‰‹æŠ–ã€åƒµç¡¬å’Œè¿Ÿç¼“ç­‰ç—‡çŠ¶ã€‚</li>
<li>ä¼ ç»Ÿçš„æ‰‹å†™æ•°æ®åˆ†ææ–¹æ³•åœ¨PDæ£€æµ‹ä¸­å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹åŠ¨æ€å˜åŒ–å…³æ³¨åŠç»†å¾®è¿åŠ¨æ•æ‰ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ–°çš„åŠ¨æ€è¿åŠ¨ç‰¹å¾å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„ä¼˜åŒ–PDæ£€æµ‹æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶ä»æ‰‹å†™ä»»åŠ¡çš„ç‰¹å®šé˜¶æ®µæå–ç‰¹å¾ï¼Œå¹¶è®¡ç®—å±‚æ¬¡ç‰¹å¾ä»¥åŒºåˆ†PDæ‚£è€…å’Œå¥åº·äººã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ab0d6a1dd151c61391d610089d52c1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f794cf7a0b80caeb1ce3daad43b71b4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Potential-of-Convolutional-Neural-Networks-for-Cancer-Detection"><a href="#The-Potential-of-Convolutional-Neural-Networks-for-Cancer-Detection" class="headerlink" title="The Potential of Convolutional Neural Networks for Cancer Detection"></a>The Potential of Convolutional Neural Networks for Cancer Detection</h2><p><strong>Authors:Hossein Molaeian, Kaveh Karamjani, Sina Teimouri, Saeed Roshani, Sobhan Roshani</strong></p>
<p>Early detection of cancer is critical in improving treatment outcomes and increasing survival rates, particularly for common cancers such as lung, breast, and prostate which collectively contribute to a significant global mortality burden. With advancements in imaging technologies and data processing, Convolutional Neural Networks (CNNs) have emerged as a powerful tool for analyzing and classifying medical images, enabling more precise cancer detection. This paper provides a comprehensive review of recent studies leveraging CNN models for detecting ten different types of cancer. Each study employs distinct CNN architectures to identify patterns associated with these cancers, utilizing diverse datasets. Key differences and strengths of these architectures are meticulously compared and analyzed, highlighting their efficacy in improving early detection. Beyond reviewing the performance and limitations of CNN-based cancer detection methods, this study explores the feasibility of integrating CNNs into clinical settings as an early detection tool, potentially complementing or replacing traditional methods. Despite significant progress, challenges remain, including data diversity, result interpretation, and ethical considerations. By identifying the best-performing CNN architectures and providing a comparative analysis, this study aims to contribute a comprehensive perspective on the application of CNNs in cancer detection and their role in advancing diagnostic capabilities in healthcare. </p>
<blockquote>
<p>ç™Œç—‡çš„æ—©æœŸå‘ç°å¯¹äºæé«˜æ²»ç–—æ•ˆæœå’Œå¢åŠ å­˜æ´»ç‡è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè‚ºç™Œã€ä¹³è…ºç™Œå’Œå‰åˆ—è…ºç™Œç­‰å¸¸è§ç™Œç—‡ï¼Œå®ƒä»¬å…±åŒé€ æˆäº†å…¨çƒå¤§é‡çš„æ­»äº¡è´Ÿæ‹…ã€‚éšç€æˆåƒæŠ€æœ¯å’Œæ•°æ®å¤„ç†çš„å‘å±•ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å·²æˆä¸ºåˆ†æå’Œåˆ†ç±»åŒ»å­¦å›¾åƒçš„å¼ºå¤§å·¥å…·ï¼Œèƒ½å¤Ÿå®ç°æ›´ç²¾ç¡®çš„ç™Œç—‡æ£€æµ‹ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†æœ€è¿‘åˆ©ç”¨CNNæ¨¡å‹æ£€æµ‹åç§ä¸åŒç±»å‹ç™Œç—‡çš„ç ”ç©¶ã€‚æ¯é¡¹ç ”ç©¶éƒ½é‡‡ç”¨ä¸åŒçš„CNNæ¶æ„æ¥è¯†åˆ«ä¸è¿™äº›ç™Œç—‡ç›¸å…³çš„æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨å„ç§æ•°æ®é›†ã€‚æœ¬æ–‡ç²¾å¿ƒæ¯”è¾ƒå’Œåˆ†æäº†è¿™äº›æ¶æ„çš„ä¸»è¦å·®å¼‚å’Œä¼˜ç‚¹ï¼Œçªå‡ºäº†å®ƒä»¬åœ¨æé«˜æ—©æœŸæ£€æµ‹ç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é™¤äº†å›é¡¾åŸºäºCNNçš„ç™Œç—‡æ£€æµ‹æ–¹æ³•çš„æ€§èƒ½å’Œå±€é™æ€§å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æ¢è®¨äº†å°†CNNé›†æˆåˆ°ä¸´åºŠç¯å¢ƒä¸­ä½œä¸ºæ—©æœŸæ£€æµ‹å·¥å…·çš„å¯è¡Œæ€§ï¼Œå¯èƒ½è¡¥å……æˆ–æ›¿ä»£ä¼ ç»Ÿæ–¹æ³•ã€‚å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ä»å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®å¤šæ ·æ€§ã€ç»“æœè§£é‡Šå’Œä¼¦ç†è€ƒé‡ã€‚é€šè¿‡ç¡®å®šè¡¨ç°æœ€ä½³çš„CNNæ¶æ„å¹¶æä¾›æ¯”è¾ƒåˆ†æï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ä¸ºCNNåœ¨ç™Œç—‡æ£€æµ‹ä¸­çš„åº”ç”¨åŠå…¶åœ¨æé«˜åŒ»ç–—è¯Šæ–­èƒ½åŠ›æ–¹é¢çš„ä½œç”¨æä¾›å…¨é¢è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17155v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†è¿‘æœŸåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹æ£€æµ‹åç§ä¸åŒç±»å‹ç™Œç—‡çš„ç ”ç©¶ã€‚è¿™äº›ç ”ç©¶é‡‡ç”¨ä¸åŒçš„CNNæ¶æ„è¯†åˆ«ä¸ç™Œç—‡ç›¸å…³çš„æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨å¤šæ ·æ•°æ®é›†è¿›è¡ŒéªŒè¯ã€‚æ–‡ç« æ¯”è¾ƒåˆ†æäº†è¿™äº›æ¶æ„çš„å…³é”®å·®å¼‚å’Œä¼˜åŠ¿ï¼Œå¼ºè°ƒäº†å®ƒä»¬åœ¨æé«˜ç™Œç—‡æ—©æœŸæ£€æµ‹æ–¹é¢çš„æ•ˆèƒ½ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†å°†CNNé›†æˆåˆ°ä¸´åºŠç¯å¢ƒä¸­ä½œä¸ºæ—©æœŸæ£€æµ‹å·¥å…·çš„å¯è¡Œæ€§ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ—©æœŸç™Œç—‡æ£€æµ‹å¯¹æ”¹å–„æ²»ç–—ç»“æœå’Œæé«˜å­˜æ´»ç‡è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè‚ºç™Œã€ä¹³è…ºç™Œå’Œå‰åˆ—è…ºç™Œç­‰å¸¸è§ç™Œç—‡ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨åˆ†æå’Œåˆ†ç±»åŒ»ç–—å›¾åƒæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œèƒ½æ›´ç²¾ç¡®åœ°æ£€æµ‹ç™Œç—‡ã€‚</li>
<li>ç»¼è¿°äº†åˆ©ç”¨CNNæ¨¡å‹æ£€æµ‹åç§ä¸åŒç±»å‹ç™Œç—‡çš„æœ€è¿‘ç ”ç©¶ï¼Œè¿™äº›ç ”ç©¶é‡‡ç”¨ä¸åŒçš„CNNæ¶æ„å¹¶åˆ©ç”¨å¤šæ ·æ•°æ®é›†è¿›è¡ŒéªŒè¯ã€‚</li>
<li>CNNæ¶æ„åœ¨ç™Œç—‡æ£€æµ‹ä¸­çš„æ•ˆèƒ½å¾—åˆ°äº†æ¯”è¾ƒå’Œåˆ†æï¼Œå¼ºè°ƒäº†å®ƒä»¬å¯¹æé«˜æ—©æœŸæ£€æµ‹ç‡çš„é‡è¦æ€§ã€‚</li>
<li>æ¢è®¨äº†å°†CNNé›†æˆåˆ°ä¸´åºŠç¯å¢ƒä¸­ä½œä¸ºç™Œç—‡æ—©æœŸæ£€æµ‹å·¥å…·çš„å¯è¡Œæ€§ï¼Œè¿™å¯èƒ½ä¼šè¡¥å……æˆ–æ›¿ä»£ä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®å¤šæ ·æ€§ã€ç»“æœè§£è¯»å’Œä¼¦ç†è€ƒè™‘ç­‰å› ç´ ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd988f7d9174b0e9e27be42da427637f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-907795154a6617154b7645e370678f01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-321627207022e1b4202bf80e12f4de79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d09b635973bacdd60f62efa26f2702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4810ee33394b1a8999251a694faf4d9c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MaskCLIP-A-Mask-Based-CLIP-Fine-tuning-Framework-for-Open-Vocabulary-Image-Segmentation"><a href="#MaskCLIP-A-Mask-Based-CLIP-Fine-tuning-Framework-for-Open-Vocabulary-Image-Segmentation" class="headerlink" title="MaskCLIP++: A Mask-Based CLIP Fine-tuning Framework for Open-Vocabulary   Image Segmentation"></a>MaskCLIP++: A Mask-Based CLIP Fine-tuning Framework for Open-Vocabulary   Image Segmentation</h2><p><strong>Authors:Quan-Sheng Zeng, Yunheng Li, Daquan Zhou, Guanbin Li, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>Open-vocabulary image segmentation has been advanced through the synergy between mask generators and vision-language models like Contrastive Language-Image Pre-training (CLIP). Previous approaches focus on generating masks while aligning mask features with text embeddings during training. In this paper, we observe that relying on generated low-quality masks can weaken the alignment of vision and language in regional representations. This motivates us to present a new fine-tuning framework, named MaskCLIP++, which uses ground-truth masks instead of generated masks to enhance the mask classification capability of CLIP. Due to the limited diversity of image segmentation datasets with mask annotations, we propose incorporating a consistency alignment constraint during fine-tuning, which alleviates categorical bias toward the fine-tuning dataset. After low-cost fine-tuning, combining with the mask generator in previous state-of-the-art mask-based open vocabulary segmentation methods, we achieve performance improvements of +1.7, +2.3, +2.1, +3.1, and +0.3 mIoU on the A-847, PC-459, A-150, PC-59, and PAS-20 datasets, respectively. Code is released at <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/MaskCLIPpp">https://github.com/HVision-NKU/MaskCLIPpp</a> . </p>
<blockquote>
<p>åŸºäºæ©è†œç”Ÿæˆå™¨å’Œå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ç­‰è§†è§‰è¯­è¨€æ¨¡å‹ä¹‹é—´çš„ååŒä½œç”¨ï¼Œå¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²æŠ€æœ¯å·²ç»å¾—åˆ°äº†å‘å±•ã€‚æ—©æœŸçš„æ–¹æ³•ä¾§é‡äºç”Ÿæˆæ©è†œï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†æ©è†œç‰¹å¾ä¸æ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¾èµ–ç”Ÿæˆçš„ä½è´¨é‡æ©è†œä¼šå‰Šå¼±åŒºåŸŸè¡¨ç¤ºä¸­çš„è§†è§‰å’Œè¯­è¨€å¯¹é½ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºä¸€ä¸ªæ–°çš„å¾®è°ƒæ¡†æ¶ï¼Œåä¸ºMaskCLIP++ï¼Œå®ƒä½¿ç”¨çœŸå®æ©è†œä»£æ›¿ç”Ÿæˆçš„æ©è†œï¼Œä»¥æé«˜CLIPçš„æ©è†œåˆ†ç±»èƒ½åŠ›ã€‚ç”±äºå¸¦æœ‰æ©è†œæ ‡æ³¨çš„å›¾åƒåˆ†å‰²æ•°æ®é›†å¤šæ ·æ€§æœ‰é™ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥ä¸€è‡´æ€§å¯¹é½çº¦æŸï¼Œè¿™å‡è½»äº†å¯¹å¾®è°ƒæ•°æ®é›†çš„ç±»åˆ«åè§ã€‚ç»è¿‡ä½æˆæœ¬å¾®è°ƒåï¼Œä¸ä¹‹å‰æœ€å…ˆè¿›çš„åŸºäºæ©è†œå¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ä¸­çš„æ©è†œç”Ÿæˆå™¨ç›¸ç»“åˆï¼Œæˆ‘ä»¬åœ¨A-847ã€PC-459ã€A-150ã€PC-59å’ŒPAS-20æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†+1.7ã€+2.3ã€+2.1ã€+3.1å’Œ+0.3çš„mIoUæ€§èƒ½æå‡ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/MaskCLIPpp%E4%B8%8A%E3%80%82">https://github.com/HVision-NKU/MaskCLIPppä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11464v2">PDF</a> 20 pages, 8 figures. Add code link</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMaskCLIP++çš„æ–°å¾®è°ƒæ¡†æ¶ï¼Œä½¿ç”¨çœŸå®æ©è†œæ›¿ä»£ç”Ÿæˆçš„æ©è†œæå‡CLIPçš„æ©è†œåˆ†ç±»èƒ½åŠ›ã€‚ä¸ºæé«˜å›¾åƒåˆ†å‰²æ•°æ®é›†æ©è†œæ ‡æ³¨çš„å¤šæ ·æ€§ï¼Œæå‡ºäº†ä¸€è‡´æ€§å¯¹é½çº¦æŸã€‚ç»“åˆä¹‹å‰çš„å…ˆè¿›æ©è†œç”Ÿæˆå™¨è¿›è¡Œå¾®è°ƒåï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaskCLIP++ä½¿ç”¨çœŸå®æ©è†œæ›¿ä»£ç”Ÿæˆæ©è†œï¼Œä»¥å¢å¼ºCLIPçš„æ©è†œåˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>ç”Ÿæˆçš„ä½è´¨é‡æ©è†œå¯èƒ½å¯¼è‡´è§†è§‰å’Œè¯­è¨€åŒºåŸŸè¡¨ç¤ºçš„å¯¹é½å‡å¼±ã€‚</li>
<li>ä¸ºæé«˜å›¾åƒåˆ†å‰²æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œæå‡ºäº†åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŠ å…¥ä¸€è‡´æ€§å¯¹é½çº¦æŸã€‚</li>
<li>MaskCLIP++é€šè¿‡å¯¹Mask generatorçš„èå…¥å’Œå¾®è°ƒæå‡æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œå®ç°äº†mIoUæ€§èƒ½çš„æå‡ã€‚</li>
<li>é¡¹ç›®çš„ä»£ç å·²ç»å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d183b1f03677ada7567a3e9471c1840d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d945c1fa3f2ce53a5d796eb538c10ebb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20d6f62612372e9ccd67cfdf8959dbec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ce850d0612e0ef2833af25f3f12743.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e459efecb94ed01fc6976d02caba439.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AgriBench-A-Hierarchical-Agriculture-Benchmark-for-Multimodal-Large-Language-Models"><a href="#AgriBench-A-Hierarchical-Agriculture-Benchmark-for-Multimodal-Large-Language-Models" class="headerlink" title="AgriBench: A Hierarchical Agriculture Benchmark for Multimodal Large   Language Models"></a>AgriBench: A Hierarchical Agriculture Benchmark for Multimodal Large   Language Models</h2><p><strong>Authors:Yutong Zhou, Masahiro Ryo</strong></p>
<p>We introduce AgriBench, the first agriculture benchmark designed to evaluate MultiModal Large Language Models (MM-LLMs) for agriculture applications. To further address the agriculture knowledge-based dataset limitation problem, we propose MM-LUCAS, a multimodal agriculture dataset, that includes 1,784 landscape images, segmentation masks, depth maps, and detailed annotations (geographical location, country, date, land cover and land use taxonomic details, quality scores, aesthetic scores, etc), based on the Land Use&#x2F;Cover Area Frame Survey (LUCAS) dataset, which contains comparable statistics on land use and land cover for the European Union (EU) territory. This work presents a groundbreaking perspective in advancing agriculture MM-LLMs and is still in progress, offering valuable insights for future developments and innovations in specific expert knowledge-based MM-LLMs. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Agribenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹å†œä¸šåº”ç”¨çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMM-LLMï¼‰è®¾è®¡çš„å†œä¸šåŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³åŸºäºå†œä¸šçš„æ•°æ®åº“é™åˆ¶é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€å†œä¸šæ•°æ®é›†MM-LUCASã€‚è¯¥æ•°æ®é›†åŸºäºåœŸåœ°åˆ©ç”¨&#x2F;è¦†ç›–é¢ç§¯æ¡†æ¶è°ƒæŸ¥ï¼ˆLUCASï¼‰æ•°æ®é›†ï¼ŒåŒ…å«äº†1784å¼ æ™¯è§‚å›¾åƒã€åˆ†å‰²æ©è†œã€æ·±åº¦å›¾ä»¥åŠè¯¦ç»†çš„æ³¨é‡Šï¼ˆåŒ…æ‹¬åœ°ç†ä½ç½®ã€å›½å®¶ã€æ—¥æœŸã€åœŸåœ°è¦†ç›–å’ŒåœŸåœ°åˆ©ç”¨åˆ†ç±»è¯¦æƒ…ã€è´¨é‡åˆ†æ•°ã€ç¾å­¦åˆ†æ•°ç­‰ï¼‰ã€‚LUCASæ•°æ®é›†åŒ…å«æ¬§ç›Ÿé¢†åœŸå†…åœŸåœ°åˆ©ç”¨å’ŒåœŸåœ°è¦†ç›–çš„å¯æ¯”ç»Ÿè®¡æ•°æ®ã€‚è¿™é¡¹å·¥ä½œä¸ºæ¨è¿›å†œä¸šMM-LLMæä¾›äº†å…¨æ–°çš„è§†è§’ï¼Œå¹¶ä¸”ä»åœ¨è¿›è¡Œä¸­ï¼Œä¸ºæœªæ¥åœ¨ç‰¹å®šä¸“ä¸šçŸ¥è¯†ä¸ºåŸºç¡€çš„MM-LLMé¢†åŸŸçš„å‘å±•å’Œåˆ›æ–°æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00465v2">PDF</a> Accepted by CVPPA @ECCV2024. Dataset:   <a target="_blank" rel="noopener" href="https://github.com/Yutong-Zhou-cv/AgriBench">https://github.com/Yutong-Zhou-cv/AgriBench</a></p>
<p><strong>Summary</strong><br>    æ¨å‡ºå†œä¸šåŸºå‡†æµ‹è¯•AgriBenchï¼Œè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†œä¸šåº”ç”¨çš„è¡¨ç°ã€‚ä¸ºè§£å†³å†œä¸šçŸ¥è¯†æ•°æ®é›†é™åˆ¶é—®é¢˜ï¼ŒåŸºäºåœŸåœ°åˆ©ç”¨&#x2F;è¦†ç›–é¢ç§¯æ¡†æ¶è°ƒæŸ¥ï¼ˆLUCASï¼‰æ•°æ®é›†æå‡ºå¤šæ¨¡æ€å†œä¸šæ•°æ®é›†MM-LUCASï¼ŒåŒ…å«æ™¯è§‚å›¾åƒã€åˆ†å‰²æ©è†œã€æ·±åº¦å›¾åŠè¯¦ç»†æ³¨è§£ï¼Œæ¨åŠ¨å†œä¸šMM-LLMçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AgriBenchæ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†œä¸šåº”ç”¨æ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>MM-LUCASæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å†œä¸šæ•°æ®é›†ï¼ŒåŸºäºLUCASæ•°æ®é›†æ„å»ºï¼ŒåŒ…å«ä¸°å¯Œçš„å†œä¸šç›¸å…³æ•°æ®å’Œè¯¦ç»†æ³¨è§£ã€‚</li>
<li>MM-LUCASæ•°æ®é›†æ—¨åœ¨è§£å†³å†œä¸šçŸ¥è¯†æ•°æ®é›†é™åˆ¶çš„é—®é¢˜ã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ™¯è§‚å›¾åƒã€åˆ†å‰²æ©è†œã€æ·±åº¦å›¾ç­‰å¤šåª’ä½“æ•°æ®ã€‚</li>
<li>æ•°æ®é›†åŒ…å«åœ°ç†ä½ç½®ã€å›½å®¶ã€æ—¥æœŸã€åœŸåœ°è¦†ç›–å’Œåˆ©ç”¨åˆ†ç±»è¯¦æƒ…ã€è´¨é‡è¯„åˆ†ã€ç¾å­¦è¯„åˆ†ç­‰è¯¦ç»†æ³¨è§£ã€‚</li>
<li>AgriBenchå’ŒMM-LUCASçš„æ¨å‡ºä¸ºå†œä¸šMM-LLMçš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œæ´å¯ŸåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e69221c6427ec234e47c958fe55ab46d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fc7e42a440b37db97956807ecb41777.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Concept-Complement-Bottleneck-Model-for-Interpretable-Medical-Image-Diagnosis"><a href="#Concept-Complement-Bottleneck-Model-for-Interpretable-Medical-Image-Diagnosis" class="headerlink" title="Concept Complement Bottleneck Model for Interpretable Medical Image   Diagnosis"></a>Concept Complement Bottleneck Model for Interpretable Medical Image   Diagnosis</h2><p><strong>Authors:Hongmei Wang, Junlin Hou, Hao Chen</strong></p>
<p>Models based on human-understandable concepts have received extensive attention to improve model interpretability for trustworthy artificial intelligence in the field of medical image analysis. These methods can provide convincing explanations for model decisions but heavily rely on the detailed annotation of pre-defined concepts. Consequently, they may not be effective in cases where concepts or annotations are incomplete or low-quality. Although some methods automatically discover effective and new visual concepts rather than using pre-defined concepts or could find some human-understandable concepts via large Language models, they are prone to veering away from medical diagnostic evidence and are challenging to understand. In this paper, we propose a concept complement bottleneck model for interpretable medical image diagnosis with the aim of complementing the existing concept set and finding new concepts bridging the gap between explainable models. Specifically, we propose to use concept adapters for specific concepts to mine the concept differences and score concepts in their own attention channels to support almost fairly concept learning. Then, we devise a concept complement strategy to learn new concepts while jointly using known concepts to improve model performance. Comprehensive experiments on medical datasets demonstrate that our model outperforms the state-of-the-art competitors in concept detection and disease diagnosis tasks while providing diverse explanations to ensure model interpretability effectively. </p>
<blockquote>
<p>åŸºäºäººç±»å¯ç†è§£æ¦‚å¿µçš„æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œä»¥æé«˜äººå·¥æ™ºèƒ½æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä»è€Œå®ç°å¯ä¿¡èµ–çš„äººå·¥æ™ºèƒ½ã€‚è¿™äº›æ–¹æ³•å¯ä»¥ä¸ºæ¨¡å‹å†³ç­–æä¾›ä»¤äººä¿¡æœçš„è§£é‡Šï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–äºé¢„å…ˆå®šä¹‰æ¦‚å¿µçš„è¯¦ç»†æ³¨é‡Šã€‚å› æ­¤ï¼Œåœ¨æ¦‚å¿µæˆ–æ³¨é‡Šä¸å®Œæ•´æˆ–è´¨é‡ä½çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬å¯èƒ½æ— æ³•æœ‰æ•ˆå‘æŒ¥ä½œç”¨ã€‚å°½ç®¡ä¸€äº›æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨å‘ç°æœ‰æ•ˆçš„æ–°è§†è§‰æ¦‚å¿µï¼Œè€Œä¸æ˜¯ä½¿ç”¨é¢„å…ˆå®šä¹‰çš„æ¦‚å¿µï¼Œæˆ–è€…å¯ä»¥é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å‘ç°ä¸€äº›äººç±»å¯ç†è§£çš„æ¦‚å¿µï¼Œä½†å®ƒä»¬å®¹æ˜“åç¦»åŒ»å­¦è¯Šæ–­è¯æ®ï¼Œä¸”éš¾ä»¥ç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¦‚å¿µè¡¥å……ç“¶é¢ˆæ¨¡å‹ï¼Œæ—¨åœ¨ç”¨äºè§£é‡ŠåŒ»å­¦å›¾åƒè¯Šæ–­ï¼Œç›®æ ‡æ˜¯è¡¥å……ç°æœ‰æ¦‚å¿µé›†ï¼Œå¹¶æ‰¾åˆ°å¡«è¡¥è§£é‡Šæ€§æ¨¡å‹ä¹‹é—´å·®è·çš„æ–°æ¦‚å¿µã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ç‰¹å®šæ¦‚å¿µçš„æ¦‚å¿µé€‚é…å™¨æ¥æŒ–æ˜æ¦‚å¿µå·®å¼‚ï¼Œå¹¶åœ¨å„è‡ªçš„æ³¨æ„åŠ›é€šé“ä¸­å¯¹æ¦‚å¿µè¿›è¡Œè¯„åˆ†ï¼Œä»¥æ”¯æŒå‡ ä¹å…¬å¹³çš„æ¦‚å¿µå­¦ä¹ ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ¦‚å¿µè¡¥å……ç­–ç•¥ï¼Œåœ¨å­¦ä¹ æ–°æ¦‚å¿µçš„åŒæ—¶ï¼Œç»“åˆå·²çŸ¥æ¦‚å¿µæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¦‚å¿µæ£€æµ‹å’Œç–¾ç—…è¯Šæ–­ä»»åŠ¡ä¸Šä¼˜äºæœ€æ–°ç«äº‰å¯¹æ‰‹ï¼ŒåŒæ—¶æä¾›å¤šæ ·åŒ–çš„è§£é‡Šï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆè§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15446v2">PDF</a> 27 pages, 5 figures,</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸï¼ŒåŸºäºäººç±»å¯ç†è§£æ¦‚å¿µçš„æ¨¡å‹å—åˆ°å¹¿æ³›å…³æ³¨ä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚è¯¥æ–¹æ³•å¯ä¸ºæ¨¡å‹å†³ç­–æä¾›æœ‰åŠ›è§£é‡Šï¼Œä½†ä¾èµ–äºé¢„å®šä¹‰æ¦‚å¿µçš„è¯¦ç»†æ ‡æ³¨ï¼Œè‹¥æ¦‚å¿µæˆ–æ ‡æ³¨ä¸å®Œæ•´æˆ–è´¨é‡ä½ä¸‹åˆ™æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡æå‡ºæ¦‚å¿µè¡¥å……ç“¶é¢ˆæ¨¡å‹ï¼Œæ—¨åœ¨è¡¥å……ç°æœ‰æ¦‚å¿µé›†å¹¶å‘ç°æ–°æ¦‚å¿µä»¥ç¼©å°è§£é‡Šæ€§æ¨¡å‹çš„å·®è·ã€‚é€šè¿‡æ¦‚å¿µé€‚é…å™¨æŒ–æ˜æ¦‚å¿µå·®å¼‚ï¼Œå¹¶åœ¨å„è‡ªæ³¨æ„åŠ›é€šé“è¯„åˆ†æ–°æ¦‚å¿µä»¥æ”¯æŒå‡ ä¹å…¬å¹³çš„æ¦‚å¿µå­¦ä¹ ã€‚å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šè¶…è¶Šæœ€æ–°ç«äº‰å¯¹æ‰‹ï¼Œåœ¨æ¦‚å¿µæ£€æµ‹å’Œç–¾ç—…è¯Šæ–­ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶æä¾›å¤šæ ·çš„è§£é‡Šä»¥ç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code>1. åŸºäºäººç±»å¯ç†è§£æ¦‚å¿µçš„æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå¤‡å—å…³æ³¨ï¼Œä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚
2. è¿™äº›æ–¹æ³•éœ€è¦é¢„å®šä¹‰æ¦‚å¿µçš„è¯¦ç»†æ ‡æ³¨ï¼Œè‹¥æ ‡æ³¨ä¸å®Œæ•´æˆ–è´¨é‡ä½ä¸‹å¯èƒ½ä¼šå½±å“æ•ˆæœã€‚
3. æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¦‚å¿µè¡¥å……ç“¶é¢ˆæ¨¡å‹ï¼Œæ—¨åœ¨è¡¥å……ç°æœ‰æ¦‚å¿µé›†å¹¶å‘ç°æ–°æ¦‚å¿µã€‚
4. ä½¿ç”¨æ¦‚å¿µé€‚é…å™¨æŒ–æ˜æ¦‚å¿µå·®å¼‚ï¼Œå¹¶åœ¨å„è‡ªçš„æ³¨æ„åŠ›é€šé“è¯„åˆ†æ–°æ¦‚å¿µã€‚
5. æå‡ºäº†ä¸€ç§æ¦‚å¿µè¡¥å……ç­–ç•¥ï¼Œä»¥å­¦ä¹ æ–°æ¦‚å¿µçš„åŒæ—¶åˆ©ç”¨å·²çŸ¥æ¦‚å¿µæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚
6. åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨æ¦‚å¿µæ£€æµ‹å’Œç–¾ç—…è¯Šæ–­ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abeca2c0d429fb0a7ead80914c9519a2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models"><a href="#Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models" class="headerlink" title="Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models"></a>Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models</h2><p><strong>Authors:Pujing Yang, Guangyi Zhang, Yunlong Cai</strong></p>
<p>Recent advances in deep learning-based joint source-channel coding (DJSCC) have shown promise for end-to-end semantic image transmission. However, most existing schemes primarily focus on optimizing pixel-wise metrics, which often fail to align with human perception, leading to lower perceptual quality. In this letter, we propose a novel generative DJSCC approach using conditional diffusion models to enhance the perceptual quality of transmitted images. Specifically, by utilizing entropy models, we effectively manage transmission bandwidth based on the estimated entropy of transmitted sym-bols. These symbols are then used at the receiver as conditional information to guide a conditional diffusion decoder in image reconstruction. Our model is built upon the emerging advanced mamba-like linear attention (MLLA) skeleton, which excels in image processing tasks while also offering fast inference speed. Besides, we introduce a multi-stage training strategy to ensure the stability and improve the overall performance of the model. Simulation results demonstrate that our proposed method significantly outperforms existing approaches in terms of perceptual quality. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„è”åˆæºä¿¡é“ç¼–ç ï¼ˆDJSCCï¼‰çš„æœ€æ–°è¿›å±•ä¸ºç«¯åˆ°ç«¯çš„è¯­ä¹‰å›¾åƒä¼ è¾“å±•ç°äº†å‰æ™¯ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ¡ˆä¸»è¦å…³æ³¨ä¼˜åŒ–åƒç´ çº§çš„æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡é€šå¸¸ä¸äººç±»æ„ŸçŸ¥ä¸åŒ¹é…ï¼Œå¯¼è‡´æ„ŸçŸ¥è´¨é‡è¾ƒä½ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°å‹ç”Ÿæˆå¼DJSCCæ–¹æ³•ï¼Œä»¥æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ç†µæ¨¡å‹ï¼Œæ ¹æ®ä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µæœ‰æ•ˆåœ°ç®¡ç†ä¼ è¾“å¸¦å®½ã€‚è¿™äº›ç¬¦å·ç„¶ååœ¨æ¥æ”¶å™¨ç«¯ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œç”¨äºæŒ‡å¯¼å›¾åƒé‡å»ºä¸­çš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨æ–°å…´çš„é©¬å§†å·´å¼çº¿æ€§æ³¨æ„åŠ›ï¼ˆMLLAï¼‰éª¨æ¶ä¹‹ä¸Šï¼Œè¯¥éª¨æ¶åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›äº†å¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å¹¶æé«˜å…¶æ•´ä½“æ€§èƒ½ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02597v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è”åˆæºä¿¡é“ç¼–ç ï¼ˆDJSCCï¼‰æœ€æ–°è¿›å±•ä¸ºç«¯åˆ°ç«¯çš„è¯­ä¹‰å›¾åƒä¼ è¾“å±•ç°äº†æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ¡ˆä¸»è¦å…³æ³¨åƒç´ çº§çš„ä¼˜åŒ–æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡å¾€å¾€ä¸ç¬¦åˆäººç±»æ„ŸçŸ¥ï¼Œå¯¼è‡´æ„ŸçŸ¥è´¨é‡è¾ƒä½ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°å‹ç”Ÿæˆå¼DJSCCæ–¹æ³•ï¼Œä»¥æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ï¿½ï¿½onæ¨¡å‹æœ‰æ•ˆç®¡ç†ä¼ è¾“å¸¦å®½ï¼Œæ ¹æ®ä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µè¿›è¡Œä¼ è¾“ã€‚è¿™äº›ç¬¦å·éšåè¢«ç”¨ä½œæ¥æ”¶å™¨çš„æ¡ä»¶ä¿¡æ¯ï¼Œä»¥æŒ‡å¯¼å›¾åƒé‡å»ºä¸­çš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨æ–°å…´çš„é©¬å§†å·´å¼çº¿æ€§æ³¨æ„åŠ›ï¼ˆMLLAï¼‰éª¨æ¶ä¹‹ä¸Šï¼Œè¯¥éª¨æ¶åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›å¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å¹¶æé«˜å…¶æ•´ä½“æ€§èƒ½ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ çš„è”åˆæºä¿¡é“ç¼–ç ï¼ˆDJSCCï¼‰åœ¨è¯­ä¹‰å›¾åƒä¼ è¾“ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ¡ˆä¸»è¦å…³æ³¨åƒç´ çº§ä¼˜åŒ–æŒ‡æ ‡ï¼Œè¿™å¾€å¾€ä¸ç¬¦åˆäººç±»æ„ŸçŸ¥ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹ç”Ÿæˆå¼DJSCCæ–¹æ³•ï¼ŒåŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>åˆ©ç”¨ç†µæ¨¡å‹æœ‰æ•ˆç®¡ç†ä¼ è¾“å¸¦å®½ï¼Œæ ¹æ®ä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µè¿›è¡Œæ•°æ®ä¼ è¾“ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨é©¬å§†å·´å¼çº¿æ€§æ³¨æ„åŠ›ï¼ˆMLLAï¼‰éª¨æ¶ï¼Œé€‚ç”¨äºå›¾åƒå¤„ç†å¹¶å…·å¤‡å¿«é€Ÿæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæé«˜æ¨¡å‹ç¨³å®šæ€§å’Œæ•´ä½“æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eff982c45e695cae805b96e6113523d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b91ed06a7a95f47ccfceb2e9725c4363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aabf79dae9b7fc082b1b85d8c6c06d9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c893e4c70b00b37fcfbb0f773226c9c7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ProCNS-Progressive-Prototype-Calibration-and-Noise-Suppression-for-Weakly-Supervised-Medical-Image-Segmentation"><a href="#ProCNS-Progressive-Prototype-Calibration-and-Noise-Suppression-for-Weakly-Supervised-Medical-Image-Segmentation" class="headerlink" title="ProCNS: Progressive Prototype Calibration and Noise Suppression for   Weakly-Supervised Medical Image Segmentation"></a>ProCNS: Progressive Prototype Calibration and Noise Suppression for   Weakly-Supervised Medical Image Segmentation</h2><p><strong>Authors:Y. Liu, L. Lin, K. K. Y. Wong, X. Tang</strong></p>
<p>Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest with more reliable guidance. The affinities are derived from the input images and the prototype-refined predictions. Meanwhile, we propose an Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and representative prototype representations, which adaptively identifies and masks noisy regions within the pseudo proposals, reducing potential erroneous interference during prototype computation. Furthermore, we generate specialized soft pseudo-labels for the noisy regions identified by ANPM, providing supplementary supervision. Extensive experiments on six medical image segmentation tasks involving different modalities demonstrate that the proposed framework significantly outperforms representative state-of-the-art methods. </p>
<blockquote>
<p>å¼±ç›‘ç£åˆ†å‰²ï¼ˆWSSï¼‰ä½œä¸ºä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡é‡‡ç”¨ç¨€ç–æ³¨é‡Šæ ¼å¼ï¼ˆå¦‚ç‚¹ã€æ¶‚é¸¦ã€å—ç­‰ï¼‰æ¥ç¼“è§£æ³¨é‡Šæˆæœ¬ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å†²çªã€‚å…¸å‹çš„æ–¹æ³•è¯•å›¾åˆ©ç”¨è§£å‰–å­¦å’Œæ‹“æ‰‘å­¦å…ˆéªŒçŸ¥è¯†æ¥ç›´æ¥å°†ç¨€ç–æ³¨é‡Šæ‰©å±•ä¸ºä¼ªæ ‡ç­¾ã€‚ç„¶è€Œï¼Œç”±äºåŒ»å­¦å›¾åƒä¸­æ¨¡ç³Šè¾¹ç¼˜çš„å¿½è§†ä»¥åŠç¨€ç–ç›‘ç£çš„ä¸è¶³ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¼šåœ¨å™ªå£°åŒºåŸŸç”Ÿæˆé”™è¯¯ä¸”è¿‡äºè‡ªä¿¡çš„ä¼ªææ¡ˆï¼Œä»è€Œå¯¼è‡´æ¨¡å‹çš„ç´¯ç§¯è¯¯å·®å’Œæ€§èƒ½ä¸‹é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„WSSæ–¹æ³•ï¼Œåä¸ºProCNSï¼ŒåŒ…å«ä¸¤ä¸ªååŒå·¥ä½œçš„æ¨¡å—ï¼Œä»¥æ¸è¿›åŸå‹æ ¡å‡†å’Œå™ªå£°æŠ‘åˆ¶çš„åŸåˆ™è®¾è®¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºåŸå‹çš„åŒºåŸŸç©ºé—´äº²å’ŒåŠ›ï¼ˆPRSAï¼‰æŸå¤±ï¼Œä»¥æœ€å¤§åŒ–ç©ºé—´å…ƒç´ å’Œè¯­ä¹‰å…ƒç´ ä¹‹é—´çš„æˆå¯¹äº²å’ŒåŠ›ï¼Œä¸ºæˆ‘ä»¬çš„ç›®æ ‡æ¨¡å‹æä¾›æ›´å¯é çš„æŒ‡å¯¼ã€‚äº²å’ŒåŠ›æ¥æºäºè¾“å…¥å›¾åƒå’Œç»è¿‡åŸå‹ä¼˜åŒ–çš„é¢„æµ‹ç»“æœã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”å™ªå£°æ„ŸçŸ¥å’Œæ©è”½ï¼ˆANPMï¼‰æ¨¡å—ï¼Œä»¥è·å¾—æ›´ä¸°å¯Œå’Œå…·æœ‰ä»£è¡¨æ€§çš„åŸå‹è¡¨ç¤ºï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°è¯†åˆ«å’Œæ©è”½ä¼ªææ¡ˆä¸­çš„å™ªå£°åŒºåŸŸï¼Œå‡å°‘åœ¨åŸå‹è®¡ç®—è¿‡ç¨‹ä¸­æ½œåœ¨çš„é”™è¯¯å¹²æ‰°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºANPMè¯†åˆ«çš„å™ªå£°åŒºåŸŸç”Ÿæˆä¸“é—¨çš„è½¯ä¼ªæ ‡ç­¾ï¼Œä»¥æä¾›é¢å¤–çš„ç›‘ç£ã€‚åœ¨å…­ä¸ªæ¶‰åŠä¸åŒæ¨¡æ€çš„åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶æ˜¾è‘—ä¼˜äºå…·æœ‰ä»£è¡¨æ€§çš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.14074v3">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å¼±ç›‘ç£å­¦ä¹ ï¼ˆWSSï¼‰æ–¹æ³•å­˜åœ¨è¯¯å·®ç´¯ç§¯å’Œæ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºProCNSçš„æ–°å‹WSSæ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªååŒæ¨¡å—ï¼ŒåŸºäºæ¸è¿›åŸå‹æ ¡å‡†å’Œå™ªå£°æŠ‘åˆ¶åŸåˆ™ã€‚è®¾è®¡äº†ä¸€ç§åŸºäºåŒºåŸŸç©ºé—´äº²å’ŒåŠ›çš„æŸå¤±å‡½æ•°PRSAï¼Œæé«˜æ¨¡å‹å¯é æ€§ã€‚å¹¶æå‡ºè‡ªé€‚åº”å™ªå£°æ„ŸçŸ¥å’Œæ©è”½æ¨¡å—ANPMï¼Œç”¨äºæ›´ä¸°å¯Œå’Œä»£è¡¨æ€§çš„åŸå‹è¡¨ç¤ºï¼Œå‡å°‘ä¼ªææ¡ˆä¸­çš„å™ªå£°å¹²æ‰°ã€‚åœ¨å…­ç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£åˆ†å‰²ï¼ˆWSSï¼‰æ˜¯è§£å†³æ ‡æ³¨æˆæœ¬ä¸æ¨¡å‹æ€§èƒ½å†²çªçš„ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œé‡‡ç”¨ç¨€ç–æ ‡æ³¨æ ¼å¼ï¼ˆå¦‚ç‚¹ã€æ¶‚é¸¦ã€å—ç­‰ï¼‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å€¾å‘äºåœ¨å™ªå£°åŒºåŸŸäº§ç”Ÿé”™è¯¯å’Œè¿‡äºè‡ªä¿¡çš„ä¼ªææ¡ˆï¼Œå¯¼è‡´æ¨¡å‹è¯¯å·®ç´¯ç§¯å’Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹WSSæ–¹æ³•ProCNSï¼ŒåŒ…å«ä¸¤ä¸ªååŒæ¨¡å—ï¼šåŸºäºæ¸è¿›åŸå‹æ ¡å‡†å’Œå™ªå£°æŠ‘åˆ¶ã€‚</li>
<li>è®¾è®¡äº†PRSAæŸå¤±å‡½æ•°ï¼Œé€šè¿‡æœ€å¤§åŒ–ç©ºé—´å…ƒç´ å’Œè¯­ä¹‰å…ƒç´ ä¹‹é—´çš„é…å¯¹äº²å’ŒåŠ›ï¼Œä¸ºæ¨¡å‹æä¾›æ›´å¯é çš„æŒ‡å¯¼ã€‚</li>
<li>ANPMæ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°è¯†åˆ«å’Œæ©ç›–ä¼ªææ¡ˆä¸­çš„å™ªå£°åŒºåŸŸï¼Œå‡å°‘æ½œåœ¨é”™è¯¯å¹²æ‰°ã€‚</li>
<li>ProCNSç”Ÿæˆé’ˆå¯¹ANPMè¯†åˆ«çš„å™ªå£°åŒºåŸŸçš„ç‰¹æ®Šè½¯ä¼ªæ ‡ç­¾ï¼Œæä¾›é¢å¤–çš„ç›‘ç£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.14074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2c4e77143b27993cb359d4c28000b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eae7e98dd5c16ba0fab63e32d5cea0e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aad1b483ecf55a9d4c8108bcfe7d3a70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e59fe3bab2fc3479acb165072fb224.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f8161d39818c9af9cce74b56c1b23e15.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-26  Portability of Fortran's `do concurrent' on GPUs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-1ead218b7a67780339b0cc9755cc860e.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-26  PartGen Part-level 3D Generation and Reconstruction with Multi-View   Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">10254.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
