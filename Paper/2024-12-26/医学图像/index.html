<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-26  Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors   Diverse-Resolution Training Outperforms Interpolation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3ee184809f97812831c45a8673743f23.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    52 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-26-更新"><a href="#2024-12-26-更新" class="headerlink" title="2024-12-26 更新"></a>2024-12-26 更新</h1><h2 id="Resolution-Robust-3D-MRI-Reconstruction-with-2D-Diffusion-Priors-Diverse-Resolution-Training-Outperforms-Interpolation"><a href="#Resolution-Robust-3D-MRI-Reconstruction-with-2D-Diffusion-Priors-Diverse-Resolution-Training-Outperforms-Interpolation" class="headerlink" title="Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:   Diverse-Resolution Training Outperforms Interpolation"></a>Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:   Diverse-Resolution Training Outperforms Interpolation</h2><p><strong>Authors:Anselm Krainovic, Stefan Ruschke, Reinhard Heckel</strong></p>
<p>Deep learning-based 3D imaging, in particular magnetic resonance imaging (MRI), is challenging because of limited availability of 3D training data. Therefore, 2D diffusion models trained on 2D slices are starting to be leveraged for 3D MRI reconstruction. However, as we show in this paper, existing methods pertain to a fixed voxel size, and performance degrades when the voxel size is varied, as it is often the case in clinical practice. In this paper, we propose and study several approaches for resolution-robust 3D MRI reconstruction with 2D diffusion priors. As a result of this investigation, we obtain a simple resolution-robust variational 3D reconstruction approach based on diffusion-guided regularization of randomly sampled 2D slices. This method provides competitive reconstruction quality compared to posterior sampling baselines. Towards resolving the sensitivity to resolution-shifts, we investigate state-of-the-art model-based approaches including Gaussian splatting, neural representations, and infinite-dimensional diffusion models, as well as a simple data-centric approach of training the diffusion model on several resolutions. Our experiments demonstrate that the model-based approaches fail to close the performance gap in 3D MRI. In contrast, the data-centric approach of training the diffusion model on various resolutions effectively provides a resolution-robust method without compromising accuracy. </p>
<blockquote>
<p>基于深度学习的3D成像，特别是磁共振成像（MRI）面临挑战，因为高质量的三维训练数据有限。因此，人们开始利用二维扩散模型对二维切片进行训练，用于三维MRI重建。然而，我们在本文中展示，现有方法涉及固定体素大小，当体素大小发生变化时，性能会下降，这在临床实践中是常见的情况。在本文中，我们针对具有二维扩散先验的分辨率鲁棒三维MRI重建提出了几种方法并进行研究。通过这项研究，我们获得了一种基于随机采样二维切片的扩散引导正则化的简单分辨率鲁棒三维重建方法。该方法与后采样基线相比，可提供具有竞争力的重建质量。为了解决对分辨率变化的敏感性，我们调查了基于模型的最新方法，包括高斯展片法、神经表示和无限维扩散模型等，以及一种以数据为中心的简单方法，即在多种分辨率上训练扩散模型。我们的实验表明，基于模型的最新方法未能缩小三维MRI的性能差距。相比之下，以数据为中心的直接在多种分辨率上训练扩散模型的方法有效地提供了一种分辨率鲁棒的方法，不会损害准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18584v1">PDF</a> </p>
<p><strong>Summary</strong><br>     深度学习在三维成像，特别是磁共振成像（MRI）中面临挑战，因为高质量的三维训练数据有限。研究者开始利用二维扩散模型对三维MRI进行重建。然而，现有方法受限于固定体素大小，在临床实践中体素大小变化时性能下降。本文提出并研究了几种分辨率稳健的三维MRI重建方法，基于二维扩散先验。经过研究，提出了一种基于扩散引导正则化的随机采样二维切片分辨率稳健的三维重建方法。此外，本文对基于模型的最新方法如高斯涂抹、神经表示和无限维扩散模型以及一种简单的以数据为中心的训练扩散模型的方法进行了实验对比。实验表明，基于模型的方法在3D MRI上无法缩小性能差距。相反，以数据为中心的训练扩散模型在各种分辨率上的方法有效地提供了分辨率稳健且准确的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在三维磁共振成像（MRI）中面临高质量训练数据有限的挑战。</li>
<li>二维扩散模型被用于三维MRI重建。</li>
<li>现有方法受限于固定体素大小，体素大小变化时性能下降。</li>
<li>提出了几种分辨率稳健的三维MRI重建方法，基于二维扩散先验。</li>
<li>一种基于扩散引导正则化的随机采样二维切片方法提供竞争力。</li>
<li>基于模型的方法在解决分辨率变化问题上效果有限。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18584">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4ab37b91bb3c25653117a3e6afd05dc0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4277b872cd8a48906ad1edb80745a469.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3947428a182ab03c5f554d225b6327e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FedVCK-Non-IID-Robust-and-Communication-Efficient-Federated-Learning-via-Valuable-Condensed-Knowledge-for-Medical-Image-Analysis"><a href="#FedVCK-Non-IID-Robust-and-Communication-Efficient-Federated-Learning-via-Valuable-Condensed-Knowledge-for-Medical-Image-Analysis" class="headerlink" title="FedVCK: Non-IID Robust and Communication-Efficient Federated Learning   via Valuable Condensed Knowledge for Medical Image Analysis"></a>FedVCK: Non-IID Robust and Communication-Efficient Federated Learning   via Valuable Condensed Knowledge for Medical Image Analysis</h2><p><strong>Authors:Guochen Yan, Luyuan Xie, Xinyi Gao, Wentao Zhang, Qingni Shen, Yuejian Fang, Zhonghai Wu</strong></p>
<p>Federated learning has become a promising solution for collaboration among medical institutions. However, data owned by each institution would be highly heterogeneous and the distribution is always non-independent and identical distribution (non-IID), resulting in client drift and unsatisfactory performance. Despite existing federated learning methods attempting to solve the non-IID problems, they still show marginal advantages but rely on frequent communication which would incur high costs and privacy concerns. In this paper, we propose a novel federated learning method: \textbf{Fed}erated learning via \textbf{V}aluable \textbf{C}ondensed \textbf{K}nowledge (FedVCK). We enhance the quality of condensed knowledge and select the most necessary knowledge guided by models, to tackle the non-IID problem within limited communication budgets effectively. Specifically, on the client side, we condense the knowledge of each client into a small dataset and further enhance the condensation procedure with latent distribution constraints, facilitating the effective capture of high-quality knowledge. During each round, we specifically target and condense knowledge that has not been assimilated by the current model, thereby preventing unnecessary repetition of homogeneous knowledge and minimizing the frequency of communications required. On the server side, we propose relational supervised contrastive learning to provide more supervision signals to aid the global model updating. Comprehensive experiments across various medical tasks show that FedVCK can outperform state-of-the-art methods, demonstrating that it’s non-IID robust and communication-efficient. </p>
<blockquote>
<p>联邦学习已成为医疗机构间协作的具有前景的解决方案。然而，每个机构拥有的数据将是高度异构的，且分布通常是非独立同分布的（non-IID），导致客户端偏移和性能不佳。尽管现有的联邦学习方法试图解决非IID问题，但它们仍显示出有限的优势，并且依赖于频繁的通信，这会产生高昂的成本和隐私担忧。在本文中，我们提出了一种新型的联邦学习方法：通过有价值的浓缩知识（FedVCK）进行联邦学习。我们提高了浓缩知识的质量，并在模型的指导下选择最必要的知识，以在有限的通信预算内有效解决非IID问题。具体来说，在客户端，我们将每个客户端的知识浓缩成一个小数据集，并进一步通过潜在分布约束增强浓缩过程，从而有效地捕获高质量的知识。在每一轮中，我们特别针对当前模型尚未吸收的知识进行浓缩，从而防止不必要地重复同质知识并最小化所需的通信频率。在服务器端，我们提出了关系监督对比学习，为全局模型更新提供更多的监督信号。在多种医学任务上的综合实验表明，FedVCK可以超越最新方法，表明其在非IID情况下的稳健性和通信效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18557v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的联邦学习方法——FedVCK，通过凝结有价值的知识来解决医疗机构间数据高度异构和非独立同分布（non-IID）的问题，提高模型性能。该方法在客户端通过凝结知识并施加潜在分布约束来捕获高质量知识，减少通信频率；在服务器端采用关系监督对比学习，为全局模型更新提供更多监督信号。实验表明，FedVCK在医疗任务上表现出优异性能，具有non-IID鲁棒性和通信效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习已成为医疗协作领域的一种有前途的解决方案。</li>
<li>医疗机构数据存在高度异构性和非独立同分布（non-IID）问题，导致模型性能不佳。</li>
<li>现有联邦学习方法虽能解决non-IID问题，但通信成本高昂并存在隐私担忧。</li>
<li>FedVCK方法通过凝结和选择必要的知识来解决non-IID问题，并在有限通信预算内有效应对。</li>
<li>FedVCK在客户端实施知识凝结，并施加潜在分布约束以捕获高质量知识。</li>
<li>在服务器端，FedVCK采用关系监督对比学习，为全局模型更新提供更多监督信号。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18557">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4416cf665ef92011ca7d34881861b068.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19535be5ab764dcc4bc8139dc0e7e1af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd0c22a702b56d6c776d70233153126b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5df0b189373f5d5b7c40f1935073998a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Advancing-Deformable-Medical-Image-Registration-with-Multi-axis-Cross-covariance-Attention"><a href="#Advancing-Deformable-Medical-Image-Registration-with-Multi-axis-Cross-covariance-Attention" class="headerlink" title="Advancing Deformable Medical Image Registration with Multi-axis   Cross-covariance Attention"></a>Advancing Deformable Medical Image Registration with Multi-axis   Cross-covariance Attention</h2><p><strong>Authors:Mingyuan Meng, Michael Fulham, Lei Bi, Jinman Kim</strong></p>
<p>Deformable image registration is a fundamental requirement for medical image analysis. Recently, transformers have been widely used in deep learning-based registration methods for their ability to capture long-range dependency via self-attention (SA). However, the high computation and memory loads of SA (growing quadratically with the spatial resolution) hinder transformers from processing subtle textural information in high-resolution image features, e.g., at the full and half image resolutions. This limits deformable registration as the high-resolution textural information is crucial for finding precise pixel-wise correspondence between subtle anatomical structures. Cross-covariance Attention (XCA), as a “transposed” version of SA that operates across feature channels, has complexity growing linearly with the spatial resolution, providing the feasibility of capturing long-range dependency among high-resolution image features. However, existing XCA-based transformers merely capture coarse global long-range dependency, which are unsuitable for deformable image registration relying primarily on fine-grained local correspondence. In this study, we propose to improve existing deep learning-based registration methods by embedding a new XCA mechanism. To this end, we design an XCA-based transformer block optimized for deformable medical image registration, named Multi-Axis XCA (MAXCA). Our MAXCA serves as a general network block that can be embedded into various registration network architectures. It can capture both global and local long-range dependency among high-resolution image features by applying regional and dilated XCA in parallel via a multi-axis design. Extensive experiments on two well-benchmarked inter-&#x2F;intra-patient registration tasks with seven public medical datasets demonstrate that our MAXCA block enables state-of-the-art registration performance. </p>
<blockquote>
<p>可变图像配准（Deformable Image Registration）是医学图像分析的基本需求。近期，由于其通过自注意力（Self-Attention, SA）捕捉长距离依赖关系的能力，transformer在各种基于深度学习的配准方法中得到了广泛应用。然而，自注意力的计算与内存负载较高（随着空间分辨率的二次方增长），这限制了其在处理高清晰度图像特征中的微妙纹理信息方面的能力，比如在全图和半图分辨率时。这对于可变配准是一个限制，因为高清晰度的纹理信息对于在微妙的解剖结构之间找到精确的像素对应至关重要。作为在特征通道之间进行操作的跨协方差注意力（Cross-covariance Attention, XCA）是一种自注意力的“转置”版本，其复杂性随空间分辨率线性增长，为捕捉高清晰度图像特征之间的长距离依赖关系提供了可行性。然而，现有的基于XCA的transformer仅捕捉粗糙的全局长距离依赖关系，对于主要依赖于精细局部对应关系的可变图像配准来说并不适用。本研究旨在通过嵌入新的XCA机制改进现有的基于深度学习的配准方法。为此，我们设计了一个基于XCA的优化块用于可变医学图像配准，称为多轴XCA（MAXCA）。我们的MAXCA作为一个通用网络块，可以嵌入到各种配准网络架构中。它通过并行应用区域和膨胀XCA的多轴设计来捕捉高分辨率图像特征之间的全局和局部长距离依赖关系。在两个广泛认可的患者间和患者内配准任务上的七个公开医学数据集的大量实验表明，我们的MAXCA块可实现最先进的配准性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18545v1">PDF</a> Under Review</p>
<p><strong>摘要</strong><br>    本研究针对医学图像分析中的可变形图像配准问题，提出了基于交叉协方差注意力（XCA）机制的改进深度学习方法。通过设计一种名为Multi-Axis XCA（MAXCA）的XCA基础transformer块，能捕获高分辨图像特征中的全局和局部远程依赖关系，实现了对现有深度学习配准方法的改进。该MAXCA块可作为通用网络块嵌入各种配准网络架构中。在跨患者和同一患者的配准任务以及七个公开医学数据集上的广泛实验表明，MAXCA块实现了最先进的配准性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>可变形图像配准在医学图像分析中具有重要意义。</li>
<li>变压器因自我注意力（SA）机制而广泛应用于深度学习中，但其在处理高分辨图像特征时的计算和内存负载较高。</li>
<li>交叉协方差注意力（XCA）作为一种“转置”的自注意力机制，能在高分辨图像特征上捕获远程依赖性，复杂度随空间分辨率呈线性增长。</li>
<li>现有基于XCA的变压器仅捕获粗糙的全局长程依赖性，不适用于依赖精细局部对应的可变形图像配准。</li>
<li>研究中提出了Multi-Axis XCA（MAXCA）块，这是一种针对可变形医学图像配准的XCA基础变压器块。</li>
<li>MAXCA块通过并行应用区域和膨胀XCA的多轴设计，能够捕获高分辨图像特征中的全局和局部远程依赖性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18545">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b411daf4bbab786cf04acb38f2fdac58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0b3be03bd730b4c5ae9251df4b5f79d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a18c6e27ae7e189573bcb5c6fe228fd5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis"><a href="#VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis" class="headerlink" title="VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis"></a>VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis</h2><p><strong>Authors:Shicheng Yin, Kaixuan Yin, Weixing Chen, Enbo Huang, Yang Liu</strong></p>
<p>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are two dominant models for image analysis. While CNNs excel at extracting multi-scale features and ViTs effectively capture global dependencies, both suffer from high computational costs, particularly when processing high-resolution images. Recently, state-space models (SSMs) and recurrent neural networks (RNNs) have attracted attention due to their efficiency. However, their performance in image classification tasks remains limited. To address these challenges, this paper introduces VisionGRU, a novel RNN-based architecture designed for efficient image classification. VisionGRU leverages a simplified Gated Recurrent Unit (minGRU) to process large-scale image features with linear complexity. It divides images into smaller patches and progressively reduces the sequence length while increasing the channel depth, thus facilitating multi-scale feature extraction. A hierarchical 2DGRU module with bidirectional scanning captures both local and global contexts, improving long-range dependency modeling, particularly for tasks like semantic segmentation. Experimental results on the ImageNet and ADE20K datasets demonstrate that VisionGRU outperforms ViTs, significantly reducing memory usage and computational costs, especially for high-resolution images. These findings underscore the potential of RNN-based approaches for developing efficient and scalable computer vision solutions. Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a>. </p>
<blockquote>
<p>卷积神经网络（CNN）和视觉转换器（ViT）是图像分析的两种主导模型。虽然CNN擅长提取多尺度特征，而ViT能有效地捕捉全局依赖性，但它们都存在计算成本高昂的问题，特别是在处理高分辨率图像时。近年来，由于效率较高，状态空间模型（SSM）和循环神经网络（RNN）备受关注。然而，它们在图像分类任务中的表现仍然有限。为了解决这些挑战，本文提出了一种新型的基于RNN的图像分类架构——VisionGRU。VisionGRU利用简化的门控循环单元（minGRU）以线性复杂度处理大规模图像特征。它将图像分成较小的斑块，逐步减少序列长度，同时增加通道深度，从而便于多尺度特征提取。具有双向扫描的分层2DGRU模块能够捕获局部和全局上下文，改进了长距离依赖建模，尤其在语义分割等任务中表现优异。在ImageNet和ADE20K数据集上的实验结果证明，VisionGRU的性能优于ViT，显著减少了内存使用和计算成本，尤其适用于高分辨率图像。这些发现突显了基于RNN的方法在开发高效且可扩展的计算机视觉解决方案方面的潜力。相关代码将在<a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/YangLiu9208/VisionGRU上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18178v1">PDF</a> Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a></p>
<p><strong>Summary</strong></p>
<p>本论文提出一种基于RNN的新型图像分类架构——VisionGRU，它利用简化的门控循环单元（minGRU）处理大规模图像特征，具有线性复杂度。VisionGRU将图像分成较小的斑块，在增加通道深度的同时逐步减少序列长度，促进多尺度特征提取。其层次化的2DGRU模块具有双向扫描功能，能捕捉局部和全局上下文，特别是在语义分割等任务中，长期依赖建模表现优异。在ImageNet和ADE20K数据集上的实验结果表明，VisionGRU在降低内存使用和计算成本的同时，优于ViTs，特别是在处理高分辨率图像时表现更出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisionGRU是一种基于RNN的新型图像分类架构。</li>
<li>VisionGRU利用简化的Gated Recurrent Unit（minGRU）处理图像特征，具有线性复杂度。</li>
<li>VisionGRU将图像分成较小的斑块，逐步处理，促进多尺度特征提取。</li>
<li>层次化的2DGRU模块具有双向扫描，能捕捉局部和全局上下文。</li>
<li>VisionGRU在语义分割等任务中，长期依赖建模表现优异。</li>
<li>在ImageNet和ADE20K数据集上，VisionGRU表现优于ViTs，尤其是处理高分辨率图像时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3ee184809f97812831c45a8673743f23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49746cf59be5692774d012e2aab7bab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73664fb72c773b1bf490d6a20e930639.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f8b7955c0062d89f3422d901b00d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32fecfa3a65b425b6a4fd26d38b4a8f5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Optimization-of-Convolutional-Neural-Network-Hyperparameter-for-Medical-Image-Diagnosis-using-Metaheuristic-Algorithms-A-short-Recent-Review-2019-2022"><a href="#Optimization-of-Convolutional-Neural-Network-Hyperparameter-for-Medical-Image-Diagnosis-using-Metaheuristic-Algorithms-A-short-Recent-Review-2019-2022" class="headerlink" title="Optimization of Convolutional Neural Network Hyperparameter for Medical   Image Diagnosis using Metaheuristic Algorithms: A short Recent Review   (2019-2022)"></a>Optimization of Convolutional Neural Network Hyperparameter for Medical   Image Diagnosis using Metaheuristic Algorithms: A short Recent Review   (2019-2022)</h2><p><strong>Authors:Qusay Shihab Hamad, Hussein Samma, Shahrel Azmin Suandi</strong></p>
<p>Convolutional Neural Networks (CNNs) have been successfully utilized in the medical diagnosis of many illnesses. Nevertheless, identifying the optimal architecture and hyperparameters among the available possibilities might be a substantial challenge. Typically, CNN hyperparameter selection is performed manually. Nonetheless, this is a computationally costly procedure, as numerous rounds of hyperparameter settings must be evaluated to determine which produces the best results. Choosing the proper hyperparameter settings has always been a crucial and challenging task, as it depends on the researcher’s knowledge and experience. This study will present work done in recent years on the usage of metaheuristic optimization algorithms in the CNN optimization process. It looks at a number of recent studies that focus on the use of optimization methods to optimize hyperparameters in order to find high-performing CNNs. This helps researchers figure out how to set hyperparameters efficiently. </p>
<blockquote>
<p>卷积神经网络（CNNs）已在多种疾病的医学诊断中成功应用。然而，在众多的可能架构和超参数中识别出最优的架构和超参数可能是一个巨大的挑战。通常，CNN超参数的选择是手动完成的。然而，这是一个计算成本高昂的过程，因为必须评估多轮超参数设置，以确定哪种设置能产生最佳结果。选择合适的超参数设置一直是一项至关重要且具挑战性的任务，因为它取决于研究人员的知识和经验。本研究将介绍近年来在卷积神经网络优化过程中使用元启发式优化算法的工作。它关注了一些近期的研究，这些研究专注于使用优化方法来优化超参数，以找到高性能的卷积神经网络。这有助于研究人员有效地设置超参数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17956v1">PDF</a> its a conference paper; the full proceeding is available online at   <a target="_blank" rel="noopener" href="https://icogoia.utem.edu.my/proceedings.html">https://icogoia.utem.edu.my/proceedings.html</a></p>
<p><strong>Summary</strong></p>
<p>卷积神经网络（CNN）在医学诊断中已得到广泛应用。然而，在众多的网络架构与超参数中，如何选择最优的架构与超参数是一个巨大的挑战。手动选择CNN超参数是一个计算成本高昂的过程，需要评估多种超参数设置以找出最佳结果。本研究将介绍近年来使用元启发式优化算法在CNN优化过程中的研究。该研究关注使用优化方法来优化超参数，以找到性能高的CNN，有助于研究人员高效设置超参数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CNN已成功应用于医学诊断。</li>
<li>选择CNN的最优架构和超参数是一个挑战。</li>
<li>手动选择CNN超参数是一个计算成本高昂的过程。</li>
<li>元启发式优化算法被用于CNN的优化过程中。</li>
<li>使用优化方法来优化超参数，以找到高性能的CNN。</li>
<li>这些方法有助于研究人员更有效地设置CNN的超参数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17956">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b77ebfb0123fea1622a8c9c02cab7194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6171fcc0a7f6de117a916b81474eca3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eb18e3d1bb0caa4892f5a16140e4bbc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cadb74f4223e3192088b541585c5d69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a957bf543f4bee29aa7f8039e531f28b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b499514baa263269a17130565d9e1fa6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hyperbolic-Chamfer-Distance-for-Point-Cloud-Completion-and-Beyond"><a href="#Hyperbolic-Chamfer-Distance-for-Point-Cloud-Completion-and-Beyond" class="headerlink" title="Hyperbolic Chamfer Distance for Point Cloud Completion and Beyond"></a>Hyperbolic Chamfer Distance for Point Cloud Completion and Beyond</h2><p><strong>Authors:Fangzhou Lin, Songlin Hou, Haotian Liu, Shang Gao, Kazunori D Yamada, Haichong K. Zhang, Ziming Zhang</strong></p>
<p>Chamfer Distance (CD) is widely used as a metric to quantify difference between two point clouds. In point cloud completion, Chamfer Distance (CD) is typically used as a loss function in deep learning frameworks. However, it is generally acknowledged within the field that Chamfer Distance (CD) is vulnerable to the presence of outliers, which can consequently lead to the convergence on suboptimal models. In divergence from the existing literature, which largely concentrates on resolving such concerns in the realm of Euclidean space, we put forth a notably uncomplicated yet potent metric specifically designed for point cloud completion tasks: {Hyperbolic Chamfer Distance (HyperCD)}. This metric conducts Chamfer Distance computations within the parameters of hyperbolic space. During the backpropagation process, HyperCD systematically allocates greater weight to matched point pairs exhibiting reduced Euclidean distances. This mechanism facilitates the preservation of accurate point pair matches while permitting the incremental adjustment of suboptimal matches, thereby contributing to enhanced point cloud completion outcomes. Moreover, measure the shape dissimilarity is not solely work for point cloud completion task, we further explore its applications in other generative related tasks, including single image reconstruction from point cloud, and upsampling. We demonstrate state-of-the-art performance on the point cloud completion benchmark datasets, PCN, ShapeNet-55, and ShapeNet-34, and show from visualization that HyperCD can significantly improve the surface smoothness, we also provide the provide experimental results beyond completion task. </p>
<blockquote>
<p>Chamfer Distance（CD）被广泛用作衡量两个点云之间差异的指标。在点云补全中，Chamfer Distance（CD）通常作为深度学习框架中的损失函数。然而，业内普遍认为Chamfer Distance（CD）容易受到异常值的影响，这可能导致收敛到非最优模型。与现有文献大多集中在解决欧几里得空间领域的此类问题不同，我们提出了一种专门为点云补全任务设计的简洁而强大的指标：Hyperbolic Chamfer Distance（HyperCD）。该指标在双曲空间参数内进行Chamfer Distance计算。在反向传播过程中，HyperCD系统地给表现出较小欧几里得距离的匹配点分配更大的权重。这种机制有助于保留准确的点对匹配，同时允许对次优匹配进行增量调整，从而提高了点云补全的效果。此外，测量形状差异不仅仅适用于点云补全任务，我们还进一步探索了其在其他生成相关任务中的应用，包括从点云重建单张图像和上采样。我们在点云补全基准数据集PCN、ShapeNet-55和ShapeNet-34上展示了最先进的性能，并通过可视化展示HyperCD可以显著提高表面平滑度。我们还提供了超出补全任务实验的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17951v1">PDF</a> 13 pages, 6 figures</p>
<p><strong>摘要</strong><br>    提出一种针对点云完成任务的新型度量标准——Hyperbolic Chamfer Distance (HyperCD)。该度量在双曲空间中进行Chamfer Distance计算，为匹配点分配更大的权重，有助于保留准确的点匹配并调整次优匹配，从而提高点云完成结果。除了点云完成任务外，还探索了其在其他生成相关任务中的应用，包括点云到单图像的重建和采样等。在PCN、ShapeNet-55和ShapeNet-34等点云完成基准数据集上表现出卓越性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了一种新的点云完成度量标准——Hyperbolic Chamfer Distance (HyperCD)。</li>
<li>HyperCD在双曲空间中进行Chamfer Distance计算，提高了点匹配的准确性并允许调整次优匹配。</li>
<li>HyperCD能显著改善点云完成的表面平滑度。</li>
<li>除了点云完成任务外，HyperCD还应用于其他生成相关任务，如点云到单图像的重建和采样等。</li>
<li>在多个点云完成基准数据集上，HyperCD实现了卓越的性能。</li>
<li>HyperCD不仅有助于解决由于存在离群值而导致的Chamfer Distance的局限性，还为提高点云完成质量提供了新的思路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17951">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cf9d89b978a99da865d9a650f21e2780.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90f1e3834d8c3de7f1276a7f6b0354fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4ae3ead5f39ac11998b18a9e59ec4a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd5d44432f8ce2fabe2eeabafc8712ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63ae53104bc01ec83f0a56dcd88c002.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Parkinson-Disease-Detection-Based-on-In-air-Dynamics-Feature-Extraction-and-Selection-Using-Machine-Learning"><a href="#Parkinson-Disease-Detection-Based-on-In-air-Dynamics-Feature-Extraction-and-Selection-Using-Machine-Learning" class="headerlink" title="Parkinson Disease Detection Based on In-air Dynamics Feature Extraction   and Selection Using Machine Learning"></a>Parkinson Disease Detection Based on In-air Dynamics Feature Extraction   and Selection Using Machine Learning</h2><p><strong>Authors:Jungpil Shin, Abu Saleh Musa Miah, Koki Hirooka, Md. Al Mehedi Hasan, Md. Maniruzzaman</strong></p>
<p>Parkinson’s disease (PD) is a progressive neurological disorder that impairs movement control, leading to symptoms such as tremors, stiffness, and bradykinesia. Many researchers analyzing handwriting data for PD detection typically rely on computing statistical features over the entirety of the handwriting task. While this method can capture broad patterns, it has several limitations, including a lack of focus on dynamic change, oversimplified feature representation, lack of directional information, and missing micro-movements or subtle variations. Consequently, these systems face challenges in achieving good performance accuracy, robustness, and sensitivity. To overcome this problem, we proposed an optimized PD detection methodology that incorporates newly developed dynamic kinematic features and machine learning (ML)-based techniques to capture movement dynamics during handwriting tasks. In the procedure, we first extracted 65 newly developed kinematic features from the first and last 10% phases of the handwriting task rather than using the entire task. Alongside this, we also reused 23 existing kinematic features, resulting in a comprehensive new feature set. Next, we enhanced the kinematic features by applying statistical formulas to compute hierarchical features from the handwriting data. This approach allows us to capture subtle movement variations that distinguish PD patients from healthy controls. To further optimize the feature set, we applied the Sequential Forward Floating Selection method to select the most relevant features, reducing dimensionality and computational complexity. Finally, we employed an ML-based approach based on ensemble voting across top-performing tasks, achieving an impressive 96.99% accuracy on task-wise classification and 99.98% accuracy on task ensembles, surpassing the existing state-of-the-art model by 2% for the PaHaW dataset. </p>
<blockquote>
<p>帕金森病（PD）是一种进行性神经疾病，会损害运动控制，导致震颤、僵硬和行动迟缓等症状。许多研究人员在分析帕金森病的手写数据检测时，通常依赖于对整个手写任务进行统计特征的计算。虽然这种方法可以捕捉大致的模式，但它存在一些局限性，包括缺乏动态变化的关注、特征表示过于简化、缺乏方向信息和缺失的微动作或细微变化。因此，这些系统在实现性能准确性、稳健性和敏感性方面面临挑战。为了解决这个问题，我们提出了一种优化的帕金森病检测方法论，该方法结合了新开发的动态运动特征以及基于机器学习（ML）的技术，以捕捉手写任务过程中的运动动态。在该过程中，我们从手写任务的开始和结束10%的阶段中提取了65个新开发的运动特征，而不是使用整个任务。同时，我们还重用了23个现有的运动特征，形成了一个全面的新特征集。接下来，我们通过应用统计公式来计算手写数据的分层特征来增强运动特征。这种方法允许我们捕捉到细微的运动变化，以区分帕金森病患者和健康对照者。为了进一步优化特征集，我们采用了序贯前向浮动选择方法，以选择最相关的特征，降低维度和计算复杂性。最后，我们基于任务性能最佳的集合投票方法采用了一种基于机器学习的策略，在任务级分类上取得了令人印象深刻的96.99%的准确率，在任务集合上达到了惊人的99.98%的准确率，超过了PaHaW数据集上的现有最先进的模型2%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17849v1">PDF</a> </p>
<p><strong>Summary</strong><br>帕金森病（PD）是一种影响运动控制的神经性疾病。传统的手写数据分析方法存在局限性，如忽视动态变化、特征表示过于简化等。本研究提出了一种优化的PD检测法，结合新的动态运动特征和机器学习技术，以捕捉手写过程中的运动动态。该研究从书写任务的开始和结束阶段提取特征，同时利用统计公式计算层次特征，以区分PD患者和健康人。通过特征选择方法优化特征集，并采用基于集成投票的机器学习法，取得了高准确率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>帕金森病是一种影响运动控制的神经性疾病，具有手抖、僵硬和迟缓等症状。</li>
<li>传统的手写数据分析方法在PD检测中存在局限性，缺乏动态变化关注及细微运动捕捉。</li>
<li>本研究提出了一种结合新的动态运动特征和机器学习技术的优化PD检测方法。</li>
<li>研究从手写任务的特定阶段提取特征，并计算层次特征以区分PD患者和健康人。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17849">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8ab0d6a1dd151c61391d610089d52c1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f794cf7a0b80caeb1ce3daad43b71b4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Potential-of-Convolutional-Neural-Networks-for-Cancer-Detection"><a href="#The-Potential-of-Convolutional-Neural-Networks-for-Cancer-Detection" class="headerlink" title="The Potential of Convolutional Neural Networks for Cancer Detection"></a>The Potential of Convolutional Neural Networks for Cancer Detection</h2><p><strong>Authors:Hossein Molaeian, Kaveh Karamjani, Sina Teimouri, Saeed Roshani, Sobhan Roshani</strong></p>
<p>Early detection of cancer is critical in improving treatment outcomes and increasing survival rates, particularly for common cancers such as lung, breast, and prostate which collectively contribute to a significant global mortality burden. With advancements in imaging technologies and data processing, Convolutional Neural Networks (CNNs) have emerged as a powerful tool for analyzing and classifying medical images, enabling more precise cancer detection. This paper provides a comprehensive review of recent studies leveraging CNN models for detecting ten different types of cancer. Each study employs distinct CNN architectures to identify patterns associated with these cancers, utilizing diverse datasets. Key differences and strengths of these architectures are meticulously compared and analyzed, highlighting their efficacy in improving early detection. Beyond reviewing the performance and limitations of CNN-based cancer detection methods, this study explores the feasibility of integrating CNNs into clinical settings as an early detection tool, potentially complementing or replacing traditional methods. Despite significant progress, challenges remain, including data diversity, result interpretation, and ethical considerations. By identifying the best-performing CNN architectures and providing a comparative analysis, this study aims to contribute a comprehensive perspective on the application of CNNs in cancer detection and their role in advancing diagnostic capabilities in healthcare. </p>
<blockquote>
<p>癌症的早期发现对于提高治疗效果和增加存活率至关重要，特别是对于肺癌、乳腺癌和前列腺癌等常见癌症，它们共同造成了全球大量的死亡负担。随着成像技术和数据处理的发展，卷积神经网络（CNN）已成为分析和分类医学图像的强大工具，能够实现更精确的癌症检测。本文全面回顾了最近利用CNN模型检测十种不同类型癌症的研究。每项研究都采用不同的CNN架构来识别与这些癌症相关的模式，并利用各种数据集。本文精心比较和分析了这些架构的主要差异和优点，突出了它们在提高早期检测率方面的有效性。除了回顾基于CNN的癌症检测方法的性能和局限性外，本研究还探讨了将CNN集成到临床环境中作为早期检测工具的可行性，可能补充或替代传统方法。尽管取得了重大进展，但仍存在挑战，包括数据多样性、结果解释和伦理考量。通过确定表现最佳的CNN架构并提供比较分析，本研究旨在为CNN在癌症检测中的应用及其在提高医疗诊断能力方面的作用提供全面视角。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17155v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文综述了近期利用卷积神经网络（CNN）模型检测十种不同类型癌症的研究。这些研究采用不同的CNN架构识别与癌症相关的模式，并利用多样数据集进行验证。文章比较分析了这些架构的关键差异和优势，强调了它们在提高癌症早期检测方面的效能。此外，文章还探讨了将CNN集成到临床环境中作为早期检测工具的可行性，并指出了当前面临的挑战和未来的研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>早期癌症检测对改善治疗结果和提高存活率至关重要，特别是对于肺癌、乳腺癌和前列腺癌等常见癌症。</li>
<li>卷积神经网络（CNN）在分析和分类医疗图像方面表现出强大的能力，能更精确地检测癌症。</li>
<li>综述了利用CNN模型检测十种不同类型癌症的最近研究，这些研究采用不同的CNN架构并利用多样数据集进行验证。</li>
<li>CNN架构在癌症检测中的效能得到了比较和分析，强调了它们对提高早期检测率的重要性。</li>
<li>探讨了将CNN集成到临床环境中作为癌症早期检测工具的可行性，这可能会补充或替代传统方法。</li>
<li>当前面临的挑战包括数据多样性、结果解读和伦理考虑等因素。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17155">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd988f7d9174b0e9e27be42da427637f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-907795154a6617154b7645e370678f01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-321627207022e1b4202bf80e12f4de79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d09b635973bacdd60f62efa26f2702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4810ee33394b1a8999251a694faf4d9c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MaskCLIP-A-Mask-Based-CLIP-Fine-tuning-Framework-for-Open-Vocabulary-Image-Segmentation"><a href="#MaskCLIP-A-Mask-Based-CLIP-Fine-tuning-Framework-for-Open-Vocabulary-Image-Segmentation" class="headerlink" title="MaskCLIP++: A Mask-Based CLIP Fine-tuning Framework for Open-Vocabulary   Image Segmentation"></a>MaskCLIP++: A Mask-Based CLIP Fine-tuning Framework for Open-Vocabulary   Image Segmentation</h2><p><strong>Authors:Quan-Sheng Zeng, Yunheng Li, Daquan Zhou, Guanbin Li, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>Open-vocabulary image segmentation has been advanced through the synergy between mask generators and vision-language models like Contrastive Language-Image Pre-training (CLIP). Previous approaches focus on generating masks while aligning mask features with text embeddings during training. In this paper, we observe that relying on generated low-quality masks can weaken the alignment of vision and language in regional representations. This motivates us to present a new fine-tuning framework, named MaskCLIP++, which uses ground-truth masks instead of generated masks to enhance the mask classification capability of CLIP. Due to the limited diversity of image segmentation datasets with mask annotations, we propose incorporating a consistency alignment constraint during fine-tuning, which alleviates categorical bias toward the fine-tuning dataset. After low-cost fine-tuning, combining with the mask generator in previous state-of-the-art mask-based open vocabulary segmentation methods, we achieve performance improvements of +1.7, +2.3, +2.1, +3.1, and +0.3 mIoU on the A-847, PC-459, A-150, PC-59, and PAS-20 datasets, respectively. Code is released at <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/MaskCLIPpp">https://github.com/HVision-NKU/MaskCLIPpp</a> . </p>
<blockquote>
<p>基于掩膜生成器和对比语言图像预训练（CLIP）等视觉语言模型之间的协同作用，开放词汇图像分割技术已经得到了发展。早期的方法侧重于生成掩膜，并在训练过程中将掩膜特征与文本嵌入进行对齐。在本文中，我们观察到依赖生成的低质量掩膜会削弱区域表示中的视觉和语言对齐。这促使我们提出一个新的微调框架，名为MaskCLIP++，它使用真实掩膜代替生成的掩膜，以提高CLIP的掩膜分类能力。由于带有掩膜标注的图像分割数据集多样性有限，我们提出了在微调过程中引入一致性对齐约束，这减轻了对微调数据集的类别偏见。经过低成本微调后，与之前最先进的基于掩膜开放词汇分割方法中的掩膜生成器相结合，我们在A-847、PC-459、A-150、PC-59和PAS-20数据集上分别实现了+1.7、+2.3、+2.1、+3.1和+0.3的mIoU性能提升。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/MaskCLIPpp%E4%B8%8A%E3%80%82">https://github.com/HVision-NKU/MaskCLIPpp上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11464v2">PDF</a> 20 pages, 8 figures. Add code link</p>
<p><strong>Summary</strong><br>     本文提出了一种名为MaskCLIP++的新微调框架，使用真实掩膜替代生成的掩膜提升CLIP的掩膜分类能力。为提高图像分割数据集掩膜标注的多样性，提出了一致性对齐约束。结合之前的先进掩膜生成器进行微调后，在多个数据集上的性能有所提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaskCLIP++使用真实掩膜替代生成掩膜，以增强CLIP的掩膜分类能力。</li>
<li>生成的低质量掩膜可能导致视觉和语言区域表示的对齐减弱。</li>
<li>为提高图像分割数据集的多样性，提出了在微调过程中加入一致性对齐约束。</li>
<li>MaskCLIP++通过对Mask generator的融入和微调提升性能。</li>
<li>在不同数据集上进行实验验证，实现了mIoU性能的提升。</li>
<li>项目的代码已经公开发布在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11464">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d183b1f03677ada7567a3e9471c1840d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d945c1fa3f2ce53a5d796eb538c10ebb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20d6f62612372e9ccd67cfdf8959dbec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ce850d0612e0ef2833af25f3f12743.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e459efecb94ed01fc6976d02caba439.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AgriBench-A-Hierarchical-Agriculture-Benchmark-for-Multimodal-Large-Language-Models"><a href="#AgriBench-A-Hierarchical-Agriculture-Benchmark-for-Multimodal-Large-Language-Models" class="headerlink" title="AgriBench: A Hierarchical Agriculture Benchmark for Multimodal Large   Language Models"></a>AgriBench: A Hierarchical Agriculture Benchmark for Multimodal Large   Language Models</h2><p><strong>Authors:Yutong Zhou, Masahiro Ryo</strong></p>
<p>We introduce AgriBench, the first agriculture benchmark designed to evaluate MultiModal Large Language Models (MM-LLMs) for agriculture applications. To further address the agriculture knowledge-based dataset limitation problem, we propose MM-LUCAS, a multimodal agriculture dataset, that includes 1,784 landscape images, segmentation masks, depth maps, and detailed annotations (geographical location, country, date, land cover and land use taxonomic details, quality scores, aesthetic scores, etc), based on the Land Use&#x2F;Cover Area Frame Survey (LUCAS) dataset, which contains comparable statistics on land use and land cover for the European Union (EU) territory. This work presents a groundbreaking perspective in advancing agriculture MM-LLMs and is still in progress, offering valuable insights for future developments and innovations in specific expert knowledge-based MM-LLMs. </p>
<blockquote>
<p>我们介绍了Agribench，这是第一个针对农业应用的多模态大型语言模型（MM-LLM）设计的农业基准测试。为了进一步解决基于农业的数据库限制问题，我们提出了多模态农业数据集MM-LUCAS。该数据集基于土地利用&#x2F;覆盖面积框架调查（LUCAS）数据集，包含了1784张景观图像、分割掩膜、深度图以及详细的注释（包括地理位置、国家、日期、土地覆盖和土地利用分类详情、质量分数、美学分数等）。LUCAS数据集包含欧盟领土内土地利用和土地覆盖的可比统计数据。这项工作为推进农业MM-LLM提供了全新的视角，并且仍在进行中，为未来在特定专业知识为基础的MM-LLM领域的发展和创新提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00465v2">PDF</a> Accepted by CVPPA @ECCV2024. Dataset:   <a target="_blank" rel="noopener" href="https://github.com/Yutong-Zhou-cv/AgriBench">https://github.com/Yutong-Zhou-cv/AgriBench</a></p>
<p><strong>Summary</strong><br>    推出农业基准测试AgriBench，评估多模态大型语言模型在农业应用的表现。为解决农业知识数据集限制问题，基于土地利用&#x2F;覆盖面积框架调查（LUCAS）数据集提出多模态农业数据集MM-LUCAS，包含景观图像、分割掩膜、深度图及详细注解，推动农业MM-LLM的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AgriBench是首个用于评估多模态大型语言模型在农业应用方面的基准测试。</li>
<li>MM-LUCAS是一个多模态农业数据集，基于LUCAS数据集构建，包含丰富的农业相关数据和详细注解。</li>
<li>MM-LUCAS数据集旨在解决农业知识数据集限制的问题。</li>
<li>数据集包含景观图像、分割掩膜、深度图等多媒体数据。</li>
<li>数据集包含地理位置、国家、日期、土地覆盖和利用分类详情、质量评分、美学评分等详细注解。</li>
<li>AgriBench和MM-LUCAS的推出为农业MM-LLM的发展提供了有价值的见解和洞察力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00465">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e69221c6427ec234e47c958fe55ab46d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fc7e42a440b37db97956807ecb41777.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Concept-Complement-Bottleneck-Model-for-Interpretable-Medical-Image-Diagnosis"><a href="#Concept-Complement-Bottleneck-Model-for-Interpretable-Medical-Image-Diagnosis" class="headerlink" title="Concept Complement Bottleneck Model for Interpretable Medical Image   Diagnosis"></a>Concept Complement Bottleneck Model for Interpretable Medical Image   Diagnosis</h2><p><strong>Authors:Hongmei Wang, Junlin Hou, Hao Chen</strong></p>
<p>Models based on human-understandable concepts have received extensive attention to improve model interpretability for trustworthy artificial intelligence in the field of medical image analysis. These methods can provide convincing explanations for model decisions but heavily rely on the detailed annotation of pre-defined concepts. Consequently, they may not be effective in cases where concepts or annotations are incomplete or low-quality. Although some methods automatically discover effective and new visual concepts rather than using pre-defined concepts or could find some human-understandable concepts via large Language models, they are prone to veering away from medical diagnostic evidence and are challenging to understand. In this paper, we propose a concept complement bottleneck model for interpretable medical image diagnosis with the aim of complementing the existing concept set and finding new concepts bridging the gap between explainable models. Specifically, we propose to use concept adapters for specific concepts to mine the concept differences and score concepts in their own attention channels to support almost fairly concept learning. Then, we devise a concept complement strategy to learn new concepts while jointly using known concepts to improve model performance. Comprehensive experiments on medical datasets demonstrate that our model outperforms the state-of-the-art competitors in concept detection and disease diagnosis tasks while providing diverse explanations to ensure model interpretability effectively. </p>
<blockquote>
<p>基于人类可理解概念的模型在医学图像分析领域受到了广泛关注，以提高人工智能模型的解释性，从而实现可信赖的人工智能。这些方法可以为模型决策提供令人信服的解释，但它们严重依赖于预先定义概念的详细注释。因此，在概念或注释不完整或质量低的情况下，它们可能无法有效发挥作用。尽管一些方法能够自动发现有效的新视觉概念，而不是使用预先定义的概念，或者可以通过大型语言模型发现一些人类可理解的概念，但它们容易偏离医学诊断证据，且难以理解。在本文中，我们提出了一种概念补充瓶颈模型，旨在用于解释医学图像诊断，目标是补充现有概念集，并找到填补解释性模型之间差距的新概念。具体来说，我们建议使用特定概念的概念适配器来挖掘概念差异，并在各自的注意力通道中对概念进行评分，以支持几乎公平的概念学习。然后，我们设计了一种概念补充策略，在学习新概念的同时，结合已知概念来提高模型性能。在医学数据集上的综合实验表明，我们的模型在概念检测和疾病诊断任务上优于最新竞争对手，同时提供多样化的解释，以确保模型的有效解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15446v2">PDF</a> 27 pages, 5 figures,</p>
<p><strong>Summary</strong><br>     医学图像分析领域，基于人类可理解概念的模型受到广泛关注以提高模型的可解释性。该方法可为模型决策提供有力解释，但依赖于预定义概念的详细标注，若概念或标注不完整或质量低下则效果不佳。本文提出概念补充瓶颈模型，旨在补充现有概念集并发现新概念以缩小解释性模型的差距。通过概念适配器挖掘概念差异，并在各自注意力通道评分新概念以支持几乎公平的概念学习。实验证明该模型在医学数据集上超越最新竞争对手，在概念检测和疾病诊断任务上表现优异，同时提供多样的解释以确保模型的有效可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<pre><code>1. 基于人类可理解概念的模型在医学图像分析领域备受关注，以提高模型的可解释性。
2. 这些方法需要预定义概念的详细标注，若标注不完整或质量低下可能会影响效果。
3. 本文提出了一个新的概念补充瓶颈模型，旨在补充现有概念集并发现新概念。
4. 使用概念适配器挖掘概念差异，并在各自的注意力通道评分新概念。
5. 提出了一种概念补充策略，以学习新概念的同时利用已知概念来提高模型性能。
6. 在医学数据集上的实验证明该模型在概念检测和疾病诊断任务上表现优异。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15446">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-abeca2c0d429fb0a7ead80914c9519a2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models"><a href="#Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models" class="headerlink" title="Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models"></a>Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models</h2><p><strong>Authors:Pujing Yang, Guangyi Zhang, Yunlong Cai</strong></p>
<p>Recent advances in deep learning-based joint source-channel coding (DJSCC) have shown promise for end-to-end semantic image transmission. However, most existing schemes primarily focus on optimizing pixel-wise metrics, which often fail to align with human perception, leading to lower perceptual quality. In this letter, we propose a novel generative DJSCC approach using conditional diffusion models to enhance the perceptual quality of transmitted images. Specifically, by utilizing entropy models, we effectively manage transmission bandwidth based on the estimated entropy of transmitted sym-bols. These symbols are then used at the receiver as conditional information to guide a conditional diffusion decoder in image reconstruction. Our model is built upon the emerging advanced mamba-like linear attention (MLLA) skeleton, which excels in image processing tasks while also offering fast inference speed. Besides, we introduce a multi-stage training strategy to ensure the stability and improve the overall performance of the model. Simulation results demonstrate that our proposed method significantly outperforms existing approaches in terms of perceptual quality. </p>
<blockquote>
<p>近年来，基于深度学习的联合源信道编码（DJSCC）的最新进展为端到端的语义图像传输展现了前景。然而，大多数现有方案主要关注优化像素级的指标，这些指标通常与人类感知不匹配，导致感知质量较低。在这篇文章中，我们提出了一种利用条件扩散模型的新型生成式DJSCC方法，以提高传输图像的感知质量。具体来说，我们利用熵模型，根据传输符号的估计熵有效地管理传输带宽。这些符号然后在接收器端作为条件信息，用于指导图像重建中的条件扩散解码器。我们的模型建立在新兴的马姆巴式线性注意力（MLLA）骨架之上，该骨架在图像处理任务中表现出色，同时提供了快速推理速度。此外，我们引入了一种多阶段训练策略，以确保模型的稳定性并提高其整体性能。仿真结果表明，我们提出的方法在感知质量方面显著优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02597v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于深度学习的联合源信道编码（DJSCC）最新进展为端到端的语义图像传输展现了潜力。然而，大多数现有方案主要关注像素级的优化指标，这些指标往往不符合人类感知，导致感知质量较低。本研究提出了一种基于条件扩散模型的新型生成式DJSCC方法，以提高传输图像的感知质量。具体来说，我们利用��on模型有效管理传输带宽，根据传输符号的估计熵进行传输。这些符号随后被用作接收器的条件信息，以指导图像重建中的条件扩散解码器。我们的模型建立在新兴的马姆巴式线性注意力（MLLA）骨架之上，该骨架在图像处理任务上表现出色，同时提供快速推理速度。此外，我们引入了一种多阶段训练策略，以确保模型的稳定性并提高其整体性能。仿真结果表明，我们提出的方法在感知质量方面显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习的联合源信道编码（DJSCC）在语义图像传输中具有潜力。</li>
<li>现有方案主要关注像素级优化指标，这往往不符合人类感知。</li>
<li>提出一种新型生成式DJSCC方法，基于条件扩散模型提高传输图像的感知质量。</li>
<li>利用熵模型有效管理传输带宽，根据传输符号的估计熵进行数据传输。</li>
<li>模型采用马姆巴式线性注意力（MLLA）骨架，适用于图像处理并具备快速推理能力。</li>
<li>引入多阶段训练策略，提高模型稳定性和整体性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eff982c45e695cae805b96e6113523d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b91ed06a7a95f47ccfceb2e9725c4363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aabf79dae9b7fc082b1b85d8c6c06d9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c893e4c70b00b37fcfbb0f773226c9c7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ProCNS-Progressive-Prototype-Calibration-and-Noise-Suppression-for-Weakly-Supervised-Medical-Image-Segmentation"><a href="#ProCNS-Progressive-Prototype-Calibration-and-Noise-Suppression-for-Weakly-Supervised-Medical-Image-Segmentation" class="headerlink" title="ProCNS: Progressive Prototype Calibration and Noise Suppression for   Weakly-Supervised Medical Image Segmentation"></a>ProCNS: Progressive Prototype Calibration and Noise Suppression for   Weakly-Supervised Medical Image Segmentation</h2><p><strong>Authors:Y. Liu, L. Lin, K. K. Y. Wong, X. Tang</strong></p>
<p>Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest with more reliable guidance. The affinities are derived from the input images and the prototype-refined predictions. Meanwhile, we propose an Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and representative prototype representations, which adaptively identifies and masks noisy regions within the pseudo proposals, reducing potential erroneous interference during prototype computation. Furthermore, we generate specialized soft pseudo-labels for the noisy regions identified by ANPM, providing supplementary supervision. Extensive experiments on six medical image segmentation tasks involving different modalities demonstrate that the proposed framework significantly outperforms representative state-of-the-art methods. </p>
<blockquote>
<p>弱监督分割（WSS）作为一种解决方案，通过采用稀疏注释格式（如点、涂鸦、块等）来缓解注释成本与模型性能之间的冲突。典型的方法试图利用解剖学和拓扑学先验知识来直接将稀疏注释扩展为伪标签。然而，由于医学图像中模糊边缘的忽视以及稀疏监督的不足，现有方法往往会在噪声区域生成错误且过于自信的伪提案，从而导致模型的累积误差和性能下降。在这项工作中，我们提出了一种新型的WSS方法，名为ProCNS，包含两个协同工作的模块，以渐进原型校准和噪声抑制的原则设计。具体来说，我们设计了一种基于原型的区域空间亲和力（PRSA）损失，以最大化空间元素和语义元素之间的成对亲和力，为我们的目标模型提供更可靠的指导。亲和力来源于输入图像和经过原型优化的预测结果。同时，我们提出了自适应噪声感知和掩蔽（ANPM）模块，以获得更丰富和具有代表性的原型表示，该模块能够自适应地识别和掩蔽伪提案中的噪声区域，减少在原型计算过程中潜在的错误干扰。此外，我们为ANPM识别的噪声区域生成专门的软伪标签，以提供额外的监督。在六个涉及不同模态的医疗图像分割任务上的大量实验表明，所提出的框架显著优于具有代表性的最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.14074v3">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像分割中的弱监督学习（WSS）方法存在误差累积和性能下降的问题。为解决此问题，本文提出了一种名为ProCNS的新型WSS方法，包含两个协同模块，基于渐进原型校准和噪声抑制原则。设计了一种基于区域空间亲和力的损失函数PRSA，提高模型可靠性。并提出自适应噪声感知和掩蔽模块ANPM，用于更丰富和代表性的原型表示，减少伪提案中的噪声干扰。在六种医学图像分割任务上的实验表明，该方法显著优于现有先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>弱监督分割（WSS）是解决标注成本与模型性能冲突的一种解决方案，采用稀疏标注格式（如点、涂鸦、块等）。</li>
<li>现有方法倾向于在噪声区域产生错误和过于自信的伪提案，导致模型误差累积和性能下降。</li>
<li>本文提出了一种新型WSS方法ProCNS，包含两个协同模块：基于渐进原型校准和噪声抑制。</li>
<li>设计了PRSA损失函数，通过最大化空间元素和语义元素之间的配对亲和力，为模型提供更可靠的指导。</li>
<li>ANPM模块能够自适应地识别和掩盖伪提案中的噪声区域，减少潜在错误干扰。</li>
<li>ProCNS生成针对ANPM识别的噪声区域的特殊软伪标签，提供额外的监督。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.14074">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c2c4e77143b27993cb359d4c28000b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eae7e98dd5c16ba0fab63e32d5cea0e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aad1b483ecf55a9d4c8108bcfe7d3a70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e59fe3bab2fc3479acb165072fb224.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f8161d39818c9af9cce74b56c1b23e15.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2024-12-26  Portability of Fortran's `do concurrent' on GPUs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-1ead218b7a67780339b0cc9755cc860e.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-26  PartGen Part-level 3D Generation and Reconstruction with Multi-View   Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">10254.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
