<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  Vision-Language Models for Automated Chest X-ray Interpretation   Leveraging ViT and GPT-2">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-c5d4c0037d25961b8dab93d73f22f7c1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-23-æ›´æ–°"><a href="#2025-01-23-æ›´æ–°" class="headerlink" title="2025-01-23 æ›´æ–°"></a>2025-01-23 æ›´æ–°</h1><h2 id="Vision-Language-Models-for-Automated-Chest-X-ray-Interpretation-Leveraging-ViT-and-GPT-2"><a href="#Vision-Language-Models-for-Automated-Chest-X-ray-Interpretation-Leveraging-ViT-and-GPT-2" class="headerlink" title="Vision-Language Models for Automated Chest X-ray Interpretation:   Leveraging ViT and GPT-2"></a>Vision-Language Models for Automated Chest X-ray Interpretation:   Leveraging ViT and GPT-2</h2><p><strong>Authors:Md. Rakibul Islam, Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu</strong></p>
<p>Radiology plays a pivotal role in modern medicine due to its non-invasive diagnostic capabilities. However, the manual generation of unstructured medical reports is time consuming and prone to errors. It creates a significant bottleneck in clinical workflows. Despite advancements in AI-generated radiology reports, challenges remain in achieving detailed and accurate report generation. In this study we have evaluated different combinations of multimodal models that integrate Computer Vision and Natural Language Processing to generate comprehensive radiology reports. We employed a pretrained Vision Transformer (ViT-B16) and a SWIN Transformer as the image encoders. The BART and GPT-2 models serve as the textual decoders. We used Chest X-ray images and reports from the IU-Xray dataset to evaluate the usability of the SWIN Transformer-BART, SWIN Transformer-GPT-2, ViT-B16-BART and ViT-B16-GPT-2 models for report generation. We aimed at finding the best combination among the models. The SWIN-BART model performs as the best-performing model among the four models achieving remarkable results in almost all the evaluation metrics like ROUGE, BLEU and BERTScore. </p>
<blockquote>
<p>æ”¾å°„å­¦åœ¨ç°ä»£åŒ»å­¦ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå› å…¶å…·æœ‰æ— åˆ›è¯Šæ–­èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨ç”Ÿæˆéç»“æ„åŒ–åŒ»ç–—æŠ¥å‘Šæ—¢è€—æ—¶åˆå®¹æ˜“å‡ºé”™ï¼Œè¿™æˆä¸ºäº†ä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„é‡å¤§ç“¶é¢ˆã€‚å°½ç®¡äººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šæœ‰æ‰€å‘å±•ï¼Œä½†åœ¨å®ç°è¯¦ç»†å’Œå‡†ç¡®çš„æŠ¥å‘Šç”Ÿæˆæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç»“åˆè®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„å¤šæ¨¡æ€æ¨¡å‹çš„ä¸åŒç»„åˆï¼Œä»¥ç”Ÿæˆå…¨é¢çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚æˆ‘ä»¬é‡‡ç”¨äº†é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViT-B16ï¼‰å’ŒSWIN Transformerä½œä¸ºå›¾åƒç¼–ç å™¨ã€‚BARTå’ŒGPT-2æ¨¡å‹åˆ™ä½œä¸ºæ–‡æœ¬è§£ç å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨IU-Xrayæ•°æ®é›†ä¸­çš„èƒ¸éƒ¨Xå°„çº¿å›¾åƒå’ŒæŠ¥å‘Šæ¥è¯„ä¼°SWIN Transformer-BARTã€SWIN Transformer-GPT-2ã€ViT-B16-BARTå’ŒViT-B16-GPT-2æ¨¡å‹åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°æ¨¡å‹ä¹‹é—´çš„æœ€ä½³ç»„åˆã€‚åœ¨å››é¡¹æ¨¡å‹ä¸­ï¼ŒSWIN-BARTæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œåœ¨å‡ ä¹æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚ROUGEã€BLEUå’ŒBERTScoreï¼‰ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12356v1">PDF</a> Preprint, manuscript under-review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆç»¼åˆåŒ»å­¦å½±åƒæŠ¥å‘Šçš„æ–¹æ³•ã€‚é€šè¿‡å¯¹æ¯”ä½¿ç”¨ä¸åŒæ¨¡å‹çš„ç»„åˆï¼Œå‘ç°SWIN Transformerä¸BARTæ¨¡å‹ç»“åˆè¡¨ç°æœ€ä½³ï¼Œåœ¨å¤šç§è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å½±åƒåœ¨ç°ä»£åŒ»å­¦ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†æ‰‹åŠ¨ç”Ÿæˆéç»“æ„åŒ–åŒ»å­¦æŠ¥å‘Šè€—æ—¶ä¸”æ˜“å‡ºé”™ï¼Œæˆä¸ºä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„ç“¶é¢ˆã€‚</li>
<li>AIåœ¨ç”ŸæˆåŒ»å­¦å½±åƒæŠ¥å‘Šæ–¹é¢è™½æœ‰è¿›å±•ï¼Œä½†ä»é¢ä¸´è¯¦ç»†å‡†ç¡®æŠ¥å‘Šç”Ÿæˆæ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶è€…è¯„ä¼°äº†ä¸åŒå¤šæ¨¡æ€æ¨¡å‹ç»„åˆç”ŸæˆåŒ»å­¦å½±åƒæŠ¥å‘Šçš„æ•ˆæœã€‚</li>
<li>ä½¿ç”¨çš„æ¨¡å‹åŒ…æ‹¬é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViT-B16ï¼‰å’ŒSWIN Transformerä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œä»¥åŠBARTå’ŒGPT-2ä½œä¸ºæ–‡æœ¬è§£ç å™¨ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†IU-Xrayæ•°æ®é›†ä¸­çš„Chest X-rayå›¾åƒå’ŒæŠ¥å‘Šæ¥è¯„ä¼°å„æ¨¡å‹çš„ä½¿ç”¨æ•ˆæœã€‚</li>
<li>SWIN Transformerä¸BARTæ¨¡å‹ç»“åˆè¡¨ç°æœ€ä½³ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f8df32d8e52eaed8bdb31c790e1b843.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccdb0faaab13d2d26d4c143ce66449a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification"><a href="#CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification" class="headerlink" title="CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification"></a>CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification</h2><p><strong>Authors:Cristiano PatrÃ­cio, Isabel Rio-Torto, Jaime S. Cardoso, LuÃ­s F. Teixeira, JoÃ£o C. Neves</strong></p>
<p>The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the final disease prediction on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: <a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/">https://cristianopatricio.github.io/CBVLM/</a>. </p>
<blockquote>
<p>åœ¨åŒ»ç–—å·¥ä½œæµä¸­é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆçš„ä¸»è¦æŒ‘æˆ˜åœ¨äºæ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§çš„ç¼ºä¹ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMï¼‰é€šè¿‡çº¦æŸæœ€ç»ˆç–¾ç—…é¢„æµ‹åœ¨ä¸€ç»„é¢„å®šä¹‰å’Œäººç±»å¯è§£é‡Šçš„æ¦‚å¿µä¸Šè§£å†³åè€…é—®é¢˜ã€‚ç„¶è€Œï¼Œé€šè¿‡åŸºäºæ¦‚å¿µçš„è§£é‡Šå®ç°çš„å¢åŠ çš„è§£é‡Šæ€§æ„å‘³ç€æ›´é«˜çš„æ ‡æ³¨è´Ÿæ‹…ã€‚è€Œä¸”ï¼Œå¦‚æœéœ€è¦æ·»åŠ æ–°æ¦‚å¿µï¼Œæ•´ä¸ªç³»ç»Ÿéœ€è¦é‡æ–°è®­ç»ƒã€‚å—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸­çš„å“è¶Šæ€§èƒ½çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå³CBVLMï¼Œå®ƒè§£å†³äº†ä¸Šè¿°ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå¯¹äºæ¯ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬æç¤ºLVLMå›ç­”æ¦‚å¿µæ˜¯å¦å‡ºç°åœ¨è¾“å…¥å›¾åƒä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬è®©LVLMåŸºäºå…ˆå‰çš„æ¦‚å¿µé¢„æµ‹å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚è€Œä¸”ï¼Œåœ¨ä¸¤ä¸ªé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†è´Ÿè´£é€‰æ‹©æœ€ä½³ä¸Šä¸‹æ–‡å­¦ä¹ ä¾‹å­çš„æ£€ç´¢æ¨¡å—ã€‚é€šè¿‡å°†æœ€ç»ˆè¯Šæ–­å»ºç«‹åœ¨é¢„æµ‹çš„æ¦‚å¿µä¸Šï¼Œæˆ‘ä»¬ç¡®ä¿äº†å¯è§£é‡Šæ€§ï¼Œå¹¶é€šè¿‡åˆ©ç”¨LVLMçš„å°‘é‡æ ·æœ¬èƒ½åŠ›ï¼Œæˆ‘ä»¬å¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚æˆ‘ä»¬é€šè¿‡å››ä¸ªåŒ»ç–—æ•°æ®é›†å’ŒåäºŒä¸ªï¼ˆé€šç”¨å’ŒåŒ»ç–—ï¼‰LVLMçš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¡¨æ˜CBVLMå§‹ç»ˆä¼˜äºCBMå’Œç‰¹å®šä»»åŠ¡ç›‘ç£æ–¹æ³•ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒï¼Œåªéœ€ä½¿ç”¨å°‘é‡æ ‡æ³¨ä¾‹å­ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/%E3%80%82">https://cristianopatricio.github.io/CBVLM/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12266v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCBVLMçš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å·¥ä½œæµä¸­çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šæ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§çš„ç¼ºä¹ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼ŒCBVLMä¸ä»…æé«˜äº†ç³»ç»Ÿçš„è§£é‡Šæ€§ï¼Œè€Œä¸”é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚åœ¨å››ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCBVLMåœ¨ä¸éœ€è¦ä»»ä½•è®­ç»ƒå’Œåªéœ€å°‘é‡æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰å’Œä»»åŠ¡ç‰¹å®šçš„ç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CBVLMæ–¹æ³•è§£å†³äº†æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦é¢†åŸŸé¢ä¸´çš„æ ‡æ³¨æ•°æ®å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§æŒ‘æˆ˜ã€‚</li>
<li>CBVLMåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>CBVLMé€šè¿‡æ¦‚å¿µé¢„æµ‹å’Œå›¾åƒåˆ†ç±»ï¼Œæé«˜äº†ç³»ç»Ÿçš„è§£é‡Šæ€§ã€‚</li>
<li>CBVLMé€šè¿‡æ£€ç´¢æ¨¡å—é€‰æ‹©æœ€ä½³ä¸Šä¸‹æ–‡å­¦ä¹ æ ·æœ¬ã€‚</li>
<li>CBVLMæ–¹æ³•åœ¨å››ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰å’Œä»»åŠ¡ç‰¹å®šçš„ç›‘ç£æ–¹æ³•ã€‚</li>
<li>CBVLMæ–¹æ³•ä¸éœ€è¦ä»»ä½•è®­ç»ƒï¼Œä¸”ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å³å¯å®ç°é«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-206021a739805f4c025fbbbda977fff6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0afff72f19698ec85886bc603208c360.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0abc5ca96e375c7cac4c0ac188efd080.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0aea3215822b78c86eba3f65abedf78.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Early-Detection-and-Classification-of-Breast-Cancer-Using-Deep-Learning-Techniques"><a href="#Early-Detection-and-Classification-of-Breast-Cancer-Using-Deep-Learning-Techniques" class="headerlink" title="Early Detection and Classification of Breast Cancer Using Deep Learning   Techniques"></a>Early Detection and Classification of Breast Cancer Using Deep Learning   Techniques</h2><p><strong>Authors:Mst. Mumtahina Labonno, D. M. Asadujjaman, Md. Mahfujur Rahman, Abdullah Tamim, Mst. Jannatul Ferdous, Rafi Muttaki Mahi</strong></p>
<p>Breast cancer is one of the deadliest cancers causing about massive number of patients to die annually all over the world according to the WHO. It is a kind of cancer that develops when the tissues of the breast grow rapidly and unboundly. This fatality rate can be prevented if the cancer is detected before it gets malignant. Using automation for early-age detection of breast cancer, Artificial Intelligence and Machine Learning technologies can be implemented for the best outcome. In this study, we are using the Breast Cancer Image Classification dataset collected from the Kaggle depository, which comprises 9248 Breast Ultrasound Images and is classified into three categories: Benign, Malignant, and Normal which refers to non-cancerous, cancerous, and normal images.This research introduces three pretrained model featuring custom classifiers that includes ResNet50, MobileNet, and VGG16, along with a custom CNN model utilizing the ReLU activation function.The models ResNet50, MobileNet, VGG16, and a custom CNN recorded accuracies of 98.41%, 97.91%, 98.19%, and 92.94% on the dataset, correspondingly, with ResNet50 achieving the highest accuracy of 98.41%.This model, with its deep and powerful architecture, is particularly successful in detecting aberrant cells as well as cancerous or non-cancerous tumors. These accuracies show that the Machine Learning methods are more compatible for the classification and early detection of breast cancer. </p>
<blockquote>
<p>ä¹³è…ºç™Œæ˜¯è‡´æ­»ç‡è¾ƒé«˜çš„ç™Œç—‡ä¹‹ä¸€ï¼Œä¸–ç•Œå«ç”Ÿç»„ç»‡æŠ¥å‘Šç§°å…¶æ¯å¹´å¯¼è‡´å¤§é‡æ‚£è€…æ­»äº¡ã€‚å®ƒæ˜¯ä¸€ç§ä¹³æˆ¿ç»„ç»‡å¿«é€Ÿä¸”æ— é™åˆ¶å¢é•¿å½¢æˆçš„ç™Œç—‡ã€‚å¦‚æœåœ¨ç™Œç—‡æ¶åŒ–ä¹‹å‰æ£€æµ‹åˆ°ï¼Œå¯ä»¥é¿å…è¿™ç§é«˜æ­»äº¡ç‡ã€‚ä½¿ç”¨è‡ªåŠ¨åŒ–æŠ€æœ¯è¿›è¡Œä¹³è…ºç™Œçš„æ—©æœŸæ£€æµ‹ï¼Œåº”ç”¨äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯å¯ä»¥è·å¾—æœ€ä½³æ•ˆæœã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»Kaggleå­˜å‚¨åº“ä¸­æ”¶é›†çš„ä¹³è…ºç™Œå›¾åƒåˆ†ç±»æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«9248å¼ ä¹³è…ºè¶…å£°å›¾åƒï¼Œåˆ†ä¸ºä¸‰ç±»ï¼šè‰¯æ€§ã€æ¶æ€§å’Œæ­£å¸¸ï¼ˆæŒ‡éç™Œæ€§ã€ç™Œæ€§å’Œæ­£å¸¸å›¾åƒï¼‰ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸‰ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬å¸¦æœ‰è‡ªå®šä¹‰åˆ†ç±»å™¨çš„ResNet50ã€MobileNetå’ŒVGG16ï¼Œä»¥åŠä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„è‡ªå®šä¹‰CNNæ¨¡å‹ã€‚ResNet50ã€MobileNetã€VGG16å’Œè‡ªå®šä¹‰CNNåœ¨æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º98.41%ã€97.91%ã€98.19%å’Œ92.94%ï¼Œå…¶ä¸­ResNet50çš„å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°98.41%ã€‚è¯¥æ¨¡å‹å…·æœ‰æ·±åº¦å’Œå¼ºå¤§çš„æ¶æ„ï¼Œç‰¹åˆ«æ“…é•¿æ£€æµ‹å¼‚å¸¸ç»†èƒä»¥åŠç™Œæ€§æˆ–éç™Œæ€§è‚¿ç˜¤ã€‚è¿™äº›å‡†ç¡®ç‡è¡¨æ˜ï¼Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨ä¹³è…ºç™Œåˆ†ç±»å’Œæ—©æœŸæ£€æµ‹æ–¹é¢æ›´å…·å…¼å®¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12217v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¹³è…ºç™Œæ˜¯ä¸€ç§è‡´æ­»ç‡è¾ƒé«˜çš„ç™Œç—‡ï¼Œä¸–ç•Œå«ç”Ÿç»„ç»‡æ•°æ®æ˜¾ç¤ºå…¶æ¯å¹´å¯¼è‡´å¤§é‡æ‚£è€…æ­»äº¡ã€‚ä¸ºå°½æ—©å‘ç°ä¹³è…ºç™Œï¼Œå¯ä½¿ç”¨è‡ªåŠ¨åŒ–æ£€æµ‹æ‰‹æ®µï¼Œå€ŸåŠ©äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚æœ¬ç ”ç©¶é‡‡ç”¨ä¹³è…ºç™Œå›¾åƒåˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…å«è‰¯æ€§ã€æ¶æ€§åŠæ­£å¸¸ä¸‰ç±»ä¹³è…ºè¶…å£°å›¾åƒå…±9248å¼ ï¼Œé€šè¿‡è¿ç”¨ResNet50ã€MobileNetã€VGG16ä¸‰ç§é¢„è®­ç»ƒæ¨¡å‹å’Œè‡ªå®šä¹‰åˆ†ç±»å™¨ï¼Œä»¥åŠä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„è‡ªå®šä¹‰CNNæ¨¡å‹è¿›è¡Œåˆ†ç±»ï¼Œå‡†ç¡®ç‡åˆ†åˆ«ä¸º98.41%ã€97.91%ã€98.19%å’Œ92.94%ï¼Œå…¶ä¸­ResNet50æ¨¡å‹å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°98.41%ï¼Œå¯æˆåŠŸæ£€æµ‹å¼‚å¸¸ç»†èƒåŠç™Œæ€§æˆ–éç™Œæ€§è‚¿ç˜¤ã€‚è¿™äº›å‡†ç¡®ç‡è¡¨æ˜æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨ä¹³è…ºç™Œåˆ†ç±»å’Œæ—©æœŸæ£€æµ‹æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. ä¹³è…ºç™Œæ˜¯ä¸€ç§è‡´æ­»ç‡è¾ƒé«˜çš„ç™Œç—‡ï¼Œæ¯å¹´å¯¼è‡´å¤§é‡æ‚£è€…æ­»äº¡ã€‚
 2. äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯å¯ç”¨äºæ—©æœŸå‘ç°ä¹³è…ºç™Œï¼Œæœ‰åŠ©äºæé«˜æ²»æ„ˆç‡ã€‚
 3. æœ¬ç ”ç©¶é‡‡ç”¨äº†åŒ…å«è‰¯æ€§ã€æ¶æ€§åŠæ­£å¸¸ä¹³è…ºè¶…å£°å›¾åƒçš„æ•°æ®é›†è¿›è¡Œåˆ†ç±»ç ”ç©¶ã€‚
 4. ResNet50ã€MobileNetã€VGG16é¢„è®­ç»ƒæ¨¡å‹å’Œè‡ªå®šä¹‰CNNæ¨¡å‹ç”¨äºåˆ†ç±»ç ”ç©¶ã€‚
 5. ResNet50æ¨¡å‹åœ¨ä¹³è…ºç™Œå›¾åƒåˆ†ç±»ä¸­å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°98.41%ã€‚
 6. æœºå™¨å­¦ä¹ æ–¹æ³•åœ¨ä¹³è…ºç™Œåˆ†ç±»å’Œæ—©æœŸæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dac555067b25fd89fc43f8e65db696f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed042163e7d76ba1ebeffb1f84481aac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5377ddc70bf92b40c4bf7f3622151fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e00c8dd4a9b8f4b443f7ca92be6f9b63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-552659896b8ec350de7e36a4caa32dec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27b102b6d1b9d594b6422cf63702f439.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8040176c6e2d3d1a08c3aa97e3cedd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4caebc13e5feaaa9d2ac16c29686597f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f9f7f491575b9d64220f502c38351ab.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Fast-RF-Shimming-Accelerate-RF-Shimming-in-7T-MRI-using-Deep-Learning"><a href="#Fast-RF-Shimming-Accelerate-RF-Shimming-in-7T-MRI-using-Deep-Learning" class="headerlink" title="Fast-RF-Shimming: Accelerate RF Shimming in 7T MRI using Deep Learning"></a>Fast-RF-Shimming: Accelerate RF Shimming in 7T MRI using Deep Learning</h2><p><strong>Authors:Zhengyi Lu, Hao Liang, Ming Lu, Xiao Wang, Xinqiang Yan, Yuankai Huo</strong></p>
<p>Ultrahigh field (UHF) Magnetic Resonance Imaging (MRI) provides a high signal-to-noise ratio (SNR), enabling exceptional spatial resolution for clinical diagnostics and research. However, higher fields introduce challenges such as transmit radiofrequency (RF) field inhomogeneities, which result in uneven flip angles and image intensity artifacts. These artifacts degrade image quality and limit clinical adoption. Traditional RF shimming methods, including Magnitude Least Squares (MLS) optimization, mitigate RF field inhomogeneity but are time-intensive and often require the presence of the patient. Recent machine learning methods, such as RF Shim Prediction by Iteratively Projected Ridge Regression and other deep learning architectures, offer alternative approaches but face challenges such as extensive training requirements, limited complexity, and practical data constraints. This paper introduces a holistic learning-based framework called Fast RF Shimming, which achieves a 5000-fold speedup compared to MLS methods. First, random-initialized Adaptive Moment Estimation (Adam) derives reference shimming weights from multichannel RF fields. Next, a Residual Network (ResNet) maps RF fields to shimming outputs while incorporating a confidence parameter into the loss function. Finally, a Non-uniformity Field Detector (NFD) identifies extreme non-uniform outcomes. Comparative evaluations demonstrate significant improvements in both speed and predictive accuracy. The proposed pipeline also supports potential extensions, such as the integration of anatomical priors or multi-echo data, to enhance the robustness of RF field correction. This approach offers a faster and more efficient solution to RF shimming challenges in UHF MRI. </p>
<blockquote>
<p>è¶…é«˜åœºï¼ˆUHFï¼‰ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å…·æœ‰é«˜ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ï¼Œä¸ºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—æä¾›äº†å“è¶Šçš„ç©ºé—´åˆ†è¾¨ç‡ã€‚ç„¶è€Œï¼Œæ›´é«˜çš„ç£åœºå¼•å…¥äº†ä¼ è¾“å°„é¢‘ï¼ˆRFï¼‰åœºä¸å‡åŒ€æ€§çš„æŒ‘æˆ˜ï¼Œè¿™ä¼šå¯¼è‡´ç¿»è½¬è§’åº¦ä¸å‡åŒ€å’Œå›¾åƒå¼ºåº¦ä¼ªå½±ã€‚è¿™äº›ä¼ªå½±ä¼šé™ä½å›¾åƒè´¨é‡å¹¶é™åˆ¶å…¶ä¸´åºŠé‡‡ç”¨ã€‚ä¼ ç»Ÿçš„å°„é¢‘è¡¥å¿æ–¹æ³•ï¼ŒåŒ…æ‹¬å¹…åº¦æœ€å°äºŒä¹˜æ³•ï¼ˆMLSï¼‰ä¼˜åŒ–ï¼Œå¯ä»¥ç¼“è§£å°„é¢‘åœºçš„ä¸å‡åŒ€æ€§ï¼Œä½†è€—æ—¶ä¸”éœ€è¦æ‚£è€…å‚ä¸ã€‚æœ€è¿‘çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¦‚é€šè¿‡è¿­ä»£æŠ•å½±å²­å›å½’è¿›è¡Œå°„é¢‘è¡¥å¿é¢„æµ‹å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œæä¾›äº†æ›¿ä»£æ–¹æ³•ï¼Œä½†é¢ä¸´è®­ç»ƒéœ€æ±‚å¤§ã€å¤æ‚æ€§æœ‰é™å’Œå®é™…æ•°æ®çº¦æŸç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå­¦ä¹ çš„å…¨é¢æ¡†æ¶ï¼Œç§°ä¸ºå¿«é€Ÿå°„é¢‘è¡¥å¿ï¼ˆFast RF Shimmingï¼‰ï¼Œä¸MLSæ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†5000å€çš„é€Ÿåº¦æå‡ã€‚é¦–å…ˆï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„è‡ªé€‚åº”çŸ©ä¼°è®¡ï¼ˆAdamï¼‰ä»å¤šé€šé“å°„é¢‘åœºå¯¼å‡ºå‚è€ƒè¡¥å¿æƒé‡ã€‚æ¥ä¸‹æ¥ï¼Œæ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰å°†å°„é¢‘åœºæ˜ å°„åˆ°è¡¥å¿è¾“å‡ºï¼ŒåŒæ—¶å°†ç½®ä¿¡å‚æ•°çº³å…¥æŸå¤±å‡½æ•°ã€‚æœ€åï¼Œéå‡åŒ€åœºæ£€æµ‹å™¨ï¼ˆNFDï¼‰è¯†åˆ«å‡ºæç«¯çš„éå‡åŒ€ç»“æœã€‚æ¯”è¾ƒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨é€Ÿåº¦å’Œé¢„æµ‹ç²¾åº¦æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚è¯¥ç®¡é“è¿˜æ”¯æŒæ½œåœ¨çš„æ‰©å±•ï¼Œå¦‚æ•´åˆå…ˆéªŒè§£å‰–ä¿¡æ¯æˆ–å¤šå›å£°æ•°æ®ï¼Œä»¥å¢å¼ºå°„é¢‘åœºæ ¡æ­£çš„ç¨³å¥æ€§ã€‚è¯¥æ–¹æ³•ä¸ºè§£å†³è¶…é«˜åœºMRIä¸­å°„é¢‘è¡¥å¿çš„æŒ‘æˆ˜æä¾›äº†æ›´å¿«ã€æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¶…é«˜é¢‘ç£å…±æŒ¯æˆåƒï¼ˆUHF MRIï¼‰å…·æœ‰é«˜çš„ä¿¡å™ªæ¯”ï¼Œä¸ºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—æä¾›äº†å“è¶Šçš„ç©ºé—´åˆ†è¾¨ç‡ã€‚ç„¶è€Œï¼Œé«˜åœºå¼•å…¥çš„å°„é¢‘åœºä¸å‡åŒ€æ€§ç­‰é—®é¢˜ä¼šå¯¼è‡´å›¾åƒè´¨é‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„å¿«é€Ÿå°„é¢‘åŒ€åœºæ¡†æ¶ï¼Œå®ç°äº†ä¸ä¼ ç»ŸMLSæ–¹æ³•ç›¸æ¯”é«˜è¾¾5000å€çš„é€Ÿåº¦æå‡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Adamç®—æ³•æ¨å¯¼å‚è€ƒåŒ€åœºæƒé‡ï¼Œç»“åˆResNetè¿›è¡Œå°„é¢‘åœºåˆ°åŒ€åœºè¾“å‡ºçš„æ˜ å°„ï¼Œå¹¶å¼•å…¥ç½®ä¿¡å‚æ•°åˆ°æŸå¤±å‡½æ•°ä¸­ã€‚åŒæ—¶ï¼Œé‡‡ç”¨éå‡åŒ€åœºæ£€æµ‹å™¨è¯†åˆ«æç«¯éå‡åŒ€ç»“æœã€‚è¯¥æ–¹æ³•åœ¨é€Ÿåº¦å’Œé¢„æµ‹ç²¾åº¦ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶æ”¯æŒæ½œåœ¨æ‰©å±•ï¼Œå¦‚ç»“åˆè§£å‰–å­¦å…ˆéªŒæˆ–å¤šå›å£°æ•°æ®ï¼Œæé«˜å°„é¢‘åœºæ ¡æ­£çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UHF MRIæä¾›äº†é«˜çš„ä¿¡å™ªæ¯”å’Œå“è¶Šçš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œä½†é«˜åœºå¸¦æ¥çš„å°„é¢‘åœºä¸å‡åŒ€æ€§é™ä½äº†å›¾åƒè´¨é‡ã€‚</li>
<li>ä¼ ç»ŸRF shimmingæ–¹æ³•ï¼ˆå¦‚MLSä¼˜åŒ–ï¼‰è™½ç„¶æœ‰æ•ˆï¼Œä½†è€—æ—¶ä¸”éœ€è¦æ‚£è€…å‚ä¸ã€‚</li>
<li>æœºå™¨å­¦ä¹ æ–¹æ³•ä¸ºRF shimé¢„æµ‹æä¾›äº†æ–°çš„é€”å¾„ï¼Œä½†ä»é¢ä¸´è®­ç»ƒè¦æ±‚é«˜ã€æ¨¡å‹å¤æ‚åº¦å’Œå®é™…æ•°æ®çº¦æŸç­‰æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„Fast RF Shimmingæ¡†æ¶å®ç°äº†å¿«é€ŸåŒ€åœºï¼Œç›¸æ¯”MLSæ–¹æ³•æœ‰æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨Adamç®—æ³•å’ŒResNetæ¨¡å‹è¿›è¡ŒåŒ€åœºæƒé‡æ¨å¯¼å’Œå°„é¢‘åœºåˆ°åŒ€åœºè¾“å‡ºçš„æ˜ å°„ã€‚</li>
<li>ç½®ä¿¡å‚æ•°è¢«å¼•å…¥æŸå¤±å‡½æ•°ä¸­ï¼ŒåŒæ—¶é‡‡ç”¨éå‡åŒ€åœºæ£€æµ‹å™¨è¯†åˆ«æç«¯éå‡åŒ€ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba921e30a254ba6b4f9aebe309198845.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-645486cd993f720eeb0714b733bf73ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f1586b151ed55a1fd2898380ea207ee.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Using-Space-Filling-Curves-and-Fractals-to-Reveal-Spatial-and-Temporal-Patterns-in-Neuroimaging-Data"><a href="#Using-Space-Filling-Curves-and-Fractals-to-Reveal-Spatial-and-Temporal-Patterns-in-Neuroimaging-Data" class="headerlink" title="Using Space-Filling Curves and Fractals to Reveal Spatial and Temporal   Patterns in Neuroimaging Data"></a>Using Space-Filling Curves and Fractals to Reveal Spatial and Temporal   Patterns in Neuroimaging Data</h2><p><strong>Authors:Jacek Grela, Zbigniew Drogosz, Jakub Janarek, Jeremi K. Ochab, Ignacio Cifre, Ewa Gudowska-Nowak, Maciej A. Nowak, PaweÅ‚ OÅ›wiÄ™cimka, Dante R. Chialvo</strong></p>
<p>We present a novel method, Fractal Space-Curve Analysis (FSCA), which combines Space-Filling Curve (SFC) mapping for dimensionality reduction with fractal Detrended Fluctuation Analysis (DFA). The method is suitable for multidimensional geometrically embedded data, especially for neuroimaging data which is highly correlated temporally and spatially. We conduct extensive feasibility studies on diverse, artificially generated data with known fractal characteristics: the fractional Brownian motion, Cantor sets, and Gaussian processes. We compare the suitability of dimensionality reduction via Hilbert SFC and a data-driven alternative. FSCA is then successfully applied to real-world magnetic resonance imaging (MRI) and functional MRI (fMRI) scans.   The method utilizing Hilbert curves is optimized for computational efficiency, proven robust against boundary effects typical in experimental data analysis, and resistant to data sub-sampling. It is able to correctly quantify and discern correlations in both stationary and dynamic two-dimensional images. In MRI Alzheimerâ€™s dataset, patients reveal a progression of the disease associated with a systematic decrease of the Hurst exponent. In fMRI recording of breath-holding task, the change in the exponent allows distinguishing different experimental phases.   This study introduces a robust method for fractal characterization of spatial and temporal correlations in many types of multidimensional neuroimaging data. Very few assumptions allow it to be generalized to more dimensions than typical for neuroimaging and utilized in other scientific fields. The method can be particularly useful in analyzing fMRI experiments to compute markers of pathological conditions resulting from neurodegeneration. We also showcase its potential for providing insights into brain dynamics in task-related experiments. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå³åˆ†å½¢ç©ºé—´æ›²çº¿åˆ†æï¼ˆFSCAï¼‰ï¼Œå®ƒå°†ç©ºé—´å¡«å……æ›²çº¿ï¼ˆSFCï¼‰æ˜ å°„ç”¨äºé™ç»´å’Œåˆ†å½¢å»è¶‹åŠ¿æ³¢åŠ¨åˆ†æï¼ˆDFAï¼‰ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç»´å‡ ä½•åµŒå…¥æ•°æ®ï¼Œå°¤å…¶é€‚ç”¨äºåœ¨æ—¶é—´ä¸Šå’Œç©ºé—´ä¸Šé«˜åº¦ç›¸å…³ç¥ç»æˆåƒæ•°æ®ã€‚æˆ‘ä»¬å¯¹å…·æœ‰å·²çŸ¥åˆ†å½¢ç‰¹å¾çš„äººå·¥ç”Ÿæˆæ•°æ®è¿›è¡Œäº†å¹¿æ³›çš„å¯è¡Œæ€§ç ”ç©¶ï¼ŒåŒ…æ‹¬å¸ƒæœ—è¿åŠ¨åˆ†æ•°ã€åº·æ‰˜å°”é›†å’Œé«˜æ–¯è¿‡ç¨‹ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†é€šè¿‡å¸Œå°”ä¼¯ç‰¹SFCè¿›è¡Œé™ç»´å’Œæ•°æ®é©±åŠ¨æ›¿ä»£æ–¹æ¡ˆçš„é€‚ç”¨æ€§ã€‚ç„¶åï¼ŒFSCAæˆåŠŸåº”ç”¨äºç°å®ä¸–ç•Œä¸­çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’ŒåŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ‰«æã€‚ä½¿ç”¨å¸Œå°”ä¼¯ç‰¹æ›²çº¿çš„æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¯¹å®éªŒæ•°æ®åˆ†æä¸­å…¸å‹çš„è¾¹ç•Œæ•ˆåº”å…·æœ‰é²æ£’æ€§ï¼Œå¹¶ä¸”èƒ½æŠµæŠ—æ•°æ®å­é‡‡æ ·ã€‚å®ƒèƒ½å¤Ÿæ­£ç¡®é‡åŒ–å’ŒåŒºåˆ†é™æ€å’ŒåŠ¨æ€äºŒç»´å›¾åƒä¸­çš„ç›¸å…³æ€§ã€‚åœ¨MRIçš„é˜¿å°”èŒ¨æµ·é»˜ç—…æ•°æ®é›†ä¸­ï¼Œæ‚£è€…æ˜¾ç¤ºå‡ºç–¾ç—…è¿›å±•ä¸èµ«æ–¯ç‰¹æŒ‡æ•°ç³»ç»Ÿé™ä½æœ‰å…³ã€‚åœ¨å‘¼å¸ä¿æŒä»»åŠ¡çš„fMRIè®°å½•ä¸­ï¼ŒæŒ‡æ•°çš„å˜åŒ–èƒ½å¤ŸåŒºåˆ†ä¸åŒçš„å®éªŒé˜¶æ®µã€‚è¯¥ç ”ç©¶ä¸ºå¤šç§ç±»å‹çš„å¤šç»´ç¥ç»æˆåƒæ•°æ®çš„æ—¶ç©ºç›¸å…³æ€§æä¾›äº†ç¨³å¥çš„åˆ†å½¢è¡¨å¾æ–¹æ³•ã€‚å¾ˆå°‘çš„å‡è®¾ä½¿å¾—å®ƒå¯ä»¥æ¨å¹¿åˆ°ç¥ç»æˆåƒçš„å…¸å‹ç»´åº¦ä¹‹å¤–å¹¶åœ¨å…¶ä»–ç§‘å­¦é¢†åŸŸä¸­ä½¿ç”¨ã€‚è¯¥æ–¹æ³•åœ¨åˆ†æå’Œè®¡ç®—ç¥ç»é€€è¡Œæ€§ç—…å˜å¼•èµ·çš„ç—…ç†çŠ¶å†µçš„æ ‡è®°ç‰©æ–¹é¢ç‰¹åˆ«æœ‰ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†æfMRIå®éªŒæ—¶ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å…¶åœ¨ä»»åŠ¡ç›¸å…³å®éªŒä¸­äº†è§£å¤§è„‘åŠ¨åŠ›å­¦çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12111v1">PDF</a> Grela, J., Drogosz, Z., Janarek, J., Ochab, J.K., Cifre, I.,   Gudowska-Nowak, E., Nowak, M.A., Oswiecimka, P., Chialvo, D., 2025. Using   space-filling curves and fractals to reveal spatial and temporal patterns in   neuroimaging data. J. Neural Eng. [accepted]   <a target="_blank" rel="noopener" href="https://doi.org/10.1088/1741-2552/ada705">https://doi.org/10.1088/1741-2552/ada705</a></p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§æ–°é¢–çš„æ–¹æ³•â€”â€”åˆ†å½¢ç©ºé—´æ›²çº¿åˆ†æï¼ˆFSCAï¼‰ï¼Œç»“åˆç©ºé—´å¡«å……æ›²çº¿ï¼ˆSFCï¼‰æ˜ å°„è¿›è¡Œé™ç»´å’Œåˆ†å½¢å»è¶‹åŠ¿æ³¢åŠ¨åˆ†æï¼ˆDFAï¼‰ï¼Œé€‚ç”¨äºå¤šç»´å‡ ä½•åµŒå…¥æ•°æ®ï¼Œå°¤å…¶é€‚ç”¨äºå…·æœ‰æ—¶é—´å’Œç©ºé—´é«˜åº¦ç›¸å…³æ€§çš„ç¥ç»æˆåƒæ•°æ®ã€‚å¯¹åˆ†å½¢å¸ƒæœ—è¿åŠ¨ã€åº·æ‰˜å°”é›†å’Œé«˜æ–¯è¿‡ç¨‹ç­‰å…·æœ‰å·²çŸ¥åˆ†å½¢ç‰¹å¾çš„äººå·¥ç”Ÿæˆæ•°æ®è¿›è¡Œäº†å¯è¡Œæ€§ç ”ç©¶ã€‚è¯¥æ–¹æ³•ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼ŒæŠµæŠ—è¾¹ç•Œæ•ˆåº”åŠæ•°æ®å­é‡‡æ ·ï¼Œåœ¨é™æ€å’ŒåŠ¨æ€äºŒç»´å›¾åƒä¸­éƒ½èƒ½å‡†ç¡®é‡åŒ–å¹¶è¾¨åˆ«å…³è”ã€‚åº”ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…MRIæ•°æ®é›†å’Œå‘¼å¸ä¿æŒä»»åŠ¡fMRIè®°å½•ä¸­ï¼ŒHurstæŒ‡æ•°å˜åŒ–æœ‰åŠ©äºé‰´åˆ«ä¸åŒå®éªŒé˜¶æ®µå’Œç—…ç†çŠ¶å†µã€‚è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºå…¶ä»–ç§‘å­¦é¢†åŸŸçš„å¤šç»´ç¥ç»æˆåƒæ•°æ®ç©ºé—´å’Œæ—¶é—´å…³è”çš„åˆ†å½¢è¡¨å¾ï¼Œå°¤å…¶åœ¨åˆ†æfMRIå®éªŒä»¥è®¡ç®—ç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„ç—…ç†æ ‡å¿—ç‰©æ–¹é¢å…·æœ‰é‡è¦æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•â€”â€”åˆ†å½¢ç©ºé—´æ›²çº¿åˆ†æï¼ˆFSCAï¼‰ï¼Œç»“åˆäº†ç©ºé—´å¡«å……æ›²çº¿æ˜ å°„ä¸åˆ†å½¢æ³¢åŠ¨åˆ†æï¼Œé€‚ç”¨äºå¤šç»´æ•°æ®ã€‚</li>
<li>å¯¹å¤šç§äººå·¥ç”Ÿæˆæ•°æ®è¿›è¡Œäº†å¯è¡Œæ€§ç ”ç©¶ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨åˆ†å½¢ç‰¹å¾æ•°æ®ä¸Šçš„è¡¨ç°ã€‚</li>
<li>ä¼˜åŒ–äº†è®¡ç®—æ•ˆç‡ï¼Œèƒ½å¤ŸæŠµæŠ—è¾¹ç•Œæ•ˆåº”å’Œæ•°æ®å­é‡‡æ ·ã€‚</li>
<li>åœ¨MRIå’ŒfMRIæ•°æ®ä¸­æˆåŠŸåº”ç”¨ï¼Œå±•ç¤ºäº†å…¶åœ¨é‡åŒ–ç–¾ç—…è¿›ç¨‹å’ŒåŒºåˆ†å®éªŒé˜¶æ®µæ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>é€šè¿‡HurstæŒ‡æ•°çš„å˜åŒ–èƒ½å¤Ÿé‰´åˆ«ä¸åŒçš„ç—…ç†çŠ¶å†µå’Œå®éªŒé˜¶æ®µã€‚</li>
<li>å¯å¹¿æ³›åº”ç”¨äºå…¶ä»–ç§‘å­¦é¢†åŸŸçš„å¤šç»´ç¥ç»æˆåƒæ•°æ®åˆ†å½¢è¡¨å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9434328ad4cd98bab658a470af75d097.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84d2fc01128f39a03d06b8bc5e8c0d6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b8ac61cb3ff3c9a90f8f7d549750b58.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes"><a href="#Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes" class="headerlink" title="Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes"></a>Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes</h2><p><strong>Authors:Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer</strong></p>
<p>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctorsâ€™ notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from <a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval">https://github.com/stefan-m-lenz/UroLlmEval</a>. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP. </p>
<blockquote>
<p>åœ¨å¾·å›½ï¼Œè‚¿ç˜¤è®°å½•å·¥ä½œå¤§å¤šä»¥æ‰‹åŠ¨æ–¹å¼è¿›è¡Œï¼Œéœ€è¦é˜…è¯»æ‚£è€…ç—…å†å¹¶å°†æ•°æ®è¾“å…¥ç»“æ„åŒ–æ•°æ®åº“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æ½œåŠ›é€šè¿‡æé«˜æ•ˆç‡å’Œå¯é æ€§æ¥å¢å¼ºè¿™ä¸€æµç¨‹ã€‚æœ¬æ¬¡è¯„ä¼°å¯¹ä¸‰ç§åŸºæœ¬ä»»åŠ¡ï¼ˆè¯†åˆ«è‚¿ç˜¤è¯Šæ–­ã€åˆ†é…ICD-10ä»£ç å’Œæå–é¦–æ¬¡è¯Šæ–­æ—¥æœŸï¼‰ä¸Šï¼Œæµ‹è¯•äº†è§„æ¨¡ä»1äº¿è‡³70äº¿æ¨¡å‹å‚æ•°çš„11ä¸ªä¸åŒå¼€æºLLMsã€‚ä¸ºäº†è¯„ä¼°è¿™äº›LLMsåœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‡†å¤‡äº†ä¸€ä»½åŸºäºæ³Œå°¿ç§‘åŒ¿ååŒ»ç”Ÿç¬”è®°çš„æ³¨é‡Šæ–‡æœ¬ç‰‡æ®µæ•°æ®é›†ã€‚ä½¿ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥æ¥ç ”ç©¶å°‘é‡æç¤ºä¸­ç¤ºä¾‹æ•°é‡çš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMsçš„ä¸€èˆ¬èƒ½åŠ›ã€‚Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚è®­ç»ƒæ•°æ®è¾ƒå°‘çš„æ¨¡å‹æˆ–å‚æ•°å°‘äº7äº¿çš„æ¨¡å‹è¡¨ç°æ˜æ˜¾è¾ƒå·®ï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹å¹¶æœªæ˜¾ç¤ºå‡ºæ€§èƒ½æå‡ã€‚æ¥è‡ªæ³Œå°¿ç§‘ä»¥å¤–çš„å…¶ä»–åŒ»å­¦é¢†åŸŸçš„ä¾‹å­ä¹Ÿèƒ½åœ¨å°‘é‡æç¤ºä¸­æ”¹å–„ç»“æœï¼Œè¿™è¯æ˜äº†LLMså¤„ç†è‚¿ç˜¤è®°å½•æ‰€éœ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¼€æºLLMsåœ¨è‡ªåŠ¨è¿›è¡Œè‚¿ç˜¤è®°å½•æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚å‚æ•°åœ¨7äº¿è‡³12äº¿ä¹‹é—´çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚é€šè¿‡å®šåˆ¶å¾®è°ƒä»¥åŠç²¾å¿ƒè®¾è®¡æç¤ºï¼Œè¿™äº›æ¨¡å‹æœ‰å¯èƒ½æˆä¸ºæœªæ¥ä¸´åºŠè®°å½•çš„é‡è¦å·¥å…·ã€‚è¯„ä¼°çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%8F%91%E5%B8%83%E4%BA%86%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E8%A7%A3%E5%86%B3%E5%BE%B7%E5%9B%BD%E5%8C%BB%E5%AD%A6NLP%E4%B8%AD%E7%9C%9F%E5%AE%9E%E3%80%81%E6%98%93%E4%BA%8E%E8%AE%BF%E9%97%AE%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%B5%84%E6%BA%90%E7%9F%AD%E7%BC%BA%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84%E6%9C%89%E4%BB%B7%E5%80%BC%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/stefan-m-lenz/UroLlmEvalè·å–ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†è¯¥æ•°æ®é›†ï¼Œä½œä¸ºè§£å†³å¾·å›½åŒ»å­¦NLPä¸­çœŸå®ã€æ˜“äºè®¿é—®çš„åŸºå‡†æµ‹è¯•èµ„æºçŸ­ç¼ºé—®é¢˜çš„ä¸€ä¸ªæ–°çš„æœ‰ä»·å€¼èµ„æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12106v1">PDF</a> 48 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‚¿ç˜¤è®°å½•è‡ªåŠ¨åŒ–æ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹11ç§ä¸åŒè§„æ¨¡ï¼ˆä»1äº¿åˆ°70äº¿å‚æ•°ï¼‰çš„å¼€æºLLMsè¿›è¡Œè¯„ä»·ï¼Œå‘ç°å®ƒä»¬åœ¨è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ã€ICD-10ä»£ç åˆ†é…å’Œé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚æ¨¡å‹å‚æ•°åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚é€šè¿‡å¾®è°ƒå’Œå®Œå–„æç¤ºï¼Œè¿™äº›æ¨¡å‹æœ‰æ½œåŠ›æˆä¸ºæœªæ¥ä¸´åºŠæ–‡æ¡£ç®¡ç†çš„é‡è¦å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯æå‡è‚¿ç˜¤è®°å½•è¿‡ç¨‹çš„æ•ˆç‡å’Œå¯é æ€§ã€‚</li>
<li>å¯¹11ç§ä¸åŒè§„æ¨¡çš„å¼€æºLLMsè¿›è¡Œäº†è¯„ä»·ï¼Œæ¶‰åŠä»»åŠ¡åŒ…æ‹¬è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ã€ICD-10ä»£ç åˆ†é…å’Œé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ã€‚</li>
<li>LLMsåœ¨å°‘æ ·æœ¬æç¤ºä¸‹çš„è¡¨ç°å—ç¤ºä¾‹æ•°é‡å’ŒåŒ»å­¦é¢†åŸŸå¤šæ ·æ€§çš„å½±å“ã€‚</li>
<li>å‚æ•°åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°å¹³è¡¡ã€‚</li>
<li>å®šåˆ¶å¾®è°ƒå’Œå®Œå–„æç¤ºå¯ä½¿è¿™äº›æ¨¡å‹æˆä¸ºæœªæ¥ä¸´åºŠæ–‡æ¡£ç®¡ç†çš„é‡è¦å·¥å…·ã€‚</li>
<li>å…¬å¼€çš„æ•°æ®é›†è§£å†³äº†å¾·å›½åŒ»å­¦NLPé¢†åŸŸä¸­çœŸå®ã€æ˜“è®¿é—®åŸºå‡†æµ‹è¯•æ•°æ®çš„çŸ­ç¼ºé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-764cd5a8d839a45da36d211b0c673397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fbd2fce63b2cad7e3b967780aa11e83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b245a6522b0d161dce0f49b1bb1190c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Momentum-dependent-electron-phonon-coupling-in-cuprates-by-RIXS-the-roles-of-phonon-symmetry-and-electronic-structure"><a href="#Momentum-dependent-electron-phonon-coupling-in-cuprates-by-RIXS-the-roles-of-phonon-symmetry-and-electronic-structure" class="headerlink" title="Momentum-dependent electron-phonon coupling in cuprates by RIXS: the   roles of phonon symmetry and electronic structure"></a>Momentum-dependent electron-phonon coupling in cuprates by RIXS: the   roles of phonon symmetry and electronic structure</h2><p><strong>Authors:Maryia Zinouyeva, Rolf Heid, Giacomo Merzoni, Riccardo Arpaia, Nikolai Andreev, Marco Biagi, Nicholas B. Brookes, Daniele Di Castro, Alexei Kalaboukhov, Kurt Kummer, Floriana Lombardi, Leonardo Martinelli, Francesco Rosa, Flora Yakhou-Harris, Lucio Braicovich, Marco Moretti Sala, Paolo G. Radaelli, Giacomo Ghiringhelli</strong></p>
<p>The experimental determination of the magnitude and momentum dependence of electron-phonon coupling (EPC) is an outstanding problem in condensed matter physics. Resonant inelastic x-ray scattering (RIXS) has been previously employed to determine the EPC, since the intensity of phonon peaks in RIXS spectra has been directly related to the underlying EPC strength. In order to assess the limits of validity of such a relation, we compare experimental results and theoretical predictions for several high-T$<em>c$ superconducting cuprates. Namely, we investigate the intensity of the bond-stretching phonon mode in CaCuO$<em>2$, La$<em>2$CuO$</em>{4+\delta}$, La$</em>{1.84}$Sr$</em>{0.16}$CuO$_4$ and YBa$_2$Cu$<em>3$O$</em>{7-\delta}$ along the high symmetry ($\zeta$,0), ($\zeta$,$\zeta$) directions and as a function of the azimuthal angle $\phi$ at fixed modulus of the in-plane momentum $\mathbf{q_\parallel}$. Using two different theoretical approaches for the description of the RIXS scattering from phonons, we find that the $\mathbf{q_\parallel}$-dependence of the RIXS intensity can be largely ascribed to the symmetry of the phonon mode, and that satisfactory prediction of the experimental results cannot be obtained without including realistic details of the electronic structure in the calculations. Regardless of the theoretical model, RIXS provides a reliable momentum dependence of EPC in cuprates and can be used to test advanced theoretical predictions. </p>
<blockquote>
<p>å®éªŒç¡®å®šç”µå­-å£°å­è€¦åˆï¼ˆEPCï¼‰çš„å¤§å°å’ŒåŠ¨é‡ä¾èµ–æ€§æ˜¯å‡èšæ€ç‰©ç†ä¸­çš„ä¸€ä¸ªçªå‡ºé—®é¢˜ã€‚ä»¥å¾€æ›¾ç”¨å…±æŒ¯éå¼¹æ€§Xå°„çº¿æ•£å°„ï¼ˆRIXSï¼‰æ¥ç¡®å®šEPCï¼Œå› ä¸ºRIXSå…‰è°±ä¸­å£°å­å³°çš„å¼ºåº¦ä¸åŸºæœ¬çš„EPCå¼ºåº¦ç›´æ¥ç›¸å…³ã€‚ä¸ºäº†è¯„ä¼°è¿™ç§å…³ç³»çš„æœ‰æ•ˆæ€§é™åˆ¶ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å‡ ç§é«˜æ¸©è¶…å¯¼é“œæ°§åŒ–ç‰©çš„å®éªŒç»“æœå’Œç†è®ºé¢„æµ‹ï¼ŒåŒ…æ‹¬CaCuO2ã€La2CuO4+Î´ã€La1.84Sr0.16CuO4å’ŒYBa2Cu3O7âˆ’Î´ã€‚æˆ‘ä»¬æ²¿ç€é«˜å¯¹ç§°ï¼ˆÎ¶ï¼Œ0ï¼‰ã€ï¼ˆÎ¶ï¼ŒÎ¶ï¼‰æ–¹å‘ï¼Œå¹¶ä½œä¸ºæ–¹ä½è§’Ï†çš„å‡½æ•°ï¼Œåœ¨å¹³é¢åŠ¨é‡å›ºå®šæ¨¡æ•°qâˆ¥ä¸‹ï¼Œç ”ç©¶é”®æ‹‰ä¼¸å£°å­æ¨¡å¼çš„å¼ºåº¦ã€‚é€šè¿‡ä½¿ç”¨ä¸¤ç§ä¸åŒçš„ç†è®ºæ–¹æ³•æ¥æè¿°RIXSçš„å£°å­æ•£å°„ï¼Œæˆ‘ä»¬å‘ç°RIXSå¼ºåº¦çš„qâˆ¥ä¾èµ–æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¯ä»¥å½’å› äºå£°å­æ¨¡å¼çš„å¯¹ç§°æ€§ï¼Œå¹¶ä¸”åœ¨è®¡ç®—ä¸­å¦‚æœä¸åŒ…å«ç”µå­ç»“æ„çš„ç°å®ç»†èŠ‚ï¼Œåˆ™æ— æ³•å¯¹å®éªŒç»“æœè¿›è¡Œä»¤äººæ»¡æ„çš„é¢„æµ‹ã€‚æ— è®ºé‡‡ç”¨å“ªç§ç†è®ºæ¨¡å‹ï¼ŒRIXSéƒ½èƒ½å¯é åœ°æä¾›é“œé…¸ç›ä¸­EPCçš„åŠ¨é‡ä¾èµ–æ€§ï¼Œå¯ç”¨äºæ£€éªŒå…ˆè¿›çš„ç†è®ºé¢„æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12089v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>æ‘˜è¦</strong><br>     è¯¥å®éªŒæ—¨åœ¨ç¡®å®šç”µå­å£°å­è€¦åˆï¼ˆEPCï¼‰çš„å¤§å°åŠå…¶åŠ¨é‡ä¾èµ–æ€§ï¼Œè¿™æ˜¯å‡èšæ€ç‰©ç†å­¦ä¸­çš„ä¸€é¡¹éš¾é¢˜ã€‚ä¸ºäº†éªŒè¯è°æŒ¯éå¼¹æ€§Xå°„çº¿æ•£å°„ï¼ˆRIXSï¼‰åœ¨ç¡®å®šEPCæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯¹æ¯”äº†å‡ ç§é«˜æ¸©è¶…å¯¼é“œæ°§åŒ–ç‰©çš„å®éªŒç»“æœå’Œç†è®ºé¢„æµ‹ã€‚é€šè¿‡å¯¹CaCuO2ã€La2CuO4+Î´ã€La1.84Sr0.16CuO4å’ŒYBa2Cu3O7âˆ’Î´ä¸­é”®æ‹‰ä¼¸å£°å­æ¨¡å¼çš„å¼ºåº¦è¿›è¡Œç ”ç©¶ï¼Œå‘ç°RIXSå¼ºåº¦å¯¹åŠ¨é‡å¹³è¡Œåˆ†é‡qâˆ¥çš„ä¾èµ–æ€§å¯ä¸»è¦å½’å› äºå£°å­æ¨¡å¼çš„å¯¹ç§°æ€§ã€‚å¦‚æœä¸è€ƒè™‘ç”µå­ç»“æ„çš„ç°å®ç»†èŠ‚ï¼Œå°±æ— æ³•å¯¹å®éªŒç»“æœè¿›è¡Œä»¤äººæ»¡æ„çš„é¢„æµ‹ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ— è®ºç†è®ºæ¨¡å‹å¦‚ä½•ï¼ŒRIXSéƒ½èƒ½å¯é åœ°æä¾›é“œæ°§åŒ–ç‰©ä¸­EPCçš„åŠ¨é‡ä¾èµ–æ€§ï¼Œå¯ç”¨äºæ£€éªŒé«˜çº§ç†è®ºé¢„æµ‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”µå­å£°å­è€¦åˆï¼ˆEPCï¼‰çš„åŠ¨é‡å’Œå¼ºåº¦æ˜¯å‡èšæ€ç‰©ç†å­¦çš„é‡è¦ç ”ç©¶å†…å®¹ã€‚</li>
<li>RIXSå·²è¢«ç”¨äºç¡®å®šEPCï¼Œå…¶å£°å­å³°å¼ºåº¦ä¸EPCå¼ºåº¦ç›´æ¥ç›¸å…³ã€‚</li>
<li>å¯¹å‡ ç§é«˜æ¸©è¶…å¯¼é“œæ°§åŒ–ç‰©çš„å®éªŒå’Œç†è®ºé¢„æµ‹è¿›è¡Œäº†æ¯”è¾ƒå’Œè¯„ä¼°ã€‚</li>
<li>RIXSå¼ºåº¦çš„åŠ¨é‡ä¾èµ–æ€§ä¸»è¦å½’å› äºå£°å­æ¨¡å¼çš„å¯¹ç§°æ€§ã€‚</li>
<li>åœ¨è®¡ç®—è¿‡ç¨‹ä¸­éœ€è¦è€ƒè™‘ç”µå­ç»“æ„çš„ç°å®ç»†èŠ‚ï¼Œä»¥ä¾¿æ›´å¥½åœ°é¢„æµ‹å®éªŒç»“æœã€‚</li>
<li>RIXSæ–¹æ³•åœ¨æµ‹è¯•é«˜çº§ç†è®ºé¢„æµ‹æ–¹é¢è¡¨ç°å¯é ï¼Œå¹¶ä¸ºé“œæ°§åŒ–ç‰©çš„EPCæä¾›äº†åŠ¨é‡ä¾èµ–æ€§çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02e4c89ee8268a65a00aeb2837a652e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13edb54d1d98223d01f59fce2ccf3b8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ee52697e74f6315e487b8f8b8f5adff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10856957e0b522e20fa2ba0866070070.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ea710dcee33bbf5cb4f9c009c2d081a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WaveNet-SF-A-Hybrid-Network-for-Retinal-Disease-Detection-Based-on-Wavelet-Transform-in-the-Spatial-Frequency-Domain"><a href="#WaveNet-SF-A-Hybrid-Network-for-Retinal-Disease-Detection-Based-on-Wavelet-Transform-in-the-Spatial-Frequency-Domain" class="headerlink" title="WaveNet-SF: A Hybrid Network for Retinal Disease Detection Based on   Wavelet Transform in the Spatial-Frequency Domain"></a>WaveNet-SF: A Hybrid Network for Retinal Disease Detection Based on   Wavelet Transform in the Spatial-Frequency Domain</h2><p><strong>Authors:Jilan Cheng, Guoli Long, Zeyu Zhang, Zhenjia Qi, Hanyu Wang, Libin Lu, Shuihua Wang, Yudong Zhang, Jin Hong</strong></p>
<p>Retinal diseases are a leading cause of vision impairment and blindness, with timely diagnosis being critical for effective treatment. Optical Coherence Tomography (OCT) has become a standard imaging modality for retinal disease diagnosis, but OCT images often suffer from issues such as speckle noise, complex lesion shapes, and varying lesion sizes, making interpretation challenging. In this paper, we propose a novel framework, WaveNet-SF, to enhance retinal disease detection by integrating spatial-domain and frequency-domain learning. The framework utilizes wavelet transforms to decompose OCT images into low- and high-frequency components, enabling the model to extract both global structural features and fine-grained details. To improve lesion detection, we introduce a multi-scale wavelet spatial attention (MSW-SA) module, which enhances the modelâ€™s focus on regions of interest at multiple scales. Additionally, a high-frequency feature compensation block (HFFC) is incorporated to recover edge information lost during wavelet decomposition, suppress noise, and preserve fine details crucial for lesion detection. Our approach achieves state-of-the-art (SOTA) classification accuracies of 97.82% and 99. 58% on the OCT-C8 and OCT2017 datasets, respectively, surpassing existing methods. These results demonstrate the efficacy of WaveNet-SF in addressing the challenges of OCT image analysis and its potential as a powerful tool for retinal disease diagnosis. </p>
<blockquote>
<p>è§†ç½‘è†œç–¾ç—…æ˜¯å¯¼è‡´è§†åŠ›å’Œå¤±æ˜çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼ŒåŠæ—¶è¯Šæ–­å¯¹äºæœ‰æ•ˆæ²»ç–—è‡³å…³é‡è¦ã€‚å…‰å­¦ç›¸å¹²å±‚ææˆåƒæœ¯ï¼ˆOCTï¼‰å·²æˆä¸ºè§†ç½‘è†œç–¾ç—…è¯Šæ–­çš„æ ‡å‡†æˆåƒæŠ€æœ¯ï¼Œä½†OCTå›¾åƒç»å¸¸å—åˆ°æ–‘ç‚¹å™ªå£°ã€ç—…å˜å½¢çŠ¶å¤æ‚å’Œç—…å˜å¤§å°ä¸ä¸€ç­‰é—®é¢˜çš„å½±å“ï¼Œä½¿å¾—è§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶WaveNet-SFï¼Œé€šè¿‡ç»“åˆç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸå­¦ä¹ æ¥æé«˜è§†ç½‘è†œç–¾ç—…çš„æ£€æµ‹èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å°æ³¢å˜æ¢å°†OCTå›¾åƒåˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæå–å…¨å±€ç»“æ„ç‰¹å¾å’Œç²¾ç»†ç»†èŠ‚ã€‚ä¸ºäº†æé«˜ç—…å˜æ£€æµ‹èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šå°ºåº¦å°æ³¢ç©ºé—´æ³¨æ„åŠ›ï¼ˆMSW-SAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿåœ¨å¤šä¸ªå°ºåº¦ä¸Šå¢å¼ºæ¨¡å‹å¯¹æ„Ÿå…´è¶£åŒºåŸŸçš„å…³æ³¨ã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†ä¸€ä¸ªé«˜é¢‘ç‰¹å¾è¡¥å¿å—ï¼ˆHFFCï¼‰ï¼Œä»¥æ¢å¤å°æ³¢åˆ†è§£è¿‡ç¨‹ä¸­ä¸¢å¤±çš„è¾¹ç¼˜ä¿¡æ¯ï¼ŒæŠ‘åˆ¶å™ªå£°ï¼Œå¹¶ä¿ç•™å¯¹ç—…å˜æ£€æµ‹è‡³å…³é‡è¦çš„ç²¾ç»†ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨OCT-C8å’ŒOCT2017æ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†ç±»å‡†ç¡®ç‡97.82%å’Œ99.58%ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†WaveNet-SFåœ¨è§£å†³OCTå›¾åƒåˆ†ææŒ‘æˆ˜æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå…¶ä½œä¸ºè§†ç½‘è†œç–¾ç—…è¯Šæ–­çš„å¼ºå¤§å·¥å…·æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11854v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºWaveNet-SFçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸå­¦ä¹ ï¼Œå¢å¼ºè§†ç½‘è†œç–¾ç—…æ£€æµ‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å°æ³¢å˜æ¢å°†OCTå›¾åƒåˆ†è§£ä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œå¼•å…¥å¤šå°ºåº¦å°æ³¢ç©ºé—´æ³¨æ„åŠ›æ¨¡å—å’Œé«˜é¢‘ç‰¹å¾è¡¥å¿å—ï¼Œæé«˜ç—…å˜æ£€æµ‹æ€§èƒ½ã€‚åœ¨OCT-C8å’ŒOCT2017æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†ç½‘è†œç–¾ç—…æ˜¯å¯¼è‡´è§†åŠ›å—æŸå’Œå¤±æ˜çš„ä¸»è¦åŸå› ï¼ŒåŠæ—¶è¯Šæ–­å¯¹æœ‰æ•ˆæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>Optical Coherence Tomography (OCT) æ˜¯è§†ç½‘è†œç–¾ç—…è¯Šæ–­çš„æ ‡å‡†æˆåƒæ–¹å¼ï¼Œä½†å…¶å›¾åƒå­˜åœ¨æ–‘ç‚¹å™ªå£°ã€å¤æ‚ç—…å˜å½¢çŠ¶å’Œä¸åŒçš„ç—…å˜å¤§å°ç­‰é—®é¢˜ï¼Œä½¿å¾—è§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>WaveNet-SFæ¡†æ¶é€šè¿‡ç»“åˆç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸå­¦ä¹ æ¥å¢å¼ºè§†ç½‘è†œç–¾ç—…æ£€æµ‹ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨å°æ³¢å˜æ¢åˆ†è§£OCTå›¾åƒä¸ºä½é¢‘å’Œé«˜é¢‘æˆåˆ†ï¼Œä»¥ä¾¿æå–å…¨å±€ç»“æ„ç‰¹å¾å’Œç»†ç²’åº¦ç»†èŠ‚ã€‚</li>
<li>å¤šå°ºåº¦å°æ³¢ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼ˆMSW-SAï¼‰çš„å¼•å…¥æé«˜äº†æ¨¡å‹å¯¹å¤šä¸ªå°ºåº¦ä¸Šæ„Ÿå…´è¶£åŒºåŸŸçš„å…³æ³¨ï¼Œä»è€Œæé«˜äº†ç—…å˜æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é«˜é¢‘ç‰¹å¾è¡¥å¿å—ï¼ˆHFFCï¼‰çš„åŠ å…¥æ—¨åœ¨æ¢å¤å°æ³¢åˆ†è§£è¿‡ç¨‹ä¸­ä¸¢å¤±çš„è¾¹ç¼˜ä¿¡æ¯ï¼ŒæŠ‘åˆ¶å™ªå£°å¹¶ä¿ç•™å¯¹ç—…å˜æ£€æµ‹è‡³å…³é‡è¦çš„ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a8d8dbfb7ed45430f3ed813e92f6f134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6814732a22c1215857b3b56f1d316f3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-806367c5af42ae283217bca2adc53b6f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhanced-imaging-of-M87-Simulations-with-the-EHT-and-extended-KVN"><a href="#Enhanced-imaging-of-M87-Simulations-with-the-EHT-and-extended-KVN" class="headerlink" title="Enhanced imaging of M87*: Simulations with the EHT and extended-KVN"></a>Enhanced imaging of M87*: Simulations with the EHT and extended-KVN</h2><p><strong>Authors:Ilje Cho, Jongho Park, Do-Young Byun, Taehyun Jung, Lindy Blackburn, Freek Roelofs, Andrew Chael, Dominic W. Pesce, Sheperd S. Doeleman, Sara Issaoun, Jae-Young Kim, Junhan Kim, Jose L. Gomez, Keiichi Asada, Bong Won Sohn, Sang-Sung Lee, Jongsoo Kim, Sascha Trippe, Aeree Chung</strong></p>
<p>The Event Horizon Telescope (EHT) has successfully revealed the shadow of the supermassive black hole, M87*, with an unprecedented angular resolution of approximately 20 uas at 230 GHz. However, because of limited short baseline lengths, the EHT has been constrained in its ability to recover larger scale jet structures. The extended Korean VLBI Network (eKVN) is committed to joining the EHT from 2024 that can improve short baseline coverage. This study evaluates the impact of the participation of eKVN in the EHT on the recovery of the M87* jet. Synthetic data, derived from a simulated M87* model, were observed using both the EHT and the combined EHT+eKVN arrays, followed by image reconstructions from both configurations. The results indicate that the inclusion of eKVN significantly improves the recovery of jet structures by reducing residual noise. Furthermore, jackknife tests, in which one or two EHT telescopes were omitted - simulating potential data loss due to poor weather - demonstrate that eKVN effectively compensates for these missing telescopes, particularly in short baseline coverage. Multi-frequency synthesis imaging at 86-230 GHz shows that the EHT+eKVN array enhances the recovered spectral index distribution compared to the EHT alone and improves image reconstruction at each frequency over single-frequency imaging. As the EHT continues to expand its array configuration and observing capabilities to probe black hole physics more in depth, the integration of eKVN into the EHT will significantly enhance the stability of observational results and improve image fidelity. This advancement will be particularly valuable for future regular monitoring observations, where consistent data quality is essential. </p>
<blockquote>
<p>äº‹ä»¶è§†ç•Œæœ›è¿œé•œï¼ˆEHTï¼‰å·²æˆåŠŸä»¥çº¦20å¾®è§’çš„æé«˜åˆ†è¾¨ç‡ï¼Œåœ¨230 GHzä¸‹æ­ç¤ºäº†è¶…å¤§è´¨é‡é»‘æ´M87<em>çš„å½±å­ã€‚ç„¶è€Œï¼Œç”±äºåŸºçº¿é•¿åº¦è¾ƒçŸ­ï¼ŒEHTåœ¨æ¢å¤æ›´å¤§è§„æ¨¡çš„å–·å°„æµç»“æ„æ–¹é¢å—åˆ°äº†é™åˆ¶ã€‚éŸ©å›½ç”šé•¿åŸºçº¿å¹²æ¶‰æµ‹é‡ç½‘ç»œï¼ˆeKVNï¼‰å·²æ‰¿è¯ºä»2024å¹´å¼€å§‹åŠ å…¥EHTï¼Œä»¥æé«˜çŸ­åŸºçº¿è¦†ç›–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†eKVNå‚ä¸EHTå¯¹æ¢å¤M87</em>å–·å°„æµçš„å½±å“ã€‚ä½¿ç”¨æ¨¡æ‹Ÿçš„M87*æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œé€šè¿‡EHTå’Œç»„åˆçš„EHT+eKVNé˜µåˆ—è¿›è¡Œè§‚å¯Ÿï¼Œç„¶åè¿›è¡Œä¸¤ç§é…ç½®ä¸‹çš„å›¾åƒé‡å»ºã€‚ç»“æœè¡¨æ˜ï¼ŒåŠ å…¥eKVNé€šè¿‡å‡å°‘æ®‹ç•™å™ªå£°æ˜¾è‘—æé«˜äº†å–·å°„æµç»“æ„çš„æ¢å¤ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ‹”é™¤æ³•æµ‹è¯•ï¼Œå…¶ä¸­çœç•¥äº†ä¸€ä¸ªæˆ–ä¸¤ä¸ªEHTæœ›è¿œé•œâ€”â€”æ¨¡æ‹Ÿå› å¤©æ°”ä¸ä½³å¯¼è‡´çš„æ•°æ®ä¸¢å¤±â€”â€”è¡¨æ˜eKVNæœ‰æ•ˆåœ°å¼¥è¡¥äº†è¿™äº›ç¼ºå¤±çš„æœ›è¿œé•œï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ­åŸºçº¿è¦†ç›–æ–¹é¢ã€‚åœ¨86-230 GHzçš„å¤šé¢‘åˆæˆæˆåƒæ˜¾ç¤ºï¼Œä¸å•ä¸€çš„EHTç›¸æ¯”ï¼ŒEHT+eKVNé˜µåˆ—æé«˜äº†æ¢å¤çš„è°±æŒ‡æ•°åˆ†å¸ƒï¼Œå¹¶åœ¨æ¯ä¸ªé¢‘ç‡ä¸Šæ”¹è¿›äº†å•é¢‘æˆåƒçš„å›¾åƒé‡å»ºã€‚éšç€EHTä¸æ–­æ‰©å¤§å…¶é˜µåˆ—é…ç½®å’Œè§‚æµ‹èƒ½åŠ›ï¼Œä»¥æ›´æ·±å…¥åœ°ç ”ç©¶é»‘æ´ç‰©ç†å­¦ï¼Œå°†eKVNé›†æˆåˆ°EHTä¸­å°†æ˜¾è‘—æé«˜è§‚æµ‹ç»“æœçš„ç¨³å®šæ€§å¹¶æé«˜å›¾åƒä¿çœŸåº¦ã€‚è¿™ä¸€è¿›å±•å¯¹äºæœªæ¥å®šæœŸç›‘æµ‹è§‚æµ‹å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºåœ¨è¿™äº›è§‚æµ‹ä¸­ï¼Œä¸€è‡´çš„æ•°æ®è´¨é‡æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11822v1">PDF</a> 13 pages, 13 figures, 2 tables</p>
<p><strong>Summary</strong><br>     éŸ©å›½VLBIç½‘ç»œï¼ˆeKVNï¼‰å‚ä¸äº‹ä»¶è§†ç•Œæœ›è¿œé•œï¼ˆEHTï¼‰å¯¹M87*é»‘æ´å°„æµæ¢å¤çš„å½±å“ç ”ç©¶ï¼Œåˆ©ç”¨åˆæˆæ•°æ®è¯„ä¼°eKVNçš„åŠ å…¥èƒ½æ˜¾è‘—æé«˜å°„æµç»“æ„çš„æ¢å¤æ•ˆæœï¼Œå‡å°‘æ®‹ä½™å™ªå£°ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿæ½œåœ¨æ•°æ®ä¸¢å¤±çš„æƒ…å†µä¸‹èƒ½æœ‰æ•ˆè¡¥å¿ç¼ºå¤±çš„æœ›è¿œé•œï¼Œç‰¹åˆ«æ˜¯æé«˜çŸ­åŸºçº¿è¦†ç›–åŒºåŸŸçš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒEHTä¸eKVNè”åˆé˜µåˆ—èƒ½æé«˜æ¢å¤å…‰è°±æŒ‡æ•°åˆ†å¸ƒçš„å‡†ç¡®æ€§ï¼Œå¹¶æ”¹å–„å„é¢‘ç‡çš„å›¾åƒé‡å»ºæ•ˆæœã€‚éšç€EHTä¸æ–­æ‰©å±•å…¶é˜µåˆ—é…ç½®å’Œè§‚æµ‹èƒ½åŠ›ï¼ŒeKVNçš„æ•´åˆå°†å¤§å¤§æé«˜è§‚æµ‹ç»“æœçš„ç¨³å®šæ€§å’Œå›¾åƒä¿çœŸåº¦ï¼Œå¯¹æœªæ¥å¸¸è§„ç›‘æµ‹è§‚æµ‹å°¤ä¸ºé‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EHTæˆåŠŸä»¥çº¦20å¾®å¼§ç§’çš„ç©ºå‰è§’åˆ†è¾¨ç‡åœ¨230 GHzæ­ç¤ºäº†è¶…å¤§è´¨é‡é»‘æ´M87*çš„å½±å­ã€‚</li>
<li>ç”±äºåŸºçº¿é•¿åº¦è¾ƒçŸ­ï¼ŒEHTåœ¨æ¢å¤æ›´å¤§è§„æ¨¡çš„å°„æµç»“æ„ä¸Šå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>eKVNè®¡åˆ’ä»2024å¹´åŠ å…¥EHTï¼Œæ—¨åœ¨æé«˜çŸ­åŸºçº¿è¦†ç›–ã€‚</li>
<li>åˆæˆæ•°æ®è¯„ä¼°æ˜¾ç¤ºï¼ŒeKVNçš„åŠ å…¥èƒ½æ˜¾è‘—æé«˜å°„æµç»“æ„çš„æ¢å¤æ•ˆæœï¼Œå‡å°‘æ®‹ä½™å™ªå£°ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿæ½œåœ¨æ•°æ®ä¸¢å¤±çš„æƒ…å†µä¸‹ï¼ŒeKVNèƒ½æœ‰æ•ˆè¡¥å¿ç¼ºå¤±çš„æœ›è¿œé•œï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ­åŸºçº¿è¦†ç›–æ–¹é¢ã€‚</li>
<li>EHTä¸eKVNè”åˆé˜µåˆ—èƒ½æé«˜å›¾åƒé‡å»ºçš„ä¿çœŸåº¦å’Œå…‰è°±æŒ‡æ•°åˆ†å¸ƒçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11822">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e6445a743e613bf058c53eefa28885be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6870885f1401e902784f16f7fec7db80.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5d4c0037d25961b8dab93d73f22f7c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67ba8ecb1db0fc52c14207a937ec7a2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bb75c0efe43f33862776f66bd0d79ee.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-generalizable-3D-framework-and-model-for-self-supervised-learning-in-medical-imaging"><a href="#A-generalizable-3D-framework-and-model-for-self-supervised-learning-in-medical-imaging" class="headerlink" title="A generalizable 3D framework and model for self-supervised learning in   medical imaging"></a>A generalizable 3D framework and model for self-supervised learning in   medical imaging</h2><p><strong>Authors:Tony Xu, Sepehr Hosseini, Chris Anderson, Anthony Rinaldi, Rahul G. Krishnan, Anne L. Martel, Maged Goubran</strong></p>
<p>Current self-supervised learning methods for 3D medical imaging rely on simple pretext formulations and organ- or modality-specific datasets, limiting their generalizability and scalability. We present 3DINO, a cutting-edge SSL method adapted to 3D datasets, and use it to pretrain 3DINO-ViT: a general-purpose medical imaging model, on an exceptionally large, multimodal, and multi-organ dataset of ~100,000 3D medical imaging scans from over 10 organs. We validate 3DINO-ViT using extensive experiments on numerous medical imaging segmentation and classification tasks. Our results demonstrate that 3DINO-ViT generalizes across modalities and organs, including out-of-distribution tasks and datasets, outperforming state-of-the-art methods on the majority of evaluation metrics and labeled dataset sizes. Our 3DINO framework and 3DINO-ViT will be made available to enable research on 3D foundation models or further finetuning for a wide range of medical imaging applications. </p>
<blockquote>
<p>å½“å‰ç”¨äºä¸‰ç»´åŒ»å­¦å½±åƒçš„è‡ªå­¦æ–¹æ³•ä¾èµ–äºç®€å•çš„é¢„è®¾å…¬å¼å’Œç‰¹å®šå™¨å®˜æˆ–ç‰¹å®šæ¨¡æ€çš„æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†åä¸ºâ€œä¸‰ç»´å˜å¼‚æ·±åº¦ï¼ˆ3DINOï¼‰â€çš„å‰æ²¿SSLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºä¸‰ç»´æ•°æ®é›†ï¼Œå¹¶ç”¨äºé¢„è®­ç»ƒåä¸ºâ€œä¸‰ç»´è¯ºç»´Tï¼ˆ3DINO-ViTï¼‰â€çš„é€šç”¨åŒ»å­¦å½±åƒæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨åŒ…å«æ¥è‡ªè¶…è¿‡åä¸ªå™¨å®˜çš„çº¦åä¸‡ä¸ªä¸‰ç»´åŒ»å­¦å½±åƒæ‰«ææ•°æ®çš„å¤§è§„æ¨¡ã€å¤šæ¨¡æ€å’Œå¤šå™¨å®˜æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†åœ¨å¤šä¸ªåŒ»å­¦å½±åƒåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¸Šçš„ä¸‰ç»´è¯ºç»´Tã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸‰ç»´è¯ºç»´Tåœ¨è·¨æ¨¡æ€å’Œè·¨å™¨å®˜ä¸­å‡è¡¨ç°å‡ºå¾ˆå¥½çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å¤„ç†å¼‚å¸¸ä»»åŠ¡å’Œæ•°æ®é›†çš„è¡¨ç°ä¼˜å¼‚ã€‚åœ¨å„ç§è¯„ä»·æŒ‡æ ‡å’Œæœ‰æ ‡ç­¾æ•°æ®é›†å¤§å°æ–¹é¢ï¼Œå®ƒä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä¸‰ç»´è¯ºç»´æ¡†æ¶å’Œä¸‰ç»´è¯ºç»´Tå°†æä¾›ä½¿ç”¨ï¼Œä»¥æ”¯æŒå…³äºä¸‰ç»´åŸºç¡€æ¨¡å‹çš„ç ”ç©¶æˆ–ç”¨äºè¿›ä¸€æ­¥å¾®è°ƒå„ç§åŒ»å­¦å½±åƒåº”ç”¨çš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11755v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹3Dæ•°æ®é›†çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º3DINOçš„å‰æ²¿SSLæ–¹æ³•ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªé€šç”¨åŒ»ç–—æˆåƒæ¨¡å‹3DINO-ViTã€‚è¯¥æ¨¡å‹åœ¨åŒ…å«è¶…è¿‡åä¸‡ä¸ªæ¥è‡ªåå¤šä¸ªå™¨å®˜çš„3DåŒ»å­¦å›¾åƒçš„å¤§å‹å¤šæ¨¡æ€å¤šå™¨å®˜æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒæ¨¡æ€å’Œå™¨å®˜ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼ŒåŒ…æ‹¬è¶…å‡ºåˆ†å¸ƒçš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°è¯„ä¼°æŒ‡æ ‡å’Œæ ‡æ³¨æ•°æ®é›†å¤§å°ä¸Šå‡ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚æœ¬æ–‡å…¬å¼€çš„3DINOæ¡†æ¶å’Œ3DINO-ViTæ¨¡å‹å°†ä¸ºç ”ç©¶3DåŸºç¡€æ¨¡å‹å’Œè¿›ä¸€æ­¥å¾®è°ƒä»¥åº”ç”¨äºå¹¿æ³›çš„åŒ»å­¦æˆåƒä»»åŠ¡æä¾›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ®µæ–‡å­—çš„ä¸»è¦è§è§£å’Œæ”¶è·ç‚¹ï¼š</p>
<ul>
<li>å½“å‰é’ˆå¯¹ä¸‰ç»´åŒ»ç–—å½±åƒçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå±€é™äºç®€å•çš„é¢„è®¾å½¢å¼å’Œç‰¹å®šå™¨å®˜æˆ–æ¨¡æ€çš„æ•°æ®é›†ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…ˆè¿›çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆç§°ä¸º3DINOï¼‰ï¼Œä¸“ä¸ºä¸‰ç»´æ•°æ®é›†è®¾è®¡ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªé€šç”¨çš„åŒ»ç–—æˆåƒæ¨¡å‹ï¼ˆç§°ä¸º3DINO-ViTï¼‰ã€‚</li>
<li>3DINO-ViTæ¨¡å‹åœ¨ä¸€ä¸ªå¤§å‹çš„å¤šæ¨¡æ€å’Œå¤šå™¨å®˜æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«è¶…è¿‡åä¸‡ä¸ªä¸‰ç»´åŒ»å­¦å›¾åƒæ‰«ææ•°æ®ã€‚</li>
<li>é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæ˜¾ç¤º3DINO-ViTæ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨ä¸åŒæ¨¡æ€å’Œå™¨å®˜ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-420e208eac7c2820ad8f56b7fd587cb6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedicoSAM-Towards-foundation-models-for-medical-image-segmentation"><a href="#MedicoSAM-Towards-foundation-models-for-medical-image-segmentation" class="headerlink" title="MedicoSAM: Towards foundation models for medical image segmentation"></a>MedicoSAM: Towards foundation models for medical image segmentation</h2><p><strong>Authors:Anwai Archit, Luca Freckmann, Constantin Pape</strong></p>
<p>Medical image segmentation is an important analysis task in clinical practice and research. Deep learning has massively advanced the field, but current approaches are mostly based on models trained for a specific task. Training such models or adapting them to a new condition is costly due to the need for (manually) labeled data. The emergence of vision foundation models, especially Segment Anything, offers a path to universal segmentation for medical images, overcoming these issues. Here, we study how to improve Segment Anything for medical images by comparing different finetuning strategies on a large and diverse dataset. We evaluate the finetuned models on a wide range of interactive and (automatic) semantic segmentation tasks. We find that the performance can be clearly improved for interactive segmentation. However, semantic segmentation does not benefit from pretraining on medical images. Our best model, MedicoSAM, is publicly available at <a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/medico-sam">https://github.com/computational-cell-analytics/medico-sam</a>. We show that it is compatible with existing tools for data annotation and believe that it will be of great practical value. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯ä¸´åºŠå®è·µå’Œç ”ç©¶ä¸­é‡è¦çš„åˆ†æä»»åŠ¡ã€‚æ·±åº¦å­¦ä¹ æå¤§åœ°æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•å¤§å¤šåŸºäºç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ã€‚ç”±äºéœ€è¦ï¼ˆæ‰‹åŠ¨ï¼‰æ ‡è®°æ•°æ®ï¼Œè®­ç»ƒæ­¤ç±»æ¨¡å‹æˆ–å°†å…¶é€‚åº”æ–°æ¡ä»¶æˆæœ¬é«˜æ˜‚ã€‚è§†è§‰åŸºç¡€æ¨¡å‹çš„å‡ºç°ï¼Œç‰¹åˆ«æ˜¯Segment Anythingï¼Œä¸ºåŒ»å­¦å›¾åƒçš„é€šç”¨åˆ†å‰²æä¾›äº†è·¯å¾„ï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•é€šè¿‡åœ¨ä¸€ä¸ªå¤§å‹å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šæ¯”è¾ƒä¸åŒçš„å¾®è°ƒç­–ç•¥ï¼Œæ¥æé«˜Segment Anythingåœ¨åŒ»å­¦å›¾åƒä¸Šçš„åº”ç”¨ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¾®è°ƒæ¨¡å‹åœ¨å¹¿æ³›çš„äº¤äº’å¼å’Œï¼ˆè‡ªåŠ¨ï¼‰è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°äº¤äº’å¼åˆ†å‰²çš„æ€§èƒ½å¯ä»¥å¾—åˆ°æ˜æ˜¾æé«˜ã€‚ç„¶è€Œï¼Œè¯­ä¹‰åˆ†å‰²å¹¶ä¸å—ç›ŠäºåŒ»å­¦å›¾åƒä¸Šçš„é¢„è®­ç»ƒã€‚æˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹MedicoSAMå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/computational-cell-analytics/medico-sam%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E5%AE%83%E4%B8%8E%E7%8E%B0%E6%9C%89%E6%95%B0%E6%8D%AE%E6%A0%87%E6%B3%A8%E5%B7%A5%E5%85%B7%E5%85%BC%E5%AE%B9%EF%BC%8C%E5%B9%B6%E7%9B%B8%E4%BF%A1%E5%AE%83%E5%85%B7%E6%9C%89%E5%B7%A8%E5%A4%A7%E7%9A%84%E5%AE%9E%E7%94%A8%E4%BB%B7%E5%80%BC%E3%80%82">https://github.com/computational-cell-analytics/medico-samå…¬å¼€è·å–ã€‚æˆ‘ä»¬å±•ç¤ºäº†å®ƒä¸ç°æœ‰æ•°æ®æ ‡æ³¨å·¥å…·å…¼å®¹ï¼Œå¹¶ç›¸ä¿¡å®ƒå…·æœ‰å·¨å¤§çš„å®ç”¨ä»·å€¼ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11734v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å½“å‰æ–¹æ³•å¤§å¤šåŸºäºç‰¹å®šä»»åŠ¡è®­ç»ƒçš„æ¨¡å‹ã€‚ç”±äºéœ€è¦æ‰‹åŠ¨æ ‡æ³¨æ•°æ®ï¼Œè®­ç»ƒè¿™æ ·çš„æ¨¡å‹æˆ–å°†å…¶é€‚åº”æ–°æ¡ä»¶æˆæœ¬é«˜æ˜‚ã€‚æ–°å…´çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå°¤å…¶æ˜¯Segment Anythingï¼Œä¸ºå®ç°åŒ»å­¦å›¾åƒçš„é€šç”¨åˆ†å‰²æä¾›äº†é€”å¾„ã€‚é€šè¿‡å¯¹ä¸åŒå¾®è°ƒç­–ç•¥åœ¨å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„æ¯”è¾ƒï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•æ”¹è¿›Segment Anythingåœ¨åŒ»å­¦å›¾åƒä¸Šçš„åº”ç”¨ã€‚è¯„ä¼°å¾®è°ƒæ¨¡å‹åœ¨äº¤äº’å¼å’Œè‡ªåŠ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬å‘ç°äº¤äº’å¼åˆ†å‰²çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œä½†è¯­ä¹‰åˆ†å‰²å¹¶æœªä»åŒ»å­¦å›¾åƒä¸Šçš„é¢„è®­ç»ƒä¸­å—ç›Šã€‚æˆ‘ä»¬æœ€ä½³çš„æ¨¡å‹MedicoSAMå·²åœ¨å…¬å¼€å¹³å°ä¸Šå‘å¸ƒï¼Œä¸ç°æœ‰æ•°æ®æ ‡æ³¨å·¥å…·å…¼å®¹ï¼Œå…·æœ‰æå¤§çš„å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯ä¸´åºŠå®è·µå’Œç ”ç©¶ä¸­é‡è¦çš„åˆ†æä»»åŠ¡ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å·²å¤§å¤§æ¨è¿›äº†åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„å‘å±•ï¼Œä½†è®­ç»ƒç‰¹å®šä»»åŠ¡æ¨¡å‹çš„æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>Segment Anythingè¿™æ ·çš„è§†è§‰åŸºç¡€æ¨¡å‹çš„å‡ºç°ï¼Œä¸ºåŒ»å­¦å›¾åƒçš„é€šç”¨åˆ†å‰²æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡å¯¹ä¸åŒå¾®è°ƒç­–ç•¥çš„ç ”ç©¶ï¼Œæ”¹è¿›äº†Segment Anythingåœ¨åŒ»å­¦å›¾åƒä¸Šçš„åº”ç”¨ã€‚</li>
<li>äº¤äº’å¼åˆ†å‰²çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œè€Œè¯­ä¹‰åˆ†å‰²å¹¶æœªä»åŒ»å­¦å›¾åƒé¢„è®­ç»ƒä¸­å—ç›Šã€‚</li>
<li>æœ€ä½³æ¨¡å‹MedicoSAMå·²å…¬å¼€å‘å¸ƒï¼Œå¹¶ä¸ç°æœ‰æ•°æ®æ ‡æ³¨å·¥å…·å…¼å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb3e4319bd00f59c5179480243f0f704.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1dfecdd1c621e0d8ce5a0b807de31ecc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f0d21543b8a5409ff6bdf96a1fdfc86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e58135f2f0594273f515d9840a8a960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6714219eaf30ba23b72f46a444f5f1f2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-OPTICAM-Reveals-the-Hidden-Spin-of-the-WZ-Sge-Star-GOTO-065054-49-593624-51"><a href="#Bridging-the-Gap-OPTICAM-Reveals-the-Hidden-Spin-of-the-WZ-Sge-Star-GOTO-065054-49-593624-51" class="headerlink" title="Bridging the Gap: OPTICAM Reveals the Hidden Spin of the WZ Sge Star   GOTO 065054.49+593624.51"></a>Bridging the Gap: OPTICAM Reveals the Hidden Spin of the WZ Sge Star   GOTO 065054.49+593624.51</h2><p><strong>Authors:N. Castro Segura, Z. A. Irving, F. M. Vincentelli, D. Altamirano, Y. Tampo, C. Knigge, I. Pelisoli, D. L. Coppejans, N. Rawat, A. Castro, A. Sahu, J. V. HernÃ¡ndez Santisteban, M. Kimura, M. Veresvarska, R. Michel, S. Scaringi, M. Najera</strong></p>
<p>WZ Sge stars are highly evolved accreting white dwarf systems (AWDs) exhibiting remarkably large amplitude outbursts (a.k.a. super-outbursts), typically followed by short rebrightenings&#x2F;echo outbursts. These systems have some of the lowest mass transfer rates among AWDs, making even low magnetic fields dynamically important. Such magnetic fields are often invoked to explain the phenomenology observed in these systems, such as their X-ray luminosity and long periods of quiescence (30+ years). However, the detection of these is very elusive given the quenching of the accretion columns during outburst and the low luminosity of these systems during quiescence. Here we present high-cadence multi-band observations with {\it OPTICAM} of the recent outburst of the recently discovered WZ Sge star GOTO065054.49+593624.51, during the end of the main outburst and the dip in-between rebrightenings, covering 2 orders of magnitude in brightness. Our observations reveal the presence of a statistically significant signal with $P_{\omega}\simeq148$ seconds in the bluer ($g$) band which is detected only during the dip between the main outburst and the rebrigthenings. We interpret this signal as the spin period of the AWD. If confirmed, GOTO 0650 would bridge the gap between intermediate- and fast-rotating intermediate polars (IPs) below the period gap. </p>
<blockquote>
<p>WZ Sgeå‹æ˜Ÿæ˜¯é«˜åº¦è¿›åŒ–çš„å¸ç§¯å‹ç™½çŸ®æ˜Ÿç³»ç»Ÿï¼ˆAWDï¼‰ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„å¤§æŒ¯å¹…çˆ†å‘ï¼ˆä¹Ÿç§°ä¸ºè¶…çº§çˆ†å‘ï¼‰ï¼Œéšåé€šå¸¸æ˜¯çŸ­æš‚çš„å†æ¬¡å¢äº®&#x2F;å›å£°çˆ†å‘ã€‚è¿™äº›ç³»ç»Ÿçš„è´¨é‡ä¼ é€’ç‡åœ¨å…¶AWDä¸­ç›¸å¯¹è¾ƒä½ï¼Œä½¿å¾—å³ä½¿æ˜¯ä½ç£åœºä¹Ÿå…·æœ‰åŠ¨æ€é‡è¦æ€§ã€‚è¿™äº›ç£åœºé€šå¸¸ç”¨äºè§£é‡Šè¿™äº›ç³»ç»Ÿä¸­è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼Œå¦‚å®ƒä»¬çš„Xå°„çº¿å…‰åº¦ä»¥åŠé•¿æ—¶é—´çš„é™æ­¢æœŸï¼ˆè¶…è¿‡30å¹´ï¼‰ã€‚ç„¶è€Œï¼Œç”±äºçˆ†å‘æœŸé—´çš„ç§¯æŸ±ç†„ç­ä»¥åŠé™æ­¢æœŸé—´çš„ä½å…‰åº¦ï¼Œè¿™äº›ç£åœºçš„æ£€æµ‹éå¸¸éš¾ä»¥æ‰æ‘¸ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨OPTICAMçš„é«˜çµæ•åº¦å¤šæ³¢æ®µè§‚æµ‹ï¼Œå¯¹æœ€è¿‘å‘ç°çš„WZ Sgeæ˜ŸGOTO065054.49+593624.51çš„æœ€è¿‘ä¸€æ¬¡çˆ†å‘è¿›è¡Œäº†è§‚æµ‹ï¼Œæ¶µç›–äº†ä¸»çˆ†å‘ç»“æŸå’Œåœ¨å†æ¬¡å¢äº®ä¹‹é—´çš„ä½è°·æœŸï¼Œäº®åº¦å˜åŒ–äº†2ä¸ªæ•°é‡çº§ã€‚æˆ‘ä»¬çš„è§‚æµ‹æ˜¾ç¤ºï¼Œåœ¨è¾ƒè“çš„gæ³¢æ®µå‡ºç°äº†ä¸€ä¸ªç»Ÿè®¡ä¸Šæ˜¾è‘—çš„ä¿¡å·ï¼Œå…¶å‘¨æœŸçº¦ä¸º148ç§’ï¼Œè¯¥ä¿¡å·ä»…åœ¨ä¸»çˆ†å‘å’Œå†æ¬¡å¢äº®ä¹‹é—´çš„ä½è°·æœŸè¢«æ£€æµ‹åˆ°ã€‚æˆ‘ä»¬å°†æ­¤ä¿¡å·è§£é‡Šä¸ºAWDçš„è‡ªè½¬å‘¨æœŸã€‚å¦‚æœå¾—åˆ°è¯å®ï¼ŒGOTO 0650å°†æˆä¸ºå‘¨æœŸç¼ºå£ä¸‹æ–¹è¿æ¥ä¸­é—´ææ€§å’Œå¿«é€Ÿæ—‹è½¬ä¸­é—´ææ€§çš„æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11669v1">PDF</a> Submitted to MRNAS</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†WZ Sgeæ˜Ÿçš„ç ”ç©¶ï¼Œè¿™äº›æ˜Ÿæ˜¯é«˜åº¦æ¼”åŒ–çš„å¸ç§¯å‹ç™½çŸ®æ˜Ÿç³»ç»Ÿï¼ˆAWDsï¼‰ï¼Œè¡¨ç°å‡ºå·¨å¤§çš„æŒ¯å¹…çˆ†å‘ï¼ˆè¶…çº§çˆ†å‘ï¼‰ï¼Œéšåæ˜¯çŸ­æš‚çš„å†å¢äº®æˆ–å›å£°çˆ†å‘ã€‚è¿™äº›ç³»ç»Ÿçš„è´¨é‡è½¬ç§»ç‡è¾ƒä½ï¼Œä½¿å¾—ä½ç£åœºä¹Ÿå…·æœ‰åŠ¨æ€é‡è¦æ€§ã€‚è™½ç„¶å­˜åœ¨å…³äºè¿™äº›ç£åœºçš„è§£é‡Šç°è±¡å­¦çš„å‡è¯´ï¼Œä½†å¯¹è¿™äº›ç³»ç»Ÿç£åœºçš„æ£€æµ‹ä»ç„¶éå¸¸éš¾ä»¥æ‰æ‘¸ã€‚ç°åœ¨å¯¹åä¸ºGOTO 0650çš„æ–°å‘ç°çš„WZ Sgeæ˜Ÿè¿›è¡Œäº†é«˜æ—¶é—´åˆ†è¾¨ç‡çš„å¤šæ³¢æ®µè§‚æµ‹ï¼Œæ­ç¤ºäº†åœ¨ä¸»è¦çˆ†å‘å’Œå†å¢äº®ä¹‹é—´çš„ä½è°·ä¸­ä»…æ£€æµ‹åˆ°è“æ³¢æ®µï¼ˆgæ³¢æ®µï¼‰å­˜åœ¨æ˜¾è‘—çš„ä¿¡å·ã€‚è§£é‡Šè¿™ä¸ªä¿¡å·ä¸ºAWDçš„è‡ªè½¬å‘¨æœŸï¼Œå¦‚æœå¾—åˆ°è¯å®ï¼ŒGOTO 0650å°†æˆä¸ºè¿æ¥å‘¨æœŸé—´éš™ä¸‹æ–¹ä¸­é—´ææ€§å’Œå¿«é€Ÿæ—‹è½¬ä¸­é—´ææ€§çš„æ¡¥æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WZ Sgeæ˜Ÿæ˜¯é«˜åº¦æ¼”åŒ–çš„å¸ç§¯å‹ç™½çŸ®æ˜Ÿç³»ç»Ÿï¼Œå…¶ç‰¹ç‚¹æ˜¯å‘ç”Ÿè¶…çº§çˆ†å‘ï¼Œä¹‹åè·ŸéšçŸ­æš‚å†å¢äº®æˆ–å›å£°çˆ†å‘ã€‚</li>
<li>è¿™äº›ç³»ç»Ÿçš„è´¨é‡è½¬ç§»ç‡è¾ƒä½ï¼Œå¯¼è‡´ä½ç£åœºä¹Ÿæœ‰åŠ¨æ€é‡è¦æ€§ã€‚</li>
<li>è™½ç„¶å­˜åœ¨å…³äºç£åœºçš„è§£é‡Šç°è±¡å­¦çš„å‡è¯´ï¼Œä½†æ£€æµ‹è¿™äº›ç£åœºéå¸¸å›°éš¾ï¼Œå› ä¸ºçˆ†å‘æœŸé—´ä¼šæŠ‘åˆ¶ç§¯æŸ±çš„å‘å…‰å’Œåœ¨é™æ­¢æœŸé—´çš„ä½å…‰åº¦ã€‚</li>
<li>å¯¹æ–°å‘ç°çš„WZ Sgeæ˜ŸGOTO 0650è¿›è¡Œäº†é«˜æ—¶é—´åˆ†è¾¨ç‡çš„å¤šæ³¢æ®µè§‚æµ‹ã€‚</li>
<li>åœ¨ä¸»è¦çˆ†å‘å’Œå†å¢äº®ä¹‹é—´çš„ä½è°·ä¸­ä»…æ£€æµ‹åˆ°è“æ³¢æ®µå­˜åœ¨æ˜¾è‘—çš„ä¿¡å·ï¼Œè¿™å¯èƒ½æ˜¯AWDçš„è‡ªè½¬å‘¨æœŸä¿¡å·ã€‚</li>
<li>å¦‚æœç¡®è®¤è¿™ä¸€å‘ç°ï¼ŒGOTO 0650å°†æˆä¸ºè¿æ¥ä¸­é—´ææ€§å’Œå¿«é€Ÿæ—‹è½¬ä¸­é—´ææ€§ä¹‹é—´çš„æ¡¥æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d18528ed873991cc5db7b7633cdd275.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-088993905961c33d013b856181175d9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e444f489a599d102edc55c4788ae114.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2014b30e13f6ab08fd899b5518fd5c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-836a14ffaf44804a68dd52ea036a5336.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7fba7a2d4260b09ef5ad20967a8f46a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b452c41c48f84d509b9361731251767e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="How-Well-Do-Supervised-3D-Models-Transfer-to-Medical-Imaging-Tasks"><a href="#How-Well-Do-Supervised-3D-Models-Transfer-to-Medical-Imaging-Tasks" class="headerlink" title="How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?"></a>How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?</h2><p><strong>Authors:Wenxuan Li, Alan Yuille, Zongwei Zhou</strong></p>
<p>The pre-training and fine-tuning paradigm has become prominent in transfer learning. For example, if the model is pre-trained on ImageNet and then fine-tuned to PASCAL, it can significantly outperform that trained on PASCAL from scratch. While ImageNet pre-training has shown enormous success, it is formed in 2D, and the learned features are for classification tasks; when transferring to more diverse tasks, like 3D image segmentation, its performance is inevitably compromised due to the deviation from the original ImageNet context. A significant challenge lies in the lack of large, annotated 3D datasets rivaling the scale of ImageNet for model pre-training. To overcome this challenge, we make two contributions. Firstly, we construct AbdomenAtlas 1.1 that comprises 9,262 three-dimensional computed tomography (CT) volumes with high-quality, per-voxel annotations of 25 anatomical structures and pseudo annotations of seven tumor types. Secondly, we develop a suite of models that are pre-trained on our AbdomenAtlas 1.1 for transfer learning. Our preliminary analyses indicate that the model trained only with 21 CT volumes, 672 masks, and 40 GPU hours has a transfer learning ability similar to the model trained with 5,050 (unlabeled) CT volumes and 1,152 GPU hours. More importantly, the transfer learning ability of supervised models can further scale up with larger annotated datasets, achieving significantly better performance than preexisting pre-trained models, irrespective of their pre-training methodologies or data sources. We hope this study can facilitate collective efforts in constructing larger 3D medical datasets and more releases of supervised pre-trained models. </p>
<blockquote>
<p>é¢„è®­ç»ƒå’Œå¾®è°ƒèŒƒå¼åœ¨è¿ç§»å­¦ä¹ ä¸­å·²ç»å˜å¾—éå¸¸çªå‡ºã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹åœ¨ImageNetä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨PASCALä¸Šè¿›è¡Œå¾®è°ƒï¼Œå…¶æ€§èƒ½ä¼šæ˜æ˜¾ä¼˜äºç›´æ¥åœ¨PASCALä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚è™½ç„¶ImageNeté¢„è®­ç»ƒå·²ç»å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½†å®ƒæ˜¯åœ¨äºŒç»´ç©ºé—´ä¸­å½¢æˆçš„ï¼Œå¹¶ä¸”å­¦åˆ°çš„ç‰¹å¾é€‚ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼›å½“è¿ç§»åˆ°æ›´å¤æ‚çš„ä»»åŠ¡ï¼ˆå¦‚ä¸‰ç»´å›¾åƒåˆ†å‰²ï¼‰æ—¶ï¼Œç”±äºä¸åŸå§‹ImageNetä¸Šä¸‹æ–‡çš„åå·®ï¼Œå…¶æ€§èƒ½ä¸å¯é¿å…åœ°ä¼šå—åˆ°å½±å“ã€‚ä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜åœ¨äºç¼ºä¹å¤§è§„æ¨¡ã€æ ‡æ³¨çš„ä¸‰ç»´æ•°æ®é›†ï¼Œæ— æ³•ä¸ImageNetçš„è§„æ¨¡ç›¸åŒ¹é…çš„æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸¤é¡¹è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«9262ä¸ªä¸‰ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä½“ç§¯çš„AbdomenAtlas 1.1æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†é«˜è´¨é‡çš„ä¸‰ç»´åƒç´ çº§åˆ«çš„æ³¨é‡Šæ•°æ®æ ‡æ³¨äº†è…¹éƒ¨25ä¸ªè§£å‰–ç»“æ„å’Œä¸ƒç§è‚¿ç˜¤ç±»å‹çš„ä¼ªæ³¨é‡Šã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—åœ¨AbdomenAtlas 1.1ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ç”¨äºè¿ç§»å­¦ä¹ ã€‚æˆ‘ä»¬çš„åˆæ­¥åˆ†æè¡¨æ˜ï¼Œä»…ä½¿ç”¨21ä¸ªCTä½“ç§¯ã€672ä¸ªæ©ç å’Œ40ä¸ªGPUå°æ—¶è®­ç»ƒçš„æ¨¡å‹å…·æœ‰ä¸ä½¿ç”¨å¤§é‡æœªæ ‡è®°CTä½“ç§¯å’Œå¤§é‡GPUå°æ—¶è®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç›‘ç£æ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›å¯ä»¥éšç€æ›´å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„å¢åŠ è€Œè¿›ä¸€æ­¥æå‡ï¼Œæ— è®ºå…¶é¢„è®­ç»ƒæ–¹æ³•è®ºæˆ–æ•°æ®æ¥æºå¦‚ä½•ï¼Œéƒ½èƒ½å®ç°æ¯”ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½ä¿ƒè¿›æ„å»ºæ›´å¤§è§„æ¨¡çš„ä¸‰ç»´åŒ»å­¦æ•°æ®é›†å’Œæ›´å¤šç›‘ç£é¢„è®­ç»ƒæ¨¡å‹çš„å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11253v1">PDF</a> Accepted to ICLR-2024</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„é¢„è®­ç»ƒå’Œå¾®è°ƒèŒƒå¼é¢ä¸´çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆã€‚ç”±äºImageNeté¢„è®­ç»ƒä¸»è¦é’ˆå¯¹äºŒç»´å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå¯¹äºä¸‰ç»´å›¾åƒåˆ†å‰²ç­‰æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå…¶æ€§èƒ½ä¼šå—åˆ°å½±å“ã€‚ä¸ºè§£å†³ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨çš„3Dæ•°æ®é›†çš„é—®é¢˜ï¼Œæœ¬æ–‡æ„å»ºäº†AbdomenAtlas 1.1æ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†ä¸€ç³»åˆ—åœ¨è¯¥æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚åˆæ­¥åˆ†æè¡¨æ˜ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œè¿™äº›æ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ä¸åœ¨å¤§é‡æ— æ ‡ç­¾æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰©å¤§æ ‡æ³¨æ•°æ®é›†ï¼Œç›‘ç£æ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›æœ‰æœ›è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒå’Œå¾®è°ƒèŒƒå¼åœ¨è¿ç§»å­¦ä¹ ä¸­å æ®é‡è¦åœ°ä½ï¼Œå¯¹äºåŒ»å­¦å›¾åƒåˆ†æä¹Ÿæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ImageNeté¢„è®­ç»ƒåœ¨åŒ»å­¦å›¾åƒé¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸‰ç»´å›¾åƒåˆ†å‰²ç­‰å¤æ‚ä»»åŠ¡æ—¶ã€‚</li>
<li>ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨çš„3Dæ•°æ®é›†æ˜¯é™åˆ¶è¿ç§»å­¦ä¹ åº”ç”¨çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚</li>
<li>AbdomenAtlas 1.1æ•°æ®é›†çš„æ„å»ºä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆï¼ŒåŒ…å«é«˜è´¨é‡çš„ä¸‰ç»´åŒ»å­¦å›¾åƒæ•°æ®å’Œè¯¦ç»†çš„æ ‡æ³¨ä¿¡æ¯ã€‚</li>
<li>åœ¨AbdomenAtlas 1.1ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›åˆæ­¥å¾—åˆ°éªŒè¯ï¼Œå³ä½¿åœ¨è¾ƒå°‘çš„æ•°æ®å’Œè®¡ç®—èµ„æºä¸‹ä¹Ÿèƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>ç›‘ç£æ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›æœ‰æœ›é€šè¿‡æ‰©å¤§æ ‡æ³¨æ•°æ®é›†è¿›ä¸€æ­¥æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cc499e4452af0fc875d84fd95e31e88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bb9b40a078efcb38e91be309c08ecae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f09d974025068e7adb45c4451924dfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-101d684fc979bbc0a5acc8d3ce746ce1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92171ccab7013644784ef001548c8487.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Finding-Reproducible-and-Prognostic-Radiomic-Features-in-Variable-Slice-Thickness-Contrast-Enhanced-CT-of-Colorectal-Liver-Metastases"><a href="#Finding-Reproducible-and-Prognostic-Radiomic-Features-in-Variable-Slice-Thickness-Contrast-Enhanced-CT-of-Colorectal-Liver-Metastases" class="headerlink" title="Finding Reproducible and Prognostic Radiomic Features in Variable Slice   Thickness Contrast Enhanced CT of Colorectal Liver Metastases"></a>Finding Reproducible and Prognostic Radiomic Features in Variable Slice   Thickness Contrast Enhanced CT of Colorectal Liver Metastases</h2><p><strong>Authors:Jacob J. Peoples, Mohammad Hamghalam, Imani James, Maida Wasim, Natalie Gangai, Hyunseon Christine Kang, X. John Rong, Yun Shin Chun, Richard K. G. Do, Amber L. Simpson</strong></p>
<p>Establishing the reproducibility of radiomic signatures is a critical step in the path to clinical adoption of quantitative imaging biomarkers; however, radiomic signatures must also be meaningfully related to an outcome of clinical importance to be of value for personalized medicine. In this study, we analyze both the reproducibility and prognostic value of radiomic features extracted from the liver parenchyma and largest liver metastases in contrast enhanced CT scans of patients with colorectal liver metastases (CRLM). A prospective cohort of 81 patients from two major US cancer centers was used to establish the reproducibility of radiomic features extracted from images reconstructed with different slice thicknesses. A publicly available, single-center cohort of 197 preoperative scans from patients who underwent hepatic resection for treatment of CRLM was used to evaluate the prognostic value of features and models to predict overall survival. A standard set of 93 features was extracted from all images, with a set of eight different extractor settings. The feature extraction settings producing the most reproducible, as well as the most prognostically discriminative feature values were highly dependent on both the region of interest and the specific feature in question. While the best overall predictive model was produced using features extracted with a particular setting, without accounting for reproducibility, (C-index &#x3D; 0.630 (0.603â€“0.649)) an equivalent-performing model (C-index &#x3D; 0.629 (0.605â€“0.645)) was produced by pooling features from all extraction settings, and thresholding features with low reproducibility ($\mathrm{CCC} \geq 0.85$), prior to feature selection. Our findings support a data-driven approach to feature extraction and selection, preferring the inclusion of many features, and narrowing feature selection based on reproducibility when relevant data is available. </p>
<blockquote>
<p>å»ºç«‹æ”¾å°„ç»„å­¦ç‰¹å¾çš„å¯é‡å¤æ€§æ˜¯åœ¨ä¸´åºŠé‡‡ç”¨å®šé‡æˆåƒç”Ÿç‰©æ ‡å¿—ç‰©é“è·¯ä¸Šçš„å…³é”®æ­¥éª¤ã€‚ç„¶è€Œï¼Œæ”¾å°„ç»„å­¦ç‰¹å¾å¿…é¡»ä¸å…·æœ‰ä¸´åºŠé‡è¦æ€§çš„ç»“æœæœ‰æ„ä¹‰åœ°ç›¸å…³è”ï¼Œæ‰èƒ½å¯¹ä¸ªæ€§åŒ–åŒ»ç–—å…·æœ‰ä»·å€¼ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†ä»æ‚£æœ‰ç»“ç›´è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰æ‚£è€…çš„å¢å¼ºCTæ‰«æä¸­æå–çš„è‚è„å®è´¨å’Œæœ€å¤§è‚è½¬ç§»ç¶çš„æ”¾å°„å­¦ç‰¹å¾çš„å¯é‡å¤æ€§å’Œé¢„åä»·å€¼ã€‚ä½¿ç”¨æ¥è‡ªç¾å›½ä¸¤ä¸ªä¸»è¦ç™Œç—‡ä¸­å¿ƒçš„81åæ‚£è€…çš„å‰ç»æ€§é˜Ÿåˆ—ï¼Œä»¥å»ºç«‹ä»ä¸åŒåˆ‡ç‰‡åšåº¦é‡å»ºçš„å›¾åƒä¸­æå–çš„æ”¾å°„å­¦ç‰¹å¾çš„å¯é‡å¤æ€§ã€‚è¿˜ä½¿ç”¨äº†å…¬å¼€å¯ç”¨çš„ã€å•ä¸­å¿ƒé˜Ÿåˆ—çš„197ä¾‹æœ¯å‰æ‰«æå›¾åƒï¼Œè¿™äº›å›¾åƒæ¥è‡ªæ¥å—CRLMæ²»ç–—è‚åˆ‡é™¤æœ¯çš„æ‚£è€…ï¼Œç”¨äºè¯„ä¼°ç‰¹å¾çš„é¢„åä»·å€¼ä»¥åŠé¢„æµ‹æ€»ä½“ç”Ÿå­˜æœŸçš„æ¨¡å‹ã€‚ä»æ‰€æœ‰å›¾åƒä¸­æå–äº†ä¸€å¥—93ä¸ªç‰¹å¾ï¼Œä½¿ç”¨å…«ç§ä¸åŒçš„æå–å™¨è®¾ç½®ã€‚äº§ç”Ÿæœ€å¯é‡å¤ä»¥åŠæœ€å…·é¢„åé‰´åˆ«åŠ›çš„ç‰¹å¾å€¼çš„ç‰¹å¾æå–è®¾ç½®é«˜åº¦ä¾èµ–äºæ„Ÿå…´è¶£åŒºåŸŸå’Œç‰¹å®šç‰¹å¾ã€‚è™½ç„¶ä½¿ç”¨ç‰¹å®šè®¾ç½®æå–çš„ç‰¹å¾äº§ç”Ÿäº†æœ€ä½³çš„é¢„æµ‹æ¨¡å‹ï¼Œä½†åœ¨ä¸è€ƒè™‘å¯é‡å¤æ€§çš„æƒ…å†µä¸‹ï¼Œï¼ˆCæŒ‡æ•°ä¸º0.630ï¼ˆ0.603-0.649ï¼‰ï¼‰ä¸€ä¸ªè¡¨ç°ç›¸å½“çš„æ¨¡å‹ï¼ˆCæŒ‡æ•°ä¸º0.629ï¼ˆ0.605-0.645ï¼‰ï¼‰æ˜¯é€šè¿‡æ±‡é›†æ‰€æœ‰æå–è®¾ç½®çš„ç‰¹å¾ï¼Œå¹¶åœ¨ç‰¹å¾é€‰æ‹©ä¹‹å‰å¯¹ä½å¯é‡å¤æ€§çš„ç‰¹å¾è¿›è¡Œé˜ˆå€¼å¤„ç†ï¼ˆCCCâ‰¥0.85ï¼‰è€Œäº§ç”Ÿçš„ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ”¯æŒæ•°æ®é©±åŠ¨çš„ç‰¹å¾æå–å’Œé€‰æ‹©æ–¹æ³•ï¼Œæ›´å€¾å‘äºåŒ…å«è®¸å¤šç‰¹å¾ï¼Œå¹¶åœ¨æœ‰ç›¸å…³æ•°æ®çš„æƒ…å†µä¸‹åŸºäºå¯é‡å¤æ€§è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11221v1">PDF</a> Accepted for publication at the Journal of Machine Learning for   Biomedical Imaging (MELBA) <a target="_blank" rel="noopener" href="https://melba-journal.org/2024:032">https://melba-journal.org/2024:032</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶åˆ†æäº†ä»ç»“ç›´è‚ ç™Œè‚è½¬ç§»æ‚£è€…ï¼ˆCRLMï¼‰çš„å¯¹æ¯”å¢å¼ºCTæ‰«æä¸­æå–çš„è‚å®è´¨å’Œæœ€å¤§è‚è½¬ç§»ç—…ç¶çš„æ”¾å°„ç»„å­¦ç‰¹å¾çš„å†ç°æ€§å’Œé¢„åä»·å€¼ã€‚é€šè¿‡å¯¹ä¸åŒåˆ‡ç‰‡åšåº¦é‡å»ºçš„å›¾åƒè¿›è¡Œç‰¹å¾æå–ï¼Œå»ºç«‹äº†æ”¾å°„å­¦ç‰¹å¾çš„å†ç°æ€§ã€‚åˆ©ç”¨å…¬å¼€çš„å•ä¸­å¿ƒé˜Ÿåˆ—æ•°æ®è¯„ä¼°ç‰¹å¾é¢„åä»·å€¼å¹¶å»ºç«‹é¢„æµ‹æ€»ä½“ç”Ÿå­˜ç‡çš„æ¨¡å‹ã€‚ç»“æœå¼ºè°ƒäº†ç‰¹å¾æå–è®¾ç½®çš„æœ€ä½³é€‰æ‹©å’Œåˆ©ç”¨å¯å†ç°æ€§é«˜çš„ç‰¹å¾å€¼çš„é‡è¦æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶æ”¯æŒæ•°æ®é©±åŠ¨çš„ç‰¹å¾æå–å’Œé€‰æ‹©æ–¹æ³•ï¼Œå€¾å‘äºåŒ…å«æ›´å¤šç‰¹å¾ï¼Œå¹¶åœ¨å¿…è¦æ—¶åŸºäºå¯å†ç°æ€§ç¼©å°ç‰¹å¾é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„ç»„å­¦ç‰¹å¾çš„å†ç°æ€§å’Œä¸ä¸´åºŠé‡è¦ç»“æœçš„ç›¸å…³æ€§å¯¹äºä¸ªæ€§åŒ–åŒ»å­¦å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ†æäº†ä»ç»“ç›´è‚ ç™Œè‚è½¬ç§»æ‚£è€…çš„å¯¹æ¯”å¢å¼ºCTæ‰«æä¸­æå–çš„è‚å®è´¨å’Œè‚è½¬ç§»ç¶çš„æ”¾å°„ç»„å­¦ç‰¹å¾ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸åŒåˆ‡ç‰‡åšåº¦çš„å›¾åƒé‡å»ºæ¥å»ºç«‹æ”¾å°„å­¦ç‰¹å¾çš„å†ç°æ€§ã€‚</li>
<li>ä½¿ç”¨å…¬å¼€æ•°æ®è¯„ä¼°ç‰¹å¾çš„é¢„åä»·å€¼å¹¶å»ºç«‹é¢„æµ‹æ€»ä½“ç”Ÿå­˜ç‡çš„æ¨¡å‹ã€‚</li>
<li>ç‰¹å¾æå–è®¾ç½®çš„æœ€ä½³é€‰æ‹©ä¸å…·ä½“å…³æ³¨çš„ç‰¹å¾å’Œæ„Ÿå…´è¶£åŒºåŸŸæœ‰å…³ã€‚</li>
<li>ç ”ç©¶æ”¯æŒæ•°æ®é©±åŠ¨çš„ç‰¹å¾æå–å’Œé€‰æ‹©æ–¹æ³•ï¼Œå€¾å‘äºåŒ…å«æ›´å¤šç‰¹å¾ï¼Œå¹¶æ ¹æ®éœ€è¦åŸºäºå¯å†ç°æ€§è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3329d3e8ef76ad31a1da036c6545eff5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d30d5cc7ed6495fe8bb04fa3f60e1ee6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Enhancing-Brain-Tumor-Segmentation-Using-Channel-Attention-and-Transfer-learning"><a href="#Enhancing-Brain-Tumor-Segmentation-Using-Channel-Attention-and-Transfer-learning" class="headerlink" title="Enhancing Brain Tumor Segmentation Using Channel Attention and Transfer   learning"></a>Enhancing Brain Tumor Segmentation Using Channel Attention and Transfer   learning</h2><p><strong>Authors:Majid Behzadpour, Ebrahim Azizi, Kai Wu, Bengie L. Ortiz</strong></p>
<p>Accurate and efficient segmentation of brain tumors is critical for diagnosis, treatment planning, and monitoring in clinical practice. In this study, we present an enhanced ResUNet architecture for automatic brain tumor segmentation, integrating an EfficientNetB0 encoder, a channel attention mechanism, and an Atrous Spatial Pyramid Pooling (ASPP) module. The EfficientNetB0 encoder leverages pre-trained features to improve feature extraction efficiency, while the channel attention mechanism enhances the modelâ€™s focus on tumor-relevant features. ASPP enables multiscale contextual learning, crucial for handling tumors of varying sizes and shapes. The proposed model was evaluated on two benchmark datasets: TCGA LGG and BraTS 2020. Experimental results demonstrate that our method consistently outperforms the baseline ResUNet and its EfficientNet variant, achieving Dice coefficients of 0.903 and 0.851 and HD95 scores of 9.43 and 3.54 for whole tumor and tumor core regions on the BraTS 2020 dataset, respectively. compared with state-of-the-art methods, our approach shows competitive performance, particularly in whole tumor and tumor core segmentation. These results indicate that combining a powerful encoder with attention mechanisms and ASPP can significantly enhance brain tumor segmentation performance. The proposed approach holds promise for further optimization and application in other medical image segmentation tasks. </p>
<blockquote>
<p>åœ¨ä¸´åºŠå®è·µä¸­ï¼Œè„‘è‚¿ç˜¤çš„ç²¾ç¡®å’Œé«˜æ•ˆåˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’åˆ¶å®šå’Œç›‘æµ‹è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºå‹ResUNetæ¶æ„ï¼Œç”¨äºè‡ªåŠ¨è„‘è‚¿ç˜¤åˆ†å‰²ï¼Œè¯¥æ¶æ„é›†æˆäº†EfficientNetB0ç¼–ç å™¨ã€é€šé“æ³¨æ„æœºåˆ¶å’Œç©ºæ´ç©ºé—´é‡‘å­—å¡”æ± åŒ–ï¼ˆASPPï¼‰æ¨¡å—ã€‚EfficientNetB0ç¼–ç å™¨åˆ©ç”¨é¢„è®­ç»ƒç‰¹å¾æ¥æé«˜ç‰¹å¾æå–æ•ˆç‡ï¼Œè€Œé€šé“æ³¨æ„æœºåˆ¶åˆ™å¢å¼ºäº†æ¨¡å‹å¯¹è‚¿ç˜¤ç›¸å…³ç‰¹å¾çš„å…³æ³¨ã€‚ASPPæ¨¡å—å®ç°äº†å¤šå°ºåº¦ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¯¹äºå¤„ç†ä¸åŒå¤§å°å’Œå½¢çŠ¶çš„è‚¿ç˜¤è‡³å…³é‡è¦ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨TCGA LGGå’ŒBraTS 2020ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºåŸºçº¿ResUNetåŠå…¶EfficientNetå˜ä½“ï¼Œåœ¨BraTS 2020æ•°æ®é›†ä¸Šçš„æ•´ä¸ªè‚¿ç˜¤å’Œè‚¿ç˜¤æ ¸å¿ƒåŒºåŸŸåˆ†åˆ«å®ç°äº†Diceç³»æ•°0.903å’Œ0.851ä»¥åŠHD95å¾—åˆ†åˆ†åˆ«ä¸º9.43å’Œ3.54ã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•´ä½“è‚¿ç˜¤å’Œè‚¿ç˜¤æ ¸å¿ƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†å¼ºå¤§çš„ç¼–ç å™¨ä¸æ³¨æ„æœºåˆ¶å’ŒASPPç›¸ç»“åˆå¯ä»¥æ˜¾è‘—æé«˜è„‘è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨è¿›ä¸€æ­¥ä¼˜åŒ–å’Œå…¶ä»–åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11196v1">PDF</a> 13 pages, 1 figure</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§å¢å¼ºå‹ResUNetæ¶æ„ï¼Œç”¨äºè‡ªåŠ¨è„‘è‚¿ç˜¤åˆ†å‰²ã€‚è¯¥æ¶æ„ç»“åˆäº†EfficientNetB0ç¼–ç å™¨ã€é€šé“æ³¨æ„åŠ›æœºåˆ¶å’Œç©ºæ´ç©ºé—´é‡‘å­—å¡”æ± åŒ–ï¼ˆASPPï¼‰æ¨¡å—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨TCGA LGGå’ŒBraTS 2020ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾ƒåŸºçº¿ResUNetåŠå…¶EfficientNetå˜ä½“è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨BraTS 2020æ•°æ®é›†ä¸Šçš„å…¨è‚¿ç˜¤å’Œè‚¿ç˜¤æ ¸å¿ƒåŒºåˆ†å‰²æ–¹é¢ã€‚è¯¥ç ”ç©¶ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–å’Œå…¶ä»–åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„åº”ç”¨æä¾›äº†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºå‹ResUNetæ¶æ„ç”¨äºè‡ªåŠ¨è„‘è‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>è¯¥æ¶æ„ç»“åˆäº†EfficientNetB0ç¼–ç å™¨ã€é€šé“æ³¨æ„åŠ›æœºåˆ¶å’ŒASPPæ¨¡å—ã€‚</li>
<li>EfficientNetB0ç¼–ç å™¨åˆ©ç”¨é¢„è®­ç»ƒç‰¹å¾æé«˜ç‰¹å¾æå–æ•ˆç‡ã€‚</li>
<li>é€šé“æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºäº†æ¨¡å‹å¯¹è‚¿ç˜¤ç›¸å…³ç‰¹å¾çš„å…³æ³¨ã€‚</li>
<li>ASPPæ¨¡å—å®ç°äº†å¤šå°ºåº¦ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¯¹äºå¤„ç†ä¸åŒå¤§å°å’Œå½¢çŠ¶çš„è‚¿ç˜¤è‡³å…³é‡è¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨TCGA LGGå’ŒBraTS 2020æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¾ƒåŸºçº¿ResUNetåŠå…¶EfficientNetå˜ä½“è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3174b658cbbbae29b7e7bd3b9b04a012.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Transfer-Learning-Strategies-for-Pathological-Foundation-Models-A-Systematic-Evaluation-in-Brain-Tumor-Classification"><a href="#Transfer-Learning-Strategies-for-Pathological-Foundation-Models-A-Systematic-Evaluation-in-Brain-Tumor-Classification" class="headerlink" title="Transfer Learning Strategies for Pathological Foundation Models: A   Systematic Evaluation in Brain Tumor Classification"></a>Transfer Learning Strategies for Pathological Foundation Models: A   Systematic Evaluation in Brain Tumor Classification</h2><p><strong>Authors:Ken Enda, Yoshitaka Oda, Zen-ichi Tanei, Wang Lei, Masumi Tsuda, Takahiro Ogawa, Shinya Tanaka</strong></p>
<p>Foundation models pretrained on large-scale pathology datasets have shown promising results across various diagnostic tasks. Here, we present a systematic evaluation of transfer learning strategies for brain tumor classification using these models. We analyzed 252 cases comprising five major tumor types: glioblastoma, astrocytoma, oligodendroglioma, primary central nervous system lymphoma, and metastatic tumors. Comparing state-of-the-art foundation models with conventional approaches, we found that foundation models demonstrated robust classification performance with as few as 10 patches per case, challenging the traditional assumption that extensive per-case image sampling is necessary. Furthermore, our evaluation revealed that simple transfer learning strategies like linear probing were sufficient, while fine-tuning often degraded model performance. These findings suggest a paradigm shift from extensive data collection to efficient utilization of pretrained features, providing practical implications for implementing AI-assisted diagnosis in clinical pathology. </p>
<blockquote>
<p>åŸºäºå¤§è§„æ¨¡ç—…ç†å­¦æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒçš„åŸºçŸ³æ¨¡å‹åœ¨å„ç§è¯Šæ–­ä»»åŠ¡ä¸­å·²æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨è¿™äº›æ¨¡å‹è¿›è¡Œè„‘è‚¿ç˜¤åˆ†ç±»çš„è¿ç§»å­¦ä¹ ç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æˆ‘ä»¬åˆ†æäº†åŒ…å«äº”ç§ä¸»è¦è‚¿ç˜¤ç±»å‹çš„252ä¸ªç—…ä¾‹ï¼šèƒ¶è´¨æ¯ç»†èƒç˜¤ã€æ˜Ÿå½¢ç»†èƒç˜¤ã€å°‘çªèƒ¶è´¨ç»†èƒç˜¤ã€åŸå‘æ€§ä¸­æ¢ç¥ç»ç³»ç»Ÿæ·‹å·´ç˜¤å’Œè½¬ç§»æ€§è‚¿ç˜¤ã€‚é€šè¿‡æ¯”è¾ƒæœ€æ–°çš„åŸºçŸ³æ¨¡å‹ä¸ä¼ ç»Ÿæ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°åŸºçŸ³æ¨¡å‹åœ¨æ¯ä¸ªç—…ä¾‹ä»…éœ€10ä¸ªè¡¥ä¸çš„æƒ…å†µä¸‹å°±è¡¨ç°å‡ºäº†ç¨³å¥çš„åˆ†ç±»æ€§èƒ½ï¼Œè¿™æŒ‘æˆ˜äº†ä¼ ç»Ÿå‡è®¾ï¼Œå³æ¯ä¸ªç—…ä¾‹çš„å¹¿æ³›å›¾åƒé‡‡æ ·æ˜¯å¿…è¦çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œç®€å•çš„è¿ç§»å­¦ä¹ ç­–ç•¥å¦‚çº¿æ€§æ¢æµ‹å°±å·²ç»è¶³å¤Ÿï¼Œè€Œå¾®è°ƒå¾€å¾€ä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚è¿™äº›å‘ç°é¢„ç¤ºç€ä»å¹¿æ³›çš„æ•°æ®æ”¶é›†åˆ°æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒç‰¹å¾çš„èŒƒå¼è½¬å˜ï¼Œä¸ºåœ¨ä¸´åºŠç—…ç†å­¦ä¸­å®ç°äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­æä¾›äº†å®é™…å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11014v1">PDF</a> 25 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡ç—…ç†å­¦æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒçš„åŸºé‡‘æ¨¡å‹åœ¨å„ç§è¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚æœ¬æ–‡å¯¹ç”¨äºè„‘è‚¿ç˜¤åˆ†ç±»çš„åŸºé‡‘æ¨¡å‹è¿ç§»å­¦ä¹ ç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼ŒåŸºé‡‘æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬ä¸‹å±•ç°å‡ºå¼ºå¤§çš„åˆ†ç±»æ€§èƒ½ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿè®¤çŸ¥ä¸­å¯¹æ¯ä¾‹ç—…ä¾‹å¹¿æ³›é‡‡é›†å›¾åƒçš„å‡è®¾ã€‚æ­¤å¤–ï¼Œç®€å•è¿ç§»å­¦ä¹ ç­–ç•¥å¦‚çº¿æ€§æ¢æŸ¥æ³•å°±å·²è¶³å¤Ÿï¼Œè€Œå¾®è°ƒç­–ç•¥å¾€å¾€ä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚è¿™äº›å‘ç°é¢„ç¤ºç€ä»å¤§é‡æ•°æ®é‡‡é›†è½¬å‘æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒç‰¹å¾çš„è½¬å˜ï¼Œä¸ºä¸´åºŠç—…ç†å­¦å®æ–½äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­æä¾›äº†å®é™…å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºé‡‘æ¨¡å‹åœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>å°‘é‡æ ·æœ¬ä¸‹åŸºé‡‘æ¨¡å‹å³å¯å®ç°å¼ºå¤§çš„åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>ç®€å•è¿ç§»å­¦ä¹ ç­–ç•¥å¦‚çº¿æ€§æ¢æŸ¥æ³•æœ‰æ•ˆï¼Œè€Œå¾®è°ƒç­–ç•¥å¯èƒ½é™ä½æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ‰“ç ´ä¼ ç»Ÿè®¤çŸ¥ä¸­å¯¹æ¯ä¾‹ç—…ä¾‹å¹¿æ³›é‡‡é›†å›¾åƒçš„å‡è®¾ã€‚</li>
<li>ä»å¤§é‡æ•°æ®é‡‡é›†è½¬å‘æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒç‰¹å¾ã€‚</li>
<li>åŸºé‡‘æ¨¡å‹çš„åº”ç”¨å¯¹ä¸´åºŠç—…ç†å­¦å®æ–½äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­å…·æœ‰å®é™…å¯ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0aae9b2ddc669ce99c18ea634301621.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="An-analysis-of-the-combination-of-feature-selection-and-machine-learning-methods-for-an-accurate-and-timely-detection-of-lung-cancer"><a href="#An-analysis-of-the-combination-of-feature-selection-and-machine-learning-methods-for-an-accurate-and-timely-detection-of-lung-cancer" class="headerlink" title="An analysis of the combination of feature selection and machine learning   methods for an accurate and timely detection of lung cancer"></a>An analysis of the combination of feature selection and machine learning   methods for an accurate and timely detection of lung cancer</h2><p><strong>Authors:Omid Shahriyar, Babak Nuri Moghaddam, Davoud Yousefi, Abbas Mirzaei, Farnaz Hoseini</strong></p>
<p>One of the deadliest cancers, lung cancer necessitates an early and precise diagnosis. Because patients have a better chance of recovering, early identification of lung cancer is crucial. This review looks at how to diagnose lung cancer using sophisticated machine learning techniques like Random Forest (RF) and Support Vector Machine (SVM). The Chi-squared test is one feature selection strategy that has been successfully applied to find related features and enhance model performance. The findings demonstrate that these techniques can improve detection efficiency and accuracy while also assisting in runtime reduction. This study produces recommendations for further research as well as ideas to enhance diagnostic techniques. In order to improve healthcare and create automated methods for detecting lung cancer, this research is a critical first step. </p>
<blockquote>
<p>è‚ºç™Œæ˜¯æœ€è‡´å‘½çš„ç™Œç—‡ä¹‹ä¸€ï¼Œéœ€è¦è¿›è¡Œæ—©æœŸå’Œç²¾ç¡®çš„è¯Šæ–­ã€‚ç”±äºæ‚£è€…åº·å¤çš„æœºä¼šæ›´å¤§ï¼Œå› æ­¤æ—©æœŸå‘ç°è‚ºç™Œè‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨éšæœºæ£®æ—ï¼ˆRFï¼‰å’Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ç­‰å…ˆè¿›çš„æœºå™¨å­¦ä¹ æŠ€æœ¯æ¥è¯Šæ–­è‚ºç™Œã€‚å¡æ–¹æ£€éªŒæ˜¯ä¸€ç§ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œå·²æˆåŠŸåº”ç”¨äºå¯»æ‰¾ç›¸å…³ç‰¹å¾å¹¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥æé«˜æ£€æµ‹æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶æœ‰åŠ©äºå‡å°‘è¿è¡Œæ—¶é—´ã€‚æœ¬ç ”ç©¶ä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶æä¾›äº†å»ºè®®ï¼Œå¹¶ä¸ºæ”¹è¿›è¯Šæ–­æŠ€æœ¯æä¾›äº†æ€è·¯ã€‚ä¸ºäº†æ”¹å–„åŒ»ç–—ä¿å¥å¹¶åˆ›å»ºç”¨äºæ£€æµ‹è‚ºç™Œçš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œè¿™é¡¹ç ”ç©¶æ˜¯è‡³å…³é‡è¦çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10980v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è‚ºç™Œä½œä¸ºè‡´å‘½çš„ç™Œç—‡ä¹‹ä¸€ï¼Œæ—©æœŸç²¾å‡†è¯Šæ–­å¯¹äºæ‚£è€…çš„åº·å¤è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç»¼è¿°äº†åˆ©ç”¨éšæœºæ£®æ—ï¼ˆRFï¼‰å’Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ç­‰å…ˆè¿›æœºå™¨å­¦ä¹ æŠ€æœ¯è¯Šæ–­è‚ºç™Œçš„æ–¹æ³•ã€‚Chiå¹³æ–¹æ£€éªŒä½œä¸ºä¸€ç§ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œå·²æˆåŠŸåº”ç”¨äºå¯»æ‰¾ç›¸å…³ç‰¹å¾ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥æé«˜æ£€æµ‹æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶æœ‰åŠ©äºå‡å°‘è¿è¡Œæ—¶é—´ã€‚æœ¬æ–‡ä¸ä»…ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å»ºè®®ï¼Œè¿˜æå‡ºäº†æ”¹è¿›è¯Šæ–­æŠ€æœ¯çš„æƒ³æ³•ã€‚è¿™é¡¹ç ”ç©¶å¯¹äºæé«˜åŒ»ç–—ä¿å¥å’Œåˆ›å»ºè‚ºç™Œè‡ªåŠ¨æ£€æµ‹æ–¹æ³•æ˜¯å…³é”®çš„ç¬¬ä¸€æ­¥ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è‚ºç™Œçš„æ—©æœŸè¯Šæ–­å¯¹æ‚£è€…çš„åº·å¤è‡³å…³é‡è¦ã€‚</li>
<li>æœºå™¨å­¦ä¹ æŠ€æœ¯å¦‚éšæœºæ£®æ—å’Œæ”¯æŒå‘é‡æœºåœ¨è‚ºç™Œè¯Šæ–­ä¸­å…·æœ‰åº”ç”¨ä»·å€¼ã€‚</li>
<li>Chiå¹³æ–¹æ£€éªŒæ˜¯ä¸€ç§æœ‰æ•ˆçš„ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œèƒ½æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æœºå™¨å­¦ä¹ æŠ€æœ¯å¯ä»¥æé«˜è‚ºç™Œæ£€æµ‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¿™äº›æŠ€æœ¯æœ‰åŠ©äºå‡å°‘è¯Šæ–­è¿‡ç¨‹çš„è¿è¡Œæ—¶é—´ã€‚</li>
<li>ç ”ç©¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å»ºè®®å’Œæƒ³æ³•ï¼Œä»¥æ”¹è¿›è¯Šæ–­æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e061f124de0a3606adcbddda6b086b4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfa69466c7b3d1de32a55b3ae3b75f27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f348afa6b3f65ac5533c4e7694024693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dacfd0dc1866d9cf30ab26e3129e960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35844ed7aaf129d02f7edbea1c83bf90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adc29095e1ddd41b3917aadcceb19ac3.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Rethinking-Early-Fusion-Strategies-for-Improved-Multimodal-Image-Segmentation"><a href="#Rethinking-Early-Fusion-Strategies-for-Improved-Multimodal-Image-Segmentation" class="headerlink" title="Rethinking Early-Fusion Strategies for Improved Multimodal Image   Segmentation"></a>Rethinking Early-Fusion Strategies for Improved Multimodal Image   Segmentation</h2><p><strong>Authors:Zhengwen Shen, Yulian Li, Han Zhang, Yuchen Weng, Jun Wang</strong></p>
<p>RGB and thermal image fusion have great potential to exhibit improved semantic segmentation in low-illumination conditions. Existing methods typically employ a two-branch encoder framework for multimodal feature extraction and design complicated feature fusion strategies to achieve feature extraction and fusion for multimodal semantic segmentation. However, these methods require massive parameter updates and computational effort during the feature extraction and fusion. To address this issue, we propose a novel multimodal fusion network (EFNet) based on an early fusion strategy and a simple but effective feature clustering for training efficient RGB-T semantic segmentation. In addition, we also propose a lightweight and efficient multi-scale feature aggregation decoder based on Euclidean distance. We validate the effectiveness of our method on different datasets and outperform previous state-of-the-art methods with lower parameters and computation. </p>
<blockquote>
<p>RGBä¸çƒ­æˆåƒèåˆåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹å±•ç°å‡ºå·¨å¤§çš„è¯­ä¹‰åˆ†å‰²æ½œåŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨åŒåˆ†æ”¯ç¼–ç å™¨æ¡†æ¶è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾æå–ï¼Œå¹¶è®¾è®¡å¤æ‚çš„ç‰¹å¾èåˆç­–ç•¥æ¥å®ç°å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²çš„ç‰¹å¾æå–å’Œèåˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ç‰¹å¾æå–å’Œèåˆè¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„å‚æ•°æ›´æ–°å’Œè®¡ç®—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ—©æœŸèåˆç­–ç•¥å’Œç®€å•æœ‰æ•ˆçš„ç‰¹å¾èšç±»çš„æ–°å‹å¤šæ¨¡æ€èåˆç½‘ç»œï¼ˆEFNetï¼‰ï¼Œç”¨äºè®­ç»ƒé«˜æ•ˆçš„RGB-Tè¯­ä¹‰åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºæ¬§æ°è·ç¦»çš„è½»é‡çº§é«˜æ•ˆå¤šå°ºåº¦ç‰¹å¾èšåˆè§£ç å™¨ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä»¥æ›´ä½çš„å‚æ•°å’Œè®¡ç®—é‡è¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10958v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong><br>    èåˆRGBå’Œçƒ­æˆåƒæŠ€æœ¯å¯æ˜¾è‘—æé«˜ä½å…‰ç…§æ¡ä»¶ä¸‹çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨åŒåˆ†æ”¯ç¼–ç å™¨æ¡†æ¶è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾æå–ï¼Œå¹¶è®¾è®¡å¤æ‚çš„ç‰¹å¾èåˆç­–ç•¥æ¥å®ç°å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²çš„ç‰¹å¾æå–å’Œèåˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ç‰¹å¾æå–å’Œèåˆè¿‡ç¨‹ä¸­éœ€è¦å¤§é‡å‚æ•°æ›´æ–°å’Œè®¡ç®—ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ—©æœŸèåˆç­–ç•¥å’Œç®€å•æœ‰æ•ˆçš„ç‰¹å¾èšç±»çš„å…¨æ–°å¤šæ¨¡æ€èåˆç½‘ç»œï¼ˆEFNetï¼‰ï¼Œä»¥å®ç°é«˜æ•ˆçš„RGB-Tè¯­ä¹‰åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºæ¬§æ°è·ç¦»çš„é«˜æ•ˆè½»é‡çº§å¤šå°ºåº¦ç‰¹å¾èšåˆè§£ç å™¨ã€‚åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå…·æœ‰æ›´ä½çš„å‚æ•°å’Œè®¡ç®—é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RGBä¸çƒ­æˆåƒèåˆåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æé«˜äº†è¯­ä¹‰åˆ†å‰²çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•é‡‡ç”¨åŒåˆ†æ”¯ç¼–ç å™¨æ¡†æ¶å’Œå¤šæ¨¡æ€ç‰¹å¾æå–å¤æ‚èåˆç­–ç•¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡å‚æ•°æ›´æ–°å’Œè®¡ç®—èµ„æºã€‚</li>
<li>æå‡ºäº†åŸºäºæ—©æœŸèåˆç­–ç•¥å’Œç‰¹å¾èšç±»çš„å…¨æ–°å¤šæ¨¡æ€èåˆç½‘ç»œï¼ˆEFNetï¼‰ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é«˜æ•ˆè½»é‡çº§çš„å¤šå°ºåº¦ç‰¹å¾èšåˆè§£ç å™¨ã€‚</li>
<li>åœ¨ä¸åŒæ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´ä½çš„å‚æ•°å’Œè®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b571fdde387cb2b823458db25fc5f18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab1f64eec8252fdd89cb86a83fd34a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f871eda1ca0e331d70388569d7f483f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-802017413759665d97b05728c093cfd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d860e4fbcf43f07b8284aa2719b05a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-584d8cf977279708a7c989e8685a77a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46e3eedf3c542aa6c9a5d5cbeb41abc4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Semi-supervised-Semantic-Segmentation-for-Remote-Sensing-Images-via-Multi-scale-Uncertainty-Consistency-and-Cross-Teacher-Student-Attention"><a href="#Semi-supervised-Semantic-Segmentation-for-Remote-Sensing-Images-via-Multi-scale-Uncertainty-Consistency-and-Cross-Teacher-Student-Attention" class="headerlink" title="Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention"></a>Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention</h2><p><strong>Authors:Shanwen Wang, Changrui Chen, Xin Sun, Danfeng Hong, Jungong Han</strong></p>
<p>Semi-supervised learning offers an appealing solution for remote sensing (RS) image segmentation to relieve the burden of labor-intensive pixel-level labeling. However, RS images pose unique challenges, including rich multi-scale features and high inter-class similarity. To address these problems, this paper proposes a novel semi-supervised Multi-Scale Uncertainty and Cross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation tasks. Specifically, MUCA constrains the consistency among feature maps at different layers of the network by introducing a multi-scale uncertainty consistency regularization. It improves the multi-scale learning capability of semi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a Cross-Teacher-Student attention mechanism to guide the student network, guiding the student network to construct more discriminative feature representations through complementary features from the teacher network. This design effectively integrates weak and strong augmentations (WA and SA) to further boost segmentation performance. To verify the effectiveness of our model, we conduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The experimental results show the superiority of our method over state-of-the-art semi-supervised methods. Notably, our model excels in distinguishing highly similar objects, showcasing its potential for advancing semi-supervised RS image segmentation tasks. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ä¸ºé¥æ„Ÿï¼ˆRSï¼‰å›¾åƒåˆ†å‰²æä¾›äº†ä¸€ä¸ªå¸å¼•äººçš„è§£å†³æ–¹æ¡ˆï¼Œå‡è½»äº†åŠ³åŠ¨å¯†é›†å‹çš„åƒç´ çº§æ ‡ç­¾çš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œé¥æ„Ÿå›¾åƒå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸°å¯Œçš„å¤šå°ºåº¦ç‰¹å¾å’Œé«˜çš„ç±»é—´ç›¸ä¼¼æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„æ–°å‹åŠç›‘ç£å¤šå°ºåº¦ä¸ç¡®å®šæ€§å’Œè·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›ï¼ˆMUCAï¼‰æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒMUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œçº¦æŸç½‘ç»œä¸åŒå±‚ç‰¹å¾å›¾ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®ƒæé«˜äº†åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡è®°æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMUCAåˆ©ç”¨è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶æ¥å¼•å¯¼å­¦ç”Ÿç½‘ç»œï¼Œé€šè¿‡æ•™å¸ˆç½‘ç»œçš„äº’è¡¥ç‰¹å¾æ¥æ„å»ºæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆåœ°ç»“åˆäº†å¼±å¢å¼ºå’Œå¼ºå¢å¼ºï¼ˆWAå’ŒSAï¼‰ï¼Œè¿›ä¸€æ­¥æé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°çš„åŠç›‘ç£æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åŒºåˆ†é«˜åº¦ç›¸ä¼¼ç‰©ä½“æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨è¿›åŠç›‘ç£é¥æ„Ÿå›¾åƒåˆ†å‰²ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10736v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŠç›‘ç£å­¦ä¹ æ˜¯è§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä»»åŠ¡çš„æœ‰æ•ˆæ‰‹æ®µï¼Œå‡å°‘äº†éœ€è¦å¤§é‡æ‰‹åŠ¨åƒç´ çº§åˆ«æ ‡ç­¾çš„éº»çƒ¦ã€‚æœ¬æ–‡é’ˆå¯¹é¥æ„Ÿå›¾åƒå¤šå°ºåº¦ç‰¹å¾ä¸°å¯Œå’Œç±»åˆ«é—´ç›¸ä¼¼åº¦é«˜çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºå¤šå°ºåº¦ä¸ç¡®å®šæ€§å’Œè·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶çš„åŠç›‘ç£MUCAæ¨¡å‹ã€‚MUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæé«˜äº†åŠç›‘ç£ç®—æ³•åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼å­¦ç”Ÿç½‘ç»œæ„å»ºæ›´å…·åˆ¤åˆ«æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–å…ˆè¿›åŠç›‘ç£æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒºåˆ†é«˜åº¦ç›¸ä¼¼ç‰©ä½“æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ æ˜¯é¥æ„Ÿå›¾åƒåˆ†å‰²çš„å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘äº†æ‰‹åŠ¨åƒç´ çº§åˆ«æ ‡ç­¾çš„éœ€æ±‚ã€‚</li>
<li>MUCAæ¨¡å‹é’ˆå¯¹é¥æ„Ÿå›¾åƒçš„å¤šå°ºåº¦ç‰¹å¾å’Œç±»åˆ«é—´é«˜ç›¸ä¼¼åº¦é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
<li>MUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæé«˜äº†åŠç›‘ç£ç®—æ³•åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶æœ‰åŠ©äºå¼•å¯¼å­¦ç”Ÿç½‘ç»œæ„å»ºæ›´å…·åˆ¤åˆ«æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>å®éªŒéªŒè¯MUCAæ¨¡å‹åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>MUCAæ¨¡å‹åœ¨åŒºåˆ†é«˜åº¦ç›¸ä¼¼ç‰©ä½“æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bb74e0a18299ed9ba2f97634cf9b358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61ce95a1382642609abf2b4b986f6af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c45bd3808da93a9462adfab336f102ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca2190f0bafc91633737901d4b2bd05b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85527f50bc9469eda24c3fe44a55a42c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="In-the-Picture-Medical-Imaging-Datasets-Artifacts-and-their-Living-Review"><a href="#In-the-Picture-Medical-Imaging-Datasets-Artifacts-and-their-Living-Review" class="headerlink" title="In the Picture: Medical Imaging Datasets, Artifacts, and their Living   Review"></a>In the Picture: Medical Imaging Datasets, Artifacts, and their Living   Review</h2><p><strong>Authors:Amelia JimÃ©nez-SÃ¡nchez, Natalia-Rozalia Avlona, Sarah de Boer, VÃ­ctor M. Campello, Aasa Feragen, Enzo Ferrante, Melanie Ganz, Judy Wawira Gichoya, Camila GonzÃ¡lez, Steff Groefsema, Alessa Hering, Adam Hulman, Leo Joskowicz, Dovile Juodelyte, Melih Kandemir, Thijs Kooi, Jorge del Pozo LÃ©rida, Livie Yumeng Li, Andre Pacheco, Tim RÃ¤dsch, Mauricio Reyes, ThÃ©o Sourget, Bram van Ginneken, David Wen, Nina Weng, Jack Junchi Xu, Hubert Dariusz ZajÄ…c, Maria A. Zuluaga, Veronika Cheplygina</strong></p>
<p>Datasets play a critical role in medical imaging research, yet issues such as label quality, shortcuts, and metadata are often overlooked. This lack of attention may harm the generalizability of algorithms and, consequently, negatively impact patient outcomes. While existing medical imaging literature reviews mostly focus on machine learning (ML) methods, with only a few focusing on datasets for specific applications, these reviews remain static â€“ they are published once and not updated thereafter. This fails to account for emerging evidence, such as biases, shortcuts, and additional annotations that other researchers may contribute after the dataset is published. We refer to these newly discovered findings of datasets as research artifacts. To address this gap, we propose a living review that continuously tracks public datasets and their associated research artifacts across multiple medical imaging applications. Our approach includes a framework for the living review to monitor data documentation artifacts, and an SQL database to visualize the citation relationships between research artifact and dataset. Lastly, we discuss key considerations for creating medical imaging datasets, review best practices for data annotation, discuss the significance of shortcuts and demographic diversity, and emphasize the importance of managing datasets throughout their entire lifecycle. Our demo is publicly available at <a target="_blank" rel="noopener" href="http://130.226.140.142/">http://130.226.140.142</a>. </p>
<blockquote>
<p>æ•°æ®é›†åœ¨åŒ»å­¦æˆåƒç ”ç©¶ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œç„¶è€Œæ ‡ç­¾è´¨é‡ã€æ·å¾„å’Œå…ƒæ•°æ®ç­‰é—®é¢˜å¾€å¾€è¢«å¿½è§†ã€‚è¿™ç§ç¼ºä¹å…³æ³¨å¯èƒ½ä¼šæŸå®³ç®—æ³•çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿›è€Œå¯¹ç—…äººç»“æœäº§ç”Ÿè´Ÿé¢å½±å“ã€‚å°½ç®¡ç°æœ‰çš„åŒ»å­¦æˆåƒæ–‡çŒ®ç»¼è¿°ä¸»è¦é›†ä¸­åœ¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•ä¸Šï¼Œåªæœ‰å°‘æ•°å…³æ³¨ç‰¹å®šåº”ç”¨çš„æ•°æ®é›†ï¼Œä½†è¿™äº›ç»¼è¿°ä¿æŒé™æ€â€”â€”ä¸€ç»å‘å¸ƒï¼Œå°±ä¸å†æ›´æ–°ã€‚è¿™æœªèƒ½è€ƒè™‘åˆ°æ–°å…´è¯æ®ï¼Œå¦‚åè§ã€æ·å¾„å’Œå…¶ä»–ç ”ç©¶äººå‘˜åœ¨æ•°æ®é›†å‘å¸ƒåå¯èƒ½åšå‡ºçš„é¢å¤–æ³¨é‡Šã€‚æˆ‘ä»¬å°†è¿™äº›æ•°æ®é›†çš„æ–°å‘ç°ç§°ä¸ºç ”ç©¶æ–‡ç‰©ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒç»­å›é¡¾çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè·¨å¤šä¸ªåŒ»å­¦æˆåƒåº”ç”¨è¿½è¸ªå…¬å¼€æ•°æ®é›†åŠå…¶ç›¸å…³çš„ç ”ç©¶æ–‡ç‰©ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªç”¨äºç›‘ç£æ•°æ®æ–‡æ¡£æ–‡ç‰©çš„ç”Ÿæ´»è¯„å®¡æ¡†æ¶ï¼Œä»¥åŠä¸€ä¸ªSQLæ•°æ®åº“ï¼Œä»¥å¯è§†åŒ–ç ”ç©¶æ–‡ç‰©ä¸æ•°æ®é›†ä¹‹é—´çš„å¼•è¯å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åˆ›å»ºåŒ»å­¦æˆåƒæ•°æ®é›†çš„å…³é”®æ³¨æ„äº‹é¡¹ï¼Œå›é¡¾äº†æ•°æ®æ³¨é‡Šçš„æœ€ä½³å®è·µï¼Œè®¨è®ºäº†æ·å¾„å’Œäººå£å¤šæ ·æ€§çš„é‡è¦æ€§ï¼Œå¹¶å¼ºè°ƒäº†æ•´ä¸ªæ•°æ®é›†çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸç®¡ç†çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºå…¬å¼€å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="http://130.226.140.142/">http://130.226.140.142ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10727v1">PDF</a> Manuscript under review</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒç ”ç©¶ä¸­çš„æ•°æ®é›†è‡³å…³é‡è¦ï¼Œä½†æ ‡ç­¾è´¨é‡ã€æ·å¾„å’Œå…ƒæ•°æ®ç­‰é—®é¢˜å¸¸è¢«å¿½è§†ã€‚ç°æœ‰æ–‡çŒ®ç»¼è¿°å¤šå…³æ³¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•ï¼Œä»…å°‘æ•°å…³æ³¨ç‰¹å®šåº”ç”¨çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æŒç»­è¿½è¸ªå¤šä¸ªåŒ»å­¦æˆåƒåº”ç”¨å…¬å¼€æ•°æ®é›†åŠå…¶ç›¸å…³ç ”ç©¶æˆæœçš„â€œåŠ¨æ€ç»¼è¿°â€ã€‚è¯¥ç»¼è¿°åŒ…æ‹¬ç›‘æ§æ•°æ®æ–‡æ¡£æˆæœçš„æ–¹æ³•å’Œå¯è§†åŒ–æ•°æ®é›†ä¸ç ”ç©¶æˆæœé—´å¼•ç”¨å…³ç³»çš„SQLæ•°æ®åº“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†åˆ›å»ºåŒ»å­¦æˆåƒæ•°æ®é›†çš„å…³é”®è€ƒé‡å› ç´ ã€æ•°æ®æ ‡æ³¨çš„æœ€ä½³å®è·µã€æ·å¾„å’Œäººå£å¤šæ ·æ€§çš„é‡è¦æ€§ä»¥åŠæ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­æ•°æ®é›†çš„ç®¡ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é›†åœ¨åŒ»å­¦æˆåƒç ”ç©¶ä¸­å…·æœ‰å…³é”®ä½œç”¨ï¼Œä½†å¸¸å¸¸è¢«å¿½è§†çš„æ˜¯æ ‡ç­¾è´¨é‡ã€æ·å¾„å’Œå…ƒæ•°æ®ç­‰æ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰åŒ»å­¦æˆåƒæ–‡çŒ®ç»¼è¿°å¤§å¤šé™æ€ï¼Œæ— æ³•æ¶µç›–æ–°å…´è¯æ®ï¼Œå¦‚åè§ã€æ·å¾„å’Œå…¶ä»–ç ”ç©¶äººå‘˜åœ¨æ•°æ®é›†å‘å¸ƒåçš„é™„åŠ æ³¨é‡Šã€‚</li>
<li>æå‡ºçš„â€œåŠ¨æ€ç»¼è¿°â€æ—¨åœ¨æŒç»­è¿½è¸ªå…¬å¼€æ•°æ®é›†åŠå…¶ç›¸å…³ç ”ç©¶æˆæœã€‚</li>
<li>åŠ¨æ€ç»¼è¿°åŒ…æ‹¬ç›‘æ§æ•°æ®æ–‡æ¡£æˆæœçš„æ–¹æ³•å’Œå¯è§†åŒ–æ•°æ®é›†ä¸ç ”ç©¶æˆæœé—´å…³ç³»çš„SQLæ•°æ®åº“ã€‚</li>
<li>åˆ›å»ºåŒ»å­¦æˆåƒæ•°æ®é›†æ—¶éœ€è€ƒè™‘çš„å…³é”®å› ç´ åŒ…æ‹¬æ•°æ®æ ‡æ³¨çš„æœ€ä½³å®è·µã€æ·å¾„çš„åˆ©ç”¨ã€äººå£å¤šæ ·æ€§çš„é‡è¦æ€§ç­‰ã€‚</li>
<li>å¼ºè°ƒåœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­ç®¡ç†æ•°æ®é›†çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e4fd060373c71f618ce0c0ee7d6bb5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66a1a70c5ac1653021029e492f1a3102.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fedbe0e9baff5bdf30543b31a1eaaa01.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-abda622fffad7c206a6ff36f23587686.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  MathReader  Text-to-Speech for Mathematical Documents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2405.14477v2/page_5_0.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  GPS as a Control Signal for Image Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23394.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
