<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  Parameterised Quantum Circuits for Novel Representation Learning in   Speech Emotion Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6857988b08e164a01853b7c55b382965.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-23-æ›´æ–°"><a href="#2025-01-23-æ›´æ–°" class="headerlink" title="2025-01-23 æ›´æ–°"></a>2025-01-23 æ›´æ–°</h1><h2 id="Parameterised-Quantum-Circuits-for-Novel-Representation-Learning-in-Speech-Emotion-Recognition"><a href="#Parameterised-Quantum-Circuits-for-Novel-Representation-Learning-in-Speech-Emotion-Recognition" class="headerlink" title="Parameterised Quantum Circuits for Novel Representation Learning in   Speech Emotion Recognition"></a>Parameterised Quantum Circuits for Novel Representation Learning in   Speech Emotion Recognition</h2><p><strong>Authors:Thejan Rajapakshe, Rajib Rana, Farina Riaz, Sara Khalifa, BjÃ¶rn W. Schuller</strong></p>
<p>Speech Emotion Recognition (SER) is a complex and challenging task in human-computer interaction due to the intricate dependencies of features and the overlapping nature of emotional expressions conveyed through speech. Although traditional deep learning methods have shown effectiveness, they often struggle to capture subtle emotional variations and overlapping states. This paper introduces a hybrid classical-quantum framework that integrates Parameterised Quantum Circuits (PQCs) with conventional Convolutional Neural Network (CNN) architectures. By leveraging quantum properties such as superposition and entanglement, the proposed model enhances feature representation and captures complex dependencies more effectively than classical methods. Experimental evaluations conducted on benchmark datasets, including IEMOCAP, RECOLA, and MSP-Improv, demonstrate that the hybrid model achieves higher accuracy in both binary and multi-class emotion classification while significantly reducing the number of trainable parameters. While a few existing studies have explored the feasibility of using Quantum Circuits to reduce model complexity, none have successfully shown how they can enhance accuracy. This study is the first to demonstrate that Quantum Circuits has the potential to improve the accuracy of SER. The findings highlight the promise of QML to transform SER, suggesting a promising direction for future research and practical applications in emotion-aware systems. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ˜¯äººæœºäº¤äº’ä¸­çš„ä¸€é¡¹å¤æ‚ä¸”å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç”±äºè¯­éŸ³ç‰¹å¾ä¹‹é—´çš„å¤æ‚ä¾èµ–æ€§å’Œæƒ…æ„Ÿè¡¨è¾¾çš„é‡å æ€§ã€‚å°½ç®¡ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ï¼Œä½†å®ƒä»¬åœ¨æ•æ‰å¾®å¦™çš„æƒ…æ„Ÿå˜åŒ–å’Œé‡å çŠ¶æ€æ–¹é¢å¾€å¾€é‡åˆ°å›°éš¾ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ··åˆçš„ç»å…¸-é‡å­æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCsï¼‰ä¸ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„ç›¸ç»“åˆã€‚é€šè¿‡åˆ©ç”¨é‡å­å åŠ å’Œçº ç¼ ç­‰ç‰¹æ€§ï¼Œæ‰€æå‡ºçš„æ¨¡å‹å¢å¼ºäº†ç‰¹å¾è¡¨ç¤ºï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰äº†å¤æ‚çš„ä¾èµ–æ€§ï¼Œæ¯”ç»å…¸æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚åœ¨IEMOCAPã€RECOLAå’ŒMSP-Improvç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ··åˆæ¨¡å‹åœ¨äºŒå…ƒå’Œå¤šå…ƒæƒ…æ„Ÿåˆ†ç±»ä¸­éƒ½å®ç°äº†æ›´é«˜çš„ç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚å°½ç®¡å·²æœ‰ä¸€äº›ç°æœ‰ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨é‡å­ç”µè·¯å‡å°‘æ¨¡å‹å¤æ‚æ€§çš„å¯è¡Œæ€§ï¼Œä½†æ²¡æœ‰ç ”ç©¶æˆåŠŸè¡¨æ˜å®ƒä»¬å¦‚ä½•æé«˜å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶é¦–æ¬¡è¯æ˜äº†é‡å­ç”µè·¯åœ¨æé«˜SERå‡†ç¡®æ€§æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶ç»“æœçªå‡ºäº†é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰åœ¨æ”¹å˜SERæ–¹é¢çš„å‰æ™¯ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œæƒ…æ„Ÿæ„ŸçŸ¥ç³»ç»Ÿçš„å®é™…åº”ç”¨æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12050v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§èåˆä¼ ç»Ÿæ·±åº¦å­¦ä¹ ä¸é‡å­è®¡ç®—çš„æ–°å‹æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCï¼‰ä¸å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„ï¼Œé€šè¿‡é‡å­è®¡ç®—çš„å åŠ å’Œçº ç¼ ç‰¹æ€§ï¼Œæé«˜ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰æƒ…æ„Ÿè¡¨è¾¾çš„å¤æ‚ä¾èµ–å…³ç³»ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ··åˆæ¨¡å‹åœ¨äºŒå…ƒå’Œå¤šç±»æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸Šå®ç°æ›´é«˜çš„ç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘è®­ç»ƒå‚æ•°æ•°é‡ã€‚æœ¬ç ”ç©¶é¦–æ¬¡è¯æ˜äº†é‡å­ç”µè·¯åœ¨æé«˜è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶å±•æœ›äº†é‡å­æœºå™¨å­¦ä¹ åœ¨æƒ…æ„Ÿè¯†åˆ«é¢†åŸŸçš„æœªæ¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„ä¸€é¡¹å¤æ‚ä»»åŠ¡ï¼Œå› ç‰¹å¾é—´çš„å¤æ‚ä¾èµ–æ€§å’Œæƒ…æ„Ÿè¡¨è¾¾çš„é‡å æ€§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ•æ‰ç»†å¾®æƒ…æ„Ÿå˜åŒ–å’Œé‡å çŠ¶æ€æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>èåˆé‡å­è®¡ç®—ä¸æ·±åº¦å­¦ä¹ çš„æ–°å‹æ¨¡å‹é€šè¿‡åˆ©ç”¨é‡å­ç‰¹æ€§å¦‚å åŠ å’Œçº ç¼ ï¼Œæé«˜äº†ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œæ–°å‹æ··åˆæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æ›´é«˜çš„æƒ…æ„Ÿåˆ†ç±»ç²¾åº¦ã€‚</li>
<li>ä¸ç°æœ‰ç ”ç©¶ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å‡å°‘è®­ç»ƒå‚æ•°æ•°é‡çš„åŒæ—¶æé«˜äº†å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡è¯æ˜äº†é‡å­ç”µè·¯åœ¨æå‡è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b62317e141f0520954b873265d0a1d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-834cfdd71ecda3f8481b8848c7c14e93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e79fe8aa6ae16b85a9b6c0b971507c76.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2501.12050v1/page_4_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b0bbf6d3652b799469cb1c708e08faa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d4c3dad4a62dd84afc11599f8266ed2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-912142fc8e413b480be5f8b7a97fa5a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Speech-Enhancement-with-Overlapped-Frame-Information-Fusion-and-Causal-Self-Attention"><a href="#Speech-Enhancement-with-Overlapped-Frame-Information-Fusion-and-Causal-Self-Attention" class="headerlink" title="Speech Enhancement with Overlapped-Frame Information Fusion and Causal   Self-Attention"></a>Speech Enhancement with Overlapped-Frame Information Fusion and Causal   Self-Attention</h2><p><strong>Authors:Yuewei Zhang, Huanbin Zou, Jie Zhu</strong></p>
<p>For time-frequency (TF) domain speech enhancement (SE) methods, the overlap-and-add operation in the inverse TF transformation inevitably leads to an algorithmic delay equal to the window size. However, typical causal SE systems fail to utilize the future speech information within this inherent delay, thereby limiting SE performance. In this paper, we propose an overlapped-frame information fusion scheme. At each frame index, we construct several pseudo overlapped-frames, fuse them with the original speech frame, and then send the fused results to the SE model. Additionally, we introduce a causal time-frequency-channel attention (TFCA) block to boost the representation capability of the neural network. This block parallelly processes the intermediate feature maps through self-attention-based operations in the time, frequency, and channel dimensions. Experiments demonstrate the superiority of these improvements, and the proposed SE system outperforms the current advanced methods. </p>
<blockquote>
<p>å¯¹äºæ—¶é¢‘åŸŸï¼ˆTFï¼‰è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•ï¼Œé€†TFå˜æ¢ä¸­çš„å åŠ æ“ä½œä¸å¯é¿å…åœ°ä¼šå¯¼è‡´ç­‰äºçª—å£å¤§å°çš„ç®—æ³•å»¶è¿Ÿã€‚ç„¶è€Œï¼Œå…¸å‹çš„å› æœSEç³»ç»Ÿæœªèƒ½åˆ©ç”¨æ­¤å›ºæœ‰å»¶è¿Ÿå†…çš„æœªæ¥è¯­éŸ³ä¿¡æ¯ï¼Œä»è€Œé™åˆ¶äº†SEæ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡å å¸§ä¿¡æ¯èåˆæ–¹æ¡ˆã€‚åœ¨æ¯ä¸ªå¸§ç´¢å¼•å¤„ï¼Œæˆ‘ä»¬æ„å»ºå¤šä¸ªä¼ªé‡å å¸§ï¼Œå°†å®ƒä»¬ä¸åŸå§‹è¯­éŸ³å¸§èåˆï¼Œç„¶åå°†èåˆç»“æœå‘é€åˆ°SEæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå› æœæ—¶é¢‘é€šé“æ³¨æ„åŠ›ï¼ˆTFCAï¼‰å—ï¼Œä»¥æé«˜ç¥ç»ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›ã€‚è¯¥å—é€šè¿‡æ—¶é—´ã€é¢‘ç‡å’Œé€šé“ç»´åº¦ä¸Šçš„è‡ªæ³¨æ„åŠ›æ“ä½œå¹¶è¡Œå¤„ç†ä¸­é—´ç‰¹å¾å›¾ã€‚å®éªŒè¯æ˜äº†è¿™äº›æ”¹è¿›çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”æ‰€æå‡ºçš„SEç³»ç»Ÿä¼˜äºå½“å‰çš„é«˜çº§æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12004v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>æœ¬è®ºæ–‡é’ˆå¯¹æ—¶é¢‘åŸŸè¯­éŸ³å¢å¼ºæ–¹æ³•ä¸­çš„é‡å æ·»åŠ æ“ä½œå¯¼è‡´çš„ç®—æ³•å»¶è¿Ÿé—®é¢˜è¿›è¡Œç ”ç©¶ï¼Œå¹¶æå‡ºä¸€ç§é‡å å¸§ä¿¡æ¯èåˆæ–¹æ¡ˆã€‚é€šè¿‡æ„å»ºä¼ªé‡å å¸§å¹¶ä¸åŸå§‹è¯­éŸ³å¸§èåˆï¼Œæé«˜è¯­éŸ³å¢å¼ºæ€§èƒ½ã€‚åŒæ—¶ï¼Œå¼•å…¥å› æœæ—¶é¢‘é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡ç¥ç»ç½‘ç»œçš„è¡¨ç°èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œæ‰€ææ–¹æ¡ˆä¼˜äºå½“å‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é¢‘åŸŸè¯­éŸ³å¢å¼ºæ–¹æ³•ä¸­çš„é‡å æ·»åŠ æ“ä½œä¼šå¯¼è‡´ç®—æ³•å»¶è¿Ÿï¼Œé™åˆ¶æ€§èƒ½ã€‚</li>
<li>æå‡ºé‡å å¸§ä¿¡æ¯èåˆæ–¹æ¡ˆï¼Œåˆ©ç”¨ä¼ªé‡å å¸§ä¸åŸå§‹è¯­éŸ³å¸§èåˆï¼Œæé«˜è¯­éŸ³å¢å¼ºæ•ˆæœã€‚</li>
<li>å¼•å…¥å› æœæ—¶é¢‘é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºç¥ç»ç½‘ç»œçš„è¡¨ç°èƒ½åŠ›ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡è‡ªæˆ‘å…³æ³¨æ“ä½œå¤„ç†ä¸­é—´ç‰¹å¾å›¾ï¼Œæ¶‰åŠæ—¶é—´ã€é¢‘ç‡å’Œé€šé“ç»´åº¦ã€‚</li>
<li>å®éªŒè¯æ˜æ‰€ææ–¹æ¡ˆä¼˜äºå½“å‰å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>é‡å å¸§ä¿¡æ¯èåˆä¸æ³¨æ„åŠ›æœºåˆ¶ç»“åˆï¼Œæœ‰æ•ˆæ”¹å–„è¯­éŸ³å¢å¼ºæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a13e28dcc86fa1ca8842cce2d6507bd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad4057ccb44c055a6ef148561b4a3acd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b46566ab0d2651fdc2d483b7e30ec8f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bb995980a22161425728f0983ef770b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfead16ae8df7147d35566b40bc224f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75b56f55160e31ff10fe018f3a009341.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Rate-Aware-Learned-Speech-Compression"><a href="#Rate-Aware-Learned-Speech-Compression" class="headerlink" title="Rate-Aware Learned Speech Compression"></a>Rate-Aware Learned Speech Compression</h2><p><strong>Authors:Jun Xu, Zhengxue Cheng, Guangchuan Chi, Yuhan Liu, Yuelin Hu, Li Song</strong></p>
<p>The rapid rise of real-time communication and large language models has significantly increased the importance of speech compression. Deep learning-based neural speech codecs have outperformed traditional signal-level speech codecs in terms of rate-distortion (RD) performance. Typically, these neural codecs employ an encoder-quantizer-decoder architecture, where audio is first converted into latent code feature representations and then into discrete tokens. However, this architecture exhibits insufficient RD performance due to two main drawbacks: (1) the inadequate performance of the quantizer, challenging training processes, and issues such as codebook collapse; (2) the limited representational capacity of the encoder and decoder, making it difficult to meet feature representation requirements across various bitrates. In this paper, we propose a rate-aware learned speech compression scheme that replaces the quantizer with an advanced channel-wise entropy model to improve RD performance, simplify training, and avoid codebook collapse. We employ multi-scale convolution and linear attention mixture blocks to enhance the representational capacity and flexibility of the encoder and decoder. Experimental results demonstrate that the proposed method achieves state-of-the-art RD performance, obtaining 53.51% BD-Rate bitrate saving in average, and achieves 0.26 BD-VisQol and 0.44 BD-PESQ gains. </p>
<blockquote>
<p>å®æ—¶é€šä¿¡å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿…é€Ÿå´›èµ·ï¼Œæå¤§åœ°æé«˜äº†è¯­éŸ³å‹ç¼©çš„é‡è¦æ€§ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨åœ¨é€Ÿç‡å¤±çœŸï¼ˆRDï¼‰æ€§èƒ½ä¸Šå·²ç»è¶…è¶Šäº†ä¼ ç»Ÿçš„ä¿¡å·çº§è¯­éŸ³ç¼–è§£ç å™¨ã€‚è¿™äº›ç¥ç»ç½‘ç»œç¼–è§£ç å™¨é€šå¸¸é‡‡ç”¨ç¼–ç å™¨-é‡åŒ–å™¨-è§£ç å™¨çš„æ¶æ„ï¼Œé¦–å…ˆå°†éŸ³é¢‘è½¬æ¢ä¸ºæ½œåœ¨ä»£ç ç‰¹å¾è¡¨ç¤ºï¼Œç„¶åè½¬æ¢ä¸ºç¦»æ•£ä»¤ç‰Œã€‚ç„¶è€Œï¼Œç”±äºä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼Œè¿™ç§æ¶æ„çš„RDæ€§èƒ½è¡¨ç°ä¸è¶³ï¼šï¼ˆ1ï¼‰é‡åŒ–å™¨çš„æ€§èƒ½ä¸è¶³ï¼Œè®­ç»ƒè¿‡ç¨‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä»¥åŠè¯¸å¦‚ç æœ¬å´©æºƒä¹‹ç±»çš„é—®é¢˜ï¼›ï¼ˆ2ï¼‰ç¼–ç å™¨å’Œè§£ç å™¨çš„è¡¨ç¤ºèƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥æ»¡è¶³å„ç§æ¯”ç‰¹ç‡ä¸‹çš„ç‰¹å¾è¡¨ç¤ºè¦æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€Ÿç‡æ„ŸçŸ¥çš„è¯­éŸ³å‹ç¼©æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆç”¨å…ˆè¿›çš„é€šé“æ„ŸçŸ¥ç†µæ¨¡å‹å–ä»£äº†é‡åŒ–å™¨ï¼Œä»¥æé«˜RDæ€§èƒ½ï¼Œç®€åŒ–è®­ç»ƒå¹¶é¿å…ç æœ¬å´©æºƒã€‚æˆ‘ä»¬é‡‡ç”¨å¤šå°ºåº¦å·ç§¯å’Œçº¿æ€§æ³¨æ„åŠ›æ··åˆå—æ¥å¢å¼ºç¼–ç å™¨å’Œè§£ç å™¨çš„è¡¨ç¤ºèƒ½åŠ›å’Œçµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„RDæ€§èƒ½ï¼Œå¹³å‡èŠ‚çœæ¯”ç‰¹ç‡53.51ï¼…çš„BD-Rateï¼Œå®ç°äº†0.26çš„BD-VisQolå’Œ0.44çš„BD-PESQå¢ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11999v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¯­éŸ³å‹ç¼©æŠ€æœ¯æé«˜äº†è¯­éŸ³é€šä¿¡æ•ˆç‡ã€‚æ–°æ–¹æ¡ˆåˆ©ç”¨å…ˆè¿›çš„é€šé“æ„ŸçŸ¥ç†µæ¨¡å‹æ›¿æ¢é‡åŒ–å™¨ï¼Œå¢å¼ºç¼–ç å™¨è§£ç å™¨æ€§èƒ½ï¼Œå®ç°ç ç‡å¤±çœŸä¼˜åŒ–ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œå¹³å‡èŠ‚çœæ¯”ç‰¹ç‡è¾¾53.51%ï¼Œè§†è§‰è´¨é‡å’Œæ„ŸçŸ¥è´¨é‡è¯„ä»·åˆ†åˆ«æå‡0.26å’Œ0.44ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶é€šä¿¡å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¢åŠ äº†è¯­éŸ³å‹ç¼©çš„é‡è¦æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨åœ¨é€Ÿç‡å¤±çœŸæ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿä¿¡å·çº§è¯­éŸ³ç¼–è§£ç å™¨ã€‚</li>
<li>å½“å‰ç¥ç»ç½‘ç»œç¼–è§£ç å™¨å­˜åœ¨é‡åŒ–å™¨æ€§èƒ½ä¸è¶³å’Œç¼–ç å™¨è§£ç å™¨ä»£è¡¨æ€§å®¹é‡æœ‰é™çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ã€‚</li>
<li>æ–°çš„è¯­éŸ³å‹ç¼©æ–¹æ¡ˆåˆ©ç”¨å…ˆè¿›çš„é€šé“æ„ŸçŸ¥ç†µæ¨¡å‹æ›¿æ¢é‡åŒ–å™¨ï¼Œæ”¹å–„é€Ÿç‡å¤±çœŸæ€§èƒ½ï¼Œç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…ç æœ¬å´©æºƒã€‚</li>
<li>è¯¥æ–¹æ¡ˆé‡‡ç”¨å¤šå°ºåº¦å·ç§¯å’Œçº¿æ€§æ³¨æ„åŠ›æ··åˆå—ï¼Œå¢å¼ºç¼–ç å™¨è§£ç å™¨çš„è¡¨ç°åŠ›å’Œçµæ´»æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ–°æ–¹æ¡ˆå®ç°äº†ä¸šç•Œé¢†å…ˆçš„é€Ÿç‡å¤±çœŸæ€§èƒ½ï¼Œå¹³å‡èŠ‚çœæ¯”ç‰¹ç‡è¾¾53.51%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ff8874b7406598e6317f8abf794c110.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-503470ce4926cc94e57b0574507f1af6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4dba2520bbbb13ceb29e0d252ae03e3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f2e92709b7c386335784d6b571a9956.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="30-Years-of-Source-Separation-Research-Achievements-and-Future-Challenges"><a href="#30-Years-of-Source-Separation-Research-Achievements-and-Future-Challenges" class="headerlink" title="30+ Years of Source Separation Research: Achievements and Future   Challenges"></a>30+ Years of Source Separation Research: Achievements and Future   Challenges</h2><p><strong>Authors:Shoko Araki, Nobutaka Ito, Reinhold Haeb-Umbach, Gordon Wichern, Zhong-Qiu Wang, Yuki Mitsufuji</strong></p>
<p>Source separation (SS) of acoustic signals is a research field that emerged in the mid-1990s and has flourished ever since. On the occasion of ICASSPâ€™s 50th anniversary, we review the major contributions and advancements in the past three decades in the speech, audio, and music SS research field. We will cover both single- and multi-channel SS approaches. We will also look back on key efforts to foster a culture of scientific evaluation in the research field, including challenges, performance metrics, and datasets. We will conclude by discussing current trends and future research directions. </p>
<blockquote>
<p>éŸ³é¢‘ä¿¡å·çš„æºåˆ†ç¦»ï¼ˆSSï¼‰æ˜¯ä¸Šä¸–çºª90å¹´ä»£ä¸­æœŸå‡ºç°å¹¶è‡ªæ­¤ç¹è£èµ·æ¥çš„ç ”ç©¶é¢†åŸŸã€‚æ­£å€¼ICASSPæˆç«‹äº”åå‘¨å¹´ä¹‹é™…ï¼Œæˆ‘ä»¬å›é¡¾äº†è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹SSç ”ç©¶é¢†åŸŸè¿‡å»ä¸‰åå¹´çš„ä¸»è¦è´¡çŒ®å’Œè¿›å±•ã€‚æˆ‘ä»¬å°†ä»‹ç»å•é€šé“å’Œå¤šé€šé“SSæ–¹æ³•ã€‚æˆ‘ä»¬ä¹Ÿå›é¡¾äº†åœ¨è¯¥é¢†åŸŸæ¨åŠ¨ç§‘å­¦è¯„ä»·æ–‡åŒ–çš„å…³é”®ä¸¾æªï¼ŒåŒ…æ‹¬æŒ‘æˆ˜ã€æ€§èƒ½æŒ‡æ ‡å’Œæ•°æ®é›†ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è®¨è®ºå½“å‰è¶‹åŠ¿å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11837v1">PDF</a> Accepted by IEEE ICASSP 2025</p>
<p><strong>Summary</strong><br>     å£°æºåˆ†ç¦»ï¼ˆSSï¼‰æ˜¯ä¸Šä¸–çºªä¹åå¹´ä»£æ¶Œç°å¹¶åœ¨ä¹‹åè“¬å‹ƒå‘å±•çš„ä¸€ä¸ªç ”ç©¶é¢†åŸŸã€‚å€¼æ­¤ICASSPäº”åå‘¨å¹´ä¹‹é™…ï¼Œå›é¡¾è¿‡å»ä¸‰åå¹´è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹SSç ”ç©¶é¢†åŸŸçš„é‡å¤§è´¡çŒ®å’Œè¿›å±•ã€‚æœ¬æ–‡å°†ä»‹ç»å•é€šé“å’Œå¤šé€šé“SSæ–¹æ³•ï¼Œå¹¶å›é¡¾æ¨åŠ¨è¯¥é¢†åŸŸç§‘å­¦è¯„ä¼°æ–‡åŒ–å‘å±•çš„å…³é”®å·¥ä½œï¼ŒåŒ…æ‹¬æŒ‘æˆ˜ã€æ€§èƒ½åº¦å’Œæ•°æ®é›†ã€‚æœ¬æ–‡è¿˜å°†è®¨è®ºå½“å‰è¶‹åŠ¿å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å£°æºåˆ†ç¦»é¢†åŸŸåœ¨ä¹åå¹´ä»£æ¶Œç°å¹¶è¿…é€Ÿå‘å±•ã€‚</li>
<li>æ­¤é¢†åŸŸæ¶µç›–äº†å•é€šé“å’Œå¤šé€šé“å£°æºåˆ†ç¦»æ–¹æ³•ã€‚</li>
<li>åœ¨å£°æºåˆ†ç¦»é¢†åŸŸçš„å‘å±•è¿‡ç¨‹ä¸­ï¼Œå…³é”®åŠªåŠ›ä¹‹ä¸€æ˜¯å»ºç«‹ç§‘å­¦è¯„ä¼°æ–‡åŒ–ï¼ŒåŒ…æ‹¬åˆ¶å®šæŒ‘æˆ˜ã€æ€§èƒ½æŒ‡æ ‡å’Œæ•°æ®é›†ã€‚</li>
<li>å£°æºåˆ†ç¦»åœ¨è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹é¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>å½“å‰å£°æºåˆ†ç¦»é¢†åŸŸçš„è¶‹åŠ¿æ˜¯ä¸æ–­å‘å±•å’Œå®Œå–„ç°æœ‰æŠ€æœ¯ï¼ŒåŒæ—¶æ¢ç´¢æ–°çš„åº”ç”¨æ–¹å‘ã€‚</li>
<li>ICASSPåœ¨å£°æºåˆ†ç¦»é¢†åŸŸçš„ç ”ç©¶ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-782a763f316facc8ad5a9e7d8b7d28d9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLM-supervised-Pre-training-for-Multimodal-Emotion-Recognition-in-Conversations"><a href="#LLM-supervised-Pre-training-for-Multimodal-Emotion-Recognition-in-Conversations" class="headerlink" title="LLM supervised Pre-training for Multimodal Emotion Recognition in   Conversations"></a>LLM supervised Pre-training for Multimodal Emotion Recognition in   Conversations</h2><p><strong>Authors:Soumya Dutta, Sriram Ganapathy</strong></p>
<p>Emotion recognition in conversations (ERC) is challenging due to the multimodal nature of the emotion expression. In this paper, we propose to pretrain a text-based recognition model from unsupervised speech transcripts with LLM guidance. These transcriptions are obtained from a raw speech dataset with a pre-trained ASR system. A text LLM model is queried to provide pseudo-labels for these transcripts, and these pseudo-labeled transcripts are subsequently used for learning an utterance level text-based emotion recognition model. We use the utterance level text embeddings for emotion recognition in conversations along with speech embeddings obtained from a recently proposed pre-trained model. A hierarchical way of training the speech-text model is proposed, keeping in mind the conversational nature of the dataset. We perform experiments on three established datasets, namely, IEMOCAP, MELD, and CMU- MOSI, where we illustrate that the proposed model improves over other benchmarks and achieves state-of-the-art results on two out of these three datasets. </p>
<blockquote>
<p>å¯¹è¯ä¸­çš„æƒ…ç»ªè¯†åˆ«ï¼ˆERCï¼‰ç”±äºæƒ…ç»ªè¡¨è¾¾çš„å¤šæ¨¡å¼æ€§è´¨è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä»å¸¦æœ‰LLMæŒ‡å¯¼çš„æ— ç›‘ç£è¯­éŸ³è½¬å½•æœ¬ä¸­é¢„è®­ç»ƒåŸºäºæ–‡æœ¬çš„è¯†åˆ«æ¨¡å‹ã€‚è¿™äº›è½¬å½•æœ¬æ˜¯ç”±å¸¦æœ‰é¢„è®­ç»ƒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„åŸå§‹è¯­éŸ³æ•°æ®é›†è·å¾—çš„ã€‚æ–‡æœ¬LLMæ¨¡å‹è¢«æŸ¥è¯¢ä»¥æä¾›è¿™äº›è½¬å½•æœ¬çš„ä¼ªæ ‡ç­¾ï¼Œéšåè¿™äº›ä¼ªæ ‡ç­¾è½¬å½•æœ¬è¢«ç”¨äºå­¦ä¹ åŸºäºæ–‡æœ¬çš„å¯¹è¯çº§æƒ…ç»ªè¯†åˆ«æ¨¡å‹çš„å‘è¨€æƒæ°´å¹³ã€‚æˆ‘ä»¬ä½¿ç”¨å¯¹è¯ä¸­çš„å‘è¨€çº§æ–‡æœ¬åµŒå…¥å’Œä»æœ€è¿‘æå‡ºçš„é¢„è®­ç»ƒæ¨¡å‹è·å¾—çš„è¯­éŸ³åµŒå…¥æ¥è¿›è¡Œæƒ…ç»ªè¯†åˆ«ã€‚è€ƒè™‘åˆ°æ•°æ®é›†çš„å¯¹è¯æ€§è´¨ï¼Œæå‡ºäº†ä¸€ç§è®­ç»ƒè¯­éŸ³æ–‡æœ¬æ¨¡å‹çš„åˆ†å±‚æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå»ºç«‹çš„æ•°æ®é›†IEMOCAPã€MELDå’ŒCMU-MOSIä¸Šè¿›è¡Œäº†å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ¨¡å‹åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šçš„æ”¹è¿›ï¼Œå¹¶åœ¨å…¶ä¸­ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11468v1">PDF</a> ICASSP 2025; 5 pages, 4 figures, 2 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå¯¹è¯çš„æƒ…æ„Ÿè¯†åˆ«ï¼ˆERCï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦æ˜¯å› ä¸ºæƒ…æ„Ÿè¡¨è¾¾çš„å¤šåª’ä½“æ€§è´¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼æ¥è®­ç»ƒæ–‡æœ¬æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„æ–¹æ³•ã€‚è¿™äº›æ–‡æœ¬è½¬è¯‘é€šè¿‡å¸¦æœ‰é¢„è®­ç»ƒè¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„åŸå§‹è¯­éŸ³æ•°æ®é›†è·å¾—ã€‚æˆ‘ä»¬å‘æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹è¯¢é—®å¹¶è·å–è¿™äº›è½¬è¯‘çš„ä¼ªæ ‡ç­¾ï¼Œç„¶åå°†è¿™äº›ä¼ªæ ‡ç­¾è½¬è¯‘ç”¨äºå­¦ä¹ åŸºäºæ–‡æœ¬çš„æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„å‘è¨€çº§åˆ«è¡¨è¾¾ã€‚æˆ‘ä»¬å°†å‘è¨€çº§åˆ«çš„æ–‡æœ¬åµŒå…¥ä¸æ¥è‡ªæœ€è¿‘æå‡ºçš„é¢„è®­ç»ƒæ¨¡å‹çš„è¯­éŸ³åµŒå…¥ç›¸ç»“åˆï¼Œç”¨äºå¯¹è¯ä¸­çš„æƒ…æ„Ÿè¯†åˆ«ã€‚è€ƒè™‘åˆ°æ•°æ®é›†çš„å¯¹è¯æ€§è´¨ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®­ç»ƒè¯­éŸ³æ–‡æœ¬æ¨¡å‹çš„åˆ†å±‚æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå·²å»ºç«‹çš„æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œå³IEMOCAPã€MELDå’ŒCMU-MOSIã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å…¶ä¸­ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºå¯¹è¯çš„æƒ…æ„Ÿè¯†åˆ«ï¼ˆERCï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæƒ…æ„Ÿçš„å¤šåª’ä½“æ€§è´¨ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼è¿›è¡Œæ–‡æœ¬æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„è®­ç»ƒæ˜¯ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿè·å–åŸå§‹è¯­éŸ³æ•°æ®é›†ä»¥ç”Ÿæˆæ–‡æœ¬è½¬è¯‘ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç”¨äºè·å–æ–‡æœ¬è½¬è¯‘çš„ä¼ªæ ‡ç­¾ï¼Œè¿›è€Œç”¨äºå­¦ä¹ å‘è¨€çº§åˆ«çš„æ–‡æœ¬æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ã€‚</li>
<li>ç»“åˆæ–‡æœ¬åµŒå…¥å’Œè¯­éŸ³åµŒå…¥è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ï¼Œä»¥åº”å¯¹å¯¹è¯ä¸­çš„å¤æ‚æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ†å±‚è®­ç»ƒè¯­éŸ³æ–‡æœ¬æ¨¡å‹çš„ç­–ç•¥ï¼Œè€ƒè™‘äº†æ•°æ®é›†çš„å¯¹è¯æ€§è´¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-75062cea5c36f41f554f31a9c1f379e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d270c271533c07dd329af849f3a54d61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2032c10af37e8045b181ebca2e291f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-578046f0a24579070bd548eecee12aa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da9bc93e3447f730dba0b480387487c7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Investigation-of-Whisper-ASR-Hallucinations-Induced-by-Non-Speech-Audio"><a href="#Investigation-of-Whisper-ASR-Hallucinations-Induced-by-Non-Speech-Audio" class="headerlink" title="Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio"></a>Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio</h2><p><strong>Authors:Mateusz BaraÅ„ski, Jan JasiÅ„ski, Julitta Bartolewska, StanisÅ‚aw Kacprzak, Marcin Witkowski, Konrad Kowalczyk</strong></p>
<p>Hallucinations of deep neural models are amongst key challenges in automatic speech recognition (ASR). In this paper, we investigate hallucinations of the Whisper ASR model induced by non-speech audio segments present during inference. By inducting hallucinations with various types of sounds, we show that there exists a set of hallucinations that appear frequently. We then study hallucinations caused by the augmentation of speech with such sounds. Finally, we describe the creation of a bag of hallucinations (BoH) that allows to remove the effect of hallucinations through the post-processing of text transcriptions. The results of our experiments show that such post-processing is capable of reducing word error rate (WER) and acts as a good safeguard against problematic hallucinations. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„å¹»è§‰æ˜¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”±éè¯­éŸ³éŸ³é¢‘ç‰‡æ®µå¼•èµ·çš„Whisper ASRæ¨¡å‹çš„å¹»è§‰ã€‚é€šè¿‡ç”¨å„ç§å£°éŸ³è¯±å‘å¹»è§‰ï¼Œæˆ‘ä»¬å‘ç°å­˜åœ¨ä¸€ç³»åˆ—ç»å¸¸å‡ºç°çš„å¹»è§‰ã€‚ç„¶åï¼Œæˆ‘ä»¬ç ”ç©¶äº†é€šè¿‡ç”¨è¿™äº›å£°éŸ³å¢å¼ºè¯­éŸ³è€Œäº§ç”Ÿçš„å¹»è§‰ã€‚æœ€åï¼Œæˆ‘ä»¬æè¿°äº†å¹»è§‰åŒ…ï¼ˆBoHï¼‰çš„åˆ›å»ºï¼Œè¯¥åŒ…å¯ä»¥é€šè¿‡æ–‡æœ¬è½¬å½•çš„åå¤„ç†æ¥æ¶ˆé™¤å¹»è§‰çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åå¤„ç†èƒ½å¤Ÿé™ä½è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œå¹¶ä½œä¸ºé˜²æ­¢é—®é¢˜å¹»è§‰çš„è‰¯å¥½ä¿éšœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11378v1">PDF</a> Accepted for IEEE ICASSP 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹äº§ç”Ÿçš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€â€”â€”hallucinationç°è±¡ã€‚ç ”ç©¶å‘ç°åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çš„éè¯­éŸ³éŸ³é¢‘ç‰‡æ®µä¼šå¼•å‘hallucinationï¼Œå­˜åœ¨é¢‘ç¹å‡ºç°çš„hallucinationç±»å‹ã€‚é€šè¿‡å¯¹è¯­éŸ³å¢åŠ è¿™äº›å£°éŸ³çš„ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§æ¶ˆé™¤hallucinationå½±å“çš„â€œhallucinationåŒ…ï¼ˆBoHï¼‰â€çš„åˆ›å»ºæ–¹æ³•ï¼Œé€šè¿‡æ–‡æœ¬è½¬å½•çš„åå¤„ç†æ¥å‡å°‘word error rateï¼ˆWERï¼‰ï¼Œå¹¶ä½œä¸ºå¯¹æŠ—é—®é¢˜hallucinationçš„è‰¯å¥½ä¿éšœã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>éè¯­éŸ³éŸ³é¢‘ç‰‡æ®µåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¸­ä¼šå¼•å‘hallucinationç°è±¡ã€‚</li>
<li>å­˜åœ¨ä¸€ç³»åˆ—é¢‘ç¹å‡ºç°çš„hallucinationç±»å‹ã€‚</li>
<li>é€šè¿‡å¯¹è¯­éŸ³å¢åŠ ç‰¹å®šå£°éŸ³ï¼Œå¯ä»¥äº§ç”Ÿæ›´å¤šçš„hallucinationã€‚</li>
<li>æå‡ºäº†é€šè¿‡åˆ›å»ºâ€œhallucinationåŒ…ï¼ˆBoHï¼‰â€æ¥æ¶ˆé™¤hallucinationå½±å“çš„æ–¹æ³•ã€‚</li>
<li>æ–‡æœ¬è½¬å½•çš„åå¤„ç†èƒ½å¤Ÿå‡å°‘word error rateï¼ˆWERï¼‰ã€‚</li>
<li>åå¤„ç†ä½œä¸ºä¸€ç§ä¿éšœæªæ–½ï¼Œå¯ä»¥æœ‰æ•ˆå¯¹æŠ—é—®é¢˜hallucinationã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2ee163930d43125e0a5a7dafe0ea9553.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1be1d7452f0c9659c03f7530aec7f51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd1b5432a9ece0115b7e80b96262b9cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd6d3dadceeb0f666774ae84727c126e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c57419868ecedc3e82a141d04d88e7f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5c6f268c7d8620524b6142caee76f5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70c9514d171efe0dfc20e96de4ab1c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e56b4f3f15eb232bc3c0d2a04d8cad1a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SEF-PNet-Speaker-Encoder-Free-Personalized-Speech-Enhancement-with-Local-and-Global-Contexts-Aggregation"><a href="#SEF-PNet-Speaker-Encoder-Free-Personalized-Speech-Enhancement-with-Local-and-Global-Contexts-Aggregation" class="headerlink" title="SEF-PNet: Speaker Encoder-Free Personalized Speech Enhancement with   Local and Global Contexts Aggregation"></a>SEF-PNet: Speaker Encoder-Free Personalized Speech Enhancement with   Local and Global Contexts Aggregation</h2><p><strong>Authors:Ziling Huang, Haixin Guan, Haoran Wei, Yanhua Long</strong></p>
<p>Personalized speech enhancement (PSE) methods typically rely on pre-trained speaker verification models or self-designed speaker encoders to extract target speaker clues, guiding the PSE model in isolating the desired speech. However, these approaches suffer from significant model complexity and often underutilize enrollment speaker information, limiting the potential performance of the PSE model. To address these limitations, we propose a novel Speaker Encoder-Free PSE network, termed SEF-PNet, which fully exploits the information present in both the enrollment speech and noisy mixtures. SEF-PNet incorporates two key innovations: Interactive Speaker Adaptation (ISA) and Local-Global Context Aggregation (LCA). ISA dynamically modulates the interactions between enrollment and noisy signals to enhance the speaker adaptation, while LCA employs advanced channel attention within the PSE encoder to effectively integrate local and global contextual information, thus improving feature learning. Experiments on the Libri2Mix dataset demonstrate that SEF-PNet significantly outperforms baseline models, achieving state-of-the-art PSE performance. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–è¯­éŸ³å¢å¼ºï¼ˆPSEï¼‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„å…ˆè®­ç»ƒçš„è¯´è¯äººéªŒè¯æ¨¡å‹æˆ–è‡ªè¡Œè®¾è®¡çš„è¯´è¯äººç¼–ç å™¨æ¥æå–ç›®æ ‡è¯´è¯äººçš„çº¿ç´¢ï¼ŒæŒ‡å¯¼PSEæ¨¡å‹åˆ†ç¦»å‡ºæ‰€éœ€çš„è¯­éŸ³ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨æ¨¡å‹å¤æ‚åº¦é«˜ã€å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨æ³¨å†Œè¯´è¯äººçš„ä¿¡æ¯ç­‰é—®é¢˜ï¼Œé™åˆ¶äº†PSEæ¨¡å‹çš„æ½œåœ¨æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— éœ€è¯´è¯äººç¼–ç å™¨çš„PSEç½‘ç»œï¼Œç§°ä¸ºSEF-PNetï¼Œå®ƒå……åˆ†åˆ©ç”¨äº†æ³¨å†Œè¯­éŸ³å’Œå™ªå£°æ··åˆç‰©ä¸­çš„ä¿¡æ¯ã€‚SEF-PNetæœ‰ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šäº¤äº’å¼è¯´è¯äººé€‚åº”ï¼ˆISAï¼‰å’Œå±€éƒ¨-å…¨å±€ä¸Šä¸‹æ–‡èšåˆï¼ˆLCAï¼‰ã€‚ISAåŠ¨æ€è°ƒåˆ¶æ³¨å†Œè¯­éŸ³å’Œå™ªå£°ä¿¡å·ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œä»¥å¢å¼ºè¯´è¯äººé€‚åº”ï¼Œè€ŒLCAåœ¨PSEç¼–ç å™¨å†…é‡‡ç”¨å…ˆè¿›çš„é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°æ•´åˆå±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæ”¹è¿›ç‰¹å¾å­¦ä¹ ã€‚åœ¨Libri2Mixæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSEF-PNetæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„PSEæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11274v1">PDF</a> accpeted by ICASSP2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— éœ€Speaker Encoderçš„ä¸ªæ€§åŒ–è¯­éŸ³å¢å¼ºç½‘ç»œï¼ˆSEF-PNetï¼‰ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•æ¨¡å‹å¤æ‚åº¦é«˜å’Œæœªèƒ½å……åˆ†åˆ©ç”¨æ³¨å†Œè¯­éŸ³ä¿¡æ¯çš„é—®é¢˜ã€‚SEF-PNetåˆ›æ–°åœ°å¼•å…¥äº¤äº’å¼è¯´è¯äººé€‚åº”ï¼ˆISAï¼‰å’Œå±€éƒ¨å…¨å±€ä¸Šä¸‹æ–‡èšåˆï¼ˆLCAï¼‰ä¸¤å¤§æ¨¡å—ï¼Œåˆ†åˆ«åŠ¨æ€è°ƒèŠ‚æ³¨å†Œè¯­éŸ³ä¸å™ªå£°ä¿¡å·çš„äº¤äº’ä»¥å¢å¼ºè¯´è¯äººé€‚åº”æ€§ï¼Œå¹¶åœ¨PSEç¼–ç å™¨ä¸­ä½¿ç”¨å…ˆè¿›çš„é€šé“æ³¨æ„åŠ›æœºåˆ¶æ¥æœ‰æ•ˆæ•´åˆå±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæ”¹è¿›ç‰¹å¾å­¦ä¹ ã€‚åœ¨Libri2Mixæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSEF-PNetæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œè¾¾åˆ°äº†å…ˆè¿›çš„ä¸ªæ€§åŒ–è¯­éŸ³å¢å¼ºæ€§èƒ½ã€‚</p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ol>
<li>SEF-PNetè§£å†³äº†ä¸ªæ€§åŒ–è¯­éŸ³å¢å¼ºï¼ˆPSEï¼‰æ–¹æ³•ä¸­æ¨¡å‹å¤æ‚åº¦é«˜çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰PSEæ–¹æ³•é€šå¸¸ä¾èµ–é¢„è®­ç»ƒçš„è¯´è¯äººéªŒè¯æ¨¡å‹æˆ–è‡ªè®¾è®¡çš„è¯´è¯äººç¼–ç å™¨ï¼Œè€ŒSEF-PNetæ— éœ€è¿™äº›ã€‚</li>
<li>SEF-PNeté€šè¿‡å¼•å…¥äº¤äº’å¼è¯´è¯äººé€‚åº”ï¼ˆISAï¼‰æ¨¡å—ï¼ŒåŠ¨æ€è°ƒèŠ‚æ³¨å†Œè¯­éŸ³ä¸å™ªå£°ä¿¡å·çš„äº¤äº’ã€‚</li>
<li>SEF-PNeté‡‡ç”¨å±€éƒ¨å…¨å±€ä¸Šä¸‹æ–‡èšåˆï¼ˆLCAï¼‰æ¨¡å—ï¼Œé€šè¿‡å…ˆè¿›çš„é€šé“æ³¨æ„åŠ›æœºåˆ¶æ•´åˆå±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSEF-PNetåœ¨Libri2Mixæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</li>
<li>SEF-PNetå……åˆ†åˆ©ç”¨äº†æ³¨å†Œè¯­éŸ³ä¿¡æ¯å’Œå™ªå£°æ··åˆä¸­çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11274">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e25f4d00ac90ccd36b7031dc0f838632.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e63ef8627ab2056bac44be17b51f00b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccf5eac002faebb0edad161cb4d8c0ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b87941a4ed98d87d17e4d344032d4da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-556236078ce211478155c2006f05fb9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4df7b990fe141321018b9cddc6cae07e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37e0af368af3703e14e6213d058fa317.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6076fbc2aea35ac14e9ab2235c545cc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Benchmark-of-French-ASR-Systems-Based-on-Error-Severity"><a href="#A-Benchmark-of-French-ASR-Systems-Based-on-Error-Severity" class="headerlink" title="A Benchmark of French ASR Systems Based on Error Severity"></a>A Benchmark of French ASR Systems Based on Error Severity</h2><p><strong>Authors:Antoine Tholly, Jane Wottawa, Mickael Rouvier, Richard Dufour</strong></p>
<p>Automatic Speech Recognition (ASR) transcription errors are commonly assessed using metrics that compare them with a reference transcription, such as Word Error Rate (WER), which measures spelling deviations from the reference, or semantic score-based metrics. However, these approaches often overlook what is understandable to humans when interpreting transcription errors. To address this limitation, a new evaluation is proposed that categorizes errors into four levels of severity, further divided into subtypes, based on objective linguistic criteria, contextual patterns, and the use of content words as the unit of analysis. This metric is applied to a benchmark of 10 state-of-the-art ASR systems on French language, encompassing both HMM-based and end-to-end models. Our findings reveal the strengths and weaknesses of each system, identifying those that provide the most comfortable reading experience for users. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è½¬å½•é”™è¯¯é€šå¸¸é€šè¿‡å°†å…¶ä¸å‚è€ƒè½¬å½•è¿›è¡Œæ¯”è¾ƒçš„åº¦é‡æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼Œä¾‹å¦‚å­—é”™è¯¯ç‡ï¼ˆWERï¼‰è¡¡é‡çš„æ˜¯æ‹¼å†™ä¸å‚è€ƒä¹‹é—´çš„å·®å¼‚ï¼Œæˆ–è€…åŸºäºè¯­ä¹‰å¾—åˆ†çš„åº¦é‡æ ‡å‡†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨è§£é‡Šè½¬å½•é”™è¯¯æ—¶å¾€å¾€å¿½ç•¥äº†äººç±»çš„ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®å®¢è§‚çš„è¯­è¨€æ ‡å‡†ã€ä¸Šä¸‹æ–‡æ¨¡å¼ä»¥åŠä»¥å†…å®¹è¯ä½œä¸ºåˆ†æå•ä½ï¼Œå°†é”™è¯¯åˆ†ä¸ºå››ä¸ªä¸¥é‡ç¨‹åº¦ç±»åˆ«ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åˆ†ä¸ºå„ç§äºšå‹ã€‚è¯¥æŒ‡æ ‡é€‚ç”¨äºæ³•è¯­ä¸­10ä¸ªæœ€æ–°ASRç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬åŸºäºéšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰å’Œç«¯åˆ°ç«¯æ¨¡å‹çš„ç³»ç»Ÿã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†æ¯ä¸ªç³»ç»Ÿçš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œå¹¶ç¡®å®šäº†å“ªäº›ç³»ç»Ÿä¸ºç”¨æˆ·æä¾›äº†æœ€èˆ’é€‚çš„é˜…è¯»ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10879v1">PDF</a> To be published in COLING 2025 Proceedings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•é”™è¯¯è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†è¯¯å·®åˆ†ä¸ºå››ä¸ªä¸¥é‡ç¨‹åº¦çº§åˆ«å¹¶è¿›ä¸€æ­¥åˆ’åˆ†äºšå‹ï¼ŒåŸºäºå®¢è§‚è¯­è¨€æ ‡å‡†ã€ä¸Šä¸‹æ–‡æ¨¡å¼ä»¥åŠä»¥å†…å®¹è¯ä½œä¸ºåˆ†æå•ä½è¿›è¡Œè¯„ä¼°ã€‚æ–°æ–¹æ³•åº”ç”¨äºæ³•è¯­é¢†åŸŸçš„åä¸ªå‰æ²¿ASRç³»ç»ŸåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å„ç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œç‰¹åˆ«æ˜¯ç”¨æˆ·é˜…è¯»ä½“éªŒæ–¹é¢çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRè½¬å½•é”™è¯¯çš„è¯„ä¼°å¸¸ä½¿ç”¨ä¸å‚è€ƒè½¬å½•å¯¹æ¯”çš„åº¦é‡æŒ‡æ ‡ï¼Œå¦‚Word Error Rateï¼ˆWERï¼‰å’Œè¯­ä¹‰è¯„åˆ†ã€‚</li>
<li>æ–°æå‡ºçš„è¯„ä¼°æ–¹æ³•å°†è¯¯å·®åˆ†ä¸ºå››ä¸ªä¸¥é‡ç¨‹åº¦çº§åˆ«å¹¶ç»†åˆ†äºšå‹ï¼Œè€ƒè™‘å®¢è§‚è¯­è¨€æ ‡å‡†ã€ä¸Šä¸‹æ–‡æ¨¡å¼ã€‚</li>
<li>æ–°æ–¹æ³•ä»¥å†…å®¹è¯ä½œä¸ºåˆ†æå•ä½ï¼Œæ›´æ³¨é‡äººç±»ç†è§£åº¦ã€‚</li>
<li>ç ”ç©¶å¯¹åŸºäºHMMå’Œç«¯åˆ°ç«¯æ¨¡å‹çš„å…ˆè¿›ASRç³»ç»Ÿåœ¨æ³•è¯­ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ç»“æœæ­ç¤ºäº†å„ASRç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ã€‚</li>
<li>æ–°çš„è¯„ä¼°æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å‡ºä¸ºç”¨æˆ·æä¾›æœ€èˆ’é€‚é˜…è¯»ä½“éªŒçš„ASRç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-28fcd0ca65e1e3379e6bd46834e7a5a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9784811288777f0f8b5c2a1ad8736396.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b9200bd7bb8070578a53d2cda6b99cc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FlashSR-One-step-Versatile-Audio-Super-resolution-via-Diffusion-Distillation"><a href="#FlashSR-One-step-Versatile-Audio-Super-resolution-via-Diffusion-Distillation" class="headerlink" title="FlashSR: One-step Versatile Audio Super-resolution via Diffusion   Distillation"></a>FlashSR: One-step Versatile Audio Super-resolution via Diffusion   Distillation</h2><p><strong>Authors:Jaekwon Im, Juhan Nam</strong></p>
<p>Versatile audio super-resolution (SR) is the challenging task of restoring high-frequency components from low-resolution audio with sampling rates between 4kHz and 32kHz in various domains such as music, speech, and sound effects. Previous diffusion-based SR methods suffer from slow inference due to the need for a large number of sampling steps. In this paper, we introduce FlashSR, a single-step diffusion model for versatile audio super-resolution aimed at producing 48kHz audio. FlashSR achieves fast inference by utilizing diffusion distillation with three objectives: distillation loss, adversarial loss, and distribution-matching distillation loss. We further enhance performance by proposing the SR Vocoder, which is specifically designed for SR models operating on mel-spectrograms. FlashSR demonstrates competitive performance with the current state-of-the-art model in both objective and subjective evaluations while being approximately 22 times faster. </p>
<blockquote>
<p>å¤šåŠŸèƒ½éŸ³é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå³ä»é‡‡æ ·ç‡ä¸º4kHzè‡³32kHzçš„ä½åˆ†è¾¨ç‡éŸ³é¢‘ä¸­æ¢å¤å„ç§é¢†åŸŸï¼ˆå¦‚éŸ³ä¹ã€è¯­éŸ³å’ŒéŸ³æ•ˆï¼‰çš„é«˜é¢‘æˆåˆ†ã€‚ä¹‹å‰çš„åŸºäºæ‰©æ•£çš„SRæ–¹æ³•ç”±äºéœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†FlashSRï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šåŠŸèƒ½éŸ³é¢‘è¶…åˆ†è¾¨ç‡çš„å•æ­¥æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨ç”Ÿæˆ48kHzéŸ³é¢‘ã€‚FlashSRé€šè¿‡åˆ©ç”¨æ‰©æ•£è’¸é¦çš„ä¸‰ä¸ªç›®æ ‡æ¥å®ç°å¿«é€Ÿæ¨ç†ï¼ŒåŒ…æ‹¬è’¸é¦æŸå¤±ã€å¯¹æŠ—æŸå¤±å’Œåˆ†å¸ƒåŒ¹é…è’¸é¦æŸå¤±ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡æå‡ºä¸“é—¨ç”¨äºæ“ä½œæ¢…å°”é¢‘è°±å›¾çš„SR Vocoderæ¥æé«˜æ€§èƒ½ã€‚FlashSRåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡è¡¨ç°å‡ºä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ï¼ŒåŒæ—¶é€Ÿåº¦å¤§çº¦å¿«22å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10807v1">PDF</a> 4 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFlashSRçš„å•æ­¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºé€šç”¨éŸ³é¢‘è¶…åˆ†è¾¨ç‡ï¼Œæ—¨åœ¨ç”Ÿæˆ48kHzéŸ³é¢‘ã€‚å®ƒé€šè¿‡åˆ©ç”¨æ‰©æ•£è’¸é¦æŠ€æœ¯å®ç°å¿«é€Ÿæ¨ç†ï¼Œå¹¶å®ç°äº†ä¸‰ä¸ªç›®æ ‡ï¼šè’¸é¦æŸå¤±ã€å¯¹æŠ—æŸå¤±å’Œåˆ†å¸ƒåŒ¹é…è’¸é¦æŸå¤±ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸“é—¨ä¸ºæ¢…å°”é¢‘è°±å›¾è®¾è®¡çš„SR Vocoderï¼Œä»¥æé«˜æ€§èƒ½ã€‚FlashSRåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­éƒ½è¡¨ç°å‡ºä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ï¼Œå¹¶ä¸”é€Ÿåº¦æé«˜äº†çº¦22å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlashSRæ˜¯ä¸€ç§ç”¨äºéŸ³é¢‘è¶…åˆ†è¾¨ç‡çš„å•æ­¥æ‰©æ•£æ¨¡å‹ï¼Œå¯ç”Ÿæˆé«˜è´¨é‡48kHzéŸ³é¢‘ã€‚</li>
<li>é€šè¿‡æ‰©æ•£è’¸é¦æŠ€æœ¯å®ç°å¿«é€Ÿæ¨ç†ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>å®ç°ä¸‰ä¸ªç›®æ ‡ï¼šè’¸é¦æŸå¤±ã€å¯¹æŠ—æŸå¤±å’Œåˆ†å¸ƒåŒ¹é…è’¸é¦æŸå¤±ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥SR Vocoderï¼Œä¸“é—¨ä¸ºæ¢…å°”é¢‘è°±å›¾è®¾è®¡çš„å·¥å…·ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>FlashSRåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ã€‚</li>
<li>FlashSRæ¨¡å‹æ¨ç†é€Ÿåº¦å¤§å¹…æé«˜ï¼Œçº¦æ˜¯ç°æœ‰æ–¹æ³•çš„22å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c7110e7d2b2277916d626724ff686f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4d8c34cb1bc5bd2a16cc5180e2fe926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b30569852bb09293bb72d8772e8bcaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faab36a0f4f090e3db62744bd3586f5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6857988b08e164a01853b7c55b382965.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd6a5f4e686a2d7113886da2998e3e5d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GEC-RAG-Improving-Generative-Error-Correction-via-Retrieval-Augmented-Generation-for-Automatic-Speech-Recognition-Systems"><a href="#GEC-RAG-Improving-Generative-Error-Correction-via-Retrieval-Augmented-Generation-for-Automatic-Speech-Recognition-Systems" class="headerlink" title="GEC-RAG: Improving Generative Error Correction via Retrieval-Augmented   Generation for Automatic Speech Recognition Systems"></a>GEC-RAG: Improving Generative Error Correction via Retrieval-Augmented   Generation for Automatic Speech Recognition Systems</h2><p><strong>Authors:Amin Robatian, Mohammad Hajipour, Mohammad Reza Peyghan, Fatemeh Rajabi, Sajjad Amini, Shahrokh Ghaemmaghami, Iman Gholampour</strong></p>
<p>Automatic Speech Recognition (ASR) systems have demonstrated remarkable performance across various applications. However, limited data and the unique language features of specific domains, such as low-resource languages, significantly degrade their performance and lead to higher Word Error Rates (WER). In this study, we propose Generative Error Correction via Retrieval-Augmented Generation (GEC-RAG), a novel approach designed to improve ASR accuracy for low-resource domains, like Persian. Our approach treats the ASR system as a black-box, a common practice in cloud-based services, and proposes a Retrieval-Augmented Generation (RAG) approach within the In-Context Learning (ICL) scheme to enhance the quality of ASR predictions. By constructing a knowledge base that pairs ASR predictions (1-best and 5-best hypotheses) with their corresponding ground truths, GEC-RAG retrieves lexically similar examples to the ASR transcription using the Term Frequency-Inverse Document Frequency (TF-IDF) measure. This process provides relevant error patterns of the system alongside the ASR transcription to the Generative Large Language Model (LLM), enabling targeted corrections. Our results demonstrate that this strategy significantly reduces WER in Persian and highlights a potential for domain adaptation and low-resource scenarios. This research underscores the effectiveness of using RAG in enhancing ASR systems without requiring direct model modification or fine-tuning, making it adaptable to any domain by simply updating the transcription knowledge base with domain-specific data. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨å„ç§åº”ç”¨ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç‰¹å®šé¢†åŸŸçš„æœ‰é™æ•°æ®å’Œç‹¬ç‰¹è¯­è¨€ç‰¹å¾ï¼Œå¦‚ä½èµ„æºè¯­è¨€ï¼Œä¼šæ˜¾è‘—åœ°é™ä½å…¶æ€§èƒ½ï¼Œå¯¼è‡´æ›´é«˜çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¿›è¡Œç”Ÿæˆé”™è¯¯æ ¡æ­£ï¼ˆGEC-RAGï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä½èµ„æºé¢†åŸŸï¼ˆå¦‚æ³¢æ–¯è¯­ï¼‰çš„ASRå‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ASRç³»ç»Ÿè§†ä¸ºä¸€ä¸ªé»‘ç›’ï¼Œè¿™æ˜¯äº‘æœåŠ¡ä¸­çš„å¸¸è§åšæ³•ï¼Œå¹¶æå‡ºäº†åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹æ¡ˆå†…ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•ï¼Œä»¥æé«˜ASRé¢„æµ‹çš„è´¨é‡ã€‚é€šè¿‡æ„å»ºé…å¯¹ASRé¢„æµ‹ï¼ˆæœ€ä½³å‡è®¾å’Œå‰äº”ä¸ªå‡è®¾ï¼‰ä¸å…¶ç›¸åº”çœŸå®å€¼çš„çŸ¥è¯†åº“ï¼ŒGEC-RAGä½¿ç”¨è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ï¼ˆTF-IDFï¼‰åº¦é‡æ¥æ£€ç´¢ä¸ASRè½¬å½•è¯æ±‡ä¸Šç›¸ä¼¼çš„ä¾‹å­ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸ºç³»ç»Ÿç›¸å…³çš„é”™è¯¯æ¨¡å¼ä»¥åŠASRè½¬å½•æä¾›äº†ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»è€Œå®ç°æœ‰é’ˆå¯¹æ€§çš„æ ¡æ­£ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨æ³¢æ–¯è¯­ä¸­æ˜¾è‘—é™ä½äº†WERï¼Œå¹¶çªå‡ºäº†é¢†åŸŸé€‚åº”å’Œä½èµ„æºåœºæ™¯çš„æ½œåŠ›ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†ä½¿ç”¨RAGåœ¨å¢å¼ºASRç³»ç»Ÿæ•ˆæœä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä¸”æ— éœ€è¿›è¡Œç›´æ¥çš„æ¨¡å‹ä¿®æ”¹æˆ–å¾®è°ƒï¼Œé€šè¿‡ç®€å•åœ°æ›´æ–°è½¬å½•çŸ¥è¯†åº“å¹¶å¼•å…¥ç‰¹å®šé¢†åŸŸçš„æ•°æ®ï¼Œå³å¯é€‚åº”ä»»ä½•é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10734v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨ä½èµ„æºé¢†åŸŸå¦‚æ³¢æ–¯è¯­ä¸­çš„æ€§èƒ½é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç”Ÿæˆè¯¯å·®æ ¡æ­£ï¼ˆGEC-RAGï¼‰æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†ASRç³»ç»Ÿè§†ä¸ºé»‘ç›’ï¼Œé€šè¿‡æ„å»ºä¸ASRé¢„æµ‹åŠå…¶å¯¹åº”çœŸå®å€¼ç›¸åŒ¹é…çš„çŸ¥è¯†åº“ï¼Œä½¿ç”¨TF-IDFåº¦é‡æ ‡å‡†æ£€ç´¢ä¸ASRè½¬å½•è¯æ±‡ç›¸ä¼¼çš„ä¾‹å­ã€‚è¿™ç§æ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘ç³»ç»Ÿé”™è¯¯æ¨¡å¼ï¼Œé’ˆå¯¹ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé’ˆå¯¹æ€§æ ¡æ­£ï¼Œæ˜¾è‘—æé«˜æ³¢æ–¯è¯­çš„ASRå‡†ç¡®æ€§ã€‚æ­¤æ–¹æ³•æ— éœ€å¯¹æ¨¡å‹è¿›è¡Œç›´æ¥ä¿®æ”¹æˆ–å¾®è°ƒï¼Œä»…é€šè¿‡æ›´æ–°è½¬å½•çŸ¥è¯†åº“å³å¯é€‚åº”ä»»ä½•é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRç³»ç»Ÿåœ¨ä½èµ„æºé¢†åŸŸå¦‚æ³¢æ–¯è¯­ä¸­é¢ä¸´æ€§èƒ½æŒ‘æˆ˜ï¼Œå¯¼è‡´è¾ƒé«˜çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GEC-RAGï¼Œæ—¨åœ¨æé«˜ASRç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</li>
<li>GEC-RAGé‡‡ç”¨é»‘ç›’æ–¹å¼å¤„ç†ASRç³»ç»Ÿï¼Œé€‚åº”äº‘æœåŠ¡çš„å¸¸è§å®è·µã€‚</li>
<li>é€šè¿‡æ„å»ºçŸ¥è¯†åº“å’Œé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¡†æ¶ä¸‹æé«˜ASRé¢„æµ‹è´¨é‡ã€‚</li>
<li>ä½¿ç”¨TF-IDFåº¦é‡æ ‡å‡†æ£€ç´¢ä¸ASRè½¬å½•è¯æ±‡ç›¸ä¼¼çš„ä¾‹å­ï¼Œä»¥è·å–ç³»ç»Ÿçš„ç›¸å…³é”™è¯¯æ¨¡å¼ã€‚</li>
<li>GEC-RAGèƒ½æ˜¾è‘—å‡å°‘æ³¢æ–¯è¯­çš„WERï¼Œå¹¶å±•ç¤ºåœ¨é¢†åŸŸé€‚åº”å’Œä½èµ„æºåœºæ™¯ä¸‹çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-61a219e26e8b6dd09cc25af66f1aef3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9677f02f300557b46e2468306feebfc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19549dc605a17bedff36f250f3214da3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aba9baeb33082b1e61216ffe33d7a557.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38a6f6cfb2c048464c95051552dea90b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdfe18539015dfd8faa1e4cb33ef5e96.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PIER-A-Novel-Metric-for-Evaluating-What-Matters-in-Code-Switching"><a href="#PIER-A-Novel-Metric-for-Evaluating-What-Matters-in-Code-Switching" class="headerlink" title="PIER: A Novel Metric for Evaluating What Matters in Code-Switching"></a>PIER: A Novel Metric for Evaluating What Matters in Code-Switching</h2><p><strong>Authors:Enes Yavuz Ugan, Ngoc-Quan Pham, Leonard BÃ¤rmann, Alex Waibel</strong></p>
<p>Code-switching, the alternation of languages within a single discourse, presents a significant challenge for Automatic Speech Recognition. Despite the unique nature of the task, performance is commonly measured with established metrics such as Word-Error-Rate (WER). However, in this paper, we question whether these general metrics accurately assess performance on code-switching. Specifically, using both Connectionist-Temporal-Classification and Encoder-Decoder models, we show fine-tuning on non-code-switched data from both matrix and embedded language improves classical metrics on code-switching test sets, although actual code-switched words worsen (as expected). Therefore, we propose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only on specific words of interest. We instantiate PIER on code-switched utterances and show that this more accurately describes the code-switching performance, showing huge room for improvement in future work. This focused evaluation allows for a more precise assessment of model performance, particularly in challenging aspects such as inter-word and intra-word code-switching. </p>
<blockquote>
<p>è¯­è¨€è½¬æ¢ï¼ˆcode-switchingï¼‰æŒ‡çš„æ˜¯åœ¨åŒä¸€è¯­å¢ƒä¸­äº¤æ›¿ä½¿ç”¨ä¸åŒçš„è¯­è¨€ï¼Œè¿™å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ„æˆäº†ä¸€å¤§æŒ‘æˆ˜ã€‚å°½ç®¡è¿™é¡¹ä»»åŠ¡å…·æœ‰ç‹¬ç‰¹æ€§ï¼Œä½†å…¶æ€§èƒ½é€šå¸¸ä½¿ç”¨æ—¢å®šçš„æŒ‡æ ‡ï¼ˆå¦‚å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼‰æ¥è¡¡é‡ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è´¨ç–‘è¿™äº›é€šç”¨æŒ‡æ ‡æ˜¯å¦å‡†ç¡®è¯„ä¼°äº†è¯­è¨€è½¬æ¢çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿æ¥æ—¶åºåˆ†ç±»å’Œç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œå±•ç¤ºäº†é€šè¿‡å¯¹æ¥è‡ªçŸ©é˜µå’ŒåµŒå…¥å¼è¯­è¨€çš„éä»£ç è½¬æ¢æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥åœ¨ä»£ç è½¬æ¢æµ‹è¯•é›†ä¸Šæé«˜ç»å…¸æŒ‡æ ‡çš„æ€§èƒ½ï¼Œå°½ç®¡å®é™…çš„ä»£ç è½¬æ¢å•è¯æœ‰æ‰€æ¶åŒ–ï¼ˆç¬¦åˆé¢„æœŸï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å…´è¶£ç‚¹é”™è¯¯ç‡ï¼ˆPIERï¼‰ï¼Œå®ƒæ˜¯WERçš„ä¸€ç§å˜ä½“ï¼Œåªå…³æ³¨ç‰¹å®šçš„å…´è¶£è¯æ±‡ã€‚æˆ‘ä»¬åœ¨ä»£ç è½¬æ¢çš„ç‰‡æ®µä¸­å®ä¾‹åŒ–PIERï¼Œå¹¶è¯æ˜å®ƒèƒ½æ›´å‡†ç¡®åœ°æè¿°ä»£ç è½¬æ¢çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºæœªæ¥å·¥ä½œä¸­æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„è¯„ä¼°å…è®¸å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œæ›´ç²¾ç¡®çš„è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯é—´å’Œè¯å†…ä»£ç è½¬æ¢ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09512v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä»£ç åˆ‡æ¢ï¼ˆä¸åŒè¯­è¨€åœ¨å•ä¸€è¯­å¢ƒä¸­çš„äº¤æ›¿ä½¿ç”¨ï¼‰å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ç°æœ‰è¯„ä¼°æŒ‡æ ‡å¦‚å­—é”™è¯¯ç‡ï¼ˆWERï¼‰æœªèƒ½å‡†ç¡®è¡¡é‡ASRåœ¨ä»£ç åˆ‡æ¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç ”ç©¶é€šè¿‡é‡‡ç”¨è¿æ¥æ—¶åºåˆ†ç±»å’Œç¼–ç å™¨è§£ç å™¨æ¨¡å‹å‘ç°ï¼Œåœ¨éä»£ç åˆ‡æ¢æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒè™½èƒ½æé«˜ç»å…¸æŒ‡æ ‡åœ¨ä»£ç åˆ‡æ¢æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ï¼Œä½†å®é™…çš„ä»£ç åˆ‡æ¢è¯æ±‡è¡¨ç°ä»ä¼šä¸‹é™ã€‚å› æ­¤ï¼Œæå‡ºä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”å…´è¶£ç‚¹é”™è¯¯ç‡ï¼ˆPIERï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…³æ³¨ç‰¹å®šå…´è¶£è¯æ±‡çš„WERå˜ä½“ã€‚åœ¨ä»£ç åˆ‡æ¢çš„è¯­éŸ³ç‰‡æ®µä¸­å®æ–½PIERï¼Œèƒ½æ›´å‡†ç¡®åœ°æè¿°ä»£ç åˆ‡æ¢æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºæœªæ¥æ”¹è¿›çš„å·¨å¤§ç©ºé—´ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„è¯„ä¼°æ–¹æ³•èƒ½æ›´ç²¾ç¡®åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨è¯å’Œè¯å†…ä»£ç åˆ‡æ¢ç­‰æŒ‘æˆ˜æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ„æˆæŒ‘æˆ˜ï¼Œéœ€è¦ç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡å¦‚å­—é”™è¯¯ç‡ï¼ˆWERï¼‰æœªèƒ½å‡†ç¡®è¡¡é‡ASRåœ¨ä»£ç åˆ‡æ¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>éä»£ç åˆ‡æ¢æ•°æ®çš„å¾®è°ƒèƒ½æé«˜ç»å…¸æŒ‡æ ‡åœ¨ä»£ç åˆ‡æ¢æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ï¼Œä½†å®é™…åº”ç”¨ä¸­ä»£ç åˆ‡æ¢è¯æ±‡çš„è¡¨ç°ä»ç„¶ä¸ä½³ã€‚</li>
<li>æå‡ºæ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”å…´è¶£ç‚¹é”™è¯¯ç‡ï¼ˆPIERï¼‰ï¼Œä¸“æ³¨äºç‰¹å®šè¯æ±‡çš„è¯„ä¼°ï¼Œæ›´èƒ½å‡†ç¡®åæ˜ ä»£ç åˆ‡æ¢çš„æ€§èƒ½ã€‚</li>
<li>PIERåœ¨ä»£ç åˆ‡æ¢çš„è¯­éŸ³ç‰‡æ®µä¸­å®æ–½ï¼Œæ˜¾ç¤ºå‡ºæœªæ¥æ”¹è¿›çš„å·¨å¤§ç©ºé—´ã€‚</li>
<li>é’ˆå¯¹æ€§çš„è¯„ä¼°æ–¹æ³•èƒ½æ›´ç²¾ç¡®åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨è¯å’Œè¯å†…ä»£ç åˆ‡æ¢ç­‰æŒ‘æˆ˜æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59274d75199b99016a6ba80fe1e1de23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c187cfffd44e50ad1c3a36a94339a29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3dbc2713c789a5ac63ea18ed99c05e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec41090a2a3cd6a3668b66f99bffe23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c15603fc7336fafc052fb7b4b16b17a5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Multimodal-Marvels-of-Deep-Learning-in-Medical-Diagnosis-A-Comprehensive-Review-of-COVID-19-Detection"><a href="#Multimodal-Marvels-of-Deep-Learning-in-Medical-Diagnosis-A-Comprehensive-Review-of-COVID-19-Detection" class="headerlink" title="Multimodal Marvels of Deep Learning in Medical Diagnosis: A   Comprehensive Review of COVID-19 Detection"></a>Multimodal Marvels of Deep Learning in Medical Diagnosis: A   Comprehensive Review of COVID-19 Detection</h2><p><strong>Authors:Md Shofiqul Islam, Khondokar Fida Hasan, Hasibul Hossain Shajeeb, Humayan Kabir Rana, Md Saifur Rahmand, Md Munirul Hasan, AKM Azad, Ibrahim Abdullah, Mohammad Ali Moni</strong></p>
<p>This study presents a comprehensive review of the potential of multimodal deep learning (DL) in medical diagnosis, using COVID-19 as a case example. Motivated by the success of artificial intelligence applications during the COVID-19 pandemic, this research aims to uncover the capabilities of DL in disease screening, prediction, and classification, and to derive insights that enhance the resilience, sustainability, and inclusiveness of science, technology, and innovation systems. Adopting a systematic approach, we investigate the fundamental methodologies, data sources, preprocessing steps, and challenges encountered in various studies and implementations. We explore the architecture of deep learning models, emphasising their data-specific structures and underlying algorithms. Subsequently, we compare different deep learning strategies utilised in COVID-19 analysis, evaluating them based on methodology, data, performance, and prerequisites for future research. By examining diverse data types and diagnostic modalities, this research contributes to scientific understanding and knowledge of the multimodal application of DL and its effectiveness in diagnosis. We have implemented and analysed 11 deep learning models using COVID-19 image, text, and speech (ie, cough) data. Our analysis revealed that the MobileNet model achieved the highest accuracy of 99.97% for COVID-19 image data and 93.73% for speech data (i.e., cough). However, the BiGRU model demonstrated superior performance in COVID-19 text classification with an accuracy of 99.89%. The broader implications of this research suggest potential benefits for other domains and disciplines that could leverage deep learning techniques for image, text, and speech analysis. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å…¨é¢å›é¡¾äº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä»¥COVID-19ä½œä¸ºæ¡ˆä¾‹è¿›è¡Œè¯´æ˜ã€‚æœ¬ç ”ç©¶å—COVID-19ç–«æƒ…æœŸé—´äººå·¥æ™ºèƒ½åº”ç”¨æˆåŠŸçš„å¯å‘ï¼Œæ—¨åœ¨æ­ç¤ºæ·±åº¦å­¦ä¹ åœ¨ç–¾ç—…ç­›æŸ¥ã€é¢„æµ‹å’Œåˆ†ç±»æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å€Ÿæ­¤è·å¾—èƒ½å¤Ÿå¢å¼ºç§‘å­¦ã€æŠ€æœ¯å’Œåˆ›æ–°ç³»ç»ŸéŸ§æ€§ã€å¯æŒç»­æ€§å’ŒåŒ…å®¹æ€§çš„è§è§£ã€‚æˆ‘ä»¬é‡‡ç”¨ç³»ç»Ÿæ–¹æ³•ï¼Œç ”ç©¶å„ç§ç ”ç©¶å’Œå®è·µä¸­çš„åŸºæœ¬æ–¹æ³•ã€æ•°æ®æ¥æºã€é¢„å¤„ç†æ­¥éª¤å’Œé‡åˆ°çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¢è®¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¶æ„ï¼Œé‡ç‚¹ä»‹ç»å…¶æ•°æ®ç‰¹å®šç»“æ„å’ŒåŸºç¡€ç®—æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹COVID-19åˆ†æä¸­ä½¿ç”¨çš„ä¸åŒæ·±åº¦å­¦ä¹ ç­–ç•¥è¿›è¡Œæ¯”è¾ƒï¼Œæ ¹æ®æ–¹æ³•ã€æ•°æ®ã€æ€§èƒ½å’Œæœªæ¥ç ”ç©¶çš„å‰ææ¡ä»¶è¿›è¡Œè¯„ä¼°ã€‚æœ¬ç ”ç©¶é€šè¿‡æ£€æŸ¥å¤šç§æ•°æ®ç±»å‹å’Œè¯Šæ–­æ¨¡å¼ï¼Œæœ‰åŠ©äºç†è§£å’Œè®¤è¯†æ·±åº¦å­¦ä¹ å¤šæ¨¡æ€åº”ç”¨çš„ç§‘å­¦çŸ¥è¯†åŠå…¶è¯Šæ–­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨å’Œåˆ†æäº†11ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨COVID-19å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ï¼ˆå³å’³å—½å£°ï¼‰æ•°æ®ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼ŒMobileNetæ¨¡å‹å¯¹COVID-19å›¾åƒæ•°æ®çš„å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾99.97%ï¼Œå¯¹è¯­éŸ³æ•°æ®çš„å‡†ç¡®ç‡ä¸º93.73%ï¼ˆå³å’³å—½å£°è¯†åˆ«ï¼‰ã€‚ç„¶è€Œï¼ŒBiGRUæ¨¡å‹åœ¨COVID-19æ–‡æœ¬åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º99.89%ã€‚è¿™é¡¹ç ”ç©¶çš„æ›´å¹¿æ³›æ„ä¹‰è¡¨æ˜ï¼Œå…¶ä»–é¢†åŸŸå’Œå­¦ç§‘ä¹Ÿæœ‰å¯èƒ½å—ç›Šäºæ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³åˆ†æä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09506v2">PDF</a> 43 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å…¨é¢ç»¼è¿°äº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œä»¥COVID-19ä¸ºä¾‹ã€‚è¯¥ç ”ç©¶æ—¨åœ¨æ­ç¤ºæ·±åº¦å­¦ä¹ åœ¨ç–¾ç—…ç­›æŸ¥ã€é¢„æµ‹å’Œåˆ†ç±»æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶ä»ç§‘å­¦ã€æŠ€æœ¯å’Œåˆ›æ–°ç³»ç»Ÿçš„éŸ§æ€§ã€å¯æŒç»­æ€§å’ŒåŒ…å®¹æ€§ä¸­æ±²å–è§è§£ã€‚æ–‡ç« é‡‡ç”¨ç³»ç»Ÿæ–¹æ³•ï¼Œæ¢è®¨äº†å„ç§ç ”ç©¶å’Œå®è·µä¸­çš„åŸºæœ¬æ–¹æ³•ã€æ•°æ®æ¥æºã€é¢„å¤„ç†æ­¥éª¤å’Œé‡åˆ°çš„æŒ‘æˆ˜ã€‚æ–‡ç« æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¶æ„ï¼Œå¼ºè°ƒå…¶æ•°æ®ç‰¹å®šç»“æ„å’ŒåŸºç¡€ç®—æ³•ã€‚é€šè¿‡å¯¹COVID-19åˆ†æä¸­ä½¿ç”¨çš„ä¸åŒæ·±åº¦å­¦ä¹ ç­–ç•¥è¿›è¡Œæ¯”è¾ƒï¼Œæœ¬æ–‡è¯„ä¼°äº†å®ƒä»¬åœ¨æ–¹æ³•ã€æ•°æ®ã€æ€§èƒ½å’Œæœªæ¥ç ”ç©¶éœ€æ±‚æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡å¯¹å¤šç§æ•°æ®ç±»å‹å’Œè¯Šæ–­æ–¹å¼çš„ç ”ç©¶ï¼Œæœ¬æ–‡ä¸ºæ·±åº¦å­¦ä¹ åœ¨å¤šæ¨¡æ€åº”ç”¨ä¸­çš„ç§‘å­¦ç†è§£å’Œæœ‰æ•ˆæ€§è¯Šæ–­åšå‡ºäº†è´¡çŒ®ã€‚ç ”ç©¶å‘ç°ï¼ŒMobileNetæ¨¡å‹åœ¨COVID-19å›¾åƒå’Œè¯­éŸ³æ•°æ®ä¸Šçš„å‡†ç¡®ç‡æœ€é«˜ï¼Œåˆ†åˆ«ä¸º99.97%å’Œ93.73%ï¼ˆå³å’³å—½å£°ï¼‰ã€‚ç„¶è€Œï¼ŒBiGRUæ¨¡å‹åœ¨COVID-19æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º99.89%ã€‚è¯¥ç ”ç©¶çš„æ›´å¹¿æ³›å½±å“è¡¨æ˜ï¼Œå…¶ä»–é¢†åŸŸå’Œå­¦ç§‘ä¹Ÿå¯èƒ½ä»æ·±åº¦å­¦ä¹ ä¸­å—ç›Šï¼Œç”¨äºå›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³åˆ†æã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦è¯Šæ–­ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨COVID-19ç­‰ç–¾ç—…çš„ç­›æŸ¥ã€é¢„æµ‹å’Œåˆ†ç±»æ–¹é¢ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨COVID-19å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³æ•°æ®ä¸Šçš„è¡¨ç°ï¼Œå‘ç°MobileNetæ¨¡å‹åœ¨å›¾åƒå’Œè¯­éŸ³æ•°æ®ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒBiGRUæ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ•°æ®ç‰¹å®šç»“æ„å’ŒåŸºç¡€ç®—æ³•çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºäº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®ç±»å‹å¤šæ ·æ€§å’Œè¯Šæ–­æ–¹å¼çš„å¤æ‚æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯èƒ½å¤Ÿæé«˜ç§‘å­¦ã€æŠ€æœ¯å’Œåˆ›æ–°ç³»ç»Ÿçš„éŸ§æ€§ã€å¯æŒç»­æ€§å’ŒåŒ…å®¹æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶çš„å‘ç°ä¸ä»…é™äºåŒ»å­¦é¢†åŸŸï¼Œå¯¹å…¶ä»–éœ€è¦å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³åˆ†æçš„é¢†åŸŸä¹Ÿæœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74f86bf3b7dd3d3932c7ba39bb10090c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8252398af67b74141fcbd6291b255a1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abcb8d99a7f8560b0bf3c11d50be1e77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28e5bcced15d881ab182c2f4028cd717.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a16057f105f4272461c74284394bb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-164a52d64536f7d41479e4d42e09553f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Comparing-Self-Supervised-Learning-Models-Pre-Trained-on-Human-Speech-and-Animal-Vocalizations-for-Bioacoustics-Processing"><a href="#Comparing-Self-Supervised-Learning-Models-Pre-Trained-on-Human-Speech-and-Animal-Vocalizations-for-Bioacoustics-Processing" class="headerlink" title="Comparing Self-Supervised Learning Models Pre-Trained on Human Speech   and Animal Vocalizations for Bioacoustics Processing"></a>Comparing Self-Supervised Learning Models Pre-Trained on Human Speech   and Animal Vocalizations for Bioacoustics Processing</h2><p><strong>Authors:Eklavya Sarkar, Mathew Magimai. -Doss</strong></p>
<p>Self-supervised learning (SSL) foundation models have emerged as powerful, domain-agnostic, general-purpose feature extractors applicable to a wide range of tasks. Such models pre-trained on human speech have demonstrated high transferability for bioacoustic processing. This paper investigates (i) whether SSL models pre-trained directly on animal vocalizations offer a significant advantage over those pre-trained on speech, and (ii) whether fine-tuning speech-pretrained models on automatic speech recognition (ASR) tasks can enhance bioacoustic classification. We conduct a comparative analysis using three diverse bioacoustic datasets and two different bioacoustic tasks. Results indicate that pre-training on bioacoustic data provides only marginal improvements over speech-pretrained models, with comparable performance in most scenarios. Fine-tuning on ASR tasks yields mixed outcomes, suggesting that the general-purpose representations learned during SSL pre-training are already well-suited for bioacoustic tasks. These findings highlight the robustness of speech-pretrained SSL models for bioacoustics and imply that extensive fine-tuning may not be necessary for optimal performance. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åŸºç¡€æ¨¡å‹å·²ç»ä½œä¸ºå¼ºå¤§ã€é¢†åŸŸæ— å…³ã€é€šç”¨çš„ç‰¹å¾æå–å™¨å‡ºç°ï¼Œé€‚ç”¨äºå„ç§ä»»åŠ¡ã€‚è¿™ç§åœ¨äººç±»è¯­éŸ³ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºåœ¨ç”Ÿç‰©å£°éŸ³å¤„ç†ä¸Šçš„é«˜å¯è¿ç§»æ€§ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶ï¼ˆiï¼‰ç›´æ¥å¯¹åŠ¨ç‰©é¸£å«å£°è¿›è¡Œé¢„è®­ç»ƒçš„è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹æ˜¯å¦ç›¸å¯¹äºåœ¨è¯­éŸ³ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼›ï¼ˆiiï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸Šå¯¹è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæ˜¯å¦èƒ½æé«˜ç”Ÿç‰©å£°éŸ³åˆ†ç±»èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªä¸åŒçš„ç”Ÿç‰©å£°éŸ³æ•°æ®é›†å’Œä¸¤ä¸ªä¸åŒçš„ç”Ÿç‰©å£°éŸ³ä»»åŠ¡è¿›è¡Œäº†å¯¹æ¯”åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ç”Ÿç‰©å£°éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒåªåœ¨å¯¹è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šæœ‰å¾®å°çš„æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ€§èƒ½ç›¸å½“ã€‚åœ¨ASRä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒä¼šäº§ç”Ÿæ··åˆçš„ç»“æœï¼Œè¿™è¡¨æ˜åœ¨SSLé¢„è®­ç»ƒæœŸé—´å­¦ä¹ çš„é€šç”¨è¡¨ç¤ºå·²ç»éå¸¸é€‚åˆç”Ÿç‰©å£°éŸ³ä»»åŠ¡ã€‚è¿™äº›å‘ç°çªå‡ºäº†è¯­éŸ³é¢„è®­ç»ƒSSLæ¨¡å‹åœ¨ç”Ÿç‰©å£°å­¦æ–¹é¢çš„ç¨³å¥æ€§ï¼Œå¹¶ä¸”æš—ç¤ºå¯èƒ½ä¸éœ€è¦å¤§é‡å¾®è°ƒå°±èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05987v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬ç ”ç©¶æ¢è®¨äº†è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹åœ¨åŠ¨ç‰©å‘å£°å’Œè¯­éŸ³ä¸Šçš„é¢„è®­ç»ƒå¯¹ç”Ÿç‰©å£°å­¦å¤„ç†çš„ä¼˜è¶Šæ€§ï¼Œå¹¶ç ”ç©¶äº†åœ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šå¾®è°ƒè¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹å¯¹ç”Ÿç‰©å£°å­¦åˆ†ç±»çš„å¢å¼ºæ•ˆæœã€‚å®éªŒä½¿ç”¨ä¸‰ä¸ªä¸åŒçš„ç”Ÿç‰©å£°å­¦æ•°æ®é›†å’Œä¸¤ä¸ªä»»åŠ¡è¿›è¡Œæ¯”è¾ƒåˆ†æï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨ç”Ÿç‰©å£°å­¦æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒåªå¸¦æ¥äº†å¾®å°çš„æ”¹è¿›ï¼Œåœ¨å¤§å¤šæ•°åœºæ™¯ä¸­æ€§èƒ½ç›¸å½“ã€‚å¯¹è¯­éŸ³è¯†åˆ«ä»»åŠ¡è¿›è¡Œå¾®è°ƒäº§ç”Ÿäº†æ··åˆçš„ç»“æœï¼Œè¿™è¡¨æ˜åœ¨SSLé¢„è®­ç»ƒæœŸé—´å­¦ä¹ çš„é€šç”¨è¡¨ç¤ºå·²ç»é€‚åˆç”Ÿç‰©å£°å­¦ä»»åŠ¡ã€‚è¿™äº›å‘ç°çªæ˜¾äº†è¯­éŸ³é¢„è®­ç»ƒSSLæ¨¡å‹åœ¨ç”Ÿç‰©å£°å­¦ä¸­çš„ç¨³å¥æ€§ï¼Œå¹¶æš—ç¤ºå¯¹äºæœ€ä½³æ€§èƒ½å¯èƒ½ä¸éœ€è¦å¤§é‡å¾®è°ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹å·²æˆä¸ºå¼ºå¤§çš„ã€é¢†åŸŸæ— å…³çš„ã€é€šç”¨çš„ç‰¹å¾æå–å™¨ï¼Œé€‚ç”¨äºå„ç§ä»»åŠ¡ã€‚</li>
<li>SSLæ¨¡å‹åœ¨åŠ¨ç‰©å‘å£°ä¸Šçš„é¢„è®­ç»ƒç›¸è¾ƒäºè¯­éŸ³é¢„è®­ç»ƒåªå¸¦æ¥å¾®å°ä¼˜åŠ¿ã€‚</li>
<li>åœ¨å¤§å¤šæ•°åœºæ™¯ä¸­ï¼ŒåŸºäºè¯­éŸ³é¢„è®­ç»ƒçš„SSLæ¨¡å‹ä¸åŸºäºåŠ¨ç‰©å‘å£°é¢„è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ç›¸å½“ã€‚</li>
<li>å¯¹è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡è¿›è¡Œå¾®è°ƒåœ¨ç”Ÿç‰©å£°å­¦åˆ†ç±»ä¸­çš„æ•ˆæœä¸ä¸€ï¼Œè¿™è¡¨æ˜é€šç”¨è¡¨ç¤ºå·²é€‚åˆç”Ÿç‰©å£°å­¦ä»»åŠ¡ã€‚</li>
<li>è¯­éŸ³é¢„è®­ç»ƒçš„SSLæ¨¡å‹åœ¨ç”Ÿç‰©å£°å­¦é¢†åŸŸå…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>å¯¹äºæœ€ä½³æ€§èƒ½ï¼Œå¯èƒ½ä¸éœ€è¦å¤§é‡çš„å¾®è°ƒã€‚</li>
<li>ç ”ç©¶ç»“æœæœ‰åŠ©äºæ›´æ·±å…¥åœ°ç†è§£SSLæ¨¡å‹åœ¨ç”Ÿç‰©å£°å­¦åŠå…¶ä»–é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4944ff86a0237a5406ae4b1bc7864f61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c08c06f9de495e48f294d048cb21c58d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f54bee40c8bf576685b0bb7e8f0e28b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-013734a19434ad3355429ef72837a685.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfa449ef4ffcda8e14f06b120e102d28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19f9ac0a31ade36e9103623a239e9979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1211716696a9632d9e61dc653c8fd7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4afd605b5be960e4be194a3c6151c7d5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦å…³æ³¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„èåˆï¼Œå¯¹è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨é‡è§†ä¸è¶³ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç”±äºåœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¹‹é—´å­˜åœ¨åŸºæœ¬æ¨¡å¼å·®å¼‚ï¼Œå› æ­¤åœ¨ä¸¤è€…ä¸Šå®ç°é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œé€æ­¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆä½¿æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜èƒ½å¤Ÿåœ¨æ²¡æœ‰å•ç‹¬çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°å…ˆè¿›æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œèƒ½å¤Ÿå®ç°è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v3">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a> (2K+ Stars by now)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ†é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œè®©å¤§å‹è¯­è¨€æ¨¡å‹åŒæ—¶ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œå®ç°æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€èƒ½åŠ›ï¼Œè¿˜èƒ½å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—ï¼Œæ˜¾è‘—åŠ é€Ÿäº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå®ç°äº†è¿‘å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ›´å¤šåœ°å…³æ³¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„èåˆï¼Œå¿½ç•¥äº†è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚</li>
<li>è¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼ŒåŒæ—¶åœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­å®ç°é«˜æ€§èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä½¿LLMèƒ½å¤Ÿç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œå®ç°æµç•…çš„å¤šæ¨¡æ€äº¤äº’ã€‚</li>
<li>è¯¥æ–¹æ³•ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€èƒ½åŠ›ï¼ŒåŒæ—¶å¢å¼ºäº†æ¨¡å‹çš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ä¸Šæœ‰æ‰€çªç ´ï¼Œæ˜¾è‘—æé«˜äº†å®æ—¶æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6cc6cb41ad9073971c3311922bdf00ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-458784e265fee8832f68789bf9d105f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e262945a140a0fbcdb7ddae1a7741766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-396c9589614e6d25affa510578046020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-200583adacfa33383773d2e9f3835530.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MathSpeech-Leveraging-Small-LMs-for-Accurate-Conversion-in-Mathematical-Speech-to-Formula"><a href="#MathSpeech-Leveraging-Small-LMs-for-Accurate-Conversion-in-Mathematical-Speech-to-Formula" class="headerlink" title="MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical   Speech-to-Formula"></a>MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical   Speech-to-Formula</h2><p><strong>Authors:Sieun Hyeon, Kyudan Jung, Jaehee Won, Nam-Joon Kim, Hyun Gon Ryu, Hyuk-Jae Lee, Jaeyoung Do</strong></p>
<p>In various academic and professional settings, such as mathematics lectures or research presentations, it is often necessary to convey mathematical expressions orally. However, reading mathematical expressions aloud without accompanying visuals can significantly hinder comprehension, especially for those who are hearing-impaired or rely on subtitles due to language barriers. For instance, when a presenter reads Eulerâ€™s Formula, current Automatic Speech Recognition (ASR) models often produce a verbose and error-prone textual description (e.g., e to the power of i x equals cosine of x plus i $\textit{side}$ of x), instead of the concise $\LaTeX{}$ format (i.e., $ e^{ix} &#x3D; \cos(x) + i\sin(x) $), which hampers clear understanding and communication. To address this issue, we introduce MathSpeech, a novel pipeline that integrates ASR models with small Language Models (sLMs) to correct errors in mathematical expressions and accurately convert spoken expressions into structured $\LaTeX{}$ representations. Evaluated on a new dataset derived from lecture recordings, MathSpeech demonstrates $\LaTeX{}$ generation capabilities comparable to leading commercial Large Language Models (LLMs), while leveraging fine-tuned small language models of only 120M parameters. Specifically, in terms of CER, BLEU, and ROUGE scores for $\LaTeX{}$ translation, MathSpeech demonstrated significantly superior capabilities compared to GPT-4o. We observed a decrease in CER from 0.390 to 0.298, and higher ROUGE&#x2F;BLEU scores compared to GPT-4o. </p>
<blockquote>
<p>åœ¨å„ç§å­¦æœ¯å’Œä¸“ä¸šåœºåˆï¼Œå¦‚æ•°å­¦è®²åº§æˆ–ç ”ç©¶æŠ¥å‘Šä¼šä¸­ï¼Œä¼ è¾¾æ•°å­¦è¡¨è¾¾å¼æ˜¯å¸¸æœ‰çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰è§†è§‰è¾…åŠ©çš„æƒ…å†µä¸‹å¤§å£°é˜…è¯»æ•°å­¦è¡¨è¾¾å¼å¯èƒ½ä¼šæå¤§åœ°é˜»ç¢ç†è§£ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¬åŠ›å—æŸæˆ–ç”±äºè¯­è¨€éšœç¢è€Œä¾èµ–å­—å¹•çš„äººæ¥è¯´ã€‚ä¾‹å¦‚ï¼Œå½“æ¼”è®²è€…æœ—è¯»æ¬§æ‹‰å…¬å¼æ—¶ï¼Œå½“å‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹å¾€å¾€ä¼šç”Ÿæˆå†—é•¿å’Œæ˜“å‡ºé”™çš„æ–‡æœ¬æè¿°ï¼ˆä¾‹å¦‚ï¼Œâ€œeçš„iä¹˜ä»¥xæ¬¡æ–¹ç­‰äºxçš„ä½™å¼¦å€¼åŠ ä¸Šiå€çš„xæ­£å¼¦å€¼â€ï¼‰ï¼Œè€Œä¸æ˜¯ç®€æ´çš„LaTeXæ ¼å¼ï¼ˆå³ï¼Œ$ e^{ix} &#x3D; \cos(x) + i\sin(x) $ï¼‰ã€‚è¿™é˜»ç¢äº†æ¸…æ™°çš„ç†è§£å’Œæ²Ÿé€šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MathSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç®¡é“ï¼Œå®ƒå°†ASRæ¨¡å‹ä¸å°è¯­è¨€æ¨¡å‹ï¼ˆsLMsï¼‰ç»“åˆèµ·æ¥ï¼Œä»¥çº æ­£æ•°å­¦è¡¨è¾¾å¼ä¸­çš„é”™è¯¯ï¼Œå¹¶å°†å£å¤´è¡¨è¾¾å‡†ç¡®è½¬æ¢ä¸ºç»“æ„åŒ–çš„LaTeXè¡¨ç¤ºã€‚åœ¨æ–°çš„æ•°æ®é›†ä¸Šè¯„ä¼°å‘ç°è¯¥æ•°æ®é›†æ¥è‡ªè®²åº§å½•éŸ³ï¼ŒMathSpeechè¡¨ç°å‡ºå¯ä¸é¢†å…ˆå•†ä¸šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸åª²ç¾çš„LaTeXç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶åˆ©ç”¨ä»…åŒ…å«1.2äº¿å‚æ•°çš„å¾®è°ƒå°å‹è¯­è¨€æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨LaTeXç¿»è¯‘çš„CERã€BLEUå’ŒROUGEåˆ†æ•°æ–¹é¢ï¼ŒMathSpeechç›¸å¯¹äºGPT-4oå±•ç°å‡ºæ˜¾è‘—ä¼˜è¶Šçš„èƒ½åŠ›ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°CERä»0.390é™è‡³0.298ï¼Œå¹¶ä¸”ä¸GPT-4oç›¸æ¯”å…·æœ‰æ›´é«˜çš„ROUGE&#x2F;BLEUåˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15655v2">PDF</a> Accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ•°å­¦è¡¨è¾¾å¼åœ¨å­¦æœ¯å’Œä¸“ä¸šåœºåˆçš„å£å¤´ä¼ è¾¾è‡³å…³é‡è¦ï¼Œä½†æ— è§†è§‰è¾…åŠ©æ—¶å­˜åœ¨ç†è§£éšœç¢ã€‚ä¸ºè§£å†³ç°æœ‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨å¤„ç†æ•°å­¦è¡¨è¾¾æ—¶çš„å†—é•¿å’Œè¯¯å·®é—®é¢˜ï¼Œå¼•å…¥MathSpeechç®¡é“ã€‚å®ƒæ•´åˆäº†å°è¯­è¨€æ¨¡å‹ï¼Œèƒ½çº æ­£æ•°å­¦è¡¨è¾¾ä¸­çš„é”™è¯¯ï¼Œå‡†ç¡®å°†å£è¯­è½¬åŒ–ä¸ºç»“æ„åŒ–LaTeXè¡¨ç¤ºã€‚æ–°æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMathSpeechåœ¨LaTeXç”Ÿæˆèƒ½åŠ›ä¸Šè¡¨ç°å“è¶Šï¼Œä¸”ä»…éœ€å¾®è°ƒ12äº¿å‚æ•°çš„å°å‹è¯­è¨€æ¨¡å‹å³å¯å®ç°ã€‚ç›¸è¾ƒäºGPT-4oï¼ŒMathSpeechåœ¨CERã€BLEUå’ŒROUGEå¾—åˆ†ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å£å¤´ä¼ è¾¾æ•°å­¦è¡¨è¾¾åœ¨å­¦æœ¯å’Œä¸“ä¸šåœºåˆå¾ˆé‡è¦ï¼Œä½†ç¼ºä¹è§†è§‰è¾…åŠ©ä¼šå¢åŠ ç†è§£éš¾åº¦ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨å¤„ç†æ•°å­¦è¡¨è¾¾æ—¶å­˜åœ¨å†—é•¿å’Œè¯¯å·®é—®é¢˜ã€‚</li>
<li>MathSpeechæ˜¯ä¸€ä¸ªæ•´åˆäº†å°è¯­è¨€æ¨¡å‹çš„ç®¡é“ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜å¹¶æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>MathSpeechèƒ½å¤Ÿçº æ­£æ•°å­¦è¡¨è¾¾ä¸­çš„é”™è¯¯å¹¶è½¬åŒ–ä¸ºç»“æ„åŒ–çš„LaTeXè¡¨ç¤ºã€‚</li>
<li>MathSpeechåœ¨æ–°æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨ç°å‡ºå“è¶Šçš„LaTeXç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>MathSpeechä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨å‚æ•°è§„æ¨¡è¾ƒå°çš„æ¨¡å‹ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ffa78d2b4f56a9cd4930ad608f51093.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05a5cf8d4ca64c5cc20f2423412a1091.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a592a59eba266bd68116fad348ee50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ab3f9bd2b19eda66d2c923e65210e5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a15f34f1d571524465543c3fd880be7.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2412.15655v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2412.15655v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2412.15655v2/page_4_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Recent-Advances-in-Speech-Language-Models-A-Survey"><a href="#Recent-Advances-in-Speech-Language-Models-A-Survey" class="headerlink" title="Recent Advances in Speech Language Models: A Survey"></a>Recent Advances in Speech Language Models: A Survey</h2><p><strong>Authors:Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, Irwin King</strong></p>
<p>Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of &#96;&#96;Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)â€, where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) â€“ end-to-end models that generate speech without converting from text â€“ have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at <a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿‘æœŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä¸»è¦å› å…¶åŸºäºæ–‡æœ¬çš„äº¤äº’èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè‡ªç„¶äººç±»äº’åŠ¨é€šå¸¸ä¾èµ–äºè¯­éŸ³ï¼Œå› æ­¤éœ€è¦å‘åŸºäºè¯­éŸ³çš„æ¨¡å‹è½¬å˜ã€‚å®ç°è¿™ä¸€ç›®æ ‡çš„ä¸€ç§ç›´æ¥æ–¹æ³•æ¶‰åŠâ€œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰+ LLM +æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰â€çš„ç®¡é“ï¼Œå…¶ä¸­è¾“å…¥è¯­éŸ³è¢«è½¬å½•ä¸ºæ–‡æœ¬ï¼Œç”±LLMå¤„ç†ï¼Œç„¶åè½¬å›è¯­éŸ³ã€‚å°½ç®¡è¿™ç§æ–¹æ³•å¾ˆç›´æ¥ï¼Œä½†å®ƒå­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼Œä¾‹å¦‚åœ¨æ¨¡æ€è½¬æ¢è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŸå¤±ã€ç”±äºå¤æ‚ç®¡é“å¯¼è‡´çš„æ˜¾è‘—å»¶è¿Ÿä»¥åŠä¸‰ä¸ªé˜¶æ®µçš„é”™è¯¯ç´¯ç§¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMï¼‰â€”â€”ä¸€ç§æ— éœ€ä»æ–‡æœ¬è½¬æ¢å³å¯ç”Ÿæˆè¯­éŸ³çš„ç«¯åˆ°ç«¯æ¨¡å‹â€”â€”ä½œä¸ºæœ‰å‰é€”çš„æ›¿ä»£å“åº”è¿è€Œç”Ÿã€‚è¿™ç¯‡ç»¼è¿°è®ºæ–‡é¦–æ¬¡å…¨é¢æ¦‚è¿°äº†æ„å»ºSpeechLMçš„æœ€æ–°æ–¹æ³•ï¼Œè¯¦ç»†æè¿°äº†å…¶æ¶æ„çš„å…³é”®ç»„ä»¶ä»¥åŠå¯¹å…¶å‘å±•è‡³å…³é‡è¦çš„å„ç§åŸ¹è®­é…æ–¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç³»ç»Ÿåœ°è°ƒæŸ¥äº†SpeechLMçš„å„ç§åŠŸèƒ½ï¼Œå¯¹å…¶è¯„ä¼°æŒ‡æ ‡è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è®¨è®ºäº†è¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚GitHubä»“åº“å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey%E3%80%82]%EF%BC%88%E6%B3%A8%EF%BC%9A%E8%AF%A5%E9%93%BE%E6%8E%A5%E4%B8%BA%E8%99%9A%E6%9E%84%E9%93%BE%E6%8E%A5%EF%BC%8C%E5%AE%9E%E9%99%85%E8%AE%BF%E9%97%AE%E5%8F%AF%E8%83%BD%E6%97%A0%E6%B3%95%E6%89%93%E5%BC%80%EF%BC%89">https://github.com/dreamtheater123/Awesome-SpeechLM-Surveyã€‚]ï¼ˆæ³¨ï¼šè¯¥é“¾æ¥ä¸ºè™šæ„é“¾æ¥ï¼Œå®é™…è®¿é—®å¯èƒ½æ— æ³•æ‰“å¼€ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03751v2">PDF</a> Work in progress</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬äº¤äº’ä¸­å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œè‡ªç„¶äººç±»äº¤äº’é€šå¸¸ä¾èµ–äºè¯­éŸ³ï¼Œå› æ­¤éœ€è¦è½¬å‘è¯­éŸ³æ¨¡å‹ã€‚ä¸€ç§å®ç°è¿™ä¸€ç›®æ ‡çš„ç›´æ¥æ–¹æ³•æ˜¯é‡‡ç”¨â€œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰+å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰+æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰â€çš„ç®¡é“ï¼Œå°†è¾“å…¥è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ï¼Œç”±LLMå¤„ç†åå†è½¬å›è¯­éŸ³ã€‚å°½ç®¡è¿™ç§æ–¹æ³•çœ‹ä¼¼ç®€å•ç›´æ¥ï¼Œä½†å®ƒå­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼Œå¦‚æ¨¡æ€è½¬æ¢ä¸­çš„ä¿¡æ¯æŸå¤±ã€å¤æ‚çš„ç®¡é“å¯¼è‡´çš„å»¶è¿Ÿä»¥åŠä¸‰ä¸ªé˜¶æ®µä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMsï¼‰åº”è¿è€Œç”Ÿï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€ä»æ–‡æœ¬è½¬æ¢å³å¯ç”Ÿæˆè¯­éŸ³çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚è¿™ç¯‡ç»¼è¿°è®ºæ–‡é¦–æ¬¡å…¨é¢æ¦‚è¿°äº†æ„å»ºSpeechLMsçš„æœ€æ–°æ–¹æ³•ï¼Œè¯¦ç»†ä»‹ç»äº†å…¶æ¶æ„çš„å…³é”®ç»„ä»¶ä»¥åŠå¼€å‘è¿‡ç¨‹ä¸­ä¸å¯æˆ–ç¼ºçš„å„ç§è®­ç»ƒé…æ–¹ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿåœ°å¯¹SpeechLMsçš„å„é¡¹åŠŸèƒ½è¿›è¡Œäº†æ¦‚è¿°ï¼Œåˆ†ç±»äº†è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®¨è®ºäº†è¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚GitHubä»“åº“åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey%E3%80%82">https://github.com/dreamtheater123/Awesome-SpeechLM-Surveyã€‚</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬äº¤äº’ä¸­å—åˆ°å…³æ³¨ï¼Œä½†è‡ªç„¶äº¤äº’æ›´å¤šä¾èµ–è¯­éŸ³ï¼Œéœ€è¦å‘å±•è¯­éŸ³æ¨¡å‹ã€‚</li>
<li>ç°æœ‰çš„â€œASR+LLM+TTSâ€æ–¹æ³•åœ¨æ¨¡æ€è½¬æ¢ã€å»¶è¿Ÿå’Œè¯¯å·®ç´¯ç§¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMsï¼‰ä½œä¸ºæ— éœ€æ–‡æœ¬è½¬æ¢çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œä¸ºè§£å†³ä¸Šè¿°é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ç»¼è¿°è®ºæ–‡é¦–æ¬¡å…¨é¢æ¦‚è¿°äº†SpeechLMsçš„æ„å»ºæ–¹æ³•ã€æ¶æ„ç»„ä»¶ã€è®­ç»ƒé…æ–¹åŠåŠŸèƒ½ç‰¹ç‚¹ã€‚</li>
<li>SpeechLMsçš„èƒ½åŠ›å¾—åˆ°ç³»ç»Ÿæ¦‚è¿°ï¼Œè¯„ä¼°æŒ‡æ ‡è¢«åˆ†ç±»è®¨è®ºã€‚</li>
<li>è¯¥é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éœ€è¦è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€è§£å†³å®é™…åº”ç”¨ä¸­çš„å¤æ‚æ€§ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2410.03751v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2410.03751v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2410.03751v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2410.03751v2/page_1_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2410.03751v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2410.03751v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2410.03751v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Investigating-Training-Objectives-for-Generative-Speech-Enhancement"><a href="#Investigating-Training-Objectives-for-Generative-Speech-Enhancement" class="headerlink" title="Investigating Training Objectives for Generative Speech Enhancement"></a>Investigating Training Objectives for Generative Speech Enhancement</h2><p><strong>Authors:Julius Richter, Danilo de Oliveira, Timo Gerkmann</strong></p>
<p>Generative speech enhancement has recently shown promising advancements in improving speech quality in noisy environments. Multiple diffusion-based frameworks exist, each employing distinct training objectives and learning techniques. This paper aims to explain the differences between these frameworks by focusing our investigation on score-based generative models and the Schr&quot;odinger bridge. We conduct a series of comprehensive experiments to compare their performance and highlight differing training behaviors. Furthermore, we propose a novel perceptual loss function tailored for the Schr&quot;odinger bridge framework, demonstrating enhanced performance and improved perceptual quality of the enhanced speech signals. All experimental code and pre-trained models are publicly available to facilitate further research and development in this domain. </p>
<blockquote>
<p>ç”Ÿæˆå¼è¯­éŸ³å¢å¼ºåœ¨æ”¹å–„å™ªå£°ç¯å¢ƒä¸­çš„è¯­éŸ³è´¨é‡æ–¹é¢æœ€è¿‘æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„è¿›å±•ã€‚å­˜åœ¨å¤šä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œæ¯ä¸ªæ¡†æ¶é‡‡ç”¨ä¸åŒçš„è®­ç»ƒç›®æ ‡å’Œå­¦ä¹ æŠ€æœ¯ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡é‡ç‚¹ç ”ç©¶åŸºäºè¯„åˆ†çš„ç”Ÿæˆæ¨¡å‹å’Œè–›å®šè°”æ¡¥æ¥è§£é‡Šè¿™äº›æ¡†æ¶ä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—å…¨é¢çš„å®éªŒæ¥æ¯”è¾ƒå®ƒä»¬çš„æ€§èƒ½å¹¶çªå‡ºä¸åŒçš„è®­ç»ƒè¡Œä¸ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é’ˆå¯¹è–›å®šè°”æ¡¥æ¡†æ¶é‡èº«å®šåˆ¶äº†ä¸€ç§æ–°å‹æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œå±•ç¤ºäº†å…¶å¢å¼ºçš„æ€§èƒ½å’Œæ”¹å–„çš„æ„ŸçŸ¥è¯­éŸ³ä¿¡å·è´¨é‡ã€‚æ‰€æœ‰å®éªŒä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å‡å…¬å¼€æä¾›ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10753v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºæŠ€æœ¯åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„è¯­éŸ³è´¨é‡æå‡æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚æ–‡ç« ä¸»è¦èšç„¦äºåŸºäºè¯„åˆ†çš„ç”Ÿæˆæ¨¡å‹å’ŒSchrÃ¶dinger bridgeæ¡†æ¶çš„å·®å¼‚ï¼Œé€šè¿‡ä¸€ç³»åˆ—å®éªŒæ¯”è¾ƒäº†å®ƒä»¬çš„æ€§èƒ½å’Œä¸åŒçš„è®­ç»ƒè¡Œä¸ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹SchrÃ¶dinger bridgeæ¡†æ¶çš„æ–°å‹æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œå±•ç¤ºäº†å…¶æå‡è¯­éŸ³ä¿¡å·å¢å¼ºæ€§èƒ½å’Œæ„ŸçŸ¥è´¨é‡çš„æ½œåŠ›ã€‚æ–‡ç« çš„å®éªŒä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å…¬å¼€ï¼Œæœ‰åŠ©äºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºæŠ€æœ¯åœ¨å™ªå£°ç¯å¢ƒä¸‹æå‡è¯­éŸ³è´¨é‡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>åŸºäºè¯„åˆ†çš„ç”Ÿæˆæ¨¡å‹å’ŒSchrÃ¶dinger bridgeæ¡†æ¶åœ¨ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºé¢†åŸŸä¸­å„å…·ç‰¹è‰²ã€‚</li>
<li>æ–‡ç« é€šè¿‡å…¨é¢å®éªŒæ¯”è¾ƒäº†ä¸åŒæ‰©æ•£æ¡†æ¶çš„æ€§èƒ½å’Œè®­ç»ƒè¡Œä¸ºå·®å¼‚ã€‚</li>
<li>é’ˆå¯¹SchrÃ¶dinger bridgeæ¡†æ¶ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ„ŸçŸ¥æŸå¤±å‡½æ•°ã€‚</li>
<li>æ­¤æ–°å‹æ„ŸçŸ¥æŸå¤±å‡½æ•°å¢å¼ºäº†è¯­éŸ³ä¿¡å·å¢å¼ºçš„æ€§èƒ½å¹¶æé«˜äº†æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>æ–‡ç« çš„å®éªŒä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›ä¾¿åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.10753v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.10753v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.10753v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.10753v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Study-on-Zero-shot-Non-intrusive-Speech-Assessment-using-Large-Language-Models"><a href="#A-Study-on-Zero-shot-Non-intrusive-Speech-Assessment-using-Large-Language-Models" class="headerlink" title="A Study on Zero-shot Non-intrusive Speech Assessment using Large   Language Models"></a>A Study on Zero-shot Non-intrusive Speech Assessment using Large   Language Models</h2><p><strong>Authors:Ryandhimas E. Zezario, Sabato M. Siniscalchi, Hsin-Min Wang, Yu Tsao</strong></p>
<p>This work investigates two strategies for zero-shot non-intrusive speech assessment leveraging large language models. First, we explore the audio analysis capabilities of GPT-4o. Second, we propose GPT-Whisper, which uses Whisper as an audio-to-text module and evaluates the naturalness of text via targeted prompt engineering. We evaluate the assessment metrics predicted by GPT-4o and GPT-Whisper, examining their correlation with human-based quality and intelligibility assessments and the character error rate (CER) of automatic speech recognition. Experimental results show that GPT-4o alone is less effective for audio analysis, while GPT-Whisper achieves higher prediction accuracy, has moderate correlation with speech quality and intelligibility, and has higher correlation with CER. Compared to SpeechLMScore and DNSMOS, GPT-Whisper excels in intelligibility metrics, but performs slightly worse than SpeechLMScore in quality estimation. Furthermore, GPT-Whisper outperforms supervised non-intrusive models MOS-SSL and MTI-Net in Spearmanâ€™s rank correlation for CER of Whisper. These findings validate GPT-Whisperâ€™s potential for zero-shot speech assessment without requiring additional training data. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸¤ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬éä¾µå…¥æ€§è¯­éŸ³è¯„ä¼°çš„ç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ¢ç´¢äº†GPT-4oçš„éŸ³é¢‘åˆ†æåŠŸèƒ½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†GPT-Whisperï¼Œå®ƒä½¿ç”¨Whisperä½œä¸ºéŸ³é¢‘åˆ°æ–‡æœ¬çš„æ¨¡å—ï¼Œå¹¶é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„æç¤ºå·¥ç¨‹è¯„ä¼°æ–‡æœ¬çš„è‡ªç„¶åº¦ã€‚æˆ‘ä»¬è¯„ä¼°äº†GPT-4oå’ŒGPT-Whisperçš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ£€æŸ¥å®ƒä»¬ä¸äººç±»åŸºäºçš„è´¨é‡å’Œå¯ç†è§£æ€§è¯„ä¼°ä»¥åŠè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰çš„ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4oå•ç‹¬è¿›è¡ŒéŸ³é¢‘åˆ†æçš„æ•ˆæœè¾ƒå·®ï¼Œè€ŒGPT-Whisperçš„é¢„æµ‹ç²¾åº¦æ›´é«˜ï¼Œä¸è¯­éŸ³è´¨é‡å’Œå¯ç†è§£æ€§çš„ç›¸å…³æ€§é€‚ä¸­ï¼Œä¸CERçš„ç›¸å…³æ€§æ›´é«˜ã€‚ä¸SpeechLMScoreå’ŒDNSMOSç›¸æ¯”ï¼ŒGPT-Whisperåœ¨å¯ç†è§£æ€§æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨è´¨é‡ä¼°è®¡æ–¹é¢ç•¥é€ŠäºSpeechLMScoreã€‚æ­¤å¤–ï¼Œåœ¨Whisperçš„CERçš„æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³æ€§æ–¹é¢ï¼ŒGPT-Whisperè¡¨ç°ä¼˜äºç›‘ç£éä¾µå…¥æ€§æ¨¡å‹MOS-SSLå’ŒMTI-Netã€‚è¿™äº›å‘ç°éªŒè¯äº†GPT-Whisperåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒæ•°æ®çš„é›¶æ ·æœ¬è¯­éŸ³è¯„ä¼°ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09914v2">PDF</a> Accepted to IEEE ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸¤ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬éä¾µå…¥å¼è¯­éŸ³è¯„ä¼°çš„ç­–ç•¥ã€‚é¦–å…ˆï¼Œæ¢ç´¢äº†GPT-4oçš„éŸ³é¢‘åˆ†æåŠŸèƒ½ï¼›å…¶æ¬¡ï¼Œæå‡ºäº†GPT-Whisperï¼Œå®ƒä½¿ç”¨Whisperä½œä¸ºéŸ³é¢‘è½¬æ–‡æœ¬æ¨¡å—ï¼Œå¹¶é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„æç¤ºå·¥ç¨‹è¯„ä¼°æ–‡æœ¬çš„è‡ªç„¶æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-Whisperåœ¨é¢„æµ‹å‡†ç¡®æ€§ã€è¯­éŸ³è´¨é‡å’Œå¯ç†è§£æ€§ç›¸å…³æ€§ä»¥åŠå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œä¸å…¶ä»–è¯„ä¼°æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oåœ¨éŸ³é¢‘åˆ†ææ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚</li>
<li>GPT-Whisperç»“åˆäº†éŸ³é¢‘è½¬æ–‡æœ¬æ¨¡å—Whisperå’Œæœ‰é’ˆå¯¹æ€§çš„æç¤ºå·¥ç¨‹ï¼Œæé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>GPT-Whisperåœ¨è¯­éŸ³è´¨é‡å’Œå¯ç†è§£æ€§æ–¹é¢ä¸äººç±»è¯„ä¼°æœ‰ä¸­ç­‰ç›¸å…³æ€§ã€‚</li>
<li>GPT-Whisperä¸å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰æœ‰è¾ƒé«˜çš„ç›¸å…³æ€§ã€‚</li>
<li>GPT-Whisperåœ¨å¯ç†è§£æ€§æŒ‡æ ‡ä¸Šä¼˜äºSpeechLMScoreï¼Œä½†åœ¨è´¨é‡è¯„ä¼°ä¸Šç•¥é€ŠäºSpeechLMScoreã€‚</li>
<li>GPT-Whisperåœ¨Spearmanç­‰çº§ç›¸å…³æ€§æ–¹é¢ä¼˜äºå…¶ä»–éä¾µå…¥å¼æ¨¡å‹ï¼Œå¦‚MOS-SSLå’ŒMTI-Netã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09914v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09914v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09914v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09914v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ASR-Error-Correction-using-Large-Language-Models"><a href="#ASR-Error-Correction-using-Large-Language-Models" class="headerlink" title="ASR Error Correction using Large Language Models"></a>ASR Error Correction using Large Language Models</h2><p><strong>Authors:Rao Ma, Mengjie Qian, Mark Gales, Kate Knill</strong></p>
<p>Error correction (EC) models play a crucial role in refining Automatic Speech Recognition (ASR) transcriptions, enhancing the readability and quality of transcriptions. Without requiring access to the underlying code or model weights, EC can improve performance and provide domain adaptation for black-box ASR systems. This work investigates the use of large language models (LLMs) for error correction across diverse scenarios. 1-best ASR hypotheses are commonly used as the input to EC models. We propose building high-performance EC models using ASR N-best lists which should provide more contextual information for the correction process. Additionally, the generation process of a standard EC model is unrestricted in the sense that any output sequence can be generated. For some scenarios, such as unseen domains, this flexibility may impact performance. To address this, we introduce a constrained decoding approach based on the N-best list or an ASR lattice. Finally, most EC models are trained for a specific ASR system requiring retraining whenever the underlying ASR system is changed. This paper explores the ability of EC models to operate on the output of different ASR systems. This concept is further extended to zero-shot error correction using LLMs, such as ChatGPT. Experiments on three standard datasets demonstrate the efficacy of our proposed methods for both Transducer and attention-based encoder-decoder ASR systems. In addition, the proposed method can serve as an effective method for model ensembling. </p>
<blockquote>
<p>é”™è¯¯ä¿®æ­£ï¼ˆECï¼‰æ¨¡å‹åœ¨ä¼˜åŒ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ã€æé«˜è½¬å½•çš„æ¸…æ™°åº¦å’Œè´¨é‡æ–¹é¢æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚æ— éœ€è®¿é—®åº•å±‚ä»£ç æˆ–æ¨¡å‹æƒé‡ï¼ŒECå°±èƒ½æé«˜æ€§èƒ½å¹¶ä¸ºé»‘ç›’ASRç³»ç»Ÿè¿›è¡Œé¢†åŸŸé€‚åº”ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§åœºæ™¯ä¸­çš„é”™è¯¯ä¿®æ­£åº”ç”¨ã€‚é€šå¸¸ä½¿ç”¨ASRçš„1-bestå‡è®¾ä½œä¸ºECæ¨¡å‹çš„è¾“å…¥ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨ASRçš„N-beståˆ—è¡¨æ„å»ºé«˜æ€§èƒ½ECæ¨¡å‹ï¼Œè¿™èƒ½ä¸ºä¿®æ­£è¿‡ç¨‹æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæ ‡å‡†ECæ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹æ˜¯æ— é™åˆ¶çš„ï¼Œå³å¯ä»¥ç”Ÿæˆä»»ä½•è¾“å‡ºåºåˆ—ã€‚å¯¹äºæŸäº›åœºæ™¯ï¼ˆå¦‚æœªè§é¢†åŸŸï¼‰ï¼Œè¿™ç§çµæ´»æ€§å¯èƒ½ä¼šå½±å“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºN-beståˆ—è¡¨æˆ–ASRæ™¶æ ¼çš„çº¦æŸè§£ç æ–¹æ³•ã€‚æœ€åï¼Œå¤§å¤šæ•°ECæ¨¡å‹æ˜¯ä¸ºç‰¹å®šASRç³»ç»Ÿè®­ç»ƒçš„ï¼Œæ¯å½“åº•å±‚ASRç³»ç»Ÿå‘ç”Ÿå˜åŒ–æ—¶éƒ½éœ€è¦é‡æ–°è®­ç»ƒã€‚æœ¬æ–‡æ¢è®¨äº†ECæ¨¡å‹å¯¹ä¸åŒçš„ASRç³»ç»Ÿè¾“å‡ºçš„å¤„ç†èƒ½åŠ›ã€‚è¿™ä¸€æ¦‚å¿µè¿›ä¸€æ­¥æ‰©å±•åˆ°äº†ä½¿ç”¨LLMçš„é›¶æ ·æœ¬é”™è¯¯ä¿®æ­£ï¼Œå¦‚ChatGPTã€‚åœ¨ä¸‰ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯¹äºè½¬æ¢å™¨å’ŒåŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨-è§£ç å™¨ASRç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä½œä¸ºæ¨¡å‹é›†æˆçš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09554v2">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†é”™è¯¯ä¿®æ­£ï¼ˆECï¼‰æ¨¡å‹åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ä¸­çš„é‡è¦æ€§ã€‚ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè·¨åœºæ™¯çš„é”™è¯¯ä¿®æ­£ï¼Œæå‡ºä½¿ç”¨ASR N-beståˆ—è¡¨æ„å»ºé«˜æ€§èƒ½ECæ¨¡å‹ï¼Œä»¥æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºä¿®æ­£è¿‡ç¨‹ã€‚ä¸ºè§£å†³åœ¨æŸäº›åœºæ™¯ï¼ˆå¦‚æœªè§é¢†åŸŸï¼‰ä¸­æ ‡å‡†ECæ¨¡å‹çš„çµæ´»æ€§å¯èƒ½å½±å“æ€§èƒ½çš„é—®é¢˜ï¼Œå¼•å…¥åŸºäºN-beståˆ—è¡¨æˆ–ASRæ™¶æ ¼çš„çº¦æŸè§£ç æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ECæ¨¡å‹åœ¨ä¸åŒASRç³»ç»Ÿè¾“å‡ºä¸Šçš„æ“ä½œèƒ½åŠ›ï¼Œå¹¶æ‰©å±•åˆ°é›¶æ ·æœ¬é”™è¯¯ä¿®æ­£çš„LLMï¼Œå¦‚ChatGPTã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯¹è½¬æ¢å™¨ï¼ˆTransducerï¼‰å’ŒåŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨è§£ç å™¨ASRç³»ç»Ÿå‡æœ‰æ•ˆï¼Œå¹¶ä¸”å¯ä»¥ä½œä¸ºæœ‰æ•ˆçš„æ¨¡å‹é›†æˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é”™è¯¯ä¿®æ­£ï¼ˆECï¼‰æ¨¡å‹åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­èµ·å…³é”®ä½œç”¨ï¼Œèƒ½æé«˜è½¬å½•è´¨é‡å’Œå¯è¯»æ€§ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé”™è¯¯ä¿®æ­£ï¼Œé€‚ç”¨äºå¤šç§åœºæ™¯ã€‚</li>
<li>æå‡ºåˆ©ç”¨ASR N-beståˆ—è¡¨æ„å»ºé«˜æ€§èƒ½ECæ¨¡å‹ï¼Œä»¥æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºä¿®æ­£ã€‚</li>
<li>é’ˆå¯¹æŸäº›åœºæ™¯ï¼ˆå¦‚æœªè§é¢†åŸŸï¼‰ï¼Œå¼•å…¥åŸºäºN-bestæˆ–ASRæ™¶æ ¼çš„çº¦æŸè§£ç æ¥è§£å†³çµæ´»æ€§é—®é¢˜ã€‚</li>
<li>ECæ¨¡å‹èƒ½åœ¨ä¸åŒASRç³»ç»Ÿè¾“å‡ºä¸Šæ“ä½œï¼Œå…·æœ‰è·¨ç³»ç»Ÿé€‚åº”æ€§ã€‚</li>
<li>æ‰©å±•åˆ°é›¶æ ·æœ¬é”™è¯¯ä¿®æ­£ï¼Œåˆ©ç”¨LLMå¦‚ChatGPTã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09554v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09554v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09554v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09554v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09554v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09554v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2409.09554v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="NEST-Self-supervised-Fast-Conformer-as-All-purpose-Seasoning-to-Speech-Processing-Tasks"><a href="#NEST-Self-supervised-Fast-Conformer-as-All-purpose-Seasoning-to-Speech-Processing-Tasks" class="headerlink" title="NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech   Processing Tasks"></a>NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech   Processing Tasks</h2><p><strong>Authors:He Huang, Taejin Park, Kunal Dhawan, Ivan Medennikov, Krishna C. Puvvada, Nithin Rao Koluguri, Weiqing Wang, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition&#x2F;translation, speaker verification and diarization, etc. However, most of current approaches are computationally expensive. In this paper, we propose a simplified and more efficient self-supervised learning framework termed as NeMo Encoder for Speech Tasks (NEST). Specifically, we adopt the FastConformer architecture with 8x sub-sampling rate, which is faster than Transformer or Conformer architectures. Instead of clustering-based quantization, we use fixed random projection for its simplicity and effectiveness. We also implement a generalized noisy speech augmentation that teaches the model to disentangle the main speaker from noise or other speakers. Experiments show that \model improves over existing self-supervised models and achieves new state-of-the-art performance on a variety of speech processing tasks, such as speech recognition&#x2F;translation, speaker diarization, spoken language understanding, etc. Code and checkpoints are publicly available via NVIDIA NeMo framework. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ å·²è¢«è¯æ˜å¯¹å¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡ï¼ˆå¦‚è¯­éŸ³è¯†åˆ«&#x2F;ç¿»è¯‘ã€è¯´è¯äººéªŒè¯å’ŒéŸ³é¢‘è½¬æ–‡å­—ç­‰ï¼‰éƒ½æœ‰ç›Šå¤„ã€‚ç„¶è€Œï¼Œå½“å‰å¤§å¤šæ•°æ–¹æ³•çš„è®¡ç®—æˆæœ¬éƒ½å¾ˆé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€åŒ–ä¸”æ›´é«˜æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºç”¨äºè¯­éŸ³ä»»åŠ¡çš„NeMoç¼–ç å™¨ï¼ˆNESTï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å…·æœ‰8å€å­é‡‡æ ·ç‡çš„FastConformeræ¶æ„ï¼Œå…¶é€Ÿåº¦æ¯”Transformeræˆ–Conformeræ¶æ„æ›´å¿«ã€‚æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨åŸºäºèšç±»çš„é‡åŒ–æ–¹æ³•ï¼Œè€Œæ˜¯é‡‡ç”¨ç®€å•çš„å›ºå®šéšæœºæŠ•å½±æ³•ã€‚æˆ‘ä»¬è¿˜å®ç°äº†ä¸€ç§é€šç”¨çš„å™ªå£°è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼Œä½¿æ¨¡å‹å­¦ä¼šä»å™ªå£°æˆ–å…¶ä»–è¯´è¯è€…ä¸­åˆ†ç¦»ä¸»è¦è¯´è¯è€…ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„è‡ªç›‘ç£æ¨¡å‹ï¼Œå¹¶è¾¾åˆ°äº†æ–°çš„é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«&#x2F;ç¿»è¯‘ã€è¯´è¯äººéŸ³é¢‘è½¬æ–‡å­—ã€è¯­éŸ³ç†è§£ç­‰ä»»åŠ¡ã€‚æ¨¡å‹å’Œæ£€æŸ¥ç‚¹å¯é€šè¿‡NVIDIA NeMoæ¡†æ¶å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13106v6">PDF</a> Published in ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€åŒ–ä¸”é«˜æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºNeMo Encoder for Speech Tasks (NEST)ã€‚é‡‡ç”¨FastConformeræ¶æ„ï¼Œå…·æœ‰æ›´å¿«çš„è®¡ç®—é€Ÿåº¦ã€‚ä½¿ç”¨å›ºå®šéšæœºæŠ•å½±è¿›è¡Œé‡åŒ–ï¼Œå¹¶å®ç°äº†é€šç”¨çš„å™ªå£°è¯­éŸ³å¢å¼ºæŠ€æœ¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šè¶…è¿‡äº†ç°æœ‰è‡ªç›‘ç£æ¨¡å‹ï¼Œå¹¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶NESTï¼Œç”¨äºè¯­éŸ³å¤„ç†ä»»åŠ¡ã€‚</li>
<li>NESTé‡‡ç”¨FastConformeræ¶æ„ï¼Œå…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>é‡åŒ–æ–¹æ³•é‡‡ç”¨å›ºå®šéšæœºæŠ•å½±ï¼Œæ—¢ç®€å•åˆæœ‰æ•ˆã€‚</li>
<li>å®ç°äº†é€šç”¨çš„å™ªå£°è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å˜ˆæ‚ç¯å¢ƒä¸­æ›´å¥½åœ°è¯†åˆ«ä¸»è¦è¯´è¯äººã€‚</li>
<li>NESTåœ¨å¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€ç¿»è¯‘ã€è¯´è¯äººåˆ†åŒ–å’Œå£è¯­ç†è§£ç­‰ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡NVIDIA NeMoæ¡†æ¶å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2408.13106v6/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2408.13106v6/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2408.13106v6/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Speech/2408.13106v6/page_3_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dc599e290ce2a53513fef5abc76dd8cc.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  EfficientVITON An Efficient Virtual Try-On Model using Optimized   Diffusion Process
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4f9f7f491575b9d64220f502c38351ab.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  Early Detection and Classification of Breast Cancer Using Deep Learning   Techniques
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">10467.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
