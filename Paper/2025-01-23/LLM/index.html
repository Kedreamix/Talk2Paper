<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  InternVideo2.5 Empowering Video MLLMs with Long and Rich Context   Modeling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12231v1/page_4_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-23-æ›´æ–°"><a href="#2025-01-23-æ›´æ–°" class="headerlink" title="2025-01-23 æ›´æ–°"></a>2025-01-23 æ›´æ–°</h1><h2 id="InternVideo2-5-Empowering-Video-MLLMs-with-Long-and-Rich-Context-Modeling"><a href="#InternVideo2-5-Empowering-Video-MLLMs-with-Long-and-Rich-Context-Modeling" class="headerlink" title="InternVideo2.5: Empowering Video MLLMs with Long and Rich Context   Modeling"></a>InternVideo2.5: Empowering Video MLLMs with Long and Rich Context   Modeling</h2><p><strong>Authors:Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, Limin Wang</strong></p>
<p>This paper aims to improve the performance of video multimodal large language models (MLLM) via long and rich context (LRC) modeling. As a result, we develop a new version of InternVideo2.5 with a focus on enhancing the original MLLMsâ€™ ability to perceive fine-grained details and capture long-form temporal structure in videos. Specifically, our approach incorporates dense vision task annotations into MLLMs using direct preference optimization and develops compact spatiotemporal representations through adaptive hierarchical token compression. Experimental results demonstrate this unique design of LRC greatly improves the results of video MLLM in mainstream video understanding benchmarks (short &amp; long), enabling the MLLM to memorize significantly longer video inputs (at least 6x longer than the original), and master specialized vision capabilities like object tracking and segmentation. Our work highlights the importance of multimodal context richness (length and fineness) in empowering MLLMâ€™s innate abilites (focus and memory), providing new insights for future research on video MLLM. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5">https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5</a> </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡é•¿ä¸°å¯Œä¸Šä¸‹æ–‡ï¼ˆLRCï¼‰å»ºæ¨¡æé«˜è§†é¢‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ–°çš„InternVideo2.5ç‰ˆæœ¬ï¼Œé‡ç‚¹å¢å¼ºåŸå§‹MLLMæ„ŸçŸ¥ç»†å¾®ç»†èŠ‚çš„èƒ½åŠ›ï¼Œå¹¶æ•è·è§†é¢‘ä¸­çš„é•¿å½¢å¼æ—¶é—´ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–å°†å¯†é›†è§†è§‰ä»»åŠ¡æ³¨é‡Šèå…¥åˆ°MLLMä¸­ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”åˆ†å±‚ä»¤ç‰Œå‹ç¼©å¼€å‘ç´§å‡‘çš„æ—¶ç©ºè¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLRCçš„ç‹¬ç‰¹è®¾è®¡æå¤§åœ°æé«˜äº†ä¸»æµè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆçŸ­&amp;é•¿ï¼‰ä¸­è§†é¢‘MLLMçš„ç»“æœï¼Œä½¿MLLMèƒ½å¤Ÿè®°å¿†æ˜æ˜¾æ›´é•¿çš„è§†é¢‘è¾“å…¥ï¼ˆè‡³å°‘ä¸ºåŸå§‹ç‰ˆæœ¬çš„6å€ï¼‰ï¼Œå¹¶æŒæ¡è¯¸å¦‚å¯¹è±¡è·Ÿè¸ªå’Œåˆ†å‰²ç­‰ä¸“ä¸šè§†è§‰åŠŸèƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸°å¯Œæ€§ï¼ˆé•¿åº¦å’Œç²¾ç»†åº¦ï¼‰åœ¨å¢å¼ºMLLMå›ºæœ‰èƒ½åŠ›ï¼ˆç„¦ç‚¹å’Œè®°å¿†ï¼‰æ–¹é¢çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥è§†é¢‘MLLMçš„ç ”ç©¶æä¾›äº†æ–°çš„è§è§£ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5">https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12386v1">PDF</a> technical report</p>
<p><strong>æ‘˜è¦</strong><br>MLLMåœ¨ä¸°å¯Œä¸”é•¿æ—¶é—´çš„ä¸Šä¸‹æ–‡ä¸‹å®ç°ä¼˜åŒ–æ”¹è¿›ï¼Œä»¥æ›´æ·±å…¥åœ°ç†è§£è§†é¢‘ä¿¡æ¯ã€‚æ¨å‡ºæ–°ç‰ˆæœ¬çš„InternVideo2.5æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºåŸå§‹MLLMæ•æ‰è§†é¢‘ç²¾ç»†ç»†èŠ‚å’Œé•¿æœŸæ—¶é—´ç»“æ„çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•å°†å¯†é›†è§†è§‰ä»»åŠ¡æ³¨é‡Šèå…¥MLLMä¸­ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”åˆ†å±‚ä»¤ç‰Œå‹ç¼©æŠ€æœ¯ç”Ÿæˆç´§å‡‘çš„æ—¶ç©ºè¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLRCè®¾è®¡æ˜¾è‘—æé«˜äº†ä¸»æµè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­çš„è§†é¢‘MLLMç»“æœï¼Œä½¿MLLMèƒ½å¤Ÿå¤„ç†æ˜¾è‘—æ›´é•¿çš„è§†é¢‘è¾“å…¥ï¼ˆè‡³å°‘æ˜¯åŸå§‹ç‰ˆæœ¬çš„å…­å€é•¿ï¼‰ï¼Œå¹¶æŒæ¡å¯¹è±¡è·Ÿè¸ªå’Œåˆ†å‰²ç­‰ä¸“ä¸šæŠ€èƒ½ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†å¤šåª’ä½“ä¸Šä¸‹æ–‡ä¸°å¯Œæ€§ï¼ˆé•¿åº¦å’Œç²¾ç»†åº¦ï¼‰åœ¨å¢å¼ºMLLMå›ºæœ‰èƒ½åŠ›ï¼ˆä¸“æ³¨å’Œè®°å¿†ï¼‰ä¸­çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥è§†é¢‘MLLMç ”ç©¶æä¾›äº†æ–°è§è§£ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨æ­¤å¤„ä¸‹è½½ï¼šé“¾æ¥[å¼€æ”¾GVå®éªŒå®¤ç½‘ç«™]ï¼ˆè¿™æ˜¯ä¸€ä¸ªè™šæ„çš„ç½‘ç«™åœ°å€ï¼Œä¾›æ–‡æœ¬ç”Ÿæˆä½¿ç”¨ï¼‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸°å¯Œä¸”é•¿æ—¶é—´çš„ä¸Šä¸‹æ–‡ï¼ˆLRCï¼‰æ¥æé«˜è§†é¢‘å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ€§èƒ½ã€‚</li>
<li>æ–°ç‰ˆInternVideo2.5æ¨¡å‹å¼ºåŒ–äº†æ•æ‰è§†é¢‘ç²¾ç»†ç»†èŠ‚å’Œé•¿æœŸæ—¶é—´ç»“æ„çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–èå…¥å¯†é›†è§†è§‰ä»»åŠ¡æ³¨é‡Šï¼Œå¹¶åˆ›å»ºç´§å‡‘çš„æ—¶ç©ºè¡¨ç¤ºã€‚</li>
<li>LRCè®¾è®¡æ˜¾è‘—æé«˜äº†ä¸»æµè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•çš„ç»“æœã€‚</li>
<li>MLLMèƒ½å¤Ÿå¤„ç†æ˜¾è‘—æ›´é•¿çš„è§†é¢‘è¾“å…¥ã€‚</li>
<li>æ–°æ¨¡å‹æŒæ¡å¯¹è±¡è·Ÿè¸ªå’Œåˆ†å‰²ç­‰ä¸“ä¸šæŠ€èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0518e9a7e38a8ffa6f1c88641294df50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c7210fc0565a8383a4a2bc459dcfee8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86505c2204bdb1045b42eaf7b24a4701.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Expertise-elevates-AI-usage-experimental-evidence-comparing-laypeople-and-professional-artists"><a href="#Expertise-elevates-AI-usage-experimental-evidence-comparing-laypeople-and-professional-artists" class="headerlink" title="Expertise elevates AI usage: experimental evidence comparing laypeople   and professional artists"></a>Expertise elevates AI usage: experimental evidence comparing laypeople   and professional artists</h2><p><strong>Authors:Thomas F. Eisenmann, Andres Karjus, Mar Canet Sola, Levin Brinkmann, Bramantyo Ibrahim Supriyatno, Iyad Rahwan</strong></p>
<p>Novel capacities of generative AI to analyze and generate cultural artifacts raise inevitable questions about the nature and value of artistic education and human expertise. Has AI already leveled the playing field between professional artists and laypeople, or do trained artistic expressive capacity, curation skills and experience instead enhance the ability to use these new tools? In this pre-registered study, we conduct experimental comparisons between 50 active artists and a demographically matched sample of laypeople. We designed two tasks to approximate artistic practice for testing their capabilities in both faithful and creative image creation: replicating a reference image, and moving as far away as possible from it. We developed a bespoke platform where participants used a modern text-to-image model to complete both tasks. We also collected and compared participantsâ€™ sentiments towards AI. On average, artists produced more faithful and creative outputs than their lay counterparts, although only by a small margin. While AI may ease content creation, professional expertise is still valuable - even within the confined space of generative AI itself. Finally, we also explored how well an exemplary vision-capable large language model (GPT-4o) would complete the same tasks, if given the role of an image generation agent, and found it performed on par in copying but outperformed even artists in the creative task. The very best results were still produced by humans in both tasks. These outcomes highlight the importance of integrating artistic skills with AI training to prepare artists and other visual professionals for a technologically evolving landscape. We see a potential in collaborative synergy with generative AI, which could reshape creative industries and education in the arts. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åˆ†æå’Œç”Ÿæˆæ–‡åŒ–äº§å“çš„æ–°èƒ½åŠ›ï¼Œä¸å¯é¿å…åœ°å¼•å‘äº†å…³äºè‰ºæœ¯æ•™è‚²å’Œäººç±»ä¸“ä¸šçŸ¥è¯†æ€§è´¨ä¸ä»·å€¼çš„ç–‘é—®ã€‚äººå·¥æ™ºèƒ½æ˜¯å¦å·²ç»ä½¿ä¸“ä¸šè‰ºæœ¯å®¶å’Œä¸šä½™çˆ±å¥½è€…å¤„äºåŒä¸€ç«äº‰æ°´å¹³ï¼Œæˆ–è€…ä¸“ä¸šçš„è‰ºæœ¯è¡¨è¾¾æŠ€å·§ã€ç­–åˆ’æŠ€èƒ½å’Œç»éªŒæ˜¯å¦åè€Œå¢å¼ºäº†ä½¿ç”¨è¿™äº›æ–°å·¥å…·çš„èƒ½åŠ›ï¼Ÿåœ¨è¿™é¡¹é¢„å…ˆæ³¨å†Œçš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹50åæ´»è·ƒè‰ºæœ¯å®¶å’Œä¸ä¹‹äººå£å­¦ç‰¹å¾åŒ¹é…çš„ä¸šä½™çˆ±å¥½è€…æ ·æœ¬è¿›è¡Œäº†å®éªŒå¯¹æ¯”ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªä»»åŠ¡æ¥æ¨¡æ‹Ÿè‰ºæœ¯åˆ›ä½œå®è·µï¼Œæµ‹è¯•ä»–ä»¬åœ¨å¿ å®å’Œåˆ›é€ æ€§å›¾åƒåˆ›ä½œæ–¹é¢çš„èƒ½åŠ›ï¼šå¤åˆ¶å‚è€ƒå›¾åƒï¼Œå¹¶å°½å¯èƒ½è¿œç¦»å®ƒã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸“ç”¨å¹³å°ï¼Œå‚ä¸è€…ä½¿ç”¨ç°ä»£æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ¥å®Œæˆè¿™ä¸¤ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜æ”¶é›†äº†å‚ä¸è€…å¯¹äººå·¥æ™ºèƒ½çš„çœ‹æ³•å¹¶è¿›è¡Œäº†æ¯”è¾ƒã€‚å¹³å‡è€Œè¨€ï¼Œè‰ºæœ¯å®¶æ¯”ä¸šä½™çˆ±å¥½è€…äº§å‡ºæ›´å¿ å®ã€æ›´å…·åˆ›æ„çš„ä½œå“ï¼Œä½†å·®è·å¾ˆå°ã€‚è™½ç„¶äººå·¥æ™ºèƒ½å¯èƒ½ä½¿å†…å®¹åˆ›å»ºå˜å¾—æ›´å®¹æ˜“ï¼Œä½†ä¸“ä¸šçŸ¥è¯†ä»ç„¶å¾ˆæœ‰ä»·å€¼ï¼Œç”šè‡³åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœ¬èº«çš„æœ‰é™ç©ºé—´å†…ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä¸€ä¸ªå…¸å‹çš„å…·å¤‡è§†è§‰åŠŸèƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-4oï¼‰åœ¨æ‰®æ¼”å›¾åƒç”Ÿæˆä»£ç†è§’è‰²æ—¶å®Œæˆç›¸åŒçš„ä»»åŠ¡æœ‰å¤šå¥½ï¼Œå‘ç°å®ƒåœ¨å¤åˆ¶æ–¹é¢è¡¨ç°ç›¸å½“ï¼Œä½†åœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸Šç”šè‡³è¶…è¿‡äº†è‰ºæœ¯å®¶ã€‚ä¸¤ä¸ªä»»åŠ¡ä¸­æœ€å¥½çš„ç»“æœä»ç„¶æ˜¯ç”±äººç±»äº§ç”Ÿçš„ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å°†è‰ºæœ¯æŠ€èƒ½ä¸äººå·¥æ™ºèƒ½åŸ¹è®­ç›¸ç»“åˆçš„é‡è¦æ€§ï¼Œä¸ºè‰ºæœ¯å®¶å’Œå…¶ä»–è§†è§‰ä¸“ä¸šäººå£«å‡†å¤‡åº”å¯¹æŠ€æœ¯ä¸æ–­å‘å±•çš„ç¯å¢ƒã€‚æˆ‘ä»¬çœ‹åˆ°ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ååŒåˆä½œçš„æ½œåŠ›ï¼Œè¿™å¯èƒ½ä¼šé‡å¡‘åˆ›æ„äº§ä¸šå’Œè‰ºæœ¯æ•™è‚²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12374v1">PDF</a> Eisenmann and Karjus contributed equally to this work and share first   authorship</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…³äºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ–‡åŒ–äº§ç‰©åˆ†æä¸ç”Ÿæˆæ–¹é¢çš„æ–°å…´èƒ½åŠ›ï¼Œå¯¹äºè‰ºæœ¯æ•™è‚²ä¸äººç±»ä¸“ä¸šçŸ¥è¯†æœ¬è´¨åŠå…¶ä»·å€¼æå‡ºäº†æ— å¯é¿å…çš„é—®é¢˜ã€‚åœ¨AIä»‹å…¥ä¸‹ï¼Œä¸“ä¸šè‰ºæœ¯å®¶ä¸æ™®é€šäººä¹‹é—´çš„å·®è·æ˜¯å¦å·²ç»ç¼©å°ï¼Ÿåˆæˆ–è€…ï¼Œè®­ç»ƒåçš„è‰ºæœ¯è¡¨è¾¾åŠ›ã€ç­–åˆ’èƒ½åŠ›åŠç»éªŒæ˜¯å¦æœ‰åŠ©äºæ›´å¥½åœ°è¿ç”¨è¿™äº›æ–°å·¥å…·ï¼Ÿæœ¬ç ”ç©¶å¯¹äº”ååæ´»è·ƒè‰ºæœ¯å®¶ä¸ç›¸åŒ¹é…çš„éè‰ºæœ¯ä»ä¸šè€…è¿›è¡Œå¯¹æ¯”å®éªŒï¼Œé€šè¿‡è®¾è®¡è´´è¿‘è‰ºæœ¯å®è·µçš„ä»»åŠ¡æ¯”è¾ƒäºŒè€…èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¿ äºåŸä½œçš„å›¾åƒå¤åˆ¶ä¸åˆ›æ„å›¾åƒåˆ›ä½œä»»åŠ¡ã€‚å‚ä¸è€…ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹å®Œæˆä»»åŠ¡ï¼Œå¹¶æ”¶é›†ä»–ä»¬å¯¹äºAIçš„æƒ…æ„Ÿæ€åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œè‰ºæœ¯å®¶æ— è®ºåœ¨å¿ å®åº¦è¿˜æ˜¯åˆ›é€ æ€§ä¸Šè¡¨ç°å‡ç•¥é«˜äºéä¸“ä¸šäººå£«ï¼Œä½†ä¼˜åŠ¿ä¸å¤§ã€‚AIè™½æœ‰åŠ©äºå†…å®¹åˆ›ä½œï¼Œä½†ä¸“ä¸šçŸ¥è¯†ä»å…·ä»·å€¼ï¼Œå³ä½¿åœ¨ç”Ÿæˆå¼AIçš„å±€é™ç©ºé—´å†…äº¦ç„¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿæ¢ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹GPT-4oä½œä¸ºå›¾åƒç”Ÿæˆä»£ç†å®Œæˆç›¸åŒä»»åŠ¡çš„æ•ˆæœï¼Œå‘ç°å…¶åœ¨å¤åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ç›¸å½“ï¼Œè€Œåœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸­ç”šè‡³è¶…è¶Šè‰ºæœ¯å®¶ç¾¤ä½“ã€‚ç„¶è€Œæ— è®ºç»“æœå¦‚ä½•ï¼Œæœ€ä¼˜ç§€çš„ä½œå“ä»ç”±äººç±»åˆ›ä½œå®Œæˆã€‚ç»“æœå¼ºè°ƒäº†å°†è‰ºæœ¯æŠ€èƒ½ä¸AIåŸ¹è®­ç›¸ç»“åˆçš„é‡è¦æ€§ï¼Œä¸ºè‰ºæœ¯å®¶åŠå…¶ä»–è§†è§‰ä¸“ä¸šäººå£«åº”å¯¹æŠ€æœ¯å˜é©æä¾›æ–¹å‘ã€‚æˆ‘ä»¬æœŸå¾…äººå·¥æ™ºèƒ½ä¸åˆ›é€ æ€§äº§ä¸šå’Œè‰ºæœ¯æ•™è‚²çš„ç»“åˆåä½œå°†å¸¦æ¥å˜é©æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ul>
<li>AIåœ¨åˆ†æå’Œç”Ÿæˆæ–‡åŒ–äº§å“æ–¹é¢çš„èƒ½åŠ›å¯¹è‰ºæœ¯æ•™è‚²å’Œäººç±»ä¸“ä¸šçŸ¥è¯†æå‡ºäº†è´¨ç–‘ã€‚</li>
<li>åœ¨ç‰¹å®šä»»åŠ¡ä¸­ï¼Œè‰ºæœ¯å®¶è¡¨ç°ç¨ä¼˜äºéä¸“ä¸šäººå£«ï¼Œä½†å·®è·ä¸å¤§ã€‚</li>
<li>AIä½¿å†…å®¹åˆ›ä½œæ›´ä¸ºä¾¿æ·ï¼Œä½†ä¸“ä¸šçŸ¥è¯†åœ¨ç”Ÿæˆå¼AIé¢†åŸŸä»å…·ä»·å€¼ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†åœ¨å¿ å®åº¦æ–¹é¢ä»éœ€æå‡ã€‚</li>
<li>äººç±»ä¸AIçš„ç»“åˆèƒ½åˆ›ä½œå‡ºæœ€ä¼˜ç§€çš„ä½œå“ï¼Œçªæ˜¾äº†æŠ€æœ¯ä¸è‰ºæœ¯ç»“åˆåŸ¹è®­çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a7866f7ef7ab7d0ec51f1185ba9e717.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c63581037c2e6e209f26b25281743e4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Is-Long-Context-All-You-Need-Leveraging-LLMâ€™s-Extended-Context-for-NL2SQL"><a href="#Is-Long-Context-All-You-Need-Leveraging-LLMâ€™s-Extended-Context-for-NL2SQL" class="headerlink" title="Is Long Context All You Need? Leveraging LLMâ€™s Extended Context for   NL2SQL"></a>Is Long Context All You Need? Leveraging LLMâ€™s Extended Context for   NL2SQL</h2><p><strong>Authors:Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.   In this work, we explore the performance and the latency trade-offs of the extended context window (a.k.a., long context) offered by Googleâ€™s state-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Googleâ€™s \textit{gemini-pro-1.5} achieve a strong performance with 67.41% on BIRD benchmark (dev) without finetuning and expensive self-consistency based techniques. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯æ¨ç†èƒ½åŠ›çš„æ”¹è¿›å’Œä¸Šä¸‹æ–‡çª—å£çš„æ‰©å±•ï¼Œä¸ºåˆ©ç”¨è¿™äº›å¼ºå¤§æ¨¡å‹å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚NL2SQLçš„æŒ‘æˆ˜åœ¨äºè‡ªç„¶è¯­è¨€é—®é¢˜æœ¬è´¨ä¸Šæ˜¯æ¨¡ç³Šçš„ï¼Œè€ŒSQLç”Ÿæˆåˆ™éœ€è¦ç²¾ç¡®ç†è§£å¤æ‚çš„æ•°æ®ç»“æ„å’Œè¯­ä¹‰ã€‚è§£å†³è¿™ç§è¯­ä¹‰æ¨¡ç³Šé—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯æä¾›æ›´å¤šå’Œè¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12372v1">PDF</a> 14 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡çª—å£æ‰©å±•æ–¹é¢ã€‚å¯¹äºNL2SQLé—®é¢˜ï¼Œç”±äºå…¶å¤©ç„¶çš„è¯­è¨€æ­§ä¹‰æ€§å’ŒSQLç”Ÿæˆçš„å¤æ‚æ€§ï¼Œæä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ˜¯ä¸€ç§è§£å†³è¯­ä¹‰æ¨¡ç³Šé—®é¢˜çš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†Googleå…ˆè¿›LLMï¼ˆåŒå­åº§-ä¸“ä¸šç‰ˆï¼‰æä¾›çš„æ‰©å±•ä¸Šä¸‹æ–‡çª—å£å’Œé¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹NL2SQLç”Ÿæˆçš„å½±å“ï¼ŒåŒ…æ‹¬åˆ—ç¤ºä¾‹å€¼ã€é—®ç­”å¯¹ã€ç”¨æˆ·æç¤ºã€SQLæ–‡æ¡£å’Œæ¨¡å¼ç­‰ã€‚å®éªŒè¡¨æ˜ï¼Œé•¿ä¸Šä¸‹æ–‡LLMå…·æœ‰ç¨³å¥æ€§ï¼Œä¸ä¼šè¿·å¤±åœ¨æ‰©å±•çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­ã€‚åŸºäºGoogleçš„åŒå­åº§ä¸“ä¸šç‰ˆæ„å»ºçš„é•¿ä¸Šä¸‹æ–‡NL2SQLç®¡é“åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œæœªç»å¾®è°ƒä¸”æœªé‡‡ç”¨æ˜‚è´µçš„è‡ªä¸€è‡´æ€§æŠ€æœ¯å³è¾¾åˆ°äº†67.41%çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å’Œä¸Šä¸‹æ–‡çª—å£æ‰©å±•æ–¹é¢ã€‚</li>
<li>NL2SQLé—®é¢˜å› å…¶å¤©ç„¶çš„è¯­è¨€æ­§ä¹‰æ€§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ˜¯è§£å†³NL2SQLè¯­ä¹‰æ¨¡ç³Šé—®é¢˜çš„ä¸€ç§æ–¹æ³•ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†æ‰©å±•ä¸Šä¸‹æ–‡çª—å£å’Œé¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹NL2SQLç”Ÿæˆçš„å½±å“ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œé•¿ä¸Šä¸‹æ–‡LLMå…·æœ‰å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç¨³å¥æ€§ã€‚</li>
<li>åŸºäºGoogleçš„åŒå­åº§ä¸“ä¸šç‰ˆæ„å»ºçš„é•¿ä¸Šä¸‹æ–‡NL2SQLç®¡é“åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-95e8b47bb2caf715ff79d7d74157535c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e066b6de037fe71a5074255fd96d6a61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa7b6b619cf382d7cf5ddec223976672.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4314f3a82e8060be4253491f0ad1f99f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a4b0e700d8d499684ad3a43309dcae4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="InternLM-XComposer2-5-Reward-A-Simple-Yet-Effective-Multi-Modal-Reward-Model"><a href="#InternLM-XComposer2-5-Reward-A-Simple-Yet-Effective-Multi-Modal-Reward-Model" class="headerlink" title="InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward   Model"></a>InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward   Model</h2><p><strong>Authors:Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang</strong></p>
<p>Despite the promising performance of Large Vision Language Models (LVLMs) in visual understanding, they occasionally generate incorrect outputs. While reward models (RMs) with reinforcement learning or test-time scaling offer the potential for improving generation quality, a critical gap remains: publicly available multi-modal RMs for LVLMs are scarce, and the implementation details of proprietary models are often unclear. We bridge this gap with InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective multi-modal reward model that aligns LVLMs with human preferences. To ensure the robustness and versatility of IXC-2.5-Reward, we set up a high-quality multi-modal preference corpus spanning text, image, and video inputs across diverse domains, such as instruction following, general understanding, text-rich documents, mathematical reasoning, and video understanding. IXC-2.5-Reward achieves excellent results on the latest multi-modal reward model benchmark and shows competitive performance on text-only reward model benchmarks. We further demonstrate three key applications of IXC-2.5-Reward: (1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows consistent improvements in instruction following and multi-modal open-ended dialogue; (2) Selecting the best response from candidate responses for test-time scaling; and (3) Filtering outlier or noisy samples from existing image and video instruction tuning training data. To ensure reproducibility and facilitate further research, we have open-sourced all model weights and training recipes at <a target="_blank" rel="noopener" href="https://github.com/InternLM/InternLM-XComposer">https://github.com/InternLM/InternLM-XComposer</a> </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¶å°”ä¼šäº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚è™½ç„¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æˆ–æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰æœ‰æé«˜ç”Ÿæˆè´¨é‡çš„æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨ä¸€ä¸ªå…³é”®å·®è·ï¼šé’ˆå¯¹LVLMsçš„å…¬å¼€å¯ç”¨å¤šæ¨¡å¼RMså¾ˆå°‘è§ï¼Œä¸“æœ‰æ¨¡å‹çš„å®ç°ç»†èŠ‚é€šå¸¸ä¹Ÿä¸æ¸…æ¥šã€‚æˆ‘ä»¬é€šè¿‡ä¸InternLM-XComposer2.5-Rewardï¼ˆIXC-2.5-Rewardï¼‰å¼¥åˆè¿™ä¸€é¸¿æ²Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„å¤šæ¨¡å¼å¥–åŠ±æ¨¡å‹ï¼Œä½¿LVLMsä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚ä¸ºç¡®ä¿IXC-2.5-Rewardçš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡å¼åå¥½è¯­æ–™åº“ï¼Œæ¶µç›–æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘è¾“å…¥ï¼Œæ¶‰åŠä¸åŒé¢†åŸŸï¼Œå¦‚æŒ‡ä»¤éµå¾ªã€ä¸€èˆ¬ç†è§£ã€æ–‡æœ¬ä¸°å¯Œæ–‡æ¡£ã€æ•°å­¦æ¨ç†å’Œè§†é¢‘ç†è§£ã€‚IXC-2.5-Rewardåœ¨æœ€æ–°çš„å¤šæ¨¡å¼å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œå¹¶åœ¨åªæœ‰æ–‡æœ¬çš„å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºäº†ç«äº‰åŠ›ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†IXC-2.5-Rewardçš„ä¸‰ä¸ªå…³é”®åº”ç”¨ï¼šï¼ˆ1ï¼‰ä¸ºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›ç›‘ç£ä¿¡å·ã€‚æˆ‘ä»¬å°†IXC-2.5-Rewardä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç›¸ç»“åˆï¼Œäº§ç”Ÿäº†IXC-2.5-Chatï¼Œåœ¨æŒ‡ä»¤éµå¾ªå’Œå¤šæ¨¡å¼å¼€æ”¾å¼å¯¹è¯æ–¹é¢è¡¨ç°å‡ºæŒç»­æ”¹è¿›ï¼›ï¼ˆ2ï¼‰åœ¨æµ‹è¯•æ—¶é—´ç¼©æ”¾æ—¶ä»å€™é€‰å“åº”ä¸­é€‰æ‹©æœ€ä½³å“åº”ï¼›ï¼ˆ3ï¼‰ä»ç°æœ‰çš„å›¾åƒå’Œè§†é¢‘æŒ‡ä»¤è°ƒæ•´è®­ç»ƒæ•°æ®ä¸­è¿‡æ»¤å¼‚å¸¸å€¼æˆ–å™ªå£°æ ·æœ¬ã€‚ä¸ºç¡®ä¿å¯å¤åˆ¶æ€§å¹¶ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/InternLM/InternLM-XComposer%E5%85%AC%E5%BC%80%E4%BA%86%E6%89%80%E6%9C%89%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E5%92%8C%E8%AE%AD%E7%BB%83%E9%85%8D%E6%96%B9%E3%80%82">https://github.com/InternLM/InternLM-XComposerå…¬å¼€äº†æ‰€æœ‰æ¨¡å‹æƒé‡å’Œè®­ç»ƒé…æ–¹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12368v1">PDF</a> Tech Report</p>
<p><strong>Summary</strong></p>
<p>LVLMsåœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æœ‰æ—¶ä¼šç”Ÿæˆé”™è¯¯è¾“å‡ºã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†InternLM-XComposer2.5-Rewardæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œå¯å°†LVLMsä¸äººç±»åå¥½å¯¹é½ã€‚æˆ‘ä»¬å»ºç«‹äº†é«˜è´¨é‡çš„å¤šæ¨¡æ€åå¥½è¯­æ–™åº“ï¼Œæ¶µç›–æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘è¾“å…¥çš„ä¸åŒé¢†åŸŸï¼Œå¦‚æŒ‡ä»¤éµå¾ªã€é€šç”¨ç†è§£ã€æ–‡æœ¬ä¸°å¯Œæ–‡æ¡£ã€æ•°å­¦æ¨ç†å’Œè§†é¢‘ç†è§£ç­‰ã€‚è¯¥æ¨¡å‹åœ¨æœ€æ–°å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨æ–‡æœ¬å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨æä¾›ç›‘ç£ä¿¡å·ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€ä»å€™é€‰å“åº”ä¸­é€‰æ‹©æœ€ä½³å“åº”è¿›è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ä»¥åŠè¿‡æ»¤ç°æœ‰å›¾åƒå’Œè§†é¢‘æŒ‡ä»¤è°ƒæ•´è®­ç»ƒæ•°æ®ä¸­çš„å¼‚å¸¸å€¼æˆ–å™ªå£°æ ·æœ¬ç­‰ä¸‰ä¸ªå…³é”®åº”ç”¨ã€‚ä¸ºç¡®ä¿å¯å¤åˆ¶æ€§å’Œä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/InternLM/InternLM-XComposer%E5%85%AC%E5%BC%80%E4%BA%86%E6%89%80%E6%9C%89%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E5%92%8C%E8%AE%AD%E7%BB%83%E9%85%8D%E6%96%B9%E3%80%82">https://github.com/InternLM/InternLM-XComposerå…¬å¼€äº†æ‰€æœ‰æ¨¡å‹æƒé‡å’Œè®­ç»ƒé…æ–¹ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InternLM-XComposer2.5-Rewardæ˜¯ä¸€ä¸ªç”¨äºLVLMsçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼Œä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡é«˜è´¨é‡çš„å¤šæ¨¡æ€åå¥½è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸã€‚</li>
<li>InternLM-XComposer2.5-Rewardåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹å¯ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç›‘ç£ä¿¡å·æä¾›ã€ä»å€™é€‰å“åº”ä¸­é€‰æ‹©æœ€ä½³å“åº”ä»¥åŠè¿‡æ»¤è®­ç»ƒæ•°æ®ä¸­çš„å¼‚å¸¸å€¼æˆ–å™ªå£°æ ·æœ¬ã€‚</li>
<li>æ¨¡å‹çš„åº”ç”¨åœºæ™¯åŒ…æ‹¬æŒ‡ä»¤éµå¾ªã€é€šç”¨ç†è§£ã€æ–‡æœ¬ä¸°å¯Œæ–‡æ¡£ã€æ•°å­¦æ¨ç†å’Œè§†é¢‘ç†è§£ç­‰ã€‚</li>
<li>è¯¥æ¨¡å‹å·²å¼€æºï¼Œæ‰€æœ‰æ¨¡å‹æƒé‡å’Œè®­ç»ƒé…æ–¹å¯åœ¨æŒ‡å®šç½‘ç«™æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9606d53c67dc58d3adc8ce01d1d72624.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f697b9e98f01fdf530dd92817b1ec90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30bb231c375bac5e5a30520cf36b655f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95483b4ba768bf59d3d55ab955766299.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a779027eb2e8e5ac4472a24298cd281.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VARGPT-Unified-Understanding-and-Generation-in-a-Visual-Autoregressive-Multimodal-Large-Language-Model"><a href="#VARGPT-Unified-Understanding-and-Generation-in-a-Visual-Autoregressive-Multimodal-Large-Language-Model" class="headerlink" title="VARGPT: Unified Understanding and Generation in a Visual Autoregressive   Multimodal Large Language Model"></a>VARGPT: Unified Understanding and Generation in a Visual Autoregressive   Multimodal Large Language Model</h2><p><strong>Authors:Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou</strong></p>
<p>We present VARGPT, a novel multimodal large language model (MLLM) that unifies visual understanding and generation within a single autoregressive framework. VARGPT employs a next-token prediction paradigm for visual understanding and a next-scale prediction paradigm for visual autoregressive generation. VARGPT innovatively extends the LLaVA architecture, achieving efficient scale-wise autoregressive visual generation within MLLMs while seamlessly accommodating mixed-modal input and output within a single model framework. Our VARGPT undergoes a three-stage unified training process on specially curated datasets, comprising a pre-training phase and two mixed visual instruction-tuning phases. The unified training strategy are designed to achieve alignment between visual and textual features, enhance instruction following for both understanding and generation, and improve visual generation quality, respectively. Despite its LLAVA-based architecture for multimodel understanding, VARGPT significantly outperforms LLaVA-1.5 across various vision-centric benchmarks, such as visual question-answering and reasoning tasks. Notably, VARGPT naturally supports capabilities in autoregressive visual generation and instruction-to-image synthesis, showcasing its versatility in both visual understanding and generation tasks. Project page is at: \url{<a target="_blank" rel="noopener" href="https://vargpt-1.github.io/%7D">https://vargpt-1.github.io/}</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†VARGPTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒåœ¨ä¸€ä¸ªå•ä¸€çš„è‡ªå›å½’æ¡†æ¶å†…ç»Ÿä¸€äº†è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚VARGPTé‡‡ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼è¿›è¡Œè§†è§‰ç†è§£ï¼Œå¹¶é‡‡ç”¨ä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹èŒƒå¼è¿›è¡Œè§†è§‰è‡ªå›å½’ç”Ÿæˆã€‚VARGPTåˆ›æ–°åœ°æ‰©å±•äº†LLaVAæ¶æ„ï¼Œåœ¨MLLMsä¸­å®ç°äº†é«˜æ•ˆçš„å°ºåº¦è‡ªå›å½’è§†è§‰ç”Ÿæˆï¼ŒåŒæ—¶åœ¨ä¸€ä¸ªæ¨¡å‹æ¡†æ¶ä¸­æ— ç¼åœ°å®¹çº³äº†æ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚æˆ‘ä»¬çš„VARGPTåœ¨ä¸“é—¨å®šåˆ¶çš„æ•°æ®é›†ä¸Šç»å†äº†ä¸‰é˜¶æ®µç»Ÿä¸€è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒé˜¶æ®µå’Œä¸¤ä¸ªæ··åˆè§†è§‰æŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚ç»Ÿä¸€è®­ç»ƒç­–ç•¥æ—¨åœ¨å®ç°è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„å¯¹é½ï¼Œå¢å¼ºç†è§£å’Œç”Ÿæˆæ–¹é¢çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œä»¥åŠæé«˜è§†è§‰ç”Ÿæˆè´¨é‡ã€‚å°½ç®¡å…¶åŸºäºLLAVAçš„æ¶æ„ç”¨äºå¤šæ¨¡æ€ç†è§£ï¼Œä½†VARGPTåœ¨å„ç§ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¦‚è§†è§‰é—®ç­”å’Œæ¨ç†ä»»åŠ¡ä¸­ï¼Œéƒ½æ˜¾è‘—ä¼˜äºLLaVA-1.5ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVARGPTè‡ªç„¶åœ°æ”¯æŒè‡ªå›å½’è§†è§‰ç”Ÿæˆå’ŒæŒ‡ä»¤åˆ°å›¾åƒåˆæˆçš„åŠŸèƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚é¡¹ç›®é¡µé¢ä½äºï¼š[<a target="_blank" rel="noopener" href="https://vargpt-1.github.io/]">https://vargpt-1.github.io/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12327v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VARGPTæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ƒå°†è§†è§‰ç†è§£å’Œç”Ÿæˆç»Ÿä¸€åœ¨ä¸€ä¸ªå•ä¸€çš„è‡ªå›å½’æ¡†æ¶å†…ã€‚å®ƒé‡‡ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼è¿›è¡Œè§†è§‰ç†è§£ï¼Œå¹¶é‡‡ç”¨ä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹èŒƒå¼è¿›è¡Œè§†è§‰è‡ªå›å½’ç”Ÿæˆã€‚VARGPTåˆ›æ–°åœ°æ‰©å±•äº†LLaVAæ¶æ„ï¼Œå®ç°äº†MLLMå†…çš„æœ‰æ•ˆå°ºåº¦è‡ªå›å½’è§†è§‰ç”Ÿæˆï¼ŒåŒæ—¶åœ¨ä¸€ä¸ªæ¨¡å‹æ¡†æ¶å†…æ— ç¼åœ°å®¹çº³æ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚VARGPTç»å†äº†åœ¨ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†ä¸Šçš„ä¸‰é˜¶æ®µç»Ÿä¸€è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒé˜¶æ®µå’Œä¸¤ä¸ªæ··åˆè§†è§‰æŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚å…¶è®¾è®¡æ—¨åœ¨å®ç°è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„å¯¹é½ï¼Œå¢å¼ºç†è§£å’Œç”Ÿæˆæ–¹é¢çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¹¶æé«˜è§†è§‰ç”Ÿæˆè´¨é‡ã€‚ä¸LLaVA-1.5ç›¸æ¯”ï¼ŒVARGPTåœ¨å„ç§ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¦‚è§†è§‰é—®ç­”å’Œæ¨ç†ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒVARGPTè‡ªç„¶åœ°æ”¯æŒè‡ªå›å½’è§†è§‰ç”Ÿæˆå’ŒæŒ‡ä»¤åˆ°å›¾åƒçš„åˆæˆèƒ½åŠ›ï¼Œå±•ç¤ºå…¶åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VARGPTæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>å®ƒé‡‡ç”¨ä¸‹ä¸€ä»¤ç‰Œ&#x2F;å°ºåº¦é¢„æµ‹èŒƒå¼è¿›è¡Œè§†è§‰ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>VARGPTæ‰©å±•äº†LLaVAæ¶æ„ï¼Œå®ç°äº†åœ¨å•ä¸€æ¨¡å‹å†…çš„å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºã€‚</li>
<li>VARGPTç»å†äº†ä¸‰é˜¶æ®µçš„ç»Ÿä¸€è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’Œä¸¤ä¸ªæ··åˆè§†è§‰æŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚</li>
<li>VARGPTå®ç°äº†è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„å¯¹é½ï¼Œå¢å¼ºäº†æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œæé«˜äº†è§†è§‰ç”Ÿæˆè´¨é‡ã€‚</li>
<li>ä¸LLaVA-1.5ç›¸æ¯”ï¼ŒVARGPTåœ¨è§†è§‰é—®ç­”å’Œæ¨ç†ç­‰ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12327v1/page_0_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12709642b3c3f76c897f6fe70fa45fd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e1dcbd8bef78d6f8d4531c19ade9187.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2d2e0ddc0b43b2080a832ea6b2c5377.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e93045a31dad5af1ea17c953b61e916.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12327v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MoGERNN-An-Inductive-Traffic-Predictor-for-Unobserved-Locations-in-Dynamic-Sensing-Networks"><a href="#MoGERNN-An-Inductive-Traffic-Predictor-for-Unobserved-Locations-in-Dynamic-Sensing-Networks" class="headerlink" title="MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in   Dynamic Sensing Networks"></a>MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in   Dynamic Sensing Networks</h2><p><strong>Authors:Qishen Zhou, Yifan Zhang, Michail A. Makridis, Anastasios Kouvelas, Yibing Wang, Simon Hu</strong></p>
<p>Given a partially observed road network, how can we predict the traffic state of unobserved locations? While deep learning approaches show exceptional performance in traffic prediction, most assume sensors at all locations of interest, which is impractical due to financial constraints. Furthermore, these methods typically require costly retraining when sensor configurations change. We propose MoGERNN, an inductive spatio-temporal graph representation model, to address these challenges. Inspired by the Mixture of Experts approach in Large Language Models, we introduce a Mixture of Graph Expert (MoGE) block to model complex spatial dependencies through multiple graph message aggregators and a sparse gating network. This block estimates initial states for unobserved locations, which are then processed by a GRU-based Encoder-Decoder that integrates a graph message aggregator to capture spatio-temporal dependencies and predict future states. Experiments on two real-world datasets show MoGERNN consistently outperforms baseline methods for both observed and unobserved locations. MoGERNN can accurately predict congestion evolution even in areas without sensors, offering valuable information for traffic management. Moreover, MoGERNN is adaptable to dynamic sensing networks, maintaining competitive performance even compared to its retrained counterpart. Tests with different numbers of available sensors confirm its consistent superiority, and ablation studies validate the effectiveness of its key modules. </p>
<blockquote>
<p>å¯¹äºéƒ¨åˆ†è§‚æµ‹çš„è·¯ç½‘ï¼Œæˆ‘ä»¬å¦‚ä½•é¢„æµ‹æœªè§‚æµ‹ä½ç½®çš„äº¤é€šçŠ¶æ€ï¼Ÿè™½ç„¶æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨äº¤é€šé¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•éƒ½å‡è®¾æ‰€æœ‰æ„Ÿå…´è¶£çš„ä½ç½®éƒ½æœ‰ä¼ æ„Ÿå™¨ï¼Œè¿™åœ¨è´¢åŠ¡ä¸Šæ˜¯ä¸åˆ‡å®é™…çš„ã€‚æ­¤å¤–ï¼Œå½“ä¼ æ„Ÿå™¨é…ç½®å‘ç”Ÿå˜åŒ–æ—¶ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MoGERNNï¼Œè¿™æ˜¯ä¸€ç§å½’çº³å¼æ—¶ç©ºå›¾è¡¨ç¤ºæ¨¡å‹ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸“å®¶æ··åˆæ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†å›¾ä¸“å®¶æ··åˆï¼ˆMoGEï¼‰å—ï¼Œé€šè¿‡å¤šä¸ªå›¾æ¶ˆæ¯èšåˆå™¨å’Œç¨€ç–é—¨æ§ç½‘ç»œæ¥å»ºæ¨¡å¤æ‚çš„ç©ºé—´ä¾èµ–æ€§ã€‚è¯¥å—ä¼°è®¡æœªè§‚æµ‹ä½ç½®çš„åˆå§‹çŠ¶æ€ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªåŸºäºGRUçš„ç¼–ç å™¨-è§£ç å™¨è¿›è¡Œå¤„ç†ï¼Œè¯¥ç¼–ç å™¨-è§£ç å™¨é›†æˆäº†å›¾æ¶ˆæ¯èšåˆå™¨æ¥æ•æ‰æ—¶ç©ºä¾èµ–æ€§å¹¶é¢„æµ‹æœªæ¥çŠ¶æ€ã€‚åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMoGERNNåœ¨è§‚æµ‹å’Œæœªè§‚æµ‹ä½ç½®ä¸Šçš„è¡¨ç°å‡æŒç»­è¶…è¶ŠåŸºå‡†æ–¹æ³•ã€‚MoGERNNå³ä½¿åœ¨æ— ä¼ æ„Ÿå™¨çš„åœ°åŒºä¹Ÿèƒ½å‡†ç¡®é¢„æµ‹æ‹¥å µæ¼”å˜ï¼Œä¸ºäº¤é€šç®¡ç†æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒMoGERNNèƒ½å¤Ÿé€‚åº”åŠ¨æ€ä¼ æ„Ÿç½‘ç»œï¼Œå³ä½¿åœ¨é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿä¿æŒäº†ç«äº‰åŠ›ã€‚ä¸åŒå¯ç”¨ä¼ æ„Ÿå™¨æ•°é‡çš„æµ‹è¯•è¯å®äº†å…¶å§‹ç»ˆå‡ºè‰²çš„æ€§èƒ½ï¼Œè€Œæ¶ˆèç ”ç©¶éªŒè¯äº†å…¶å…³é”®æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12281v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºéƒ¨åˆ†è§‚æµ‹çš„è·¯ç½‘æ•°æ®ï¼Œå¦‚ä½•é¢„æµ‹æœªè§‚æµ‹ä½ç½®çš„äº¤é€šçŠ¶æ€ï¼Ÿæœ¬æ–‡æå‡ºMoGERNNæ¨¡å‹ï¼Œé‡‡ç”¨å½’çº³æ€§æ—¶ç©ºå›¾è¡¨ç¤ºæ–¹æ³•åº”å¯¹æŒ‘æˆ˜ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸“å®¶æ··åˆæ–¹æ³•çš„å¯å‘ï¼Œå¼•å…¥å›¾ä¸“å®¶æ··åˆï¼ˆMoGEï¼‰æ¨¡å—å»ºæ¨¡å¤æ‚çš„ç©ºé—´ä¾èµ–æ€§ï¼Œå¹¶ç»“åˆç¨€ç–é—¨æ§ç½‘ç»œè¿›è¡Œåˆå§‹çŠ¶æ€ä¼°è®¡ã€‚æœªè§‚æµ‹ä½ç½®çš„åˆå§‹çŠ¶æ€è¿›ä¸€æ­¥ç”±åŸºäºGRUçš„ç¼–ç å™¨è§£ç å™¨å¤„ç†ï¼Œç»“åˆå›¾æ¶ˆæ¯èšåˆå™¨æ•æ‰æ—¶ç©ºä¾èµ–æ€§å¹¶è¿›è¡Œæœªæ¥çŠ¶æ€é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒMoGERNNåœ¨è§‚æµ‹å’Œæœªè§‚æµ‹ä½ç½®å‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œèƒ½å‡†ç¡®é¢„æµ‹æ— ä¼ æ„Ÿå™¨åŒºåŸŸçš„æ‹¥å µæ¼”å˜ï¼Œå¯¹äº¤é€šç®¡ç†å…·æœ‰å®ç”¨ä»·å€¼ã€‚æ­¤å¤–ï¼ŒMoGERNNèƒ½å¤Ÿé€‚åº”åŠ¨æ€ä¼ æ„Ÿç½‘ç»œï¼Œå³ä½¿ä¸é‡æ–°è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ä¹Ÿä¿æŒç«äº‰åŠ›ã€‚ä¸åŒä¼ æ„Ÿå™¨æ•°é‡çš„æµ‹è¯•éªŒè¯äº†å…¶æŒç»­ä¼˜åŠ¿ï¼Œè€Œæ¶ˆèç ”ç©¶ä¹ŸéªŒè¯äº†å…¶å…³é”®æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºMoGERNNæ¨¡å‹ï¼Œåˆ©ç”¨å½’çº³æ€§æ—¶ç©ºå›¾è¡¨ç¤ºæ–¹æ³•é¢„æµ‹äº¤é€šçŠ¶æ€ã€‚</li>
<li>MoGEæ¨¡å—ç”¨äºå»ºæ¨¡å¤æ‚çš„ç©ºé—´ä¾èµ–æ€§ï¼Œç»“åˆç¨€ç–é—¨æ§ç½‘ç»œè¿›è¡Œåˆå§‹çŠ¶æ€ä¼°è®¡ã€‚</li>
<li>GRU-based Encoder-Decoderç»“åˆå›¾æ¶ˆæ¯èšåˆå™¨æ•æ‰æ—¶ç©ºä¾èµ–æ€§å¹¶è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>MoGERNNåœ¨è§‚æµ‹å’Œæœªè§‚æµ‹ä½ç½®å‡è¡¨ç°å‡ºä¼˜äºåŸºå‡†æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>MoGERNNèƒ½å‡†ç¡®é¢„æµ‹æ— ä¼ æ„Ÿå™¨åŒºåŸŸçš„æ‹¥å µæ¼”å˜ï¼Œå¯¹äº¤é€šç®¡ç†æœ‰å®ç”¨ä»·å€¼ã€‚</li>
<li>MoGERNNé€‚åº”åŠ¨æ€ä¼ æ„Ÿç½‘ç»œï¼Œæ€§èƒ½ç¨³å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12281">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12281v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12281v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="InsTALL-Context-aware-Instructional-Task-Assistance-with-Multi-modal-Large-Language-Models"><a href="#InsTALL-Context-aware-Instructional-Task-Assistance-with-Multi-modal-Large-Language-Models" class="headerlink" title="InsTALL: Context-aware Instructional Task Assistance with Multi-modal   Large Language Models"></a>InsTALL: Context-aware Instructional Task Assistance with Multi-modal   Large Language Models</h2><p><strong>Authors:Pha Nguyen, Sailik Sengupta, Girik Malik, Arshit Gupta, Bonan Min</strong></p>
<p>The improved competence of generative models can help building multi-modal virtual assistants that leverage modalities beyond language. By observing humans performing multi-step tasks, one can build assistants that have situational awareness of actions and tasks being performed, enabling them to cater assistance based on this understanding. In this paper, we develop a Context-aware Instructional Task Assistant with Multi-modal Large Language Models (InsTALL) that leverages an online visual stream (e.g. a userâ€™s screen share or video recording) and responds in real-time to user queries related to the task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal model on task videos and paired textual data, and 2) automatically extracts task graph from video data and leverages it at training and inference time. We show InsTALL achieves state-of-the-art performance across proposed sub-tasks considered for multimodal activity understanding â€“ task recognition (TR), action recognition (AR), next action prediction (AP), and plan prediction (PP) â€“ and outperforms existing baselines on two novel sub-tasks related to automatic error identification. </p>
<blockquote>
<p>æé«˜ç”Ÿæˆæ¨¡å‹çš„ç«äº‰åŠ›æœ‰åŠ©äºæ„å»ºåˆ©ç”¨å¤šç§è¯­è¨€ä¹‹å¤–çš„æ¨¡æ€çš„å¤šæ¨¡æ€è™šæ‹ŸåŠ©ç†ã€‚é€šè¿‡è§‚å¯Ÿäººç±»æ‰§è¡Œå¤šä»»åŠ¡ï¼Œå¯ä»¥æ„å»ºå¯¹æ­£åœ¨æ‰§è¡Œçš„åŠ¨ä½œå’Œä»»åŠ¡æœ‰æƒ…å¢ƒæ„è¯†çš„åŠ©ç†ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿæ ¹æ®è¿™ç§ç†è§£æä¾›ååŠ©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä»»åŠ¡åŠ©ç†ï¼ˆInsTALLï¼‰ã€‚å®ƒåˆ©ç”¨åœ¨çº¿è§†é¢‘æµï¼ˆä¾‹å¦‚ç”¨æˆ·çš„å±å¹•å…±äº«æˆ–è§†é¢‘å½•åˆ¶ï¼‰ï¼Œå®æ—¶å“åº”ç”¨æˆ·ä¸å½“å‰ä»»åŠ¡ç›¸å…³çš„æŸ¥è¯¢ã€‚ä¸ºäº†å®ç°æœ‰ç”¨çš„å¸®åŠ©åŠŸèƒ½ï¼ŒInsTALL 1ï¼‰åœ¨ä»»åŠ¡è§†é¢‘å’Œé…å¯¹æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹ï¼›2ï¼‰è‡ªåŠ¨ä»è§†é¢‘æ•°æ®ä¸­æå–ä»»åŠ¡å›¾ï¼Œå¹¶åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶ä½¿ç”¨å®ƒã€‚æˆ‘ä»¬è¯æ˜InsTALLåœ¨å¤„ç†æè®®çš„å¤šæ¨¡æ€æ´»åŠ¨ç†è§£çš„å­ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½â€”â€”ä»»åŠ¡è¯†åˆ«ï¼ˆTRï¼‰ã€åŠ¨ä½œè¯†åˆ«ï¼ˆARï¼‰ã€ä¸‹ä¸€æ­¥åŠ¨ä½œé¢„æµ‹ï¼ˆAPï¼‰å’Œè®¡åˆ’é¢„æµ‹ï¼ˆPPï¼‰â€”â€”å¹¶ä¸”åœ¨ä¸¤ä¸ªä¸è‡ªåŠ¨é”™è¯¯è¯†åˆ«ç›¸å…³çš„ä¸¤ä¸ªæ–°å­ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12231v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ”¹è¿›åçš„ç”Ÿæˆæ¨¡å‹æœ‰åŠ©äºæ„å»ºåˆ©ç”¨å¤šç§æ¨¡æ€ï¼ˆè¶…è¶Šè¯­è¨€ï¼‰çš„å¤šæ¨¡æ€è™šæ‹ŸåŠ©ç†ã€‚é€šè¿‡è§‚å¯Ÿäººç±»æ‰§è¡Œå¤šä»»åŠ¡ï¼Œå¯ä»¥æ„å»ºå…·æœ‰æƒ…å¢ƒæ„è¯†çš„åŠ©ç†ï¼Œäº†è§£æ­£åœ¨æ‰§è¡Œçš„åŠ¨ä½œå’Œä»»åŠ¡ï¼Œå¹¶æ ¹æ®è¿™ç§ç†è§£æä¾›ååŠ©ã€‚æœ¬æ–‡å¼€å‘äº†åˆ©ç”¨åœ¨çº¿è§†é¢‘æµï¼ˆå¦‚ç”¨æˆ·å±å¹•å…±äº«æˆ–è§†é¢‘å½•åˆ¶ï¼‰çš„åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æƒ…å¢ƒæ„ŸçŸ¥ä»»åŠ¡åŠ©ç†ï¼ˆInsTALLï¼‰ï¼Œå¹¶å®æ—¶å“åº”ä¸å½“å‰ä»»åŠ¡ç›¸å…³çš„ç”¨æˆ·æŸ¥è¯¢ã€‚ä¸ºäº†å®ç°æœ‰ç”¨çš„ååŠ©ï¼ŒInsTALL 1) åœ¨ä»»åŠ¡è§†é¢‘å’Œé…å¯¹æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹ï¼Œ2) è‡ªåŠ¨ä»è§†é¢‘æ•°æ®ä¸­æå–ä»»åŠ¡å›¾å¹¶åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶ä½¿ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒInsTALLåœ¨å¤šæ¨¡æ€æ´»åŠ¨ç†è§£çš„ç›¸å…³å­ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ä»»åŠ¡è¯†åˆ«ï¼ˆTRï¼‰ã€åŠ¨ä½œè¯†åˆ«ï¼ˆARï¼‰ã€ä¸‹ä¸€ä¸ªåŠ¨ä½œé¢„æµ‹ï¼ˆAPï¼‰å’Œè®¡åˆ’é¢„æµ‹ï¼ˆPPï¼‰ï¼Œå¹¶åœ¨ä¸¤ä¸ªæ–°çš„å­ä»»åŠ¡ä¸Šçš„åŸºçº¿è¡¨ç°å‡ºè‰²ã€‚èƒ½å¤Ÿåœ¨è‡ªåŠ¨åŒ–é”™è¯¯è¯†åˆ«æ–¹é¢æä¾›å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹çš„æ”¹è¿›æœ‰åŠ©äºæ„å»ºå¤šæ¨¡æ€è™šæ‹ŸåŠ©ç†ï¼Œè¿™äº›åŠ©ç†èƒ½å¤Ÿç†è§£å¹¶å“åº”å¤šç§æ¨¡æ€çš„ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯è¯­è¨€ã€‚</li>
<li>é€šè¿‡è§‚å¯Ÿäººç±»æ‰§è¡Œå¤šä»»åŠ¡ï¼Œè™šæ‹ŸåŠ©ç†å¯ä»¥è·å¾—æƒ…å¢ƒæ„è¯†ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£å¹¶å“åº”äººç±»çš„åŠ¨ä½œå’Œä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>InsTALLç³»ç»Ÿåˆ©ç”¨åœ¨çº¿è§†é¢‘æµæä¾›å®æ—¶ä»»åŠ¡ç›¸å…³çš„è™šæ‹ŸåŠ©ç†æœåŠ¡ã€‚</li>
<li>InsTALLç³»ç»Ÿé€šè¿‡è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹å’Œåœ¨è®­ç»ƒå’Œæ¨ç†æ—¶è‡ªåŠ¨æå–ä»»åŠ¡å›¾æ¥å®ç°æœ‰æ•ˆååŠ©ã€‚</li>
<li>InsTALLåœ¨å¤šæ¨¡æ€æ´»åŠ¨ç†è§£çš„å¤šä¸ªå­ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>InsTALLåœ¨è‡ªåŠ¨é”™è¯¯è¯†åˆ«æ–¹é¢çš„æ€§èƒ½ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12231v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12231v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12231v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12231v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12231v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12231v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12231v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CDW-CoT-Clustered-Distance-Weighted-Chain-of-Thoughts-Reasoning"><a href="#CDW-CoT-Clustered-Distance-Weighted-Chain-of-Thoughts-Reasoning" class="headerlink" title="CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning"></a>CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning</h2><p><strong>Authors:Yuanheng Fang, Guoqing Chao, Wenqiang Lei, Shaobo Li, Dianhui Chu</strong></p>
<p>Large Language Models (LLMs) have recently achieved impressive results in complex reasoning tasks through Chain of Thought (CoT) prompting. However, most existing CoT methods rely on using the same prompts, whether manually designed or automatically generated, to handle the entire dataset. This one-size-fits-all approach may fail to meet the specific needs arising from the diversities within a single dataset. To solve this problem, we propose the Clustered Distance-Weighted Chain of Thought (CDW-CoT) method, which dynamically constructs prompts tailored to the characteristics of each data instance by integrating clustering and prompt optimization techniques. Our method employs clustering algorithms to categorize the dataset into distinct groups, from which a candidate pool of prompts is selected to reflect the inherent diversity within the dataset. For each cluster, CDW-CoT trains the optimal prompt probability distribution tailored to their specific characteristics. Finally, it dynamically constructs a unique prompt probability distribution for each test instance, based on its proximity to cluster centers, from which prompts are selected for reasoning. CDW-CoT consistently outperforms traditional CoT methods across six datasets, including commonsense, symbolic, and mathematical reasoning tasks. Specifically, when compared to manual CoT, CDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and 15.72% on LLaMA3 (8B). </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ³•åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„CoTæ–¹æ³•éƒ½ä¾èµ–äºä½¿ç”¨ç›¸åŒçš„æç¤ºï¼Œæ— è®ºæ˜¯æ‰‹åŠ¨è®¾è®¡è¿˜æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œæ¥å¤„ç†æ•´ä¸ªæ•°æ®é›†ã€‚è¿™ç§ä¸€åˆ€åˆ‡çš„æ–¹æ³•å¯èƒ½æ— æ³•æ»¡è¶³ç”±å•ä¸€æ•°æ®é›†å†…éƒ¨å¤šæ ·æ€§äº§ç”Ÿçš„ç‰¹å®šéœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†èšç±»è·ç¦»åŠ æƒæ€ç»´é“¾ï¼ˆCDW-CoTï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡é›†æˆèšç±»å’Œæç¤ºä¼˜åŒ–æŠ€æœ¯ï¼ŒåŠ¨æ€æ„å»ºé€‚åº”æ¯ä¸ªæ•°æ®å®ä¾‹ç‰¹æ€§çš„æç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨èšç±»ç®—æ³•å°†æ•°æ®é›†åˆ†æˆä¸åŒçš„ç»„ï¼Œä»ä¸­é€‰æ‹©å€™é€‰æç¤ºæ± æ¥åæ˜ æ•°æ®é›†çš„å›ºæœ‰å¤šæ ·æ€§ã€‚å¯¹äºæ¯ä¸ªé›†ç¾¤ï¼ŒCDW-CoTè®­ç»ƒä¸å…¶ç‰¹å®šç‰¹å¾ç›¸åŒ¹é…çš„ä¼˜åŒ–æç¤ºæ¦‚ç‡åˆ†å¸ƒã€‚æœ€åï¼Œå®ƒåŸºäºæµ‹è¯•å®ä¾‹ä¸ç°‡ä¸­å¿ƒçš„æ¥è¿‘ç¨‹åº¦ï¼ŒåŠ¨æ€æ„å»ºæ¯ä¸ªæµ‹è¯•å®ä¾‹çš„å”¯ä¸€æç¤ºæ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶ä»ä¸­é€‰æ‹©æç¤ºè¿›è¡Œæ¨ç†ã€‚åœ¨åŒ…æ‹¬å¸¸è¯†ã€ç¬¦å·å’Œæ•°å­¦æ¨ç†ä»»åŠ¡åœ¨å†…çš„å…­ä¸ªæ•°æ®é›†ä¸Šï¼ŒCDW-CoTå§‹ç»ˆä¼˜äºä¼ ç»ŸCoTæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œä¸æ‰‹åŠ¨CoTç›¸æ¯”ï¼ŒCDW-CoTåœ¨LLaMA2ï¼ˆ13Bï¼‰ä¸Šçš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†25.34%ï¼Œåœ¨LLaMA3ï¼ˆ8Bï¼‰ä¸Šæé«˜äº†15.72%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12226v1">PDF</a> aaai25(poster)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°CoTæ–¹æ³•ä½¿ç”¨ç›¸åŒçš„æ‰‹å·¥è®¾è®¡æˆ–è‡ªåŠ¨ç”Ÿæˆçš„æç¤ºæ¥å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œè¿™ç§ä¸€åˆ€åˆ‡çš„æ–¹æ³•å¯èƒ½æ— æ³•æ»¡è¶³å•ä¸€æ•°æ®é›†å†…éƒ¨äº§ç”Ÿçš„ç‰¹å®šéœ€æ±‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†èšç±»è·ç¦»åŠ æƒé“¾å¼æ€ç»´ï¼ˆCDW-CoTï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡é›†æˆèšç±»å’Œæç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œæ ¹æ®æ¯ä¸ªæ•°æ®å®ä¾‹çš„ç‰¹æ€§åŠ¨æ€æ„å»ºé’ˆå¯¹æ€§çš„æç¤ºã€‚CDW-CoTé€šè¿‡èšç±»ç®—æ³•å°†æ•°æ®é›†åˆ†æˆä¸åŒçš„ç»„ï¼Œå¹¶ä»ä¸­é€‰æ‹©æç¤ºå€™é€‰æ± æ¥åæ˜ æ•°æ®é›†çš„å†…åœ¨å¤šæ ·æ€§ã€‚é’ˆå¯¹æ¯ä¸ªé›†ç¾¤ï¼ŒCDW-CoTè®­ç»ƒä¸å…¶ç‰¹å®šç‰¹å¾ç›¸åŒ¹é…çš„ä¼˜åŒ–æç¤ºæ¦‚ç‡åˆ†å¸ƒã€‚æœ€åï¼Œå®ƒåŸºäºæµ‹è¯•å®ä¾‹ä¸é›†ç¾¤ä¸­å¿ƒçš„æ¥è¿‘ç¨‹åº¦ï¼ŒåŠ¨æ€æ„å»ºç‹¬ç‰¹çš„æç¤ºæ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶ä»ä¸­é€‰æ‹©æç¤ºè¿›è¡Œæ¨ç†ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šï¼ŒCDW-CoTæŒç»­ä¼˜äºä¼ ç»ŸCoTæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¸¸è¯†ã€ç¬¦å·å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ›´å‡ºè‰²ã€‚ä¸æ‰‹åŠ¨CoTç›¸æ¯”ï¼ŒCDW-CoTåœ¨LLaMA2ï¼ˆ13Bï¼‰å’ŒLLaMA3ï¼ˆ8Bï¼‰ä¸Šçš„å¹³å‡å‡†ç¡®åº¦åˆ†åˆ«æé«˜äº†25.34%å’Œ15.72%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé€šè¿‡CoTæç¤ºåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>ç°æœ‰CoTæ–¹æ³•ä½¿ç”¨å›ºå®šæç¤ºå¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œå¯èƒ½æ— æ³•æ»¡è¶³ç‰¹å®šéœ€æ±‚ã€‚</li>
<li>CDW-CoTæ–¹æ³•é€šè¿‡èšç±»ç®—æ³•å°†æ•°æ®é›†åˆ†æˆä¸åŒç»„ï¼Œå¹¶é’ˆå¯¹æ¯ç»„ç‰¹æ€§æ„å»ºæç¤ºã€‚</li>
<li>CDW-CoTæ ¹æ®æµ‹è¯•å®ä¾‹ä¸é›†ç¾¤ä¸­å¿ƒçš„æ¥è¿‘ç¨‹åº¦åŠ¨æ€é€‰æ‹©æç¤ºã€‚</li>
<li>CDW-CoTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»ŸCoTæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¸¸è¯†ã€ç¬¦å·å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ã€‚</li>
<li>ä¸æ‰‹åŠ¨CoTç›¸æ¯”ï¼ŒCDW-CoTåœ¨LLaMA2å’ŒLLaMA3ä¸Šçš„å¹³å‡å‡†ç¡®åº¦æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12226v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12226v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12226v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12226v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12226v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Realizing-Truly-Intelligent-User-Interfaces"><a href="#Leveraging-Large-Language-Models-for-Realizing-Truly-Intelligent-User-Interfaces" class="headerlink" title="Leveraging Large Language Models for Realizing Truly Intelligent User   Interfaces"></a>Leveraging Large Language Models for Realizing Truly Intelligent User   Interfaces</h2><p><strong>Authors:Allard Oelen, SÃ¶ren Auer</strong></p>
<p>The number of published scholarly articles is growing at a significant rate, making scholarly knowledge organization increasingly important. Various approaches have been proposed to organize scholarly information, including describing scholarly knowledge semantically leveraging knowledge graphs. Transforming unstructured knowledge, presented within articles, to structured and semantically represented knowledge generally requires human intelligence and labor since natural language processing methods alone typically do not render sufficient precision and recall for many applications. With the recent developments of Large Language Models (LLMs), it becomes increasingly possible to provide truly intelligent user interfaces guiding humans in the transformation process. We present an approach to integrate non-intrusive LLMs guidance into existing user interfaces. More specifically, we integrate LLM-supported user interface components into an existing scholarly knowledge infrastructure. Additionally, we provide our experiences with LLM integration, detailing best practices and obstacles. Finally, we evaluate the approach using a small-scale user evaluation with domain experts. </p>
<blockquote>
<p>å­¦æœ¯è®ºæ–‡çš„å‘è¡¨æ•°é‡æ­£åœ¨ä»¥æ˜¾è‘—çš„é€Ÿåº¦å¢é•¿ï¼Œè¿™ä½¿å¾—å­¦æœ¯çŸ¥è¯†ç»„ç»‡å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ä¸ºç»„ç»‡å­¦æœ¯ä¿¡æ¯ï¼Œå·²ç»æå‡ºäº†å„ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ©ç”¨çŸ¥è¯†å›¾è°±è¿›è¡Œè¯­ä¹‰æè¿°ã€‚å°†æ–‡ç« ä¸­çš„éç»“æ„åŒ–çŸ¥è¯†è½¬åŒ–ä¸ºç»“æ„å’Œè¯­ä¹‰è¡¨ç¤ºçš„çŸ¥è¯†é€šå¸¸éœ€è¦äººç±»æ™ºæ…§å’ŒåŠ³åŠ¨ï¼Œå› ä¸ºå•é è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•é€šå¸¸ä¸èƒ½ä¸ºè®¸å¤šåº”ç”¨æä¾›è¶³å¤Ÿçš„ç²¾åº¦å’Œå¬å›ç‡ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€è¿‘å‘å±•ï¼Œæä¾›çœŸæ­£æ™ºèƒ½çš„ç”¨æˆ·ç•Œé¢æŒ‡å¯¼äººä»¬å®Œæˆè½¬åŒ–è¿‡ç¨‹å˜å¾—è¶Šæ¥è¶Šæœ‰å¯èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†éä¾µå…¥å¼LLMæŒ‡å¯¼é›†æˆåˆ°ç°æœ‰ç”¨æˆ·ç•Œé¢ä¸­çš„æ–¹æ³•ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å°†LLMæ”¯æŒçš„ç”¨æˆ·ç•Œé¢ç»„ä»¶é›†æˆåˆ°ç°æœ‰çš„å­¦æœ¯çŸ¥è¯†åŸºç¡€è®¾æ–½ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†äº«äº†LLMé›†æˆçš„ç»éªŒï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†æœ€ä½³å®è·µå’Œéšœç¢ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å°è§„æ¨¡çš„ç”¨æˆ·è¯„ä»·ä¸“å®¶è¯„ä¼°äº†è¿™ç§æ–¹æ³•çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å­¦æœ¯æ–‡ç« æ•°é‡çš„å¿«é€Ÿå¢é•¿ï¼Œå­¦æœ¯çŸ¥è¯†ç»„ç»‡å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æŒ‡å¯¼ç”¨æˆ·ç•Œé¢çš„æ–¹æ³•ï¼Œå°†éä¾µå…¥å¼çš„LLMsæŒ‡å¯¼é›†æˆåˆ°ç°æœ‰çš„å­¦æœ¯çŸ¥è¯†åŸºç¡€è®¾æ–½ä¸­ï¼Œä»¥å®ç°å¯¹å­¦æœ¯ä¿¡æ¯çš„è¯­ä¹‰æè¿°å’Œç»„ç»‡ã€‚é€šè¿‡å°å‹ç”¨æˆ·è¯„ä»·éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦æœ¯æ–‡ç« æ•°é‡å¢é•¿è¿…é€Ÿï¼Œå­¦æœ¯çŸ¥è¯†ç»„ç»‡å˜å¾—é‡è¦ã€‚</li>
<li>çŸ¥è¯†å›¾è°±å¯ç”¨äºè¯­ä¹‰æè¿°å­¦æœ¯çŸ¥è¯†ã€‚</li>
<li>å°†æ— ç»“æ„çš„å­¦æœ¯çŸ¥è¯†è½¬åŒ–ä¸ºç»“æ„å’Œè¯­ä¹‰è¡¨ç¤ºéœ€è¦äººç±»æ™ºèƒ½å’ŒåŠ³åŠ¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºå­¦æœ¯çŸ¥è¯†çš„è½¬åŒ–è¿‡ç¨‹æä¾›äº†æ™ºèƒ½ç”¨æˆ·ç•Œé¢çš„å¯èƒ½æ€§ã€‚</li>
<li>æå‡ºä¸€ç§å°†LLMsæŒ‡å¯¼é›†æˆåˆ°ç°æœ‰å­¦æœ¯çŸ¥è¯†åŸºç¡€è®¾æ–½ä¸­çš„æ–¹æ³•ã€‚</li>
<li>LLMsé›†æˆæä¾›äº†å®è·µç»éªŒï¼Œå¹¶è¯¦ç»†è¯´æ˜äº†æœ€ä½³å®è·µå’Œéšœç¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12221v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12221v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12221v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12221v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12221v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="You-Canâ€™t-Eat-Your-Cake-and-Have-It-Too-The-Performance-Degradation-of-LLMs-with-Jailbreak-Defense"><a href="#You-Canâ€™t-Eat-Your-Cake-and-Have-It-Too-The-Performance-Degradation-of-LLMs-with-Jailbreak-Defense" class="headerlink" title="You Canâ€™t Eat Your Cake and Have It Too: The Performance Degradation of   LLMs with Jailbreak Defense"></a>You Canâ€™t Eat Your Cake and Have It Too: The Performance Degradation of   LLMs with Jailbreak Defense</h2><p><strong>Authors:Wuyuao Mai, Geng Hong, Pei Chen, Xudong Pan, Baojun Liu, Yuan Zhang, Haixin Duan, Min Yang</strong></p>
<p>With the rise of generative large language models (LLMs) like LLaMA and ChatGPT, these models have significantly transformed daily life and work by providing advanced insights. However, as jailbreak attacks continue to circumvent built-in safety mechanisms, exploiting carefully crafted scenarios or tokens, the safety risks of LLMs have come into focus. While numerous defense strategiesâ€“such as prompt detection, modification, and model fine-tuningâ€“have been proposed to counter these attacks, a critical question arises: do these defenses compromise the utility and usability of LLMs for legitimate users? Existing research predominantly focuses on the effectiveness of defense strategies without thoroughly examining their impact on performance, leaving a gap in understanding the trade-offs between LLM safety and performance. Our research addresses this gap by conducting a comprehensive study on the utility degradation, safety elevation, and exaggerated-safety escalation of LLMs with jailbreak defense strategies. We propose USEBench, a novel benchmark designed to evaluate these aspects, along with USEIndex, a comprehensive metric for assessing overall model performance. Through experiments on seven state-of-the-art LLMs, we found that mainstream jailbreak defenses fail to ensure both safety and performance simultaneously. Although model-finetuning performs the best overall, their effectiveness varies across LLMs. Furthermore, vertical comparisons reveal that developers commonly prioritize performance over safety when iterating or fine-tuning their LLMs. </p>
<blockquote>
<p>éšç€ç±»ä¼¼LLaMAå’ŒChatGPTçš„ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡æä¾›é«˜çº§æ´å¯Ÿæ˜¾è‘—åœ°æ”¹å˜äº†æ—¥å¸¸ç”Ÿæ´»å’Œå·¥ä½œã€‚ç„¶è€Œï¼Œéšç€è¶Šç‹±æ”»å‡»ç»§ç»­ç»•è¿‡å†…ç½®çš„å®‰å…¨æœºåˆ¶ï¼Œåˆ©ç”¨ç²¾å¿ƒç­–åˆ’çš„åœºæ™¯æˆ–ä»¤ç‰Œï¼ŒLLMçš„å®‰å…¨é£é™©å·²æˆä¸ºå…³æ³¨çš„ç„¦ç‚¹ã€‚è™½ç„¶æå‡ºäº†è®¸å¤šé˜²å¾¡ç­–ç•¥ï¼Œå¦‚æç¤ºæ£€æµ‹ã€ä¿®æ”¹å’Œæ¨¡å‹å¾®è°ƒï¼Œæ¥åº”å¯¹è¿™äº›æ”»å‡»ï¼Œä½†ä¸€ä¸ªé—®é¢˜å‡ºç°äº†ï¼šè¿™äº›é˜²å¾¡æªæ–½æ˜¯å¦æŸå®³LLMå¯¹åˆæ³•ç”¨æˆ·çš„å®ç”¨æ€§å’Œå¯ç”¨æ€§ï¼Ÿç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é˜²å¾¡ç­–ç•¥çš„æœ‰æ•ˆæ€§ä¸Šï¼Œè€Œæ²¡æœ‰å½»åº•ç ”ç©¶å®ƒä»¬å¯¹æ€§èƒ½çš„å½±å“ï¼Œå› æ­¤åœ¨å¯¹LLMå®‰å…¨ä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡æ–¹é¢å­˜åœ¨ç†è§£ä¸Šçš„ç©ºç™½ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡ä¸€é¡¹ç»¼åˆç ”ç©¶æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼ŒåŒ…æ‹¬ç ”ç©¶å®ç”¨ç¨‹åºé™çº§ã€å®‰å…¨æå‡å’Œå…·æœ‰è¶Šç‹±é˜²å¾¡ç­–ç•¥çš„LLMçš„è¿‡åº¦å®‰å…¨å‡çº§ã€‚æˆ‘ä»¬æå‡ºäº†USEBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¿™äº›æ–¹é¢çš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠUSEIndexï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨¡å‹æ€»ä½“æ€§èƒ½çš„å…¨é¢æŒ‡æ ‡ã€‚é€šè¿‡å¯¹ä¸ƒæ¬¾æœ€å…ˆè¿›çš„LLMè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬å‘ç°ä¸»æµçš„è¶Šç‹±é˜²å¾¡æ— æ³•ä¿è¯åŒæ—¶å®ç°å®‰å…¨å’Œæ€§èƒ½ã€‚å°½ç®¡æ¨¡å‹å¾®è°ƒæ€»ä½“ä¸Šè¡¨ç°æœ€ä½³ï¼Œä½†å®ƒä»¬åœ¨LLMä¹‹é—´çš„æ•ˆæœå„ä¸ç›¸åŒã€‚æ­¤å¤–ï¼Œå‚ç›´å¯¹æ¯”è¡¨æ˜ï¼Œå¼€å‘äººå‘˜åœ¨è¿­ä»£æˆ–å¾®è°ƒå…¶LLMæ—¶é€šå¸¸ä¼šä¼˜å…ˆè€ƒè™‘æ€§èƒ½è€Œéå®‰å…¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12210v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚LLaMAå’ŒChatGPTçš„å…´èµ·ï¼Œè¿™äº›æ¨¡å‹ä¸ºæ—¥å¸¸ç”Ÿæ´»å’Œå·¥ä½œæä¾›äº†æ·±åˆ»çš„è§è§£ï¼Œä½†å®‰å…¨æ¼æ´é—®é¢˜ä¹Ÿé€æ¸å‡¸æ˜¾ã€‚å°½ç®¡å·²æœ‰è®¸å¤šé˜²å¾¡ç­–ç•¥å¦‚æç¤ºæ£€æµ‹ã€ä¿®æ”¹å’Œæ¨¡å‹å¾®è°ƒç­‰è¢«æå‡ºä»¥åº”å¯¹æ”»å‡»ï¼Œä½†é˜²å¾¡æªæ–½æ˜¯å¦ä¼šå½±å“æ¨¡å‹çš„å®ç”¨æ€§å’Œç”¨æˆ·å‹å¥½æ€§æˆä¸ºå…³é”®é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç¼ºå£ï¼Œæ¢è®¨é˜²å¾¡æªæ–½å¯¼è‡´çš„è¯­è¨€æ¨¡å‹æ•ˆç”¨ä¸‹é™ã€å®‰å…¨æ€§æå‡å’Œå¤¸å¤§å®‰å…¨æ€§çš„é—®é¢˜ã€‚é€šè¿‡åœ¨ä¸€ç³»åˆ—å…ˆè¿›çš„LLMä¸Šè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬å‘ç°ä¸»æµçš„å®‰å…¨é˜²æŠ¤æªæ–½æ— æ³•åœ¨ä¿éšœå®‰å…¨æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æ¨¡å‹å¾®è°ƒæ•´ä½“è¡¨ç°æœ€ä½³ï¼Œä½†ä¸åŒè¯­è¨€æ¨¡å‹ä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚å‚ç›´å¯¹æ¯”è¡¨æ˜ï¼Œåœ¨è¿­ä»£æˆ–å¾®è°ƒLLMæ—¶ï¼Œå¼€å‘äººå‘˜é€šå¸¸ä¼˜å…ˆè€ƒè™‘æ€§èƒ½è€Œéå®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚LLaMAå’ŒChatGPTä¸ºæ—¥å¸¸ç”Ÿæ´»å’Œå·¥ä½œæä¾›äº†å…ˆè¿›çš„è§è§£ã€‚</li>
<li>LLMçš„å®‰å…¨æ¼æ´é—®é¢˜é€æ¸å‡¸æ˜¾ï¼Œå­˜åœ¨è¢«ç²¾å¿ƒè®¾è®¡çš„åœºæ™¯æˆ–ä»¤ç‰Œåˆ©ç”¨çš„é£é™©ã€‚</li>
<li>ç›®å‰æœ‰è®¸å¤šé˜²å¾¡ç­–ç•¥æ¥åº”å¯¹LLMçš„å®‰å…¨æ”»å‡»ï¼Œä½†å¯¹é˜²å¾¡ç­–ç•¥çš„å®æ–½å¯¹æ¨¡å‹æ€§èƒ½å’Œå®ç”¨æ€§çš„å½±å“çš„ç ”ç©¶å­˜åœ¨ç©ºç™½ã€‚</li>
<li>æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨æ¢è®¨é˜²å¾¡æªæ–½å¯¼è‡´çš„è¯­è¨€æ¨¡å‹æ•ˆç”¨ä¸‹é™ã€å®‰å…¨æ€§æå‡å’Œå¤¸å¤§å®‰å…¨æ€§çš„é—®é¢˜ã€‚</li>
<li>ä¸»æµçš„å®‰å…¨é˜²æŠ¤æªæ–½æ— æ³•åœ¨ä¿éšœå®‰å…¨æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>æ¨¡å‹å¾®è°ƒæ˜¯è¡¨ç°æœ€ä½³çš„é˜²å¾¡ç­–ç•¥ä¹‹ä¸€ï¼Œä½†ä¸åŒè¯­è¨€æ¨¡å‹ä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12210v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12210v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12210v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12210v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12210v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AdaServe-SLO-Customized-LLM-Serving-with-Fine-Grained-Speculative-Decoding"><a href="#AdaServe-SLO-Customized-LLM-Serving-with-Fine-Grained-Speculative-Decoding" class="headerlink" title="AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative   Decoding"></a>AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative   Decoding</h2><p><strong>Authors:Zikun Li, Zhuofu Chen, Remi Delacourt, Gabriele Oliaro, Zeyu Wang, Qinghan Chen, Shuhuai Lin, April Yang, Zhihao Zhang, Zhuoming Chen, Sean Lai, Xupeng Miao, Zhihao Jia</strong></p>
<p>This paper introduces AdaServe, the first LLM serving system to support SLO customization through fine-grained speculative decoding. AdaServe leverages the logits of a draft model to predict the speculative accuracy of tokens and employs a theoretically optimal algorithm to construct token trees for verification. To accommodate diverse SLO requirements without compromising throughput, AdaServe employs a speculation-and-selection scheme that first constructs candidate token trees for each request and then dynamically selects tokens to meet individual SLO constraints while optimizing throughput. Comprehensive evaluations demonstrate that AdaServe achieves up to 73% higher SLO attainment and 74% higher goodput compared to state-of-the-art systems. These results underscore AdaServeâ€™s potential to enhance the efficiency and adaptability of LLM deployments across varied application scenarios. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†AdaServeï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªæ”¯æŒé€šè¿‡ç²¾ç»†ç²’åº¦çš„æ¨æµ‹è§£ç è¿›è¡ŒSLOå®šåˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡ç³»ç»Ÿã€‚AdaServeåˆ©ç”¨è‰æ¡ˆæ¨¡å‹çš„logitsæ¥é¢„æµ‹ç¬¦å·çš„æ¨æµ‹å‡†ç¡®æ€§ï¼Œå¹¶é‡‡ç”¨ç†è®ºä¸Šçš„æœ€ä¼˜ç®—æ³•æ„å»ºç¬¦å·æ ‘è¿›è¡ŒéªŒè¯ã€‚ä¸ºäº†é€‚åº”å„ç§SLOè¦æ±‚è€Œä¸å½±å“ååé‡ï¼ŒAdaServeé‡‡ç”¨äº†ä¸€ç§æ¨æµ‹å’Œé€‰æ‹©æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé¦–å…ˆä¸ºæ¯ä¸ªè¯·æ±‚æ„å»ºå€™é€‰ç¬¦å·æ ‘ï¼Œç„¶ååŠ¨æ€é€‰æ‹©ç¬¦å·ä»¥æ»¡è¶³ä¸ªåˆ«çš„SLOçº¦æŸï¼ŒåŒæ—¶ä¼˜åŒ–ååé‡ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€æ–°ç³»ç»Ÿç›¸æ¯”ï¼ŒAdaServeçš„SLOè¾¾æˆåº¦æé«˜äº†é«˜è¾¾73%ï¼Œgoodputæé«˜äº†74%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†AdaServeåœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸­æé«˜å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²æ•ˆç‡å’Œé€‚åº”æ€§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12162v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†AdaServeç³»ç»Ÿï¼Œå®ƒæ˜¯é¦–ä¸ªæ”¯æŒSLOå®šåˆ¶çš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼Œé€šè¿‡ç²¾ç»†çš„æ¨æµ‹è§£ç æä¾›æœåŠ¡ã€‚AdaServeåˆ©ç”¨æ¨¡å‹è‰æ¡ˆçš„logitsé¢„æµ‹æ ‡è®°çš„æ¨æµ‹å‡†ç¡®æ€§ï¼Œå¹¶é‡‡ç”¨ç†è®ºä¸Šçš„æœ€ä½³ç®—æ³•æ„å»ºç”¨äºéªŒè¯çš„æ ‡è®°æ ‘ã€‚ä¸ºäº†æ»¡è¶³ä¸åŒçš„SLOè¦æ±‚è€Œä¸å½±å“ååé‡ï¼ŒAdaServeé‡‡ç”¨æ¨æµ‹å’Œé€‰æ‹©æ–¹æ¡ˆï¼Œä¸ºæ¯ä¸ªè¯·æ±‚æ„å»ºå€™é€‰æ ‡è®°æ ‘ï¼Œç„¶ååŠ¨æ€é€‰æ‹©æ ‡è®°ä»¥æ»¡è¶³å•ç‹¬çš„SLOçº¦æŸï¼ŒåŒæ—¶ä¼˜åŒ–ååé‡ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒAdaServeçš„SLOè¾¾æˆåº¦æ¯”ç°æœ‰ç³»ç»Ÿé«˜å‡ºé«˜è¾¾73%ï¼Œgoodputé«˜å‡º74%ã€‚è¿™çªæ˜¾äº†AdaServeåœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸­æé«˜LLMéƒ¨ç½²æ•ˆç‡å’Œé€‚åº”æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AdaServeæ˜¯é¦–ä¸ªæ”¯æŒSLOå®šåˆ¶çš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡ç³»ç»Ÿã€‚</li>
<li>AdaServeé€šè¿‡ç²¾ç»†çš„æ¨æµ‹è§£ç æä¾›æœåŠ¡ï¼Œåˆ©ç”¨æ¨¡å‹è‰æ¡ˆçš„logitsé¢„æµ‹æ ‡è®°çš„æ¨æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>AdaServeé‡‡ç”¨ç†è®ºä¸Šçš„æœ€ä½³ç®—æ³•æ„å»ºç”¨äºéªŒè¯çš„æ ‡è®°æ ‘ã€‚</li>
<li>AdaServeé€šè¿‡æ„å»ºå€™é€‰æ ‡è®°æ ‘å¹¶åŠ¨æ€é€‰æ‹©æ ‡è®°ä»¥æ»¡è¶³SLOçº¦æŸå’Œååé‡ä¼˜åŒ–æ¥æ»¡è¶³ä¸åŒSLOè¦æ±‚ã€‚</li>
<li>AdaServeä¸ç°æœ‰ç³»ç»Ÿç›¸æ¯”ï¼ŒSLOè¾¾æˆåº¦æé«˜äº†é«˜è¾¾73%ï¼Œgoodputæé«˜äº†74%ã€‚</li>
<li>AdaServeæé«˜äº†å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²çš„æ•ˆç‡å’Œé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12162v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12162v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12162v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12162v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Improving-Influence-based-Instruction-Tuning-Data-Selection-for-Balanced-Learning-of-Diverse-Capabilities"><a href="#Improving-Influence-based-Instruction-Tuning-Data-Selection-for-Balanced-Learning-of-Diverse-Capabilities" class="headerlink" title="Improving Influence-based Instruction Tuning Data Selection for Balanced   Learning of Diverse Capabilities"></a>Improving Influence-based Instruction Tuning Data Selection for Balanced   Learning of Diverse Capabilities</h2><p><strong>Authors:Qirun Dai, Dylan Zhang, Jiaqi W. Ma, Hao Peng</strong></p>
<p>Selecting appropriate training data is crucial for effective instruction fine-tuning of large language models (LLMs), which aims to (1) elicit strong capabilities, and (2) achieve balanced performance across a diverse range of tasks. Influence-based methods show promise in achieving (1) by estimating the contribution of each training example to the modelâ€™s predictions, but often struggle with (2). Our systematic investigation reveals that this underperformance can be attributed to an inherent bias where certain tasks intrinsically have greater influence than others. As a result, data selection is often biased towards these tasks, not only hurting the modelâ€™s performance on others but also, counterintuitively, harms performance on these high-influence tasks themselves.   As a remedy, we propose BIDS, a Balanced and Influential Data Selection algorithm. BIDS first normalizes influence scores of the training data, and then iteratively balances data selection by choosing the training example with the highest influence on the most underrepresented task. Experiments with both Llama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities show that BIDS consistently outperforms both state-of-the-art influence-based algorithms and other non-influence-based selection frameworks. Surprisingly, training on a 15% subset selected by BIDS can even outperform full-dataset training with a much more balanced performance. Our analysis further highlights the importance of both instance-level normalization and iterative optimization of selected data for balanced learning of diverse capabilities. </p>
<blockquote>
<p>é€‰æ‹©é€‚å½“çš„è®­ç»ƒæ•°æ®å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæŒ‡ä»¤å¾®è°ƒè‡³å…³é‡è¦ï¼Œå…¶æ—¨åœ¨ï¼ˆ1ï¼‰æ¿€å‘å¼ºå¤§çš„èƒ½åŠ›ï¼Œä»¥åŠï¼ˆ2ï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°å¹³è¡¡çš„æ€§èƒ½ã€‚åŸºäºå½±å“çš„æ–¹æ³•åœ¨é€šè¿‡ä¼°è®¡æ¯ä¸ªè®­ç»ƒæ ·æœ¬å¯¹æ¨¡å‹é¢„æµ‹çš„è´¡çŒ®æ¥å®ç°ï¼ˆ1ï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å¾€å¾€éš¾ä»¥åº”å¯¹ï¼ˆ2ï¼‰ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè°ƒæŸ¥è¡¨æ˜ï¼Œè¿™ç§æ€§èƒ½ä¸ä½³å¯å½’å› äºä¸€ç§å›ºæœ‰åè§ï¼Œå³æŸäº›ä»»åŠ¡å¤©ç”Ÿæ¯”å…¶ä»–ä»»åŠ¡å…·æœ‰æ›´å¤§çš„å½±å“åŠ›ã€‚å› æ­¤ï¼Œæ•°æ®é€‰æ‹©å¾€å¾€åå‘äºè¿™äº›ä»»åŠ¡ï¼Œè¿™ä¸ä»…ä¼šæŸå®³æ¨¡å‹åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè€Œä¸”å‡ºäººæ„æ–™çš„æ˜¯ï¼Œè¿˜ä¼šæŸå®³è¿™äº›é«˜å½±å“åŠ›ä»»åŠ¡æœ¬èº«çš„æ€§èƒ½ã€‚ä½œä¸ºä¸€ç§è¡¥æ•‘æªæ–½ï¼Œæˆ‘ä»¬æå‡ºäº†BIDSï¼ˆå¹³è¡¡å’Œå½±å“æ•°æ®é€‰æ‹©ç®—æ³•ï¼‰ã€‚BIDSé¦–å…ˆæ ‡å‡†åŒ–è®­ç»ƒæ•°æ®çš„å½±å“åˆ†æ•°ï¼Œç„¶åé€šè¿‡é€‰æ‹©å¯¹æœ€ä¸å…·ä»£è¡¨æ€§çš„ä»»åŠ¡å…·æœ‰æœ€é«˜å½±å“çš„è®­ç»ƒæ ·æœ¬æ¥è¿­ä»£å¹³è¡¡æ•°æ®é€‰æ‹©ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„Llama-3å’ŒMistral-v0.3å®éªŒï¼Œæ¶µç›–äº†äº”ç§ä¸åŒçš„èƒ½åŠ›ï¼Œè¡¨æ˜BIDSå§‹ç»ˆä¼˜äºæœ€æ–°çš„åŸºäºå½±å“åŠ›å’Œå…¶ä»–éå½±å“åŠ›é€‰æ‹©æ¡†æ¶çš„ç®—æ³•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé€šè¿‡BIDSé€‰æ‹©çš„15%å­é›†è¿›è¡Œè®­ç»ƒç”šè‡³å¯ä»¥è¶…è¶Šå…¨æ•°æ®é›†è®­ç»ƒçš„æ€§èƒ½ï¼Œä¸”è¡¨ç°æ›´ä¸ºå¹³è¡¡ã€‚æˆ‘ä»¬çš„åˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†å®ä¾‹çº§æ ‡å‡†åŒ–å’Œè¿­ä»£ä¼˜åŒ–æ‰€é€‰æ•°æ®åœ¨å¹³è¡¡å­¦ä¹ å¤šç§èƒ½åŠ›æ–¹é¢çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12147v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è®­ç»ƒæ•°æ®çš„é€‰æ‹©å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæŒ‡ä»¤å¾®è°ƒè‡³å…³é‡è¦ï¼Œæ—¨åœ¨ï¼ˆ1ï¼‰æ¿€å‘å¼ºå¤§çš„èƒ½åŠ›ï¼Œå¹¶ï¼ˆ2ï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°å¹³è¡¡æ€§èƒ½ã€‚åŸºäºå½±å“çš„æ–¹æ³•åœ¨å®ç°ï¼ˆ1ï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œé€šè¿‡ä¼°è®¡æ¯ä¸ªè®­ç»ƒæ ·æœ¬å¯¹æ¨¡å‹é¢„æµ‹çš„è´¡çŒ®ï¼Œä½†å¾€å¾€éš¾ä»¥å…¼é¡¾ï¼ˆ2ï¼‰ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè°ƒæŸ¥è¡¨æ˜ï¼Œè¿™ç§æ€§èƒ½ä¸ä½³å¯å½’å› äºå›ºæœ‰åè§ï¼ŒæŸäº›ä»»åŠ¡çš„å½±å“å¤©ç”Ÿè¾ƒå¤§ã€‚å› æ­¤ï¼Œæ•°æ®é€‰æ‹©å¾€å¾€åå‘äºè¿™äº›ä»»åŠ¡ï¼Œè¿™ä¸ä»…ä¼šæŸå®³æ¨¡å‹åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè€Œä¸”ä¼šå‡ºä¹æ„æ–™åœ°æŸå®³è¿™äº›é«˜å½±å“åŠ›ä»»åŠ¡æœ¬èº«çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BIDSç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¹³è¡¡å’Œæœ‰å½±å“åŠ›çš„æ•°æ®é€‰æ‹©ç®—æ³•ã€‚BIDSé¦–å…ˆæ ‡å‡†åŒ–è®­ç»ƒæ•°æ®çš„å½±å“åŠ›åˆ†æ•°ï¼Œç„¶åé€šè¿‡é€‰æ‹©å¯¹æœ€ä»£è¡¨æ€§ä¸è¶³çš„ä»»åŠ¡å…·æœ‰æœ€é«˜å½±å“åŠ›çš„è®­ç»ƒæ ·æœ¬æ¥è¿›è¡Œè¿­ä»£å¹³è¡¡æ•°æ®é€‰æ‹©ã€‚åœ¨Llama-3å’ŒMistral-v0.3ä¸Šçš„ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº”ç§ä¸åŒèƒ½åŠ›çš„å®éªŒè¡¨æ˜ï¼ŒBIDSå§‹ç»ˆä¼˜äºæœ€æ–°å½±å“ç®—æ³•å’Œå…¶ä»–éå½±å“é€‰æ‹©æ¡†æ¶ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé€šè¿‡BIDSé€‰æ‹©çš„å­é›†ä»…å æ•´ä½“çš„15%ï¼Œä½†å…¶è®­ç»ƒæ•ˆæœç”šè‡³å¯ä»¥è¶…è¶Šå…¨æ•°æ®é›†è®­ç»ƒï¼Œä¸”æ€§èƒ½æ›´ä¸ºå¹³è¡¡ã€‚æˆ‘ä»¬çš„åˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†å®ä¾‹çº§æ ‡å‡†åŒ–ä»¥åŠä¼˜åŒ–é€‰å®šæ•°æ®çš„è¿­ä»£é€‰æ‹©å¯¹äºå¹³è¡¡å­¦ä¹ ä¸åŒèƒ½åŠ›çš„é‡è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®­ç»ƒæ•°æ®çš„é€‰æ‹©å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒè‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºå½±å“çš„æ–¹æ³•åœ¨è¯„ä¼°è®­ç»ƒæ ·æœ¬å¯¹æ¨¡å‹é¢„æµ‹çš„è´¡çŒ®æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨å®ç°è·¨ä»»åŠ¡å¹³è¡¡æ€§èƒ½æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>æŸäº›ä»»åŠ¡åœ¨æ•°æ®é€‰æ‹©ä¸­å¯èƒ½å­˜åœ¨å›ºæœ‰åè§ï¼Œå½±å“æ•°æ®é€‰æ‹©çš„å¹³è¡¡æ€§ã€‚</li>
<li>BIDSç®—æ³•é€šè¿‡æ ‡å‡†åŒ–å½±å“åŠ›åˆ†æ•°å’Œè¿­ä»£å¹³è¡¡æ•°æ®é€‰æ‹©æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>BIDSç®—æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨å°è§„æ¨¡æ•°æ®å­é›†ä¸Šä¹Ÿèƒ½è¶…è¶Šå…¨æ•°æ®é›†è®­ç»ƒã€‚</li>
<li>å®ä¾‹çº§æ ‡å‡†åŒ–å¯¹å¹³è¡¡å­¦ä¹ ä¸åŒèƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12147v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12147v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12147v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12147v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Harnessing-Generative-Pre-Trained-Transformer-for-Datacenter-Packet-Trace-Generation"><a href="#Harnessing-Generative-Pre-Trained-Transformer-for-Datacenter-Packet-Trace-Generation" class="headerlink" title="Harnessing Generative Pre-Trained Transformer for Datacenter Packet   Trace Generation"></a>Harnessing Generative Pre-Trained Transformer for Datacenter Packet   Trace Generation</h2><p><strong>Authors:Chen Griner</strong></p>
<p>Today, the rapid growth of applications reliant on datacenters calls for new advancements to meet the increasing traffic and computational demands. Traffic traces from datacenters are essential for further development and optimization of future datacenters. However, traces are rarely released to the public. Researchers often use simplified mathematical models that lack the depth needed to recreate intricate traffic patterns and, thus, miss optimization opportunities found in realistic traffic. In this preliminary work, we introduce DTG-GPT, a packet-level Datacenter Traffic Generator (DTG), based on the generative pre-trained transformer (GPT) architecture used by many state-of-the-art large language models. We train our model on a small set of available traffic traces from different domains and offer a simple methodology to evaluate the fidelity of the generated traces to their original counterparts. We show that DTG-GPT can synthesize novel traces that mimic the spatiotemporal patterns found in real traffic traces. We further demonstrate that DTG-GPT can generate traces for networks of different scales while maintaining fidelity. Our findings indicate the potential that, in the future, similar models to DTG-GPT will allow datacenter operators to release traffic information to the research community via trained GPT models. </p>
<blockquote>
<p>å½“ä»Šï¼Œä¾èµ–äºæ•°æ®ä¸­å¿ƒçš„åº”ç”¨ç¨‹åºçš„å¿«é€Ÿå¢é•¿è¦æ±‚æ–°çš„è¿›å±•ä»¥æ»¡è¶³æ—¥ç›Šå¢é•¿çš„æµé‡å’Œè®¡ç®—éœ€æ±‚ã€‚æ•°æ®ä¸­å¿ƒçš„æµé‡è·Ÿè¸ªå¯¹äºæœªæ¥æ•°æ®ä¸­å¿ƒçš„è¿›ä¸€æ­¥å‘å±•å’Œä¼˜åŒ–è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™äº›è·Ÿè¸ªå¾ˆå°‘å…¬å¼€å‘å¸ƒã€‚ç ”ç©¶äººå‘˜é€šå¸¸ä½¿ç”¨ç®€åŒ–çš„æ•°å­¦æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ç¼ºä¹æ‰€éœ€çš„æ·±åº¦ï¼Œæ— æ³•é‡ç°å¤æ‚çš„æµé‡æ¨¡å¼ï¼Œä»è€Œé”™è¿‡äº†ç°å®æµé‡ä¸­å‘ç°çš„ä¼˜åŒ–æœºä¼šã€‚åœ¨è¿™é¡¹åˆæ­¥å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰æ¶æ„çš„æ•°æ®ä¸­å¿ƒæµé‡ç”Ÿæˆå™¨ï¼ˆDTG-GPTï¼‰ï¼Œè¯¥æ¶æ„è¢«è®¸å¤šæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹æ‰€é‡‡ç”¨ã€‚æˆ‘ä»¬å¯¹ä¸åŒé¢†åŸŸå¯ç”¨çš„å°‘é‡æµé‡è·Ÿè¸ªæ•°æ®è¿›è¡Œè®­ç»ƒæ¨¡å‹ï¼Œå¹¶æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥è¯„ä¼°ç”Ÿæˆè·Ÿè¸ªä¸å…¶åŸå§‹å¯¹åº”ç‰©çš„ä¿çœŸåº¦ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒDTG-GPTå¯ä»¥åˆæˆæ¨¡ä»¿çœŸå®æµé‡è·Ÿè¸ªä¸­æ—¶ç©ºæ¨¡å¼çš„æ–°å‹è·Ÿè¸ªã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼ŒDTG-GPTå¯ä»¥åœ¨ä¿æŒä¿çœŸåº¦çš„åŒæ—¶ï¼Œä¸ºä¸åŒè§„æ¨¡çš„ç½‘ç»œç”Ÿæˆè·Ÿè¸ªã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœªæ¥ç±»ä¼¼DTG-GPTçš„æ¨¡å‹å°†å…è®¸æ•°æ®ä¸­å¿ƒè¿è¥å•†é€šè¿‡è®­ç»ƒçš„GPTæ¨¡å‹å‘ç ”ç©¶ç¤¾åŒºå‘å¸ƒæµé‡ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12033v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®ä¸­å¿ƒçš„åº”ç”¨å¿«é€Ÿå‘å±•ï¼Œäº§ç”Ÿäº†å·¨å¤§çš„æµé‡å’Œè®¡ç®—éœ€æ±‚ã€‚ç ”ç©¶äººå‘˜é€šå¸¸ä¾èµ–ç®€åŒ–çš„æ•°å­¦æ¨¡å‹ï¼Œæ— æ³•é‡ç°çœŸå®æµé‡æ¨¡å¼ä¸­çš„æœºä¼šï¼Œéœ€è¦è¿›è¡Œè¿›ä¸€æ­¥çš„å‘å±•å’Œä¼˜åŒ–ã€‚æœ¬å·¥ä½œä»‹ç»äº†åŸºäºGPTæ¶æ„çš„æ•°æ®ä¸­å¿ƒæµé‡ç”Ÿæˆå™¨DTG-GPTï¼Œå¯ä»¥åˆæˆæ¨¡æ‹ŸçœŸå®æµé‡æ¨¡å¼çš„æµé‡è½¨è¿¹ï¼Œå¹¶å±•ç¤ºå…¶åœ¨ä¸åŒè§„æ¨¡ç½‘ç»œä¸­çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®ä¸­å¿ƒåº”ç”¨çš„å¿«é€Ÿå‘å±•å¯¼è‡´å¯¹æµé‡å’Œè®¡ç®—éœ€æ±‚çš„å¢åŠ ã€‚</li>
<li>æµé‡è½¨è¿¹å¯¹äºæ•°æ®ä¸­å¿ƒçš„å‘å±•å’Œä¼˜åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰çš„æµé‡è½¨è¿¹å¾ˆå°‘å…¬å¼€ï¼Œç ”ç©¶äººå‘˜é€šå¸¸ä½¿ç”¨ç®€åŒ–çš„æ•°å­¦æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†ä¼˜åŒ–æœºä¼šã€‚</li>
<li>DTG-GPTæ˜¯ä¸€ç§åŸºäºGPTæ¶æ„çš„æ•°æ®ä¸­å¿ƒæµé‡ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿç”Ÿæˆæ¨¡æ‹ŸçœŸå®æµé‡æ¨¡å¼çš„æµé‡è½¨è¿¹ã€‚</li>
<li>DTG-GPTæä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•è®ºæ¥è¯„ä¼°ç”Ÿæˆè½¨è¿¹ä¸åŸå§‹è½¨è¿¹çš„ä¿çœŸåº¦ã€‚</li>
<li>DTG-GPTèƒ½å¤Ÿåœ¨ä¸åŒè§„æ¨¡çš„ç½‘ç»œä¸­ç”Ÿæˆæµé‡è½¨è¿¹ï¼Œå¹¶ä¿æŒè¾ƒé«˜çš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12033v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12033v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12033v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12033v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Trustformer-A-Trusted-Federated-Transformer"><a href="#Trustformer-A-Trusted-Federated-Transformer" class="headerlink" title="Trustformer: A Trusted Federated Transformer"></a>Trustformer: A Trusted Federated Transformer</h2><p><strong>Authors:Ali Abbasi Tadi, Dima Alhadidi, Luis Rueda</strong></p>
<p>Transformers, a cornerstone of deep-learning architectures for sequential data, have achieved state-of-the-art results in tasks like Natural Language Processing (NLP). Models such as BERT and GPT-3 exemplify their success and have driven the rise of large language models (LLMs). However, a critical challenge persists: safeguarding the privacy of data used in LLM training. Privacy-preserving techniques like Federated Learning (FL) offer potential solutions, but practical limitations hinder their effectiveness for Transformer training. Two primary issues are (I) the risk of sensitive information leakage due to aggregation methods like FedAvg or FedSGD, and (II) the high communication overhead caused by the large size of Transformer models.   This paper introduces a novel FL method that reduces communication overhead while maintaining competitive utility. Our approach avoids sharing full model weights by simulating a global model locally. We apply k-means clustering to each Transformer layer, compute centroids locally, and transmit only these centroids to the server instead of full weights or gradients. To enhance security, we leverage Intel SGX for secure transmission of centroids. Evaluated on a translation task, our method achieves utility comparable to state-of-the-art baselines while significantly reducing communication costs. This provides a more efficient and privacy-preserving FL solution for Transformer models. </p>
<blockquote>
<p>Transformeræ˜¯åºåˆ—æ•°æ®æ·±åº¦å­¦ä¹ æ¶æ„çš„åŸºçŸ³ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚BERTå’ŒGPT-3ç­‰æ¨¡å‹ä½“ç°äº†å…¶æˆåŠŸï¼Œå¹¶æ¨åŠ¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å´›èµ·ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼šä¿æŠ¤ç”¨äºLLMè®­ç»ƒçš„æ•°æ®éšç§ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ç­‰éšç§ä¿æŠ¤æŠ€æœ¯æä¾›äº†æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®é™…åº”ç”¨ä¸­çš„é™åˆ¶é˜»ç¢äº†å…¶åœ¨Transformerè®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸¤ä¸ªé—®é¢˜ä¸»è¦æ˜¯ï¼ˆIï¼‰ç”±äºFedAvgæˆ–FedSGDç­‰èšåˆæ–¹æ³•å¯¼è‡´çš„æ•æ„Ÿä¿¡æ¯æ³„éœ²é£é™©ï¼Œï¼ˆIIï¼‰ç”±äºTransformeræ¨¡å‹è§„æ¨¡è¾ƒå¤§å¯¼è‡´çš„é€šä¿¡å¼€é”€è¾ƒé«˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„FLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶å‡å°‘äº†é€šä¿¡å¼€é”€ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨æœ¬åœ°æ¨¡æ‹Ÿå…¨å±€æ¨¡å‹ï¼Œé¿å…å…±äº«å®Œæ•´çš„æ¨¡å‹æƒé‡ã€‚æˆ‘ä»¬å¯¹æ¯ä¸ªTransformerå±‚åº”ç”¨k-meansèšç±»ï¼Œæœ¬åœ°è®¡ç®—è´¨å¿ƒï¼Œå¹¶å°†è¿™äº›è´¨å¿ƒè€Œä¸æ˜¯å®Œæ•´çš„æƒé‡æˆ–æ¢¯åº¦ä¼ è¾“åˆ°æœåŠ¡å™¨ã€‚ä¸ºäº†æé«˜å®‰å…¨æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨Intel SGXè¿›è¡Œè´¨å¿ƒçš„å®‰å…¨ä¼ è¾“ã€‚åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸æœ€æ–°æŠ€æœ¯åŸºçº¿ç›¸å½“çš„æ•ˆæœï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†é€šä¿¡æˆæœ¬ã€‚è¿™ä¸ºTransformeræ¨¡å‹çš„è”é‚¦å­¦ä¹ æä¾›äº†æ›´é«˜æ•ˆã€æ›´ä¿æŠ¤éšç§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11706v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹è”é‚¦å­¦ä¹ ï¼ˆFederated Learningï¼ŒFLï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘è®­ç»ƒTransformeræ¨¡å‹æ—¶çš„é€šä¿¡å¼€é”€å¹¶ä¿æŠ¤æ•°æ®éšç§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨¡æ‹Ÿå…¨å±€æ¨¡å‹è¿›è¡Œæœ¬åœ°è®­ç»ƒï¼Œåˆ©ç”¨k-meansèšç±»è®¡ç®—å„Transformerå±‚çš„è´¨å¿ƒï¼Œä»…ä¼ è¾“è¿™äº›è´¨å¿ƒåˆ°æœåŠ¡å™¨ï¼ŒåŒæ—¶é‡‡ç”¨Intel SGXæŠ€æœ¯ä¿éšœå®‰å…¨ä¼ è¾“ã€‚åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†é€šä¿¡æˆæœ¬ï¼Œä¸ºTransformeræ¨¡å‹æä¾›é«˜æ•ˆä¸”éšç§ä¿æŠ¤çš„è”é‚¦å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†æ•°æ®éšç§ä¿æŠ¤ä»æ˜¯å…¶é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿçš„éšç§ä¿æŠ¤æŠ€æœ¯å¦‚è”é‚¦å­¦ä¹ åœ¨Transformerè®­ç»ƒä¸­å­˜åœ¨å±€é™æ€§ï¼Œé¢ä¸´ç€ä¿¡æ¯æ³„éœ²å’Œé€šä¿¡å¼€é”€é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿå…¨å±€æ¨¡å‹è¿›è¡Œæœ¬åœ°è®­ç»ƒæ¥å‡å°‘é€šä¿¡å¼€é”€ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨k-meansèšç±»è®¡ç®—Transformerå„å±‚çš„è´¨å¿ƒï¼Œå¹¶ä»…ä¼ è¾“è¿™äº›è´¨å¿ƒåˆ°æœåŠ¡å™¨ï¼Œé™ä½äº†é€šä¿¡æˆæœ¬ã€‚</li>
<li>ä¸ºäº†å¢å¼ºå®‰å…¨æ€§ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨Intel SGXæŠ€æœ¯ä¿éšœè´¨å¿ƒçš„å®‰å…¨ä¼ è¾“ã€‚</li>
<li>åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†é€šä¿¡æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.11706v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.11706v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.11706v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.11706v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Question-to-Question-Retrieval-for-Hallucination-Free-Knowledge-Access-An-Approach-for-Wikipedia-and-Wikidata-Question-Answering"><a href="#Question-to-Question-Retrieval-for-Hallucination-Free-Knowledge-Access-An-Approach-for-Wikipedia-and-Wikidata-Question-Answering" class="headerlink" title="Question-to-Question Retrieval for Hallucination-Free Knowledge Access:   An Approach for Wikipedia and Wikidata Question Answering"></a>Question-to-Question Retrieval for Hallucination-Free Knowledge Access:   An Approach for Wikipedia and Wikidata Question Answering</h2><p><strong>Authors:Santhosh Thottingal</strong></p>
<p>This paper introduces an approach to question answering over knowledge bases like Wikipedia and Wikidata by performing â€œquestion-to-questionâ€ matching and retrieval from a dense vector embedding store. Instead of embedding document content, we generate a comprehensive set of questions for each logical content unit using an instruction-tuned LLM. These questions are vector-embedded and stored, mapping to the corresponding content. Vector embedding of user queries are then matched against this question vector store. The highest similarity score leads to direct retrieval of the associated article content, eliminating the need for answer generation. Our method achieves high cosine similarity ( &gt; 0.9 ) for relevant question pairs, enabling highly precise retrieval. This approach offers several advantages including computational efficiency, rapid response times, and increased scalability. We demonstrate its effectiveness on Wikipedia and Wikidata, including multimedia content through structured fact retrieval from Wikidata, opening up new pathways for multimodal question answering. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åœ¨çŸ¥è¯†åº“ï¼ˆå¦‚Wikipediaå’ŒWikidataï¼‰ä¸Šè¿›è¡Œé—®ç­”çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰§è¡Œâ€œé—®é¢˜åˆ°é—®é¢˜â€çš„åŒ¹é…å’Œä»å¯†é›†å‘é‡åµŒå…¥å­˜å‚¨åº“ä¸­è¿›è¡Œæ£€ç´¢æ¥å®ç°ã€‚æˆ‘ä»¬ä¸æ˜¯åµŒå…¥æ–‡æ¡£å†…å®¹ï¼Œè€Œæ˜¯ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¯ä¸ªé€»è¾‘å†…å®¹å•å…ƒç”Ÿæˆä¸€ç»„å…¨é¢çš„é—®é¢˜ã€‚è¿™äº›é—®é¢˜è¢«å‘é‡åµŒå…¥å¹¶å­˜å‚¨ï¼Œæ˜ å°„åˆ°ç›¸åº”çš„å†…å®¹ã€‚ç„¶åï¼Œå°†ç”¨æˆ·æŸ¥è¯¢çš„å‘é‡åµŒå…¥ä¸æ­¤é—®é¢˜å‘é‡å­˜å‚¨åº“è¿›è¡ŒåŒ¹é…ã€‚æœ€é«˜ç›¸ä¼¼åº¦å¾—åˆ†ç›´æ¥å¯¼è‡´ç›´æ¥æ£€ç´¢ç›¸å…³çš„æ–‡ç« å†…å®¹ï¼Œæ— éœ€ç”Ÿæˆç­”æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ç›¸å…³é—®é¢˜å¯¹çš„é«˜ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ&gt; 0.9ï¼‰ï¼Œèƒ½å¤Ÿå®ç°é«˜åº¦ç²¾ç¡®çš„æ£€ç´¢ã€‚è¿™ç§æ–¹æ³•æä¾›äº†å‡ ä¸ªä¼˜ç‚¹ï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡é«˜ã€å“åº”é€Ÿåº¦å¿«å’Œå¯æ‰©å±•æ€§å¼ºã€‚æˆ‘ä»¬åœ¨Wikipediaå’ŒWikidataä¸Šå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬é€šè¿‡ä»Wikidataè¿›è¡Œç»“æ„åŒ–äº‹å®æ£€ç´¢æ¥åŒ…å«å¤šåª’ä½“å†…å®¹ï¼Œä¸ºå¤šåª’ä½“é—®ç­”å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11301v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>è®ºæ–‡ä»‹ç»äº†ä¸€ç§é€šè¿‡æ‰§è¡Œâ€œé—®é¢˜åˆ°é—®é¢˜â€åŒ¹é…å’Œä»å¯†é›†å‘é‡åµŒå…¥å­˜å‚¨ä¸­è¿›è¡Œæ£€ç´¢ï¼Œå®ç°åŸºäºçŸ¥è¯†åº“ï¼ˆå¦‚Wikipediaå’ŒWikidataï¼‰çš„é—®é¢˜å›ç­”çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸åµŒå…¥æ–‡æ¡£å†…å®¹ï¼Œè€Œæ˜¯ä¸ºé€»è¾‘å†…å®¹å•å…ƒç”Ÿæˆä¸€ç»„é—®é¢˜å¹¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è°ƒæ•´æŒ‡ä»¤ã€‚è¿™äº›é—®é¢˜è¿›è¡Œå‘é‡åµŒå…¥å¹¶å­˜å‚¨ï¼Œæ˜ å°„åˆ°å¯¹åº”çš„å†…å®¹ã€‚ç„¶ååŒ¹é…ç”¨æˆ·æŸ¥è¯¢çš„å‘é‡åµŒå…¥ä¸é—®é¢˜å‘é‡å­˜å‚¨ã€‚æœ€é«˜ç›¸ä¼¼åº¦å¾—åˆ†å¯ç›´æ¥æ£€ç´¢ç›¸å…³æ–‡ç« å†…å®¹ï¼Œæ— éœ€ç”Ÿæˆç­”æ¡ˆã€‚è¯¥æ–¹æ³•å®ç°äº†é«˜ä½™å¼¦ç›¸ä¼¼æ€§ï¼ˆ&gt; 0.9ï¼‰ï¼Œä¸ºWikipediaå’ŒWikidataç­‰å¤šåª’ä½“å†…å®¹æä¾›äº†é«˜æ•ˆã€å¿«é€Ÿå“åº”å’Œé«˜åº¦å¯æ‰©å±•çš„ç²¾å‡†æ£€ç´¢æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>è¯¥æ–¹æ³•é€šè¿‡æ‰§è¡Œâ€œé—®é¢˜åˆ°é—®é¢˜â€åŒ¹é…å’Œä»å¯†é›†å‘é‡åµŒå…¥å­˜å‚¨ä¸­æ£€ç´¢ï¼Œå®ç°äº†åŸºäºçŸ¥è¯†åº“çš„é—®é¢˜å›ç­”ã€‚</li>
<li>æ–¹æ³•ä¸åµŒå…¥æ–‡æ¡£å†…å®¹ï¼Œè€Œæ˜¯ä¸ºé€»è¾‘å†…å®¹å•å…ƒç”Ÿæˆä¸€ç»„é—®é¢˜å¹¶ä½¿ç”¨LLMè°ƒæ•´æŒ‡ä»¤ã€‚</li>
<li>é€šè¿‡åŒ¹é…ç”¨æˆ·æŸ¥è¯¢ä¸é—®é¢˜å‘é‡å­˜å‚¨ï¼Œå®ç°ç›´æ¥æ£€ç´¢ç›¸å…³æ–‡ç« å†…å®¹ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†é«˜ä½™å¼¦ç›¸ä¼¼æ€§ï¼ˆ&gt; 0.9ï¼‰ï¼Œæé«˜äº†æ£€ç´¢çš„ç²¾å‡†åº¦ã€‚</li>
<li>æ–¹æ³•å…·æœ‰è®¡ç®—æ•ˆç‡é«˜ã€å“åº”é€Ÿåº¦å¿«å’Œå¯æ‰©å±•æ€§å¼ºçš„ä¼˜ç‚¹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨Wikipediaå’ŒWikidataç­‰å¤šåª’ä½“å†…å®¹ä¸Šå±•ç¤ºäº†æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.11301v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.11301v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.11301v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.11301v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.11301v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Robust-Hybrid-Classical-Quantum-Transfer-Learning-Model-for-Text-Classification-Using-GPT-Neo-125M-with-LoRA-SMOTE-Enhancement"><a href="#Robust-Hybrid-Classical-Quantum-Transfer-Learning-Model-for-Text-Classification-Using-GPT-Neo-125M-with-LoRA-SMOTE-Enhancement" class="headerlink" title="Robust Hybrid Classical-Quantum Transfer Learning Model for Text   Classification Using GPT-Neo 125M with LoRA &amp; SMOTE Enhancement"></a>Robust Hybrid Classical-Quantum Transfer Learning Model for Text   Classification Using GPT-Neo 125M with LoRA &amp; SMOTE Enhancement</h2><p><strong>Authors:Santanam Wishal</strong></p>
<p>This research introduces a hybrid classical-quantum framework for text classification, integrating GPT-Neo 125M with Low-Rank Adaptation (LoRA) and Synthetic Minority Over-sampling Technique (SMOTE) using quantum computing backends. While the GPT-Neo 125M baseline remains the best-performing model, the implementation of LoRA and SMOTE enhances the hybrid model, resulting in improved accuracy, faster convergence, and better generalization. Experiments on IBMâ€™s 127-qubit quantum backend and Pennylaneâ€™s 32-qubit simulation demonstrate the viability of combining classical neural networks with quantum circuits. This framework underscores the potential of hybrid architectures for advancing natural language processing applications. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ç”¨äºæ–‡æœ¬åˆ†ç±»çš„æ··åˆç»å…¸-é‡å­æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†GPT-Neo 125Mä¸ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å’Œåˆæˆå°‘æ•°è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEï¼‰ï¼Œå¹¶ä½¿ç”¨é‡å­è®¡ç®—åç«¯ã€‚è™½ç„¶GPT-Neo 125MåŸºçº¿ä»ç„¶æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œä½†LoRAå’ŒSMOTEçš„å®æ–½å¢å¼ºäº†æ··åˆæ¨¡å‹ï¼Œæé«˜äº†å‡†ç¡®æ€§ã€æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨IBMçš„127é‡å­æ¯”ç‰¹åç«¯å’ŒPennylaneçš„32é‡å­æ¯”ç‰¹æ¨¡æ‹Ÿä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†å°†ç»å…¸ç¥ç»ç½‘ç»œä¸é‡å­ç”µè·¯ç›¸ç»“åˆæ˜¯å¯è¡Œçš„ã€‚è¯¥æ¡†æ¶å¼ºè°ƒäº†æ··åˆæ¶æ„åœ¨æ¨è¿›è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10435v1">PDF</a> 8 pages, 11 figures</p>
<p><strong>Summary</strong><br>åŸºäºé‡å­è®¡ç®—åç«¯æŠ€æœ¯çš„æ”¯æŒï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªèåˆç»å…¸ä¸é‡å­è®¡ç®—çš„æ–‡æœ¬åˆ†ç±»æ¡†æ¶ã€‚å®ƒç»“åˆäº†GPT-Neo 125Mæ¨¡å‹ã€ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ä»¥åŠåˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEï¼‰ã€‚å…¶ä¸­GPT-Neo 125Mè¡¨ç°æœ€ä¼˜ï¼Œç»“åˆäº†LoRAå’ŒSMOTEæŠ€æœ¯çš„æ··åˆæ¨¡å‹åœ¨å‡†ç¡®æ€§ã€æ”¶æ•›é€Ÿåº¦ä»¥åŠæ³›åŒ–èƒ½åŠ›ä¸Šæœ‰æ‰€æå‡ã€‚åœ¨IBMçš„127é‡å­æ¯”ç‰¹åç«¯å’ŒPennylaneçš„32é‡å­æ¯”ç‰¹æ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†ç»“åˆç»å…¸ç¥ç»ç½‘ç»œä¸é‡å­ç”µè·¯çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾äº†æ··åˆæ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªèåˆç»å…¸ä¸é‡å­è®¡ç®—çš„æ–‡æœ¬åˆ†ç±»æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†GPT-Neo 125Mæ¨¡å‹ã€ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å’Œåˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEï¼‰ã€‚</li>
<li>GPT-Neo 125Mè¡¨ç°æœ€ä¼˜ï¼Œä½†ç»“åˆLoRAå’ŒSMOTEæŠ€æœ¯åï¼Œæ··åˆæ¨¡å‹çš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
<li>æ··åˆæ¨¡å‹åœ¨å‡†ç¡®æ€§ã€æ”¶æ•›é€Ÿåº¦å’Œæ³›åŒ–èƒ½åŠ›ä¸Šæœ‰æ‰€æ”¹è¿›ã€‚</li>
<li>åœ¨IBMçš„é‡å­åç«¯å’ŒPennylaneæ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥ç ”ç©¶çªæ˜¾äº†æ··åˆæ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.10435v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.10435v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸»è¦èšç„¦äºæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¾ƒå°‘å¼ºè°ƒè¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç”±äºåŸºæœ¬æ¨¡æ€å·®å¼‚ï¼Œåœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­å®ç°é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œé€æ­¥è®­ç»ƒLLMç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆä½¿æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜å®ç°äº†é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼Œæ— éœ€ä½¿ç”¨å•ç‹¬çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ï¼Œä»è€Œå¤§å¤§åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°å…ˆè¿›æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œå¯å®ç°è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v3">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a> (2K+ Stars by now)</p>
<p><strong>Summary</strong>ï¼š<br>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦èåˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¾ƒå°‘å…³æ³¨è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé€æ­¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œå®ç°æµç•…çš„è§†å¬äº¤äº’ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿æŒå¼ºå¤§çš„è§†è§‰è¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œè¿˜èƒ½å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—ï¼Œæ˜¾è‘—æé«˜å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚ä¸å‰æ²¿æŠ€æœ¯å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹å…·å¤‡å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œå®ç°è¿‘ä¹å®æ—¶çš„è§†å¬äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦å…³æ³¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„èåˆã€‚</li>
<li>è¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼ŒåŒæ—¶å¤„ç†è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹é€æ­¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ã€‚</li>
<li>è®­ç»ƒç­–ç•¥å®ç°æµç•…çš„è§†å¬äº¤äº’ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„è§†è§‰è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å…·å¤‡é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡æœ¬è½¬è¯­éŸ³æ¨¡å—ã€‚</li>
<li>æ¨¡å‹æ˜¾è‘—æé«˜å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.01957v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.01957v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.01957v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.01957v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.01957v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Training-Inference-Gap-in-LLMs-by-Leveraging-Self-Generated-Tokens"><a href="#Bridging-the-Training-Inference-Gap-in-LLMs-by-Leveraging-Self-Generated-Tokens" class="headerlink" title="Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated   Tokens"></a>Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated   Tokens</h2><p><strong>Authors:Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhari, Huzefa Rangwala, George Karypis, Rasool Fakoor</strong></p>
<p>Language models are often trained to maximize the likelihood of the next token given past tokens in the training dataset. However, during inference time, they are utilized differently, generating text sequentially and auto-regressively by using previously generated tokens as input to predict the next one. Marginal differences in predictions at each step can cascade over successive steps, resulting in different distributions from what the models were trained for and potentially leading to unpredictable behavior. This paper proposes two simple approaches based on model own generation to address this discrepancy between the training and inference time. Our first approach is Batch-Scheduled Sampling, where, during training, we stochastically choose between the ground-truth token from the dataset and the modelâ€™s own generated token as input to predict the next token. This is done in an offline manner, modifying the context window by interleaving ground-truth tokens with those generated by the model. Our second approach is Reference-Answer-based Correction, where we explicitly incorporate a self-correction capability into the model during training. This enables the model to effectively self-correct the gaps between the generated sequences and the ground truth data without relying on an external oracle model. By incorporating our proposed strategies during training, we have observed an overall improvement in performance compared to baseline methods, as demonstrated by our extensive experiments using summarization, general question-answering, and math question-answering tasks. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹é€šå¸¸ç»è¿‡è®­ç»ƒï¼Œä»¥æœ€å¤§åŒ–åœ¨ç»™å®šçš„è®­ç»ƒæ•°æ®é›†ä¸­è¿‡å»æ ‡è®°ä¸‹ä¸€ä¸ªæ ‡è®°çš„å¯èƒ½æ€§ã€‚ä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒä»¬çš„ä½¿ç”¨æ–¹å¼ä¸åŒã€‚é€šè¿‡å…ˆå‰ç”Ÿæˆçš„æ ‡è®°æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œä»¥è‡ªå›å½’çš„æ–¹å¼è¿ç»­ç”Ÿæˆæ–‡æœ¬ã€‚æ¯ä¸€æ­¥é¢„æµ‹ä¸­çš„å¾®å°å·®å¼‚å¯èƒ½ä¼šåœ¨åç»­æ­¥éª¤ä¸­ç´¯ç§¯ï¼Œå¯¼è‡´ä¸æ¨¡å‹è®­ç»ƒæ—¶çš„åˆ†å¸ƒä¸åŒï¼Œå¹¶å¯èƒ½å¯¼è‡´ä¸å¯é¢„æµ‹çš„è¡Œä¸ºã€‚æœ¬æ–‡é’ˆå¯¹è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä¹‹é—´çš„å·®å¼‚æå‡ºäº†ä¸¤ç§ç®€å•çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç¬¬ä¸€ç§æ–¹æ³•æ˜¯æ‰¹è°ƒåº¦é‡‡æ ·ï¼ˆBatch-Scheduled Samplingï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éšæœºé€‰æ‹©æ•°æ®é›†çš„çœŸå®æ ‡è®°å’Œæ¨¡å‹è‡ªèº«ç”Ÿæˆçš„æ ‡è®°ä½œä¸ºè¾“å…¥æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚è¿™æ˜¯ç¦»çº¿å®Œæˆçš„ï¼Œé€šè¿‡äº¤æ›¿çœŸå®æ ‡è®°å’Œæ¨¡å‹ç”Ÿæˆçš„æ ‡è®°æ¥ä¿®æ”¹ä¸Šä¸‹æ–‡çª—å£ã€‚ç¬¬äºŒç§æ–¹æ³•æ˜¯åŸºäºå‚è€ƒç­”æ¡ˆçš„æ ¡æ­£ï¼ˆReference-Answer-based Correctionï¼‰ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å¼åœ°å°†è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›èå…¥æ¨¡å‹ã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°è‡ªæˆ‘æ ¡æ­£ç”Ÿæˆåºåˆ—å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„å·®è·ï¼Œè€Œä¸ä¾èµ–å¤–éƒ¨æ ‡å‡†æ¨¡å‹ã€‚é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥æˆ‘ä»¬æå‡ºçš„ç­–ç•¥ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ•´ä½“æ€§èƒ½æœ‰æ‰€æå‡ï¼Œè¿™åœ¨æˆ‘ä»¬ä½¿ç”¨æ‘˜è¦ã€é€šç”¨é—®ç­”å’Œæ•°å­¦é—®ç­”ä»»åŠ¡è¿›è¡Œçš„å¹¿æ³›å®éªŒä¸­å¾—åˆ°äº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14655v2">PDF</a> Published in TMLR</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶æå‡ºäº†ä¸¤ç§ç®€å•çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç¬¬ä¸€ç§æ–¹æ³•æ˜¯Batch-Scheduled Samplingï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºé€‰æ‹©çœŸå®æˆ–æ¨¡å‹ç”Ÿæˆçš„tokenä½œä¸ºè¾“å…¥è¿›è¡Œé¢„æµ‹ã€‚ç¬¬äºŒç§æ–¹æ³•æ˜¯Reference-Answer-based Correctionï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ºæ¨¡å‹å¢åŠ è‡ªæˆ‘çº é”™èƒ½åŠ›ï¼Œä»è€Œæé«˜ç”Ÿæˆçš„åºåˆ—ä¸çœŸå®æ•°æ®ä¹‹é—´çš„åŒ¹é…åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•å‡èƒ½æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´å­˜åœ¨å·®å¼‚ï¼Œå¯èƒ½å¯¼è‡´ä¸å¯é¢„æµ‹çš„è¡Œä¸ºã€‚</li>
<li>Batch-Scheduled Samplingæ–¹æ³•é€šè¿‡éšæœºé€‰æ‹©çœŸå®æˆ–æ¨¡å‹ç”Ÿæˆçš„tokenè¿›è¡Œè®­ç»ƒï¼Œç¼©å°äº†è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„å·®è·ã€‚</li>
<li>Reference-Answer-based Correctionæ–¹æ³•è®­ç»ƒæ¨¡å‹æ—¶å¢åŠ äº†è‡ªæˆ‘çº é”™èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è¿™ä¸¤ç§æ–¹æ³•ï¼Œå¯ä»¥æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„ç­–ç•¥åœ¨æ€»ç»“ã€é€šç”¨é—®ç­”å’Œæ•°å­¦é—®ç­”ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</li>
<li>Batch-Scheduled Samplingæ˜¯é€šè¿‡ç¦»çº¿æ–¹å¼ä¿®æ”¹ä¸Šä¸‹æ–‡çª—å£æ¥å®ç°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2410.14655v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2410.14655v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2410.14655v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FLARE-Faithful-Logic-Aided-Reasoning-and-Exploration"><a href="#FLARE-Faithful-Logic-Aided-Reasoning-and-Exploration" class="headerlink" title="FLARE: Faithful Logic-Aided Reasoning and Exploration"></a>FLARE: Faithful Logic-Aided Reasoning and Exploration</h2><p><strong>Authors:Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle Augenstein</strong></p>
<p>Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç°ä»£é—®ç­”ï¼ˆQAï¼‰å’Œæ¨ç†æ–¹æ³•é€šå¸¸ä½¿ç”¨æç¤ºæŠ€æœ¯ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œå‡è®¾ç”Ÿæˆçš„ç­”æ¡ˆå°†å¯¹é—®é¢˜ç©ºé—´å’ŒèŒƒå›´è¿›è¡Œæ›´è¯¦ç»†çš„æ¢ç´¢å’Œç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ç”Ÿæˆå¿ å®äºæ¨¡å‹äº§ç”Ÿçš„ä¸­é—´æ¨ç†é“¾çš„è¾“å‡ºæ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼Œç¥ç»ç¬¦å·æ–¹æ³•ï¼Œå¦‚å¿ å®æ€ç»´é“¾ï¼ˆF-CoTï¼‰ï¼Œæè®®å°†LLMä¸å¤–éƒ¨ç¬¦å·æ±‚è§£å™¨ç›¸ç»“åˆã€‚è™½ç„¶è¿™ç§æ–¹æ³•å…·æœ‰å¾ˆé«˜çš„å¿ å®åº¦ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦ç»è¿‡ä»£ç ç”Ÿæˆçš„æ¨¡å‹è®­ç»ƒï¼Œå¹¶ä¸”åœ¨å¤„ç†æ¨¡ç³Šæˆ–éš¾ä»¥ä¸¥æ ¼å½¢å¼åŒ–çš„ä»»åŠ¡æ—¶é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬å¼•å…¥äº†â€œå¿ å®é€»è¾‘è¾…åŠ©æ¨ç†ä¸æ¢ç´¢â€ï¼ˆFLAREï¼‰è¿™ä¸€å…¨æ–°çš„å¯è§£é‡Šæ–¹æ³•ï¼Œé€šè¿‡ä»»åŠ¡åˆ†è§£æ¥éå†é—®é¢˜ç©ºé—´ã€‚æˆ‘ä»¬ä½¿ç”¨LLMæ¥è§„åˆ’è§£å†³æ–¹æ¡ˆï¼Œå°†æŸ¥è¯¢è½¯å½¢å¼åŒ–ä¸ºäº‹å®å’Œè°“è¯ï¼Œå¹¶ä½¿ç”¨é€»è¾‘ç¼–ç¨‹ä»£ç æ¨¡æ‹Ÿä»£ç æ‰§è¡Œï¼Œåœ¨å®šä¹‰çš„ç©ºé—´ä¸Šè¿›è¡Œè¯¦å°½çš„å¤šè·³æœç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸æˆ‘ä»¬è®¡ç®—æ¨ç†è¿‡ç¨‹ç›¸å¯¹äºç”Ÿæˆä»£ç çš„å¿ å®åº¦ï¼Œå¹¶åˆ†æäº†å¤šè·³æœç´¢çš„æ­¥éª¤ï¼Œè€Œæ— éœ€ä¾èµ–å¤–éƒ¨æ±‚è§£å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¹ä¸ªä¸åŒæ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„ä¸ƒä¸ªä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚æˆ‘ä»¬è¿˜è¡¨æ˜æ¨¡å‹çš„å¿ å®åº¦ä¸æ•´ä½“æ€§èƒ½å‘ˆæ­£ç›¸å…³ï¼Œå¹¶è¿›ä¸€æ­¥è¯æ˜FLAREèƒ½å¤Ÿè¯†åˆ«å‡ºåœ¨å¤šè·³æœç´¢è¿‡ç¨‹ä¸­å¾—å‡ºæ­£ç¡®ç­”æ¡ˆçš„å†³å®šæ€§å…³é”®å› ç´ ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°æœ€ä¼˜é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11900v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç°ä»£é—®ç­”å’Œæ¨ç†æ–¹æ³•é€šå¸¸é‡‡ç”¨é“¾å¼æ€ç»´ç­‰æç¤ºæŠ€æœ¯ï¼Œå‡è®¾ç”Ÿæˆçš„ç»“æœä¼šåœ¨é—®é¢˜çš„ç©ºé—´å’ŒèŒƒå›´å†…è¿›è¡Œæ›´ç»†è‡´çš„æ¢ç´¢å’Œæ¨ç†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ç”Ÿæˆä¸æ¨¡å‹äº§ç”Ÿçš„ä¸­é—´æ¨ç†é“¾ç›¸ç¬¦çš„è¾“å‡ºæ–¹é¢å­˜åœ¨å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼Œç¥ç»ç¬¦å·æ–¹æ³•å¦‚å¿ å®é“¾å¼æ€ç»´ç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œå¤–éƒ¨ç¬¦å·æ±‚è§£å™¨ï¼Œè™½ç„¶å…·æœ‰å¾ˆé«˜çš„å¿ å®åº¦ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦é’ˆå¯¹ä»£ç ç”Ÿæˆè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨ä»»åŠ¡æ¨¡ç³Šæˆ–éš¾ä»¥ä¸¥æ ¼å½¢å¼åŒ–æ—¶è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¯è§£é‡Šæ–¹æ³•â€”â€”å¿ å®é€»è¾‘è¾…åŠ©æ¨ç†ä¸æ¢ç´¢ï¼ˆFLAREï¼‰ï¼Œé€šè¿‡ä»»åŠ¡åˆ†è§£æ¥éå†é—®é¢˜ç©ºé—´ã€‚æˆ‘ä»¬ä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥è§„åˆ’è§£å†³æ–¹æ¡ˆï¼Œå°†æŸ¥è¯¢è½¯å½¢å¼åŒ–ä¸ºäº‹å®å’Œè°“è¯ï¼Œå¹¶ä½¿ç”¨é€»è¾‘ç¼–ç¨‹ä»£ç æ¨¡æ‹Ÿä»£ç æ‰§è¡Œï¼Œåœ¨å®šä¹‰çš„ç©ºé—´ä¸Šè¿›è¡Œè¯¦å°½çš„å¤šè·³æœç´¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸æˆ‘ä»¬è®¡ç®—æ¨ç†è¿‡ç¨‹ç›¸å¯¹äºç”Ÿæˆä»£ç çš„å¿ å®æ€§ï¼Œå¹¶åˆ†æäº†å¤šè·³æœç´¢çš„æ­¥éª¤ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨æ±‚è§£å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¹ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„ä¸ƒä¸ªä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ¨¡å‹çš„å¿ å®æ€§ä¸æ€»ä½“æ€§èƒ½å‘ˆæ­£ç›¸å…³ï¼Œå¹¶è¿›ä¸€æ­¥è¯æ˜FLAREèƒ½å¤Ÿç¡®å®šå¤šè·³æœç´¢è¿‡ç¨‹ä¸­å¾—å‡ºæ­£ç¡®ç­”æ¡ˆçš„å…³é”®å› ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£é—®ç­”å’Œæ¨ç†æ–¹æ³•åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼Œå¸¸ç”¨é“¾å¼æ€ç»´ç­‰æç¤ºæŠ€æœ¯ï¼Œä½†åœ¨ç”Ÿæˆä¸ä¸­é—´æ¨ç†é“¾ç›¸ç¬¦çš„è¾“å‡ºæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç¥ç»ç¬¦å·æ–¹æ³•å¦‚å¿ å®é“¾å¼æ€ç»´ç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œå¤–éƒ¨ç¬¦å·æ±‚è§£å™¨ï¼Œè™½é«˜å¿ å®åº¦ä½†é¢ä¸´ç‰¹å®šæŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„å¯è§£é‡Šæ–¹æ³•â€”â€”å¿ å®é€»è¾‘è¾…åŠ©æ¨ç†ä¸æ¢ç´¢ï¼ˆFLAREï¼‰ï¼Œé€šè¿‡ä»»åŠ¡åˆ†è§£éå†é—®é¢˜ç©ºé—´ï¼Œç»“åˆè¯­è¨€æ¨¡å‹å’Œé€»è¾‘ç¼–ç¨‹ã€‚</li>
<li>FLAREå…è®¸è®¡ç®—æ¨ç†è¿‡ç¨‹çš„å¿ å®æ€§ï¼Œåˆ†æå¤šè·³æœç´¢æ­¥éª¤ï¼Œä¸ä¾èµ–å¤–éƒ¨æ±‚è§£å™¨ã€‚</li>
<li>FLAREåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚</li>
<li>æ¨¡å‹å¿ å®æ€§ä¸æ€»ä½“æ€§èƒ½æ­£ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2410.11900v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2410.11900v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2410.11900v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2410.11900v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2410.11900v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Does-GPT-Really-Get-It-A-Hierarchical-Scale-to-Quantify-Human-vs-AIâ€™s-Understanding-of-Algorithms"><a href="#Does-GPT-Really-Get-It-A-Hierarchical-Scale-to-Quantify-Human-vs-AIâ€™s-Understanding-of-Algorithms" class="headerlink" title="Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AIâ€™s   Understanding of Algorithms"></a>Does GPT Really Get It? A Hierarchical Scale to Quantify Human vs AIâ€™s   Understanding of Algorithms</h2><p><strong>Authors:Mirabel Reid, Santosh S. Vempala</strong></p>
<p>As Large Language Models (LLMs) perform (and sometimes excel at) more and more complex cognitive tasks, a natural question is whether AI really understands. The study of understanding in LLMs is in its infancy, and the community has yet to incorporate well-trodden research in philosophy, psychology, and education. We initiate this, specifically focusing on understanding algorithms, and propose a hierarchy of levels of understanding. We use the hierarchy to design and conduct a study with human subjects (undergraduate and graduate students) as well as large language models (generations of GPT), revealing interesting similarities and differences. We expect that our rigorous criteria will be useful to keep track of AIâ€™s progress in such cognitive domains. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¶Šæ¥è¶Šå¤šå¤æ‚çš„è®¤çŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜æ˜¯äººå·¥æ™ºèƒ½æ˜¯å¦çœŸçš„ç†è§£ã€‚å…³äºLLMçš„ç†è§£ç ”ç©¶è¿˜å¤„äºèµ·æ­¥é˜¶æ®µï¼Œå°šæœªå……åˆ†èå…¥å“²å­¦ã€å¿ƒç†å­¦å’Œæ•™è‚²é¢†åŸŸçš„æˆç†Ÿç ”ç©¶ã€‚æˆ‘ä»¬ä»¥æ­¤ä¸ºé‡ç‚¹ï¼Œç‰¹åˆ«æ˜¯ç†è§£ç®—æ³•ï¼Œå¹¶æå‡ºäº†ç†è§£å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬ä½¿ç”¨å±‚æ¬¡ç»“æ„è®¾è®¡å’Œå¼€å±•äº†ä¸€é¡¹ç ”ç©¶ï¼Œç ”ç©¶å¯¹è±¡åŒ…æ‹¬æœ¬ç§‘ç”Ÿå’Œç ”ç©¶ç”Ÿä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPTå„ä»£ï¼‰ï¼Œæ­ç¤ºäº†æœ‰è¶£çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ã€‚æˆ‘ä»¬é¢„è®¡æˆ‘ä»¬ä¸¥æ ¼çš„è¯„ä»·æ ‡å‡†å°†ç”¨äºè·Ÿè¸ªAIåœ¨è®¤çŸ¥é¢†åŸŸçš„è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14722v3">PDF</a> 13 pages, 10 figures. To be published at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¶Šæ¥è¶Šå¤šå¤æ‚è®¤çŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ˆæœ‰æ—¶ç”šè‡³è¡¨ç°å‡ºè‰²ï¼‰ï¼Œå…³äºäººå·¥æ™ºèƒ½æ˜¯å¦çœŸçš„ç†è§£çš„é—®é¢˜è‡ªç„¶äº§ç”Ÿã€‚å¯¹äºLLMçš„ç†è§£ç ”ç©¶å°šå¤„äºèµ·æ­¥é˜¶æ®µï¼Œå°šæœªå……åˆ†èå…¥å“²å­¦ã€å¿ƒç†å­¦å’Œæ•™è‚²ç­‰é¢†åŸŸçš„æˆç†Ÿç ”ç©¶ã€‚æœ¬ç ”ç©¶ä¸“æ³¨äºç†è§£ç®—æ³•ï¼Œå¹¶æå‡ºäº†ç†è§£å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€å±‚æ¬¡ç»“æ„ï¼Œè®¾è®¡å¹¶å¼€å±•äº†ä¸€é¡¹æ¶‰åŠäººç±»å—è¯•è€…ï¼ˆæœ¬ç§‘ç”Ÿå’Œç ”ç©¶ç”Ÿï¼‰ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå„ä»£GPTï¼‰çš„ç ”ç©¶ï¼Œæ­ç¤ºäº†æœ‰è¶£çš„ä¸€è‡´æ€§å’Œå·®å¼‚æ€§ã€‚æˆ‘ä»¬é¢„è®¡ï¼Œæˆ‘ä»¬ä¸¥æ ¼çš„è¯„ä¼°æ ‡å‡†å°†æœ‰åŠ©äºè·Ÿè¸ªAIåœ¨è®¤çŸ¥é¢†åŸŸçš„è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚è®¤çŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°æ—¥ç›Šçªå‡ºï¼Œå¼•å‘å…³äºAIæ˜¯å¦çœŸæ­£ç†è§£çš„ç–‘é—®ã€‚</li>
<li>LLMçš„ç†è§£ç ”ç©¶å°šå¤„äºèµ·æ­¥é˜¶æ®µï¼Œéœ€å€Ÿé‰´å“²å­¦ã€å¿ƒç†å­¦å’Œæ•™è‚²çš„æˆç†Ÿç ”ç©¶ã€‚</li>
<li>ç ”ç©¶èšç„¦äºç†è§£ç®—æ³•ï¼Œå¹¶æå‡ºç†è§£å±‚æ¬¡ç»“æ„ã€‚</li>
<li>é€šè¿‡äººç±»å—è¯•è€…ï¼ˆæœ¬ç§‘ç”Ÿå’Œç ”ç©¶ç”Ÿï¼‰åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå„ä»£GPTï¼‰çš„ç ”ç©¶ï¼Œå‘ç°æœ‰è¶£çš„ä¸€è‡´æ€§å’Œå·®å¼‚æ€§ã€‚</li>
<li>æå‡ºçš„ä¸¥æ ¼è¯„ä¼°æ ‡å‡†æœ‰åŠ©äºè·Ÿè¸ªAIåœ¨è®¤çŸ¥é¢†åŸŸçš„è¿›å±•ã€‚</li>
<li>é€šè¿‡æ­¤ç ”ç©¶ï¼Œå¼ºè°ƒäº†å°†ä¸åŒé¢†åŸŸç ”ç©¶æˆæœç»“åˆä»¥æ¨åŠ¨AIç†è§£å‘å±•çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2406.14722v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2406.14722v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2406.14722v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2406.14722v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0f77fec60f654cdb6c143df0db2e8469.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  UI-TARS Pioneering Automated GUI Interaction with Native Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-21/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ed5f4caaf385c320e5ac3085f7eb7f49.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-21  TalkingEyes Pluralistic Speech-Driven 3D Eye Gaze Animation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16470.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
