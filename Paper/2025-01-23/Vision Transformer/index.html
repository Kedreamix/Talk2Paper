<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  Vision-Language Models for Automated Chest X-ray Interpretation   Leveraging ViT and GPT-2">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4cc498aef04ec38629810f038a08e728.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    41 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-23-æ›´æ–°"><a href="#2025-01-23-æ›´æ–°" class="headerlink" title="2025-01-23 æ›´æ–°"></a>2025-01-23 æ›´æ–°</h1><h2 id="Vision-Language-Models-for-Automated-Chest-X-ray-Interpretation-Leveraging-ViT-and-GPT-2"><a href="#Vision-Language-Models-for-Automated-Chest-X-ray-Interpretation-Leveraging-ViT-and-GPT-2" class="headerlink" title="Vision-Language Models for Automated Chest X-ray Interpretation:   Leveraging ViT and GPT-2"></a>Vision-Language Models for Automated Chest X-ray Interpretation:   Leveraging ViT and GPT-2</h2><p><strong>Authors:Md. Rakibul Islam, Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu</strong></p>
<p>Radiology plays a pivotal role in modern medicine due to its non-invasive diagnostic capabilities. However, the manual generation of unstructured medical reports is time consuming and prone to errors. It creates a significant bottleneck in clinical workflows. Despite advancements in AI-generated radiology reports, challenges remain in achieving detailed and accurate report generation. In this study we have evaluated different combinations of multimodal models that integrate Computer Vision and Natural Language Processing to generate comprehensive radiology reports. We employed a pretrained Vision Transformer (ViT-B16) and a SWIN Transformer as the image encoders. The BART and GPT-2 models serve as the textual decoders. We used Chest X-ray images and reports from the IU-Xray dataset to evaluate the usability of the SWIN Transformer-BART, SWIN Transformer-GPT-2, ViT-B16-BART and ViT-B16-GPT-2 models for report generation. We aimed at finding the best combination among the models. The SWIN-BART model performs as the best-performing model among the four models achieving remarkable results in almost all the evaluation metrics like ROUGE, BLEU and BERTScore. </p>
<blockquote>
<p>æ”¾å°„å­¦åœ¨ç°ä»£åŒ»å­¦ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå› å…¶å…·æœ‰éä¾µå…¥æ€§çš„è¯Šæ–­èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨ç”Ÿæˆéç»“æ„åŒ–åŒ»ç–—æŠ¥å‘Šæ—¢è€—æ—¶åˆå®¹æ˜“å‡ºé”™ï¼Œè¿™æˆä¸ºä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„é‡å¤§ç“¶é¢ˆã€‚å°½ç®¡äººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šæœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨å®ç°è¯¦ç»†å‡†ç¡®çš„æŠ¥å‘Šç”Ÿæˆæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç»“åˆè®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„å¤šæ¨¡å¼æ¨¡å‹çš„ä¸åŒç»„åˆï¼Œä»¥ç”Ÿæˆå…¨é¢çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚æˆ‘ä»¬é‡‡ç”¨äº†é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViT-B16ï¼‰å’ŒSWIN Transformerä½œä¸ºå›¾åƒç¼–ç å™¨ã€‚BARTå’ŒGPT-2æ¨¡å‹åˆ™ä½œä¸ºæ–‡æœ¬è§£ç å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨IU-Xrayæ•°æ®é›†ä¸­çš„èƒ¸éƒ¨Xå°„çº¿å›¾åƒå’ŒæŠ¥å‘Šæ¥è¯„ä¼°SWIN Transformer-BARTã€SWIN Transformer-GPT-2ã€ViT-B16-BARTå’ŒViT-B16-GPT-2æ¨¡å‹åœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°æ¨¡å‹ä¹‹é—´çš„æœ€ä½³ç»„åˆã€‚åœ¨å››é¡¹æ¨¡å‹ä¸­ï¼ŒSWIN-BARTæ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œåœ¨å‡ ä¹æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚ROUGEã€BLEUå’ŒBERTScoreï¼‰ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12356v1">PDF</a> Preprint, manuscript under-review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ”¾å°„å­¦åœ¨ç°ä»£åŒ»å­¦ä¸­çš„é‡è¦ä½œç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶ä¸­ï¼Œé€šè¿‡ç»“åˆè®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œè¯„ä¼°äº†ä¸åŒç»„åˆæ¨¡å‹åœ¨ç”Ÿæˆç»¼åˆæ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ã€‚é‡‡ç”¨é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViT-B16ï¼‰å’ŒSWIN Transformerä½œä¸ºå›¾åƒç¼–ç å™¨ï¼ŒBARTå’ŒGPT-2æ¨¡å‹ä½œä¸ºæ–‡æœ¬è§£ç å™¨ã€‚ä½¿ç”¨IU-Xrayæ•°æ®é›†ä¸Šçš„Chest X-rayå›¾åƒå’ŒæŠ¥å‘Šè¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒSWIN-BARTæ¨¡å‹åœ¨å‡ ä¹æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¦‚ROUGEã€BLEUå’ŒBERTScoreã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„å­¦åœ¨ç°ä»£åŒ»å­¦ä¸­å…·æœ‰éä¾µå…¥æ€§è¯Šæ–­èƒ½åŠ›çš„é‡è¦è§’è‰²ã€‚</li>
<li>è‡ªåŠ¨åŒ–ç”ŸæˆåŒ»ç–—æŠ¥å‘Šé¢ä¸´æ—¶é—´æ¶ˆè€—å’Œè¯¯å·®ç‡é—®é¢˜ï¼Œæˆä¸ºä¸´åºŠå·¥ä½œæµç¨‹çš„ç“¶é¢ˆã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ç»“åˆè®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆç»¼åˆæ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨äº†Vision Transformerï¼ˆViT-B16ï¼‰å’ŒSWIN Transformerä½œä¸ºå›¾åƒç¼–ç å™¨ã€‚</li>
<li>BARTå’ŒGPT-2æ¨¡å‹è¢«ç”¨ä½œæ–‡æœ¬è§£ç å™¨ã€‚</li>
<li>ä½¿ç”¨IU-Xrayæ•°æ®é›†ä¸­çš„Chest X-rayå›¾åƒå’ŒæŠ¥å‘Šè¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9f8df32d8e52eaed8bdb31c790e1b843.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccdb0faaab13d2d26d4c143ce66449a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification"><a href="#CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification" class="headerlink" title="CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification"></a>CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification</h2><p><strong>Authors:Cristiano PatrÃ­cio, Isabel Rio-Torto, Jaime S. Cardoso, LuÃ­s F. Teixeira, JoÃ£o C. Neves</strong></p>
<p>The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the final disease prediction on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: <a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/">https://cristianopatricio.github.io/CBVLM/</a>. </p>
<blockquote>
<p>åœ¨åŒ»ç–—å·¥ä½œæµä¸­é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆçš„ä¸»è¦æŒ‘æˆ˜åœ¨äºæ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œæ­¤ç±»ç³»ç»Ÿç¼ºä¹å¯è§£é‡Šæ€§ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡çº¦æŸæœ€ç»ˆç–¾ç—…é¢„æµ‹åœ¨ä¸€ç»„é¢„å…ˆå®šä¹‰å’Œå¯è§£é‡Šçš„æ¦‚å¿µä¸Šæ¥è§£å†³åè€…çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œé€šè¿‡åŸºäºæ¦‚å¿µçš„è§£é‡Šå®ç°çš„å¢åŠ çš„å¯è§£é‡Šæ€§æ„å‘³ç€æ›´é«˜çš„æ ‡æ³¨è´Ÿæ‹…ã€‚è€Œä¸”ï¼Œå¦‚æœè¦æ·»åŠ æ–°æ¦‚å¿µï¼Œæ•´ä¸ªç³»ç»Ÿéœ€è¦é‡æ–°è®­ç»ƒã€‚å—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸­çš„å‡ºè‰²æ€§èƒ½çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå³CBVLMï¼Œè¯¥æ–¹æ³•è§£å†³äº†ä¸Šè¿°ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå¯¹äºæ¯ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬æç¤ºLVLMå›ç­”æ¦‚å¿µæ˜¯å¦å‡ºç°åœ¨è¾“å…¥å›¾åƒä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬è®©LVLMåŸºäºå…ˆå‰çš„æ¦‚å¿µé¢„æµ‹å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚è€Œä¸”ï¼Œåœ¨ä¸¤ä¸ªé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨æ£€ç´¢æ¨¡å—è´Ÿè´£é€‰æ‹©æœ€ä½³æ ·æœ¬è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚é€šè¿‡å°†æœ€ç»ˆè¯Šæ–­å»ºç«‹åœ¨é¢„æµ‹çš„æ¦‚å¿µä¸Šï¼Œæˆ‘ä»¬ç¡®ä¿äº†å¯è§£é‡Šæ€§ï¼Œå¹¶é€šè¿‡åˆ©ç”¨LVLMsçš„å°‘é‡æ ·æœ¬èƒ½åŠ›ï¼Œæˆ‘ä»¬å¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚æˆ‘ä»¬é€šè¿‡å››ä¸ªåŒ»ç–—æ•°æ®é›†å’ŒåäºŒä¸ªï¼ˆé€šç”¨å’ŒåŒ»ç–—ï¼‰LVLMsçš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¡¨æ˜CBVLMåœ¨ä¸éœ€è¦ä»»ä½•è®­ç»ƒå’Œä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºCBMså’Œç‰¹å®šä»»åŠ¡ç›‘ç£æ–¹æ³•ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/%E3%80%82">https://cristianopatricio.github.io/CBVLM/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12266v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»ç–—å·¥ä½œæµä¸­é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆæ—¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ï¼Œå³æ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§çš„ç¼ºä¹ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡åœ¨ä¸€ç»„é¢„å®šä¹‰å’Œå¯è§£é‡Šçš„æ¦‚å¿µä¸Šçº¦æŸç–¾ç—…é¢„æµ‹æ¥è§£å†³åè€…çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œé€šè¿‡æ¦‚å¿µè§£é‡Šæé«˜çš„å¯è§£é‡Šæ€§æ„å‘³ç€æ›´é«˜çš„æ ‡æ³¨è´Ÿæ‹…ï¼Œå¹¶ä¸”å¦‚æœéœ€è¦æ·»åŠ æ–°æ¦‚å¿µï¼Œåˆ™éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªç³»ç»Ÿã€‚å—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­çš„å‡ºè‰²æ€§èƒ½çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•â€”â€”CBVLMï¼Œè¯¥æ–¹æ³•è§£å†³äº†ä¸Šè¿°ä¸¤ä¸ªæŒ‘æˆ˜ã€‚CBVLMé¦–å…ˆé€šè¿‡LVLMåˆ¤æ–­è¾“å…¥å›¾åƒä¸­æ˜¯å¦å­˜åœ¨æ¯ä¸ªæ¦‚å¿µï¼Œç„¶åæ ¹æ®ä¹‹å‰çš„æ¦‚å¿µé¢„æµ‹å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚æ­¤å¤–ï¼Œä¸¤ä¸ªé˜¶æ®µéƒ½å¼•å…¥äº†æ£€ç´¢æ¨¡å—ï¼Œè´Ÿè´£é€‰æ‹©æœ€ä½³çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç¤ºä¾‹ã€‚é€šè¿‡å°†æœ€ç»ˆè¯Šæ–­ç»“æœåŸºäºé¢„æµ‹çš„æ¦‚å¿µæ¥ç¡®ä¿è§£é‡Šæ€§ï¼Œå¹¶åˆ©ç”¨LVLMsçš„å°‘é‡æ ·æœ¬èƒ½åŠ›æ¥å¤§å¹…é™ä½æ ‡æ³¨æˆæœ¬ã€‚ç»è¿‡åœ¨å››ä¸ªåŒ»ç–—æ•°æ®é›†å’ŒåäºŒä¸ªï¼ˆé€šç”¨å’ŒåŒ»ç–—ï¼‰LVLMsä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜CBVLMæŒç»­ä¼˜äºCBMså’Œä»»åŠ¡ç‰¹å®šçš„ç›‘ç£æ–¹æ³•ï¼Œè€Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒï¼Œåªéœ€ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŒ»ç–—å·¥ä½œæµä¸­çš„é‡‡çº³é¢ä¸´æ ‡æ³¨æ•°æ®å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡çº¦æŸç–¾ç—…é¢„æµ‹åœ¨é¢„å®šä¹‰æ¦‚å¿µä¸Šè§£å†³è§£é‡Šæ€§é—®é¢˜ï¼Œä½†å¢åŠ äº†æ ‡æ³¨è´Ÿæ‹…å’Œé‡æ–°è®­ç»ƒéœ€æ±‚ã€‚</li>
<li>CBVLMæ–¹æ³•ç»“åˆå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>CBVLMåˆ©ç”¨LVLMsè¿›è¡Œæ¦‚å¿µé¢„æµ‹å’Œå›¾åƒåˆ†ç±»ï¼Œç¡®ä¿è§£é‡Šæ€§å¹¶é™ä½æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>CBVLMåœ¨å››ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒæŒç»­ä¼˜äºCBMså’Œä»»åŠ¡ç‰¹å®šç›‘ç£æ–¹æ³•ã€‚</li>
<li>CBVLMæ–¹æ³•ä¸éœ€è¦ä»»ä½•è®­ç»ƒï¼Œä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å³å¯å®ç°é«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-206021a739805f4c025fbbbda977fff6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0afff72f19698ec85886bc603208c360.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0abc5ca96e375c7cac4c0ac188efd080.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0aea3215822b78c86eba3f65abedf78.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Pre-trained-Deep-Learning-Models-and-DINOv2-for-Cushingâ€™s-Syndrome-Diagnosis-in-Facial-Analysis"><a href="#Comparative-Analysis-of-Pre-trained-Deep-Learning-Models-and-DINOv2-for-Cushingâ€™s-Syndrome-Diagnosis-in-Facial-Analysis" class="headerlink" title="Comparative Analysis of Pre-trained Deep Learning Models and DINOv2 for   Cushingâ€™s Syndrome Diagnosis in Facial Analysis"></a>Comparative Analysis of Pre-trained Deep Learning Models and DINOv2 for   Cushingâ€™s Syndrome Diagnosis in Facial Analysis</h2><p><strong>Authors:Hongjun Liu, Changwei Song, Jiaqi Qiang, Jianqiang Li, Hui Pan, Lin Lu, Xiao Long, Qing Zhao, Jiuzuo Huang, Shi Chen</strong></p>
<p>Cushingâ€™s syndrome is a condition caused by excessive glucocorticoid secretion from the adrenal cortex, often manifesting with moon facies and plethora, making facial data crucial for diagnosis. Previous studies have used pre-trained convolutional neural networks (CNNs) for diagnosing Cushingâ€™s syndrome using frontal facial images. However, CNNs are better at capturing local features, while Cushingâ€™s syndrome often presents with global facial features. Transformer-based models like ViT and SWIN, which utilize self-attention mechanisms, can better capture long-range dependencies and global features. Recently, DINOv2, a foundation model based on visual Transformers, has gained interest. This study compares the performance of various pre-trained models, including CNNs, Transformer-based models, and DINOv2, in diagnosing Cushingâ€™s syndrome. We also analyze gender bias and the impact of freezing mechanisms on DINOv2. Our results show that Transformer-based models and DINOv2 outperformed CNNs, with ViT achieving the highest F1 score of 85.74%. Both the pre-trained model and DINOv2 had higher accuracy for female samples. DINOv2 also showed improved performance when freezing parameters. In conclusion, Transformer-based models and DINOv2 are effective for Cushingâ€™s syndrome classification. </p>
<blockquote>
<p>åº“æ¬£ç»¼åˆå¾æ˜¯ç”±è‚¾ä¸Šè…ºçš®è´¨è¿‡åº¦åˆ†æ³Œç³–çš®è´¨æ¿€ç´ å¼•èµ·çš„ç–¾ç—…ï¼Œé€šå¸¸è¡¨ç°ä¸ºæ»¡æœˆè„¸å’Œæ°´ç‰›èƒŒï¼Œä½¿å¾—é¢éƒ¨æ•°æ®å¯¹è¯Šæ–­è‡³å…³é‡è¦ã€‚ä»¥å¾€çš„ç ”ç©¶ä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰é€šè¿‡æ­£é¢é¢éƒ¨å›¾åƒè¯Šæ–­åº“æ¬£ç»¼åˆå¾ã€‚ç„¶è€Œï¼ŒCNNæ›´æ“…é•¿æ•æ‰å±€éƒ¨ç‰¹å¾ï¼Œè€Œåº“æ¬£ç»¼åˆå¾é€šå¸¸è¡¨ç°ä¸ºé¢éƒ¨æ•´ä½“ç‰¹å¾ã€‚åŸºäºTransformerçš„æ¨¡å‹ï¼Œå¦‚ViTå’ŒSWINï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥æ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œå…¨å±€ç‰¹å¾ã€‚æœ€è¿‘ï¼ŒåŸºäºè§†è§‰Transformerçš„DINOv2è¿™ä¸€åŸºç¡€æ¨¡å‹å¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚æœ¬ç ”ç©¶æ¯”è¾ƒäº†åŒ…æ‹¬CNNã€åŸºäºTransformerçš„æ¨¡å‹å’ŒDINOv2åœ¨å†…çš„å„ç§é¢„è®­ç»ƒæ¨¡å‹åœ¨è¯Šæ–­åº“æ¬£ç»¼åˆå¾æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†æ€§åˆ«åè§ä»¥åŠå†»ç»“æœºåˆ¶å¯¹DINOv2çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºTransformerçš„æ¨¡å‹å’ŒDINOv2åœ¨åº“æ¬£ç»¼åˆå¾è¯Šæ–­ä¸Šçš„æ€§èƒ½ä¼˜äºCNNï¼Œå…¶ä¸­ViTçš„F1å¾—åˆ†æœ€é«˜ï¼Œè¾¾åˆ°85.74%ã€‚æ— è®ºæ˜¯é¢„è®­ç»ƒæ¨¡å‹è¿˜æ˜¯DINOv2ï¼Œå¯¹å¥³æ€§æ ·æœ¬çš„å‡†ç¡®ç‡éƒ½è¾ƒé«˜ã€‚å†»ç»“å‚æ•°åï¼ŒDINOv2çš„æ€§èƒ½ä¹Ÿæœ‰æ‰€æé«˜ã€‚ç»¼ä¸Šæ‰€è¿°ï¼ŒåŸºäºTransformerçš„æ¨¡å‹å’ŒDINOv2åœ¨åº“æ¬£ç»¼åˆå¾åˆ†ç±»ä¸­æ•ˆæœæ˜¾è‘—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12023v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨ä¸åŒé¢„è®­ç»ƒæ¨¡å‹è¯Šæ–­åº“æ¬£ç»¼åˆå¾çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€åŸºäºTransformerçš„æ¨¡å‹å’ŒDINOv2æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºTransformerçš„æ¨¡å‹å’ŒDINOv2åœ¨è¯Šæ–­åº“æ¬£ç»¼åˆå¾æ–¹é¢çš„æ€§èƒ½ä¼˜äºCNNsï¼Œå…¶ä¸­ViTè·å¾—äº†æœ€é«˜çš„F1åˆ†æ•°ã€‚åŒæ—¶ï¼Œç ”ç©¶å‘ç°å¥³æ€§æ ·æœ¬çš„å‡†ç¡®ç‡æ›´é«˜ï¼Œå†»ç»“æœºåˆ¶å¯¹DINOv2çš„æ€§èƒ½ä¹Ÿæœ‰ç§¯æå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åº“æ¬£ç»¼åˆå¾æ˜¯ç”±è‚¾ä¸Šè…ºçš®è´¨è¿‡åº¦åˆ†æ³Œç³–çš®è´¨æ¿€ç´ å¼•èµ·çš„ï¼Œé¢éƒ¨æ•°æ®å¯¹å…¶è¯Šæ–­è‡³å…³é‡è¦ã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶ä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰è¿›è¡Œåº“æ¬£ç»¼åˆå¾çš„è¯Šæ–­ï¼Œä½†CNNsæ›´æ“…é•¿æ•æ‰å±€éƒ¨ç‰¹å¾ï¼Œè€Œåº“æ¬£ç»¼åˆå¾é€šå¸¸è¡¨ç°ä¸ºå…¨å±€é¢éƒ¨ç‰¹å¾ã€‚</li>
<li>åŸºäºTransformerçš„æ¨¡å‹ï¼ˆå¦‚ViTå’ŒSWINï¼‰ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½æ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>DINOv2æ¨¡å‹åœ¨è¯Šæ–­åº“æ¬£ç»¼åˆå¾æ–¹é¢çš„æ€§èƒ½å¾—åˆ°äº†ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†åŒ…æ‹¬CNNsã€åŸºäºTransformerçš„æ¨¡å‹å’ŒDINOv2åœ¨å†…çš„ä¸åŒé¢„è®­ç»ƒæ¨¡å‹åœ¨è¯Šæ–­åº“æ¬£ç»¼åˆå¾æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºTransformerçš„æ¨¡å‹å’ŒDINOv2åœ¨è¯Šæ–­åº“æ¬£ç»¼åˆå¾æ–¹é¢ä¼˜äºCNNsï¼ŒViTè·å¾—æœ€é«˜F1åˆ†æ•°85.74%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-751e018f687da6e99ecaab58d9da0bec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c482f15366faa437a99efabf1361c8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d183cb374151a56556ea7508e8858e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420556e05427d348fffc3d92b25f4e73.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-generalizable-3D-framework-and-model-for-self-supervised-learning-in-medical-imaging"><a href="#A-generalizable-3D-framework-and-model-for-self-supervised-learning-in-medical-imaging" class="headerlink" title="A generalizable 3D framework and model for self-supervised learning in   medical imaging"></a>A generalizable 3D framework and model for self-supervised learning in   medical imaging</h2><p><strong>Authors:Tony Xu, Sepehr Hosseini, Chris Anderson, Anthony Rinaldi, Rahul G. Krishnan, Anne L. Martel, Maged Goubran</strong></p>
<p>Current self-supervised learning methods for 3D medical imaging rely on simple pretext formulations and organ- or modality-specific datasets, limiting their generalizability and scalability. We present 3DINO, a cutting-edge SSL method adapted to 3D datasets, and use it to pretrain 3DINO-ViT: a general-purpose medical imaging model, on an exceptionally large, multimodal, and multi-organ dataset of ~100,000 3D medical imaging scans from over 10 organs. We validate 3DINO-ViT using extensive experiments on numerous medical imaging segmentation and classification tasks. Our results demonstrate that 3DINO-ViT generalizes across modalities and organs, including out-of-distribution tasks and datasets, outperforming state-of-the-art methods on the majority of evaluation metrics and labeled dataset sizes. Our 3DINO framework and 3DINO-ViT will be made available to enable research on 3D foundation models or further finetuning for a wide range of medical imaging applications. </p>
<blockquote>
<p>å½“å‰ç”¨äºä¸‰ç»´åŒ»å­¦å½±åƒçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ä¾èµ–äºç®€å•çš„é¢„æ–‡æœ¬åˆ¶å®šå’Œç‰¹å®šå™¨å®˜æˆ–ç‰¹å®šæ¨¡æ€çš„æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†é€‚ç”¨äºä¸‰ç»´æ•°æ®é›†çš„å°–ç«¯SSLæ–¹æ³•3DINOï¼Œå¹¶ä½¿ç”¨å®ƒæ¥é¢„è®­ç»ƒé¢å‘åŒ»å­¦å½±åƒçš„ä¸‰ç»´é€šç”¨æ¨¡å‹3DINO-ViTã€‚è¯¥æ¨¡å‹åœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡åä¸‡ä¸ªæ¥è‡ªè¶…è¿‡åä¸ªå™¨å®˜çš„ä¸‰ç»´åŒ»å­¦å½±åƒæ‰«æçš„å¤§å‹å¤šæ¨¡æ€å¤šå™¨å®˜æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡åŒ»å­¦å½±åƒåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡çš„å®éªŒéªŒè¯äº†3DINO-ViTçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨è·¨æ¨¡æ€å’Œå™¨å®˜çš„æƒ…å†µä¸‹ï¼Œè¿˜æ˜¯åœ¨ç¦»ç¾¤ä»»åŠ¡å’Œæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹éƒ½è¡¨ç°å‡ºå¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤§å¤šæ•°è¯„ä¼°æŒ‡æ ‡å’Œæ ‡æ³¨æ•°æ®é›†å¤§å°ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„3DINOæ¡†æ¶å’Œ3DINO-ViTæ¨¡å‹å°†å¼€æ”¾ç»™å…¬ä¼—ä½¿ç”¨ï¼Œä»¥ä¾›è¿›è¡Œå…³äºä¸‰ç»´åŸºç¡€æ¨¡å‹çš„ç ”ç©¶ï¼Œæˆ–å¯¹ä¸€ç³»åˆ—åŒ»å­¦å½±åƒåº”ç”¨è¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11755v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹3Dæ•°æ®é›†çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•3DINOï¼Œå¹¶åˆ©ç”¨å…¶é¢„è®­ç»ƒäº†ä¸€ä¸ªé€šç”¨çš„åŒ»å­¦å½±åƒæ¨¡å‹3DINO-ViTã€‚è¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€ã€å¤šå™¨å®˜çš„å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥åœ¨ä¼—å¤šåŒ»å­¦å½±åƒåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡ŒéªŒè¯ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¨¡å‹åœ¨è·¨æ¨¡æ€å’Œå™¨å®˜ã€åŒ…æ‹¬è¶…å‡ºåˆ†å¸ƒçš„ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šæ•°è¯„ä¼°æŒ‡æ ‡å’Œæ ‡æ³¨æ•°æ®é›†å¤§å°ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚æœ¬æ–‡æä¾›çš„3DINOæ¡†æ¶å’Œ3DINO-ViTå°†ä¿ƒè¿›å¯¹3DåŸºç¡€æ¨¡å‹çš„ç ”ç©¶åŠå…¶åœ¨åŒ»å­¦å½±åƒåº”ç”¨ä¸­çš„è¿›ä¸€æ­¥å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨3DåŒ»å­¦å½±åƒå¤„ç†ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦ä¸€ç§é€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸º3DINOçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºå¤„ç†å¤§è§„æ¨¡çš„ä¸‰ç»´æ•°æ®é›†ã€‚æ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºåŒ»å­¦å½±åƒæ•°æ®ã€‚</li>
<li>åˆ©ç”¨3DINOæ–¹æ³•é¢„è®­ç»ƒäº†ä¸€ä¸ªåŒ»å­¦å½±åƒæ¨¡å‹â€”â€”3DINO-ViTï¼Œæ—¨åœ¨å®ç°å¤šæ¨¡æ€å’Œå¤šå™¨å®˜çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹è®­ç»ƒæ•°æ®é›†è§„æ¨¡åºå¤§ï¼ŒåŒ…æ‹¬è¶…è¿‡ç™¾ä¸‡ä¸ªåŒ»å­¦æ‰«ææ ·æœ¬ã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—åŒ»å­¦å½±åƒåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡éªŒè¯äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¯æ˜äº†è¯¥æ¨¡å‹åœ¨è·¨æ¨¡æ€å’Œå™¨å®˜ã€ä¸åŒæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¼˜äºå½“å‰å…ˆè¿›æŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºå‡ºè¯¥æ¨¡å‹åœ¨ä¸åŒè¯„ä¼°æŒ‡æ ‡ä¸Šçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ ‡æ³¨æ•°æ®é›†å¤§å°ä¸Šè¡¨ç°ä¼˜è¶Šã€‚è¿™è¡¨æ˜å³ä½¿åœ¨æœ‰é™çš„æ•°æ®æ¡ä»¶ä¸‹ï¼Œè¯¥æ¨¡å‹ä¹Ÿèƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›çš„æ¨¡å‹æ¡†æ¶å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå³3DINO-ViTï¼‰å°†ä¿ƒè¿›åç»­ç ”ç©¶ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæä¾›å¼ºå¤§çš„å·¥å…·ï¼ŒåŒ…æ‹¬ç”¨äºæ„å»ºæ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹æˆ–é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒç­‰åº”ç”¨ã€‚è¿™è¡¨æ˜è¯¥ç ”ç©¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œæ½œåœ¨ä»·å€¼ã€‚è¯¥ç ”ç©¶ä¸ºåŒ»å­¦å½±åƒåˆ†æé¢†åŸŸå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼ŒåŒ…æ‹¬æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€æ¨åŠ¨ä¸ªæ€§åŒ–åŒ»ç–—ç­‰æ–¹é¢çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-420e208eac7c2820ad8f56b7fd587cb6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MedFILIP-Medical-Fine-grained-Language-Image-Pre-training"><a href="#MedFILIP-Medical-Fine-grained-Language-Image-Pre-training" class="headerlink" title="MedFILIP: Medical Fine-grained Language-Image Pre-training"></a>MedFILIP: Medical Fine-grained Language-Image Pre-training</h2><p><strong>Authors:Xinjie Liang, Xiangyu Li, Fanding Li, Jie Jiang, Qing Dong, Wei Wang, Kuanquan Wang, Suyu Dong, Gongning Luo, Shuo Li</strong></p>
<p>Medical vision-language pretraining (VLP) that leverages naturally-paired medical image-report data is crucial for medical image analysis. However, existing methods struggle to accurately characterize associations between images and diseases, leading to inaccurate or incomplete diagnostic results. In this work, we propose MedFILIP, a fine-grained VLP model, introduces medical image-specific knowledge through contrastive learning, specifically: 1) An information extractor based on a large language model is proposed to decouple comprehensive disease details from reports, which excels in extracting disease deals through flexible prompt engineering, thereby effectively reducing text complexity while retaining rich information at a tiny cost. 2) A knowledge injector is proposed to construct relationships between categories and visual attributes, which help the model to make judgments based on image features, and fosters knowledge extrapolation to unfamiliar disease categories. 3) A semantic similarity matrix based on fine-grained annotations is proposed, providing smoother, information-richer labels, thus allowing fine-grained image-text alignment. 4) We validate MedFILIP on numerous datasets, e.g., RSNA-Pneumonia, NIH ChestX-ray14, VinBigData, and COVID-19. For single-label, multi-label, and fine-grained classification, our model achieves state-of-the-art performance, the classification accuracy has increased by a maximum of 6.69%. The code is available in <a target="_blank" rel="noopener" href="https://github.com/PerceptionComputingLab/MedFILIP">https://github.com/PerceptionComputingLab/MedFILIP</a>. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åˆ©ç”¨è‡ªç„¶é…å¯¹çš„åŒ»ç–—å›¾åƒæŠ¥å‘Šæ•°æ®å¯¹åŒ»ç–—å›¾åƒåˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾ˆéš¾å‡†ç¡®åˆ»ç”»å›¾åƒä¸ç–¾ç—…ä¹‹é—´çš„å…³è”ï¼Œå¯¼è‡´è¯Šæ–­ç»“æœä¸å‡†ç¡®æˆ–ä¸å®Œæ•´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MedFILIPï¼Œè¿™æ˜¯ä¸€ç§ç²¾ç»†çš„VLPæ¨¡å‹ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¼•å…¥åŒ»ç–—å›¾åƒç‰¹å®šçŸ¥è¯†ï¼Œå…·ä½“åŒ…æ‹¬ï¼š1ï¼‰æå‡ºä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯æå–å™¨ï¼Œä»æŠ¥å‘Šä¸­è§£è€¦å‡ºç»¼åˆç–¾ç—…ç»†èŠ‚ï¼Œè¯¥æå–å™¨æ“…é•¿é€šè¿‡çµæ´»çš„æç¤ºå·¥ç¨‹æå–ç–¾ç—…ä¿¡æ¯ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘æ–‡æœ¬å¤æ‚æ€§ï¼ŒåŒæ—¶ä»¥å¾®å°æˆæœ¬ä¿ç•™ä¸°å¯Œä¿¡æ¯ã€‚2ï¼‰æå‡ºäº†ä¸€ç§æ„å»ºç±»åˆ«ä¸è§†è§‰å±æ€§ä¹‹é—´å…³ç³»çš„çŸ¥è¯†æ³¨å…¥å™¨ï¼Œå¸®åŠ©æ¨¡å‹æ ¹æ®å›¾åƒç‰¹å¾è¿›è¡Œåˆ¤æ–­ï¼Œå¹¶ä¿ƒè¿›å¯¹ä¸ç†Ÿæ‚‰ç–¾ç—…ç±»åˆ«çš„çŸ¥è¯†æ¨æ–­ã€‚3ï¼‰æå‡ºäº†ä¸€ç§åŸºäºç²¾ç»†æ³¨é‡Šçš„è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µï¼Œæä¾›æ›´å¹³æ»‘ã€ä¿¡æ¯æ›´ä¸°å¯Œçš„æ ‡ç­¾ï¼Œä»è€Œå®ç°ç²¾ç»†çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚4ï¼‰æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†MedFILIPçš„æ•ˆæœï¼Œä¾‹å¦‚RSNA-Pneumoniaã€NIH ChestX-ray14ã€VinBigDataå’ŒCOVID-19ã€‚å¯¹äºå•æ ‡ç­¾ã€å¤šæ ‡ç­¾å’Œç²¾ç»†åˆ†ç±»ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†ç±»å‡†ç¡®ç‡æœ€é«˜æé«˜äº†6.69%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PerceptionComputingLab/MedFILIP">https://github.com/PerceptionComputingLab/MedFILIP</a>ä¸­è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10775v1">PDF</a> 10 pages, 5 figures, IEEE Journal of Biomedical and Health   Informatics 2025</p>
<p><strong>Summary</strong>ï¼š<br>åŒ»ç–—é¢†åŸŸè§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰å¯¹åŒ»ç–—å›¾åƒåˆ†æè‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®æè¿°å›¾åƒä¸ç–¾ç—…ä¹‹é—´çš„å…³è”ï¼Œå¯¼è‡´è¯Šæ–­ç»“æœä¸å‡†ç¡®æˆ–ä¸å®Œæ•´ã€‚æœ¬ç ”ç©¶æå‡ºMedFILIPï¼Œä¸€ç§ç²¾ç»†çš„VLPæ¨¡å‹ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¼•å…¥åŒ»ç–—å›¾åƒç‰¹å®šçŸ¥è¯†ã€‚åŒ…æ‹¬æå‡ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯æå–å™¨ï¼Œæœ‰æ•ˆå‡å°‘æ–‡æœ¬å¤æ‚åº¦å¹¶ä¿ç•™ä¸°å¯Œä¿¡æ¯ï¼›æ„å»ºç±»åˆ«ä¸è§†è§‰å±æ€§ä¹‹é—´å…³ç³»çš„çŸ¥è¯†æ³¨å…¥å™¨ï¼Œå¸®åŠ©æ¨¡å‹åŸºäºå›¾åƒç‰¹å¾è¿›è¡Œåˆ¤æ–­ï¼Œå¹¶å¯¹æœªçŸ¥ç–¾ç—…ç±»åˆ«è¿›è¡ŒçŸ¥è¯†æ¨æ–­ï¼›ä»¥åŠåŸºäºç²¾ç»†æ³¨é‡Šçš„è¯­ä¹‰ç›¸ä¼¼æ€§çŸ©é˜µï¼Œæä¾›å¹³æ»‘ã€ä¿¡æ¯ä¸°å¯Œçš„æ ‡ç­¾ï¼Œå®ç°ç²¾ç»†çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯ï¼ŒMedFILIPåœ¨å•æ ‡ç­¾ã€å¤šæ ‡ç­¾å’Œç²¾ç»†åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œåˆ†ç±»å‡†ç¡®ç‡æœ€é«˜æå‡6.69%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŒ»ç–—è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åœ¨åŒ»ç–—å›¾åƒåˆ†æä¸­å¾ˆé‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®æè¿°å›¾åƒä¸ç–¾ç—…ä¹‹é—´çš„å…³è”ã€‚</li>
<li>MedFILIPæ¨¡å‹é€šè¿‡å¯¹æ¯”å­¦ä¹ å¼•å…¥åŒ»ç–—å›¾åƒç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>ä¿¡æ¯æå–å™¨èƒ½å¤Ÿå‡å°‘æ–‡æœ¬å¤æ‚åº¦å¹¶ä¿ç•™ä¸°å¯Œä¿¡æ¯ã€‚</li>
<li>çŸ¥è¯†æ³¨å…¥å™¨å¸®åŠ©æ¨¡å‹æ„å»ºç±»åˆ«å’Œè§†è§‰å±æ€§ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>è¯­ä¹‰ç›¸ä¼¼æ€§çŸ©é˜µæä¾›å¹³æ»‘ã€ä¿¡æ¯ä¸°å¯Œçš„æ ‡ç­¾ï¼Œå®ç°å›¾åƒæ–‡æœ¬ç²¾ç»†å¯¹é½ã€‚</li>
<li>MedFILIPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œåˆ†ç±»å‡†ç¡®ç‡æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ad8771602d4586eaae648c2fcc9cfc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5a2511ef41306695d832e1b91e7a584.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07eee1c23688c18b117e505534a9428a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee6f5d8b2953411468e94aebcc1f34d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f02ec7ff8fd06e534d755557d55e7fb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ClusterViG-Efficient-Globally-Aware-Vision-GNNs-via-Image-Partitioning"><a href="#ClusterViG-Efficient-Globally-Aware-Vision-GNNs-via-Image-Partitioning" class="headerlink" title="ClusterViG: Efficient Globally Aware Vision GNNs via Image Partitioning"></a>ClusterViG: Efficient Globally Aware Vision GNNs via Image Partitioning</h2><p><strong>Authors:Dhruv Parikh, Jacob Fein-Ashley, Tian Ye, Rajgopal Kannan, Viktor Prasanna</strong></p>
<p>Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have dominated the field of Computer Vision (CV). Graph Neural Networks (GNN) have performed remarkably well across diverse domains because they can represent complex relationships via unstructured graphs. However, the applicability of GNNs for visual tasks was unexplored till the introduction of Vision GNNs (ViG). Despite the success of ViGs, their performance is severely bottlenecked due to the expensive $k$-Nearest Neighbors ($k$-NN) based graph construction. Recent works addressing this bottleneck impose constraints on the flexibility of GNNs to build unstructured graphs, undermining their core advantage while introducing additional inefficiencies. To address these issues, in this paper, we propose a novel method called Dynamic Efficient Graph Convolution (DEGC) for designing efficient and globally aware ViGs. DEGC partitions the input image and constructs graphs in parallel for each partition, improving graph construction efficiency. Further, DEGC integrates local intra-graph and global inter-graph feature learning, enabling enhanced global context awareness. Using DEGC as a building block, we propose a novel CNN-GNN architecture, ClusterViG, for CV tasks. Extensive experiments indicate that ClusterViG reduces end-to-end inference latency for vision tasks by up to $5\times$ when compared against a suite of models such as ViG, ViHGNN, PVG, and GreedyViG, with a similar model parameter count. Additionally, ClusterViG reaches state-of-the-art performance on image classification, object detection, and instance segmentation tasks, demonstrating the effectiveness of the proposed globally aware learning strategy. Finally, input partitioning performed by DEGC enables ClusterViG to be trained efficiently on higher-resolution images, underscoring the scalability of our approach. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ã€‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰åœ¨ä¸åŒçš„é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥é€šè¿‡éç»“æ„åŒ–çš„å›¾æ¥è¡¨ç¤ºå¤æ‚çš„å…³ç³»ã€‚ç„¶è€Œï¼Œç›´åˆ°å¼•å…¥è§†è§‰å›¾ç¥ç»ç½‘ç»œï¼ˆViGï¼‰ï¼Œå›¾ç¥ç»ç½‘ç»œåœ¨è§†è§‰ä»»åŠ¡ä¸Šçš„åº”ç”¨å°šæœªå¾—åˆ°æ¢ç´¢ã€‚å°½ç®¡ViGå–å¾—äº†æˆåŠŸï¼Œä½†ç”±äºåŸºäºæ˜‚è´µçš„kè¿‘é‚»ï¼ˆk-NNï¼‰çš„å›¾æ„å»ºï¼Œå…¶æ€§èƒ½å—åˆ°äº†ä¸¥é‡é™åˆ¶ã€‚æœ€è¿‘çš„å·¥ä½œè§£å†³è¿™ä¸ªç“¶é¢ˆå¯¹å›¾ç¥ç»ç½‘ç»œçš„çµæ´»æ€§æ–½åŠ çº¦æŸï¼Œä»¥æ„å»ºéç»“æ„åŒ–å›¾ï¼Œè¿™å‰Šå¼±äº†å…¶æ ¸å¿ƒç«äº‰åŠ›å¹¶å¼•å…¥äº†é¢å¤–çš„æ•ˆç‡é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€é«˜æ•ˆå›¾å·ç§¯ï¼ˆDEGCï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºè®¾è®¡é«˜æ•ˆä¸”å…¨å±€æ„ŸçŸ¥çš„ViGã€‚DEGCå¯¹è¾“å…¥å›¾åƒè¿›è¡Œåˆ†åŒºï¼Œå¹¶ä¸ºæ¯ä¸ªåˆ†åŒºå¹¶è¡Œæ„å»ºå›¾ï¼Œæé«˜å›¾æ„å»ºæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒDEGCç»“åˆäº†å±€éƒ¨å›¾å†…å’Œå…¨å±€å›¾é—´ç‰¹å¾å­¦ä¹ ï¼Œå¢å¼ºäº†å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚ä½¿ç”¨DEGCä½œä¸ºæ„å»ºå—ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ–°å‹CNN-GNNæ¶æ„ClusterViGã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ViGã€ViHGNNã€PVGå’ŒGreedyViGç­‰ä¸€ç³»åˆ—æ¨¡å‹ç›¸æ¯”ï¼ŒClusterViGåœ¨è§†è§‰ä»»åŠ¡çš„ç«¯åˆ°ç«¯æ¨ç†å»¶è¿Ÿæ–¹é¢å‡å°‘äº†é«˜è¾¾5å€ï¼Œå…·æœ‰ç›¸ä¼¼çš„æ¨¡å‹å‚æ•°è®¡æ•°ã€‚æ­¤å¤–ï¼ŒClusterViGåœ¨å›¾åƒåˆ†ç±»ã€å¯¹è±¡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œè¯æ˜äº†æ‰€æå‡ºçš„å…¨å±€æ„ŸçŸ¥å­¦ä¹ ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼ŒDEGCæ‰§è¡Œçš„è¾“å…¥åˆ†åŒºä½¿ClusterViGèƒ½å¤Ÿåœ¨æ›´é«˜çš„åˆ†è¾¨ç‡å›¾åƒä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10640v1">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºåŠ¨æ€é«˜æ•ˆå›¾å·ç§¯ï¼ˆDEGCï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºè®¾è®¡é«˜æ•ˆä¸”å…¨å±€æ„ŸçŸ¥çš„è§†åŠ›å›¾ç¥ç»ç½‘ç»œï¼ˆViGï¼‰ã€‚è¯¥æ–¹æ³•å¯¹è¾“å…¥å›¾åƒè¿›è¡Œåˆ†åŒºï¼Œå¹¶è¡Œæ„å»ºæ¯ä¸ªåˆ†åŒºçš„å›¾ï¼Œæé«˜å›¾æ„å»ºæ•ˆç‡ã€‚DEGCç»“åˆäº†å±€éƒ¨å›¾å†…å’Œå…¨å±€å›¾é—´ç‰¹å¾å­¦ä¹ ï¼Œå¢å¼ºäº†å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚ä½¿ç”¨DEGCä½œä¸ºæ„å»ºå—ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å·ç§¯ç¥ç»ç½‘ç»œ-å›¾ç¥ç»ç½‘ç»œæ¶æ„ClusterViGï¼Œç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¸€ç³»åˆ—æ¨¡å‹ç›¸æ¯”ï¼ŒClusterViGåœ¨ç«¯åˆ°ç«¯çš„è§†è§‰ä»»åŠ¡æ¨ç†æ—¶é—´ä¸Šå‡å°‘äº†é«˜è¾¾5å€ï¼ŒåŒæ—¶è¾¾åˆ°äº†å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡çš„æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†å…¨å±€æ„ŸçŸ¥å­¦ä¹ ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†åŠ¨æ€é«˜æ•ˆå›¾å·ç§¯ï¼ˆDEGCï¼‰æ–¹æ³•ï¼Œç”¨äºè®¾è®¡æ›´ä¸ºé«˜æ•ˆçš„è§†åŠ›å›¾ç¥ç»ç½‘ç»œï¼ˆViGï¼‰ã€‚</li>
<li>DEGCé€šè¿‡å¯¹è¾“å…¥å›¾åƒè¿›è¡Œåˆ†åŒºå¹¶å¹¶è¡Œæ„å»ºå›¾ï¼Œæé«˜äº†å›¾æ„å»ºçš„æ•ˆç‡ã€‚</li>
<li>DEGCç»“åˆäº†å±€éƒ¨å’Œå…¨å±€ç‰¹å¾å­¦ä¹ ï¼Œå¢å¼ºäº†å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†æ–°å‹çš„å·ç§¯ç¥ç»ç½‘ç»œ-å›¾ç¥ç»ç½‘ç»œæ¶æ„ClusterViGã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒClusterViGåœ¨è§†è§‰ä»»åŠ¡æ¨ç†æ—¶é—´ä¸Šå‡å°‘äº†é«˜è¾¾5å€ã€‚</li>
<li>ClusterViGåœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
<li>è¾“å…¥å›¾åƒçš„åˆ†åŒºä½¿å¾—ClusterViGèƒ½å¤Ÿåœ¨æ›´é«˜çš„åˆ†è¾¨ç‡å›¾åƒä¸Šè¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒï¼Œæ˜¾ç¤ºäº†å…¶å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d4d154851b4c36b2781f21995fa2758f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8de65c83d46f984b24ad882d4805654.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Vision Transformer/2501.10640v1/page_3_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-664d50c5c1fca2b3b751663d6277b493.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f9ac1bc0bb8b9d3f8e791e35256e63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e63302ca9dc3ae83c227f057c8a3926e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="IDEA-Image-Description-Enhanced-CLIP-Adapter"><a href="#IDEA-Image-Description-Enhanced-CLIP-Adapter" class="headerlink" title="IDEA: Image Description Enhanced CLIP-Adapter"></a>IDEA: Image Description Enhanced CLIP-Adapter</h2><p><strong>Authors:Zhipeng Ye, Feng Jiang, Qiufeng Wang, Kaizhu Huang, Jiaqi Huang</strong></p>
<p>CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the modelâ€™s performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named â€œIMD-11â€. Our code and data are released at <a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA">https://github.com/FourierAI/IDEA</a>. </p>
<blockquote>
<p>CLIPï¼ˆå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰åœ¨æ¨¡å¼è¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚å°†CLIPåº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆä¾‹å¦‚é›¶æ ·æœ¬æˆ–å°æ ·æœ¬åˆ†ç±»ï¼‰æ˜¯å¤šæ¨¡æ€å­¦ä¹ çš„çƒ­é—¨è¯é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨æ–‡æœ¬æç¤ºå­¦ä¹ æˆ–è§†è§‰é€‚é…å™¨è°ƒæ•´ï¼Œè€Œæ²¡æœ‰å……åˆ†åˆ©ç”¨å›¾åƒæ–‡æœ¬å¯¹ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯å’Œå…³è”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å›¾åƒæè¿°å¢å¼ºCLIPé€‚é…å™¨ï¼ˆIDEAï¼‰æ–¹æ³•ï¼Œå°†CLIPé€‚åº”äºå°æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å›¾åƒçš„è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬æè¿°æ¥æ•æ‰ç²¾ç»†ç‰¹å¾ã€‚IDEAæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„CLIPæ–¹æ³•ï¼Œå®ƒåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°å¯ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ï¼Œç”šè‡³è¶…è¿‡å®ƒä»¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯è®­ç»ƒçš„IDEAï¼ˆT-IDEAï¼‰ï¼Œå®ƒé€šè¿‡æ·»åŠ ä¸¤ä¸ªè½»é‡çº§çš„å­¦ä¹ ç»„ä»¶ï¼ˆå³æŠ•å½±å™¨å’Œå¯å­¦ä¹ çš„æ½œåœ¨ç©ºé—´ï¼‰æ¥æ‰©å±•IDEAï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨11ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœã€‚ä½œä¸ºä¸€é¡¹é‡è¦è´¡çŒ®ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†Llamaæ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„æµç¨‹æ¥ç”Ÿæˆè¿™11ä¸ªæ•°æ®é›†çš„å›¾åƒæ–‡æœ¬æè¿°ï¼Œå…±ç”Ÿæˆäº†1,637,795ä¸ªå›¾åƒæ–‡æœ¬å¯¹ï¼Œåä¸ºâ€œIMD-11â€ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/FourierAI/IDEAä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08816v2">PDF</a> </p>
<p><strong>Summary</strong><br>CLIPåœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­å¤‡å—å…³æ³¨ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒæè¿°å¢å¼ºçš„CLIPé€‚é…å™¨ï¼ˆIDEAï¼‰æ–¹æ³•ï¼Œç”¨äºé€‚åº”å°‘é‡å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰ç‰¹å¾å’Œå›¾åƒæ–‡æœ¬æè¿°æ¥æ•æ‰ç²¾ç»†ç‰¹å¾ï¼Œæ— éœ€è®­ç»ƒCLIPå³å¯ä¸æœ€æ–°æ¨¡å‹ç›¸æ¯”ç”šè‡³è¶…è¶Šã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†å¯è®­ç»ƒçš„IDEAï¼ˆT-IDEAï¼‰ï¼Œé€šè¿‡æ·»åŠ ä¸¤ä¸ªè½»é‡çº§çš„å­¦ä¹ ç»„ä»¶ï¼ˆæŠ•å½±ä»ªå’Œå­¦ä¹ æ½œåœ¨ç©ºé—´ï¼‰è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œå¹¶åœ¨11ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPåœ¨æ¨¡å¼è¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œåœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨æ–‡æœ¬æç¤ºå­¦ä¹ æˆ–è§†è§‰é€‚é…å™¨è°ƒæ•´ï¼Œä½†æœªå……åˆ†åˆ©ç”¨å›¾åƒæ–‡æœ¬å¯¹ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯å’Œç›¸å…³æ€§ã€‚</li>
<li>IDEAæ–¹æ³•é€šè¿‡ç»“åˆè§†è§‰ç‰¹å¾å’Œå›¾åƒæ–‡æœ¬æè¿°ï¼Œé€‚åº”å°‘é‡å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œæ•æ‰ç²¾ç»†ç‰¹å¾ã€‚</li>
<li>IDEAæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„CLIPæ–¹æ³•ï¼Œå¯åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¸æœ€æ–°æ¨¡å‹ç›¸æ¯”ç”šè‡³è¶…è¶Šã€‚</li>
<li>T-IDEAçš„å¼•å…¥è¿›ä¸€æ­¥æé«˜äº†IDEAçš„æ€§èƒ½ï¼Œé€šè¿‡åœ¨IDEAçš„åŸºç¡€ä¸Šæ·»åŠ ä¸¤ä¸ªè½»é‡çº§çš„å­¦ä¹ ç»„ä»¶ï¼ˆæŠ•å½±ä»ªå’Œå­¦ä¹ æ½œåœ¨ç©ºé—´ï¼‰ã€‚</li>
<li>åœ¨11ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœï¼Œå¹¶é‡Šæ”¾äº†ä»£ç å’Œæ•°æ®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-98efde1f62194ce4925ee67837cc4d2c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ee0029e3fce28293a8ddc11aa8fd0bf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM"><a href="#UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM" class="headerlink" title="UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM"></a>UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM</h2><p><strong>Authors:Xuhui Guo, Tanmoy Dam, Rohan Dhamdhere, Gourav Modanwal, Anant Madabhushi</strong></p>
<p>3D medical image segmentation has progressed considerably due to Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these methods struggle to balance long-range dependency acquisition with computational efficiency. To address this challenge, we propose UNETVL (U-Net Vision-LSTM), a novel architecture that leverages recent advancements in temporal information processing. UNETVL incorporates Vision-LSTM (ViL) for improved scalability and memory functions, alongside an efficient Chebyshev Kolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency patterns more effectively. We validated our method on the ACDC and AMOS2022 (post challenge Task 2) benchmark datasets, showing a significant improvement in mean Dice score compared to recent state-of-the-art approaches, especially over its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS, respectively. Extensive ablation studies were conducted to demonstrate the impact of each component in UNETVL, providing a comprehensive understanding of its architecture. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL">https://github.com/tgrex6/UNETVL</a>, facilitating further research and applications in this domain. </p>
<blockquote>
<p>3DåŒ»å­¦å›¾åƒåˆ†å‰²ç”±äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„å‘å±•è€Œå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¹³è¡¡è¿œç¨‹ä¾èµ–è·å–ä¸è®¡ç®—æ•ˆç‡æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UNETVLï¼ˆU-Net Vision-LSTMï¼‰è¿™ä¸€æ–°å‹æ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨æœ€æ–°çš„æ—¶åºä¿¡æ¯å¤„ç†æŠ€æœ¯çš„ä¼˜åŠ¿ã€‚UNETVLç»“åˆäº†è§†è§‰LSTMï¼ˆViLï¼‰ä»¥æé«˜å¯æ‰©å±•æ€§å’Œå†…å­˜åŠŸèƒ½ï¼ŒåŒæ—¶é‡‡ç”¨é«˜æ•ˆçš„åˆ‡æ¯”é›ªå¤«-æŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå°”å¾·ç½‘ç»œï¼ˆKANï¼‰ï¼Œæ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚ä¸”è¿œç¨‹ä¾èµ–æ¨¡å¼ã€‚æˆ‘ä»¬åœ¨ACDCå’ŒAMOS2022ï¼ˆåæŒ‘æˆ˜ä»»åŠ¡2ï¼‰åŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç›¸è¾ƒäºæœ€æ–°çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶å‰èº«UNETRä¸Šï¼ŒDiceç³»æ•°çš„å¹³å‡å¾—åˆ†æœ‰äº†æ˜¾è‘—æé«˜ï¼ŒACDCä¸Šæé«˜äº†7.3%ï¼ŒAMOSä¸Šæé«˜äº†15.6%ã€‚è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œä»¥å±•ç¤ºUNETVLä¸­æ¯ä¸ªç»„ä»¶çš„å½±å“ï¼Œå¯¹å…¶æ¶æ„æä¾›äº†å…¨é¢çš„ç†è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL%E6%89%BE%E5%88%B0%EF%BC%8C%E4%B8%BA%E8%BF%99%E4%B8%80%E9%A2%86%E5%9F%9F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BA%94%E7%94%A8%E6%8F%90%E4%BE%9B%E4%BA%86%E4%BE%BF%E5%88%A9%E3%80%82">https://github.com/tgrex6/UNETVLæ‰¾åˆ°ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨æä¾›äº†ä¾¿åˆ©ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07017v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„UNETVLï¼ˆU-Net Vision-LSTMï¼‰ï¼Œç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„ä¼˜åŠ¿ï¼Œé€šè¿‡å¼•å…¥Vision-LSTMå¢å¼ºå¯æ‰©å±•æ€§å’Œè®°å¿†åŠŸèƒ½ï¼Œå¹¶ä½¿ç”¨é«˜æ•ˆçš„åˆ‡æ¯”é›ªå¤«æŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå°”å¾·ç½‘ç»œï¼ˆKANï¼‰æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚å’Œè¿œç¨‹ä¾èµ–æ¨¡å¼ã€‚åœ¨ACDCå’ŒAMOS2022åŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºæœ€æ–°å…ˆè¿›æŠ€æœ¯æœ‰æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ACDCä¸Šæå‡äº†7.3%ï¼Œåœ¨AMOSä¸Šæå‡äº†15.6%ã€‚è¯¦å°½çš„æ¶ˆèç ”ç©¶å±•ç¤ºäº†UNETVLå„ç»„ä»¶çš„å½±å“ï¼Œä¸ºå…¶æ¶æ„æä¾›äº†å…¨é¢çš„ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UNETVLç»“åˆäº†CNNå’ŒViTçš„ä¼˜åŠ¿ï¼Œé’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸè¿›è¡Œäº†åˆ›æ–°è®¾è®¡ã€‚</li>
<li>Vision-LSTMçš„å¼•å…¥å¢å¼ºäº†æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œè®°å¿†åŠŸèƒ½ã€‚</li>
<li>åˆ‡æ¯”é›ªå¤«æŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå°”å¾·ç½‘ç»œï¼ˆKANï¼‰èƒ½æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„è¿œç¨‹ä¾èµ–æ¨¡å¼ã€‚</li>
<li>åœ¨ACDCå’ŒAMOS2022åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒUNETVLåœ¨ACDCå’ŒAMOSæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>æ¶ˆèç ”ç©¶å±•ç¤ºäº†UNETVLæ¶æ„ä¸­å„ä¸ªç»„ä»¶çš„é‡è¦æ€§å’Œå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9d7c8c699d1ff63f923f508555bda630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-878046a9be14a4344667b66bd6a4d7ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3871d2bf4438644024fcdf844b4760fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89d5f5cf8fff3dec48d4df4779011549.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Token-Turing-Machines-are-Efficient-Vision-Models"><a href="#Token-Turing-Machines-are-Efficient-Vision-Models" class="headerlink" title="Token Turing Machines are Efficient Vision Models"></a>Token Turing Machines are Efficient Vision Models</h2><p><strong>Authors:Purvish Jajal, Nick John Eliopoulos, Benjamin Shiue-Hal Chou, George K. Thiravathukal, James C. Davis, Yung-Hsiang Lu</strong></p>
<p>We propose Vision Token Turing Machines (ViTTM), an efficient, low-latency, memory-augmented Vision Transformer (ViT). Our approach builds on Neural Turing Machines and Token Turing Machines, which were applied to NLP and sequential visual understanding tasks. ViTTMs are designed for non-sequential computer vision tasks such as image classification and segmentation. Our model creates two sets of tokens: process tokens and memory tokens; process tokens pass through encoder blocks and read-write from memory tokens at each encoder block in the network, allowing them to store and retrieve information from memory. By ensuring that there are fewer process tokens than memory tokens, we are able to reduce the inference time of the network while maintaining its accuracy. On ImageNet-1K, the state-of-the-art ViT-B has median latency of 529.5ms and 81.0% accuracy, while our ViTTM-B is 56% faster (234.1ms), with 2.4 times fewer FLOPs, with an accuracy of 82.9%. On ADE20K semantic segmentation, ViT-B achieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model acheives a 45.17 mIoU with 26.8 FPS (+94%). </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†è§†è§‰ä»¤ç‰Œå›¾çµæœºï¼ˆVision Token Turing Machinesï¼Œç®€ç§°ViTTMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆã€ä½å»¶è¿Ÿã€å¢å¼ºè®°å¿†çš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºç¥ç»å›¾çµæœºå’Œä»¤ç‰Œå›¾çµæœºï¼Œåè€…è¢«åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œåºåˆ—è§†è§‰ç†è§£ä»»åŠ¡ã€‚ViTTMè¢«è®¾è®¡ç”¨äºéåºåˆ—è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ›å»ºäº†ä¸¤ç»„ä»¤ç‰Œï¼šå¤„ç†ä»¤ç‰Œå’Œå†…å­˜ä»¤ç‰Œï¼›å¤„ç†ä»¤ç‰Œé€šè¿‡ç¼–ç å™¨å—ï¼Œå¹¶ä»ç½‘ç»œä¸­çš„æ¯ä¸ªç¼–ç å™¨å—è¯»å†™å†…å­˜ä»¤ç‰Œï¼Œå…è®¸å®ƒä»¬ä»å†…å­˜ä¸­å­˜å‚¨å’Œæ£€ç´¢ä¿¡æ¯ã€‚é€šè¿‡ç¡®ä¿å¤„ç†ä»¤ç‰Œçš„æ•°ç›®å°‘äºå†…å­˜ä»¤ç‰Œï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¿æŒç½‘ç»œç²¾åº¦çš„åŒæ—¶å‡å°‘å…¶æ¨ç†æ—¶é—´ã€‚åœ¨ImageNet-1Kä¸Šï¼Œæœ€å…ˆè¿›çš„ViT-Bå…·æœ‰529.5æ¯«ç§’çš„ä¸­ä½å»¶è¿Ÿå’Œ81.0%çš„å‡†ç¡®ç‡ï¼Œè€Œæˆ‘ä»¬çš„ViTTM-Bæ›´å¿«ï¼ˆ234.1æ¯«ç§’ï¼‰ï¼ŒFLOPså‡å°‘äº†2.4å€ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†82.9%ã€‚åœ¨ADE20Kè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒViT-Bä»¥æ¯ç§’13.8å¸§çš„é€Ÿåº¦è¾¾åˆ°45.65 mIoUï¼Œè€Œæˆ‘ä»¬çš„ViTTM-Bæ¨¡å‹ä»¥æ¯ç§’26.8å¸§çš„é€Ÿåº¦è¾¾åˆ°45.17 mIoUï¼ˆ+ 94%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07613v2">PDF</a> Accepted to WACV 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†Vision Token Turing Machinesï¼ˆViTTMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆã€ä½å»¶è¿Ÿã€å¸¦æœ‰è®°å¿†åŠŸèƒ½çš„Vision Transformerï¼ˆViTï¼‰ã€‚ViTTMå€Ÿé‰´äº†ç¥ç»å›¾çµæœºå’Œä»¤ç‰Œå›¾çµæœºåœ¨NLPå’Œè§†è§‰ç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä¸“é—¨ç”¨äºéåºåˆ—è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ã€‚é€šè¿‡åˆ›å»ºä¸¤ç»„ä»¤ç‰Œï¼šå¤„ç†ä»¤ç‰Œå’Œå†…å­˜ä»¤ç‰Œï¼ŒViTTMèƒ½å¤Ÿåœ¨ç½‘ç»œä¸­ä»å†…å­˜ä»¤ç‰Œè¯»å–å’Œå†™å…¥ä¿¡æ¯ã€‚é€šè¿‡ç¡®ä¿å¤„ç†ä»¤ç‰Œçš„æ•°é‡å°‘äºå†…å­˜ä»¤ç‰Œï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶å‡å°‘ç½‘ç»œçš„æ¨ç†æ—¶é—´ã€‚åœ¨ImageNet-1Kä¸Šï¼ŒViTTM-Bæ¨¡å‹æ¯”å…ˆè¿›çš„ViT-Bæ¨¡å‹æ›´å¿«ã€æ›´æœ‰æ•ˆç‡ï¼Œå¹¶ä¸”å‡†ç¡®æ€§æ›´é«˜ã€‚åœ¨ADE20Kè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒViTTM-Bæ¨¡å‹ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Token Turing Machines (ViTTM) æ˜¯ä¸€ç§é’ˆå¯¹è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ”¹è¿›æ¨¡å‹ï¼ŒåŸºäº Vision Transformerï¼ˆViTï¼‰ã€‚</li>
<li>ViTTMå€Ÿé‰´äº†ç¥ç»å›¾çµæœºå’Œä»¤ç‰Œå›¾çµæœºï¼Œç‰¹åˆ«é€‚ç”¨äºéåºåˆ—è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ã€‚</li>
<li>ViTTMé€šè¿‡åˆ›å»ºå¤„ç†ä»¤ç‰Œå’Œå†…å­˜ä»¤ç‰Œä¸¤ç»„ï¼Œèƒ½å¤Ÿåœ¨ç½‘ç»œä¸­å­˜å‚¨å’Œæ£€ç´¢ä¿¡æ¯ã€‚</li>
<li>ViTTMèƒ½å¤Ÿå‡å°‘ç½‘ç»œçš„æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶é€šè¿‡ç¡®ä¿å¤„ç†ä»¤ç‰Œæ•°é‡å°‘äºå†…å­˜ä»¤ç‰Œæ¥æé«˜æ•ˆç‡ã€‚</li>
<li>åœ¨ImageNet-1Kå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼ŒViTTM-Bæ¨¡å‹ç›¸æ¯”å…ˆè¿›çš„ViT-Bæ¨¡å‹æ›´å¿«ã€æ›´æœ‰æ•ˆç‡ï¼Œå¹¶ä¸”å‡†ç¡®æ€§æ›´é«˜ã€‚</li>
<li>åœ¨ADE20Kè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒViTTM-Bæ¨¡å‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸ViT-Bæ¨¡å‹ç›¸æ¯”å…·æœ‰æ›´é«˜çš„å¸§ç‡å’Œç›¸ä¼¼çš„mIoUã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2dc88b89b28bfe513d666261a1ee613.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25d09a13e2364fb85f6046a574031586.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5aeb00e2f7b7e482626c579b794392a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9033f6d2ddc6be8ee9bcd9744830803e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56c4a182d76d8c7dda602d8210c0ee8d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NoteLLM-2-Multimodal-Large-Representation-Models-for-Recommendation"><a href="#NoteLLM-2-Multimodal-Large-Representation-Models-for-Recommendation" class="headerlink" title="NoteLLM-2: Multimodal Large Representation Models for Recommendation"></a>NoteLLM-2: Multimodal Large Representation Models for Recommendation</h2><p><strong>Authors:Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, Enhong Chen</strong></p>
<p>Large Language Models (LLMs) have demonstrated exceptional proficiency in text understanding and embedding tasks. However, their potential in multimodal representation, particularly for item-to-item (I2I) recommendations, remains underexplored. While leveraging existing Multimodal Large Language Models (MLLMs) for such tasks is promising, challenges arise due to their delayed release compared to corresponding LLMs and the inefficiency in representation tasks. To address these issues, we propose an end-to-end fine-tuning method that customizes the integration of any existing LLMs and vision encoders for efficient multimodal representation. Preliminary experiments revealed that fine-tuned LLMs often neglect image content. To counteract this, we propose NoteLLM-2, a novel framework that enhances visual information. Specifically, we propose two approaches: first, a prompt-based method that segregates visual and textual content, employing a multimodal In-Context Learning strategy to balance focus across modalities; second, a late fusion technique that directly integrates visual information into the final representations. Extensive experiments, both online and offline, demonstrate the effectiveness of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/NoteLLM">https://github.com/Applied-Machine-Learning-Lab/NoteLLM</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ç†è§£å’ŒåµŒå…¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤šæ¨¡æ€è¡¨ç¤ºä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é¡¹ç›®åˆ°é¡¹ç›®ï¼ˆI2Iï¼‰çš„æ¨èï¼Œä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚è™½ç„¶åˆ©ç”¨ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥å®Œæˆæ­¤ç±»ä»»åŠ¡æ˜¯æœ‰å‰é€”çš„ï¼Œä½†ç”±äºå®ƒä»¬ç›¸è¾ƒäºå¯¹åº”çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å¸ƒå­˜åœ¨å»¶è¿Ÿï¼Œä»¥åŠåœ¨è¡¨ç¤ºä»»åŠ¡ä¸­çš„æ•ˆç‡ä¸é«˜ï¼Œå› æ­¤ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥å®šåˆ¶ä»»ä½•ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚åˆæ­¥å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å¾€å¾€ä¼šå¿½ç•¥å›¾åƒå†…å®¹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†NoteLLM-2è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œä»¥å¢å¼ºè§†è§‰ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼šé¦–å…ˆï¼Œä¸€ç§åŸºäºæç¤ºçš„æ–¹æ³•ï¼Œå®ƒåˆ†ç¦»è§†è§‰å’Œæ–‡æœ¬å†…å®¹ï¼Œé‡‡ç”¨å¤šæ¨¡å¼ä¸Šä¸‹æ–‡å­¦ä¹ ç­–ç•¥æ¥å¹³è¡¡è·¨æ¨¡æ€çš„ç„¦ç‚¹ï¼›å…¶æ¬¡ï¼Œä¸€ç§æ™šæœŸèåˆæŠ€æœ¯ï¼Œå®ƒç›´æ¥å°†è§†è§‰ä¿¡æ¯èåˆåˆ°æœ€ç»ˆè¡¨ç¤ºä¸­ã€‚çº¿ä¸Šå’Œçº¿ä¸‹çš„å¹¿æ³›å®éªŒéƒ½è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/NoteLLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/NoteLLMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16789v2">PDF</a> Accepted by KDDâ€™25 ADS track</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç†è§£å’ŒåµŒå…¥ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨å¤šæ¨¡æ€è¡¨ç¤ºï¼Œå°¤å…¶æ˜¯ç‰©å“åˆ°ç‰©å“ï¼ˆI2Iï¼‰æ¨èæ–¹é¢ï¼Œå…¶æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†åˆ©ç”¨ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯å®šåˆ¶ä»»ä½•ç°æœ‰çš„LLMså’Œè§†è§‰ç¼–ç å™¨ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–°æ–¹æ³•NoteLLM-2é€šè¿‡ä¸¤ç§æ–°ç­–ç•¥æ¥å¢å¼ºè§†è§‰ä¿¡æ¯ï¼šä¸€æ˜¯åŸºäºæç¤ºçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ†å‰²è§†è§‰å’Œæ–‡æœ¬å†…å®¹ï¼Œé‡‡ç”¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ç­–ç•¥æ¥å¹³è¡¡è·¨æ¨¡æ€çš„å…³æ³¨åº¦ï¼›äºŒæ˜¯åæœŸèåˆæŠ€æœ¯ï¼Œå®ƒç›´æ¥å°†è§†è§‰ä¿¡æ¯é›†æˆåˆ°æœ€ç»ˆè¡¨ç¤ºä¸­ã€‚ç»è¿‡åœ¨çº¿å’Œç¦»çº¿çš„å¤§é‡å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•ˆæœæ˜¾è‘—ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/NoteLLM%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/NoteLLMã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€è¡¨ç¤ºæ–¹é¢æ½œåŠ›å·¨å¤§ï¼Œä½†åœ¨ç‰©å“åˆ°ç‰©å“æ¨èç­‰ç‰¹å®šä»»åŠ¡ä¸Šçš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚</li>
<li>åˆ©ç”¨ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šä»»åŠ¡å¤„ç†é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å»¶è¿Ÿå‘å¸ƒå’Œè¡¨ç¤ºä»»åŠ¡æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨å®šåˆ¶LLMså’Œè§†è§‰ç¼–ç å™¨ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚è¿™æ˜¯é€šè¿‡æ•´åˆä»»ä½•ç°æœ‰çš„LLMså’Œè§†è§‰ç¼–ç å™¨æ¥å®ç°çš„ã€‚</li>
<li>æ–°æ–¹æ³•NoteLLM-2æ—¨åœ¨å¢å¼ºè§†è§‰ä¿¡æ¯ï¼Œé€šè¿‡ä¸¤ç§ç­–ç•¥ï¼šåŸºäºæç¤ºçš„æ–¹æ³•å’ŒåæœŸèåˆæŠ€æœ¯ã€‚</li>
<li>åŸºäºæç¤ºçš„æ–¹æ³•é€šè¿‡åˆ†å‰²è§†è§‰å’Œæ–‡æœ¬å†…å®¹ï¼Œå¹¶é‡‡ç”¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ç­–ç•¥æ¥å¹³è¡¡è·¨æ¨¡æ€å…³æ³¨åº¦ã€‚</li>
<li>åæœŸèåˆæŠ€æœ¯ç›´æ¥é›†æˆè§†è§‰ä¿¡æ¯åˆ°æœ€ç»ˆè¡¨ç¤ºä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0567255a0bfa3131897ad071481a03f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17bae8445b79f7aaa2aa43b8ee1e7556.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3844dc8eddeca9c7d3bc9c979c7d1da1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cc498aef04ec38629810f038a08e728.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb4f4803ae534c8665068f76324c831c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-277744f779dffa7b09efc773b92de1dd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e1a869d029f8dfbef879e0e9fc6d50f9.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2305bace8fd1a620c2df7ba09faba2f0.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  MMVU Measuring Expert-Level Multi-Discipline Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
