<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-01-23  Vision-Language Models for Automated Chest X-ray Interpretation   Leveraging ViT and GPT-2">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4cc498aef04ec38629810f038a08e728.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    41 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-23-更新"><a href="#2025-01-23-更新" class="headerlink" title="2025-01-23 更新"></a>2025-01-23 更新</h1><h2 id="Vision-Language-Models-for-Automated-Chest-X-ray-Interpretation-Leveraging-ViT-and-GPT-2"><a href="#Vision-Language-Models-for-Automated-Chest-X-ray-Interpretation-Leveraging-ViT-and-GPT-2" class="headerlink" title="Vision-Language Models for Automated Chest X-ray Interpretation:   Leveraging ViT and GPT-2"></a>Vision-Language Models for Automated Chest X-ray Interpretation:   Leveraging ViT and GPT-2</h2><p><strong>Authors:Md. Rakibul Islam, Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu</strong></p>
<p>Radiology plays a pivotal role in modern medicine due to its non-invasive diagnostic capabilities. However, the manual generation of unstructured medical reports is time consuming and prone to errors. It creates a significant bottleneck in clinical workflows. Despite advancements in AI-generated radiology reports, challenges remain in achieving detailed and accurate report generation. In this study we have evaluated different combinations of multimodal models that integrate Computer Vision and Natural Language Processing to generate comprehensive radiology reports. We employed a pretrained Vision Transformer (ViT-B16) and a SWIN Transformer as the image encoders. The BART and GPT-2 models serve as the textual decoders. We used Chest X-ray images and reports from the IU-Xray dataset to evaluate the usability of the SWIN Transformer-BART, SWIN Transformer-GPT-2, ViT-B16-BART and ViT-B16-GPT-2 models for report generation. We aimed at finding the best combination among the models. The SWIN-BART model performs as the best-performing model among the four models achieving remarkable results in almost all the evaluation metrics like ROUGE, BLEU and BERTScore. </p>
<blockquote>
<p>放射学在现代医学中扮演着至关重要的角色，因其具有非侵入性的诊断能力。然而，手动生成非结构化医疗报告既耗时又容易出错，这成为临床工作流程中的重大瓶颈。尽管人工智能生成的放射学报告有所进展，但在实现详细准确的报告生成方面仍存在挑战。在这项研究中，我们评估了结合计算机视觉和自然语言处理的多模式模型的不同组合，以生成全面的放射学报告。我们采用了预训练的Vision Transformer（ViT-B16）和SWIN Transformer作为图像编码器。BART和GPT-2模型则作为文本解码器。我们使用IU-Xray数据集中的胸部X射线图像和报告来评估SWIN Transformer-BART、SWIN Transformer-GPT-2、ViT-B16-BART和ViT-B16-GPT-2模型在报告生成方面的实用性。我们的目标是找到模型之间的最佳组合。在四项模型中，SWIN-BART模型表现最佳，在几乎所有评估指标（如ROUGE、BLEU和BERTScore）中都取得了显著的成绩。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12356v1">PDF</a> Preprint, manuscript under-review</p>
<p><strong>Summary</strong></p>
<p>本文探讨了放射学在现代医学中的重要作用及其面临的挑战。研究中，通过结合计算机视觉和自然语言处理技术，评估了不同组合模型在生成综合放射学报告方面的性能。采用预训练的Vision Transformer（ViT-B16）和SWIN Transformer作为图像编码器，BART和GPT-2模型作为文本解码器。使用IU-Xray数据集上的Chest X-ray图像和报告进行评估。结果显示，SWIN-BART模型在几乎所有评估指标上表现最佳，如ROUGE、BLEU和BERTScore。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>放射学在现代医学中具有非侵入性诊断能力的重要角色。</li>
<li>自动化生成医疗报告面临时间消耗和误差率问题，成为临床工作流程的瓶颈。</li>
<li>研究评估了结合计算机视觉和自然语言处理的多模态模型在生成综合放射学报告方面的性能。</li>
<li>采用了Vision Transformer（ViT-B16）和SWIN Transformer作为图像编码器。</li>
<li>BART和GPT-2模型被用作文本解码器。</li>
<li>使用IU-Xray数据集中的Chest X-ray图像和报告进行评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12356">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9f8df32d8e52eaed8bdb31c790e1b843.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccdb0faaab13d2d26d4c143ce66449a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification"><a href="#CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification" class="headerlink" title="CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification"></a>CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification</h2><p><strong>Authors:Cristiano Patrício, Isabel Rio-Torto, Jaime S. Cardoso, Luís F. Teixeira, João C. Neves</strong></p>
<p>The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the final disease prediction on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: <a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/">https://cristianopatricio.github.io/CBVLM/</a>. </p>
<blockquote>
<p>在医疗工作流中采用基于深度学习的解决方案的主要挑战在于标注数据的可用性和此类系统缺乏可解释性。概念瓶颈模型（CBMs）通过约束最终疾病预测在一组预先定义和可解释的概念上来解决后者的问题。然而，通过基于概念的解释实现的增加的可解释性意味着更高的标注负担。而且，如果要添加新概念，整个系统需要重新训练。受大型视觉语言模型（LVLMs）在少量样本设置中的出色性能的启发，我们提出了一种简单而有效的方法，即CBVLM，该方法解决了上述两个挑战。首先，对于每个概念，我们提示LVLM回答概念是否出现在输入图像中。然后，我们让LVLM基于先前的概念预测对图像进行分类。而且，在两个阶段中，我们采用检索模块负责选择最佳样本进行上下文学习。通过将最终诊断建立在预测的概念上，我们确保了可解释性，并通过利用LVLMs的少量样本能力，我们大大降低了标注成本。我们通过四个医疗数据集和十二个（通用和医疗）LVLMs的广泛实验验证了我们的方法，并表明CBVLM在不需要任何训练和仅使用少量标注样本的情况下，始终优于CBMs和特定任务监督方法。更多信息请参见我们的项目页面：<a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/%E3%80%82">https://cristianopatricio.github.io/CBVLM/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12266v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在医疗工作流中采用基于深度学习的解决方案时面临的主要挑战，即标注数据的可用性和系统解释性的缺乏。概念瓶颈模型（CBMs）通过在一组预定义和可解释的概念上约束疾病预测来解决后者的问题。然而，通过概念解释提高的可解释性意味着更高的标注负担，并且如果需要添加新概念，则需要重新训练整个系统。受大型视觉语言模型（LVLMs）在少样本设置中的出色性能的启发，本文提出了一种简单而有效的方法——CBVLM，该方法解决了上述两个挑战。CBVLM首先通过LVLM判断输入图像中是否存在每个概念，然后根据之前的概念预测对图像进行分类。此外，两个阶段都引入了检索模块，负责选择最佳的上下文学习示例。通过将最终诊断结果基于预测的概念来确保解释性，并利用LVLMs的少量样本能力来大幅降低标注成本。经过在四个医疗数据集和十二个（通用和医疗）LVLMs上的广泛实验验证，结果表明CBVLM持续优于CBMs和任务特定的监督方法，而且无需任何训练，只需使用少量标注样本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习方法在医疗工作流中的采纳面临标注数据可用性和系统解释性的挑战。</li>
<li>概念瓶颈模型（CBMs）通过约束疾病预测在预定义概念上解决解释性问题，但增加了标注负担和重新训练需求。</li>
<li>CBVLM方法结合大型视觉语言模型（LVLMs）解决上述挑战。</li>
<li>CBVLM利用LVLMs进行概念预测和图像分类，确保解释性并降低标注成本。</li>
<li>CBVLM在四个医疗数据集上表现优越，持续优于CBMs和任务特定监督方法。</li>
<li>CBVLM方法不需要任何训练，仅使用少量标注样本即可实现高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12266">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-206021a739805f4c025fbbbda977fff6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0afff72f19698ec85886bc603208c360.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0abc5ca96e375c7cac4c0ac188efd080.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0aea3215822b78c86eba3f65abedf78.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Pre-trained-Deep-Learning-Models-and-DINOv2-for-Cushing’s-Syndrome-Diagnosis-in-Facial-Analysis"><a href="#Comparative-Analysis-of-Pre-trained-Deep-Learning-Models-and-DINOv2-for-Cushing’s-Syndrome-Diagnosis-in-Facial-Analysis" class="headerlink" title="Comparative Analysis of Pre-trained Deep Learning Models and DINOv2 for   Cushing’s Syndrome Diagnosis in Facial Analysis"></a>Comparative Analysis of Pre-trained Deep Learning Models and DINOv2 for   Cushing’s Syndrome Diagnosis in Facial Analysis</h2><p><strong>Authors:Hongjun Liu, Changwei Song, Jiaqi Qiang, Jianqiang Li, Hui Pan, Lin Lu, Xiao Long, Qing Zhao, Jiuzuo Huang, Shi Chen</strong></p>
<p>Cushing’s syndrome is a condition caused by excessive glucocorticoid secretion from the adrenal cortex, often manifesting with moon facies and plethora, making facial data crucial for diagnosis. Previous studies have used pre-trained convolutional neural networks (CNNs) for diagnosing Cushing’s syndrome using frontal facial images. However, CNNs are better at capturing local features, while Cushing’s syndrome often presents with global facial features. Transformer-based models like ViT and SWIN, which utilize self-attention mechanisms, can better capture long-range dependencies and global features. Recently, DINOv2, a foundation model based on visual Transformers, has gained interest. This study compares the performance of various pre-trained models, including CNNs, Transformer-based models, and DINOv2, in diagnosing Cushing’s syndrome. We also analyze gender bias and the impact of freezing mechanisms on DINOv2. Our results show that Transformer-based models and DINOv2 outperformed CNNs, with ViT achieving the highest F1 score of 85.74%. Both the pre-trained model and DINOv2 had higher accuracy for female samples. DINOv2 also showed improved performance when freezing parameters. In conclusion, Transformer-based models and DINOv2 are effective for Cushing’s syndrome classification. </p>
<blockquote>
<p>库欣综合征是由肾上腺皮质过度分泌糖皮质激素引起的疾病，通常表现为满月脸和水牛背，使得面部数据对诊断至关重要。以往的研究使用预训练的卷积神经网络（CNNs）通过正面面部图像诊断库欣综合征。然而，CNN更擅长捕捉局部特征，而库欣综合征通常表现为面部整体特征。基于Transformer的模型，如ViT和SWIN，利用自注意力机制，可以更好地捕捉长距离依赖关系和全局特征。最近，基于视觉Transformer的DINOv2这一基础模型引起了人们的关注。本研究比较了包括CNN、基于Transformer的模型和DINOv2在内的各种预训练模型在诊断库欣综合征方面的性能。我们还分析了性别偏见以及冻结机制对DINOv2的影响。结果表明，基于Transformer的模型和DINOv2在库欣综合征诊断上的性能优于CNN，其中ViT的F1得分最高，达到85.74%。无论是预训练模型还是DINOv2，对女性样本的准确率都较高。冻结参数后，DINOv2的性能也有所提高。综上所述，基于Transformer的模型和DINOv2在库欣综合征分类中效果显著。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12023v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文研究了使用不同预训练模型诊断库欣综合征的性能，包括卷积神经网络（CNNs）、基于Transformer的模型和DINOv2模型。研究表明，基于Transformer的模型和DINOv2在诊断库欣综合征方面的性能优于CNNs，其中ViT获得了最高的F1分数。同时，研究发现女性样本的准确率更高，冻结机制对DINOv2的性能也有积极影响。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>库欣综合征是由肾上腺皮质过度分泌糖皮质激素引起的，面部数据对其诊断至关重要。</li>
<li>之前的研究使用预训练的卷积神经网络（CNNs）进行库欣综合征的诊断，但CNNs更擅长捕捉局部特征，而库欣综合征通常表现为全局面部特征。</li>
<li>基于Transformer的模型（如ViT和SWIN）使用自注意力机制，能更好地捕捉长距离依赖关系和全局特征。</li>
<li>DINOv2模型在诊断库欣综合征方面的性能得到了研究。</li>
<li>研究比较了包括CNNs、基于Transformer的模型和DINOv2在内的不同预训练模型在诊断库欣综合征方面的性能。</li>
<li>基于Transformer的模型和DINOv2在诊断库欣综合征方面优于CNNs，ViT获得最高F1分数85.74%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12023">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-751e018f687da6e99ecaab58d9da0bec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c482f15366faa437a99efabf1361c8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d183cb374151a56556ea7508e8858e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-420556e05427d348fffc3d92b25f4e73.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-generalizable-3D-framework-and-model-for-self-supervised-learning-in-medical-imaging"><a href="#A-generalizable-3D-framework-and-model-for-self-supervised-learning-in-medical-imaging" class="headerlink" title="A generalizable 3D framework and model for self-supervised learning in   medical imaging"></a>A generalizable 3D framework and model for self-supervised learning in   medical imaging</h2><p><strong>Authors:Tony Xu, Sepehr Hosseini, Chris Anderson, Anthony Rinaldi, Rahul G. Krishnan, Anne L. Martel, Maged Goubran</strong></p>
<p>Current self-supervised learning methods for 3D medical imaging rely on simple pretext formulations and organ- or modality-specific datasets, limiting their generalizability and scalability. We present 3DINO, a cutting-edge SSL method adapted to 3D datasets, and use it to pretrain 3DINO-ViT: a general-purpose medical imaging model, on an exceptionally large, multimodal, and multi-organ dataset of ~100,000 3D medical imaging scans from over 10 organs. We validate 3DINO-ViT using extensive experiments on numerous medical imaging segmentation and classification tasks. Our results demonstrate that 3DINO-ViT generalizes across modalities and organs, including out-of-distribution tasks and datasets, outperforming state-of-the-art methods on the majority of evaluation metrics and labeled dataset sizes. Our 3DINO framework and 3DINO-ViT will be made available to enable research on 3D foundation models or further finetuning for a wide range of medical imaging applications. </p>
<blockquote>
<p>当前用于三维医学影像的自我监督学习方法依赖于简单的预文本制定和特定器官或特定模态的数据集，这限制了其通用性和可扩展性。我们提出了适用于三维数据集的尖端SSL方法3DINO，并使用它来预训练面向医学影像的三维通用模型3DINO-ViT。该模型在一个包含超过十万个来自超过十个器官的三维医学影像扫描的大型多模态多器官数据集上进行训练。我们通过大量医学影像分割和分类任务的实验验证了3DINO-ViT的有效性。我们的结果表明，无论是在跨模态和器官的情况下，还是在离群任务和数据集的情况下，该模型都表现出很强的泛化能力，并在大多数评估指标和标注数据集大小上优于最新方法。我们的3DINO框架和3DINO-ViT模型将开放给公众使用，以供进行关于三维基础模型的研究，或对一系列医学影像应用进行进一步微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11755v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对3D数据集的自监督学习方法3DINO，并利用其预训练了一个通用的医学影像模型3DINO-ViT。该模型在多模态、多器官的大型数据集上进行训练，可以在众多医学影像分割和分类任务中进行验证。实验结果证明了该模型在跨模态和器官、包括超出分布的任务和数据集上的泛化能力，并在多数评估指标和标注数据集大小上优于现有方法。本文提供的3DINO框架和3DINO-ViT将促进对3D基础模型的研究及其在医学影像应用中的进一步微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前自监督学习方法在3D医学影像处理上存在局限性，需要一种适用于大规模数据集的方法。</li>
<li>提出了一种名为3DINO的自监督学习方法，专门用于处理大规模的三维数据集。此方法特别适用于医学影像数据。</li>
<li>利用3DINO方法预训练了一个医学影像模型——3DINO-ViT，旨在实现多模态和多器官的泛化能力。该模型训练数据集规模庞大，包括超过百万个医学扫描样本。</li>
<li>通过一系列医学影像分割和分类任务验证了模型的性能。证明了该模型在跨模态和器官、不同数据集上的泛化能力优于当前先进技术。实验结果显示出该模型在不同评估指标上的优势。此外，模型在标注数据集大小上表现优越。这表明即使在有限的数据条件下，该模型也能表现出良好的性能。</li>
<li>该研究提供的模型框架和预训练模型（即3DINO-ViT）将促进后续研究，为医学图像分析领域提供强大的工具，包括用于构建更强大的基础模型或针对特定任务的微调等应用。这表明该研究具有广泛的应用前景和潜在价值。该研究为医学影像分析领域开辟了新的可能性，包括提高诊断准确性、推动个性化医疗等方面的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11755">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-420e208eac7c2820ad8f56b7fd587cb6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MedFILIP-Medical-Fine-grained-Language-Image-Pre-training"><a href="#MedFILIP-Medical-Fine-grained-Language-Image-Pre-training" class="headerlink" title="MedFILIP: Medical Fine-grained Language-Image Pre-training"></a>MedFILIP: Medical Fine-grained Language-Image Pre-training</h2><p><strong>Authors:Xinjie Liang, Xiangyu Li, Fanding Li, Jie Jiang, Qing Dong, Wei Wang, Kuanquan Wang, Suyu Dong, Gongning Luo, Shuo Li</strong></p>
<p>Medical vision-language pretraining (VLP) that leverages naturally-paired medical image-report data is crucial for medical image analysis. However, existing methods struggle to accurately characterize associations between images and diseases, leading to inaccurate or incomplete diagnostic results. In this work, we propose MedFILIP, a fine-grained VLP model, introduces medical image-specific knowledge through contrastive learning, specifically: 1) An information extractor based on a large language model is proposed to decouple comprehensive disease details from reports, which excels in extracting disease deals through flexible prompt engineering, thereby effectively reducing text complexity while retaining rich information at a tiny cost. 2) A knowledge injector is proposed to construct relationships between categories and visual attributes, which help the model to make judgments based on image features, and fosters knowledge extrapolation to unfamiliar disease categories. 3) A semantic similarity matrix based on fine-grained annotations is proposed, providing smoother, information-richer labels, thus allowing fine-grained image-text alignment. 4) We validate MedFILIP on numerous datasets, e.g., RSNA-Pneumonia, NIH ChestX-ray14, VinBigData, and COVID-19. For single-label, multi-label, and fine-grained classification, our model achieves state-of-the-art performance, the classification accuracy has increased by a maximum of 6.69%. The code is available in <a target="_blank" rel="noopener" href="https://github.com/PerceptionComputingLab/MedFILIP">https://github.com/PerceptionComputingLab/MedFILIP</a>. </p>
<blockquote>
<p>医疗视觉语言预训练（VLP）利用自然配对的医疗图像报告数据对医疗图像分析至关重要。然而，现有方法很难准确刻画图像与疾病之间的关联，导致诊断结果不准确或不完整。在这项工作中，我们提出了MedFILIP，这是一种精细的VLP模型，通过对比学习引入医疗图像特定知识，具体包括：1）提出一种基于大型语言模型的信息提取器，从报告中解耦出综合疾病细节，该提取器擅长通过灵活的提示工程提取疾病信息，从而有效减少文本复杂性，同时以微小成本保留丰富信息。2）提出了一种构建类别与视觉属性之间关系的知识注入器，帮助模型根据图像特征进行判断，并促进对不熟悉疾病类别的知识推断。3）提出了一种基于精细注释的语义相似度矩阵，提供更平滑、信息更丰富的标签，从而实现精细的图像文本对齐。4）我们在多个数据集上验证了MedFILIP的效果，例如RSNA-Pneumonia、NIH ChestX-ray14、VinBigData和COVID-19。对于单标签、多标签和精细分类，我们的模型实现了最先进的性能，分类准确率最高提高了6.69%。代码可在<a target="_blank" rel="noopener" href="https://github.com/PerceptionComputingLab/MedFILIP">https://github.com/PerceptionComputingLab/MedFILIP</a>中获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10775v1">PDF</a> 10 pages, 5 figures, IEEE Journal of Biomedical and Health   Informatics 2025</p>
<p><strong>Summary</strong>：<br>医疗领域视觉语言预训练（VLP）对医疗图像分析至关重要，但现有方法难以准确描述图像与疾病之间的关联，导致诊断结果不准确或不完整。本研究提出MedFILIP，一种精细的VLP模型，通过对比学习引入医疗图像特定知识。包括提出基于大型语言模型的信息提取器，有效减少文本复杂度并保留丰富信息；构建类别与视觉属性之间关系的知识注入器，帮助模型基于图像特征进行判断，并对未知疾病类别进行知识推断；以及基于精细注释的语义相似性矩阵，提供平滑、信息丰富的标签，实现精细的图像文本对齐。在多个数据集上验证，MedFILIP在单标签、多标签和精细分类任务上实现最佳性能，分类准确率最高提升6.69%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>医疗视觉语言预训练（VLP）在医疗图像分析中很重要。</li>
<li>现有方法难以准确描述图像与疾病之间的关联。</li>
<li>MedFILIP模型通过对比学习引入医疗图像特定知识。</li>
<li>信息提取器能够减少文本复杂度并保留丰富信息。</li>
<li>知识注入器帮助模型构建类别和视觉属性之间的关系。</li>
<li>语义相似性矩阵提供平滑、信息丰富的标签，实现图像文本精细对齐。</li>
<li>MedFILIP在多个数据集上实现最佳性能，分类准确率有所提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7ad8771602d4586eaae648c2fcc9cfc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5a2511ef41306695d832e1b91e7a584.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07eee1c23688c18b117e505534a9428a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee6f5d8b2953411468e94aebcc1f34d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f02ec7ff8fd06e534d755557d55e7fb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ClusterViG-Efficient-Globally-Aware-Vision-GNNs-via-Image-Partitioning"><a href="#ClusterViG-Efficient-Globally-Aware-Vision-GNNs-via-Image-Partitioning" class="headerlink" title="ClusterViG: Efficient Globally Aware Vision GNNs via Image Partitioning"></a>ClusterViG: Efficient Globally Aware Vision GNNs via Image Partitioning</h2><p><strong>Authors:Dhruv Parikh, Jacob Fein-Ashley, Tian Ye, Rajgopal Kannan, Viktor Prasanna</strong></p>
<p>Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have dominated the field of Computer Vision (CV). Graph Neural Networks (GNN) have performed remarkably well across diverse domains because they can represent complex relationships via unstructured graphs. However, the applicability of GNNs for visual tasks was unexplored till the introduction of Vision GNNs (ViG). Despite the success of ViGs, their performance is severely bottlenecked due to the expensive $k$-Nearest Neighbors ($k$-NN) based graph construction. Recent works addressing this bottleneck impose constraints on the flexibility of GNNs to build unstructured graphs, undermining their core advantage while introducing additional inefficiencies. To address these issues, in this paper, we propose a novel method called Dynamic Efficient Graph Convolution (DEGC) for designing efficient and globally aware ViGs. DEGC partitions the input image and constructs graphs in parallel for each partition, improving graph construction efficiency. Further, DEGC integrates local intra-graph and global inter-graph feature learning, enabling enhanced global context awareness. Using DEGC as a building block, we propose a novel CNN-GNN architecture, ClusterViG, for CV tasks. Extensive experiments indicate that ClusterViG reduces end-to-end inference latency for vision tasks by up to $5\times$ when compared against a suite of models such as ViG, ViHGNN, PVG, and GreedyViG, with a similar model parameter count. Additionally, ClusterViG reaches state-of-the-art performance on image classification, object detection, and instance segmentation tasks, demonstrating the effectiveness of the proposed globally aware learning strategy. Finally, input partitioning performed by DEGC enables ClusterViG to be trained efficiently on higher-resolution images, underscoring the scalability of our approach. </p>
<blockquote>
<p>卷积神经网络（CNN）和视觉转换器（ViT）在计算机视觉（CV）领域占据主导地位。图神经网络（GNN）在不同的领域表现优异，因为它们可以通过非结构化的图来表示复杂的关系。然而，直到引入视觉图神经网络（ViG），图神经网络在视觉任务上的应用尚未得到探索。尽管ViG取得了成功，但由于基于昂贵的k近邻（k-NN）的图构建，其性能受到了严重限制。最近的工作解决这个瓶颈对图神经网络的灵活性施加约束，以构建非结构化图，这削弱了其核心竞争力并引入了额外的效率问题。针对这些问题，本文提出了一种名为动态高效图卷积（DEGC）的新方法，用于设计高效且全局感知的ViG。DEGC对输入图像进行分区，并为每个分区并行构建图，提高图构建效率。此外，DEGC结合了局部图内和全局图间特征学习，增强了全局上下文感知能力。使用DEGC作为构建块，我们提出了用于计算机视觉任务的新型CNN-GNN架构ClusterViG。大量实验表明，与ViG、ViHGNN、PVG和GreedyViG等一系列模型相比，ClusterViG在视觉任务的端到端推理延迟方面减少了高达5倍，具有相似的模型参数计数。此外，ClusterViG在图像分类、对象检测和实例分割任务上达到了最新技术性能，证明了所提出的全局感知学习策略的有效性。最后，DEGC执行的输入分区使ClusterViG能够在更高的分辨率图像上进行高效训练，凸显了我们方法的可扩展性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10640v1">PDF</a> Preprint</p>
<p><strong>摘要</strong></p>
<p>本文提出一种名为动态高效图卷积（DEGC）的新方法，用于设计高效且全局感知的视力图神经网络（ViG）。该方法对输入图像进行分区，并行构建每个分区的图，提高图构建效率。DEGC结合了局部图内和全局图间特征学习，增强了全局上下文感知能力。使用DEGC作为构建块，本文提出了一种新型的卷积神经网络-图神经网络架构ClusterViG，用于计算机视觉任务。实验表明，与一系列模型相比，ClusterViG在端到端的视觉任务推理时间上减少了高达5倍，同时达到了图像分类、目标检测和实例分割任务的最新性能水平，证明了全局感知学习策略的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>论文提出了动态高效图卷积（DEGC）方法，用于设计更为高效的视力图神经网络（ViG）。</li>
<li>DEGC通过对输入图像进行分区并并行构建图，提高了图构建的效率。</li>
<li>DEGC结合了局部和全局特征学习，增强了全局上下文感知能力。</li>
<li>论文提出了新型的卷积神经网络-图神经网络架构ClusterViG。</li>
<li>与其他模型相比，ClusterViG在视觉任务推理时间上减少了高达5倍。</li>
<li>ClusterViG在图像分类、目标检测和实例分割任务上达到了最新性能水平。</li>
<li>输入图像的分区使得ClusterViG能够在更高的分辨率图像上进行有效的训练，显示了其可扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10640">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d4d154851b4c36b2781f21995fa2758f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8de65c83d46f984b24ad882d4805654.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Vision Transformer/2501.10640v1/page_3_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-664d50c5c1fca2b3b751663d6277b493.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0f9ac1bc0bb8b9d3f8e791e35256e63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e63302ca9dc3ae83c227f057c8a3926e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="IDEA-Image-Description-Enhanced-CLIP-Adapter"><a href="#IDEA-Image-Description-Enhanced-CLIP-Adapter" class="headerlink" title="IDEA: Image Description Enhanced CLIP-Adapter"></a>IDEA: Image Description Enhanced CLIP-Adapter</h2><p><strong>Authors:Zhipeng Ye, Feng Jiang, Qiufeng Wang, Kaizhu Huang, Jiaqi Huang</strong></p>
<p>CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model’s performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named “IMD-11”. Our code and data are released at <a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA">https://github.com/FourierAI/IDEA</a>. </p>
<blockquote>
<p>CLIP（对比语言图像预训练）在模式识别和计算机视觉领域取得了巨大的成功。将CLIP应用于下游任务（例如零样本或小样本分类）是多模态学习的热门话题。然而，当前的研究主要关注文本提示学习或视觉适配器调整，而没有充分利用图像文本对之间的互补信息和关联。在本文中，我们提出了一种图像描述增强CLIP适配器（IDEA）方法，将CLIP适应于小样本图像分类任务。该方法通过利用图像的视觉特征和文本描述来捕捉精细特征。IDEA是一种无需训练的CLIP方法，它在多个任务上的表现可与最先进的模型相当，甚至超过它们。此外，我们引入了可训练的IDEA（T-IDEA），它通过添加两个轻量级的学习组件（即投影器和可学习的潜在空间）来扩展IDEA，进一步提高了模型的性能，并在11个数据集上实现了最新结果。作为一项重要贡献，我们采用了Llama模型，并设计了一个全面的流程来生成这11个数据集的图像文本描述，共生成了1,637,795个图像文本对，名为“IMD-11”。我们的代码和数据已在<a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/FourierAI/IDEA上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08816v2">PDF</a> </p>
<p><strong>Summary</strong><br>CLIP在多模态学习中备受关注，本文提出了一种基于图像描述增强的CLIP适配器（IDEA）方法，用于适应少量图像分类任务。该方法利用视觉特征和图像文本描述来捕捉精细特征，无需训练CLIP即可与最新模型相比甚至超越。此外，本文还介绍了可训练的IDEA（T-IDEA），通过添加两个轻量级的学习组件（投影仪和学习潜在空间）进一步提高性能，并在11个数据集上达到最新结果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIP在模式识别和计算机视觉方面取得了巨大成功，在多模态学习中受到广泛关注。</li>
<li>当前研究主要关注文本提示学习或视觉适配器调整，但未充分利用图像文本对之间的互补信息和相关性。</li>
<li>IDEA方法通过结合视觉特征和图像文本描述，适应少量图像分类任务，捕捉精细特征。</li>
<li>IDEA是一种无需训练的CLIP方法，可在多个任务上与最新模型相比甚至超越。</li>
<li>T-IDEA的引入进一步提高了IDEA的性能，通过在IDEA的基础上添加两个轻量级的学习组件（投影仪和学习潜在空间）。</li>
<li>在11个数据集上实现了最新结果，并释放了代码和数据。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08816">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-98efde1f62194ce4925ee67837cc4d2c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ee0029e3fce28293a8ddc11aa8fd0bf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM"><a href="#UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM" class="headerlink" title="UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM"></a>UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM</h2><p><strong>Authors:Xuhui Guo, Tanmoy Dam, Rohan Dhamdhere, Gourav Modanwal, Anant Madabhushi</strong></p>
<p>3D medical image segmentation has progressed considerably due to Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these methods struggle to balance long-range dependency acquisition with computational efficiency. To address this challenge, we propose UNETVL (U-Net Vision-LSTM), a novel architecture that leverages recent advancements in temporal information processing. UNETVL incorporates Vision-LSTM (ViL) for improved scalability and memory functions, alongside an efficient Chebyshev Kolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency patterns more effectively. We validated our method on the ACDC and AMOS2022 (post challenge Task 2) benchmark datasets, showing a significant improvement in mean Dice score compared to recent state-of-the-art approaches, especially over its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS, respectively. Extensive ablation studies were conducted to demonstrate the impact of each component in UNETVL, providing a comprehensive understanding of its architecture. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL">https://github.com/tgrex6/UNETVL</a>, facilitating further research and applications in this domain. </p>
<blockquote>
<p>3D医学图像分割由于卷积神经网络（CNN）和视觉转换器（ViT）的发展而取得了显著进展，但这些方法在平衡远程依赖获取与计算效率方面遇到了困难。为了应对这一挑战，我们提出了UNETVL（U-Net Vision-LSTM）这一新型架构，该架构利用最新的时序信息处理技术的优势。UNETVL结合了视觉LSTM（ViL）以提高可扩展性和内存功能，同时采用高效的切比雪夫-柯尔莫哥洛夫-阿诺尔德网络（KAN），更有效地处理复杂且远程依赖模式。我们在ACDC和AMOS2022（后挑战任务2）基准数据集上验证了我们的方法，相较于最新的最先进方法，特别是在其前身UNETR上，Dice系数的平均得分有了显著提高，ACDC上提高了7.3%，AMOS上提高了15.6%。进行了广泛的消融研究，以展示UNETVL中每个组件的影响，对其架构提供了全面的理解。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL%E6%89%BE%E5%88%B0%EF%BC%8C%E4%B8%BA%E8%BF%99%E4%B8%80%E9%A2%86%E5%9F%9F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BA%94%E7%94%A8%E6%8F%90%E4%BE%9B%E4%BA%86%E4%BE%BF%E5%88%A9%E3%80%82">https://github.com/tgrex6/UNETVL找到，为这一领域的进一步研究和应用提供了便利。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07017v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的医学图像分割架构UNETVL（U-Net Vision-LSTM），结合了卷积神经网络（CNNs）和视觉转换器（ViTs）的优势，通过引入Vision-LSTM增强可扩展性和记忆功能，并使用高效的切比雪夫柯尔莫哥洛夫-阿诺尔德网络（KAN）更有效地处理复杂和远程依赖模式。在ACDC和AMOS2022基准数据集上验证，该方法相对于最新先进技术有显著提升，特别是在ACDC上提升了7.3%，在AMOS上提升了15.6%。详尽的消融研究展示了UNETVL各组件的影响，为其架构提供了全面的理解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UNETVL结合了CNN和ViT的优势，针对医学图像分割领域进行了创新设计。</li>
<li>Vision-LSTM的引入增强了模型的可扩展性和记忆功能。</li>
<li>切比雪夫柯尔莫哥洛夫-阿诺尔德网络（KAN）能更有效地处理复杂的远程依赖模式。</li>
<li>在ACDC和AMOS2022基准数据集上的实验结果证明了该方法的优越性。</li>
<li>与最新技术相比，UNETVL在ACDC和AMOS数据集上的表现均有显著提升。</li>
<li>消融研究展示了UNETVL架构中各个组件的重要性和影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07017">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9d7c8c699d1ff63f923f508555bda630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-878046a9be14a4344667b66bd6a4d7ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3871d2bf4438644024fcdf844b4760fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89d5f5cf8fff3dec48d4df4779011549.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Token-Turing-Machines-are-Efficient-Vision-Models"><a href="#Token-Turing-Machines-are-Efficient-Vision-Models" class="headerlink" title="Token Turing Machines are Efficient Vision Models"></a>Token Turing Machines are Efficient Vision Models</h2><p><strong>Authors:Purvish Jajal, Nick John Eliopoulos, Benjamin Shiue-Hal Chou, George K. Thiravathukal, James C. Davis, Yung-Hsiang Lu</strong></p>
<p>We propose Vision Token Turing Machines (ViTTM), an efficient, low-latency, memory-augmented Vision Transformer (ViT). Our approach builds on Neural Turing Machines and Token Turing Machines, which were applied to NLP and sequential visual understanding tasks. ViTTMs are designed for non-sequential computer vision tasks such as image classification and segmentation. Our model creates two sets of tokens: process tokens and memory tokens; process tokens pass through encoder blocks and read-write from memory tokens at each encoder block in the network, allowing them to store and retrieve information from memory. By ensuring that there are fewer process tokens than memory tokens, we are able to reduce the inference time of the network while maintaining its accuracy. On ImageNet-1K, the state-of-the-art ViT-B has median latency of 529.5ms and 81.0% accuracy, while our ViTTM-B is 56% faster (234.1ms), with 2.4 times fewer FLOPs, with an accuracy of 82.9%. On ADE20K semantic segmentation, ViT-B achieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model acheives a 45.17 mIoU with 26.8 FPS (+94%). </p>
<blockquote>
<p>我们提出了视觉令牌图灵机（Vision Token Turing Machines，简称ViTTM），这是一种高效、低延迟、增强记忆的视觉转换器（ViT）。我们的方法基于神经图灵机和令牌图灵机，后者被应用于自然语言处理和序列视觉理解任务。ViTTM被设计用于非序列计算机视觉任务，如图像分类和分割。我们的模型创建了两组令牌：处理令牌和内存令牌；处理令牌通过编码器块，并从网络中的每个编码器块读写内存令牌，允许它们从内存中存储和检索信息。通过确保处理令牌的数目少于内存令牌，我们能够在保持网络精度的同时减少其推理时间。在ImageNet-1K上，最先进的ViT-B具有529.5毫秒的中位延迟和81.0%的准确率，而我们的ViTTM-B更快（234.1毫秒），FLOPs减少了2.4倍，准确率达到了82.9%。在ADE20K语义分割任务上，ViT-B以每秒13.8帧的速度达到45.65 mIoU，而我们的ViTTM-B模型以每秒26.8帧的速度达到45.17 mIoU（+ 94%）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07613v2">PDF</a> Accepted to WACV 2025</p>
<p><strong>Summary</strong><br>     本文提出了Vision Token Turing Machines（ViTTM），这是一种高效、低延迟、带有记忆功能的Vision Transformer（ViT）。ViTTM借鉴了神经图灵机和令牌图灵机在NLP和视觉理解任务中的应用，专门用于非序列计算机视觉任务，如图像分类和分割。通过创建两组令牌：处理令牌和内存令牌，ViTTM能够在网络中从内存令牌读取和写入信息。通过确保处理令牌的数量少于内存令牌，我们能够在保持准确性的同时减少网络的推理时间。在ImageNet-1K上，ViTTM-B模型比先进的ViT-B模型更快、更有效率，并且准确性更高。在ADE20K语义分割任务上，ViTTM-B模型也表现出优异的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Token Turing Machines (ViTTM) 是一种针对计算机视觉任务的改进模型，基于 Vision Transformer（ViT）。</li>
<li>ViTTM借鉴了神经图灵机和令牌图灵机，特别适用于非序列计算机视觉任务，如图像分类和分割。</li>
<li>ViTTM通过创建处理令牌和内存令牌两组，能够在网络中存储和检索信息。</li>
<li>ViTTM能够减少网络的推理时间，同时通过确保处理令牌数量少于内存令牌来提高效率。</li>
<li>在ImageNet-1K图像分类任务上，ViTTM-B模型相比先进的ViT-B模型更快、更有效率，并且准确性更高。</li>
<li>在ADE20K语义分割任务上，ViTTM-B模型表现出优异的性能，与ViT-B模型相比具有更高的帧率和相似的mIoU。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07613">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e2dc88b89b28bfe513d666261a1ee613.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25d09a13e2364fb85f6046a574031586.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5aeb00e2f7b7e482626c579b794392a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9033f6d2ddc6be8ee9bcd9744830803e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56c4a182d76d8c7dda602d8210c0ee8d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NoteLLM-2-Multimodal-Large-Representation-Models-for-Recommendation"><a href="#NoteLLM-2-Multimodal-Large-Representation-Models-for-Recommendation" class="headerlink" title="NoteLLM-2: Multimodal Large Representation Models for Recommendation"></a>NoteLLM-2: Multimodal Large Representation Models for Recommendation</h2><p><strong>Authors:Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, Enhong Chen</strong></p>
<p>Large Language Models (LLMs) have demonstrated exceptional proficiency in text understanding and embedding tasks. However, their potential in multimodal representation, particularly for item-to-item (I2I) recommendations, remains underexplored. While leveraging existing Multimodal Large Language Models (MLLMs) for such tasks is promising, challenges arise due to their delayed release compared to corresponding LLMs and the inefficiency in representation tasks. To address these issues, we propose an end-to-end fine-tuning method that customizes the integration of any existing LLMs and vision encoders for efficient multimodal representation. Preliminary experiments revealed that fine-tuned LLMs often neglect image content. To counteract this, we propose NoteLLM-2, a novel framework that enhances visual information. Specifically, we propose two approaches: first, a prompt-based method that segregates visual and textual content, employing a multimodal In-Context Learning strategy to balance focus across modalities; second, a late fusion technique that directly integrates visual information into the final representations. Extensive experiments, both online and offline, demonstrate the effectiveness of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/NoteLLM">https://github.com/Applied-Machine-Learning-Lab/NoteLLM</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在文本理解和嵌入任务中表现出了卓越的能力。然而，它们在多模态表示中的潜力，特别是针对项目到项目（I2I）的推荐，仍然未被充分探索。虽然利用现有的多模态大型语言模型（MLLM）来完成此类任务是有前途的，但由于它们相较于对应的大型语言模型的发布存在延迟，以及在表示任务中的效率不高，因此仍面临挑战。为了解决这些问题，我们提出了一种端到端的微调方法，可以定制任何现有的大型语言模型和视觉编码器，以实现高效的多模态表示。初步实验表明，经过微调的大型语言模型往往会忽略图像内容。为了应对这一问题，我们提出了NoteLLM-2这一新型框架，以增强视觉信息。具体来说，我们提出了两种方法：首先，一种基于提示的方法，它分离视觉和文本内容，采用多模式上下文学习策略来平衡跨模态的焦点；其次，一种晚期融合技术，它直接将视觉信息融合到最终表示中。线上和线下的广泛实验都证明了我们方法的有效性。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/NoteLLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/NoteLLM获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16789v2">PDF</a> Accepted by KDD’25 ADS track</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在文本理解和嵌入任务上表现出卓越的能力，但在多模态表示，尤其是物品到物品（I2I）推荐方面，其潜力尚未得到充分探索。为了利用现有的多模态大型语言模型（MLLMs）应对这些挑战，我们提出了一种端到端微调方法，该方法可定制任何现有的LLMs和视觉编码器，以实现高效的多模态表示。我们的新方法NoteLLM-2通过两种新策略来增强视觉信息：一是基于提示的方法，它通过分割视觉和文本内容，采用多模态上下文学习策略来平衡跨模态的关注度；二是后期融合技术，它直接将视觉信息集成到最终表示中。经过在线和离线的大量实验验证，我们的方法效果显著。相关代码已发布在：<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/NoteLLM%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/NoteLLM。</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在多模态表示方面潜力巨大，但在物品到物品推荐等特定任务上的研究仍然不足。</li>
<li>利用现有的多模态大型语言模型进行多任务处理面临挑战，如延迟发布和表示任务效率问题。</li>
<li>提出了一种端到端微调方法，旨在定制LLMs和视觉编码器，以实现高效的多模态表示。这是通过整合任何现有的LLMs和视觉编码器来实现的。</li>
<li>新方法NoteLLM-2旨在增强视觉信息，通过两种策略：基于提示的方法和后期融合技术。</li>
<li>基于提示的方法通过分割视觉和文本内容，并采用多模态上下文学习策略来平衡跨模态关注度。</li>
<li>后期融合技术直接集成视觉信息到最终表示中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16789">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f0567255a0bfa3131897ad071481a03f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17bae8445b79f7aaa2aa43b8ee1e7556.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3844dc8eddeca9c7d3bc9c979c7d1da1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cc498aef04ec38629810f038a08e728.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb4f4803ae534c8665068f76324c831c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-277744f779dffa7b09efc773b92de1dd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e1a869d029f8dfbef879e0e9fc6d50f9.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-01-23  Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2305bace8fd1a620c2df7ba09faba2f0.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-01-23  MMVU Measuring Expert-Level Multi-Discipline Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
