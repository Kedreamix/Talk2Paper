<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  UI-TARS Pioneering Automated GUI Interaction with Native Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0f77fec60f654cdb6c143df0db2e8469.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-23-æ›´æ–°"><a href="#2025-01-23-æ›´æ–°" class="headerlink" title="2025-01-23 æ›´æ–°"></a>2025-01-23 æ›´æ–°</h1><h2 id="UI-TARS-Pioneering-Automated-GUI-Interaction-with-Native-Agents"><a href="#UI-TARS-Pioneering-Automated-GUI-Interaction-with-Native-Agents" class="headerlink" title="UI-TARS: Pioneering Automated GUI Interaction with Native Agents"></a>UI-TARS: Pioneering Automated GUI Interaction with Native Agents</h2><p><strong>Authors:Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, Guang Shi</strong></p>
<p>This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†UI-TARSï¼Œè¿™æ˜¯ä¸€ç§åŸç”ŸGUIä»£ç†æ¨¡å‹ï¼Œå®ƒä»…å°†å±å¹•æˆªå›¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ‰§è¡Œç±»ä¼¼äººç±»çš„äº¤äº’ï¼ˆä¾‹å¦‚ï¼Œé”®ç›˜å’Œé¼ æ ‡æ“ä½œï¼‰ã€‚ä¸åŒäºä¾èµ–å¤§é‡å°è£…çš„å•†ä¸šæ¨¡å‹ï¼ˆä¾‹å¦‚GPT-4oï¼‰çš„æµè¡Œä»£ç†æ¡†æ¶ï¼Œéœ€è¦ä¸“å®¶åˆ¶ä½œçš„æç¤ºå’Œå·¥ä½œæµç¨‹ï¼ŒUI-TARSæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œè¡¨ç°å‡ºè¶…è¿‡è¿™äº›å¤æ‚æ¡†æ¶çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜äº†å…¶å“è¶Šçš„æ€§èƒ½ï¼šUI-TARSåœ¨10å¤šä¸ªGUIä»£ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ„ŸçŸ¥ã€æ¥åœ°å’ŒGUIä»»åŠ¡æ‰§è¡Œæ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUI-TARSåœ¨50æ­¥æ—¶å¾—åˆ†ä¸º24.6ï¼Œåœ¨15æ­¥æ—¶å¾—åˆ†ä¸º22.7ï¼Œè¶…è¿‡äº†Claudeï¼ˆåˆ†åˆ«ä¸º22.0å’Œ14.9ï¼‰ã€‚åœ¨AndroidWorldä¸­ï¼ŒUI-TARSå®ç°46.6ï¼Œè¶…è¿‡äº†GPT-4oçš„34.5ã€‚UI-TARSèåˆäº†å‡ ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰å¢å¼ºæ„ŸçŸ¥ï¼šåˆ©ç”¨å¤§è§„æ¨¡çš„GUIå±å¹•æˆªå›¾æ•°æ®é›†è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„UIå…ƒç´ ç†è§£å’Œç²¾ç¡®æè¿°ï¼›ï¼ˆ2ï¼‰ç»Ÿä¸€åŠ¨ä½œå»ºæ¨¡ï¼Œå°†åŠ¨ä½œæ ‡å‡†åŒ–åˆ°ç»Ÿä¸€çš„ç©ºé—´å¹³å°ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡çš„åŠ¨ä½œè½¨è¿¹å®ç°ç²¾ç¡®æ¥åœ°å’Œäº¤äº’ï¼›ï¼ˆ3ï¼‰ç³»ç»Ÿ2æ¨ç†ï¼Œå°†æ·±æ€ç†Ÿè™‘çš„æ¨ç†èå…¥å¤šæ­¥éª¤å†³ç­–åˆ¶å®šä¸­ï¼Œæ¶‰åŠä»»åŠ¡åˆ†è§£ã€åæ€æ€è€ƒã€é‡Œç¨‹ç¢‘è¯†åˆ«ç­‰å¤šç§æ¨ç†æ¨¡å¼ï¼›ï¼ˆ4ï¼‰é€šè¿‡è¿­ä»£è®­ç»ƒä¸åæ€åœ¨çº¿è½¨è¿¹æ¥è§£å†³æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œè‡ªåŠ¨æ”¶é›†ã€è¿‡æ»¤å’Œåæ€åœ°ç²¾ç‚¼æ•°ç™¾å°è™šæ‹Ÿæœºä¸Šçš„æ–°äº¤äº’è½¨è¿¹ã€‚é€šè¿‡è¿­ä»£è®­ç»ƒå’Œåæ€è°ƒæ•´ï¼ŒUI-TARSä¸æ–­ä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œå¹¶é€‚åº”æ„å¤–æƒ…å†µï¼Œæœ€å°‘éœ€è¦äººå·¥å¹²é¢„ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†GUIä»£ç†çš„å‘å±•è·¯å¾„ï¼Œä»¥æŒ‡å¯¼è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12326v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºUI-TARSçš„åŸç”ŸGUIä»£ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»…é€šè¿‡æˆªå›¾ä½œä¸ºè¾“å…¥ï¼Œæ‰§è¡Œç±»ä¼¼äººç±»çš„äº¤äº’æ“ä½œï¼ˆå¦‚é”®ç›˜å’Œé¼ æ ‡æ“ä½œï¼‰ã€‚ä¸ä¾èµ–é«˜åº¦åŒ…è£…çš„å•†ä¸šæ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰çš„ç°æœ‰ä»£ç†æ¡†æ¶ä¸åŒï¼ŒUI-TARSæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œåœ¨è¶…è¿‡10ä¸ªGUIä»£ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨OSWorldå’ŒAndroidWorldåŸºå‡†æµ‹è¯•ä¸­ã€‚UI-TARSçš„å…³é”®åˆ›æ–°åŒ…æ‹¬å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ã€ç»Ÿä¸€åŠ¨ä½œå»ºæ¨¡ã€ç³»ç»Ÿ2æ¨ç†å’Œè¿­ä»£è®­ç»ƒä¸åæ€è°ƒè¯•ã€‚
 
**Key Takeaways**
 
1. UI-TARSæ˜¯ä¸€ä¸ªåŸç”ŸGUIä»£ç†æ¨¡å‹ï¼Œé€šè¿‡æˆªå›¾ä½œä¸ºè¾“å…¥å¹¶æ‰§è¡Œç±»ä¼¼äººç±»çš„äº¤äº’æ“ä½œã€‚
2. UI-TARSåœ¨å¤šä¸ªGUIä»£ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨OSWorldå’ŒAndroidWorldæµ‹è¯•ä¸­ã€‚
3. UI-TARSå…·æœ‰å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ï¼Œåˆ©ç”¨å¤§è§„æ¨¡çš„GUIæˆªå›¾æ•°æ®é›†è¿›è¡ŒUIå…ƒç´ çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç†è§£å’Œç²¾ç¡®æè¿°ã€‚
4. ç»Ÿä¸€åŠ¨ä½œå»ºæ¨¡æ˜¯UI-TARSçš„ä¸€ä¸ªå…³é”®åˆ›æ–°ï¼Œå®ƒæ ‡å‡†åŒ–äº†è·¨å¹³å°çš„æ“ä½œï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡çš„åŠ¨ä½œè½¨è¿¹å®ç°äº†ç²¾ç¡®çš„æ¥åœ°å’Œäº¤äº’ã€‚
5. UI-TARSé‡‡ç”¨ç³»ç»Ÿ2æ¨ç†ï¼Œå°†æ·±æ€ç†Ÿè™‘çš„æ¨ç†èå…¥å¤šæ­¥éª¤å†³ç­–åˆ¶å®šï¼Œæ¶‰åŠä»»åŠ¡åˆ†è§£ã€åæ€æ€è€ƒã€é‡Œç¨‹ç¢‘è¯†åˆ«ç­‰ã€‚
6. UI-TARSé€šè¿‡è¿­ä»£è®­ç»ƒä¸åæ€è°ƒè¯•è§£å†³äº†æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ”¶é›†ã€è¿‡æ»¤å’Œåæ€å®Œå–„æ–°çš„äº¤äº’è½¨è¿¹ã€‚
7. UI-TARSèƒ½ä»é”™è¯¯ä¸­å­¦ä¹ å¹¶é€‚åº”ä¸å¯é¢„è§çš„æƒ…å†µï¼Œå¹¶ä¸”éœ€è¦æœ€å°‘çš„äººå·¥å¹²é¢„ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4800e65a525f8355b2c8eb145cf53a3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dfe4227cf271fe744e380282eaa5197.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="mmCooper-A-Multi-agent-Multi-stage-Communication-efficient-and-Collaboration-robust-Cooperative-Perception-Framework"><a href="#mmCooper-A-Multi-agent-Multi-stage-Communication-efficient-and-Collaboration-robust-Cooperative-Perception-Framework" class="headerlink" title="mmCooper: A Multi-agent Multi-stage Communication-efficient and   Collaboration-robust Cooperative Perception Framework"></a>mmCooper: A Multi-agent Multi-stage Communication-efficient and   Collaboration-robust Cooperative Perception Framework</h2><p><strong>Authors:Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, Libing Wu</strong></p>
<p>Collaborative perception significantly enhances individual vehicle perception performance through the exchange of sensory information among agents. However, real-world deployment faces challenges due to bandwidth constraints and inevitable calibration errors during information exchange. To address these issues, we propose mmCooper, a novel multi-agent, multi-stage, communication-efficient, and collaboration-robust cooperative perception framework. Our framework leverages a multi-stage collaboration strategy that dynamically and adaptively balances intermediate- and late-stage information to share among agents, enhancing perceptual performance while maintaining communication efficiency. To support robust collaboration despite potential misalignments and calibration errors, our framework captures multi-scale contextual information for robust fusion in the intermediate stage and calibrates the received detection results to improve accuracy in the late stage. We validate the effectiveness of mmCooper through extensive experiments on real-world and simulated datasets. The results demonstrate the superiority of our proposed framework and the effectiveness of each component. </p>
<blockquote>
<p>åä½œæ„ŸçŸ¥é€šè¿‡å„æ™ºèƒ½ä½“ä¹‹é—´çš„æ„ŸçŸ¥ä¿¡æ¯äº¤æ¢ï¼Œå¯ä»¥æ˜¾è‘—æå‡ä¸ªä½“è½¦è¾†çš„æ„ŸçŸ¥æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´ç€å¸¦å®½çº¦æŸä»¥åŠä¿¡æ¯ä¼ é€’è¿‡ç¨‹ä¸­ä¸å¯é¿å…çš„æ ¡å‡†é”™è¯¯ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†mmCooperï¼Œä¸€ä¸ªæ–°å‹çš„å¤šæ™ºèƒ½ä½“ã€å¤šé˜¶æ®µã€é«˜æ•ˆé€šä¿¡å’Œç¨³å¥åä½œçš„æ„ŸçŸ¥æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨å¤šé˜¶æ®µåä½œç­–ç•¥ï¼ŒåŠ¨æ€åœ°è‡ªé€‚åº”åœ°å¹³è¡¡ä¸­é—´é˜¶æ®µå’ŒåæœŸé˜¶æ®µçš„ä¿¡æ¯å…±äº«ï¼Œä»¥æé«˜æ„ŸçŸ¥æ€§èƒ½å¹¶ä¿æŒé€šä¿¡æ•ˆç‡ã€‚ä¸ºäº†æ”¯æŒåœ¨æ½œåœ¨çš„ä¸å¯¹é½å’Œæ ¡å‡†é”™è¯¯æƒ…å†µä¸‹çš„ç¨³å¥åä½œï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸­é—´é˜¶æ®µæ•è·å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥å®ç°ç¨³å¥èåˆï¼Œå¹¶åœ¨åæœŸé˜¶æ®µæ ¡å‡†æ¥æ”¶åˆ°çš„æ£€æµ‹ç»“æœä»¥æé«˜å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒå¯¹çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿæ•°æ®é›†éªŒè¯äº†mmCooperçš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜æˆ‘ä»¬æå‡ºçš„æ¡†æ¶åŠå…¶å„ç»„ä»¶çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12263v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ååŒæ„ŸçŸ¥é€šè¿‡å„æ™ºèƒ½ä½“ä¹‹é—´äº¤æ¢æ„ŸçŸ¥ä¿¡æ¯ï¼Œå¯æ˜¾è‘—æå‡ä¸ªä½“è½¦è¾†çš„æ„ŸçŸ¥æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®é™…éƒ¨ç½²ä¸­é¢ä¸´å¸¦å®½é™åˆ¶å’Œä¿¡æ¯äº¤æ¢ä¸­çš„æ ¡å‡†è¯¯å·®ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†mmCooperï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šæ™ºèƒ½ä½“ã€å¤šé˜¶æ®µã€é€šä¿¡é«˜æ•ˆã€åä½œç¨³å¥çš„ååŒæ„ŸçŸ¥æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¤šé˜¶æ®µåä½œç­–ç•¥ï¼ŒåŠ¨æ€å¹³è¡¡ä¸­é—´é˜¶æ®µå’ŒåæœŸé˜¶æ®µçš„ä¿¡æ¯å…±äº«ï¼Œä»è€Œæé«˜æ„ŸçŸ¥æ€§èƒ½å¹¶ä¿æŒé€šä¿¡æ•ˆç‡ã€‚å°½ç®¡å­˜åœ¨æ½œåœ¨çš„ä¸å¯¹é½å’Œæ ¡å‡†è¯¯å·®ï¼Œæˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡æ•è·ä¸­é—´é˜¶æ®µçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ”¯æŒç¨³å¥èåˆï¼Œå¹¶åœ¨åæœŸæ ¡å‡†æ£€æµ‹ç»“æœä»¥æé«˜å‡†ç¡®æ€§ã€‚é€šè¿‡åœ¨å®é™…å’Œæ¨¡æ‹Ÿæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†mmCooperçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶æ¡†æ¶åŠå…¶å„ç»„ä»¶çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ååŒæ„ŸçŸ¥èƒ½æé«˜è½¦è¾†æ„ŸçŸ¥æ€§èƒ½ï¼Œä½†éœ€è¦è§£å†³å¸¦å®½é™åˆ¶å’Œä¿¡æ¯äº¤æ¢æ ¡å‡†è¯¯å·®ç­‰æŒ‘æˆ˜ã€‚</li>
<li>mmCooperæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ã€å¤šé˜¶æ®µçš„ååŒæ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é€šä¿¡æ•ˆç‡å’Œæ„ŸçŸ¥æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŠ¨æ€å¹³è¡¡ä¿¡æ¯å…±äº«çš„ç­–ç•¥ï¼Œåœ¨å¤šä¸ªé˜¶æ®µè¿›è¡Œä¿¡æ¯èåˆå’Œæ ¡å‡†ã€‚</li>
<li>mmCooperèƒ½å¤„ç†æ½œåœ¨çš„ä¿¡æ¯ä¸å¯¹é½å’Œæ ¡å‡†è¯¯å·®é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä¸­é—´é˜¶æ®µçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ•è·æ”¯æŒç¨³å¥èåˆã€‚</li>
<li>åœ¨åæœŸé˜¶æ®µè¿›è¡Œç»“æœæ ¡å‡†ä»¥æé«˜å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e4a0a01ae2ee602923b4559f87decb3a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc6c8db17a2d7d3877592e6cf25248d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebbc89324f2a6a19f530578952fac4f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-562fbad2831d6174107540ca0e110ec0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RL-RC-DoT-A-Block-level-RL-agent-for-Task-Aware-Video-Compression"><a href="#RL-RC-DoT-A-Block-level-RL-agent-for-Task-Aware-Video-Compression" class="headerlink" title="RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression"></a>RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression</h2><p><strong>Authors:Uri Gadot, Assaf Shocher, Shie Mannor, Gal Chechik, Assaf Hallak</strong></p>
<p>Video encoders optimize compression for human perception by minimizing reconstruction error under bit-rate constraints. In many modern applications such as autonomous driving, an overwhelming majority of videos serve as input for AI systems performing tasks like object recognition or segmentation, rather than being watched by humans. It is therefore useful to optimize the encoder for a downstream task instead of for perceptual image quality. However, a major challenge is how to combine such downstream optimization with existing standard video encoders, which are highly efficient and popular. Here, we address this challenge by controlling the Quantization Parameters (QPs) at the macro-block level to optimize the downstream task. This granular control allows us to prioritize encoding for task-relevant regions within each frame. We formulate this optimization problem as a Reinforcement Learning (RL) task, where the agent learns to balance long-term implications of choosing QPs on both task performance and bit-rate constraints. Notably, our policy does not require the downstream task as an input during inference, making it suitable for streaming applications and edge devices such as vehicles. We demonstrate significant improvements in two tasks, car detection, and ROI (saliency) encoding. Our approach improves task performance for a given bit rate compared to traditional task agnostic encoding methods, paving the way for more efficient task-aware video compression. </p>
<blockquote>
<p>è§†é¢‘ç¼–ç å™¨é€šè¿‡æœ€å°åŒ–æ¯”ç‰¹ç‡çº¦æŸä¸‹çš„é‡å»ºè¯¯å·®ï¼Œé’ˆå¯¹äººç±»æ„ŸçŸ¥è¿›è¡Œä¼˜åŒ–å‹ç¼©ã€‚åœ¨è®¸å¤šç°ä»£åº”ç”¨ï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶ï¼‰ä¸­ï¼Œç»å¤§å¤šæ•°è§†é¢‘æ˜¯ä½œä¸ºæ‰§è¡Œå¯¹è±¡è¯†åˆ«æˆ–åˆ†å‰²ç­‰ä»»åŠ¡çš„AIç³»ç»Ÿçš„è¾“å…¥ï¼Œè€Œä¸æ˜¯ç”±äººç±»è§‚çœ‹ã€‚å› æ­¤ï¼Œä¼˜åŒ–ç¼–ç å™¨ä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡è€Œéæ„ŸçŸ¥å›¾åƒè´¨é‡æ˜¯æœ‰ç”¨çš„ã€‚ç„¶è€Œï¼Œä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•å°†è¿™ç§ä¸‹æ¸¸ä¼˜åŒ–ä¸ç°æœ‰çš„æ ‡å‡†è§†é¢‘ç¼–ç å™¨ç›¸ç»“åˆï¼Œåè€…æ•ˆç‡é«˜ä¸”å—æ¬¢è¿ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡æ§åˆ¶å®å—çº§çš„é‡åŒ–å‚æ•°ï¼ˆQPï¼‰æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä»¥ä¼˜åŒ–ä¸‹æ¸¸ä»»åŠ¡ã€‚è¿™ç§é¢—ç²’åº¦çš„æ§åˆ¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¼˜å…ˆå¯¹æ¯å¸§å†…ä¸ä»»åŠ¡ç›¸å…³çš„åŒºåŸŸè¿›è¡Œç¼–ç ã€‚æˆ‘ä»¬å°†è¿™ä¸€ä¼˜åŒ–é—®é¢˜è¡¨è¿°ä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»»åŠ¡ï¼Œä»£ç†åœ¨æ­¤å­¦ä¹ é€‰æ‹©é‡åŒ–å‚æ•°æ—¶å¦‚ä½•å¹³è¡¡é•¿æœŸå½±å“ä»»åŠ¡å’Œæ¯”ç‰¹ç‡çº¦æŸçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç­–ç•¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸éœ€è¦ä¸‹æ¸¸ä»»åŠ¡ä½œä¸ºè¾“å…¥ï¼Œå› æ­¤é€‚ç”¨äºæµåª’ä½“åº”ç”¨å’Œè¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚è½¦è¾†ï¼‰ã€‚æˆ‘ä»¬åœ¨æ±½è½¦æ£€æµ‹å’ŒROIï¼ˆæ˜¾è‘—æ€§ï¼‰ç¼–ç ä¸¤é¡¹ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¸ä¼ ç»Ÿä»»åŠ¡æ— å…³çš„ç¼–ç æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»™å®šæ¯”ç‰¹ç‡ä¸‹æé«˜äº†ä»»åŠ¡æ€§èƒ½ï¼Œä¸ºæ›´é«˜æ•ˆçš„é¢å‘ä»»åŠ¡çš„è§†é¢‘å‹ç¼©é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12216v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç°ä»£åº”ç”¨å¦‚è‡ªåŠ¨é©¾é©¶çš„è§†é¢‘ç¼–ç ä¼˜åŒ–æ–¹æ³•ã€‚ä¼ ç»Ÿè§†é¢‘ç¼–ç å™¨ä¸»è¦å…³æ³¨äººç±»æ„ŸçŸ¥çš„é‡å»ºè¯¯å·®æœ€å°åŒ–ï¼Œä½†åœ¨è®¸å¤šåº”ç”¨ä¸­è§†é¢‘æ˜¯AIç³»ç»Ÿçš„è¾“å…¥ï¼Œç”¨äºæ‰§è¡Œå¯¹è±¡è¯†åˆ«æˆ–åˆ†å‰²ç­‰ä»»åŠ¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§ç»“åˆä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–çš„è§†é¢‘ç¼–ç å™¨æ–¹æ³•ï¼Œé€šè¿‡æ§åˆ¶é‡åŒ–å‚æ•°ï¼ˆQPsï¼‰åœ¨å®å—çº§åˆ«è¿›è¡Œä¼˜åŒ–ï¼Œå…è®¸å¯¹ä»»åŠ¡ç›¸å…³åŒºåŸŸè¿›è¡Œç¼–ç ä¼˜å…ˆçº§è°ƒæ•´ã€‚è¯¥ç ”ç©¶å°†ä¼˜åŒ–é—®é¢˜è¡¨è¿°ä¸ºå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œå­¦ä¹ åœ¨é€‰æ‹©QPsæ—¶å¹³è¡¡é•¿æœŸä»»åŠ¡æ€§èƒ½å’Œæ¯”ç‰¹ç‡çº¦æŸã€‚æ–°æ–¹æ³•åœ¨è½¦è¾†æ£€æµ‹å’ŒROIï¼ˆæ˜¾è‘—æ€§ï¼‰ç¼–ç ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œä¸ºæé«˜ä»»åŠ¡æ„ŸçŸ¥çš„è§†é¢‘å‹ç¼©æ•ˆç‡æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç¼–ç å™¨ä¼˜åŒ–ä¸å†ä»…é’ˆå¯¹äººç±»æ„ŸçŸ¥ï¼Œè€Œæ˜¯é’ˆå¯¹ç°ä»£åº”ç”¨ä¸­çš„AIä»»åŠ¡ã€‚</li>
<li>é€šè¿‡æ§åˆ¶é‡åŒ–å‚æ•°ï¼ˆQPsï¼‰åœ¨å®å—çº§åˆ«è¿›è¡Œä¼˜åŒ–ï¼Œé€‚åº”ä¸‹æ¸¸ä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è¢«ç”¨äºå­¦ä¹ åœ¨é€‰æ‹©QPsæ—¶å¦‚ä½•å¹³è¡¡ä»»åŠ¡æ€§èƒ½å’Œæ¯”ç‰¹ç‡çº¦æŸã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºæµå¼åº”ç”¨å’Œè¾¹ç¼˜è®¾å¤‡ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶è½¦è¾†ã€‚</li>
<li>åœ¨è½¦è¾†æ£€æµ‹å’ŒROIç¼–ç ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„ä»»åŠ¡æ€§èƒ½æå‡ã€‚</li>
<li>ä¸ä¼ ç»Ÿä»»åŠ¡æ— å…³çš„è§†é¢‘ç¼–ç æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç»™å®šæ¯”ç‰¹ç‡ä¸‹æé«˜äº†ä»»åŠ¡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30b5ed0f1029a28ef5913b223ede1643.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5be5fea155636f866c43bcbbdbb71bbb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Tackling-Uncertainties-in-Multi-Agent-Reinforcement-Learning-through-Integration-of-Agent-Termination-Dynamics"><a href="#Tackling-Uncertainties-in-Multi-Agent-Reinforcement-Learning-through-Integration-of-Agent-Termination-Dynamics" class="headerlink" title="Tackling Uncertainties in Multi-Agent Reinforcement Learning through   Integration of Agent Termination Dynamics"></a>Tackling Uncertainties in Multi-Agent Reinforcement Learning through   Integration of Agent Termination Dynamics</h2><p><strong>Authors:Somnath Hazra, Pallab Dasgupta, Soumyajit Dey</strong></p>
<p>Multi-Agent Reinforcement Learning (MARL) has gained significant traction for solving complex real-world tasks, but the inherent stochasticity and uncertainty in these environments pose substantial challenges to efficient and robust policy learning. While Distributional Reinforcement Learning has been successfully applied in single-agent settings to address risk and uncertainty, its application in MARL is substantially limited. In this work, we propose a novel approach that integrates distributional learning with a safety-focused loss function to improve convergence in cooperative MARL tasks. Specifically, we introduce a Barrier Function based loss that leverages safety metrics, identified from inherent faults in the system, into the policy learning process. This additional loss term helps mitigate risks and encourages safer exploration during the early stages of training. We evaluate our method in the StarCraft II micromanagement benchmark, where our approach demonstrates improved convergence and outperforms state-of-the-art baselines in terms of both safety and task completion. Our results suggest that incorporating safety considerations can significantly enhance learning performance in complex, multi-agent environments. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è§£å†³å¤æ‚çš„çœŸå®ä¸–ç•Œä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›ç¯å¢ƒä¸­å›ºæœ‰çš„éšæœºæ€§å’Œä¸ç¡®å®šæ€§ç»™æœ‰æ•ˆå’Œç¨³å¥çš„ç­–ç•¥å­¦ä¹ å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ åœ¨å•æ™ºèƒ½ä½“ç¯å¢ƒä¸­å·²è¢«æˆåŠŸåº”ç”¨äºè§£å†³é£é™©å’Œä¸ç¡®å®šæ€§é—®é¢˜ï¼Œä½†å…¶åœ¨MARLä¸­çš„åº”ç”¨å—åˆ°äº†å¾ˆå¤§çš„é™åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†åˆ†å¸ƒå­¦ä¹ ä¸ä»¥å®‰å…¨ä¸ºé‡ç‚¹çš„æŸå¤±å‡½æ•°ç›¸ç»“åˆçš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜åˆä½œMARLä»»åŠ¡çš„æ”¶æ•›æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºéšœç¢å‡½æ•°çš„æŸå¤±ï¼Œå®ƒå°†ä»ç³»ç»Ÿå›ºæœ‰æ•…éšœä¸­ç¡®å®šçš„å®‰å…¨æŒ‡æ ‡çº³å…¥ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ã€‚è¿™ä¸ªé¢å¤–çš„æŸå¤±é¡¹æœ‰åŠ©äºé™ä½é£é™©ï¼Œå¹¶åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µé¼“åŠ±æ›´å®‰å…¨çš„æ¢ç´¢ã€‚æˆ‘ä»¬åœ¨æ˜Ÿé™…äº‰éœ¸IIå¾®è§‚ç®¡ç†åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‚£é‡Œè¡¨ç°å‡ºäº†æ”¹è¿›çš„æ”¶æ•›æ€§ï¼Œå¹¶ä¸”åœ¨å®‰å…¨æ€§å’Œä»»åŠ¡å®Œæˆæ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè€ƒè™‘å®‰å…¨å› ç´ å¯ä»¥æ˜¾è‘—å¢å¼ºå¤æ‚å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„å­¦ä¹ æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12061v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†ç¯å¢ƒä¸­çš„å›ºæœ‰éšæœºæ€§å’Œä¸ç¡®å®šæ€§ç»™ç­–ç•¥å’Œå­¦ä¹ çš„æ•ˆç‡å’Œç¨³å¥æ€§å¸¦æ¥äº†æŒ‘æˆ˜ã€‚åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ åœ¨å•æ™ºèƒ½ä½“ç¯å¢ƒä¸­å·²æˆåŠŸåº”ç”¨äºè§£å†³é£é™©å’Œä¸ç¡®å®šæ€§é—®é¢˜ï¼Œä½†åœ¨MARLä¸­çš„åº”ç”¨æœ‰é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆåˆ†å¸ƒå­¦ä¹ ä¸å®‰å…¨å¯¼å‘æŸå¤±å‡½æ•°çš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜åˆä½œMARLä»»åŠ¡çš„æ”¶æ•›æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå±éšœå‡½æ•°çš„æŸå¤±ï¼Œåˆ©ç”¨ä»ç³»ç»Ÿå›ºæœ‰æ•…éšœä¸­è¯†åˆ«çš„å®‰å…¨æŒ‡æ ‡ï¼Œå°†å…¶çº³å…¥ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ã€‚è¿™ç§é¢å¤–çš„æŸå¤±é¡¹æœ‰åŠ©äºé™ä½é£é™©ï¼Œå¹¶åœ¨è®­ç»ƒæ—©æœŸé¼“åŠ±æ›´å®‰å…¨çš„æ¢ç´¢ã€‚æˆ‘ä»¬åœ¨æ˜Ÿé™…äº‰éœ¸IIå¾®è§‚ç®¡ç†åŸºå‡†æµ‹è¯•ä¸­å¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæ›´å¥½çš„æ”¶æ•›æ€§ï¼Œå¹¶åœ¨å®‰å…¨æ€§å’Œä»»åŠ¡å®Œæˆåº¦æ–¹é¢ä¼˜äºæœ€æ–°åŸºçº¿ã€‚ç»“æœè¡¨æ˜ï¼Œè€ƒè™‘å®‰å…¨æ€§å› ç´ å¯ä»¥æ˜¾è‘—å¢å¼ºå¤æ‚å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„å­¦ä¹ æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡æ—¶é¢ä¸´å›ºæœ‰éšæœºæ€§å’Œä¸ç¡®å®šæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ åœ¨å•æ™ºèƒ½ä½“ç¯å¢ƒä¸­å·²æˆåŠŸåº”ç”¨ï¼Œä½†åœ¨MARLä¸­çš„åº”ç”¨å—é™ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆåˆ†å¸ƒå­¦ä¹ ä¸å®‰å…¨å¯¼å‘æŸå¤±å‡½æ•°çš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜åˆä½œMARLä»»åŠ¡çš„æ”¶æ•›æ€§ã€‚</li>
<li>å¼•å…¥åŸºäºå±éšœå‡½æ•°çš„æŸå¤±ï¼Œåˆ©ç”¨å®‰å…¨æŒ‡æ ‡çº³å…¥ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>é¢å¤–çš„æŸå¤±é¡¹æœ‰åŠ©äºé™ä½é£é™©ï¼Œå¹¶åœ¨è®­ç»ƒæ—©æœŸé¼“åŠ±æ›´å®‰å…¨çš„æ¢ç´¢ã€‚</li>
<li>åœ¨æ˜Ÿé™…äº‰éœ¸IIå¾®è§‚ç®¡ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾ç¤ºå‡ºæ›´å¥½çš„æ”¶æ•›æ€§å’Œä¼˜äºæœ€æ–°åŸºçº¿çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12061">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-148616e223aa17cd23f8c9d961faa678.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-078dd05ed2d714abf50751b1d1c68b58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7bbe5462c3296c8b537890b26dfefe9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f77fec60f654cdb6c143df0db2e8469.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daa87285de17138b240e502e769ca546.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3958169600545adba5506ceb31ab2722.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EmbodiedEval-Evaluate-Multimodal-LLMs-as-Embodied-Agents"><a href="#EmbodiedEval-Evaluate-Multimodal-LLMs-as-Embodied-Agents" class="headerlink" title="EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents"></a>EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents</h2><p><strong>Authors:Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun</strong></p>
<p>Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at <a target="_blank" rel="noopener" href="https://github.com/thunlp/EmbodiedEval">https://github.com/thunlp/EmbodiedEval</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºå®ä½“ä»£ç†æä¾›äº†å…‰æ˜çš„æœªæ¥å‰æ™¯ã€‚ç°æœ‰çš„è¯„ä¼°MLLMçš„åŸºå‡†æµ‹è¯•ä¸»è¦åˆ©ç”¨é™æ€å›¾åƒæˆ–è§†é¢‘ï¼Œå°†è¯„ä¼°é™åˆ¶åœ¨éäº¤äº’å¼åœºæ™¯ä¸­ã€‚ä¸æ­¤åŒæ—¶ï¼Œç°æœ‰çš„å®ä½“AIåŸºå‡†æµ‹è¯•éƒ½æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ï¼Œå¹¶ä¸å¤Ÿå¤šæ ·åŒ–ï¼Œæ— æ³•å……åˆ†è¯„ä¼°MLLMçš„å®ä½“èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EmbodiedEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ã€äº¤äº’å¼çš„è¯„ä¼°MLLMçš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å®ä½“ä»»åŠ¡ã€‚EmbodiedEvalåœ¨125ä¸ªå¤šæ ·åŒ–çš„3Dåœºæ™¯ä¸­åŒ…å«328ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½ç»è¿‡ä¸¥æ ¼é€‰æ‹©å’Œæ³¨é‡Šã€‚å®ƒæ¶µç›–äº†å¹¿æ³›çš„ç°æœ‰å®ä½“AIä»»åŠ¡ï¼Œå…·æœ‰æ˜¾è‘—å¢å¼ºçš„å¤šæ ·æ€§ï¼Œå…¨éƒ¨éƒ½åœ¨ç»Ÿä¸€çš„æ¨¡æ‹Ÿå’Œè¯„ä¼°æ¡†æ¶å†…ï¼Œè¯¥æ¡†æ¶ä¸“ä¸ºMLLMé‡èº«å®šåˆ¶ã€‚ä»»åŠ¡åˆ†ä¸ºäº”ä¸ªç±»åˆ«ï¼šå¯¼èˆªã€å¯¹è±¡äº¤äº’ã€ç¤¾ä¼šäº¤äº’ã€å±æ€§é—®ç­”å’Œç©ºé—´é—®ç­”ï¼Œä»¥è¯„ä¼°ä»£ç†çš„ä¸åŒèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨EmbodiedEvalä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„MLLMï¼Œå‘ç°å®ƒä»¬åœ¨å®ä½“ä»»åŠ¡ä¸Šä¸äººç±»æ°´å¹³å­˜åœ¨å¾ˆå¤§å·®è·ã€‚æˆ‘ä»¬çš„åˆ†æå±•ç¤ºäº†ç°æœ‰MLLMåœ¨å®ä½“èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºå®ƒä»¬çš„æœªæ¥å‘å±•æä¾›äº†è§è§£ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/thunlp/EmbodiedEval%E5%85%AC%E5%BC%80%E6%89%80%E6%9C%89%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E5%92%8C%E6%A8%A1%E6%8B%9F%E6%A1%86%E6%9E%B6%E3%80%82">https://github.com/thunlp/EmbodiedEvalå…¬å¼€æ‰€æœ‰è¯„ä¼°æ•°æ®å’Œæ¨¡æ‹Ÿæ¡†æ¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11858v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ™ºèƒ½ä½“é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—è¿›æ­¥å’Œå¹¿é˜”å‰æ™¯ã€‚å½“å‰è¯„ä¼°MLLMçš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–äºé™æ€å›¾åƒæˆ–è§†é¢‘ï¼Œä»…é™äºéäº¤äº’åœºæ™¯ï¼Œæ— æ³•å…¨é¢è¯„ä¼°MLLMçš„æ™ºèƒ½ä½“èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºEmbodiedEvalåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ˜¯ä¸€ä¸ªå…¨é¢ã€äº¤äº’å¼çš„MLLMè¯„ä¼°å¹³å°ï¼ŒåŒ…å«328é¡¹ç‹¬ç‰¹ä»»åŠ¡å’Œ125ç§ä¸åŒçš„3Dåœºæ™¯ã€‚ä»»åŠ¡æ¶µç›–å¯¼èˆªã€å¯¹è±¡äº¤äº’ã€ç¤¾ä¼šäº¤äº’ã€å±æ€§é—®ç­”ã€ç©ºé—´é—®ç­”ç­‰äº”å¤§ç±»åˆ«ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“çš„ä¸åŒèƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„MLLMåœ¨EmbodiedEvalä¸Šçš„è¡¨ç°ï¼Œå‘ç°ä¸äººç±»æ°´å¹³ç›¸æ¯”ä»å­˜åœ¨å·¨å¤§å·®è·ã€‚è¿™æ­ç¤ºäº†ç°æœ‰MLLMåœ¨æ™ºèƒ½ä½“èƒ½åŠ›æ–¹é¢çš„å±€é™ï¼Œä¸ºæœªæ¥å‘å±•æä¾›äº†å¯ç¤ºã€‚æˆ‘ä»¬å…¬å¼€äº†æ‰€æœ‰è¯„ä¼°æ•°æ®å’Œä»¿çœŸæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ™ºèƒ½ä½“é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸»è¦åŸºäºé™æ€å›¾åƒæˆ–è§†é¢‘ï¼Œç¼ºä¹äº¤äº’æ€§ï¼Œæ— æ³•å…¨é¢è¯„ä¼°MLLMçš„æ™ºèƒ½ä½“èƒ½åŠ›ã€‚</li>
<li>EmbodiedEvalæ˜¯ä¸€ä¸ªå…¨é¢ã€äº¤äº’å¼çš„MLLMè¯„ä¼°å¹³å°ï¼ŒåŒ…å«å¤šç§ä»»åŠ¡å’Œ3Dåœºæ™¯ã€‚</li>
<li>ä»»åŠ¡æ¶µç›–äº”å¤§ç±»åˆ«ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“çš„ä¸åŒèƒ½åŠ›ï¼Œå¦‚å¯¼èˆªã€å¯¹è±¡äº¤äº’ã€ç¤¾ä¼šäº¤äº’ç­‰ã€‚</li>
<li>æœ€å…ˆè¿›çš„MLLMåœ¨EmbodiedEvalä¸Šçš„è¡¨ç°ä¸äººç±»æ°´å¹³ç›¸æ¯”ä»æœ‰æ˜¾è‘—å·®è·ã€‚</li>
<li>ç°æœ‰MLLMåœ¨æ™ºèƒ½ä½“èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f32e3cf9ce03b35344549f5ca9c045e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d74a967fe4adad63f7d005083d3ad75b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26a9d9fcd34c80de7d9724ea763a7311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-794c0b2e7c16c655950deba0d3af160f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c59232b09233efdef4ad3ea81757e6c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PlotEdit-Natural-Language-Driven-Accessible-Chart-Editing-in-PDFs-via-Multimodal-LLM-Agents"><a href="#PlotEdit-Natural-Language-Driven-Accessible-Chart-Editing-in-PDFs-via-Multimodal-LLM-Agents" class="headerlink" title="PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via   Multimodal LLM Agents"></a>PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via   Multimodal LLM Agents</h2><p><strong>Authors:Kanika Goswami, Puneet Mathur, Ryan Rossi, Franck Dernoncourt</strong></p>
<p>Chart visualizations, while essential for data interpretation and communication, are predominantly accessible only as images in PDFs, lacking source data tables and stylistic information. To enable effective editing of charts in PDFs or digital scans, we present PlotEdit, a novel multi-agent framework for natural language-driven end-to-end chart image editing via self-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1) Chart2Table for data table extraction, (2) Chart2Vision for style attribute identification, (3) Chart2Code for retrieving rendering code, (4) Instruction Decomposition Agent for parsing user requests into executable steps, and (5) Multimodal Editing Agent for implementing nuanced chart component modifications - all coordinated through multimodal feedback to maintain visual fidelity. PlotEdit outperforms existing baselines on the ChartCraft dataset across style, layout, format, and data-centric edits, enhancing accessibility for visually challenged users and improving novice productivity. </p>
<blockquote>
<p>å›¾è¡¨å¯è§†åŒ–å¯¹äºæ•°æ®è§£è¯»å’Œæ²Ÿé€šè‡³å…³é‡è¦ï¼Œä½†ç›®å‰ä¸»è¦ä»¥PDFä¸­çš„å›¾ç‰‡å½¢å¼å­˜åœ¨ï¼Œç¼ºä¹æºæ•°æ®è¡¨å’Œé£æ ¼ä¿¡æ¯ã€‚ä¸ºäº†å®ç°PDFæˆ–æ•°å­—æ‰«æä¸­çš„å›¾è¡¨çš„æœ‰æ•ˆç¼–è¾‘ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PlotEditï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹å¤šä»£ç†æ¡†æ¶ï¼Œé€šè¿‡è‡ªæˆ‘åæ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œå®ç°è‡ªç„¶è¯­è¨€é©±åŠ¨ç«¯åˆ°ç«¯çš„å›¾è¡¨å›¾åƒç¼–è¾‘ã€‚PlotEditåè°ƒäº†äº”ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰Chart2Tableç”¨äºæ•°æ®è¡¨æå–ï¼Œï¼ˆ2ï¼‰Chart2Visionç”¨äºè¯†åˆ«æ ·å¼å±æ€§ï¼Œï¼ˆ3ï¼‰Chart2Codeç”¨äºæ£€ç´¢æ¸²æŸ“ä»£ç ï¼Œï¼ˆ4ï¼‰æŒ‡ä»¤åˆ†è§£ä»£ç†ç”¨äºå°†ç”¨æˆ·è¯·æ±‚è§£æä¸ºå¯æ‰§è¡Œæ­¥éª¤ï¼Œï¼ˆ5ï¼‰å¤šæ¨¡å¼ç¼–è¾‘ä»£ç†ç”¨äºæ‰§è¡Œç»†å¾®çš„å›¾è¡¨ç»„ä»¶ä¿®æ”¹â€”â€”æ‰€æœ‰è¿™äº›éƒ½é€šè¿‡å¤šæ¨¡å¼åé¦ˆè¿›è¡Œåè°ƒï¼Œä»¥ä¿æŒè§†è§‰ä¿çœŸåº¦ã€‚åœ¨ChartCraftæ•°æ®é›†ä¸Šï¼ŒPlotEditåœ¨é£æ ¼ã€å¸ƒå±€ã€æ ¼å¼å’Œæ•°æ®ä¸­å¿ƒç¼–è¾‘æ–¹é¢è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œæé«˜äº†è§†è§‰éšœç¢ç”¨æˆ·çš„ä½¿ç”¨ä¾¿æ·æ€§å’Œæ–°æ‰‹çš„ç”Ÿäº§åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11233v1">PDF</a> Accepted at ECIR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PlotEditè¿™ä¸€å…¨æ–°çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºè‡ªç„¶è¯­è¨€é©±åŠ¨çš„ç«¯åˆ°ç«¯å›¾è¡¨å›¾åƒç¼–è¾‘ã€‚è¯¥æ¡†æ¶å…·å¤‡äº”å¤§æ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬è¡¨æ ¼æå–çš„æ™ºèƒ½ä½“ã€é£æ ¼å±æ€§è¯†åˆ«çš„æ™ºèƒ½ä½“ã€æ¸²æŸ“ä»£ç æ£€ç´¢çš„æ™ºèƒ½ä½“ã€æŒ‡ä»¤åˆ†è§£çš„æ™ºèƒ½ä½“ä»¥åŠå®ç°ç»†å¾®å›¾è¡¨ç»„ä»¶ä¿®æ”¹çš„å¤šæ¨¡æ€ç¼–è¾‘æ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“é€šè¿‡å¤šæ¨¡æ€åé¦ˆè¿›è¡Œåè°ƒï¼Œæ—¨åœ¨æé«˜å›¾è¡¨çš„å¯è®¿é—®æ€§å¹¶æ”¹å–„è§†è§‰éšœç¢ç”¨æˆ·å’Œæ–°æ‰‹çš„ç”Ÿäº§åŠ›ã€‚PlotEditåœ¨ChartCraftæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ¶µç›–é£æ ¼ã€å¸ƒå±€ã€æ ¼å¼å’Œæ•°æ®ä¸­å¿ƒç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PlotEditæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºè‡ªç„¶è¯­è¨€é©±åŠ¨çš„ç«¯åˆ°ç«¯å›¾è¡¨å›¾åƒç¼–è¾‘ã€‚</li>
<li>å®ƒåŒ…å«äº”å¤§æ™ºèƒ½ä½“ï¼šè¡¨æ ¼æå–ã€é£æ ¼å±æ€§è¯†åˆ«ã€æ¸²æŸ“ä»£ç æ£€ç´¢ã€æŒ‡ä»¤åˆ†è§£å’Œå¤šæ¨¡æ€ç¼–è¾‘ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€åé¦ˆåè°ƒè¿™äº›æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨å®ç°å›¾è¡¨çš„ç²¾å‡†ç¼–è¾‘ã€‚</li>
<li>PlotEditæé«˜äº†å›¾è¡¨çš„å¯è®¿é—®æ€§ï¼Œæ”¹å–„äº†è§†è§‰éšœç¢ç”¨æˆ·å’Œæ–°æ‰‹çš„ç”Ÿäº§åŠ›ã€‚</li>
<li>åœ¨ChartCraftæ•°æ®é›†ä¸Šï¼ŒPlotEditçš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-adeb9e8a6f737f71a8b8533e71a90523.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4424cbb52c7405248f98878d0354a478.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2515204ed6e5d012cf2e80b1fc7149aa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CART-MPC-Coordinating-Assistive-Devices-for-Robot-Assisted-Transferring-with-Multi-Agent-Model-Predictive-Control"><a href="#CART-MPC-Coordinating-Assistive-Devices-for-Robot-Assisted-Transferring-with-Multi-Agent-Model-Predictive-Control" class="headerlink" title="CART-MPC: Coordinating Assistive Devices for Robot-Assisted Transferring   with Multi-Agent Model Predictive Control"></a>CART-MPC: Coordinating Assistive Devices for Robot-Assisted Transferring   with Multi-Agent Model Predictive Control</h2><p><strong>Authors:Ruolin Ye, Shuaixing Chen, Yunting Yan, Joyce Yang, Christina Ge, Jose Barreiros, Kate Tsui, Tom Silver, Tapomayukh Bhattacharjee</strong></p>
<p>Bed-to-wheelchair transferring is a ubiquitous activity of daily living (ADL), but especially challenging for caregiving robots with limited payloads. We develop a novel algorithm that leverages the presence of other assistive devices: a Hoyer sling and a wheelchair for coarse manipulation of heavy loads, alongside a robot arm for fine-grained manipulation of deformable objects (Hoyer sling straps). We instrument the Hoyer sling and wheelchair with actuators and sensors so that they can become intelligent agents in the algorithm. We then focus on one subtask of the transferring ADL â€“ tying Hoyer sling straps to the sling bar â€“ that exemplifies the challenges of transfer: multi-agent planning, deformable object manipulation, and generalization to varying hook shapes, sling materials, and care recipient bodies. To address these challenges, we propose CART-MPC, a novel algorithm based on turn-taking multi-agent model predictive control that uses a learned neural dynamics model for a keypoint-based representation of the deformable Hoyer sling strap, and a novel cost function that leverages linking numbers from knot theory and neural amortization to accelerate inference. We validate it in both RCareWorld simulation and real-world environments. In simulation, CART-MPC successfully generalizes across diverse hook designs, sling materials, and care recipient body shapes. In the real world, we show zero-shot sim-to-real generalization capabilities to tie deformable Hoyer sling straps on a sling bar towards transferring a manikin from a hospital bed to a wheelchair. See our website for supplementary materials: <a target="_blank" rel="noopener" href="https://emprise.cs.cornell.edu/cart-mpc/">https://emprise.cs.cornell.edu/cart-mpc/</a>. </p>
<blockquote>
<p>åºŠä½è‡³è½®æ¤…è½¬ç§»æ˜¯ä¸€é¡¹æ™®éçš„æ—¥å¸¸æ´»åŠ¨ï¼ˆADLï¼‰ï¼Œå¯¹äºè½½è·æœ‰é™çš„æŠ¤ç†æœºå™¨äººæ¥è¯´å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹ç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨å…¶ä»–è¾…åŠ©è®¾å¤‡ï¼šHoyerå‡é™åŠç´¢å’Œè½®æ¤…ï¼Œç”¨äºç²—æ“çºµé‡ç‰©ï¼Œä»¥åŠæœºå™¨äººæ‰‹è‡‚ç”¨äºå¯¹å¯å˜å½¢ç‰©ä½“ï¼ˆHoyerå‡é™åŠç´¢å¸¦ï¼‰è¿›è¡Œç²¾ç»†æ“çºµã€‚æˆ‘ä»¬å¯¹Hoyerå‡é™åŠç´¢å’Œè½®æ¤…è¿›è¡Œä»ªå™¨é…å¤‡ï¼Œä½¿å…¶èƒ½å¤Ÿæˆä¸ºç®—æ³•ä¸­çš„æ™ºèƒ½ä¸»ä½“ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¸“æ³¨äºè½¬ç§»ADLçš„ä¸€ä¸ªå­ä»»åŠ¡â€”â€”å°†Hoyerå‡é™åŠç´¢å¸¦ç»‘åœ¨åŠç´¢æ†ä¸Šâ€”â€”è¿™ä¸ªä¾‹å­ä½“ç°äº†è½¬ç§»æŒ‘æˆ˜ï¼šå¤šä¸»ä½“è§„åˆ’ã€å¯å˜å½¢ç‰©ä½“çš„æ“çºµä»¥åŠé’ˆå¯¹é’©å½¢ã€åŠç´¢ææ–™å’ŒæŠ¤ç†å¯¹è±¡çš„ä¸åŒä½“å‹è¿›è¡Œå½’çº³ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CART-MPCç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè½®æµåˆ¶çš„å¤šä¸»ä½“æ¨¡å‹é¢„æµ‹æ§åˆ¶çš„æ–°å‹ç®—æ³•ã€‚å®ƒä½¿ç”¨åŸºäºå…³é”®ç‚¹çš„å¯å˜å½¢Hoyerå‡é™åŠç´¢å¸¦ç¥ç»ç½‘ç»œåŠ¨åŠ›å­¦æ¨¡å‹ä»¥åŠåˆ©ç”¨ç»“ç†è®ºä¸­çš„é“¾æ¥æ•°å’Œç¥ç»ç½‘ç»œæ‘Šé”€æ¥åŠ é€Ÿæ¨æ–­çš„æ–°å‹æˆæœ¬å‡½æ•°ã€‚æˆ‘ä»¬åœ¨RCareWorldæ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ç¯å¢ƒä¸­éƒ½å¯¹å…¶è¿›è¡Œäº†éªŒè¯ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼ŒCART-MPCæˆåŠŸåœ°å½’çº³äº†å¤šç§é’©å½¢è®¾è®¡ã€åŠç´¢ææ–™å’ŒæŠ¤ç†å¯¹è±¡ä½“å‹ã€‚åœ¨ç°å®ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä»æ¨¡æ‹Ÿåˆ°ç°å®çš„é›¶èµ·ç‚¹å½’çº³èƒ½åŠ›ï¼Œèƒ½å¤Ÿç»‘ä¸Šå¯å˜å½¢çš„Hoyerå‡é™åŠç´¢å¸¦ï¼Œå°†æ¨¡æ‹Ÿäººä»ç—…åºŠè½¬ç§»åˆ°è½®æ¤…ä¸Šã€‚æœ‰å…³è¡¥å……ææ–™ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://emprise.cs.cornell.edu/cart-mpc/%E3%80%82">https://emprise.cs.cornell.edu/cart-mpc/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11149v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ—¥å¸¸æŠ¤ç†æœºå™¨äººåºŠåˆ°è½®æ¤…è½¬ç§»æ“ä½œä¸­çš„è´Ÿè½½é™åˆ¶é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨è¾…åŠ©è®¾å¤‡å¦‚éœè€¶å‡é™å¸¦å’Œè½®æ¤…è¿›è¡Œç²—é‡è´Ÿè½½æ“ä½œï¼ŒåŒæ—¶ä½¿ç”¨æœºå™¨äººæ‰‹è‡‚è¿›è¡Œå¯å˜å½¢ç‰©ä½“çš„ç²¾ç»†æ“ä½œã€‚é€šè¿‡ä»ªå™¨åŒ–éœè€¶å‡é™å¸¦å’Œè½®æ¤…ï¼Œä½¿å…¶æˆä¸ºç®—æ³•ä¸­çš„æ™ºèƒ½ä»£ç†ã€‚é’ˆå¯¹è½¬ç§»æ´»åŠ¨ä¸­çš„ä¸€é¡¹å­ä»»åŠ¡â€”â€”å°†éœè€¶å‡é™å¸¦ç»‘åœ¨å‡é™æ†ä¸Šï¼Œè¯¥ç®—æ³•è§£å†³äº†å¤šä»£ç†è§„åˆ’ã€å¯å˜å½¢ç‰©ä½“æ“ä½œå’Œä¸åŒé’©å­å½¢çŠ¶ã€å‡é™å¸¦ææ–™å’ŒæŠ¤ç†å¯¹è±¡ä½“å‹çš„é€šç”¨åŒ–æŒ‘æˆ˜ã€‚æå‡ºä¸€ç§åŸºäºè½®æµåˆ¶çš„å¤šä»£ç†æ¨¡å‹é¢„æµ‹æ§åˆ¶CART-MPCç®—æ³•ï¼Œé‡‡ç”¨åŸºäºå…³é”®ç‚¹çš„å¯å˜å½¢éœè€¶å‡é™å¸¦å¸¦ç¥ç»ç½‘ç»œåŠ¨åŠ›å­¦æ¨¡å‹ï¼Œä»¥åŠåˆ©ç”¨ç»“ç†è®ºä¸­çš„è¿æ¥æ•°å’Œç¥ç»ç½‘ç»œæ‘Šé”€åŠ é€Ÿæ¨ç†çš„æ–°å‹æˆæœ¬å‡½æ•°ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ç¯å¢ƒä¸­è¿›è¡Œäº†éªŒè¯ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¸åŒé’©å­è®¾è®¡ã€å‡é™å¸¦ææ–™å’ŒæŠ¤ç†å¯¹è±¡ä½“å‹æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆåŠŸå®ç°æ¨¡æ‹Ÿåˆ°çœŸå®çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨è½¬ç§»æŠ¤ç†æ¨¡å‹ä¸­å®Œæˆç»‘éœè€¶å‡é™å¸¦å¹¶å°†å…¶åº”ç”¨äºè½¬ç§»çœŸå®ç—…äººçš„ä»»åŠ¡ã€‚æ›´å¤šææ–™è¯·è§æˆ‘ä»¬çš„ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://emprise.cs.cornell.edu/cart-mpc/">https://emprise.cs.cornell.edu/cart-mpc/</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹åºŠåˆ°è½®æ¤…è½¬ç§»æ“ä½œä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§ç»“åˆæœºå™¨äººä¸è¾…åŠ©è®¾å¤‡çš„ç®—æ³•ã€‚</li>
<li>åˆ©ç”¨éœè€¶å‡é™å¸¦å’Œè½®æ¤…è¿›è¡Œç²—è´Ÿè½½æ“ä½œï¼Œæœºå™¨äººæ‰‹è‡‚è¿›è¡Œç²¾ç»†æ“ä½œã€‚</li>
<li>ä»ªå™¨åŒ–éœè€¶å‡é™å¸¦å’Œè½®æ¤…ï¼Œä½¿å…¶æˆä¸ºæ™ºèƒ½ä»£ç†å‚ä¸ç®—æ³•æ‰§è¡Œã€‚</li>
<li>æå‡ºCART-MPCç®—æ³•ï¼Œè§£å†³å¤šä»£ç†è§„åˆ’ã€å¯å˜å½¢ç‰©ä½“æ“ä½œå’Œæ³›åŒ–é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨åŸºäºå…³é”®ç‚¹çš„ç¥ç»ç½‘ç»œåŠ¨åŠ›å­¦æ¨¡å‹è¡¨ç¤ºå¯å˜å½¢ç‰©ä½“ï¼ˆéœè€¶å‡é™å¸¦ï¼‰ã€‚</li>
<li>åˆ©ç”¨ç»“ç†è®ºä¸­çš„è¿æ¥æ•°å’Œç¥ç»ç½‘ç»œæ‘Šé”€åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af43ff57a2d88555d5f57294a4292f88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9bc27ade8b5ea72934bdab05d2fdb31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4689be4132df0b1aa4e2a7fcf2cd4aa3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e3785964a990e282e88e0275194e2a7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="IntellAgent-A-Multi-Agent-Framework-for-Evaluating-Conversational-AI-Systems"><a href="#IntellAgent-A-Multi-Agent-Framework-for-Evaluating-Conversational-AI-Systems" class="headerlink" title="IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI   Systems"></a>IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI   Systems</h2><p><strong>Authors:Elad Levi, Ilan Kadar</strong></p>
<p>Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at <a target="_blank" rel="noopener" href="https://github.com/plurai-ai/intellagent">https://github.com/plurai-ai/intellagent</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨æ”¹å˜äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œé€æ¸æ¼”å˜ä¸ºèƒ½å¤Ÿè‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œçš„ä»»åŠ¡å¯¼å‘å‹ç³»ç»Ÿã€‚LLMçš„ä¸»è¦åº”ç”¨ä¹‹ä¸€æ˜¯å¯¹è¯å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿå¿…é¡»åº”å¯¹å¤šè½®å¯¹è¯ã€é›†æˆç‰¹å®šé¢†åŸŸçš„APIå¹¶éµå¾ªä¸¥æ ¼çš„æ”¿ç­–çº¦æŸã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›ä»£ç†ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºä¼ ç»Ÿæ–¹æ³•æ— æ³•æ•æ‰ç°å®ä¸–ç•Œä¸­äº’åŠ¨çš„å¤æ‚æ€§å’Œå˜åŒ–æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†IntellAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æºå¤šä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å¯¹è¯å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚IntellAgenté€šè¿‡ç»“åˆæ”¿ç­–é©±åŠ¨çš„å›¾å»ºæ¨¡ã€ç°å®äº‹ä»¶ç”Ÿæˆå’Œäº¤äº’å¼ç”¨æˆ·ä»£ç†æ¨¡æ‹Ÿï¼Œè‡ªåŠ¨åˆ›å»ºå¤šæ ·åŒ–çš„åˆæˆåŸºå‡†æµ‹è¯•ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•æä¾›äº†ç²¾ç»†çš„è¯Šæ–­ï¼Œè§£å†³äº†é™æ€å’Œæ‰‹åŠ¨ç­–åˆ’çš„åŸºå‡†æµ‹è¯•ä¸ç²—ç•¥æŒ‡æ ‡ç›¸ç»“åˆçš„å±€é™æ€§ã€‚IntellAgentä»£è¡¨äº†è¯„ä¼°å¯¹è¯å¼äººå·¥æ™ºèƒ½çš„èŒƒå¼è½¬å˜ã€‚é€šè¿‡æ¨¡æ‹Ÿä¸åŒå¤æ‚ç¨‹åº¦ä¸Šçš„çœŸå®ã€å¤šæ”¿ç­–åœºæ™¯ï¼ŒIntellAgentæ•æ‰äº†ä»£ç†èƒ½åŠ›å’Œæ”¿ç­–çº¦æŸä¹‹é—´å¾®å¦™çš„ç›¸äº’ä½œç”¨ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œå®ƒé‡‡ç”¨åŸºäºå›¾çš„æ”¿ç­–æ¨¡å‹æ¥è¡¨ç¤ºæ”¿ç­–äº’åŠ¨çš„å…³è”ã€å¯èƒ½æ€§å’Œå¤æ‚æ€§ï¼Œä»è€Œå®ç°é«˜åº¦è¯¦ç»†çš„è¯Šæ–­ã€‚IntellAgentè¿˜èƒ½å‘ç°å…³é”®æ€§èƒ½å·®è·ï¼Œä¸ºæœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–æä¾›å¯æ“ä½œçš„è§è§£ã€‚å…¶æ¨¡å—åŒ–ã€å¼€æºçš„è®¾è®¡æ”¯æŒæ–°é¢†åŸŸã€æ”¿ç­–å’ŒAPIçš„æ— ç¼é›†æˆï¼Œä¿ƒè¿›äº†å¯é‡å¤æ€§å’Œç¤¾åŒºåˆä½œã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒIntellAgentæ˜¯è§£å†³ç ”ç©¶å’Œéƒ¨ç½²ä¹‹é—´æ¡¥æ¢æŒ‘æˆ˜çš„æœ‰æ•ˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/plurai-ai/intellagent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/plurai-ai/intellagentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11067v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£æ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å˜é©ï¼Œå®ƒä»¬é€æ¸å‘å±•ä¸ºå…·å¤‡è‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œèƒ½åŠ›çš„ä»»åŠ¡å¯¼å‘å‹ç³»ç»Ÿã€‚å¯¹äºLLMçš„ä¸»è¦åº”ç”¨ä¹‹ä¸€â€”â€”å¯¹è¯å¼AIç³»ç»Ÿæ¥è¯´ï¼Œå…¶å¿…é¡»åº”å¯¹å¤šè½®å¯¹è¯å¯¼èˆªã€é›†æˆç‰¹å®šé¢†åŸŸAPIä»¥åŠéµå®ˆä¸¥æ ¼æ”¿ç­–çº¦æŸç­‰æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›AIä»£ç†ä»å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•æ— æ³•æ•æ‰ç°å®ä¸–ç•Œäº’åŠ¨çš„å¤æ‚æ€§å’Œå¤šå˜æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†IntellAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æºå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å¯¹è¯å¼AIç³»ç»Ÿã€‚IntellAgenté€šè¿‡ç»“åˆæ”¿ç­–é©±åŠ¨çš„å›¾å»ºæ¨¡ã€ç°å®äº‹ä»¶ç”Ÿæˆå’Œäº¤äº’å¼ç”¨æˆ·ä»£ç†æ¨¡æ‹Ÿï¼Œè‡ªåŠ¨åˆ›å»ºå¤šæ ·åŒ–çš„åˆæˆåŸºå‡†æµ‹è¯•ã€‚è¿™ä¸€åˆ›æ–°æ–¹æ³•æä¾›äº†ç²¾ç»†çš„è¯Šæ–­ï¼Œè§£å†³äº†é™æ€å’Œæ‰‹åŠ¨ç­–åˆ’åŸºå‡†æµ‹è¯•çš„å±€é™æ€§åŠå…¶ç²—ç•¥çš„åº¦é‡æ ‡å‡†ã€‚IntellAgentä»£è¡¨ç€è¯„ä¼°å¯¹è¯å¼AIçš„ä¸€ä¸ªèŒƒå¼è½¬å˜ã€‚å®ƒé€šè¿‡æ¨¡æ‹Ÿå¤æ‚çš„ç°å®æƒ…å¢ƒå’Œå¤šæ”¿ç­–åœºæ™¯ï¼Œæ•æ‰ä»£ç†èƒ½åŠ›å’Œæ”¿ç­–çº¦æŸä¹‹é—´çš„å¾®å¦™äº’åŠ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæ­£åœ¨æ¨åŠ¨å˜é©ï¼Œå‘å±•ä¸ºä»»åŠ¡å¯¼å‘å‹ç³»ç»Ÿï¼Œå…·å¤‡è‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œèƒ½åŠ›ã€‚</li>
<li>å¯¹è¯å¼AIç³»ç»Ÿæ˜¯LLMçš„ä¸»è¦åº”ç”¨ä¹‹ä¸€ï¼Œéœ€åº”å¯¹å¤šè½®å¯¹è¯å¯¼èˆªã€é›†æˆç‰¹å®šé¢†åŸŸAPIå’Œéµå®ˆä¸¥æ ¼æ”¿ç­–çº¦æŸç­‰æŒ‘æˆ˜ã€‚</li>
<li>è¯„ä¼°å¯¹è¯å¼AIç³»ç»Ÿå­˜åœ¨æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•æ— æ³•æ•æ‰ç°å®äº’åŠ¨çš„å¤æ‚æ€§å’Œå¤šå˜æ€§ã€‚</li>
<li>IntellAgentæ˜¯ä¸€ä¸ªåˆ›æ–°çš„ã€å¼€æºçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå…¨é¢è¯„ä¼°å¯¹è¯å¼AIç³»ç»Ÿã€‚</li>
<li>IntellAgenté€šè¿‡æ”¿ç­–é©±åŠ¨çš„å›¾å»ºæ¨¡ã€ç°å®äº‹ä»¶ç”Ÿæˆå’Œäº¤äº’å¼ç”¨æˆ·ä»£ç†æ¨¡æ‹Ÿè‡ªåŠ¨åˆ›å»ºåˆæˆåŸºå‡†æµ‹è¯•ã€‚</li>
<li>IntellAgentæä¾›ç²¾ç»†çš„è¯Šæ–­ï¼Œå¹¶è§£å†³é™æ€å’Œæ‰‹åŠ¨ç­–åˆ’åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdec38d0fe2acaacdc2d6ba6e521285e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83e35f73372d2a423c7b3d5842a5f02a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5f32326bfca71549817618ec69c561e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Machine-Learning-Surrogates-for-Optimizing-Transportation-Policies-with-Agent-Based-Models"><a href="#Machine-Learning-Surrogates-for-Optimizing-Transportation-Policies-with-Agent-Based-Models" class="headerlink" title="Machine Learning Surrogates for Optimizing Transportation Policies with   Agent-Based Models"></a>Machine Learning Surrogates for Optimizing Transportation Policies with   Agent-Based Models</h2><p><strong>Authors:Elena Natterer, Roman Engelhardt, Sebastian HÃ¶rl, Klaus Bogenberger</strong></p>
<p>Rapid urbanization and growing urban populations worldwide present significant challenges for cities, including increased traffic congestion and air pollution. Effective strategies are needed to manage traffic volumes and reduce emissions. In practice, traditional traffic flow simulations are used to test those strategies. However, high computational intensity usually limits their applicability in investigating a magnitude of different scenarios to evaluate best policies. This paper presents a first approach of using Graph Neural Networks (GNN) as surrogates for large-scale agent-based simulation models. In a case study using the MATSim model of Paris, the GNN effectively learned the impacts of capacity reduction policies on citywide traffic flow. Performance analysis across various road types and scenarios revealed that the GNN could accurately capture policy-induced effects on edge-based traffic volumes, particularly on roads directly affected by the policies and those with higher traffic volumes. </p>
<blockquote>
<p>å¿«é€ŸåŸå¸‚åŒ–å’Œå…¨çƒåŸå¸‚äººå£çš„å¢åŠ ç»™åŸå¸‚å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬äº¤é€šæ‹¥å µå’Œç©ºæ°”æ±¡æŸ“é—®é¢˜ã€‚ä¸ºäº†ç®¡ç†äº¤é€šæµé‡å’Œå‡å°‘æ’æ”¾ï¼Œéœ€è¦é‡‡å–æœ‰æ•ˆçš„ç­–ç•¥ã€‚åœ¨å®è·µä¸­ï¼Œä¼ ç»Ÿçš„äº¤é€šæµé‡æ¨¡æ‹Ÿè¢«ç”¨æ¥æµ‹è¯•è¿™äº›ç­–ç•¥ã€‚ç„¶è€Œï¼Œè¾ƒé«˜çš„è®¡ç®—å¼ºåº¦é€šå¸¸é™åˆ¶äº†å…¶åœ¨è°ƒæŸ¥å¤§é‡ä¸åŒåœºæ™¯ä»¥è¯„ä¼°æœ€ä½³æ”¿ç­–æ–¹é¢çš„é€‚ç”¨æ€§ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä½œä¸ºå¤§è§„æ¨¡åŸºäºä¸»ä½“çš„ä»¿çœŸæ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨å·´é»MATSimæ¨¡å‹çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼ŒGNNæœ‰æ•ˆåœ°å­¦ä¹ äº†å®¹é‡å‡å°‘æ”¿ç­–å¯¹åŸå¸‚èŒƒå›´å†…äº¤é€šæµé‡çš„å½±å“ã€‚å¯¹ä¸åŒç±»å‹çš„é“è·¯å’Œåœºæ™¯çš„ç»©æ•ˆåˆ†æè¡¨æ˜ï¼ŒGNNèƒ½å¤Ÿå‡†ç¡®åœ°æ•æ‰æ”¿ç­–å¯¹åŸºäºè¾¹ç¼˜çš„äº¤é€šæµé‡çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›´æ¥å—æ”¿ç­–å½±å“çš„é“è·¯å’Œäº¤é€šæµé‡è¾ƒå¤§çš„é“è·¯ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11057v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡åŸå¸‚åŒ–åŠåŸå¸‚äººå£çš„å¿«é€Ÿå¢é•¿ç»™å…¨çƒåŸå¸‚å¸¦æ¥äº†è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚äº¤é€šæ‹¥å µå’Œç©ºæ°”æ±¡æŸ“é—®é¢˜æ—¥ç›Šä¸¥é‡ã€‚ä¸ºç®¡ç†äº¤é€šæµé‡å¹¶å‡å°‘æ’æ”¾ï¼Œéœ€è¦æœ‰æ•ˆçš„ç­–ç•¥ã€‚ä¼ ç»Ÿäº¤é€šæµé‡æ¨¡æ‹Ÿå¸¸ç”¨äºæµ‹è¯•è¿™äº›ç­–ç•¥ï¼Œä½†å…¶é«˜è®¡ç®—å¼ºåº¦é™åˆ¶äº†å…¶åœ¨è¯„ä¼°æœ€ä½³æ”¿ç­–æ—¶å¯¹ä¸åŒåœºæ™¯çš„å¹¿æ³›è°ƒæŸ¥ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä½œä¸ºå¤§è§„æ¨¡åŸºäºä»£ç†çš„æ¨¡æ‹Ÿæ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä»¥å·´é»çš„MATSimæ¨¡å‹ä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼ŒGNNæœ‰æ•ˆåœ°å­¦ä¹ äº†å®¹é‡å‡å°‘æ”¿ç­–å¯¹åŸå¸‚èŒƒå›´å†…äº¤é€šæµé‡çš„å½±å“ã€‚é’ˆå¯¹ä¸åŒé“è·¯ç±»å‹å’Œåœºæ™¯çš„æ€§èƒ½åˆ†æè¡¨æ˜ï¼ŒGNNèƒ½å¤Ÿå‡†ç¡®æ•æ‰æ”¿ç­–å¯¹è¾¹ç¼˜äº¤é€šæµé‡çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å—æ”¿ç­–ç›´æ¥å½±å“å’Œäº¤é€šæµé‡è¾ƒé«˜çš„é“è·¯ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚åŒ–å’Œäººå£å¢é•¿å¸¦æ¥äº¤é€šæ‹¥å µå’Œç©ºæ°”æ±¡æŸ“ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿäº¤é€šæµé‡æ¨¡æ‹Ÿå¸¸ç”¨äºæµ‹è¯•ç­–ç•¥ï¼Œä½†é«˜è®¡ç®—å¼ºåº¦é™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯çš„å¹¿æ³›æ€§ã€‚</li>
<li>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å¯ä½œä¸ºå¤§è§„æ¨¡åŸºäºä»£ç†çš„æ¨¡æ‹Ÿæ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>GNNåœ¨å·´é»çš„MATSimæ¨¡å‹ä¸Šæœ‰æ•ˆå­¦ä¹ å®¹é‡å‡å°‘æ”¿ç­–å¯¹äº¤é€šæµé‡çš„å½±å“ã€‚</li>
<li>GNNèƒ½å¤Ÿå‡†ç¡®æ•æ‰æ”¿ç­–å¯¹è¾¹ç¼˜äº¤é€šæµé‡çš„å½±å“ã€‚</li>
<li>å—æ”¿ç­–ç›´æ¥å½±å“å’Œäº¤é€šæµé‡è¾ƒé«˜çš„é“è·¯å¯¹GNNçš„å‡†ç¡®åº¦æœ‰è¾ƒé«˜è¦æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c0b4aa3ec3bad2dfe1c11ce5f61ef9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a381659ecae3a8a7fa7bf4e0af9ca3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88dc5f9e4a20940af4d4ef374f1d0aa0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HIVEX-A-High-Impact-Environment-Suite-for-Multi-Agent-Research-extended-version"><a href="#HIVEX-A-High-Impact-Environment-Suite-for-Multi-Agent-Research-extended-version" class="headerlink" title="HIVEX: A High-Impact Environment Suite for Multi-Agent Research   (extended version)"></a>HIVEX: A High-Impact Environment Suite for Multi-Agent Research   (extended version)</h2><p><strong>Authors:Philipp Dominic Siedler</strong></p>
<p>Games have been vital test beds for the rapid development of Agent-based research. Remarkable progress has been achieved in the past, but it is unclear if the findings equip for real-world problems. While pressure grows, some of the most critical ecological challenges can find mitigation and prevention solutions through technology and its applications. Most real-world domains include multi-agent scenarios and require machine-machine and human-machine collaboration. Open-source environments have not advanced and are often toy scenarios, too abstract or not suitable for multi-agent research. By mimicking real-world problems and increasing the complexity of environments, we hope to advance state-of-the-art multi-agent research and inspire researchers to work on immediate real-world problems. Here, we present HIVEX, an environment suite to benchmark multi-agent research focusing on ecological challenges. HIVEX includes the following environments: Wind Farm Control, Wildfire Resource Management, Drone-Based Reforestation, Ocean Plastic Collection, and Aerial Wildfire Suppression. We provide environments, training examples, and baselines for the main and sub-tasks. All trained models resulting from the experiments of this work are hosted on Hugging Face. We also provide a leaderboard on Hugging Face and encourage the community to submit models trained on our environment suite. </p>
<blockquote>
<p>æ¸¸æˆå¯¹äºåŸºäºä»£ç†ç ”ç©¶çš„å¿«é€Ÿå‘å±•èµ·åˆ°äº†è‡³å…³é‡è¦çš„æµ‹è¯•å¹³å°ä½œç”¨ã€‚è¿‡å»å–å¾—äº†ä»¤äººç©ç›®çš„è¿›å±•ï¼Œä½†å°šä¸æ¸…æ¥šè¿™äº›å‘ç°æ˜¯å¦é€‚ç”¨äºç°å®ä¸–ç•Œçš„é—®é¢˜ã€‚éšç€å‹åŠ›è¶Šæ¥è¶Šå¤§ï¼Œä¸€äº›æœ€é‡è¦çš„ç”Ÿæ€æŒ‘æˆ˜å¯ä»¥é€šè¿‡æŠ€æœ¯å’Œå…¶åº”ç”¨æ‰¾åˆ°ç¼“è§£å’Œé¢„é˜²çš„è§£å†³æ–¹æ¡ˆã€‚å¤§å¤šæ•°ç°å®ä¸–ç•Œé¢†åŸŸéƒ½åŒ…å«å¤šä»£ç†åœºæ™¯ï¼Œéœ€è¦æœºå™¨ä¸æœºå™¨ä»¥åŠäººä¸æœºå™¨ä¹‹é—´çš„åä½œã€‚ç„¶è€Œï¼Œå¼€æºç¯å¢ƒå¹¶æœªå–å¾—è¿›å±•ï¼Œå¸¸å¸¸æ˜¯ç©å…·åœºæ™¯è¿‡äºæŠ½è±¡æˆ–ä¸é€‚ç”¨äºå¤šä»£ç†ç ”ç©¶ã€‚é€šè¿‡æ¨¡æ‹Ÿç°å®é—®é¢˜å’Œå¢åŠ ç¯å¢ƒå¤æ‚æ€§ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡å¤šä»£ç†ç ”ç©¶æ¥æå‡å‰æ²¿æŠ€æœ¯å’Œæ¿€åŠ±ç ”ç©¶äººå‘˜è§£å†³å½“å‰ç°å®ä¸–ç•Œçš„é—®é¢˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¨å‡ºHIVEXï¼Œä¸€ä¸ªä¸“æ³¨äºç”Ÿæ€æŒ‘æˆ˜çš„å¤šä»£ç†ç ”ç©¶åŸºå‡†æµ‹è¯•ç¯å¢ƒå¥—ä»¶ã€‚HIVEXåŒ…æ‹¬ä»¥ä¸‹ç¯å¢ƒï¼šé£åŠ›å‘ç”µå‚æ§åˆ¶ã€é‡ç«èµ„æºç®¡ç†ã€æ— äººæœºå‚ä¸é€ æ—ã€æµ·æ´‹å¡‘æ–™æ”¶é›†ä»¥åŠèˆªç©ºé‡ç«å‹åˆ¶ã€‚æˆ‘ä»¬æä¾›äº†ä¸»è¦ä»»åŠ¡å’Œå­ä»»åŠ¡çš„ç¯å¢ƒã€è®­ç»ƒç¤ºä¾‹å’ŒåŸºå‡†çº¿ã€‚æ‰€æœ‰ç”±æ­¤é¡¹å·¥ä½œå®éªŒè®­ç»ƒå¾—åˆ°çš„æ¨¡å‹éƒ½æ‰˜ç®¡åœ¨Hugging Faceä¸Šã€‚æˆ‘ä»¬è¿˜å°†åœ¨Hugging Faceä¸Šæä¾›ä¸€ä¸ªæ’è¡Œæ¦œï¼Œé¼“åŠ±ç¤¾åŒºæäº¤åœ¨æˆ‘ä»¬ç¯å¢ƒå¥—ä»¶ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04180v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ¸¸æˆåœ¨åŸºäºAgentçš„ç ”ç©¶ä¸­èµ·åˆ°äº†é‡è¦çš„æµ‹è¯•å¹³å°ä½œç”¨ã€‚å°½ç®¡è¿‡å»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°šä¸æ¸…æ¥šè¿™äº›å‘ç°æ˜¯å¦é€‚ç”¨äºç°å®ä¸–ç•Œçš„é—®é¢˜ã€‚å¤§å¤šæ•°ç°å®ä¸–ç•Œé¢†åŸŸæ¶‰åŠå¤šæ™ºèƒ½ä½“åœºæ™¯ï¼Œéœ€è¦æœºå™¨ä¸æœºå™¨ä»¥åŠäººä¸æœºå™¨ä¹‹é—´çš„åä½œã€‚é€šè¿‡æ¨¡æ‹ŸçœŸå®é—®é¢˜å¹¶å¢åŠ ç¯å¢ƒå¤æ‚æ€§ï¼Œæˆ‘ä»¬æœŸæœ›æ¨åŠ¨æœ€æ–°çš„å¤šæ™ºèƒ½ä½“ç ”ç©¶ï¼Œå¹¶æ¿€åŠ±ç ”ç©¶äººå‘˜å…³æ³¨å³æ—¶è§£å†³ç°å®é—®é¢˜çš„èƒ½åŠ›ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬ä»‹ç»äº†HIVEXï¼Œä¸€ä¸ªæ—¨åœ¨é’ˆå¯¹ç”Ÿæ€æŒ‘æˆ˜è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„å¤šæ™ºèƒ½ä½“ç ”ç©¶ç¯å¢ƒå¥—ä»¶ã€‚HIVEXåŒ…æ‹¬å¦‚ä¸‹ç¯å¢ƒï¼šé£åŠ›å†œåœºæ§åˆ¶ã€é‡ç«èµ„æºç®¡ç†ã€æ— äººæœºé€ æ—ã€æµ·æ´‹å¡‘æ–™æ”¶é›†å’Œç©ºä¸­ç­ç«ã€‚æˆ‘ä»¬æä¾›ç¯å¢ƒã€è®­ç»ƒç¤ºä¾‹å’Œä¸»è¦ä»»åŠ¡åŠå­ä»»åŠ¡çš„åŸºå‡†çº¿ã€‚æ‰€æœ‰ç”±æ­¤å®éªŒè®­ç»ƒå¾—åˆ°çš„æ¨¡å‹éƒ½æ‰˜ç®¡åœ¨Hugging Faceä¸Šã€‚æˆ‘ä»¬ä¹Ÿé¼“åŠ±ç¤¾åŒºæäº¤åœ¨æˆ‘ä»¬ç¯å¢ƒå¥—ä»¶ä¸Šè®­ç»ƒçš„æ¨¡å‹å¹¶åœ¨Hugging Faceä¸Šå±•ç¤ºæ’åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¸¸æˆå¯¹äºåŸºäºAgentçš„ç ”ç©¶èµ·åˆ°äº†é‡è¦çš„æµ‹è¯•å¹³å°ä½œç”¨ã€‚</li>
<li>å°½ç®¡åœ¨AgentæŠ€æœ¯æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°å®ä¸–ç•Œé—®é¢˜çš„å¤æ‚æ€§éœ€è¦æ›´æ·±å…¥çš„ç ”ç©¶æ¥åº”ç”¨ç°æœ‰æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-078c67b0d6d0f76a015ce39a240add00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9dd015289f5f82d4720380603244348.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99aca0218394a17d87a1c0819b93230f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Brained-GUI-Agents-A-Survey"><a href="#Large-Language-Model-Brained-GUI-Agents-A-Survey" class="headerlink" title="Large Language Model-Brained GUI Agents: A Survey"></a>Large Language Model-Brained GUI Agents: A Survey</h2><p><strong>Authors:Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</strong></p>
<p>GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.   To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents. </p>
<blockquote>
<p>é•¿ä¹…ä»¥æ¥ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰éƒ½æ˜¯äººæœºäº¤äº’çš„æ ¸å¿ƒï¼Œå®ƒæä¾›äº†ä¸€ç§ç›´è§‚ã€è§†è§‰é©±åŠ¨çš„æ–¹å¼æ¥è®¿é—®å’Œä¸æ•°å­—ç³»ç»Ÿäº¤äº’ã€‚å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œç‰¹åˆ«æ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œå·²ç»å¼€å¯äº†GUIè‡ªåŠ¨åŒ–çš„æ–°æ—¶ä»£ã€‚å®ƒä»¬åœ¨è‡ªç„¶è¯­è¨€ç†è§£ã€ä»£ç ç”Ÿæˆå’Œè§†è§‰å¤„ç†ç­‰æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚è¿™ä¸ºæ–°ä¸€ä»£åŸºäºLLMçš„GUIä»£ç†é“ºå¹³äº†é“è·¯ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿè§£é‡Šå¤æ‚çš„GUIå…ƒç´ ï¼Œå¹¶åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤è‡ªä¸»æ‰§è¡Œæ“ä½œã€‚è¿™äº›ä»£ç†ä»£è¡¨äº†èŒƒå¼è½¬å˜ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç®€å•çš„å‘½ä»¤æ‰§è¡Œå¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚å®ƒä»¬çš„åº”ç”¨ç¨‹åºè·¨è¶Šç½‘é¡µå¯¼èˆªã€ç§»åŠ¨åº”ç”¨ç¨‹åºäº¤äº’å’Œæ¡Œé¢è‡ªåŠ¨åŒ–ï¼Œæä¾›å˜é©æ€§çš„ç”¨æˆ·ä½“éªŒï¼Œå½»åº•æ”¹å˜ä¸ªäººä¸è½¯ä»¶çš„äº¤äº’æ–¹å¼ã€‚è¿™ä¸ªæ–°å…´é¢†åŸŸæ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œåœ¨ç ”ç©¶å’Œå·¥ä¸šé¢†åŸŸéƒ½å–å¾—äº†é‡å¤§è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18279v7">PDF</a> The collection of papers reviewed in this survey will be hosted and   regularly updated on the GitHub repository:   <a target="_blank" rel="noopener" href="https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey">https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey</a> Additionally, a   searchable webpage is available at <a target="_blank" rel="noopener" href="https://aka.ms/gui-agent">https://aka.ms/gui-agent</a> for easier access   and exploration</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GUIåœ¨äººæœºäº¤äº’ä¸­çš„é•¿æœŸé‡è¦åœ°ä½ï¼Œä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹çš„å‡ºç°ä¸ºGUIè‡ªåŠ¨åŒ–å¸¦æ¥çš„é©æ–°ã€‚æ–°ä¸€ä»£LLMé©±åŠ¨çš„GUIä»£ç†èƒ½è§£è¯»å¤æ‚çš„GUIå…ƒç´ ï¼Œå¹¶æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤è‡ªä¸»æ‰§è¡ŒåŠ¨ä½œã€‚è¿™äº›ä»£ç†ä»£è¡¨äº†ç”¨æˆ·æ‰§è¡Œå¤æ‚å¤šæ­¥éª¤ä»»åŠ¡çš„è½¬å˜æ–¹å¼ï¼Œåº”ç”¨èŒƒå›´éåŠç½‘é¡µå¯¼èˆªã€ç§»åŠ¨åº”ç”¨äº¤äº’å’Œæ¡Œé¢è‡ªåŠ¨åŒ–ã€‚æœ¬æ–‡è¿˜å¯¹LLMé©±åŠ¨çš„GUIä»£ç†çš„å†å²æ¼”å˜ã€æ ¸å¿ƒç»„ä»¶å’Œå…ˆè¿›æŠ€æœ¯è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œæ¢è®¨äº†ç°æœ‰çš„GUIä»£ç†æ¡†æ¶ç­‰é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIåœ¨äººæœºäº¤äº’ä¸­å æ®é‡è¦åœ°ä½ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹ä¸ºGUIè‡ªåŠ¨åŒ–å¸¦æ¥é©æ–°ã€‚</li>
<li>LLMé©±åŠ¨çš„GUIä»£ç†èƒ½è§£è¯»å¤æ‚çš„GUIå…ƒç´ å¹¶æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤è‡ªä¸»æ‰§è¡ŒåŠ¨ä½œã€‚</li>
<li>LLMé©±åŠ¨çš„GUIä»£ç†åœ¨ç”¨æˆ·æ‰§è¡Œå¤æ‚å¤šæ­¥éª¤ä»»åŠ¡æ—¶æä¾›äº†é©å‘½æ€§çš„ä½“éªŒã€‚</li>
<li>LLMé©±åŠ¨çš„GUIä»£ç†çš„åº”ç”¨èŒƒå›´å¹¿æ³›ï¼ŒåŒ…æ‹¬ç½‘é¡µå¯¼èˆªã€ç§»åŠ¨åº”ç”¨äº¤äº’å’Œæ¡Œé¢è‡ªåŠ¨åŒ–ç­‰ã€‚</li>
<li>å½“å‰çš„ç ”ç©¶é¢†åŸŸæ¶µç›–äº†GUIä»£ç†æ¡†æ¶ã€æ•°æ®æ”¶é›†å’Œåˆ©ç”¨ã€é’ˆå¯¹GUIä»»åŠ¡çš„è¡ŒåŠ¨æ¨¡å‹å¼€å‘ç­‰æ–¹å‘ã€‚</li>
<li>å½“å‰ç ”ç©¶ä»å­˜åœ¨å…³é”®çš„ç ”ç©¶ç©ºç™½å’Œæ½œåœ¨çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f1db171a0f7fa7a2d3ce95abbfaf55cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6a4be3fb98a0b63af42b7b053140fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58397a39ca33d56b26bd204d67f82c65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff2f9194ea9b361fc79cc5d28a051b8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73be54aa7eec28653cbbb5bc3d597611.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Embodied-Agent-Interface-Benchmarking-LLMs-for-Embodied-Decision-Making"><a href="#Embodied-Agent-Interface-Benchmarking-LLMs-for-Embodied-Decision-Making" class="headerlink" title="Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making"></a>Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</h2><p><strong>Authors:Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu</strong></p>
<p>We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc. Overall, our benchmark offers a comprehensive assessment of LLMsâ€™ performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems, and providing insights for effective and selective use of LLMs in embodied decision making. </p>
<blockquote>
<p>æˆ‘ä»¬æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®ä½“å†³ç­–ä¸­çš„è¡¨ç°ã€‚è™½ç„¶å·²æœ‰å¤§é‡å·¥ä½œåˆ©ç”¨LLMè¿›è¡Œå®ä½“ç¯å¢ƒä¸­çš„å†³ç­–ï¼Œä½†ç”±äºLLMé€šå¸¸åœ¨ä¸åŒçš„é¢†åŸŸã€ä¸åŒçš„ç›®çš„ä¸­åº”ç”¨ï¼Œå¹¶ä¸”åŸºäºä¸åŒçš„è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œæ„å»ºï¼Œå› æ­¤æˆ‘ä»¬å¯¹å®ƒä»¬çš„æ€§èƒ½ç¼ºä¹ç³»ç»Ÿçš„ç†è§£ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è¯„ä¼°å¾€å¾€åªä¾èµ–äºæœ€ç»ˆçš„æˆåŠŸç‡ï¼Œè¿™ä½¿å¾—å¾ˆéš¾ç¡®å®šLLMä¸­ç¼ºå°‘çš„èƒ½åŠ›ä»¥åŠé—®é¢˜çš„æ‰€åœ¨ï¼Œä»è€Œé˜»ç¢äº†å®ä½“ä»£ç†æœ‰æ•ˆåœ°é€‰æ‹©æ€§åˆ©ç”¨LLMã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨æ¥å£ï¼ˆå®ä½“ä»£ç†æ¥å£ï¼‰ï¼Œè¯¥æ¥å£æ”¯æŒå„ç§ä»»åŠ¡çš„å½¢å¼åŒ–ä»¥åŠåŸºäºLLMçš„æ¨¡å—è¾“å…¥è¾“å‡ºè§„èŒƒã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå…è®¸æˆ‘ä»¬ç»Ÿä¸€1ï¼‰æ¶‰åŠçŠ¶æ€å’Œä¸´æ—¶ç›®æ ‡çš„å¹¿æ³›å®ä½“å†³ç­–ä»»åŠ¡ï¼›2ï¼‰ç”¨äºå†³ç­–çš„å››ç§å¸¸ç”¨åŸºäºLLMçš„æ¨¡å—ï¼šç›®æ ‡è§£é‡Šã€å­ç›®æ ‡åˆ†è§£ã€åŠ¨ä½œåºåˆ—å’Œè¿‡æ¸¡å»ºæ¨¡ï¼›3ï¼‰ä¸€ç§ç²¾ç»†çš„åº¦é‡é›†åˆï¼Œå°†è¯„ä¼°åˆ†è§£ä¸ºå„ç§ç±»å‹çš„é”™è¯¯ï¼Œå¦‚å¹»è§‰é”™è¯¯ã€å¯è®¿é—®æ€§é”™è¯¯ã€å„ç§ç±»å‹çš„è§„åˆ’é”™è¯¯ç­‰ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ºLLMåœ¨ä¸åŒå­ä»»åŠ¡ä¸Šçš„è¡¨ç°æä¾›äº†å…¨é¢çš„è¯„ä¼°ï¼ŒæŒ‡å‡ºäº†LLMåœ¨å®ä½“AIç³»ç»Ÿä¸­çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ä¸ºåœ¨å®ä½“å†³ç­–ä¸­æœ‰æ•ˆå’Œé€‰æ‹©æ€§ä½¿ç”¨LLMæä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07166v3">PDF</a> Accepted for oral presentation at NeurIPS 2024 in the Datasets and   Benchmarks track. Final Camera version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®ä½“å†³ç­–åˆ¶å®šä¸­çš„è¡¨ç°ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡å·²æœ‰å¤§é‡å·¥ä½œåˆ©ç”¨LLMsè¿›è¡Œå®ä½“ç¯å¢ƒå†³ç­–ï¼Œä½†ç”±äºLLMsé€šå¸¸åº”ç”¨äºä¸åŒé¢†åŸŸã€ä¸åŒç›®çš„ï¼Œå¹¶åŸºäºä¸åŒè¾“å…¥å’Œè¾“å‡ºæ„å»ºï¼Œæˆ‘ä»¬ä»ç¼ºä¹å¯¹å…¶æ€§èƒ½çš„ç³»ç»Ÿæ€§ç†è§£ã€‚ç°æœ‰è¯„ä¼°å¾€å¾€ä»…ä¾èµ–æœ€ç»ˆæˆåŠŸç‡ï¼Œè¿™ä½¿å¾—éš¾ä»¥ç¡®å®šLLMsç¼ºå°‘å“ªäº›èƒ½åŠ›ï¼Œé—®é¢˜å‡ºåœ¨å“ªé‡Œï¼Œè¿›è€Œé˜»ç¢å®ä½“ä»£ç†äººæœ‰æ•ˆåœ°é€‰æ‹©æ€§åˆ©ç”¨LLMsã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« æå‡ºä¸€ç§é€šç”¨æ¥å£ï¼ˆEmbodied Agent Interfaceï¼‰ï¼Œæ”¯æŒå„ç±»ä»»åŠ¡çš„æ­£è§„åŒ–ä»¥åŠLLMæ¨¡å—è¾“å…¥è¾“å‡ºçš„ç‰¹å®šè§„èŒƒã€‚è¯¥æ¥å£å¯ç»Ÿä¸€å®ä½“å†³ç­–åˆ¶å®šä»»åŠ¡çš„å¹¿æ³›é›†åˆã€å››ä¸ªå¸¸ç”¨çš„LLMå†³ç­–æ¨¡å—ä»¥åŠç²¾ç»†åº¦é‡çš„é›†åˆã€‚é€šè¿‡ç»†åŒ–è¯„ä¼°ä¸ºå„ç§ç±»å‹çš„é”™è¯¯ï¼Œå¦‚å¹»æƒ³é”™è¯¯ã€è´Ÿæ‹…èƒ½åŠ›é”™è¯¯ã€å„ç§è§„åˆ’é”™è¯¯ç­‰ï¼Œè¯¥åŸºå‡†æµ‹è¯•ä¸ºLLMsåœ¨ä¸åŒå­ä»»åŠ¡ä¸Šçš„è¡¨ç°æä¾›äº†å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶æŒ‡å‡ºäº†LLMé©±åŠ¨çš„å®ä½“AIç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºæœ‰æ•ˆé€‰æ‹©æ€§ä½¿ç”¨LLMsè¿›è¡Œå®ä½“å†³ç­–åˆ¶å®šæä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æ—¨åœ¨å…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®ä½“å†³ç­–åˆ¶å®šä¸­çš„è¡¨ç°ã€‚</li>
<li>LLMsåœ¨å¤šä¸ªé¢†åŸŸã€ç›®çš„å’Œè¾“å…¥&#x2F;è¾“å‡ºåŸºç¡€ä¸Šæ„å»ºï¼Œç¼ºä¹ç³»ç»Ÿæ€§ç†è§£ã€‚</li>
<li>ç°æœ‰è¯„ä¼°ä¸»è¦ä¾èµ–æœ€ç»ˆæˆåŠŸç‡ï¼Œéš¾ä»¥ç¡®å®šLLMsçš„å…·ä½“é—®é¢˜æ‰€åœ¨ã€‚</li>
<li>æå‡ºä¸€ä¸ªé€šç”¨æ¥å£ï¼ˆEmbodied Agent Interfaceï¼‰ä»¥ç»Ÿä¸€ä¸åŒç±»å‹çš„ä»»åŠ¡ã€LLMæ¨¡å—å’Œç²¾ç»†åº¦é‡æ ‡å‡†ã€‚</li>
<li>æ¥å£æ¶µç›–å®ä½“å†³ç­–åˆ¶å®šä»»åŠ¡çš„å¹¿æ³›é›†åˆã€‚</li>
<li>æ¥å£åŒ…æ‹¬å››ä¸ªå¸¸ç”¨çš„LLMå†³ç­–æ¨¡å—ï¼šç›®æ ‡è§£è¯»ã€å­ç›®æ ‡åˆ†è§£ã€è¡ŒåŠ¨æ’åºå’Œè¿‡æ¸¡å»ºæ¨¡ã€‚</li>
<li>é€šè¿‡ç»†åŒ–è¯„ä¼°ï¼Œè¯¥åŸºå‡†æµ‹è¯•ä¸ºLLMsè¡¨ç°æä¾›äº†å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶æŒ‡å‡ºäº†å…¶ä¼˜ç¼ºç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35ff3a45e63fcf217cd1a949d42cd760.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85fe89e005c793cff1099bf36cb8a335.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f253181e2b9357a987af28321727e8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b38b7054387bb62a6ca3a02aad90302.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-394f7fb94dfc0428cc51ded468686bda.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VLM-Agents-Generate-Their-Own-Memories-Distilling-Experience-into-Embodied-Programs-of-Thought"><a href="#VLM-Agents-Generate-Their-Own-Memories-Distilling-Experience-into-Embodied-Programs-of-Thought" class="headerlink" title="VLM Agents Generate Their Own Memories: Distilling Experience into   Embodied Programs of Thought"></a>VLM Agents Generate Their Own Memories: Distilling Experience into   Embodied Programs of Thought</h2><p><strong>Authors:Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, Katerina Fragkiadaki</strong></p>
<p>Large-scale LLMs and VLMs excel at few-shot learning but require high-quality examples. We introduce In-Context Abstraction Learning (ICAL), which iteratively refines suboptimal trajectories into high-quality data with optimized actions and detailed reasoning. Given an inefficient demonstration, a VLM corrects actions and annotates causal relationships, object states, subgoals, and task-relevant visuals, forming â€œprograms of thought.â€ With human feedback, these programs are improved as the agent executes them in a similar environment. The resulting examples, used as prompt context or fine-tuning data, significantly boost decision-making while reducing human feedback needs. ICAL surpasses state-of-the-art in TEACh (dialogue-based instruction following), VisualWebArena (multimodal web agents), and Ego4D (egocentric video action anticipation). In TEACh, combining fine-tuning and retrieval on ICAL examples outperforms raw human demonstrations and expert examples, achieving a 17.5% increase in goal-condition success. In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success rate 1.6x over GPT-4V, while fine-tuning Qwen2-VL achieves a 2.8x improvement. In Ego4D, ICAL outperforms few-shot GPT-4V and remains competitive with supervised models. Overall, ICAL scales 2x better than raw human demonstrations and reduces manual prompt engineering. </p>
<blockquote>
<p>å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘é‡å­¦ä¹ æ ·æœ¬ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦é«˜è´¨é‡çš„ä¾‹å­ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸Šä¸‹æ–‡æŠ½è±¡å­¦ä¹ ï¼ˆICALï¼‰ï¼Œå®ƒèƒ½å¤Ÿè¿­ä»£åœ°ä¼˜åŒ–æ¬¡ä¼˜è½¨è¿¹ï¼Œå½¢æˆé«˜è´¨é‡æ•°æ®ï¼Œå…¶ä¸­åŒ…å«ä¼˜åŒ–åçš„åŠ¨ä½œå’Œè¯¦ç»†æ¨ç†ã€‚å¯¹äºä½æ•ˆçš„æ¼”ç¤ºï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ä¼šçº æ­£åŠ¨ä½œï¼Œå¹¶æ³¨é‡Šå› æœå…³ç³»ã€å¯¹è±¡çŠ¶æ€ã€å­ç›®æ ‡å’Œä»»åŠ¡ç›¸å…³è§†è§‰ï¼Œå½¢æˆâ€œæ€ç»´ç¨‹åºâ€ã€‚éšç€ä»£ç†åœ¨ç±»ä¼¼ç¯å¢ƒä¸­æ‰§è¡Œè¿™äº›ç¨‹åºï¼Œé€šè¿‡äººç±»åé¦ˆï¼Œè¿™äº›ç¨‹åºå¾—åˆ°äº†æ”¹è¿›ã€‚å°†ç»“æœç¤ºä¾‹ç”¨ä½œæç¤ºä¸Šä¸‹æ–‡æˆ–å¾®è°ƒæ•°æ®ï¼Œå¯æ˜¾è‘—æé«˜å†³ç­–èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äººç±»åé¦ˆéœ€æ±‚ã€‚ICALåœ¨TEAChï¼ˆåŸºäºå¯¹è¯çš„æŒ‡ä»¤éµå¾ªï¼‰ã€VisualWebArenaï¼ˆå¤šæ¨¡å¼ç½‘ç»œä»£ç†ï¼‰å’ŒEgo4Dï¼ˆä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘åŠ¨ä½œé¢„æµ‹ï¼‰ç­‰é¢†åŸŸè¶…è¶Šäº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚åœ¨TEAChä¸­ï¼Œç»“åˆå¾®è°ƒä¸åœ¨ICALç¤ºä¾‹ä¸Šè¿›è¡Œæ£€ç´¢ä¼˜äºåŸå§‹äººç±»æ¼”ç¤ºå’Œä¸“å®¶ç¤ºä¾‹ï¼Œç›®æ ‡æ¡ä»¶æˆåŠŸç‡æé«˜äº†17.5%ã€‚åœ¨VisualWebArenaä¸­ï¼Œä½¿ç”¨ICALå¢å¼ºæ£€ç´¢çš„GPT-4Vä»»åŠ¡æˆåŠŸç‡æé«˜äº†1.6å€ï¼Œè€Œä½¿ç”¨ICALå¯¹Qwen2-VLè¿›è¡Œå¾®è°ƒçš„ä»»åŠ¡æˆåŠŸç‡æé«˜äº†2.8å€ã€‚åœ¨Ego4Dä¸­ï¼ŒICALè¡¨ç°ä¼˜äºå°‘é‡GPT-4Væ ·æœ¬ï¼Œå¹¶ä¸ç›‘ç£æ¨¡å‹ä¿æŒç«äº‰åŠ›ã€‚æ€»ä½“è€Œè¨€ï¼ŒICALçš„è¡¨ç°æ¯”åŸå§‹äººç±»æ¼”ç¤ºé«˜å‡ºä¸¤å€ï¼Œå¹¶å‡å°‘äº†æ‰‹åŠ¨æç¤ºå·¥ç¨‹çš„å·¥ä½œé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14596v5">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://ical-learning.github.io/">https://ical-learning.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹æ“…é•¿äºå°‘æ ·æœ¬å­¦ä¹ ï¼Œä½†éœ€è¦é«˜è´¨é‡èŒƒä¾‹ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œä¸Šä¸‹æ–‡æŠ½è±¡å­¦ä¹ â€ï¼ˆICALï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–åŠ¨ä½œå’Œè¯¦ç»†æ¨ç†ï¼Œå°†éä¼˜è´¨è½¨è¿¹è¿­ä»£ä¼˜åŒ–ä¸ºé«˜è´¨é‡æ•°æ®ã€‚é’ˆå¯¹ä½æ•ˆç¤ºèŒƒï¼Œè§†è§‰è¯­è¨€æ¨¡å‹å¯çº æ­£åŠ¨ä½œå¹¶æ ‡æ³¨å› æœå…³ç³»ã€å¯¹è±¡çŠ¶æ€ã€å­ç›®æ ‡å’Œä»»åŠ¡ç›¸å…³è§†è§‰å†…å®¹ï¼Œå½¢æˆâ€œæ€ç»´ç¨‹åºâ€ã€‚å€ŸåŠ©äººç±»åé¦ˆï¼Œè¿™äº›ç¨‹åºåœ¨æ‰§è¡Œç±»ä¼¼ç¯å¢ƒçš„ä»£ç†æ—¶å¾—åˆ°æ”¹è¿›ã€‚ä½¿ç”¨è¿™äº›èŒƒä¾‹ä½œä¸ºæç¤ºä¸Šä¸‹æ–‡æˆ–å¾®è°ƒæ•°æ®ï¼Œå¯æ˜¾è‘—æé«˜å†³ç­–èƒ½åŠ›å¹¶å‡å°‘äººç±»åé¦ˆéœ€æ±‚ã€‚åœ¨å¤šä¸ªä»»åŠ¡ä¸Šï¼ŒICALè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚æ€»ä½“è€Œè¨€ï¼ŒICALæ¯”åŸå§‹äººç±»ç¤ºèŒƒè¡¨ç°æ›´å¥½ï¼Œæ‰‹åŠ¨æç¤ºå·¥ç¨‹æ›´å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹æ“…é•¿å°‘æ ·æœ¬å­¦ä¹ ï¼Œä½†éœ€é«˜è´¨é‡èŒƒä¾‹ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºâ€œä¸Šä¸‹æ–‡æŠ½è±¡å­¦ä¹ â€ï¼ˆICALï¼‰çš„æ–¹æ³•ï¼Œå¯å°†éä¼˜è´¨ç¤ºèŒƒè½¬åŒ–ä¸ºé«˜è´¨é‡æ•°æ®ã€‚</li>
<li>VLMåœ¨ICALä¸­èƒ½å¤Ÿçº æ­£åŠ¨ä½œå¹¶æ ‡æ³¨è¯¦ç»†ä¿¡æ¯ï¼Œå½¢æˆâ€œæ€ç»´ç¨‹åºâ€ã€‚</li>
<li>é€šè¿‡äººç±»åé¦ˆï¼Œè¿™äº›â€œæ€ç»´ç¨‹åºâ€åœ¨æ‰§è¡Œç¯å¢ƒä¸­å¾—åˆ°æ”¹è¿›ã€‚</li>
<li>ICALèŒƒä¾‹ç”¨äºæç¤ºä¸Šä¸‹æ–‡æˆ–å¾®è°ƒæ•°æ®ï¼Œå¯æ˜¾è‘—æé«˜å†³ç­–èƒ½åŠ›å¹¶å‡å°‘äººç±»åé¦ˆéœ€æ±‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-92054ba819dbd3dc5c56d31b1298c0f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-017606ef990bd6441847d049b022518f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3348ca3dd297579731df7739f634fb4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TDAG-A-Multi-Agent-Framework-based-on-Dynamic-Task-Decomposition-and-Agent-Generation"><a href="#TDAG-A-Multi-Agent-Framework-based-on-Dynamic-Task-Decomposition-and-Agent-Generation" class="headerlink" title="TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and   Agent Generation"></a>TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and   Agent Generation</h2><p><strong>Authors:Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, Jinsong Su</strong></p>
<p>The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agentsâ€™ abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios. </p>
<blockquote>
<p>éšç€åƒChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œå·²ç»æ¿€å‘äº†åŸºäºLLMçš„ä»£ç†äººçš„å‘å±•ï¼Œè¿™äº›ä»£ç†äººèƒ½å¤Ÿå¤„ç†å¤æ‚ã€ç°å®ä¸–ç•Œçš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†äººåœ¨æ‰§è¡Œä»»åŠ¡æ—¶å¸¸å¸¸ç”±äºæ–¹æ³•ä¸Šçš„çº¦æŸï¼Œå¦‚è¯¯å·®ä¼ æ’­å’Œé€‚åº”æ€§æœ‰é™è€Œé‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€ä»»åŠ¡åˆ†è§£å’Œä»£ç†ç”Ÿæˆï¼ˆTDAGï¼‰çš„å¤šä»£ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŠ¨æ€åœ°å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºè¾ƒå°çš„å­ä»»åŠ¡ï¼Œå¹¶å°†æ¯ä¸ªå­ä»»åŠ¡åˆ†é…ç»™ä¸“é—¨ç”Ÿæˆçš„å­ä»£ç†ï¼Œä»è€Œåœ¨å¤šæ ·åŒ–å’Œä¸å¯é¢„æµ‹çš„ç°å®ä¸–ç•Œä»»åŠ¡ä¸­æé«˜é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ç¼ºä¹è¯„ä¼°å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡çš„å¢é‡è¿›å±•æ‰€éœ€çš„ç²’åº¦ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬åœ¨æ—…è¡Œè§„åˆ’çš„æƒ…å¢ƒä¸­å¼•å…¥äº†ItineraryBenchï¼Œå®ƒåŒ…å«ç›¸äº’å…³è”ã€é€æ­¥å¤æ‚çš„ä»»åŠ¡ä»¥åŠç²¾ç»†çš„è¯„ä¼°ç³»ç»Ÿã€‚ItineraryBenchæ—¨åœ¨è¯„ä¼°ä»£ç†åœ¨ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡ä¸­çš„è®°å¿†åŠ›ã€è§„åˆ’èƒ½åŠ›å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTDAGæ˜¾è‘—ä¼˜äºæ—¢å®šåŸºå‡†ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ä»»åŠ¡åœºæ™¯ä¸­çš„å“è¶Šé€‚åº”æ€§å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.10178v2">PDF</a> Accepted by Neural Networks</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ChatGPTçš„å‡ºç°æ¿€å‘äº†èƒ½å¤Ÿå¤„ç†å¤æ‚ã€ç°å®ä»»åŠ¡çš„LLMåŸºç¡€ä»£ç†çš„å‘å±•ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†åœ¨æ‰§è¡Œä»»åŠ¡æ—¶å¸¸å¸¸ç”±äºæ–¹æ³•è®ºä¸Šçš„é™åˆ¶ï¼Œå¦‚è¯¯å·®ä¼ æ’­å’Œé€‚åº”æ€§æœ‰é™è€Œé‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŠ¨æ€ä»»åŠ¡åˆ†è§£å’Œä»£ç†ç”Ÿæˆï¼ˆTDAGï¼‰çš„å¤šä»£ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŠ¨æ€åœ°å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªå­ä»»åŠ¡ç”Ÿæˆç‰¹å®šçš„å­ä»£ç†ï¼Œä»è€Œæé«˜åœ¨ä¸åŒä¸”ä¸å¯é¢„æµ‹çš„ç°å®ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ç¼ºä¹å¯¹å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡ä¸­å¢é‡è¿›å±•çš„ç²¾ç»†è¯„ä¼°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨æ—…è¡Œè®¡åˆ’çš„èƒŒæ™¯ä¸‹æ¨å‡ºäº†ItineraryBenchï¼Œå®ƒåŒ…å«ç›¸äº’å…³è”ã€é€æ­¥å¤æ‚çš„ä»»åŠ¡ä»¥åŠç²¾ç»†çš„è¯„ä»·ä½“ç³»ã€‚ItineraryBenchæ—¨åœ¨è¯„ä¼°ä»£ç†åœ¨ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡ä¸­çš„è®°å¿†ã€è§„åˆ’å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTDAGæ˜¾è‘—ä¼˜äºæ—¢å®šåŸºå‡†çº¿ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚ä»»åŠ¡åœºæ™¯ä¸­çš„å“è¶Šé€‚åº”æ€§å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¿€åŠ±äº†LLMåŸºç¡€ä»£ç†çš„å‘å±•ï¼Œè¿™äº›ä»£ç†èƒ½å¤„ç†å¤æ‚ã€ç°å®ä»»åŠ¡ã€‚</li>
<li>LLMä»£ç†åœ¨æ‰§è¡Œä»»åŠ¡æ—¶é¢ä¸´è¯¯å·®ä¼ æ’­å’Œé€‚åº”æ€§æœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†åŸºäºåŠ¨æ€ä»»åŠ¡åˆ†è§£å’Œä»£ç†ç”Ÿæˆï¼ˆTDAGï¼‰çš„å¤šä»£ç†æ¡†æ¶ã€‚</li>
<li>TDAGèƒ½åŠ¨æ€åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºå­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªå­ä»»åŠ¡ç”Ÿæˆç‰¹å®šå­ä»£ç†ï¼Œæé«˜é€‚åº”æ€§ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ç¼ºä¹è¯„ä¼°å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡çš„ç²¾ç»†æ ‡å‡†ã€‚</li>
<li>æ¨å‡ºItineraryBenchï¼Œç”¨äºè¯„ä¼°ä»£ç†åœ¨æ—…è¡Œè®¡åˆ’ä¸­çš„è®°å¿†ã€è§„åˆ’å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.10178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acc4b53a44ed901298ca4d83caeb4de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d896dd941f8daba033931d1a1c8292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-731a2778cacec38e7dac51af36ddba78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df05071146f8ef0e38b6a25d47eccb9a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Memory-Gym-Towards-Endless-Tasks-to-Benchmark-Memory-Capabilities-of-Agents"><a href="#Memory-Gym-Towards-Endless-Tasks-to-Benchmark-Memory-Capabilities-of-Agents" class="headerlink" title="Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of   Agents"></a>Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of   Agents</h2><p><strong>Authors:Marco Pleines, Matthias Pallasch, Frank Zimmer, Mike Preuss</strong></p>
<p>Memory Gym presents a suite of 2D partially observable environments, namely Mortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark memory capabilities in decision-making agents. These environments, originally with finite tasks, are expanded into innovative, endless formats, mirroring the escalating challenges of cumulative memory games such as â€œI packed my bagâ€. This progression in task design shifts the focus from merely assessing sample efficiency to also probing the levels of memory effectiveness in dynamic, prolonged scenarios. To address the gap in available memory-based Deep Reinforcement Learning baselines, we introduce an implementation within the open-source CleanRL library that integrates Transformer-XL (TrXL) with Proximal Policy Optimization. This approach utilizes TrXL as a form of episodic memory, employing a sliding window technique. Our comparative study between the Gated Recurrent Unit (GRU) and TrXL reveals varied performances across our finite and endless tasks. TrXL, on the finite environments, demonstrates superior effectiveness over GRU, but only when utilizing an auxiliary loss to reconstruct observations. Notably, GRU makes a remarkable resurgence in all endless tasks, consistently outperforming TrXL by significant margins. Website and Source Code: <a target="_blank" rel="noopener" href="https://marcometer.github.io/jmlr_2024.github.io/">https://marcometer.github.io/jmlr_2024.github.io/</a> </p>
<blockquote>
<p>â€œMemory Gymæä¾›äº†ä¸€ç³»åˆ—äºŒç»´éƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒï¼Œå³Mortar Mayhemã€Mystery Pathå’ŒSearing Spotlightsï¼Œæ—¨åœ¨è¯„ä¼°å†³ç­–åˆ¶å®šä»£ç†çš„è®°å¿†èƒ½åŠ›ã€‚è¿™äº›ç¯å¢ƒæœ€åˆå…·æœ‰æœ‰é™çš„ä»»åŠ¡ï¼Œç°å·²æ‰©å±•ä¸ºåˆ›æ–°çš„æ— å°½æ ¼å¼ï¼Œåæ˜ äº†ç´¯ç§¯è®°å¿†æ¸¸æˆï¼ˆå¦‚â€œæˆ‘æ‰“åŒ…äº†æˆ‘çš„è¡Œæâ€ï¼‰ä¸­ä¸æ–­å‡çº§çš„æŒ‘æˆ˜ã€‚ä»»åŠ¡è®¾è®¡çš„è¿›æ­¥ä½¿é‡ç‚¹ä»ä»…ä»…è¯„ä¼°æ ·æœ¬æ•ˆç‡è½¬å‘äº†æ¢ç©¶åŠ¨æ€æŒä¹…åœºæ™¯ä¸­çš„è®°å¿†æœ‰æ•ˆæ€§æ°´å¹³ã€‚ä¸ºäº†è§£å†³ç°æœ‰åŸºäºè®°å¿†çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ åŸºå‡†çº¿ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬åœ¨å¼€æºCleanRLåº“ä¸­å¼•å…¥äº†ä¸€ç§å®ç°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†Transformer-XLï¼ˆTrXLï¼‰ä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨TrXLä½œä¸ºä¸€ç§æƒ…èŠ‚è®°å¿†ï¼Œé‡‡ç”¨æ»‘åŠ¨çª—å£æŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨æœ‰é™å’Œæ— é™ä»»åŠ¡ä¹‹é—´è¿›è¡Œçš„æ¯”è¾ƒç ”ç©¶æ˜¾ç¤ºï¼ŒGRUä¸TrXLè¡¨ç°ä¸åŒã€‚åœ¨æœ‰é™ç¯å¢ƒä¸­ï¼Œå½“åˆ©ç”¨è¾…åŠ©æŸå¤±æ¥é‡å»ºè§‚å¯Ÿç»“æœæ—¶ï¼ŒTrXLç›¸è¾ƒäºGRUè¡¨ç°å‡ºæ›´é«˜çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ‰€æœ‰æ— å°½çš„ä»»åŠ¡ä¸­ï¼ŒGRUéƒ½è¡¨ç°å‡ºäº†æ˜¾è‘—çš„å¤è‹ï¼Œå§‹ç»ˆæ˜¾è‘—ä¼˜äºTrXLã€‚ç½‘ç«™å’Œæºä»£ç ï¼š<a target="_blank" rel="noopener" href="https://marcometer.github.io/jmlr_2024.github.io/">https://marcometer.github.io/jmlr_2024.github.io/</a>â€œ</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17207v6">PDF</a> 40 pages, 12 figures, 7 tables, accepted at JMLR</p>
<p><strong>Summary</strong></p>
<p>è®°å¿†è®­ç»ƒç¯å¢ƒè®¾è®¡ç”¨äºè¯„ä¼°å†³ç­–ä»£ç†äººçš„è®°å¿†èƒ½åŠ›ã€‚é€šè¿‡ä¸€ç³»åˆ—åˆ›æ–°çš„æ— é™æ ¼å¼ç¯å¢ƒï¼Œå¦‚Mortar Mayhemã€Mystery Pathå’ŒSearing Spotlightsï¼Œå¯¹è®°å¿†èƒ½åŠ›è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ä¸ºè§£å†³ç°æœ‰åŸºäºè®°å¿†çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ åŸºå‡†ä¸è¶³çš„é—®é¢˜ï¼Œå¼•å…¥äº†ä¸å¼€æºCleanRLåº“ç›¸ç»“åˆçš„Transformer-XLï¼ˆTrXLï¼‰ä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å®ç°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œåœ¨æœ‰é™ç¯å¢ƒä¸­ï¼Œä½¿ç”¨è¾…åŠ©æŸå¤±é‡æ„è§‚å¯Ÿçš„TrXLè¡¨ç°ä¼˜äºé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰ï¼Œè€Œåœ¨æ— é™ä»»åŠ¡ä¸­ï¼ŒGRUè¡¨ç°çªå‡ºï¼Œæ˜¾è‘—ä¼˜äºTrXLã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Memory Gymè®¾è®¡äº†ä¸€ç³»åˆ—ç¯å¢ƒç”¨äºè¯„ä¼°å†³ç­–ä»£ç†äººçš„è®°å¿†èƒ½åŠ›ã€‚</li>
<li>ç¯å¢ƒä»¥åˆ›æ–°çš„æ— é™æ ¼å¼å‘ˆç°ï¼Œæ¨¡æ‹Ÿç´¯ç§¯è®°å¿†æ¸¸æˆçš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³å½“å‰åŸºäºè®°å¿†çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ åŸºå‡†çš„ä¸è¶³ï¼Œæ•´åˆäº†Transformer-XLï¼ˆTrXLï¼‰ä¸è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å®ç°ã€‚</li>
<li>åœ¨æœ‰é™ç¯å¢ƒä¸­ï¼Œä½¿ç”¨è¾…åŠ©æŸå¤±é‡æ„è§‚å¯Ÿçš„TrXLè¡¨ç°ä¼˜äºGRUã€‚</li>
<li>åœ¨æ— é™ä»»åŠ¡ä¸­ï¼ŒGRUè¡¨ç°å“è¶Šï¼Œæ˜¾è‘—è¶…è¿‡TrXLçš„è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.17207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-afaf48f41d1fa8b0b99a17963ae665fe.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2409.11111v2/page_2_0.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  CBVLM Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_LLM/2501.12231v1/page_4_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  InternVideo2.5 Empowering Video MLLMs with Long and Rich Context   Modeling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
