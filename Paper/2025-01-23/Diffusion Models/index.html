<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  GPS as a Control Signal for Image Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2405.14477v2/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-23-æ›´æ–°"><a href="#2025-01-23-æ›´æ–°" class="headerlink" title="2025-01-23 æ›´æ–°"></a>2025-01-23 æ›´æ–°</h1><h2 id="GPS-as-a-Control-Signal-for-Image-Generation"><a href="#GPS-as-a-Control-Signal-for-Image-Generation" class="headerlink" title="GPS as a Control Signal for Image Generation"></a>GPS as a Control Signal for Image Generation</h2><p><strong>Authors:Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens</strong></p>
<p>We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure. </p>
<blockquote>
<p>æˆ‘ä»¬è¯æ˜ï¼Œç…§ç‰‡å…ƒæ•°æ®ä¸­åŒ…å«çš„GPSæ ‡ç­¾ä¸ºå›¾åƒç”Ÿæˆæä¾›äº†æœ‰ç”¨çš„æ§åˆ¶ä¿¡å·ã€‚æˆ‘ä»¬è®­ç»ƒGPSåˆ°å›¾åƒçš„æ¨¡å‹ï¼Œå¹¶å°†å…¶ç”¨äºéœ€è¦ç²¾ç»†ç†è§£å›¾åƒåœ¨åŸå¸‚å†…å¦‚ä½•å˜åŒ–çš„ä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œä»¥æ ¹æ®GPSå’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒã€‚å­¦ä¹ åˆ°çš„æ¨¡å‹ç”Ÿæˆäº†æ•æ‰ä¸åŒè¡—åŒºã€å…¬å›­å’Œåœ°æ ‡ç‹¬ç‰¹å¤–è§‚çš„å›¾åƒã€‚æˆ‘ä»¬è¿˜é€šè¿‡è¯„åˆ†è’¸é¦é‡‡æ ·ä»äºŒç»´GPSåˆ°å›¾åƒçš„æ¨¡å‹ä¸­æå–ä¸‰ç»´æ¨¡å‹ï¼Œä½¿ç”¨GPSæ¡ä»¶æ¥çº¦æŸä»æ¯ä¸ªè§†è§’é‡å»ºçš„å¤–è§‚ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„GPSæ¡ä»¶æ¨¡å‹æˆåŠŸå­¦ä¼šäº†æ ¹æ®ä½ç½®ç”Ÿæˆä¸åŒçš„å›¾åƒï¼ŒGPSæ¡ä»¶æ”¹è¿›äº†ä¼°è®¡çš„3Dç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12390v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨ç…§ç‰‡å…ƒæ•°æ®ä¸­çš„GPSæ ‡ç­¾ä½œä¸ºå›¾åƒç”Ÿæˆçš„æ§åˆ¶ä¿¡å·ã€‚é€šè¿‡è®­ç»ƒGPSåˆ°å›¾åƒçš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºéœ€è¦ç²¾ç»†ç†è§£åŸå¸‚å†…å›¾åƒå˜åŒ–çš„ä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®GPSå’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒã€‚ç”Ÿæˆçš„å›¾åƒèƒ½å¤Ÿæ•æ‰ä¸åŒè¡—åŒºã€å…¬å›­å’Œåœ°æ ‡çš„ç‹¬ç‰¹å¤–è§‚ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è¯„åˆ†è’¸é¦é‡‡æ ·ä»2D GPSåˆ°å›¾åƒçš„æ¨¡å‹ä¸­æå–3Dæ¨¡å‹ï¼Œä½¿ç”¨GPSæ¡ä»¶æ¥çº¦æŸä»æ¯ä¸ªè§†è§’çš„é‡å»ºå¤–è§‚ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒGPSæ¡ä»¶æ¨¡å‹æˆåŠŸå­¦ä¹ æ ¹æ®ä½ç½®ç”Ÿæˆå›¾åƒï¼ŒGPSæ¡ä»¶æ”¹è¿›äº†ä¼°è®¡çš„3Dç»“æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPSæ ‡ç­¾åœ¨å›¾åƒç”Ÿæˆä¸­ä½œä¸ºæœ‰ç”¨çš„æ§åˆ¶ä¿¡å·ã€‚</li>
<li>è®­ç»ƒäº†GPSåˆ°å›¾åƒçš„æ¨¡å‹ï¼Œç”¨äºç²¾ç»†ç†è§£åŸå¸‚å†…å›¾åƒå˜åŒ–çš„ä»»åŠ¡ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ ¹æ®GPSå’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒï¼Œæ•æ‰ä¸åŒåœ°ç‚¹çš„ç‹¬ç‰¹å¤–è§‚ã€‚</li>
<li>é€šè¿‡è¯„åˆ†è’¸é¦é‡‡æ ·ä»2D GPSåˆ°å›¾åƒçš„æ¨¡å‹æå–3Dæ¨¡å‹ã€‚</li>
<li>GPSæ¡ä»¶ç”¨äºçº¦æŸä»å„ä¸ªè§†è§’çš„é‡å»ºå¤–è§‚ã€‚</li>
<li>GPSæ¡ä»¶æ¨¡å‹èƒ½å¤ŸæˆåŠŸæ ¹æ®ä½ç½®ç”Ÿæˆå›¾åƒã€‚</li>
<li>GPSæ¡ä»¶æ”¹è¿›äº†ä¼°è®¡çš„3Dç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ebaae3ace00ce81e9393988f32bb8d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0a76df4bcf6fe913aa566dcc5c18ed7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ecfe5f0da70970b64df689ab69f1b85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0285c3db34c02b3c60e4053cdd434b04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cec582bfbfea2b01e76c1a34c76786e1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VipDiff-Towards-Coherent-and-Diverse-Video-Inpainting-via-Training-free-Denoising-Diffusion-Models"><a href="#VipDiff-Towards-Coherent-and-Diverse-Video-Inpainting-via-Training-free-Denoising-Diffusion-Models" class="headerlink" title="VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free   Denoising Diffusion Models"></a>VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free   Denoising Diffusion Models</h2><p><strong>Authors:Chaohao Xie, Kai Han, Kwan-Yee K. Wong</strong></p>
<p>Recent video inpainting methods have achieved encouraging improvements by leveraging optical flow to guide pixel propagation from reference frames either in the image space or feature space. However, they would produce severe artifacts in the mask center when the masked area is too large and no pixel correspondences can be found for the center. Recently, diffusion models have demonstrated impressive performance in generating diverse and high-quality images, and have been exploited in a number of works for image inpainting. These methods, however, cannot be applied directly to videos to produce temporal-coherent inpainting results. In this paper, we propose a training-free framework, named VipDiff, for conditioning diffusion model on the reverse diffusion process to produce temporal-coherent inpainting results without requiring any training data or fine-tuning the pre-trained diffusion models. VipDiff takes optical flow as guidance to extract valid pixels from reference frames to serve as constraints in optimizing the randomly sampled Gaussian noise, and uses the generated results for further pixel propagation and conditional generation. VipDiff also allows for generating diverse video inpainting results over different sampled noise. Experiments demonstrate that VipDiff can largely outperform state-of-the-art video inpainting methods in terms of both spatial-temporal coherence and fidelity. </p>
<blockquote>
<p>æœ€è¿‘çš„è§†é¢‘è¡¥å…¨æ–¹æ³•é€šè¿‡åˆ©ç”¨å…‰å­¦æµæ¥æŒ‡å¯¼åƒç´ åœ¨å›¾åƒç©ºé—´æˆ–ç‰¹å¾ç©ºé—´ä¸­çš„ä¼ æ’­ï¼Œå–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ã€‚ç„¶è€Œï¼Œå½“é®æŒ¡åŒºåŸŸè¿‡å¤§ä¸”æ— æ³•æ‰¾åˆ°ä¸­å¿ƒåƒç´ çš„å¯¹åº”ç‚¹æ—¶ï¼Œå®ƒä»¬ä¼šåœ¨é®æŒ¡ä¸­å¿ƒäº§ç”Ÿä¸¥é‡çš„ä¼ªå½±ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå¤šæ ·åŒ–å’Œé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œå¹¶å·²åº”ç”¨äºå›¾åƒè¡¥å…¨å·¥ä½œçš„å¤šä¸ªé¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºè§†é¢‘ç”Ÿæˆæ—¶é—´è¿è´¯çš„è¡¥å…¨ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œåä¸ºVipDiffã€‚VipDiffé€šè¿‡åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œäº§ç”Ÿæ—¶é—´è¿è´¯çš„è¡¥å…¨ç»“æœï¼Œæ— éœ€ä»»ä½•è®­ç»ƒæ•°æ®æˆ–å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚VipDiffä»¥å…‰å­¦æµä¸ºæŒ‡å¯¼ï¼Œä»å‚è€ƒå¸§ä¸­æå–æœ‰æ•ˆåƒç´ ä½œä¸ºä¼˜åŒ–éšæœºé‡‡æ ·é«˜æ–¯å™ªå£°çš„çº¦æŸï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆçš„ç»“æœè¿›è¡Œè¿›ä¸€æ­¥çš„åƒç´ ä¼ æ’­å’Œæ¡ä»¶ç”Ÿæˆã€‚VipDiffè¿˜å…è®¸åœ¨ä¸åŒé‡‡æ ·çš„å™ªå£°ä¸Šç”Ÿæˆå¤šæ ·çš„è§†é¢‘è¡¥å…¨ç»“æœã€‚å®éªŒè¡¨æ˜ï¼ŒVipDiffåœ¨æ—¶ç©ºè¿è´¯æ€§å’Œä¿çœŸåº¦æ–¹é¢èƒ½å¤Ÿå¤§å¤§è¶…è¶Šæœ€æ–°çš„è§†é¢‘è¡¥å…¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12267v1">PDF</a> 10 pages, 5 Figures (Accepted at WACV 2025)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— è®­ç»ƒè§†é¢‘ä¿®å¤æ¡†æ¶ç ”ç©¶ï¼Œä½¿ç”¨åå‘æ‰©æ•£è¿‡ç¨‹äº§ç”Ÿæ—¶é—´ä¸Šè¿è´¯çš„ä¿®å¤ç»“æœã€‚è¯¥ç ”ç©¶é€šè¿‡å…‰å­¦æµå¼•å¯¼åƒç´ ä¼ æ’­ï¼Œæå–å‚è€ƒå¸§çš„æœ‰æ•ˆåƒç´ ä½œä¸ºçº¦æŸä¼˜åŒ–éšæœºé‡‡æ ·çš„é«˜æ–¯å™ªå£°ï¼Œè¿›è€Œç”Ÿæˆç»“æœç”¨äºè¿›ä¸€æ­¥çš„åƒç´ ä¼ æ’­å’Œæ¡ä»¶ç”Ÿæˆã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒé‡‡æ ·å™ªå£°ä¸‹ç”Ÿæˆå¤šæ ·åŒ–çš„è§†é¢‘ä¿®å¤ç»“æœï¼Œå¹¶ä¼˜äºå½“å‰å…ˆè¿›çš„è§†é¢‘ä¿®å¤æ–¹æ³•ï¼Œåœ¨æ—¶ç©ºè¿è´¯æ€§å’Œä¿çœŸåº¦æ–¹é¢éƒ½æœ‰è¾ƒå¤§æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘ä¿®å¤ï¼Œæ— éœ€è®­ç»ƒæ•°æ®æˆ–å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºVipDiffçš„è®­ç»ƒå¤–æ¡†æ¶ï¼ŒåŸºäºåå‘æ‰©æ•£è¿‡ç¨‹äº§ç”Ÿæ—¶é—´ä¸Šè¿è´¯çš„ä¿®å¤ç»“æœã€‚</li>
<li>é€šè¿‡å…‰å­¦æµå¼•å¯¼åƒç´ ä¼ æ’­ï¼Œä»å‚è€ƒå¸§ä¸­æå–æœ‰æ•ˆåƒç´ ä½œä¸ºçº¦æŸã€‚</li>
<li>ä½¿ç”¨éšæœºé‡‡æ ·çš„é«˜æ–¯å™ªå£°è¿›è¡Œä¼˜åŒ–ï¼Œç”Ÿæˆç»“æœç”¨äºè¿›ä¸€æ­¥åƒç´ ä¼ æ’­å’Œæ¡ä»¶ç”Ÿæˆã€‚</li>
<li>VipDiffå…è®¸ç”Ÿæˆå¤šæ ·åŒ–çš„è§†é¢‘ä¿®å¤ç»“æœï¼ŒåŸºäºä¸åŒçš„å™ªå£°é‡‡æ ·ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒVipDiffåœ¨æ—¶ç©ºè¿è´¯æ€§å’Œä¿çœŸåº¦æ–¹é¢æ˜¾è‘—ä¼˜äºå½“å‰å…ˆè¿›çš„è§†é¢‘ä¿®å¤æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c57e5c712f6f156e38f317de9baf55d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12ef5269814b76a9286ee10fb5e8f5b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd57f90685ba528d9f17b4a56f3bca2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8a915460cea8348481ecde08e94b8a5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Joint-Reconstruction-and-Motion-Estimation-in-Sparse-View-4DCT-Using-Diffusion-Models-within-a-Blind-Inverse-Problem-Framework"><a href="#Joint-Reconstruction-and-Motion-Estimation-in-Sparse-View-4DCT-Using-Diffusion-Models-within-a-Blind-Inverse-Problem-Framework" class="headerlink" title="Joint Reconstruction and Motion Estimation in Sparse-View 4DCT Using   Diffusion Models within a Blind Inverse Problem Framework"></a>Joint Reconstruction and Motion Estimation in Sparse-View 4DCT Using   Diffusion Models within a Blind Inverse Problem Framework</h2><p><strong>Authors:Antoine De Paepe, Alexandre Bousse, ClÃ©mentine Phung-Ngoc, Dimitris Visvikis</strong></p>
<p>Four-dimensional computed tomography (4DCT) is essential for medical imaging applications like radiotherapy, which demand precise respiratory motion representation. Traditional methods for reconstructing 4DCT data suffer from artifacts and noise, especially in sparse-view, low-dose contexts. We propose a novel framework that integrates motion correction and diffusion models (DMs) within a blind inverse problem formulation. By leveraging prior probability distributions from DMs, we enhance the joint reconstruction and motion estimation process, improving image quality and preserving resolution. Experiments on extended cardiac-torso (XCAT) phantom data demonstrate that our method outperforms existing techniques, yielding artifact-free, high-resolution reconstructions even under irregular breathing conditions. These results showcase the potential of combining DMs with motion correction to advance sparse-view 4DCT imaging. </p>
<blockquote>
<p>å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4DCTï¼‰åœ¨åŒ»å­¦æˆåƒåº”ç”¨ï¼ˆå¦‚æ”¾å°„æ²»ç–—ï¼‰ä¸­è‡³å…³é‡è¦ï¼Œè¿™äº›åº”ç”¨éœ€è¦ç²¾ç¡®çš„å‘¼å¸è¿åŠ¨è¡¨ç¤ºã€‚ä¼ ç»Ÿçš„é‡å»º4DCTæ•°æ®çš„æ–¹æ³•åœ¨ç¨€ç–è§†è§’ã€ä½å‰‚é‡çš„æƒ…å†µä¸‹ä¼šå‡ºç°ä¼ªå½±å’Œå™ªå£°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ç›²åé—®é¢˜å…¬å¼ä¸­æ•´åˆäº†è¿åŠ¨æ ¡æ­£å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ã€‚é€šè¿‡åˆ©ç”¨DMsçš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å¢å¼ºäº†è”åˆé‡å»ºå’Œè¿åŠ¨ä¼°è®¡è¿‡ç¨‹ï¼Œæé«˜äº†å›¾åƒè´¨é‡å¹¶ä¿æŒåˆ†è¾¨ç‡ã€‚åœ¨æ‰©å±•å¿ƒè„èº¯å¹²ï¼ˆXCATï¼‰å¹»å½±æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå³ä½¿åœ¨ä¸è§„åˆ™å‘¼å¸æ¡ä»¶ä¸‹ä¹Ÿèƒ½äº§ç”Ÿæ— ä¼ªå½±ã€é«˜åˆ†è¾¨ç‡çš„é‡å»ºç»“æœã€‚è¿™äº›ç»“æœå±•ç¤ºäº†å°†DMsä¸è¿åŠ¨æ ¡æ­£ç›¸ç»“åˆåœ¨ç¨€ç–è§†è§’4DCTæˆåƒä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12249v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4DCTï¼‰åœ¨åŒ»å­¦æˆåƒåº”ç”¨ä¸­çš„éœ€æ±‚ï¼Œå¦‚æ”¾å°„æ²»ç–—ä¸­å¯¹å‘¼å¸è¿åŠ¨çš„ç²¾ç¡®è¡¨ç¤ºï¼Œä¼ ç»Ÿé‡å»ºæ–¹æ³•æ˜“å‡ºç°ä¼ªå½±å’Œå™ªå£°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–è§†è§’å’Œä½å‰‚é‡æƒ…å†µä¸‹ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§ç»“åˆè¿åŠ¨æ ¡æ­£å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„æ–°æ¡†æ¶ï¼Œåˆ©ç”¨DMsçš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒï¼Œå¢å¼ºè”åˆé‡å»ºå’Œè¿åŠ¨ä¼°è®¡è¿‡ç¨‹ï¼Œæé«˜å›¾åƒè´¨é‡å¹¶ä¿ç•™åˆ†è¾¨ç‡ã€‚åœ¨XCAT Phantomæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå³ä½¿åœ¨ä¸è§„åˆ™å‘¼å¸æ¡ä»¶ä¸‹ä¹Ÿèƒ½å®ç°æ— ä¼ªå½±ã€é«˜åˆ†è¾¨ç‡çš„é‡å»ºã€‚å±•ç¤ºäº†ç»“åˆDMså’Œè¿åŠ¨æ ¡æ­£æ¨åŠ¨ç¨€ç–è§†è§’4DCTæˆåƒçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆ4DCTï¼‰åœ¨åŒ»å­¦æˆåƒä¸­çš„é‡è¦ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¾å°„æ²»ç–—é¢†åŸŸã€‚</li>
<li>ä¼ ç»Ÿ4DCTæ•°æ®é‡å»ºæ–¹æ³•å­˜åœ¨ä¼ªå½±å’Œå™ªå£°é—®é¢˜ï¼Œå°¤å…¶åœ¨ç¨€ç–è§†è§’å’Œä½å‰‚é‡ç¯å¢ƒä¸‹ã€‚</li>
<li>æå‡ºä¸€ç§ç»“åˆè¿åŠ¨æ ¡æ­£å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›å›¾åƒè´¨é‡å’Œåˆ†è¾¨ç‡ã€‚</li>
<li>åˆ©ç”¨DMsçš„å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒï¼Œå¢å¼ºè”åˆé‡å»ºå’Œè¿åŠ¨ä¼°è®¡è¿‡ç¨‹ã€‚</li>
<li>åœ¨XCAT Phantomæ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æ–°æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸è§„åˆ™å‘¼å¸æ¡ä»¶ä¸‹å®ç°æ— ä¼ªå½±ã€é«˜åˆ†è¾¨ç‡çš„é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b092a93b8cdbb486bcadb49a7f6ff92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-867e8c0ff9af922255b028d98b6dea40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f5bf97f0ffddaac56a1f6025b25c46c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-630797383b69606276cd794992c054e9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TokenVerse-Versatile-Multi-concept-Personalization-in-Token-Modulation-Space"><a href="#TokenVerse-Versatile-Multi-concept-Personalization-in-Token-Modulation-Space" class="headerlink" title="TokenVerse: Versatile Multi-concept Personalization in Token Modulation   Space"></a>TokenVerse: Versatile Multi-concept Personalization in Token Modulation   Space</h2><p><strong>Authors:Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, Tali Dekel</strong></p>
<p>We present TokenVerse â€“ a method for multi-concept personalization, leveraging a pre-trained text-to-image diffusion model. Our framework can disentangle complex visual elements and attributes from as little as a single image, while enabling seamless plug-and-play generation of combinations of concepts extracted from multiple images. As opposed to existing works, TokenVerse can handle multiple images with multiple concepts each, and supports a wide-range of concepts, including objects, accessories, materials, pose, and lighting. Our work exploits a DiT-based text-to-image model, in which the input text affects the generation through both attention and modulation (shift and scale). We observe that the modulation space is semantic and enables localized control over complex concepts. Building on this insight, we devise an optimization-based framework that takes as input an image and a text description, and finds for each word a distinct direction in the modulation space. These directions can then be used to generate new images that combine the learned concepts in a desired configuration. We demonstrate the effectiveness of TokenVerse in challenging personalization settings, and showcase its advantages over existing methods. projectâ€™s webpage in <a target="_blank" rel="noopener" href="https://token-verse.github.io/">https://token-verse.github.io/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†TokenVerseâ€”â€”ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šæ¦‚å¿µä¸ªæ€§åŒ–çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿä»å•å¼ å›¾åƒä¸­åˆ†ç¦»å‡ºå¤æ‚çš„è§†è§‰å…ƒç´ å’Œå±æ€§ï¼ŒåŒæ—¶èƒ½å¤Ÿæ— ç¼åœ°ç”Ÿæˆä»å¤šå¼ å›¾åƒä¸­æå–çš„æ¦‚å¿µçš„ç»„åˆã€‚ä¸ç°æœ‰ä½œå“ä¸åŒï¼ŒTokenVerseèƒ½å¤Ÿå¤„ç†æ¯å¼ åŒ…å«å¤šä¸ªæ¦‚å¿µçš„å¤šå¼ å›¾åƒï¼Œå¹¶æ”¯æŒåŒ…æ‹¬ç‰©ä½“ã€é…ä»¶ã€æè´¨ã€å§¿åŠ¿å’Œå…‰ç…§åœ¨å†…çš„å¹¿æ³›æ¦‚å¿µã€‚æˆ‘ä»¬çš„å·¥ä½œåˆ©ç”¨åŸºäºDiTçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå…¶ä¸­è¾“å…¥æ–‡æœ¬é€šè¿‡æ³¨æ„åŠ›å’Œè°ƒåˆ¶ï¼ˆç§»ä½å’Œç¼©æ”¾ï¼‰å½±å“ç”Ÿæˆã€‚æˆ‘ä»¬å‘ç°è°ƒåˆ¶ç©ºé—´æ˜¯è¯­ä¹‰åŒ–çš„ï¼Œèƒ½å¤Ÿå¯¹å¤æ‚æ¦‚å¿µè¿›è¡Œå±€éƒ¨æ§åˆ¶ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºä¼˜åŒ–çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥å›¾åƒå’Œæ–‡æœ¬æè¿°ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯åœ¨è°ƒåˆ¶ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªç‹¬ç‰¹çš„æ–¹å‘ã€‚ç„¶åå¯ä»¥ä½¿ç”¨è¿™äº›æ–¹å‘ç”Ÿæˆæ–°å›¾åƒï¼Œä»¥åœ¨æ‰€éœ€é…ç½®ä¸­ç»“åˆæ‰€å­¦æ¦‚å¿µã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸ªæ€§åŒ–è®¾ç½®ä¸­å±•ç¤ºäº†TokenVerseçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶ä¸ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿ã€‚é¡¹ç›®ç½‘é¡µåœ¨<a target="_blank" rel="noopener" href="https://token-verse.github.io/%E3%80%82">https://token-verse.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12224v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TokenVerseæ˜¯ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šæ¦‚å¿µä¸ªæ€§åŒ–çš„æ–¹æ³•ã€‚å®ƒèƒ½å¤Ÿä»ä¸€ä¸ªæˆ–å¤šä¸ªå›¾åƒä¸­è§£è€¦å¤æ‚çš„è§†è§‰å…ƒç´ å’Œå±æ€§ï¼Œå¹¶è½»æ¾ç”Ÿæˆæå–è‡ªä¸åŒå›¾åƒçš„æ¦‚å¿µçš„ç»„åˆã€‚å…¶åˆ©ç”¨äº†ä¸€ç§åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å…·æœ‰å¤šä¸ªæ¦‚å¿µçš„å¤šä¸ªå›¾åƒå¹¶æ”¯æŒå¤šç§æ¦‚å¿µã€‚å®ƒç»“åˆäº†æ–‡æœ¬æ³¨æ„åŠ›ä¸è°ƒåˆ¶ç©ºé—´æ§åˆ¶è¿›è¡Œç”Ÿæˆå›¾åƒçš„æŠ€æœ¯ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–æ¡†æ¶å®ç°å›¾åƒä¸æ–‡æœ¬æè¿°çš„è¾“å…¥ã€‚æ­¤æ¡†æ¶ä¸ºè¾“å…¥çš„æ¯ä¸ªå•è¯åœ¨è°ƒåˆ¶ç©ºé—´ä¸­æ‰¾åˆ°äº†ä¸€ä¸ªç‹¬ç‰¹æ–¹å‘ï¼Œè¿›è€Œç”Ÿæˆç»“åˆå­¦ä¹ æ¦‚å¿µçš„æ–°å›¾åƒã€‚TokenVerseåœ¨ä¸ªæ€§åŒ–è®¾ç½®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨å±•ç¤ºå…¶å¯¹ç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿æ–¹é¢å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®å…¶é¡¹ç›®ç½‘é¡µã€‚<a target="_blank" rel="noopener" href="https://token-verse.github.io/">https://token-verse.github.io/</a> ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TokenVerseåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šæ¦‚å¿µä¸ªæ€§åŒ–ã€‚</li>
<li>å®ƒèƒ½å¤Ÿå¤„ç†å¤æ‚çš„è§†è§‰å…ƒç´ å’Œå±æ€§çš„è§£è€¦ï¼Œæ”¯æŒä»å•ä¸ªæˆ–å¤šä¸ªå›¾åƒä¸­æå–æ¦‚å¿µç»„åˆã€‚</li>
<li>TokenVerseæ”¯æŒå¤šç§æ¦‚å¿µï¼ŒåŒ…æ‹¬ç‰©ä½“ã€é…ä»¶ã€æè´¨ã€å§¿åŠ¿å’Œç…§æ˜ç­‰ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æ–‡æœ¬æ³¨æ„åŠ›ä¸è°ƒåˆ¶ç©ºé—´æ§åˆ¶æ¥ç”Ÿæˆå›¾åƒã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–æ¡†æ¶å®ç°è¾“å…¥å›¾åƒå’Œæ–‡æœ¬æè¿°çš„ç»“åˆï¼Œä¸ºæ¯ä¸ªå•è¯æ‰¾åˆ°è°ƒåˆ¶ç©ºé—´ä¸­çš„ç‹¬ç‰¹æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c036ab548a3ecb1506605693d73b1aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5c0f96b23b5fa87692b4d74b118c540.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c3c560eb455d93d9609f614c2afe404.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ac9fbf6931dc1cda9b2bab18e0ea820.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4b5b5cdc52f2f97125c9297414c97dc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EfficientVITON-An-Efficient-Virtual-Try-On-Model-using-Optimized-Diffusion-Process"><a href="#EfficientVITON-An-Efficient-Virtual-Try-On-Model-using-Optimized-Diffusion-Process" class="headerlink" title="EfficientVITON: An Efficient Virtual Try-On Model using Optimized   Diffusion Process"></a>EfficientVITON: An Efficient Virtual Try-On Model using Optimized   Diffusion Process</h2><p><strong>Authors:Mostafa Atef, Mariam Ayman, Ahmed Rashed, Ashrakat Saeed, Abdelrahman Saeed, Ahmed Fares</strong></p>
<p>Would not it be much more convenient for everybody to try on clothes by only looking into a mirror ? The answer to that problem is virtual try-on, enabling users to digitally experiment with outfits. The core challenge lies in realistic image-to-image translation, where clothing must fit diverse human forms, poses, and figures. Early methods, which used 2D transformations, offered speed, but image quality was often disappointing and lacked the nuance of deep learning. Though GAN-based techniques enhanced realism, their dependence on paired data proved limiting. More adaptable methods offered great visuals but demanded significant computing power and time. Recent advances in diffusion models have shown promise for high-fidelity translation, yet the current crop of virtual try-on tools still struggle with detail loss and warping issues. To tackle these challenges, this paper proposes EfficientVITON, a new virtual try-on system leveraging the impressive pre-trained Stable Diffusion model for better images and deployment feasibility. The system includes a spatial encoder to maintain clothings finer details and zero cross-attention blocks to capture the subtleties of how clothes fit a human body. Input images are carefully prepared, and the diffusion process has been tweaked to significantly cut generation time without image quality loss. The training process involves two distinct stages of fine-tuning, carefully incorporating a balance of loss functions to ensure both accurate try-on results and high-quality visuals. Rigorous testing on the VITON-HD dataset, supplemented with real-world examples, has demonstrated that EfficientVITON achieves state-of-the-art results. </p>
<blockquote>
<p>è¯•è¡£æ—¶ï¼Œå¦‚æœåªéœ€è¦ç…§é•œå­ä¸æ˜¯æ›´åŠ æ–¹ä¾¿å—ï¼Ÿè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆæ˜¯è™šæ‹Ÿè¯•è¡£åŠŸèƒ½ï¼Œå®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿæ•°å­—åŒ–åœ°å°è¯•ä¸åŒçš„æœè£…ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå®ç°çœŸå®çš„å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ï¼Œè¿™éœ€è¦æœè£…èƒ½å¤Ÿé€‚åº”å„ç§äººç±»å½¢æ€ã€å§¿åŠ¿å’Œä½“å‹ã€‚æ—©æœŸä½¿ç”¨2Dè½¬æ¢çš„æ–¹æ³•è™½ç„¶é€Ÿåº¦å¾ˆå¿«ï¼Œä½†å›¾åƒè´¨é‡å¾€å¾€ä»¤äººå¤±æœ›ï¼Œç¼ºä¹æ·±åº¦å­¦ä¹ æ‰€å¸¦æ¥çš„ç»†å¾®å·®åˆ«ã€‚è™½ç„¶åŸºäºGANçš„æŠ€æœ¯å¢å¼ºäº†ç°å®æ„Ÿï¼Œä½†å®ƒä»¬å¯¹é…å¯¹æ•°æ®çš„ä¾èµ–è¯æ˜æ˜¯æœ‰é™åˆ¶çš„ã€‚é€‚åº”æ€§æ›´å¼ºçš„æ–¹æ³•åœ¨è§†è§‰ä¸Šæä¾›äº†å¾ˆå¥½çš„è¡¨ç°ï¼Œä½†éœ€è¦å·¨å¤§çš„è®¡ç®—èƒ½åŠ›å’Œæ—¶é—´ã€‚æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºå‡ºé«˜ä¿çœŸç¿»è¯‘çš„å‰æ™¯ï¼Œç„¶è€Œç›®å‰çš„è™šæ‹Ÿè¯•è¡£å·¥å…·ä»ç„¶é¢ä¸´ç»†èŠ‚æŸå¤±å’Œå˜å½¢é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†EfficientVITONï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è™šæ‹Ÿè¯•è¡£ç³»ç»Ÿï¼Œåˆ©ç”¨ä»¤äººå°è±¡æ·±åˆ»çš„é¢„è®­ç»ƒStable Diffusionæ¨¡å‹æ¥ç”Ÿæˆæ›´å¥½çš„å›¾åƒå¹¶æé«˜éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬ä¸€ä¸ªç©ºé—´ç¼–ç å™¨ï¼Œä»¥ä¿ç•™æœè£…çš„ç²¾ç»†ç»†èŠ‚å’Œé›¶äº¤å‰æ³¨æ„åŠ›å—æ¥æ•æ‰æœè£…å¦‚ä½•è´´åˆäººä½“ç»†å¾®ä¹‹å¤„ã€‚è¾“å…¥å›¾åƒç»è¿‡ç²¾å¿ƒå‡†å¤‡ï¼Œè°ƒæ•´äº†æ‰©æ•£è¿‡ç¨‹ä»¥æ˜¾è‘—å‡å°‘ç”Ÿæˆæ—¶é—´è€Œä¸æŸå¤±å›¾åƒè´¨é‡ã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªç²¾ç»†è°ƒæ•´çš„ç‹¬ç‰¹é˜¶æ®µï¼Œç²¾å¿ƒå¹³è¡¡æŸå¤±å‡½æ•°ä»¥ç¡®ä¿å‡†ç¡®çš„è¯•ç©¿ç»“æœå’Œé«˜è´¨é‡çš„è§†è§‰æ•ˆæœã€‚åœ¨VITON-HDæ•°æ®é›†ä¸Šè¿›è¡Œä¸¥æ ¼æµ‹è¯•ï¼Œå¹¶ç»“åˆçœŸå®ä¸–ç•Œä¾‹å­è¿›è¡Œè¡¥å……ï¼Œè¯æ˜äº†EfficientVITONå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11776v1">PDF</a> 7 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„è™šæ‹Ÿè¯•è¡£ç³»ç»ŸEfficientVITONç ”ç©¶ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ï¼Œé€šè¿‡ç©ºé—´ç¼–ç å™¨å’Œé›¶äº¤å‰æ³¨æ„åŠ›å—ç­‰æŠ€æœ¯ï¼Œå®ç°äº†é«˜è´¨é‡çš„è™šæ‹Ÿè¯•è¡£æ•ˆæœã€‚é€šè¿‡ç²¾ç»†çš„å›¾åƒå¤„ç†å’Œæ‰©æ•£è¿‡ç¨‹è°ƒæ•´ï¼Œæé«˜äº†ç”Ÿæˆé€Ÿåº¦å¹¶ä¿è¯äº†å›¾åƒè´¨é‡ã€‚ç»è¿‡ä¸¤é˜¶æ®µå¾®è°ƒè®­ç»ƒï¼ŒEfficientVITONåœ¨VITON-HDæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è™šæ‹Ÿè¯•è¡£ç³»ç»Ÿé¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯å®ç°çœŸå®æ„Ÿå›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ï¼Œéœ€è¦é€‚åº”ä¸åŒçš„äººç±»å½¢æ€ã€å§¿åŠ¿å’Œä½“å‹ã€‚</li>
<li>æ—©æœŸæ–¹æ³•ä¸»è¦ä½¿ç”¨2Dè½¬æ¢ï¼Œè™½ç„¶é€Ÿåº¦è¾ƒå¿«ä½†å›¾åƒè´¨é‡ä¸ä½³ï¼Œç¼ºä¹æ·±åº¦å­¦ä¹ æ‰€å¸¦æ¥çš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>åŸºäºGANçš„æŠ€æœ¯æé«˜äº†çœŸå®æ€§ï¼Œä½†ä¾èµ–äºé…å¯¹æ•°æ®ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿè¯•è¡£æ–¹é¢æ˜¾ç¤ºå‡ºé«˜ä¿çœŸç¿»è¯‘çš„æ½œåŠ›ï¼Œä½†ç°æœ‰å·¥å…·ä»å­˜åœ¨ç»†èŠ‚æŸå¤±å’Œå˜å½¢é—®é¢˜ã€‚</li>
<li>EfficientVITONç³»ç»Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹å®ç°æ›´å¥½çš„å›¾åƒæ•ˆæœå’Œæé«˜éƒ¨ç½²å¯è¡Œæ€§ã€‚</li>
<li>EfficientVITONé€šè¿‡ç©ºé—´ç¼–ç å™¨å’Œé›¶äº¤å‰æ³¨æ„åŠ›å—ç­‰æŠ€æœ¯æ•æ‰è¡£ç‰©ç»†èŠ‚å’Œè´´åˆäººä½“çš„å¾®å¦™ä¹‹å¤„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33af870249ab7e82efdc249ce4fef16d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4a487210bd44e98e9c04cae2f1e715c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc599e290ce2a53513fef5abc76dd8cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-067426b3f175e90013ccaad2dc0acca7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a85be08c79dfb0e209bcb6f0942b4cb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1bbd95001a6137db5092ef0d8814ce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b50a75ffe10e52e0edbb41938c0b3acc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62ec5f77bfe2b4769afa962a938eb6ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-505dd82030bf9f168c9fbf90ffb05ee3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SILO-Solving-Inverse-Problems-with-Latent-Operators"><a href="#SILO-Solving-Inverse-Problems-with-Latent-Operators" class="headerlink" title="SILO: Solving Inverse Problems with Latent Operators"></a>SILO: Solving Inverse Problems with Latent Operators</h2><p><strong>Authors:Ron Raphaeli, Sean Man, Michael Elad</strong></p>
<p>Consistent improvement of image priors over the years has led to the development of better inverse problem solvers. Diffusion models are the newcomers to this arena, posing the strongest known prior to date. Recently, such models operating in a latent space have become increasingly predominant due to their efficiency. In recent works, these models have been applied to solve inverse problems. Working in the latent space typically requires multiple applications of an Autoencoder during the restoration process, which leads to both computational and restoration quality challenges. In this work, we propose a new approach for handling inverse problems with latent diffusion models, where a learned degradation function operates within the latent space, emulating a known image space degradation. Usage of the learned operator reduces the dependency on the Autoencoder to only the initial and final steps of the restoration process, facilitating faster sampling and superior restoration quality. We demonstrate the effectiveness of our method on a variety of image restoration tasks and datasets, achieving significant improvements over prior art. </p>
<blockquote>
<p>éšç€å¤šå¹´æ¥å›¾åƒå…ˆéªŒçš„æŒç»­æ”¹è¿›ï¼Œæ›´å¥½çš„é€†é—®é¢˜æ±‚è§£å™¨å¾—ä»¥å¼€å‘ã€‚æ‰©æ•£æ¨¡å‹ä½œä¸ºè¯¥é¢†åŸŸçš„æ–°æ¥è€…ï¼Œæå‡ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¼ºå¤§çš„å·²çŸ¥å…ˆéªŒã€‚æœ€è¿‘ï¼Œç”±äºæ•ˆç‡åŸå› ï¼Œåœ¨æ½œåœ¨ç©ºé—´è¿ä½œçš„è¿™ç§æ¨¡å‹è¶Šæ¥è¶Šå æ®ä¸»å¯¼åœ°ä½ã€‚åœ¨æœ€è¿‘çš„ä½œå“ä¸­ï¼Œè¿™äº›æ¨¡å‹å·²è¢«åº”ç”¨äºè§£å†³é€†é—®é¢˜ã€‚åœ¨æ½œåœ¨ç©ºé—´å·¥ä½œé€šå¸¸åœ¨æ¢å¤è¿‡ç¨‹ä¸­éœ€è¦å¤šæ¬¡åº”ç”¨è‡ªåŠ¨ç¼–ç å™¨ï¼Œè¿™å¯¼è‡´è®¡ç®—å’Œæ¢å¤è´¨é‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å¤„ç†é€†é—®é¢˜ï¼Œå…¶ä¸­å­¦ä¹ åˆ°çš„é€€åŒ–å‡½æ•°åœ¨æ½œåœ¨ç©ºé—´å†…è¿è¡Œï¼Œæ¨¡æ‹Ÿå·²çŸ¥çš„å›¾åƒç©ºé—´é€€åŒ–ã€‚ä½¿ç”¨å­¦ä¹ åˆ°çš„æ“ä½œç¬¦ä»…å°†è‡ªåŠ¨ç¼–ç å™¨çš„ä¾èµ–æ€§é™ä½åˆ°äº†æ¢å¤è¿‡ç¨‹çš„åˆå§‹å’Œæœ€ç»ˆæ­¥éª¤ï¼Œå®ç°äº†æ›´å¿«çš„é‡‡æ ·å’Œæ›´é«˜çš„æ¢å¤è´¨é‡ã€‚æˆ‘ä»¬åœ¨å„ç§å›¾åƒæ¢å¤ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å…ˆéªŒæŠ€æœ¯æ–¹é¢å–å¾—äº†é‡å¤§æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11746v1">PDF</a> Project page in <a target="_blank" rel="noopener" href="https://ronraphaeli.github.io/SILO-website/">https://ronraphaeli.github.io/SILO-website/</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå…ˆéªŒé¢†åŸŸçš„æŒç»­è¿›æ­¥æ¨åŠ¨äº†æ›´å¥½çš„é€†é—®é¢˜æ±‚è§£å™¨çš„å¼€å‘ã€‚è¿‘å¹´æ¥ï¼Œå·¥ä½œåœ¨æ½œåœ¨ç©ºé—´çš„æ‰©æ•£æ¨¡å‹å› å…¶æ•ˆç‡è€Œé€æ¸æˆä¸ºä¸»æµã€‚ä¸ºè§£å†³é€†é—®é¢˜ï¼Œè¿™äº›æ¨¡å‹è¢«å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œåœ¨æ¢å¤è¿‡ç¨‹ä¸­ï¼Œé€šå¸¸éœ€è¦å¤šæ¬¡ä½¿ç”¨è‡ªç¼–ç å™¨ï¼Œè¿™å¸¦æ¥äº†è®¡ç®—å’Œæ¢å¤è´¨é‡çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹å¤„ç†é€†é—®é¢˜çš„æ–¹æ³•ï¼Œå…¶ä¸­å­¦ä¹ åˆ°çš„é€€åŒ–å‡½æ•°åœ¨æ½œåœ¨ç©ºé—´å†…è¿è¡Œï¼Œæ¨¡æ‹Ÿå·²çŸ¥çš„å›¾åƒç©ºé—´é€€åŒ–ã€‚ä½¿ç”¨å­¦ä¹ åˆ°çš„ç®—å­å‡å°‘äº†è‡ªç¼–ç å™¨ä»…åœ¨æ¢å¤è¿‡ç¨‹çš„åˆå§‹å’Œæœ€ç»ˆæ­¥éª¤ä¸­çš„ä¾èµ–æ€§ï¼Œå®ç°äº†æ›´å¿«çš„é‡‡æ ·å’Œæ›´é«˜çš„æ¢å¤è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå…ˆéªŒé¢†åŸŸæŒç»­è¿›æ­¥ï¼Œæ¨åŠ¨äº†é€†é—®é¢˜æ±‚è§£å™¨çš„å‘å±•ã€‚</li>
<li>æ½œåœ¨ç©ºé—´çš„æ‰©æ•£æ¨¡å‹å› å…¶æ•ˆç‡è€Œé€æ¸æµè¡Œã€‚</li>
<li>è§£å†³é€†é—®é¢˜éœ€è¦å¤šæ¬¡ä½¿ç”¨è‡ªç¼–ç å™¨ï¼Œå¸¦æ¥è®¡ç®—å’Œæ¢å¤è´¨é‡çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹å¤„ç†é€†é—®é¢˜çš„æ–¹æ³•ï¼Œä½¿ç”¨å­¦ä¹ åˆ°çš„é€€åŒ–å‡½æ•°æ¨¡æ‹Ÿå›¾åƒç©ºé—´é€€åŒ–ã€‚</li>
<li>å­¦ä¹ åˆ°çš„ç®—å­å‡å°‘äº†è‡ªç¼–ç å™¨åœ¨æ¢å¤è¿‡ç¨‹ä¸­çš„ä¾èµ–æ€§ï¼Œæé«˜äº†é‡‡æ ·é€Ÿåº¦å’Œæ¢å¤è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§å›¾åƒæ¢å¤ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9f10c5f600bf56829b7db6b2bb21008e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05938aff7db90a4e625bf24579f21374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9283aa007cc4640dc9522b0dcf44b80.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2501.11746v1/page_4_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bf2e6f596defa6b21a8dcf95294bf4d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Diffusion-Models-for-Anomaly-Detection"><a href="#A-Survey-on-Diffusion-Models-for-Anomaly-Detection" class="headerlink" title="A Survey on Diffusion Models for Anomaly Detection"></a>A Survey on Diffusion Models for Anomaly Detection</h2><p><strong>Authors:Jing Liu, Zhenchao Ma, Zepu Wang, Yang Liu, Zehua Wang, Peng Sun, Liang Song, Bo Hu, Azzedine Boukerche, Victor C. M. Leung</strong></p>
<p>Diffusion models (DMs) have emerged as a powerful class of generative AI models, showing remarkable potential in anomaly detection (AD) tasks across various domains, such as cybersecurity, fraud detection, healthcare, and manufacturing. The intersection of these two fields, termed diffusion models for anomaly detection (DMAD), offers promising solutions for identifying deviations in increasingly complex and high-dimensional data. In this survey, we systematically review recent advances in DMAD research and investigate their capabilities. We begin by presenting the fundamental concepts of AD and DMs, followed by a comprehensive analysis of classic DM architectures including DDPMs, DDIMs, and Score SDEs. We further categorize existing DMAD methods into reconstruction-based, density-based, and hybrid approaches, providing detailed examinations of their methodological innovations. We also explore the diverse tasks across different data modalities, encompassing image, time series, video, and multimodal data analysis. Furthermore, we discuss critical challenges and emerging research directions, including computational efficiency, model interpretability, robustness enhancement, edge-cloud collaboration, and integration with large language models. The collection of DMAD research papers and resources is available at <a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD">https://github.com/fdjingliu/DMAD</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºä¸€ç±»å¼ºå¤§çš„ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹å·²ç»å´­éœ²å¤´è§’ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—æ½œåŠ›ï¼Œå¹¿æ³›åº”ç”¨äºç½‘ç»œå®‰å…¨ã€æ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—å¥åº·å’Œåˆ¶é€ ç­‰é¢†åŸŸã€‚è¿™ä¸¤ä¸ªé¢†åŸŸçš„äº¤å‰ç‚¹â€”â€”å¼‚å¸¸æ£€æµ‹çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMADï¼‰â€”â€”ä¸ºè§£å†³æ—¥ç›Šå¤æ‚çš„é«˜ç»´æ•°æ®ä¸­çš„åå·®æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾äº†DMADç ”ç©¶çš„æœ€æ–°è¿›å±•ï¼Œå¹¶è°ƒæŸ¥äº†å®ƒä»¬çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»ADå’ŒDMçš„åŸºæœ¬æ¦‚å¿µï¼Œç„¶åå…¨é¢åˆ†æç»å…¸çš„DMæ¶æ„ï¼ŒåŒ…æ‹¬DDPMsã€DDIIMså’ŒScore SDEsã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†ç°æœ‰çš„DMADæ–¹æ³•åˆ†ä¸ºåŸºäºé‡å»ºçš„ã€åŸºäºå¯†åº¦çš„å’Œæ··åˆæ–¹æ³•ï¼Œå¹¶å¯¹å…¶æ–¹æ³•åˆ›æ–°è¿›è¡Œè¯¦ç»†è€ƒå¯Ÿã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†ä¸åŒæ•°æ®æ¨¡æ€çš„å¤šæ ·åŒ–ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†å…³é”®æŒ‘æˆ˜å’Œæ–°å…´ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€é²æ£’æ€§å¢å¼ºã€è¾¹ç¼˜äº‘åä½œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆã€‚DMADç ”ç©¶è®ºæ–‡å’Œèµ„æºé›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/fdjingliu/DMADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11430v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä»‹ç»äº†å¼‚å¸¸æ£€æµ‹ä¸æ‰©æ•£æ¨¡å‹çš„äº¤é›†â€”â€”æ‰©æ•£æ¨¡å‹å¼‚å¸¸æ£€æµ‹ï¼ˆDMADï¼‰ã€‚æ–‡ç« è¯¦ç»†é˜è¿°äº†DMADçš„æ–¹æ³•åˆ›æ–°ï¼ŒåŒ…æ‹¬åŸºäºé‡å»ºã€åŸºäºå¯†åº¦å’Œæ··åˆæ–¹æ³•ï¼Œä»¥åŠå…¶åœ¨ä¸åŒæ•°æ®æ¨¡æ€ï¼ˆå›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®ï¼‰çš„åº”ç”¨ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€é²æ£’æ€§å¢å¼ºã€è¾¹ç¼˜äº‘åä½œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆç­‰å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚æœ‰å…³DMADç ”ç©¶è®ºæ–‡å’Œèµ„æºçš„é›†åˆå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºç”Ÿæˆå¼AIæ¨¡å‹çš„ä¸€ä¸ªå¼ºå¤§ç±»åˆ«ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œåº”ç”¨é¢†åŸŸå¹¿æ³›ï¼Œå¦‚ç½‘ç»œå®‰å…¨ã€æ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—ä¿å¥å’Œåˆ¶é€ ç­‰ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¼‚å¸¸æ£€æµ‹ï¼ˆDMADï¼‰æ˜¯ADä¸DMsçš„äº¤é›†ï¼Œèƒ½è¯†åˆ«é«˜ç»´åº¦æ•°æ®ä¸­çš„å¼‚å¸¸åå·®ã€‚</li>
<li>ç°æœ‰DMADæ–¹æ³•ä¸»è¦åˆ†ä¸ºåŸºäºé‡å»ºã€åŸºäºå¯†åº¦å’Œæ··åˆæ–¹æ³•ã€‚</li>
<li>DMADèƒ½å¤„ç†ä¸åŒçš„æ•°æ®æ¨¡æ€ï¼ŒåŒ…æ‹¬å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æã€‚</li>
<li>DMADé¢ä¸´çš„å…³é”®æŒ‘æˆ˜åŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€é²æ£’æ€§å¢å¼ºç­‰ã€‚</li>
<li>è¾¹ç¼˜äº‘åä½œå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆæ˜¯DMADæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4014faaeecfd4b16303ed2309a82f70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e7f0a5be3d2d397220b3803c18248e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5dacae6b903fda6aca1b5d64a0477fbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a3c50d96860ea340ceb8fa33369b840.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9128c388ef97f69a25dd3c701a3b182.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Nested-Annealed-Training-Scheme-for-Generative-Adversarial-Networks"><a href="#Nested-Annealed-Training-Scheme-for-Generative-Adversarial-Networks" class="headerlink" title="Nested Annealed Training Scheme for Generative Adversarial Networks"></a>Nested Annealed Training Scheme for Generative Adversarial Networks</h2><p><strong>Authors:Chang Wan, Ming-Hsuan Yang, Minglu Li, Yunliang Jiang, Zhonglong Zheng</strong></p>
<p>Recently, researchers have proposed many deep generative models, including generative adversarial networks(GANs) and denoising diffusion models. Although significant breakthroughs have been made and empirical success has been achieved with the GAN, its mathematical underpinnings remain relatively unknown. This paper focuses on a rigorous mathematical theoretical framework: the composite-functional-gradient GAN (CFG)[1]. Specifically, we reveal the theoretical connection between the CFG model and score-based models. We find that the training objective of the CFG discriminator is equivalent to finding an optimal D(x). The optimal gradient of D(x) differentiates the integral of the differences between the score functions of real and synthesized samples. Conversely, training the CFG generator involves finding an optimal G(x) that minimizes this difference. In this paper, we aim to derive an annealed weight preceding the weight of the CFG discriminator. This new explicit theoretical explanation model is called the annealed CFG method. To overcome the limitation of the annealed CFG method, as the method is not readily applicable to the SOTA GAN model, we propose a nested annealed training scheme (NATS). This scheme keeps the annealed weight from the CFG method and can be seamlessly adapted to various GAN models, no matter their structural, loss, or regularization differences. We conduct thorough experimental evaluations on various benchmark datasets for image generation. The results show that our annealed CFG and NATS methods significantly improve the quality and diversity of the synthesized samples. This improvement is clear when comparing the CFG method and the SOTA GAN models. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†è®¸å¤šæ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œé™å™ªæ‰©æ•£æ¨¡å‹ã€‚è™½ç„¶GANåœ¨æ•°å­¦ä¸Šå–å¾—äº†é‡å¤§çªç ´å¹¶è·å¾—äº†å®è¯æˆåŠŸï¼Œä½†å…¶æ•°å­¦åŸºç¡€ç›¸å¯¹æœªçŸ¥ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»ä¸€ä¸ªä¸¥è°¨çš„æ•°å­¦ç†è®ºæ¡†æ¶ï¼šå¤åˆåŠŸèƒ½æ¢¯åº¦GANï¼ˆCFGï¼‰[1]ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ­ç¤ºäº†CFGæ¨¡å‹ä¸åŸºäºåˆ†æ•°çš„æ¨¡å‹ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚æˆ‘ä»¬å‘ç°CFGåˆ¤åˆ«å™¨çš„è®­ç»ƒç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä¼˜çš„D(x)ã€‚D(x)çš„æœ€ä¼˜æ¢¯åº¦åŒºåˆ†äº†çœŸå®æ ·æœ¬å’Œåˆæˆæ ·æœ¬åˆ†æ•°å‡½æ•°ä¹‹é—´çš„å·®å¼‚ç§¯åˆ†ã€‚ç›¸åï¼ŒCFGç”Ÿæˆå™¨çš„è®­ç»ƒæ˜¯å¯»æ‰¾ä¸€ä¸ªæœ€ä¼˜çš„G(x)ï¼Œä»¥æœ€å°åŒ–è¿™ç§å·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨å¯¼å‡ºé€€ç«æƒé‡ï¼Œè¯¥æƒé‡ä½äºCFGåˆ¤åˆ«å™¨æƒé‡ä¹‹å‰ã€‚è¿™ç§æ–°çš„æ˜¾å¼ç†è®ºè§£é‡Šæ¨¡å‹è¢«ç§°ä¸ºé€€ç«CFGæ–¹æ³•ã€‚ä¸ºäº†å…‹æœé€€ç«CFGæ–¹æ³•çš„å±€é™æ€§ï¼ˆè¯¥æ–¹æ³•ä¸é€‚ç”¨äºSOTA GANæ¨¡å‹ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åµŒå¥—é€€ç«è®­ç»ƒæ–¹æ¡ˆï¼ˆNATSï¼‰ã€‚è¯¥æ–¹æ¡ˆä¿ç•™äº†CFGæ–¹æ³•çš„é€€ç«æƒé‡ï¼Œå¹¶èƒ½æ— ç¼åœ°é€‚åº”å„ç§GANæ¨¡å‹ï¼Œæ— è®ºå…¶ç»“æ„ã€æŸå¤±æˆ–æ­£åˆ™åŒ–å·®å¼‚å¦‚ä½•ã€‚æˆ‘ä»¬åœ¨å„ç§å›¾åƒç”ŸæˆåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å½»åº•çš„å®éªŒè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é€€ç«CFGå’ŒNATSæ–¹æ³•æ˜¾è‘—æé«˜äº†åˆæˆæ ·æœ¬çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚åœ¨ä¸SOTA GANæ¨¡å‹çš„æ¯”è¾ƒä¸­ï¼Œè¿™ç§æ”¹è¿›æ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11318v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡èšç„¦å¤åˆåŠŸèƒ½æ¢¯åº¦GANï¼ˆCFGï¼‰çš„ä¸¥æ ¼æ•°å­¦ç†è®ºæ¡†æ¶ï¼Œæ­ç¤ºCFGæ¨¡å‹ä¸åŸºäºåˆ†æ•°çš„æ¨¡å‹ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚ç ”ç©¶å‘ç°CFGåˆ¤åˆ«å™¨çš„è®­ç»ƒç›®æ ‡æ˜¯å¯»æ‰¾æœ€ä¼˜çš„D(x)ï¼Œå…¶æ¢¯åº¦æœ€ä¼˜è§£èƒ½åŒºåˆ†çœŸå®ä¸åˆæˆæ ·æœ¬åˆ†æ•°å‡½æ•°çš„ç§¯åˆ†å·®ã€‚è€ŒCFGç”Ÿæˆå™¨çš„è®­ç»ƒåˆ™æ˜¯å¯»æ‰¾æœ€ä¼˜çš„G(x)ï¼Œä»¥æœ€å°åŒ–è¿™ä¸€å·®å¼‚ã€‚ä¸ºæ”¹è¿›ç°æœ‰æ–¹æ³•ï¼Œæœ¬æ–‡æå‡ºäº†é€€ç«æƒé‡å‰ç½®çš„CFGé‰´åˆ«å™¨æƒé‡çš„æ–°ç†è®ºè§£é‡Šæ¨¡å‹ï¼Œç§°ä¸ºé€€ç«CFGæ–¹æ³•ã€‚é’ˆå¯¹é€€ç«CFGæ–¹æ³•ä¸é€‚ç”¨äºå½“å‰ä¸»æµGANæ¨¡å‹çš„å±€é™æ€§ï¼Œè¿›ä¸€æ­¥æå‡ºäº†åµŒå¥—é€€ç«è®­ç»ƒæ–¹æ¡ˆï¼ˆNATSï¼‰ã€‚è¯¥æ–¹æ¡ˆä¿ç•™äº†CFGæ–¹æ³•çš„é€€ç«æƒé‡ï¼Œå¹¶èƒ½æ— ç¼é€‚åº”å„ç§GANæ¨¡å‹ï¼Œæ— è®ºå…¶ç»“æ„ã€æŸå¤±æˆ–æ­£åˆ™åŒ–å·®å¼‚å¦‚ä½•ã€‚åœ¨å›¾åƒç”Ÿæˆçš„å„ç§åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œé€€ç«CFGå’ŒNATSæ–¹æ³•æ˜¾è‘—æé«˜äº†åˆæˆæ ·æœ¬çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚ä¸CFGæ–¹æ³•å’Œå½“å‰ä¸»æµGANæ¨¡å‹ç›¸æ¯”ï¼Œæ”¹è¿›æ•ˆæœååˆ†æ˜æ˜¾ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡é‡ç‚¹å…³æ³¨å¤åˆåŠŸèƒ½æ¢¯åº¦GANï¼ˆCFGï¼‰çš„ä¸¥æ ¼æ•°å­¦ç†è®ºæ¡†æ¶ã€‚</li>
<li>æ­ç¤ºäº†CFGæ¨¡å‹ä¸åŸºäºåˆ†æ•°çš„æ¨¡å‹ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚</li>
<li>CFGåˆ¤åˆ«å™¨çš„è®­ç»ƒç›®æ ‡æ˜¯å¯»æ‰¾æœ€ä¼˜çš„D(x)ï¼Œå…¶æ¢¯åº¦åŒºåˆ†çœŸå®ä¸åˆæˆæ ·æœ¬çš„åˆ†æ•°å‡½æ•°å·®å¼‚ã€‚</li>
<li>æå‡ºäº†é€€ç«æƒé‡å‰ç½®çš„CFGé‰´åˆ«å™¨æƒé‡çš„æ–°ç†è®ºè§£é‡Šæ¨¡å‹â€”â€”é€€ç«CFGæ–¹æ³•ã€‚</li>
<li>é’ˆå¯¹é€€ç«CFGæ–¹æ³•ä¸é€‚ç”¨äºå½“å‰ä¸»æµGANæ¨¡å‹çš„å±€é™æ€§ï¼Œæå‡ºäº†åµŒå¥—é€€ç«è®­ç»ƒæ–¹æ¡ˆï¼ˆNATSï¼‰ã€‚</li>
<li>NATSæ–¹æ¡ˆèƒ½æ— ç¼é€‚åº”å„ç§GANæ¨¡å‹ï¼Œæ— è®ºå…¶ç»“æ„ã€æŸå¤±æˆ–æ­£åˆ™åŒ–å·®å¼‚ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œé€€ç«CFGå’ŒNATSæ–¹æ³•æ˜¾è‘—æé«˜äº†åˆæˆæ ·æœ¬çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-553d0cae76237c10ec5ae67f6d349b41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59025a4301b7f2b6baeadd7e94ad4b6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4296d7a2e45bbcca65ea92f91417dc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb5b8b20b2e9caa3dbcfc141957c2573.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba2ef089eef6b9bbdc4b1608c1bf417d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="EliGen-Entity-Level-Controlled-Image-Generation-with-Regional-Attention"><a href="#EliGen-Entity-Level-Controlled-Image-Generation-with-Regional-Attention" class="headerlink" title="EliGen: Entity-Level Controlled Image Generation with Regional Attention"></a>EliGen: Entity-Level Controlled Image Generation with Regional Attention</h2><p><strong>Authors:Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yu Zhang</strong></p>
<p>Recent advancements in diffusion models have significantly advanced text-to-image generation, yet global text prompts alone remain insufficient for achieving fine-grained control over individual entities within an image. To address this limitation, we present EliGen, a novel framework for Entity-Level controlled Image Generation. We introduce regional attention, a mechanism for diffusion transformers that requires no additional parameters, seamlessly integrating entity prompts and arbitrary-shaped spatial masks. By contributing a high-quality dataset with fine-grained spatial and semantic entity-level annotations, we train EliGen to achieve robust and accurate entity-level manipulation, surpassing existing methods in both spatial precision and image quality. Additionally, we propose an inpainting fusion pipeline, extending EliGenâ€™s capabilities to multi-entity image inpainting tasks. We further demonstrate its flexibility by integrating it with other open-source models such as IP-Adapter, In-Context LoRA and MLLM, unlocking new creative possibilities. The source code, model, and dataset are published at <a target="_blank" rel="noopener" href="https://github.com/modelscope/DiffSynth-Studio">https://github.com/modelscope/DiffSynth-Studio</a>. </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯è¿›æ­¥æå¤§åœ°æ¨åŠ¨äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œä½†ä»…ä½¿ç”¨å…¨å±€æ–‡æœ¬æç¤ºä»ä¸è¶³ä»¥å®ç°å¯¹å›¾åƒå†…ä¸ªåˆ«å®ä½“çš„ç²¾ç»†æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EliGenï¼Œä¸€ä¸ªç”¨äºå®ä½“çº§åˆ«æ§åˆ¶çš„å›¾åƒç”Ÿæˆæ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†åŒºåŸŸæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºæ‰©æ•£å˜å‹å™¨çš„æœºåˆ¶ï¼Œæ— éœ€æ·»åŠ é¢å¤–å‚æ•°ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆå®ä½“æç¤ºå’Œä»»æ„å½¢çŠ¶çš„ç©ºé—´æ©ç ã€‚é€šè¿‡è´¡çŒ®ä¸€ä¸ªå…·æœ‰ç²¾ç»†ç©ºé—´ç²’åº¦å’Œè¯­ä¹‰å®ä½“çº§åˆ«çš„ä¼˜è´¨æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒEliGenä»¥å®ç°ç¨³å¥å’Œå‡†ç¡®çš„å®ä½“çº§æ“æ§ï¼Œåœ¨ç©ºé—´ç²¾åº¦å’Œå›¾åƒè´¨é‡æ–¹é¢éƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¡¥å…¨èåˆç®¡é“ï¼Œæ‰©å±•äº†EliGenåœ¨å¤šå®ä½“å›¾åƒè¡¥å…¨ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚é€šè¿‡å°†å…¶ä¸å…¶ä»–å¼€æºæ¨¡å‹ï¼ˆå¦‚IP-Adapterã€In-Context LoRAå’ŒMLLMï¼‰é›†æˆï¼Œè¿›ä¸€æ­¥å±•ç¤ºäº†å…¶çµæ´»æ€§ï¼Œå¼€å¯äº†æ–°çš„åˆ›æ„å¯èƒ½æ€§ã€‚æºä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/modelscope/DiffSynth-Studio%E3%80%82">https://github.com/modelscope/DiffSynth-Studioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01097v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯è¿›æ­¥æå¤§åœ°æ¨åŠ¨äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œä»…ä¾èµ–å…¨å±€æ–‡æœ¬æç¤ºæ— æ³•å®ç°å›¾åƒçš„ä¸ªä½“å®ä½“ç²¾ç»†æ§åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EliGenæ¡†æ¶ï¼Œå¼•å…¥åŒºåŸŸæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æ— éœ€é¢å¤–å‚æ•°çš„æ‰©æ•£å˜å‹å™¨ï¼Œå®ç°å®ä½“æç¤ºå’Œä»»æ„å½¢çŠ¶çš„ç©ºé—´æ©ç çš„æ— ç¼é›†æˆã€‚é€šè¿‡è´¡çŒ®å¸¦æœ‰ç²¾ç»†ç©ºé—´è¯­ä¹‰å’Œå®ä½“çº§åˆ«æ ‡æ³¨çš„é«˜è´¨é‡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒEliGenå®ç°äº†ç¨³å¥å‡†ç¡®çš„å®ä½“çº§åˆ«æ“ä½œï¼Œåœ¨ç©ºé—´å’Œå›¾åƒè´¨é‡æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¡«å……èåˆç®¡é“ï¼Œæ‰©å±•äº†EliGenåœ¨å¤šå®ä½“å›¾åƒå¡«å……ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚ä¸IP-Adapterç­‰å…¶ä»–å¼€æºæ¨¡å‹çš„é›†æˆè¿›ä¸€æ­¥å±•ç¤ºäº†å…¶çµæ´»æ€§ã€‚ç›¸å…³èµ„æºå·²å‘å¸ƒåœ¨GitHubä¸Šï¼š<a target="_blank" rel="noopener" href="https://github.com/modelscope/DiffSynth-Studio%E3%80%82">https://github.com/modelscope/DiffSynth-Studioã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²æ˜¾è‘—æ¨åŠ¨æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆã€‚</li>
<li>ç°æœ‰æŠ€æœ¯ä»…é€šè¿‡å…¨å±€æ–‡æœ¬æç¤ºéš¾ä»¥å®ç°å›¾åƒä¸ªä½“å®ä½“çš„ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>æå‡ºEliGenæ¡†æ¶æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå¼•å…¥åŒºåŸŸæ³¨æ„åŠ›æœºåˆ¶å’Œæ‰©æ•£å˜å‹å™¨æŠ€æœ¯ã€‚</li>
<li>EliGené€šè¿‡ç»“åˆå®ä½“æç¤ºå’Œä»»æ„å½¢çŠ¶çš„ç©ºé—´æ©ç å®ç°æ— ç¼é›†æˆã€‚</li>
<li>åˆ©ç”¨é«˜è´¨é‡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«ç²¾ç»†çš„ç©ºé—´è¯­ä¹‰å’Œå®ä½“çº§åˆ«æ ‡æ³¨ã€‚</li>
<li>EliGenå®ç°äº†ç¨³å¥å‡†ç¡®çš„å®ä½“çº§åˆ«æ“ä½œï¼Œåœ¨ç©ºé—´å’Œå›¾åƒè´¨é‡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a77db58cfa8787c38f20cec31e9884fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-539944f573a718dd36c331e8adcf3332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f01a12ea2b5ccfa91f3e1f4177e0664.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e23638d3b152f1506cbc1ee3b099a7af.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DreamFit-Garment-Centric-Human-Generation-via-a-Lightweight-Anything-Dressing-Encoder"><a href="#DreamFit-Garment-Centric-Human-Generation-via-a-Lightweight-Anything-Dressing-Encoder" class="headerlink" title="DreamFit: Garment-Centric Human Generation via a Lightweight   Anything-Dressing Encoder"></a>DreamFit: Garment-Centric Human Generation via a Lightweight   Anything-Dressing Encoder</h2><p><strong>Authors:Ente Lin, Xujie Zhang, Fuwei Zhao, Yuxuan Luo, Xin Dong, Long Zeng, Xiaodan Liang</strong></p>
<p>Diffusion models for garment-centric human generation from text or image prompts have garnered emerging attention for their great application potential. However, existing methods often face a dilemma: lightweight approaches, such as adapters, are prone to generate inconsistent textures; while finetune-based methods involve high training costs and struggle to maintain the generalization capabilities of pretrained diffusion models, limiting their performance across diverse scenarios. To address these challenges, we propose DreamFit, which incorporates a lightweight Anything-Dressing Encoder specifically tailored for the garment-centric human generation. DreamFit has three key advantages: (1) \textbf{Lightweight training}: with the proposed adaptive attention and LoRA modules, DreamFit significantly minimizes the model complexity to 83.4M trainable parameters. (2)\textbf{Anything-Dressing}: Our model generalizes surprisingly well to a wide range of (non-)garments, creative styles, and prompt instructions, consistently delivering high-quality results across diverse scenarios. (3) \textbf{Plug-and-play}: DreamFit is engineered for smooth integration with any community control plugins for diffusion models, ensuring easy compatibility and minimizing adoption barriers. To further enhance generation quality, DreamFit leverages pretrained large multi-modal models (LMMs) to enrich the prompt with fine-grained garment descriptions, thereby reducing the prompt gap between training and inference. We conduct comprehensive experiments on both $768 \times 512$ high-resolution benchmarks and in-the-wild images. DreamFit surpasses all existing methods, highlighting its state-of-the-art capabilities of garment-centric human generation. </p>
<blockquote>
<p>ä»¥æœè£…ä¸ºä¸­å¿ƒçš„äººç±»ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä»æ–‡æœ¬æˆ–å›¾åƒæç¤ºä¸­è·å¾—äº†æ–°å…´çš„å…³æ³¨ï¼Œå› å…¶å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸é¢ä¸´ä¸€ä¸ªå›°å¢ƒï¼šè½»é‡çº§æ–¹æ³•ï¼ˆå¦‚é€‚é…å™¨ï¼‰å®¹æ˜“äº§ç”Ÿä¸ä¸€è‡´çš„çº¹ç†ï¼›è€ŒåŸºäºå¾®è°ƒçš„æ–¹æ³•æ¶‰åŠè¾ƒé«˜çš„è®­ç»ƒæˆæœ¬ï¼Œä¸”éš¾ä»¥ç»´æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œåœ¨å¤šç§åœºæ™¯ä¸‹çš„æ€§èƒ½å—é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DreamFitã€‚å®ƒèå…¥äº†ä¸€ä¸ªè½»é‡çº§çš„Anything-Dressingç¼–ç å™¨ï¼Œä¸“é—¨ç”¨äºä»¥æœè£…ä¸ºä¸­å¿ƒçš„äººç±»ç”Ÿæˆã€‚DreamFitæœ‰ä¸‰ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰<strong>è½»é‡çº§è®­ç»ƒ</strong>ï¼šé€šè¿‡æå‡ºçš„è‡ªé€‚åº”æ³¨æ„åŠ›å’ŒLoRAæ¨¡å—ï¼ŒDreamFitå°†æ¨¡å‹å¤æ‚åº¦å¤§å¤§é™ä½è‡³ä»…åŒ…å«çº¦å¯è®­ç»ƒçš„å‚æ•°ã€‚è¿™ä¸ä»…å¤§å¤§ç®€åŒ–äº†è®­ç»ƒæµç¨‹å¹¶å‡å°‘äº†æ‰€éœ€çš„èµ„æºã€‚ï¼ˆä»¥åŸæ–‡è¡¨è¿°ä¸ºåŸºç¡€è‡ªè¡Œç¿»è¯‘ï¼‰ã€‚ï¼ˆæ³¨ï¼šæ­¤å¤„éœ€è¦æä¾›å…·ä½“çš„å‚æ•°æ•°é‡ä»¥ä¾›å‡†ç¡®ç¿»è¯‘ï¼‰ï¼ˆ2ï¼‰<strong>å¹¿æ³›é€‚åº”æ€§</strong>ï¼šæˆ‘ä»¬çš„æ¨¡å‹å¯¹å„ç§æœè£…ï¼ˆåŒ…æ‹¬éæœè£…ï¼‰ã€åˆ›æ„é£æ ¼å’Œæç¤ºæŒ‡ä»¤çš„é€‚åº”æ€§ä»¤äººæƒŠè®¶åœ°å¥½ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯ä¸‹æŒç»­æä¾›é«˜è´¨é‡çš„ç»“æœã€‚ï¼ˆ3ï¼‰<strong>å³æ’å³ç”¨</strong>ï¼šDreamFitçš„è®¾è®¡æ—¨åœ¨ä¸æ‰©æ•£æ¨¡å‹çš„ä»»ä½•ç¤¾åŒºæ§åˆ¶æ’ä»¶æ— ç¼é›†æˆï¼Œç¡®ä¿æ˜“äºå…¼å®¹å¹¶å°½é‡å‡å°‘é‡‡ç”¨éšœç¢ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ç”Ÿæˆè´¨é‡ï¼ŒDreamFitåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ¥ä¸°å¯Œæç¤ºä¸­çš„ç²¾ç»†æœè£…æè¿°ï¼Œä»è€Œç¼©å°è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„æç¤ºå·®è·ã€‚æˆ‘ä»¬åœ¨é«˜æ¸…åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œå›¾åƒä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒéªŒè¯ã€‚DreamFitè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å…¶åœ¨ä»¥æœè£…ä¸ºä¸­å¿ƒçš„äººç±»ç”Ÿæˆæ–¹é¢çš„æœ€æ–°èƒ½åŠ›å’ŒæŠ€æœ¯é¢†å…ˆåœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17644v3">PDF</a> Accepted at AAAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹æœè£…ä¸­å¿ƒåŒ–äººç±»ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ï¼ŒåŒ…æ‹¬ç°æœ‰æ–¹æ³•çš„æŒ‘æˆ˜ä»¥åŠåº”å¯¹æ–¹æ³•ã€‚DreamFitæ¨¡å‹çš„æå‡ºè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œå…·æœ‰è½»é‡çº§è®­ç»ƒã€é€‚åº”æ€§å¼ºå’Œæ’ä»¶å…¼å®¹æ€§ç­‰ç‰¹ç‚¹ï¼Œå¹¶å¯é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹å¢å¼ºç”Ÿæˆè´¨é‡ã€‚æ€»ç»“ï¼šDreamFitæ¨¡å‹ä¸ºè§£å†³æœè£…ä¸­å¿ƒåŒ–äººç±»ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ä¸­çš„æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆæ–¹æ³•ï¼Œå…·æœ‰ä¼˜ç§€æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœè£…ä¸­å¿ƒåŒ–äººç±»ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´è½»é‡çº§æ–¹æ³•æ˜“äº§ç”Ÿçº¹ç†ä¸ä¸€è‡´å’ŒåŸºäºå¾®è°ƒçš„æ–¹æ³•è®­ç»ƒæˆæœ¬é«˜ã€éš¾ä»¥ç»´æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ç­‰é—®é¢˜ã€‚</li>
<li>DreamFitæ¨¡å‹é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ³¨æ„åŠ›å’ŒLoRAæ¨¡å—å®ç°äº†è½»é‡çº§è®­ç»ƒã€‚</li>
<li>DreamFitæ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œèƒ½å¤„ç†å¹¿æ³›çš„æœè£…å’Œéæœè£…ç±»åˆ«ã€åˆ›æ„é£æ ¼å’Œæç¤ºæŒ‡ä»¤ã€‚</li>
<li>DreamFitæ¨¡å‹è®¾è®¡ç”¨äºä¸æ‰©æ•£æ¨¡å‹çš„ä»»ä½•ç¤¾åŒºæ§åˆ¶æ’ä»¶æ— ç¼é›†æˆï¼Œç¡®ä¿è‰¯å¥½çš„å…¼å®¹æ€§å¹¶æœ€å°åŒ–é‡‡ç”¨éšœç¢ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸°å¯Œäº†æç¤ºï¼Œç¼©å°äº†è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„æç¤ºå·®è·ã€‚</li>
<li>DreamFitæ¨¡å‹åœ¨å®éªŒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08b1fb1c069801d19d93ea58298fe463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c266b2b69d443bf15c32ac61f9e4b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85489edabeca79565cd0cd571a8b238.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-237d8d9fea20ce8fa5ebcf60b0aaa8d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e8fb5f46271415f0808b5394e80d533.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Schedule-On-the-Fly-Diffusion-Time-Prediction-for-Faster-and-Better-Image-Generation"><a href="#Schedule-On-the-Fly-Diffusion-Time-Prediction-for-Faster-and-Better-Image-Generation" class="headerlink" title="Schedule On the Fly: Diffusion Time Prediction for Faster and Better   Image Generation"></a>Schedule On the Fly: Diffusion Time Prediction for Faster and Better   Image Generation</h2><p><strong>Authors:Zilyu Ye, Zhiyang Chen, Tiancheng Li, Zemin Huang, Weijian Luo, Guo-Jun Qi</strong></p>
<p>Diffusion and flow models have achieved remarkable successes in various applications such as text-to-image generation. However, these models typically rely on the same predetermined denoising schedules during inference for each prompt, which potentially limits the inference efficiency as well as the flexibility when handling different prompts. In this paper, we argue that the optimal noise schedule should adapt to each inference instance, and introduce the Time Prediction Diffusion Model (TPDM) to accomplish this. TPDM employs a plug-and-play Time Prediction Module (TPM) that predicts the next noise level based on current latent features at each denoising step. We train the TPM using reinforcement learning, aiming to maximize a reward that discounts the final image quality by the number of denoising steps. With such an adaptive scheduler, TPDM not only generates high-quality images that are aligned closely with human preferences but also adjusts the number of denoising steps and time on the fly, enhancing both performance and efficiency. We train TPDMs on multiple diffusion model benchmarks. With Stable Diffusion 3 Medium architecture, TPDM achieves an aesthetic score of 5.44 and a human preference score (HPS) of 29.59, while using around 50% fewer denoising steps to achieve better performance. We will release our best model alongside this paper. </p>
<blockquote>
<p>æ‰©æ•£å’ŒæµåŠ¨æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒç­‰å„ç§åº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šå¸¸ä¾èµ–äºä¸ºæ¯ä¸ªæç¤ºé¢„å…ˆç¡®å®šçš„å»å™ªæ—¶é—´è¡¨ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶æ¨ç†æ•ˆç‡å’Œå¤„ç†ä¸åŒæç¤ºæ—¶çš„çµæ´»æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸»å¼ æœ€ä½³çš„å™ªå£°æ—¶é—´è¡¨åº”é€‚åº”æ¯ä¸ªæ¨ç†å®ä¾‹ï¼Œå¹¶å¼•å…¥æ—¶é—´é¢„æµ‹æ‰©æ•£æ¨¡å‹ï¼ˆTPDMï¼‰æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚TPDMé‡‡ç”¨å³æ’å³ç”¨çš„æ—¶é—´é¢„æµ‹æ¨¡å—ï¼ˆTPMï¼‰ï¼Œè¯¥æ¨¡å—æ ¹æ®æ¯ä¸ªå»å™ªæ­¥éª¤çš„å½“å‰æ½œåœ¨ç‰¹å¾é¢„æµ‹ä¸‹ä¸€ä¸ªå™ªå£°æ°´å¹³ã€‚æˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒTPMï¼Œæ—¨åœ¨æœ€å¤§åŒ–å¥–åŠ±ï¼Œè¯¥å¥–åŠ±é€šè¿‡å»å™ªæ­¥éª¤çš„æ•°é‡æ¥è´´ç°æœ€ç»ˆå›¾åƒè´¨é‡ã€‚æœ‰äº†è¿™æ ·çš„è‡ªé€‚åº”è°ƒåº¦å™¨ï¼ŒTPDMä¸ä»…ç”Ÿæˆä¸äººç±»åå¥½ç´§å¯†å¯¹é½çš„é«˜è´¨é‡å›¾åƒï¼Œè¿˜å¯ä»¥å®æ—¶è°ƒæ•´å»å™ªæ­¥éª¤çš„æ•°é‡å’Œæ—¶é—´ï¼Œä»è€Œæé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ‰©æ•£æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸Šå¯¹TPDMè¿›è¡Œäº†è®­ç»ƒã€‚ä½¿ç”¨Stable Diffusion 3 Mediumæ¶æ„ï¼ŒTPDMè¾¾åˆ°äº†ç¾å­¦è¯„åˆ†5.44å’Œäººç±»åå¥½è¯„åˆ†ï¼ˆHPSï¼‰29.59ï¼ŒåŒæ—¶ä½¿ç”¨çº¦50%æ›´å°‘çš„å»å™ªæ­¥éª¤å®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†éšæ­¤è®ºæ–‡å‘å¸ƒæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01243v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†æ—¶é—´é¢„æµ‹æ‰©æ•£æ¨¡å‹ï¼ˆTPDMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹æ¯ä¸ªæ¨æ–­å®ä¾‹è¿›è¡Œè‡ªé€‚åº”å™ªå£°è°ƒåº¦ã€‚é€šè¿‡ä½¿ç”¨åŸºäºå½“å‰æ½œåœ¨ç‰¹å¾çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ—¶é—´é¢„æµ‹æ¨¡å—ï¼ˆTPMï¼‰ï¼ŒTPDMèƒ½å¤Ÿåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤é¢„æµ‹ä¸‹ä¸€ä¸ªå™ªå£°æ°´å¹³ã€‚è¿™ç§è‡ªé€‚åº”è°ƒåº¦å™¨ä¸ä»…èƒ½ç”Ÿæˆä¸äººç±»åå¥½ç´§å¯†å¯¹é½çš„é«˜è´¨é‡å›¾åƒï¼Œè¿˜èƒ½å®æ—¶è°ƒæ•´å»å™ªæ­¥éª¤å’Œæ—¶é—´ï¼Œæé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚åœ¨å¤šä¸ªæ‰©æ•£æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸Šï¼Œä½¿ç”¨Stable Diffusion 3 Mediumæ¶æ„çš„TPDMè¾¾åˆ°äº†ç¾å­¦è¯„åˆ†5.44å’Œäººç±»åå¥½è¯„åˆ†ï¼ˆHPSï¼‰29.59ï¼ŒåŒæ—¶ä½¿ç”¨çº¦50%çš„æ›´å°‘çš„å»å™ªæ­¥éª¤å®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰åº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨æ¨æ–­æ—¶å¯¹æ¯ä¸ªæç¤ºä½¿ç”¨é¢„å®šçš„å»å™ªæ—¶é—´è¡¨ï¼Œè¿™é™åˆ¶äº†æ¨æ–­æ•ˆç‡å’Œçµæ´»æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†æ—¶é—´é¢„æµ‹æ‰©æ•£æ¨¡å‹ï¼ˆTPDMï¼‰ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªæ¨æ–­å®ä¾‹çš„å™ªå£°è°ƒåº¦ã€‚</li>
<li>TPDMé€šè¿‡ä½¿ç”¨æ—¶é—´é¢„æµ‹æ¨¡å—ï¼ˆTPMï¼‰æ¥é¢„æµ‹æ¯ä¸ªå»å™ªæ­¥éª¤çš„ä¸‹ä¸€ä¸ªå™ªå£°æ°´å¹³ï¼Œè¯¥æ¨¡å—åŸºäºå½“å‰æ½œåœ¨ç‰¹å¾è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚</li>
<li>TPDMä¸ä»…èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œè¿˜æé«˜äº†æ€§èƒ½å’Œæ•ˆç‡ï¼Œèƒ½å¤Ÿå®æ—¶è°ƒæ•´å»å™ªæ­¥éª¤å’Œæ—¶é—´ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨Stable Diffusion 3 Mediumæ¶æ„çš„TPDMå®ç°äº†ç¾å­¦è¯„åˆ†å’Œäººç±»åå¥½è¯„åˆ†çš„æ˜¾è‘—æå‡ã€‚</li>
<li>TPDMä½¿ç”¨çº¦50%çš„æ›´å°‘çš„å»å™ªæ­¥éª¤è¾¾åˆ°äº†æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2e44213ba7a42e32eb5f0f7f82d6c219.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-491a744035944832f1d68966402055f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df8aa54e2d41f562e33878347e9baa3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d02424faaa68b900c8c69658e783349d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c67ea76197c6d0336a376a097f5a3bb2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Bayesian-Deconvolution-of-Astronomical-Images-with-Diffusion-Models-Quantifying-Prior-Driven-Features-in-Reconstructions"><a href="#Bayesian-Deconvolution-of-Astronomical-Images-with-Diffusion-Models-Quantifying-Prior-Driven-Features-in-Reconstructions" class="headerlink" title="Bayesian Deconvolution of Astronomical Images with Diffusion Models:   Quantifying Prior-Driven Features in Reconstructions"></a>Bayesian Deconvolution of Astronomical Images with Diffusion Models:   Quantifying Prior-Driven Features in Reconstructions</h2><p><strong>Authors:Alessio Spagnoletti, Alexandre Boucaud, Marc Huertas-Company, Wassim Kabalan, Biswajit Biswas</strong></p>
<p>Deconvolution of astronomical images is a key aspect of recovering the intrinsic properties of celestial objects, especially when considering ground-based observations. This paper explores the use of diffusion models (DMs) and the Diffusion Posterior Sampling (DPS) algorithm to solve this inverse problem task. We apply score-based DMs trained on high-resolution cosmological simulations, through a Bayesian setting to compute a posterior distribution given the observations available. By considering the redshift and the pixel scale as parameters of our inverse problem, the tool can be easily adapted to any dataset. We test our model on Hyper Supreme Camera (HSC) data and show that we reach resolutions comparable to those obtained by Hubble Space Telescope (HST) images. Most importantly, we quantify the uncertainty of reconstructions and propose a metric to identify prior-driven features in the reconstructed images, which is key in view of applying these methods for scientific purposes. </p>
<blockquote>
<p>å¤©æ–‡å›¾åƒçš„è§£å·ç§¯æ˜¯æ¢å¤å¤©ä½“å›ºæœ‰å±æ€§çš„å…³é”®æ–¹é¢ï¼Œç‰¹åˆ«æ˜¯åœ¨è€ƒè™‘åœ°é¢è§‚æµ‹æ—¶ã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ç®—æ³•æ¥è§£å†³è¿™ä¸ªåé—®é¢˜ä»»åŠ¡ã€‚æˆ‘ä»¬åº”ç”¨åŸºäºé«˜åˆ†å®‡å®™æ¨¡æ‹Ÿè®­ç»ƒçš„é«˜åˆ†æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è´å¶æ–¯è®¾ç½®æ¥è®¡ç®—ç»™å®šå¯ç”¨è§‚æµ‹æ•°æ®çš„åéªŒåˆ†å¸ƒã€‚è€ƒè™‘åˆ°çº¢ç§»å’Œåƒç´ å°ºåº¦ä½œä¸ºæˆ‘ä»¬åé—®é¢˜çš„å‚æ•°ï¼Œè¯¥å·¥å…·å¯ä»¥å¾ˆå®¹æ˜“åœ°é€‚åº”ä»»ä½•æ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨Hyper Supreme Cameraï¼ˆHSCï¼‰æ•°æ®ä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶è¯æ˜æˆ‘ä»¬è¾¾åˆ°äº†ä¸å“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œï¼ˆHSTï¼‰å›¾åƒç›¸å½“çš„åˆ†è¾¨ç‡ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬é‡åŒ–äº†é‡å»ºçš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§è¯†åˆ«é‡å»ºå›¾åƒä¸­å…ˆéªŒç‰¹å¾çš„åº¦é‡æŒ‡æ ‡ï¼Œè¿™å¯¹äºå°†è¿™äº›æ–¹æ³•åº”ç”¨äºç§‘å­¦ç›®çš„è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19158v2">PDF</a> 5+5 pages, 16 figures, Machine Learning and the Physical Sciences   Workshop, NeurIPS 2024</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ç®—æ³•æ¥è§£ç®—å¤©æ–‡å­¦å›¾åƒåé—®é¢˜çš„ä»»åŠ¡ã€‚é€šè¿‡åœ¨é«˜åˆ†è¾¨ç‡å®‡å®™å­¦æ¨¡æ‹Ÿä¸Šè®­ç»ƒåŸºäºåˆ†æ•°çš„DMsï¼Œå¹¶åœ¨è´å¶æ–¯è®¾ç½®ä¸‹è®¡ç®—ç»™å®šè§‚æµ‹æ•°æ®çš„åéªŒåˆ†å¸ƒï¼Œè¯¥å·¥å…·å¯è½»æ¾é€‚åº”ä»»ä½•æ•°æ®é›†ã€‚å¯¹Hyper Supreme Cameraï¼ˆHSCï¼‰æ•°æ®çš„æµ‹è¯•è¡¨æ˜ï¼Œå…¶åˆ†è¾¨ç‡ä¸å“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œï¼ˆHSTï¼‰å›¾åƒç›¸å½“ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæœ¬æ–‡é‡åŒ–äº†é‡å»ºçš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§è¯†åˆ«é‡å»ºå›¾åƒä¸­å…ˆéªŒç‰¹å¾çš„åº¦é‡æ ‡å‡†ï¼Œè¿™å¯¹äºå°†è¿™äº›æ–¹æ³•åº”ç”¨äºç§‘å­¦ç›®çš„è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ç®—æ³•ç”¨äºè§£å†³å¤©æ–‡å­¦å›¾åƒåé—®é¢˜ã€‚</li>
<li>é€šè¿‡åœ¨é«˜åˆ†è¾¨ç‡å®‡å®™å­¦æ¨¡æ‹Ÿä¸Šè®­ç»ƒåŸºäºåˆ†æ•°çš„DMsæ¥å¤„ç†å¤©æ–‡å›¾åƒã€‚</li>
<li>åœ¨è´å¶æ–¯è®¾ç½®ä¸‹è®¡ç®—åéªŒåˆ†å¸ƒä»¥é€‚åº”ä¸åŒçš„æ•°æ®é›†ã€‚</li>
<li>å¯¹Hyper Supreme Cameraï¼ˆHSCï¼‰æ•°æ®çš„æµ‹è¯•æ˜¾ç¤ºï¼Œé‡å»ºçš„å›¾åƒåˆ†è¾¨ç‡ä¸å“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œç›¸å½“ã€‚</li>
<li>é‡åŒ–é‡å»ºå›¾åƒçš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>æå‡ºä¸€ç§è¯†åˆ«é‡å»ºå›¾åƒä¸­ç”±å…ˆéªŒç‰¹å¾é©±åŠ¨çš„åº¦é‡æ ‡å‡†ã€‚</li>
<li>è¿™äº›æ–¹æ³•åœ¨ç§‘å­¦åº”ç”¨ä¸­çš„æ½œåœ¨é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebd7c4c4c5e8f6af0b95d8652698452c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca204112c39c33d5b33ce10471c0424d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cc5f5efcbc8fbb75359df2e5dfd2c69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b23a213fbe0cd1ddced1d7ff48e05495.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4315bc1923935bc924db7e2a37fbba24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2f14ba2aed4089c36d6fa634ad2ac51.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="On-Improved-Conditioning-Mechanisms-and-Pre-training-Strategies-for-Diffusion-Models"><a href="#On-Improved-Conditioning-Mechanisms-and-Pre-training-Strategies-for-Diffusion-Models" class="headerlink" title="On Improved Conditioning Mechanisms and Pre-training Strategies for   Diffusion Models"></a>On Improved Conditioning Mechanisms and Pre-training Strategies for   Diffusion Models</h2><p><strong>Authors:Tariq Berrada Ifriqi, Pietro Astolfi, Melissa Hall, Reyhane Askari-Hemmat, Yohann Benchetrit, Marton Havasi, Matthew Muckley, Karteek Alahari, Adriana Romero-Soriano, Jakob Verbeek, Michal Drozdzal</strong></p>
<p>Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, the key components of the best performing LDM training recipes are oftentimes not available to the research community, preventing apple-to-apple comparisons and hindering the validation of progress in the field. In this work, we perform an in-depth study of LDM training recipes focusing on the performance of models and their training efficiency. To ensure apple-to-apple comparisons, we re-implement five previously published models with their corresponding recipes. Through our study, we explore the effects of (i)<del>the mechanisms used to condition the generative model on semantic information (e.g., text prompt) and control metadata (e.g., crop size, random flip flag, etc.) on the model performance, and (ii)</del>the transfer of the representations learned on smaller and lower-resolution datasets to larger ones on the training efficiency and model performance. We then propose a novel conditioning mechanism that disentangles semantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset â€“ with FID improvements of 7% on 256 and 8% on 512 resolutions â€“ as well as text-to-image generation on the CC12M dataset â€“ with FID improvements of 8% on 256 and 23% on 512 resolution. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ½œæ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„è®­ç»ƒå·²ç»å®ç°äº†å›¾åƒç”Ÿæˆå‰æ‰€æœªæœ‰çš„è´¨é‡ã€‚ç„¶è€Œï¼Œè¡¨ç°æœ€ä½³çš„LDMè®­ç»ƒæ–¹æ¡ˆçš„å…³é”®ç»„æˆéƒ¨åˆ†é€šå¸¸ä¸å¯¹ç ”ç©¶ç¤¾åŒºå¼€æ”¾ï¼Œè¿™é˜»ç¢äº†ç›´æ¥çš„å¯¹æ¯”éªŒè¯ï¼Œä¹Ÿé˜»ç¢äº†è¯¥é¢†åŸŸçš„è¿›å±•éªŒè¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹LDMè®­ç»ƒæ–¹æ¡ˆè¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹æ€§èƒ½åŠå…¶è®­ç»ƒæ•ˆç‡ã€‚ä¸ºäº†ç¡®ä¿ç›´æ¥çš„å¯¹æ¯”éªŒè¯ï¼Œæˆ‘ä»¬é‡æ–°å®ç°äº†äº”ä¸ªä¹‹å‰å‘å¸ƒçš„æ¨¡å‹åŠå…¶ç›¸åº”çš„æ–¹æ¡ˆã€‚é€šè¿‡æˆ‘ä»¬çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ï¼ˆiï¼‰ç”¨äºå°†ç”Ÿæˆæ¨¡å‹æ ¹æ®è¯­ä¹‰ä¿¡æ¯ï¼ˆä¾‹å¦‚æ–‡æœ¬æç¤ºï¼‰è¿›è¡Œæ¡ä»¶è®¾ç½®å’Œæ§åˆ¶å…ƒæ•°æ®ï¼ˆä¾‹å¦‚è£å‰ªå¤§å°ã€éšæœºç¿»è½¬æ ‡å¿—ç­‰ï¼‰çš„æœºåˆ¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä»¥åŠï¼ˆiiï¼‰åœ¨è¾ƒå°å’Œä½åˆ†è¾¨ç‡æ•°æ®é›†ä¸Šå­¦ä¹ çš„è¡¨ç¤ºè½¬ç§»åˆ°è¾ƒå¤§æ•°æ®é›†æ—¶å¯¹è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡ä»¶è®¾ç½®æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿåˆ†ç¦»è¯­ä¹‰å’Œæ§åˆ¶å…ƒæ•°æ®æ¡ä»¶è®¾ç½®ï¼Œå¹¶åœ¨ImageNet-1kæ•°æ®é›†ä¸Šå®ç°äº†ç±»æ¡ä»¶ç”Ÿæˆçš„æ–°æŠ€æœ¯æ°´å¹³â€”â€”åœ¨256å’Œ512åˆ†è¾¨ç‡ä¸‹FIDæ”¹å–„äº†7%å’Œ8%â€”â€”ä»¥åŠåœ¨CC12Mæ•°æ®é›†ä¸Šçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆâ€”â€”åœ¨256å’Œ512åˆ†è¾¨ç‡ä¸‹FIDåˆ†åˆ«æé«˜äº†8%å’Œ23%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03177v2">PDF</a> Accepted as a conference paper (poster) for NeurIPS 2024</p>
<p><strong>Summary</strong>:<br>æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å¤§è§„æ¨¡è®­ç»ƒä¸ºå›¾åƒç”Ÿæˆå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„è´¨é‡ã€‚æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ‰©æ•£æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹æ€§èƒ½ä¸è®­ç»ƒæ•ˆç‡ã€‚ç ”ç©¶ä¸­é‡æ–°å®ç°äº†äº”ä¸ªå…ˆå‰å‘è¡¨çš„æ¨¡å‹åŠå…¶ç­–ç•¥ä»¥ç¡®ä¿å¯¹æ¯”åˆ†æçš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è¯­ä¹‰ä¿¡æ¯å’Œæ§åˆ¶å…ƒæ•°æ®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä»¥åŠåœ¨å°åˆ†è¾¨ç‡æ•°æ®é›†ä¸Šå­¦ä¹ çš„è¡¨ç¤ºè½¬ç§»åˆ°é«˜åˆ†è¾¨ç‡æ•°æ®é›†ä¸Šçš„è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æå‡ºä¸€ç§æ–°å‹æ¡ä»¶æœºåˆ¶ï¼Œå°†è¯­ä¹‰å’Œæ§åˆ¶å…ƒæ•°æ®æ¡ä»¶åˆ†ç¦»ï¼Œå¹¶åœ¨ImageNet-1kæ•°æ®é›†ä¸Šå®ç°äº†ç±»æ¡ä»¶ç”Ÿæˆçš„æ–°æ°´å¹³ï¼ŒFIDåœ¨256å’Œ512åˆ†è¾¨ç‡ä¸Šåˆ†åˆ«æé«˜äº†7%å’Œ8%ã€‚åŒæ—¶ä¹Ÿåœ¨CC12Mæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ”¹è¿›ï¼ŒFIDåœ¨è¾ƒé«˜åˆ†è¾¨ç‡ä¸‹æå‡æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§è§„æ¨¡è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä¸ºå›¾åƒç”Ÿæˆå¸¦æ¥å“è¶Šè´¨é‡ã€‚</li>
<li>ç ”ç©¶é›†ä¸­åœ¨LDMçš„è®­ç»ƒç­–ç•¥ï¼Œå°¤å…¶æ˜¯æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡çš„æ¯”è¾ƒã€‚</li>
<li>é€šè¿‡é‡æ–°å®ç°äº”ä¸ªå…ˆå‰å‘è¡¨çš„æ¨¡å‹åŠå…¶ç­–ç•¥ï¼Œç¡®ä¿äº†å¯¹æ¯”åˆ†æçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¢è®¨äº†è¯­ä¹‰ä¿¡æ¯å’Œæ§åˆ¶å…ƒæ•°æ®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>ç ”ç©¶äº†å°†å°åˆ†è¾¨ç‡æ•°æ®é›†ä¸Šå­¦ä¹ çš„è¡¨ç¤ºè½¬ç§»åˆ°å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡ä»¶æœºåˆ¶ï¼Œå®ç°äº†ç±»æ¡ä»¶ç”Ÿæˆçš„æ–°æ°´å¹³ï¼Œåœ¨ImageNet-1kæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†FIDå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.03177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2411.03177v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2411.03177v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2411.03177v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2411.03177v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2411.03177v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2411.03177v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Lotus-Diffusion-based-Visual-Foundation-Model-for-High-quality-Dense-Prediction"><a href="#Lotus-Diffusion-based-Visual-Foundation-Model-for-High-quality-Dense-Prediction" class="headerlink" title="Lotus: Diffusion-based Visual Foundation Model for High-quality Dense   Prediction"></a>Lotus: Diffusion-based Visual Foundation Model for High-quality Dense   Prediction</h2><p><strong>Authors:Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, Ying-Cong Chen</strong></p>
<p>Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising&#x2F;denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotusâ€™ superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single&#x2F;multi-view 3D reconstruction, etc. Project page: <a target="_blank" rel="noopener" href="https://lotus3d.github.io/">https://lotus3d.github.io/</a>. </p>
<blockquote>
<p>åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œä¸ºè§£å†³é›¶æ ·æœ¬é¢„æµ‹ä»»åŠ¡ä¸­çš„å¯†é›†é¢„æµ‹é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¸åŠ æ‰¹åˆ¤åœ°ä½¿ç”¨åŸå§‹æ‰©æ•£å…¬å¼ï¼Œè¿™å¯èƒ½ä¼šå› ä¸ºå¯†é›†é¢„æµ‹å’Œå›¾åƒç”Ÿæˆä¹‹é—´çš„åŸºæœ¬å·®å¼‚è€Œå¯¼è‡´æ•ˆæœä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹å¯†é›†é¢„æµ‹çš„æ‰©æ•£å…¬å¼è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œé‡ç‚¹è€ƒè™‘äº†è´¨é‡å’Œæ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œç”¨äºå›¾åƒç”Ÿæˆçš„åŸå§‹å‚æ•°åŒ–ç±»å‹ï¼ˆå­¦ä¹ é¢„æµ‹å™ªå£°ï¼‰å¯¹å¯†é›†é¢„æµ‹æ˜¯æœ‰å®³çš„ï¼›å¤šæ­¥åŠ å™ª&#x2F;å»å™ªæ‰©æ•£è¿‡ç¨‹ä¹Ÿæ˜¯ä¸å¿…è¦çš„ï¼Œå¹¶ä¸”éš¾ä»¥ä¼˜åŒ–ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†Lotusï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰ç”¨äºå¯†é›†é¢„æµ‹çš„ç®€å•æœ‰æ•ˆçš„é€‚åº”åè®®ã€‚å…·ä½“æ¥è¯´ï¼ŒLotusç»è¿‡è®­ç»ƒï¼Œç›´æ¥é¢„æµ‹æ³¨é‡Šè€Œä¸æ˜¯å™ªå£°ï¼Œä»è€Œé¿å…äº†æœ‰å®³çš„æ–¹å·®ã€‚æˆ‘ä»¬è¿˜é‡æ–°åˆ¶å®šäº†æ‰©æ•£è¿‡ç¨‹ä¸ºå•æ­¥ç¨‹åºï¼Œç®€åŒ–äº†ä¼˜åŒ–å¹¶å¤§å¤§æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è°ƒæ•´ç­–ç•¥ï¼Œç§°ä¸ºç»†èŠ‚ä¿ç•™å™¨ï¼Œå®ç°äº†æ›´å‡†ç¡®å’Œç²¾ç»†çš„é¢„æµ‹ã€‚åœ¨ä¸æ‰©å¤§è®­ç»ƒæ•°æ®æˆ–æ¨¡å‹å®¹é‡çš„å‰æä¸‹ï¼ŒLotusåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬æ·±åº¦å’Œæ³•çº¿ä¼°è®¡çš„æœ€æ–°æ€§èƒ½ã€‚å®ƒä¹Ÿæé«˜äº†æ•ˆç‡ï¼Œæ¯”å¤§å¤šæ•°ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•æ›´å¿«ã€‚Lotusçš„å“è¶Šè´¨é‡å’Œæ•ˆç‡è¿˜å¯ç”¨äº†å¹¿æ³›çš„å®é™…åº”ç”¨ï¼Œå¦‚è”åˆä¼°è®¡ã€å•&#x2F;å¤šè§†å›¾3Dé‡å»ºç­‰ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://lotus3d.github.io/%E3%80%82">https://lotus3d.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18124v5">PDF</a> The first two authors contributed equally. Project page:   <a target="_blank" rel="noopener" href="https://lotus3d.github.io/">https://lotus3d.github.io/</a></p>
<p><strong>Summary</strong><br>    åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬å›¾åƒæ‰©æ•£æ¨¡å‹çš„è§†è§‰å…ˆéªŒçŸ¥è¯†å¯¹äºå¢å¼ºé›¶æ ·æœ¬åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›å…·æœ‰å‰æ™¯ã€‚æœ¬æ–‡å¯¹å¯†é›†é¢„æµ‹çš„æ‰©æ•£å…¬å¼è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œå¹¶å‘ç°åŸå§‹å‚æ•°åŒ–ç±»å‹å¯¹å¯†é›†é¢„æµ‹æœ‰å®³ï¼Œä¸”å¤šæ­¥å™ªå£°&#x2F;å»å™ªå£°æ‰©æ•£è¿‡ç¨‹ä¸å¿…è¦ä¸”éš¾ä»¥ä¼˜åŒ–ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†åä¸ºLotusçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç®€åŒ–æ‰©æ•£è¿‡ç¨‹å¹¶é‡‡ç”¨ç›´æ¥é¢„æµ‹æ ‡æ³¨çš„æ–¹æ³•æ”¹è¿›äº†å¯†é›†é¢„æµ‹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åä¸ºç»†èŠ‚ä¿ç•™çš„æ–°å¾®è°ƒç­–ç•¥ï¼Œå®ç°äº†æ›´å‡†ç¡®å’Œç²¾ç»†çš„é¢„æµ‹ã€‚Lotusåœ¨ä¸å¢åŠ è®­ç»ƒæ•°æ®æˆ–æ¨¡å‹å®¹é‡çš„å‰æä¸‹ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†é›¶æ ·æœ¬æ·±åº¦å’Œæ³•çº¿ä¼°è®¡çš„å“è¶Šæ€§èƒ½ï¼Œå¹¶æé«˜äº†æ•ˆç‡ï¼Œæ˜¾è‘—å¿«äºç°æœ‰çš„å¤§å¤šæ•°æ‰©æ•£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬å›¾åƒæ‰©æ•£æ¨¡å‹çš„è§†è§‰å…ˆéªŒå¯ä»¥æé«˜é›¶æ ·æœ¬åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŸå‚æ•°åŒ–ç±»å‹ç”¨äºå¯†é›†é¢„æµ‹å¯èƒ½å¹¶éæœ€ä¼˜ï¼Œå­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚</li>
<li>Lotusæ¨¡å‹é€šè¿‡ç›´æ¥é¢„æµ‹æ ‡æ³¨è€Œéå™ªå£°ç®€åŒ–äº†æ‰©æ•£è¿‡ç¨‹ï¼Œæé«˜äº†å¯†é›†é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>Lotuså¼•å…¥äº†ä¸€ç§ç®€åŒ–çš„æ‰©æ•£è¿‡ç¨‹åŠæ–°çš„å¾®è°ƒç­–ç•¥ï¼ˆç»†èŠ‚ä¿ç•™ï¼‰ã€‚</li>
<li>Lotusåœ¨é›¶æ ·æœ¬æ·±åº¦å’Œæ³•çº¿ä¼°è®¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18124">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2409.18124v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2409.18124v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2409.18124v5/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2409.18124v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EditBoard-Towards-a-Comprehensive-Evaluation-Benchmark-for-Text-Based-Video-Editing-Models"><a href="#EditBoard-Towards-a-Comprehensive-Evaluation-Benchmark-for-Text-Based-Video-Editing-Models" class="headerlink" title="EditBoard: Towards a Comprehensive Evaluation Benchmark for Text-Based   Video Editing Models"></a>EditBoard: Towards a Comprehensive Evaluation Benchmark for Text-Based   Video Editing Models</h2><p><strong>Authors:Yupeng Chen, Penglin Chen, Xiaoyu Zhang, Yixian Huang, Qian Xie</strong></p>
<p>The rapid development of diffusion models has significantly advanced AI-generated content (AIGC), particularly in Text-to-Image (T2I) and Text-to-Video (T2V) generation. Text-based video editing, leveraging these generative capabilities, has emerged as a promising field, enabling precise modifications to videos based on text prompts. Despite the proliferation of innovative video editing models, there is a conspicuous lack of comprehensive evaluation benchmarks that holistically assess these modelsâ€™ performance across various dimensions. Existing evaluations are limited and inconsistent, typically summarizing overall performance with a single score, which obscures modelsâ€™ effectiveness on individual editing tasks. To address this gap, we propose EditBoard, the first comprehensive evaluation benchmark for text-based video editing models. EditBoard encompasses nine automatic metrics across four dimensions, evaluating models on four task categories and introducing three new metrics to assess fidelity. This task-oriented benchmark facilitates objective evaluation by detailing model performance and providing insights into each modelâ€™s strengths and weaknesses. By open-sourcing EditBoard, we aim to standardize evaluation and advance the development of robust video editing models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•æå¤§åœ°æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰å’Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆæ–¹é¢ã€‚åŸºäºæ–‡æœ¬çš„è§†é¢‘ç¼–è¾‘ï¼Œåˆ©ç”¨è¿™äº›ç”Ÿæˆèƒ½åŠ›ï¼Œå·²ç»æˆä¸ºä¸€ä¸ªå……æ»¡å¸Œæœ›çš„é¢†åŸŸï¼Œå®ƒå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºå¯¹è§†é¢‘è¿›è¡Œç²¾ç¡®ä¿®æ”¹ã€‚å°½ç®¡åˆ›æ–°çš„è§†é¢‘ç¼–è¾‘æ¨¡å‹å±‚å‡ºä¸ç©·ï¼Œä½†ç¼ºä¹å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨å„ç§ç»´åº¦ä¸Šæ€§èƒ½çš„ç»¼åˆæ€§è¯„ä¼°åŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜æ˜¾çš„é—®é¢˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•æœ‰é™ä¸”ä¸ä¸€è‡´ï¼Œé€šå¸¸ä½¿ç”¨å•ä¸€åˆ†æ•°æ¥æ€»ç»“æ•´ä½“æ€§èƒ½ï¼Œè¿™æ©ç›–äº†æ¨¡å‹åœ¨å•ä¸ªç¼–è¾‘ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†EditBoardï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„è§†é¢‘ç¼–è¾‘æ¨¡å‹çš„ç»¼åˆæ€§è¯„ä¼°åŸºå‡†ã€‚EditBoardåŒ…å«å››ä¸ªç»´åº¦ä¸‹çš„ä¹ä¸ªè‡ªåŠ¨æŒ‡æ ‡ï¼Œå¯¹å››ä¸ªä»»åŠ¡ç±»åˆ«çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶å¼•å…¥ä¸‰ä¸ªæ–°æŒ‡æ ‡æ¥è¯„ä¼°ä¿çœŸåº¦ã€‚è¿™ä¸€ä»»åŠ¡å¯¼å‘çš„åŸºå‡†æœ‰åŠ©äºé€šè¿‡è¯¦ç»†äº†è§£æ¨¡å‹æ€§èƒ½å¹¶æä¾›æœ‰å…³æ¯ä¸ªæ¨¡å‹çš„ä¼˜ç‚¹å’Œå¼±ç‚¹çš„æ´å¯Ÿæ¥è¿›è¡Œå®¢è§‚è¯„ä¼°ã€‚æˆ‘ä»¬é€šè¿‡å¼€æºEditBoardï¼Œæ—¨åœ¨æ ‡å‡†åŒ–è¯„ä¼°å¹¶æ¨åŠ¨ç¨³å¥è§†é¢‘ç¼–è¾‘æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09668v2">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•æå¤§åœ°æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰å’Œæ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆé¢†åŸŸã€‚åŸºäºè¿™äº›ç”Ÿæˆèƒ½åŠ›çš„æ–‡æœ¬è§†é¢‘ç¼–è¾‘åº”è¿è€Œç”Ÿï¼Œå®ƒå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºå¯¹è§†é¢‘è¿›è¡Œç²¾ç¡®ä¿®æ”¹ã€‚å°½ç®¡æ¶Œç°äº†è®¸å¤šè§†é¢‘ç¼–è¾‘æ¨¡å‹ï¼Œä½†ç¼ºä¹å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°çš„ç»¼åˆè¯„ä»·åŸºå‡†ã€‚ç°æœ‰è¯„ä¼°æ–¹å¼æœ‰é™ä¸”ä¸ä¸€è‡´ï¼Œé€šå¸¸ä½¿ç”¨å•ä¸€åˆ†æ•°æ¥æ€»ç»“æ•´ä½“æ€§èƒ½ï¼Œè¿™æ©ç›–äº†æ¨¡å‹åœ¨ä¸ªåˆ«ç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ•ˆæœã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºEditBoardï¼Œé¦–ä¸ªé’ˆå¯¹æ–‡æœ¬è§†é¢‘ç¼–è¾‘æ¨¡å‹çš„å…¨é¢è¯„ä»·åŸºå‡†ã€‚EditBoardåŒ…å«å››ä¸ªç»´åº¦çš„ä¹ä¸ªè‡ªåŠ¨æŒ‡æ ‡ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å››ä¸ªä»»åŠ¡ç±»åˆ«ä¸Šçš„è¡¨ç°ï¼Œå¹¶å¼•å…¥ä¸‰ä¸ªæ–°æŒ‡æ ‡æ¥è¯„ä¼°ä¿çœŸåº¦ã€‚æ­¤ä»»åŠ¡å¯¼å‘çš„åŸºå‡†ä¾¿äºå®¢è§‚è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æä¾›æ¨¡å‹ä¼˜ç¼ºç‚¹æ´å¯Ÿã€‚é€šè¿‡å¼€æºEditBoardï¼Œæˆ‘ä»¬æ—¨åœ¨æ ‡å‡†åŒ–è¯„ä¼°å¹¶æ¨åŠ¨ç¨³å¥è§†é¢‘ç¼–è¾‘æ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åœ¨æ–‡æœ¬è½¬å›¾åƒå’Œæ–‡æœ¬è½¬è§†é¢‘é¢†åŸŸçš„æ˜¾è‘—å‘å±•ã€‚</li>
<li>æ–‡æœ¬è§†é¢‘ç¼–è¾‘æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºå¯¹è§†é¢‘è¿›è¡Œç²¾ç¡®ä¿®æ”¹ã€‚</li>
<li>å½“å‰ç¼ºä¹å…¨é¢è¯„ä¼°æ–‡æœ¬è§†é¢‘ç¼–è¾‘æ¨¡å‹çš„ç»¼åˆè¯„ä»·åŸºå‡†ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹å¼é€šå¸¸ä½¿ç”¨å•ä¸€åˆ†æ•°æ¥æ€»ç»“æ¨¡å‹æ•´ä½“æ€§èƒ½ï¼Œè¿™æ— æ³•åæ˜ æ¨¡å‹åœ¨ä¸ªåˆ«ç¼–è¾‘ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>EditBoardæ˜¯é¦–ä¸ªé’ˆå¯¹æ–‡æœ¬è§†é¢‘ç¼–è¾‘æ¨¡å‹çš„å…¨é¢è¯„ä»·åŸºå‡†ï¼ŒåŒ…å«ä¹ä¸ªè‡ªåŠ¨æŒ‡æ ‡ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å››ä¸ªä»»åŠ¡ç±»åˆ«ä¸Šçš„è¡¨ç°ã€‚</li>
<li>EditBoardå¼•å…¥äº†ä¸‰ä¸ªæ–°æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2409.09668v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2409.09668v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2409.09668v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2409.09668v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2409.09668v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Latent-Diffusion-for-Medical-Image-Segmentation-End-to-end-learning-for-fast-sampling-and-accuracy"><a href="#Latent-Diffusion-for-Medical-Image-Segmentation-End-to-end-learning-for-fast-sampling-and-accuracy" class="headerlink" title="Latent Diffusion for Medical Image Segmentation: End to end learning for   fast sampling and accuracy"></a>Latent Diffusion for Medical Image Segmentation: End to end learning for   fast sampling and accuracy</h2><p><strong>Authors:Fahim Ahmed Zaman, Mathews Jacob, Amanda Chang, Kan Liu, Milan Sonka, Xiaodong Wu</strong></p>
<p>Diffusion Probabilistic Models (DPMs) suffer from inefficient inference due to their slow sampling and high memory consumption, which limits their applicability to various medical imaging applications. In this work, we propose a novel conditional diffusion modeling framework (LDSeg) for medical image segmentation, utilizing the learned inherent low-dimensional latent shape manifolds of the target objects and the embeddings of the source image with an end-to-end framework. Conditional diffusion in latent space not only ensures accurate image segmentation for multiple interacting objects, but also tackles the fundamental issues of traditional DPM-based segmentation methods: (1) high memory consumption, (2) time-consuming sampling process, and (3) unnatural noise injection in the forward and reverse processes. The end-to-end training strategy enables robust representation learning in the latent space related to segmentation features, ensuring significantly faster sampling from the posterior distribution for segmentation generation in the inference phase. Our experiments demonstrate that LDSeg achieved state-of-the-art segmentation accuracy on three medical image datasets with different imaging modalities. In addition, we showed that our proposed model was significantly more robust to noise compared to traditional deterministic segmentation models. The code is available at <a target="_blank" rel="noopener" href="https://github.com/FahimZaman/LDSeg.git">https://github.com/FahimZaman/LDSeg.git</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMï¼‰ç”±äºç¼“æ…¢çš„é‡‡æ ·å’Œè¾ƒé«˜çš„å†…å­˜æ¶ˆè€—ï¼Œå¯¼è‡´å…¶æ¨ç†æ•ˆç‡ä½ä¸‹ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å„ç§åŒ»å­¦æˆåƒåº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°å‹æ¡ä»¶æ‰©æ•£å»ºæ¨¡æ¡†æ¶ï¼ˆLDSegï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç›®æ ‡å¯¹è±¡çš„å›ºæœ‰ä½ç»´æ½œåœ¨å½¢çŠ¶æµå½¢å’Œæºå›¾åƒçš„åµŒå…¥ï¼Œä»¥åŠç«¯åˆ°ç«¯çš„æ¡†æ¶è¿›è¡Œå­¦ä¹ ã€‚æ½œåœ¨ç©ºé—´ä¸­çš„æ¡ä»¶æ‰©æ•£ä¸ä»…ç¡®ä¿äº†å¤šä¸ªäº¤äº’å¯¹è±¡çš„ç²¾ç¡®å›¾åƒåˆ†å‰²ï¼Œè¿˜è§£å†³äº†åŸºäºä¼ ç»ŸDPMçš„åˆ†å‰²æ–¹æ³•çš„åŸºæœ¬é—®é¢˜ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰é«˜å†…å­˜æ¶ˆè€—ï¼Œï¼ˆ2ï¼‰è€—æ—¶çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä»¥åŠï¼ˆ3ï¼‰å‰å‘å’Œåå‘è¿‡ç¨‹ä¸­çš„ä¸è‡ªç„¶å™ªå£°æ³¨å…¥ã€‚ç«¯åˆ°ç«¯çš„è®­ç»ƒç­–ç•¥èƒ½å¤Ÿåœ¨ä¸åˆ†å‰²ç‰¹å¾ç›¸å…³çš„æ½œåœ¨ç©ºé—´ä¸­å®ç°ç¨³å¥çš„è¡¨ç¤ºå­¦ä¹ ï¼Œç¡®ä¿åœ¨æ¨ç†é˜¶æ®µä»åéªŒåˆ†å¸ƒä¸­è¿›è¡Œåˆ†å‰²ç”Ÿæˆæ—¶å®ç°æ›´å¿«çš„é‡‡æ ·ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLDSegåœ¨ä¸‰ç§ä¸åŒæˆåƒæ¨¡å¼çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ä¸ä¼ ç»Ÿçš„ç¡®å®šæ€§åˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹å¯¹å™ªå£°å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/FahimZaman/LDSeg.git%E3%80%82">https://github.com/FahimZaman/LDSeg.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12952v2">PDF</a> 10 pages, 10 figures, journal article</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„æ–°å‹æ¡ä»¶æ‰©æ•£å»ºæ¨¡æ¡†æ¶ï¼ˆLDSegï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç›®æ ‡å¯¹è±¡çš„å›ºæœ‰ä½ç»´æ½œåœ¨å½¢çŠ¶æµå½¢å’Œæºå›¾åƒçš„åµŒå…¥ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„æ¡†æ¶è¿›è¡Œè®­ç»ƒã€‚æ¡ä»¶æ‰©æ•£åœ¨æ½œåœ¨ç©ºé—´ä¸­ä¸ä»…ç¡®ä¿äº†å¤šäº¤äº’å¯¹è±¡çš„ç²¾ç¡®å›¾åƒåˆ†å‰²ï¼Œè¿˜è§£å†³äº†ä¼ ç»ŸDPMåˆ†å‰²æ–¹æ³•çš„åŸºæœ¬é—®é¢˜ï¼Œå¦‚é«˜å†…å­˜æ¶ˆè€—ã€è€—æ—¶çš„é‡‡æ ·è¿‡ç¨‹ä»¥åŠæ­£å‘å’Œåå‘è¿‡ç¨‹ä¸­çš„ä¸è‡ªç„¶å™ªå£°æ³¨å…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒLDSegåœ¨ä¸‰ä¸ªä¸åŒæˆåƒæ¨¡æ€çš„åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²ç²¾åº¦ï¼Œå¹¶ä¸”ä¸ä¼ ç»Ÿçš„ç¡®å®šæ€§åˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼Œå¯¹å™ªå£°å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDSegåˆ©ç”¨æ¡ä»¶æ‰©æ•£å»ºæ¨¡æ¡†æ¶è¿›è¡ŒåŒ»ç–—å›¾åƒåˆ†å‰²ï¼Œé’ˆå¯¹DPMsåœ¨åŒ»ç–—æˆåƒåº”ç”¨ä¸­çš„ä½æ•ˆæ¨ç†é—®é¢˜ã€‚</li>
<li>LDSegåˆ©ç”¨ç›®æ ‡å¯¹è±¡çš„ä½ç»´æ½œåœ¨å½¢çŠ¶æµå½¢å’Œæºå›¾åƒçš„åµŒå…¥è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚</li>
<li>æ¡ä»¶æ‰©æ•£åœ¨æ½œåœ¨ç©ºé—´ç¡®ä¿äº†å¤šäº¤äº’å¯¹è±¡çš„ç²¾ç¡®å›¾åƒåˆ†å‰²ã€‚</li>
<li>LDSegè§£å†³äº†ä¼ ç»ŸDPMåˆ†å‰²æ–¹æ³•çš„é«˜å†…å­˜æ¶ˆè€—ã€è€—æ—¶é‡‡æ ·è¿‡ç¨‹å’Œä¸è‡ªç„¶å™ªå£°æ³¨å…¥é—®é¢˜ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLDSegåœ¨åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>LDSegç›¸è¾ƒäºä¼ ç»Ÿç¡®å®šæ€§åˆ†å‰²æ¨¡å‹å¯¹å™ªå£°å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.12952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2407.12952v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2407.12952v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2407.12952v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2407.12952v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2407.12952v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2407.12952v2/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2407.12952v2/page_5_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2407.12952v2/page_5_3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Generative-Topological-Networks"><a href="#Generative-Topological-Networks" class="headerlink" title="Generative Topological Networks"></a>Generative Topological Networks</h2><p><strong>Authors:Alona Levy-Jurgenson, Zohar Yakhini</strong></p>
<p>Generative methods have recently seen significant improvements by generating in a lower-dimensional latent representation of the data. However, many of the generative methods applied in the latent space remain complex and difficult to train. Further, it is not entirely clear why transitioning to a lower-dimensional latent space can improve generative quality. In this work, we introduce a new and simple generative method grounded in topology theory â€“ Generative Topological Networks (GTNs) â€“ which also provides insights into why lower-dimensional latent-space representations might be better-suited for data generation. GTNs are simple to train â€“ they employ a standard supervised learning approach and do not suffer from common generative pitfalls such as mode collapse, posterior collapse or the need to pose constraints on the neural network architecture. We demonstrate the use of GTNs on several datasets, including MNIST, CelebA, CIFAR-10 and the Hands and Palm Images dataset by training GTNs on a lower-dimensional latent representation of the data. We show that GTNs can improve upon VAEs and that they are quick to converge, generating realistic samples in early epochs. Further, we use the topological considerations behind the development of GTNs to offer insights into why generative models may benefit from operating on a lower-dimensional latent space, highlighting the important link between the intrinsic dimension of the data and the dimension in which the data is generated. Particularly, we demonstrate that generating in high dimensional ambient spaces may be a contributing factor to out-of-distribution samples generated by diffusion models. We also highlight other topological properties that are important to consider when using and designing generative models. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/alonalj/GTN">https://github.com/alonalj/GTN</a> </p>
<blockquote>
<p>ç”Ÿæˆå¼æ–¹æ³•é€šè¿‡åœ¨æ•°æ®çš„ä½ç»´æ½œåœ¨è¡¨ç¤ºä¸­è¿›è¡Œç”Ÿæˆï¼Œæœ€è¿‘å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œè®¸å¤šåº”ç”¨äºæ½œåœ¨ç©ºé—´çš„ç”Ÿæˆæ–¹æ³•ä»ç„¶å¤æ‚ä¸”éš¾ä»¥è®­ç»ƒã€‚æ­¤å¤–ï¼Œè½¬å‘ä½ç»´æ½œåœ¨ç©ºé—´å¯ä»¥æé«˜ç”Ÿæˆè´¨é‡çš„åŸå› å°šä¸å®Œå…¨æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ‹“æ‰‘ç†è®ºçš„æ–°é¢–ç®€å•ç”Ÿæˆæ–¹æ³•â€”â€”ç”Ÿæˆæ‹“æ‰‘ç½‘ç»œï¼ˆGTNsï¼‰ï¼Œè¯¥æ–¹æ³•ä¹Ÿæä¾›äº†ä¸ºä»€ä¹ˆä½ç»´æ½œåœ¨ç©ºé—´è¡¨ç¤ºæ›´é€‚åˆæ•°æ®ç”Ÿæˆçš„è§è§£ã€‚GTNsè®­ç»ƒç®€å•â€”â€”å®ƒä»¬é‡‡ç”¨æ ‡å‡†çš„æœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä¸ä¼šé™·å…¥å¸¸è§çš„ç”Ÿæˆé™·é˜±ï¼Œå¦‚æ¨¡å¼å´©æºƒã€åéªŒå´©æºƒæˆ–å¯¹ç¥ç»ç½‘ç»œæ¶æ„æ–½åŠ çº¦æŸçš„éœ€è¦ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå±•ç¤ºäº†GTNsçš„ä½¿ç”¨ï¼ŒåŒ…æ‹¬MNISTã€CelebAã€CIFAR-10å’Œæ‰‹éƒ¨ä¸æ‰‹æŒå›¾åƒæ•°æ®é›†ï¼Œé€šè¿‡åœ¨æ•°æ®çš„ä½ç»´æ½œåœ¨è¡¨ç¤ºä¸Šè®­ç»ƒGTNsã€‚æˆ‘ä»¬å±•ç¤ºäº†GTNså¯ä»¥æ”¹è¿›VAEsï¼Œå¹¶ä¸”å®ƒä»¬å¯ä»¥å¿«é€Ÿæ”¶æ•›ï¼Œåœ¨æ—©æœŸå‘¨æœŸç”Ÿæˆé€¼çœŸçš„æ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨GTNså¼€å‘èƒŒåçš„æ‹“æ‰‘è€ƒè™‘ä¸ºä»€ä¹ˆç”Ÿæˆæ¨¡å‹å¯ä»¥ä»ä½ç»´æ½œåœ¨ç©ºé—´ä¸­å—ç›Šæä¾›è§è§£ï¼Œçªå‡ºäº†æ•°æ®å†…åœ¨ç»´åº¦å’Œç”Ÿæˆæ•°æ®çš„ç»´åº¦ä¹‹é—´çš„é‡è”ç³»ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜åœ¨é«˜ç»´ç¯å¢ƒç©ºé—´ä¸­ç”Ÿæˆå¯èƒ½æ˜¯æ‰©æ•£æ¨¡å‹äº§ç”Ÿåˆ†å¸ƒå¤–æ ·æœ¬çš„ä¸€ä¸ªå› ç´ ã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†åœ¨ä½¿ç”¨å’Œè®¾è®¡ç”Ÿæˆæ¨¡å‹æ—¶éœ€è¦è€ƒè™‘çš„å…¶ä»–é‡è¦æ‹“æ‰‘å±æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/alonalj/GTN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alonalj/GTNæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15152v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‹“æ‰‘ç†è®ºçš„æ–°å‹ç®€å•ç”Ÿæˆæ–¹æ³•â€”â€”ç”Ÿæˆæ‹“æ‰‘ç½‘ç»œï¼ˆGTNsï¼‰ï¼Œè¿™ç§æ–¹æ³•å¯¹äºåœ¨ä½ç»´æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ•°æ®ç”Ÿæˆæœ‰äº†æ·±å…¥ç†è§£ã€‚GTNsè®­ç»ƒç®€å•ï¼Œé‡‡ç”¨æ ‡å‡†ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé¿å…äº†å¸¸è§çš„ç”Ÿæˆé™·é˜±ï¼Œå¦‚æ¨¡å¼å´©æºƒã€åå´©æºƒæˆ–å¯¹ç¥ç»ç½‘ç»œæ¶æ„çš„çº¦æŸã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGTNsèƒ½å¤Ÿæ”¹è¿›å˜è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰ï¼Œå¿«é€Ÿæ”¶æ•›ï¼Œå¹¶åœ¨æ—©æœŸé˜¶æ®µç”Ÿæˆé€¼çœŸçš„æ ·æœ¬ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»æ‹“æ‰‘å­¦çš„è§’åº¦æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹ä»ä½ç»´æ½œåœ¨ç©ºé—´æ“ä½œä¸­çš„è·ç›ŠåŸå› ï¼Œå¼ºè°ƒäº†æ•°æ®å†…åœ¨ç»´åº¦å’Œç”Ÿæˆæ•°æ®çš„ç»´åº¦ä¹‹é—´çš„è”ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ç”Ÿæˆæ–¹æ³•â€”â€”ç”Ÿæˆæ‹“æ‰‘ç½‘ç»œï¼ˆGTNsï¼‰ï¼ŒåŸºäºæ‹“æ‰‘ç†è®ºï¼Œä¸ºæ•°æ®ç”Ÿæˆæä¾›äº†æ–°è§†è§’ã€‚</li>
<li>GTNsåœ¨ä½ç»´æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ•°æ®ç”Ÿæˆï¼Œç®€åŒ–äº†è®­ç»ƒå¤æ‚æ€§ã€‚</li>
<li>GTNsé‡‡ç”¨æ ‡å‡†ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé¿å…å¸¸è§çš„ç”Ÿæˆé™·é˜±ï¼Œå¦‚æ¨¡å¼å´©æºƒå’Œåå´©æºƒã€‚</li>
<li>GTNsåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæ”¹è¿›å˜è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰ï¼Œå¹¶å¿«é€Ÿæ”¶æ•›ã€‚</li>
<li>æ–‡ç« ä»æ‹“æ‰‘å­¦è§’åº¦æ·±å…¥æ¢è®¨äº†ä¸ºä»€ä¹ˆä½ç»´æ½œåœ¨ç©ºé—´è¡¨ç¤ºæ›´é€‚åˆæ•°æ®ç”Ÿæˆï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®çš„å†…åœ¨ç»´åº¦å’Œç”Ÿæˆç»´åº¦ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>ç”Ÿæˆé«˜ç»´ç¯å¢ƒä¸­çš„æ•°æ®å¯èƒ½å¯¼è‡´æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬åç¦»åˆ†å¸ƒã€‚</li>
<li>æ–‡ç« çš„æœ€åè¿˜å¼ºè°ƒäº†åœ¨ä½¿ç”¨å’Œè®¾è®¡ç”Ÿæˆæ¨¡å‹æ—¶éœ€è¦è€ƒè™‘çš„å…¶ä»–é‡è¦æ‹“æ‰‘å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.15152v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.15152v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.15152v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.15152v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.15152v3/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.15152v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.15152v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.15152v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.15152v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Denoising-Capability-of-Diffusion-Prior-for-Solving-Inverse-Problems"><a href="#Unleashing-the-Denoising-Capability-of-Diffusion-Prior-for-Solving-Inverse-Problems" class="headerlink" title="Unleashing the Denoising Capability of Diffusion Prior for Solving   Inverse Problems"></a>Unleashing the Denoising Capability of Diffusion Prior for Solving   Inverse Problems</h2><p><strong>Authors:Jiawei Zhang, Jiaxin Zhuang, Cheng Jin, Gen Li, Yuantao Gu</strong></p>
<p>The recent emergence of diffusion models has significantly advanced the precision of learnable priors, presenting innovative avenues for addressing inverse problems. Since inverse problems inherently entail maximum a posteriori estimation, previous works have endeavored to integrate diffusion priors into the optimization frameworks. However, prevailing optimization-based inverse algorithms primarily exploit the prior information within the diffusion models while neglecting their denoising capability. To bridge this gap, this work leverages the diffusion process to reframe noisy inverse problems as a two-variable constrained optimization task by introducing an auxiliary optimization variable. By employing gradient truncation, the projection gradient descent method is efficiently utilized to solve the corresponding optimization problem. The proposed algorithm, termed ProjDiff, effectively harnesses the prior information and the denoising capability of a pre-trained diffusion model within the optimization framework. Extensive experiments on the image restoration tasks and source separation and partial generation tasks demonstrate that ProjDiff exhibits superior performance across various linear and nonlinear inverse problems, highlighting its potential for practical applications. Code is available at <a target="_blank" rel="noopener" href="https://github.com/weigerzan/ProjDiff/">https://github.com/weigerzan/ProjDiff/</a>. </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹çš„å…´èµ·å¤§å¤§æé«˜äº†å¯å­¦ä¹ å…ˆéªŒçš„ç²¾åº¦ï¼Œä¸ºè§£å†³åé—®é¢˜æä¾›äº†æ–°çš„é€”å¾„ã€‚ç”±äºåé—®é¢˜æœ¬è´¨ä¸Šæ¶‰åŠæœ€å¤§åéªŒä¼°è®¡ï¼Œæ—©æœŸçš„ç ”ç©¶å·¥ä½œè¯•å›¾å°†æ‰©æ•£å…ˆéªŒæ•´åˆåˆ°ä¼˜åŒ–æ¡†æ¶ä¸­ã€‚ç„¶è€Œï¼Œå½“å‰æµè¡Œçš„åŸºäºä¼˜åŒ–çš„åç®—æ³•ä¸»è¦åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒä¿¡æ¯ï¼Œå´å¿½è§†äº†å…¶å»å™ªèƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£è¿‡ç¨‹ï¼Œé€šè¿‡å¼•å…¥è¾…åŠ©ä¼˜åŒ–å˜é‡ï¼Œå°†å™ªå£°åé—®é¢˜é‡æ–°æ„å»ºä¸ºä¸¤å˜é‡çº¦æŸä¼˜åŒ–ä»»åŠ¡ã€‚é€šè¿‡é‡‡ç”¨æ¢¯åº¦æˆªæ–­ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•è§£å†³ç›¸åº”çš„ä¼˜åŒ–é—®é¢˜ã€‚æ‰€æç®—æ³•ç§°ä¸ºProjDiffï¼Œå®ƒæœ‰æ•ˆåœ°åœ¨ä¼˜åŒ–æ¡†æ¶å†…åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯å’Œå»å™ªèƒ½åŠ›ã€‚åœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä»¥åŠæºåˆ†ç¦»å’Œéƒ¨åˆ†ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒProjDiffåœ¨å¤„ç†å„ç§çº¿æ€§å’Œéçº¿æ€§åé—®é¢˜æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/weigerzan/ProjDiff/]%E6%9F%A5%E9%98%85%E3%80%82">https://github.com/weigerzan/ProjDiff/]æŸ¥é˜…ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06959v2">PDF</a> Accepted by NeurIPS 2024</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹çš„æœ€æ–°å‡ºç°å¤§å¤§æé«˜äº†å…ˆéªŒçŸ¥è¯†çš„ç²¾åº¦ï¼Œä¸ºè§£å†³é€†é—®é¢˜æä¾›äº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºä¼˜åŒ–çš„é€†ç®—æ³•ä¸»è¦åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒä¿¡æ¯ï¼Œè€Œå¿½è§†äº†å…¶å»å™ªèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶é€šè¿‡å¼•å…¥è¾…åŠ©ä¼˜åŒ–å˜é‡ï¼Œå°†å¸¦å™ªå£°çš„é€†é—®é¢˜é‡æ–°æ„å»ºä¸ºåŒå˜é‡çº¦æŸä¼˜åŒ–ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨æ¢¯åº¦æˆªæ–­æ³•æœ‰æ•ˆåœ°åˆ©ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•æ¥è§£å†³ç›¸åº”çš„ä¼˜åŒ–é—®é¢˜ã€‚æ‰€æç®—æ³•ProjDiffæœ‰æ•ˆåœ°åœ¨ä¼˜åŒ–æ¡†æ¶å†…ç»“åˆäº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯å’Œå»å™ªèƒ½åŠ›ã€‚åœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä»¥åŠæºåˆ†ç¦»å’Œéƒ¨åˆ†ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒProjDiffåœ¨è§£å†³å„ç§çº¿æ€§å’Œéçº¿æ€§é€†é—®é¢˜ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡¸æ˜¾å…¶åœ¨å®è·µåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æœ€æ–°è¿›å±•æé«˜äº†å…ˆéªŒçŸ¥è¯†çš„ç²¾åº¦ï¼Œä¸ºè§£å†³é€†é—®é¢˜æä¾›æ–°é€”å¾„ã€‚</li>
<li>ç°æœ‰ä¼˜åŒ–é€†ç®—æ³•ä¸»è¦å…³æ³¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯ï¼Œä½†å¿½è§†äº†å…¶å»å™ªèƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥è¾…åŠ©ä¼˜åŒ–å˜é‡ï¼Œå°†å™ªå£°é€†é—®é¢˜è½¬åŒ–ä¸ºåŒå˜é‡çº¦æŸä¼˜åŒ–ä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨æ¢¯åº¦æˆªæ–­æ³•ï¼Œæœ‰æ•ˆè¿ç”¨æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•è§£å†³ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>æ‰€æç®—æ³•ProjDiffç»“åˆæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯å’Œå»å™ªèƒ½åŠ›ï¼Œåœ¨ä¼˜åŒ–æ¡†æ¶å†…å®ç°æœ‰æ•ˆåˆ©ç”¨ã€‚</li>
<li>åœ¨å›¾åƒæ¢å¤ã€æºåˆ†ç¦»å’Œéƒ¨åˆ†ç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ProjDiffåœ¨è§£å†³çº¿æ€§å’Œéçº¿æ€§é€†é—®é¢˜ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.06959v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2406.06959v2/page_1_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LiteVAE-Lightweight-and-Efficient-Variational-Autoencoders-for-Latent-Diffusion-Models"><a href="#LiteVAE-Lightweight-and-Efficient-Variational-Autoencoders-for-Latent-Diffusion-Models" class="headerlink" title="LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent   Diffusion Models"></a>LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent   Diffusion Models</h2><p><strong>Authors:Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber</strong></p>
<p>Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored. In this paper, we introduce LiteVAE, a new autoencoder design for LDMs, which leverages the 2D discrete wavelet transform to enhance scalability and computational efficiency over standard variational autoencoders (VAEs) with no sacrifice in output quality. We investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality. Our base LiteVAE model matches the quality of the established VAEs in current LDMs with a six-fold reduction in encoder parameters, leading to faster training and lower GPU memory requirements, while our larger model outperforms VAEs of comparable complexity across all evaluated metrics (rFID, LPIPS, PSNR, and SSIM). </p>
<blockquote>
<p>æ½œæ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„è¿›æ­¥ä¸ºé«˜è´¨é‡å›¾åƒç”Ÿæˆå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ï¼Œä½†è¿™äº›ç³»ç»Ÿçš„æ ¸å¿ƒè‡ªç¼–ç å™¨çš„è®¾è®¡ç©ºé—´ä»é²œæœ‰ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç”¨äºLDMçš„æ–°å‹è‡ªç¼–ç å™¨è®¾è®¡LiteVAEã€‚å®ƒåˆ©ç”¨äºŒç»´ç¦»æ•£å°æ³¢å˜æ¢æ¥æé«˜å¯æ‰©å±•æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œä¸ä¼ ç»Ÿçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ç›¸æ¯”ï¼Œåœ¨è¾“å‡ºè´¨é‡ä¸Šæ²¡æœ‰ä»»ä½•æŸå¤±ã€‚æˆ‘ä»¬ç ”ç©¶äº†LiteVAEçš„è®­ç»ƒæ–¹æ³•å’Œè§£ç å™¨æ¶æ„ï¼Œå¹¶æå‡ºäº†å‡ é¡¹æ”¹è¿›ï¼Œæé«˜äº†è®­ç»ƒåŠ¨åŠ›å’Œé‡å»ºè´¨é‡ã€‚æˆ‘ä»¬çš„åŸºç¡€LiteVAEæ¨¡å‹ä¸å½“å‰LDMä¸­çš„æ—¢å®šVAEè´¨é‡ç›¸å½“ï¼Œç¼–ç å™¨å‚æ•°å‡å°‘äº†å…­å€ï¼Œå¯¼è‡´è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼ŒGPUå†…å­˜è¦æ±‚æ›´ä½ï¼Œè€Œæˆ‘ä»¬çš„æ›´å¤§æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ï¼ˆrFIDã€LPIPSã€PSNRå’ŒSSIMï¼‰ä¸Šéƒ½ä¼˜äºåŒç­‰å¤æ‚åº¦çš„VAEã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14477v2">PDF</a> Published as a conference paper at NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LiteVAEï¼Œä¸€ç§é’ˆå¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰çš„æ–°å‹è‡ªåŠ¨ç¼–ç å™¨è®¾è®¡ã€‚å®ƒåˆ©ç”¨äºŒç»´ç¦»æ•£å°æ³¢å˜æ¢æé«˜å¯æ‰©å±•æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œåœ¨è¾“å‡ºè´¨é‡ä¸Šæ— æŸå¤±ã€‚é€šè¿‡æ”¹è¿›è®­ç»ƒæ–¹æ³•å’Œè§£ç å™¨æ¶æ„ï¼ŒLiteVAEåœ¨å‡å°‘ç¼–ç å™¨å‚æ•°çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†å½“å‰LDMsä¸­VAEsçš„è´¨é‡æ°´å¹³ï¼Œå®ç°äº†æ›´å¿«çš„è®­ç»ƒå’Œæ›´ä½çš„GPUå†…å­˜è¦æ±‚ã€‚åŒæ—¶ï¼Œå…¶æ›´å¤§æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºåŒç­‰å¤æ‚åº¦çš„VAEsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LiteVAEæ˜¯ä¸€ç§æ–°å‹çš„è‡ªåŠ¨ç¼–ç å™¨è®¾è®¡ï¼Œä¸“é—¨ç”¨äºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ã€‚</li>
<li>LiteVAEåˆ©ç”¨äºŒç»´ç¦»æ•£å°æ³¢å˜æ¢æé«˜å¯æ‰©å±•æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>LiteVAEåœ¨è¾“å‡ºè´¨é‡æ–¹é¢æ²¡æœ‰æŸå¤±ã€‚</li>
<li>LiteVAEæ”¹è¿›äº†è®­ç»ƒæ–¹æ³•å’Œè§£ç å™¨æ¶æ„ã€‚</li>
<li>LiteVAEçš„åŸºç¡€æ¨¡å‹åœ¨å‡å°‘ç¼–ç å™¨å‚æ•°çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†å½“å‰LDMsä¸­VAEsçš„è´¨é‡æ°´å¹³ã€‚</li>
<li>LiteVAEå®ç°äº†æ›´å¿«çš„è®­ç»ƒå’Œæ›´ä½çš„GPUå†…å­˜è¦æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14477">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2405.14477v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2405.14477v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2405.14477v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2405.14477v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Diversify-Donâ€™t-Fine-Tune-Scaling-Up-Visual-Recognition-Training-with-Synthetic-Images"><a href="#Diversify-Donâ€™t-Fine-Tune-Scaling-Up-Visual-Recognition-Training-with-Synthetic-Images" class="headerlink" title="Diversify, Donâ€™t Fine-Tune: Scaling Up Visual Recognition Training with   Synthetic Images"></a>Diversify, Donâ€™t Fine-Tune: Scaling Up Visual Recognition Training with   Synthetic Images</h2><p><strong>Authors:Zhuoran Yu, Chenchen Zhu, Sean Culatana, Raghuraman Krishnamoorthi, Fanyi Xiao, Yong Jae Lee</strong></p>
<p>Recent advances in generative deep learning have enabled the creation of high-quality synthetic images in text-to-image generation. Prior work shows that fine-tuning a pretrained diffusion model on ImageNet and generating synthetic training images from the finetuned model can enhance an ImageNet classifierâ€™s performance. However, performance degrades as synthetic images outnumber real ones. In this paper, we explore whether generative fine-tuning is essential for this improvement and whether it is possible to further scale up training using more synthetic data. We present a new framework leveraging off-the-shelf generative models to generate synthetic training images, addressing multiple challenges: class name ambiguity, lack of diversity in naive prompts, and domain shifts. Specifically, we leverage large language models (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we propose contextualized diversification (CD) and stylized diversification (SD) methods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage domain adaptation techniques with auxiliary batch normalization for synthetic images. Our framework consistently enhances recognition model performance with more synthetic data, up to 6x of original ImageNet size showcasing the potential of synthetic data for improved recognition models and strong out-of-domain generalization. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆæ·±åº¦å­¦ä¹ é¢†åŸŸçš„è¿›å±•ä½¿å¾—æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­åˆ›å»ºé«˜è´¨é‡åˆæˆå›¾åƒæˆä¸ºå¯èƒ½ã€‚å…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼Œå¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶åœ¨ImageNetä¸Šç”Ÿæˆåˆæˆè®­ç»ƒå›¾åƒï¼Œå¯ä»¥æé«˜ImageNetåˆ†ç±»å™¨çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€åˆæˆå›¾åƒæ•°é‡è¶…è¿‡çœŸå®å›¾åƒï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ç”Ÿæˆå¾®è°ƒæ˜¯å¦å¯¹æ­¤æ”¹è¿›è‡³å…³é‡è¦ï¼Œä»¥åŠæ˜¯å¦å¯ä»¥ä½¿ç”¨æ›´å¤šçš„åˆæˆæ•°æ®è¿›ä¸€æ­¥æ‰©å±•è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨ç°æˆçš„ç”Ÿæˆæ¨¡å‹æ¥ç”Ÿæˆåˆæˆè®­ç»ƒå›¾åƒï¼Œè§£å†³å¤šä¸ªæŒ‘æˆ˜ï¼šç±»åæ¨¡ç³Šã€æç¤ºç¼ºä¹å¤šæ ·æ€§å’Œé¢†åŸŸåç§»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒCLIPæ¥è§£å†³ç±»åæ¨¡ç³Šé—®é¢˜ã€‚ä¸ºäº†å¤šæ ·åŒ–å›¾åƒï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸Šä¸‹æ–‡å’Œé£æ ¼çš„å¤šæ ·åŒ–æ–¹æ³•ï¼ˆCDå’ŒSDï¼‰ï¼Œä¹Ÿæ˜¯ç”±LLMé©±åŠ¨çš„ã€‚æœ€åï¼Œä¸ºäº†ç¼“è§£é¢†åŸŸåç§»é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å¸¦æœ‰è¾…åŠ©æ‰¹å½’ä¸€åŒ–çš„é¢†åŸŸé€‚åº”æŠ€æœ¯æ¥å¤„ç†åˆæˆå›¾åƒã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨æ›´å¤šçš„åˆæˆæ•°æ®å§‹ç»ˆæé«˜äº†è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ï¼Œè¾¾åˆ°åŸå§‹ImageNetè§„æ¨¡çš„6å€ï¼Œå±•ç¤ºäº†åˆæˆæ•°æ®åœ¨æ”¹è¿›è¯†åˆ«æ¨¡å‹å’Œå¼ºå¤§åŸŸå¤–æ³›åŒ–æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02253v2">PDF</a> Accepted by Transactions on Machine Learning Research (TMLR)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢ç´¢äº†ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒå¯¹äºå¢å¼ºå›¾åƒåˆ†ç±»å™¨æ€§èƒ½çš„ä½œç”¨ã€‚æ–‡ç« æå‡ºä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨ç°æˆçš„ç”Ÿæˆæ¨¡å‹ç”Ÿæˆåˆæˆè®­ç»ƒå›¾åƒï¼Œè§£å†³ç±»åˆ«åç§°æ¨¡ç³Šã€æç¤ºç¼ºä¹å¤šæ ·æ€§å’Œé¢†åŸŸåç§»ç­‰é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’ŒCLIPè§£å†³ç±»åˆ«åç§°æ¨¡ç³Šé—®é¢˜ï¼Œå¹¶æå‡ºåŸºäºä¸Šä¸‹æ–‡å’Œé£æ ¼çš„å¤šæ ·åŒ–æ–¹æ³•æ¥å¢åŠ å›¾åƒå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡è¾…åŠ©æ‰¹å½’ä¸€åŒ–æŠ€æœ¯å‡è½»é¢†åŸŸåç§»é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨å¤§é‡åˆæˆæ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½æŒç»­æå‡è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºåˆæˆæ•°æ®å¯¹æ”¹è¿›è¯†åˆ«æ¨¡å‹å’Œå¼ºå¤§åŸŸå¤–æ³›åŒ–çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•ä½¿å¾—åˆ›å»ºé«˜è´¨é‡åˆæˆå›¾åƒæˆä¸ºå¯èƒ½ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶ç”Ÿæˆåˆæˆè®­ç»ƒå›¾åƒå¯ä»¥å¢å¼ºImageNetåˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>å½“åˆæˆå›¾åƒæ•°é‡è¶…è¿‡çœŸå®å›¾åƒæ—¶ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚</li>
<li>æå‡ºçš„æ–°æ¡†æ¶è§£å†³äº†ç±»åˆ«åç§°æ¨¡ç³Šã€æç¤ºç¼ºä¹å¤šæ ·æ€§å’Œé¢†åŸŸåç§»ç­‰æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’ŒCLIPè§£å†³ç±»åˆ«åç§°æ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>æå‡ºåŸºäºä¸Šä¸‹æ–‡å’Œé£æ ¼çš„å¤šæ ·åŒ–æ–¹æ³•æ¥å¢åŠ å›¾åƒå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2312.02253v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2312.02253v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2312.02253v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Diffusion Models/2312.02253v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c5d4c0037d25961b8dab93d73f22f7c1.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  Vision-Language Models for Automated Chest X-ray Interpretation   Leveraging ViT and GPT-2
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e9119d160fc76e800ef9d7ecf8b48cd5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  Fast Underwater Scene Reconstruction using Multi-View Stereo and   Physical Imaging
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19017.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
