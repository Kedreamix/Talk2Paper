<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  CBVLM Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2409.11111v2/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    73 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-23-æ›´æ–°"><a href="#2025-01-23-æ›´æ–°" class="headerlink" title="2025-01-23 æ›´æ–°"></a>2025-01-23 æ›´æ–°</h1><h2 id="CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification"><a href="#CBVLM-Training-free-Explainable-Concept-based-Large-Vision-Language-Models-for-Medical-Image-Classification" class="headerlink" title="CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification"></a>CBVLM: Training-free Explainable Concept-based Large Vision Language   Models for Medical Image Classification</h2><p><strong>Authors:Cristiano PatrÃ­cio, Isabel Rio-Torto, Jaime S. Cardoso, LuÃ­s F. Teixeira, JoÃ£o C. Neves</strong></p>
<p>The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the final disease prediction on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: <a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/">https://cristianopatricio.github.io/CBVLM/</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å·¥ä½œæµä¸­é‡‡ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆçš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ ‡æ³¨æ•°æ®çš„ä¸æ˜“è·å–ä»¥åŠè¿™äº›ç³»ç»Ÿç¼ºä¹å¯è§£é‡Šæ€§ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡é™åˆ¶åœ¨é¢„å®šä¹‰å’Œå¯è§£é‡Šçš„æ¦‚å¿µé›†ä¸Šçš„æœ€ç»ˆç–¾ç—…é¢„æµ‹æ¥è§£å†³åè€…çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œé€šè¿‡åŸºäºæ¦‚å¿µçš„è§£é‡Šæ‰€æé«˜çš„å¯è§£é‡Šæ€§æ„å‘³ç€æ›´é«˜çš„æ ‡æ³¨è´Ÿæ‹…ã€‚è€Œä¸”ï¼Œå¦‚æœè¦æ·»åŠ æ–°æ¦‚å¿µï¼Œæ•´ä¸ªç³»ç»Ÿéƒ½éœ€è¦é‡æ–°è®­ç»ƒã€‚å—å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸­çš„å‡ºè‰²è¡¨ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå³CBVLMï¼Œå¯ä»¥è§£å†³ä¸Šè¿°ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå¯¹äºæ¯ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬æç¤ºLVLMå›ç­”æ¦‚å¿µæ˜¯å¦å‡ºç°åœ¨è¾“å…¥å›¾åƒä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬è¦æ±‚LVLMåŸºäºå…ˆå‰çš„æ¦‚å¿µé¢„æµ‹å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚æ­¤å¤–ï¼Œåœ¨ä¸¤ä¸ªé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬éƒ½èå…¥äº†ä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼Œè´Ÿè´£é€‰æ‹©æœ€ä½³æ ·æœ¬è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚é€šè¿‡å°†æœ€ç»ˆè¯Šæ–­åŸºäºé¢„æµ‹çš„æ¦‚å¿µï¼Œæˆ‘ä»¬ç¡®ä¿äº†å¯è§£é‡Šæ€§ï¼Œå¹¶é€šè¿‡åˆ©ç”¨LVLMsçš„å°‘æ ·æœ¬èƒ½åŠ›ï¼Œæˆ‘ä»¬å¤§å¤§é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚æˆ‘ä»¬åœ¨å››ä¸ªåŒ»å­¦æ•°æ®é›†å’ŒåäºŒä¸ªï¼ˆé€šç”¨å’ŒåŒ»å­¦ï¼‰LVLMä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜CBVLMåœ¨ä¸éœ€è¦ä»»ä½•è®­ç»ƒå’Œä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºCBMså’Œç‰¹å®šä»»åŠ¡ç›‘ç£æ–¹æ³•ã€‚æ›´å¤šå…³äºæˆ‘ä»¬é¡¹ç›®çš„ä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://cristianopatricio.github.io/CBVLM/%E3%80%82">https://cristianopatricio.github.io/CBVLM/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12266v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å·¥ä½œæµç¨‹åº”ç”¨ä¸­çš„ä¸»è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§çš„ç¼ºå¤±ã€‚æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡çº¦æŸç–¾ç—…é¢„æµ‹åœ¨ä¸€ç»„é¢„å…ˆå®šä¹‰å’Œå¯è§£é‡Šçš„æ¦‚å¿µä¸Šæ¥è§£å†³åè€…çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œæ¦‚å¿µæ€§è§£é‡Šå¸¦æ¥çš„è§£é‡Šæ€§çš„æå‡æ„å‘³ç€æ›´é«˜çš„æ ‡æ³¨è´Ÿæ‹…ã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ä¸ªæ–°çš„æ–¹æ³•CBVLMæ¥è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸‹çš„å‡ºè‰²æ€§èƒ½ï¼Œé€šè¿‡é¢„æµ‹æ¦‚å¿µå¹¶åŸºäºè¿™äº›æ¦‚å¿µå¯¹å›¾åƒè¿›è¡Œåˆ†ç±»æ¥å®ç°åŒ»ç–—è¯Šæ–­ã€‚é€šè¿‡é€‰æ‹©æœ€ä½³çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¾‹å­ï¼ŒCBVLMç¡®ä¿äº†è¯Šæ–­çš„è§£é‡Šæ€§å¹¶é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚å®éªŒè¯æ˜ï¼ŒCBVLMåœ¨å››ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºCBMså’Œç‰¹å®šä»»åŠ¡ç›‘ç£æ–¹æ³•ï¼Œä¸”æ— éœ€ä»»ä½•è®­ç»ƒï¼Œåªéœ€ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å³å¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å·¥ä½œæµä¸­é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§å’Œç³»ç»Ÿè§£é‡Šæ€§çš„ç¼ºå¤±ã€‚</li>
<li>æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆCBMsï¼‰é€šè¿‡çº¦æŸç–¾ç—…é¢„æµ‹åœ¨ä¸€ç»„é¢„å…ˆå®šä¹‰å’Œå¯è§£é‡Šçš„æ¦‚å¿µä¸Šè§£å†³äº†è§£é‡Šæ€§é—®é¢˜ï¼Œä½†å¸¦æ¥äº†æ›´é«˜çš„æ ‡æ³¨è´Ÿæ‹…å’Œéœ€è¦é‡å¤è®­ç»ƒçš„é—®é¢˜ã€‚</li>
<li>CBVLMæ–¹æ³•ç»“åˆäº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„ä¼˜å¼‚æ€§èƒ½å’Œæ¦‚å¿µé¢„æµ‹æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>CBVLMé€šè¿‡åˆ©ç”¨LVLMsçš„å°‘é‡æ ·æœ¬èƒ½åŠ›æ¥é™ä½æ ‡æ³¨æˆæœ¬å¹¶å®ç°åŒ»ç–—è¯Šæ–­ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCBVLMåœ¨å¤šä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºCBMså’Œå…¶ä»–ç‰¹å®šä»»åŠ¡ç›‘ç£æ–¹æ³•ã€‚</li>
<li>CBVLMæ— éœ€é¢å¤–è®­ç»ƒï¼Œå¹¶èƒ½åˆ©ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬è¿›è¡Œé«˜æ•ˆé¢„æµ‹å’Œåˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-206021a739805f4c025fbbbda977fff6.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.12266v1/page_2_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0abc5ca96e375c7cac4c0ac188efd080.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0aea3215822b78c86eba3f65abedf78.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes"><a href="#Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes" class="headerlink" title="Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes"></a>Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes</h2><p><strong>Authors:Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer</strong></p>
<p>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctorsâ€™ notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from <a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval">https://github.com/stefan-m-lenz/UroLlmEval</a>. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP. </p>
<blockquote>
<p>åœ¨å¾·å›½ï¼Œè‚¿ç˜¤è®°å½•å·¥ä½œå¤§å¤šä»¥æ‰‹åŠ¨æ–¹å¼è¿›è¡Œï¼Œéœ€è¦é˜…è¯»æ‚£è€…ç—…å†å¹¶å°†æ•°æ®å½•å…¥ç»“æ„åŒ–æ•°æ®åº“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰æ½œåŠ›é€šè¿‡æé«˜æ•ˆç‡å’Œå¯é æ€§æ¥å¢å¼ºè¿™ä¸€æµç¨‹ã€‚æœ¬æ¬¡è¯„ä¼°å¯¹ä¸‰ç§è‚¿ç˜¤è®°å½•åŸºæœ¬ä»»åŠ¡ï¼ˆè¯†åˆ«è‚¿ç˜¤è¯Šæ–­ã€åˆ†é…ICD-10ä»£ç å’Œæå–é¦–æ¬¡è¯Šæ–­æ—¥æœŸï¼‰ä¸­ï¼Œä½¿ç”¨äº†11ç§ä¸åŒå¼€æºçš„LLMï¼Œè¿™äº›æ¨¡å‹å‚æ•°è§„æ¨¡ä»1äº¿åˆ°70äº¿ä¸ç­‰ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬å‡†å¤‡äº†ä¸€ä»½åŸºäºæ³Œå°¿ç§‘åŒ¿ååŒ»ç”Ÿç¬”è®°çš„æ ‡æ³¨æ–‡æœ¬ç‰‡æ®µæ•°æ®é›†ã€‚ä½¿ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥æ¥ç ”ç©¶å°‘é‡ç¤ºä¾‹æç¤ºä¸­ç¤ºä¾‹æ•°é‡å¯¹æ¨¡å‹çš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMçš„ä¸€èˆ¬èƒ½åŠ›ã€‚Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚å…·æœ‰è¾ƒå°‘è®­ç»ƒæ•°æ®æˆ–å‚æ•°å°‘äº7äº¿çš„æ¨¡å‹è¡¨ç°æ˜æ˜¾è¾ƒå·®ï¼Œè€Œæ›´å¤§çš„æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºæ€§èƒ½æå‡ã€‚æ¥è‡ªæ³Œå°¿ç§‘ä»¥å¤–çš„å…¶ä»–åŒ»å­¦é¢†åŸŸçš„ä¾‹å­ä¹Ÿèƒ½åœ¨å°‘é‡æç¤ºä¸­æ”¹å–„ç»“æœï¼Œè¿™è¯æ˜äº†LLMå¤„ç†è‚¿ç˜¤è®°å½•æ‰€éœ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¼€æºLLMåœ¨è‡ªåŠ¨åŒ–è‚¿ç˜¤è®°å½•æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚å‚æ•°è§„æ¨¡åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå’Œç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œè¿™äº›æ¨¡å‹å¯èƒ½ä¼šæˆä¸ºæœªæ¥ä¸´åºŠè®°å½•çš„é‡è¦å·¥å…·ã€‚è¯„ä¼°çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%85%AC%E5%BC%80%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E8%A7%A3%E5%86%B3%E5%BE%B7%E8%AF%AD%E5%8C%BB%E5%AD%A6NLP%E9%A2%86%E5%9F%9F%E4%B8%AD%E7%9C%9F%E5%AE%9E%E3%80%81%E6%98%93%E4%BA%8E%E8%AE%BF%E9%97%AE%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%9F%AD%E7%BC%BA%E9%97%AE%E9%A2%98%E7%9A%84%E6%96%B0%E6%9C%89%E4%BB%B7%E5%80%BC%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/stefan-m-lenz/UroLlmEvalè·å–ã€‚æˆ‘ä»¬è¿˜å…¬å¼€äº†æ•°æ®é›†ï¼Œä½œä¸ºè§£å†³å¾·è¯­åŒ»å­¦NLPé¢†åŸŸä¸­çœŸå®ã€æ˜“äºè®¿é—®çš„åŸºå‡†æµ‹è¯•çŸ­ç¼ºé—®é¢˜çš„æ–°æœ‰ä»·å€¼èµ„æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12106v1">PDF</a> 48 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åœ¨è‚¿ç˜¤è®°å½•è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿæé«˜æ•ˆç‡å’Œå¯é æ€§ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸åŒè§„æ¨¡çš„LLMsåœ¨ä¸‰é¡¹åŸºæœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ã€ICD-10ä»£ç åˆ†é…ä»¥åŠé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ã€‚ç ”ç©¶ä¸­å…¬å¼€çš„ä»£ç å’Œæ•°æ®é›†å¯ä¾›ç ”ç©¶ä¹‹ç”¨ã€‚ç ”ç©¶å‘ç°è§„æ¨¡ä¸º7è‡³12äº¿å‚æ•°çš„æ¨¡å‹åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ï¼Œæœ‰æœ›åœ¨æœªæ¥æˆä¸ºä¸´åºŠæ–‡æ¡£çš„é‡è¦å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åœ¨å¾·å›½çš„è‚¿ç˜¤è®°å½•è¿‡ç¨‹ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿæé«˜æ•ˆç‡å’Œå¯é æ€§ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸åŒè§„æ¨¡çš„LLMsåœ¨ä¸‰é¡¹åŸºæœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šè‚¿ç˜¤è¯Šæ–­è¯†åˆ«ã€ICD-10ä»£ç åˆ†é…å’Œé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ã€‚</li>
<li>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼ŒLlama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹åœ¨ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡è¾ƒå°çš„LLMsæ€§èƒ½è¾ƒä½ï¼Œè€Œæ›´å¤§çš„æ¨¡å‹å¹¶æ²¡æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚</li>
<li>ä¸åŒåŒ»å­¦é¢†åŸŸçš„ç¤ºä¾‹æ•°æ®èƒ½å¤Ÿæé«˜å°‘æ ·æœ¬æç¤ºçš„æ•ˆæœï¼Œè¯æ˜LLMsæœ‰èƒ½åŠ›å¤„ç†è‚¿ç˜¤è®°å½•æ‰€éœ€çš„ä»»åŠ¡ã€‚</li>
<li>å…¬å¼€çš„ä»£ç å’Œæ•°æ®é›†æœ‰åŠ©äºè§£å†³å¾·å›½åŒ»ç–—NLPé¢†åŸŸä¸­çœŸå®å’Œå¯è®¿é—®çš„åŸºå‡†æµ‹è¯•æ•°æ®çŸ­ç¼ºçš„é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-764cd5a8d839a45da36d211b0c673397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fbd2fce63b2cad7e3b967780aa11e83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b245a6522b0d161dce0f49b1bb1190c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Directional-Diffusion-Style-Code-Editing-Pre-training"><a href="#Directional-Diffusion-Style-Code-Editing-Pre-training" class="headerlink" title="Directional Diffusion-Style Code Editing Pre-training"></a>Directional Diffusion-Style Code Editing Pre-training</h2><p><strong>Authors:Qingyuan Liang, Zeyu Sun, Qihao Zhu, Junhao Hu, Yifan Zhao, Yizhou Chen, Mingxuan Zhu, Guoqing Wang, Lu Zhang</strong></p>
<p>Code pre-trained models have shown promising effectiveness in various software engineering tasks. Among these tasks, many tasks are related to software evolution and&#x2F;or code editing. However, existing code pre-trained models often overlook the real-world code editing data and the evolutionary nature of the editing process. In this paper, to simulate the step-by-step code editing process of human developers, we propose DivoT5, a pre-trained model based on directional diffusion at the data level. In DivoT5, we adopt two categories of pre-training tasks. The first category is mask and denoising tasks augmented with a diffusion direction representing code evolution. That is, we first apply a noising process to the code snippets before evolution, and then ask the pre-training process to restore the snippets with noise into the code snippets after evolution. The second category is tasks aiming to reinforce the evolutionary direction. That is, we first generate various intermediate versions for each pair of snippets before and after evolution, and then ask the pre-training process to transform the intermediate versions into the snippet after evolution for each pair. We evaluate DivoT5 for two code-editing scenarios and one non-editing scenario using five downstream tasks. Given each downstream task, we fine-tune the pre-trained DivoT5 to evaluate its effectiveness. Our experimental results show that DivoT5 achieves state-of-the-art (SOTA) performance on most tasks in comparison to models of the same scale (220M), large scale (770M) models in fine-tuning, and billion-scale (6.7B, 8B, ChatGPT) models in few-shot settings. For one code-editing task (i.e., automated code review), DivoT5 pre-trained on top of CodeT5-small (60M) can even outperform CodeT5-base (220M) and other pre-trained models with 220M parameters except for DivoT5 pre-trained on top of CodeT5-base (220M). </p>
<blockquote>
<p>é¢„è®­ç»ƒä»£ç æ¨¡å‹åœ¨å„ç§è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†è‰¯å¥½çš„æœ‰æ•ˆæ€§ã€‚å…¶ä¸­è®¸å¤šä»»åŠ¡ä¸è½¯ä»¶æ¼”è¿›å’Œ&#x2F;æˆ–ä»£ç ç¼–è¾‘ç›¸å…³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç é¢„è®­ç»ƒæ¨¡å‹å¾€å¾€å¿½ç•¥äº†ç°å®ä¸–ç•Œçš„ä»£ç ç¼–è¾‘æ•°æ®å’Œç¼–è¾‘è¿‡ç¨‹çš„æ¼”åŒ–æ€§è´¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä¸ºäº†æ¨¡æ‹Ÿäººç±»å¼€å‘è€…é€æ­¥çš„ä»£ç ç¼–è¾‘è¿‡ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ•°æ®çº§åˆ«æ–¹å‘æ‰©æ•£çš„é¢„è®­ç»ƒæ¨¡å‹DivoT5ã€‚åœ¨DivoT5ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ç±»é¢„è®­ç»ƒä»»åŠ¡ã€‚ç¬¬ä¸€ç±»æ˜¯é€šè¿‡å¸¦æœ‰ä»£è¡¨ä»£ç æ¼”åŒ–çš„æ‰©æ•£æ–¹å‘çš„æ©è†œå’Œå»å™ªä»»åŠ¡ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåº”ç”¨å™ªå£°è¿‡ç¨‹å¯¹æ¼”åŒ–å‰çš„ä»£ç ç‰‡æ®µè¿›è¡Œå¹²æ‰°ï¼Œç„¶åè¦æ±‚é¢„è®­ç»ƒè¿‡ç¨‹å°†å¸¦æœ‰å™ªå£°çš„ç‰‡æ®µæ¢å¤ä¸ºæ¼”åŒ–åçš„ä»£ç ç‰‡æ®µã€‚ç¬¬äºŒç±»æ˜¯æ—¨åœ¨åŠ å¼ºæ¼”åŒ–æ–¹å‘çš„ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä¸ºæ¼”åŒ–å‰åçš„æ¯ä¸€å¯¹ç‰‡æ®µç”Ÿæˆå„ç§ä¸­é—´ç‰ˆæœ¬ï¼Œç„¶åè¦æ±‚é¢„è®­ç»ƒè¿‡ç¨‹å°†è¿™äº›ä¸­é—´ç‰ˆæœ¬è½¬åŒ–ä¸ºæ¯ä¸€å¯¹çš„æ¼”åŒ–åç‰‡æ®µã€‚æˆ‘ä»¬è¯„ä¼°äº†DivoT5åœ¨ä¸¤ä¸ªä»£ç ç¼–è¾‘åœºæ™¯å’Œä¸€ä¸ªéç¼–è¾‘åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œä½¿ç”¨äº”ä¸ªä¸‹æ¸¸ä»»åŠ¡ã€‚å¯¹äºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œæˆ‘ä»¬å¾®è°ƒé¢„è®­ç»ƒçš„DivoT5ä»¥è¯„ä¼°å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç›¸åŒè§„æ¨¡ï¼ˆ220Mï¼‰ã€å¤§è§„æ¨¡ï¼ˆ770Mï¼‰æ¨¡å‹çš„å¾®è°ƒç›¸æ¯”ï¼ŒDivoT5åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œä¸æ•°åäº¿è§„æ¨¡ï¼ˆ6.7Bã€8Bã€ChatGPTï¼‰çš„æ¨¡å‹ç›¸æ¯”ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚å¯¹äºä¸€é¡¹ä»£ç ç¼–è¾‘ä»»åŠ¡ï¼ˆå³è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥ï¼‰ï¼ŒåŸºäºCodeT5-smallï¼ˆ60Mï¼‰çš„DivoT5é¢„è®­ç»ƒç”šè‡³è¶…è¶Šäº†CodeT5-baseï¼ˆ220Mï¼‰å’Œå…¶ä»–å…·æœ‰220Må‚æ•°çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œé™¤éæ˜¯åŸºäºCodeT5-baseï¼ˆ220Mï¼‰çš„DivoT5è¿›è¡Œé¢„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12079v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®å±‚é¢æ–¹å‘æ‰©æ•£çš„é¢„è®­ç»ƒæ¨¡å‹DivoT5ï¼Œç”¨äºæ¨¡æ‹Ÿäººç±»å¼€å‘è€…çš„é€æ­¥ä»£ç ç¼–è¾‘è¿‡ç¨‹ã€‚DivoT5é‡‡ç”¨ä¸¤ç±»é¢„è®­ç»ƒä»»åŠ¡ï¼šä¸€ç±»æ˜¯å¸¦æœ‰æ‰©æ•£æ–¹å‘ä»£è¡¨ä»£ç æ¼”åŒ–çš„é®è”½å’Œå»å™ªä»»åŠ¡ï¼›å¦ä¸€ç±»æ˜¯åŠ å¼ºæ¼”åŒ–æ–¹å‘çš„ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDivoT5åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šå®ç°äº†ä¸ç›¸åŒè§„æ¨¡æ¨¡å‹ç›¸æ¯”çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¾®è°ƒæ–¹é¢ï¼Œå³ä½¿æ˜¯åŸºäºCodeT5-smallçš„é¢„è®­ç»ƒæ¨¡å‹ä¹Ÿèƒ½è¶…è¶ŠCodeT5-baseå’Œå…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ï¼Œé™¤éæ˜¯åŸºäºCodeT5-baseçš„DivoT5é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç é¢„è®­ç»ƒæ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½æ•ˆæœï¼Œä½†å¿½ç•¥äº†çœŸå®ä¸–ç•Œçš„ä»£ç ç¼–è¾‘æ•°æ®å’Œç¼–è¾‘è¿‡ç¨‹çš„æ¼”åŒ–æ€§è´¨ã€‚</li>
<li>æå‡ºäº†åŸºäºæ•°æ®å±‚é¢æ–¹å‘æ‰©æ•£çš„é¢„è®­ç»ƒæ¨¡å‹DivoT5ï¼Œæ¨¡æ‹Ÿäººç±»å¼€å‘è€…çš„é€æ­¥ä»£ç ç¼–è¾‘è¿‡ç¨‹ã€‚</li>
<li>DivoT5é‡‡ç”¨ä¸¤ç±»é¢„è®­ç»ƒä»»åŠ¡ï¼šé®è”½å’Œå»å™ªä»»åŠ¡ï¼Œä»¥åŠåŠ å¼ºæ¼”åŒ–æ–¹å‘çš„ä»»åŠ¡ã€‚</li>
<li>DivoT5åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œç”šè‡³åœ¨å¾®è°ƒæ–¹é¢è¶…è¶Šäº†ä¸€äº›è§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚</li>
<li>åŸºäºCodeT5-smallçš„DivoT5é¢„è®­ç»ƒæ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥ç­‰ä»£ç ç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¶Šäº†CodeT5-baseå’Œå…¶ä»–ä¸€äº›é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>DivoT5çš„æˆåŠŸå¯èƒ½æºäºå…¶è€ƒè™‘åˆ°äº†ä»£ç çš„æ¼”åŒ–è¿‡ç¨‹å’ŒçœŸå®ä¸–ç•Œçš„ä»£ç ç¼–è¾‘æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf062675a052af8fe7880634dd8f3f80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97c689099e0739bba92c64f13bf76a93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f71b5595ddfda20f4069d274ab099b0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Value-of-Nothing-Multimodal-Extraction-of-Human-Values-Expressed-by-TikTok-Influencers"><a href="#The-Value-of-Nothing-Multimodal-Extraction-of-Human-Values-Expressed-by-TikTok-Influencers" class="headerlink" title="The Value of Nothing: Multimodal Extraction of Human Values Expressed by   TikTok Influencers"></a>The Value of Nothing: Multimodal Extraction of Human Values Expressed by   TikTok Influencers</h2><p><strong>Authors:Alina Starovolsky-Shitrit, Alon Neduva, Naama Appel Doron, Ella Daniel, Oren Tsur</strong></p>
<p>Societal and personal values are transmitted to younger generations through interaction and exposure. Traditionally, children and adolescents learned values from parents, educators, or peers. Nowadays, social platforms serve as a significant channel through which youth (and adults) consume information, as the main medium of entertainment, and possibly the medium through which they learn different values. In this paper we extract implicit values from TikTok movies uploaded by online influencers targeting children and adolescents. We curated a dataset of hundreds of TikTok movies and annotated them according to the Schwartz Theory of Personal Values. We then experimented with an array of Masked and Large language model, exploring how values can be detected. Specifically, we considered two pipelines â€“ direct extraction of values from video and a 2-step approach in which videos are first converted to elaborated scripts and then values are extracted.   Achieving state-of-the-art results, we find that the 2-step approach performs significantly better than the direct approach and that using a trainable Masked Language Model as a second step significantly outperforms a few-shot application of a number of Large Language Models. We further discuss the impact of fine-tuning and compare the performance of the different models on identification of values present or contradicted in the TikTok. Finally, we share the first values-annotated dataset of TikTok videos. Our results pave the way to further research on influence and value transmission in video-based social platforms. </p>
<blockquote>
<p>ç¤¾ä¼šå’Œä¸ªäººä»·å€¼è§‚é€šè¿‡äº’åŠ¨å’Œæ¥è§¦ä¼ é€’ç»™å¹´è½»ä¸€ä»£ã€‚ä¼ ç»Ÿä¸Šï¼Œå„¿ç«¥å’Œé’å°‘å¹´ä»çˆ¶æ¯ã€æ•™è‚²å·¥ä½œè€…æˆ–åŒé¾„äººé‚£é‡Œå­¦ä¹ ä»·å€¼è§‚ã€‚å¦‚ä»Šï¼Œç¤¾äº¤å¹³å°æˆä¸ºé’å¹´ï¼ˆå’Œæˆå¹´äººï¼‰è·å–ä¿¡æ¯ã€ä¸»è¦å¨±ä¹åª’ä»‹ä»¥åŠå¯èƒ½çš„å­¦ä¹ ä¸åŒä»·å€¼è§‚çš„æ¸ é“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»é’ˆå¯¹å„¿ç«¥å’Œé’å°‘å¹´ä¸Šä¼ çš„TikTokç”µå½±ä¸­æå–äº†éšå«çš„ä»·å€¼è§‚ã€‚æˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªåŒ…å«æ•°ç™¾éƒ¨TikTokç”µå½±çš„æ•°æ®åº“ï¼Œå¹¶æ ¹æ®æ–½ç“¦èŒ¨ä¸ªäººä»·å€¼è§‚ç†è®ºå¯¹å®ƒä»¬è¿›è¡Œäº†æ³¨é‡Šã€‚ç„¶åï¼Œæˆ‘ä»¬å°è¯•äº†ä¸€ç³»åˆ—é®ç½©å’Œå¤§è¯­è¨€æ¨¡å‹ï¼Œæ¢ç´¢å¦‚ä½•æ£€æµ‹ä»·å€¼è§‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸¤ä¸ªæµç¨‹â€”â€”ç›´æ¥ä»è§†é¢‘ä¸­æå–ä»·å€¼è§‚å’Œä¸¤æ­¥æ³•ï¼ˆé¦–å…ˆè§†é¢‘è½¬æ¢ä¸ºè¯¦ç»†çš„è„šæœ¬ï¼Œç„¶åæå–ä»·å€¼è§‚ï¼‰ã€‚é€šè¿‡è·å¾—æœ€æ–°ç»“æœï¼Œæˆ‘ä»¬å‘ç°ä¸¤æ­¥æ³•æ˜æ˜¾ä¼˜äºç›´æ¥æ³•ï¼Œä½¿ç”¨å¯è®­ç»ƒé®ç½©è¯­è¨€æ¨¡å‹ä½œä¸ºç¬¬äºŒæ­¥æ˜æ˜¾ä¼˜äºå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„å°‘æ•°ç”¨ä¾‹åº”ç”¨ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºäº†å¾®è°ƒçš„å½±å“ï¼Œæ¯”è¾ƒäº†ä¸åŒæ¨¡å‹åœ¨è¯†åˆ«TikTokä¸­å­˜åœ¨çš„æˆ–ç›¸äº’çŸ›ç›¾çš„ä»·å€¼è§‚æ–¹é¢çš„è¡¨ç°ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†äº«äº†ç¬¬ä¸€ä¸ªTikTokè§†é¢‘çš„ä»·å€¼è§‚æ³¨é‡Šæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºè§†é¢‘ç¤¾äº¤å¹³å°ä¸Šçš„å½±å“å’Œä»·å€¼è§‚ä¼ é€’ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11770v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶ç¤¾äº¤åª’ä½“å¹³å°TikTokå¯¹å„¿ç«¥åŠé’å°‘å¹´ä»·å€¼è§‚çš„å½±å“ã€‚é€šè¿‡å¯¹æ•°ç™¾éƒ¨TikTokè§†é¢‘çš„åˆ†æï¼Œå¹¶åŸºäºæ–½ç“¦èŒ¨ä»·å€¼è§‚ç†è®ºè¿›è¡Œæ ‡æ³¨ï¼Œå®éªŒäº†ä¸¤ç§ä»·å€¼è§‚æ£€æµ‹æ–¹æ³•ï¼šç›´æ¥ä»è§†é¢‘æå–ä»·å€¼è§‚å’Œä¸¤æ­¥æ³•ï¼ˆè§†é¢‘è½¬åŒ–ä¸ºå‰§æœ¬åå†æå–ä»·å€¼è§‚ï¼‰ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºä¸¤æ­¥æ³•æ•ˆæœæ›´ä½³ï¼Œå°¤å…¶æ˜¯ç¬¬äºŒæ­¥ä½¿ç”¨å¯è®­ç»ƒçš„æ©ç è¯­è¨€æ¨¡å‹æ—¶ï¼Œæ˜æ˜¾ä¼˜äºå‡ ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ æ¬¡åº”ç”¨ã€‚æ–‡ç« è¿˜è®¨è®ºäº†å¾®è°ƒçš„å½±å“ï¼Œå¹¶åˆ†äº«äº†é¦–ä¸ªTikTokè§†é¢‘çš„ä»·å€¼æ ‡æ³¨æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TikTokæˆä¸ºä¼ é€’ä»·å€¼è§‚ç»™é’å°‘å¹´ç”šè‡³æˆäººçš„é‡è¦æ¸ é“ã€‚</li>
<li>å¯¹TikTokè§†é¢‘çš„ç ”ç©¶å‘ç°ä¸¤æ­¥æ³•æ£€æµ‹ä»·å€¼è§‚æ›´ä¸ºå‡†ç¡®ã€‚</li>
<li>æ©ç è¯­è¨€æ¨¡å‹åœ¨ä¸¤æ­¥æ³•ä¸­çš„ç¬¬äºŒæ­¥è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç²¾ç»†è°ƒæ•´å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>åˆ†äº«äº†ä¸€ä¸ªæ ‡æ³¨äº†ä»·å€¼çš„TikTokè§†é¢‘æ•°æ®é›†ã€‚</li>
<li>ä¼ ç»Ÿä»·å€¼è§‚ä¼ æ’­æ–¹å¼ï¼ˆå¦‚çˆ¶æ¯ã€æ•™è‚²è€…ã€åŒé¾„äººï¼‰æ­£é€æ¸è¢«ç¤¾äº¤åª’ä½“å–ä»£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11770">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f70c13fd14a81c5b8af20defb00cfb4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9258b41bebf28da03c06d2cc7c2eeb84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c078e8b7774bb857af4395857bbd0f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbac969454e21aac3752e0972468b100.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff13343ca15ceffee1e90ad04e790af1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fc3345f3420687d4c23f97831be7251.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ProKeR-A-Kernel-Perspective-on-Few-Shot-Adaptation-of-Large-Vision-Language-Models"><a href="#ProKeR-A-Kernel-Perspective-on-Few-Shot-Adaptation-of-Large-Vision-Language-Models" class="headerlink" title="ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large   Vision-Language Models"></a>ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large   Vision-Language Models</h2><p><strong>Authors:Yassir Bendou, Amine Ouasfi, Vincent Gripon, Adnane Boukhayma</strong></p>
<p>The growing popularity of Contrastive Language-Image Pretraining (CLIP) has led to its widespread application in various visual downstream tasks. To enhance CLIPâ€™s effectiveness and versatility, efficient few-shot adaptation techniques have been widely adopted. Among these approaches, training-free methods, particularly caching methods exemplified by Tip-Adapter, have gained attention for their lightweight adaptation without the need for additional fine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective, showing that caching methods function as local adapters and are connected to a well-established kernel literature. Drawing on this insight, we offer a theoretical understanding of how these methods operate and suggest multiple avenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the importance of incorporating global information in local adapters. Therefore, we subsequently propose a global method that learns a proximal regularizer in a reproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our method, which we call ProKeR (Proximal Kernel ridge Regression), has a closed form solution and achieves state-of-the-art performances across 11 datasets in the standard few-shot adaptation benchmark. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰çš„æ—¥ç›Šæ™®åŠå·²å¯¼è‡´å…¶åœ¨å„ç§è§†è§‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†æé«˜CLIPçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œé«˜æ•ˆçš„å°‘æ ·æœ¬è‡ªé€‚åº”æŠ€æœ¯å·²å¾—åˆ°å¹¿æ³›é‡‡ç”¨ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œæ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä»¥Tip-Adapterä¸ºä»£è¡¨çš„ç¼“å­˜æ–¹æ³•ï¼Œå› å…¶æ— éœ€é¢å¤–å¾®è°ƒå³å¯å®ç°è½»é‡çº§è‡ªé€‚åº”è€Œå¤‡å—å…³æ³¨ã€‚æœ¬æ–‡å°†ä»æ ¸å¿ƒè§†è§’é‡æ–°å®¡è§†Tip-Adapterï¼Œå±•ç¤ºç¼“å­˜æ–¹æ³•ä½œä¸ºå±€éƒ¨é€‚é…å™¨ä¸å·²å»ºç«‹çš„æ ¸å¿ƒæ–‡çŒ®ä¹‹é—´çš„è”ç³»ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹è¿™äº›æ–¹æ³•å¦‚ä½•è¿ä½œçš„ç†è®ºç†è§£ï¼Œå¹¶ç»™å‡ºäº†å¢å¼ºTip-AdapteråŸºçº¿æ€§èƒ½çš„å¤šç§é€”å¾„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜åœ¨å±€éƒ¨é€‚é…å™¨ä¸­èå…¥å…¨å±€ä¿¡æ¯çš„é‡è¦æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éšåæå‡ºäº†ä¸€ç§å…¨å±€æ–¹æ³•ï¼Œåœ¨å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰ä¸­å­¦ä¹ è¿‘ç«¯æ­£åˆ™åŒ–å™¨ï¼Œä»¥CLIPä¸ºåŸºç¡€å­¦ä¹ å™¨ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¢«ç§°ä¸ºProKeRï¼ˆè¿‘ç«¯æ ¸å²­å›å½’ï¼‰ï¼Œå®ƒå…·æœ‰å°é—­å½¢å¼çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨æ ‡å‡†å°‘æ ·æœ¬è‡ªé€‚åº”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è·¨11ä¸ªæ•°æ®é›†çš„æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11175v1">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://ybendou.github.io/ProKeR">https://ybendou.github.io/ProKeR</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆCLIPï¼‰çš„æ™®åŠå’Œå…¶å¹¿æ³›åº”ç”¨äºå„ç§è§†è§‰ä¸‹æ¸¸ä»»åŠ¡çš„éœ€æ±‚ï¼Œæœ¬æ–‡ç ”ç©¶äº†é«˜æ•ˆçš„CLIPå°‘æ ·æœ¬é€‚åº”æŠ€æœ¯ã€‚ç‰¹åˆ«å…³æ³¨äº†æ— è®­ç»ƒæ–¹æ³•ä¸­çš„ç¼“å­˜æ–¹æ³•ï¼Œå¦‚Tip-Adapterã€‚æœ¬æ–‡ä»æ ¸å¿ƒè§’åº¦é‡æ–°å®¡è§†Tip-Adapterï¼Œå°†å…¶ä¸ç°æœ‰çš„å†…æ ¸æ–‡çŒ®è”ç³»èµ·æ¥ï¼Œæä¾›äº†å…¶å·¥ä½œåŸç†çš„ç†è®ºç†è§£ï¼Œå¹¶å»ºè®®æ”¹è¿›åŸºçº¿Tip-Adapterçš„æ–¹æ³•ã€‚æ–‡ç« æå‡ºç»“åˆå…¨å±€ä¿¡æ¯å¯¹å±€éƒ¨é€‚é…å™¨è¿›è¡Œæ”¹è¿›çš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§åŸºäºCLIPçš„å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰è¿‘ç«¯æ­£åˆ™åŒ–çš„æ–°æ–¹æ³•ProKeRã€‚è¯¥æ–¹æ³•å…·æœ‰é—­å¼è§£ï¼Œåœ¨æ ‡å‡†å°‘æ ·æœ¬é€‚åº”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è·¨11ä¸ªæ•°æ®é›†çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPçš„æ™®åŠå’Œåœ¨å„ç§è§†è§‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨æ¨åŠ¨äº†å¯¹å…¶é«˜æ•ˆå°‘æ ·æœ¬é€‚åº”æŠ€æœ¯çš„ç ”ç©¶ã€‚</li>
<li>æ— è®­ç»ƒæ–¹æ³•çš„ç¼“å­˜æ–¹æ³•ï¼Œå¦‚Tip-Adapterï¼Œç”±äºå…¶æ— éœ€é¢å¤–ç²¾ç»†è°ƒæ•´è€Œå—åˆ°äº†å…³æ³¨ã€‚</li>
<li>Tip-Adapterä¸å†…æ ¸æ–‡çŒ®ä¹‹é—´çš„è”ç³»å¾—åˆ°äº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶æ­ç¤ºäº†å…¶å·¥ä½œåŸç†çš„ç†è®ºåŸºç¡€ã€‚</li>
<li>ç»“åˆå…¨å±€ä¿¡æ¯å¯¹å±€éƒ¨é€‚é…å™¨è¿›è¡Œæ”¹è¿›æ˜¯å¿…è¦çš„ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„æ–°æ–¹æ³•ProKeRï¼Œç”¨äºå­¦ä¹ å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­çš„è¿‘ç«¯æ­£åˆ™åŒ–å™¨ã€‚</li>
<li>ProKeRå®ç°äº†åœ¨æ ‡å‡†å°‘æ ·æœ¬é€‚åº”åŸºå‡†æµ‹è¯•ä¸­è·¨å¤šä¸ªæ•°æ®é›†çš„æœ€ä½³æ€§èƒ½è¡¨ç°ã€‚è¿™æ„å‘³ç€è¿™ç§æ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºäº†ç¨³å¥è€Œå‡ºè‰²çš„æ•ˆæœã€‚è¿™å¯èƒ½ä¸ºåç»­çš„ç ”ç©¶æä¾›æ–°çš„æ€è·¯æˆ–å¯ç¤ºï¼Œå¹¶å¯èƒ½åœ¨å®é™…åº”ç”¨ä¸­å¸¦æ¥æ˜¾è‘—çš„æå‡ã€‚å®ƒä¹Ÿæœ‰åŠ©äºæˆ‘ä»¬æ›´æ·±å…¥åœ°ç†è§£CLIPæ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚å› æ­¤ï¼Œæœªæ¥å¯èƒ½ä¼šæœ‰æ›´å¤šçš„ç ”ç©¶å…³æ³¨äºå¦‚ä½•è¿›ä¸€æ­¥æ”¹è¿›å’Œå®Œå–„è¿™ä¸€æ¨¡å‹ä»¥å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•å…·å¤‡å‡ºè‰²çš„é€šç”¨æ€§å’Œé€‚åº”æ€§æœªæ¥æˆ–å°†å¼•å‘æ›´å¤šçš„æ¢ç´¢å’Œæ”¹è¿›å®è·µä»¥å¢å¼ºå…¶åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­çš„æ€§èƒ½å¹¶æ‰©å±•å…¶åº”ç”¨èŒƒå›´è¯¥æ–¹æ³•è¿˜å…·æœ‰å·¨å¤§çš„æ½œåŠ›åœ¨æœªæ¥ä¸ºè®¸å¤šä¸åŒçš„åº”ç”¨é¢†åŸŸå¸¦æ¥åˆ›æ–°å’Œæ”¹è¿›æˆ‘ä»¬æœŸå¾…æœªæ¥å¯¹æ­¤æ–¹æ³•çš„æ›´å¤šç ”ç©¶å’Œæ¢ç´¢ä»¥å®ç°æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯å’Œæå‡æ•ˆæœã€‚ã€‚è¿™äº›æ–¹æ³•ä¹Ÿç»™æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„è§†è§’æ¥è¿›ä¸€æ­¥ç†è§£å’Œä¼˜åŒ–CLIPæ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°å’Œåº”ç”¨èŒƒå›´ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ç¯‡æ–‡ç« ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…³äºå¦‚ä½•ä¼˜åŒ–å’Œæ”¹è¿›CLIPæ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­æ€§èƒ½çš„é‡è¦è§†è§’å’Œæ–°çš„æ€è·¯ã€‚è¿™å¯¹äºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨å…·æœ‰é‡è¦çš„æ„ä¹‰å’Œä»·å€¼ã€‚   å› æ­¤å®ƒçš„æˆåŠŸå®ç°å¯èƒ½ä¸ºæœºå™¨å­¦ä¹ é¢†åŸŸçš„è®¸å¤šå…¶ä»–é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆå’Œåˆ›æ–°æ€è·¯ï¼Œã€‚æœ€åæœŸå¾…æœ‰æ›´å¤šå¯¹è¯¥æ–¹æ³•å’Œæ¨¡å‹çš„æ¢ç´¢å’Œä¼˜åŒ–èƒ½å¤Ÿåœ¨ä¸åŒåœºæ™¯ä¸‹è·å¾—æ›´ä¸ºä¼˜ç§€çš„æ€§èƒ½è¡¨ç°å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯è¢«å¼€å‘å‡ºæ­¤å¤–å¯¹æ­¤ä¸»é¢˜çš„æŒç»­æ¢ç´¢å’Œç ”ç©¶å°†æœ‰åŠ©äºæ¨åŠ¨è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æˆ‘ä»¬ä¹ŸæœŸå¾…æœªæ¥çœ‹åˆ°æ›´å¤šç›¸å…³çš„ç ”ç©¶å’Œåº”ç”¨æ¡ˆä¾‹å‡ºç°ä»¥è§£å†³æ›´å¤šç°å®ç”Ÿæ´»ä¸­çš„é—®é¢˜å¹¶ä¸ºç›¸å…³é¢†åŸŸçš„å‘å±•åšå‡ºæ›´å¤§çš„è´¡çŒ®ï¼Œã€‚ä¹Ÿç›¸ä¿¡æœªæ¥çš„ç ”ç©¶å°†èƒ½å¤Ÿæ¨åŠ¨è¯¥é¢†åŸŸå–å¾—æ›´å¤§çš„çªç ´å’Œè¿›å±•ä¸ºå®é™…åº”ç”¨å¸¦æ¥æ›´å¤šåˆ›æ–°å’Œä»·å€¼åŒæ—¶æ¨åŠ¨æ•´ä¸ªæœºå™¨å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥å’Œå‘å±•å› æ­¤å¯¹è¯¥ä¸»é¢˜çš„ç ”ç©¶å…·æœ‰é‡è¦çš„ä»·å€¼å’Œæ„ä¹‰å¹¶æœ‰æœ›åœ¨æœªæ¥äº§ç”Ÿæ·±è¿œçš„å½±å“ã€‚æ€»çš„æ¥è¯´è¿™ç¯‡æ–‡ç« æä¾›äº†ä¸€ä¸ªå…³äºCLIPæ¨¡å‹å°‘æ ·æœ¬é€‚åº”æŠ€æœ¯çš„æ·±å…¥ç†è§£ä»¥åŠåœ¨æœªæ¥ç ”ç©¶çš„å¯èƒ½æ€§æä¾›äº†å¯Œæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘è¿™äº›æ´è§éƒ½å°†ä¸ºæˆ‘ä»¬æ‰“å¼€è¿›ä¸€æ­¥ç†è§£å¹¶æŒæ¡è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å…³é”®æœºé‡å°†æˆ‘ä»¬æ¨å‘äº†ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½æŠ€æœ¯å’Œè®¡ç®—æœºè§†è§‰çš„æœªæ¥å‰æ²¿éšç€äººå·¥æ™ºèƒ½çš„é£é€Ÿå‘å±•ä»¥åŠç ”ç©¶çš„ä¸æ–­è¿›æ­¥è¿™ä¸€é¢†åŸŸå¿…å°†äº§ç”Ÿæ›´å¤šçš„åˆ›æ–°å’Œçªç ´å€¼å¾—æˆ‘ä»¬æŒç»­å…³æ³¨å¹¶æŠ•å…¥ç ”ç©¶ä»¥æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ•´ä½“è¿›æ­¥å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11175v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11175v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11175v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Any-Shot-Adaptation-Predicting-Optimization-Outcome-for-Robustness-Gains-without-Extra-Pay"><a href="#Beyond-Any-Shot-Adaptation-Predicting-Optimization-Outcome-for-Robustness-Gains-without-Extra-Pay" class="headerlink" title="Beyond Any-Shot Adaptation: Predicting Optimization Outcome for   Robustness Gains without Extra Pay"></a>Beyond Any-Shot Adaptation: Predicting Optimization Outcome for   Robustness Gains without Extra Pay</h2><p><strong>Authors:Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv, Xiangyang Ji</strong></p>
<p>The foundation model enables fast problem-solving without learning from scratch, and such a desirable adaptation property benefits from its adopted cross-task generalization paradigms, e.g., pretraining, meta-training, or finetuning. Recent trends have focused on the curation of task datasets during optimization, which includes task selection as an indispensable consideration for either adaptation robustness or sampling efficiency purposes. Despite some progress, selecting crucial task batches to optimize over iteration mostly exhausts massive task queries and requires intensive evaluation and computations to secure robust adaptation. This work underscores the criticality of both robustness and learning efficiency, especially in scenarios where tasks are risky to collect or costly to evaluate. To this end, we present Model Predictive Task Sampling (MPTS), a novel active task sampling framework to establish connections between the task space and adaptation risk landscape achieve robust adaptation. Technically, MPTS characterizes the task episodic information with a generative model and predicts optimization outcome after adaptation from posterior inference, i.e., forecasting task-specific adaptation risk values. The resulting risk learner amortizes expensive annotation, evaluation, or computation operations in task robust adaptation learning paradigms. Extensive experimental results show that MPTS can be seamlessly integrated into zero-shot, few-shot, and many-shot learning paradigms, increases adaptation robustness, and retains learning efficiency without affording extra cost. The code will be available at the project site <a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS">https://github.com/thu-rllab/MPTS</a>. </p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹èƒ½å¤Ÿå®ç°å¿«é€Ÿçš„é—®é¢˜è§£å†³ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹å­¦ä¹ ï¼Œè¿™ç§ç†æƒ³çš„é€‚åº”æ€§å¾—ç›Šäºå…¶é‡‡ç”¨çš„è·¨ä»»åŠ¡æ³›åŒ–èŒƒå¼ï¼Œä¾‹å¦‚é¢„è®­ç»ƒã€å…ƒè®­ç»ƒæˆ–å¾®è°ƒã€‚æœ€è¿‘çš„è¶‹åŠ¿é›†ä¸­åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä»»åŠ¡æ•°æ®é›†ç­›é€‰ä¸Šï¼Œä»»åŠ¡é€‰æ‹©æˆä¸ºé€‚åº”ç¨³å¥æ€§æˆ–é‡‡æ ·æ•ˆç‡ç›®çš„ä¸å¯æˆ–ç¼ºçš„é‡è¦å› ç´ ã€‚å°½ç®¡å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†é€‰æ‹©å…³é”®ä»»åŠ¡æ‰¹æ¬¡è¿›è¡Œä¼˜åŒ–åœ¨è¿­ä»£è¿‡ç¨‹ä¸­å¤§å¤šæ¶ˆè€—äº†å¤§é‡çš„ä»»åŠ¡æŸ¥è¯¢ï¼Œå¹¶éœ€è¦å¤§é‡è¯„ä¼°å’Œè®¡ç®—ä»¥ç¡®ä¿ç¨³å¥çš„é€‚åº”ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡æ”¶é›†é£é™©å¤§æˆ–è¯„ä¼°æˆæœ¬é«˜çš„åœºæ™¯ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸»åŠ¨ä»»åŠ¡é‡‡æ ·æ¡†æ¶ï¼Œæ—¨åœ¨å»ºç«‹ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚ä¹‹é—´çš„è”ç³»ï¼Œä»¥å®ç°ç¨³å¥çš„é€‚åº”ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼ŒMPTSä½¿ç”¨ç”Ÿæˆæ¨¡å‹å¯¹ä»»åŠ¡ç‰‡æ®µä¿¡æ¯è¿›è¡Œè¡¨å¾ï¼Œå¹¶é€šè¿‡åéªŒæ¨æ–­é¢„æµ‹é€‚åº”åçš„ä¼˜åŒ–ç»“æœï¼Œå³é¢„æµ‹ç‰¹å®šä»»åŠ¡çš„é€‚åº”é£é™©å€¼ã€‚æ‰€å¾—çš„é£é™©å­¦ä¹ è€…åœ¨ä»»åŠ¡ç¨³å¥é€‚åº”å­¦ä¹ èŒƒå¼ä¸­å‡å°‘äº†æ˜‚è´µçš„æ ‡æ³¨ã€è¯„ä¼°æˆ–è®¡ç®—æ“ä½œã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒMPTSå¯ä»¥æ— ç¼åœ°èå…¥é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¤šæ ·æœ¬å­¦ä¹ èŒƒå¼ä¸­ï¼Œæé«˜é€‚åº”çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒå­¦ä¹ æ•ˆç‡ä¸”æ— éœ€é¢å¤–æˆæœ¬ã€‚ä»£ç å°†åœ¨é¡¹ç›®ç½‘ç«™<a target="_blank" rel="noopener" href="https://github.com/thu-rllab/MPTS%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/thu-rllab/MPTSä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11039v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹é¢„è®­ç»ƒã€å…ƒè®­ç»ƒæˆ–å¾®è°ƒç­‰è·¨ä»»åŠ¡æ³›åŒ–èŒƒå¼ï¼Œä½¿æ¨¡å‹å…·å¤‡å¿«é€Ÿè§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹å­¦ä¹ ã€‚è¿‘æœŸè¶‹åŠ¿èšç„¦äºä¼˜åŒ–ä»»åŠ¡æ•°æ®é›†çš„é€‰æ‹©ï¼Œè€ƒè™‘ä»»åŠ¡é€‰æ‹©çš„é€‚åº”ç¨³å¥æ€§æˆ–é‡‡æ ·æ•ˆç‡ã€‚æœ¬æ–‡å¼ºè°ƒåœ¨ä»»åŠ¡é‡‡é›†é£é™©å¤§æˆ–è¯„ä¼°æˆæœ¬é«˜çš„åœºæ™¯ä¸‹ï¼Œç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡çš„é‡è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åŸºäºé¢„æµ‹ä»»åŠ¡é‡‡æ ·çš„æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰æ¡†æ¶ï¼Œå»ºç«‹ä»»åŠ¡ç©ºé—´ä¸é€‚åº”é£é™©æ™¯è§‚ä¹‹é—´çš„è”ç³»ï¼Œå®ç°ç¨³å¥é€‚åº”ã€‚MPTSåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å¯¹ä»»åŠ¡ç‰‡æ®µä¿¡æ¯è¿›è¡Œè¡¨å¾ï¼Œé€šè¿‡åå¤©æ¨ç†é¢„æµ‹ä¼˜åŒ–ç»“æœï¼Œå³é¢„æµ‹ä»»åŠ¡ç‰¹å®šçš„é€‚åº”é£é™©å€¼ã€‚è¿™é™ä½äº†æ˜‚è´µçš„æ ‡æ³¨ã€è¯„ä¼°æˆ–è®¡ç®—æ“ä½œæˆæœ¬ï¼Œæé«˜äº†ä»»åŠ¡ç¨³å¥é€‚åº”å­¦ä¹ çš„æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒMPTSå¯æ— ç¼èå…¥é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¤šæ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œæé«˜é€‚åº”æ€§å¹¶ä¿ç•™å­¦ä¹ æ•ˆç‡ï¼Œä¸”ä¸å¢åŠ é¢å¤–æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨ä»»åŠ¡æ³›åŒ–èŒƒå¼ï¼ˆå¦‚é¢„è®­ç»ƒã€å…ƒè®­ç»ƒã€å¾®è°ƒï¼‰ä½¿æ¨¡å‹å…·å¤‡å¿«é€Ÿè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>ä»»åŠ¡æ•°æ®é›†çš„é€‰æ‹©æ˜¯ä¼˜åŒ–ä¸­çš„ä¸€ä¸ªé‡è¦è€ƒè™‘å› ç´ ï¼Œç‰¹åˆ«æ˜¯åœ¨é€‚åº”ç¨³å¥æ€§å’Œé‡‡æ ·æ•ˆç‡æ–¹é¢ã€‚</li>
<li>åœ¨ä»»åŠ¡é‡‡é›†é£é™©å¤§æˆ–è¯„ä¼°æˆæœ¬é«˜çš„åœºæ™¯ä¸‹ï¼Œéœ€è¦å¼ºè°ƒç¨³å¥æ€§å’Œå­¦ä¹ æ•ˆç‡çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä¸»åŠ¨ä»»åŠ¡é‡‡æ ·æ¡†æ¶â€”â€”æ¨¡å‹é¢„æµ‹ä»»åŠ¡é‡‡æ ·ï¼ˆMPTSï¼‰ï¼Œå®ƒé€šè¿‡é¢„æµ‹ä»»åŠ¡ç‰¹å®šçš„é€‚åº”é£é™©å€¼æ¥å»ºç«‹ä»»åŠ¡ç©ºé—´å’Œé€‚åº”é£é™©æ™¯è§‚ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>MPTSæ¡†æ¶é™ä½äº†æ ‡æ³¨ã€è¯„ä¼°å’Œè®¡ç®—æ“ä½œçš„æˆæœ¬ï¼Œæé«˜äº†ä»»åŠ¡ç¨³å¥é€‚åº”å­¦ä¹ çš„æ•ˆç‡ã€‚</li>
<li>MPTSå¯å¹¿æ³›åº”ç”¨äºé›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¤šæ ·æœ¬å­¦ä¹ èŒƒå¼ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11039v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11039v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11039v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11039v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Few-shot-Human-Motion-Recognition-through-Multi-Aspect-mmWave-FMCW-Radar-Data"><a href="#Few-shot-Human-Motion-Recognition-through-Multi-Aspect-mmWave-FMCW-Radar-Data" class="headerlink" title="Few-shot Human Motion Recognition through Multi-Aspect mmWave FMCW Radar   Data"></a>Few-shot Human Motion Recognition through Multi-Aspect mmWave FMCW Radar   Data</h2><p><strong>Authors:Hao Fan, Lingfeng Chen, Chengbai Xu, Jiadong Zhou, Yongpeng Dai, Panhe HU</strong></p>
<p>Radar human motion recognition methods based on deep learning models has been a heated spot of remote sensing in recent years, yet the existing methods are mostly radial-oriented. In practical application, the test data could be multi-aspect and the sample number of each motion could be very limited, causing model overfitting and reduced recognition accuracy. This paper proposed channel-DN4, a multi-aspect few-shot human motion recognition method. First, local descriptors are introduced for a precise classification metric. Moreover, episodic training strategy was adopted to reduce model overfitting. To utilize the invariant sematic information in multi-aspect conditions, we considered channel attention after the embedding network to obtain precise implicit high-dimensional representation of sematic information. We tested the performance of channel-DN4 and methods for comparison on measured mmWave FMCW radar data. The proposed channel-DN4 produced competitive and convincing results, reaching the highest 87.533% recognition accuracy in 3-way 10-shot condition while other methods suffer from overfitting. Codes are available at: <a target="_blank" rel="noopener" href="https://github.com/MountainChenCad/channel-DN4">https://github.com/MountainChenCad/channel-DN4</a> </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é›·è¾¾äººä½“è¿åŠ¨è¯†åˆ«æ–¹æ³•è¿‘å¹´æ¥å·²æˆä¸ºé¥æ„Ÿé¢†åŸŸçš„çƒ­ç‚¹ï¼Œä½†ç°æœ‰æ–¹æ³•å¤§å¤šä»¥å¾„å‘ä¸ºå¯¼å‘ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæµ‹è¯•æ•°æ®å¯èƒ½æ˜¯å¤šæ–¹é¢çš„ï¼Œæ¯ç§è¿åŠ¨çš„æ ·æœ¬æ•°é‡å¯èƒ½éå¸¸æœ‰é™ï¼Œå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆå’Œè¯†åˆ«ç²¾åº¦é™ä½ã€‚æœ¬æ–‡æå‡ºäº†channel-DN4ï¼Œä¸€ç§å¤šæ–¹é¢çš„å°æ ·æœ¬äººä½“è¿åŠ¨è¯†åˆ«æ–¹æ³•ã€‚é¦–å…ˆï¼Œå¼•å…¥å±€éƒ¨æè¿°ç¬¦è¿›è¡Œç²¾ç¡®çš„åˆ†ç±»åº¦é‡ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å‘¨æœŸè®­ç»ƒç­–ç•¥ä»¥å‡å°‘æ¨¡å‹è¿‡æ‹Ÿåˆã€‚ä¸ºäº†åˆ©ç”¨å¤šæ–¹é¢æ¡ä»¶ä¸‹çš„ä¸å˜è¯­ä¹‰ä¿¡æ¯ï¼Œæˆ‘ä»¬åœ¨åµŒå…¥ç½‘ç»œåè€ƒè™‘é€šé“æ³¨æ„åŠ›ï¼Œä»¥è·å¾—ç²¾ç¡®çš„éšå¼é«˜ç»´è¯­ä¹‰ä¿¡æ¯è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨æµ‹é‡çš„æ¯«ç±³æ³¢FMCWé›·è¾¾æ•°æ®ä¸Šæµ‹è¯•äº†channel-DN4åŠå…¶å¯¹æ¯”æ–¹æ³•çš„æ€§èƒ½ã€‚æ‰€æå‡ºçš„channel-DN4è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨3ç±»10æ ·æœ¬çš„æ¡ä»¶ä¸‹è¾¾åˆ°äº†æœ€é«˜çš„87.533%çš„è¯†åˆ«ç²¾åº¦ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MountainChenCad/channel-DN4%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MountainChenCad/channel-DN4æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11028v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„é›·è¾¾äººä½“è¿åŠ¨è¯†åˆ«æ–¹æ³•å·²æˆä¸ºé¥æ„Ÿé¢†åŸŸçš„çƒ­ç‚¹ã€‚ä½†ç°æœ‰æ–¹æ³•å¤šä¸ºå¾„å‘æ–¹å‘ï¼Œå®é™…åº”ç”¨ä¸­æµ‹è¯•æ•°æ®å¯èƒ½å…·æœ‰å¤šæ–¹é¢ä¸”æ¯ç§è¿åŠ¨çš„æ ·æœ¬æ•°é‡æœ‰é™ï¼Œå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆå’Œè¯†åˆ«ç²¾åº¦é™ä½ã€‚æœ¬æ–‡æå‡ºchannel-DN4ï¼Œä¸€ç§å¤šæ–¹é¢çš„å°æ ·æœ¬äººä½“è¿åŠ¨è¯†åˆ«æ–¹æ³•ã€‚å¼•å…¥å±€éƒ¨æè¿°ç¬¦è¿›è¡Œç²¾ç¡®åˆ†ç±»åº¦é‡ï¼Œé‡‡ç”¨å‘¨æœŸè®­ç»ƒç­–ç•¥å‡è½»æ¨¡å‹è¿‡æ‹Ÿåˆã€‚é€šè¿‡åµŒå…¥ç½‘ç»œåçš„é€šé“æ³¨æ„åŠ›åˆ©ç”¨å¤šæ–¹é¢æ¡ä»¶ä¸‹çš„ä¸å˜è¯­ä¹‰ä¿¡æ¯ï¼Œè·å¾—ç²¾ç¡®çš„é«˜ç»´è¯­ä¹‰è¡¨ç¤ºã€‚åœ¨å®æµ‹æ¯«ç±³æ³¢FMCWé›·è¾¾æ•°æ®ä¸Šæµ‹è¯•channel-DN4å’Œå…¶ä»–å¯¹æ¯”æ–¹æ³•ï¼Œchannel-DN4è¡¨ç°å‡ºç«äº‰æ€§å’Œä»¤äººä¿¡æœçš„ç»“æœï¼Œåœ¨3ç±»10æ ·æœ¬çš„æ¡ä»¶ä¸‹è¾¾åˆ°æœ€é«˜çš„87.533%è¯†åˆ«ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›·è¾¾äººä½“è¿åŠ¨è¯†åˆ«åŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹å·²æˆä¸ºé¥æ„Ÿçƒ­ç‚¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¸ºå¾„å‘æ–¹å‘ï¼Œå®é™…åº”ç”¨é¢ä¸´å¤šæ–¹é¢å’Œæœ‰é™æ ·æœ¬çš„æŒ‘æˆ˜ã€‚</li>
<li>channel-DN4æ–¹æ³•å¼•å…¥å±€éƒ¨æè¿°ç¬¦è¿›è¡Œç²¾ç¡®åˆ†ç±»ã€‚</li>
<li>é‡‡ç”¨å‘¨æœŸè®­ç»ƒç­–ç•¥å‡è½»æ¨¡å‹è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>é€šè¿‡é€šé“æ³¨æ„åŠ›åˆ©ç”¨å¤šæ–¹é¢æ¡ä»¶ä¸‹çš„ä¸å˜è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>åœ¨å®æµ‹æ•°æ®ä¸Šæµ‹è¯•ï¼Œchannel-DN4è¡¨ç°å‡ºé«˜è¯†åˆ«ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11028v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11028v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11028v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11028v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11028v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.11028v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Visual-RAG-Expanding-MLLM-visual-knowledge-without-fine-tuning"><a href="#Visual-RAG-Expanding-MLLM-visual-knowledge-without-fine-tuning" class="headerlink" title="Visual RAG: Expanding MLLM visual knowledge without fine-tuning"></a>Visual RAG: Expanding MLLM visual knowledge without fine-tuning</h2><p><strong>Authors:Mirco Bonomo, Simone Bianco</strong></p>
<p>Multimodal Large Language Models (MLLMs) have achieved notable performance in computer vision tasks that require reasoning across visual and textual modalities, yet their capabilities are limited to their pre-trained data, requiring extensive fine-tuning for updates. Recent researches have explored the use of In-Context Learning (ICL) to overcome these challenges by providing a set of demonstrating examples as context to augment MLLMs performance in several tasks, showing that many-shot ICL leads to substantial improvements compared to few-shot ICL. However, the reliance on numerous demonstrating examples and the limited MLLMs context windows presents significant obstacles. This paper aims to address these challenges by introducing a novel approach, Visual RAG, that synergically combines the MLLMs capability to learn from the context, with a retrieval mechanism. The crux of this approach is to ensure to augment the MLLM knowledge by selecting only the most relevant demonstrating examples for the query, pushing it to learn by analogy. In this way, relying on the new information provided dynamically during inference time, the resulting system is not limited to the knowledge extracted from the training data, but can be updated rapidly and easily without fine-tuning. Furthermore, this greatly reduces the computational costs for improving the model image classification performance, and augments the model knowledge to new visual domains and tasks it was not trained for. Extensive experiments on eight different datasets in the state of the art spanning several domains and image classification tasks show that the proposed Visual RAG, compared to the most recent state of the art (i.e., many-shot ICL), is able to obtain an accuracy that is very close or even higher (approx. +2% improvement on average) while using a much smaller set of demonstrating examples (approx. only 23% on average). </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†å¼•äººæ³¨ç›®çš„æˆæœï¼Œè¿™äº›ä»»åŠ¡éœ€è¦åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´è¿›è¡Œæ¨ç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„èƒ½åŠ›ä»…é™äºé¢„è®­ç»ƒæ•°æ®ï¼Œéœ€è¦è¿›è¡Œå¹¿æ³›çš„å¾®è°ƒæ‰èƒ½æ›´æ–°ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡æä¾›ä¸€ç³»åˆ—æ¼”ç¤ºä¾‹å­ä½œä¸ºä¸Šä¸‹æ–‡æ¥å¢å¼ºMLLMsåœ¨å¤šä¸ªä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œè¡¨æ˜å¤šç¤ºä¾‹ICLä¸å°‘ç¤ºä¾‹ICLç›¸æ¯”ï¼Œå¯ä»¥å¸¦æ¥å®è´¨æ€§çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¯¹å¤§é‡æ¼”ç¤ºä¾‹å­çš„ä¾èµ–ä»¥åŠMLLMsä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶æ„æˆäº†é‡å¤§éšœç¢ã€‚</p>
</blockquote>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ç§æ–°æ–¹æ³•æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå³Visual RAGã€‚è¯¥æ–¹æ³•ååŒç»“åˆäº†MLLMsä»ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ çš„èƒ½åŠ›ä¸æ£€ç´¢æœºåˆ¶ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ç¡®ä¿é€šè¿‡é€‰æ‹©ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ¼”ç¤ºä¾‹å­æ¥å¢å¼ºMLLMçš„çŸ¥è¯†ï¼Œæ¨åŠ¨å…¶é€šè¿‡ç±»æ¯”è¿›è¡Œå­¦ä¹ ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä¾èµ–æ¨ç†æ—¶åŠ¨æ€æä¾›çš„æ–°ä¿¡æ¯ï¼Œç»“æœç³»ç»Ÿä¸é™äºä»è®­ç»ƒæ•°æ®ä¸­æå–çš„çŸ¥è¯†ï¼Œä½†å¯ä»¥å¿«é€Ÿè½»æ¾åœ°æ›´æ–°è€Œæ— éœ€å¾®è°ƒã€‚æ­¤å¤–ï¼Œè¿™å¤§å¤§é™ä½äº†æé«˜æ¨¡å‹å›¾åƒåˆ†ç±»æ€§èƒ½çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶å¢å¼ºäº†æ¨¡å‹å¯¹æœªè®­ç»ƒçš„æ–°è§†è§‰é¢†åŸŸå’Œä»»åŠ¡çš„çŸ¥è¯†ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10834v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„è·¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶å—é™äºé¢„è®­ç»ƒæ•°æ®ï¼Œéœ€å¤§é‡å¾®è°ƒæ›´æ–°ã€‚è¿‘æœŸç ”ç©¶é€šè¿‡æä¾›æ¼”ç¤ºä¾‹å­ä¸Šä¸‹æ–‡æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæ¢ç´¢äº†è¯­å¢ƒå­¦ä¹ ï¼ˆICLï¼‰çš„æ½œåŠ›ï¼Œå¤šæ¨¡æ€æ˜¾ç¤ºç›¸æ¯”å°‘æ¨¡æ€æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹å–„ã€‚ç„¶è€Œä¾èµ–ä¼—å¤šæ¼”ç¤ºä¾‹å­ä»¥åŠæœ‰é™çš„MLLMè¯­å¢ƒçª—å£å­˜åœ¨æ˜æ˜¾éšœç¢ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ç§æ–°å‹æ–¹æ³•Visual RAGæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†MLLMä»è¯­å¢ƒä¸­å­¦ä¹ çš„èƒ½åŠ›ä¸æ£€ç´¢æœºåˆ¶ã€‚è¯¥æ–¹æ³•æ ¸å¿ƒåœ¨äºä»…é€‰æ‹©å¯¹æŸ¥è¯¢æœ€ç›¸å…³çš„æ¼”ç¤ºä¾‹å­æ¥æ‰©å……MLLMçš„çŸ¥è¯†ï¼Œæ¨åŠ¨å…¶é€šè¿‡ç±»æ¯”å­¦ä¹ ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ–°ä¿¡æ¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€æä¾›ï¼Œä½¿å¾—ç³»ç»Ÿä¸å†å±€é™äºä»è®­ç»ƒæ•°æ®ä¸­æå–çš„çŸ¥è¯†ï¼Œå¯å¿«é€Ÿè½»æ¾æ›´æ–°è€Œæ— éœ€å¾®è°ƒã€‚æ­¤å¤–ï¼Œè¿™å¤§å¤§é™ä½äº†æ”¹è¿›æ¨¡å‹å›¾åƒåˆ†ç±»æ€§èƒ½çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶æ‰©å……äº†æ¨¡å‹çŸ¥è¯†åˆ°æœªè®­ç»ƒçš„æ–°è§†è§‰é¢†åŸŸå’Œä»»åŠ¡ã€‚åœ¨æ¶µç›–å¤šä¸ªé¢†åŸŸå’Œå›¾åƒåˆ†ç±»ä»»åŠ¡çš„å…«ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œæå‡ºçš„Visual RAGèƒ½è·å¾—ç›¸è¿‘æˆ–æ›´é«˜çš„å‡†ç¡®ç‡ï¼ˆå¹³å‡æé«˜çº¦+2%ï¼‰ï¼ŒåŒæ—¶ä½¿ç”¨æ›´å°‘çš„æ¼”ç¤ºä¾‹å­ï¼ˆå¹³å‡ä»…ä½¿ç”¨çº¦23%ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä½†éœ€å¤§é‡å¾®è°ƒæ›´æ–°ã€‚</li>
<li>è¯­å¢ƒå­¦ä¹ ï¼ˆICLï¼‰é€šè¿‡æä¾›æ¼”ç¤ºä¾‹å­ä¸Šä¸‹æ–‡æ¥å…‹æœæŒ‘æˆ˜ï¼Œä½†ä¾èµ–ä¼—å¤šæ¼”ç¤ºä¾‹å­å’Œæœ‰é™çš„è¯­å¢ƒçª—å£å­˜åœ¨éšœç¢ã€‚</li>
<li>Visual RAGæ–¹æ³•ç»“åˆäº†MLLMä»è¯­å¢ƒä¸­å­¦ä¹ çš„èƒ½åŠ›ä¸æ£€ç´¢æœºåˆ¶ï¼Œé€šè¿‡é€‰æ‹©æœ€ç›¸å…³çš„æ¼”ç¤ºä¾‹å­æ¥æ‰©å……çŸ¥è¯†ã€‚</li>
<li>Visual RAGèƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨å¤§é‡æ¼”ç¤ºä¾‹å­çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹æ€§èƒ½ï¼Œé™ä½è®¡ç®—æˆæœ¬å¹¶æ‰©å……æ¨¡å‹çŸ¥è¯†åˆ°æ–°çš„è§†è§‰é¢†åŸŸå’Œä»»åŠ¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒVisual RAGç›¸æ¯”æœ€æ–°æŠ€æœ¯è·å¾—ç›¸è¿‘æˆ–æ›´é«˜çš„å‡†ç¡®ç‡ã€‚</li>
<li>Visual RAGåŠ¨æ€åˆ©ç”¨æ–°ä¿¡æ¯æé«˜æ¨¡å‹é€‚åº”æ€§ï¼Œæ— éœ€å¾®è°ƒå³å¯å¿«é€Ÿæ›´æ–°çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.10834v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.10834v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.10834v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.10834v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.10834v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Class-Incremental-Fault-Diagnosis-under-Limited-Fault-Data-via-Supervised-Contrastive-Knowledge-Distillation"><a href="#Class-Incremental-Fault-Diagnosis-under-Limited-Fault-Data-via-Supervised-Contrastive-Knowledge-Distillation" class="headerlink" title="Class Incremental Fault Diagnosis under Limited Fault Data via   Supervised Contrastive Knowledge Distillation"></a>Class Incremental Fault Diagnosis under Limited Fault Data via   Supervised Contrastive Knowledge Distillation</h2><p><strong>Authors:Hanrong Zhang, Yifei Yao, Zixuan Wang, Jiayuan Su, Mengxuan Li, Peng Peng, Hongwei Wang</strong></p>
<p>Class-incremental fault diagnosis requires a model to adapt to new fault classes while retaining previous knowledge. However, limited research exists for imbalanced and long-tailed data. Extracting discriminative features from few-shot fault data is challenging, and adding new fault classes often demands costly model retraining. Moreover, incremental training of existing methods risks catastrophic forgetting, and severe class imbalance can bias the modelâ€™s decisions toward normal classes. To tackle these issues, we introduce a Supervised Contrastive knowledge distiLlation for class Incremental Fault Diagnosis (SCLIFD) framework proposing supervised contrastive knowledge distillation for improved representation learning capability and less forgetting, a novel prioritized exemplar selection method for sample replay to alleviate catastrophic forgetting, and the Random Forest Classifier to address the class imbalance. Extensive experimentation on simulated and real-world industrial datasets across various imbalance ratios demonstrates the superiority of SCLIFD over existing approaches. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/Zhang-Henry/SCLIFD_TII">https://github.com/Zhang-Henry/SCLIFD_TII</a>. </p>
<blockquote>
<p>ç±»å¢é‡æ•…éšœè¯Šæ–­éœ€è¦æ¨¡å‹åœ¨é€‚åº”æ–°æ•…éšœç±»çš„åŒæ—¶ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚ç„¶è€Œï¼Œå¯¹äºä¸å¹³è¡¡å’Œé•¿å°¾æ•°æ®çš„ç ”ç©¶æœ‰é™ã€‚ä»å°‘é‡æ•…éšœæ•°æ®ä¸­æå–åˆ¤åˆ«ç‰¹å¾å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæ·»åŠ æ–°æ•…éšœç±»é€šå¸¸éœ€è¦æ˜‚è´µçš„æ¨¡å‹é‡æ–°è®­ç»ƒã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•çš„å¢é‡è®­ç»ƒå­˜åœ¨ç¾éš¾æ€§é—å¿˜çš„é£é™©ï¼Œä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡å¯èƒ½ä½¿æ¨¡å‹å†³ç­–åå‘æ­£å¸¸ç±»åˆ«ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºç±»å¢é‡æ•…éšœè¯Šæ–­çš„ç›‘ç£å¯¹æ¯”çŸ¥è¯†è’¸é¦ï¼ˆSCLIFDï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æå‡ºç›‘ç£å¯¹æ¯”çŸ¥è¯†è’¸é¦ï¼Œä»¥æé«˜è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›å¹¶å‡å°‘é—å¿˜ï¼Œä¸€ç§ç”¨äºæ ·æœ¬å›æ”¾çš„æ–°å‹ä¼˜å…ˆçº§ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œä»¥ç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œä»¥åŠéšæœºæ£®æ—åˆ†ç±»å™¨æ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®å·¥ä¸šæ•°æ®é›†ä¸Šè¿›è¡Œçš„å„ç§ä¸å¹³è¡¡æ¯”ç‡çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSCLIFDä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/Zhang-Henry/SCLIFD_TII%E3%80%82">https://github.com/Zhang-Henry/SCLIFD_TIIã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09525v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç±»å¢é‡æ•…éšœè¯Šæ–­çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºSCLIFDçš„æ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å¯¹æ¯”çŸ¥è¯†è’¸é¦æé«˜è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›å¹¶å‡å°‘é—å¿˜ï¼Œé‡‡ç”¨æ–°å‹ä¼˜å…ˆç¤ºä¾‹é€‰æ‹©æ–¹æ³•è¿›è¡Œæ ·æœ¬å›æ”¾ä»¥ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶ä½¿ç”¨éšæœºæ£®æ—åˆ†ç±»å™¨è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒSCLIFDåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®å·¥ä¸šæ•°æ®é›†ä¸Šï¼Œåœ¨ä¸åŒä¸å¹³è¡¡æ¯”ç‡ä¸‹å‡è¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SCLIFDæ¡†æ¶è¢«æå‡ºç”¨äºè§£å†³ç±»å¢é‡æ•…éšœè¯Šæ–­ä¸­çš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé€‚åº”æ–°æ•…éšœç±»å¹¶ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚</li>
<li>åœ¨é¢ä¸´ä¸å¹³è¡¡å’Œé•¿å°¾æ•°æ®çš„é—®é¢˜æ—¶ï¼Œæå–å°‘æ•°æ•…éšœæ•°æ®çš„åˆ¤åˆ«ç‰¹å¾å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å¢é‡è®­ç»ƒç°æœ‰æ–¹æ³•å­˜åœ¨ç¾éš¾æ€§é—å¿˜çš„é£é™©ã€‚</li>
<li>SCLIFDé‡‡ç”¨ç›‘ç£å¯¹æ¯”çŸ¥è¯†è’¸é¦ï¼Œæé«˜è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›å¹¶å‡å°‘é—å¿˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¼˜å…ˆç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºæ ·æœ¬å›æ”¾ï¼Œä»¥å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>ä½¿ç”¨éšæœºæ£®æ—åˆ†ç±»å™¨æ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä»¥æ›´å‡†ç¡®åœ°è¯†åˆ«æ•…éšœç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.09525v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.09525v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.09525v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.09525v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.09525v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.09525v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.09525v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.09525v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.09525v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="IDEA-Image-Description-Enhanced-CLIP-Adapter"><a href="#IDEA-Image-Description-Enhanced-CLIP-Adapter" class="headerlink" title="IDEA: Image Description Enhanced CLIP-Adapter"></a>IDEA: Image Description Enhanced CLIP-Adapter</h2><p><strong>Authors:Zhipeng Ye, Feng Jiang, Qiufeng Wang, Kaizhu Huang, Jiaqi Huang</strong></p>
<p>CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the modelâ€™s performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named â€œIMD-11â€. Our code and data are released at <a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA">https://github.com/FourierAI/IDEA</a>. </p>
<blockquote>
<p>CLIPï¼ˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼‰åœ¨æ¨¡å¼è¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚å°†CLIPè¿ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼ˆä¾‹å¦‚é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬åˆ†ç±»ï¼‰æ˜¯å¤šæ¨¡æ€å­¦ä¹ çš„çƒ­é—¨è¯é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºæ–‡æœ¬æç¤ºå­¦ä¹ æˆ–è§†è§‰é€‚é…å™¨è°ƒæ•´ï¼Œè€Œæ²¡æœ‰å……åˆ†åˆ©ç”¨å›¾åƒ-æ–‡æœ¬å¯¹ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯å’Œå…³è”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å›¾åƒæè¿°å¢å¼ºCLIPé€‚é…å™¨ï¼ˆIDEAï¼‰æ–¹æ³•ï¼Œå°†CLIPé€‚åº”äºå°æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å›¾åƒçš„è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬æè¿°æ¥æ•è·ç²¾ç»†ç‰¹å¾ã€‚IDEAæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„CLIPæ–¹æ³•ï¼Œå®ƒåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä»¥ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ï¼Œç”šè‡³è¶…è¿‡å®ƒä»¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯è®­ç»ƒçš„IDEAï¼ˆT-IDEAï¼‰ï¼Œå®ƒé€šè¿‡æ·»åŠ ä¸¤ä¸ªè½»é‡çº§çš„å¯å­¦ä¹ ç»„ä»¶ï¼ˆå³æŠ•å½±å™¨å’Œå¯å­¦ä¹ æ½œåœ¨ç©ºé—´ï¼‰æ¥æ‰©å±•IDEAï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨11ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœã€‚ä½œä¸ºé‡è¦è´¡çŒ®ä¹‹ä¸€ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†Llamaæ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„æµç¨‹æ¥ç”Ÿæˆè¿™11ä¸ªæ•°æ®é›†çš„å›¾åƒæ–‡æœ¬æè¿°ï¼Œå…±ç”Ÿæˆäº†1,637,795ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹ï¼Œåä¸ºâ€œIMD-11â€ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/FourierAI/IDEA%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/FourierAI/IDEAä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08816v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CLIPåœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­å–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨ä¸‹æ¸¸ä»»åŠ¡å¦‚é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬åˆ†ç±»ä¸Šçš„è½¬ç§»åº”ç”¨ä»æ˜¯ç ”ç©¶çƒ­ç‚¹ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›¾åƒæè¿°å¢å¼ºçš„CLIPé€‚é…å™¨ï¼ˆIDEAï¼‰æ–¹æ³•ï¼Œç”¨äºé€‚åº”å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰ç‰¹å¾å’Œå›¾åƒæ–‡æœ¬æè¿°ï¼Œæ•è·ç²¾ç»†ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†å¯è®­ç»ƒçš„IDEAï¼ˆT-IDEAï¼‰ï¼Œé€šè¿‡æ·»åŠ ä¸¤ä¸ªè½»é‡çº§çš„å­¦ä¹ ç»„ä»¶ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨11ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°ç»“æœã€‚è¯¥ç ”ç©¶ä½¿ç”¨Llamaæ¨¡å‹ç”Ÿæˆå›¾åƒæ–‡æœ¬æè¿°ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹æ•°æ®åº“IMD-11ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPåœ¨æ¨¡å¼è¯†åˆ«å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å¾ˆå¤§æˆåŠŸã€‚</li>
<li>å°†CLIPè½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬åˆ†ç±»ï¼‰æ˜¯å½“å‰çš„çƒ­é—¨è¯é¢˜ã€‚</li>
<li>IDEAæ–¹æ³•é€šè¿‡ç»“åˆè§†è§‰ç‰¹å¾å’Œå›¾åƒæ–‡æœ¬æè¿°ï¼Œé€‚åº”å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>IDEAæ˜¯ä¸€ç§é’ˆå¯¹CLIPçš„æ— è®­ç»ƒæ–¹æ³•ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯ä¸æˆ–è¶…è¿‡ç°æœ‰æœ€æ–°æ¨¡å‹ã€‚</li>
<li>T-IDEAé€šè¿‡æ·»åŠ å­¦ä¹ ç»„ä»¶è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœã€‚</li>
<li>ç ”ç©¶äººå‘˜ä½¿ç”¨Llamaæ¨¡å‹ç”Ÿæˆäº†å¤§è§„æ¨¡çš„å›¾åƒæ–‡æœ¬å¯¹æ•°æ®åº“IMD-11ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.08816v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.08816v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ValuesRAG-Enhancing-Cultural-Alignment-Through-Retrieval-Augmented-Contextual-Learning"><a href="#ValuesRAG-Enhancing-Cultural-Alignment-Through-Retrieval-Augmented-Contextual-Learning" class="headerlink" title="ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented   Contextual Learning"></a>ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented   Contextual Learning</h2><p><strong>Authors:Wonduk Seo, Zonghao Yuan, Yi Bu</strong></p>
<p>Cultural values alignment in Large Language Models (LLMs) is a critical challenge due to their tendency to embed Western-centric biases from training data, leading to misrepresentations and fairness issues in cross-cultural contexts. Recent approaches, such as role-assignment and few-shot learning, often struggle with reliable cultural alignment as they heavily rely on pre-trained knowledge, lack scalability, and fail to capture nuanced cultural values effectively. To address these issues, we propose ValuesRAG, a novel and effective framework that applies Retrieval-Augmented Generation (RAG) with In-Context Learning (ICL) to integrate cultural and demographic knowledge dynamically during text generation. Leveraging the World Values Survey (WVS) dataset, ValuesRAG first generates summaries of values for each individual. Subsequently, we curate several representative regional datasets to serve as test datasets and retrieve relevant summaries of values based on demographic features, followed by a reranking step to select the top-k relevant summaries. ValuesRAG consistently outperforms baseline methods, both in the main experiment and in the ablation study where only the values summary was provided. Notably, ValuesRAG demonstrates an accuracy of 21% improvement over other baseline methods, highlighting its potential to foster culturally aligned AI systems and enhance the inclusivity of AI-driven applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ–‡åŒ–ä»·å€¼è§‚å¯¹é½æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¾€å¾€åµŒå…¥ä»è®­ç»ƒæ•°æ®ä¸­è·å¾—çš„è¥¿æ–¹ä¸­å¿ƒåè§ï¼Œå¯¼è‡´è·¨æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¯¯è¡¨å¾å’Œå…¬å¹³æ€§é—®é¢˜ã€‚æœ€è¿‘çš„æ–¹æ³•ï¼Œå¦‚è§’è‰²åˆ†é…å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œé€šå¸¸éš¾ä»¥å¯é åœ°è¿›è¡Œæ–‡åŒ–å¯¹é½ï¼Œå› ä¸ºå®ƒä»¬ä¸¥é‡ä¾èµ–äºé¢„è®­ç»ƒçŸ¥è¯†ï¼Œç¼ºä¹å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”æœªèƒ½æœ‰æ•ˆåœ°æ•æ‰å¾®å¦™çš„æ–‡åŒ–ä»·å€¼è§‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ValuesRAGï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–æœ‰æ•ˆçš„æ¡†æ¶ï¼Œå®ƒåº”ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€æ•´åˆæ–‡åŒ–å’Œäººå£ç»Ÿè®¡æ•°æ®çŸ¥è¯†ã€‚å€ŸåŠ©ä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥ï¼ˆWVSï¼‰æ•°æ®é›†ï¼ŒValuesRAGé¦–å…ˆä¸ºæ¯ä¸ªäººç”Ÿæˆä»·å€¼è§‚æ‘˜è¦ã€‚éšåï¼Œæˆ‘ä»¬ç­–åˆ’äº†å‡ ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„åŒºåŸŸæ•°æ®é›†ä½œä¸ºæµ‹è¯•æ•°æ®é›†ï¼Œå¹¶åŸºäºäººå£ç»Ÿè®¡ç‰¹å¾æ£€ç´¢ç›¸å…³çš„ä»·å€¼è§‚æ‘˜è¦ï¼Œç„¶åè¿›è¡Œé‡æ–°æ’åºæ­¥éª¤ä»¥é€‰æ‹©å‰kä¸ªç›¸å…³æ‘˜è¦ã€‚ValuesRAGåœ¨ä¸»å®éªŒå’Œä»…æä¾›ä»·å€¼è§‚æ‘˜è¦çš„æ¶ˆèç ”ç©¶ä¸­å‡ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒValuesRAGçš„å‡†ç¡®ç‡è¾ƒå…¶ä»–åŸºå‡†æ–¹æ³•æé«˜äº†21%ï¼Œè¿™å‡¸æ˜¾äº†å…¶åœ¨ä¿ƒè¿›æ–‡åŒ–å¯¹é½çš„AIç³»ç»Ÿå’Œæé«˜AIé©±åŠ¨åº”ç”¨çš„åŒ…å®¹æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01031v2">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡åŒ–ä»·å€¼è§‚å¯¹é½æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå› è®­ç»ƒæ•°æ®ä¸­çš„è¥¿æ–¹ä¸­å¿ƒåè§å¯¼è‡´è·¨æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¯¯è¡¨ç¤ºå’Œå…¬å¹³é—®é¢˜ã€‚ValuesRAGæ¡†æ¶é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ç»“åˆï¼ŒåŠ¨æ€æ•´åˆæ–‡åŒ–å’Œäººå£ç»Ÿè®¡çŸ¥è¯†æ¥è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œè§£å†³è¿™ä¸€é—®é¢˜ã€‚ValuesRAGåˆ©ç”¨ä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥ï¼ˆWVSï¼‰æ•°æ®é›†ï¼Œé¦–å…ˆä¸ºä¸ªäººç”Ÿæˆä»·å€¼è§‚æ‘˜è¦ï¼Œç„¶ååŸºäºäººå£ç»Ÿè®¡ç‰¹å¾æ£€ç´¢ç›¸å…³ä»·å€¼è§‚æ‘˜è¦ï¼Œå¹¶è¿›è¡Œé‡æ–°æ’åºã€‚ValuesRAGåœ¨ä¸»è¦å®éªŒå’Œä»…æä¾›ä»·å€¼è§‚æ‘˜è¦çš„æ¶ˆèç ”ç©¶ä¸­å‡è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜21%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä¿ƒè¿›æ–‡åŒ–å¯¹é½çš„AIç³»ç»Ÿå’Œæé«˜AIåº”ç”¨ç¨‹åºçš„åŒ…å®¹æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡åŒ–ä»·å€¼è§‚å¯¹é½æ–¹é¢å…·æœ‰æŒ‘æˆ˜ï¼Œå› è®­ç»ƒæ•°æ®ä¸­çš„è¥¿æ–¹ä¸­å¿ƒåè§å¯¼è‡´é—®é¢˜ã€‚</li>
<li>ValuesRAGæ¡†æ¶é€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼ŒåŠ¨æ€æ•´åˆæ–‡åŒ–å’Œäººå£ç»Ÿè®¡çŸ¥è¯†æ¥è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>ValuesRAGåˆ©ç”¨ä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥ï¼ˆWVSï¼‰æ•°æ®é›†ç”Ÿæˆä¸ªäººä»·å€¼è§‚æ‘˜è¦ã€‚</li>
<li>ValuesRAGåŸºäºäººå£ç»Ÿè®¡ç‰¹å¾æ£€ç´¢ç›¸å…³ä»·å€¼è§‚æ‘˜è¦ï¼Œå¹¶è¿›è¡Œé‡æ–°æ’åºã€‚</li>
<li>ValuesRAGåœ¨å®éªŒä¸­è¡¨ç°ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜21%ã€‚</li>
<li>ValuesRAGæœ‰åŠ©äºä¿ƒè¿›æ–‡åŒ–å¯¹é½çš„AIç³»ç»Ÿçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.01031v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.01031v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.01031v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2501.01031v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="3DGS-CD-3D-Gaussian-Splatting-based-Change-Detection-for-Physical-Object-Rearrangement"><a href="#3DGS-CD-3D-Gaussian-Splatting-based-Change-Detection-for-Physical-Object-Rearrangement" class="headerlink" title="3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical   Object Rearrangement"></a>3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical   Object Rearrangement</h2><p><strong>Authors:Ziqi Lu, Jianbo Ye, John Leonard</strong></p>
<p>We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for detecting physical object rearrangements in 3D scenes. Our approach estimates 3D object-level changes by comparing two sets of unaligned images taken at different times. Leveraging 3DGSâ€™s novel view rendering and EfficientSAMâ€™s zero-shot segmentation capabilities, we detect 2D object-level changes, which are then associated and fused across views to estimate 3D change masks and object transformations. Our method can accurately identify changes in cluttered environments using sparse (as few as one) post-change images within as little as 18s. It does not rely on depth input, user instructions, pre-defined object classes, or object models â€“ An object is recognized simply if it has been re-arranged. Our approach is evaluated on both public and self-collected real-world datasets, achieving up to 14% higher accuracy and three orders of magnitude faster performance compared to the state-of-the-art radiance-field-based change detection method. This significant performance boost enables a broad range of downstream applications, where we highlight three key use cases: object reconstruction, robot workspace reset, and 3DGS model update. Our code and data will be made available at <a target="_blank" rel="noopener" href="https://github.com/520xyxyzq/3DGS-CD">https://github.com/520xyxyzq/3DGS-CD</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†3DGS-CDæ–¹æ³•ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºä¸‰ç»´é«˜æ–¯æ’å€¼ï¼ˆ3DGSï¼‰çš„ç‰©ç†å¯¹è±¡é‡æ’æ£€æµ‹çš„ä¸‰ç»´åœºæ™¯æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ¯”è¾ƒåœ¨ä¸åŒæ—¶é—´æ‹æ‘„çš„ä¸å¯¹é½çš„ä¸¤ç»„å›¾åƒæ¥ä¼°è®¡ä¸‰ç»´ç‰©ä½“çº§åˆ«çš„å˜åŒ–ã€‚æˆ‘ä»¬åˆ©ç”¨3DGSçš„æ–°å‹è§†å›¾æ¸²æŸ“å’ŒEfficientSAMçš„é›¶æ ·æœ¬åˆ†å‰²èƒ½åŠ›ï¼Œæ£€æµ‹äºŒç»´ç‰©ä½“çº§åˆ«çš„å˜åŒ–ï¼Œç„¶ååœ¨ä¸åŒè§†å›¾ä¸­å…³è”å’Œèåˆè¿™äº›å˜åŒ–ï¼Œä»¥ä¼°è®¡ä¸‰ç»´å˜åŒ–æ©è†œå’Œç‰©ä½“å˜æ¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨æ‚ä¹±çš„ç¯å¢ƒä¸­å‡†ç¡®åœ°è¯†åˆ«å˜åŒ–ï¼Œä»…ä½¿ç”¨å°‘æ•°ï¼ˆä¸€å¼ ï¼‰å˜åŒ–åçš„å›¾åƒåœ¨çŸ­çŸ­18ç§’å†…å³å¯å®Œæˆã€‚å®ƒä¸ä¾èµ–äºæ·±åº¦è¾“å…¥ã€ç”¨æˆ·æŒ‡ä»¤ã€é¢„å®šä¹‰çš„å¯¹è±¡ç±»åˆ«æˆ–å¯¹è±¡æ¨¡å‹â€”â€”åªè¦å¯¹è±¡è¢«é‡æ–°æ’åˆ—ï¼Œå®ƒå°±èƒ½è¢«è¯†åˆ«ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¬å…±å’Œè‡ªè¡Œæ”¶é›†çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸æœ€æ–°çš„åŸºäºè¾å°„åœºçš„å˜åŒ–æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†é«˜è¾¾14%çš„å‡†ç¡®æ€§å’Œä¸‰ä¸ªæ•°é‡çº§çš„æ€§èƒ½ã€‚è¿™ä¸€æ˜¾è‘—çš„æ€§èƒ½æå‡ä¸ºä¸‹æ¸¸åº”ç”¨æä¾›äº†å¹¿æ³›çš„å¯èƒ½æ€§ï¼Œæˆ‘ä»¬é‡ç‚¹ä»‹ç»äº†ä¸‰ä¸ªå…³é”®ç”¨ä¾‹ï¼šå¯¹è±¡é‡å»ºã€æœºå™¨äººå·¥ä½œç©ºé—´é‡ç½®å’Œ3DGSæ¨¡å‹æ›´æ–°ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/520xyxyzq/3DGS-CD%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/520xyxyzq/3DGS-CDä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03706v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäº3Dé«˜æ–¯æ¸²æŸ“ï¼ˆ3DGSï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹ä¸‰ç»´åœºæ™¯ä¸­çš„ç‰©ä½“é‡æ–°æ’åˆ—ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¯”è¾ƒä¸åŒæ—¶é—´æ‹æ‘„çš„ä¸¤ç»„æœªå¯¹é½å›¾åƒæ¥ä¼°è®¡ä¸‰ç»´ç‰©ä½“çº§åˆ«çš„å˜åŒ–ã€‚åˆ©ç”¨3DGSçš„æ–°å‹è§†å›¾æ¸²æŸ“å’ŒEfficientSAMçš„é›¶æ ·æœ¬åˆ†å‰²èƒ½åŠ›ï¼Œæ£€æµ‹äºŒç»´ç‰©ä½“çº§åˆ«çš„å˜åŒ–ï¼Œå¹¶åœ¨ä¸åŒè§†å›¾ä¸­å…³è”å’Œèåˆè¿™äº›å˜åŒ–ï¼Œä»¥ä¼°è®¡ä¸‰ç»´å˜åŒ–æ©è†œå’Œç‰©ä½“å˜æ¢ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ‚ä¹±çš„ç¯å¢ƒä¸­å‡†ç¡®è¯†åˆ«ç‰©ä½“å˜åŒ–ï¼Œä»…ä½¿ç”¨å°‘æ•°å‡ å¼ ï¼ˆç”šè‡³ä¸€å¼ ï¼‰å˜åŒ–åçš„å›¾åƒåœ¨çŸ­çŸ­18ç§’å†…å®Œæˆã€‚å®ƒä¸ä¾èµ–äºæ·±åº¦è¾“å…¥ã€ç”¨æˆ·æŒ‡ä»¤ã€é¢„å®šä¹‰ç‰©ä½“ç±»åˆ«æˆ–ç‰©ä½“æ¨¡å‹ï¼Œåªè¦ç‰©ä½“è¢«é‡æ–°æ’åˆ—å³å¯è¯†åˆ«ã€‚åœ¨å…¬å…±å’Œè‡ªè¡Œæ”¶é›†çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸åŸºäºè¾å°„åœºçš„å˜åŒ–æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å‡†ç¡®æ€§æé«˜é«˜è¾¾14%ï¼Œæ€§èƒ½æå‡ä¸‰ä¸ªæ•°é‡çº§ã€‚æ˜¾è‘—çš„æ€§èƒ½æå‡ä½¿å¾—ä¸‹æ¸¸åº”ç”¨å¹¿æ³›ï¼Œæœ¬æ–‡çªå‡ºäº†ä¸‰ä¸ªå…³é”®åº”ç”¨åœºæ™¯ï¼šç‰©ä½“é‡å»ºã€æœºå™¨äººå·¥ä½œç©ºé—´é‡ç½®å’Œ3DGSæ¨¡å‹æ›´æ–°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºé¦–ä¸ªåŸºäº3Dé«˜æ–¯æ¸²æŸ“ï¼ˆ3DGSï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹ä¸‰ç»´åœºæ™¯ä¸­çš„ç‰©ä½“é‡æ–°æ’åˆ—ã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒä¸åŒæ—¶é—´çš„æœªå¯¹é½å›¾åƒé›†æ¥ä¼°è®¡ä¸‰ç»´ç‰©ä½“çº§åˆ«çš„å˜åŒ–ã€‚</li>
<li>åˆ©ç”¨3DGSçš„æ–°å‹è§†å›¾æ¸²æŸ“å’ŒEfficientSAMçš„é›¶æ ·æœ¬åˆ†å‰²èƒ½åŠ›ï¼Œæ£€æµ‹äºŒç»´ç‰©ä½“çº§åˆ«çš„å˜åŒ–ã€‚</li>
<li>æ–¹æ³•èƒ½åœ¨æ‚ä¹±ç¯å¢ƒä¸­å‡†ç¡®å·¥ä½œï¼Œä»…ä½¿ç”¨å°‘é‡æˆ–ä¸€å¼ å˜åŒ–åçš„å›¾åƒã€‚</li>
<li>ä¸ä¾èµ–äºæ·±åº¦è¾“å…¥ã€ç”¨æˆ·æŒ‡ä»¤ç­‰ï¼Œå…·æœ‰çµæ´»æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.03706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2411.03706v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2411.03706v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2411.03706v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2411.03706v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2411.03706v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2411.03706v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2411.03706v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Domain-Adaptation-for-Learned-Image-Compression"><a href="#Few-Shot-Domain-Adaptation-for-Learned-Image-Compression" class="headerlink" title="Few-Shot Domain Adaptation for Learned Image Compression"></a>Few-Shot Domain Adaptation for Learned Image Compression</h2><p><strong>Authors:Tianyu Zhang, Haotian Zhang, Yuqi Li, Li Li, Dong Liu</strong></p>
<p>Learned image compression (LIC) has achieved state-of-the-art rate-distortion performance, deemed promising for next-generation image compression techniques. However, pre-trained LIC models usually suffer from significant performance degradation when applied to out-of-training-domain images, implying their poor generalization capabilities. To tackle this problem, we propose a few-shot domain adaptation method for LIC by integrating plug-and-play adapters into pre-trained models. Drawing inspiration from the analogy between latent channels and frequency components, we examine domain gaps in LIC and observe that out-of-training-domain images disrupt pre-trained channel-wise decomposition. Consequently, we introduce a method for channel-wise re-allocation using convolution-based adapters and low-rank adapters, which are lightweight and compatible to mainstream LIC schemes. Extensive experiments across multiple domains and multiple representative LIC schemes demonstrate that our method significantly enhances pre-trained models, achieving comparable performance to H.266&#x2F;VVC intra coding with merely 25 target-domain samples. Additionally, our method matches the performance of full-model finetune while transmitting fewer than $2%$ of the parameters. </p>
<blockquote>
<p>å­¦ä¹ å›¾åƒå‹ç¼©ï¼ˆLICï¼‰å·²ç»è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é€Ÿç‡å¤±çœŸæ€§èƒ½ï¼Œè¢«è®¤ä¸ºæ˜¯ä¸‹ä¸€ä»£å›¾åƒå‹ç¼©æŠ€æœ¯çš„æœ‰åŠ›å€™é€‰è€…ã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒçš„LICæ¨¡å‹åœ¨åº”ç”¨äºè®­ç»ƒåŸŸå¤–çš„å›¾åƒæ—¶é€šå¸¸ä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™è¡¨æ˜å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å°†å³æ’å³ç”¨é€‚é…å™¨é›†æˆåˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œä¸ºLICæå‡ºäº†ä¸€ç§å°æ ·æœ¬åŸŸé€‚åº”æ–¹æ³•ã€‚æˆ‘ä»¬ä»æ½œåœ¨é€šé“å’Œé¢‘ç‡æˆåˆ†ä¹‹é—´çš„ç±»æ¯”ä¸­æ±²å–çµæ„Ÿï¼Œç ”ç©¶LICä¸­çš„åŸŸå·®è·ï¼Œå¹¶è§‚å¯Ÿåˆ°è®­ç»ƒåŸŸå¤–çš„å›¾åƒä¼šç ´åé¢„è®­ç»ƒçš„é€šé“åˆ†è§£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå·ç§¯é€‚é…å™¨å’Œä½ç§©é€‚é…å™¨çš„é€šé“é‡æ–°åˆ†é…æ–¹æ³•ï¼Œå®ƒä»¬æ—¢è½»ä¾¿åˆå…¼å®¹ä¸»æµLICæ–¹æ¡ˆã€‚åœ¨å¤šä¸ªé¢†åŸŸå’Œå¤šä¸ªä»£è¡¨æ€§LICæ–¹æ¡ˆçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»…ä½¿ç”¨25ä¸ªç›®æ ‡åŸŸæ ·æœ¬å³å¯å®ç°ä¸H.266&#x2F;VVCå¸§å†…ç¼–ç ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¼ è¾“å°‘äº2%çš„å‚æ•°æ—¶ï¼Œå°±èƒ½è¾¾åˆ°ä¸å…¨æ¨¡å‹å¾®è°ƒç›¸åŒ¹é…çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11111v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒå›¾åƒå‹ç¼©ï¼ˆLICï¼‰æ¨¡å‹åœ¨åº”ç”¨äºè®­ç»ƒåŸŸå¤–å›¾åƒæ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå·ç§¯é€‚é…å™¨å’Œä½ç§©é€‚é…å™¨çš„å°‘é‡åŸŸé€‚åº”æ–¹æ³•ç”¨äºLICæ¨¡å‹ï¼Œå®ç°é€šé“çº§é‡æ–°åˆ†é…ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä»…éœ€ä½¿ç”¨å°‘é‡ç›®æ ‡åŸŸæ ·æœ¬å³å¯æ˜¾è‘—æé«˜é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œè¾¾åˆ°ä¸H.266&#x2F;VVCå¸§å†…ç¼–ç ç›¸è¿‘çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæ­¤æ–¹æ³•èƒ½åœ¨ä»…ä¼ è¾“å°‘é‡å‚æ•°çš„æƒ…å†µä¸‹åŒ¹é…å…¨æ¨¡å‹å¾®è°ƒçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒå›¾åƒå‹ç¼©ï¼ˆLICï¼‰æ¨¡å‹åœ¨åº”ç”¨äºè®­ç»ƒåŸŸå¤–å›¾åƒæ—¶å­˜åœ¨æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åŸºäºå·ç§¯é€‚é…å™¨å’Œä½ç§©é€‚é…å™¨çš„å°‘é‡åŸŸé€‚åº”æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>é€šè¿‡å®ç°é€šé“çº§é‡æ–°åˆ†é…ï¼Œæé«˜äº†LICæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•ä»…éœ€ä½¿ç”¨å°‘é‡ç›®æ ‡åŸŸæ ·æœ¬å³å¯æ˜¾è‘—æé«˜é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•è¾¾åˆ°ä¸H.266&#x2F;VVCå¸§å†…ç¼–ç ç›¸è¿‘çš„æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä»…ä¼ è¾“å°‘é‡å‚æ•°çš„æƒ…å†µä¸‹åŒ¹é…å…¨æ¨¡å‹å¾®è°ƒçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2409.11111v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2409.11111v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2409.11111v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2409.11111v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2409.11111v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2409.11111v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2409.11111v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VLM-Agents-Generate-Their-Own-Memories-Distilling-Experience-into-Embodied-Programs-of-Thought"><a href="#VLM-Agents-Generate-Their-Own-Memories-Distilling-Experience-into-Embodied-Programs-of-Thought" class="headerlink" title="VLM Agents Generate Their Own Memories: Distilling Experience into   Embodied Programs of Thought"></a>VLM Agents Generate Their Own Memories: Distilling Experience into   Embodied Programs of Thought</h2><p><strong>Authors:Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, Katerina Fragkiadaki</strong></p>
<p>Large-scale LLMs and VLMs excel at few-shot learning but require high-quality examples. We introduce In-Context Abstraction Learning (ICAL), which iteratively refines suboptimal trajectories into high-quality data with optimized actions and detailed reasoning. Given an inefficient demonstration, a VLM corrects actions and annotates causal relationships, object states, subgoals, and task-relevant visuals, forming â€œprograms of thought.â€ With human feedback, these programs are improved as the agent executes them in a similar environment. The resulting examples, used as prompt context or fine-tuning data, significantly boost decision-making while reducing human feedback needs. ICAL surpasses state-of-the-art in TEACh (dialogue-based instruction following), VisualWebArena (multimodal web agents), and Ego4D (egocentric video action anticipation). In TEACh, combining fine-tuning and retrieval on ICAL examples outperforms raw human demonstrations and expert examples, achieving a 17.5% increase in goal-condition success. In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success rate 1.6x over GPT-4V, while fine-tuning Qwen2-VL achieves a 2.8x improvement. In Ego4D, ICAL outperforms few-shot GPT-4V and remains competitive with supervised models. Overall, ICAL scales 2x better than raw human demonstrations and reduces manual prompt engineering. </p>
<blockquote>
<p>å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦é«˜è´¨é‡çš„ä¾‹å­ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸Šä¸‹æ–‡æŠ½è±¡å­¦ä¹ ï¼ˆICALï¼‰ï¼Œå®ƒèƒ½å¤Ÿè¿­ä»£åœ°ä¼˜åŒ–æ¬¡ä¼˜è½¨è¿¹ï¼Œå½¢æˆå…·æœ‰ä¼˜åŒ–åŠ¨ä½œå’Œè¯¦ç»†æ¨ç†çš„é«˜è´¨é‡æ•°æ®ã€‚å¯¹äºä½æ•ˆçš„æ¼”ç¤ºï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ä¼šçº æ­£åŠ¨ä½œï¼Œå¹¶æ³¨é‡Šå› æœå…³ç³»ã€å¯¹è±¡çŠ¶æ€ã€å­ç›®æ ‡å’Œä»»åŠ¡ç›¸å…³è§†è§‰ï¼Œå½¢æˆâ€œæ€ç»´ç¨‹åºâ€ã€‚éšç€ä»£ç†åœ¨ç±»ä¼¼ç¯å¢ƒä¸­æ‰§è¡Œè¿™äº›ç¨‹åºï¼Œè¿™äº›ç¨‹åºä¼šéšç€äººç±»åé¦ˆè€Œä¸æ–­æ”¹è¿›ã€‚è¿™äº›ç»“æœç¤ºä¾‹è¢«ç”¨ä½œæç¤ºä¸Šä¸‹æ–‡æˆ–å¾®è°ƒæ•°æ®ï¼Œå¯æ˜¾è‘—æé«˜å†³ç­–èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äººç±»åé¦ˆçš„éœ€æ±‚ã€‚ICALè¶…è¶Šäº†TEAChï¼ˆåŸºäºå¯¹è¯çš„æŒ‡ä»¤éµå¾ªï¼‰ã€VisualWebArenaï¼ˆå¤šæ¨¡å¼ç½‘ç»œä»£ç†ï¼‰å’ŒEgo4Dï¼ˆä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘åŠ¨ä½œé¢„æµ‹ï¼‰çš„æœ€æ–°æ°´å¹³ã€‚åœ¨TEAChä¸­ï¼Œç»“åˆå¾®è°ƒä¸åœ¨ICALç¤ºä¾‹ä¸Šçš„æ£€ç´¢ä¼˜äºåŸå§‹äººç±»æ¼”ç¤ºå’Œä¸“å®¶ç¤ºä¾‹ï¼Œç›®æ ‡æ¡ä»¶æˆåŠŸç‡æé«˜äº†17.5%ã€‚åœ¨VisualWebArenaä¸­ï¼Œä½¿ç”¨ICALå¢å¼ºçš„GPT-4Væ£€ç´¢æé«˜äº†ä»»åŠ¡æˆåŠŸç‡1.6å€ï¼Œè€Œä½¿ç”¨Qwen2-VLçš„å¾®è°ƒåˆ™å®ç°äº†2.8å€çš„æ”¹è¿›ã€‚åœ¨Ego4Dä¸­ï¼ŒICALè¡¨ç°ä¼˜äºå°‘é‡çš„GPT-4Vï¼Œå¹¶ä¸ç›‘ç£æ¨¡å‹ä¿æŒç«äº‰åŠ›ã€‚æ€»çš„æ¥è¯´ï¼ŒICALçš„è¡¨ç°æ¯”åŸå§‹äººç±»æ¼”ç¤ºé«˜å‡ºä¸¤å€ï¼Œå¹¶å‡å°‘äº†æ‰‹åŠ¨æç¤ºå·¥ç¨‹çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14596v5">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://ical-learning.github.io/">https://ical-learning.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦é«˜è´¨é‡æ ·æœ¬ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºIn-Context Abstraction Learningï¼ˆICALï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–åŠ¨ä½œå’Œè¯¦ç»†æ¨ç†ï¼Œå°†ä¸ä½³çš„è½¨è¿¹è½¬åŒ–ä¸ºé«˜è´¨é‡æ•°æ®ã€‚VLMèƒ½å¤Ÿåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ä¿®æ­£åŠ¨ä½œå¹¶æ ‡æ³¨å› æœå…³ç³»ã€ç‰©ä½“çŠ¶æ€ã€å­ç›®æ ‡å’Œä»»åŠ¡ç›¸å…³è§†è§‰ï¼Œå½¢æˆâ€œæ€ç»´ç¨‹åºâ€ã€‚é€šè¿‡äººç±»åé¦ˆï¼Œè¿™äº›ç¨‹åºå¯ä»¥åœ¨ç±»ä¼¼ç¯å¢ƒä¸­æ”¹å–„ä»£ç†æ‰§è¡Œæ•ˆæœã€‚ä½¿ç”¨ICALç”Ÿæˆçš„ä¾‹å­ä½œä¸ºæç¤ºä¸Šä¸‹æ–‡æˆ–å¾®è°ƒæ•°æ®ï¼Œèƒ½æ˜¾è‘—æé«˜å†³ç­–èƒ½åŠ›å¹¶å‡å°‘äººç±»åé¦ˆéœ€æ±‚ã€‚åœ¨TEAChã€VisualWebArenaå’ŒEgo4Dç­‰ä»»åŠ¡ä¸Šï¼ŒICALè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†éœ€é«˜è´¨é‡æ ·æœ¬ã€‚</li>
<li>å¼•å…¥In-Context Abstraction Learningï¼ˆICALï¼‰æ–¹æ³•ï¼Œèƒ½å°†ä¸ä½³çš„è½¨è¿¹è½¬åŒ–ä¸ºé«˜è´¨é‡æ•°æ®ã€‚</li>
<li>VLMèƒ½å¤Ÿæ ‡æ³¨å› æœå…³ç³»ã€ç‰©ä½“çŠ¶æ€ã€å­ç›®æ ‡å’Œä»»åŠ¡ç›¸å…³è§†è§‰ï¼Œå½¢æˆâ€œæ€ç»´ç¨‹åºâ€ã€‚</li>
<li>é€šè¿‡äººç±»åé¦ˆï¼Œè¿™äº›â€œæ€ç»´ç¨‹åºâ€å¯ä»¥åœ¨ç±»ä¼¼ç¯å¢ƒä¸­æ”¹å–„ä»£ç†æ‰§è¡Œæ•ˆæœã€‚</li>
<li>ICALåœ¨TEAChã€VisualWebArenaå’ŒEgo4Dç­‰ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ç›¸è¾ƒäºåŸå§‹äººç±»ç¤ºèŒƒï¼ŒICALèƒ½æé«˜å†³ç­–èƒ½åŠ›å¹¶å‡å°‘äººç±»åé¦ˆéœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2406.14596v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2406.14596v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2406.14596v5/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="One-size-doesnâ€™t-fit-all-Predicting-the-Number-of-Examples-for-In-Context-Learning"><a href="#One-size-doesnâ€™t-fit-all-Predicting-the-Number-of-Examples-for-In-Context-Learning" class="headerlink" title="One size doesnâ€™t fit all: Predicting the Number of Examples for   In-Context Learning"></a>One size doesnâ€™t fit all: Predicting the Number of Examples for   In-Context Learning</h2><p><strong>Authors:Manish Chandra, Debasis Ganguly, Iadh Ounis</strong></p>
<p>In-context learning (ICL) refers to the process of adding a small number of localized examples from a training set of labelled data to an LLMâ€™s prompt with an objective to effectively control the generative process seeking to improve the downstream task performance. Existing ICL approaches use an identical number of examples (a pre-configured hyper-parameter) for each data instance. Our work alleviates the limitations of this â€˜one fits allâ€™ approach by dynamically predicting the number of examples for each data instance to be used in few-shot inference with LLMs. In particular, we employ a multi-label classifier, the parameters of which are fitted using a training set, where the label for each instance in this training set indicates if using a specific value of k (number of most similar examples from 0 up to a maximum value) leads to correct k-shot downstream predictions. Our experiments on a number of text classification benchmarks show that AICL substantially outperforms standard ICL by up to 17%. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æŒ‡çš„æ˜¯åœ¨LLMæç¤ºä¸­æ·»åŠ å°‘é‡æ¥è‡ªæ ‡æ³¨æ•°æ®è®­ç»ƒé›†çš„æœ¬åœ°åŒ–ç¤ºä¾‹ï¼Œæ—¨åœ¨æœ‰æ•ˆæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„è¿‡ç¨‹ã€‚ç°æœ‰çš„ICLæ–¹æ³•ä¸ºæ¯ä¸ªæ•°æ®å®ä¾‹ä½¿ç”¨ç›¸åŒæ•°é‡çš„ç¤ºä¾‹ï¼ˆä¸€ä¸ªé¢„å…ˆé…ç½®çš„è¶…å‚æ•°ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡åŠ¨æ€é¢„æµ‹æ¯ä¸ªæ•°æ®å®ä¾‹åœ¨å°‘æ ·æœ¬æ¨æ–­ä¸­ä½¿ç”¨LLMçš„ç¤ºä¾‹æ•°é‡æ¥ç¼“è§£è¿™ç§â€œä¸€åˆ€åˆ‡â€æ–¹æ³•çš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ ‡ç­¾åˆ†ç±»å™¨ï¼Œå…¶å‚æ•°ä½¿ç”¨è®­ç»ƒé›†è¿›è¡Œæ‹Ÿåˆï¼Œè¯¥è®­ç»ƒé›†ä¸­æ¯ä¸ªå®ä¾‹çš„æ ‡ç­¾è¡¨ç¤ºä½¿ç”¨ç‰¹å®šçš„kå€¼ï¼ˆä»æœ€æ¥è¿‘çš„æœ€å¤škä¸ªç¤ºä¾‹æ•°å¼€å§‹åˆ°æœ€å¤§å€¼ï¼‰æ˜¯å¦èƒ½å¯¼è‡´æ­£ç¡®çš„kæ¬¡ä¸‹æ¸¸é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAICLæ˜¾è‘—ä¼˜äºæ ‡å‡†ICLï¼Œæœ€é«˜æå‡äº†çº¦17%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06402v3">PDF</a> </p>
<p><strong>Summary</strong><br>é€‚åº”å‹å®ä¾‹å­¦ä¹ ï¼ˆAICLï¼‰é’ˆå¯¹å°‘æ ·æœ¬æ¨æ–­æå‡ºäº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡åŠ¨æ€é¢„æµ‹æ¯ä¸ªæ•°æ®å®ä¾‹åœ¨LLMä¸­ä½¿ç”¨ç¤ºä¾‹çš„æ•°é‡æ¥æ”¹è¿›ç°æœ‰å®ä¾‹å­¦ä¹ ï¼ˆICLï¼‰çš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé‡‡ç”¨å¤šæ ‡ç­¾åˆ†ç±»å™¨ï¼Œå¹¶ä½¿ç”¨è®­ç»ƒé›†æ‹Ÿåˆå‚æ•°ï¼Œè¯¥è®­ç»ƒé›†çš„æ ‡ç­¾è¡¨ç¤ºä½¿ç”¨ç‰¹å®šæ•°é‡çš„ç¤ºä¾‹æ˜¯å¦å¯¼è‡´æ­£ç¡®çš„ä¸‹æ¸¸é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒAICLåœ¨å¤šä¸ªæ–‡æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†ICLã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å®ä¾‹å­¦ä¹ ï¼ˆICLï¼‰æ—¨åœ¨é€šè¿‡å‘LLMæç¤ºä¸­æ·»åŠ å°‘é‡å±€éƒ¨ç¤ºä¾‹æ¥æ”¹å–„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ç°æœ‰ICLæ–¹æ³•ä¸ºæ¯ä¸ªæ•°æ®å®ä¾‹ä½¿ç”¨ç›¸åŒæ•°é‡çš„ç¤ºä¾‹ï¼Œè€ŒAICLåˆ™åŠ¨æ€é¢„æµ‹æ¯ä¸ªå®ä¾‹æ‰€éœ€çš„ç¤ºä¾‹æ•°é‡ã€‚</li>
<li>AICLé‡‡ç”¨å¤šæ ‡ç­¾åˆ†ç±»å™¨å¹¶ä½¿ç”¨è®­ç»ƒé›†æ‹Ÿåˆå‚æ•°ã€‚</li>
<li>è®­ç»ƒé›†çš„æ ‡ç­¾è¡¨ç¤ºä½¿ç”¨ç‰¹å®šæ•°é‡çš„ç¤ºä¾‹æ˜¯å¦å¯¼è‡´æ­£ç¡®çš„ä¸‹æ¸¸é¢„æµ‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.06402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2403.06402v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2403.06402v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2403.06402v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="UniGraph-Learning-a-Unified-Cross-Domain-Foundation-Model-for-Text-Attributed-Graphs"><a href="#UniGraph-Learning-a-Unified-Cross-Domain-Foundation-Model-for-Text-Attributed-Graphs" class="headerlink" title="UniGraph: Learning a Unified Cross-Domain Foundation Model for   Text-Attributed Graphs"></a>UniGraph: Learning a Unified Cross-Domain Foundation Model for   Text-Attributed Graphs</h2><p><strong>Authors:Yufei He, Yuan Sui, Xiaoxin He, Bryan Hooi</strong></p>
<p>Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we recognize text as an effective unifying medium and employ Text-Attributed Graphs (TAGs) to leverage this potential. We present our UniGraph framework, designed to learn a foundation model for TAGs, which is capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages textual features for unifying node representations, even for graphs such as molecular graphs that do not naturally have textual features. We propose a novel cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks. Additionally, we propose the first pre-training algorithm specifically designed for large-scale self-supervised learning on TAGs, based on Masked Graph Modeling. We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the modelâ€™s effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets. </p>
<blockquote>
<p>ChatGPTå’ŒGPT-4ç­‰åŸºç¡€æ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå±•ç°å‡ºäº†ä»¤äººç©ç›®çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¹¿æ³›çš„ä»»åŠ¡å’Œåº”ç”¨ä¸­æ³›åŒ–ï¼Œè¶…å‡ºå…¶åˆå§‹è®­ç»ƒç›®æ ‡èŒƒå›´ã€‚ç„¶è€Œï¼Œå›¾å­¦ä¹ ä¸»è¦é›†ä¸­åœ¨é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–æ•°æ®é›†çš„å•ä¸€å›¾æ¨¡å‹ä¸Šï¼Œç¼ºä¹å°†æ‰€å­¦çŸ¥è¯†è¿ç§»åˆ°ä¸åŒé¢†åŸŸçš„èƒ½åŠ›ã€‚è¿™ä¸€å±€é™æ€§æºäºå›¾ç»“æ„å›ºæœ‰çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œä»¥åŠå›¾æ•°æ®ç‰¹å®šçš„ç‰¹å¾å’Œæ ‡ç­¾ç©ºé—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤è¯†åˆ°æ–‡æœ¬æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç»Ÿä¸€åª’ä»‹ï¼Œå¹¶é‡‡ç”¨æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰æ¥åˆ©ç”¨è¿™ä¸€æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†UniGraphæ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ æ–‡æœ¬å±æ€§å›¾çš„åŸºå‡†æ¨¡å‹ï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°ä¸åŒé¢†åŸŸçš„æœªè§å›¾å’Œä»»åŠ¡ã€‚ä¸é‚£äº›ä½¿ç”¨é¢„è®¡ç®—èŠ‚ç‚¹ç‰¹å¾ä½œä¸ºè¾“å…¥çš„å•ä¸€å›¾æ¨¡å‹ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ–‡æœ¬ç‰¹å¾æ¥ç»Ÿä¸€èŠ‚ç‚¹è¡¨ç¤ºï¼Œå³ä½¿å¯¹äºæ²¡æœ‰è‡ªç„¶æ–‡æœ¬ç‰¹å¾çš„å›¾å½¢ï¼ˆå¦‚åˆ†å­å›¾å½¢ï¼‰ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„çº§è”æ¶æ„ï¼Œå°†è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä½œä¸ºä¸»å¹²ç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºTAGä¸Šçš„å¤§è§„æ¨¡è‡ªç›‘ç£å­¦ä¹ è®¾è®¡çš„é¢„è®­ç»ƒç®—æ³•ï¼ŒåŸºäºæ©ç å›¾å»ºæ¨¡ã€‚æˆ‘ä»¬å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥å®ç°é›¶æ ·æœ¬é¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å„ç§å›¾å­¦ä¹ ä»»åŠ¡å’Œé¢†åŸŸè¿›è¡Œçš„å…¨é¢å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœªè§å›¾çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€å°‘é‡ä¸Šä¸‹æ–‡è¿ç§»å’Œé›¶æ ·æœ¬è¿ç§»æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œç”šè‡³è¶…è¶Šæˆ–åŒ¹é…äº†åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£è®­ç»ƒçš„GNNçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.13630v3">PDF</a> KDD 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†UniGraphæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºText-Attributed Graphsï¼ˆTAGsï¼‰çš„åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§è¿‡çš„å›¾å’Œè·¨åŸŸä»»åŠ¡ã€‚å®ƒé‡‡ç”¨æ–‡æœ¬ç‰¹å¾æ¥ç»Ÿä¸€èŠ‚ç‚¹è¡¨ç¤ºï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„çº§è”æ¶æ„ï¼ŒåŒ…æ‹¬è¯­è¨€æ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†é’ˆå¯¹TAGsçš„å¤§è§„æ¨¡è‡ªç›‘ç£å­¦ä¹ çš„é¢„è®­ç»ƒç®—æ³•ï¼Œä»¥åŠåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå›¾å½¢æŒ‡ä»¤è°ƒæ•´çš„æ–¹æ³•ï¼Œä»¥å®ç°é›¶å°„å‡»é¢„æµ‹èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœªè§è¿‡çš„å›¾çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€å°‘æ ·æœ¬ä¸Šä¸‹æ–‡è¿ç§»å’Œé›¶æ ·æœ¬è¿ç§»æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniGraphæ¡†æ¶å¼•å…¥Text-Attributed Graphsï¼ˆTAGsï¼‰æ¦‚å¿µï¼Œå®ç°äº†è·¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç»“åˆè¯­è¨€æ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œï¼Œé‡‡ç”¨æ–‡æœ¬ç‰¹å¾æ¥ç»Ÿä¸€èŠ‚ç‚¹è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹TAGsçš„å¤§è§„æ¨¡è‡ªç›‘ç£å­¦ä¹ çš„é¢„è®­ç»ƒç®—æ³•ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå›¾å½¢æŒ‡ä»¤è°ƒæ•´ï¼Œå®ç°äº†é›¶å°„å‡»é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>UniGraphæ¡†æ¶åœ¨æœªè§è¿‡çš„å›¾çš„è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒå°‘æ ·æœ¬ä¸Šä¸‹æ–‡è¿ç§»å’Œé›¶æ ·æœ¬è¿ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.13630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2402.13630v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2402.13630v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2402.13630v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2402.13630v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AutoMix-Automatically-Mixing-Language-Models"><a href="#AutoMix-Automatically-Mixing-Language-Models" class="headerlink" title="AutoMix: Automatically Mixing Language Models"></a>AutoMix: Automatically Mixing Language Models</h2><p><strong>Authors:Pranjal Aggarwal, Aman Madaan, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Manaal Faruqui,  Mausam</strong></p>
<p>Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present Automix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to Automix are two key technical contributions. First, it has a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring extensive training. Second, given that self-verification can be noisy, it employs a POMDP based router that can effectively select an appropriately sized model, based on answer confidence. Experiments across five language models and five challenging datasets show that Automix consistently surpasses strong baselines, reducing computational cost by over 50% for comparable performance. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç°åœ¨å¯ä»¥ä»äº‘APIæä¾›å•†å¤„è·å¾—å„ç§è§„æ¨¡å’Œé…ç½®ã€‚è™½ç„¶è¿™ç§å¤šæ ·æ€§æä¾›äº†å¹¿æ³›çš„é€‰æ‹©ï¼Œä½†æœ‰æ•ˆåˆ©ç”¨è¿™äº›é€‰é¡¹æ¥ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œæ€§èƒ½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Automixæ–¹æ³•ï¼Œå®ƒæ ¹æ®è¾ƒå°LMçš„è¾“å‡ºè¿‘ä¼¼æ­£ç¡®æ€§æ¥æˆ˜ç•¥æ€§åœ°å°†æŸ¥è¯¢è·¯ç”±åˆ°è¾ƒå¤§çš„LMã€‚Automixçš„ä¸¤ä¸ªå…³é”®æŠ€æœ¯è´¡çŒ®æ˜¯æ ¸å¿ƒã€‚é¦–å…ˆï¼Œå®ƒæ‹¥æœ‰ä¸€ç§å°æ ·æœ¬è‡ªæˆ‘éªŒè¯æœºåˆ¶ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦å¤§é‡è®­ç»ƒçš„æƒ…å†µä¸‹ä¼°è®¡è‡ªèº«è¾“å‡ºçš„å¯é æ€§ã€‚å…¶æ¬¡ï¼Œç”±äºè‡ªæˆ‘éªŒè¯å¯èƒ½å¸¦æœ‰å™ªå£°ï¼Œå®ƒé‡‡ç”¨åŸºäºPOMDPçš„è·¯ç”±å™¨ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ ¹æ®ç­”æ¡ˆç½®ä¿¡åº¦é€‰æ‹©é€‚å½“å¤§å°çš„æ¨¡å‹ã€‚åœ¨äº”ä¸ªè¯­è¨€æ¨¡å‹å’Œäº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAutomixå§‹ç»ˆè¶…è¿‡å¼ºå¤§çš„åŸºå‡†çº¿ï¼Œåœ¨å¯æ¯”æ€§èƒ½çš„æƒ…å†µä¸‹å°†è®¡ç®—æˆæœ¬é™ä½è¶…è¿‡50%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12963v5">PDF</a> 38th Conference on Neural Information Processing Systems (NeurIPS   2024). The first two authors contributed equally. Work started and partly   done during Amanâ€™s internship at Google. This version adds results on   additional models and datasets</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»äº‘APIæä¾›å•†å¤„è·å¾—ï¼Œç§ç±»å’Œé…ç½®å¤šæ ·ã€‚å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è¿™äº›æ¨¡å‹ä»¥ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œæ€§èƒ½æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºAutomixæ–¹æ³•ï¼Œæ ¹æ®å°å‹LMçš„è¾“å‡ºè¿‘ä¼¼æ­£ç¡®æ€§æ¥æˆ˜ç•¥æ€§åœ°å°†æŸ¥è¯¢å¯¼å‘å¤§å‹LMã€‚Automixæœ‰ä¸¤ä¸ªå…³é”®æŠ€æœ¯è´¡çŒ®ï¼šä¸€æ˜¯å…·æœ‰å°‘æ ·æœ¬è‡ªæˆ‘éªŒè¯æœºåˆ¶ï¼Œæ— éœ€å¤§é‡è®­ç»ƒå³å¯ä¼°è®¡è¾“å‡ºå¯é æ€§ï¼›äºŒæ˜¯é‡‡ç”¨åŸºäºPOMDPçš„è·¯ç”±å™¨ï¼Œåœ¨ç­”æ¡ˆç½®ä¿¡åº¦çš„åŸºç¡€ä¸Šæœ‰æ•ˆåœ°é€‰æ‹©é€‚å½“å¤§å°çš„æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒAutomixåœ¨äº”ä¸ªè¯­è¨€æ¨¡å‹å’Œäº”ä¸ªæŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸€ç›´ä¼˜äºå¼ºåŸºçº¿ï¼Œè®¡ç®—æˆæœ¬é™ä½äº†50%ä»¥ä¸Šï¼Œæ€§èƒ½ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤šæ ·æ€§å’Œé…ç½®é€‰æ‹©å¸¦æ¥äº†æœ‰æ•ˆåˆ©ç”¨çš„æŒ‘æˆ˜ã€‚</li>
<li>Automixæ–¹æ³•æ ¹æ®å°å‹LMçš„è¾“å‡ºè¿‘ä¼¼æ­£ç¡®æ€§æ¥æŒ‡å¯¼æŸ¥è¯¢åˆ°å¤§å‹LMã€‚</li>
<li>Automixå…·æœ‰å°‘æ ·æœ¬è‡ªæˆ‘éªŒè¯æœºåˆ¶ï¼Œæ— éœ€å¤§é‡è®­ç»ƒå³å¯è¯„ä¼°è¾“å‡ºå¯é æ€§ã€‚</li>
<li>Automixé‡‡ç”¨åŸºäºPOMDPçš„è·¯ç”±å™¨ï¼Œæœ‰æ•ˆé€‰æ‹©é€‚å½“å¤§å°çš„æ¨¡å‹ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜Automixåœ¨å¤šä¸ªè¯­è¨€æ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿ã€‚</li>
<li>Automixèƒ½å¤Ÿé™ä½è®¡ç®—æˆæœ¬ï¼Œè¾¾åˆ°è¶…è¿‡50%çš„èŠ‚çœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.12963">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2310.12963v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2310.12963v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2310.12963v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2310.12963v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-23\./crop_Few-Shot/2310.12963v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-23/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-505dd82030bf9f168c9fbf90ffb05ee3.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  EfficientVITON An Efficient Virtual Try-On Model using Optimized   Diffusion Process
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0f77fec60f654cdb6c143df0db2e8469.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-23  UI-TARS Pioneering Automated GUI Interaction with Native Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
