<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  StageDesigner Artistic Stage Generation for Scenography via Theater   Scripts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-32398b30f5a321f06b6bf6909b04c372.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-06-æ›´æ–°"><a href="#2025-03-06-æ›´æ–°" class="headerlink" title="2025-03-06 æ›´æ–°"></a>2025-03-06 æ›´æ–°</h1><h2 id="StageDesigner-Artistic-Stage-Generation-for-Scenography-via-Theater-Scripts"><a href="#StageDesigner-Artistic-Stage-Generation-for-Scenography-via-Theater-Scripts" class="headerlink" title="StageDesigner: Artistic Stage Generation for Scenography via Theater   Scripts"></a>StageDesigner: Artistic Stage Generation for Scenography via Theater   Scripts</h2><p><strong>Authors:Zhaoxing Gan, Mengtian Li, Ruhua Chen, Zhongxia Ji, Sichen Guo, Huanling Hu, Guangnan Ye, Zuo Hu</strong></p>
<p>In this work, we introduce StageDesigner, the first comprehensive framework for artistic stage generation using large language models combined with layout-controlled diffusion models. Given the professional requirements of stage scenography, StageDesigner simulates the workflows of seasoned artists to generate immersive 3D stage scenes. Specifically, our approach is divided into three primary modules: Script Analysis, which extracts thematic and spatial cues from input scripts; Foreground Generation, which constructs and arranges essential 3D objects; and Background Generation, which produces a harmonious background aligned with the narrative atmosphere and maintains spatial coherence by managing occlusions between foreground and background elements. Furthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276 unique stage scenes spanning different historical styles and annotated with scripts, images, and detailed 3D layouts, specifically tailored for this task. Finally, evaluations using both standard and newly proposed metrics, along with extensive user studies, demonstrate the effectiveness of StageDesigner. Project can be found at: <a target="_blank" rel="noopener" href="https://deadsmither5.github.io/2025/01/03/StageDesigner/">https://deadsmither5.github.io/2025/01/03/StageDesigner/</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†StageDesignerï¼Œè¿™æ˜¯ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¸ƒå±€æ§åˆ¶æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œç”¨äºè‰ºæœ¯èˆå°ç”Ÿæˆçš„é¦–ä¸ªç»¼åˆæ¡†æ¶ã€‚é‰´äºèˆå°å¸ƒæ™¯çš„ä¸“ä¸šè¦æ±‚ï¼ŒStageDesigneræ¨¡æ‹Ÿèµ„æ·±è‰ºæœ¯å®¶çš„å·¥ä½œæµç¨‹æ¥ç”Ÿæˆæ²‰æµ¸å¼3Dèˆå°åœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè„šæœ¬åˆ†æï¼Œä»è¾“å…¥è„šæœ¬ä¸­æå–ä¸»é¢˜å’Œç©ºé—´çº¿ç´¢ï¼›å‰æ™¯ç”Ÿæˆï¼Œæ„å»ºå’Œå®‰æ’é‡è¦çš„3Dç‰©ä½“ï¼›èƒŒæ™¯ç”Ÿæˆï¼Œäº§ç”Ÿä¸å™äº‹æ°›å›´å’Œè°ç»Ÿä¸€çš„èƒŒæ™¯ï¼Œå¹¶é€šè¿‡ç®¡ç†å‰æ™¯å’ŒèƒŒæ™¯å…ƒç´ ä¹‹é—´çš„é®æŒ¡å…³ç³»æ¥ä¿æŒç©ºé—´è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†StagePro-V1æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ç”¨æ•°æ®é›†ï¼ŒåŒ…å«276ä¸ªç‹¬ç‰¹çš„èˆå°åœºæ™¯ï¼Œè·¨è¶Šä¸åŒçš„å†å²é£æ ¼ï¼Œå¹¶é™„æœ‰è„šæœ¬ã€å›¾åƒå’Œè¯¦ç»†çš„3Då¸ƒå±€æ³¨é‡Šï¼Œä¸“é—¨ç”¨äºæ­¤ä»»åŠ¡ã€‚æœ€åï¼Œé€šè¿‡æ ‡å‡†å’Œæ–°æå‡ºçš„æŒ‡æ ‡è¯„ä¼°ä»¥åŠå¹¿æ³›çš„ç”¨æˆ·ç ”ç©¶ï¼Œè¯æ˜äº†StageDesignerçš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®ç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://deadsmither5.github.io/2025/01/03/StageDesigner/">https://deadsmither5.github.io/2025/01/03/StageDesigner/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02595v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ä¸å¸ƒå±€æ§åˆ¶æ‰©æ•£æ¨¡å‹ï¼Œæ¨å‡ºé¦–ä¸ªç»¼åˆæ€§èˆå°ç”Ÿæˆæ¡†æ¶StageDesignerã€‚StageDesigneræ¨¡æ‹Ÿèµ„æ·±è‰ºæœ¯å®¶å·¥ä½œæµç¨‹ï¼Œç”Ÿæˆæ²‰æµ¸å¼3Dèˆå°åœºæ™¯ã€‚åˆ†ä¸ºä¸‰å¤§æ¨¡å—ï¼šè„šæœ¬åˆ†æï¼Œæå–è¾“å…¥è„šæœ¬çš„ä¸»é¢˜å’Œç©ºé—´çº¿ç´¢ï¼›å‰æ™¯ç”Ÿæˆï¼Œæ„å»ºå’Œå®‰æ’é‡è¦çš„3Dç‰©ä½“ï¼›èƒŒæ™¯ç”Ÿæˆï¼Œäº§ç”Ÿä¸å™äº‹æ°›å›´å’Œè°ä¸€è‡´çš„èƒŒæ™¯ï¼Œå¹¶ç®¡ç†å‰æ™¯å’ŒèƒŒæ™¯å…ƒç´ ä¹‹é—´çš„é®æŒ¡ï¼Œä»¥ä¿æŒç©ºé—´è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸ºè¿™ä¸€ä»»åŠ¡é‡èº«å®šåˆ¶çš„StagePro-V1æ•°æ®é›†ï¼ŒåŒ…å«276ä¸ªæ¶µç›–ä¸åŒå†å²é£æ ¼çš„ç‹¬ç‰¹èˆå°åœºæ™¯ï¼Œå¹¶é™„æœ‰è„šæœ¬ã€å›¾åƒå’Œè¯¦ç»†çš„3Då¸ƒå±€ã€‚è¯„ä¼°å’Œç”¨æˆ·ç ”ç©¶è¯æ˜äº†StageDesignerçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StageDesigneræ˜¯é¦–ä¸ªç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¸ƒå±€æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„ç»¼åˆæ€§èˆå°ç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>å®ƒæ¨¡æ‹Ÿèµ„æ·±è‰ºæœ¯å®¶çš„å·¥ä½œæµç¨‹ï¼Œç”Ÿæˆæ²‰æµ¸å¼3Dèˆå°åœºæ™¯ã€‚</li>
<li>StageDesigneråˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šè„šæœ¬åˆ†æã€å‰æ™¯ç”Ÿæˆå’ŒèƒŒæ™¯ç”Ÿæˆã€‚</li>
<li>StagePro-V1æ•°æ®é›†ä¸“é—¨ä¸ºè¿™ä¸€ä»»åŠ¡é‡èº«å®šåˆ¶ï¼ŒåŒ…å«å¤šç§ç‹¬ç‰¹èˆå°åœºæ™¯ã€è„šæœ¬ã€å›¾åƒå’Œè¯¦ç»†çš„3Då¸ƒå±€ã€‚</li>
<li>StageDesigneré€šè¿‡ç®¡ç†å’Œå¤„ç†å‰æ™¯ä¸èƒŒæ™¯å…ƒç´ ä¹‹é—´çš„é®æŒ¡æ¥ä¿æŒç©ºé—´è¿è´¯æ€§ã€‚</li>
<li>è¯„ä¼°å’Œå®éªŒè¯æ˜StageDesignerçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dae2af0b959f0eb0f448b9657512ecca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb3431972afcc475108a9235ed6e21d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be2a25c2c5af4d06a20c707f17908657.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SPG-Improving-Motion-Diffusion-by-Smooth-Perturbation-Guidance"><a href="#SPG-Improving-Motion-Diffusion-by-Smooth-Perturbation-Guidance" class="headerlink" title="SPG: Improving Motion Diffusion by Smooth Perturbation Guidance"></a>SPG: Improving Motion Diffusion by Smooth Perturbation Guidance</h2><p><strong>Authors:Boseong Jeon</strong></p>
<p>This paper presents a test-time guidance method to improve the output quality of the human motion diffusion models without requiring additional training. To have negative guidance, Smooth Perturbation Guidance (SPG) builds a weak model by temporally smoothing the motion in the denoising steps. Compared to model-agnostic methods originating from the image generation field, SPG effectively mitigates out-of-distribution issues when perturbing motion diffusion models. In SPG guidance, the nature of motion structure remains intact. This work conducts a comprehensive analysis across distinct model architectures and tasks. Despite its extremely simple implementation and no need for additional training requirements, SPG consistently enhances motion fidelity. Project page can be found at <a target="_blank" rel="noopener" href="https://spg-blind.vercel.app/">https://spg-blind.vercel.app/</a> </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶æŒ‡å¯¼æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜äººç±»è¿åŠ¨æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºè´¨é‡ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚ä¸ºäº†è¿›è¡Œè´Ÿå‘æŒ‡å¯¼ï¼ŒSPGï¼ˆå¹³æ»‘æ‰°åŠ¨æŒ‡å¯¼ï¼‰é€šè¿‡å™ªå£°å»é™¤æ­¥éª¤ä¸­çš„è¿åŠ¨æ—¶é—´å¹³æ»‘æ¥æ„å»ºå¼±æ¨¡å‹ã€‚ä¸æºè‡ªå›¾åƒç”Ÿæˆé¢†åŸŸçš„æ¨¡å‹æ— å…³æ–¹æ³•ç›¸æ¯”ï¼ŒSPGåœ¨æ‰°åŠ¨è¿åŠ¨æ‰©æ•£æ¨¡å‹æ—¶æœ‰æ•ˆå‡è½»äº†åˆ†å¸ƒå¤–çš„é—®é¢˜ã€‚åœ¨SPGæŒ‡å¯¼ä¸‹ï¼Œè¿åŠ¨ç»“æ„çš„æœ¬è´¨ä¿æŒä¸å˜ã€‚æœ¬å·¥ä½œå¯¹ä¸åŒæ¨¡å‹æ¶æ„å’Œä»»åŠ¡è¿›è¡Œäº†ç»¼åˆåˆ†æã€‚å°½ç®¡å…¶å®ç°æä¸ºç®€å•ä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒè¦æ±‚ï¼Œä½†SPGå§‹ç»ˆæé«˜äº†è¿åŠ¨ä¿çœŸåº¦ã€‚é¡¹ç›®é¡µé¢ä½äºï¼š[<a target="_blank" rel="noopener" href="https://spg-blind.vercel.app/]">https://spg-blind.vercel.app/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02577v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æµ‹è¯•æ—¶æŒ‡å¯¼æ–¹æ³•ï¼Œç”¨äºæé«˜äººç±»è¿åŠ¨æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºè´¨é‡ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚é€šè¿‡å¼•å…¥å¹³æ»‘æ‰°åŠ¨æŒ‡å¯¼ï¼ˆSPGï¼‰ï¼Œå»ºç«‹å¼±æ¨¡å‹åœ¨é™å™ªæ­¥éª¤ä¸­å¯¹è¿åŠ¨è¿›è¡Œæ—¶é—´å¹³æ»‘ï¼Œå®ç°è´Ÿå‘æŒ‡å¯¼ã€‚ç›¸è¾ƒäºæºè‡ªå›¾åƒç”Ÿæˆé¢†åŸŸçš„æ¨¡å‹æ— å…³æ–¹æ³•ï¼ŒSPGåœ¨æ‰°åŠ¨è¿åŠ¨æ‰©æ•£æ¨¡å‹æ—¶ï¼Œèƒ½æœ‰æ•ˆå‡è½»åˆ†å¸ƒå¤–é—®é¢˜ï¼Œä¿æŒè¿åŠ¨ç»“æ„çš„å®Œæ•´æ€§ã€‚æ­¤æ–¹æ¡ˆåœ¨ä¸åŒæ¨¡å‹æ¶æ„å’Œä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢åˆ†æï¼Œå³ä½¿å®ç°æ–¹å¼æä¸ºç®€æ´ä¸”æ— éœ€é¢å¤–è®­ç»ƒè¦æ±‚ï¼Œä¹Ÿèƒ½æŒç»­æå‡è¿åŠ¨ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†æµ‹è¯•æ—¶æŒ‡å¯¼æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜äººç±»è¿åŠ¨æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚</li>
<li>å¼•å…¥äº†å¹³æ»‘æ‰°åŠ¨æŒ‡å¯¼ï¼ˆSPGï¼‰ï¼Œé€šè¿‡æ—¶é—´å¹³æ»‘è¿åŠ¨å»ºç«‹å¼±æ¨¡å‹ã€‚</li>
<li>SPGå®ç°äº†è´Ÿå‘æŒ‡å¯¼ï¼Œæœ‰æ•ˆå‡è½»åˆ†å¸ƒå¤–é—®é¢˜ã€‚</li>
<li>SPGä¿æŒè¿åŠ¨ç»“æ„çš„å®Œæ•´æ€§ã€‚</li>
<li>è¯¥æ–¹æ¡ˆåœ¨ä¸åŒæ¨¡å‹æ¶æ„å’Œä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢åˆ†æã€‚</li>
<li>SPGæé«˜è¿åŠ¨ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d1513612b15e7a93146518fb0d64b2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3830bdb0b6eeed66c62bc24fa2130ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03ff277d9eb31c18fd186f949987fe88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac342214c4b8628f3c1e2750b0392262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f80c7c04ac98ff7f16ad225041187d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7af65519dc78a7d2f00872c71dae83d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CQ-CNN-A-Hybrid-Classical-Quantum-Convolutional-Neural-Network-for-Alzheimerâ€™s-Disease-Detection-Using-Diffusion-Generated-and-U-Net-Segmented-3D-MRI"><a href="#CQ-CNN-A-Hybrid-Classical-Quantum-Convolutional-Neural-Network-for-Alzheimerâ€™s-Disease-Detection-Using-Diffusion-Generated-and-U-Net-Segmented-3D-MRI" class="headerlink" title="CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for   Alzheimerâ€™s Disease Detection Using Diffusion Generated and U Net Segmented   3D MRI"></a>CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for   Alzheimerâ€™s Disease Detection Using Diffusion Generated and U Net Segmented   3D MRI</h2><p><strong>Authors:Mominul Islam, Mohammad Junayed Hasan, M. R. C. Mahdy</strong></p>
<p>The detection of Alzheimer disease (AD) from clinical MRI data is an active area of research in medical imaging. Recent advances in quantum computing, particularly the integration of parameterized quantum circuits (PQCs) with classical machine learning architectures, offer new opportunities to develop models that may outperform traditional methods. However, quantum machine learning (QML) remains in its early stages and requires further experimental analysis to better understand its behavior and limitations. In this paper, we propose an end to end hybrid classical quantum convolutional neural network (CQ CNN) for AD detection using clinically formatted 3D MRI data. Our approach involves developing a framework to make 3D MRI data usable for machine learning, designing and training a brain tissue segmentation model (Skull Net), and training a diffusion model to generate synthetic images for the minority class. Our converged models exhibit potential quantum advantages, achieving higher accuracy in fewer epochs than classical models. The proposed beta8 3 qubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA) models while requiring significantly fewer computational resources. In particular, the architecture employs only 13K parameters (0.48 MB), reducing the parameter count by more than 99.99% compared to current SOTA models. Furthermore, the diffusion-generated data used to train our quantum models, in conjunction with real samples, preserve clinical structural standards, representing a notable first in the field of QML. We conclude that CQCNN architecture like models, with further improvements in gradient optimization techniques, could become a viable option and even a potential alternative to classical models for AD detection, especially in data limited and resource constrained clinical settings. </p>
<blockquote>
<p>ä½¿ç”¨ä¸´åºŠMRIæ•°æ®æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯åŒ»å­¦å½±åƒç ”ç©¶çš„ä¸€ä¸ªæ´»è·ƒé¢†åŸŸã€‚æœ€è¿‘é‡å­è®¡ç®—æŠ€æœ¯çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCï¼‰ä¸ç»å…¸æœºå™¨å­¦ä¹ æ¶æ„çš„é›†æˆï¼Œä¸ºå¼€å‘å¯èƒ½è¶…è¶Šä¼ ç»Ÿæ–¹æ³•çš„æ¨¡å‹æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œé‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œéœ€è¦è¿›ä¸€æ­¥å®éªŒåˆ†æä»¥æ›´å¥½åœ°äº†è§£å…¶è¡Œä¸ºå’Œå±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ··åˆç»å…¸é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCQCNNï¼‰ï¼Œç”¨äºä½¿ç”¨ä¸´åºŠæ ¼å¼çš„3D MRIæ•°æ®è¿›è¡ŒADæ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å¼€å‘ä¸€ä¸ªä½¿3D MRIæ•°æ®å¯ç”¨äºæœºå™¨å­¦ä¹ æ¡†æ¶ã€è®¾è®¡å’Œè®­ç»ƒè„‘ç»„ç»‡åˆ†å‰²æ¨¡å‹ï¼ˆSkullNetï¼‰ã€è®­ç»ƒæ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆå°‘æ•°ç±»çš„åˆæˆå›¾åƒç­‰æ­¥éª¤ã€‚æˆ‘ä»¬çš„æ”¶æ•›æ¨¡å‹å±•ç°å‡ºæ½œåœ¨çš„é‡å­ä¼˜åŠ¿ï¼Œåœ¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°å†…å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®åº¦ï¼Œè¶…è¿‡äº†ç»å…¸æ¨¡å‹ã€‚æ‰€æå‡ºçš„beta8</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02345v1">PDF</a> Application of hybrid quantum-classical machine learning for (early   stage) disease detection</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§ç«¯åˆ°ç«¯çš„æ··åˆç»å…¸é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCQCNNï¼‰ï¼Œç”¨äºåˆ©ç”¨ä¸´åºŠæ ¼å¼çš„3D MRIæ•°æ®è¿›è¡Œé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹ã€‚é€šè¿‡å¼€å‘ä½¿3D MRIæ•°æ®é€‚ç”¨äºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ã€è®¾è®¡å’Œè®­ç»ƒè„‘ç»„ç»‡çš„åˆ†å‰²æ¨¡å‹ï¼ˆSkull Netï¼‰ï¼Œä»¥åŠè®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå°‘æ•°ç±»çš„åˆæˆå›¾åƒï¼Œå±•ç°å‡ºæ½œåœ¨çš„é‡å­ä¼˜åŠ¿ã€‚æ‰€æå‡ºçš„beta8 3é‡å­ä½æ¨¡å‹å®ç°äº†97.50%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—é™ä½è®¡ç®—èµ„æºéœ€æ±‚ã€‚è¯¥æ¶æ„ä»…æœ‰13Kå‚æ•°ï¼ˆ0.48 MBï¼‰ï¼Œå‡å°‘äº†è¶…è¿‡99.99%çš„å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼Œæ‰©æ•£ç”Ÿæˆçš„æ•°æ®ç”¨äºè®­ç»ƒé‡å­æ¨¡å‹ï¼Œä¸çœŸå®æ ·æœ¬ä¸€èµ·ä¿ç•™ä¸´åºŠç»“æ„æ ‡å‡†ï¼Œä»£è¡¨ç€é‡å­æœºå™¨å­¦ä¹ é¢†åŸŸçš„é‡è¦è¿›æ­¥ã€‚ç»“è®ºæ˜¯ï¼ŒCQCNNæ¶æ„ç­‰æ¨¡å‹åœ¨æ¢¯åº¦ä¼˜åŒ–æŠ€æœ¯è¿›ä¸€æ­¥æ”¹è¿›åï¼Œå¯èƒ½æˆä¸ºADæ£€æµ‹çš„å¯è¡Œé€‰æ‹©ï¼Œç”šè‡³åœ¨æ•°æ®æœ‰é™å’Œèµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­æˆä¸ºæ½œåœ¨æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†ä¸€ç§æ··åˆç»å…¸é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCQCNNï¼‰ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹çš„æœ€æ–°ç ”ç©¶ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†é‡å­è®¡ç®—å’Œæœºå™¨å­¦ä¹ ï¼Œå®ç°äº†è¾ƒé«˜çš„æ£€æµ‹å‡†ç¡®ç‡ã€‚</li>
<li>æ‰€æå‡ºçš„beta8 3é‡å­ä½æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°äº†97.50%ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—èµ„æºéœ€æ±‚ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼€å‘é€‚ç”¨äºæœºå™¨å­¦ä¹ çš„3D MRIæ•°æ®å¤„ç†æ–¹æ³•ã€è®¾è®¡å¹¶è®­ç»ƒè„‘ç»„ç»‡åˆ†å‰²æ¨¡å‹ï¼ˆSkull Netï¼‰ä»¥åŠä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºå…¶æ€§èƒ½ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ•°æ®èƒ½å¤Ÿä¿ç•™ä¸´åºŠç»“æ„æ ‡å‡†ï¼Œå¹¶ä¸çœŸå®æ ·æœ¬ä¸€èµ·ä½¿ç”¨ï¼Œè¿™åœ¨é‡å­æœºå™¨å­¦ä¹ é¢†åŸŸæ˜¯ä¸€ä¸ªé‡è¦çš„è¿›æ­¥ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºï¼Œéšç€æ¢¯åº¦ä¼˜åŒ–æŠ€æœ¯çš„è¿›ä¸€æ­¥æ”¹è¿›ï¼Œæ­¤ç±»æ¨¡å‹å¯èƒ½æˆä¸ºç»å…¸æ¨¡å‹çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96a969b5873c7b5ffcec70f2925fd108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b83648d11db9a64f40e590d8969f47bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5487dee610cff690eda703f5737c5ebe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c101506b9fc752d21a063eab4fc83491.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e68e4738f82a7d6555b9690e77252424.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-mmWave-Radar-Point-Cloud-Enhancement-Driven-by-Range-Images"><a href="#Diffusion-Based-mmWave-Radar-Point-Cloud-Enhancement-Driven-by-Range-Images" class="headerlink" title="Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range   Images"></a>Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range   Images</h2><p><strong>Authors:Ruixin Wu, Zihan Li, Jin Wang, Xiangyu Xu, Huan Yu, Zhi Zheng, Kaixiang Huang, Guodong Lu</strong></p>
<p>Millimeter-wave (mmWave) radar has attracted significant attention in robotics and autonomous driving. However, despite the perception stability in harsh environments, the point cloud generated by mmWave radar is relatively sparse while containing significant noise, which limits its further development. Traditional mmWave radar enhancement approaches often struggle to leverage the effectiveness of diffusion models in super-resolution, largely due to the unnatural range-azimuth heatmap (RAH) or birdâ€™s eye view (BEV) representation. To overcome this limitation, we propose a novel method that pioneers the application of fusing range images with image diffusion models, achieving accurate and dense mmWave radar point clouds that are similar to LiDAR. Benefitting from the projection that aligns with human observation, the range image representation of mmWave radar is close to natural images, allowing the knowledge from pre-trained image diffusion models to be effectively transferred, significantly improving the overall performance. Extensive evaluations on both public datasets and self-constructed datasets demonstrate that our approach provides substantial improvements, establishing a new state-of-the-art performance in generating truly three-dimensional LiDAR-like point clouds via mmWave radar. </p>
<blockquote>
<p>æ¯«ç±³æ³¢é›·è¾¾ï¼ˆmmWave radarï¼‰åœ¨æœºå™¨äººæŠ€æœ¯å’Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå°½ç®¡æ¯«ç±³æ³¢é›·è¾¾åœ¨æ¶åŠ£ç¯å¢ƒä¸‹çš„æ„ŸçŸ¥ç¨³å®šæ€§è‰¯å¥½ï¼Œä½†å…¶ç”Ÿæˆçš„ç‚¹äº‘ç›¸å¯¹ç¨€ç–ä¸”å«æœ‰è¾ƒå¤§å™ªå£°ï¼Œé™åˆ¶äº†å…¶è¿›ä¸€æ­¥å‘å±•ã€‚ä¼ ç»Ÿçš„æ¯«ç±³æ³¢é›·è¾¾å¢å¼ºæ–¹æ³•å¾€å¾€éš¾ä»¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºèŒƒå›´-æ–¹ä½çƒ­å›¾ï¼ˆRAHï¼‰æˆ–é¸Ÿç°å›¾ï¼ˆBEVï¼‰è¡¨ç¤ºçš„ä¸è‡ªç„¶èŒƒå›´ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§èåˆèŒƒå›´å›¾åƒä¸å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œå®ç°äº†å‡†ç¡®ä¸”å¯†é›†çš„æ¯«ç±³æ³¢é›·è¾¾ç‚¹äº‘ï¼Œç±»ä¼¼äºæ¿€å…‰é›·è¾¾ã€‚å—ç›Šäºä¸äººç±»è§‚å¯Ÿç›¸ä¸€è‡´çš„æŠ•å½±ï¼Œæ¯«ç±³æ³¢é›·è¾¾çš„èŒƒå›´å›¾åƒè¡¨ç¤ºæ¥è¿‘è‡ªç„¶å›¾åƒï¼Œä½¿å¾—é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†å¯ä»¥æœ‰æ•ˆåœ°è¿ç§»ï¼Œä»è€Œå¤§å¤§æé«˜äº†æ•´ä½“æ€§èƒ½ã€‚åœ¨å…¬å…±æ•°æ®é›†å’Œè‡ªæˆ‘æ„å»ºçš„æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”ŸæˆçœŸæ­£çš„ä¸‰ç»´æ¿€å…‰é›·è¾¾å¼ç‚¹äº‘æ–¹é¢æä¾›äº†å·¨å¤§æ”¹è¿›ï¼Œç¡®ç«‹äº†é€šè¿‡æ¯«ç±³æ³¢é›·è¾¾ç”Ÿæˆç‚¹äº‘çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02300v1">PDF</a> 8 pages, 7 figures, submitted to 2025 IROS. This work has been   submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æ¯«ç±³æ³¢é›·è¾¾åœ¨æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸå¤‡å—å…³æ³¨ï¼Œä½†å…¶ç”Ÿæˆçš„ç‚¹äº‘ç¨€ç–ä¸”å«æœ‰è¾ƒå¤šå™ªå£°ã€‚ä¼ ç»Ÿå¢å¼ºæ–¹æ³•éš¾ä»¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°è¶…åˆ†è¾¨ç‡æ•ˆæœã€‚æœ¬ç ”ç©¶åˆ›æ–°æ€§åœ°èåˆèŒƒå›´å›¾åƒä¸å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆå‡†ç¡®ã€å¯†é›†çš„æ¯«ç±³æ³¢é›·è¾¾ç‚¹äº‘ï¼Œç±»ä¼¼äºæ¿€å…‰é›·è¾¾ç‚¹äº‘ã€‚èŒƒå›´å›¾åƒè¡¨ç¤ºæ³•ä¸è‡ªç„¶å›¾åƒç›¸ä¼¼ï¼Œå¯å€ŸåŠ©é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹çŸ¥è¯†è¿›è¡Œæœ‰æ•ˆè¿ç§»ï¼Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚åœ¨å…¬å…±å’Œè‡ªåˆ¶æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†ç”ŸæˆçœŸæ­£ä¸‰ç»´æ¿€å…‰é›·è¾¾ç‚¹äº‘çš„æ€§èƒ½ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¯«ç±³æ³¢é›·è¾¾åœ¨æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸå—åˆ°å…³æ³¨ï¼Œä½†ç‚¹äº‘ç¨€ç–ä¸”å«å™ªå£°ï¼Œé™åˆ¶äº†å…¶å‘å±•ã€‚</li>
<li>ä¼ ç»Ÿå¢å¼ºæ–¹æ³•éš¾ä»¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°è¶…åˆ†è¾¨ç‡æ•ˆæœï¼Œå› ä¸ºèŒƒå›´æ–¹ä½çƒ­å›¾æˆ–é¸Ÿç°å›¾è¡¨ç¤ºæ–¹æ³•ä¸å¤Ÿè‡ªç„¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§èåˆèŒƒå›´å›¾åƒä¸å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”Ÿæˆå‡†ç¡®ã€å¯†é›†çš„æ¯«ç±³æ³¢é›·è¾¾ç‚¹äº‘ã€‚</li>
<li>èŒƒå›´å›¾åƒè¡¨ç¤ºæ³•æ¥è¿‘è‡ªç„¶å›¾åƒï¼Œæœ‰åˆ©äºé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹çŸ¥è¯†çš„è¿ç§»ã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆçš„ä¸‰ç»´ç‚¹äº‘ç±»ä¼¼äºæ¿€å…‰é›·è¾¾ç‚¹äº‘ã€‚</li>
<li>åœ¨å…¬å…±å’Œè‡ªåˆ¶æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ç”Ÿæˆæ¯«ç±³æ³¢é›·è¾¾ç‚¹äº‘çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ef7f1eb62f2c7eb0ebdfd95e7dd61c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c59dc938a59ba58a08ecd298a816dd81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddcbe50bb8c5e0e5c3f7536381fcb83d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10eedca9402b868688cea017b39f9a91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c696cd89622363eeabc97c345570e9b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="h-Edit-Effective-and-Flexible-Diffusion-Based-Editing-via-Doobâ€™s-h-Transform"><a href="#h-Edit-Effective-and-Flexible-Diffusion-Based-Editing-via-Doobâ€™s-h-Transform" class="headerlink" title="h-Edit: Effective and Flexible Diffusion-Based Editing via Doobâ€™s   h-Transform"></a>h-Edit: Effective and Flexible Diffusion-Based Editing via Doobâ€™s   h-Transform</h2><p><strong>Authors:Toan Nguyen, Kien Do, Duc Kieu, Thin Nguyen</strong></p>
<p>We introduce a theoretical framework for diffusion-based image editing by formulating it as a reverse-time bridge modeling problem. This approach modifies the backward process of a pretrained diffusion model to construct a bridge that converges to an implicit distribution associated with the editing target at time 0. Building on this framework, we propose h-Edit, a novel editing method that utilizes Doobâ€™s h-transform and Langevin Monte Carlo to decompose the update of an intermediate edited sample into two components: a â€œreconstructionâ€ term and an â€œeditingâ€ term. This decomposition provides flexibility, allowing the reconstruction term to be computed via existing inversion techniques and enabling the combination of multiple editing terms to handle complex editing tasks. To our knowledge, h-Edit is the first training-free method capable of performing simultaneous text-guided and reward-model-based editing. Extensive experiments, both quantitative and qualitative, show that h-Edit outperforms state-of-the-art baselines in terms of editing effectiveness and faithfulness. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/nktoan/h-edit">https://github.com/nktoan/h-edit</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘ç†è®ºæ¡†æ¶ï¼Œå°†å…¶åˆ¶å®šä¸ºä¸€ä¸ªåå‘æ—¶é—´æ¡¥æ¢å»ºæ¨¡é—®é¢˜ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä¿®æ”¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„é€†å‘è¿‡ç¨‹æ¥æ„å»ºä¸€åº§æ¡¥æ¢ï¼Œè¯¥æ¡¥æ¢åœ¨æ—¶é—´ä¸º0æ—¶æ”¶æ•›åˆ°ä¸ç¼–è¾‘ç›®æ ‡ç›¸å…³çš„éšå¼åˆ†å¸ƒã€‚åŸºäºè¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†h-Editï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç¼–è¾‘æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨Doobçš„hå˜æ¢å’ŒLangevin Monte Carloå°†ä¸­é—´ç¼–è¾‘æ ·æœ¬çš„æ›´æ–°åˆ†è§£ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šâ€œé‡å»ºâ€é¡¹å’Œâ€œç¼–è¾‘â€é¡¹ã€‚è¿™ç§åˆ†è§£æä¾›äº†çµæ´»æ€§ï¼Œå…è®¸é€šè¿‡ç°æœ‰çš„åæ¼”æŠ€æœ¯æ¥è®¡ç®—é‡å»ºé¡¹ï¼Œå¹¶èƒ½å¤Ÿç»“åˆå¤šä¸ªç¼–è¾‘é¡¹æ¥å¤„ç†å¤æ‚çš„ç¼–è¾‘ä»»åŠ¡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œh-Editæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ—¶è¿›è¡Œæ–‡æœ¬å¼•å¯¼å’ŒåŸºäºå¥–åŠ±æ¨¡å‹çš„ç¼–è¾‘ã€‚å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬å®šé‡å’Œå®šæ€§å®éªŒï¼Œè¡¨æ˜h-Editåœ¨ç¼–è¾‘æ•ˆæœå’Œå¿ å®åº¦æ–¹é¢è¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nktoan/h-edit%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nktoan/h-editä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02187v1">PDF</a> Accepted in CVPR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘ç†è®ºæ¡†æ¶ï¼Œå°†å…¶å»ºæ¨¡ä¸ºåå‘æ—¶é—´æ¡¥æ¨¡å‹é—®é¢˜ã€‚é€šè¿‡ä¿®æ”¹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é€†å‘è¿‡ç¨‹ï¼Œæ„å»ºä¸€åº§åœ¨æ—¶é—´0ä¸ç¼–è¾‘ç›®æ ‡ç›¸å…³è”çš„éšå¼åˆ†å¸ƒçš„æ¡¥æ¢ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†h-Editç¼–è¾‘æ–¹æ³•ï¼Œåˆ©ç”¨Doobçš„h-å˜æ¢å’ŒLangevin Monte Carloï¼Œå°†ä¸­é—´ç¼–è¾‘æ ·æœ¬çš„æ›´æ–°åˆ†è§£ä¸ºâ€œé‡å»ºâ€å’Œâ€œç¼–è¾‘â€ä¸¤éƒ¨åˆ†ã€‚è¿™ä½¿å¾—h-Editèƒ½ç»“åˆå¤šç§ç¼–è¾‘æŠ€æœ¯å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä¸”ä¸ºç›®å‰é¦–ä¸ªæ— éœ€è®­ç»ƒå³å¯åŒæ—¶è¿›è¡Œæ–‡æœ¬å¼•å¯¼å’Œå¥–åŠ±æ¨¡å‹ç¼–è¾‘çš„æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œh-Editåœ¨ç¼–è¾‘æ•ˆæœå’Œå¿ å®åº¦æ–¹é¢è¶…è¶Šç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸ºåŸºç¡€ï¼Œæ„å»ºäº†ä¸€ä¸ªåå‘æ—¶é—´æ¡¥æ¨¡å‹çš„ç†è®ºæ¡†æ¶ç”¨äºå›¾åƒç¼–è¾‘ã€‚</li>
<li>é€šè¿‡ä¿®æ”¹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é€†å‘è¿‡ç¨‹ï¼Œåˆ›å»ºäº†ä¸€åº§æ¡¥æ¢ä»¥è¿æ¥ç¼–è¾‘ç›®æ ‡å’Œéšå¼åˆ†å¸ƒã€‚</li>
<li>æå‡ºäº†h-Editç¼–è¾‘æ–¹æ³•ï¼Œåˆ©ç”¨Doobçš„h-å˜æ¢å’ŒLangevin Monte Carloè¿›è¡Œæ›´æ–°åˆ†è§£ã€‚</li>
<li>h-Editæ–¹æ³•å…è®¸å°†â€œé‡å»ºâ€å’Œâ€œç¼–è¾‘â€åˆ†å¼€å¤„ç†ï¼Œæé«˜äº†çµæ´»æ€§ã€‚</li>
<li>h-Editæ˜¯é¦–ä¸ªæ— éœ€è®­ç»ƒå³å¯ç»“åˆæ–‡æœ¬å¼•å¯¼å’Œå¥–åŠ±æ¨¡å‹è¿›è¡Œç¼–è¾‘çš„æ–¹æ³•ã€‚</li>
<li>å®éªŒè¯æ˜h-Editåœ¨ç¼–è¾‘æ•ˆæœå’Œå¿ å®åº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>h-Editçš„æºä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›ä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02187">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b550e99a0436f7dc1f12ae2dfe206703.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f60e686a5cdb6f978f86143be9f216d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa4acf5c2234370a67a65d74dc240d47.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HanDrawer-Leveraging-Spatial-Information-to-Render-Realistic-Hands-Using-a-Conditional-Diffusion-Model-in-Single-Stage"><a href="#HanDrawer-Leveraging-Spatial-Information-to-Render-Realistic-Hands-Using-a-Conditional-Diffusion-Model-in-Single-Stage" class="headerlink" title="HanDrawer: Leveraging Spatial Information to Render Realistic Hands   Using a Conditional Diffusion Model in Single Stage"></a>HanDrawer: Leveraging Spatial Information to Render Realistic Hands   Using a Conditional Diffusion Model in Single Stage</h2><p><strong>Authors:Qifan Fu, Xu Chen, Muhammad Asad, Shanxin Yuan, Changjae Oh, Gregory Slabaugh</strong></p>
<p>Although diffusion methods excel in text-to-image generation, generating accurate hand gestures remains a major challenge, resulting in severe artifacts, such as incorrect number of fingers or unnatural gestures. To enable the diffusion model to learn spatial information to improve the quality of the hands generated, we propose HanDrawer, a module to condition the hand generation process. Specifically, we apply graph convolutional layers to extract the endogenous spatial structure and physical constraints implicit in MANO hand mesh vertices. We then align and fuse these spatial features with other modalities via cross-attention. The spatially fused features are used to guide a single stage diffusion model denoising process for high quality generation of the hand region. To improve the accuracy of spatial feature fusion, we propose a Position-Preserving Zero Padding (PPZP) fusion strategy, which ensures that the features extracted by HanDrawer are fused into the region of interest in the relevant layers of the diffusion model. HanDrawer learns the entire image features while paying special attention to the hand region thanks to an additional hand reconstruction loss combined with the denoising loss. To accurately train and evaluate our approach, we perform careful cleansing and relabeling of the widely used HaGRID hand gesture dataset and obtain high quality multimodal data. Quantitative and qualitative analyses demonstrate the state-of-the-art performance of our method on the HaGRID dataset through multiple evaluation metrics. Source code and our enhanced dataset will be released publicly if the paper is accepted. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ–¹æ³•åœ¨å›¾æ–‡ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”Ÿæˆå‡†ç¡®çš„æ‰‹åŠ¿ä»æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œä¼šå¯¼è‡´ä¸¥é‡çš„äººå·¥ç—•è¿¹ï¼Œä¾‹å¦‚æ‰‹æŒ‡æ•°é‡ä¸æ­£ç¡®æˆ–æ‰‹åŠ¿ä¸è‡ªç„¶ã€‚ä¸ºäº†èƒ½å¤Ÿè®©æ‰©æ•£æ¨¡å‹å­¦ä¹ ç©ºé—´ä¿¡æ¯ä»¥æé«˜ç”Ÿæˆçš„æ‰‹éƒ¨è´¨é‡ï¼Œæˆ‘ä»¬æå‡ºäº†HanDraweræ¨¡å—ï¼Œç”¨äºè°ƒèŠ‚æ‰‹éƒ¨ç”Ÿæˆè¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åº”ç”¨å›¾å·ç§¯å±‚æ¥æå–MANOæ‰‹éƒ¨ç½‘æ ¼é¡¶ç‚¹ä¸­éšå«çš„å†…ç”Ÿç©ºé—´ç»“æ„å’Œç‰©ç†çº¦æŸã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡äº¤å‰æ³¨æ„åŠ›å°†è¿™äº›ç©ºé—´ç‰¹å¾ä¸å…¶ä»–æ¨¡æ€è¿›è¡Œå¯¹é½å’Œèåˆã€‚è¿™äº›ç©ºé—´èåˆçš„ç‰¹å¾è¢«ç”¨æ¥å¼•å¯¼å•é˜¶æ®µæ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ï¼Œä»¥é«˜è´¨é‡ç”Ÿæˆæ‰‹éƒ¨åŒºåŸŸã€‚ä¸ºäº†æé«˜ç©ºé—´ç‰¹å¾èåˆçš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½ç½®ä¿æŒé›¶å¡«å……ï¼ˆPPZPï¼‰èåˆç­–ç•¥ï¼Œç¡®ä¿HanDraweræå–çš„ç‰¹å¾èåˆåˆ°æ‰©æ•£æ¨¡å‹ç›¸å…³å±‚çš„æ„Ÿå…´è¶£åŒºåŸŸä¸­ã€‚HanDraweråœ¨å…³æ³¨æ‰‹éƒ¨åŒºåŸŸçš„åŒæ—¶å­¦ä¹ æ•´ä¸ªå›¾åƒçš„ç‰¹å¾ï¼Œè¿™å¾—ç›Šäºä¸å»å™ªæŸå¤±ç›¸ç»“åˆçš„æ‰‹éƒ¨é‡å»ºæŸå¤±ã€‚ä¸ºäº†å‡†ç¡®è®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯¹å¹¿æ³›ä½¿ç”¨çš„HaGRIDæ‰‹åŠ¿æ•°æ®é›†è¿›è¡Œäº†ä»”ç»†æ¸…ç†å’Œé‡æ–°æ ‡æ³¨ï¼Œè·å¾—äº†é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®ã€‚å®šé‡å’Œå®šæ€§åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨HaGRIDæ•°æ®é›†ä¸Šé€šè¿‡å¤šä¸ªè¯„ä»·æŒ‡æ ‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å¦‚æœè®ºæ–‡è¢«æ¥å—ï¼Œæˆ‘ä»¬å°†å…¬å¼€æºä»£ç å’Œå¢å¼ºçš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02127v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHanDrawerçš„æ¨¡å—ï¼Œç”¨äºæ”¹å–„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ‰‹éƒ¨æ—¶çš„å‡†ç¡®æ€§é—®é¢˜ã€‚é€šè¿‡åº”ç”¨å›¾å·ç§¯å±‚æå–MANOæ‰‹ç½‘æ ¼é¡¶ç‚¹ä¸­çš„å†…æºæ€§ç©ºé—´ç»“æ„å’Œç‰©ç†çº¦æŸï¼Œç»“åˆè·¨æ³¨æ„åŠ›æœºåˆ¶ä¸å…¶ä»–æ¨¡æ€ç‰¹å¾å¯¹é½èåˆã€‚èåˆåçš„ç‰¹å¾è¢«ç”¨äºå¼•å¯¼å•é˜¶æ®µæ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ï¼Œä»¥æé«˜æ‰‹éƒ¨åŒºåŸŸçš„ç”Ÿæˆè´¨é‡ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§ä½ç½®ä¿æŒé›¶å¡«å……ï¼ˆPPZPï¼‰èåˆç­–ç•¥ï¼Œç¡®ä¿HanDraweræå–çš„ç‰¹å¾èƒ½å¤Ÿèåˆåˆ°æ‰©æ•£æ¨¡å‹çš„ç›¸å…³å±‚ä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸã€‚é€šè¿‡å¯¹æ‰‹éƒ¨é‡å»ºæŸå¤±ä¸å»å™ªæŸå¤±çš„ç»“åˆï¼ŒHanDraweråœ¨å…³æ³¨æ‰‹éƒ¨åŒºåŸŸçš„åŒæ—¶å­¦ä¹ æ•´ä¸ªå›¾åƒç‰¹å¾ã€‚å¯¹å¹¿æ³›ä½¿ç”¨çš„HaGRIDæ‰‹åŠ¿æ•°æ®é›†è¿›è¡Œäº†ç²¾å¿ƒæ¸…ç†å’Œé‡æ–°æ ‡æ³¨ï¼Œè·å¾—äº†é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®ã€‚å®šé‡å’Œå®šæ€§åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨HaGRIDæ•°æ®é›†ä¸Šçš„è¡¨ç°è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç”Ÿæˆæ‰‹éƒ¨æ—¶å­˜åœ¨å‡†ç¡®æ€§çš„æŒ‘æˆ˜ï¼Œå¦‚æ‰‹åŠ¿çš„ç”Ÿæˆå®¹æ˜“å‡ºç°é”™è¯¯ã€‚</li>
<li>HanDraweræ¨¡å—è¢«æå‡ºæ¥æ”¹å–„è¿™ä¸ªé—®é¢˜ï¼Œå®ƒé€šè¿‡å›¾å·ç§¯å±‚æå–MANOæ‰‹ç½‘æ ¼ä¸­çš„ç©ºé—´ä¿¡æ¯å’Œç‰©ç†çº¦æŸã€‚</li>
<li>HanDrawerä½¿ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶å°†ç©ºé—´ç‰¹å¾ä¸å…¶ä»–æ¨¡æ€è¿›è¡Œå¯¹é½å’Œèåˆã€‚</li>
<li>èåˆåçš„ç‰¹å¾è¢«ç”¨äºå¼•å¯¼æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ï¼Œä»è€Œæé«˜æ‰‹éƒ¨åŒºåŸŸçš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç‰¹å¾èåˆç­–ç•¥â€”â€”ä½ç½®ä¿æŒé›¶å¡«å……ï¼ˆPPZPï¼‰ï¼Œç¡®ä¿ç‰¹å¾åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„æ­£ç¡®èåˆã€‚</li>
<li>HanDraweré€šè¿‡ç»“åˆæ‰‹éƒ¨é‡å»ºæŸå¤±å’Œå»å™ªæŸå¤±ï¼Œèƒ½å¤Ÿåœ¨å…³æ³¨æ‰‹éƒ¨åŒºåŸŸçš„åŒæ—¶å­¦ä¹ æ•´ä¸ªå›¾åƒçš„ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8ce1762e89f538f90a20cd061aa4466a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b60765818206574f37a8301a18c63df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b320e7dde6cf250b5d6fc37040becb67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86d4254337ae5c4cdbfa9184c7261ca6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb55053e7782e6115b2e73bcd19f1b1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Generalized-Diffusion-Detector-Mining-Robust-Features-from-Diffusion-Models-for-Domain-Generalized-Detection"><a href="#Generalized-Diffusion-Detector-Mining-Robust-Features-from-Diffusion-Models-for-Domain-Generalized-Detection" class="headerlink" title="Generalized Diffusion Detector: Mining Robust Features from Diffusion   Models for Domain-Generalized Detection"></a>Generalized Diffusion Detector: Mining Robust Features from Diffusion   Models for Domain-Generalized Detection</h2><p><strong>Authors:Boyong He, Yuxiang Ji, Qianwen Ye, Zhuoyue Tan, Liaoni Wu</strong></p>
<p>Domain generalization (DG) for object detection aims to enhance detectorsâ€™ performance in unseen scenarios. This task remains challenging due to complex variations in real-world applications. Recently, diffusion models have demonstrated remarkable capabilities in diverse scene generation, which inspires us to explore their potential for improving DG tasks. Instead of generating images, our method extracts multi-step intermediate features during the diffusion process to obtain domain-invariant features for generalized detection. Furthermore, we propose an efficient knowledge transfer framework that enables detectors to inherit the generalization capabilities of diffusion models through feature and object-level alignment, without increasing inference time. We conduct extensive experiments on six challenging DG benchmarks. The results demonstrate that our method achieves substantial improvements of 14.0% mAP over existing DG approaches across different domains and corruption types. Notably, our method even outperforms most domain adaptation methods without accessing any target domain data. Moreover, the diffusion-guided detectors show consistent improvements of 15.9% mAP on average compared to the baseline. Our work aims to present an effective approach for domain-generalized detection and provide potential insights for robust visual recognition in real-world scenarios. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/heboyong/Generalized-Diffusion-Detector%7D%7BGeneralized">https://github.com/heboyong/Generalized-Diffusion-Detector}{Generalized</a> Diffusion Detector} </p>
<blockquote>
<p>ç›®æ ‡æ£€æµ‹ä¸­çš„åŸŸæ³›åŒ–ï¼ˆDGï¼‰æ—¨åœ¨å¢å¼ºæ£€æµ‹å™¨åœ¨æœªè§åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚ç”±äºçœŸå®ä¸–ç•Œåº”ç”¨ä¸­å¤æ‚çš„å˜é‡ï¼Œè¿™é¡¹ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨åœºæ™¯ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œè¿™æ¿€å‘äº†æˆ‘ä»¬æ¢ç´¢å…¶æé«˜DGä»»åŠ¡çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸åŒäºç”Ÿæˆå›¾åƒï¼Œè€Œæ˜¯åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­æå–å¤šæ­¥ä¸­é—´ç‰¹å¾ï¼Œä»¥è·å¾—ç”¨äºé€šç”¨æ£€æµ‹çš„åŸŸä¸å˜ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»æ¡†æ¶ï¼Œä½¿æ£€æµ‹å™¨èƒ½å¤Ÿé€šè¿‡ç‰¹å¾å’Œå¯¹è±¡çº§åˆ«çš„å¯¹é½ï¼Œç»§æ‰¿æ‰©æ•£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸ä¼šå¢åŠ æ¨ç†æ—¶é—´ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„DGåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨ä¸åŒé¢†åŸŸå’Œè…è´¥ç±»å‹çš„æƒ…å†µä¸‹ï¼Œè¾ƒç°æœ‰çš„DGæ–¹æ³•æé«˜äº†14.0%çš„mAPã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— éœ€è®¿é—®ä»»ä½•ç›®æ ‡åŸŸæ•°æ®çš„æƒ…å†µä¸‹ï¼Œç”šè‡³è¶…è¶Šäº†å¤§å¤šæ•°é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œæ‰©æ•£å¼•å¯¼çš„æ£€æµ‹å™¨å¹³å‡æé«˜äº†15.9%çš„mAPã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨æä¾›ä¸€ç§æœ‰æ•ˆçš„åŸŸæ³›åŒ–æ£€æµ‹æ–¹æ³•å’Œä¸ºçœŸå®ä¸–ç•Œåœºæ™¯çš„é²æ£’è§†è§‰è¯†åˆ«æä¾›æ½œåœ¨è§è§£ã€‚[è¯¥é¡¹ç›®ä»£ç å¯åœ¨Generalized Diffusion Detectorï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/heboyong/Generalized-Diffusion-Detector%EF%BC%89%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/heboyong/Generalized-Diffusion-Detectorï¼‰ä¸Šè·å–ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02101v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨åŸŸæ³›åŒ–ï¼ˆDGï¼‰ç›®æ ‡æ£€æµ‹ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚é€šè¿‡æå–æ‰©æ•£è¿‡ç¨‹ä¸­çš„å¤šæ­¥ä¸­é—´ç‰¹å¾ï¼Œè·å¾—åŸŸä¸å˜ç‰¹å¾ï¼Œæé«˜æ£€æµ‹å™¨çš„æ³›åŒ–æ€§èƒ½ã€‚æå‡ºé«˜æ•ˆçŸ¥è¯†è½¬ç§»æ¡†æ¶ï¼Œä½¿æ£€æµ‹å™¨ç»§æ‰¿æ‰©æ•£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡ç‰¹å¾å’Œå¯¹è±¡çº§åˆ«çš„å¯¹é½ï¼Œåœ¨ä¸å¢åŠ æ¨ç†æ—¶é—´çš„æƒ…å†µä¸‹å®ç°ã€‚åœ¨å…­ä¸ªæŒ‘æˆ˜æ€§çš„DGåŸºå‡†æµ‹è¯•ä¸Šå®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸå’Œè…è´¥ç±»å‹ä¸Šæ¯”ç°æœ‰DGæ–¹æ³•æé«˜äº†14.0%çš„mAPã€‚ç”šè‡³åœ¨æ²¡æœ‰è®¿é—®ä»»ä½•ç›®æ ‡åŸŸæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨åŸŸé€‚åº”æ–¹æ³•ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚æ‰©æ•£å¼•å¯¼çš„æ£€æµ‹å™¨ä¸åŸºçº¿ç›¸æ¯”ï¼Œå¹³å‡æé«˜äº†15.9%çš„mAPã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŸŸæ³›åŒ–ç›®æ ‡æ£€æµ‹ä¸­å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>é€šè¿‡æå–æ‰©æ•£è¿‡ç¨‹ä¸­çš„ä¸­é—´ç‰¹å¾ï¼Œè·å¾—åŸŸä¸å˜ç‰¹å¾ï¼Œæé«˜æ£€æµ‹å™¨æ€§èƒ½ã€‚</li>
<li>æå‡ºä¸€ç§é«˜æ•ˆçŸ¥è¯†è½¬ç§»æ¡†æ¶ï¼Œä½¿æ£€æµ‹å™¨èƒ½å¤Ÿç»§æ‰¿æ‰©æ•£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªDGåŸºå‡†æµ‹è¯•ä¸Šå®éªŒï¼Œç»“æœæ˜¾ç¤ºæ˜¾è‘—æé«˜äº†mAPã€‚</li>
<li>æ–¹æ³•åœ¨ä¸è®¿é—®ç›®æ ‡åŸŸæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¶…è¿‡äº†ä¸€äº›åŸŸé€‚åº”æ–¹æ³•ã€‚</li>
<li>æ‰©æ•£å¼•å¯¼çš„æ£€æµ‹å™¨ç›¸è¾ƒäºåŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6a347839a04807ba8320a88f00fb415.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e855bef69f75eca887a5383fd2faf7ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46995828e5e1c5747eae206f55438533.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a699f6c46c139f240dba35e62874c2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd024701954b164a90678d8ce10e0397.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FRMD-Fast-Robot-Motion-Diffusion-with-Consistency-Distilled-Movement-Primitives-for-Smooth-Action-Generation"><a href="#FRMD-Fast-Robot-Motion-Diffusion-with-Consistency-Distilled-Movement-Primitives-for-Smooth-Action-Generation" class="headerlink" title="FRMD: Fast Robot Motion Diffusion with Consistency-Distilled Movement   Primitives for Smooth Action Generation"></a>FRMD: Fast Robot Motion Diffusion with Consistency-Distilled Movement   Primitives for Smooth Action Generation</h2><p><strong>Authors:Xirui Shi, Jun Jin</strong></p>
<p>We consider the problem of using diffusion models to generate fast, smooth, and temporally consistent robot motions. Although diffusion models have demonstrated superior performance in robot learning due to their task scalability and multi-modal flexibility, they suffer from two fundamental limitations: (1) they often produce non-smooth, jerky motions due to their inability to capture temporally consistent movement dynamics, and (2) their iterative sampling process incurs prohibitive latency for many robotic tasks. Inspired by classic robot motion generation methods such as DMPs and ProMPs, which capture temporally and spatially consistent dynamic of trajectories using low-dimensional vectors â€“ and by recent advances in diffusion-based image generation that use consistency models with probability flow ODEs to accelerate the denoising process, we propose Fast Robot Motion Diffusion (FRMD). FRMD uniquely integrates Movement Primitives (MPs) with Consistency Models to enable efficient, single-step trajectory generation. By leveraging probabilistic flow ODEs and consistency distillation, our method models trajectory distributions while learning a compact, time-continuous motion representation within an encoder-decoder architecture. This unified approach eliminates the slow, multi-step denoising process of conventional diffusion models, enabling efficient one-step inference and smooth robot motion generation. We extensively evaluated our FRMD on the well-recognized Meta-World and ManiSkills Benchmarks, ranging from simple to more complex manipulation tasks, comparing its performance against state-of-the-art baselines. Our results show that FRMD generates significantly faster, smoother trajectories while achieving higher success rates. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶å¦‚ä½•ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¿«é€Ÿã€æµç•…ã€æ—¶é—´ä¸Šä¸€è‡´çš„æœºå™¨äººè¿åŠ¨ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨æœºå™¨äººå­¦ä¹ æ–¹é¢ç”±äºå…¶ä»»åŠ¡å¯æ‰©å±•æ€§å’Œå¤šæ¨¡å¼çµæ´»æ€§è€Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸¤ä¸ªåŸºæœ¬å±€é™ï¼šä¸€æ˜¯å®ƒä»¬é€šå¸¸æ— æ³•æ•æ‰æ—¶é—´ä¸Šä¸€è‡´çš„è¿åŠ¨åŠ¨åŠ›å­¦ï¼Œä»è€Œäº§ç”Ÿéæµç•…ã€ç¬¨æ‹™çš„è¿åŠ¨ï¼›äºŒæ˜¯å…¶è¿­ä»£é‡‡æ ·è¿‡ç¨‹å¯¹äºè®¸å¤šæœºå™¨äººä»»åŠ¡è€Œè¨€ä¼šå¼•å…¥è¿‡é«˜çš„å»¶è¿Ÿã€‚æˆ‘ä»¬å—åˆ°ç»å…¸æœºå™¨äººè¿åŠ¨ç”Ÿæˆæ–¹æ³•ï¼ˆå¦‚DMPå’ŒProMPï¼‰çš„å¯å‘ï¼Œè¿™äº›æ–¹æ³•ä½¿ç”¨ä½ç»´å‘é‡æ•æ‰è½¨è¿¹çš„æ—¶é—´å’Œç©ºé—´ä¸€è‡´æ€§åŠ¨æ€â€”â€”ä»¥åŠæœ€è¿‘åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆçš„è¿›å±•ï¼Œè¿™äº›è¿›å±•ä½¿ç”¨å…·æœ‰æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹çš„å…±è¯†æ¨¡å‹æ¥åŠ é€Ÿå»å™ªè¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†å¿«é€Ÿæœºå™¨äººè¿åŠ¨æ‰©æ•£ï¼ˆFRMDï¼‰ã€‚FRMDç‹¬ç‰¹åœ°å°†è¿åŠ¨åŸè¯­ï¼ˆMPsï¼‰ä¸ä¸€è‡´æ€§æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥å®ç°é«˜æ•ˆçš„ä¸€æ­¥è½¨è¿¹ç”Ÿæˆã€‚é€šè¿‡åˆ©ç”¨æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹å’Œä¸€è‡´æ€§è’¸é¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å»ºæ¨¡è½¨è¿¹åˆ†å¸ƒçš„åŒæ—¶ï¼Œåœ¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­å­¦ä¹ ç´§å‡‘ã€æ—¶é—´è¿ç»­çš„è¿åŠ¨è¡¨ç¤ºã€‚è¿™ç§ç»Ÿä¸€çš„æ–¹æ³•æ¶ˆé™¤äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹çš„ç¼“æ…¢å¤šæ­¥å»å™ªè¿‡ç¨‹ï¼Œå®ç°äº†ä¸€æ¬¡æ€§æ¨æ–­å’Œæµç•…çš„æœºå™¨äººè¿åŠ¨ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨å…¬è®¤çš„Meta-Worldå’ŒManiSkillsåŸºå‡†æµ‹è¯•ä¸Šå¯¹FRMDè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæ¶µç›–äº†ä»ç®€å•åˆ°å¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œå¹¶å°†å…¶æ€§èƒ½ä¸æœ€æ–°åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒFRMDç”Ÿæˆçš„è½¨è¿¹æ˜¾è‘—æ›´å¿«ã€æ›´æµç•…ï¼ŒåŒæ—¶æˆåŠŸç‡æ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02048v1">PDF</a> arXiv admin note: text overlap with arXiv:2406.01586 by other authors</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¿«é€Ÿã€å¹³æ»‘ä¸”æ—¶é—´ä¸€è‡´çš„æœºå™¨äººè¿åŠ¨çš„é—®é¢˜ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨æœºå™¨äººå­¦ä¹ ä¸­çš„ä¸¤ä¸ªä¸»è¦å±€é™â€”â€”äº§ç”Ÿéå¹³æ»‘ã€æ–­æ–­ç»­ç»­çš„è¿åŠ¨ä»¥åŠè¿­ä»£é‡‡æ ·è¿‡ç¨‹å¯¼è‡´çš„å»¶è¿Ÿï¼Œæå‡ºäº†Fast Robot Motion Diffusion (FRMD)æ–¹æ³•ã€‚FRMDç»“åˆäº†è¿åŠ¨åŸè¯­å’Œä¸€è‡´æ€§æ¨¡å‹ï¼Œé‡‡ç”¨æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹å’Œä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œåœ¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„å†…å­¦ä¹ ç´§å‡‘ã€æ—¶é—´è¿ç»­çš„è¿åŠ¨è¡¨ç¤ºã€‚è¯¥æ–¹æ³•å®ç°äº†é«˜æ•ˆçš„ä¸€æ­¥æ¨ç†ï¼Œç”Ÿæˆå¹³æ»‘çš„æœºå™¨äººè¿åŠ¨ã€‚åœ¨Meta-Worldå’ŒManiSkills Benchmarkä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFRMDç”Ÿæˆçš„è¿åŠ¨æ›´å¿«ã€æ›´å¹³æ»‘ï¼Œä¸”æˆåŠŸç‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºæœºå™¨äººè¿åŠ¨ç”Ÿæˆé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šéå¹³æ»‘ã€æ–­æ–­ç»­ç»­çš„è¿åŠ¨ä»¥åŠé«˜å»¶è¿Ÿã€‚</li>
<li>Fast Robot Motion Diffusion (FRMD)æ–¹æ³•ç»“åˆè¿åŠ¨åŸè¯­å’Œä¸€è‡´æ€§æ¨¡å‹æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>FRMDåˆ©ç”¨æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹å’Œä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œåœ¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„å†…å­¦ä¹ è¿åŠ¨è¡¨ç¤ºã€‚</li>
<li>FRMDå®ç°äº†é«˜æ•ˆçš„ä¸€æ­¥æ¨ç†ï¼Œé¿å…äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè¿‡ç¨‹ã€‚</li>
<li>FRMDåœ¨Meta-Worldå’ŒManiSkills Benchmarkä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç”Ÿæˆçš„è¿åŠ¨æ›´å¿«ã€æ›´å¹³æ»‘ï¼Œä¸”æˆåŠŸç‡æ›´é«˜ã€‚</li>
<li>FRMDæ–¹æ³•æ•´åˆäº†ç»å…¸æœºå™¨äººè¿åŠ¨ç”Ÿæˆæ–¹æ³•å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b09e78c92f0045918f369a2de4982061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73fdeb40aa8bc302ff28b13c2bbf9451.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4854aca2020c759747db38f33874bc71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-398e4530df7d81713228342b0c014c7b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bc2bced597d394b5854e8db41cd2ea2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Difix3D-Improving-3D-Reconstructions-with-Single-Step-Diffusion-Models"><a href="#Difix3D-Improving-3D-Reconstructions-with-Single-Step-Diffusion-Models" class="headerlink" title="Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models"></a>Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models</h2><p><strong>Authors:Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling</strong></p>
<p>Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2$\times$ improvement in FID score over baselines while maintaining 3D consistency. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºå’Œ3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯å·²ç»å½»åº•æ”¹å˜äº†3Dé‡å»ºå’Œæ–°å‹è§†å›¾åˆæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œä»æç«¯æ–°å‹è§†è§’å®ç°ç…§ç‰‡çº§çœŸå®çš„æ¸²æŸ“ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºåœ¨å„ç§è¡¨ç¤ºä¸­ä»ç„¶å­˜åœ¨ä¼ªå½±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Difix3D+ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹ç®¡é“ï¼Œæ—¨åœ¨é€šè¿‡å•æ­¥æ‰©æ•£æ¨¡å‹å¢å¼º3Dé‡å»ºå’Œæ–°å‹è§†å›¾åˆæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯Difixï¼Œè¿™æ˜¯ä¸€ç§å•æ­¥å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç»è¿‡è®­ç»ƒç”¨äºå¢å¼ºå’Œæ¶ˆé™¤ç”±äº3Dè¡¨ç¤ºçš„çº¦æŸä¸è¶³è€Œåœ¨æ¸²æŸ“çš„æ–°è§†å›¾ä¸­äº§ç”Ÿçš„ä¼ªå½±ã€‚Difixåœ¨æˆ‘ä»¬çš„ç®¡é“ä¸­æ‰®æ¼”äº†ä¸¤ä¸ªå…³é”®è§’è‰²ã€‚é¦–å…ˆï¼Œå®ƒç”¨äºé‡å»ºé˜¶æ®µï¼Œæ¸…ç†ä»é‡å»ºä¸­æ¸²æŸ“å¹¶å†è’¸é¦å›3Dçš„ä¼ªè®­ç»ƒè§†å›¾ã€‚è¿™æå¤§åœ°å¢å¼ºäº†çº¦æŸä¸è¶³çš„åŒºåŸŸï¼Œæé«˜äº†æ•´ä½“çš„3Dè¡¨ç¤ºè´¨é‡ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒDifixè¿˜å……å½“ç¥ç»å¢å¼ºå™¨ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†ç”±äºä¸å®Œç¾çš„3Dç›‘ç£å’Œå½“å‰é‡å»ºæ¨¡å‹çš„æœ‰é™å®¹é‡è€Œäº§ç”Ÿçš„æ®‹ä½™ä¼ªå½±ã€‚Difix3D+æ˜¯ä¸€ç§é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œä¸€ä¸ªä¸NeRFå’Œ3DGSè¡¨ç¤ºå…¼å®¹çš„å•æ¨¡å‹ï¼Œå®ƒåœ¨ä¿æŒ3Dä¸€è‡´æ€§çš„åŒæ—¶ï¼Œç›¸è¾ƒäºåŸºçº¿å®ç°äº†å¹³å‡2å€çš„FIDåˆ†æ•°æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01774v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¾å°„åœºä¸ä¸‰ç»´é«˜æ–¯æ‹¼è´´æŠ€æœ¯å·²å¯¹ä¸‰ç»´é‡å»ºä¸æ–°å‹è§†è§’åˆæˆäº§ç”Ÿæ·±è¿œå½±å“ï¼Œä½†åœ¨æç«¯æ–°è§†è§’å®ç°é€¼çœŸæ¸²æŸ“ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¡¨ç°ä¸­çš„ç‘•ç–µä¸€ç›´å›°æ‰°ç€è¡¨ç°å±‚ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€æ¬¾åä¸ºDifix3D+çš„æ–°å‹ç®¡é“ï¼Œå®ƒé€šè¿‡å•æ­¥æ‰©æ•£æ¨¡å‹å¼ºåŒ–äº†ä¸‰ç»´é‡å»ºä¸æ–°å‹è§†è§’åˆæˆã€‚å…¶æ ¸å¿ƒæ–¹æ³•æ˜¯ä¸€ç§è®­ç»ƒæœ‰ç´ çš„å•æ­¥å›¾åƒæ‰©æ•£æ¨¡å‹â€”â€”Difixï¼Œå…¶é’ˆå¯¹å› ä¸‰ç»´è¡¨ç°æ¬ çº¦æŸåŒºåŸŸå¯¼è‡´çš„æ–°å‹æ¸²æŸ“è§†è§’ä¸­çš„ç‘•ç–µè¿›è¡Œå¢å¼ºä¸ç§»é™¤ã€‚Difixåœ¨ç®¡é“ä¸­æ‰®æ¼”åŒé‡è§’è‰²ï¼šé¦–å…ˆï¼Œåœ¨é‡å»ºé˜¶æ®µï¼Œå®ƒæ¸…ç†ä»é‡å»ºä¸­æ¸²æŸ“å‡ºçš„ä¼ªè®­ç»ƒè§†å›¾ï¼Œå¹¶å°†å…¶è’¸é¦å›ä¸‰ç»´ç©ºé—´ï¼Œæå¤§åœ°å¢å¼ºäº†æ¬ çº¦æŸåŒºåŸŸå¹¶æé«˜äº†æ•´ä½“ä¸‰ç»´è¡¨ç°è´¨é‡ï¼›æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒä½œä¸ºç¥ç»å¢å¼ºå™¨æœ‰æ•ˆæ¶ˆé™¤äº†å› ä¸‰ç»´ç›‘ç£ä¸è¶³å’Œå½“å‰é‡å»ºæ¨¡å‹å®¹é‡æœ‰é™è€Œäº§ç”Ÿçš„æ®‹ä½™ç‘•ç–µã€‚Difix3D+æ˜¯ä¸€ç§é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œä¸€ä¸ªå…¼å®¹NeRFå’Œ3DGSè¡¨ç°çš„å•ä¸€æ¨¡å‹ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨FIDå¾—åˆ†ä¸Šå¹³å‡æé«˜äº†ä¸¤å€ï¼ŒåŒæ—¶ä¿æŒäº†ä¸‰ç»´ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œè¾å°„åœºä¸ä¸‰ç»´é«˜æ–¯æ‹¼è´´æŠ€æœ¯å·²å¹¿æ³›åº”ç”¨äºä¸‰ç»´é‡å»ºä¸æ–°å‹è§†è§’åˆæˆã€‚</li>
<li>åœ¨æç«¯æ–°è§†è§’çš„é€¼çœŸæ¸²æŸ“ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¡¨ç°ä¸ºè¡¨ç°ä¸­çš„ç‘•ç–µã€‚</li>
<li>Difix3D+æ˜¯ä¸€ç§æ–°å‹ç®¡é“ï¼Œé€šè¿‡å•æ­¥æ‰©æ•£æ¨¡å‹å¼ºåŒ–ä¸‰ç»´é‡å»ºä¸æ–°å‹è§†è§’åˆæˆã€‚</li>
<li>æ ¸å¿ƒæ–¹æ³•â€”â€”Difixæ˜¯ä¸€ç§å•æ­¥å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¢å¼ºå¹¶ç§»é™¤å› ä¸‰ç»´è¡¨ç°æ¬ çº¦æŸåŒºåŸŸå¯¼è‡´çš„æ–°å‹æ¸²æŸ“è§†è§’ä¸­çš„ç‘•ç–µã€‚</li>
<li>Difixåœ¨ç®¡é“ä¸­æ‰®æ¼”åŒé‡è§’è‰²ï¼Œæ—¢åœ¨é‡å»ºé˜¶æ®µæ¸…ç†ä¼ªè®­ç»ƒè§†å›¾ï¼Œåˆåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½œä¸ºç¥ç»å¢å¼ºå™¨æ¶ˆé™¤æ®‹ä½™ç‘•ç–µã€‚</li>
<li>Difix3D+å…¼å®¹NeRFå’Œ3DGSè¡¨ç°ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨FIDå¾—åˆ†ä¸Šæœ‰æ‰€æé«˜ï¼ŒåŒæ—¶ä¿æŒäº†ä¸‰ç»´ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a75e326b590d50fda10ba198bf2d16c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-386bba37085eb54dd2daee0af8dfe5c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47cfbf3b63feeb9d4654954352571447.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04dbcc58e6d6212b602cf89b86e2c3ad.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ToLo-A-Two-Stage-Training-Free-Layout-To-Image-Generation-Framework-For-High-Overlap-Layouts"><a href="#ToLo-A-Two-Stage-Training-Free-Layout-To-Image-Generation-Framework-For-High-Overlap-Layouts" class="headerlink" title="ToLo: A Two-Stage, Training-Free Layout-To-Image Generation Framework   For High-Overlap Layouts"></a>ToLo: A Two-Stage, Training-Free Layout-To-Image Generation Framework   For High-Overlap Layouts</h2><p><strong>Authors:Linhao Huang, Jing Yu</strong></p>
<p>Recent training-free layout-to-image diffusion models have demonstrated remarkable performance in generating high-quality images with controllable layouts. These models follow a one-stage framework: Encouraging the model to focus the attention map of each concept on its corresponding region by defining attention map-based losses. However, these models still struggle to accurately follow layouts with significant overlap, often leading to issues like attribute leakage and missing entities. In this paper, we propose ToLo, a two-stage, training-free layout-to-image generation framework for high-overlap layouts. Our framework consists of two stages: the aggregation stage and the separation stage, each with its own loss function based on the attention map. To provide a more effective evaluation, we partition the HRS dataset based on the Intersection over Union (IoU) of the input layouts, creating a new dataset for layout-to-image generation with varying levels of overlap. Through extensive experiments on this dataset, we demonstrate that ToLo significantly enhances the performance of existing methods when dealing with high-overlap layouts. Our code and dataset are available here: <a target="_blank" rel="noopener" href="https://github.com/misaka12435/ToLo">https://github.com/misaka12435/ToLo</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„æ— è®­ç»ƒå¸ƒå±€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå…·æœ‰å¯æ§å¸ƒå±€çš„é«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹éµå¾ªä¸€ä¸ªé˜¶æ®µæ¡†æ¶ï¼šé€šè¿‡å®šä¹‰åŸºäºæ³¨æ„åŠ›å›¾çš„æŸå¤±æ¥é¼“åŠ±æ¨¡å‹å°†æ¯ä¸ªæ¦‚å¿µçš„é‡ç‚¹æ”¾åœ¨ç›¸åº”çš„åŒºåŸŸä¸Šã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†æœ‰è¾ƒå¤§é‡å çš„å¸ƒå±€æ—¶ï¼Œä»éš¾ä»¥å‡†ç¡®è·Ÿéšå¸ƒå±€ï¼Œç»å¸¸å¯¼è‡´å±æ€§æ³„æ¼å’Œç¼ºå¤±å®ä½“ç­‰é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ToLoï¼Œä¸€ä¸ªç”¨äºé«˜é‡å å¸ƒå±€çš„ä¸¤é˜¶æ®µæ— è®­ç»ƒå¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆï¼šèšåˆé˜¶æ®µå’Œåˆ†ç¦»é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰è‡ªå·±çš„åŸºäºæ³¨æ„åŠ›å›¾çš„æŸå¤±å‡½æ•°ã€‚ä¸ºäº†æä¾›æ›´æœ‰æ•ˆçš„è¯„ä¼°ï¼Œæˆ‘ä»¬æ ¹æ®è¾“å…¥å¸ƒå±€çš„äº¤å¹¶æ¯”ï¼ˆIoUï¼‰å¯¹HRSæ•°æ®é›†è¿›è¡Œäº†åˆ’åˆ†ï¼Œåˆ›å»ºäº†ä¸€ä¸ªç”¨äºå¸ƒå±€åˆ°å›¾åƒç”Ÿæˆçš„æ–°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å…·æœ‰ä¸åŒçº§åˆ«çš„é‡å ã€‚é€šè¿‡åœ¨æ­¤æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å¤„ç†é«˜é‡å å¸ƒå±€æ—¶ï¼ŒToLoæ˜¾è‘—æé«˜äº†ç°æœ‰æ–¹æ³•çš„æ•ˆæœã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/misaka12435/ToLo%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/misaka12435/ToLoè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ— è®­ç»ƒå¸ƒå±€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå…·æœ‰å¯æ§å¸ƒå±€çš„é«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ï¼Œé€šè¿‡å®šä¹‰åŸºäºæ³¨æ„åŠ›å›¾çš„æŸå¤±æ¥å¼•å¯¼æ¨¡å‹å…³æ³¨æ¯ä¸ªæ¦‚å¿µå¯¹åº”çš„åŒºåŸŸã€‚ç„¶è€Œï¼Œå¯¹äºé«˜é‡å å¸ƒå±€ï¼Œè¿™äº›æ¨¡å‹ä»é¢ä¸´å‡†ç¡®è·Ÿéšå¸ƒå±€çš„æŒ‘æˆ˜ï¼Œä¼šå¯¼è‡´å±æ€§æ³„æ¼å’Œå®ä½“ç¼ºå¤±ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºToLoï¼Œä¸€ç§ä¸¤é˜¶æ®µã€æ— éœ€è®­ç»ƒçš„é«˜é‡å å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚é€šè¿‡åŸºäºæ³¨æ„åŠ›å›¾çš„æŸå¤±å‡½æ•°ï¼Œåˆ†ä¸ºèšåˆé˜¶æ®µå’Œåˆ†ç¦»é˜¶æ®µã€‚ä¸ºæä¾›æ›´æœ‰æ•ˆçš„è¯„ä¼°ï¼Œæˆ‘ä»¬æ ¹æ®è¾“å…¥å¸ƒå±€çš„IoUå¯¹HRSæ•°æ®é›†è¿›è¡Œåˆ†åŒºï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ç”¨äºå¸ƒå±€åˆ°å›¾åƒç”Ÿæˆçš„ä¸åŒé‡å ç¨‹åº¦çš„æ•°æ®é›†ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼ŒToLoåœ¨å¤„ç†é«˜é‡å å¸ƒå±€æ—¶æ˜¾è‘—æé«˜äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ— è®­ç»ƒçŠ¶æ€ä¸‹å®ç°äº†ä»å¸ƒå±€åˆ°é«˜è´¨é‡å›¾åƒçš„ç”Ÿæˆï¼Œå¹¶å¯é€šè¿‡æ§åˆ¶æ³¨æ„åŠ›å›¾æ¥å®ç°å¸ƒå±€çš„å¯æ§æ€§ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨å¤„ç†é«˜é‡å å¸ƒå±€æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å±æ€§æ³„æ¼å’Œå®ä½“ç¼ºå¤±ã€‚</li>
<li>æå‡ºäº†ToLoæ¡†æ¶ï¼ŒåŒ…å«èšåˆå’Œåˆ†ç¦»ä¸¤ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰åŸºäºæ³¨æ„åŠ›å›¾çš„æŸå¤±å‡½æ•°ã€‚</li>
<li>ä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæ ¹æ®è¾“å…¥å¸ƒå±€çš„IoUå¯¹HRSæ•°æ®é›†è¿›è¡Œäº†åˆ†åŒºï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†ã€‚</li>
<li>ToLoæ¡†æ¶æ˜¾è‘—æé«˜äº†å¤„ç†é«˜é‡å å¸ƒå±€æ—¶çš„æ€§èƒ½ã€‚</li>
<li>ToLoä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å¯ä¾›ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9238e8386c55d3e174856414ba7968a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fa09c965fbaef018bffc881e9ce47ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d6f04dbf5331d1e2b34d7dede6941b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6557ee6ffec8fb6e9845fa752b6b88e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d5c4efb8cae9563682c56eab219c36c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c55352f17e395d9af679ecac95be7ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1475169930711cc89d15dc07b7e1080.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Kiss3DGen-Repurposing-Image-Diffusion-Models-for-3D-Asset-Generation"><a href="#Kiss3DGen-Repurposing-Image-Diffusion-Models-for-3D-Asset-Generation" class="headerlink" title="Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation"></a>Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation</h2><p><strong>Authors:Jiantao Lin, Xin Yang, Meixi Chen, Yingjie Xu, Dongyu Yan, Leyi Wu, Xinli Xu, Lie XU, Shunsi Zhang, Ying-Cong Chen</strong></p>
<p>Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate â€˜â€™3D Bundle Imageâ€™â€™, a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”ŸæˆäºŒç»´å›¾åƒæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œåœ¨ä¸‰ç»´å†…å®¹ç”Ÿæˆæ–¹é¢ï¼Œå…¶è´¨é‡å’Œé€šç”¨æ€§ä»ç„¶æœ‰é™ã€‚æœ€å…ˆè¿›çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§è§„æ¨¡çš„ä¸‰ç»´èµ„äº§è¿›è¡Œè®­ç»ƒï¼Œè€Œè¿™äº›èµ„äº§çš„æ”¶é›†å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Kiss3DGenï¼ˆåœ¨ä¸‰ç»´ç”Ÿæˆä¸­ä¿æŒç®€å•ç›´æ¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¡†æ¶ï¼Œé€šè¿‡é‡æ–°åˆ©ç”¨è®­ç»ƒè‰¯å¥½çš„äºŒç»´å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œä¸‰ç»´ç”Ÿæˆï¼Œä»¥ç”Ÿæˆã€ç¼–è¾‘å’Œå¢å¼ºä¸‰ç»´ç‰©ä½“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆâ€œä¸‰ç»´æ†ç»‘å›¾åƒâ€ï¼Œè¿™æ˜¯ä¸€ç§ç”±å¤šè§†å›¾å›¾åƒåŠå…¶ç›¸åº”çš„æ³•çº¿å›¾ç»„æˆçš„å¹³é“ºè¡¨ç¤ºã€‚ç„¶åï¼Œä½¿ç”¨æ³•çº¿å›¾é‡å»ºä¸‰ç»´ç½‘æ ¼ï¼Œå¤šè§†å›¾å›¾åƒæä¾›çº¹ç†æ˜ å°„ï¼Œä»è€Œç”Ÿæˆå®Œæ•´çš„ä¸‰ç»´æ¨¡å‹ã€‚è¿™ç§ç®€å•çš„æ–¹æ³•æœ‰æ•ˆåœ°å°†ä¸‰ç»´ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºäºŒç»´å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œæœ€å¤§é™åº¦åœ°åˆ©ç”¨äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜Kiss3DGenæ¨¡å‹ä¸å„ç§æ‰©æ•£æ¨¡å‹æŠ€æœ¯å…¼å®¹ï¼Œèƒ½å¤Ÿå®ç°å¦‚ä¸‰ç»´ç¼–è¾‘ã€ç½‘æ ¼å’Œçº¹ç†å¢å¼ºç­‰é«˜çº§åŠŸèƒ½ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ä¸‰ç»´æ¨¡å‹çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01370v1">PDF</a> The first three authors contributed equally to this work</p>
<p><strong>Summary</strong><br>     Kiss3DGenåˆ©ç”¨äºŒç»´å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸‰ç»´ç‰©ä½“ï¼Œé€šè¿‡å¾®è°ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆç”±å¤šè§†è§’å›¾åƒå’Œå¯¹åº”çš„æ³•çº¿å›¾ç»„æˆçš„â€œä¸‰ç»´æ†ç»‘å›¾åƒâ€ï¼Œå†ç”±æ­¤é‡å»ºä¸‰ç»´ç½‘æ ¼å¹¶æä¾›çº¹ç†æ˜ å°„ï¼Œå°†ä¸‰ç»´ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºäºŒç»´å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ–¹æ³•é«˜æ•ˆã€å…¼å®¹å¤šç§æ‰©æ•£æ¨¡å‹æŠ€æœ¯ï¼Œå¯å®ç°ä¸‰ç»´ç¼–è¾‘ã€ç½‘æ ¼å’Œçº¹ç†å¢å¼ºç­‰åŠŸèƒ½ï¼Œèƒ½æœ‰æ•ˆç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Kiss3DGenæˆåŠŸå°†äºŒç»´å›¾åƒæ‰©æ•£æ¨¡å‹åº”ç”¨äºä¸‰ç»´ç‰©ä½“çš„ç”Ÿæˆã€ç¼–è¾‘å’Œå¢å¼ºã€‚</li>
<li>é€šè¿‡å¾®è°ƒæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆâ€œä¸‰ç»´æ†ç»‘å›¾åƒâ€ï¼ŒåŒ…å«å¤šè§†è§’å›¾åƒå’Œå¯¹åº”çš„æ³•çº¿å›¾ã€‚</li>
<li>åˆ©ç”¨æ³•çº¿å›¾é‡å»ºä¸‰ç»´ç½‘æ ¼ï¼Œå¤šè§†è§’å›¾åƒæä¾›çº¹ç†æ˜ å°„ï¼Œç”Ÿæˆå®Œæ•´ä¸‰ç»´æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•æœ€å¤§åŒ–åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„çŸ¥è¯†ï¼Œå°†ä¸‰ç»´ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºäºŒç»´å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>Kiss3DGenå…¼å®¹å„ç§æ‰©æ•£æ¨¡å‹æŠ€æœ¯ï¼Œå¯å®ç°é«˜çº§åŠŸèƒ½ï¼Œå¦‚ä¸‰ç»´ç¼–è¾‘ã€ç½‘æ ¼å’Œçº¹ç†å¢å¼ºç­‰ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒï¼Œè¯æ˜è¯¥æ–¹æ³•èƒ½é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9897af5d015909f493af98925ea7b8aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a27c4c7f7d89c8007d569a400123977a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-866d073b919f42289292d496f0fa392d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e821e7714437d05bad5f9375bd7b219.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-Virtual-Staining-from-Polarimetric-Mueller-Matrix-Imaging"><a href="#Diffusion-based-Virtual-Staining-from-Polarimetric-Mueller-Matrix-Imaging" class="headerlink" title="Diffusion-based Virtual Staining from Polarimetric Mueller Matrix   Imaging"></a>Diffusion-based Virtual Staining from Polarimetric Mueller Matrix   Imaging</h2><p><strong>Authors:Xiaoyu Zheng, Jing Wen, Jiaxin Zhuang, Yao Du, Jing Cong, Limei Guo, Chao He, Lin Luo, Hao Chen</strong></p>
<p>Polarization, as a new optical imaging tool, has been explored to assist in the diagnosis of pathology. Moreover, converting the polarimetric Mueller Matrix (MM) to standardized stained images becomes a promising approach to help pathologists interpret the results. However, existing methods for polarization-based virtual staining are still in the early stage, and the diffusion-based model, which has shown great potential in enhancing the fidelity of the generated images, has not been studied yet. In this paper, a Regulated Bridge Diffusion Model (RBDM) for polarization-based virtual staining is proposed. RBDM utilizes the bidirectional bridge diffusion process to learn the mapping from polarization images to other modalities such as H&amp;E and fluorescence. And to demonstrate the effectiveness of our model, we conduct the experiment on our manually collected dataset, which consists of 18,000 paired polarization, fluorescence and H&amp;E images, due to the unavailability of the public dataset. The experiment results show that our model greatly outperforms other benchmark methods. Our dataset and code will be released upon acceptance. </p>
<blockquote>
<p>åæŒ¯ä½œä¸ºä¸€ç§æ–°çš„å…‰å­¦æˆåƒå·¥å…·ï¼Œå·²è¢«æ¢ç´¢ç”¨äºç—…ç†è¯Šæ–­çš„è¾…åŠ©ã€‚æ­¤å¤–ï¼Œå°†åæŒ¯ç©†å‹’çŸ©é˜µï¼ˆMMï¼‰è½¬æ¢ä¸ºæ ‡å‡†åŒ–æŸ“è‰²å›¾åƒçš„æ–¹æ³•æˆä¸ºå¸®åŠ©ç—…ç†å­¦å®¶è§£é‡Šç»“æœçš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºåæŒ¯çš„è™šæ‹ŸæŸ“è‰²æ–¹æ³•ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œå…·æœ‰æé«˜ç”Ÿæˆå›¾åƒä¿çœŸåº¦æ½œåŠ›çš„åŸºäºæ‰©æ•£çš„æ¨¡å‹å°šæœªè¿›è¡Œç ”ç©¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåæŒ¯è™šæ‹ŸæŸ“è‰²çš„è°ƒæ§æ¡¥æ¢æ‰©æ•£æ¨¡å‹ï¼ˆRBDMï¼‰ã€‚RBDMåˆ©ç”¨åŒå‘æ¡¥æ¢æ‰©æ•£è¿‡ç¨‹ï¼Œå­¦ä¹ ä»åæŒ¯å›¾åƒåˆ°å…¶ä»–æ¨¡æ€ï¼ˆå¦‚Hï¼†Eå’Œè§å…‰ï¼‰çš„æ˜ å°„ã€‚ä¸ºäº†è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨è‡ªå·±æ‰‹åŠ¨æ”¶é›†çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯¥æ•°æ®é›†åŒ…å«18000å¯¹åæŒ¯ã€è§å…‰å’ŒHï¼†Eå›¾åƒï¼Œç”±äºæ— æ³•è·å¾—å…¬å…±æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¤§å¤§è¶…è¿‡äº†å…¶ä»–åŸºå‡†æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01352v1">PDF</a> </p>
<p><strong>Summary</strong><br>æåŒ–ä½œä¸ºä¸€ç§æ–°çš„å…‰å­¦æˆåƒå·¥å…·åœ¨ç—…ç†å­¦è¯Šæ–­ä¸­å¾—åˆ°æ¢ç´¢ã€‚å°†æåŒ–MuellerçŸ©é˜µè½¬æ¢ä¸ºæ ‡å‡†åŒ–æŸ“è‰²å›¾åƒçš„æ–¹æ³•ä¸ºç—…ç†å­¦å®¶è§£è¯»ç»“æœæä¾›äº†å¸®åŠ©ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè°ƒæ§æ¡¥æ‰©æ•£æ¨¡å‹çš„æåŒ–è™šæ‹ŸæŸ“è‰²æ–¹æ³•ï¼ˆRBDMï¼‰ï¼Œåˆ©ç”¨åŒå‘æ¡¥æ‰©æ•£è¿‡ç¨‹å­¦ä¹ ä»æåŒ–å›¾åƒåˆ°å…¶ä»–æ¨¡æ€ï¼ˆå¦‚H&amp;Eå’Œè§å…‰ï¼‰çš„æ˜ å°„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ‰‹åŠ¨æ”¶é›†çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æåŒ–ä½œä¸ºä¸€ç§æ–°çš„å…‰å­¦æˆåƒå·¥å…·åœ¨ç—…ç†å­¦è¯Šæ–­ä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>å°†æåŒ–MuellerçŸ©é˜µè½¬æ¢ä¸ºæ ‡å‡†åŒ–æŸ“è‰²å›¾åƒæ˜¯å¸®åŠ©ç—…ç†å­¦å®¶è§£è¯»ç»“æœçš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>ç›®å‰åŸºäºæåŒ–çš„è™šæ‹ŸæŸ“è‰²æ–¹æ³•ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œæ‰©æ•£æ¨¡å‹åœ¨å¢å¼ºå›¾åƒä¿çœŸåº¦æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å°šæœªè¿›è¡Œç ”ç©¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè°ƒæ§æ¡¥æ‰©æ•£æ¨¡å‹çš„æåŒ–è™šæ‹ŸæŸ“è‰²æ–¹æ³•ï¼ˆRBDMï¼‰ã€‚</li>
<li>RBDMåˆ©ç”¨åŒå‘æ¡¥æ‰©æ•£è¿‡ç¨‹å­¦ä¹ ä»æåŒ–å›¾åƒåˆ°H&amp;Eå’Œè§å…‰ç­‰å…¶ä»–æ¨¡æ€çš„æ˜ å°„ã€‚</li>
<li>å®éªŒæ˜¯åœ¨æ‰‹åŠ¨æ”¶é›†çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„ï¼ŒåŒ…å«18ï¼Œ000å¼ æåŒ–ã€è§å…‰å’ŒH&amp;Eå›¾åƒé…å¯¹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRBDMåœ¨æ€§èƒ½ä¸Šå¤§å¤§ä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-973c2cbb9db8efe0e990e8a79db38e4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0532fc462f3d45ed134027c93f019c86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e69af83b1197a2abebb92e39a6a2557.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CacheQuant-Comprehensively-Accelerated-Diffusion-Models"><a href="#CacheQuant-Comprehensively-Accelerated-Diffusion-Models" class="headerlink" title="CacheQuant: Comprehensively Accelerated Diffusion Models"></a>CacheQuant: Comprehensively Accelerated Diffusion Models</h2><p><strong>Authors:Xuewen Liu, Zhikai Li, Qingyi Gu</strong></p>
<p>Diffusion models have gradually gained prominence in the field of image synthesis, showcasing remarkable generative capabilities. Nevertheless, the slow inference and complex networks, resulting from redundancy at both temporal and structural levels, hinder their low-latency applications in real-world scenarios. Current acceleration methods for diffusion models focus separately on temporal and structural levels. However, independent optimization at each level to further push the acceleration limits results in significant performance degradation. On the other hand, integrating optimizations at both levels can compound the acceleration effects. Unfortunately, we find that the optimizations at these two levels are not entirely orthogonal. Performing separate optimizations and then simply integrating them results in unsatisfactory performance. To tackle this issue, we propose CacheQuant, a novel training-free paradigm that comprehensively accelerates diffusion models by jointly optimizing model caching and quantization techniques. Specifically, we employ a dynamic programming approach to determine the optimal cache schedule, in which the properties of caching and quantization are carefully considered to minimize errors. Additionally, we propose decoupled error correction to further mitigate the coupled and accumulated errors step by step. Experimental results show that CacheQuant achieves a 5.18 speedup and 4 compression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP score. Our code are open-sourced: <a target="_blank" rel="noopener" href="https://github.com/BienLuky/CacheQuant">https://github.com/BienLuky/CacheQuant</a> . </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆé¢†åŸŸé€æ¸å´­éœ²å¤´è§’ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ—¶é—´å’Œç»“æ„ä¸¤ä¸ªå±‚é¢çš„å†—ä½™å¯¼è‡´çš„æ…¢é€Ÿæ¨ç†å’Œå¤æ‚ç½‘ç»œï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„ä½å»¶è¿Ÿåº”ç”¨ã€‚å½“å‰é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ—¶é—´å’Œç»“æ„ä¸¤ä¸ªå±‚é¢ã€‚ç„¶è€Œï¼Œåœ¨æ¯ä¸ªå±‚é¢è¿›è¡Œç‹¬ç«‹ä¼˜åŒ–ä»¥è¿›ä¸€æ­¥æ¨åŠ¨åŠ é€Ÿæé™ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚å¦ä¸€æ–¹é¢ï¼Œæ•´åˆè¿™ä¸¤ä¸ªå±‚é¢çš„ä¼˜åŒ–å¯ä»¥å åŠ åŠ é€Ÿæ•ˆæœã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸¤ä¸ªå±‚é¢çš„ä¼˜åŒ–å¹¶ä¸æ˜¯å®Œå…¨æ­£äº¤çš„ã€‚è¿›è¡Œå•ç‹¬çš„ä¼˜åŒ–ç„¶åç®€å•åœ°é›†æˆå®ƒä»¬ä¼šå¯¼è‡´æ€§èƒ½ä¸å°½å¦‚äººæ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CacheQuantï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„æ— éœ€è®­ç»ƒçš„æ¨¡å¼ï¼Œé€šè¿‡ç»¼åˆä¼˜åŒ–æ¨¡å‹ç¼“å­˜å’Œé‡åŒ–æŠ€æœ¯ï¼Œå…¨é¢åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨åŠ¨æ€è§„åˆ’çš„æ–¹æ³•æ¥ç¡®å®šæœ€ä½³ç¼“å­˜è°ƒåº¦ï¼Œå……åˆ†è€ƒè™‘ç¼“å­˜å’Œé‡åŒ–çš„å±æ€§ä»¥æœ€å°åŒ–è¯¯å·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†ç¦»è¯¯å·®æ ¡æ­£æ¥è¿›ä¸€æ­¥é€æ­¥ç¼“è§£è€¦åˆå’Œç´¯ç§¯è¯¯å·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCacheQuantåœ¨MS-COCOä¸Šçš„Stable Diffusionå®ç°äº†5.18å€çš„åŠ é€Ÿå’Œ4å€çš„å‹ç¼©ï¼ŒCLIPåˆ†æ•°ä»…ä¸‹é™0.02ã€‚æˆ‘ä»¬çš„ä»£ç å·²å¼€æºï¼š<a target="_blank" rel="noopener" href="https://github.com/BienLuky/CacheQuant%E3%80%82">https://github.com/BienLuky/CacheQuantã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01323v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆé¢†åŸŸé€æ¸å—åˆ°é‡è§†ï¼Œå±•ç°å‡ºå“è¶Šçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å…¶ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦å’Œå¤æ‚çš„ç½‘ç»œç»“æ„é˜»ç¢äº†å…¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„ä½å»¶è¿Ÿåº”ç”¨ã€‚ç°æœ‰åŠ é€Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ‰©æ•£æ¨¡å‹çš„æ—¶åºå’Œç»“æ„å±‚é¢è¿›è¡Œä¼˜åŒ–ï¼Œä½†ç‹¬ç«‹ä¼˜åŒ–å¾€å¾€å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºCacheQuantï¼Œä¸€ç§å…¨æ–°çš„æ— éœ€è®­ç»ƒçš„è®­ç»ƒååŠ é€ŸèŒƒå¼ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹ç¼“å­˜å’Œé‡åŒ–æŠ€æœ¯æ¥å…¨é¢åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡åŠ¨æ€è§„åˆ’ç¡®å®šæœ€ä½³ç¼“å­˜è°ƒåº¦ï¼Œè¿›ä¸€æ­¥é€šè¿‡è§£è€¦è¯¯å·®æ ¡æ­£æ¥é€æ­¥å‡è½»è€¦åˆå’Œç´¯ç§¯è¯¯å·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCacheQuantåœ¨MS-COCOä¸Šçš„Stable Diffusionå®ç°äº†5.18å€çš„åŠ é€Ÿå’Œ4å€çš„å‹ç¼©ï¼ŒåŒæ—¶CLIPåˆ†æ•°ä»…ä¸‹é™0.02ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆé¢†åŸŸå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†æ¨ç†é€Ÿåº¦æ…¢å’Œå¤æ‚ç½‘ç»œç»“æ„é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>å½“å‰åŠ é€Ÿæ–¹æ³•ä¸»è¦å…³æ³¨æ—¶åºå’Œç»“æ„å±‚é¢çš„ä¼˜åŒ–ï¼Œä½†ç‹¬ç«‹ä¼˜åŒ–å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>CacheQuantæ˜¯ä¸€ç§å…¨æ–°çš„è®­ç»ƒååŠ é€ŸèŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹ç¼“å­˜å’Œé‡åŒ–æŠ€æœ¯å…¨é¢åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>CacheQuantåˆ©ç”¨åŠ¨æ€è§„åˆ’ç¡®å®šæœ€ä½³ç¼“å­˜è°ƒåº¦ï¼Œä»¥æœ€å°åŒ–è¯¯å·®ã€‚</li>
<li>è§£è€¦è¯¯å·®æ ¡æ­£æ–¹æ³•è¢«ç”¨æ¥é€æ­¥å‡è½»ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è€¦åˆå’Œç´¯ç§¯è¯¯å·®ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCacheQuantåœ¨MS-COCOæ•°æ®é›†ä¸Šçš„Stable Diffusionæ¨¡å‹å®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿå’Œå‹ç¼©æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a943fdfd18fda6ac30484aea940083a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e67e3c37a1217b0ab64625849e0610f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8478bbd216b9e26fe95a0cea003c940.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-957c51ffad5d5bdffd4d707d3ed3d6dc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Controllable-Apparel-Showcase-Image-Generation-via-Garment-Centric-Outpainting"><a href="#Fine-Grained-Controllable-Apparel-Showcase-Image-Generation-via-Garment-Centric-Outpainting" class="headerlink" title="Fine-Grained Controllable Apparel Showcase Image Generation via   Garment-Centric Outpainting"></a>Fine-Grained Controllable Apparel Showcase Image Generation via   Garment-Centric Outpainting</h2><p><strong>Authors:Rong Zhang, Jingnan Wang, Zhiwen Zuo, Jianfeng Dong, Wei Li, Chi Wang, Weiwei Xu, Xun Wang</strong></p>
<p>In this paper, we propose a novel garment-centric outpainting (GCO) framework based on the latent diffusion model (LDM) for fine-grained controllable apparel showcase image generation. The proposed framework aims at customizing a fashion model wearing a given garment via text prompts and facial images. Different from existing methods, our framework takes a garment image segmented from a dressed mannequin or a person as the input, eliminating the need for learning cloth deformation and ensuring faithful preservation of garment details. The proposed framework consists of two stages. In the first stage, we introduce a garment-adaptive pose prediction model that generates diverse poses given the garment. Then, in the next stage, we generate apparel showcase images, conditioned on the garment and the predicted poses, along with specified text prompts and facial images. Notably, a multi-scale appearance customization module (MS-ACM) is designed to allow both overall and fine-grained text-based control over the generated modelâ€™s appearance. Moreover, we leverage a lightweight feature fusion operation without introducing any extra encoders or modules to integrate multiple conditions, which is more efficient. Extensive experiments validate the superior performance of our framework compared to state-of-the-art methods. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æ–°å‹æœè£…ä¸­å¿ƒå¤–ç”»ï¼ˆGCOï¼‰æ¡†æ¶ï¼Œç”¨äºç»†ç²’åº¦å¯æ§æœè£…å±•ç¤ºå›¾åƒç”Ÿæˆã€‚æ‰€æå‡ºçš„æ¡†æ¶æ—¨åœ¨é€šè¿‡æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒå®šåˆ¶ç©¿ç€ç»™å®šæœè£…çš„æ—¶å°šæ¨¡å‹ã€‚ä¸åŒäºç°æœ‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä»¥ä»ç€è£…çš„äººä½“æ¨¡ç‰¹æˆ–ä¸ªäººèº«ä¸Šåˆ†å‰²å‡ºæ¥çš„æœè£…å›¾åƒä½œä¸ºè¾“å…¥ï¼Œæ— éœ€å­¦ä¹ å¸ƒæ–™å˜å½¢ï¼ŒåŒæ—¶ç¡®ä¿æœè£…ç»†èŠ‚çš„å¿ å®ä¿ç•™ã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœè£…é€‚åº”æ€§å§¿æ€é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®æœè£…ç”Ÿæˆå„ç§å§¿æ€ã€‚ç„¶åï¼Œåœ¨ä¸‹ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä»¥æœè£…ã€é¢„æµ‹å§¿æ€ã€æŒ‡å®šçš„æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒä¸ºæ¡ä»¶ï¼Œç”Ÿæˆæœè£…å±•ç¤ºå›¾åƒã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦å¤–è§‚å®šåˆ¶æ¨¡å—ï¼ˆMS-ACMï¼‰ï¼Œå…è®¸å¯¹ç”Ÿæˆçš„æ¨¡å‹çš„å¤–è§‚è¿›è¡Œæ•´ä½“å’Œç»†ç²’åº¦çš„æ–‡æœ¬æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨è½»é‡çº§ç‰¹å¾èåˆæ“ä½œï¼Œæ— éœ€å¼•å…¥ä»»ä½•é¢å¤–çš„ç¼–ç å™¨æˆ–æ¨¡å—æ¥æ•´åˆå¤šç§æ¡ä»¶ï¼Œè¿™æ›´åŠ é«˜æ•ˆã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01294v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æ–°å‹æœè£…ä¸­å¿ƒå¤–ç”»ï¼ˆGCOï¼‰æ¡†æ¶ï¼Œç”¨äºç²¾ç»†å¯æ§çš„æ—¶è£…å±•ç¤ºå›¾åƒç”Ÿæˆã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒå®šåˆ¶ç©¿ç€ç»™å®šæœè£…çš„æ—¶å°šæ¨¡å‹ã€‚ä¸åŒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥æ¡†æ¶ä»¥ä»ç€è£…æ¨¡ç‰¹æˆ–äººç‰©åˆ†å‰²å‡ºçš„æœè£…å›¾åƒä¸ºè¾“å…¥ï¼Œæ— éœ€å­¦ä¹ å¸ƒæ–™å˜å½¢ï¼Œç¡®ä¿æœè£…ç»†èŠ‚çš„çœŸå®ä¿ç•™ã€‚æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯æœè£…é€‚åº”æ€§å§¿åŠ¿é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ ¹æ®æœè£…ç”Ÿæˆå¤šæ ·åŒ–çš„å§¿åŠ¿ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯æ ¹æ®æœè£…ã€é¢„æµ‹å§¿åŠ¿ã€æŒ‡å®šçš„æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒç”Ÿæˆæ—¶è£…å±•ç¤ºå›¾åƒã€‚è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦å¤–è§‚å®šåˆ¶æ¨¡å—ï¼ˆMS-ACMï¼‰ï¼Œå®ç°å¯¹ç”Ÿæˆæ¨¡å‹å¤–è§‚çš„æ•´ä½“å’Œç²¾ç»†çº¹ç†çš„æ–‡æœ¬æ§åˆ¶ã€‚åˆ©ç”¨è½»é‡çº§ç‰¹å¾èåˆæ“ä½œï¼Œæ— éœ€å¼•å…¥ä»»ä½•é¢å¤–çš„ç¼–ç å™¨æˆ–æ¨¡å—ï¼Œå³å¯æ•´åˆå¤šç§æ¡ä»¶ï¼Œæé«˜æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æœè£…ä¸­å¿ƒå¤–ç”»ï¼ˆGCOï¼‰æ¡†æ¶ï¼Œç”¨äºæ—¶è£…å±•ç¤ºå›¾åƒç”Ÿæˆã€‚</li>
<li>æ¡†æ¶èƒ½å®šåˆ¶ç©¿ç€ç»™å®šæœè£…çš„æ—¶å°šæ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒå®ç°ç²¾ç»†åŒ–æ§åˆ¶ã€‚</li>
<li>è¾“å…¥ä¸ºä»æ¨¡ç‰¹æˆ–äººç‰©åˆ†å‰²å‡ºçš„æœè£…å›¾åƒï¼Œæ— éœ€å­¦ä¹ å¸ƒæ–™å˜å½¢ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šæœè£…é€‚åº”æ€§å§¿åŠ¿é¢„æµ‹å’Œæ—¶è£…å±•ç¤ºå›¾åƒç”Ÿæˆã€‚</li>
<li>è®¾è®¡äº†å¤šå°ºåº¦å¤–è§‚å®šåˆ¶æ¨¡å—ï¼ˆMS-ACMï¼‰ï¼Œå®ç°æ•´ä½“å’Œç²¾ç»†çº¹ç†çš„æ–‡æœ¬æ§åˆ¶ã€‚</li>
<li>åˆ©ç”¨è½»é‡çº§ç‰¹å¾èåˆæ“ä½œæ•´åˆå¤šç§ç”Ÿæˆæ¡ä»¶ï¼Œæé«˜æ•ˆç‡ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16103e04c2c8f1f3b163e74b49982f1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a9741b51277859872df220c9940717c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87d5b7fc3173fa633df4e6449faa6ce7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdeaaffe92f1fa7b643258960f597976.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8faa81ea3bab3a075102f19d11a4692.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Reconciling-Stochastic-and-Deterministic-Strategies-for-Zero-shot-Image-Restoration-using-Diffusion-Model-in-Dual"><a href="#Reconciling-Stochastic-and-Deterministic-Strategies-for-Zero-shot-Image-Restoration-using-Diffusion-Model-in-Dual" class="headerlink" title="Reconciling Stochastic and Deterministic Strategies for Zero-shot Image   Restoration using Diffusion Model in Dual"></a>Reconciling Stochastic and Deterministic Strategies for Zero-shot Image   Restoration using Diffusion Model in Dual</h2><p><strong>Authors:Chong Wang, Lanqing Guo, Zixuan Fu, Siyuan Yang, Hao Cheng, Alex C. Kot, Bihan Wen</strong></p>
<p>Plug-and-play (PnP) methods offer an iterative strategy for solving image restoration (IR) problems in a zero-shot manner, using a learned \textit{discriminative denoiser} as the implicit prior. More recently, a sampling-based variant of this approach, which utilizes a pre-trained \textit{generative diffusion model}, has gained great popularity for solving IR problems through stochastic sampling. The IR results using PnP with a pre-trained diffusion model demonstrate distinct advantages compared to those using discriminative denoisers, \ie improved perceptual quality while sacrificing the data fidelity. The unsatisfactory results are due to the lack of integration of these strategies in the IR tasks. In this work, we propose a novel zero-shot IR scheme, dubbed Reconciling Diffusion Model in Dual (RDMD), which leverages only a \textbf{single} pre-trained diffusion model to construct \textbf{two} complementary regularizers. Specifically, the diffusion model in RDMD will iteratively perform deterministic denoising and stochastic sampling, aiming to achieve high-fidelity image restoration with appealing perceptual quality. RDMD also allows users to customize the distortion-perception tradeoff with a single hyperparameter, enhancing the adaptability of the restoration process in different practical scenarios. Extensive experiments on several IR tasks demonstrate that our proposed method could achieve superior results compared to existing approaches on both the FFHQ and ImageNet datasets. </p>
<blockquote>
<p>Plug-and-Playï¼ˆPnPï¼‰æ–¹æ³•ä¸ºé›¶æ ·æœ¬æ–¹å¼è§£å†³å›¾åƒæ¢å¤ï¼ˆIRï¼‰é—®é¢˜æä¾›äº†è¿­ä»£ç­–ç•¥ï¼Œå®ƒä½¿ç”¨å­¦ä¹ åˆ°çš„åˆ¤åˆ«å»å™ªå™¨ä½œä¸ºéšå¼å…ˆéªŒã€‚æœ€è¿‘ï¼Œè¯¥æ–¹æ³•çš„åŸºäºé‡‡æ ·çš„å˜ä½“å—åˆ°æ¬¢è¿ï¼Œå®ƒé€šè¿‡é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¥è§£å†³IRé—®é¢˜ï¼Œé€šè¿‡éšæœºé‡‡æ ·å®ç°ã€‚ä¸ä½¿ç”¨åˆ¤åˆ«å»å™ªå™¨çš„IRç»“æœç›¸æ¯”ï¼Œä½¿ç”¨å¸¦æœ‰é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„PnPæ–¹æ³•æ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œå³æé«˜äº†æ„ŸçŸ¥è´¨é‡è€Œç‰ºç‰²äº†æ•°æ®ä¿çœŸåº¦ã€‚è¿™äº›ä¸å°½å¦‚äººæ„çš„ç»“æœæ˜¯ç”±äºç¼ºä¹å°†è¿™äº›ç­–ç•¥æ•´åˆåˆ°IRä»»åŠ¡ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹é›¶æ ·æœ¬IRæ–¹æ¡ˆï¼Œç§°ä¸ºâ€œåŒé‡è°ƒå’Œæ‰©æ•£æ¨¡å‹â€ï¼ˆRDMDï¼‰ï¼Œå®ƒä»…ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥æ„å»ºä¸¤ä¸ªäº’è¡¥çš„æ­£åˆ™åŒ–å™¨ã€‚å…·ä½“æ¥è¯´ï¼ŒRDMDä¸­çš„æ‰©æ•£æ¨¡å‹å°†è¿­ä»£æ‰§è¡Œç¡®å®šæ€§å»å™ªå’Œéšæœºé‡‡æ ·ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸå›¾åƒæ¢å¤ï¼ŒåŒæ—¶å…·æœ‰è‰¯å¥½çš„æ„ŸçŸ¥è´¨é‡ã€‚RDMDè¿˜å…è®¸ç”¨æˆ·ç”¨ä¸€ä¸ªè¶…å‚æ•°æ¥å®šåˆ¶å¤±çœŸ-æ„ŸçŸ¥æŠ˜è¡·ï¼Œä»¥å¢å¼ºæ¢å¤è¿‡ç¨‹åœ¨ä¸åŒå®é™…åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚åœ¨å‡ ä¸ªIRä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨FFHQå’ŒImageNetæ•°æ®é›†ä¸Šå®ç°ä¼˜äºç°æœ‰æ–¹æ³•çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01288v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRDMDçš„é›¶æ ·æœ¬å›¾åƒæ¢å¤æ–¹æ¡ˆï¼Œåˆ©ç”¨å•ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ„å»ºä¸¤ä¸ªäº’è¡¥æ­£åˆ™åŒ–å™¨ï¼Œé€šè¿‡è¿­ä»£æ‰§è¡Œç¡®å®šæ€§å»å™ªå’Œéšæœºé‡‡æ ·ï¼Œå®ç°é«˜ä¿çœŸå›¾åƒæ¢å¤å’Œå¸å¼•äººçš„æ„ŸçŸ¥è´¨é‡ã€‚è¯¥æ–¹æ¡ˆå…è®¸ç”¨æˆ·ç”¨ä¸€ä¸ªè¶…å‚æ•°æ¥å®šåˆ¶å¤±çœŸä¸æ„ŸçŸ¥ä¹‹é—´çš„æƒè¡¡ï¼Œæé«˜äº†ä¸åŒå®é™…åœºæ™¯ä¸­æ¢å¤çš„é€‚åº”æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå›¾åƒæ¢å¤ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PnPæ–¹æ³•é‡‡ç”¨è¿­ä»£ç­–ç•¥ï¼Œåˆ©ç”¨å­¦ä¹ åˆ°çš„åˆ¤åˆ«å»å™ªå™¨ä½œä¸ºéšå…ˆéªŒæ¥è§£å†³å›¾åƒæ¢å¤é—®é¢˜ã€‚</li>
<li>æœ€è¿‘çš„é‡‡æ ·å‹å˜ä½“æ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡éšæœºé‡‡æ ·è§£å†³å›¾åƒæ¢å¤é—®é¢˜ã€‚</li>
<li>ä¸ä½¿ç”¨åˆ¤åˆ«å»å™ªå™¨ç›¸æ¯”ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„PnPæ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡ä¸Šæœ‰æ‰€æé«˜ï¼Œä½†åœ¨æ•°æ®ä¿çœŸåº¦ä¸Šæœ‰æ‰€ç‰ºç‰²ã€‚</li>
<li>RDMDæ˜¯ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬å›¾åƒæ¢å¤æ–¹æ¡ˆï¼Œåˆ©ç”¨å•ä¸ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ„å»ºä¸¤ä¸ªäº’è¡¥æ­£åˆ™åŒ–å™¨ã€‚</li>
<li>RDMDé€šè¿‡è¿­ä»£æ‰§è¡Œç¡®å®šæ€§å»å™ªå’Œéšæœºé‡‡æ ·ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸå›¾åƒæ¢å¤å’Œå¸å¼•äººçš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>RDMDå…è®¸ç”¨æˆ·é€šè¿‡ä¸€ä¸ªè¶…å‚æ•°æ¥å®šåˆ¶å¤±çœŸä¸æ„ŸçŸ¥ä¹‹é—´çš„æƒè¡¡ï¼Œå¢å¼ºäº†ä¸åŒåœºæ™¯ä¸‹çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5750aa0090334d66e55e055b0040cc1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-362d3d4445f3d3ecf8e44811f831d401.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0aba975c5a0f3685199114946bddf92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-290c421b01d3208eb039dd4d7647d00b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Finding-Local-Diffusion-Schrodinger-Bridge-using-Kolmogorov-Arnold-Network"><a href="#Finding-Local-Diffusion-Schrodinger-Bridge-using-Kolmogorov-Arnold-Network" class="headerlink" title="Finding Local Diffusion SchrÃ¶dinger Bridge using Kolmogorov-Arnold   Network"></a>Finding Local Diffusion SchrÃ¶dinger Bridge using Kolmogorov-Arnold   Network</h2><p><strong>Authors:Xingyu Qiu, Mengying Yang, Xinghua Ma, Fanding Li, Dong Liang, Gongning Luo, Wei Wang, Kuanquan Wang, Shuo Li</strong></p>
<p>In image generation, Schr&quot;odinger Bridge (SB)-based methods theoretically enhance the efficiency and quality compared to the diffusion models by finding the least costly path between two distributions. However, they are computationally expensive and time-consuming when applied to complex image data. The reason is that they focus on fitting globally optimal paths in high-dimensional spaces, directly generating images as next step on the path using complex networks through self-supervised training, which typically results in a gap with the global optimum. Meanwhile, most diffusion models are in the same path subspace generated by weights $f_A(t)$ and $f_B(t)$, as they follow the paradigm ($x_t &#x3D; f_A(t)x_{Img} + f_B(t)\epsilon$). To address the limitations of SB-based methods, this paper proposes for the first time to find local Diffusion Schr&quot;odinger Bridges (LDSB) in the diffusion path subspace, which strengthens the connection between the SB problem and diffusion models. Specifically, our method optimizes the diffusion paths using Kolmogorov-Arnold Network (KAN), which has the advantage of resistance to forgetting and continuous output. The experiment shows that our LDSB significantly improves the quality and efficiency of image generation using the same pre-trained denoising network and the KAN for optimising is only less than 0.1MB. The FID metric is reduced by more than 15%, especially with a reduction of 48.50% when NFE of DDIM is $5$ for the CelebA dataset. Code is available at <a target="_blank" rel="noopener" href="https://github.com/PerceptionComputingLab/LDSB">https://github.com/PerceptionComputingLab/LDSB</a>. </p>
<blockquote>
<p>åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸï¼ŒåŸºäºSchrÃ¶dinger Bridgeï¼ˆSBï¼‰çš„æ–¹æ³•é€šè¿‡å¯»æ‰¾ä¸¤ç§åˆ†å¸ƒä¹‹é—´æˆæœ¬æœ€ä½çš„é€”å¾„ï¼Œç†è®ºä¸Šæé«˜äº†ä¸æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡å’Œè´¨é‡ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºå¤æ‚çš„å›¾åƒæ•°æ®æ—¶ï¼Œå®ƒä»¬è®¡ç®—é‡å¤§ä¸”è€—æ—¶ã€‚åŸå› æ˜¯å®ƒä»¬ä¸“æ³¨äºåœ¨é«˜ç»´ç©ºé—´ä¸­æ‹Ÿåˆå…¨å±€æœ€ä¼˜è·¯å¾„ï¼Œé€šè¿‡è‡ªç›‘ç£è®­ç»ƒä½¿ç”¨å¤æ‚ç½‘ç»œç›´æ¥ç”Ÿæˆè·¯å¾„ä¸Šçš„ä¸‹ä¸€æ­¥å›¾åƒï¼Œè¿™é€šå¸¸ä¸å…¨å±€æœ€ä¼˜å­˜åœ¨å·®è·ã€‚åŒæ—¶ï¼Œå¤§å¤šæ•°æ‰©æ•£æ¨¡å‹çš„è·¯å¾„éƒ½å—åˆ°æƒé‡fA(t)å’ŒfB(t)ç”Ÿæˆçš„ç›¸åŒè·¯å¾„å­ç©ºé—´çš„å½±å“ï¼Œå› ä¸ºå®ƒä»¬éµå¾ªï¼ˆxt&#x3D;fA(t)xImg+fB(t)Ïµï¼‰è¿™ä¸€èŒƒå¼ã€‚ä¸ºäº†å…‹æœSBæ–¹æ³•çš„å±€é™æ€§ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºäº†åœ¨æ‰©æ•£è·¯å¾„å­ç©ºé—´ä¸­æ‰¾åˆ°å±€éƒ¨æ‰©æ•£SchrÃ¶dinger Bridgesï¼ˆLDSBï¼‰çš„æ–¹æ³•ï¼Œå¢å¼ºäº†SBé—®é¢˜ä¸æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„è”ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ä¼˜åŒ–æ‰©æ•£è·¯å¾„ï¼Œè¯¥ç½‘ç»œå…·æœ‰æŠµæŠ—é—å¿˜å’Œè¿ç»­è¾“å‡ºçš„ä¼˜åŠ¿ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒé™å™ªç½‘ç»œå’Œä¼˜åŒ–çš„KANï¼Œæˆ‘ä»¬çš„LDSBåœ¨å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡æ–¹é¢æœ‰äº†æ˜¾è‘—æé«˜ï¼Œå…¶ä¸­ä¼˜åŒ–ç½‘ç»œçš„å¤§å°ä»…å°äº0.1MBã€‚åœ¨CelebAæ•°æ®é›†ä¸Šï¼Œå½“DDIMçš„NFEä¸º5æ—¶ï¼ŒFIDæŒ‡æ ‡é™ä½äº†è¶…è¿‡15%ï¼Œå°¤å…¶æ˜¯é™ä½äº†48.5%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PerceptionComputingLab/LDSB%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PerceptionComputingLab/LDSBæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19754v2">PDF</a> 16 pages, 10 figures, accepted by CVPR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†åŸºäºå±€éƒ¨æ‰©æ•£SchrÃ¶dingeræ¡¥ï¼ˆLDSBï¼‰çš„æ–¹æ³•ï¼Œä¼˜åŒ–äº†å›¾åƒç”Ÿæˆä¸­çš„æ‰©æ•£è·¯å¾„ã€‚è¯¥æ–¹æ³•ç»“åˆäº†SchrÃ¶dinger Bridgeï¼ˆSBï¼‰ç†è®ºå’Œæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡åœ¨æ‰©æ•£è·¯å¾„å­ç©ºé—´ä¸­å¯»æ‰¾å±€éƒ¨æ‰©æ•£æ¡¥ï¼Œå¼ºåŒ–äº†SBé—®é¢˜ä¸æ‰©æ•£æ¨¡å‹çš„å…³è”ã€‚å®éªŒè¡¨æ˜ï¼ŒLDSBåœ¨åˆ©ç”¨ç›¸åŒçš„é¢„è®­ç»ƒå»å™ªç½‘ç»œå’Œä¼˜åŒ–çš„Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æ—¶ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚å¯¹äºCelebAæ•°æ®é›†ï¼Œå½“DDIMçš„NFEä¸º5æ—¶ï¼ŒFIDæŒ‡æ ‡é™ä½äº†è¶…è¿‡15%ï¼Œå°¤å…¶æ˜¯é™ä½äº†48.5%ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SchrÃ¶dinger Bridgeï¼ˆSBï¼‰ç†è®ºåœ¨å›¾åƒç”Ÿæˆä¸­èƒ½æé«˜æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡å’Œè´¨é‡ï¼Œä½†åº”ç”¨äºå¤æ‚å›¾åƒæ•°æ®æ—¶è®¡ç®—æˆæœ¬é«˜ã€è€—æ—¶é•¿ã€‚</li>
<li>SBæ–¹æ³•ä¾§é‡äºåœ¨é«˜ç»´ç©ºé—´ä¸­å¯»æ‰¾å…¨å±€æœ€ä¼˜è·¯å¾„ï¼Œè€Œæ‰©æ•£æ¨¡å‹é€šå¸¸éµå¾ªç‰¹å®šçš„è·¯å¾„å­ç©ºé—´ã€‚</li>
<li>è®ºæ–‡é¦–æ¬¡æå‡ºäº†å±€éƒ¨æ‰©æ•£SchrÃ¶dingeræ¡¥ï¼ˆLDSBï¼‰æ–¹æ³•ï¼Œå¼ºåŒ–äº†SBé—®é¢˜ä¸æ‰©æ•£æ¨¡å‹çš„å…³è”ï¼Œä¼˜åŒ–äº†æ‰©æ•£è·¯å¾„ã€‚</li>
<li>LDSBæ–¹æ³•ä½¿ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œå…·æœ‰æŠµæŠ—é—å¿˜å’Œè¿ç»­è¾“å‡ºçš„ä¼˜åŠ¿ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLDSBæ–¹æ³•åœ¨åˆ©ç”¨é¢„è®­ç»ƒå»å™ªç½‘ç»œæ—¶ï¼Œèƒ½æ˜¾è‘—æé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>å¯¹äºCelebAæ•°æ®é›†ï¼Œå½“DDIMçš„NFEä¸ºç‰¹å®šå€¼æ—¶ï¼ŒLDSBæ–¹æ³•ä½¿FIDæŒ‡æ ‡é™ä½äº†è¶…è¿‡15%ï¼Œå°¤å…¶æ˜¯é™ä½äº†48.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-476a28763a868ac5da2a9a37cec25270.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be29d00793045dfd7a33836bd0205b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a14106826ad974bbd37b09d287d4b49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d5fac5329a8ab3a6c69697fadb1b50c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5738f829f99e5430b6a8a602598990fa.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Language-Informed-Hyperspectral-Image-Synthesis-for-Imbalanced-Small-Sample-Classification-via-Semi-Supervised-Conditional-Diffusion-Model"><a href="#Language-Informed-Hyperspectral-Image-Synthesis-for-Imbalanced-Small-Sample-Classification-via-Semi-Supervised-Conditional-Diffusion-Model" class="headerlink" title="Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small   Sample Classification via Semi-Supervised Conditional Diffusion Model"></a>Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small   Sample Classification via Semi-Supervised Conditional Diffusion Model</h2><p><strong>Authors:Yimin Zhu, Lincoln Linlin Xu</strong></p>
<p>Data augmentation effectively addresses the imbalanced-small sample data (ISSD) problem in hyperspectral image classification (HSIC). While most methodologies extend features in the latent space, few leverage text-driven generation to create realistic and diverse samples. Recently, text-guided diffusion models have gained significant attention due to their ability to generate highly diverse and high-quality images based on text prompts in natural image synthesis. Motivated by this, this paper proposes Txt2HSI-LDM(VAE), a novel language-informed hyperspectral image synthesis method to address the ISSD in HSIC. The proposed approach uses a denoising diffusion model, which iteratively removes Gaussian noise to generate hyperspectral samples conditioned on textual descriptions. First, to address the high-dimensionality of hyperspectral data, a universal variational autoencoder (VAE) is designed to map the data into a low-dimensional latent space, which provides stable features and reduces the inference complexity of diffusion model. Second, a semi-supervised diffusion model is designed to fully take advantage of unlabeled data. Random polygon spatial clipping (RPSC) and uncertainty estimation of latent feature (LF-UE) are used to simulate the varying degrees of mixing. Third, the VAE decodes HSI from latent space generated by the diffusion model with the language conditions as input. In our experiments, we fully evaluate synthetic samplesâ€™ effectiveness from statistical characteristics and data distribution in 2D-PCA space. Additionally, visual-linguistic cross-attention is visualized on the pixel level to prove that our proposed model can capture the spatial layout and geometry of the generated data. Experiments demonstrate that the performance of the proposed Txt2HSI-LDM(VAE) surpasses the classical backbone models, state-of-the-art CNNs, and semi-supervised methods. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºæœ‰æ•ˆè§£å†³äº†é«˜å…‰è°±å›¾åƒåˆ†ç±»ï¼ˆHSICï¼‰ä¸­çš„ä¸å¹³è¡¡å°æ ·æœ¬æ•°æ®ï¼ˆISSDï¼‰é—®é¢˜ã€‚è™½ç„¶å¤§å¤šæ•°æ–¹æ³•éƒ½åœ¨æ½œåœ¨ç©ºé—´æ‰©å±•ç‰¹å¾ï¼Œä½†å¾ˆå°‘æœ‰æ–¹æ³•åˆ©ç”¨æ–‡æœ¬é©±åŠ¨ç”Ÿæˆæ¥åˆ›å»ºçœŸå®å’Œå¤šæ ·åŒ–çš„æ ·æœ¬ã€‚æœ€è¿‘ï¼Œæ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹å› å…¶èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜åº¦å¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„å›¾åƒçš„èƒ½åŠ›è€Œåœ¨è‡ªç„¶å›¾åƒåˆæˆä¸­å—åˆ°å¹¿æ³›å…³æ³¨ã€‚å—æ­¤å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†Txt2HSI-LDMï¼ˆVAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è¯­è¨€ä¿¡æ¯å¼•å¯¼çš„é«˜å…‰è°±å›¾åƒåˆæˆæ–¹æ³•ï¼Œä»¥è§£å†³HSICä¸­çš„ISSDé—®é¢˜ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä½¿ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£å»é™¤é«˜æ–¯å™ªå£°ï¼Œæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜å…‰è°±æ ·æœ¬ã€‚é¦–å…ˆï¼Œä¸ºäº†è§£å†³é«˜å…‰è°±æ•°æ®çš„é«˜ç»´æ€§é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§é€šç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå°†æ•°æ®æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œè¿™æä¾›äº†ç¨³å®šçš„ç‰¹å¾å¹¶é™ä½äº†æ‰©æ•£æ¨¡å‹çš„æ¨ç†å¤æ‚æ€§ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†åŠç›‘ç£æ‰©æ•£æ¨¡å‹ï¼Œä»¥å……åˆ†åˆ©ç”¨æœªæ ‡è®°æ•°æ®ã€‚ä½¿ç”¨éšæœºå¤šè¾¹å½¢ç©ºé—´è£å‰ªï¼ˆRPSCï¼‰å’Œæ½œåœ¨ç‰¹å¾çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆLF-UEï¼‰æ¥æ¨¡æ‹Ÿä¸åŒç¨‹åº¦çš„æ··åˆã€‚ç¬¬ä¸‰ï¼ŒVAEæ ¹æ®æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ä¸­çš„è¯­è¨€æ¡ä»¶å¯¹HSIè¿›è¡Œè§£ç ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å…¨é¢è¯„ä¼°äº†åˆæˆæ ·æœ¬åœ¨ç»Ÿè®¡ç‰¹æ€§å’ŒäºŒç»´PCAç©ºé—´ä¸­çš„æ•°æ®åˆ†å¸ƒçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå¯¹è§†è§‰è¯­è¨€è·¨æ³¨æ„åŠ›è¿›è¡Œäº†åƒç´ çº§å¯è§†åŒ–ï¼Œè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç”Ÿæˆæ•°æ®çš„ç©ºé—´å¸ƒå±€å’Œå‡ ä½•ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Txt2HSI-LDMï¼ˆVAEï¼‰çš„æ€§èƒ½è¶…è¿‡äº†ç»å…¸çš„åå¤‡æ¨¡å‹ã€æœ€å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œå’ŒåŠç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19700v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ•°æ®å¢å¼ºå¯æœ‰æ•ˆè§£å†³é«˜å…‰è°±å›¾åƒåˆ†ç±»ä¸­çš„å°æ ·æœ¬ä¸å‡è¡¡é—®é¢˜ã€‚è™½ç„¶å¤§å¤šæ•°æ–¹æ³•ä¾§é‡äºåœ¨æ½œåœ¨ç©ºé—´æ‰©å±•ç‰¹å¾ï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶åˆ©ç”¨æ–‡æœ¬é©±åŠ¨ç”Ÿæˆæ¥åˆ›å»ºçœŸå®å’Œå¤šæ ·åŒ–çš„æ ·æœ¬ã€‚å—è¿‘æœŸæ–‡æœ¬å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆæˆä¸­ç”Ÿæˆé«˜åº¦å¤šæ ·åŒ–å’Œé«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºTxt2HSI-LDMï¼ˆVAEï¼‰çš„æ–°å‹è¯­è¨€ä¿¡æ¯é«˜å…‰è°±å›¾åƒåˆæˆæ–¹æ³•ï¼Œä»¥è§£å†³é«˜å…‰è°±å›¾åƒåˆ†ç±»ä¸­çš„å°æ ·æœ¬ä¸å‡è¡¡é—®é¢˜ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£å»é™¤é«˜æ–¯å™ªå£°ï¼Œæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜å…‰è°±æ ·æœ¬ã€‚é¦–å…ˆï¼Œä¸ºäº†è§£å†³é«˜å…‰è°±æ•°æ®çš„é«˜ç»´æ€§é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§é€šç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†æ•°æ®æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œä»¥æä¾›ç¨³å®šç‰¹å¾å’Œé™ä½æ‰©æ•£æ¨¡å‹çš„æ¨ç†å¤æ‚æ€§ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†åŠç›‘ç£æ‰©æ•£æ¨¡å‹ä»¥å……åˆ†åˆ©ç”¨æœªæ ‡è®°æ•°æ®ã€‚ä½¿ç”¨éšæœºå¤šè¾¹å½¢ç©ºé—´è£å‰ªï¼ˆRPSCï¼‰å’Œæ½œåœ¨ç‰¹å¾çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆLF-UEï¼‰æ¥æ¨¡æ‹Ÿä¸åŒç¨‹åº¦çš„æ··åˆã€‚æœ€åï¼ŒVAEå°†æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ä¸­çš„HSIä¸è¯­è¨€æ¡ä»¶ä½œä¸ºè¾“å…¥è¿›è¡Œè§£ç ã€‚å®éªŒå…¨é¢è¯„ä¼°äº†åˆæˆæ ·æœ¬åœ¨äºŒç»´ä¸»æˆåˆ†åˆ†æç©ºé—´ä¸­çš„ç»Ÿè®¡ç‰¹æ€§å’Œæ•°æ®åˆ†å¸ƒçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå¯¹è§†è§‰è¯­è¨€äº¤å‰æ³¨æ„åŠ›è¿›è¡Œäº†åƒç´ çº§å¯è§†åŒ–ï¼Œè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç”Ÿæˆæ•°æ®çš„ç©ºé—´å¸ƒå±€å’Œå‡ ä½•ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Txt2HSI-LDMï¼ˆVAEï¼‰çš„æ€§èƒ½è¶…è¿‡äº†ç»å…¸çš„åç«¯æ¨¡å‹ã€æœ€å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œå’ŒåŠç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºæ˜¯è§£å†³é«˜å…‰è°±å›¾åƒåˆ†ç±»ä¸­å°æ ·æœ¬ä¸å‡è¡¡é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>Txt2HSI-LDMï¼ˆVAEï¼‰æ˜¯ä¸€ç§æ–°å‹è¯­è¨€ä¿¡æ¯é«˜å…‰è°±å›¾åƒåˆæˆæ–¹æ³•ï¼Œåˆ©ç”¨æ–‡æœ¬é©±åŠ¨çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜å…‰è°±æ ·æœ¬ã€‚</li>
<li>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ç”¨äºå°†æ•°æ®æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œæä¾›ç¨³å®šç‰¹å¾å¹¶é™ä½æ‰©æ•£æ¨¡å‹çš„å¤æ‚æ€§ã€‚</li>
<li>åŠç›‘ç£æ‰©æ•£æ¨¡å‹åˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œé€šè¿‡éšæœºå¤šè¾¹å½¢ç©ºé—´è£å‰ªå’Œæ½œåœ¨ç‰¹å¾çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¥æ¨¡æ‹Ÿä¸åŒç¨‹åº¦çš„æ··åˆã€‚</li>
<li>å®éªŒè¯æ˜Txt2HSI-LDMï¼ˆVAEï¼‰åœ¨åˆæˆæ ·æœ¬çš„æœ‰æ•ˆæ€§å’Œæ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ¨¡å‹å’Œæœ€æ–°æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9ec07e8ae92ba45e7ae6ce5b777f709.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13f9f4152f141d8ebe2fa1ce33d19991.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32398b30f5a321f06b6bf6909b04c372.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-405087e4b97c6c5bef1bab38dab01352.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="D-2-DPM-Dual-Denoising-for-Quantized-Diffusion-Probabilistic-Models"><a href="#D-2-DPM-Dual-Denoising-for-Quantized-Diffusion-Probabilistic-Models" class="headerlink" title="D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models"></a>D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models</h2><p><strong>Authors:Qian Zeng, Jie Song, Han Zheng, Hao Jiang, Mingli Song</strong></p>
<p>Diffusion models have achieved cutting-edge performance in image generation. However, their lengthy denoising process and computationally intensive score estimation network impede their scalability in low-latency and resource-constrained scenarios. Post-training quantization (PTQ) compresses and accelerates diffusion models without retraining, but it inevitably introduces additional quantization noise, resulting in mean and variance deviations. In this work, we propose D2-DPM, a dual denoising mechanism aimed at precisely mitigating the adverse effects of quantization noise on the noise estimation network. Specifically, we first unravel the impact of quantization noise on the sampling equation into two components: the mean deviation and the variance deviation. The mean deviation alters the drift coefficient of the sampling equation, influencing the trajectory trend, while the variance deviation magnifies the diffusion coefficient, impacting the convergence of the sampling trajectory. The proposed D2-DPM is thus devised to denoise the quantization noise at each time step, and then denoise the noisy sample through the inverse diffusion iterations. Experimental results demonstrate that D2-DPM achieves superior generation quality, yielding a 1.42 lower FID than the full-precision model while achieving 3.99x compression and 11.67x bit-operation acceleration. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†å‰æ²¿æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶å†—é•¿çš„å»å™ªè¿‡ç¨‹å’Œè®¡ç®—å¯†é›†å‹çš„è¯„åˆ†ä¼°è®¡ç½‘ç»œé˜»ç¢äº†å…¶åœ¨ä½å»¶è¿Ÿå’Œèµ„æºå—é™åœºæ™¯ä¸­çš„å¯æ‰©å±•æ€§ã€‚åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰åœ¨ä¸è¿›è¡Œå†è®­ç»ƒçš„æƒ…å†µä¸‹å‹ç¼©å’ŒåŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹ï¼Œä½†å®ƒä¸å¯é¿å…åœ°å¼•å…¥äº†é¢å¤–çš„é‡åŒ–å™ªå£°ï¼Œå¯¼è‡´å‡å€¼å’Œæ–¹å·®åå·®ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†D2-DPMï¼Œè¿™æ˜¯ä¸€ç§åŒå»å™ªæœºåˆ¶ï¼Œæ—¨åœ¨ç²¾ç¡®ç¼“è§£é‡åŒ–å™ªå£°å¯¹å™ªå£°ä¼°è®¡ç½‘ç»œçš„ä¸åˆ©å½±å“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æäº†é‡åŒ–å™ªå£°å¯¹é‡‡æ ·æ–¹ç¨‹çš„å½±å“å¹¶å°†å…¶åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šå‡å€¼åå·®å’Œæ–¹å·®åå·®ã€‚å‡å€¼åå·®ä¼šæ”¹å˜é‡‡æ ·æ–¹ç¨‹çš„æ¼‚ç§»ç³»æ•°ï¼Œå½±å“è½¨è¿¹è¶‹åŠ¿ï¼Œè€Œæ–¹å·®åå·®ä¼šæ”¾å¤§æ‰©æ•£ç³»æ•°ï¼Œå½±å“é‡‡æ ·è½¨è¿¹çš„æ”¶æ•›æ€§ã€‚å› æ­¤ï¼Œæå‡ºçš„D2-DPMæ—¨åœ¨åœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿å»é™¤é‡åŒ–å™ªå£°ï¼Œç„¶åé€šè¿‡é€†å‘æ‰©æ•£è¿­ä»£è¿›ä¸€æ­¥å»é™¤å™ªå£°æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD2-DPMåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸æ¯”é™ä½äº†1.42çš„FIDå¾—åˆ†ï¼ŒåŒæ—¶å®ç°äº†3.99å€çš„å‹ç¼©å’Œ11.67å€çš„ä½æ“ä½œåŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08180v2">PDF</a> 9 pages, 4 figures, acceptted by AAAI2025, the code is available at   <a target="_blank" rel="noopener" href="https://github.com/taylorjocelyn/d2-dpm">https://github.com/taylorjocelyn/d2-dpm</a></p>
<p><strong>Summary</strong><br>    æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶å†—é•¿çš„å»å™ªè¿‡ç¨‹å’Œè®¡ç®—å¯†é›†å‹çš„è¯„åˆ†ä¼°è®¡ç½‘ç»œé™åˆ¶äº†å…¶åœ¨ä½å»¶è¿Ÿå’Œèµ„æºå—é™åœºæ™¯ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºD2-DPMåŒå»å™ªæœºåˆ¶ï¼Œæ—¨åœ¨ç²¾ç¡®ç¼“è§£é‡åŒ–å™ªå£°å¯¹å™ªå£°ä¼°è®¡ç½‘ç»œçš„ä¸è‰¯å½±å“ã€‚æ–°æœºåˆ¶é€šè¿‡å¯¹é‡‡æ ·æ–¹ç¨‹è¿›è¡Œæ‹†è§£ï¼Œåˆ†åˆ«å¤„ç†å‡å€¼åå·®å’Œæ–¹å·®åå·®ï¼Œè¿›è€Œåœ¨æ¯ä¸€æ­¥å»é™¤é‡åŒ–å™ªå£°ï¼Œå¹¶é€šè¿‡é€†æ‰©æ•£è¿­ä»£å»é™¤å™ªå£°æ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒD2-DPMåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºå…¨ç²¾åº¦æ¨¡å‹é™ä½äº†1.42çš„FIDå¾—åˆ†ï¼ŒåŒæ—¶å®ç°äº†3.99å€çš„å‹ç¼©ç‡å’Œ11.67å€çš„ä½æ“ä½œåŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œä½†å­˜åœ¨å»å™ªè¿‡ç¨‹å†—é•¿å’Œè®¡ç®—æˆæœ¬è¾ƒé«˜çš„é—®é¢˜ã€‚</li>
<li>é‡åŒ–å™ªå£°åœ¨å»å™ªè¿‡ç¨‹ä¸­å¼•å…¥é¢å¤–çš„å™ªå£°ï¼Œå¯¼è‡´å‡å€¼å’Œæ–¹å·®åå·®ã€‚</li>
<li>D2-DPMåŒå»å™ªæœºåˆ¶æ—¨åœ¨ç²¾ç¡®ç¼“è§£é‡åŒ–å™ªå£°å¯¹å™ªå£°ä¼°è®¡ç½‘ç»œçš„å½±å“ã€‚</li>
<li>D2-DPMé€šè¿‡æ‹†è§£é‡‡æ ·æ–¹ç¨‹ï¼Œå¤„ç†å‡å€¼åå·®å’Œæ–¹å·®åå·®ã€‚</li>
<li>D2-DPMåœ¨æ¯ä¸€æ­¥å»é™¤é‡åŒ–å™ªå£°ï¼Œå¹¶é€šè¿‡é€†æ‰©æ•£è¿­ä»£å»é™¤å™ªå£°æ ·æœ¬ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒD2-DPMåœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºå…¨ç²¾åº¦æ¨¡å‹é™ä½äº†FIDå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c6a4d18ddd3b64c153374199b6ee5c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16492c8312d40ae65dca7cf804d809e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b16f874b12b741145663dc724d22f37.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SVDQuant-Absorbing-Outliers-by-Low-Rank-Components-for-4-Bit-Diffusion-Models"><a href="#SVDQuant-Absorbing-Outliers-by-Low-Rank-Components-for-4-Bit-Diffusion-Models" class="headerlink" title="SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion   Models"></a>SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion   Models</h2><p><strong>Authors:Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han</strong></p>
<p>Diffusion models can effectively generate high-quality images. However, as they scale, rising memory demands and higher latency pose substantial deployment challenges. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where existing post-training quantization methods like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing, which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights. Then, we use a high-precision, low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD), while a low-bit quantized branch handles the residuals. This process eases the quantization on both sides. However, naively running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without re-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5$\times$, achieving 3.0$\times$ speedup over the 4-bit weight-only quantization (W4A16) baseline on the 16GB laptop 4090 GPU with INT4 precision. On the latest RTX 5090 desktop with Blackwell architecture, we achieve a 3.1$\times$ speedup compared to the W4A16 model using NVFP4 precision. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œéšç€å…¶è§„æ¨¡æ‰©å¤§ï¼Œå†…å­˜éœ€æ±‚çš„å¢é•¿å’Œå»¶è¿Ÿé—®é¢˜ä¸ºéƒ¨ç½²å¸¦æ¥äº†å®è´¨æ€§çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡å°†æ‰©æ•£æ¨¡å‹çš„æƒé‡å’Œæ¿€æ´»é‡é‡åŒ–åˆ°4ä½æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚åœ¨è¿™æ ·æç«¯çš„é‡åŒ–çº§åˆ«ä¸‹ï¼Œæƒé‡å’Œæ¿€æ´»é‡éƒ½é«˜åº¦æ•æ„Ÿï¼Œç°æœ‰çš„è®­ç»ƒåé‡åŒ–æ–¹æ³•ï¼ˆå¦‚å¹³æ»‘ï¼‰å˜å¾—ä¸è¶³ä»¥åº”å¯¹ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SVDQuantè¿™ä¸€å…¨æ–°çš„4ä½é‡åŒ–èŒƒå¼ã€‚ä¸é€šè¿‡é‡æ–°åˆ†é…æƒé‡å’Œæ¿€æ´»å€¼ä¸­çš„å¼‚å¸¸å€¼è¿›è¡Œå¹³æ»‘çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä½é˜¶åˆ†æ”¯å¸æ”¶è¿™äº›å¼‚å¸¸å€¼ã€‚æˆ‘ä»¬é¦–å…ˆå°†å¼‚å¸¸å€¼ä»æ¿€æ´»å€¼è½¬ç§»åˆ°æƒé‡è¿›è¡Œåˆå¹¶ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜ç²¾ç¡®åº¦ã€ä½é˜¶åˆ†æ”¯é‡‡ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ¥æ¥æ”¶æƒé‡å¼‚å¸¸å€¼ï¼Œè€Œä½ä½é‡åŒ–åˆ†æ”¯åˆ™å¤„ç†æ®‹å·®ã€‚è¿™ä¸ªè¿‡ç¨‹ä½¿ä¸¤è¾¹çš„é‡åŒ–å˜å¾—æ›´å®¹æ˜“ã€‚ç„¶è€Œï¼Œå•çº¯ç‹¬ç«‹è¿è¡Œä½é˜¶åˆ†æ”¯ä¼šç”±äºæ¿€æ´»å€¼çš„é¢å¤–æ•°æ®ç§»åŠ¨è€Œäº§ç”Ÿå¤§é‡å¼€é”€ï¼Œä»è€ŒæŠµæ¶ˆé‡åŒ–çš„åŠ é€Ÿæ•ˆæœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å…±åŒè®¾è®¡äº†ä¸€ä¸ªæ¨ç†å¼•æ“Nunchakuï¼Œå®ƒå°†ä½é˜¶åˆ†æ”¯çš„æ ¸å¿ƒèåˆåˆ°ä½ä½åˆ†æ”¯çš„æ ¸å¿ƒä¸­ï¼Œä»¥åˆ‡æ–­å†—ä½™çš„å†…å­˜è®¿é—®ã€‚å®ƒè¿˜å¯ä»¥æ— ç¼æ”¯æŒç°æˆçš„ä½é˜¶é€‚é…å™¨ï¼ˆLoRAsï¼‰è€Œæ— éœ€é‡æ–°é‡åŒ–ã€‚åœ¨SDXLã€PixArt-$\Sigma$å’ŒFLUX.1ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†SVDQuantåœ¨ä¿æŒå›¾åƒè´¨é‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨å…·æœ‰INT4ç²¾åº¦çš„16GB laptop 4090 GPUä¸Šï¼Œæˆ‘ä»¬å‡å°‘äº†FLUX.1æ¨¡å‹çš„å†…å­˜ä½¿ç”¨é‡ï¼ˆä¹˜æ•°æ•ˆåº”ä¸º3.5å€ï¼‰ï¼Œç›¸å¯¹äºä»…æƒé‡é‡åŒ–çš„W4A16åŸºçº¿å®ç°äº†é«˜è¾¾3.0å€çš„åŠ é€Ÿæ•ˆæœã€‚åœ¨å…·æœ‰Blackwellæ¶æ„çš„æœ€æ–°RTX 5090å°å¼æœºä¸Šï¼Œä¸ä½¿ç”¨NVFP4ç²¾åº¦çš„W4A16æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†é«˜è¾¾3.1å€çš„åŠ é€Ÿæ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05007v3">PDF</a> ICLR 2025 Spotlight Quantization Library:   <a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/deepcompressor">https://github.com/mit-han-lab/deepcompressor</a> Inference Engine:   <a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/nunchaku">https://github.com/mit-han-lab/nunchaku</a> Website:   <a target="_blank" rel="noopener" href="https://hanlab.mit.edu/projects/svdquant">https://hanlab.mit.edu/projects/svdquant</a> Demo: <a target="_blank" rel="noopener" href="https://svdquant.mit.edu/">https://svdquant.mit.edu</a> Blog:   <a target="_blank" rel="noopener" href="https://hanlab.mit.edu/blog/svdquant">https://hanlab.mit.edu/blog/svdquant</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹èƒ½æœ‰æ•ˆç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†éšç€è§„æ¨¡æ‰©å¤§ï¼Œå†…å­˜éœ€æ±‚å¢åŠ å’Œå»¶è¿Ÿå¢é«˜ï¼Œéƒ¨ç½²æŒ‘æˆ˜ä¹Ÿç›¸åº”å¢å¤§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å°†æ‰©æ•£æ¨¡å‹çš„æƒé‡å’Œæ¿€æ´»å€¼é‡åŒ–è‡³4ä½æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚æå‡ºä¸€ç§åä¸ºSVDQuantçš„æ–°4ä½é‡åŒ–æ–¹æ³•ï¼Œä¸åŒäºç°æœ‰çš„å¹³æ»‘åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä½é˜¶åˆ†æ”¯å¸æ”¶å¼‚å¸¸å€¼ã€‚é¦–å…ˆï¼Œå°†å¼‚å¸¸å€¼ä»æ¿€æ´»è½¬ç§»åˆ°æƒé‡è¿›è¡Œåˆå¹¶ï¼Œç„¶åä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å¤„ç†æƒé‡å¼‚å¸¸å€¼ï¼ŒåŒæ—¶ä½ä½é‡åŒ–åˆ†æ”¯å¤„ç†æ®‹å·®ã€‚è¿™ä¸€è¿‡ç¨‹ä½¿ä¸¤ä¾§é‡åŒ–æ›´ä¸ºè½»æ¾ã€‚ç„¶è€Œï¼Œå•ç‹¬è¿è¡Œä½é˜¶åˆ†æ”¯ä¼šå¯¼è‡´æ¿€æ´»é¢å¤–æ•°æ®ä¼ è¾“å¼€é”€ï¼ŒæŠµæ¶ˆé‡åŒ–åŠ é€Ÿæ•ˆæœã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶è®¾è®¡äº†ä¸€ç§æ¨ç†å¼•æ“Nunchakuï¼Œå°†ä½é˜¶åˆ†æ”¯å†…æ ¸èå…¥ä½ä½åˆ†æ”¯å†…æ ¸ä¸­ï¼Œå‡å°‘å†…å­˜è®¿é—®ï¼Œå¹¶æ”¯æŒç¦»çº¿ä½é˜¶é€‚é…å™¨ï¼ˆLoRAsï¼‰æ— ç¼ä½¿ç”¨ã€‚å®éªŒè¯æ˜SVDQuantåœ¨ä¿æŒå›¾åƒè´¨é‡æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå‡å°‘äº†FLUX.1æ¨¡å‹çš„å†…å­˜ä½¿ç”¨ï¼Œå¹¶åœ¨ä¸åŒè®¾å¤‡ä¸Šå®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†éšç€è§„æ¨¡æ‰©å¤§ï¼Œéƒ¨ç½²æŒ‘æˆ˜å¢åŠ ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡æ˜¯é€šè¿‡å°†æƒé‡å’Œæ¿€æ´»å€¼é‡åŒ–è‡³4ä½æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºSVDQuantçš„æ–°é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡åˆå¹¶å¼‚å¸¸å€¼å¹¶ä½¿ç”¨ä½é˜¶åˆ†æ”¯å¤„ç†æ¥æé«˜é‡åŒ–æ•ˆç‡ã€‚</li>
<li>Nunchakuæ¨ç†å¼•æ“è¢«è®¾è®¡æ¥å‡å°‘å†…å­˜è®¿é—®å¹¶å¢å¼ºé‡åŒ–åŠ é€Ÿæ•ˆæœã€‚</li>
<li>SVDQuantåœ¨ä¿æŒå›¾åƒè´¨é‡æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå‡å°‘äº†å†…å­˜ä½¿ç”¨å¹¶å®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§è®¾å¤‡ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cd05a3c91417833cda363862fd62feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-667d930f5abfbc40ea214ceac5e19a7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be9d50b59dbd25a14fa6c411bc73af0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76796b844ab76c99cf220285b9f6c04e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VILA-U-a-Unified-Foundation-Model-Integrating-Visual-Understanding-and-Generation"><a href="#VILA-U-a-Unified-Foundation-Model-Integrating-Visual-Understanding-and-Generation" class="headerlink" title="VILA-U: a Unified Foundation Model Integrating Visual Understanding and   Generation"></a>VILA-U: a Unified Foundation Model Integrating Visual Understanding and   Generation</h2><p><strong>Authors:Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, Yao Lu</strong></p>
<p>VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework. </p>
<blockquote>
<p>VILA-Uæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºç¡€æ¨¡å‹ï¼Œèåˆäº†è§†é¢‘ã€å›¾åƒã€è¯­è¨€ç†è§£å’Œç”Ÿæˆã€‚ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½¿ç”¨å•ç‹¬çš„æ¨¡å—æ¥ç†è§£å’Œç”Ÿæˆè§†è§‰å†…å®¹ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´å¯¹é½é—®é¢˜å’Œå¢åŠ å¤æ‚æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒVILA-Ué‡‡ç”¨å•ä¸ªçš„è‡ªå›å½’ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æ¡†æ¶æ¥å®Œæˆè¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œæ— éœ€ä½¿ç”¨å¦‚æ‰©æ•£æ¨¡å‹ç­‰é¢å¤–ç»„ä»¶ã€‚è¿™ç§æ–¹æ³•ä¸ä»…ç®€åŒ–äº†æ¨¡å‹ï¼Œè€Œä¸”åœ¨è§†è§‰è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æ¥è¿‘æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚VILA-Uçš„æˆåŠŸå½’å› äºä¸¤ä¸ªä¸»è¦å› ç´ ï¼šåœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œç¦»æ•£è§†è§‰ä»¤ç‰Œä¸æ–‡æœ¬è¾“å…¥çš„ç»Ÿä¸€è§†è§‰å¡”å¯¹é½ï¼Œè¿™å¢å¼ºäº†è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼›è‡ªå›å½’å›¾åƒç”Ÿæˆå¯ä»¥ä½¿ç”¨é«˜è´¨é‡æ•°æ®é›†è¾¾åˆ°ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“çš„è´¨é‡ã€‚è¿™ä½¿å¾—VILA-Uèƒ½å¤Ÿåœ¨å®Œå…¨åŸºäºä»¤ç‰Œçš„è‡ªå›å½’æ¡†æ¶ä¸‹ä¸æ›´å¤æ‚çš„æ¨¡å‹è¿›è¡Œç›¸å½“çš„æ€§èƒ½è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.04429v3">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/vila-u">https://github.com/mit-han-lab/vila-u</a>. The first two authors   contributed equally to this work</p>
<p><strong>Summary</strong></p>
<p>VILA-Uæ˜¯ä¸€ç§ç»Ÿä¸€åŸºç¡€æ¨¡å‹ï¼Œèåˆäº†è§†é¢‘ã€å›¾åƒã€è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ç›¸æ¯”ä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡å‹ä½¿ç”¨åˆ†ç¦»æ¨¡å—è¿›è¡Œè§†è§‰å†…å®¹ç†è§£å’Œç”Ÿæˆå¯¼è‡´çš„é”™ä½å’Œå¤æ‚æ€§å¢åŠ é—®é¢˜ï¼ŒVILA-Ué‡‡ç”¨å•ä¸€çš„è‡ªå›å½’ä¸‹ä¸€ä»£æ ‡è®°é¢„æµ‹æ¡†æ¶ï¼ŒåŒæ—¶æ”¯æŒä¸¤é¡¹ä»»åŠ¡ï¼Œæ— éœ€ä½¿ç”¨å¦‚æ‰©æ•£æ¨¡å‹ç­‰é¢å¤–ç»„ä»¶ã€‚æ­¤æ–¹æ³•ç®€åŒ–äº†æ¨¡å‹ç»“æ„ï¼ŒåŒæ—¶å®ç°äº†è¿‘ä¹æœ€å…ˆè¿›æ°´å¹³çš„è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆæ€§èƒ½ã€‚å…¶æˆåŠŸå½’åŠŸäºä¸¤å¤§å› ç´ ï¼šåœ¨é¢„è®­ç»ƒé˜¶æ®µå°†ç¦»æ•£è§†è§‰æ ‡è®°ä¸æ–‡æœ¬è¾“å…¥å¯¹é½çš„ç»Ÿä¸€è§†è§‰å¡”ï¼Œå¢å¼ºäº†è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼›ä»¥åŠä½¿ç”¨é«˜è´¨é‡æ•°æ®é›†å¯å®ç°ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“è´¨é‡çš„è‡ªå›å½’å›¾åƒç”Ÿæˆã€‚è¿™ä½¿å¾—VILA-Uåœ¨å®Œå…¨åŸºäºæ ‡è®°çš„è‡ªå›å½’æ¡†æ¶ä¸‹ï¼Œè¡¨ç°ä¸æ›´å¤æ‚æ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VILA-Uæ˜¯ä¸€ä¸ªç»Ÿä¸€åŸºç¡€æ¨¡å‹ï¼Œé›†æˆäº†è§†é¢‘ã€å›¾åƒã€è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡å‹ä½¿ç”¨åˆ†ç¦»æ¨¡å—ï¼Œå¯èƒ½å¯¼è‡´é”™ä½å’Œå¤æ‚æ€§å¢åŠ ã€‚</li>
<li>VILA-Ué‡‡ç”¨å•ä¸€è‡ªå›å½’ä¸‹ä¸€ä»£æ ‡è®°é¢„æµ‹æ¡†æ¶ï¼Œç®€åŒ–æ¨¡å‹ç»“æ„ã€‚</li>
<li>VILA-Uä¸éœ€è¦é¢å¤–çš„ç»„ä»¶ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>VILA-Uå®ç°äº†è¿‘ä¹æœ€å…ˆè¿›æ°´å¹³çš„è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>VILA-Uçš„æˆåŠŸå½’åŠŸäºç»Ÿä¸€è§†è§‰å¡”å’Œè‡ªå›å½’å›¾åƒç”ŸæˆæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.04429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7edfc24dbe7a4ce718bb1ea780830538.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88acd49bda32e5891611c21ec5704758.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-911aad5262aec12eda5095b3197edfb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0c00a3ddc8ffa155dc331e4362fae4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-feb235d19286c490e79ff2d8b6caebcd.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e48aad53b5338a6cb41467f17f6403a4.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  Multimodal Deep Learning for Subtype Classification in Breast Cancer   Using Histopathological Images and Gene Expression Data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-190fb996ee21d987789559f1b5cf54f5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  2DGS-Avatar Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
