<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  A Hypernetwork-Based Approach to KAN Representation of Audio Signals">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bb33e342eb5d889f700d0fd1429ca977.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-06-æ›´æ–°"><a href="#2025-03-06-æ›´æ–°" class="headerlink" title="2025-03-06 æ›´æ–°"></a>2025-03-06 æ›´æ–°</h1><h2 id="A-Hypernetwork-Based-Approach-to-KAN-Representation-of-Audio-Signals"><a href="#A-Hypernetwork-Based-Approach-to-KAN-Representation-of-Audio-Signals" class="headerlink" title="A Hypernetwork-Based Approach to KAN Representation of Audio Signals"></a>A Hypernetwork-Based Approach to KAN Representation of Audio Signals</h2><p><strong>Authors:Patryk MarszaÅ‚ek, Maciej Rut, Piotr Kawa, Piotr Syga</strong></p>
<p>Implicit neural representations (INR) have gained prominence for efficiently encoding multimedia data, yet their applications in audio signals remain limited. This study introduces the Kolmogorov-Arnold Network (KAN), a novel architecture using learnable activation functions, as an effective INR model for audio representation. KAN demonstrates superior perceptual performance over previous INRs, achieving the lowest Log-SpectralDistance of 1.29 and the highest Perceptual Evaluation of Speech Quality of 3.57 for 1.5 s audio. To extend KANâ€™s utility, we propose FewSound, a hypernetwork-based architecture that enhances INR parameter updates. FewSound outperforms the state-of-the-art HyperSound, with a 33.3% improvement in MSE and 60.87% in SI-SNR. These results show KAN as a robust and adaptable audio representation with the potential for scalability and integration into various hypernetwork frameworks. The source code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git">https://github.com/gmum/fewsound.git</a>. </p>
<blockquote>
<p>éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰åœ¨å¤šåª’ä½“æ•°æ®ç¼–ç æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éŸ³é¢‘ä¿¡å·æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨å¯å­¦ä¹ æ¿€æ´»å‡½æ•°çš„æ–°å‹æ¶æ„ï¼Œä½œä¸ºéŸ³é¢‘è¡¨ç¤ºçš„æœ‰æ•ˆINRæ¨¡å‹ã€‚KANåœ¨æ„ŸçŸ¥æ€§èƒ½ä¸Šä¼˜äºå…ˆå‰çš„INRï¼Œåœ¨1.5ç§’éŸ³é¢‘ä¸Šå®ç°äº†æœ€ä½çš„Log-SpectralDistanceä¸º1.29å’Œæœ€é«˜çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ä¸º3.57ã€‚ä¸ºäº†æ‹“å±•KANçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¶…ç½‘ç»œçš„FewSoundæ¶æ„ï¼Œç”¨äºå¢å¼ºINRå‚æ•°æ›´æ–°ã€‚FewSoundçš„è¡¨ç°ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„HyperSoundï¼Œåœ¨MSEä¸Šæé«˜äº†33.3%ï¼Œåœ¨SI-SNRä¸Šæé«˜äº†60.87%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒKANæ˜¯ä¸€ç§ç¨³å¥ä¸”å¯é€‚åº”çš„éŸ³é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œé›†æˆåˆ°å„ç§è¶…ç½‘ç»œæ¡†æ¶çš„æ½œåŠ›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/gmum/fewsound.gitè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02585v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰çš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„é‡‡ç”¨å¯å­¦ä¹ çš„æ¿€æ´»å‡½æ•°ï¼Œä½œä¸ºéŸ³é¢‘è¡¨ç¤ºçš„æœ‰æ•ˆéšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ¨¡å‹ã€‚KANåœ¨éŸ³é¢‘ä¿¡å·ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„INRæ¨¡å‹ï¼Œå¹¶åœ¨çŸ­çŸ­çš„1.5ç§’éŸ³é¢‘ä¸Šå®ç°äº†æœ€ä½çš„é€»è¾‘è°±è·ç¦»å’Œæœ€é«˜çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†åŸºäºè¶…ç½‘ç»œçš„FewSoundæ¶æ„ï¼Œå¯å¢å¼ºINRå‚æ•°æ›´æ–°ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„HyperSoundï¼Œåœ¨MSEå’ŒSI-SNRæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒKANä½œä¸ºä¸€ç§ç¨³å¥ä¸”å¯é€‚åº”çš„éŸ³é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œé›†æˆåˆ°å„ç§è¶…ç½‘ç»œæ¡†æ¶çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Kolmogorv-Arnoldç½‘ç»œï¼ˆKANï¼‰æ˜¯ä¸€ç§æ–°å‹çš„éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ¨¡å‹ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„æ¿€æ´»å‡½æ•°ï¼Œèƒ½æœ‰æ•ˆç¼–ç éŸ³é¢‘æ•°æ®ã€‚</li>
<li>KANåœ¨éŸ³é¢‘è¡¨ç°ä¸Šè¶…è¶Šäº†å…ˆå‰çš„INRæ¨¡å‹ï¼Œå®ç°äº†æ›´ä½çš„é€»è¾‘è°±è·ç¦»å’Œæ›´é«˜çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ã€‚</li>
<li>FewSoundæ¶æ„æ˜¯åŸºäºè¶…ç½‘ç»œçš„è®¾è®¡ï¼Œèƒ½å¢å¼ºINRæ¨¡å‹çš„å‚æ•°æ›´æ–°ã€‚</li>
<li>FewSoundåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„HyperSoundï¼Œåœ¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰å’Œä¿¡å·å¹²æ‰°æºåˆ°å™ªå£°æ¯”ç‡ï¼ˆSI-SNRï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>KANæ¨¡å‹å…·æœ‰å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ï¼Œå¯åº”ç”¨äºå„ç§è¶…ç½‘ç»œæ¡†æ¶ã€‚</li>
<li>è¯¥ç ”ç©¶çš„æºä»£ç å·²å…¬å¼€ï¼Œå¯æ–¹ä¾¿åç»­ç ”ç©¶å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cb9f41d8cb87402459d5430738cf78a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c4f7c48b31dfb96976d786656cfb4c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d090ab0619b083e39491455333ca26eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9e7b776c8e24e69cab579d207f0e9a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e893db96daf84353ce23981e9ef98456.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a338323207308a508f7be0fd754e2d4d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MM-OR-A-Large-Multimodal-Operating-Room-Dataset-for-Semantic-Understanding-of-High-Intensity-Surgical-Environments"><a href="#MM-OR-A-Large-Multimodal-Operating-Room-Dataset-for-Semantic-Understanding-of-High-Intensity-Surgical-Environments" class="headerlink" title="MM-OR: A Large Multimodal Operating Room Dataset for Semantic   Understanding of High-Intensity Surgical Environments"></a>MM-OR: A Large Multimodal Operating Room Dataset for Semantic   Understanding of High-Intensity Surgical Environments</h2><p><strong>Authors:Ege Ã–zsoy, Chantal Pellegrini, Tobias Czempiel, Felix Tristram, Kun Yuan, David Bani-Harouni, Ulrich Eck, Benjamin Busam, Matthias Keicher, Nassir Navab</strong></p>
<p>Operating rooms (ORs) are complex, high-stakes environments requiring precise understanding of interactions among medical staff, tools, and equipment for enhancing surgical assistance, situational awareness, and patient safety. Current datasets fall short in scale, realism and do not capture the multimodal nature of OR scenes, limiting progress in OR modeling. To this end, we introduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR dataset, and the first dataset to enable multimodal scene graph generation. MM-OR captures comprehensive OR scenes containing RGB-D data, detail views, audio, speech transcripts, robotic logs, and tracking data and is annotated with panoptic segmentations, semantic scene graphs, and downstream task labels. Further, we propose MM2SG, the first multimodal large vision-language model for scene graph generation, and through extensive experiments, demonstrate its ability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG establish a new benchmark for holistic OR understanding, and open the path towards multimodal scene analysis in complex, high-stakes environments. Our code, and data is available at <a target="_blank" rel="noopener" href="https://github.com/egeozsoy/MM-OR">https://github.com/egeozsoy/MM-OR</a>. </p>
<blockquote>
<p>æ‰‹æœ¯å®¤ï¼ˆORsï¼‰æ˜¯ä¸€ä¸ªå¤æ‚ä¸”é«˜é£é™©çš„ç¯å¢ƒï¼Œéœ€è¦ç²¾ç¡®ç†è§£åŒ»ç–—äººå‘˜ã€å·¥å…·å’Œè®¾å¤‡ä¹‹é—´çš„äº¤äº’ï¼Œä»¥æé«˜æ‰‹æœ¯è¾…åŠ©ã€æƒ…å¢ƒæ„è¯†å’Œç—…äººå®‰å…¨ã€‚å½“å‰çš„æ•°æ®é›†åœ¨è§„æ¨¡ã€çœŸå®æ„Ÿæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¹¶ä¸”æœªèƒ½æ•æ‰åˆ°æ‰‹æœ¯å®¤åœºæ™¯çš„å¤šæ¨¡å¼æ€§è´¨ï¼Œä»è€Œé™åˆ¶äº†æ‰‹æœ¯å®¤å»ºæ¨¡çš„è¿›å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MM-ORï¼Œè¿™æ˜¯ä¸€ä¸ªçœŸå®ä¸”å¤§è§„æ¨¡çš„å¤šæ¨¡å¼æ—¶ç©ºæ‰‹æœ¯å®¤æ•°æ®é›†ï¼Œä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå®ç°å¤šæ¨¡å¼åœºæ™¯å›¾ç”Ÿæˆçš„æ•°æ®é›†ã€‚MM-ORæ•æ‰äº†åŒ…å«RGB-Dæ•°æ®ã€ç»†èŠ‚è§†å›¾ã€éŸ³é¢‘ã€è¯­éŸ³è®°å½•ã€æœºå™¨äººæ—¥å¿—å’Œè·Ÿè¸ªæ•°æ®çš„å…¨é¢æ‰‹æœ¯å®¤åœºæ™¯ï¼Œå¹¶å¸¦æœ‰å…¨æ™¯åˆ†å‰²ã€è¯­ä¹‰åœºæ™¯å›¾å’Œä¸‹æ¸¸ä»»åŠ¡æ ‡ç­¾çš„æ³¨é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†MM2SGï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºåœºæ™¯å›¾ç”Ÿæˆçš„å¤§å‹å¤šæ¨¡å¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¤§é‡å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡å¼è¾“å…¥çš„èƒ½åŠ›ã€‚æ€»ä¹‹ï¼ŒMM-ORå’ŒMM2SGä¸ºå…¨é¢çš„æ‰‹æœ¯å®¤ç†è§£å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºå¤æ‚é«˜é£é™©ç¯å¢ƒä¸­çš„å¤šæ¨¡å¼åœºæ™¯åˆ†æå¼€è¾Ÿäº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/egeozsoy/MM-OR">https://github.com/egeozsoy/MM-OR</a> ä¸­è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02579v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰‹æœ¯ç¯å¢ƒå¤æ‚ä¸”é£é™©é«˜ï¼Œéœ€è¦æ·±å…¥ç†è§£æ‰‹æœ¯å®¤å†…çš„äº¤äº’æƒ…å†µä»¥æå‡æ‰‹æœ¯è¾…åŠ©ã€æ€åŠ¿æ„ŸçŸ¥å’Œæ‚£è€…å®‰å…¨ã€‚å½“å‰æ•°æ®é›†åœ¨è§„æ¨¡ã€ç°å®æ€§å’Œå¤šæ¨¡æ€æ•æ‰æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†æ‰‹æœ¯å®¤å»ºæ¨¡çš„è¿›å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºMM-ORæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€ç°å®çš„å¤šæ¨¡æ€æ—¶ç©ºæ‰‹æœ¯å®¤æ•°æ®é›†ï¼Œä¹Ÿæ˜¯é¦–ä¸ªèƒ½å¤Ÿå®ç°å¤šæ¨¡æ€åœºæ™¯å›¾ç”Ÿæˆçš„æ•°æ®é›†ã€‚MM-ORæ•æ‰äº†å…¨é¢çš„æ‰‹æœ¯å®¤åœºæ™¯ï¼ŒåŒ…æ‹¬RGB-Dæ•°æ®ã€ç»†èŠ‚è§†å›¾ã€éŸ³é¢‘ã€è¯­éŸ³è®°å½•ã€æœºå™¨äººæ—¥å¿—å’Œè·Ÿè¸ªæ•°æ®ï¼Œå¹¶å¸¦æœ‰å…¨æ™¯åˆ†å‰²ã€è¯­ä¹‰åœºæ™¯å›¾å’Œä¸‹æ¸¸ä»»åŠ¡æ ‡ç­¾çš„æ³¨é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†MM2SGï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºåœºæ™¯å›¾ç”Ÿæˆçš„å¤§å‹å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¤§é‡å®éªŒè¯æ˜å…¶æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡æ€è¾“å…¥çš„èƒ½åŠ›ã€‚æ€»ä¹‹ï¼ŒMM-ORå’ŒMM2SGä¸ºå…¨é¢çš„æ‰‹æœ¯å®¤ç†è§£å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºå¤æ‚é«˜é£é™©ç¯å¢ƒä¸­çš„å¤šæ¨¡æ€åœºæ™¯åˆ†æå¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹æœ¯å®¤æ˜¯å¤æ‚çš„åŒ»ç–—ç¯å¢ƒï¼Œéœ€è¦å¢å¼ºå¯¹åŒ»ç–—äººå‘˜ã€å·¥å…·å’Œè®¾å¤‡çš„äº¤äº’ç†è§£ä»¥æå‡æ‰‹æœ¯æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚</li>
<li>å½“å‰æ•°æ®é›†åœ¨è§„æ¨¡ã€ç°å®æ€§å’Œå¤šæ¨¡æ€æ•æ‰æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œé™åˆ¶äº†æ‰‹æœ¯å®¤ç¯å¢ƒçš„å»ºæ¨¡ç ”ç©¶ã€‚</li>
<li>MM-ORæ•°æ®é›†æ˜¯é¦–ä¸ªå¤§è§„æ¨¡ã€ç°å®çš„å¤šæ¨¡æ€æ—¶ç©ºæ‰‹æœ¯å®¤æ•°æ®é›†ï¼Œèƒ½å¤Ÿå…¨é¢æ•æ‰æ‰‹æœ¯å®¤çš„å¤šç§ä¿¡æ¯ã€‚</li>
<li>MM-ORæ•°æ®é›†åŒ…å«RGB-Dæ•°æ®ã€ç»†èŠ‚è§†å›¾ã€éŸ³é¢‘ã€è¯­éŸ³è®°å½•ç­‰å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶å¸¦æœ‰å…¨é¢çš„æ³¨é‡Šã€‚</li>
<li>MM2SGæ˜¯é¦–ä¸ªç”¨äºåœºæ™¯å›¾ç”Ÿæˆçš„å¤§å‹å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡æ€è¾“å…¥ã€‚</li>
<li>MM-ORå’ŒMM2SGå…±åŒä¸ºæ‰‹æœ¯å®¤çš„å…¨é¢ç†è§£è®¾ç«‹äº†æ–°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-209d089f15e1abd82c196c77ea56c479.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5294af7bf5a73eba5dd02661d43c0099.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f2d4ca49da20623f7ae8a57896fc411.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-541178215dd7736b81e3bbc374e8bab4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Pruning-Deep-Neural-Networks-via-a-Combination-of-the-Marchenko-Pastur-Distribution-and-Regularization"><a href="#Pruning-Deep-Neural-Networks-via-a-Combination-of-the-Marchenko-Pastur-Distribution-and-Regularization" class="headerlink" title="Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur   Distribution and Regularization"></a>Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur   Distribution and Regularization</h2><p><strong>Authors:Leonid Berlyand, Theo Bourdais, Houman Owhadi, Yitzchak Shmalo</strong></p>
<p>Deep neural networks (DNNs) have brought significant advancements in various applications in recent years, such as image recognition, speech recognition, and natural language processing. In particular, Vision Transformers (ViTs) have emerged as a powerful class of models in the field of deep learning for image classification. In this work, we propose a novel Random Matrix Theory (RMT)-based method for pruning pre-trained DNNs, based on the sparsification of weights and singular vectors, and apply it to ViTs. RMT provides a robust framework to analyze the statistical properties of large matrices, which has been shown to be crucial for understanding and optimizing the performance of DNNs. We demonstrate that our RMT-based pruning can be used to reduce the number of parameters of ViT models (trained on ImageNet) by 30-50% with less than 1% loss in accuracy. To our knowledge, this represents the state-of-the-art in pruning for these ViT models. Furthermore, we provide a rigorous mathematical underpinning of the above numerical studies, namely we proved a theorem for fully connected DNNs, and other more general DNN structures, describing how the randomness in the weight matrices of a DNN decreases as the weights approach a local or global minimum (during training). We verify this theorem through numerical experiments on fully connected DNNs, providing empirical support for our theoretical findings. Moreover, we prove a theorem that describes how DNN loss decreases as we remove randomness in the weight layers, and show a monotone dependence of the decrease in loss with the amount of randomness that we remove. Our results also provide significant RMT-based insights into the role of regularization during training and pruning. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰åœ¨å„ç§åº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¦‚å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚ç‰¹åˆ«æ˜¯ï¼Œè§†è§‰Transformerï¼ˆViTsï¼‰ä½œä¸ºæ·±åº¦å­¦ä¹ é¢†åŸŸçš„ä¸€ç§å¼ºå¤§æ¨¡å‹ï¼Œåœ¨å›¾åƒåˆ†ç±»ä¸­å´­éœ²å¤´è§’ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„é¢„è®­ç»ƒDNNå‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºæƒé‡å’Œå¥‡å¼‚å‘é‡çš„ç¨€ç–åŒ–ï¼Œå¹¶åº”ç”¨äºViTsã€‚RMTæä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶æ¥åˆ†æå¤§å‹çŸ©é˜µçš„ç»Ÿè®¡ç‰¹æ€§ï¼Œè¿™å·²è¢«è¯æ˜å¯¹äºç†è§£å’Œä¼˜åŒ–DNNçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºRMTçš„å‹ç¼©æŠ€æœ¯å¯ç”¨äºå°†ç»è¿‡ImageNetè®­ç»ƒçš„ViTæ¨¡å‹çš„å‚æ•°æ•°é‡å‡å°‘30-50%ï¼ŒåŒæ—¶ç²¾åº¦æŸå¤±ä¸åˆ°1%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é’ˆå¯¹è¿™äº›ViTæ¨¡å‹çš„æœ€æ–°å‹ç¼©æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ä¸Šè¿°æ•°å€¼ç ”ç©¶è¿›è¡Œäº†ä¸¥æ ¼çš„æ•°å­¦æ”¯æ’‘ï¼Œå³æˆ‘ä»¬ä¸ºå…¨è¿æ¥DNNå’Œå…¶ä»–æ›´ä¸€èˆ¬çš„DNNç»“æ„è¯æ˜äº†ä¸€ä¸ªå®šç†ï¼Œæè¿°äº†DNNæƒé‡çŸ©é˜µä¸­çš„éšæœºæ€§å¦‚ä½•éšç€æƒé‡æ¥è¿‘å±€éƒ¨æˆ–å…¨å±€æœ€å°å€¼ï¼ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼‰è€Œå‡å°‘ã€‚æˆ‘ä»¬é€šè¿‡å…¨è¿æ¥DNNçš„æ•°å€¼å®éªŒéªŒè¯äº†è¿™ä¸€å®šç†ï¼Œä¸ºæˆ‘ä»¬çš„ç†è®ºå‘ç°æä¾›äº†å®è¯æ”¯æŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†å¦ä¸€ä¸ªå®šç†ï¼Œæè¿°äº†éšç€æˆ‘ä»¬åœ¨æƒé‡å±‚ä¸­æ¶ˆé™¤éšæœºæ€§ï¼ŒDNNæŸå¤±å¦‚ä½•å‡å°‘ï¼Œä»¥åŠæŸå¤±å‡å°‘ä¸æˆ‘ä»¬æ¶ˆé™¤çš„éšæœºæ€§é‡ä¹‹é—´çš„å•è°ƒä¾èµ–æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜æä¾›äº†åŸºäºRMTçš„å…³äºè®­ç»ƒå’Œå‹ç¼©è¿‡ç¨‹ä¸­æ­£åˆ™åŒ–ä½œç”¨çš„æ·±åˆ»è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01922v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å‰ªææ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºå›¾åƒåˆ†ç±»é¢†åŸŸçš„Vision Transformersï¼ˆViTsï¼‰ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åˆ©ç”¨æƒé‡ç¨€ç–åŒ–å’Œå¥‡å¼‚å‘é‡è¿›è¡Œå‰ªæçš„æ–°æ–¹æ³•ï¼Œå¯¹åœ¨ImageNetä¸Šè®­ç»ƒçš„ViTæ¨¡å‹å‚æ•°å‡å°‘äº†30%~50%ï¼Œè€Œç²¾åº¦æŸå¤±ä¸åˆ°1%ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æä¾›äº†ä¸¥è°¨çš„æ•°å­¦ç†è®ºåŸºç¡€ï¼Œé€šè¿‡ç†è®ºæ¨å¯¼å’Œæ•°å€¼å®éªŒè¯æ˜äº†å¯¹å…¨è¿æ¥ç¥ç»ç½‘ç»œç»“æ„ä¸­çš„éšæœºæ€§å‡å°‘ä¸æŸå¤±é™ä½çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­£åˆ™åŒ–ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºåŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„å‰ªææ–°æ–¹æ³•ç”¨äºé¢„è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ã€‚</li>
<li>æ–¹æ³•åº”ç”¨äºVision Transformersï¼ˆViTsï¼‰åœ¨å›¾åƒåˆ†ç±»é¢†åŸŸè¡¨ç°ä¼˜ç§€ï¼Œå¯å¤§å¹…å‡å°‘æ¨¡å‹å‚æ•°è€Œç²¾åº¦æŸå¤±æå°ã€‚</li>
<li>ç ”ç©¶æä¾›äº†ä¸¥è°¨çš„æ•°å­¦ç†è®ºæ”¯æ’‘ï¼ŒåŒ…æ‹¬è¯æ˜å…¨è¿æ¥ç¥ç»ç½‘ç»œç»“æ„ä¸­çš„éšæœºæ€§å‡å°‘ä¸æŸå¤±é™ä½çš„å…³ç³»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62f9b4e1a581ec35c7b310b4d84b7e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18678ad6b7f893f31ed1928aca7f7c8c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="KeyFace-Expressive-Audio-Driven-Facial-Animation-for-Long-Sequences-via-KeyFrame-Interpolation"><a href="#KeyFace-Expressive-Audio-Driven-Facial-Animation-for-Long-Sequences-via-KeyFrame-Interpolation" class="headerlink" title="KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via   KeyFrame Interpolation"></a>KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via   KeyFrame Interpolation</h2><p><strong>Authors:Antoni Bigata, MichaÅ‚ StypuÅ‚kowski, Rodrigo Mira, Stella Bounareli, Konstantinos Vougioukas, Zoe Landgraf, Nikita Drobyshev, Maciej Zieba, Stavros Petridis, Maja Pantic</strong></p>
<p>Current audio-driven facial animation methods achieve impressive results for short videos but suffer from error accumulation and identity drift when extended to longer durations. Existing methods attempt to mitigate this through external spatial control, increasing long-term consistency but compromising the naturalness of motion. We propose KeyFace, a novel two-stage diffusion-based framework, to address these issues. In the first stage, keyframes are generated at a low frame rate, conditioned on audio input and an identity frame, to capture essential facial expressions and movements over extended periods of time. In the second stage, an interpolation model fills in the gaps between keyframes, ensuring smooth transitions and temporal coherence. To further enhance realism, we incorporate continuous emotion representations and handle a wide range of non-speech vocalizations (NSVs), such as laughter and sighs. We also introduce two new evaluation metrics for assessing lip synchronization and NSV generation. Experimental results show that KeyFace outperforms state-of-the-art methods in generating natural, coherent facial animations over extended durations, successfully encompassing NSVs and continuous emotions. </p>
<blockquote>
<p>å½“å‰åŸºäºéŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»æ–¹æ³•åœ¨çŸ­è§†é¢‘ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœï¼Œä½†å½“æ‰©å±•åˆ°æ›´é•¿æ—¶é—´æ—¶ï¼Œä¼šå‡ºç°è¯¯å·®ç´¯ç§¯å’Œèº«ä»½æ¼‚ç§»çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å¤–éƒ¨ç©ºé—´æ§åˆ¶æ¥å‡è½»è¿™ä¸€é—®é¢˜ï¼Œè™½ç„¶å¢åŠ äº†é•¿æœŸä¸€è‡´æ€§ï¼Œä½†ç‰ºç‰²äº†åŠ¨ä½œçš„è‡ªç„¶æ€§ã€‚æˆ‘ä»¬æå‡ºäº†KeyFaceï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ–°å‹ä¸¤é˜¶æ®µæ‰©æ•£çš„æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä»¥éŸ³é¢‘è¾“å…¥å’Œèº«ä»½å¸§ä¸ºæ¡ä»¶ï¼Œåœ¨ä½å¸§ç‡ä¸‹ç”Ÿæˆå…³é”®å¸§ï¼Œä»¥æ•æ‰é•¿æ—¶é—´æ®µå†…çš„é‡è¦é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ’å€¼æ¨¡å‹å¡«è¡¥äº†å…³é”®å¸§ä¹‹é—´çš„ç©ºç™½ï¼Œç¡®ä¿äº†å¹³æ»‘è¿‡æ¸¡å’Œæ—¶é—´è¿è´¯æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é€¼çœŸåº¦ï¼Œæˆ‘ä»¬èå…¥äº†è¿ç»­çš„æƒ…ç»ªè¡¨ç¤ºï¼Œå¹¶å¤„ç†äº†å„ç§éè¯­éŸ³å£°éŸ³ï¼ˆNSVsï¼‰ï¼Œå¦‚ç¬‘å£°å’Œå¹æ¯å£°ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°å”‡åŒæ­¥å’Œéè¯­éŸ³ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKeyFaceåœ¨ç”Ÿæˆé•¿æ—¶é—´çš„è‡ªç„¶ã€è¿è´¯çš„é¢éƒ¨åŠ¨ç”»æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒæˆåŠŸæ¶µç›–äº†éè¯­éŸ³å£°éŸ³å’Œè¿ç»­æƒ…ç»ªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01715v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºKeyFaceçš„æ–°å‹ä¸¤é˜¶æ®µæ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³éŸ³é¢‘é©±åŠ¨é¢éƒ¨åŠ¨ç”»åœ¨é•¿æ—¶é—´åºåˆ—ä¸­é¢ä¸´çš„è¯¯å·®ç´¯ç§¯å’Œèº«ä»½æ¼‚ç§»é—®é¢˜ã€‚ç¬¬ä¸€é˜¶æ®µæ ¹æ®éŸ³é¢‘è¾“å…¥å’Œèº«ä»½å¸§åœ¨ä½å¸§ç‡ä¸‹ç”Ÿæˆå…³é”®å¸§ï¼Œæ•æ‰é•¿æ—¶é—´çš„é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ’å€¼æ¨¡å‹å¡«å……å…³é”®å¸§ä¹‹é—´çš„é—´éš™ï¼Œç¡®ä¿å¹³æ»‘è¿‡æ¸¡å’Œæ—¶é—´è¿è´¯æ€§ã€‚ä¸ºå¢å¼ºçœŸå®æ„Ÿï¼ŒKeyFaceè¿˜èå…¥äº†è¿ç»­æƒ…æ„Ÿè¡¨ç¤ºï¼Œå¹¶å¤„ç†å„ç§éè¯­éŸ³å‘å£°ï¼ˆNSVsï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKeyFaceåœ¨ç”Ÿæˆè‡ªç„¶ã€è¿è´¯çš„é•¿æ—¶é—´é¢éƒ¨åŠ¨ç”»æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒæˆåŠŸæ¶µç›–NSVså’Œè¿ç»­æƒ…æ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»æ–¹æ³•åœ¨çŸ­æ—¶é—´è§†é¢‘ä¸Šè¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨é•¿æ—¶é—´åºåˆ—ä¸Šé­é‡è¯¯å·®ç´¯ç§¯å’Œèº«ä»½æ¼‚ç§»çš„æŒ‘æˆ˜ã€‚</li>
<li>KeyFaceæ˜¯ä¸€ä¸ªæ–°å‹çš„ä¸¤é˜¶æ®µæ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µæ ¹æ®éŸ³é¢‘è¾“å…¥å’Œèº«ä»½å¸§ç”Ÿæˆä½å¸§ç‡çš„å…³é”®å¸§ï¼Œæ•æ‰é•¿æ—¶é—´å†…çš„é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ’å€¼æ¨¡å‹ç¡®ä¿å…³é”®å¸§ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡å’Œæ—¶é—´è¿è´¯æ€§ã€‚</li>
<li>KeyFaceèå…¥è¿ç»­æƒ…æ„Ÿè¡¨ç¤ºï¼Œå¢å¼ºåŠ¨ç”»çš„çœŸå®æ„Ÿã€‚</li>
<li>KeyFaceèƒ½å¤Ÿå¤„ç†å„ç§éè¯­éŸ³å‘å£°ï¼ˆNSVsï¼‰ï¼Œå¦‚ç¬‘å£°å’Œå¹æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db928d98c51645cb7f366bf1ff08b77b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a1bbd4f60c2cc9c98a32f4b5975fbb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32e0c4ccd04950d52d53fcc672fdf14f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04a645b763b17e3fb20d3c54b18914c5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Social-Media-Rumor-Detection-A-Semantic-and-Graph-Neural-Network-Approach-for-the-2024-Global-Election"><a href="#Enhancing-Social-Media-Rumor-Detection-A-Semantic-and-Graph-Neural-Network-Approach-for-the-2024-Global-Election" class="headerlink" title="Enhancing Social Media Rumor Detection: A Semantic and Graph Neural   Network Approach for the 2024 Global Election"></a>Enhancing Social Media Rumor Detection: A Semantic and Graph Neural   Network Approach for the 2024 Global Election</h2><p><strong>Authors:Liu Yan, Liu Yunpeng, Zhao Liang</strong></p>
<p>The development of social media platforms has revolutionized the speed and manner in which information is disseminated, leading to both beneficial and detrimental effects on society. While these platforms facilitate rapid communication, they also accelerate the spread of rumors and extremist speech, impacting public perception and behavior significantly. This issue is particularly pronounced during election periods, where the influence of social media on election outcomes has become a matter of global concern. With the unprecedented number of elections in 2024, against this backdrop, the election ecosystem has encountered unprecedented challenges. This study addresses the urgent need for effective rumor detection on social media by proposing a novel method that combines semantic analysis with graph neural networks. We have meticulously collected a dataset from PolitiFact and Twitter, focusing on politically relevant rumors. Our approach involves semantic analysis using a fine-tuned BERT model to vectorize text content and construct a directed graph where tweets and comments are nodes, and interactions are edges. The core of our method is a graph neural network, SAGEWithEdgeAttention, which extends the GraphSAGE model by incorporating first-order differences as edge attributes and applying an attention mechanism to enhance feature aggregation. This innovative approach allows for the fine-grained analysis of the complex social network structure, improving rumor detection accuracy. The study concludes that our method significantly outperforms traditional content analysis and time-based models, offering a theoretically sound and practically efficient solution. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“å¹³å°çš„å‘å±•åœ¨ä¿¡æ¯ä¼ æ’­çš„é€Ÿåº¦å’Œæ–¹å¼ä¸Šå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ï¼Œå¯¹ç¤¾ä¼šäº§ç”Ÿäº†æœ‰ç›Šå’Œæœ‰å®³çš„å½±å“ã€‚è¿™äº›å¹³å°è™½ç„¶ä¿ƒè¿›äº†å¿«é€Ÿé€šä¿¡ï¼Œä½†ä¹ŸåŠ é€Ÿäº†è°£è¨€å’Œæç«¯è¨€è®ºçš„ä¼ æ’­ï¼Œå¯¹å…¬ä¼—è®¤çŸ¥å’Œè¡Œä¸ºäº§ç”Ÿäº†é‡å¤§å½±å“ã€‚åœ¨é€‰ä¸¾æœŸé—´ï¼Œè¿™ä¸ªé—®é¢˜å°¤ä¸ºçªå‡ºï¼Œç¤¾äº¤åª’ä½“å¯¹é€‰ä¸¾ç»“æœçš„å½±å“å·²æˆä¸ºå…¨çƒå…³æ³¨çš„é—®é¢˜ã€‚åœ¨å³å°†åˆ°æ¥çš„2024å¹´ä¼—å¤šé€‰ä¸¾çš„èƒŒæ™¯ä¸‹ï¼Œé€‰ä¸¾ç”Ÿæ€ç³»ç»Ÿé¢ä¸´ç€å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸Šè¿«åˆ‡éœ€è¦çš„æœ‰æ•ˆè°£è¨€æ£€æµ‹ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè¯­ä¹‰åˆ†æå’Œå›¾ç¥ç»ç½‘ç»œçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬ä»PolitiFactå’ŒTwitterç²¾å¿ƒæ”¶é›†äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œé‡ç‚¹å…³æ³¨ä¸æ”¿æ²»ç›¸å…³çš„è°£è¨€ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¾®è°ƒåçš„BERTæ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†æï¼Œå°†æ–‡æœ¬å†…å®¹å‘é‡åŒ–ï¼Œå¹¶æ„å»ºä¸€ä¸ªç”±æ¨æ–‡å’Œè¯„è®ºä½œä¸ºèŠ‚ç‚¹ã€äº’åŠ¨ä½œä¸ºè¾¹çš„æœ‰å‘å›¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å›¾ç¥ç»ç½‘ç»œSAGEWithEdgeAttentionï¼Œå®ƒé€šè¿‡å¼•å…¥ä¸€é˜¶å·®åˆ†ä½œä¸ºè¾¹å±æ€§å¹¶åœ¨ç‰¹å¾èšåˆè¿‡ç¨‹ä¸­åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥æ‰©å±•GraphSAGEæ¨¡å‹ã€‚è¿™ç§åˆ›æ–°çš„æ–¹æ³•å…è®¸å¯¹å¤æ‚çš„ç¤¾äº¤ç½‘ç»œç»“æ„è¿›è¡Œç²¾ç»†åˆ†æï¼Œæé«˜äº†è°£è¨€æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºå†…å®¹å’ŒåŸºäºæ—¶é—´æ¨¡å‹ï¼Œæä¾›äº†ä¸€ä¸ªç†è®ºä¸Šæœ‰ä¾æ®ã€å®è·µä¸Šæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01394v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>ç¤¾äº¤åª’ä½“å¹³å°çš„å‘å±•ä¸ºä¿¡æ¯ä¼ æ’­çš„é€Ÿåº¦å’Œæ–¹å¼å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œæ—¢äº§ç”Ÿäº†ç§¯æçš„å½±å“ï¼Œä¹Ÿå¸¦æ¥äº†æ¶ˆæçš„å½±å“ã€‚è¿™äº›å¹³å°ä¿ƒè¿›äº†å¿«é€Ÿæ²Ÿé€šï¼Œä½†ä¹ŸåŠ é€Ÿäº†è°£è¨€å’Œæç«¯è¨€è®ºçš„ä¼ æ’­ï¼Œå¯¹å…¬ä¼—è®¤çŸ¥å’Œè¡Œä¸ºäº§ç”Ÿäº†é‡å¤§å½±å“ã€‚ç‰¹åˆ«æ˜¯åœ¨é€‰ä¸¾æœŸé—´ï¼Œç¤¾äº¤åª’ä½“å¯¹é€‰ä¸¾ç»“æœçš„å½±å“å·²æˆä¸ºå…¨çƒå…³æ³¨çš„é—®é¢˜ã€‚éšç€2024å¹´é€‰ä¸¾æ•°é‡çš„ç©ºå‰å¢åŠ ï¼Œé€‰ä¸¾ç”Ÿæ€ç³»ç»Ÿé¢ä¸´ç€å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è°£è¨€æ£€æµ‹æ³•åº”å¯¹è¿™ä¸€ç´§è¿«éœ€æ±‚ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†è¯­ä¹‰åˆ†æä¸å›¾ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬ä»PolitiFactå’ŒTwitteræ”¶é›†äº†æ”¿æ²»ç›¸å…³çš„è°£è¨€æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¾®è°ƒåçš„BERTæ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†æï¼Œå°†æ–‡æœ¬å†…å®¹å‘é‡åŒ–ï¼Œå¹¶æ„å»ºä¸€ä¸ªä»¥æ¨æ–‡å’Œè¯„è®ºä¸ºèŠ‚ç‚¹ã€äº’åŠ¨ä¸ºè¾¹çš„æœ‰å‘å›¾ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å›¾ç¥ç»ç½‘ç»œSAGEWithEdgeAttentionï¼Œå®ƒæ‰©å±•äº†GraphSAGEæ¨¡å‹ï¼Œå°†ä¸€é˜¶å·®åˆ†ä½œä¸ºè¾¹å±æ€§å¹¶åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ä»¥å¢å¼ºç‰¹å¾èšåˆã€‚è¿™ç§åˆ›æ–°çš„æ–¹æ³•å…è®¸å¯¹å¤æ‚çš„ç¤¾ä¼šç½‘ç»œç»“æ„è¿›è¡Œç²¾ç»†åˆ†æï¼Œæé«˜äº†è°£è¨€æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸºäºå†…å®¹å’ŒåŸºäºæ—¶é—´çš„æ¨¡å‹ï¼Œæä¾›äº†ä¸€ä¸ªç†è®ºæ‰å®ã€å®è·µé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å¹³å°çš„å‘å±•åœ¨ä¿¡æ¯ä¼ æ’­æ–¹é¢äº§ç”Ÿäº†é©å‘½æ€§çš„å½±å“ï¼Œæ—¢æœ‰æ­£é¢ä¹Ÿæœ‰è´Ÿé¢æ•ˆæœã€‚</li>
<li>ç¤¾äº¤åª’ä½“åœ¨é€‰ä¸¾æœŸé—´å¯¹å…¬ä¼—è®¤çŸ¥å’Œè¡Œä¸ºçš„å†²å‡»å°¤ä¸ºæ˜¾è‘—ï¼Œå·²æˆä¸ºå…¨çƒå…³æ³¨çš„è®®é¢˜ã€‚</li>
<li>é¢å¯¹2024å¹´é€‰ä¸¾å¢å¤šçš„èƒŒæ™¯ï¼Œé€‰ä¸¾ç”Ÿæ€ç³»ç»Ÿé¢ä¸´å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè¯­ä¹‰åˆ†æä¸å›¾ç¥ç»ç½‘ç»œçš„å…¨æ–°è°£è¨€æ£€æµ‹æ³•ã€‚</li>
<li>æ•°æ®é›†æ¥è‡ªPolitiFactå’ŒTwitterï¼Œé›†ä¸­äºæ”¿æ²»ç›¸å…³çš„è°£è¨€ã€‚</li>
<li>æ–¹æ³•çš„æ ¸å¿ƒæ˜¯SAGEWithEdgeAttentionå›¾ç¥ç»ç½‘ç»œï¼Œå®ƒæ‰©å±•äº†GraphSAGEæ¨¡å‹å¹¶åº”ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d19b66e043847e45194788068f3b651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb33e342eb5d889f700d0fd1429ca977.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HOP-Heterogeneous-Topology-based-Multimodal-Entanglement-for-Co-Speech-Gesture-Generation"><a href="#HOP-Heterogeneous-Topology-based-Multimodal-Entanglement-for-Co-Speech-Gesture-Generation" class="headerlink" title="HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech   Gesture Generation"></a>HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech   Gesture Generation</h2><p><strong>Authors:Hongye Cheng, Tianyu Wang, Guangsi Shi, Zexing Zhao, Yanwei Fu</strong></p>
<p>Co-speech gestures are crucial non-verbal cues that enhance speech clarity and expressiveness in human communication, which have attracted increasing attention in multimodal research. While the existing methods have made strides in gesture accuracy, challenges remain in generating diverse and coherent gestures, as most approaches assume independence among multimodal inputs and lack explicit modeling of their interactions. In this work, we propose a novel multimodal learning method named HOP for co-speech gesture generation that captures the heterogeneous entanglement between gesture motion, audio rhythm, and text semantics, enabling the generation of coordinated gestures. By leveraging spatiotemporal graph modeling, we achieve the alignment of audio and action. Moreover, to enhance modality coherence, we build the audio-text semantic representation based on a reprogramming module, which is beneficial for cross-modality adaptation. Our approach enables the trimodal system to learn each otherâ€™s features and represent them in the form of topological entanglement. Extensive experiments demonstrate that HOP achieves state-of-the-art performance, offering more natural and expressive co-speech gesture generation. More information, codes, and demos are available here: <a target="_blank" rel="noopener" href="https://star-uu-wang.github.io/HOP/">https://star-uu-wang.github.io/HOP/</a> </p>
<blockquote>
<p>å¯¹è¯ä¸­çš„ååŒè¯­éŸ³æ‰‹åŠ¿æ˜¯éè¯­è¨€çº¿ç´¢ä¸­è‡³å…³é‡è¦çš„éƒ¨åˆ†ï¼Œèƒ½å¢å¼ºäººç±»äº¤æµä¸­çš„è¯­éŸ³æ¸…æ™°åº¦å’Œè¡¨è¾¾åŠ›ï¼Œåœ¨å¤šæ¨¡æ€ç ”ç©¶ä¸­å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•åœ¨æ‰‹åŠ¿å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå¤šæ ·å’Œè¿è´¯çš„æ‰‹åŠ¿æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚å¤§å¤šæ•°æ–¹æ³•å‡è®¾å¤šæ¨¡æ€è¾“å…¥æ˜¯ç‹¬ç«‹çš„ï¼Œç¼ºä¹å¯¹å…¶äº¤äº’çš„æ˜¾å¼å»ºæ¨¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºHOPçš„ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆçš„å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ•æ‰æ‰‹åŠ¿è¿åŠ¨ã€éŸ³é¢‘èŠ‚å¥å’Œæ–‡æœ¬è¯­ä¹‰ä¹‹é—´çš„å¼‚è´¨çº ç¼ ï¼Œä»è€Œå®ç°åè°ƒæ‰‹åŠ¿çš„ç”Ÿæˆã€‚é€šè¿‡åˆ©ç”¨æ—¶ç©ºå›¾å»ºæ¨¡ï¼Œæˆ‘ä»¬å®ç°äº†éŸ³é¢‘å’ŒåŠ¨ä½œçš„å¯¹é½ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºæ¨¡æ€è¿è´¯æ€§ï¼Œæˆ‘ä»¬åŸºäºé‡æ„æ¨¡å—æ„å»ºäº†éŸ³é¢‘-æ–‡æœ¬è¯­ä¹‰è¡¨ç¤ºï¼Œè¿™æœ‰åˆ©äºè·¨æ¨¡æ€é€‚åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ä¸‰æ¨¡æ€ç³»ç»Ÿèƒ½å¤Ÿç›¸äº’å­¦ä¹ ç‰¹å¾å¹¶ä»¥æ‹“æ‰‘çº ç¼ çš„å½¢å¼è¡¨ç¤ºå®ƒä»¬ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHOPè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå®ç°äº†æ›´è‡ªç„¶ã€æ›´å¯Œæœ‰è¡¨ç°åŠ›çš„ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆã€‚æ›´å¤šä¿¡æ¯ã€ä»£ç å’Œæ¼”ç¤ºå¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://star-uu-wang.github.io/HOP/">https://star-uu-wang.github.io/HOP/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01175v1">PDF</a> Accepted by CVPR 2025. See <a target="_blank" rel="noopener" href="https://star-uu-wang.github.io/HOP/">https://star-uu-wang.github.io/HOP/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHOPçš„å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºç”ŸæˆååŒè¯­éŸ³æ‰‹åŠ¿ã€‚è¯¥æ–¹æ³•é€šè¿‡æ—¶ç©ºå›¾å»ºæ¨¡å®ç°éŸ³é¢‘å’ŒåŠ¨ä½œçš„åŒæ­¥ï¼Œå¹¶é€šè¿‡ç¼–ç¨‹æ¨¡å—æ„å»ºéŸ³é¢‘æ–‡æœ¬è¯­ä¹‰è¡¨ç¤ºï¼Œå¢å¼ºæ¨¡æ€ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”è¿è´¯çš„æ‰‹åŠ¿ï¼Œå®ç°è¯­éŸ³ã€æ‰‹åŠ¿å’Œæ–‡æœ¬ä¹‹é—´çš„åè°ƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ååŒè¯­éŸ³æ‰‹åŠ¿æ˜¯äººç±»æ²Ÿé€šä¸­é‡è¦çš„éè¯­è¨€çº¿ç´¢ï¼Œèƒ½æé«˜è¯­éŸ³çš„æ¸…æ™°åº¦å’Œè¡¨è¾¾åŠ›ã€‚</li>
<li>å½“å‰çš„æ‰‹åŠ¿ç”Ÿæˆæ–¹æ³•ä¸»è¦é¢ä¸´ç”Ÿæˆå¤šæ ·æ€§å’Œè¿è´¯æ€§æŒ‘æˆ˜ï¼Œä¸”å¤§å¤šå‡è®¾å¤šæ¨¡æ€è¾“å…¥ç‹¬ç«‹ï¼Œç¼ºä¹å¯¹å…¶äº¤äº’çš„æ˜¾å¼å»ºæ¨¡ã€‚</li>
<li>HOPæ–¹æ³•é€šè¿‡æ—¶ç©ºå›¾å»ºæ¨¡å®ç°éŸ³é¢‘å’ŒåŠ¨ä½œçš„åŒæ­¥ï¼Œæé«˜æ‰‹åŠ¿çš„åè°ƒæ€§ã€‚</li>
<li>HOPæ–¹æ³•åˆ©ç”¨ç¼–ç¨‹æ¨¡å—æ„å»ºéŸ³é¢‘æ–‡æœ¬è¯­ä¹‰è¡¨ç¤ºï¼Œå¢å¼ºæ¨¡æ€ä¸€è‡´æ€§ï¼Œæœ‰åˆ©äºè·¨æ¨¡æ€é€‚åº”ã€‚</li>
<li>HOPæ–¹æ³•å®ç°äº†è¯­éŸ³ã€æ‰‹åŠ¿å’Œæ–‡æœ¬ä¹‹é—´çš„åè°ƒï¼Œç”Ÿæˆæ›´è‡ªç„¶å’Œè¡¨è¾¾æ€§æ›´å¼ºçš„ååŒè¯­éŸ³æ‰‹åŠ¿ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒHOPæ–¹æ³•åœ¨ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d785d07f5eb2a80bb18ed513ae53d08c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec1bec98a4858df2676aa5a483bb21dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-779e1871e48e64651f7de76bf9495f8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5dec7254e78ea28baf07ee9a1496f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cab55f1cda4c882919bc0fc3de3d292.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unveiling-Biases-while-Embracing-Sustainability-Assessing-the-Dual-Challenges-of-Automatic-Speech-Recognition-Systems"><a href="#Unveiling-Biases-while-Embracing-Sustainability-Assessing-the-Dual-Challenges-of-Automatic-Speech-Recognition-Systems" class="headerlink" title="Unveiling Biases while Embracing Sustainability: Assessing the Dual   Challenges of Automatic Speech Recognition Systems"></a>Unveiling Biases while Embracing Sustainability: Assessing the Dual   Challenges of Automatic Speech Recognition Systems</h2><p><strong>Authors:Ajinkya Kulkarni, Atharva Kulkarni, Miguel Couceiro, Isabel Trancoso</strong></p>
<p>In this paper, we present a bias and sustainability focused investigation of Automatic Speech Recognition (ASR) systems, namely Whisper and Massively Multilingual Speech (MMS), which have achieved state-of-the-art (SOTA) performances. Despite their improved performance in controlled settings, there remains a critical gap in understanding their efficacy and equity in real-world scenarios. We analyze ASR biases w.r.t. gender, accent, and age group, as well as their effect on downstream tasks. In addition, we examine the environmental impact of ASR systems, scrutinizing the use of large acoustic models on carbon emission and energy consumption. We also provide insights into our empirical analyses, offering a valuable contribution to the claims surrounding bias and sustainability in ASR systems. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹å…³æ³¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„åè§å’Œå¯æŒç»­æ€§è°ƒæŸ¥ï¼Œç‰¹åˆ«æ˜¯å·²ç»è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰çš„Whisperå’Œå¤§è§„æ¨¡å¤šè¯­ç§è¯­éŸ³ï¼ˆMMSï¼‰ã€‚å°½ç®¡å®ƒä»¬åœ¨å—æ§ç¯å¢ƒä¸­çš„æ€§èƒ½å¾—åˆ°äº†æé«˜ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­çš„æ•ˆç‡å’Œå…¬å¹³ä»å­˜åœ¨å…³é”®å·®è·ã€‚æˆ‘ä»¬åˆ†æäº†ASRä¸æ€§åˆ«ã€å£éŸ³å’Œå¹´é¾„ç»„ä¹‹é—´çš„åè§åŠå…¶å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è€ƒå¯Ÿäº†ASRç³»ç»Ÿçš„ç¯å¢ƒå½±å“ï¼Œè¯¦ç»†åˆ†æäº†å¤§å‹å£°å­¦æ¨¡å‹å¯¹ç¢³æ’æ”¾å’Œèƒ½æºæ¶ˆè€—çš„å½±å“ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å®è¯åˆ†æçš„è§è§£ï¼Œä¸ºASRç³»ç»Ÿä¸­æœ‰å…³åè§å’Œå¯æŒç»­æ€§çš„ä¸»å¼ åšå‡ºäº†å®è´µè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00907v1">PDF</a> Interspeech 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„åè§å’Œå¯æŒç»­æ€§ï¼Œç‰¹åˆ«æ˜¯è¡¨ç°æœ€ä½³çš„Whisperå’ŒMassively Multilingual Speechï¼ˆMMSï¼‰ã€‚æ–‡ç« æ¢è®¨äº†ASRç³»ç»Ÿåœ¨å®é™…åœºæ™¯ä¸­çš„æ•ˆç‡å’Œå…¬å¹³æ€§é—®é¢˜ï¼Œé‡ç‚¹åˆ†æäº†å…¶åœ¨æ€§åˆ«ã€å£éŸ³å’Œå¹´é¾„ç»„æ–¹é¢çš„åè§åŠå…¶å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ASRç³»ç»Ÿçš„ç¯å¢ƒå½±å“ï¼Œç‰¹åˆ«æ˜¯å¤§å‹å£°å­¦æ¨¡å‹å¯¹ç¢³æ’æ”¾å’Œèƒ½æºæ¶ˆè€—çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå­˜åœ¨åè§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ€§åˆ«ã€å£éŸ³å’Œå¹´é¾„ç»„æ–¹é¢ã€‚</li>
<li>ASRç³»ç»Ÿçš„æ€§èƒ½åœ¨å®é™…åœºæ™¯ä¸­å¯èƒ½å­˜åœ¨å…¬å¹³æ€§é—®é¢˜ã€‚</li>
<li>ASRç³»ç»Ÿçš„åè§å¯¹ä¸‹æ¸¸ä»»åŠ¡äº§ç”Ÿå½±å“ã€‚</li>
<li>å¤§å‹å£°å­¦æ¨¡å‹åœ¨ç¢³æ’æ”¾å’Œèƒ½æºæ¶ˆè€—æ–¹é¢å­˜åœ¨ç¯å¢ƒå½±å“ã€‚</li>
<li>ASRç³»ç»Ÿçš„å¯æŒç»­æ€§æ˜¯ç ”ç©¶çš„é‡ç‚¹ä¹‹ä¸€ã€‚</li>
<li>æ–‡ç« æä¾›äº†å…³äºASRç³»ç»Ÿä¸­åè§å’Œå¯æŒç»­æ€§çš„å®è¯åˆ†æå’Œè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5b2eef227e70cddc6c9449df0a73b93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1baa52b14457ec933fb7bd0abcd4c661.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c53c9552fbf49e4a7f960da6d9b4e28b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8188a7f31ab2e2e794962bccdabf806c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5a246efb62027921dbe06ceb6c8a0d4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UniWav-Towards-Unified-Pre-training-for-Speech-Representation-Learning-and-Generation"><a href="#UniWav-Towards-Unified-Pre-training-for-Speech-Representation-Learning-and-Generation" class="headerlink" title="UniWav: Towards Unified Pre-training for Speech Representation Learning   and Generation"></a>UniWav: Towards Unified Pre-training for Speech Representation Learning   and Generation</h2><p><strong>Authors:Alexander H. Liu, Sang-gil Lee, Chao-Han Huck Yang, Yuan Gong, Yu-Chiang Frank Wang, James R. Glass, Rafael Valle, Bryan Catanzaro</strong></p>
<p>Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training. </p>
<blockquote>
<p>é¢„è®­ç»ƒå’Œè¡¨ç¤ºå­¦ä¹ åœ¨ç°ä»£è¯­éŸ³è¯†åˆ«ä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œä¸åŒçš„åº”ç”¨ä¾èµ–äºä¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œå› ä¸ºä¸»æµçš„é¢„è®­ç»ƒæŠ€æœ¯è¦ä¹ˆæ˜¯ä¸ºåˆ¤åˆ«ä»»åŠ¡è®¾è®¡çš„ï¼Œè¦ä¹ˆæ˜¯ä¸ºç”Ÿæˆä»»åŠ¡è®¾è®¡çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å°è¯•ä¸ºè¿™ä¸¤ç§ç±»å‹çš„ä»»åŠ¡æ„å»ºç»Ÿä¸€çš„é¢„è®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡é€‚å½“çš„é¢„è®­ç»ƒè®¾è®¡é€‰æ‹©ï¼Œå¯ä»¥è”åˆå­¦ä¹ ä¸€ä¸ªè¡¨ç¤ºç¼–ç å™¨å’Œä¸€ä¸ªç”ŸæˆéŸ³é¢‘è§£ç å™¨ï¼Œè¿™ä¸¤ç§ç±»å‹çš„åº”ç”¨éƒ½å¯ä»¥ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†UniWavï¼Œè¿™æ˜¯ä¸€ç§ä¸ºç»Ÿä¸€é¢„è®­ç»ƒè¡¨ç¤ºå­¦ä¹ å’Œç”Ÿæˆä»»åŠ¡è€Œè®¾è®¡çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ã€‚åœ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ°è¯­éŸ³å’Œè¯­éŸ³æ ‡è®°åŒ–æ–¹é¢ï¼ŒUniWavå®ç°äº†ä¸é’ˆå¯¹ä¸åŒä»»åŠ¡è®­ç»ƒçš„ç°æœ‰åŸºç¡€æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªé€šç”¨çš„å•ä¸€åŸºç¡€æ¨¡å‹æ¥è¿›è¡Œè¯­éŸ³è®­ç»ƒï¼Œä»¥æ›¿ä»£ä¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œä»è€Œé™ä½é¢„è®­ç»ƒçš„å¼€é”€å’Œæˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00733v1">PDF</a> ICLR 2025; demo page at   <a target="_blank" rel="noopener" href="https://alexander-h-liu.github.io/uniwav-demo.github.io/">https://alexander-h-liu.github.io/uniwav-demo.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è¯­éŸ³å¤„ç†ä¸­é¢„è®­ç»ƒå’Œè¡¨ç¤ºå­¦ä¹ çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºç›®å‰ä¸åŒçš„åº”ç”¨ä¾èµ–äºä¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œä¸»æµçš„é¢„è®­ç»ƒæŠ€æœ¯ä¸»è¦æ˜¯ä¸ºåˆ¤åˆ«ä»»åŠ¡æˆ–ç”Ÿæˆä»»åŠ¡è®¾è®¡çš„ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å°è¯•ä¸ºè¿™ä¸¤ç§ä»»åŠ¡æ„å»ºç»Ÿä¸€çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå¹¶è¯æ˜é€šè¿‡é€‚å½“çš„é¢„è®­ç»ƒè®¾è®¡é€‰æ‹©ï¼Œå¯ä»¥è”åˆå­¦ä¹ é€‚ç”¨äºè¿™ä¸¤ç§ä»»åŠ¡çš„è¡¨ç¤ºç¼–ç å™¨å’Œç”ŸæˆéŸ³é¢‘è§£ç å™¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†UniWavè¿™ä¸€ç¼–ç è§£ç æ¡†æ¶ï¼Œæ—¨åœ¨ç»Ÿä¸€é¢„è®­ç»ƒè¡¨ç¤ºå­¦ä¹ å’Œç”Ÿæˆä»»åŠ¡ã€‚åœ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ°è¯­éŸ³å’Œè¯­éŸ³æ ‡è®°åŒ–æ–¹é¢ï¼ŒUniWavçš„æ€§èƒ½ä¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„ç°æœ‰åŸºç¡€æ¨¡å‹ç›¸å½“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯ä»¥æ„å»ºå•ä¸€é€šç”¨çš„è¯­éŸ³åŸºç¡€æ¨¡å‹æ¥æ›¿ä»£ä¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œä»è€Œé™ä½é¢„è®­ç»ƒçš„å¼€é”€å’Œæˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒå’Œè¡¨ç¤ºå­¦ä¹ åœ¨ç°ä»£è¯­éŸ³å¤„ç†ä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚</li>
<li>ç›®å‰ä¸åŒçš„è¯­éŸ³åº”ç”¨ä¾èµ–äºä¸åŒçš„åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸»æµé¢„è®­ç»ƒæŠ€æœ¯ä¸»è¦é¢å‘åˆ¤åˆ«ä»»åŠ¡æˆ–ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡å°è¯•ä¸ºåˆ¤åˆ«ä»»åŠ¡å’Œç”Ÿæˆä»»åŠ¡æ„å»ºç»Ÿä¸€çš„é¢„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>é€šè¿‡é€‚å½“çš„é¢„è®­ç»ƒè®¾è®¡é€‰æ‹©ï¼Œå¯ä»¥è”åˆå­¦ä¹ è¡¨ç¤ºç¼–ç å™¨å’Œç”ŸæˆéŸ³é¢‘è§£ç å™¨ï¼Œé€‚ç”¨äºä¸¤ç§ä»»åŠ¡ã€‚</li>
<li>æå‡ºçš„UniWavæ¡†æ¶æ—¨åœ¨ç»Ÿä¸€é¢„è®­ç»ƒè¡¨ç¤ºå­¦ä¹ å’Œç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>UniWavåœ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ°è¯­éŸ³å’Œè¯­éŸ³æ ‡è®°åŒ–æ–¹é¢çš„æ€§èƒ½ä¸ç‰¹å®šä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-71275a33dc4cac9869c77b183c6d25de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca51e17f246f6e061f10adb66c1b6c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd720ae79d64f2b7cb1679a79d65606e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Implementing-Spiking-World-Model-with-Multi-Compartment-Neurons-for-Model-based-Reinforcement-Learning"><a href="#Implementing-Spiking-World-Model-with-Multi-Compartment-Neurons-for-Model-based-Reinforcement-Learning" class="headerlink" title="Implementing Spiking World Model with Multi-Compartment Neurons for   Model-based Reinforcement Learning"></a>Implementing Spiking World Model with Multi-Compartment Neurons for   Model-based Reinforcement Learning</h2><p><strong>Authors:Yinqian Sun, Feifei Zhao, Mingyang Lv, Yi Zeng</strong></p>
<p>Brain-inspired spiking neural networks (SNNs) have garnered significant research attention in algorithm design and perception applications. However, their potential in the decision-making domain, particularly in model-based reinforcement learning, remains underexplored. The difficulty lies in the need for spiking neurons with long-term temporal memory capabilities, as well as network optimization that can integrate and learn information for accurate predictions. The dynamic dendritic information integration mechanism of biological neurons brings us valuable insights for addressing these challenges. In this study, we propose a multi-compartment neuron model capable of nonlinearly integrating information from multiple dendritic sources to dynamically process long sequential inputs. Based on this model, we construct a Spiking World Model (Spiking-WM), to enable model-based deep reinforcement learning (DRL) with SNNs. We evaluated our model using the DeepMind Control Suite, demonstrating that Spiking-WM outperforms existing SNN-based models and achieves performance comparable to artificial neural network (ANN)-based world models employing Gated Recurrent Units (GRUs). Furthermore, we assess the long-term memory capabilities of the proposed model in speech datasets, including SHD, TIMIT, and LibriSpeech 100h, showing that our multi-compartment neuron model surpasses other SNN-based architectures in processing long sequences. Our findings underscore the critical role of dendritic information integration in shaping neuronal function, emphasizing the importance of cooperative dendritic processing in enhancing neural computation. </p>
<blockquote>
<p>å—å¤§è„‘å¯å‘çš„è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰åœ¨ç®—æ³•è®¾è®¡å’Œæ„ŸçŸ¥åº”ç”¨æ–¹é¢å¼•èµ·äº†ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å†³ç­–é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ä¸­çš„æ½œåŠ›ï¼Œå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚éš¾ç‚¹åœ¨äºéœ€è¦å…·æœ‰é•¿æœŸæ—¶é—´è®°å¿†èƒ½åŠ›çš„è„‰å†²ç¥ç»å…ƒï¼Œä»¥åŠèƒ½å¤Ÿæ•´åˆå’Œå­¦ä¹ ä¿¡æ¯è¿›è¡Œå‡†ç¡®é¢„æµ‹çš„ç½‘ç»œä¼˜åŒ–ã€‚ç”Ÿç‰©ç¥ç»å…ƒçš„åŠ¨æ€æ ‘çªä¿¡æ¯æ•´åˆæœºåˆ¶ä¸ºæˆ‘ä»¬è§£å†³è¿™äº›æŒ‘æˆ˜æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00713v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŸºäºè„‘å¯å‘è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„å†³ç­–åˆ¶å®šé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹åŸºç¡€ä¸Šçš„å¼ºåŒ–å­¦ä¹ ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šå®¤ç¥ç»å…ƒæ¨¡å‹ï¼Œèƒ½å¤ŸåŠ¨æ€å¤„ç†é•¿åºåˆ—è¾“å…¥ï¼Œå¹¶åŸºäºæ­¤æ„å»ºäº†Spiking World Modelï¼ˆSpiking-WMï¼‰ã€‚åœ¨DeepMind Control Suiteä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒSpiking-WMçš„è¡¨ç°åœ¨ç°æœ‰SNNæ¨¡å‹ä¹‹ä¸Šï¼Œä¸”ä¸é‡‡ç”¨GRUçš„åŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„ä¸–ç•Œæ¨¡å‹è¡¨ç°ç›¸å½“ã€‚æ­¤å¤–ï¼Œåœ¨è¯­éŸ³æ•°æ®é›†ä¸Šçš„é•¿æœŸè®°å¿†èƒ½åŠ›è¯„ä¼°ä¹Ÿè¯æ˜äº†è¯¥æ¨¡å‹çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰åœ¨å†³ç­–åˆ¶å®šé¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>å¤šå®¤ç¥ç»å…ƒæ¨¡å‹èƒ½å¤Ÿéçº¿æ€§åœ°æ•´åˆæ¥è‡ªå¤šä¸ªæ ‘çªæ¥æºçš„ä¿¡æ¯ï¼Œä»¥åŠ¨æ€å¤„ç†é•¿åºåˆ—è¾“å…¥ã€‚</li>
<li>Spiking World Modelï¼ˆSpiking-WMï¼‰ç»“åˆäº†å¤šå®¤ç¥ç»å…ƒæ¨¡å‹ï¼Œå®ç°äº†åŸºäºæ¨¡å‹çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ã€‚</li>
<li>Spiking-WMåœ¨DeepMind Control Suiteä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰SNNæ¨¡å‹ï¼Œå¹¶ä¸åŸºäºGRUçš„ANNæ¨¡å‹è¡¨ç°ç›¸å½“ã€‚</li>
<li>åœ¨è¯­éŸ³æ•°æ®é›†ä¸Šï¼Œå¤šå®¤ç¥ç»å…ƒæ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ–¹é¢è¶…è¶Šäº†å…¶ä»–SNNæ¶æ„ã€‚</li>
<li>ç¥ç»å…ƒä¸­çš„æ ‘çªä¿¡æ¯æ•´åˆå¯¹ç¥ç»å…ƒåŠŸèƒ½è‡³å…³é‡è¦ï¼Œåˆä½œæ ‘çªå¤„ç†æœ‰åŠ©äºå¢å¼ºç¥ç»è®¡ç®—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-54ea85fcc8233468e6c5d25c8d8029f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-688262681e4e444f7f338c32aedf3571.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8dadc00b8b1cf530d0853ce92d40ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-736ce8785edefc1f9704c8dc47b65755.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement"><a href="#LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement" class="headerlink" title="LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement"></a>LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement</h2><p><strong>Authors:Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie</strong></p>
<p>Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area. </p>
<blockquote>
<p>æœ€è¿‘çš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰è¿›å±•è¡¨æ˜ï¼Œå®ƒä»¬åœ¨è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶åœ¨ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰é¢†åŸŸè“¬å‹ƒå‘å±•ã€‚ç„¶è€Œï¼Œè®¸å¤šåŸºäºLMçš„SEæ–¹æ³•ä¸»è¦å…³æ³¨è¯­ä¹‰ä¿¡æ¯ï¼Œå¾€å¾€å¿½è§†äº†å£°éŸ³ä¿¡æ¯çš„å…³é”®ä½œç”¨ï¼Œè¿™å¯¼è‡´å¢å¼ºåçš„è¯­éŸ³å‡ºç°å£°éŸ³ä¸ä¸€è‡´ï¼Œä»¥åŠåœ¨å¤šç§SEä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LLaSE-G1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLaMAçš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ¿€åŠ±å…¶åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚LLaSE-G1æœ‰ä»¥ä¸‹ä¸»è¦è´¡çŒ®ï¼šé¦–å…ˆï¼Œä¸ºäº†ç¼“è§£å£°éŸ³ä¸ä¸€è‡´çš„é—®é¢˜ï¼ŒLLaSE-G1é‡‡ç”¨WavLMçš„è¿ç»­è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹X-Codec2çš„è¯­éŸ³ä»¤ç‰Œï¼Œæœ€å¤§é™åº¦åœ°ä¿ç•™å£°éŸ³ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æå‡æ³›åŒ–èƒ½åŠ›ï¼ŒLLaSE-G1å¼•å…¥äº†åŒé€šé“è¾“å…¥å’Œè¾“å‡ºï¼Œç»Ÿä¸€äº†å¤šä¸ªSEä»»åŠ¡ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡æ ‡è¯†ã€‚æœ€åï¼ŒLLaSE-G1è¶…è¶Šäº†å…ˆå‰çš„ç‰¹å®šä»»åŠ¡çš„åˆ¤åˆ«æ€§å’Œç”Ÿæˆæ€§SEæ¨¡å‹ï¼Œæ˜¾ç¤ºäº†æµ‹è¯•æ—¶çš„æ‰©å±•æ•ˆæœä»¥åŠåœ¨æœªè§è¿‡çš„SEä»»åŠ¡ä¸­çš„æ–°å…´èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ç›¸å…³ä»£ç å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00493v2">PDF</a> 13 pages, 2 figures, 8 tables</p>
<p><strong>æ‘˜è¦</strong><br>LMè¿‘æœŸçš„è¿›å±•ä¸ºè¯­éŸ³å¢å¼ºå¸¦æ¥äº†å¼ºå¤§çš„è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè®¸å¤šåŸºäºLMçš„SEæ–¹æ³•ä¸»è¦å…³æ³¨è¯­ä¹‰ä¿¡æ¯ï¼Œå¿½ç•¥äº†å£°å­¦ä¿¡æ¯çš„é‡è¦æ€§ã€‚æœ¬æ–‡ä»‹ç»LLaSE-G1æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜å¹¶æå‡è¯­éŸ³å¢å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚LLaSE-G1ä½¿ç”¨WavLMçš„è¿ç»­è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹X-Codec2çš„è¯­éŸ³ä»¤ç‰Œï¼Œæœ€å¤§åŒ–å£°å­¦ä¿ç•™ï¼Œç¼“è§£å£°å­¦ä¸ä¸€è‡´é—®é¢˜ã€‚é€šè¿‡å¼•å…¥åŒé€šé“è¾“å…¥è¾“å‡ºä¿ƒè¿›æ³›åŒ–èƒ½åŠ›ï¼Œç»Ÿä¸€å¤šç§SEä»»åŠ¡æ— éœ€ç‰¹å®šä»»åŠ¡æ ‡è¯†ã€‚ç›¸è¾ƒäºä¹‹å‰çš„ä»»åŠ¡ç‰¹å®šåˆ¤åˆ«å’Œç”Ÿæˆå¼SEæ¨¡å‹ï¼ŒLLaSE-G1å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ”¯æŒæœªè§è¿‡çš„SEä»»åŠ¡ã€‚æˆ‘ä»¬å…¬å¼€äº†ä»£ç å’Œæ¨¡å‹ä»¥æ”¯æŒè¯¥é¢†åŸŸè¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLaSE-G1æ¨¡å‹è§£å†³äº†å£°å­¦ä¿¡æ¯åœ¨åŸºäºLMçš„è¯­éŸ³å¢å¼ºä¸­è¢«å¿½è§†çš„é—®é¢˜ï¼Œä»è€Œå®ç°äº†å£°å­¦ä¸€è‡´æ€§ã€‚</li>
<li>LLaSE-G1é‡‡ç”¨WavLMçš„è¿ç»­è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹è¯­éŸ³ä»¤ç‰Œä»¥å¢å¼ºå£°å­¦ä¿ç•™æ•ˆæœã€‚</li>
<li>é€šè¿‡å¼•å…¥åŒé€šé“è¾“å…¥è¾“å‡ºè®¾è®¡ï¼ŒLLaSE-G1æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿç»Ÿä¸€å¤„ç†å¤šç§è¯­éŸ³å¢å¼ºä»»åŠ¡ã€‚</li>
<li>LLaSE-G1ç›¸è¾ƒäºä»»åŠ¡ç‰¹å®šçš„åˆ¤åˆ«å’Œç”Ÿæˆå¼æ¨¡å‹å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>LLaSE-G1å…·å¤‡å‡ºè‰²çš„æµ‹è¯•æ—¶æ‰©å±•æ€§ï¼Œå¹¶å±•ç°å‡ºå¤„ç†æœªè§è¿‡çš„è¯­éŸ³å¢å¼ºä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>LLaSE-G1ä»£ç å’Œæ¨¡å‹çš„å…¬å¼€å°†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-597ec488cb09e0419c2195533aa95f71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29eed87963684c5b0b13172c721b9f9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d15405e11d8e11785f5f9450f35951de.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PodAgent-A-Comprehensive-Framework-for-Podcast-Generation"><a href="#PodAgent-A-Comprehensive-Framework-for-Podcast-Generation" class="headerlink" title="PodAgent: A Comprehensive Framework for Podcast Generation"></a>PodAgent: A Comprehensive Framework for Podcast Generation</h2><p><strong>Authors:Yujia Xiao, Lei He, Haohan Guo, Fenglong Xie, Tan Lee</strong></p>
<p>Existing Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. PodAgent 1) generates informative topic-discussion content by designing a Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis method to generate expressive conversational speech. Given the absence of standardized evaluation criteria for podcast-like audio generation, we developed comprehensive assessment guidelines to effectively evaluate the modelâ€™s performance. Experimental results demonstrate PodAgentâ€™s effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion dialogue content, achieving an 87.4% voice-matching accuracy, and producing more expressive speech through LLM-guided synthesis. Demo page: <a target="_blank" rel="noopener" href="https://podcast-agent.github.io/demo/">https://podcast-agent.github.io/demo/</a>. Source code: <a target="_blank" rel="noopener" href="https://github.com/yujxx/PodAgent">https://github.com/yujxx/PodAgent</a>. </p>
<blockquote>
<p>å½“å‰å­˜åœ¨çš„è‡ªåŠ¨éŸ³é¢‘ç”Ÿæˆæ–¹æ³•åœ¨ç”Ÿæˆç±»ä¼¼æ’­å®¢çš„èŠ‚ç›®æ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ä¸»è¦å›°éš¾åœ¨äºæ·±åº¦å†…å®¹ç”Ÿæˆå’Œåˆé€‚ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„å£°éŸ³ç”Ÿæˆã€‚æœ¬æ–‡æå‡ºäº†PodAgentï¼Œä¸€ä¸ªç”¨äºåˆ›å»ºéŸ³é¢‘èŠ‚ç›®çš„ç»¼åˆæ¡†æ¶ã€‚PodAgent 1ï¼‰é€šè¿‡è®¾è®¡ä¸»æŒäºº-å˜‰å®¾-ç¼–å‰§å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„ä¸»é¢˜è®¨è®ºå†…å®¹ï¼Œ2ï¼‰æ„å»ºå£°éŸ³åº“ï¼Œè¿›è¡Œåˆé€‚çš„è¯­éŸ³è§’è‰²åŒ¹é…ï¼Œä»¥åŠ3ï¼‰åˆ©ç”¨å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯­éŸ³åˆæˆæ–¹æ³•ç”Ÿæˆå¯Œæœ‰è¡¨ç°åŠ›çš„å¯¹è¯è¯­éŸ³ã€‚ç”±äºç¼ºä¹é’ˆå¯¹ç±»ä¼¼æ’­å®¢éŸ³é¢‘ç”Ÿæˆçš„æ ‡å‡†åŒ–è¯„ä¼°æ ‡å‡†ï¼Œæˆ‘ä»¬åˆ¶å®šäº†å…¨é¢çš„è¯„ä¼°å‡†åˆ™ä»¥æœ‰æ•ˆè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜PodAgentçš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨ä¸»é¢˜è®¨è®ºå¯¹è¯å†…å®¹æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç›´æ¥çš„GPT-4ç”Ÿæˆï¼Œè¾¾åˆ°87.4%çš„è¯­éŸ³åŒ¹é…å‡†ç¡®ç‡ï¼Œå¹¶é€šè¿‡LLMå¼•å¯¼çš„åˆæˆäº§ç”Ÿäº†æ›´å…·è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚æ¼”ç¤ºé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://podcast-agent.github.io/demo/%E3%80%82%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%9Ahttps://github.com/yujxx/PodAgent%E3%80%82">https://podcast-agent.github.io/demo/ã€‚æºä»£ç ï¼šhttps://github.com/yujxx/PodAgentã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00455v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PodAgentæ¡†æ¶ï¼Œä¸€ä¸ªç”¨äºç”ŸæˆéŸ³é¢‘èŠ‚ç›®çš„ç»¼åˆç³»ç»Ÿã€‚å®ƒé€šè¿‡è®¾è®¡ä¸»æŒäºº-å˜‰å®¾-ä½œè€…å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿç”Ÿæˆå†…å®¹ï¼Œå»ºç«‹è¯­éŸ³åº“è¿›è¡Œåˆé€‚çš„è¯­éŸ³è§’è‰²åŒ¹é…ï¼Œå¹¶åˆ©ç”¨LLMå¢å¼ºçš„è¯­éŸ³åˆæˆæ–¹æ³•ç”Ÿæˆè¡¨è¾¾æ€§å¯¹è¯è¯­éŸ³ã€‚å°½ç®¡ç¼ºä¹é’ˆå¯¹ç±»ä¼¼æ’­å®¢éŸ³é¢‘ç”Ÿæˆçš„æ ‡å‡†åŒ–è¯„ä¼°æ ‡å‡†ï¼Œä½†å®éªŒç»“æœè¡¨æ˜PodAgentçš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨ä¸»é¢˜è®¨è®ºå¯¹è¯å†…å®¹æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†GPT-4ç›´æ¥ç”Ÿæˆï¼Œå®ç°äº†87.4%çš„è¯­éŸ³åŒ¹é…å‡†ç¡®ç‡ï¼Œå¹¶é€šè¿‡LLMå¼•å¯¼çš„åˆæˆäº§ç”Ÿäº†æ›´å…·è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PodAgentæ˜¯ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œç”¨äºç”ŸæˆéŸ³é¢‘èŠ‚ç›®ï¼Œæ¶µç›–äº†å†…å®¹ç”Ÿæˆã€è¯­éŸ³åŒ¹é…å’Œè¯­éŸ³åˆæˆç­‰æ–¹é¢ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼ˆä¸»æŒäºº-å˜‰å®¾-ä½œè€…ï¼‰æ¥ç”Ÿæˆå†…å®¹ä¸°å¯Œçš„ä¸»é¢˜è®¨è®ºã€‚</li>
<li>PodAgentå»ºç«‹äº†è¯­éŸ³åº“ï¼Œç”¨äºåˆé€‚çš„è¯­éŸ³è§’è‰²åŒ¹é…ï¼Œä½¿ç”Ÿæˆçš„éŸ³é¢‘èŠ‚ç›®æ›´å…·çœŸå®æ„Ÿã€‚</li>
<li>åˆ©ç”¨LLMå¢å¼ºçš„è¯­éŸ³åˆæˆæ–¹æ³•ï¼ŒPodAgentèƒ½å¤Ÿç”Ÿæˆè¡¨è¾¾æ€§å¯¹è¯è¯­éŸ³ï¼Œæé«˜éŸ³é¢‘èŠ‚ç›®çš„è´¨é‡ã€‚</li>
<li>PodAgentåœ¨ä¸»é¢˜è®¨è®ºå¯¹è¯å†…å®¹æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†GPT-4ç›´æ¥ç”Ÿæˆã€‚</li>
<li>PodAgentå®ç°äº†87.4%çš„è¯­éŸ³åŒ¹é…å‡†ç¡®ç‡ï¼Œè¡¨æ˜å…¶åœ¨è¯­éŸ³åŒ¹é…æ–¹é¢çš„å‡†ç¡®æ€§è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ec371ce006ed7e4cb9ca0809f6b90ff2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f9633258e91f64ded5f1596cac8397a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ffdbabe7100feebad5329368b357256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfcd7f18bedc75a726f9b3ee5e658fd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac6a178831a73f75aac34e1face6e6ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2251d80c758290100dfc75e84784383.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="UL-UNAS-Ultra-Lightweight-U-Nets-for-Real-Time-Speech-Enhancement-via-Network-Architecture-Search"><a href="#UL-UNAS-Ultra-Lightweight-U-Nets-for-Real-Time-Speech-Enhancement-via-Network-Architecture-Search" class="headerlink" title="UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via   Network Architecture Search"></a>UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via   Network Architecture Search</h2><p><strong>Authors:Xiaobin Rong, Dahan Wang, Yuxiang Hu, Changbao Zhu, Kai Chen, Jing Lu</strong></p>
<p>Lightweight models are essential for real-time speech enhancement applications. In recent years, there has been a growing trend toward developing increasingly compact models for speech enhancement. In this paper, we propose an Ultra-Lightweight U-net optimized by Network Architecture Search (UL-UNAS), which is suitable for implementation in low-footprint devices. Firstly, we explore the application of various efficient convolutional blocks within the U-Net framework to identify the most promising candidates. Secondly, we introduce two boosting components to enhance the capacity of these convolutional blocks: a novel activation function named affine PReLU and a causal time-frequency attention module. Furthermore, we leverage neural architecture search to discover an optimal architecture within our carefully designed search space. By integrating the above strategies, UL-UNAS not only significantly outperforms the latest ultra-lightweight models with the same or lower computational complexity, but also delivers competitive performance compared to recent baseline models that require substantially higher computational resources. </p>
<blockquote>
<p>è½»é‡çº§æ¨¡å‹å¯¹äºå®æ—¶è¯­éŸ³å¢å¼ºåº”ç”¨è‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼Œå¼€å‘ç”¨äºè¯­éŸ³å¢å¼ºçš„è¶Šæ¥è¶Šç´§å‡‘çš„æ¨¡å‹æˆä¸ºäº†ä¸€ä¸ªå¢é•¿çš„è¶‹åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆUL-UNASï¼‰ä¼˜åŒ–çš„è¶…è½»é‡çº§U-netï¼Œé€‚ç”¨äºåœ¨ä½è¶³è¿¹è®¾å¤‡ä¸­å®ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨U-Netæ¡†æ¶å†…æ¢ç´¢äº†å„ç§é«˜æ•ˆå·ç§¯å—çš„åº”ç”¨ï¼Œä»¥è¯†åˆ«æœ€æœ‰å‰é€”çš„å€™é€‰è€…ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå¢å¼ºç»„ä»¶æ¥æé«˜è¿™äº›å·ç§¯å—çš„èƒ½åŠ›ï¼šä¸€ç§åä¸ºä»¿å°„PReLUçš„æ–°å‹æ¿€æ´»å‡½æ•°å’Œä¸€ä¸ªå› æœæ—¶é¢‘æ³¨æ„æ¨¡å—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼Œåœ¨ç²¾å¿ƒè®¾è®¡çš„æœç´¢ç©ºé—´ä¸­å‘ç°æœ€ä¼˜æ¶æ„ã€‚é€šè¿‡æ•´åˆä¸Šè¿°ç­–ç•¥ï¼ŒUL-UNASä¸ä»…æ˜¾è‘—ä¼˜äºå…·æœ‰ç›¸åŒæˆ–æ›´ä½è®¡ç®—å¤æ‚åº¦çš„æœ€æ–°è¶…è½»é‡çº§æ¨¡å‹ï¼Œè€Œä¸”ä¸éœ€è¦æ›´é«˜è®¡ç®—èµ„æºçš„æœ€è¿‘åŸºçº¿æ¨¡å‹ç›¸æ¯”ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00340v1">PDF</a> 13 pages, 8 figures, submitted to Neural Networks</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½‘ç»œæ¶æ„æœç´¢çš„è¶…è½»é‡çº§Uå½¢ç½‘ç»œï¼ˆUL-UNASï¼‰ï¼Œé€‚ç”¨äºä½å ç”¨ç©ºé—´è®¾å¤‡çš„å®æ—¶è¯­éŸ³å¢å¼ºåº”ç”¨ã€‚é€šè¿‡æ¢ç´¢é«˜æ•ˆçš„å·ç§¯å—ã€å¼•å…¥æ–°å‹æ¿€æ´»å‡½æ•°å’Œæ—¶é—´é¢‘ç‡æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥åŠåˆ©ç”¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ä¼˜åŒ–ç»“æ„ï¼ŒUL-UNASåœ¨ä¿æŒè¾ƒä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œå®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–æœ€æ–°è¶…è½»é‡çº§æ¨¡å‹ï¼Œå¹¶ä¸éœ€è¦æ›´é«˜è®¡ç®—èµ„æºçš„åŸºç¡€æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¶…è½»é‡çº§æ¨¡å‹åœ¨å®æ—¶è¯­éŸ³å¢å¼ºåº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†åŸºäºç½‘ç»œæ¶æ„æœç´¢çš„è¶…è½»é‡çº§Uå½¢ç½‘ç»œï¼ˆUL-UNASï¼‰ã€‚</li>
<li>é€šè¿‡æ¢ç´¢é«˜æ•ˆçš„å·ç§¯å—æ¥ä¼˜åŒ–Uå½¢ç½‘ç»œæ¡†æ¶ã€‚</li>
<li>å¼•å…¥äº†æ–°å‹æ¿€æ´»å‡½æ•°ï¼ˆaffine PReLUï¼‰å’Œå› æœæ—¶é—´-é¢‘ç‡æ³¨æ„åŠ›æ¨¡å—ï¼Œå¢å¼ºäº†å·ç§¯å—çš„èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢åœ¨ç²¾å¿ƒè®¾è®¡æœç´¢ç©ºé—´å†…å‘ç°æœ€ä¼˜æ¶æ„ã€‚</li>
<li>UL-UNASåœ¨ä¿æŒè¾ƒä½è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-69336766c54725b5bcc381e0d674eaf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4250d2b47faf31c6e280161ee4fd6bae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e72da40c334b80ae52a7e10daa1aa70d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06b4b9147bb2cc0eefef448ed315dc50.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-efc0cf1d9bf6a436f39e7f387588a524.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  Zero-Shot Head Swapping in Real-World Scenarios
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fe61bd17d11b2f1934059555a1fdebfe.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  X2CT-CLIP Enable Multi-Abnormality Detection in Computed Tomography   from Chest Radiography via Tri-Modal Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
