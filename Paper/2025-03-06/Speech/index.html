<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-03-06  A Hypernetwork-Based Approach to KAN Representation of Audio Signals">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bb33e342eb5d889f700d0fd1429ca977.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-06-更新"><a href="#2025-03-06-更新" class="headerlink" title="2025-03-06 更新"></a>2025-03-06 更新</h1><h2 id="A-Hypernetwork-Based-Approach-to-KAN-Representation-of-Audio-Signals"><a href="#A-Hypernetwork-Based-Approach-to-KAN-Representation-of-Audio-Signals" class="headerlink" title="A Hypernetwork-Based Approach to KAN Representation of Audio Signals"></a>A Hypernetwork-Based Approach to KAN Representation of Audio Signals</h2><p><strong>Authors:Patryk Marszałek, Maciej Rut, Piotr Kawa, Piotr Syga</strong></p>
<p>Implicit neural representations (INR) have gained prominence for efficiently encoding multimedia data, yet their applications in audio signals remain limited. This study introduces the Kolmogorov-Arnold Network (KAN), a novel architecture using learnable activation functions, as an effective INR model for audio representation. KAN demonstrates superior perceptual performance over previous INRs, achieving the lowest Log-SpectralDistance of 1.29 and the highest Perceptual Evaluation of Speech Quality of 3.57 for 1.5 s audio. To extend KAN’s utility, we propose FewSound, a hypernetwork-based architecture that enhances INR parameter updates. FewSound outperforms the state-of-the-art HyperSound, with a 33.3% improvement in MSE and 60.87% in SI-SNR. These results show KAN as a robust and adaptable audio representation with the potential for scalability and integration into various hypernetwork frameworks. The source code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git">https://github.com/gmum/fewsound.git</a>. </p>
<blockquote>
<p>隐式神经表示（INR）在多媒体数据编码方面表现出色，但在音频信号方面的应用仍然有限。本研究引入了Kolmogorov-Arnold网络（KAN），这是一种使用可学习激活函数的新型架构，作为音频表示的有效INR模型。KAN在感知性能上优于先前的INR，在1.5秒音频上实现了最低的Log-SpectralDistance为1.29和最高的语音质量感知评价为3.57。为了拓展KAN的实用性，我们提出了基于超网络的FewSound架构，用于增强INR参数更新。FewSound的表现优于当前最先进的HyperSound，在MSE上提高了33.3%，在SI-SNR上提高了60.87%。这些结果表明，KAN是一种稳健且可适应的音频表示方法，具有可扩展性和集成到各种超网络框架的潜力。源代码可在<a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/gmum/fewsound.git访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02585v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Kolmogorov-Arnold网络（KAN）的新型神经网络架构，该架构采用可学习的激活函数，作为音频表示的有效隐式神经表示（INR）模型。KAN在音频信号上的表现优于先前的INR模型，并在短短的1.5秒音频上实现了最低的逻辑谱距离和最高的语音质量感知评价。此外，本研究还提出了基于超网络的FewSound架构，可增强INR参数更新，其性能优于现有的HyperSound，在MSE和SI-SNR方面取得了显著改进。这些结果表明，KAN作为一种稳健且可适应的音频表示方法，具有可扩展性和集成到各种超网络框架的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Kolmogorv-Arnold网络（KAN）是一种新型的隐式神经表示（INR）模型，使用可学习的激活函数，能有效编码音频数据。</li>
<li>KAN在音频表现上超越了先前的INR模型，实现了更低的逻辑谱距离和更高的语音质量感知评价。</li>
<li>FewSound架构是基于超网络的设计，能增强INR模型的参数更新。</li>
<li>FewSound在性能上超越了现有的HyperSound，在均方误差（MSE）和信号干扰源到噪声比率（SI-SNR）方面取得了显著改进。</li>
<li>KAN模型具有可扩展性和适应性，可应用于各种超网络框架。</li>
<li>该研究的源代码已公开，可方便后续研究和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02585">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7cb9f41d8cb87402459d5430738cf78a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c4f7c48b31dfb96976d786656cfb4c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d090ab0619b083e39491455333ca26eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9e7b776c8e24e69cab579d207f0e9a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e893db96daf84353ce23981e9ef98456.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a338323207308a508f7be0fd754e2d4d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MM-OR-A-Large-Multimodal-Operating-Room-Dataset-for-Semantic-Understanding-of-High-Intensity-Surgical-Environments"><a href="#MM-OR-A-Large-Multimodal-Operating-Room-Dataset-for-Semantic-Understanding-of-High-Intensity-Surgical-Environments" class="headerlink" title="MM-OR: A Large Multimodal Operating Room Dataset for Semantic   Understanding of High-Intensity Surgical Environments"></a>MM-OR: A Large Multimodal Operating Room Dataset for Semantic   Understanding of High-Intensity Surgical Environments</h2><p><strong>Authors:Ege Özsoy, Chantal Pellegrini, Tobias Czempiel, Felix Tristram, Kun Yuan, David Bani-Harouni, Ulrich Eck, Benjamin Busam, Matthias Keicher, Nassir Navab</strong></p>
<p>Operating rooms (ORs) are complex, high-stakes environments requiring precise understanding of interactions among medical staff, tools, and equipment for enhancing surgical assistance, situational awareness, and patient safety. Current datasets fall short in scale, realism and do not capture the multimodal nature of OR scenes, limiting progress in OR modeling. To this end, we introduce MM-OR, a realistic and large-scale multimodal spatiotemporal OR dataset, and the first dataset to enable multimodal scene graph generation. MM-OR captures comprehensive OR scenes containing RGB-D data, detail views, audio, speech transcripts, robotic logs, and tracking data and is annotated with panoptic segmentations, semantic scene graphs, and downstream task labels. Further, we propose MM2SG, the first multimodal large vision-language model for scene graph generation, and through extensive experiments, demonstrate its ability to effectively leverage multimodal inputs. Together, MM-OR and MM2SG establish a new benchmark for holistic OR understanding, and open the path towards multimodal scene analysis in complex, high-stakes environments. Our code, and data is available at <a target="_blank" rel="noopener" href="https://github.com/egeozsoy/MM-OR">https://github.com/egeozsoy/MM-OR</a>. </p>
<blockquote>
<p>手术室（ORs）是一个复杂且高风险的环境，需要精确理解医疗人员、工具和设备之间的交互，以提高手术辅助、情境意识和病人安全。当前的数据集在规模、真实感方面存在不足，并且未能捕捉到手术室场景的多模式性质，从而限制了手术室建模的进展。为此，我们引入了MM-OR，这是一个真实且大规模的多模式时空手术室数据集，也是第一个能够实现多模式场景图生成的数据集。MM-OR捕捉了包含RGB-D数据、细节视图、音频、语音记录、机器人日志和跟踪数据的全面手术室场景，并带有全景分割、语义场景图和下游任务标签的注释。此外，我们提出了MM2SG，这是第一个用于场景图生成的大型多模式视觉语言模型，并通过大量实验证明了其有效利用多模式输入的能力。总之，MM-OR和MM2SG为全面的手术室理解建立了新的基准，并为复杂高风险环境中的多模式场景分析开辟了道路。我们的代码和数据可在 <a target="_blank" rel="noopener" href="https://github.com/egeozsoy/MM-OR">https://github.com/egeozsoy/MM-OR</a> 中获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02579v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>手术环境复杂且风险高，需要深入理解手术室内的交互情况以提升手术辅助、态势感知和患者安全。当前数据集在规模、现实性和多模态捕捉方面存在不足，限制了手术室建模的进展。为此，我们推出MM-OR数据集，这是一个大规模、现实的多模态时空手术室数据集，也是首个能够实现多模态场景图生成的数据集。MM-OR捕捉了全面的手术室场景，包括RGB-D数据、细节视图、音频、语音记录、机器人日志和跟踪数据，并带有全景分割、语义场景图和下游任务标签的注释。此外，我们还提出了MM2SG，这是首个用于场景图生成的大型多模态视觉语言模型，并通过大量实验证明其有效利用多模态输入的能力。总之，MM-OR和MM2SG为全面的手术室理解建立了新的基准，并为复杂高风险环境中的多模态场景分析开辟了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>手术室是复杂的医疗环境，需要增强对医疗人员、工具和设备的交互理解以提升手术效率和安全性。</li>
<li>当前数据集在规模、现实性和多模态捕捉方面存在局限性，限制了手术室环境的建模研究。</li>
<li>MM-OR数据集是首个大规模、现实的多模态时空手术室数据集，能够全面捕捉手术室的多种信息。</li>
<li>MM-OR数据集包含RGB-D数据、细节视图、音频、语音记录等多模态信息，并带有全面的注释。</li>
<li>MM2SG是首个用于场景图生成的大型多模态视觉语言模型，能有效利用多模态输入。</li>
<li>MM-OR和MM2SG共同为手术室的全面理解设立了新标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02579">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-209d089f15e1abd82c196c77ea56c479.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5294af7bf5a73eba5dd02661d43c0099.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f2d4ca49da20623f7ae8a57896fc411.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-541178215dd7736b81e3bbc374e8bab4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Pruning-Deep-Neural-Networks-via-a-Combination-of-the-Marchenko-Pastur-Distribution-and-Regularization"><a href="#Pruning-Deep-Neural-Networks-via-a-Combination-of-the-Marchenko-Pastur-Distribution-and-Regularization" class="headerlink" title="Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur   Distribution and Regularization"></a>Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur   Distribution and Regularization</h2><p><strong>Authors:Leonid Berlyand, Theo Bourdais, Houman Owhadi, Yitzchak Shmalo</strong></p>
<p>Deep neural networks (DNNs) have brought significant advancements in various applications in recent years, such as image recognition, speech recognition, and natural language processing. In particular, Vision Transformers (ViTs) have emerged as a powerful class of models in the field of deep learning for image classification. In this work, we propose a novel Random Matrix Theory (RMT)-based method for pruning pre-trained DNNs, based on the sparsification of weights and singular vectors, and apply it to ViTs. RMT provides a robust framework to analyze the statistical properties of large matrices, which has been shown to be crucial for understanding and optimizing the performance of DNNs. We demonstrate that our RMT-based pruning can be used to reduce the number of parameters of ViT models (trained on ImageNet) by 30-50% with less than 1% loss in accuracy. To our knowledge, this represents the state-of-the-art in pruning for these ViT models. Furthermore, we provide a rigorous mathematical underpinning of the above numerical studies, namely we proved a theorem for fully connected DNNs, and other more general DNN structures, describing how the randomness in the weight matrices of a DNN decreases as the weights approach a local or global minimum (during training). We verify this theorem through numerical experiments on fully connected DNNs, providing empirical support for our theoretical findings. Moreover, we prove a theorem that describes how DNN loss decreases as we remove randomness in the weight layers, and show a monotone dependence of the decrease in loss with the amount of randomness that we remove. Our results also provide significant RMT-based insights into the role of regularization during training and pruning. </p>
<blockquote>
<p>近年来，深度神经网络（DNNs）在各种应用中取得了显著进展，如图像识别、语音识别和自然语言处理。特别是，视觉Transformer（ViTs）作为深度学习领域的一种强大模型，在图像分类中崭露头角。在这项工作中，我们提出了一种基于随机矩阵理论（RMT）的预训练DNN压缩方法，该方法基于权重和奇异向量的稀疏化，并应用于ViTs。RMT提供了一个强大的框架来分析大型矩阵的统计特性，这已被证明对于理解和优化DNN的性能至关重要。我们证明，我们的基于RMT的压缩技术可用于将经过ImageNet训练的ViT模型的参数数量减少30-50%，同时精度损失不到1%。据我们所知，这是针对这些ViT模型的最新压缩技术。此外，我们对上述数值研究进行了严格的数学支撑，即我们为全连接DNN和其他更一般的DNN结构证明了一个定理，描述了DNN权重矩阵中的随机性如何随着权重接近局部或全局最小值（在训练过程中）而减少。我们通过全连接DNN的数值实验验证了这一定理，为我们的理论发现提供了实证支持。此外，我们证明了另一个定理，描述了随着我们在权重层中消除随机性，DNN损失如何减少，以及损失减少与我们消除的随机性量之间的单调依赖性。我们的结果还提供了基于RMT的关于训练和压缩过程中正则化作用的深刻见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01922v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于随机矩阵理论（RMT）的深度学习神经网络（DNN）剪枝方法，并将其应用于图像分类领域的Vision Transformers（ViTs）。研究团队提出了利用权重稀疏化和奇异向量进行剪枝的新方法，对在ImageNet上训练的ViT模型参数减少了30%~50%，而精度损失不到1%，达到业界领先水平。同时，本文提供了严谨的数学理论基础，通过理论推导和数值实验证明了对全连接神经网络结构中的随机性减少与损失降低的关系。此外，本文还探讨了训练过程中的正则化作用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出基于随机矩阵理论（RMT）的剪枝新方法用于预训练深度神经网络（DNN）。</li>
<li>方法应用于Vision Transformers（ViTs）在图像分类领域表现优秀，可大幅减少模型参数而精度损失极小。</li>
<li>研究提供了严谨的数学理论支撑，包括证明全连接神经网络结构中的随机性减少与损失降低的关系。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01922">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-62f9b4e1a581ec35c7b310b4d84b7e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18678ad6b7f893f31ed1928aca7f7c8c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="KeyFace-Expressive-Audio-Driven-Facial-Animation-for-Long-Sequences-via-KeyFrame-Interpolation"><a href="#KeyFace-Expressive-Audio-Driven-Facial-Animation-for-Long-Sequences-via-KeyFrame-Interpolation" class="headerlink" title="KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via   KeyFrame Interpolation"></a>KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via   KeyFrame Interpolation</h2><p><strong>Authors:Antoni Bigata, Michał Stypułkowski, Rodrigo Mira, Stella Bounareli, Konstantinos Vougioukas, Zoe Landgraf, Nikita Drobyshev, Maciej Zieba, Stavros Petridis, Maja Pantic</strong></p>
<p>Current audio-driven facial animation methods achieve impressive results for short videos but suffer from error accumulation and identity drift when extended to longer durations. Existing methods attempt to mitigate this through external spatial control, increasing long-term consistency but compromising the naturalness of motion. We propose KeyFace, a novel two-stage diffusion-based framework, to address these issues. In the first stage, keyframes are generated at a low frame rate, conditioned on audio input and an identity frame, to capture essential facial expressions and movements over extended periods of time. In the second stage, an interpolation model fills in the gaps between keyframes, ensuring smooth transitions and temporal coherence. To further enhance realism, we incorporate continuous emotion representations and handle a wide range of non-speech vocalizations (NSVs), such as laughter and sighs. We also introduce two new evaluation metrics for assessing lip synchronization and NSV generation. Experimental results show that KeyFace outperforms state-of-the-art methods in generating natural, coherent facial animations over extended durations, successfully encompassing NSVs and continuous emotions. </p>
<blockquote>
<p>当前基于音频驱动的面部动画方法在短视频上取得了令人印象深刻的效果，但当扩展到更长时间时，会出现误差累积和身份漂移的问题。现有方法试图通过外部空间控制来减轻这一问题，虽然增加了长期一致性，但牺牲了动作的自然性。我们提出了KeyFace，这是一种基于新型两阶段扩散的框架来解决这些问题。在第一阶段，以音频输入和身份帧为条件，在低帧率下生成关键帧，以捕捉长时间段内的重要面部表情和动作。在第二阶段，插值模型填补了关键帧之间的空白，确保了平滑过渡和时间连贯性。为了进一步提高逼真度，我们融入了连续的情绪表示，并处理了各种非语音声音（NSVs），如笑声和叹息声。我们还引入了两个新的评估指标，用于评估唇同步和非语音生成。实验结果表明，KeyFace在生成长时间的自然、连贯的面部动画方面优于最先进的方法，成功涵盖了非语音声音和连续情绪。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01715v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为KeyFace的新型两阶段扩散框架，旨在解决音频驱动面部动画在长时间序列中面临的误差累积和身份漂移问题。第一阶段根据音频输入和身份帧在低帧率下生成关键帧，捕捉长时间的面部表情和动作。第二阶段采用插值模型填充关键帧之间的间隙，确保平滑过渡和时间连贯性。为增强真实感，KeyFace还融入了连续情感表示，并处理各种非语音发声（NSVs）。实验结果表明，KeyFace在生成自然、连贯的长时间面部动画方面优于现有技术，成功涵盖NSVs和连续情感。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前音频驱动的面部动画方法在短时间视频上表现令人印象深刻，但在长时间序列上遭遇误差累积和身份漂移的挑战。</li>
<li>KeyFace是一个新型的两阶段扩散框架，旨在解决这些问题。</li>
<li>第一阶段根据音频输入和身份帧生成低帧率的关键帧，捕捉长时间内的面部表情和动作。</li>
<li>第二阶段采用插值模型确保关键帧之间的平滑过渡和时间连贯性。</li>
<li>KeyFace融入连续情感表示，增强动画的真实感。</li>
<li>KeyFace能够处理各种非语音发声（NSVs），如笑声和叹息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01715">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-db928d98c51645cb7f366bf1ff08b77b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a1bbd4f60c2cc9c98a32f4b5975fbb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32e0c4ccd04950d52d53fcc672fdf14f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04a645b763b17e3fb20d3c54b18914c5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Social-Media-Rumor-Detection-A-Semantic-and-Graph-Neural-Network-Approach-for-the-2024-Global-Election"><a href="#Enhancing-Social-Media-Rumor-Detection-A-Semantic-and-Graph-Neural-Network-Approach-for-the-2024-Global-Election" class="headerlink" title="Enhancing Social Media Rumor Detection: A Semantic and Graph Neural   Network Approach for the 2024 Global Election"></a>Enhancing Social Media Rumor Detection: A Semantic and Graph Neural   Network Approach for the 2024 Global Election</h2><p><strong>Authors:Liu Yan, Liu Yunpeng, Zhao Liang</strong></p>
<p>The development of social media platforms has revolutionized the speed and manner in which information is disseminated, leading to both beneficial and detrimental effects on society. While these platforms facilitate rapid communication, they also accelerate the spread of rumors and extremist speech, impacting public perception and behavior significantly. This issue is particularly pronounced during election periods, where the influence of social media on election outcomes has become a matter of global concern. With the unprecedented number of elections in 2024, against this backdrop, the election ecosystem has encountered unprecedented challenges. This study addresses the urgent need for effective rumor detection on social media by proposing a novel method that combines semantic analysis with graph neural networks. We have meticulously collected a dataset from PolitiFact and Twitter, focusing on politically relevant rumors. Our approach involves semantic analysis using a fine-tuned BERT model to vectorize text content and construct a directed graph where tweets and comments are nodes, and interactions are edges. The core of our method is a graph neural network, SAGEWithEdgeAttention, which extends the GraphSAGE model by incorporating first-order differences as edge attributes and applying an attention mechanism to enhance feature aggregation. This innovative approach allows for the fine-grained analysis of the complex social network structure, improving rumor detection accuracy. The study concludes that our method significantly outperforms traditional content analysis and time-based models, offering a theoretically sound and practically efficient solution. </p>
<blockquote>
<p>社交媒体平台的发展在信息传播的速度和方式上带来了革命性的变革，对社会产生了有益和有害的影响。这些平台虽然促进了快速通信，但也加速了谣言和极端言论的传播，对公众认知和行为产生了重大影响。在选举期间，这个问题尤为突出，社交媒体对选举结果的影响已成为全球关注的问题。在即将到来的2024年众多选举的背景下，选举生态系统面临着前所未有的挑战。本研究针对社交媒体上迫切需要的有效谣言检测，提出了一种结合语义分析和图神经网络的新方法。我们从PolitiFact和Twitter精心收集了一个数据集，重点关注与政治相关的谣言。我们的方法使用微调后的BERT模型进行语义分析，将文本内容向量化，并构建一个由推文和评论作为节点、互动作为边的有向图。我们的方法的核心是图神经网络SAGEWithEdgeAttention，它通过引入一阶差分作为边属性并在特征聚合过程中应用注意力机制来扩展GraphSAGE模型。这种创新的方法允许对复杂的社交网络结构进行精细分析，提高了谣言检测的准确性。研究表明，我们的方法显著优于传统的基于内容和基于时间模型，提供了一个理论上有依据、实践上有效的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01394v1">PDF</a> </p>
<p><strong>摘要</strong><br>社交媒体平台的发展为信息传播的速度和方式带来了革命性的变化，既产生了积极的影响，也带来了消极的影响。这些平台促进了快速沟通，但也加速了谣言和极端言论的传播，对公众认知和行为产生了重大影响。特别是在选举期间，社交媒体对选举结果的影响已成为全球关注的问题。随着2024年选举数量的空前增加，选举生态系统面临着前所未有的挑战。本研究提出了一种新的谣言检测法应对这一紧迫需求，该方法结合了语义分析与图神经网络。我们从PolitiFact和Twitter收集了政治相关的谣言数据集。我们的方法使用微调后的BERT模型进行语义分析，将文本内容向量化，并构建一个以推文和评论为节点、互动为边的有向图。该方法的核心是图神经网络SAGEWithEdgeAttention，它扩展了GraphSAGE模型，将一阶差分作为边属性并应用注意力机制以增强特征聚合。这种创新的方法允许对复杂的社会网络结构进行精细分析，提高了谣言检测的准确性。研究表明，我们的方法显著优于传统的基于内容和基于时间的模型，提供了一个理论扎实、实践高效的解决方案。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>社交媒体平台的发展在信息传播方面产生了革命性的影响，既有正面也有负面效果。</li>
<li>社交媒体在选举期间对公众认知和行为的冲击尤为显著，已成为全球关注的议题。</li>
<li>面对2024年选举增多的背景，选举生态系统面临前所未有的挑战。</li>
<li>研究提出了一种结合语义分析与图神经网络的全新谣言检测法。</li>
<li>数据集来自PolitiFact和Twitter，集中于政治相关的谣言。</li>
<li>方法的核心是SAGEWithEdgeAttention图神经网络，它扩展了GraphSAGE模型并应用了注意力机制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01394">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2d19b66e043847e45194788068f3b651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb33e342eb5d889f700d0fd1429ca977.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HOP-Heterogeneous-Topology-based-Multimodal-Entanglement-for-Co-Speech-Gesture-Generation"><a href="#HOP-Heterogeneous-Topology-based-Multimodal-Entanglement-for-Co-Speech-Gesture-Generation" class="headerlink" title="HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech   Gesture Generation"></a>HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech   Gesture Generation</h2><p><strong>Authors:Hongye Cheng, Tianyu Wang, Guangsi Shi, Zexing Zhao, Yanwei Fu</strong></p>
<p>Co-speech gestures are crucial non-verbal cues that enhance speech clarity and expressiveness in human communication, which have attracted increasing attention in multimodal research. While the existing methods have made strides in gesture accuracy, challenges remain in generating diverse and coherent gestures, as most approaches assume independence among multimodal inputs and lack explicit modeling of their interactions. In this work, we propose a novel multimodal learning method named HOP for co-speech gesture generation that captures the heterogeneous entanglement between gesture motion, audio rhythm, and text semantics, enabling the generation of coordinated gestures. By leveraging spatiotemporal graph modeling, we achieve the alignment of audio and action. Moreover, to enhance modality coherence, we build the audio-text semantic representation based on a reprogramming module, which is beneficial for cross-modality adaptation. Our approach enables the trimodal system to learn each other’s features and represent them in the form of topological entanglement. Extensive experiments demonstrate that HOP achieves state-of-the-art performance, offering more natural and expressive co-speech gesture generation. More information, codes, and demos are available here: <a target="_blank" rel="noopener" href="https://star-uu-wang.github.io/HOP/">https://star-uu-wang.github.io/HOP/</a> </p>
<blockquote>
<p>对话中的协同语音手势是非语言线索中至关重要的部分，能增强人类交流中的语音清晰度和表达力，在多模态研究中引起了越来越多的关注。虽然现有方法在手势准确性方面取得了进展，但在生成多样和连贯的手势方面仍存在挑战。大多数方法假设多模态输入是独立的，缺乏对其交互的显式建模。在这项工作中，我们提出了一种名为HOP的协同语音手势生成的多模态学习方法，该方法能够捕捉手势运动、音频节奏和文本语义之间的异质纠缠，从而实现协调手势的生成。通过利用时空图建模，我们实现了音频和动作的对齐。此外，为了增强模态连贯性，我们基于重构模块构建了音频-文本语义表示，这有利于跨模态适应。我们的方法使三模态系统能够相互学习特征并以拓扑纠缠的形式表示它们。大量实验表明，HOP达到了最先进的性能，实现了更自然、更富有表现力的协同语音手势生成。更多信息、代码和演示可在以下网址找到：<a target="_blank" rel="noopener" href="https://star-uu-wang.github.io/HOP/">https://star-uu-wang.github.io/HOP/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01175v1">PDF</a> Accepted by CVPR 2025. See <a target="_blank" rel="noopener" href="https://star-uu-wang.github.io/HOP/">https://star-uu-wang.github.io/HOP/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为HOP的多模态学习方法，用于生成协同语音手势。该方法通过时空图建模实现音频和动作的同步，并通过编程模块构建音频文本语义表示，增强模态一致性。该方法能够生成多样且连贯的手势，实现语音、手势和文本之间的协调。实验证明，该方法在协同语音手势生成方面取得了最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>协同语音手势是人类沟通中重要的非语言线索，能提高语音的清晰度和表达力。</li>
<li>当前的手势生成方法主要面临生成多样性和连贯性挑战，且大多假设多模态输入独立，缺乏对其交互的显式建模。</li>
<li>HOP方法通过时空图建模实现音频和动作的同步，提高手势的协调性。</li>
<li>HOP方法利用编程模块构建音频文本语义表示，增强模态一致性，有利于跨模态适应。</li>
<li>HOP方法实现了语音、手势和文本之间的协调，生成更自然和表达性更强的协同语音手势。</li>
<li>实验证明，HOP方法在协同语音手势生成方面取得了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01175">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d785d07f5eb2a80bb18ed513ae53d08c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec1bec98a4858df2676aa5a483bb21dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-779e1871e48e64651f7de76bf9495f8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5dec7254e78ea28baf07ee9a1496f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cab55f1cda4c882919bc0fc3de3d292.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unveiling-Biases-while-Embracing-Sustainability-Assessing-the-Dual-Challenges-of-Automatic-Speech-Recognition-Systems"><a href="#Unveiling-Biases-while-Embracing-Sustainability-Assessing-the-Dual-Challenges-of-Automatic-Speech-Recognition-Systems" class="headerlink" title="Unveiling Biases while Embracing Sustainability: Assessing the Dual   Challenges of Automatic Speech Recognition Systems"></a>Unveiling Biases while Embracing Sustainability: Assessing the Dual   Challenges of Automatic Speech Recognition Systems</h2><p><strong>Authors:Ajinkya Kulkarni, Atharva Kulkarni, Miguel Couceiro, Isabel Trancoso</strong></p>
<p>In this paper, we present a bias and sustainability focused investigation of Automatic Speech Recognition (ASR) systems, namely Whisper and Massively Multilingual Speech (MMS), which have achieved state-of-the-art (SOTA) performances. Despite their improved performance in controlled settings, there remains a critical gap in understanding their efficacy and equity in real-world scenarios. We analyze ASR biases w.r.t. gender, accent, and age group, as well as their effect on downstream tasks. In addition, we examine the environmental impact of ASR systems, scrutinizing the use of large acoustic models on carbon emission and energy consumption. We also provide insights into our empirical analyses, offering a valuable contribution to the claims surrounding bias and sustainability in ASR systems. </p>
<blockquote>
<p>本文重点关注自动语音识别（ASR）系统的偏见和可持续性调查，特别是已经达到最新技术水平（SOTA）的Whisper和大规模多语种语音（MMS）。尽管它们在受控环境中的性能得到了提高，但在真实场景中的效率和公平仍存在关键差距。我们分析了ASR与性别、口音和年龄组之间的偏见及其对下游任务的影响。此外，我们还考察了ASR系统的环境影响，详细分析了大型声学模型对碳排放和能源消耗的影响。我们还提供了实证分析的见解，为ASR系统中有关偏见和可持续性的主张做出了宝贵贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00907v1">PDF</a> Interspeech 2024</p>
<p><strong>Summary</strong></p>
<p>本文研究了自动语音识别（ASR）系统的偏见和可持续性，特别是表现最佳的Whisper和Massively Multilingual Speech（MMS）。文章探讨了ASR系统在实际场景中的效率和公平性问题，重点分析了其在性别、口音和年龄组方面的偏见及其对下游任务的影响。此外，本文还探讨了ASR系统的环境影响，特别是大型声学模型对碳排放和能源消耗的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别（ASR）系统存在偏见问题，特别是在性别、口音和年龄组方面。</li>
<li>ASR系统的性能在实际场景中可能存在公平性问题。</li>
<li>ASR系统的偏见对下游任务产生影响。</li>
<li>大型声学模型在碳排放和能源消耗方面存在环境影响。</li>
<li>ASR系统的可持续性是研究的重点之一。</li>
<li>文章提供了关于ASR系统中偏见和可持续性的实证分析和见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00907">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f5b2eef227e70cddc6c9449df0a73b93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1baa52b14457ec933fb7bd0abcd4c661.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c53c9552fbf49e4a7f960da6d9b4e28b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8188a7f31ab2e2e794962bccdabf806c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5a246efb62027921dbe06ceb6c8a0d4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UniWav-Towards-Unified-Pre-training-for-Speech-Representation-Learning-and-Generation"><a href="#UniWav-Towards-Unified-Pre-training-for-Speech-Representation-Learning-and-Generation" class="headerlink" title="UniWav: Towards Unified Pre-training for Speech Representation Learning   and Generation"></a>UniWav: Towards Unified Pre-training for Speech Representation Learning   and Generation</h2><p><strong>Authors:Alexander H. Liu, Sang-gil Lee, Chao-Han Huck Yang, Yuan Gong, Yu-Chiang Frank Wang, James R. Glass, Rafael Valle, Bryan Catanzaro</strong></p>
<p>Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training. </p>
<blockquote>
<p>预训练和表示学习在现代语音识别中扮演着越来越重要的角色。然而，不同的应用依赖于不同的基础模型，因为主流的预训练技术要么是为判别任务设计的，要么是为生成任务设计的。在这项工作中，我们首次尝试为这两种类型的任务构建统一的预训练框架。我们表明，通过适当的预训练设计选择，可以联合学习一个表示编码器和一个生成音频解码器，这两种类型的应用都可以使用。我们提出了UniWav，这是一种为统一预训练表示学习和生成任务而设计的编码器-解码器框架。在语音识别、文本到语音和语音标记化方面，UniWav实现了与针对不同任务训练的现有基础模型相当的性能。我们的研究结果表明，可以构建一个通用的单一基础模型来进行语音训练，以替代不同的基础模型，从而降低预训练的开销和成本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00733v1">PDF</a> ICLR 2025; demo page at   <a target="_blank" rel="noopener" href="https://alexander-h-liu.github.io/uniwav-demo.github.io/">https://alexander-h-liu.github.io/uniwav-demo.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了在语音处理中预训练和表示学习的重要性，并指出目前不同的应用依赖于不同的基础模型，主流的预训练技术主要是为判别任务或生成任务设计的。本研究首次尝试为这两种任务构建统一的预训练框架，并证明通过适当的预训练设计选择，可以联合学习适用于这两种任务的表示编码器和生成音频解码器。本研究提出了UniWav这一编码解码框架，旨在统一预训练表示学习和生成任务。在语音识别、文本到语音和语音标记化方面，UniWav的性能与针对特定任务训练的现有基础模型相当。研究结果表明，可以构建单一通用的语音基础模型来替代不同的基础模型，从而降低预训练的开销和成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练和表示学习在现代语音处理中扮演着越来越重要的角色。</li>
<li>目前不同的语音应用依赖于不同的基础模型，而主流预训练技术主要面向判别任务或生成任务。</li>
<li>研究首次尝试为判别任务和生成任务构建统一的预训练框架。</li>
<li>通过适当的预训练设计选择，可以联合学习表示编码器和生成音频解码器，适用于两种任务。</li>
<li>提出的UniWav框架旨在统一预训练表示学习和生成任务。</li>
<li>UniWav在语音识别、文本到语音和语音标记化方面的性能与特定任务的基础模型相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-71275a33dc4cac9869c77b183c6d25de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca51e17f246f6e061f10adb66c1b6c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd720ae79d64f2b7cb1679a79d65606e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Implementing-Spiking-World-Model-with-Multi-Compartment-Neurons-for-Model-based-Reinforcement-Learning"><a href="#Implementing-Spiking-World-Model-with-Multi-Compartment-Neurons-for-Model-based-Reinforcement-Learning" class="headerlink" title="Implementing Spiking World Model with Multi-Compartment Neurons for   Model-based Reinforcement Learning"></a>Implementing Spiking World Model with Multi-Compartment Neurons for   Model-based Reinforcement Learning</h2><p><strong>Authors:Yinqian Sun, Feifei Zhao, Mingyang Lv, Yi Zeng</strong></p>
<p>Brain-inspired spiking neural networks (SNNs) have garnered significant research attention in algorithm design and perception applications. However, their potential in the decision-making domain, particularly in model-based reinforcement learning, remains underexplored. The difficulty lies in the need for spiking neurons with long-term temporal memory capabilities, as well as network optimization that can integrate and learn information for accurate predictions. The dynamic dendritic information integration mechanism of biological neurons brings us valuable insights for addressing these challenges. In this study, we propose a multi-compartment neuron model capable of nonlinearly integrating information from multiple dendritic sources to dynamically process long sequential inputs. Based on this model, we construct a Spiking World Model (Spiking-WM), to enable model-based deep reinforcement learning (DRL) with SNNs. We evaluated our model using the DeepMind Control Suite, demonstrating that Spiking-WM outperforms existing SNN-based models and achieves performance comparable to artificial neural network (ANN)-based world models employing Gated Recurrent Units (GRUs). Furthermore, we assess the long-term memory capabilities of the proposed model in speech datasets, including SHD, TIMIT, and LibriSpeech 100h, showing that our multi-compartment neuron model surpasses other SNN-based architectures in processing long sequences. Our findings underscore the critical role of dendritic information integration in shaping neuronal function, emphasizing the importance of cooperative dendritic processing in enhancing neural computation. </p>
<blockquote>
<p>受大脑启发的脉冲神经网络（SNNs）在算法设计和感知应用方面引起了研究人员的广泛关注。然而，它们在决策领域，特别是在基于模型的强化学习中的潜力，尚未得到充分探索。难点在于需要具有长期时间记忆能力的脉冲神经元，以及能够整合和学习信息进行准确预测的网络优化。生物神经元的动态树突信息整合机制为我们解决这些挑战提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00713v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探索了基于脑启发脉冲神经网络（SNNs）的决策制定领域，特别是模型基础上的强化学习。研究提出了一种多室神经元模型，能够动态处理长序列输入，并基于此构建了Spiking World Model（Spiking-WM）。在DeepMind Control Suite上的评估显示，Spiking-WM的表现在现有SNN模型之上，且与采用GRU的基于人工神经网络的世界模型表现相当。此外，在语音数据集上的长期记忆能力评估也证明了该模型的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>脉冲神经网络（SNNs）在决策制定领域的潜力尚未得到充分探索。</li>
<li>多室神经元模型能够非线性地整合来自多个树突来源的信息，以动态处理长序列输入。</li>
<li>Spiking World Model（Spiking-WM）结合了多室神经元模型，实现了基于模型的深度强化学习（DRL）。</li>
<li>Spiking-WM在DeepMind Control Suite上的表现优于现有SNN模型，并与基于GRU的ANN模型表现相当。</li>
<li>在语音数据集上，多室神经元模型在处理长序列方面超越了其他SNN架构。</li>
<li>神经元中的树突信息整合对神经元功能至关重要，合作树突处理有助于增强神经计算。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00713">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-54ea85fcc8233468e6c5d25c8d8029f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-688262681e4e444f7f338c32aedf3571.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8dadc00b8b1cf530d0853ce92d40ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-736ce8785edefc1f9704c8dc47b65755.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement"><a href="#LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement" class="headerlink" title="LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement"></a>LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement</h2><p><strong>Authors:Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie</strong></p>
<p>Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area. </p>
<blockquote>
<p>最近的语言模型（LM）进展表明，它们在语义理解和上下文建模方面表现出强大的能力，尤其在生成性语音增强（SE）领域蓬勃发展。然而，许多基于LM的SE方法主要关注语义信息，往往忽视了声音信息的关键作用，这导致增强后的语音出现声音不一致，以及在多种SE任务中的泛化能力受限。在本文中，我们介绍了LLaSE-G1，这是一个基于LLaMA的语言模型，旨在激励其在语音增强方面的泛化能力。LLaSE-G1有以下主要贡献：首先，为了缓解声音不一致的问题，LLaSE-G1采用WavLM的连续表示作为输入，并预测X-Codec2的语音令牌，最大限度地保留声音。其次，为了提升泛化能力，LLaSE-G1引入了双通道输入和输出，统一了多个SE任务，无需特定任务标识。最后，LLaSE-G1超越了先前的特定任务的判别性和生成性SE模型，显示了测试时的扩展效果以及在未见过的SE任务中的新兴能力。此外，我们发布了相关代码和模型，以支持该领域的进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00493v2">PDF</a> 13 pages, 2 figures, 8 tables</p>
<p><strong>摘要</strong><br>LM近期的进展为语音增强带来了强大的语义理解和上下文建模能力。然而，许多基于LM的SE方法主要关注语义信息，忽略了声学信息的重要性。本文介绍LLaSE-G1模型，旨在解决这一问题并提升语音增强的泛化能力。LLaSE-G1使用WavLM的连续表示作为输入，预测X-Codec2的语音令牌，最大化声学保留，缓解声学不一致问题。通过引入双通道输入输出促进泛化能力，统一多种SE任务无需特定任务标识。相较于之前的任务特定判别和生成式SE模型，LLaSE-G1展现出卓越性能，支持未见过的SE任务。我们公开了代码和模型以支持该领域进一步研究。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLaSE-G1模型解决了声学信息在基于LM的语音增强中被忽视的问题，从而实现了声学一致性。</li>
<li>LLaSE-G1采用WavLM的连续表示作为输入，预测语音令牌以增强声学保留效果。</li>
<li>通过引入双通道输入输出设计，LLaSE-G1提升了模型的泛化能力，能够统一处理多种语音增强任务。</li>
<li>LLaSE-G1相较于任务特定的判别和生成式模型展现出卓越性能。</li>
<li>LLaSE-G1具备出色的测试时扩展性，并展现出处理未见过的语音增强任务的能力。</li>
<li>LLaSE-G1代码和模型的公开将促进该领域的进一步研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-597ec488cb09e0419c2195533aa95f71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29eed87963684c5b0b13172c721b9f9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d15405e11d8e11785f5f9450f35951de.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PodAgent-A-Comprehensive-Framework-for-Podcast-Generation"><a href="#PodAgent-A-Comprehensive-Framework-for-Podcast-Generation" class="headerlink" title="PodAgent: A Comprehensive Framework for Podcast Generation"></a>PodAgent: A Comprehensive Framework for Podcast Generation</h2><p><strong>Authors:Yujia Xiao, Lei He, Haohan Guo, Fenglong Xie, Tan Lee</strong></p>
<p>Existing Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. PodAgent 1) generates informative topic-discussion content by designing a Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis method to generate expressive conversational speech. Given the absence of standardized evaluation criteria for podcast-like audio generation, we developed comprehensive assessment guidelines to effectively evaluate the model’s performance. Experimental results demonstrate PodAgent’s effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion dialogue content, achieving an 87.4% voice-matching accuracy, and producing more expressive speech through LLM-guided synthesis. Demo page: <a target="_blank" rel="noopener" href="https://podcast-agent.github.io/demo/">https://podcast-agent.github.io/demo/</a>. Source code: <a target="_blank" rel="noopener" href="https://github.com/yujxx/PodAgent">https://github.com/yujxx/PodAgent</a>. </p>
<blockquote>
<p>当前存在的自动音频生成方法在生成类似播客的节目时面临诸多挑战。主要困难在于深度内容生成和合适且富有表现力的声音生成。本文提出了PodAgent，一个用于创建音频节目的综合框架。PodAgent 1）通过设计主持人-嘉宾-编剧多智能体协作系统生成信息丰富的主题讨论内容，2）构建声音库，进行合适的语音角色匹配，以及3）利用增强的大型语言模型（LLM）语音合成方法生成富有表现力的对话语音。由于缺乏针对类似播客音频生成的标准化评估标准，我们制定了全面的评估准则以有效评估模型性能。实验结果表明PodAgent的有效性，其在主题讨论对话内容方面显著超越了直接的GPT-4生成，达到87.4%的语音匹配准确率，并通过LLM引导的合成产生了更具表现力的语音。演示页面：<a target="_blank" rel="noopener" href="https://podcast-agent.github.io/demo/%E3%80%82%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%9Ahttps://github.com/yujxx/PodAgent%E3%80%82">https://podcast-agent.github.io/demo/。源代码：https://github.com/yujxx/PodAgent。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00455v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了PodAgent框架，一个用于生成音频节目的综合系统。它通过设计主持人-嘉宾-作者多智能体协作系统生成内容，建立语音库进行合适的语音角色匹配，并利用LLM增强的语音合成方法生成表达性对话语音。尽管缺乏针对类似播客音频生成的标准化评估标准，但实验结果表明PodAgent的有效性，其在主题讨论对话内容方面显著超越了GPT-4直接生成，实现了87.4%的语音匹配准确率，并通过LLM引导的合成产生了更具表现力的语音。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PodAgent是一个全面的框架，用于生成音频节目，涵盖了内容生成、语音匹配和语音合成等方面。</li>
<li>该框架通过设计多智能体协作系统（主持人-嘉宾-作者）来生成内容丰富的主题讨论。</li>
<li>PodAgent建立了语音库，用于合适的语音角色匹配，使生成的音频节目更具真实感。</li>
<li>利用LLM增强的语音合成方法，PodAgent能够生成表达性对话语音，提高音频节目的质量。</li>
<li>PodAgent在主题讨论对话内容方面显著超越了GPT-4直接生成。</li>
<li>PodAgent实现了87.4%的语音匹配准确率，表明其在语音匹配方面的准确性较高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00455">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ec371ce006ed7e4cb9ca0809f6b90ff2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f9633258e91f64ded5f1596cac8397a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ffdbabe7100feebad5329368b357256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfcd7f18bedc75a726f9b3ee5e658fd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac6a178831a73f75aac34e1face6e6ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2251d80c758290100dfc75e84784383.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="UL-UNAS-Ultra-Lightweight-U-Nets-for-Real-Time-Speech-Enhancement-via-Network-Architecture-Search"><a href="#UL-UNAS-Ultra-Lightweight-U-Nets-for-Real-Time-Speech-Enhancement-via-Network-Architecture-Search" class="headerlink" title="UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via   Network Architecture Search"></a>UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via   Network Architecture Search</h2><p><strong>Authors:Xiaobin Rong, Dahan Wang, Yuxiang Hu, Changbao Zhu, Kai Chen, Jing Lu</strong></p>
<p>Lightweight models are essential for real-time speech enhancement applications. In recent years, there has been a growing trend toward developing increasingly compact models for speech enhancement. In this paper, we propose an Ultra-Lightweight U-net optimized by Network Architecture Search (UL-UNAS), which is suitable for implementation in low-footprint devices. Firstly, we explore the application of various efficient convolutional blocks within the U-Net framework to identify the most promising candidates. Secondly, we introduce two boosting components to enhance the capacity of these convolutional blocks: a novel activation function named affine PReLU and a causal time-frequency attention module. Furthermore, we leverage neural architecture search to discover an optimal architecture within our carefully designed search space. By integrating the above strategies, UL-UNAS not only significantly outperforms the latest ultra-lightweight models with the same or lower computational complexity, but also delivers competitive performance compared to recent baseline models that require substantially higher computational resources. </p>
<blockquote>
<p>轻量级模型对于实时语音增强应用至关重要。近年来，开发用于语音增强的越来越紧凑的模型成为了一个增长的趋势。在本文中，我们提出了一种通过神经网络架构搜索（UL-UNAS）优化的超轻量级U-net，适用于在低足迹设备中实现。首先，我们在U-Net框架内探索了各种高效卷积块的应用，以识别最有前途的候选者。其次，我们引入了两个增强组件来提高这些卷积块的能力：一种名为仿射PReLU的新型激活函数和一个因果时频注意模块。此外，我们利用神经网络架构搜索，在精心设计的搜索空间中发现最优架构。通过整合上述策略，UL-UNAS不仅显著优于具有相同或更低计算复杂度的最新超轻量级模型，而且与需要更高计算资源的最近基线模型相比也表现出竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00340v1">PDF</a> 13 pages, 8 figures, submitted to Neural Networks</p>
<p><strong>Summary</strong>：<br>本文提出了一种基于网络架构搜索的超轻量级U形网络（UL-UNAS），适用于低占用空间设备的实时语音增强应用。通过探索高效的卷积块、引入新型激活函数和时间频率注意力模块，以及利用神经网络架构搜索优化结构，UL-UNAS在保持较低计算复杂度的同时，实现了出色的性能，超越了其他最新超轻量级模型，并与需要更高计算资源的基础模型相比具有竞争力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>超轻量级模型在实时语音增强应用中至关重要。</li>
<li>论文提出了基于网络架构搜索的超轻量级U形网络（UL-UNAS）。</li>
<li>通过探索高效的卷积块来优化U形网络框架。</li>
<li>引入了新型激活函数（affine PReLU）和因果时间-频率注意力模块，增强了卷积块的能力。</li>
<li>利用神经网络架构搜索在精心设计搜索空间内发现最优架构。</li>
<li>UL-UNAS在保持较低计算复杂度的同时实现了出色的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00340">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-69336766c54725b5bcc381e0d674eaf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4250d2b47faf31c6e280161ee4fd6bae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e72da40c334b80ae52a7e10daa1aa70d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06b4b9147bb2cc0eefef448ed315dc50.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-efc0cf1d9bf6a436f39e7f387588a524.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-03-06  Zero-Shot Head Swapping in Real-World Scenarios
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fe61bd17d11b2f1934059555a1fdebfe.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-03-06  X2CT-CLIP Enable Multi-Abnormality Detection in Computed Tomography   from Chest Radiography via Tri-Modal Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
