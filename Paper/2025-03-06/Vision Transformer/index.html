<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  BiasICL In-Context Learning and Demographic Biases of Vision Language   Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e15ce61d012a67e4ea6f2fab7516dd51.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    32 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-06-æ›´æ–°"><a href="#2025-03-06-æ›´æ–°" class="headerlink" title="2025-03-06 æ›´æ–°"></a>2025-03-06 æ›´æ–°</h1><h2 id="BiasICL-In-Context-Learning-and-Demographic-Biases-of-Vision-Language-Models"><a href="#BiasICL-In-Context-Learning-and-Demographic-Biases-of-Vision-Language-Models" class="headerlink" title="BiasICL: In-Context Learning and Demographic Biases of Vision Language   Models"></a>BiasICL: In-Context Learning and Demographic Biases of Vision Language   Models</h2><p><strong>Authors:Sonnet Xu, Joseph Janizek, Yixing Jiang, Roxana Daneshjou</strong></p>
<p>Vision language models (VLMs) show promise in medical diagnosis, but their performance across demographic subgroups when using in-context learning (ICL) remains poorly understood. We examine how the demographic composition of demonstration examples affects VLM performance in two medical imaging tasks: skin lesion malignancy prediction and pneumothorax detection from chest radiographs. Our analysis reveals that ICL influences model predictions through multiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease base rates from prompts and (2) ICL leads VLMs to make predictions that perform differently across demographic groups, even after controlling for subgroup-specific disease base rates. Our empirical results inform best-practices for prompting current VLMs (specifically examining demographic subgroup performance, and matching base rates of labels to target distribution at a bulk level and within subgroups), while also suggesting next steps for improving our theoretical understanding of these models. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»å­¦è¯Šæ–­æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ—¶ï¼Œå®ƒä»¬åœ¨äººå£ç»Ÿè®¡å­¦åˆ†ç»„ä¸­çš„è¡¨ç°ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¼”ç¤ºç¤ºä¾‹çš„äººå£ç»Ÿè®¡å­¦æ„æˆå¦‚ä½•å½±å“è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸¤é¡¹åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„è¡¨ç°ï¼šçš®è‚¤ç—…å˜æ¶æ€§é¢„æµ‹å’Œä»èƒ¸éƒ¨æ”¾å°„ç‰‡ä¸­æ£€æµ‹æ°”èƒ¸ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ é€šè¿‡å¤šç§æœºåˆ¶å½±å“æ¨¡å‹é¢„æµ‹ï¼šï¼ˆ1ï¼‰ä¸Šä¸‹æ–‡å­¦ä¹ å…è®¸è§†è§‰è¯­è¨€æ¨¡å‹ä»æç¤ºä¸­å­¦ä¹ ç‰¹å®šäººç¾¤çš„ç–¾ç—…åŸºç¡€ç‡ï¼›ï¼ˆ2ï¼‰å³ä½¿åœ¨æ§åˆ¶äº†ç‰¹å®šäººç¾¤çš„ç–¾ç—…åŸºç¡€ç‡åï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ä»ä¼šå¯¼è‡´è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¸åŒäººç¾¤ç¾¤ä½“ä¸­çš„é¢„æµ‹è¡¨ç°ä¸åŒã€‚æˆ‘ä»¬çš„ç»éªŒç»“æœä¸ºå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹çš„æç¤ºä½¿ç”¨æä¾›äº†æœ€ä½³å®è·µï¼ˆç‰¹åˆ«æ˜¯æ£€æŸ¥äººå£ç»Ÿè®¡å­¦åˆ†ç»„çš„è¡¨ç°ï¼Œä»¥åŠåœ¨å¤§è§„æ¨¡å’Œå­ç»„å†…åŒ¹é…æ ‡ç­¾çš„åŸºç¡€ç‡ä¸ç›®æ ‡çš„åˆ†å¸ƒï¼‰ï¼ŒåŒæ—¶ä¹Ÿä¸ºæˆ‘ä»¬æ”¹è¿›å¯¹è¿™äº›æ¨¡å‹çš„ç†è®ºç†è§£æå‡ºäº†ä¸‹ä¸€æ­¥çš„å»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02334v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åŒ»ç–—è¯Šæ–­é¢†åŸŸï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡æ¢ç©¶äº†åœ¨çš®è‚¤ç—…å˜æ¶æ€§å’Œèƒ¸è…”æ°”èƒ¸æ£€æµ‹ä¸¤ä¸ªåŒ»å­¦å›¾åƒä»»åŠ¡ä¸­ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ å¦‚ä½•å½±å“æ¨¡å‹å¯¹ç§æ—äºšç»„çš„é¢„æµ‹è¡¨ç°ã€‚åˆ†æå‘ç°ä¸Šä¸‹æ–‡å­¦ä¹ å¯¹æ¨¡å‹é¢„æµ‹æœ‰å¤šä¸ªå½±å“æœºåˆ¶ï¼ŒåŒ…æ‹¬å­¦ä¹ ç§æ—äºšç»„ç‰¹å®šçš„ç–¾ç—…åŸºæœ¬æ¦‚ç‡ä»¥åŠé¢„æµ‹è¡¨ç°çš„å·®å¼‚ã€‚æœ¬æ–‡æä¾›äº†å…³äºå½“å‰VLMsçš„æç¤ºæœ€ä½³å®è·µï¼ŒåŒ…æ‹¬æ£€æŸ¥ç§æ—äºšç»„çš„è¡¨ç°ï¼Œä»¥åŠåŒ¹é…æ ‡ç­¾çš„åŸºç¡€æ¦‚ç‡å’Œç›®æ ‡åˆ†å¸ƒçš„å»ºè®®ï¼ŒåŒæ—¶ä¹Ÿæå‡ºäº†æ”¹è¿›æ¨¡å‹ç†è®ºç†è§£çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMsåœ¨åŒ»å­¦å›¾åƒè¯Šæ–­ä¸­å±•ç°æ½œåŠ›ã€‚</li>
<li>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å½±å“æ¨¡å‹å¯¹ç§æ—äºšç»„çš„é¢„æµ‹è¡¨ç°ã€‚</li>
<li>ICLå…è®¸æ¨¡å‹ä»æç¤ºä¸­å­¦ä¹ ç§æ—äºšç»„ç‰¹å®šçš„ç–¾ç—…åŸºç¡€æ¦‚ç‡ã€‚</li>
<li>ICLå¯¼è‡´æ¨¡å‹åœ¨ä¸åŒç§æ—äºšç»„é—´çš„é¢„æµ‹è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚</li>
<li>æœ€ä½³å®è·µåŒ…æ‹¬æ£€æŸ¥ç§æ—äºšç»„çš„è¡¨ç°ï¼ŒåŒ¹é…æ ‡ç­¾çš„åŸºç¡€æ¦‚ç‡å’Œç›®æ ‡åˆ†å¸ƒã€‚</li>
<li>éœ€è¦æ”¹è¿›å¯¹VLMsçš„ç†è®ºç†è§£ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d49fd6e115e06edd1d6a2d8869cbbfd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a688293c5788a234f741fc36d4f9f30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e15ce61d012a67e4ea6f2fab7516dd51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a94d9b44a3176484fab681bbfab95e6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Pruning-Deep-Neural-Networks-via-a-Combination-of-the-Marchenko-Pastur-Distribution-and-Regularization"><a href="#Pruning-Deep-Neural-Networks-via-a-Combination-of-the-Marchenko-Pastur-Distribution-and-Regularization" class="headerlink" title="Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur   Distribution and Regularization"></a>Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur   Distribution and Regularization</h2><p><strong>Authors:Leonid Berlyand, Theo Bourdais, Houman Owhadi, Yitzchak Shmalo</strong></p>
<p>Deep neural networks (DNNs) have brought significant advancements in various applications in recent years, such as image recognition, speech recognition, and natural language processing. In particular, Vision Transformers (ViTs) have emerged as a powerful class of models in the field of deep learning for image classification. In this work, we propose a novel Random Matrix Theory (RMT)-based method for pruning pre-trained DNNs, based on the sparsification of weights and singular vectors, and apply it to ViTs. RMT provides a robust framework to analyze the statistical properties of large matrices, which has been shown to be crucial for understanding and optimizing the performance of DNNs. We demonstrate that our RMT-based pruning can be used to reduce the number of parameters of ViT models (trained on ImageNet) by 30-50% with less than 1% loss in accuracy. To our knowledge, this represents the state-of-the-art in pruning for these ViT models. Furthermore, we provide a rigorous mathematical underpinning of the above numerical studies, namely we proved a theorem for fully connected DNNs, and other more general DNN structures, describing how the randomness in the weight matrices of a DNN decreases as the weights approach a local or global minimum (during training). We verify this theorem through numerical experiments on fully connected DNNs, providing empirical support for our theoretical findings. Moreover, we prove a theorem that describes how DNN loss decreases as we remove randomness in the weight layers, and show a monotone dependence of the decrease in loss with the amount of randomness that we remove. Our results also provide significant RMT-based insights into the role of regularization during training and pruning. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰åœ¨å„ç§åº”ç”¨ä¸­å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå¦‚å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚ç‰¹åˆ«æ˜¯ï¼ŒVision Transformersï¼ˆViTsï¼‰ä½œä¸ºæ·±åº¦å­¦ä¹ é¢†åŸŸå›¾åƒåˆ†ç±»çš„å¼ºå¤§æ¨¡å‹å·²ç»å´­éœ²å¤´è§’ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„æ–¹æ³•æ¥ä¿®å‰ªé¢„è®­ç»ƒçš„DNNsï¼Œè¯¥æ–¹æ³•åŸºäºæƒé‡å’Œå¥‡å¼‚å‘é‡çš„ç¨€ç–åŒ–ï¼Œå¹¶é€‚ç”¨äºViTsã€‚RMTæä¾›äº†ä¸€ä¸ªåˆ†æå¤§å‹çŸ©é˜µç»Ÿè®¡ç‰¹æ€§çš„ç¨³å¥æ¡†æ¶ï¼Œå·²è¢«è¯æ˜å¯¹äºç†è§£å’Œä¼˜åŒ–DNNçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒåŸºäºRMTçš„ä¿®å‰ªæ–¹æ³•å¯ç”¨äºå°†ImageNetè®­ç»ƒçš„ViTæ¨¡å‹çš„å‚æ•°æ•°é‡å‡å°‘30%~50%ï¼ŒåŒæ—¶ç²¾åº¦æŸå¤±ä¸åˆ°1%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é’ˆå¯¹è¿™äº›ViTæ¨¡å‹çš„æœ€å…ˆè¿›çš„ä¿®å‰ªæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ä¸Šè¿°æ•°å€¼ç ”ç©¶è¿›è¡Œäº†ä¸¥æ ¼çš„æ•°å­¦è¯æ˜ï¼Œå³æˆ‘ä»¬ä¸ºå…¨è¿æ¥DNNså’Œå…¶ä»–æ›´ä¸€èˆ¬çš„DNNç»“æ„è¯æ˜äº†ä¸€ä¸ªå®šç†ï¼Œæè¿°äº†DNNæƒé‡çŸ©é˜µä¸­çš„éšæœºæ€§å¦‚ä½•éšç€æƒé‡æ¥è¿‘å±€éƒ¨æˆ–å…¨å±€æœ€å°å€¼ï¼ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼‰è€Œå‡å°‘ã€‚æˆ‘ä»¬é€šè¿‡å…¨è¿æ¥DNNä¸Šçš„æ•°å€¼å®éªŒéªŒè¯äº†è¿™ä¸€å®šç†ï¼Œä¸ºæˆ‘ä»¬çš„ç†è®ºå‘ç°æä¾›äº†å®è¯æ”¯æŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†å¦ä¸€ä¸ªå®šç†ï¼Œæè¿°äº†å½“æˆ‘ä»¬æ¶ˆé™¤æƒé‡å±‚ä¸­çš„éšæœºæ€§æ—¶ï¼ŒDNNæŸå¤±å¦‚ä½•å‡å°‘ï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬æ‰€æ¶ˆé™¤çš„éšæœºæ€§ä¸æŸå¤±å‡å°‘ä¹‹é—´çš„å•è°ƒä¾èµ–æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜æä¾›äº†åŸºäºRMTçš„å…³äºè®­ç»ƒå’Œä¿®å‰ªè¿‡ç¨‹ä¸­æ­£åˆ™åŒ–ä½œç”¨çš„æ·±åˆ»è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01922v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼ŒVision Transformersï¼ˆViTsï¼‰å·²æˆä¸ºå›¾åƒåˆ†ç±»ä¸­çš„å¼ºå¤§æ¨¡å‹ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„é¢„è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰ä¿®å‰ªæ–¹æ³•ï¼Œé€šè¿‡æƒé‡å’Œå¥‡å¼‚å‘é‡çš„ç¨€ç–åŒ–åº”ç”¨äºViTsã€‚è¯¥æ–¹æ³•å¯å‡å°‘åœ¨ImageNetä¸Šè®­ç»ƒçš„ViTæ¨¡å‹çš„å‚æ•°æ•°é‡è¾¾30%~50%ï¼ŒåŒæ—¶ä¿è¯ç²¾åº¦æŸå¤±å°äº1%ã€‚æˆ‘ä»¬è¿˜ä¸ºä¸Šè¿°æ•°å€¼ç ”ç©¶æä¾›äº†ä¸¥æ ¼çš„æ•°å­¦æ”¯æ’‘ï¼ŒåŒ…æ‹¬æè¿°DNNæƒé‡çŸ©é˜µéšæœºæ€§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•å‡å°‘çš„ç†è®ºï¼Œä»¥åŠæè¿°å¦‚ä½•éšç€éšæœºæ€§çš„ç§»é™¤ï¼ŒDNNæŸå¤±å¦‚ä½•å‡å°‘çš„ç†è®ºã€‚æˆ‘ä»¬çš„ç»“æœè¿˜ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­£åˆ™åŒ–ä½œç”¨æä¾›äº†é‡è¦çš„RMTè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„é¢„è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰ä¿®å‰ªæ–¹æ³•ï¼Œå¹¶åº”ç”¨äºVision Transformersï¼ˆViTsï¼‰ã€‚</li>
<li>é€šè¿‡æ­¤æ–¹æ³•ï¼Œå¯ä»¥å‡å°‘ViTæ¨¡å‹çš„å‚æ•°æ•°é‡è¾¾30%~50%ï¼ŒåŒæ—¶ä¿è¯ç²¾åº¦æŸå¤±å°äº1%ï¼Œè¾¾åˆ°å½“å‰æœ€ä¼˜æ°´å¹³ã€‚</li>
<li>æä¾›äº†ä¸¥æ ¼çš„æ•°å­¦ç†è®ºæ”¯æ’‘ï¼ŒåŒ…æ‹¬æè¿°DNNæƒé‡çŸ©é˜µéšæœºæ€§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–å’ŒDNNæŸå¤±éšéšæœºæ€§ç§»é™¤è€Œå‡å°‘çš„ç†è®ºã€‚</li>
<li>é€šè¿‡æ•°å€¼å®éªŒéªŒè¯äº†ç†è®ºçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­£åˆ™åŒ–ä½œç”¨æä¾›äº†é‡è¦çš„è§è§£ã€‚</li>
<li>æ­¤æ–¹æ³•åŸºäºRMTï¼Œä¸ºç†è§£å¤§å‹çŸ©é˜µçš„ç»Ÿè®¡å±æ€§æä¾›äº†æ–°çš„è§†è§’ï¼Œæœ‰åŠ©äºç†è§£å’Œä¼˜åŒ–DNNçš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæœªæ¥çš„DNNå’ŒViTæ¨¡å‹çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62f9b4e1a581ec35c7b310b4d84b7e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18678ad6b7f893f31ed1928aca7f7c8c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MedUnifier-Unifying-Vision-and-Language-Pre-training-on-Medical-Data-with-Vision-Generation-Task-using-Discrete-Visual-Representations"><a href="#MedUnifier-Unifying-Vision-and-Language-Pre-training-on-Medical-Data-with-Vision-Generation-Task-using-Discrete-Visual-Representations" class="headerlink" title="MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data   with Vision Generation Task using Discrete Visual Representations"></a>MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data   with Vision Generation Task using Discrete Visual Representations</h2><p><strong>Authors:Ziyang Zhang, Yang Yu, Yucheng Chen, Xulei Yang, Si Yong Yeo</strong></p>
<p>Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content. This gap hinders the modelâ€™s ability to synthesize coherent and novel visual representations from textual prompts, thereby reducing the effectiveness of multi-modal learning. In this work, we propose MedUnifier, a unified VLP framework tailored for medical data. MedUnifier seamlessly integrates text-grounded image generation capabilities with multi-modal learning strategies, including image-text contrastive alignment, image-text matching and image-grounded text generation. Unlike traditional methods that reply on continuous visual representations, our approach employs visual vector quantization, which not only facilitates a more cohesive learning strategy for cross-modal understanding but also enhances multi-modal generation quality by effectively leveraging discrete representations. Our frameworkâ€™s effectiveness is evidenced by the experiments on established benchmarks, including uni-modal tasks (supervised fine-tuning), cross-modal tasks (image-text retrieval and zero-shot image classification), and multi-modal tasks (medical report generation, image synthesis), where it achieves state-of-the-art performance across various tasks. MedUnifier also offers a highly adaptable tool for a wide range of language and vision tasks in healthcare, marking advancement toward the development of a generalizable AI model for medical applications. </p>
<blockquote>
<p>å°½ç®¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•ä¸»è¦ä¾§é‡äºç‰¹å¾æå–å’Œè·¨æ¨¡æ€ç†è§£ï¼Œå¯¹ç”Ÿæˆæˆ–è½¬æ¢è§†è§‰å†…å®¹çš„å…³æ³¨æœ‰é™ã€‚è¿™ä¸€å·®è·é˜»ç¢äº†æ¨¡å‹ä»æ–‡æœ¬æç¤ºä¸­åˆæˆè¿è´¯ä¸”æ–°é¢–çš„è§†è§‰è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œä»è€Œé™ä½äº†å¤šæ¨¡æ€å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹åŒ»ç–—æ•°æ®çš„ç»Ÿä¸€VLPæ¡†æ¶MedUnifierã€‚MedUnifieræ— ç¼é›†æˆäº†åŸºäºæ–‡æœ¬çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ä¸å¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬å¯¹æ¯”å¯¹é½ã€å›¾åƒæ–‡æœ¬åŒ¹é…å’ŒåŸºäºå›¾åƒçš„æ–‡æœ¬ç”Ÿæˆã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ä¾èµ–è¿ç»­è§†è§‰è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è§†è§‰å‘é‡é‡åŒ–ï¼Œè¿™ä¸ä»…æœ‰åŠ©äºæ›´è¿è´¯çš„è·¨æ¨¡æ€ç†è§£å­¦ä¹ ç­–ç•¥ï¼Œè€Œä¸”é€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç¦»æ•£è¡¨ç¤ºæé«˜äº†å¤šæ¨¡æ€ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å…¬è®¤åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å•æ¨¡æ€ä»»åŠ¡ï¼ˆç›‘ç£å¾®è°ƒï¼‰ã€è·¨æ¨¡æ€ä»»åŠ¡ï¼ˆå›¾åƒæ–‡æœ¬æ£€ç´¢å’Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼ˆåŒ»å­¦æŠ¥å‘Šç”Ÿæˆã€å›¾åƒåˆæˆï¼‰ã€‚å®ƒåœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚MedUnifierè¿˜ä¸ºåŒ»ç–—ä¿å¥é¢†åŸŸçš„å„ç§è¯­è¨€å’Œè§†è§‰ä»»åŠ¡æä¾›äº†é«˜åº¦çµæ´»çš„å·¥å…·ï¼Œæ ‡å¿—ç€åœ¨å¼€å‘ç”¨äºåŒ»ç–—åº”ç”¨çš„å¯æ¨å¹¿äººå·¥æ™ºèƒ½æ¨¡å‹æ–¹é¢å–å¾—äº†è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01019v1">PDF</a> To be pubilshed in CVPR 2025</p>
<p><strong>Summary</strong>ï¼šæå‡ºäº†ä¸€ç§é’ˆå¯¹åŒ»ç–—æ•°æ®çš„ç»Ÿä¸€è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶MedUnifierã€‚è¯¥æ¡†æ¶èåˆäº†æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆèƒ½åŠ›ä¸å¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬å¯¹æ¯”å¯¹é½ã€å›¾åƒæ–‡æœ¬åŒ¹é…å’Œå›¾åƒé©±åŠ¨æ–‡æœ¬ç”Ÿæˆã€‚é€šè¿‡ç¦»æ•£è¡¨ç¤ºè€Œéè¿ç»­è§†è§‰è¡¨ç¤ºçš„æ–¹å¼ï¼Œæé«˜äº†å¤šæ¨¡æ€ç”Ÿæˆçš„è¿è´¯æ€§å’Œè´¨é‡ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰ä¸»è¦å…³æ³¨ç‰¹å¾æå–å’Œè·¨æ¨¡æ€ç†è§£ï¼Œå¿½è§†äº†å¯¹è§†è§‰å†…å®¹çš„ç”Ÿæˆæˆ–è½¬æ¢ã€‚</li>
<li>MedUnifieræ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ•´åˆæ–‡æœ¬é©±åŠ¨çš„å›¾åƒç”Ÿæˆå’Œå¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼Œæé«˜æ¨¡å‹åˆæˆè¿è´¯å’Œæ–°é¢–çš„è§†è§‰è¡¨ç¤ºçš„èƒ½åŠ›ã€‚</li>
<li>MedUnifieré‡‡ç”¨è§†è§‰å‘é‡é‡åŒ–ï¼Œæœ‰æ•ˆåˆ©ç”¨ç¦»æ•£è¡¨ç¤ºï¼Œæé«˜å¤šæ¨¡æ€ç”Ÿæˆçš„è¿è´¯æ€§å’Œè´¨é‡ã€‚</li>
<li>æ¡†æ¶åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å•æ¨¡æ€ä»»åŠ¡ã€è·¨æ¨¡æ€ä»»åŠ¡å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>MedUnifieræ¡†æ¶é«˜åº¦é€‚åº”äºå„ç§è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ï¼Œç‰¹åˆ«é€‚ç”¨äºåŒ»ç–—å¥åº·é¢†åŸŸã€‚</li>
<li>æ¡†æ¶çš„å‘å±•ä¸ºåŒ»ç–—åº”ç”¨çš„é€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹çš„å‘å±•å¸¦æ¥äº†è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8fc059427ceee98cd0731f3730a76bb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-166e1f719fc8616a59f877d53d7d8886.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5371ea9168a0ba20e591780e5157317c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evaluating-and-Predicting-Distorted-Human-Body-Parts-for-Generated-Images"><a href="#Evaluating-and-Predicting-Distorted-Human-Body-Parts-for-Generated-Images" class="headerlink" title="Evaluating and Predicting Distorted Human Body Parts for Generated   Images"></a>Evaluating and Predicting Distorted Human Body Parts for Generated   Images</h2><p><strong>Authors:Lu Ma, Kaibo Cao, Hao Liang, Jiaxin Lin, Zhuang Li, Yuhong Liu, Jihong Zhang, Wentao Zhang, Bin Cui</strong></p>
<p>Recent advancements in text-to-image (T2I) models enable high-quality image synthesis, yet generating anatomically accurate human figures remains challenging. AI-generated images frequently exhibit distortions such as proliferated limbs, missing fingers, deformed extremities, or fused body parts. Existing evaluation metrics like Inception Score (IS) and Fr&#39;echet Inception Distance (FID) lack the granularity to detect these distortions, while human preference-based metrics focus on abstract quality assessments rather than anatomical fidelity. To address this gap, we establish the first standards for identifying human body distortions in AI-generated images and introduce Distortion-5K, a comprehensive dataset comprising 4,700 annotated images of normal and malformed human figures across diverse styles and distortion types. Based on this dataset, we propose ViT-HD, a Vision Transformer-based model tailored for detecting human body distortions in AI-generated images, which outperforms state-of-the-art segmentation models and visual language models, achieving an F1 score of 0.899 and IoU of 0.831 on distortion localization. Additionally, we construct the Human Distortion Benchmark with 500 human-centric prompts to evaluate four popular T2I models using trained ViT-HD, revealing that nearly 50% of generated images contain distortions. This work pioneers a systematic approach to evaluating anatomical accuracy in AI-generated humans, offering tools to advance the fidelity of T2I models and their real-world applicability. The Distortion-5K dataset, trained ViT-HD will soon be released in our GitHub repository: \href{<a target="_blank" rel="noopener" href="https://github.com/TheRoadQaQ/Predicting-Distortion%7D%7Bhttps://github.com/TheRoadQaQ/Predicting-Distortion%7D">https://github.com/TheRoadQaQ/Predicting-Distortion}{https://github.com/TheRoadQaQ/Predicting-Distortion}</a>. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„è¿›å±•ä½¿å¾—é«˜è´¨é‡å›¾åƒåˆæˆæˆä¸ºå¯èƒ½ï¼Œä½†ç”Ÿæˆè§£å‰–ä¸Šå‡†ç¡®çš„äººä½“å›¾åƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚AIç”Ÿæˆçš„å›¾åƒç»å¸¸å‡ºç°è‚¢ä½“å¢ç”Ÿã€æ‰‹æŒ‡ç¼ºå¤±ã€æç«¯éƒ¨ä½å˜å½¢æˆ–èº«ä½“éƒ¨ä½èåˆç­‰å¤±çœŸç°è±¡ã€‚ç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚Inception Scoreï¼ˆISï¼‰å’ŒFrÃ©chet Inception Distanceï¼ˆFIDï¼‰ï¼Œç¼ºä¹æ£€æµ‹è¿™äº›å¤±çœŸçš„ç²’åº¦ï¼Œè€ŒåŸºäºäººç±»åå¥½çš„åº¦é‡æ ‡å‡†åˆ™æ›´æ³¨é‡æŠ½è±¡è´¨é‡è¯„ä¼°ï¼Œè€Œéè§£å‰–å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬åˆ¶å®šäº†è¯†åˆ«AIç”Ÿæˆå›¾åƒä¸­äººä½“å¤±çœŸçš„ç¬¬ä¸€é¡¹æ ‡å‡†ï¼Œå¹¶æ¨å‡ºäº†Distortion-5Kï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«4700å¼ æ­£å¸¸å’Œç•¸å½¢äººä½“å›¾åƒçš„ç»¼åˆæ•°æ®é›†ï¼Œå›¾åƒè·¨è¶Šå¤šç§é£æ ¼å’Œå¤±çœŸç±»å‹å¹¶å·²è¿›è¡Œæ ‡æ³¨ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºVision Transformerçš„ViT-HDæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸“ä¸ºæ£€æµ‹AIç”Ÿæˆå›¾åƒä¸­çš„äººä½“å¤±çœŸè€Œè®¾è®¡ã€‚åœ¨å¤±çœŸå®šä½æ–¹é¢ï¼Œå®ƒè¶…è¶Šäº†æœ€æ–°çš„åˆ†å‰²æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå–å¾—äº†F1åˆ†æ•°ä¸º0.899å’ŒIoUä¸º0.831çš„æˆç»©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä»¥500ä¸ªäººç±»ä¸ºä¸­å¿ƒçš„æç¤ºè¯­ä¸ºåŸºç¡€çš„äººç±»å¤±çœŸåŸºå‡†æµ‹è¯•é›†ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„ViT-HDè¯„ä¼°äº†å››ç§æµè¡Œçš„T2Iæ¨¡å‹ï¼Œå‘ç°è¿‘50%çš„ç”Ÿæˆå›¾åƒå­˜åœ¨å¤±çœŸã€‚è¿™é¡¹å·¥ä½œå¼€åˆ›äº†ä¸€ç§ç³»ç»Ÿè¯„ä¼°AIç”Ÿæˆäººä½“è§£å‰–å‡†ç¡®æ€§çš„æ–¹æ³•ï¼Œä¸ºæ”¹è¿›T2Iæ¨¡å‹çš„ä¿çœŸåº¦å’Œå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é€‚ç”¨æ€§æä¾›äº†å·¥å…·ã€‚Distortion-5Kæ•°æ®é›†å’Œè®­ç»ƒå¥½çš„ViT-HDæ¨¡å‹å°†å¾ˆå¿«åœ¨æˆ‘ä»¬çš„GitHubä»“åº“ä¸­å‘å¸ƒï¼š[<a target="_blank" rel="noopener" href="https://github.com/TheRoadQaQ/Predicting-Distortion]%E3%80%82">https://github.com/TheRoadQaQ/Predicting-Distortion]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00811v1">PDF</a> 8 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨äºäººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒä¸­çš„äººä½“å¤±çœŸé—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æŒ‡æ ‡æ— æ³•æœ‰æ•ˆæ£€æµ‹äººä½“ç»“æ„å¤±çœŸçš„é—®é¢˜ï¼Œå»ºç«‹äº†é¦–ä¸ªé’ˆå¯¹AIç”Ÿæˆå›¾åƒä¸­äººä½“å¤±çœŸçš„æ ‡å‡†ï¼Œå¹¶å¼•å…¥äº†Distortion-5Kæ•°æ®é›†ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæå‡ºäº†é’ˆå¯¹AIç”Ÿæˆå›¾åƒä¸­äººä½“å¤±çœŸçš„æ£€æµ‹æ¨¡å‹ViT-HDï¼Œè¯¥æ¨¡å‹åœ¨å¤±çœŸå®šä½æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†äººç±»å¤±çœŸåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰T2Iæ¨¡å‹ä¸­äººä½“å¤±çœŸçš„æ™®éå­˜åœ¨ã€‚æœ¬æ–‡ä¸ºè¯„ä¼°AIç”Ÿæˆäººç±»çš„è§£å‰–å­¦å‡†ç¡®æ€§æä¾›äº†ç³»ç»Ÿæ–¹æ³•ï¼Œæœ‰åŠ©äºæé«˜T2Iæ¨¡å‹çš„é€¼çœŸåº¦å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆå›¾åƒä¸­äººä½“å¤±çœŸæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è‚¢ä½“å¢ç”Ÿã€æ‰‹æŒ‡ç¼ºå¤±ã€æç«¯éƒ¨ä½å˜å½¢ã€èº«ä½“éƒ¨ä½èåˆç­‰é—®é¢˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡å¦‚Inception Scoreå’ŒFrÃ©chet Inception Distanceæ— æ³•ç»†è‡´æ£€æµ‹è¿™äº›å¤±çœŸã€‚</li>
<li>å»ºç«‹äº†é¦–ä¸ªé’ˆå¯¹AIç”Ÿæˆå›¾åƒä¸­äººä½“å¤±çœŸçš„æ ‡å‡†ã€‚</li>
<li>å¼•å…¥äº†Distortion-5Kæ•°æ®é›†ï¼ŒåŒ…å«4700å¼ æ­£å¸¸å’Œç•¸å½¢äººç±»å›¾åƒçš„æ ‡æ³¨ã€‚</li>
<li>æå‡ºäº†åŸºäºVision Transformerçš„ViT-HDæ¨¡å‹ï¼Œç”¨äºæ£€æµ‹AIç”Ÿæˆå›¾åƒä¸­çš„äººä½“å¤±çœŸï¼Œæ€§èƒ½å“è¶Šã€‚</li>
<li>æ„å»ºäº†äººç±»å¤±çœŸåŸºå‡†æµ‹è¯•ï¼Œå‘ç°è¿‘50%çš„ç”Ÿæˆå›¾åƒå­˜åœ¨å¤±çœŸã€‚</li>
<li>è¯¥å·¥ä½œä¸ºè¯„ä¼°AIç”Ÿæˆäººç±»çš„è§£å‰–å­¦å‡†ç¡®æ€§æä¾›äº†ç³»ç»Ÿæ–¹æ³•ï¼Œæœ‰åŠ©äºæå‡T2Iæ¨¡å‹çš„é€¼çœŸåº¦å’Œå®é™…åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-862fe269afcae4236e1b4bf9675f7ff4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cce911438a584a6d734b1d9398cc6a7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3748310b8de6365b3c35b967a8569225.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b159c55fe9046ffbf9eef874386360a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-895302d35d67dad149bc3f646c958ee9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PRISM-High-Resolution-Precise-Counterfactual-Medical-Image-Generation-using-Language-guided-Stable-Diffusion"><a href="#PRISM-High-Resolution-Precise-Counterfactual-Medical-Image-Generation-using-Language-guided-Stable-Diffusion" class="headerlink" title="PRISM: High-Resolution &amp; Precise Counterfactual Medical Image Generation   using Language-guided Stable Diffusion"></a>PRISM: High-Resolution &amp; Precise Counterfactual Medical Image Generation   using Language-guided Stable Diffusion</h2><p><strong>Authors:Amar Kumar, Anita Kriz, Mohammad Havaei, Tal Arbel</strong></p>
<p>Developing reliable and generalizable deep learning systems for medical imaging faces significant obstacles due to spurious correlations, data imbalances, and limited text annotations in datasets. Addressing these challenges requires architectures robust to the unique complexities posed by medical imaging data. The rapid advancements in vision-language foundation models within the natural image domain prompt the question of how they can be adapted for medical imaging tasks. In this work, we present PRISM, a framework that leverages foundation models to generate high-resolution, language-guided medical image counterfactuals using Stable Diffusion. Our approach demonstrates unprecedented precision in selectively modifying spurious correlations (the medical devices) and disease features, enabling the removal and addition of specific attributes while preserving other image characteristics. Through extensive evaluation, we show how PRISM advances counterfactual generation and enables the development of more robust downstream classifiers for clinically deployable solutions. To facilitate broader adoption and research, we make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/Amarkr1/PRISM">https://github.com/Amarkr1/PRISM</a>. </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦å½±åƒå¼€å‘å¯é ä¸”é€šç”¨çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿé¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ•°æ®é›†ä¸­çš„è™šå‡å…³è”ã€æ•°æ®ä¸å¹³è¡¡å’Œæ–‡æœ¬æ³¨é‡Šæœ‰é™ç­‰é—®é¢˜ã€‚è¦è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œéœ€è¦é’ˆå¯¹åŒ»å­¦å½±åƒæ•°æ®å¸¦æ¥çš„ç‹¬ç‰¹å¤æ‚æ€§æ„å»ºç¨³å¥çš„æ¶æ„ã€‚è‡ªç„¶å›¾åƒé¢†åŸŸä¸­çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•å°†å…¶é€‚åº”åŒ»å­¦å½±åƒä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PRISMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨Stable Diffusionç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„è¯­è¨€å¼•å¯¼åŒ»å­¦å½±åƒåäº‹å®ã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†åœ¨é€‰æ‹©æ€§ä¿®æ”¹è™šå‡å…³è”ï¼ˆåŒ»ç–—è®¾å¤‡ï¼‰å’Œç–¾ç—…ç‰¹å¾æ–¹é¢å…·æœ‰å‰æ‰€æœªæœ‰çš„ç²¾åº¦ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™å…¶ä»–å›¾åƒç‰¹å¾çš„åŒæ—¶åˆ é™¤å’Œæ·»åŠ ç‰¹å®šå±æ€§ã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†PRISMå¦‚ä½•æ¨åŠ¨åäº‹å®ç”Ÿæˆçš„å‘å±•ï¼Œå¹¶ä½¿å¾—å¼€å‘æ›´ç¨³å¥çš„ä¸‹æ¸¸åˆ†ç±»å™¨ç”¨äºä¸´åºŠéƒ¨ç½²è§£å†³æ–¹æ¡ˆæˆä¸ºå¯èƒ½ã€‚ä¸ºäº†ä¾¿äºæ›´å¹¿æ³›çš„é‡‡ç”¨å’Œç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Amarkr1/PRISM">https://github.com/Amarkr1/PRISM</a>ä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00196v1">PDF</a> Under Review for MIDL 2025</p>
<p><strong>Summary</strong><br>     è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªåä¸ºPRISMçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„è¯­è¨€å¼•å¯¼åŒ»å­¦å›¾åƒåäº‹å®ï¼Œä½¿ç”¨Stable Diffusionè§£å†³åŒ»å­¦æˆåƒæ•°æ®ä¸­çš„å¤æ‚é—®é¢˜ã€‚PRISMèƒ½å¤Ÿç²¾å‡†åœ°é€‰æ‹©æ€§ä¿®æ”¹åŒ»å­¦å›¾åƒä¸­çš„æ— å…³ç‰¹å¾å’Œç–¾ç—…ç‰¹å¾ï¼Œä½¿å¾—ç‰¹å®šå±æ€§å¯ä»¥è¢«ç§»é™¤å’Œæ·»åŠ è€Œå…¶ä»–å›¾åƒç‰¹å¾ä¿æŒä¸å˜ã€‚å®ƒæœ‰åŠ©äºå¼€å‘æ›´ç¨³å¥çš„ä¸‹æ¸¸åˆ†ç±»å™¨ä»¥å®ç°ä¸´åºŠéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PRISMæ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹è§£å†³åŒ»å­¦æˆåƒæ•°æ®ä¸­çš„å¤æ‚é—®é¢˜ï¼ŒåŒ…æ‹¬æ— å…³ç‰¹å¾å’Œç–¾ç—…ç‰¹å¾çš„å»é™¤å’Œæ·»åŠ ã€‚</li>
<li>PRISMé€šè¿‡ä½¿ç”¨Stable Diffusionç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„è¯­è¨€å¼•å¯¼åŒ»å­¦å›¾åƒåäº‹å®ã€‚</li>
<li>PRISMåœ¨åäº‹å®ç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œæœ‰åŠ©äºå¼€å‘æ›´ç¨³å¥çš„ä¸‹æ¸¸åˆ†ç±»å™¨ä»¥å®ç°ä¸´åºŠéƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04dae0498b203c0b9df097bfaf49e45b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f91729dbb831971a8f62fd3f1ab6c0da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a44f50a53eda689dd0f401684ffa8bdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e39f1904c80cc6822d2def7e029ceaf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7f6d825d0701082ffabc72abfc6b636.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Prompt-driven-Transferable-Adversarial-Attack-on-Person-Re-Identification-with-Attribute-aware-Textual-Inversion"><a href="#Prompt-driven-Transferable-Adversarial-Attack-on-Person-Re-Identification-with-Attribute-aware-Textual-Inversion" class="headerlink" title="Prompt-driven Transferable Adversarial Attack on Person   Re-Identification with Attribute-aware Textual Inversion"></a>Prompt-driven Transferable Adversarial Attack on Person   Re-Identification with Attribute-aware Textual Inversion</h2><p><strong>Authors:Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yaonan Wang</strong></p>
<p>Person re-identification (re-id) models are vital in security surveillance systems, requiring transferable adversarial attacks to explore the vulnerabilities of them. Recently, vision-language models (VLM) based attacks have shown superior transferability by attacking generalized image and textual features of VLM, but they lack comprehensive feature disruption due to the overemphasis on discriminative semantics in integral representation. In this paper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novel method that leverages VLMâ€™s image-text alignment capability to explicitly disrupt fine-grained semantic features of pedestrian images by destroying attribute-specific textual embeddings. To obtain personalized textual descriptions for individual attributes, textual inversion networks are designed to map pedestrian images to pseudo tokens that represent semantic embeddings, trained in the contrastive learning manner with images and a predefined prompt template that explicitly describes the pedestrian attributes. Inverted benign and adversarial fine-grained textual semantics facilitate attacker in effectively conducting thorough disruptions, enhancing the transferability of adversarial examples. Extensive experiments show that AP-Attack achieves state-of-the-art transferability, significantly outperforming previous methods by 22.9% on mean Drop Rate in cross-model&amp;dataset attack scenarios. </p>
<blockquote>
<p>è¡Œäººå†è¯†åˆ«ï¼ˆRe-IDï¼‰æ¨¡å‹åœ¨å®‰å…¨ç›‘æ§ç³»ç»Ÿä¸­çš„ä½œç”¨è‡³å…³é‡è¦ï¼Œéœ€è¦å¯è¿ç§»çš„å¯¹æŠ—æ€§æ”»å‡»æ¥æ¢ç´¢å…¶æ¼æ´ã€‚æœ€è¿‘ï¼ŒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ”»å‡»è¡¨ç°å‡ºé€šè¿‡æ”»å‡»VLMçš„é€šç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾è€Œå…·æœ‰çš„å‡ºè‰²å¯è¿ç§»æ€§ï¼Œä½†ç”±äºè¿‡åˆ†å¼ºè°ƒæ•´ä½“è¡¨ç¤ºä¸­çš„åˆ¤åˆ«è¯­ä¹‰ï¼Œå®ƒä»¬ç¼ºä¹å…¨é¢çš„ç‰¹å¾ç ´åã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å±æ€§æ„ŸçŸ¥æç¤ºæ”»å‡»ï¼ˆAP-Attackï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨VLMçš„å›¾åƒæ–‡æœ¬å¯¹é½èƒ½åŠ›æ¥æ˜ç¡®ç ´åè¡Œäººå›¾åƒçš„ç»†ç²’åº¦è¯­ä¹‰ç‰¹å¾çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡ç ´åç‰¹å®šå±æ€§çš„æ–‡æœ¬åµŒå…¥æ¥å®ç°ã€‚ä¸ºäº†è·å¾—é’ˆå¯¹å„ä¸ªå±æ€§çš„ä¸ªæ€§åŒ–æ–‡æœ¬æè¿°ï¼Œè®¾è®¡æ–‡æœ¬åè½¬ç½‘ç»œå°†è¡Œäººå›¾åƒæ˜ å°„åˆ°ä»£è¡¨è¯­ä¹‰åµŒå…¥çš„ä¼ªä»¤ç‰Œï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ–¹å¼ä¸å›¾åƒå’Œé¢„å…ˆå®šä¹‰çš„æ˜ç¡®æè¿°è¡Œäººå±æ€§çš„æç¤ºæ¨¡æ¿è¿›è¡Œè®­ç»ƒã€‚åè½¬çš„è‰¯æ€§å’Œå¯¹æŠ—æ€§çš„ç»†ç²’åº¦æ–‡æœ¬è¯­ä¹‰æœ‰åŠ©äºæ”»å‡»è€…è¿›è¡Œæœ‰æ•ˆçš„å…¨é¢ç ´åï¼Œæé«˜å¯¹æŠ—æ ·æœ¬çš„å¯è¿ç§»æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAP-Attackåœ¨å¯è¿ç§»æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨è·¨æ¨¡å‹å’Œæ•°æ®é›†æ”»å‡»åœºæ™¯ä¸­ï¼Œå¹³å‡ä¸‹é™ç‡æé«˜äº†22.9%ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19697v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºå±æ€§æ„ŸçŸ¥æç¤ºæ”»å‡»ï¼ˆAP-Attackï¼‰çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å›¾åƒæ–‡æœ¬å¯¹é½èƒ½åŠ›ï¼Œé€šè¿‡ç ´åè¡Œäººå›¾åƒçš„ç»†ç²’åº¦è¯­ä¹‰ç‰¹å¾æ¥è¿›è¡Œæ”»å‡»ã€‚è¯¥æ–¹æ³•è®¾è®¡æ–‡æœ¬åæ¼”ç½‘ç»œï¼Œå°†è¡Œäººå›¾åƒæ˜ å°„åˆ°ä»£è¡¨è¯­ä¹‰åµŒå…¥çš„ä¼ªä»¤ç‰Œä¸Šï¼Œé€šè¿‡ä¸å›¾åƒå’Œé¢„å®šä¹‰æç¤ºæ¨¡æ¿è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼Œæ˜ç¡®æè¿°è¡Œäººå±æ€§ã€‚è¿™ç§æ”»å‡»æ–¹æ³•èƒ½æœ‰æ•ˆå…¨é¢ç ´åç»†ç²’åº¦æ–‡æœ¬è¯­ä¹‰ï¼Œæé«˜å¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§ï¼Œå®ç°è·¨æ¨¡å‹å’Œæ•°æ®é›†æ”»å‡»åœºæ™¯ä¸‹çš„æœ€ä½³è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å±æ€§æ„ŸçŸ¥æç¤ºæ”»å‡»ï¼ˆAP-Attackï¼‰åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å›¾åƒæ–‡æœ¬å¯¹é½èƒ½åŠ›ï¼Œé’ˆå¯¹è¡Œäººå›¾åƒçš„ç»†ç²’åº¦è¯­ä¹‰ç‰¹å¾è¿›è¡Œæ”»å‡»ã€‚</li>
<li>é€šè¿‡è®¾è®¡æ–‡æœ¬åæ¼”ç½‘ç»œï¼Œå°†è¡Œäººå›¾åƒæ˜ å°„åˆ°è¯­ä¹‰åµŒå…¥çš„ä¼ªä»¤ç‰Œä¸Šã€‚</li>
<li>ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ–¹å¼è®­ç»ƒæ–‡æœ¬åæ¼”ç½‘ç»œï¼Œç»“åˆå›¾åƒå’Œé¢„å®šä¹‰çš„æç¤ºæ¨¡æ¿æè¿°è¡Œäººå±æ€§ã€‚</li>
<li>AP-Attackèƒ½æœ‰æ•ˆå…¨é¢ç ´åç»†ç²’åº¦æ–‡æœ¬è¯­ä¹‰ï¼Œå¢å¼ºå¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è·¨æ¨¡å‹å’Œè·¨æ•°æ®é›†æ”»å‡»åœºæ™¯ä¸‹è¡¨ç°å‡ºæœ€ä½³è¿ç§»æ€§ï¼Œè¾ƒä¹‹å‰çš„æ–¹æ³•å¹³å‡ä¸‹é™ç‡æé«˜äº†22.9%ã€‚</li>
<li>AP-Attacké’ˆå¯¹çš„æ˜¯äººé‡æ–°è¯†åˆ«ï¼ˆre-idï¼‰æ¨¡å‹çš„è„†å¼±æ€§ï¼Œå¯¹å®‰å…¨ç›‘æ§ç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57370d022a32132d515946ab5b20b6fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b01d53b807e3f4098bff26fb26f0923c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d32cfae29035c95d519d98f9c88d069e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03fad1fbee06a91e623de69cdaead9f6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SelaFD-Seamless-Adaptation-of-Vision-Transformer-Fine-tuning-for-Radar-based-Human-Activity-Recognition"><a href="#SelaFD-Seamless-Adaptation-of-Vision-Transformer-Fine-tuning-for-Radar-based-Human-Activity-Recognition" class="headerlink" title="SelaFD:Seamless Adaptation of Vision Transformer Fine-tuning for   Radar-based Human Activity Recognition"></a>SelaFD:Seamless Adaptation of Vision Transformer Fine-tuning for   Radar-based Human Activity Recognition</h2><p><strong>Authors:Yijun Wang, Yong Wang, Chendong xu, Shuai Yao, Qisong Wu</strong></p>
<p>Human Activity Recognition (HAR) such as fall detection has become increasingly critical due to the aging population, necessitating effective monitoring systems to prevent serious injuries and fatalities associated with falls. This study focuses on fine-tuning the Vision Transformer (ViT) model specifically for HAR using radar-based Time-Doppler signatures. Unlike traditional image datasets, these signals present unique challenges due to their non-visual nature and the high degree of similarity among various activities. Directly fine-tuning the ViT with all parameters proves suboptimal for this application. To address this challenge, we propose a novel approach that employs Low-Rank Adaptation (LoRA) fine-tuning in the weight space to facilitate knowledge transfer from pre-trained ViT models. Additionally, to extract fine-grained features, we enhance feature representation through the integration of a serial-parallel adapter in the feature space. Our innovative joint fine-tuning method, tailored for radar-based Time-Doppler signatures, significantly improves HAR accuracy, surpassing existing state-of-the-art methodologies in this domain. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/wangyijunlyy/SelaFD">https://github.com/wangyijunlyy/SelaFD</a>. </p>
<blockquote>
<p>äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰å¦‚è·Œå€’æ£€æµ‹ï¼Œç”±äºäººå£è€é¾„åŒ–è€Œå˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œéœ€è¦æœ‰æ•ˆçš„ç›‘æ§ç³»ç»Ÿæ¥é˜²æ­¢ä¸è·Œå€’ç›¸å…³çš„ä¸¥é‡ä¼¤å®³å’Œæ­»äº¡ã€‚æœ¬ç ”ç©¶ä¸“æ³¨äºä½¿ç”¨åŸºäºé›·è¾¾çš„Time-Dopplerç‰¹å¾å¯¹Vision Transformerï¼ˆViTï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åº”ç”¨äºHARã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒæ•°æ®é›†ä¸åŒï¼Œè¿™äº›ä¿¡å·ç”±äºå…¶éè§†è§‰æ€§è´¨å’Œå„ç§æ´»åŠ¨ä¹‹é—´çš„é«˜åº¦ç›¸ä¼¼æ€§è€Œå…·æœ‰ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ç›´æ¥ä½¿ç”¨æ‰€æœ‰å‚æ•°å¯¹ViTè¿›è¡Œå¾®è°ƒå¯¹äºæ­¤åº”ç”¨è€Œè¨€è¯æ˜æ˜¯æ¬¡ä¼˜çš„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡‡ç”¨æƒé‡ç©ºé—´ä¸­çš„ä½ç§©é€‚é…ï¼ˆLoRAï¼‰å¾®è°ƒçš„æ–°æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›ä»é¢„è®­ç»ƒçš„ViTæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»ã€‚æ­¤å¤–ï¼Œä¸ºäº†æå–ç»†ç²’åº¦ç‰¹å¾ï¼Œæˆ‘ä»¬é€šè¿‡ç‰¹å¾ç©ºé—´ä¸­ä¸²è¡Œå¹¶è¡Œé€‚é…å™¨çš„é›†æˆå¢å¼ºäº†ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬é’ˆå¯¹åŸºäºé›·è¾¾çš„Time-Dopplerç‰¹å¾é‡èº«å®šåˆ¶çš„åˆ›æ–°è”åˆå¾®è°ƒæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†HARçš„å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†è¯¥é¢†åŸŸçš„ç°æœ‰æœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/wangyijunlyy/SelaFD%E3%80%82">https://github.com/wangyijunlyy/SelaFDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04740v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é›·è¾¾åŸºäºæ—¶é—´çš„Dopplerç­¾åå¯¹äºäººä½“æ´»åŠ¨è¯†åˆ«è‡³å…³é‡è¦ï¼Œè¯¥ç ”ç©¶ä½¿ç”¨ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æŠ€æœ¯å¯¹é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡ä¸²è¡Œå¹¶è¡Œé€‚é…å™¨å¼ºåŒ–ç‰¹å¾è¡¨ç¤ºæ¥æå‡äººä½“æ´»åŠ¨è¯†åˆ«çš„ç²¾ç¡®åº¦ã€‚ç ”ç©¶æˆæœæœ‰æ•ˆæå‡äº†åŸºäºé›·è¾¾Time-Dopplerä¿¡å·çš„HARå‡†ç¡®åº¦ï¼Œè¶…è¶Šå½“å‰é¢†åŸŸæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨é›·è¾¾åŸºäºæ—¶é—´çš„Dopplerç­¾ååœ¨äººä½“æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰ä¸­çš„åº”ç”¨ã€‚</li>
<li>ç›´æ¥ä½¿ç”¨æ‰€æœ‰å‚æ•°å¾®è°ƒViTæ¨¡å‹å¯¹äºæ­¤åº”ç”¨æ˜¯æ¬¡ä¼˜çš„ã€‚</li>
<li>æå‡ºä½¿ç”¨ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œåœ¨æƒé‡ç©ºé—´ä¿ƒè¿›çŸ¥è¯†ä»é¢„è®­ç»ƒViTæ¨¡å‹ä¸­è¿ç§»ã€‚</li>
<li>é€šè¿‡é›†æˆä¸²è¡Œå¹¶è¡Œé€‚é…å™¨å¼ºåŒ–ç‰¹å¾è¡¨ç¤ºï¼Œä»¥æå–ç²¾ç»†ç‰¹å¾ã€‚</li>
<li>é’ˆå¯¹é›·è¾¾åŸºäºæ—¶é—´çš„Dopplerç­¾åå®šåˆ¶çš„åˆ›æ–°è”åˆå¾®è°ƒæ–¹æ³•æ˜¾è‘—æé«˜äº†HARçš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†å½“å‰é¢†åŸŸæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa5c702f3be209ee712615383aaf273d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9bf4640e033cb9212e1cb8d7bc8c4c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85bec52ce95a6c25acd884c0846b31d5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0f62feb54753660c568230916d6adc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b07d202ba633f05cf56cef356d009ae1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-543e5f499e71ac4b93610eba1e419ff9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Locality-Alignment-Improves-Vision-Language-Models"><a href="#Locality-Alignment-Improves-Vision-Language-Models" class="headerlink" title="Locality Alignment Improves Vision-Language Models"></a>Locality Alignment Improves Vision-Language Models</h2><p><strong>Authors:Ian Covert, Tony Sun, James Zou, Tatsunori Hashimoto</strong></p>
<p>Vision language models (VLMs) have seen growing adoption in recent years, but many still struggle with basic spatial reasoning errors. We hypothesize that this is due to VLMs adopting pre-trained vision backbones, specifically vision transformers (ViTs) trained with image-level supervision and minimal inductive biases. Such models may fail to encode the class contents at each position in the image, and our goal is to resolve this with a vision backbone that effectively captures both local and global image semantics. Our main insight is that we do not require new supervision to learn this capability - pre-trained models contain significant knowledge of local semantics that we can extract and use for scalable self-supervision. We propose a new efficient post-training stage for ViTs called locality alignment and a novel fine-tuning procedure called MaskEmbed that uses a masked reconstruction loss to learn semantic contributions for each image patch. We first evaluate locality alignment with a vision-only benchmark, finding that it improves a modelâ€™s performance at patch-level semantic segmentation, especially for strong backbones trained with image-caption pairs (e.g., CLIP and SigLIP). We then train a series of VLMs with and without locality alignment, and show that locality-aligned backbones improve performance across a range of benchmarks, particularly ones that involve spatial understanding (e.g., RefCOCO, OCID-Ref, TallyQA, VSR, AI2D). Overall, we demonstrate that we can efficiently learn local semantic extraction via a locality alignment stage, and that this procedure benefits VLM training recipes that use off-the-shelf vision backbones. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿‘å¹´æ¥å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„åº”ç”¨ï¼Œä½†è®¸å¤šæ¨¡å‹ä»é¢ä¸´åŸºæœ¬çš„ç©ºé—´æ¨ç†é”™è¯¯é—®é¢˜ã€‚æˆ‘ä»¬å‡è®¾è¿™æ˜¯ç”±äºVLMsé‡‡ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰ä¸»å¹²ç½‘ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å›¾åƒçº§ç›‘ç£å’Œå°‘é‡å½’çº³åç½®è®­ç»ƒçš„è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰ã€‚è¿™ç§æ¨¡å‹å¯èƒ½æ— æ³•ç¼–ç å›¾åƒä¸­æ¯ä¸ªä½ç½®çš„ç±»åˆ«å†…å®¹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡æœ‰æ•ˆåœ°æ•æ‰å±€éƒ¨å’Œå…¨å±€å›¾åƒè¯­ä¹‰çš„è§†è§‰ä¸»å¹²ç½‘æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ–°çš„ç›‘ç£æ¥å­¦ä¹ è¿™ç§èƒ½åŠ›â€”â€”é¢„è®­ç»ƒæ¨¡å‹åŒ…å«å¤§é‡çš„å±€éƒ¨è¯­ä¹‰çŸ¥è¯†ï¼Œæˆ‘ä»¬å¯ä»¥æå–å¹¶ç”¨äºå¯æ‰©å±•çš„è‡ªæˆ‘ç›‘ç£ã€‚æˆ‘ä»¬ä¸ºViTsæå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆåè®­ç»ƒé˜¶æ®µï¼Œç§°ä¸ºå±€éƒ¨å¯¹é½ï¼Œä»¥åŠä¸€ç§ä½¿ç”¨é®ç½©é‡å»ºæŸå¤±æ¥å­¦ä¹ æ¯ä¸ªå›¾åƒè¡¥ä¸è¯­ä¹‰è´¡çŒ®çš„æ–°å¾®è°ƒç¨‹åºï¼Œç§°ä¸ºMaskEmbedã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä»…è§†è§‰çš„åŸºå‡†æµ‹è¯•è¯„ä¼°å±€éƒ¨å¯¹é½ï¼Œå‘ç°å®ƒåœ¨è¡¥ä¸çº§è¯­ä¹‰åˆ†å‰²æ–¹é¢æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä½¿ç”¨å›¾åƒæ ‡é¢˜å¯¹è®­ç»ƒçš„å¼ºä¸»å¹²ç½‘ï¼ˆä¾‹å¦‚CLIPå’ŒSigLIPï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç³»åˆ—å¸¦æœ‰å’Œä¸å¸¦æœ‰å±€éƒ¨å¯¹é½çš„VLMsï¼Œå¹¶æ˜¾ç¤ºå±€éƒ¨å¯¹é½çš„ä¸»å¹²ç½‘åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠç©ºé—´ç†è§£çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚RefCOCOã€OCID-Refã€TallyQAã€VSRã€AI2Dï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬å¯ä»¥é€šè¿‡å±€éƒ¨å¯¹é½é˜¶æ®µæœ‰æ•ˆåœ°å­¦ä¹ å±€éƒ¨è¯­ä¹‰æå–ï¼Œå¹¶ä¸”è¯¥ç¨‹åºæœ‰åˆ©äºä½¿ç”¨ç°æˆçš„è§†è§‰ä¸»å¹²ç½‘çš„VLMè®­ç»ƒé…æ–¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11087v2">PDF</a> ICLR 2025 Camera-Ready</p>
<p><strong>Summary</strong><br>     è¿‘æœŸè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åº”ç”¨é€æ¸å¢å¤šï¼Œä½†è®¸å¤šæ¨¡å‹åœ¨åŸºæœ¬ç©ºé—´æ¨ç†æ–¹é¢ä»å­˜åœ¨é”™è¯¯ã€‚ç ”ç©¶è®¤ä¸ºè¿™æ˜¯å› ä¸ºVLMé‡‡ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰ä¸»å¹²ï¼Œç‰¹åˆ«æ˜¯ä»¥å›¾åƒçº§ç›‘ç£è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œç¼ºä¹å½’çº³åè§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆåè®­ç»ƒé˜¶æ®µâ€”â€”å±€éƒ¨å¯¹é½ï¼Œä»¥åŠä¸€ç§ä½¿ç”¨æ©ç é‡å»ºæŸå¤±å­¦ä¹ å›¾åƒè¡¥ä¸è¯­ä¹‰è´¡çŒ®çš„æ–°å¾®è°ƒç¨‹åºâ€”â€”MaskEmbedã€‚å®éªŒè¡¨æ˜ï¼Œå±€éƒ¨å¯¹é½èƒ½æé«˜æ¨¡å‹åœ¨ä»…è§†è§‰åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å›¾åƒ-å­—å¹•å¯¹è®­ç»ƒçš„å¼ºä¸»å¹²ä¸Šè¡¨ç°æ˜¾è‘—ã€‚æ­¤å¤–ï¼Œå±€éƒ¨å¯¹é½çš„VLMåœ¨æ¶‰åŠç©ºé—´ç†è§£çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æ€»ä¹‹ï¼Œç ”ç©¶è¯æ˜å¯é€šè¿‡å±€éƒ¨å¯¹é½é˜¶æ®µæœ‰æ•ˆåœ°å­¦ä¹ å±€éƒ¨è¯­ä¹‰æå–ï¼Œå¹¶æœ‰ç›Šäºä½¿ç”¨ç°æˆè§†è§‰ä¸»å¹²çš„VLMè®­ç»ƒé…æ–¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMåœ¨åŸºæœ¬ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨é”™è¯¯ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºé‡‡ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰ä¸»å¹²ï¼ˆå¦‚ViTï¼‰ï¼Œç¼ºä¹å½’çº³åè§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆåè®­ç»ƒé˜¶æ®µâ€”â€”å±€éƒ¨å¯¹é½ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å±€éƒ¨å¯¹é½èƒ½æé«˜æ¨¡å‹åœ¨ä»…è§†è§‰åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å›¾åƒ-å­—å¹•å¯¹è®­ç»ƒçš„å¼ºä¸»å¹²ä¸Šè¡¨ç°æ˜¾è‘—ã€‚</li>
<li>MaskEmbedæ˜¯ä¸€ç§æ–°çš„å¾®è°ƒç¨‹åºï¼Œä½¿ç”¨æ©ç é‡å»ºæŸå¤±æ¥å­¦ä¹ å›¾åƒè¡¥ä¸çš„è¯­ä¹‰è´¡çŒ®ã€‚</li>
<li>å±€éƒ¨å¯¹é½çš„VLMåœ¨æ¶‰åŠç©ºé—´ç†è§£çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ç ”ç©¶è¯æ˜äº†é€šè¿‡å±€éƒ¨å¯¹é½é˜¶æ®µå¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ å±€éƒ¨è¯­ä¹‰æå–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2939265d92855f81913aee87e719052a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03c1e694d9d672e98c96571f0a700191.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fd732232dc08440dfc1a53d598d63f5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d9b7eade04d776b8216a09055382da66.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  Exploring Token-Level Augmentation in Vision Transformer for   Semi-Supervised Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6fc744b2ed06235866916e47fe0c2d9a.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  HarmonySet A Comprehensive Dataset for Understanding Video-Music   Semantic Alignment and Temporal Synchronization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
