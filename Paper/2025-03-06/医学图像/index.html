<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  Multimodal Deep Learning for Subtype Classification in Breast Cancer   Using Histopathological Images and Gene Expression Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e48aad53b5338a6cb41467f17f6403a4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-06-æ›´æ–°"><a href="#2025-03-06-æ›´æ–°" class="headerlink" title="2025-03-06 æ›´æ–°"></a>2025-03-06 æ›´æ–°</h1><h2 id="Multimodal-Deep-Learning-for-Subtype-Classification-in-Breast-Cancer-Using-Histopathological-Images-and-Gene-Expression-Data"><a href="#Multimodal-Deep-Learning-for-Subtype-Classification-in-Breast-Cancer-Using-Histopathological-Images-and-Gene-Expression-Data" class="headerlink" title="Multimodal Deep Learning for Subtype Classification in Breast Cancer   Using Histopathological Images and Gene Expression Data"></a>Multimodal Deep Learning for Subtype Classification in Breast Cancer   Using Histopathological Images and Gene Expression Data</h2><p><strong>Authors:Amin Honarmandi Shandiz</strong></p>
<p>Molecular subtyping of breast cancer is crucial for personalized treatment and prognosis. Traditional classification approaches rely on either histopathological images or gene expression profiling, limiting their predictive power. In this study, we propose a deep multimodal learning framework that integrates histopathological images and gene expression data to classify breast cancer into BRCA.Luminal and BRCA.Basal &#x2F; Her2 subtypes. Our approach employs a ResNet-50 model for image feature extraction and fully connected layers for gene expression processing, with a cross-attention fusion mechanism to enhance modality interaction. We conduct extensive experiments using five-fold cross-validation, demonstrating that our multimodal integration outperforms unimodal approaches in terms of classification accuracy, precision-recall AUC, and F1-score. Our findings highlight the potential of deep learning for robust and interpretable breast cancer subtype classification, paving the way for improved clinical decision-making. </p>
<blockquote>
<p>ä¹³è…ºç™Œçš„åˆ†å­åˆ†å‹å¯¹äºä¸ªæ€§åŒ–æ²»ç–—å’Œé¢„åè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„åˆ†ç±»æ–¹æ³•ä¾èµ–äºç»„ç»‡ç—…ç†å­¦å›¾åƒæˆ–åŸºå› è¡¨è¾¾è°±åˆ†æï¼Œè¿™é™åˆ¶äº†å…¶é¢„æµ‹èƒ½åŠ›ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†ç»„ç»‡ç—…ç†å­¦å›¾åƒå’ŒåŸºå› è¡¨è¾¾æ•°æ®ï¼Œå°†ä¹³è…ºç™Œåˆ†ç±»ä¸ºBRCA.Luminalå’ŒBRCA.Basal &#x2F; Her2äºšå‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ResNet-50æ¨¡å‹è¿›è¡Œå›¾åƒç‰¹å¾æå–ï¼Œé‡‡ç”¨å…¨è¿æ¥å±‚å¤„ç†åŸºå› è¡¨è¾¾æ•°æ®ï¼Œå¹¶ä½¿ç”¨è·¨æ³¨æ„èåˆæœºåˆ¶å¢å¼ºæ¨¡æ€äº¤äº’ã€‚æˆ‘ä»¬é€šè¿‡äº”æŠ˜äº¤å‰éªŒè¯è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€é›†æˆåœ¨åˆ†ç±»ç²¾åº¦ã€ç²¾ç¡®å¬å›AUCå’ŒF1åˆ†æ•°æ–¹é¢ä¼˜äºå•æ¨¡æ€æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†æ·±åº¦å­¦ä¹ åœ¨ç¨³å¥å’Œå¯è§£é‡Šçš„ä¹³è…ºç™Œäºšå‹åˆ†ç±»ä¸­çš„æ½œåŠ›ï¼Œä¸ºæ”¹è¿›ä¸´åºŠå†³ç­–é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02849v1">PDF</a> 9 pages, 9 figures</p>
<p><strong>Summary</strong><br>ä¹³è…ºç™Œåˆ†å­åˆ†å‹å¯¹ä¸ªæ€§åŒ–æ²»ç–—å’Œé¢„åè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•ä¾èµ–ç—…ç†å›¾åƒæˆ–åŸºå› è¡¨è¾¾è°±åˆ†æï¼Œé¢„æµ‹èƒ½åŠ›æœ‰é™ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ·±åº¦å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œæ•´åˆç—…ç†å›¾åƒå’ŒåŸºå› è¡¨è¾¾æ•°æ®ï¼Œå°†ä¹³è…ºç™Œåˆ†ä¸ºBRCA.Luminalå’ŒBRCA.Basal &#x2F; Her2äºšå‹ã€‚é‡‡ç”¨ResNet-50æ¨¡å‹æå–å›¾åƒç‰¹å¾ï¼Œå…¨è¿æ¥å±‚å¤„ç†åŸºå› è¡¨è¾¾æ•°æ®ï¼Œé‡‡ç”¨äº¤å‰æ³¨æ„åŠ›èåˆæœºåˆ¶å¢å¼ºæ¨¡æ€äº¤äº’ã€‚å®éªŒé‡‡ç”¨äº”æŠ˜äº¤å‰éªŒè¯ï¼Œè¯æ˜å¤šæ¨¡æ€èåˆåœ¨åˆ†ç±»å‡†ç¡®ç‡ã€ç²¾ç¡®å¬å›AUCå’ŒF1åˆ†æ•°æ–¹é¢ä¼˜äºå•æ¨¡æ€æ–¹æ³•ã€‚æœ¬ç ”ç©¶å‡¸æ˜¾æ·±åº¦å­¦ä¹ åœ¨ä¹³è…ºç™Œäºšå‹åˆ†ç±»ä¸­çš„æ½œåŠ›å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºä¸´åºŠå†³ç­–æä¾›æ”¹è¿›æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œåˆ†å­åˆ†å‹å¯¹ä¸ªæ€§åŒ–æ²»ç–—ä¸é¢„åè¯„ä¼°éå¸¸é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿä¹³è…ºç™Œåˆ†ç±»æ–¹æ³•ä¾èµ–ç—…ç†å›¾åƒæˆ–åŸºå› è¡¨è¾¾è°±åˆ†æï¼Œå…·æœ‰å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ç§æ·±åº¦å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œæ•´åˆç—…ç†å›¾åƒå’ŒåŸºå› è¡¨è¾¾æ•°æ®æ¥è¿›è¡Œä¹³è…ºç™Œåˆ†ç±»ã€‚</li>
<li>é‡‡ç”¨ResNet-50æ¨¡å‹ä¸å…¨è¿æ¥å±‚å¤„ç†ä¸åŒæ•°æ®ç±»å‹ï¼Œå¹¶å¼•å…¥äº¤å‰æ³¨æ„åŠ›èåˆæœºåˆ¶ã€‚</li>
<li>å®éªŒéªŒè¯è¯¥æ¡†æ¶åœ¨åˆ†ç±»å‡†ç¡®ç‡ã€ç²¾ç¡®å¬å›AUCå’ŒF1åˆ†æ•°ç­‰æ–¹é¢ä¼˜äºå•æ¨¡æ€æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜æ·±åº¦å­¦ä¹ åœ¨ä¹³è…ºç™Œäºšå‹åˆ†ç±»ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-81f44a94efd7139375644ecea7bbb8de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49835b515737fed9ead6b8dbb5d9ad9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-334ce808e0107bd32dac3f2a419d322a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08b7355f463348eeda0789838adcc235.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4060d2f74346bf2b32a9dbf7abc9179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-567998f3dcf298696ce2f125d2a27e4f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-832dccaaec51eb3d04bd64c74e4dfd3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26b03535357cff0e9d55b294fac04821.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-138c62e695942cf43c7841d555e05049.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Comprehensive-Analysis-of-Relative-Pressure-Estimation-Methods-Utilizing-4D-Flow-MRI"><a href="#Comprehensive-Analysis-of-Relative-Pressure-Estimation-Methods-Utilizing-4D-Flow-MRI" class="headerlink" title="Comprehensive Analysis of Relative Pressure Estimation Methods Utilizing   4D Flow MRI"></a>Comprehensive Analysis of Relative Pressure Estimation Methods Utilizing   4D Flow MRI</h2><p><strong>Authors:Brandon Hardy, Judith Zimmermann, Vincent Lechner, Mia Bonini, Julio A. Sotelo, Nicholas S. Burris, Daniel B. Ennis, David Marlevi, David A. Nordsletten</strong></p>
<p>4D flow MRI allows for the estimation of three-dimensional relative pressure fields, providing rich pressure information, unlike catheterization and Doppler echocardiography, which provide one-dimensional pressure drops only. The accuracy of one-dimensional pressure drops derived from 4D flow has been explored in previous literature, but additional work must be done to evaluate the accuracy of three-dimensional relative pressure fields. This work presents an analysis of three state-of-the-art relative pressure estimators: virtual Work-Energy Relative Pressure (vWERP), the Pressure Poisson Estimator (PPE), and the Stokes Estimator (STE). Spatiotemporal behavior and sensitivity to noise were determined in silico. Estimators were validated with a type B aortic dissection (TBAD) flow phantom with varying tear geometry and an array of twelve catheter pressure measurements. Finally, the performance of each estimator was evaluated across eight patient cases. In silico pressure field errors were lower in STE compared to PPE, although PPE pressures were less affected by noise. High velocity gradients and low spatial resolution contributed most significantly to local variations in 3D error fields. Low temporal resolution leads to highly transient peak pressure events being averaged, systematically underestimating peak pressures. In the flow phantom analysis, vWERP was the most accurate method, followed by STE and PPE. Each pressure estimator strongly correlated with ground truth pressure values despite the tendency to underestimate peak pressures. Patient case results demonstrated that the pressure estimators could be feasibly integrated into a clinical workflow. </p>
<blockquote>
<p>å››ç»´è¡€æµç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰èƒ½å¤Ÿä¼°è®¡ä¸‰ç»´ç›¸å¯¹å‹åŠ›åœºï¼Œæä¾›ä¸°å¯Œçš„å‹åŠ›ä¿¡æ¯ï¼Œä¸åŒäºæ’ç®¡å’Œå¤šæ™®å‹’è¶…å£°å¿ƒåŠ¨å›¾ï¼Œåè€…ä»…æä¾›ä¸€ç»´å‹åŠ›ä¸‹é™ä¿¡æ¯ã€‚æ­¤å‰æ–‡çŒ®å·²ç»æ¢è®¨è¿‡ä»å››ç»´è¡€æµä¸­å¾—å‡ºçš„ä¸€ç»´å‹åŠ›ä¸‹é™çš„å‡†ç¡®æ€§ï¼Œä½†è¿˜éœ€è¦è¿›ä¸€æ­¥çš„å·¥ä½œæ¥è¯„ä¼°ä¸‰ç»´ç›¸å¯¹å‹åŠ›åœºçš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸‰ç§æœ€æ–°ç›¸å¯¹å‹åŠ›ä¼°è®¡å™¨ï¼šè™šæ‹ŸåŠŸèƒ½é‡ç›¸å¯¹å‹åŠ›ï¼ˆvWERPï¼‰ã€å‹åŠ›æ³Šæ¾ä¼°è®¡å™¨ï¼ˆPPEï¼‰å’Œæ–¯æ‰˜å…‹æ–¯ä¼°è®¡å™¨ï¼ˆSTEï¼‰ã€‚é€šè¿‡è®¡ç®—æœºæ¨¡æ‹Ÿç¡®å®šäº†å…¶åœ¨æ—¶ç©ºè¡Œä¸ºå’Œå¯¹å™ªå£°çš„æ•æ„Ÿæ€§ã€‚ä½¿ç”¨Bå‹ä¸»åŠ¨è„‰å¤¹å±‚ï¼ˆTBADï¼‰è¡€æµæ¨¡å‹å’Œå¤šç§æ’•è£‚å‡ ä½•å½¢çŠ¶ä»¥åŠä¸€ä¸ªåŒ…å«åäºŒæ ¹å¯¼ç®¡å‹åŠ›æµ‹é‡çš„é˜µåˆ—å¯¹ä¼°è®¡å™¨è¿›è¡Œäº†éªŒè¯ã€‚æœ€åï¼Œå¯¹æ¯ç§ä¼°è®¡å™¨åœ¨å…«ç§æ‚£è€…ç—…ä¾‹ä¸­çš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨è®¡ç®—æœºæ¨¡æ‹Ÿçš„å‹åŠ›åœºè¯¯å·®ä¸­ï¼ŒSTEä¸PPEç›¸æ¯”è¯¯å·®è¾ƒä½ï¼Œå°½ç®¡PPEçš„å‹åŠ›å—å™ªå£°å½±å“è¾ƒå°ã€‚é«˜é€Ÿæ¢¯åº¦å’Œä½ç©ºé—´åˆ†è¾¨ç‡å¯¹ä¸‰ç»´è¯¯å·®åœºçš„å±€éƒ¨å˜åŒ–è´¡çŒ®æœ€å¤§ã€‚ä½æ—¶é—´åˆ†è¾¨ç‡ä¼šå¯¼è‡´çŸ­æš‚çš„é«˜å³°å‹åŠ›äº‹ä»¶è¢«å¹³å‡åŒ–ï¼Œä»è€Œç³»ç»Ÿæ€§åœ°ä½ä¼°å³°å€¼å‹åŠ›ã€‚åœ¨è¡€æµæ¨¡å‹åˆ†æä¸­ï¼ŒvWERPæ˜¯æœ€å‡†ç¡®çš„æ–¹æ³•ï¼Œå…¶æ¬¡æ˜¯STEå’ŒPPEã€‚å°½ç®¡æœ‰ä½ä¼°å³°å€¼å‹åŠ›çš„å€¾å‘ï¼Œä½†æ¯ç§å‹åŠ›ä¼°è®¡å™¨éƒ½ä¸çœŸå®å‹åŠ›å€¼å¯†åˆ‡ç›¸å…³ã€‚æ‚£è€…ç—…ä¾‹ç»“æœè¡¨æ˜ï¼Œå‹åŠ›ä¼°è®¡å™¨å¯ä»¥åˆç†åœ°é›†æˆåˆ°ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02847v1">PDF</a> 10 pages, 8 figures. Planned submission to IEEE Transactions on   Medical Imaging</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†ä¸‰ç»´æµåœºä¸­çš„ç›¸å¯¹å‹åŠ›ä¼°è®¡æŠ€æœ¯ï¼Œå¹¶å¯¹æ¯”åˆ†æäº†ä¸‰ç§å…ˆè¿›ç›¸å¯¹å‹åŠ›ä¼°è®¡å™¨ï¼ˆè™šæ‹Ÿå·¥ä½œèƒ½é‡ç›¸å¯¹å‹åŠ›ã€å‹åŠ›æ³Šæ¾ä¼°è®¡å™¨å’Œæ–¯æ‰˜å…‹æ–¯ä¼°è®¡å™¨ï¼‰çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ–¯æ‰˜å…‹æ–¯ä¼°è®¡å™¨çš„å‹åŠ›åœºè¯¯å·®è¾ƒä½ï¼Œä½†åœ¨å™ªå£°å¹²æ‰°æ–¹é¢å‹åŠ›æ³Šæ¾ä¼°è®¡å™¨æ›´ç¨³å¥ã€‚æ­¤å¤–ï¼Œé€šè¿‡æµåœºæ¨¡å‹åˆ†æéªŒè¯äº†å„å‹åŠ›ä¼°è®¡å™¨çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨æ‚£è€…æ¡ˆä¾‹ä¸­è¯æ˜äº†å…¶ä¸´åºŠåº”ç”¨çš„å¯è¡Œæ€§ã€‚å°½ç®¡å­˜åœ¨ä½ä¼°å³°å€¼å‹åŠ›çš„è¶‹åŠ¿ï¼Œä½†å‹åŠ›ä¼°è®¡å™¨ä¸çœŸå®å‹åŠ›å€¼çš„ç›¸å…³æ€§å¾ˆå¼ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4DæµMRIèƒ½å¤Ÿä¼°è®¡ä¸‰ç»´ç›¸å¯¹å‹åŠ›åœºï¼Œæä¾›ä¸°å¯Œçš„å‹åŠ›ä¿¡æ¯ï¼Œä¸åŒäºåªèƒ½æä¾›ä¸€ç»´å‹åŠ›ä¸‹é™çš„å¯¼ç®¡åŒ–å’Œå¤šæ™®å‹’è¶…å£°å¿ƒåŠ¨å›¾ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶å¯¹ä¸‰ç§å…ˆè¿›ç›¸å¯¹å‹åŠ›ä¼°è®¡å™¨è¿›è¡Œäº†å¯¹æ¯”åˆ†æï¼ŒåŒ…æ‹¬è™šæ‹Ÿå·¥ä½œèƒ½é‡ç›¸å¯¹å‹åŠ›ã€å‹åŠ›æ³Šæ¾ä¼°è®¡å™¨å’Œæ–¯æ‰˜å…‹æ–¯ä¼°è®¡å™¨ã€‚</li>
<li>æ–¯æ‰˜å…‹æ–¯ä¼°è®¡å™¨çš„å‹åŠ›åœºè¯¯å·®è¾ƒä½ï¼Œè€Œå‹åŠ›æ³Šæ¾ä¼°è®¡å™¨å¯¹å™ªå£°å¹²æ‰°æ›´ç¨³å¥ã€‚</li>
<li>é«˜é€Ÿåº¦æ¢¯åº¦å’Œä½ç©ºé—´åˆ†è¾¨ç‡å¯¹ä¸‰ç»´è¯¯å·®åœºå½±å“æ˜¾è‘—ã€‚ä½æ—¶é—´åˆ†è¾¨ç‡ä¼šå¯¼è‡´å³°å€¼å‹åŠ›äº‹ä»¶çš„å¹³å‡åŒ–ï¼Œä»è€Œä½ä¼°å³°å€¼å‹åŠ›ã€‚</li>
<li>é€šè¿‡æµåœºæ¨¡å‹åˆ†æéªŒè¯äº†å„å‹åŠ›ä¼°è®¡å™¨çš„å‡†ç¡®æ€§ã€‚åœ¨æµåœºå¹»å½±åˆ†æä¸­ï¼Œè™šæ‹Ÿå·¥ä½œèƒ½é‡ç›¸å¯¹å‹åŠ›æ˜¯æœ€å‡†ç¡®çš„æ–¹æ³•ï¼Œå…¶æ¬¡æ˜¯æ–¯æ‰˜å…‹æ–¯ä¼°è®¡å™¨å’Œå‹åŠ›æ³Šæ¾ä¼°è®¡å™¨ã€‚</li>
<li>å‹åŠ›ä¼°è®¡å™¨ä¸çœŸå®å‹åŠ›å€¼å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œå°½ç®¡å­˜åœ¨ä½ä¼°å³°å€¼å‹åŠ›çš„è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90f44f4da4043c7f7ef1a99b30f12c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdd911906df4fe62e2ffa21520b700e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd13a5481edeb7d9f9a951524ae0965f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46b6904ddb4c0c98b3708d24ea8611ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11557942f1c2dd5bd645b511e8af615d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c15b5d9e157064ee7dce5d9eb42509c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Boltzmann-Attention-Sampling-for-Image-Analysis-with-Small-Objects"><a href="#Boltzmann-Attention-Sampling-for-Image-Analysis-with-Small-Objects" class="headerlink" title="Boltzmann Attention Sampling for Image Analysis with Small Objects"></a>Boltzmann Attention Sampling for Image Analysis with Small Objects</h2><p><strong>Authors:Theodore Zhao, Sid Kiblawi, Naoto Usuyama, Ho Hin Lee, Sam Preston, Hoifung Poon, Mu Wei</strong></p>
<p>Detecting and segmenting small objects, such as lung nodules and tumor lesions, remains a critical challenge in image analysis. These objects often occupy less than 0.1% of an image, making traditional transformer architectures inefficient and prone to performance degradation due to redundant attention computations on irrelevant regions. Existing sparse attention mechanisms rely on rigid hierarchical structures, which are poorly suited for detecting small, variable, and uncertain object locations. In this paper, we propose BoltzFormer, a novel transformer-based architecture designed to address these challenges through dynamic sparse attention. BoltzFormer identifies and focuses attention on relevant areas by modeling uncertainty using a Boltzmann distribution with an annealing schedule. Initially, a higher temperature allows broader area sampling in early layers, when object location uncertainty is greatest. As the temperature decreases in later layers, attention becomes more focused, enhancing efficiency and accuracy. BoltzFormer seamlessly integrates into existing transformer architectures via a modular Boltzmann attention sampling mechanism. Comprehensive evaluations on benchmark datasets demonstrate that BoltzFormer significantly improves segmentation performance for small objects while reducing attention computation by an order of magnitude compared to previous state-of-the-art methods. </p>
<blockquote>
<p>æ£€æµ‹å’Œåˆ†å‰²å°ç›®æ ‡ï¼ˆå¦‚è‚ºç»“èŠ‚å’Œè‚¿ç˜¤ç—…å˜ï¼‰åœ¨å›¾åƒåˆ†æä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™äº›ç›®æ ‡é€šå¸¸åªå å›¾åƒé¢ç§¯çš„ä¸åˆ°0.1%ï¼Œä½¿å¾—ä¼ ç»Ÿçš„Transformeræ¶æ„æ•ˆç‡ä½ä¸‹ï¼Œå¹¶ä¸”ç”±äºåœ¨ä¸ç›¸å…³åŒºåŸŸçš„å†—ä½™æ³¨æ„åŠ›è®¡ç®—è€Œå®¹æ˜“å‡ºç°æ€§èƒ½ä¸‹é™ã€‚ç°æœ‰çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä¾èµ–äºåƒµåŒ–çš„å±‚æ¬¡ç»“æ„ï¼Œå¯¹äºæ£€æµ‹å°ã€å¯å˜å’Œä¸ç¡®å®šçš„ä½ç½®çš„ç›®æ ‡å¹¶ä¸é€‚ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BoltzFormerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºTransformerçš„æ–°å‹æ¶æ„ï¼Œé€šè¿‡åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚BoltzFormeré€šè¿‡åˆ©ç”¨å¸¦æœ‰é€€ç«è®¡åˆ’çš„Boltzdannåˆ†å¸ƒå»ºæ¨¡ä¸ç¡®å®šæ€§æ¥è¯†åˆ«å’Œé›†ä¸­æ³¨æ„åŠ›åœ¨ç›¸å…³åŒºåŸŸã€‚åœ¨å¯¹è±¡ä½ç½®ä¸ç¡®å®šæ€§æœ€å¤§çš„æ—©æœŸå±‚æ¬¡ä¸­ï¼Œè¾ƒé«˜çš„åˆå§‹æ¸©åº¦å…è®¸æ›´å¤§çš„åŒºåŸŸé‡‡æ ·ã€‚éšç€æ¸©åº¦çš„é™ä½ï¼Œæ³¨æ„åŠ›å˜å¾—æ›´åŠ é›†ä¸­ï¼Œæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚BoltzFormeré€šè¿‡æ¨¡å—åŒ–Boltzdannæ³¨æ„åŠ›é‡‡æ ·æœºåˆ¶æ— ç¼é›†æˆåˆ°ç°æœ‰çš„Transformeræ¶æ„ä¸­ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒBoltzFormeråœ¨åˆ†å‰²å°ç›®æ ‡æ–¹é¢æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå¹¶ä¸”ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ç›¸æ¯”ï¼Œå‡å°‘äº†è®¡ç®—é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02841v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†BoltzFormerï¼Œä¸€ç§åŸºäºåŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶çš„æ–°å‹å˜å‹å™¨æ¶æ„ï¼Œç”¨äºè§£å†³å›¾åƒåˆ†æä¸­æ£€æµ‹å¹¶åˆ†å‰²å°ç‰©ä½“ï¼ˆå¦‚è‚ºç»“èŠ‚å’Œè‚¿ç˜¤ç—…å˜ï¼‰çš„æŒ‘æˆ˜ã€‚BoltzFormeråˆ©ç”¨æ³¢å°”å…¹æ›¼åˆ†å¸ƒå»ºæ¨¡ä¸ç¡®å®šæ€§ï¼Œé€šè¿‡é€€ç«è°ƒåº¦æœºåˆ¶åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›ç„¦ç‚¹ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒBoltzFormeråœ¨åˆ†å‰²å°ç‰©ä½“æ–¹é¢æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œå¹¶å°†æ³¨æ„åŠ›è®¡ç®—å‡å°‘äº†ä¸€ä¸ªæ•°é‡çº§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€æµ‹å¹¶åˆ†å‰²å°ç‰©ä½“æ˜¯å›¾åƒåˆ†æä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå˜å‹å™¨æ¶æ„åœ¨å¤„ç†å°ç‰©ä½“æ—¶å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œæ€§èƒ½é€€åŒ–çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä¾èµ–äºåƒµåŒ–çš„å±‚æ¬¡ç»“æ„ï¼Œä¸é€‚åˆæ£€æµ‹ä½ç½®ä¸ç¡®å®šçš„å°ç‰©ä½“ã€‚</li>
<li>BoltzFormeræ˜¯ä¸€ç§åŸºäºæ–°å‹åŠ¨æ€ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶çš„å˜å‹å™¨æ¶æ„ã€‚</li>
<li>BoltzFormeråˆ©ç”¨æ³¢å°”å…¹æ›¼åˆ†å¸ƒå»ºæ¨¡ä¸ç¡®å®šæ€§ï¼Œé€šè¿‡é€€ç«è°ƒåº¦è°ƒæ•´æ³¨æ„åŠ›ç„¦ç‚¹ã€‚</li>
<li>BoltzFormerèƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰å˜å‹å™¨æ¶æ„ä¸­ï¼Œé€šè¿‡æ¨¡å—åŒ–æ³¢å°”å…¹æ›¼æ³¨æ„åŠ›é‡‡æ ·æœºåˆ¶å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d3047b07f274ae61cf1d22403b346c54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12713a67a0f95221c4d990629335a0e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f60c32d7079efef2efe250d273903fcf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21402a4d6217c195cd51f7331431ce37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bebbdb5ec1a6cc8229fe46a590d9977.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Catheter-Detection-and-Segmentation-in-X-ray-Images-via-Multi-task-Learning"><a href="#Catheter-Detection-and-Segmentation-in-X-ray-Images-via-Multi-task-Learning" class="headerlink" title="Catheter Detection and Segmentation in X-ray Images via Multi-task   Learning"></a>Catheter Detection and Segmentation in X-ray Images via Multi-task   Learning</h2><p><strong>Authors:Lin Xi, Yingliang Ma, Ethan Koland, Sandra Howell, Aldo Rinaldi, Kawal S. Rhode</strong></p>
<p>Automated detection and segmentation of surgical devices, such as catheters or wires, in X-ray fluoroscopic images have the potential to enhance image guidance in minimally invasive heart surgeries. In this paper, we present a convolutional neural network model that integrates a resnet architecture with multiple prediction heads to achieve real-time, accurate localization of electrodes on catheters and catheter segmentation in an end-to-end deep learning framework. We also propose a multi-task learning strategy in which our model is trained to perform both accurate electrode detection and catheter segmentation simultaneously. A key challenge with this approach is achieving optimal performance for both tasks. To address this, we introduce a novel multi-level dynamic resource prioritization method. This method dynamically adjusts sample and task weights during training to effectively prioritize more challenging tasks, where task difficulty is inversely proportional to performance and evolves throughout the training process. Experiments on both public and private datasets have demonstrated that the accuracy of our method surpasses the existing state-of-the-art methods in both single segmentation task and in the detection and segmentation multi-task. Our approach achieves a good trade-off between accuracy and efficiency, making it well-suited for real-time surgical guidance applications. </p>
<blockquote>
<p>åœ¨Xå…‰é€è§†å½±åƒä¸­è‡ªåŠ¨æ£€æµ‹å’Œåˆ†å‰²å¦‚å¯¼ç®¡æˆ–çº¿è·¯ç­‰æ‰‹æœ¯å™¨æ¢°ï¼Œæœ‰å¯èƒ½å¢å¼ºå¾®åˆ›å¿ƒè„æ‰‹æœ¯çš„å›¾åƒå¼•å¯¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ®‹å·®ç½‘ç»œæ¶æ„å’Œå¤šä¸ªé¢„æµ‹å¤´ï¼Œä»¥å®ç°å¯¼ç®¡ä¸Šç”µæçš„å®æ—¶ã€ç²¾ç¡®å®šä½å’Œå¯¼ç®¡åˆ†æ®µï¼Œå½¢æˆä¸€ä¸ªç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹åŒæ—¶æ‰§è¡Œç²¾ç¡®çš„ç”µææ£€æµ‹å’Œå¯¼ç®¡åˆ†å‰²ã€‚æ­¤æ–¹æ³•çš„æŒ‘æˆ˜åœ¨äºå®ç°ä¸¤ä¸ªä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå±‚æ¬¡åŠ¨æ€èµ„æºä¼˜å…ˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ ·æœ¬å’Œä»»åŠ¡æƒé‡ï¼Œæœ‰æ•ˆä¼˜å…ˆå¤„ç†æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå…¶ä¸­ä»»åŠ¡éš¾åº¦ä¸æ€§èƒ½æˆåæ¯”ï¼Œå¹¶åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å˜åŒ–ã€‚åœ¨å…¬å…±å’Œç§æœ‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çš„å‡†ç¡®æ€§è¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„å•åˆ†å‰²ä»»åŠ¡ä»¥åŠåœ¨æ£€æµ‹å’Œåˆ†å‰²å¤šä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œä½¿å…¶éå¸¸é€‚åˆç”¨äºå®æ—¶æ‰‹æœ¯æŒ‡å¯¼åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02717v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆResNetæ¶æ„å’Œå¤šé¢„æµ‹å¤´çš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºå®ç°Xå…‰é€è§†å›¾åƒä¸­å¯¼ç®¡ç”µæçš„å®æ—¶ã€ç²¾å‡†å®šä½ä»¥åŠå¯¼ç®¡åˆ†å‰²ã€‚æå‡ºä¸€ç§å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥å’ŒåŠ¨æ€èµ„æºä¼˜å…ˆåŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ ·æœ¬å’Œä»»åŠ¡æƒé‡ï¼Œä»¥æœ‰æ•ˆä¼˜å…ˆå¤„ç†æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•ä¸€åˆ†å‰²ä»»åŠ¡ä»¥åŠæ£€æµ‹å’Œåˆ†å‰²å¤šä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§å‡è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå®ç°äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ï¼Œéå¸¸é€‚åˆç”¨äºå®æ—¶æ‰‹æœ¯æŒ‡å¯¼åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ç»“åˆäº†ResNetæ¶æ„å’Œå¤šé¢„æµ‹å¤´ï¼Œç”¨äºXå…‰é€è§†å›¾åƒä¸­çš„å¯¼ç®¡ç”µæå®šä½å’Œå¯¼ç®¡åˆ†å‰²ã€‚</li>
<li>æå‡ºäº†å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œæ¨¡å‹å¯åŒæ—¶è¿›è¡Œç”µææ£€æµ‹å’Œå¯¼ç®¡åˆ†å‰²ã€‚</li>
<li>é¢ä¸´åŒæ—¶ä¼˜åŒ–ä¸¤ä¸ªä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤šå±‚æ¬¡åŠ¨æ€èµ„æºä¼˜å…ˆåŒ–æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ ·æœ¬å’Œä»»åŠ¡æƒé‡ï¼Œä»¥ä¼˜å…ˆå¤„ç†æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®æ€§å’Œæ•ˆç‡å‡è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æ­¤æ–¹æ³•é€‚ç”¨äºå®æ—¶æ‰‹æœ¯æŒ‡å¯¼åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-652fb64e83c18f61574f0d4f48f12ab9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e927921ed5fafe818bd84dd24caf464e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="XFMamba-Cross-Fusion-Mamba-for-Multi-View-Medical-Image-Classification"><a href="#XFMamba-Cross-Fusion-Mamba-for-Multi-View-Medical-Image-Classification" class="headerlink" title="XFMamba: Cross-Fusion Mamba for Multi-View Medical Image Classification"></a>XFMamba: Cross-Fusion Mamba for Multi-View Medical Image Classification</h2><p><strong>Authors:Xiaoyu Zheng, Xu Chen, Shaogang Gong, Xavier Griffin, Greg Slabaugh</strong></p>
<p>Compared to single view medical image classification, using multiple views can significantly enhance predictive accuracy as it can account for the complementarity of each view while leveraging correlations between views. Existing multi-view approaches typically employ separate convolutional or transformer branches combined with simplistic feature fusion strategies. However, these approaches inadvertently disregard essential cross-view correlations, leading to suboptimal classification performance, and suffer from challenges with limited receptive field (CNNs) or quadratic computational complexity (transformers). Inspired by state space sequence models, we propose XFMamba, a pure Mamba-based cross-fusion architecture to address the challenge of multi-view medical image classification. XFMamba introduces a novel two-stage fusion strategy, facilitating the learning of single-view features and their cross-view disparity. This mechanism captures spatially long-range dependencies in each view while enhancing seamless information transfer between views. Results on three public datasets, MURA, CheXpert and DDSM, illustrate the effectiveness of our approach across diverse multi-view medical image classification tasks, showing that it outperforms existing convolution-based and transformer-based multi-view methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/XZheng0427/XFMamba">https://github.com/XZheng0427/XFMamba</a>. </p>
<blockquote>
<p>ä¸å•è§†å›¾åŒ»å­¦å›¾åƒåˆ†ç±»ç›¸æ¯”ï¼Œä½¿ç”¨å¤šä¸ªè§†å›¾å¯ä»¥æ˜¾è‘—æé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒå¯ä»¥åˆ©ç”¨æ¯ä¸ªè§†å›¾çš„äº’è¡¥æ€§ï¼ŒåŒæ—¶åˆ©ç”¨è§†å›¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚ç°æœ‰çš„å¤šè§†å›¾æ–¹æ³•é€šå¸¸é‡‡ç”¨å•ç‹¬çš„å·ç§¯æˆ–è½¬æ¢å™¨åˆ†æ”¯ï¼Œå¹¶ç»“åˆç®€å•çš„ç‰¹å¾èåˆç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æ— æ„ä¸­å¿½ç•¥äº†é‡è¦çš„è·¨è§†å›¾ç›¸å…³æ€§ï¼Œå¯¼è‡´åˆ†ç±»æ€§èƒ½ä¸ä½³ï¼Œå¹¶é¢ä¸´æ„Ÿå—é‡æœ‰é™ï¼ˆCNNï¼‰æˆ–è®¡ç®—å¤æ‚åº¦ä¸ºäºŒæ¬¡æ–¹ï¼ˆè½¬æ¢å™¨ï¼‰çš„æŒ‘æˆ˜ã€‚å—çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†XFMambaï¼Œè¿™æ˜¯ä¸€ç§åŸºäºMambaçš„çº¯è·¨èåˆæ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤šè§†å›¾åŒ»å­¦å›¾åƒåˆ†ç±»çš„æŒ‘æˆ˜ã€‚XFMambaå¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µèåˆç­–ç•¥ï¼Œä¿ƒè¿›å•è§†å›¾ç‰¹å¾åŠå…¶è·¨è§†å›¾å·®å¼‚çš„å­¦ä¹ ã€‚è¯¥æœºåˆ¶æ•è·æ¯ä¸ªè§†å›¾ä¸­çš„ç©ºé—´é•¿è·ç¦»ä¾èµ–æ€§ï¼ŒåŒæ—¶å¢å¼ºè§†å›¾ä¹‹é—´çš„æ— ç¼ä¿¡æ¯ä¼ è¾“ã€‚åœ¨MURAã€CheXpertå’ŒDDSMä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒå¤šè§†å›¾åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å®ƒä¼˜äºç°æœ‰çš„åŸºäºå·ç§¯å’ŒåŸºäºè½¬æ¢å™¨çš„å¤šè§†å›¾æ–¹æ³•ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šè§†è§’åŒ»ç–—å›¾åƒåˆ†ç±»ç›¸è¾ƒäºå•è§†è§’åˆ†ç±»èƒ½æ˜¾è‘—æé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒèƒ½åˆ©ç”¨ä¸åŒè§†è§’é—´çš„äº’è¡¥æ€§å’Œç›¸å…³æ€§ã€‚ç°æœ‰æ–¹æ³•å¤šé‡‡ç”¨å•ç‹¬çš„å·ç§¯ç¥ç»ç½‘ç»œæˆ–è½¬æ¢å™¨æ¨¡å‹ï¼Œç»“åˆç®€å•çš„ç‰¹å¾èåˆç­–ç•¥ï¼Œä½†å¿½ç•¥äº†è·¨è§†è§’çš„ç›¸å…³æ€§ä»¥åŠé¢ä¸´ç€å¦‚CNNçš„æœ‰é™æ„Ÿå—é‡å’Œè½¬æ¢å™¨è®¡ç®—å¤æ‚åº¦äºŒæ¬¡æ–¹ç­‰é—®é¢˜ã€‚å—åˆ°çŠ¶æ€ç©ºé—´åºåˆ—æ¨¡å‹çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºMambaçš„è·¨èåˆæ¶æ„XFMambaæ¥è§£å†³å¤šè§†è§’åŒ»ç–—å›¾åƒåˆ†ç±»çš„æŒ‘æˆ˜ã€‚XFMambaå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µèåˆç­–ç•¥ï¼Œèƒ½å­¦ä¹ å•è§†è§’ç‰¹å¾åŠå…¶è·¨è§†è§’å·®å¼‚ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„ç»“æœè¯æ˜äº†å…¶åœ¨å¤šè§†è§’åŒ»ç–—å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¼˜äºç°æœ‰çš„å·ç§¯å’Œè½¬æ¢å¤šè§†è§’æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè§†è§’åŒ»ç–—å›¾åƒåˆ†ç±»èƒ½æ˜¾è‘—æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†è·¨è§†è§’çš„ç›¸å…³æ€§ä»¥åŠé¢ä¸´æ„Ÿå—é‡å’Œè®¡ç®—å¤æ‚åº¦çš„æŒ‘æˆ˜ã€‚</li>
<li>XFMambaæ˜¯åŸºäºMambaçš„çº¯è·¨èåˆæ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤šè§†è§’åŒ»ç–—å›¾åƒåˆ†ç±»çš„æŒ‘æˆ˜ã€‚</li>
<li>XFMambaå¼•å…¥ä¸¤é˜¶æ®µèåˆç­–ç•¥ï¼Œå­¦ä¹ å•è§†è§’ç‰¹å¾åŠå…¶è·¨è§†è§’å·®å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ•è·æ¯ä¸ªè§†è§’çš„ç©ºé—´é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼ŒåŒæ—¶å¢å¼ºè§†è§’é—´çš„æ— ç¼ä¿¡æ¯ä¼ è¾“æ¥å®ç°æ•ˆæœã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒXFMambaåœ¨å¤šç§å¤šè§†è§’åŒ»ç–—å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e48aad53b5338a6cb41467f17f6403a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67ec99953ec67fb8d0e037648bbd6779.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Towards-a-robust-R2D2-paradigm-for-radio-interferometric-imaging-revisiting-DNN-training-and-architecture"><a href="#Towards-a-robust-R2D2-paradigm-for-radio-interferometric-imaging-revisiting-DNN-training-and-architecture" class="headerlink" title="Towards a robust R2D2 paradigm for radio-interferometric imaging:   revisiting DNN training and architecture"></a>Towards a robust R2D2 paradigm for radio-interferometric imaging:   revisiting DNN training and architecture</h2><p><strong>Authors:Amir Aghabiglou, Chung San Chu, Chao Tang, Arwa Dabbech, Yves Wiaux</strong></p>
<p>The R2D2 Deep Neural Network (DNN) series was recently introduced for image formation in radio interferometry. It can be understood as a learned version of CLEAN, whose minor cycles are substituted with DNNs. We revisit R2D2 on the grounds of series convergence, training methodology, and DNN architecture, improving its robustness in terms of generalisability beyond training conditions, capability to deliver high data fidelity, and epistemic uncertainty. Firstly, while still focusing on telescope-specific training, we enhance the learning process by randomising Fourier sampling integration times, incorporating multi-scan multi-noise configurations, and varying imaging settings, including pixel resolution and visibility-weighting scheme. Secondly, we introduce a convergence criterion whereby the reconstruction process stops when the data residual is compatible with noise, rather than simply using all available DNNs. This not only increases the reconstruction efficiency by reducing its computational cost, but also refines training by pruning out the data&#x2F;image pairs for which optimal data fidelity is reached before training the next DNN. Thirdly, we substitute R2D2â€™s early U-Net DNN with a novel architecture (U-WDSR) combining U-Net and WDSR, which leverages wide activation, dense connections, weight normalisation, and low-rank convolution to improve feature reuse and reconstruction precision. As previously, R2D2 was trained for monochromatic intensity imaging with the Very Large Array (VLA) at fixed $512 \times 512$ image size. Simulations on a wide range of inverse problems and a case study on real data reveal that the new R2D2 model consistently outperforms its earlier version in image reconstruction quality, data fidelity, and epistemic uncertainty. </p>
<blockquote>
<p>R2D2æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç³»åˆ—æœ€è¿‘è¢«å¼•å…¥åˆ°å°„ç”µå¹²æ¶‰ä»ªçš„å›¾åƒå½¢æˆä¸­ã€‚å¯ä»¥å°†å…¶ç†è§£ä¸ºCLEANçš„å­¦å¾—ç‰ˆæœ¬ï¼Œå…¶å°å‘¨æœŸè¢«DNNæ›¿ä»£ã€‚æˆ‘ä»¬ä»åºåˆ—æ”¶æ•›ã€è®­ç»ƒæ–¹æ³•å’ŒDNNæ¶æ„ç­‰æ–¹é¢é‡æ–°å®¡è§†R2D2ï¼Œæé«˜å…¶è¶…è¶Šè®­ç»ƒæ¡ä»¶çš„é€šç”¨æ€§ã€æä¾›é«˜æ•°æ®ä¿çœŸæ€§å’Œè®¤è¯†ä¸ç¡®å®šæ€§çš„ç¨³å¥æ€§ã€‚é¦–å…ˆï¼Œåœ¨ä»ä¸“æ³¨äºæœ›è¿œé•œç‰¹å®šè®­ç»ƒçš„åŒæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡éšæœºåŒ–å‚…ç«‹å¶é‡‡æ ·ç§¯åˆ†æ—¶é—´ã€èå…¥å¤šæ‰«æå¤šå™ªå£°é…ç½®ä»¥åŠå˜åŒ–æˆåƒè®¾ç½®ï¼ˆåŒ…æ‹¬åƒç´ åˆ†è¾¨ç‡å’Œå¯è§æ€§åŠ æƒæ–¹æ¡ˆï¼‰æ¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªæ”¶æ•›æ ‡å‡†ï¼Œå³å½“æ•°æ®æ®‹å·®ä¸å™ªå£°å…¼å®¹æ—¶ï¼Œé‡å»ºè¿‡ç¨‹å°†åœæ­¢ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„DNNã€‚è¿™ä¸ä»…é€šè¿‡å‡å°‘è®¡ç®—æˆæœ¬æé«˜äº†é‡å»ºæ•ˆç‡ï¼Œè€Œä¸”è¿˜é€šè¿‡å‰”é™¤é‚£äº›åœ¨è¾¾åˆ°æœ€ä½³æ•°æ®ä¿çœŸæ€§ä¹‹å‰å·²è®­ç»ƒä¸‹ä¸€ä¸ªDNNçš„æ•°æ®&#x2F;å›¾åƒå¯¹æ¥ä¼˜åŒ–è®­ç»ƒã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬ç”¨ä¸€ç§æ–°å‹æ¶æ„ï¼ˆU-WDSRï¼‰æ›¿ä»£R2D2æ—©æœŸçš„U-Net DNNï¼Œè¯¥æ¶æ„ç»“åˆäº†U-Netå’ŒWDSRï¼Œåˆ©ç”¨å®½æ¿€æ´»ã€å¯†é›†è¿æ¥ã€æƒé‡å½’ä¸€åŒ–å’Œä½ç§©å·ç§¯æ¥æé«˜ç‰¹å¾å¤ç”¨å’Œé‡å»ºç²¾åº¦ã€‚ä¸ä¹‹å‰ä¸€æ ·ï¼ŒR2D2æ˜¯é’ˆå¯¹å›ºå®š$512 \times 512$å›¾åƒå¤§å°çš„ç”šå¤§é˜µï¼ˆVLAï¼‰çš„å•è‰²å¼ºåº¦æˆåƒè¿›è¡Œè®­ç»ƒçš„ã€‚åœ¨å¹¿æ³›èŒƒå›´çš„é€†é—®é¢˜å’ŒçœŸå®æ•°æ®çš„æ¡ˆä¾‹ç ”ç©¶ä¸­çš„æ¨¡æ‹Ÿè¡¨æ˜ï¼Œæ–°çš„R2D2æ¨¡å‹åœ¨å›¾åƒé‡å»ºè´¨é‡ã€æ•°æ®ä¿çœŸæ€§å’Œè®¤è¯†ä¸ç¡®å®šæ€§æ–¹é¢å§‹ç»ˆä¼˜äºå…¶æ—©æœŸç‰ˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02554v1">PDF</a> 17 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>R2D2æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç³»åˆ—æœ€è¿‘è¢«å¼•å…¥å°„ç”µå¹²æ¶‰ä»ªçš„å›¾åƒå½¢æˆä¸­ã€‚å¯ä»¥ç†è§£ä¸ºå…¶æ˜¯CLEANçš„ä¸€ç§å­¦ä¹ ç‰ˆæœ¬ï¼Œå…¶å°å‘¨æœŸè¢«DNNå–ä»£ã€‚æœ¬æ–‡ä»åºåˆ—æ”¶æ•›ã€è®­ç»ƒæ–¹æ³•å’ŒDNNæ¶æ„ç­‰æ–¹é¢é‡æ–°å®¡è§†R2D2ï¼Œæé«˜äº†å…¶åœ¨è¶…è¶Šè®­ç»ƒæ¡ä»¶çš„ä¸€èˆ¬æ€§ã€é«˜æ•°æ®ä¿çœŸåº¦å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§æ–¹é¢çš„ç¨³å¥æ€§ã€‚é¦–å…ˆï¼Œåœ¨æœ›è¿œé•œç‰¹å®šè®­ç»ƒçš„åŸºç¡€ä¸Šï¼Œé€šè¿‡éšæœºå‚…é‡Œå¶é‡‡æ ·ç§¯åˆ†æ—¶é—´ã€èå…¥å¤šç§æ‰«æå¤šç§å™ªå£°é…ç½®ä»¥åŠå˜åŒ–æˆåƒè®¾ç½®ï¼ˆåŒ…æ‹¬åƒç´ åˆ†è¾¨ç‡å’Œå¯è§æ€§åŠ æƒæ–¹æ¡ˆï¼‰æ¥å¢å¼ºå­¦ä¹ è¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œå¼•å…¥æ”¶æ•›æ ‡å‡†ï¼Œå½“æ•°æ®æ®‹å·®ä¸å™ªå£°å…¼å®¹æ—¶ï¼Œé‡å»ºè¿‡ç¨‹åœæ­¢ï¼Œè€Œéä»…ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„DNNsã€‚è¿™ä¸ä»…æé«˜äº†é‡å»ºæ•ˆç‡å¹¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”è¿˜é€šè¿‡å‰”é™¤é‚£äº›å·²è¾¾åˆ°æœ€ä½³æ•°æ®ä¿çœŸåº¦çš„æ•°æ®&#x2F;å›¾åƒå¯¹æ¥ä¼˜åŒ–è®­ç»ƒï¼Œä¸ºè®­ç»ƒä¸‹ä¸€ä¸ªDNNæ‰“ä¸‹åŸºç¡€ã€‚å†æ¬¡ï¼Œç”¨æ–°å‹æ¶æ„U-WDSRæ›¿ä»£R2D2æ—©æœŸçš„U-Net DNNï¼Œç»“åˆU-Netå’ŒWDSRçš„ä¼˜åŠ¿ï¼Œåˆ©ç”¨å®½æ¿€æ´»ã€å¯†é›†è¿æ¥ã€æƒé‡å½’ä¸€åŒ–å’Œä½ç§©å·ç§¯æ¥æé«˜ç‰¹å¾å¤ç”¨å’Œé‡å»ºç²¾åº¦ã€‚å¦‚å…ˆå‰æ‰€è¿°ï¼ŒR2D2æ˜¯ä»¥å›ºå®š$ 512 \times 512$å›¾åƒå¤§å°ï¼Œä½¿ç”¨éå¸¸å¤§è§„æ¨¡é˜µåˆ—ï¼ˆVLAï¼‰è¿›è¡Œå•è‰²å¼ºåº¦æˆåƒè®­ç»ƒçš„ã€‚åœ¨å¤šç§åé—®é¢˜çš„æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®çš„æ¡ˆä¾‹ç ”ç©¶ä¸Šï¼Œæ–°å‹R2D2æ¨¡å‹åœ¨å›¾åƒé‡å»ºè´¨é‡ã€æ•°æ®ä¿çœŸåº¦å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§æ–¹é¢å‡è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>R2D2ç³»åˆ—åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼Œæ—¨åœ¨æ”¹è¿›å›¾åƒå½¢æˆä¸­çš„CLEANæ–¹æ³•ï¼Œä½¿ç”¨ä¸€ç³»åˆ—DNNæ›¿ä»£å°å‘¨æœŸè¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡å¯¹è®­ç»ƒæ–¹æ³•çš„æ”¹è¿›å’Œå¼•å…¥æ–°çš„æ”¶æ•›æ ‡å‡†æ¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚åŒ…æ‹¬éšæœºåŒ–å‚…é‡Œå¶é‡‡æ ·é›†æˆæ—¶é—´ã€èå…¥å¤šç§å™ªå£°é…ç½®ä»¥åŠçµæ´»çš„æˆåƒè®¾ç½®ç­‰ã€‚</li>
<li>é‡‡ç”¨æ–°å‹æ¶æ„U-WDSRæ›¿ä»£æ—©æœŸU-Net DNNï¼Œç»“åˆU-Netå’ŒWDSRçš„ç‰¹ç‚¹ä»¥æé«˜ç‰¹å¾å¤ç”¨å’Œé‡å»ºç²¾åº¦ã€‚</li>
<li>R2D2çš„æ–°æ¨¡å‹åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®çš„æµ‹è¯•ä¸­ï¼Œè¡¨ç°å‡ºè‰²çš„å›¾åƒé‡å»ºè´¨é‡ã€æ•°æ®ä¿çœŸåº¦å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5485d596c3e5699a4cd2a94f99ab9f98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-588b1ee3c9f0372de837eb24419f6ab7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Federated-nnU-Net-for-Privacy-Preserving-Medical-Image-Segmentation"><a href="#Federated-nnU-Net-for-Privacy-Preserving-Medical-Image-Segmentation" class="headerlink" title="Federated nnU-Net for Privacy-Preserving Medical Image Segmentation"></a>Federated nnU-Net for Privacy-Preserving Medical Image Segmentation</h2><p><strong>Authors:Grzegorz Skorupko, Fotios Avgoustidis, Carlos MartÃ­n-Isla, Lidia Garrucho, Dimitri A. Kessler, Esmeralda Ruiz Pujadas, Oliver DÃ­az, Maciej Bobowicz, Katarzyna GwoÅºdziewicz, Xavier BargallÃ³, Paulius JaruÅ¡eviÄius, Kaisar Kushibar, Karim Lekadir</strong></p>
<p>The nnU-Net framework has played a crucial role in medical image segmentation and has become the gold standard in multitudes of applications targeting different diseases, organs, and modalities. However, so far it has been used primarily in a centralized approach where the data collected from hospitals are stored in one center and used to train the nnU-Net. This centralized approach has various limitations, such as leakage of sensitive patient information and violation of patient privacy. Federated learning is one of the approaches to train a segmentation model in a decentralized manner that helps preserve patient privacy. In this paper, we propose FednnU-Net, a federated learning extension of nnU-Net. We introduce two novel federated learning methods to the nnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric Federated Averaging (AsymFedAvg) - and experimentally show their consistent performance for breast, cardiac and fetal segmentation using 6 datasets representing samples from 18 institutions. Additionally, to further promote research and deployment of decentralized training in privacy constrained institutions, we make our plug-n-play framework public. The source-code is available at <a target="_blank" rel="noopener" href="https://github.com/faildeny/FednnUNet">https://github.com/faildeny/FednnUNet</a> . </p>
<blockquote>
<p>nnU-Netæ¡†æ¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ‰®æ¼”äº†å…³é”®è§’è‰²ï¼Œå¹¶å·²æˆä¸ºé’ˆå¯¹ä¸åŒç–¾ç—…ã€å™¨å®˜å’Œæ¨¡å¼çš„å¤šé‡åº”ç”¨çš„é»„é‡‘æ ‡å‡†ã€‚ç„¶è€Œï¼Œè¿„ä»Šä¸ºæ­¢ï¼Œå®ƒä¸»è¦è¢«ç”¨äºé›†ä¸­å¼æ–¹æ³•ï¼Œå…¶ä¸­ä»åŒ»é™¢æ”¶é›†çš„æ•°æ®è¢«å­˜å‚¨åœ¨ä¸€ä¸ªä¸­å¿ƒå¹¶ä½¿ç”¨å®ƒæ¥è®­ç»ƒnnU-Netã€‚è¿™ç§é›†ä¸­å¼æ–¹æ³•å…·æœ‰å„ç§å±€é™æ€§ï¼Œä¾‹å¦‚æ•æ„Ÿç—…äººä¿¡æ¯çš„æ³„éœ²å’Œç—…äººéšç§çš„ä¾µçŠ¯ã€‚è”é‚¦å­¦ä¹ æ˜¯ä¸€ç§åˆ†æ•£å¼è®­ç»ƒåˆ†å‰²æ¨¡å‹çš„æ–¹æ³•ï¼Œæœ‰åŠ©äºä¿æŠ¤ç—…äººéšç§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FednnU-Netï¼Œè¿™æ˜¯nnU-Netçš„è”é‚¦å­¦ä¹ æ‰©å±•ã€‚æˆ‘ä»¬å‘nnU-Netæ¡†æ¶å¼•å…¥äº†ä¸¤ç§æ–°é¢–çš„è”é‚¦å­¦ä¹ æ–¹æ³•â€”â€”è”é‚¦æŒ‡çº¹æå–ï¼ˆFFEï¼‰å’Œä¸å¯¹ç§°è”é‚¦å¹³å‡ï¼ˆAsymFedAvgï¼‰â€”â€”å¹¶é€šè¿‡å®éªŒå±•ç¤ºäº†å®ƒä»¬åœ¨ä»£è¡¨æ¥è‡ª18ä¸ªæœºæ„çš„æ ·æœ¬çš„6ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œä¹³æˆ¿ã€å¿ƒè„å’Œèƒå„¿åˆ†å‰²çš„ä¸€è‡´æ€§æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥ä¿ƒè¿›åœ¨å—éšç§çº¦æŸçš„æœºæ„ä¸­è¿›è¡Œåˆ†æ•£å¼è®­ç»ƒçš„ç ”ç©¶å’Œéƒ¨ç½²ï¼Œæˆ‘ä»¬å…¬å¼€äº†å³æ’å³ç”¨çš„æ¡†æ¶ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/faildeny/FednnUNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/faildeny/FednnUNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02549v1">PDF</a> In review</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼ŒnnU-Netæ¡†æ¶å·²è¢«å¹¿æ³›åº”ç”¨å¹¶æˆä¸ºå¤šç§ç–¾ç—…å’Œåº”ç”¨çš„æ ‡å‡†å·¥å…·ã€‚ä½†å½“å‰å…¶ä¸»è¦ç”¨äºé›†ä¸­å¼å¤„ç†æ•°æ®çš„æ–¹å¼å­˜åœ¨æ³„éœ²æ•æ„Ÿä¿¡æ¯çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†FednnU-Netï¼Œä¸€ä¸ªåŸºäºè”é‚¦å­¦ä¹ çš„nnU-Netæ‰©å±•æ¡†æ¶ï¼Œå¹¶å¼•å…¥ä¸¤ç§æ–°æ–¹æ³•â€”â€”è”é‚¦æŒ‡çº¹æå–å’Œä¸å¯¹ç§°è”é‚¦å¹³å‡æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¹³æˆ¿ã€å¿ƒè„å’Œèƒå„¿åˆ†å‰²æ–¹é¢å…·æœ‰æŒç»­æ€§èƒ½ã€‚åŒæ—¶å…¬å¼€æºä»£ç ï¼Œä»¥ä¿ƒè¿›åœ¨éšç§å—é™æœºæ„ä¸­çš„åˆ†æ•£è®­ç»ƒç ”ç©¶å’Œéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>nnU-Netæ¡†æ¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸæœ‰é‡è¦ä½œç”¨ï¼Œå¹¶æˆä¸ºå¤šç§ç–¾ç—…åº”ç”¨çš„æ ‡å‡†å·¥å…·ã€‚</li>
<li>å½“å‰nnU-Netä¸»è¦é‡‡ç”¨çš„é›†ä¸­å¼æ•°æ®å¤„ç†æ–¹å¼å­˜åœ¨æ‚£è€…ä¿¡æ¯æ³„éœ²å’Œéšç§ä¾µçŠ¯çš„é—®é¢˜ã€‚</li>
<li>è”é‚¦å­¦ä¹ æ˜¯ä¸€ç§è®­ç»ƒåˆ†å‰²æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œæœ‰åŠ©äºä¿æŠ¤æ‚£è€…éšç§ã€‚</li>
<li>FednnU-Netæ˜¯nnU-Netçš„è”é‚¦å­¦ä¹ æ‰©å±•æ¡†æ¶ã€‚</li>
<li>FFEï¼ˆè”é‚¦æŒ‡çº¹æå–ï¼‰å’ŒAsymFedAvgï¼ˆä¸å¯¹ç§°è”é‚¦å¹³å‡æ³•ï¼‰æ˜¯FednnU-Netä¸­çš„ä¸¤ç§æ–°æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFednnU-Netåœ¨ä¹³æˆ¿ã€å¿ƒè„å’Œèƒå„¿åˆ†å‰²æ–¹é¢è¡¨ç°æŒç»­æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e631ab36678f74747d50ca1714a091f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d735ffb9a60d37f912c33a797ccab355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f20f66de2649a1ba03b47746b0fd1424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e06f7c0f55969bbb8d0f7a89bc3c3481.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f75eda7d927c44750b50f0c940a7f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-758c0f2844eed997bc50089bc5b0af00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-399eb0121bc28054090b66906a5e3b49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6ef31a921d6ff6278f1542eb6245ff3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-582290094d9bed3865b59b3a70c53bd5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CQ-CNN-A-Hybrid-Classical-Quantum-Convolutional-Neural-Network-for-Alzheimerâ€™s-Disease-Detection-Using-Diffusion-Generated-and-U-Net-Segmented-3D-MRI"><a href="#CQ-CNN-A-Hybrid-Classical-Quantum-Convolutional-Neural-Network-for-Alzheimerâ€™s-Disease-Detection-Using-Diffusion-Generated-and-U-Net-Segmented-3D-MRI" class="headerlink" title="CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for   Alzheimerâ€™s Disease Detection Using Diffusion Generated and U Net Segmented   3D MRI"></a>CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for   Alzheimerâ€™s Disease Detection Using Diffusion Generated and U Net Segmented   3D MRI</h2><p><strong>Authors:Mominul Islam, Mohammad Junayed Hasan, M. R. C. Mahdy</strong></p>
<p>The detection of Alzheimer disease (AD) from clinical MRI data is an active area of research in medical imaging. Recent advances in quantum computing, particularly the integration of parameterized quantum circuits (PQCs) with classical machine learning architectures, offer new opportunities to develop models that may outperform traditional methods. However, quantum machine learning (QML) remains in its early stages and requires further experimental analysis to better understand its behavior and limitations. In this paper, we propose an end to end hybrid classical quantum convolutional neural network (CQ CNN) for AD detection using clinically formatted 3D MRI data. Our approach involves developing a framework to make 3D MRI data usable for machine learning, designing and training a brain tissue segmentation model (Skull Net), and training a diffusion model to generate synthetic images for the minority class. Our converged models exhibit potential quantum advantages, achieving higher accuracy in fewer epochs than classical models. The proposed beta8 3 qubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA) models while requiring significantly fewer computational resources. In particular, the architecture employs only 13K parameters (0.48 MB), reducing the parameter count by more than 99.99% compared to current SOTA models. Furthermore, the diffusion-generated data used to train our quantum models, in conjunction with real samples, preserve clinical structural standards, representing a notable first in the field of QML. We conclude that CQCNN architecture like models, with further improvements in gradient optimization techniques, could become a viable option and even a potential alternative to classical models for AD detection, especially in data limited and resource constrained clinical settings. </p>
<blockquote>
<p>ä»ä¸´åºŠMRIæ•°æ®æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯åŒ»å­¦æˆåƒé¢†åŸŸçš„ä¸€ä¸ªç ”ç©¶çƒ­ç‚¹ã€‚é‡å­è®¡ç®—çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCï¼‰ä¸ç»å…¸æœºå™¨å­¦ä¹ æ¶æ„çš„é›†æˆï¼Œä¸ºå¼€å‘å¯èƒ½è¶…è¶Šä¼ ç»Ÿæ–¹æ³•çš„æ¨¡å‹æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œé‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œéœ€è¦è¿›ä¸€æ­¥å®éªŒåˆ†æä»¥æ›´å¥½åœ°äº†è§£å…¶è¡Œä¸ºå’Œå±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ··åˆç»å…¸é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCQCNNï¼‰ï¼Œç”¨äºä½¿ç”¨ä¸´åºŠæ ¼å¼çš„3D MRIæ•°æ®è¿›è¡ŒADæ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å¼€å‘ä¸€ä¸ªä½¿3D MRIæ•°æ®å¯ç”¨äºæœºå™¨å­¦ä¹ çš„æ¡†æ¶ï¼Œè®¾è®¡å’Œè®­ç»ƒè„‘ç»„ç»‡åˆ†å‰²æ¨¡å‹ï¼ˆSkull Netï¼‰ï¼Œä»¥åŠè®­ç»ƒä¸€ä¸ªæ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆå°‘æ•°ç±»åˆ«çš„åˆæˆå›¾åƒã€‚æˆ‘ä»¬æ”¶æ•›çš„æ¨¡å‹æ˜¾ç¤ºå‡ºæ½œåœ¨çš„é‡å­ä¼˜åŠ¿ï¼Œä»¥è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°å®ç°äº†æ¯”ç»å…¸æ¨¡å‹æ›´é«˜çš„ç²¾åº¦ã€‚æ‰€æå‡ºçš„beta8  3é‡å­ä½æ¨¡å‹è¾¾åˆ°äº†97.50%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†æœ€æ–°æ¨¡å‹ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®¡ç®—èµ„æºéœ€æ±‚ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯¥æ¶æ„ä»…ä½¿ç”¨13Kä¸ªå‚æ•°ï¼ˆ0.48 MBï¼‰ï¼Œä¸å½“å‰æœ€æ–°æ¨¡å‹ç›¸æ¯”ï¼Œå‚æ•°æ•°é‡å‡å°‘äº†99.99%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œç”¨äºè®­ç»ƒæˆ‘ä»¬é‡å­æ¨¡å‹çš„æ‰©æ•£ç”Ÿæˆæ•°æ®ä¸å®é™…æ ·æœ¬ç›¸ç»“åˆï¼Œä¿æŒäº†ä¸´åºŠç»“æ„æ ‡å‡†ï¼Œè¿™åœ¨è¯¥é¢†åŸŸå°šå±é¦–æ¬¡ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼ŒåƒCQCNNè¿™æ ·çš„æ¶æ„ï¼Œåœ¨æ¢¯åº¦ä¼˜åŒ–æŠ€æœ¯æ–¹é¢è¿›ä¸€æ­¥æ”¹è¿›åï¼Œæœ‰å¯èƒ½æˆä¸ºæ£€æµ‹ADçš„å¯è¡Œé€‰é¡¹ï¼Œç”šè‡³æ˜¯ç»å…¸æ¨¡å‹çš„æ½œåœ¨æ›¿ä»£æ–¹æ¡ˆï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™å’Œèµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02345v1">PDF</a> Application of hybrid quantum-classical machine learning for (early   stage) disease detection</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨ä¸´åºŠMRIæ•°æ®æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„æ–¹æ³•ï¼Œå¹¶ä»‹ç»äº†é‡å­è®¡ç®—ä¸ç»å…¸æœºå™¨å­¦ä¹ ç›¸ç»“åˆåœ¨è¯¥é¢†åŸŸçš„åº”ç”¨ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ··åˆç»å…¸é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCQCNNï¼‰æ¨¡å‹ï¼Œç”¨äºä½¿ç”¨ä¸´åºŠæ ¼å¼çš„3D MRIæ•°æ®è¿›è¡ŒADæ£€æµ‹ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼€å‘ä¸€ä¸ªä½¿3D MRIæ•°æ®é€‚ç”¨äºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ã€è®¾è®¡å¹¶è®­ç»ƒè„‘ç»„ç»‡çš„åˆ†å‰²æ¨¡å‹ï¼ˆSkull Netï¼‰ä»¥åŠè®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå°‘æ•°ç±»çš„åˆæˆå›¾åƒï¼Œå±•ç°äº†æ½œåœ¨çš„é‡å­ä¼˜åŠ¿ã€‚è¯¥æ¨¡å‹åœ¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°å†…å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³çš„æ¨¡å‹ï¼Œå¹¶ä¸”å…·æœ‰æ›´å°‘çš„è®¡ç®—èµ„æºéœ€æ±‚ã€‚è¯¥ç ”ç©¶å¾—å‡ºç»“è®ºï¼Œè¿›ä¸€æ­¥çš„æ¢¯åº¦ä¼˜åŒ–æŠ€æœ¯æ”¹è¿›åçš„CQCNNæ¨¡å‹å¯èƒ½æˆä¸ºç»å…¸æ¨¡å‹çš„å¯è¡Œé€‰æ‹©ï¼Œç”šè‡³åœ¨æ•°æ®æœ‰é™å’Œèµ„æºå—é™çš„ä¸´åºŠç¯å¢ƒä¸­æˆä¸ºæ½œåœ¨æ›¿ä»£å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é¢†åŸŸï¼šæ–‡ç« æ¢è®¨äº†åŒ»å­¦æˆåƒä¸­é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨ä¸´åºŠMRIæ•°æ®ã€‚</li>
<li>é‡å­è®¡ç®—ä¸æœºå™¨å­¦ä¹ ç»“åˆï¼šæ–‡ç« å±•ç¤ºäº†å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCï¼‰ä¸ç»å…¸æœºå™¨å­¦ä¹ æ¶æ„çš„ç»“åˆå¦‚ä½•ä¸ºADæ£€æµ‹æä¾›æ–°çš„æœºä¼šã€‚</li>
<li>æ··åˆç»å…¸é‡å­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCQCNNï¼‰ï¼šæå‡ºäº†ä¸€ç§ç”¨äºADæ£€æµ‹çš„ç«¯åˆ°ç«¯CQCNNæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨ä¸´åºŠæ ¼å¼çš„3D MRIæ•°æ®ã€‚</li>
<li>æ¨¡å‹ä¼˜åŠ¿ï¼šCQCNNæ¨¡å‹å±•ç°äº†æ½œåœ¨çš„é‡å­ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜å‡†ç¡®æ€§ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³çš„å…¶ä»–æ¨¡å‹ã€‚</li>
<li>èµ„æºæ•ˆç‡ï¼šè¯¥æ¨¡å‹éœ€è¦æ˜¾è‘—æ›´å°‘çš„è®¡ç®—èµ„æºï¼Œå¹¶ä¸”å…·æœ‰è¾ƒå°‘çš„å‚æ•°ï¼Œè¿™ä½¿å…¶æˆä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„ç†æƒ³é€‰æ‹©ã€‚</li>
<li>æ•°æ®ç”Ÿæˆï¼šæ–‡ç« æåˆ°äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒæ¥è®­ç»ƒé‡å­æ¨¡å‹çš„æ–¹æ³•ï¼Œè¿™äº›åˆæˆå›¾åƒä¸çœŸå®æ ·æœ¬ä¸€èµ·ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†ä¸´åºŠç»“æ„æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96a969b5873c7b5ffcec70f2925fd108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b83648d11db9a64f40e590d8969f47bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5487dee610cff690eda703f5737c5ebe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c101506b9fc752d21a063eab4fc83491.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e68e4738f82a7d6555b9690e77252424.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CrossFusion-A-Multi-Scale-Cross-Attention-Convolutional-Fusion-Model-for-Cancer-Survival-Prediction"><a href="#CrossFusion-A-Multi-Scale-Cross-Attention-Convolutional-Fusion-Model-for-Cancer-Survival-Prediction" class="headerlink" title="CrossFusion: A Multi-Scale Cross-Attention Convolutional Fusion Model   for Cancer Survival Prediction"></a>CrossFusion: A Multi-Scale Cross-Attention Convolutional Fusion Model   for Cancer Survival Prediction</h2><p><strong>Authors:Rustin Soraki, Huayu Wang, Joann G. Elmore, Linda Shapiro</strong></p>
<p>Cancer survival prediction from whole slide images (WSIs) is a challenging task in computational pathology due to the large size, irregular shape, and high granularity of the WSIs. These characteristics make it difficult to capture the full spectrum of patterns, from subtle cellular abnormalities to complex tissue interactions, which are crucial for accurate prognosis. To address this, we propose CrossFusion, a novel multi-scale feature integration framework that extracts and fuses information from patches across different magnification levels. By effectively modeling both scale-specific patterns and their interactions, CrossFusion generates a rich feature set that enhances survival prediction accuracy. We validate our approach across six cancer types from public datasets, demonstrating significant improvements over existing state-of-the-art methods. Moreover, when coupled with domain-specific feature extraction backbones, our method shows further gains in prognostic performance compared to general-purpose backbones. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/RustinS/CrossFusion">https://github.com/RustinS/CrossFusion</a> </p>
<blockquote>
<p>ä»å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œç™Œç—‡ç”Ÿå­˜é¢„æµ‹æ˜¯è®¡ç®—ç—…ç†å­¦ä¸­çš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå…¨å¹»ç¯ç‰‡å›¾åƒå…·æœ‰å°ºå¯¸å¤§ã€å½¢çŠ¶ä¸è§„åˆ™å’Œç²’åº¦é«˜ç­‰ç‰¹ç‚¹ã€‚è¿™äº›ç‰¹æ€§å¯¼è‡´éš¾ä»¥æ•æ‰ä»å¾®å¦™çš„ç»†èƒå¼‚å¸¸åˆ°å¤æ‚çš„ç»„ç»‡ç›¸äº’ä½œç”¨çš„å®Œæ•´æ¨¡å¼è°±ï¼Œè¿™å¯¹äºå‡†ç¡®çš„é¢„åè‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CrossFusionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦ç‰¹å¾é›†æˆæ¡†æ¶ï¼Œå¯ä»¥ä»ä¸åŒæ”¾å¤§çº§åˆ«çš„è¡¥ä¸ä¸­æå–å¹¶èåˆä¿¡æ¯ã€‚é€šè¿‡æœ‰æ•ˆåœ°å»ºæ¨¡ç‰¹å®šå°ºåº¦çš„æ¨¡å¼åŠå…¶ç›¸äº’ä½œç”¨ï¼ŒCrossFusionç”Ÿæˆäº†ä¸€ä¸ªä¸°å¯Œçš„ç‰¹å¾é›†ï¼Œæé«˜äº†ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å…¬å…±æ•°æ®é›†çš„å…­ç§ç™Œç—‡ç±»å‹ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†ä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œå½“ä¸ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾æå–éª¨å¹²ç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢„åæ€§èƒ½ä¸Šç›¸æ¯”é€šç”¨éª¨å¹²è¡¨ç°å‡ºè¿›ä¸€æ­¥çš„æå‡ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/RustinS/CrossFusion%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/RustinS/CrossFusionè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02064v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCrossFusionçš„æ–°å‹å¤šå°ºåº¦ç‰¹å¾èåˆæ¡†æ¶ï¼Œç”¨äºä»å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰ä¸­æŠ½å–å¹¶èåˆä¸åŒæ”¾å¤§å€æ•°ä¸‹çš„æ–‘å—ä¿¡æ¯ï¼Œä»¥è§£å†³è®¡ç®—ç—…ç†å­¦ä¸­çš„ç™Œç—‡ç”Ÿå­˜é¢„æµ‹æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆå»ºæ¨¡å„å°ºåº¦ä¸‹çš„æ¨¡å¼åŠå…¶ç›¸äº’ä½œç”¨ï¼Œç”Ÿæˆä¸°å¯Œçš„ç‰¹å¾é›†ï¼Œæé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å…­ç§ç™Œç—‡ç±»å‹éªŒè¯æ˜¾ç¤ºï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚å½“ä¸ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾æå–éª¨å¹²ç›¸ç»“åˆæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨é¢„åæ€§èƒ½ä¸Šè¡¨ç°å‡ºè¿›ä¸€æ­¥çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CrossFusionæ˜¯ä¸€ä¸ªå¤šå°ºåº¦ç‰¹å¾èåˆæ¡†æ¶ï¼Œç”¨äºå¤„ç†å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰çš„ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†è®¡ç®—ç—…ç†å­¦ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚å›¾åƒå¤§å°ã€å½¢çŠ¶çš„ä¸è§„åˆ™å’Œç²’åº¦é«˜ã€‚</li>
<li>CrossFusioné€šè¿‡æŠ½å–å¹¶èåˆä¸åŒæ”¾å¤§å€æ•°ä¸‹çš„æ–‘å—ä¿¡æ¯ï¼Œç”Ÿæˆä¸°å¯Œçš„ç‰¹å¾é›†ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå»ºæ¨¡å„å°ºåº¦ä¸‹çš„æ¨¡å¼åŠå…¶ç›¸äº’ä½œç”¨ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å…­ç§ç™Œç—‡ç±»å‹éªŒè¯ï¼ŒCrossFusionæ˜¾è‘—æé«˜äº†ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸é€šç”¨ç‰¹å¾æå–éª¨å¹²ç›¸æ¯”ï¼Œä¸ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾æå–éª¨å¹²ç»“åˆæ—¶ï¼ŒCrossFusionçš„é¢„åæ€§èƒ½æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed3a369da559219e45934a2f92ace1e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-888a5cb0e6471b94510f136ed0b20442.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4995ce1adec4cb67e18336fca186d204.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Primus-Enforcing-Attention-Usage-for-3D-Medical-Image-Segmentation"><a href="#Primus-Enforcing-Attention-Usage-for-3D-Medical-Image-Segmentation" class="headerlink" title="Primus: Enforcing Attention Usage for 3D Medical Image Segmentation"></a>Primus: Enforcing Attention Usage for 3D Medical Image Segmentation</h2><p><strong>Authors:Tassilo Wald, Saikat Roy, Fabian Isensee, Constantin Ulrich, Sebastian Ziegler, Dasha Trofimova, Raphael Stock, Michael Baumgartner, Gregor KÃ¶hler, Klaus Maier-Hein</strong></p>
<p>Transformers have achieved remarkable success across multiple fields, yet their impact on 3D medical image segmentation remains limited with convolutional networks still dominating major benchmarks. In this work, we a) analyze current Transformer-based segmentation models and identify critical shortcomings, particularly their over-reliance on convolutional blocks. Further, we demonstrate that in some architectures, performance is unaffected by the absence of the Transformer, thereby demonstrating their limited effectiveness. To address these challenges, we move away from hybrid architectures and b) introduce a fully Transformer-based segmentation architecture, termed Primus. Primus leverages high-resolution tokens, combined with advances in positional embeddings and block design, to maximally leverage its Transformer blocks. Through these adaptations Primus surpasses current Transformer-based methods and competes with state-of-the-art convolutional models on multiple public datasets. By doing so, we create the first pure Transformer architecture and take a significant step towards making Transformers state-of-the-art for 3D medical image segmentation. </p>
<blockquote>
<p>Transformeråœ¨å¤šé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ï¼Œå…¶å½±å“ä»ç„¶æœ‰é™ï¼Œå·ç§¯ç½‘ç»œä»ç„¶åœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸­å æ®ä¸»å¯¼åœ°ä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬a)åˆ†æäº†å½“å‰çš„åŸºäºTransformerçš„åˆ†å‰²æ¨¡å‹ï¼Œå¹¶è¯†åˆ«å‡ºäº†å…³é”®çŸ­æ¿ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬å¯¹å·ç§¯æ¨¡å—çš„è¿‡åº¦ä¾èµ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜åœ¨æŸäº›æ¶æ„ä¸­ï¼Œç§»é™¤Transformerå¯¹æ€§èƒ½å¹¶æ— å½±å“ï¼Œä»è€Œè¯æ˜äº†å…¶æœ‰é™çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ”¾å¼ƒäº†æ··åˆæ¶æ„ï¼Œb)æå‡ºäº†ä¸€ç§å…¨æ–°çš„åŸºäºTransformerçš„åˆ†å‰²æ¶æ„ï¼Œåä¸ºPrimusã€‚Primusåˆ©ç”¨é«˜åˆ†è¾¨ç‡æ ‡è®°ï¼Œç»“åˆä½ç½®åµŒå…¥å’Œå—è®¾è®¡çš„è¿›æ­¥ï¼Œæœ€å¤§é™åº¦åœ°åˆ©ç”¨å…¶Transformeræ¨¡å—ã€‚é€šè¿‡è¿™äº›è°ƒæ•´ï¼ŒPrimusè¶…è¶Šäº†å½“å‰çš„åŸºäºTransformerçš„æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šä¸æœ€å…ˆè¿›çš„å·ç§¯æ¨¡å‹ç›¸ç«äº‰ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬åˆ›å»ºäº†ç¬¬ä¸€ä¸ªçº¯Transformeræ¶æ„ï¼Œæœç€ä½¿Transformeræˆä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„æœ€å…ˆè¿›æŠ€æœ¯è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01835v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å“è¶Šè¡¨ç°åŠå…¶åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„åº”ç”¨ç°çŠ¶ã€‚æ–‡ç« åˆ†æäº†å½“å‰åŸºäºTransformerçš„åˆ†å‰²æ¨¡å‹çš„ä¸è¶³ï¼Œå¦‚è¿‡åº¦ä¾èµ–å·ç§¯å—çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§å…¨æ–°çš„çº¯Transformeræ¶æ„â€”â€”Primusï¼Œè¯¥æ¶æ„åˆ©ç”¨é«˜åˆ†è¾¨ç‡ä»¤ç‰Œã€å…ˆè¿›çš„å®šä½åµŒå…¥å’Œå—è®¾è®¡ï¼Œæœ€å¤§åŒ–åœ°åˆ©ç”¨Transformerå—ã€‚Primusåœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„åŸºäºTransformerçš„æ–¹æ³•ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„å·ç§¯æ¨¡å‹ç›¸ç«äº‰ã€‚è¿™ä¸ºTransformeråœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„åº”ç”¨æ ‘ç«‹äº†æ–°çš„é‡Œç¨‹ç¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æˆåŠŸåŠå…¶åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„æ½œåŠ›ã€‚</li>
<li>å½“å‰åŸºäºTransformerçš„åˆ†å‰²æ¨¡å‹å­˜åœ¨è¿‡åº¦ä¾èµ–å·ç§¯å—çš„ä¸è¶³ã€‚</li>
<li>ä¸€äº›æ¶æ„ä¸­Transformerçš„ä½œç”¨æœ‰é™ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å—é™ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§å…¨æ–°çš„çº¯Transformeræ¶æ„â€”â€”Primusã€‚</li>
<li>Primusåˆ©ç”¨é«˜åˆ†è¾¨ç‡ä»¤ç‰Œå’Œå…ˆè¿›çš„å®šä½åµŒå…¥åŠå—è®¾è®¡æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>Primusåœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„åŸºäºTransformerçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f8765ac2d7ee6f733d577cf0ee0ad71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff2c4138bd12dea3a227c791beb68329.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba75fbe69df62be19010d1dff5b70b04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54059dbd1da024a8bb39176764a6ded8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3332519c5a349f771d4745282d9aeb6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a844bba9e9eafa2e6e2bd3c861a69e4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OFF-CLIP-Improving-Normal-Detection-Confidence-in-Radiology-CLIP-with-Simple-Off-Diagonal-Term-Auto-Adjustment"><a href="#OFF-CLIP-Improving-Normal-Detection-Confidence-in-Radiology-CLIP-with-Simple-Off-Diagonal-Term-Auto-Adjustment" class="headerlink" title="OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with   Simple Off-Diagonal Term Auto-Adjustment"></a>OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with   Simple Off-Diagonal Term Auto-Adjustment</h2><p><strong>Authors:Junhyun Park, Chanyu Moon, Donghwan Lee, Kyungsu Kim, Minho Hwang</strong></p>
<p>Contrastive Language-Image Pre-Training (CLIP) has enabled zero-shot classification in radiology, reducing reliance on manual annotations. However, conventional contrastive learning struggles with normal case detection due to its strict intra-sample alignment, which disrupts normal sample clustering and leads to high false positives (FPs) and false negatives (FNs). To address these issues, we propose OFF-CLIP, a contrastive learning refinement that improves normal detection by introducing an off-diagonal term loss to enhance normal sample clustering and applying sentence-level text filtering to mitigate FNs by removing misaligned normal statements from abnormal reports. OFF-CLIP can be applied to radiology CLIP models without requiring any architectural modifications. Experimental results show that OFF-CLIP significantly improves normal classification, achieving a 0.61 Area under the curve (AUC) increase on VinDr-CXR over CARZero, the state-of-the-art zero-shot classification baseline, while maintaining or improving abnormal classification performance. Additionally, OFF-CLIP enhances zero-shot grounding by improving pointing game accuracy, confirming better anomaly localization. These results demonstrate OFF-CLIPâ€™s effectiveness as a robust and efficient enhancement for medical vision-language models. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å·²ç»åœ¨æ”¾å°„å­¦çš„é›¶æ ·æœ¬åˆ†ç±»ä¸­å¾—åˆ°äº†åº”ç”¨ï¼Œå‡å°‘äº†å¯¹æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¯¹æ¯”å­¦ä¹ åœ¨å¸¸è§„æƒ…å†µæ£€æµ‹æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œå› ä¸ºå…¶ä¸¥æ ¼çš„æ ·æœ¬å†…å¯¹é½ä¼šç ´åæ­£å¸¸æ ·æœ¬çš„èšç±»ï¼Œå¯¼è‡´é«˜è¯¯æŠ¥ç‡ï¼ˆFPsï¼‰å’Œå‡é˜´æ€§ï¼ˆFNsï¼‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OFF-CLIPï¼Œè¿™æ˜¯ä¸€ç§å¯¹æ¯”å­¦ä¹ çš„æ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥éå¯¹è§’çº¿é¡¹æŸå¤±æ¥å¢å¼ºæ­£å¸¸æ ·æœ¬çš„èšç±»ï¼Œå¹¶åº”ç”¨å¥å­çº§æ–‡æœ¬è¿‡æ»¤ï¼Œé€šè¿‡ä»å¼‚å¸¸æŠ¥å‘Šä¸­åˆ é™¤é”™ä½æ­£å¸¸é™ˆè¿°æ¥å‡è½»FNsï¼Œä»è€Œæé«˜äº†æ­£å¸¸æ£€æµ‹ã€‚OFF-CLIPå¯åº”ç”¨äºæ”¾å°„å­¦CLIPæ¨¡å‹ï¼Œæ— éœ€è¿›è¡Œä»»ä½•æ¶æ„ä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOFF-CLIPåœ¨VinDr-CXRä¸Šç›¸å¯¹äºå½“å‰æœ€å…ˆè¿›çš„é›¶æ ·æœ¬åˆ†ç±»åŸºçº¿CARZeroï¼Œæ˜¾è‘—æé«˜äº†æ­£å¸¸åˆ†ç±»çš„æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰æé«˜äº†0.61%ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†å¼‚å¸¸åˆ†ç±»æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒOFF-CLIPé€šè¿‡æé«˜æŒ‡å‘æ¸¸æˆå‡†ç¡®æ€§ï¼Œå¢å¼ºäº†é›¶æ ·æœ¬å®šä½ï¼Œè¯å®äº†å…¶å¼‚å¸¸å®šä½èƒ½åŠ›æ›´å¼ºã€‚è¿™äº›ç»“æœè¯æ˜äº†OFF-CLIPä½œä¸ºåŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹çš„ç¨³å¥ã€é«˜æ•ˆå¢å¼ºçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01794v1">PDF</a> 10 pages, 3 figures, and 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨æ”¾å°„å­¦é›¶æ ·æœ¬åˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿå¯¹æ¯”å­¦ä¹ åœ¨æ­£å¸¸æ ·æœ¬æ£€æµ‹æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºOFF-CLIPæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç¦»å¯¹è§’çº¿é¡¹æŸå¤±å¢å¼ºæ­£å¸¸æ ·æœ¬èšç±»ï¼Œå¹¶åº”ç”¨å¥å­çº§æ–‡æœ¬è¿‡æ»¤æ¥å‡å°‘è¯¯å¯¹é½çš„æ­£å¸¸è¯­å¥å¯¹å¼‚å¸¸æŠ¥å‘Šçš„å½±å“ã€‚OFF-CLIPå¯åº”ç”¨äºæ”¾å°„å­¦CLIPæ¨¡å‹ï¼Œæ— éœ€è¿›è¡Œä»»ä½•æ¶æ„ä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOFF-CLIPèƒ½æ˜¾è‘—æé«˜æ­£å¸¸åˆ†ç±»æ€§èƒ½ï¼Œåœ¨VinDr-CXRæ•°æ®é›†ä¸Šè¾ƒå½“å‰æœ€ä½³é›¶æ ·æœ¬åˆ†ç±»åŸºçº¿CARZeroæé«˜AUCå€¼0.61%ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜å¼‚å¸¸åˆ†ç±»æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒOFF-CLIPè¿˜æé«˜äº†é›¶æ ·æœ¬å®šä½çš„å‡†ç¡®æ€§ï¼Œè¯æ˜å…¶åœ¨åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç¨³å¥å’Œé«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å·²åº”ç”¨äºæ”¾å°„å­¦é›¶æ ·æœ¬åˆ†ç±»ï¼Œå‡å°‘äº†å¯¹æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ã€‚</li>
<li>ä¼ ç»Ÿå¯¹æ¯”å­¦ä¹ åœ¨æ­£å¸¸æ ·æœ¬æ£€æµ‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸¥æ ¼æ ·æœ¬å†…å¯¹é½è€Œç ´åæ­£å¸¸æ ·æœ¬èšç±»ï¼Œå¯¼è‡´é«˜è¯¯æŠ¥ã€‚</li>
<li>OFF-CLIPæ–¹æ³•é€šè¿‡å¼•å…¥ç¦»å¯¹è§’çº¿é¡¹æŸå¤±æ”¹å–„æ­£å¸¸æ£€æµ‹ï¼Œå¢å¼ºæ­£å¸¸æ ·æœ¬èšç±»ï¼Œå¹¶åº”ç”¨å¥å­çº§æ–‡æœ¬è¿‡æ»¤å‡å°‘è¯¯å¯¹é½å½±å“ã€‚</li>
<li>OFF-CLIPå¯æ— ç¼é›†æˆåˆ°æ”¾å°„å­¦CLIPæ¨¡å‹ï¼Œæ— éœ€æ¶æ„ä¿®æ”¹ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒOFF-CLIPæ˜¾è‘—æé«˜æ­£å¸¸åˆ†ç±»æ€§èƒ½ï¼Œåœ¨VinDr-CXRæ•°æ®é›†ä¸Šè¾ƒCARZeroæœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>OFF-CLIPåŒæ—¶ä¿æŒæˆ–æé«˜å¼‚å¸¸åˆ†ç±»æ€§èƒ½ï¼Œè¯æ˜å…¶åœ¨åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f345b61be0b110f68592db0e972c0ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65806d86100f4bca4b4cb8d47c0dadac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-512c70fcded49f0f8fda8799be158b99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6afe343f48063ef62c9cc67705ccedaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1e4e9e6df4cb3bf0f94d5606e67b51c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c967cefac3adfad07004a3d1a1353316.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0081b0d8389a086e37f2178d682bbf7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SparseMamba-PCL-Scribble-Supervised-Medical-Image-Segmentation-via-SAM-Guided-Progressive-Collaborative-Learning"><a href="#SparseMamba-PCL-Scribble-Supervised-Medical-Image-Segmentation-via-SAM-Guided-Progressive-Collaborative-Learning" class="headerlink" title="SparseMamba-PCL: Scribble-Supervised Medical Image Segmentation via   SAM-Guided Progressive Collaborative Learning"></a>SparseMamba-PCL: Scribble-Supervised Medical Image Segmentation via   SAM-Guided Progressive Collaborative Learning</h2><p><strong>Authors:Luyi Qiu, Tristan Till, Xiaobao Guo, Adams Wai-Kin Kong</strong></p>
<p>Scribble annotations significantly reduce the cost and labor required for dense labeling in large medical datasets with complex anatomical structures. However, current scribble-supervised learning methods are limited in their ability to effectively propagate sparse annotation labels to dense segmentation masks and accurately segment object boundaries. To address these issues, we propose a Progressive Collaborative Learning framework that leverages novel algorithms and the Med-SAM foundation model to enhance information quality during training. (1) We enrich ground truth scribble segmentation labels through a new algorithm, propagating scribbles to estimate object boundaries. (2) We enhance feature representation by optimizing Med-SAM-guided training through the fusion of feature embeddings from Med-SAM and our proposed Sparse Mamba network. This enriched representation also facilitates the fine-tuning of the Med-SAM decoder with enriched scribbles. (3) For inference, we introduce a Sparse Mamba network, which is highly capable of capturing local and global dependencies by replacing the traditional sequential patch processing method with a skip-sampling procedure. Experiments on the ACDC, CHAOS, and MSCMRSeg datasets validate the effectiveness of our framework, outperforming nine state-of-the-art methods. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/QLYCode/SparseMamba-PCL%7D%7BSparseMamba-PCL.git%7D">https://github.com/QLYCode/SparseMamba-PCL}{SparseMamba-PCL.git}</a>. </p>
<blockquote>
<p>æ¶‚é¸¦æ³¨é‡Šï¼ˆScribble annotationsï¼‰èƒ½å¤Ÿå¤§å¹…åº¦å‡å°‘å¤æ‚ç»“æ„å¤§å‹åŒ»å­¦æ•°æ®é›†å¯†é›†æ ‡æ³¨æ‰€éœ€çš„èŠ±è´¹ä¸åŠ³åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ¶‚é¸¦ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å°†ç¨€ç–æ ‡æ³¨æ ‡ç­¾ä¼ æ’­åˆ°å¯†é›†åˆ†å‰²æ©è†œä»¥åŠç²¾ç¡®åˆ†å‰²å¯¹è±¡è¾¹ç•Œæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ¸è¿›åä½œå­¦ä¹ ï¼ˆProgressive Collaborative Learningï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ–°å‹ç®—æ³•å’ŒMed-SAMåŸºç¡€æ¨¡å‹æ¥æå‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¿¡æ¯è´¨é‡ã€‚ï¼ˆ1ï¼‰æˆ‘ä»¬é€šè¿‡æ–°å‹ç®—æ³•ä¸°å¯ŒçœŸå®æ¶‚é¸¦åˆ†å‰²æ ‡ç­¾ï¼Œé€šè¿‡ä¼ æ’­æ¶‚é¸¦æ¥ä¼°è®¡å¯¹è±¡è¾¹ç•Œã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬é€šè¿‡èåˆMed-SAMçš„ç‰¹å¾åµŒå…¥å’Œæˆ‘ä»¬æå‡ºçš„Sparse Mambaç½‘ç»œï¼Œä¼˜åŒ–Med-SAMå¼•å¯¼çš„è®­ç»ƒï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§ä¸°å¯Œçš„è¡¨ç¤ºè¿˜ä¿ƒè¿›äº†ç”¨ä¸°å¯Œçš„æ¶‚é¸¦å¯¹Med-SAMè§£ç å™¨çš„å¾®è°ƒã€‚ï¼ˆ3ï¼‰å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªSparse Mambaç½‘ç»œï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿé€šè¿‡è·³è¿‡é‡‡æ ·ç¨‹åºä»£æ›¿ä¼ ç»Ÿçš„é¡ºåºè¡¥ä¸å¤„ç†æ–¹æ³•ï¼Œéå¸¸æ“…é•¿æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¾èµ–å…³ç³»ã€‚åœ¨ACDCã€CHAOSå’ŒMSCMRSegæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¶…è¶Šäº†ä¹ç§æœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/QLYCode/SparseMamba-PCL">SparseMamba-PCL.git</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01633v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¸è¿›åä½œå­¦ä¹ æ¡†æ¶çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡æ–°ç®—æ³•ä¸°å¯Œæ¶‚é¸¦åˆ†å‰²æ ‡ç­¾ï¼Œä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºå¹¶å¼•å…¥ç¨€ç–æ›¼å·´ç½‘ç»œè¿›è¡Œæ¨ç†ï¼Œæœ‰æ•ˆæé«˜æ¶‚é¸¦ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•å¯æ˜¾è‘—å‡å°‘å¯†é›†æ ‡æ³¨çš„æˆæœ¬å’ŒåŠ³åŠ¨åŠ›ï¼Œå¹¶åœ¨ACDCã€CHAOSå’ŒMSCMRSegæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œè¶…è¶Šäº†ä¹ç§æœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæ¸è¿›åä½œå­¦ä¹ æ¡†æ¶çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ¶‚é¸¦ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹åŒ»å­¦æ•°æ®é›†ä¸­å­˜åœ¨çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ–°ç®—æ³•ä¸°å¯Œæ¶‚é¸¦åˆ†å‰²æ ‡ç­¾ï¼Œå®ç°å¯¹ç›®æ ‡è¾¹ç•Œçš„ä¼°è®¡ã€‚</li>
<li>ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œé€šè¿‡èåˆMed-SAMå’Œç¨€ç–æ›¼å·´ç½‘ç»œçš„ç‰¹å¾åµŒå…¥ï¼Œä¸°å¯Œä¿¡æ¯è´¨é‡ã€‚</li>
<li>å¼•å…¥ç¨€ç–æ›¼å·´ç½‘ç»œè¿›è¡Œæ¨ç†ï¼Œé‡‡ç”¨è·³è·ƒé‡‡æ ·ç¨‹åºæ›¿ä»£ä¼ ç»Ÿé¡ºåºè¡¥ä¸å¤„ç†æ–¹æ³•ï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¾èµ–å…³ç³»ã€‚</li>
<li>åœ¨ACDCã€CHAOSå’ŒMSCMRSegæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ä¹ç§æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6b00f20d0a25d0b17af3a85f65300898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf9c8ab2801b2d04742de2858265fbdd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fe073c51f4d3dad62d602cee2740871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fb2ff2e74b27fcd85c15994f816b95e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6479221ca595cb52a4d9980f5470c14.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MRI-super-resolution-reconstruction-using-efficient-diffusion-probabilistic-model-with-residual-shifting"><a href="#MRI-super-resolution-reconstruction-using-efficient-diffusion-probabilistic-model-with-residual-shifting" class="headerlink" title="MRI super-resolution reconstruction using efficient diffusion   probabilistic model with residual shifting"></a>MRI super-resolution reconstruction using efficient diffusion   probabilistic model with residual shifting</h2><p><strong>Authors:Mojtaba Safari, Shansong Wang, Zach Eidex, Qiang Li, Erik H. Middlebrooks, David S. Yu, Xiaofeng Yang</strong></p>
<p>Objective:This study introduces a residual error-shifting mechanism that drastically reduces sampling steps while preserving critical anatomical details, thus accelerating MRI reconstruction. Approach:We propose a novel diffusion-based SR framework called Res-SRDiff, which integrates residual error shifting into the forward diffusion process. This enables efficient HR image reconstruction by aligning the degraded HR and LR distributions.We evaluated Res-SRDiff on ultra-high-field brain T1 MP2RAGE maps and T2-weighted prostate images, comparing it with Bicubic, Pix2pix, CycleGAN, and a conventional denoising diffusion probabilistic model with vision transformer backbone (TM-DDPM), using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), gradient magnitude similarity deviation (GMSD), and learned perceptual image patch similarity (LPIPS). Main results: Res-SRDiff significantly outperformed all comparative methods in terms of PSNR, SSIM, and GMSD across both datasets, with statistically significant improvements (p-values&lt;&lt;0.05). The model achieved high-fidelity image restoration with only four sampling steps, drastically reducing computational time to under one second per slice, which is substantially faster than conventional TM-DDPM with around 20 seconds per slice. Qualitative analyses further demonstrated that Res-SRDiff effectively preserved fine anatomical details and lesion morphology in both brain and pelvic MRI images. Significance: Our findings show that Res-SRDiff is an efficient and accurate MRI SR method, markedly improving computational efficiency and image quality. Integrating residual error shifting into the diffusion process allows for rapid and robust HR image reconstruction, enhancing clinical MRI workflows and advancing medical imaging research. The source at:<a target="_blank" rel="noopener" href="https://github.com/mosaf/Res-SRDiff">https://github.com/mosaf/Res-SRDiff</a> </p>
<blockquote>
<p>ç›®æ ‡ï¼šæœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ®‹å·®è¯¯å·®åç§»æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤çš„åŒæ—¶ä¿ç•™äº†å…³é”®çš„è§£å‰–ç»†èŠ‚ï¼Œä»è€ŒåŠ é€Ÿäº†MRIé‡å»ºã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„SRæ¡†æ¶ï¼Œç§°ä¸ºRes-SRDiffï¼Œå®ƒå°†æ®‹å·®è¯¯å·®åç§»é›†æˆåˆ°æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­ã€‚è¿™é€šè¿‡å¯¹é½é€€åŒ–çš„HRå’ŒLRåˆ†å¸ƒï¼Œå®ç°äº†é«˜æ•ˆHRå›¾åƒé‡å»ºã€‚æˆ‘ä»¬å¯¹è¶…é«˜é¢‘åœºè„‘T1 MP2RAGEå›¾å’ŒT2åŠ æƒå‰åˆ—è…ºå›¾åƒè¿›è¡Œäº†Res-SRDiffè¯„ä¼°ï¼Œå°†å…¶ä¸Bicubicã€Pix2pixã€CycleGANä»¥åŠå¸¦æœ‰è§†è§‰è½¬æ¢å™¨ä¸»å¹²ï¼ˆTM-DDPMï¼‰çš„ä¼ ç»Ÿå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œä½¿ç”¨äº†å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰ã€æ¢¯åº¦å¹…åº¦ç›¸ä¼¼æ€§åå·®ï¼ˆGMSDï¼‰å’Œå­¦ä¹ çš„æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰ç­‰å®šé‡æŒ‡æ ‡ã€‚ä¸»è¦ç»“æœï¼šRes-SRDiffåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„PSNRã€SSIMå’ŒGMSDæ–¹é¢å‡æ˜¾è‘—ä¼˜äºæ‰€æœ‰æ¯”è¾ƒæ–¹æ³•ï¼Œå…·æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ”¹å–„ï¼ˆpå€¼&lt;&lt;0.05ï¼‰ã€‚è¯¥æ¨¡å‹ä»…é€šè¿‡å››ä¸ªé‡‡æ ·æ­¥éª¤å°±å®ç°äº†é«˜ä¿çœŸå›¾åƒæ¢å¤ï¼Œè®¡ç®—æ—¶é—´ç¼©çŸ­åˆ°æ¯ç‰‡ä¸åˆ°ä¸€ç§’ï¼Œè¿™æ¯”ä¼ ç»Ÿçš„TM-DDPMæ¯ç‰‡å¤§çº¦20ç§’è¦å¿«å¾—å¤šã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¯æ˜ï¼ŒRes-SRDiffæœ‰æ•ˆåœ°ä¿ç•™äº†å¤§è„‘å’Œç›†è…”MRIå›¾åƒçš„ç²¾ç»†è§£å‰–ç»“æ„å’Œç—…ç¶å½¢æ€ã€‚æ„ä¹‰ï¼šæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRes-SRDiffæ˜¯ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„MRI SRæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚å°†æ®‹å·®è¯¯å·®åç§»é›†æˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥å®ç°å¿«é€Ÿå’Œç¨³å¥çš„HRå›¾åƒé‡å»ºï¼Œå¢å¼ºä¸´åºŠMRIå·¥ä½œæµç¨‹ï¼Œæ¨åŠ¨åŒ»å­¦æˆåƒç ”ç©¶çš„å‘å±•ã€‚æ•°æ®æºï¼š<a target="_blank" rel="noopener" href="https://github.com/mosaf/Res-SRDiff">https://github.com/mosaf/Res-SRDiff</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01576v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡é‡å»ºæ¡†æ¶Res-SRDiffï¼Œé€šè¿‡å°†æ®‹å·®è¯¯å·®ç§»ä½æŠ€æœ¯èå…¥å‰å‘æ‰©æ•£è¿‡ç¨‹ï¼Œæ˜¾è‘—æå‡äº†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„é‡å»ºé€Ÿåº¦å’Œå›¾åƒè´¨é‡ã€‚è¯¥ç ”ç©¶åœ¨è¶…é«˜åœºè„‘T1 MP2RAGEåœ°å›¾å’ŒT2åŠ æƒå‰åˆ—è…ºå›¾åƒä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœæ˜¾ç¤ºRes-SRDiffåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰å’Œæ¢¯åº¦å¹…åº¦ç›¸ä¼¼æ€§åå·®ï¼ˆGMSDï¼‰ç­‰å®šé‡æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–å¯¹æ¯”æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒRes-SRDiffä»…éœ€å››ä¸ªé‡‡æ ·æ­¥éª¤å³å¯å®ç°é«˜ä¿çœŸå›¾åƒæ¢å¤ï¼Œè®¡ç®—æ—¶é—´ç¼©çŸ­è‡³æ¯ç§’ä¸€ç‰‡ä»¥å†…ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„TM-DDPMæ–¹æ³•å¤§å¹…æå‡äº†è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£å‹è¶…åˆ†è¾¨ç‡é‡å»ºæ¡†æ¶Res-SRDiffï¼Œèåˆäº†æ®‹å·®è¯¯å·®ç§»ä½æŠ€æœ¯ã€‚</li>
<li>Res-SRDiffåœ¨è¶…é«˜åœºMRIå›¾åƒä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬è„‘éƒ¨å’Œå‰åˆ—è…ºå›¾åƒã€‚</li>
<li>Res-SRDiffåœ¨PSNRã€SSIMå’ŒGMSDç­‰å®šé‡æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å‡å°‘é‡‡æ ·æ­¥éª¤ï¼Œå®ç°äº†å¿«é€Ÿä¸”é«˜ä¿çœŸçš„MRIå›¾åƒé‡å»ºã€‚</li>
<li>Res-SRDiffçš„è®¡ç®—æ•ˆç‡æ˜¾è‘—æé«˜ï¼Œæ¯ç‰‡å›¾åƒçš„è®¡ç®—æ—¶é—´ç¼©çŸ­è‡³ä¸€ç§’ä»¥å†…ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯æ˜äº†Res-SRDiffåœ¨ä¸´åºŠåŒ»å­¦å›¾åƒé‡å»ºä¸­çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59e06c496cebb7c0a458589e393bd7a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61a0f1251e02dce2fc9d435c8d8eaba6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="From-Claims-to-Evidence-A-Unified-Framework-and-Critical-Analysis-of-CNN-vs-Transformer-vs-Mamba-in-Medical-Image-Segmentation"><a href="#From-Claims-to-Evidence-A-Unified-Framework-and-Critical-Analysis-of-CNN-vs-Transformer-vs-Mamba-in-Medical-Image-Segmentation" class="headerlink" title="From Claims to Evidence: A Unified Framework and Critical Analysis of   CNN vs. Transformer vs. Mamba in Medical Image Segmentation"></a>From Claims to Evidence: A Unified Framework and Critical Analysis of   CNN vs. Transformer vs. Mamba in Medical Image Segmentation</h2><p><strong>Authors:Pooya Mohammadi Kazaj, Giovanni Baj, Yazdan Salimi, Anselm W. Stark, Waldo Valenzuela, George CM. Siontis, Habib Zaidi, Mauricio Reyes, Christoph Graeni, Isaac Shiri</strong></p>
<p>While numerous architectures for medical image segmentation have been proposed, achieving competitive performance with state-of-the-art models networks such as nnUNet, still leave room for further innovation. In this work, we introduce nnUZoo, an open source benchmarking framework built upon nnUNet, which incorporates various deep learning architectures, including CNNs, Transformers, and Mamba-based models. Using this framework, we provide a fair comparison to demystify performance claims across different medical image segmentation tasks. Additionally, in an effort to enrich the benchmarking, we explored five new architectures based on Mamba and Transformers, collectively named X2Net, and integrated them into nnUZoo for further evaluation. The proposed models combine the features of conventional U2Net, nnUNet, CNN, Transformer, and Mamba layers and architectures, called X2Net (UNETR2Net (UNETR), SwT2Net (SwinTransformer), SS2D2Net (SwinUMamba), Alt1DM2Net (LightUMamba), and MambaND2Net (MambaND)). We extensively evaluate the performance of different models on six diverse medical image segmentation datasets, including microscopy, ultrasound, CT, MRI, and PET, covering various body parts, organs, and labels. We compare their performance, in terms of dice score and computational efficiency, against their baseline models, U2Net, and nnUNet. CNN models like nnUNet and U2Net demonstrated both speed and accuracy, making them effective choices for medical image segmentation tasks. Transformer-based models, while promising for certain imaging modalities, exhibited high computational costs. Proposed Mamba-based X2Net architecture (SS2D2Net) achieved competitive accuracy with no significantly difference from nnUNet and U2Net, while using fewer parameters. However, they required significantly longer training time, highlighting a trade-off between model efficiency and computational cost. </p>
<blockquote>
<p>å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ¶æ„ï¼Œä½†ä¸æœ€æ–°æ¨¡å‹ç½‘ç»œï¼ˆå¦‚nnUNetï¼‰ç›¸æ¯”ï¼Œä»æœ‰è®¸å¤šæ”¹è¿›çš„ç©ºé—´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºnnUNetçš„å¼€æºåŸºå‡†æµ‹è¯•æ¡†æ¶nnUZooï¼Œå®ƒç»“åˆäº†å„ç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼ŒåŒ…æ‹¬CNNã€Transformerå’ŒåŸºäºMambaçš„æ¨¡å‹ã€‚ä½¿ç”¨è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¬æ­£çš„æ¯”è¾ƒï¼Œä»¥æ­ç¤ºä¸åŒåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¹‹é—´æ€§èƒ½å£°ç§°çš„ç¥ç§˜æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä¸°å¯ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æ¢ç´¢äº†äº”ç§åŸºäºMambaå’ŒTransformerçš„æ–°æ¶æ„ï¼Œç»Ÿç§°ä¸ºX2Netï¼Œå¹¶å°†å…¶é›†æˆåˆ°nnUZooä¸­è¿›è¡Œè¿›ä¸€æ­¥è¯„ä¼°ã€‚æ‰€æå‡ºçš„æ¨¡å‹ç»“åˆäº†ä¼ ç»ŸU2Netã€nnUNetã€CNNã€Transformerå’ŒMambaå±‚å’Œæ¶æ„çš„ç‰¹ç‚¹ï¼Œç§°ä¸ºX2Netï¼ˆUNETR2Netï¼ˆUNETRï¼‰ã€SwT2Netï¼ˆSwinTransformerï¼‰ã€SS2D2Netï¼ˆSwinUMambaï¼‰ã€Alt1DM2Netï¼ˆLightUMambaï¼‰å’ŒMambaND2Netï¼ˆMambaNDï¼‰ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªä¸åŒçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šå…¨é¢è¯„ä¼°äº†ä¸åŒæ¨¡å‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬æ˜¾å¾®é•œã€è¶…å£°ã€CTã€MRIå’ŒPETï¼Œæ¶µç›–äº†å„ç§èº«ä½“éƒ¨ä½ã€å™¨å®˜å’Œæ ‡ç­¾ã€‚æˆ‘ä»¬æ ¹æ®diceåˆ†æ•°å’Œè®¡ç®—æ•ˆç‡æ¯”è¾ƒäº†å®ƒä»¬ä¸åŸºçº¿æ¨¡å‹U2Netå’ŒnnUNetçš„æ€§èƒ½ã€‚CNNæ¨¡å‹å¦‚nnUNetå’ŒU2Netåœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œæ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„æœ‰æ•ˆé€‰æ‹©ã€‚è™½ç„¶åŸºäºTransformerçš„æ¨¡å‹åœ¨æŸäº›æˆåƒæ¨¡å¼ä¸Šå…·æœ‰æ½œåŠ›ï¼Œä½†å®ƒä»¬è¡¨ç°å‡ºè¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚æå‡ºçš„åŸºäºMambaçš„X2Netæ¶æ„ï¼ˆSS2D2Netï¼‰åœ¨å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä¸nnUNetå’ŒU2Netç›¸æ¯”æ²¡æœ‰æ˜æ˜¾å·®å¼‚ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°è¾ƒå°‘ã€‚ç„¶è€Œï¼Œå®ƒä»¬éœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œè¿™çªæ˜¾äº†æ¨¡å‹æ•ˆç‡å’Œè®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01306v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºnnUNetçš„å¼€æºåŸºå‡†æµ‹è¯•æ¡†æ¶nnUZooï¼Œå…¶ç»“åˆäº†å¤šç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¦‚CNNã€Transformerå’ŒMambaæ¨¡å‹ã€‚æ–‡ç« é€šè¿‡å…¬å¹³æ¯”è¾ƒä¸åŒåŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†nnUZooçš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢ç´¢äº†äº”ç§åŸºäºMambaå’ŒTransformerçš„æ–°æ¶æ„ï¼Œå¹¶å°†å®ƒä»¬é›†æˆåˆ°nnUZooä¸­è¿›è¡Œè¿›ä¸€æ­¥è¯„ä¼°ã€‚è¿™äº›æ–°æ¨¡å‹ç»“åˆäº†ä¼ ç»ŸU2Netã€nnUNetã€CNNã€Transformerå’ŒMambaå±‚çš„ç‰¹æ€§ã€‚åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCNNæ¨¡å‹å¦‚nnUNetå’ŒU2Netåœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒåŸºäºTransformerçš„æ¨¡å‹è™½ç„¶å¯¹æŸäº›æˆåƒæ¨¡å¼å…·æœ‰å‰æ™¯ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚åŸºäºMambaçš„æ–°æ¶æ„SS2D2Netåœ¨å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä½†è®­ç»ƒæ—¶é—´è¾ƒé•¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>nnUZooæ˜¯ä¸€ä¸ªåŸºäºnnUNetçš„å¼€æºåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç»“åˆäº†å¤šç§æ·±åº¦å­¦ä¹ æ¶æ„ã€‚</li>
<li>é€šè¿‡å…¬å¹³æ¯”è¾ƒï¼Œæ–‡ç« å¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡çš„ä¸åŒæ¨¡å‹æ€§èƒ½è¿›è¡Œäº†å±•ç¤ºã€‚</li>
<li>æ–°æå‡ºçš„X2Netæ¨¡å‹ç»“åˆäº†å¤šç§æŠ€æœ¯å’Œæ¶æ„ç‰¹æ€§ã€‚</li>
<li>åœ¨å¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ã€‚</li>
<li>CNNæ¨¡å‹å¦‚nnUNetå’ŒU2Netåœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºTransformerçš„æ¨¡å‹è™½ç„¶å…·æœ‰å‰æ™¯ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3efa15b24cd1d02e5d681b407b689216.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8c0b3939938cd638a1cfea2b088e904.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Controllable-Apparel-Showcase-Image-Generation-via-Garment-Centric-Outpainting"><a href="#Fine-Grained-Controllable-Apparel-Showcase-Image-Generation-via-Garment-Centric-Outpainting" class="headerlink" title="Fine-Grained Controllable Apparel Showcase Image Generation via   Garment-Centric Outpainting"></a>Fine-Grained Controllable Apparel Showcase Image Generation via   Garment-Centric Outpainting</h2><p><strong>Authors:Rong Zhang, Jingnan Wang, Zhiwen Zuo, Jianfeng Dong, Wei Li, Chi Wang, Weiwei Xu, Xun Wang</strong></p>
<p>In this paper, we propose a novel garment-centric outpainting (GCO) framework based on the latent diffusion model (LDM) for fine-grained controllable apparel showcase image generation. The proposed framework aims at customizing a fashion model wearing a given garment via text prompts and facial images. Different from existing methods, our framework takes a garment image segmented from a dressed mannequin or a person as the input, eliminating the need for learning cloth deformation and ensuring faithful preservation of garment details. The proposed framework consists of two stages. In the first stage, we introduce a garment-adaptive pose prediction model that generates diverse poses given the garment. Then, in the next stage, we generate apparel showcase images, conditioned on the garment and the predicted poses, along with specified text prompts and facial images. Notably, a multi-scale appearance customization module (MS-ACM) is designed to allow both overall and fine-grained text-based control over the generated modelâ€™s appearance. Moreover, we leverage a lightweight feature fusion operation without introducing any extra encoders or modules to integrate multiple conditions, which is more efficient. Extensive experiments validate the superior performance of our framework compared to state-of-the-art methods. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æ–°å‹ä»¥æœè£…ä¸ºä¸­å¿ƒçš„å¤–ç»˜ï¼ˆGCOï¼‰æ¡†æ¶ï¼Œç”¨äºç»†ç²’åº¦å¯æ§çš„æœè£…å±•ç¤ºå›¾åƒç”Ÿæˆã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒæ¥å®šåˆ¶ç©¿ç€ç»™å®šæœè£…çš„æ—¶å°šæ¨¡å‹ã€‚ä¸åŒäºç°æœ‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä»¥ä»ç€è£…çš„äººä½“æ¨¡ç‰¹æˆ–çœŸäººåˆ†å‰²å¾—åˆ°çš„æœè£…å›¾åƒä½œä¸ºè¾“å…¥ï¼Œæ— éœ€å­¦ä¹ å¸ƒæ–™å˜å½¢ï¼ŒåŒæ—¶ç¡®ä¿æœè£…ç»†èŠ‚çš„å¿ å®ä¿ç•™ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœè£…è‡ªé€‚åº”å§¿æ€é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®æœè£…ç”Ÿæˆå„ç§å§¿æ€ã€‚ç„¶åï¼Œåœ¨ä¸‹ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä»¥æœè£…ã€é¢„æµ‹å§¿æ€ã€æŒ‡å®šçš„æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒä¸ºæ¡ä»¶ï¼Œç”Ÿæˆæœè£…å±•ç¤ºå›¾åƒã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦å¤–è§‚å®šåˆ¶æ¨¡å—ï¼ˆMS-ACMï¼‰ï¼Œå…è®¸å¯¹ç”Ÿæˆçš„æ¨¡å‹çš„å¤–è§‚è¿›è¡Œæ•´ä½“å’Œç»†ç²’åº¦çš„æ–‡æœ¬æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨è½»é‡çº§ç‰¹å¾èåˆæ“ä½œï¼Œæ— éœ€å¼•å…¥ä»»ä½•é¢å¤–çš„ç¼–ç å™¨æˆ–æ¨¡å—æ¥æ•´åˆå¤šç§æ¡ä»¶ï¼Œè¿™æ›´åŠ é«˜æ•ˆã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01294v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æ–°å‹æœè£…ä¸­å¿ƒå¤–ç»˜ç”»ï¼ˆGCOï¼‰æ¡†æ¶ï¼Œç”¨äºç²¾ç»†å¯æ§çš„æ—¶è£…å±•ç¤ºå›¾åƒç”Ÿæˆã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒå®šåˆ¶ç©¿ç€ç»™å®šæœè£…çš„æ—¶è£…æ¨¡å‹ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶ä»¥ä»ç€è£…æ¨¡ç‰¹æˆ–äººç‰©åˆ†å‰²å‡ºçš„æœè£…å›¾åƒä¸ºè¾“å…¥ï¼Œæ— éœ€å­¦ä¹ å¸ƒæ–™å˜å½¢ï¼Œç¡®ä¿æœè£…ç»†èŠ‚çš„çœŸå®ä¿ç•™ã€‚æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯æœè£…è‡ªé€‚åº”å§¿æ€é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ ¹æ®æœè£…ç”Ÿæˆå„ç§å§¿æ€ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯æ ¹æ®æœè£…ã€é¢„æµ‹å§¿æ€ã€æŒ‡å®šçš„æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒç”Ÿæˆæ—¶è£…å±•ç¤ºå›¾åƒã€‚ç‰¹åˆ«è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦å¤–è§‚å®šåˆ¶æ¨¡å—ï¼ˆMS-ACMï¼‰ï¼Œå…è®¸å¯¹ç”Ÿæˆçš„æ¨¡å‹çš„å¤–è§‚è¿›è¡Œæ•´ä½“å’Œç²¾ç»†çš„æ–‡æœ¬æ§åˆ¶ã€‚åˆ©ç”¨è½»é‡çº§ç‰¹å¾èåˆæ“ä½œï¼Œåœ¨ä¸éœ€è¦å¼•å…¥ä»»ä½•é¢å¤–çš„ç¼–ç å™¨æˆ–æ¨¡å—çš„æƒ…å†µä¸‹ï¼Œå°†å¤šç§æ¡ä»¶ç›¸ç»“åˆï¼Œæé«˜äº†æ•ˆç‡ã€‚å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶ç›¸è¾ƒäºæœ€å…ˆè¿›æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æœè£…ä¸­å¿ƒå¤–ç»˜ç”»ï¼ˆGCOï¼‰æ¡†æ¶ï¼Œç”¨äºæ—¶è£…å±•ç¤ºå›¾åƒç”Ÿæˆã€‚</li>
<li>æ¡†æ¶æ—¨åœ¨é€šè¿‡æ–‡æœ¬æç¤ºå’Œé¢éƒ¨å›¾åƒå®šåˆ¶ç©¿ç€ç‰¹å®šæœè£…çš„æ—¶è£…æ¨¡å‹ã€‚</li>
<li>è¾“å…¥ä¸ºä»æ¨¡ç‰¹æˆ–äººç‰©åˆ†å‰²å‡ºçš„æœè£…å›¾åƒï¼Œæ— éœ€å­¦ä¹ å¸ƒæ–™å˜å½¢ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šæœè£…è‡ªé€‚åº”å§¿æ€é¢„æµ‹å’Œå¤šå°ºåº¦å¤–è§‚å®šåˆ¶ã€‚</li>
<li>å¤šå°ºåº¦å¤–è§‚å®šåˆ¶æ¨¡å—ï¼ˆMS-ACMï¼‰å…è®¸å¯¹ç”Ÿæˆæ¨¡å‹çš„å¤–è§‚è¿›è¡Œæ•´ä½“å’Œç²¾ç»†çš„æ–‡æœ¬æ§åˆ¶ã€‚</li>
<li>åˆ©ç”¨è½»é‡çº§ç‰¹å¾èåˆæ“ä½œæ•´åˆå¤šç§æ¡ä»¶ï¼Œæé«˜æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16103e04c2c8f1f3b163e74b49982f1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a9741b51277859872df220c9940717c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87d5b7fc3173fa633df4e6449faa6ce7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fdeaaffe92f1fa7b643258960f597976.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8faa81ea3bab3a075102f19d11a4692.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Soybean-Disease-Detection-via-Interpretable-Hybrid-CNN-GNN-Integrating-MobileNetV2-and-GraphSAGE-with-Cross-Modal-Attention"><a href="#Soybean-Disease-Detection-via-Interpretable-Hybrid-CNN-GNN-Integrating-MobileNetV2-and-GraphSAGE-with-Cross-Modal-Attention" class="headerlink" title="Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating   MobileNetV2 and GraphSAGE with Cross-Modal Attention"></a>Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating   MobileNetV2 and GraphSAGE with Cross-Modal Attention</h2><p><strong>Authors:Md Abrar Jahin, Soudeep Shahriar, M. F. Mridha, Nilanjan Dey</strong></p>
<p>Soybean leaf disease detection is critical for agricultural productivity but faces challenges due to visually similar symptoms and limited interpretability in conventional methods. While Convolutional Neural Networks (CNNs) excel in spatial feature extraction, they often neglect inter-image relational dependencies, leading to misclassifications. This paper proposes an interpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that synergizes MobileNetV2 for localized feature extraction and GraphSAGE for relational modeling. The framework constructs a graph where nodes represent leaf images, with edges defined by cosine similarity-based adjacency matrices and adaptive neighborhood sampling. This design captures fine-grained lesion features and global symptom patterns, addressing inter-class similarity challenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM visualizations, generating heatmaps to highlight disease-influential regions. Evaluated on a dataset of ten soybean leaf diseases, the model achieves $97.16%$ accuracy, surpassing standalone CNNs ($\le95.04%$) and traditional machine learning models ($\le77.05%$). Ablation studies validate the sequential architectureâ€™s superiority over parallel or single-model configurations. With only 2.3 million parameters, the lightweight MobileNetV2-GraphSAGE combination ensures computational efficiency, enabling real-time deployment in resource-constrained environments. The proposed approach bridges the gap between accurate classification and practical applicability, offering a robust, interpretable tool for agricultural diagnostics while advancing CNN-GNN integration in plant pathology research. </p>
<blockquote>
<p>å¤§è±†å¶ç—…æ£€æµ‹å¯¹å†œä¸šç”Ÿäº§åŠ›è‡³å…³é‡è¦ï¼Œä½†ç”±äºç—‡çŠ¶è§†è§‰ç›¸ä¼¼æ€§å’Œä¼ ç»Ÿæ–¹æ³•çš„æœ‰é™è§£é‡Šæ€§ï¼Œå®ƒé¢ä¸´ç€æŒ‘æˆ˜ã€‚å°½ç®¡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨ç©ºé—´ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†å›¾åƒé—´çš„å…³ç³»ä¾èµ–æ€§ï¼Œä»è€Œå¯¼è‡´è¯¯åˆ†ç±»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„æ··åˆåºè´¯CNN-å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒMobileNetV2è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–å’ŒGraphSAGEè¿›è¡Œå…³ç³»å»ºæ¨¡ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å¶å›¾åƒï¼Œè¾¹ç”±åŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„é‚»æ¥çŸ©é˜µå’Œè‡ªé€‚åº”é‚»åŸŸé‡‡æ ·å®šä¹‰ã€‚è¿™ç§è®¾è®¡æ•æ‰äº†ç²¾ç»†çš„ç—…å˜ç‰¹å¾å’Œå…¨å±€ç—‡çŠ¶æ¨¡å¼ï¼Œè§£å†³äº†ç±»é—´ç›¸ä¼¼æ€§æŒ‘æˆ˜ã€‚é€šè¿‡Grad-CAMå’ŒEigen-CAMå¯è§†åŒ–å®ç°è·¨æ¨¡æ€è§£é‡Šæ€§ï¼Œç”Ÿæˆçƒ­å›¾ä»¥çªå‡ºæ˜¾ç¤ºç–¾ç—…å½±å“åŒºåŸŸã€‚åœ¨åŒ…å«åç§å¤§è±†å¶ç—…çš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†97.16%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†å•ç‹¬çš„CNNï¼ˆâ‰¤95.04%ï¼‰å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆâ‰¤77.05%ï¼‰ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†åºè´¯æ¶æ„ä¼˜äºå¹¶è¡Œæˆ–å•ä¸€æ¨¡å‹é…ç½®ã€‚ä»…æœ‰230ä¸‡çš„å‚æ•°ï¼Œè½»é‡çº§çš„MobileNetV2-GraphSAGEç»„åˆç¡®ä¿äº†è®¡ç®—æ•ˆç‡ï¼Œå¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°å®æ—¶éƒ¨ç½²ã€‚æ‰€æå‡ºçš„æ–¹æ³•å¼¥åˆäº†å‡†ç¡®åˆ†ç±»ä¸å®é™…é€‚ç”¨ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºå†œä¸šè¯Šæ–­æä¾›äº†ç¨³å¥ã€å¯è§£é‡Šçš„å·¥å…·ï¼ŒåŒæ—¶æ¨åŠ¨äº†CNN-GNNåœ¨æ¤ç‰©ç—…ç†å­¦ç ”ç©¶ä¸­çš„èåˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01284v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„æ··åˆSequential CNN-Graph Neural Networkï¼ˆGNNï¼‰æ¡†æ¶ï¼Œç”¨äºå¤§è±†å¶ç—…æ£€æµ‹ã€‚è¯¥æ¡†æ¶ç»“åˆäº†MobileNetV2è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–å’ŒGraphSAGEè¿›è¡Œå…³ç³»å»ºæ¨¡ï¼Œè§£å†³äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨æ•æ‰å›¾åƒé—´å…³ç³»ä¾èµ–æ–¹é¢çš„ä¸è¶³ã€‚æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å¶ç‰‡å›¾åƒï¼Œè¾¹ç¼˜ç”±ä½™å¼¦ç›¸ä¼¼æ€§åŸºç¡€ä¸Šçš„é‚»æ¥çŸ©é˜µå’Œè‡ªé€‚åº”é‚»åŸŸé‡‡æ ·å®šä¹‰ã€‚æ­¤è®¾è®¡æ•è·äº†ç²¾ç»†çš„ç—…å˜ç‰¹å¾å’Œå…¨å±€ç—‡çŠ¶æ¨¡å¼ï¼Œè§£å†³äº†ç±»é—´ç›¸ä¼¼æ€§çš„æŒ‘æˆ˜ã€‚é€šè¿‡Grad-CAMå’ŒEigen-CAMå¯è§†åŒ–å®ç°è·¨æ¨¡æ€è§£é‡Šæ€§ï¼Œç”Ÿæˆçƒ­å›¾ä»¥çªå‡ºç–¾ç—…å½±å“åŒºåŸŸã€‚åœ¨åç§å¤§è±†å¶ç—…æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°97.16%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å•ç‹¬çš„CNNï¼ˆâ‰¤95.04%ï¼‰å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆâ‰¤77.05%ï¼‰ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†é¡ºåºæ¶æ„ä¼˜äºå¹¶è¡Œæˆ–å•ä¸€æ¨¡å‹é…ç½®ã€‚è½»é‡çº§çš„MobileNetV2-GraphSAGEç»„åˆä»…æœ‰230ä¸‡å‚æ•°ï¼Œç¡®ä¿è®¡ç®—æ•ˆç‡ï¼Œå¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®æ—¶éƒ¨ç½²ã€‚æ‰€æå‡ºçš„æ–¹æ³•ç¼©å°äº†å‡†ç¡®åˆ†ç±»å’Œå®é™…åº”ç”¨ä¹‹é—´çš„å·®è·ï¼Œä¸ºå†œä¸šè¯Šæ–­æä¾›äº†ä¸€ä¸ªç¨³å¥ã€å¯è§£é‡Šçš„å·¥å…·ï¼ŒåŒæ—¶æ¨åŠ¨äº†æ¤ç‰©ç—…ç†å­¦ç ”ç©¶ä¸­CNN-GNNçš„é›†æˆåº”ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ä¸ªæ··åˆSequential CNN-Graph Neural Networkï¼ˆGNNï¼‰æ¡†æ¶ï¼Œç”¨äºå¤§è±†å¶ç—…æ£€æµ‹ï¼Œç»“åˆäº†MobileNetV2å’ŒGraphSAGEçš„ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡æ„å»ºå›¾åƒçš„å›¾è¡¨ç¤ºï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ•æ‰å¶ç‰‡å›¾åƒä¹‹é—´çš„ç²¾ç»†ç‰¹å¾å’Œå…³ç³»ä¾èµ–ã€‚</li>
<li>åˆ©ç”¨ä½™å¼¦ç›¸ä¼¼æ€§å®šä¹‰è¾¹ç¼˜å’Œè‡ªé€‚åº”é‚»åŸŸé‡‡æ ·ï¼Œæœ‰æ•ˆåº”å¯¹ç±»é—´è§†è§‰ç›¸ä¼¼æ€§å’Œå¤æ‚æ¨¡å¼ã€‚</li>
<li>é€šè¿‡Grad-CAMå’ŒEigen-CAMå¯è§†åŒ–å¢å¼ºæ¨¡å‹è§£é‡Šæ€§ï¼Œçªå‡ºç–¾ç—…å½±å“åŒºåŸŸã€‚</li>
<li>æ¨¡å‹åœ¨åç§å¤§è±†å¶ç—…æ•°æ®é›†ä¸Šè¾¾åˆ°97.16%çš„é«˜å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œå•ä¸€ç¥ç»ç½‘ç»œç»“æ„ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜äº†æ‰€ææ¡†æ¶è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†é¡ºåºæ¶æ„åœ¨åº”å¯¹å¤æ‚ä»»åŠ¡æ—¶çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd4e123319d22c09ea8cec679600da0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef59bade34686a0271397da418fa0476.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d824384c0ff77c125761c195bd21aac.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multi-wavelength-study-for-gamma-ray-nova-V1405-Cas"><a href="#Multi-wavelength-study-for-gamma-ray-nova-V1405-Cas" class="headerlink" title="Multi-wavelength study for gamma-ray nova V1405 Cas"></a>Multi-wavelength study for gamma-ray nova V1405 Cas</h2><p><strong>Authors:Zi-wei Ou, Pak-hin Thomas Tam, Hui-hui Wang, Song-peng Pei, Wen-jun Huang</strong></p>
<p>Novae are found to have GeV to TeV gamma-ray emission, which reveals the shock acceleration from the white dwarfs. Recently, V1405 Cas was reported to radiated suspicious gamma-ray by \textit{Fermi}-LAT with low significance ($4.1 \sigma$) after the optical maximum. Radio observations reveal that it is one of the five brightest novae surrounded by low-density ionized gas columns. Here we report continuous search for GeV gamma-ray from \textit{Fermi}-LAT. No gamma-ray were found. For V1405 Cas, the flux level is lower than other well-studied \textit{Fermi} novae, and the gamma-ray maximum appear at $t_{0} + 145$ d. Gamma-ray of V1405 Cas are used to search potential gamma-ray periodicity. No gamma-ray periodicity was found during the time of observation. By comparing multi-wavelength data, the gamma-ray upper limit to optical flux ratio with value at around $10^{-4}$ is obtained to constrain the shock acceleration. Long-term analysis from \textit{Swift}-XRT gets X-ray spectral in the post-shock phase, which indicates that V1405 Cas became a super-soft source. The best-fit black body temperature at the super soft state is 0.11 - 0.19 keV. </p>
<blockquote>
<p>æœ€è¿‘å‘ç°Novaå…·æœ‰ä»å‡ ç”µå­ä¼ç‰¹åˆ°å¤ªç”µå­ä¼ç‰¹èŒƒå›´çš„ä¼½é©¬å°„çº¿å‘å°„ï¼Œè¿™æ­ç¤ºäº†æ¥è‡ªç™½çŸ®æ˜Ÿçš„å†²å‡»åŠ é€Ÿç°è±¡ã€‚æœ€è¿‘ï¼ŒV1405 Casè¢«æŠ¥å‘Šåœ¨å…‰å­¦æœ€å¤§å€¼ä¹‹åé€šè¿‡è´¹ç±³-LATä»¥è¾ƒä½çš„æ˜¾è‘—æ€§ï¼ˆ4.1Ïƒï¼‰å‘å‡ºå¯ç–‘çš„ä¼½é©¬å°„çº¿ã€‚å°„ç”µè§‚æµ‹è¡¨æ˜ï¼Œå®ƒæ˜¯å‘¨å›´è¢«ä½å¯†åº¦ç”µç¦»æ°”ä½“æŸ±ç¯ç»•çš„äº”é¢—æœ€äº®çš„Novaä¹‹ä¸€ã€‚è¿™é‡Œæˆ‘ä»¬æŠ¥å‘Šäº†é€šè¿‡è´¹ç±³-LATå¯¹ä¼½é©¬å°„çº¿çš„è¿ç»­æœç´¢ã€‚æ²¡æœ‰æ‰¾åˆ°ä¼½é©¬å°„çº¿ã€‚å¯¹äºV1405 Casï¼Œå…¶æµé‡æ°´å¹³ä½äºå…¶ä»–ç»è¿‡è‰¯å¥½ç ”ç©¶çš„è´¹ç±³Novaï¼Œä¼½é©¬å°„çº¿æœ€å¤§å€¼å‡ºç°åœ¨t0 + 145å¤©ã€‚æˆ‘ä»¬ä½¿ç”¨V1405 Casçš„ä¼½é©¬å°„çº¿æœç´¢æ½œåœ¨çš„ä¼½é©¬å°„çº¿å‘¨æœŸæ€§ã€‚åœ¨è§‚æµ‹æœŸé—´æœªå‘ç°ä¼½é©¬å°„çº¿å‘¨æœŸæ€§ã€‚é€šè¿‡æ¯”è¾ƒå¤šæ³¢é•¿æ•°æ®ï¼Œå¾—åˆ°ä¼½é©¬å°„çº¿æµé‡ä¸Šé™ä¸å…‰å­¦æµé‡çš„æ¯”å€¼çº¦ä¸º10^-4ï¼Œä»¥é™åˆ¶å†²å‡»åŠ é€Ÿã€‚é€šè¿‡Swift-XRTçš„é•¿æœŸåˆ†æï¼Œå¾—åˆ°åå†²å‡»é˜¶æ®µçš„Xå°„çº¿å…‰è°±ï¼Œè¡¨æ˜V1405 Caså˜æˆäº†ä¸€ä¸ªè¶…è½¯æºã€‚åœ¨è¶…è½¯çŠ¶æ€ä¸‹ï¼Œæœ€ä½³æ‹Ÿåˆçš„é»‘ä½“æ¸©åº¦ä¸º0.11-0.19åƒç”µå­ä¼ç‰¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01278v2">PDF</a> Published in RAA</p>
<p><strong>Summary</strong><br>     æœ€è¿‘å¯¹V1405 Casè¿›è¡Œè§‚æµ‹ï¼Œå‘ç°å…¶å­˜åœ¨ç–‘ä¼¼ä¼½é©¬å°„çº¿è¾å°„ç°è±¡ï¼Œä½†ç»è¿‡åç»­æŒç»­çš„è´¹ç±³å«æ˜Ÿä¼½é©¬å°„çº¿æ¢æµ‹å™¨çš„æœç´¢å¹¶æœªå‘ç°æ˜æ˜¾çš„ä¼½é©¬å°„çº¿è¾å°„ä¿¡å·ã€‚è¯¥æ˜Ÿçš„ä¼½é©¬å°„çº¿ä¸å…¶ä»–å·²çŸ¥æ’æ˜Ÿæ¯”è¾ƒä½ï¼Œå¹¶åœ¨å…‰å³°åä¸€å®šæ—¶é—´è¾¾åˆ°å³°å€¼ã€‚ç›®å‰æ²¡æœ‰æ£€æµ‹åˆ°ä¼½é©¬å°„çº¿å‘¨æœŸæ€§ã€‚æ ¹æ®å¤šæ³¢é•¿æ•°æ®çš„å¯¹æ¯”ï¼Œç¡®å®šäº†ä¼½é©¬å°„çº¿ä¸å…‰å­¦æµé‡ä¸Šé™çš„æ¯”å€¼çº¦ä¸º$10^{-4}$ï¼Œæœ‰åŠ©äºé™åˆ¶å†²å‡»åŠ é€Ÿã€‚é•¿æœŸçš„åˆ†æè¿˜æ˜¾ç¤ºï¼ŒV1405 Casè¿›å…¥è¶…è½¯çŠ¶æ€ï¼Œå…¶é»‘ä½“æ¸©åº¦çš„æœ€ä½³æ‹Ÿåˆå€¼ä¸º0.11è‡³0.19åƒç”µå­ä¼ã€‚æ­¤ç»“æœæœ‰åŠ©äºäº†è§£è¯¥æ’æ˜Ÿçš„åŸºæœ¬ç‰©ç†ç‰¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>V1405 Caså­˜åœ¨ç–‘ä¼¼ä¼½é©¬å°„çº¿è¾å°„ç°è±¡ï¼Œç”±è´¹ç±³å«æ˜Ÿä¼½é©¬å°„çº¿æ¢æµ‹å™¨è§‚æµ‹åˆ°ã€‚</li>
<li>æŒç»­æœç´¢æœªå‘ç°æ˜æ˜¾çš„ä¼½é©¬å°„çº¿è¾å°„ä¿¡å·ã€‚</li>
<li>V1405 Casçš„ä¼½é©¬å°„çº¿ä¸å…¶ä»–å·²çŸ¥æ’æ˜Ÿæ¯”è¾ƒç›¸å¯¹è¾ƒä½ã€‚</li>
<li>V1405 Casçš„ä¼½é©¬å°„çº¿å³°å€¼å‡ºç°åœ¨å…‰å³°åçš„ç‰¹å®šæ—¶é—´ã€‚</li>
<li>æœªæ£€æµ‹åˆ°ä¼½é©¬å°„çº¿çš„å‘¨æœŸæ€§ã€‚</li>
<li>é€šè¿‡å¤šæ³¢é•¿æ•°æ®å¯¹æ¯”ï¼Œç¡®å®šäº†ä¼½é©¬å°„çº¿ä¸å…‰å­¦æµé‡çš„æ¯”å€¼é™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5f39c4fb7034c63e3a2a86e3a0cadcb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9db0c50de079af5025bf7d631391575f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df767c900fdabdb34609dbf42553b66c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbeeaf3057a42773643dc6976d1090bb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Cancer-Type-Stage-and-Prognosis-Assessment-from-Pathology-Reports-using-LLMs"><a href="#Cancer-Type-Stage-and-Prognosis-Assessment-from-Pathology-Reports-using-LLMs" class="headerlink" title="Cancer Type, Stage and Prognosis Assessment from Pathology Reports using   LLMs"></a>Cancer Type, Stage and Prognosis Assessment from Pathology Reports using   LLMs</h2><p><strong>Authors:Rachit Saluja, Jacob Rosenthal, Yoav Artzi, David J. Pisapia, Benjamin L. Liechty, Mert R. Sabuncu</strong></p>
<p>Large Language Models (LLMs) have shown significant promise across various natural language processing tasks. However, their application in the field of pathology, particularly for extracting meaningful insights from unstructured medical texts such as pathology reports, remains underexplored and not well quantified. In this project, we leverage state-of-the-art language models, including the GPT family, Mistral models, and the open-source Llama models, to evaluate their performance in comprehensively analyzing pathology reports. Specifically, we assess their performance in cancer type identification, AJCC stage determination, and prognosis assessment, encompassing both information extraction and higher-order reasoning tasks. Based on a detailed analysis of their performance metrics in a zero-shot setting, we developed two instruction-tuned models: Path-llama3.1-8B and Path-GPT-4o-mini-FT. These models demonstrated superior performance in zero-shot cancer type identification, staging, and prognosis assessment compared to the other models evaluated. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç—…ç†å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»ç—…ç†æŠ¥å‘Šç­‰æ— ç»“æ„åŒ»å­¦æ–‡æœ¬ä¸­æå–æœ‰æ„ä¹‰è§è§£æ–¹é¢ï¼Œä»è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ä¸”æœªå¾—åˆ°å¾ˆå¥½çš„é‡åŒ–ã€‚åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTç³»åˆ—ã€Mistralæ¨¡å‹å’Œå¼€æºçš„Llamaæ¨¡å‹ï¼Œæ¥è¯„ä¼°å®ƒä»¬åœ¨ç»¼åˆåˆ†æç—…ç†æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å®ƒä»¬åœ¨ç™Œç—‡ç±»å‹è¯†åˆ«ã€AJCCåˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ä¿¡æ¯æå–å’Œé«˜çº§æ¨ç†ä»»åŠ¡ã€‚åŸºäºå¯¹é›¶æ ·æœ¬è®¾ç½®ä¸‹æ€§èƒ½æŒ‡æ ‡çš„è¯¦ç»†åˆ†æï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼šPath-llama3.1-8Bå’ŒPath-GPT-4o-mini-FTã€‚è¿™äº›æ¨¡å‹åœ¨é›¶æ ·æœ¬ç™Œç—‡ç±»å‹è¯†åˆ«ã€åˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢è¡¨ç°å‡ºä¼˜äºå…¶ä»–è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ï¼Œä½†åœ¨ç—…ç†å­¦é¢†åŸŸï¼Œå°¤å…¶æ˜¯ä»éç»“æ„åŒ–çš„åŒ»å­¦æ–‡æœ¬ï¼ˆå¦‚ç—…ç†æŠ¥å‘Šï¼‰ä¸­æå–æœ‰æ„ä¹‰ä¿¡æ¯æ–¹é¢ï¼Œå…¶åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ä¸”æœªè¢«å……åˆ†é‡åŒ–ã€‚æœ¬é¡¹ç›®åˆ©ç”¨å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTç³»åˆ—ã€Mistralæ¨¡å‹å’Œå¼€æºLlamaæ¨¡å‹ï¼Œè¯„ä¼°å®ƒä»¬åœ¨å…¨é¢åˆ†æç—…ç†æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç™Œç—‡ç±»å‹è¯†åˆ«ã€AJCCåˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡åˆ†æé›¶æ ·æœ¬ç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ï¼Œå¼€å‘äº†ä¸¤ä¸ªæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼šPath-llama3.1-8Bå’ŒPath-GPT-4o-mini-FTã€‚è¿™ä¸¤ä¸ªæ¨¡å‹åœ¨é›¶æ ·æœ¬ç™Œç—‡ç±»å‹è¯†åˆ«ã€åˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ï¼Œä½†åœ¨ç—…ç†å­¦é¢†åŸŸåº”ç”¨ä»æœ‰é™ã€‚</li>
<li>æœ¬é¡¹ç›®åˆ©ç”¨å…ˆè¿›è¯­è¨€æ¨¡å‹è¯„ä¼°åˆ†æç—…ç†æŠ¥å‘Šçš„æ€§èƒ½ã€‚</li>
<li>è¯„ä¼°å†…å®¹åŒ…æ‹¬ç™Œç—‡ç±»å‹è¯†åˆ«ã€AJCCåˆ†æœŸå’Œé¢„åè¯„ä¼°ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹è¯¦ç»†åˆ†æäº†ä¸åŒè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¼€å‘äº†ä¸¤ä¸ªæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼šPath-llama3.1-8Bå’ŒPath-GPT-4o-mini-FTã€‚</li>
<li>è¿™ä¸¤ä¸ªæ¨¡å‹åœ¨ç™Œç—‡ç±»å‹è¯†åˆ«ã€åˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9b477ec0e225601a11bd14523441e7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a00f2d514a91775b1343adee35e44eb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6198bf5d8c1936749922a1a3c2334ec5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c73dea0ae000f5919209434f8ecfe944.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Med-LEGO-Editing-and-Adapting-toward-Generalist-Medical-Image-Diagnosis"><a href="#Med-LEGO-Editing-and-Adapting-toward-Generalist-Medical-Image-Diagnosis" class="headerlink" title="Med-LEGO: Editing and Adapting toward Generalist Medical Image Diagnosis"></a>Med-LEGO: Editing and Adapting toward Generalist Medical Image Diagnosis</h2><p><strong>Authors:Yitao Zhu, Yuan Yin, Jiaming Li, Mengjie Xu, Zihao Zhao, Honglin Xiong, Sheng Wang, Qian Wang</strong></p>
<p>The adoption of visual foundation models has become a common practice in computer-aided diagnosis (CAD). While these foundation models provide a viable solution for creating generalist medical AI, privacy concerns make it difficult to pre-train or continuously update such models across multiple domains and datasets, leading many studies to focus on specialist models. To address this challenge, we propose Med-LEGO, a training-free framework that enables the seamless integration or updating of a generalist CAD model by combining multiple specialist models, similar to assembling LEGO bricks. Med-LEGO enhances LoRA (low-rank adaptation) by incorporating singular value decomposition (SVD) to efficiently capture the domain expertise of each specialist model with minimal additional parameters. By combining these adapted weights through simple operations, Med-LEGO allows for the easy integration or modification of specific diagnostic capabilities without the need for original data or retraining. Finally, the combined model can be further adapted to new diagnostic tasks, making it a versatile generalist model. Our extensive experiments demonstrate that Med-LEGO outperforms existing methods in both cross-domain and in-domain medical tasks while using only 0.18% of full model parameters. These merged models show better convergence and generalization to new tasks, providing an effective path toward generalist medical AI. </p>
<blockquote>
<p>åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ä¸­ï¼Œé‡‡ç”¨è§†è§‰åŸºç¡€æ¨¡å‹å·²ç»æˆä¸ºä¸€ç§å¸¸è§åšæ³•ã€‚è™½ç„¶è¿™äº›åŸºç¡€æ¨¡å‹ä¸ºåˆ›å»ºé€šç”¨åŒ»ç–—äººå·¥æ™ºèƒ½æä¾›äº†ä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œä½†éšç§é—®é¢˜ä½¿å¾—åœ¨å¤šä¸ªé¢†åŸŸå’Œæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒæˆ–æŒç»­æ›´æ–°æ­¤ç±»æ¨¡å‹å˜å¾—å›°éš¾ï¼Œä»è€Œå¯¼è‡´è®¸å¤šç ”ç©¶ä¸“æ³¨äºä¸“ä¸šæ¨¡å‹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Med-LEGOï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„åŸºç¡€æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡ç»„åˆå¤šä¸ªä¸“ä¸šæ¨¡å‹æ¥å®ç°é€šç”¨CADæ¨¡å‹çš„æ— ç¼é›†æˆæˆ–æ›´æ–°ï¼Œç±»ä¼¼äºç»„è£…ä¹é«˜ç§¯æœ¨ã€‚Med-LEGOé€šè¿‡ç»“åˆå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å¢å¼ºäº†LoRAï¼ˆä½ç§©é€‚åº”ï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·æ¯ä¸ªä¸“ä¸šæ¨¡å‹çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¸”åªéœ€æå°‘çš„é¢å¤–å‚æ•°ã€‚é€šè¿‡ç®€å•çš„æ“ä½œç»„åˆè¿™äº›é€‚é…æƒé‡ï¼ŒMed-LEGOå¯ä»¥è½»æ¾åœ°é›†æˆæˆ–ä¿®æ”¹ç‰¹å®šçš„è¯Šæ–­èƒ½åŠ›ï¼Œè€Œæ— éœ€åŸå§‹æ•°æ®æˆ–é‡æ–°è®­ç»ƒã€‚æœ€åï¼Œè¯¥ç»„åˆæ¨¡å‹å¯ä»¥è¿›ä¸€æ­¥é€‚åº”æ–°çš„è¯Šæ–­ä»»åŠ¡ï¼Œä½¿å…¶æˆä¸ºé€šç”¨çš„è¯Šæ–­æ¨¡å‹ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨è·¨åŸŸè¿˜æ˜¯åœ¨å†…éƒ¨åŒ»å­¦ä»»åŠ¡ä¸­ï¼ŒMed-LEGOéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä»…ä½¿ç”¨å®Œæ•´æ¨¡å‹å‚æ•°çš„0.18%ã€‚è¿™äº›åˆå¹¶çš„æ¨¡å‹æ˜¾ç¤ºå‡ºæ›´å¥½çš„æ”¶æ•›æ€§å’Œå¯¹æ–°ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºé€šç”¨åŒ»ç–—äººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01164v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡†æ¶Med-LEGOï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ— ç¼é›†æˆæˆ–æ›´æ–°é€šç”¨CADæ¨¡å‹ã€‚é€šè¿‡ç»“åˆå¤šä¸ªä¸“ä¸šæ¨¡å‹ï¼ŒMed-LEGOåˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å¢å¼ºLoRAï¼ˆä½ç§©é€‚åº”ï¼‰æ–¹æ³•ï¼Œä»¥é«˜æ•ˆæ•è·æ¯ä¸ªä¸“ä¸šæ¨¡å‹çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼ŒåŒæ—¶æ— éœ€é¢å¤–çš„å‚æ•°ã€‚é€šè¿‡ç®€å•çš„æ“ä½œç»„åˆè¿™äº›é€‚åº”æƒé‡ï¼ŒMed-LEGOå¯ä»¥è½»æ¾åœ°é›†æˆæˆ–ä¿®æ”¹ç‰¹å®šçš„è¯Šæ–­èƒ½åŠ›ï¼Œæ— éœ€ä½¿ç”¨åŸå§‹æ•°æ®æˆ–é‡æ–°è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒMed-LEGOåœ¨è·¨åŸŸå’ŒåŸŸå†…åŒ»ç–—ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåˆå¹¶æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ”¶æ•›æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºé€šç”¨åŒ»ç–—äººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-LEGOæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é€šç”¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰æ¨¡å‹çš„é›†æˆå’Œæ›´æ–°é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤šä¸ªä¸“ä¸šæ¨¡å‹ï¼ŒMed-LEGOå®ç°äº†ç±»ä¼¼ä¹é«˜ç§¯æœ¨çš„ç»„è£…æ–¹å¼ï¼Œå®ç°é€šç”¨CADæ¨¡å‹çš„çµæ´»è°ƒæ•´å’Œä¼˜åŒ–ã€‚</li>
<li>Med-LEGOåˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å¢å¼ºLoRAæ–¹æ³•ï¼Œé«˜æ•ˆæ•è·ä¸“ä¸šæ¨¡å‹çš„é¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>è¯¥æ¡†æ¶å…è®¸åœ¨æ— éœ€åŸå§‹æ•°æ®æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè½»æ¾é›†æˆæˆ–ä¿®æ”¹ç‰¹å®šè¯Šæ–­èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMed-LEGOåœ¨è·¨åŸŸå’ŒåŸŸå†…åŒ»ç–—ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½¿ç”¨ä»…0.18%çš„å®Œæ•´æ¨¡å‹å‚æ•°å³å¯å®ç°é«˜æ€§èƒ½ã€‚</li>
<li>åˆå¹¶æ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ”¶æ•›æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-52e3d2d6edf9eb7d5dd8472d65aaab77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7708d30515cd73f1c9be5f1b090b4688.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="IteRPrimE-Zero-shot-Referring-Image-Segmentation-with-Iterative-Grad-CAM-Refinement-and-Primary-Word-Emphasis"><a href="#IteRPrimE-Zero-shot-Referring-Image-Segmentation-with-Iterative-Grad-CAM-Refinement-and-Primary-Word-Emphasis" class="headerlink" title="IteRPrimE: Zero-shot Referring Image Segmentation with Iterative   Grad-CAM Refinement and Primary Word Emphasis"></a>IteRPrimE: Zero-shot Referring Image Segmentation with Iterative   Grad-CAM Refinement and Primary Word Emphasis</h2><p><strong>Authors:Yuji Wang, Jingchen Ni, Yong Liu, Chun Yuan, Yansong Tang</strong></p>
<p>Zero-shot Referring Image Segmentation (RIS) identifies the instance mask that best aligns with a specified referring expression without training and fine-tuning, significantly reducing the labor-intensive annotation process. Despite achieving commendable results, previous CLIP-based models have a critical drawback: the models exhibit a notable reduction in their capacity to discern relative spatial relationships of objects. This is because they generate all possible masks on an image and evaluate each masked region for similarity to the given expression, often resulting in decreased sensitivity to direct positional clues in text inputs. Moreover, most methods have weak abilities to manage relationships between primary words and their contexts, causing confusion and reduced accuracy in identifying the correct target region. To address these challenges, we propose IteRPrimE (Iterative Grad-CAM Refinement and Primary word Emphasis), which leverages a saliency heatmap through Grad-CAM from a Vision-Language Pre-trained (VLP) model for image-text matching. An iterative Grad-CAM refinement strategy is introduced to progressively enhance the modelâ€™s focus on the target region and overcome positional insensitivity, creating a self-correcting effect. Additionally, we design the Primary Word Emphasis module to help the model handle complex semantic relations, enhancing its ability to attend to the intended object. Extensive experiments conducted on the RefCOCO&#x2F;+&#x2F;g, and PhraseCut benchmarks demonstrate that IteRPrimE outperforms previous state-of-the-art zero-shot methods, particularly excelling in out-of-domain scenarios. </p>
<blockquote>
<p>é›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰èƒ½å¤Ÿåœ¨æ— éœ€è®­ç»ƒå’Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œè¯†åˆ«ä¸æŒ‡å®šæŒ‡ä»£è¡¨è¾¾å¼æœ€ä½³åŒ¹é…çš„å®ä¾‹æ©è†œï¼Œä»è€Œæå¤§åœ°å‡å°‘äº†åŠ³åŠ¨å¯†é›†å‹çš„æ ‡æ³¨è¿‡ç¨‹ã€‚å°½ç®¡å·²ç»å–å¾—äº†å€¼å¾—ç§°èµçš„ç»“æœï¼Œä½†åŸºäºCLIPçš„æ¨¡å‹å­˜åœ¨ä¸€ä¸ªå…³é”®ç¼ºé™·ï¼šè¿™äº›æ¨¡å‹åœ¨è¾¨åˆ«å¯¹è±¡çš„ç›¸å¯¹ç©ºé—´å…³ç³»æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ä¸‹é™ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬ä¼šåœ¨å›¾åƒä¸Šç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æ©è†œï¼Œå¹¶è¯„ä¼°æ¯ä¸ªæ©ç åŒºåŸŸä¸ç»™å®šè¡¨è¾¾å¼çš„ç›¸ä¼¼æ€§ï¼Œè¿™å¾€å¾€å¯¼è‡´å¯¹æ–‡æœ¬è¾“å…¥ä¸­çš„ç›´æ¥ä½ç½®çº¿ç´¢çš„æ•æ„Ÿåº¦é™ä½ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æ–¹æ³•åœ¨å¤„ç†ä¸»è¦å•è¯å’Œä¸Šä¸‹æ–‡ä¹‹é—´çš„å…³ç³»æ–¹é¢èƒ½åŠ›è¾ƒå¼±ï¼Œå¯¼è‡´åœ¨è¯†åˆ«æ­£ç¡®ç›®æ ‡åŒºåŸŸæ—¶æ··æ·†å’Œå‡†ç¡®æ€§ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†IteRPrimEï¼ˆè¿­ä»£Grad-CAMç»†åŒ–ä¸ä¸»è¦å•è¯å¼ºè°ƒï¼‰ï¼Œå®ƒåˆ©ç”¨æ¥è‡ªè§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æ¨¡å‹çš„Grad-CAMçš„æ˜¾è‘—æ€§çƒ­å›¾è¿›è¡Œå›¾åƒæ–‡æœ¬åŒ¹é…ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¿­ä»£Grad-CAMç»†åŒ–ç­–ç•¥ï¼Œä»¥é€æ­¥å¢å¼ºæ¨¡å‹å¯¹ç›®æ ‡åŒºåŸŸçš„å…³æ³¨ï¼Œå…‹æœä½ç½®æ•æ„Ÿæ€§ï¼Œåˆ›é€ ä¸€ç§è‡ªæˆ‘æ ¡æ­£çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸»è¦å•è¯å¼ºè°ƒæ¨¡å—ï¼Œä»¥å¸®åŠ©æ¨¡å‹å¤„ç†å¤æ‚çš„è¯­ä¹‰å…³ç³»ï¼Œæé«˜å…¶å…³æ³¨ç›®æ ‡å¯¹è±¡çš„èƒ½åŠ›ã€‚åœ¨RefCOCO&#x2F;+&#x2F;gå’ŒPhraseCutåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIteRPrimEä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸŸå¤–åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00936v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPæ¨¡å‹çš„é›¶æ ·æœ¬å›¾åƒåˆ†å‰²æŠ€æœ¯åœ¨è‡ªç„¶è¯­è¨€ä¸å›¾åƒåŒæ­¥è¿‡ç¨‹ä¸­å­˜åœ¨ä¸€äº›å±€é™ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•IteRPrimEï¼Œå®ƒé€šè¿‡è¿­ä»£Grad-CAMç»†åŒ–ç­–ç•¥æ¥æå‡æ¨¡å‹å¯¹ç›®æ ‡åŒºåŸŸçš„å…³æ³¨ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªä¸»æ¬¡è¯å¼ºè°ƒæ¨¡å—æ¥å¤„ç†å¤æ‚çš„è¯­ä¹‰å…³ç³»ã€‚è¿™ç§æ–¹æ³•åœ¨RefCOCO&#x2F;+&#x2F;gå’ŒPhraseCutåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é›¶æ ·æœ¬å›¾åƒåˆ†å‰²æŠ€æœ¯å¯ä»¥æ˜¾è‘—é™ä½å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>åŸºäºCLIPçš„æ¨¡å‹åœ¨è¯†åˆ«å¯¹è±¡é—´çš„ç›¸å¯¹ç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>IteRPrimEæ–¹æ³•åˆ©ç”¨è¿­ä»£Grad-CAMç»†åŒ–ç­–ç•¥æ¥æå‡æ¨¡å‹å¯¹ç›®æ ‡åŒºåŸŸçš„å…³æ³¨ï¼Œå…‹æœå®šä½ä¸æ•æ„Ÿé—®é¢˜ã€‚</li>
<li>IteRPrimEè®¾è®¡äº†ä¸€ä¸ªä¸»æ¬¡è¯å¼ºè°ƒæ¨¡å—ï¼Œæé«˜æ¨¡å‹å¤„ç†å¤æ‚è¯­ä¹‰å…³ç³»çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨åŸŸåœºæ™¯ä¸­ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49d79522dc8180acc6809eb1f569ff18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad95fda3a6fccb2fb4a82837516046b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b193fcf225a7d2d26fe2b5eabdeb1026.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aef21f92107ede8411442f0b398b8820.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b13b74afecc4e6dc2b66db0d1b208b36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93e0af3217905c0c74d0e4677475395f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d57a1832455166fefc14a64e43104ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3c8ffc0a4493983d215f12db4ee41ed.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ca51e17f246f6e061f10adb66c1b6c4c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  InSerter Speech Instruction Following with Unsupervised Interleaved   Pre-training
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-32398b30f5a321f06b6bf6909b04c372.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  StageDesigner Artistic Stage Generation for Scenography via Theater   Scripts
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14773.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
