<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-03-06  2DGS-Avatar Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-767ab7fa30e8ec72218ec87b79691de4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    43 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-06-更新"><a href="#2025-03-06-更新" class="headerlink" title="2025-03-06 更新"></a>2025-03-06 更新</h1><h2 id="2DGS-Avatar-Animatable-High-fidelity-Clothed-Avatar-via-2D-Gaussian-Splatting"><a href="#2DGS-Avatar-Animatable-High-fidelity-Clothed-Avatar-via-2D-Gaussian-Splatting" class="headerlink" title="2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting"></a>2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting</h2><p><strong>Authors:Qipeng Yan, Mingyang Sun, Lihua Zhang</strong></p>
<p>Real-time rendering of high-fidelity and animatable avatars from monocular videos remains a challenging problem in computer vision and graphics. Over the past few years, the Neural Radiance Field (NeRF) has made significant progress in rendering quality but behaves poorly in run-time performance due to the low efficiency of volumetric rendering. Recently, methods based on 3D Gaussian Splatting (3DGS) have shown great potential in fast training and real-time rendering. However, they still suffer from artifacts caused by inaccurate geometry. To address these problems, we propose 2DGS-Avatar, a novel approach based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars with high-fidelity and fast training performance. Given monocular RGB videos as input, our method generates an avatar that can be driven by poses and rendered in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the advantages of fast training and rendering while also capturing detailed, dynamic, and photo-realistic appearances. We conduct abundant experiments on popular datasets such as AvatarRex and THuman4.0, demonstrating impressive performance in both qualitative and quantitative metrics. </p>
<blockquote>
<p>从单目视频中实时渲染高保真和可动画的化身仍然是计算机视觉和图形学中的一个具有挑战性的问题。过去几年，神经辐射场（NeRF）在渲染质量方面取得了显著进展，但由于体积渲染的低效率，在运行时性能上表现不佳。最近，基于三维高斯贴片技术（3DGS）的方法在快速训练和实时渲染方面显示出巨大潜力。然而，它们仍然受到几何不准确所导致的伪影的影响。为了解决这个问题，我们提出了基于二维高斯贴片技术（2DGS）的动画服装化身新方法——二维GS化身（Avatar）。我们的方法使用单目RGB视频作为输入，生成可以通过姿态驱动的、可实时渲染的化身。相较于基于三维高斯贴片技术的方法，我们的二维GS化身在保持快速训练和渲染优势的同时，捕捉到了详细的、动态的、逼真的外观。我们在如AvatarRex和THuman4.0等流行数据集上进行了大量实验，在定性和定量指标上都取得了令人印象深刻的表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02452v1">PDF</a> ICVRV 2024</p>
<p><strong>Summary</strong></p>
<p>基于单目视频的高保真可动画人物实时渲染仍是计算机视觉和图形领域的一个难题。神经辐射场（NeRF）在渲染质量方面取得了显著进展，但运行时效率较低。基于3D高斯喷涂（3DGS）的方法在快速训练和实时渲染方面显示出巨大潜力，但仍存在由几何不准确导致的伪影问题。为解决这些问题，本文提出基于2D高斯喷涂（2DGS）的2DGS-Avatar新方法，用于对可动画服装人物进行高保真建模，具有快速训练性能和实时渲染能力。该方法以单目RGB视频为输入，生成的人物模型可根据姿态驱动并以实时渲染方式呈现。相较于基于3DGS的方法，2DGS-Avatar保留了快速训练和渲染的优点，同时捕捉详细、动态和逼真的外观。在流行的数据集如AvatarRex和THuman4.0上的实验证明了其出色的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>实时渲染高保真可动画人物仍是计算机视觉领域的挑战。</li>
<li>现有方法如NeRF虽能提高渲染质量但运行时效率较低。</li>
<li>基于3DGS的方法在快速训练和实时渲染方面表现出潜力，但存在几何不准确导致的伪影问题。</li>
<li>2DGS-Avatar方法结合了2DGS技术，旨在解决上述问题，实现高保真可动画人物建模。</li>
<li>2DGS-Avatar能以单目RGB视频为输入，生成可姿态驱动的人物模型，并以实时方式渲染。</li>
<li>与基于3DGS的方法相比，2DGS-Avatar在快速训练和渲染的同时，能捕捉更详细、动态和逼真的外观。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02452">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a0336f847a70a7629aed33f587cd9ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91123f4413f9ef763a86ae6b011a81c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25f24955015d37bba5118c43e9f4117a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-559fdd72341d50e48a308055502b432a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63a28c3b242df567bcd9e689685eaee.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DQO-MAP-Dual-Quadrics-Multi-Object-mapping-with-Gaussian-Splatting"><a href="#DQO-MAP-Dual-Quadrics-Multi-Object-mapping-with-Gaussian-Splatting" class="headerlink" title="DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting"></a>DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting</h2><p><strong>Authors:Haoyuan Li, Ziqin Ye, Yue Hao, Weiyang Lin, Chao Ye</strong></p>
<p>Accurate object perception is essential for robotic applications such as object navigation. In this paper, we propose DQO-MAP, a novel object-SLAM system that seamlessly integrates object pose estimation and reconstruction. We employ 3D Gaussian Splatting for high-fidelity object reconstruction and leverage quadrics for precise object pose estimation. Both of them management is handled on the CPU, while optimization is performed on the GPU, significantly improving system efficiency. By associating objects with unique IDs, our system enables rapid object extraction from the scene. Extensive experimental results on object reconstruction and pose estimation demonstrate that DQO-MAP achieves outstanding performance in terms of precision, reconstruction quality, and computational efficiency. The code and dataset are available at: <a target="_blank" rel="noopener" href="https://github.com/LiHaoy-ux/DQO-MAP">https://github.com/LiHaoy-ux/DQO-MAP</a>. </p>
<blockquote>
<p>准确的物体感知对于物体导航等机器人应用至关重要。在本文中，我们提出了DQO-MAP，这是一种新型的对象SLAM系统，它无缝集成了对象姿态估计和重建。我们采用3D高斯拼贴实现高保真物体重建，并利用二次曲面进行精确物体姿态估计。它们两者都在CPU上处理，优化在GPU上进行，这大大提高了系统效率。通过将对象与唯一ID相关联，我们的系统能够实现从场景中快速提取对象。在物体重建和姿态估计方面的广泛实验结果证明，DQO-MAP在精度、重建质量和计算效率方面取得了出色的表现。代码和数据集可在：<a target="_blank" rel="noopener" href="https://github.com/LiHaoy-ux/DQO-MAP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LiHaoy-ux/DQO-MAP找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02223v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的基于深度学习的对象感知系统DQO-MAP，该系统集成了对象位姿估计和重建功能。通过采用3D高斯散斑技术和四元数的组合优化技术，系统在机器人对象导航等领域表现优异。系统的CPU负责数据处理，GPU进行优化的操作方式有效提高了系统运行效率。该系统通过对物体分配唯一ID实现场景的快速物体提取。实验结果表明，DQO-MAP在精度、重建质量和计算效率方面表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DQO-MAP是一种新型的对象感知系统，集成了对象位姿估计和重建功能。</li>
<li>采用3D高斯散斑技术实现高保真对象重建。</li>
<li>利用四元数进行精确的对象位姿估计。</li>
<li>系统采用CPU处理数据，GPU进行优化，提高了效率。</li>
<li>通过分配唯一ID，系统能迅速从场景中提取对象。</li>
<li>实验结果显示DQO-MAP在精度、重建质量和计算效率方面具有卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02223">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-10790ce6e1360b733dadd427cf187d20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fc0b326613b2d8272f5ab2aaabc6494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d2b698eefee832a104d89a105051331.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b4ff8676054556e1e5f0a18d1f316d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf5709f2d70f4de03720c6dcdc48763.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5ca9c5d2bb1dba6d7eb69a8602fc39f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f32bc5e95df79d05d80a0946f38dbf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8b6fc1c50a2bb9a8108247806fd7bf5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Difix3D-Improving-3D-Reconstructions-with-Single-Step-Diffusion-Models"><a href="#Difix3D-Improving-3D-Reconstructions-with-Single-Step-Diffusion-Models" class="headerlink" title="Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models"></a>Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models</h2><p><strong>Authors:Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling</strong></p>
<p>Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2$\times$ improvement in FID score over baselines while maintaining 3D consistency. </p>
<blockquote>
<p>神经辐射场和3D高斯拼贴技术已经彻底改变了3D重建和新型视图合成任务。然而，从极端新颖视角实现逼真的渲染仍然具有挑战性，因为表示中的伪影持续存在。在这项工作中，我们引入了Difix3D+，这是一种新型管道，旨在通过单步扩散模型增强3D重建和新型视图合成。我们方法的核心是Difix，这是一种单步图像扩散模型，经过训练，可提高并消除由于3D表示中约束不足的区域所导致的新型渲染视图中的伪影。Difix在我们的管道中扮演两个关键角色。首先，它在重建阶段用于清理从重建中渲染然后蒸馏回3D的伪训练视图。这极大地提高了约束不足的区域并提高了整体的3D表示质量。更重要的是，在推理过程中，Difix还充当神经增强器，有效地消除了由于不完美的3D监督和当前重建模型的有限容量而产生的残留伪影。Difix3D+是一种通用解决方案，一个与NeRF和3DGS表示兼容的单一模型，它在基线的基础上实现了FID得分的平均2倍提升，同时保持了3D一致性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01774v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>神经网络辐射场与三维高斯喷绘技术已经对三维重建和视角合成任务产生了革命性的影响。然而，从极端新视角实现逼真的渲染仍然具有挑战性，因为表示中仍存在伪影。本研究介绍了Difix3D+，这是一种新型管道，旨在通过单步扩散模型增强三维重建和视角合成。其核心是Difix单步图像扩散模型，用于增强和消除由三维表示的不受约束区域引起的渲染新视图中的伪影。Difix在管道中扮演两个关键角色。首先，它在重建阶段用于清理从重建中渲染并蒸馏回三维的伪训练视图，这极大地增强了不受约束的区域并提高了整体的三维表示质量。更重要的是，在推理过程中，Difix还充当神经增强器，有效地消除了由于三维监督不完美和当前重建模型的有限容量而产生的残余伪影。Difix3D+是一种通用解决方案，一个与NeRF和3DGS表示兼容的单模型，在基线的基础上实现了FID得分的平均两倍改进，同时保持了三维一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络辐射场与三维高斯喷绘技术在三维重建和视角合成任务中已有显著进展。</li>
<li>从极端新视角进行逼真渲染仍具挑战，表示中的伪影是主要问题。</li>
<li>Difix3D+是一种新型管道，利用单步扩散模型增强三维重建和视角合成。</li>
<li>核心组件Difix在管道中扮演双重角色：在重建阶段清理伪训练视图，并在推理过程中充当神经增强器。</li>
<li>Difix能增强不受约束的区域，提高三维表示质量，并有效去除表示中的伪影。</li>
<li>Difix3D+兼容NeRF和3DGS表示，较基线实现了FID得分的平均两倍改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7a75e326b590d50fda10ba198bf2d16c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-386bba37085eb54dd2daee0af8dfe5c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47cfbf3b63feeb9d4654954352571447.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04dbcc58e6d6212b602cf89b86e2c3ad.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OpenGS-SLAM-Open-Set-Dense-Semantic-SLAM-with-3D-Gaussian-Splatting-for-Object-Level-Scene-Understanding"><a href="#OpenGS-SLAM-Open-Set-Dense-Semantic-SLAM-with-3D-Gaussian-Splatting-for-Object-Level-Scene-Understanding" class="headerlink" title="OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for   Object-Level Scene Understanding"></a>OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for   Object-Level Scene Understanding</h2><p><strong>Authors:Dianyi Yang, Yu Gao, Xihan Wang, Yufeng Yue, Yi Yang, Mengyin Fu</strong></p>
<p>Recent advancements in 3D Gaussian Splatting have significantly improved the efficiency and quality of dense semantic SLAM. However, previous methods are generally constrained by limited-category pre-trained classifiers and implicit semantic representation, which hinder their performance in open-set scenarios and restrict 3D object-level scene understanding. To address these issues, we propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian representation to perform dense semantic SLAM in open-set environments. Our system integrates explicit semantic labels derived from 2D foundational models into the 3D Gaussian framework, facilitating robust 3D object-level scene understanding. We introduce Gaussian Voting Splatting to enable fast 2D label map rendering and scene updating. Additionally, we propose a Confidence-based 2D Label Consensus method to ensure consistent labeling across multiple views. Furthermore, we employ a Segmentation Counter Pruning strategy to improve the accuracy of semantic scene representation. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our method in scene understanding, tracking, and mapping, achieving 10 times faster semantic rendering and 2 times lower storage costs compared to existing methods. Project page: <a target="_blank" rel="noopener" href="https://young-bit.github.io/opengs-github.github.io/">https://young-bit.github.io/opengs-github.github.io/</a>. </p>
<blockquote>
<p>近年来，3D高斯摊铺技术的进展显著提高密集语义SLAM的效率和质量。然而，之前的方法通常受限于有限类别的预训练分类器和隐式语义表示，这阻碍了它们在开放场景中的性能，并限制了3D对象级别的场景理解。为了解决这些问题，我们提出了OpenGS-SLAM，这是一个利用3D高斯表示在开放环境中执行密集语义SLAM的创新框架。我们的系统将来自2D基础模型的显式语义标签集成到3D高斯框架中，促进了稳健的3D对象级别场景理解。我们引入了高斯投票摊铺技术，以实现快速2D标签图渲染和场景更新。此外，我们提出了一种基于置信度的2D标签共识方法，以确保跨多个视图的标签一致性。而且，我们采用了分割计数修剪策略，提高了语义场景表示的准确性。在合成和真实世界数据集上的大量实验表明，我们的方法在场景理解、跟踪和映射方面非常有效，与现有方法相比，实现了10倍更快的语义渲染和2倍更低的存储成本。项目页面：<a target="_blank" rel="noopener" href="https://young-bit.github.io/opengs-github.github.io/">链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01646v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了OpenGS-SLAM框架，该框架利用3D高斯表示在开放环境中进行密集语义SLAM。它通过整合来自二维基础模型的明确语义标签到三维高斯框架中，实现了稳健的三维物体级场景理解。提出的Gaussian Voting Splatting技术能够快速渲染二维标签地图并更新场景。同时，采用基于信心的二维标签共识方法确保跨多个视角的标签一致性。此外，还采用了Segmentation Counter Pruning策略来提高语义场景表示的准确性。此方法在合成和真实世界数据集上的实验结果表明，其在场景理解、跟踪和映射方面效果显著，实现了与现有方法相比10倍更快的语义渲染速度和降低一半存储成本的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OpenGS-SLAM利用三维高斯表示在开放环境中进行密集语义SLAM，提高了效率和质量。</li>
<li>该框架整合二维基础模型的明确语义标签到三维高斯框架中，促进了稳健的三维物体级场景理解。</li>
<li>Gaussian Voting Splatting技术可以快速渲染二维标签地图并更新场景。</li>
<li>基于信心的二维标签共识方法确保跨多个视角的标签一致性。</li>
<li>Segmentation Counter Pruning策略提高了语义场景表示的准确性。</li>
<li>在合成和真实世界数据集上的实验结果表明，OpenGS-SLAM在场景理解、跟踪和映射方面效果显著。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01646">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-afd5bd9b9a7e15137d16e2681497f283.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26084adddb87f088f0e0cb41f5eb072a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-966ed99593adc2317479d99f3edbdacf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-846b1cf9664c59e8b93adffcd4701410.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8a1a76d62823a160b777f2c7779f907.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61aeb188203c6763d1032efa69a48cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c265d9dc87f48258a2ad8cb44e5d0668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-394004e4a9cc56dada97e08a13192e50.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Vid2Avatar-Pro-Authentic-Avatar-from-Videos-in-the-Wild-via-Universal-Prior"><a href="#Vid2Avatar-Pro-Authentic-Avatar-from-Videos-in-the-Wild-via-Universal-Prior" class="headerlink" title="Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal   Prior"></a>Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal   Prior</h2><p><strong>Authors:Chen Guo, Junxuan Li, Yash Kant, Yaser Sheikh, Shunsuke Saito, Chen Cao</strong></p>
<p>We present Vid2Avatar-Pro, a method to create photorealistic and animatable 3D human avatars from monocular in-the-wild videos. Building a high-quality avatar that supports animation with diverse poses from a monocular video is challenging because the observation of pose diversity and view points is inherently limited. The lack of pose variations typically leads to poor generalization to novel poses, and avatars can easily overfit to limited input view points, producing artifacts and distortions from other views. In this work, we address these limitations by leveraging a universal prior model (UPM) learned from a large corpus of multi-view clothed human performance capture data. We build our representation on top of expressive 3D Gaussians with canonical front and back maps shared across identities. Once the UPM is learned to accurately reproduce the large-scale multi-view human images, we fine-tune the model with an in-the-wild video via inverse rendering to obtain a personalized photorealistic human avatar that can be faithfully animated to novel human motions and rendered from novel views. The experiments show that our approach based on the learned universal prior sets a new state-of-the-art in monocular avatar reconstruction by substantially outperforming existing approaches relying only on heuristic regularization or a shape prior of minimally clothed bodies (e.g., SMPL) on publicly available datasets. </p>
<blockquote>
<p>我们提出了Vid2Avatar-Pro方法，该方法可以从单目野生视频创建逼真且可动画的3D人类角色。从单目视频构建支持动画的优质角色，并具有各种姿势是一项挑战，因为姿势多样性和观察视角的观察本质上是有限的。姿势变化的缺乏通常导致对新姿势的泛化能力不佳，并且角色很容易过度适应有限的输入观点，导致其他观点产生伪影和失真。在这项工作中，我们通过利用从大量多视角着装人体捕获数据学习到的通用先验模型（UPM）来解决这些局限性。我们在具有跨身份共享的标准正面和背面图的表达性3D高斯之上构建我们的表示。一旦UPM能够准确地再现大规模的多视角人体图像，我们就会通过逆渲染使用野生视频对模型进行微调，从而获得个性化的逼真人类角色，可以忠实地动画化为新的人类运动，并从新视角进行渲染。实验表明，我们的基于学习通用先验的方法在单目角色重建方面树立了新的技术标杆，大大优于仅依赖启发式正则化或在最少着装身体（例如SMPL）的形状先验的现有方法在公开数据集上的表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01610v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://moygcc.github.io/vid2avatar-pro/">https://moygcc.github.io/vid2avatar-pro/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Vid2Avatar-Pro方法，该方法能够从单目野生视频创建逼真的可动画3D人类角色。通过使用从大规模多视角服装人体捕捉数据中学习的通用先验模型（UPM），解决了从单目视频构建支持动画的优质角色的挑战。通过逆渲染技术，对野生视频进行个性化微调，获得可动画的逼真人类角色，可从新的视角进行渲染。实验表明，基于学习到的通用先验的方法在单目角色重建方面表现最优，大幅优于仅依赖启发式正则化或最小穿衣身体形状的先验方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vid2Avatar-Pro能从单目野生视频创建高质量的可动画3D人类角色。</li>
<li>使用通用先验模型（UPM）解决从单目视频构建角色的挑战。</li>
<li>利用多视角服装人体捕捉数据学习UPM，实现更准确的人物表现捕捉。</li>
<li>通过逆渲染技术个性化微调模型，获得逼真的可动画人类角色。</li>
<li>新方法能够从新的视角渲染角色，提供更大的灵活性。</li>
<li>实验显示，该方法在单目角色重建方面表现最佳，优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01610">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-64ef37fee6ce81d425f09130d0bd2ed8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b371de6797a92d89230d67b02b0d054.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78a5ecce15973e354e08e2d0d742ce58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-940cbcb20b2327407747c4b4b1058397.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Evolving-High-Quality-Rendering-and-Reconstruction-in-a-Unified-Framework-with-Contribution-Adaptive-Regularization"><a href="#Evolving-High-Quality-Rendering-and-Reconstruction-in-a-Unified-Framework-with-Contribution-Adaptive-Regularization" class="headerlink" title="Evolving High-Quality Rendering and Reconstruction in a Unified   Framework with Contribution-Adaptive Regularization"></a>Evolving High-Quality Rendering and Reconstruction in a Unified   Framework with Contribution-Adaptive Regularization</h2><p><strong>Authors:You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao</strong></p>
<p>Representing 3D scenes from multiview images is a core challenge in computer vision and graphics, which requires both precise rendering and accurate reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention for its high-quality rendering and fast inference speed. Yet, due to the unstructured and irregular nature of Gaussian point clouds, ensuring accurate geometry reconstruction remains difficult. Existing methods primarily focus on geometry regularization, with common approaches including primitive-based and dual-model frameworks. However, the former suffers from inherent conflicts between rendering and reconstruction, while the latter is computationally and storage-intensive. To address these challenges, we propose CarGS, a unified model leveraging Contribution-adaptive regularization to achieve simultaneous, high-quality rendering and surface reconstruction. The essence of our framework is learning adaptive contribution for Gaussian primitives by squeezing the knowledge from geometry regularization into a compact MLP. Additionally, we introduce a geometry-guided densification strategy with clues from both normals and Signed Distance Fields (SDF) to improve the capability of capturing high-frequency details. Our design improves the mutual learning of the two tasks, meanwhile its unified structure does not require separate models as in dual-model based approaches, guaranteeing efficiency. Extensive experiments demonstrate the ability to achieve state-of-the-art (SOTA) results in both rendering fidelity and reconstruction accuracy while maintaining real-time speed and minimal storage size. </p>
<blockquote>
<p>从多视角图像表示3D场景是计算机视觉和图形的核心挑战，这需要精确渲染和准确重建。近期，3D高斯喷射（3DGS）因其高质量渲染和快速推理速度而受到广泛关注。然而，由于高斯点云的无结构和不规则性质，确保准确的几何重建仍然很困难。现有方法主要集中在几何正则化上，常见的方法包括基于原始数据和双重模型框架。然而，前者在渲染和重建之间存在内在冲突，后者则计算和存储密集。为了解决这些挑战，我们提出了CarGS，这是一个利用贡献自适应正则化方法的统一模型，实现高质量的同时渲染和表面重建。我们的框架的核心是通过从几何正则化中挤压知识来学习高斯原始数据的自适应贡献，并将其压缩到一个紧凑的MLP中。此外，我们引入了一种受几何指导的加密策略，通过法线和带符号距离场（SDF）的线索来提高捕捉高频细节的能力。我们的设计提高了两个任务的相互学习，同时其统一的结构不需要像双重模型方法那样使用单独的模型，保证了效率。大量实验证明，我们的方法在渲染保真度和重建准确性方面达到了最新水平（SOTA），同时保持了实时速度和最小的存储大小。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00881v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了在计算机视觉和图形学中，从多视角图像表示3D场景的核心挑战需要精确渲染和准确重建。近期，3D高斯拼贴（3DGS）因其高质量渲染和快速推理速度而受到关注。然而，由于高斯点云的无结构和不规则性质，确保准确的几何重建仍然具有挑战性。现有方法主要集中在几何正则化上，包括基于原始和双重模型框架的常见方法，但前者在渲染和重建之间存在固有冲突，后者计算和存储密集。为解决这些挑战，我们提出CarGS，一个利用贡献自适应正则化的统一模型，实现高质量渲染和表面重建。我们的框架的本质是通过从几何正则化中挤压知识来训练自适应贡献的高斯原始模型。此外，我们引入了一个受几何指导的密集化策略，从法线和带符号距离字段（SDF）的线索中提高捕捉高频细节的能力。我们的设计提高了两个任务的相互学习，同时其统一结构不需要像双重模型方法那样使用单独模型，保证了效率。实验证明，我们的方法在渲染保真度和重建准确性方面达到了最新水平，同时保持了实时速度和较小的存储大小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGS在计算机视觉和图形学中对于多视角图像表示的3D场景存在挑战，需同时满足精确渲染和准确重建的要求。</li>
<li>现有方法主要关注几何正则化，但存在缺陷：基于原始的方法在渲染和重建之间存在冲突；而基于双重模型的方法计算和存储密集。</li>
<li>针对以上挑战，提出了CarGS框架，该框架使用贡献自适应正则化技术，旨在实现高质量渲染和表面重建的统一。</li>
<li>CarGS利用紧凑的MLP模型从几何正则化中学习自适应贡献的知识，以改善几何重建能力。</li>
<li>通过引入受几何指导的密集化策略（利用法线和带符号距离字段），增强了捕捉高频细节的能力。</li>
<li>CarGS设计提高了渲染和重建任务的相互学习，同时保持高效性，无需使用单独模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00881">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ec288cd7abf491f08df3127780c8910f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a3d159707b496c9e82c4dd39aefeee0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-784a62be389db7178a451fd8402624e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee3f500b0d2b4ac1722e44388ea94115.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c9da0701706474d44e9d1f2f4119fae.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DoF-Gaussian-Controllable-Depth-of-Field-for-3D-Gaussian-Splatting"><a href="#DoF-Gaussian-Controllable-Depth-of-Field-for-3D-Gaussian-Splatting" class="headerlink" title="DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting"></a>DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting</h2><p><strong>Authors:Liao Shen, Tianqi Liu, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, Chen Change Loy</strong></p>
<p>Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable success in representing 3D scenes and generating high-quality, novel views in real-time. However, 3D-GS and its variants assume that input images are captured based on pinhole imaging and are fully in focus. This assumption limits their applicability, as real-world images often feature shallow depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable depth-of-field method for 3D-GS. We develop a lens-based imaging model based on geometric optics principles to control DoF effects. To ensure accurate scene geometry, we incorporate depth priors adjusted per scene, and we apply defocus-to-focus adaptation to minimize the gap in the circle of confusion. We also introduce a synthetic dataset to assess refocusing capabilities and the model’s ability to learn precise lens parameters. Our framework is customizable and supports various interactive applications. Extensive experiments confirm the effectiveness of our method. Our project is available at <a target="_blank" rel="noopener" href="https://dof-gaussian.github.io/">https://dof-gaussian.github.io</a>. </p>
<blockquote>
<p>近期三维高斯喷绘（3D-GS）的进展在表示三维场景和实时生成高质量新视角方面取得了显著的成功。然而，3D-GS及其变体假设输入图像是基于针孔成像技术捕获的，并且完全在焦点内。这一假设限制了其适用性，因为现实世界中的图像通常具有较浅的景深（DoF）。在本文中，我们介绍了DoF-Gaussian，这是一种可控景深的三维高斯喷绘方法。我们基于几何光学原理开发了一种基于镜头的成像模型来控制景深效果。为了确保场景几何的准确性，我们根据场景调整了深度先验，并应用了失焦到聚焦的适应来减小模糊圆之间的间隙。我们还引入了一个合成数据集来评估重新聚焦能力和模型学习精确镜头参数的能力。我们的框架可定制且支持各种交互应用程序。大量实验证实了我们方法的有效性。我们的项目可在<a target="_blank" rel="noopener" href="https://dof-gaussian.github.io访问./">https://dof-gaussian.github.io访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00746v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>3D高斯贴图（3D-GS）的最新进展在表示3D场景和实时生成高质量新视角方面取得了显著成功。然而，该方法和其变种假设输入图像是基于针孔成像且完全对焦的，这限制了其在现实世界的应用，因为真实图像往往具有较浅的景深（DoF）。本文介绍了一种可控景深的方法DoF-Gaussian，用于改进3D-GS。我们基于几何光学原理开发了一种基于镜头的成像模型来控制景深效果。为确保准确的场景几何结构，我们根据场景调整了深度先验知识，并采用了失焦到对焦的适应策略来减少模糊圆圈的差距。我们还引入了一个合成数据集来评估重新聚焦能力和模型学习精确镜头参数的能力。我们的框架支持各种交互式应用，并经过广泛实验验证其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D高斯贴图（3D-GS）在表示3D场景和生成新视角方面表现卓越。</li>
<li>现有方法假设输入图像完全对焦，这限制了其在现实世界的适用性。</li>
<li>引入DoF-Gaussian方法，通过镜头成像模型控制景深，提高现实应用性能。</li>
<li>结合场景深度先验知识和失焦到对焦的适应策略，确保准确场景几何结构。</li>
<li>引入合成数据集评估重新聚焦能力和模型精确学习镜头参数的能力。</li>
<li>框架支持交互式应用，具有广泛的实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f99bb0468031115528b50bc7209ae9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb0f076551e3e9eb5a2ea2ef822fd3f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bdd2917a6f721b5bc2d9ac5f19ff4e71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-767ab7fa30e8ec72218ec87b79691de4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-002ecea58f49a1205993230b3a1a2cf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7de7746e6addc6796f4df1ffd364d16.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-Monocular-3D-Scene-Completion-with-Diffusion-Model"><a href="#Enhancing-Monocular-3D-Scene-Completion-with-Diffusion-Model" class="headerlink" title="Enhancing Monocular 3D Scene Completion with Diffusion Model"></a>Enhancing Monocular 3D Scene Completion with Diffusion Model</h2><p><strong>Authors:Changlin Song, Jiaqi Wang, Liyun Zhu, He Weng</strong></p>
<p>3D scene reconstruction is essential for applications in virtual reality, robotics, and autonomous driving, enabling machines to understand and interact with complex environments. Traditional 3D Gaussian Splatting techniques rely on images captured from multiple viewpoints to achieve optimal performance, but this dependence limits their use in scenarios where only a single image is available. In this work, we introduce FlashDreamer, a novel approach for reconstructing a complete 3D scene from a single image, significantly reducing the need for multi-view inputs. Our approach leverages a pre-trained vision-language model to generate descriptive prompts for the scene, guiding a diffusion model to produce images from various perspectives, which are then fused to form a cohesive 3D reconstruction. Extensive experiments show that our method effectively and robustly expands single-image inputs into a comprehensive 3D scene, extending monocular 3D reconstruction capabilities without further training. Our code is available <a target="_blank" rel="noopener" href="https://github.com/CharlieSong1999/FlashDreamer/tree/main">https://github.com/CharlieSong1999/FlashDreamer/tree/main</a>. </p>
<blockquote>
<p>三维场景重建在虚拟现实、机器人技术和自动驾驶等领域的应用至关重要，它使机器能够理解和与复杂环境进行交互。传统的三维高斯喷涂技术依赖于从多个视角捕获的图像来达到最佳性能，但这种依赖限制了其在仅提供单张图像的场景中的使用。在这项工作中，我们介绍了FlashDreamer，这是一种从单张图像重建完整三维场景的新方法，显著减少了对多视角输入的需求。我们的方法利用预训练的视觉语言模型生成场景的描述性提示，引导扩散模型从各个视角生成图像，然后将其融合形成连贯的三维重建。大量实验表明，我们的方法有效且稳健地将单张图像输入扩展为全面的三维场景，无需进一步训练即可扩展单目三维重建能力。我们的代码可通过<a target="_blank" rel="noopener" href="https://github.com/CharliSong1999/FlashDreamer/tree/main%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/CharlieSong1999/FlashDreamer/tree/main访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00726v1">PDF</a> All authors had equal contribution</p>
<p><strong>Summary</strong></p>
<p>本文介绍了FlashDreamer技术，该技术能够从单一图像重建完整的3D场景，大大减少了对于多角度图像输入的需求。该方法结合了预训练的视觉语言模型和扩散模型，生成场景的描述性提示并产生多角度图像，再融合形成连贯的3D重建。实验证明，该方法有效且稳健地将单图像输入扩展为全面的3D场景，无需进一步训练即可扩展单目3D重建能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlashDreamer能够从单一图像重建完整的3D场景。</li>
<li>传统3D重建技术依赖于从多个视角捕获的图像，而FlashDreamer则大大减少了这一需求。</li>
<li>FlashDreamer结合了预训练的视觉语言模型和扩散模型，生成场景的描述并产生多角度图像。</li>
<li>生成的图像再融合形成连贯的3D重建。</li>
<li>实验证明FlashDreamer在单目3D重建方面具有有效性和稳健性。</li>
<li>该技术可用于虚拟现实、机器人技术和自动驾驶等多个领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00726">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1fa3b141ad05dc9d9cdca521ac6b3f7b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-605d8f966ba1824041f6235f20537788.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e879d26f3c022e8cfa50b8015b213171.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-292add365084dd11cf981f80a50a9673.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61cfd0c707610ab83228788b4afa5812.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b67b578f0a90ca5a6327b69bbcb61291.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GaussianSeal-Rooting-Adaptive-Watermarks-for-3D-Gaussian-Generation-Model"><a href="#GaussianSeal-Rooting-Adaptive-Watermarks-for-3D-Gaussian-Generation-Model" class="headerlink" title="GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation   Model"></a>GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation   Model</h2><p><strong>Authors:Runyi Li, Xuanyu Zhang, Chuhan Tong, Zhipei Xu, Jian Zhang</strong></p>
<p>With the advancement of AIGC technologies, the modalities generated by models have expanded from images and videos to 3D objects, leading to an increasing number of works focused on 3D Gaussian Splatting (3DGS) generative models. Existing research on copyright protection for generative models has primarily concentrated on watermarking in image and text modalities, with little exploration into the copyright protection of 3D object generative models. In this paper, we propose the first bit watermarking framework for 3DGS generative models, named GaussianSeal, to enable the decoding of bits as copyright identifiers from the rendered outputs of generated 3DGS. By incorporating adaptive bit modulation modules into the generative model and embedding them into the network blocks in an adaptive way, we achieve high-precision bit decoding with minimal training overhead while maintaining the fidelity of the model’s outputs. Experiments demonstrate that our method outperforms post-processing watermarking approaches for 3DGS objects, achieving superior performance of watermark decoding accuracy and preserving the quality of the generated results. </p>
<blockquote>
<p>随着AIGC技术的进步，模型生成的模态已经从图像和视频扩展到了三维物体，这导致了对三维高斯拼贴（3DGS）生成模型的研究工作数量不断增加。目前关于生成模型版权保护的研究主要集中在图像和文本模态的水印上，而对三维物体生成模型的版权保护研究探索较少。在本文中，我们提出了首个针对3DGS生成模型的位水印框架，名为GaussianSeal。它能够从生成的3DGS渲染输出中解码作为版权标识符的位。通过将自适应位调制模块集成到生成模型中，并以自适应的方式嵌入到网络块中，我们在几乎不增加训练开销的情况下实现了高精度的位解码，同时保持了模型输出的保真度。实验表明，我们的方法在三维高斯拼贴对象的后处理水印方法上具有优越性，实现了水印解码的高精度并保持了生成结果的质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00531v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着AIGC技术的发展，生成模型产生的形式已从图像和视频扩展到三维物体，引发了越来越多关于三维高斯模糊（3DGS）生成模型的研究。目前关于生成模型的版权保护研究主要集中在图像和文本的模态水印上，而对三维对象生成模型的版权保护探索甚少。本文提出了首个针对3DGS生成模型的比特水印框架，名为GaussianSeal，可从生成的3DGS渲染输出中解码作为版权标识的比特。通过将自适应比特调制模块融入生成模型并以自适应方式嵌入网络块，我们在几乎不影响模型输出保真度的前提下实现了高精度比特解码，且训练开销极小。实验证明，我们的方法相较于针对三维高斯模糊对象的后处理水印方法表现更佳，实现了水印解码的高精度和生成结果的优质表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIGC技术的进展推动了生成模型形式的扩展，现在包括三维物体生成模型的研究。</li>
<li>目前关于生成模型的版权保护研究主要集中在图像和文本模态的水印应用上。</li>
<li>针对三维对象生成模型的版权保护探索相对不足。</li>
<li>本文提出了名为GaussianSeal的比特水印框架，专门用于为三维高斯模糊生成模型提供版权保护。</li>
<li>通过结合自适应比特调制模块与网络结构，GaussianSeal能够在保证模型输出质量的同时实现高精度比特解码。</li>
<li>与传统的后处理水印方法相比，GaussianSeal提供了更优秀的性能表现，特别是在水印解码的精度方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00531">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-16c79ae42131c5a9a9b5fd30a049eb5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82fed864225a3ffd0218c9a98d8c790b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640703948c8a10c10ec2ef52fce2bb8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89b146a620292eba0b8ac18e726bf52f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-739fc50f3a350145a66b3bdbb420c352.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OpenFly-A-Versatile-Toolchain-and-Large-scale-Benchmark-for-Aerial-Vision-Language-Navigation"><a href="#OpenFly-A-Versatile-Toolchain-and-Large-scale-Benchmark-for-Aerial-Vision-Language-Navigation" class="headerlink" title="OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial   Vision-Language Navigation"></a>OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial   Vision-Language Navigation</h2><p><strong>Authors:Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li</strong></p>
<p>Vision-Language Navigation (VLN) aims to guide agents through an environment by leveraging both language instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor aerial VLN remains underexplored. The potential reason is that outdoor aerial view encompasses vast areas, making data collection more challenging, which results in a lack of benchmarks. To address this problem, we propose OpenFly, a platform comprising a versatile toolchain and large-scale benchmark for aerial VLN. Firstly, we develop a highly automated toolchain for data collection, enabling automatic point cloud acquisition, scene semantic segmentation, flight trajectory creation, and instruction generation. Secondly, based on the toolchain, we construct a large-scale aerial VLN dataset with 100k trajectories, covering diverse heights and lengths across 18 scenes. The corresponding visual data are generated using various rendering engines and advanced techniques, including Unreal Engine, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). All data exhibit high visual quality. Particularly, 3D GS supports real-to-sim rendering, further enhancing the realism of the dataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, which takes language instructions, current observations, and historical keyframes as input, and outputs flight actions directly. Extensive analyses and experiments are conducted, showcasing the superiority of our OpenFly platform and OpenFly-Agent. The toolchain, dataset, and codes will be open-sourced. </p>
<blockquote>
<p>视觉语言导航（VLN）旨在利用语言指令和视觉线索来引导代理在环境中导航，是嵌入式人工智能中的核心要素。室内VLN已经得到了广泛的研究，而户外空中VLN仍然未被充分探索。可能的原因是户外航拍涉及大面积的区域，使得数据收集更具挑战性，从而缺乏基准测试集。为了解决这个问题，我们提出了OpenFly平台，这是一个包含空中VLN的通用工具链和大规模基准测试的平台。首先，我们开发了一个高度自动化的工具链进行数据采集，能够实现点云自动采集、场景语义分割、飞行轨迹创建和指令生成。其次，基于该工具链，我们构建了一个大规模空中VLN数据集，包含10万条轨迹，涵盖18个场景的多种高度和长度。相应的视觉数据采用各种渲染引擎和先进技术生成，包括Unreal Engine、GTA V、Google Earth和3D高斯溅射（3D GS）。所有数据都表现出高质量的可视效果。特别是，3D GS支持实到模拟渲染，进一步增强了数据集的逼真性。此外，我们提出了OpenFly-Agent，一个关键帧感知的VLN模型，它接受语言指令、当前观察结果和历史关键帧作为输入，并直接输出飞行动作。进行了广泛的分析和实验，展示了OpenFly平台和OpenFly-Agent的优越性。工具链、数据集和代码将开源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18041v3">PDF</a> </p>
<p><strong>Summary</strong><br>     针对室外航拍视觉语言导航（VLN）领域缺乏大规模基准数据的问题，该研究提出了OpenFly平台，包括多功能工具链和大规模基准数据集。平台能实现自动数据采集、场景语义分割、飞行轨迹创建和指令生成，构建了涵盖不同高度和长度的超大规模航拍VLN数据集。同时推出OpenFly-Agent模型，表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenFly平台旨在解决室外航拍视觉语言导航领域缺乏大规模基准数据的问题。</li>
<li>平台提供了一个高度自动化的工具链，包括数据采集、场景语义分割等。</li>
<li>构建了一个大规模航拍VLN数据集，包含超百万轨迹数据，覆盖多种场景和高度。</li>
<li>数据集使用了多种渲染引擎和技术，包括Unreal Engine等，保证了高质量视觉效果。</li>
<li>OpenFly-Agent模型是一个基于关键帧的VLN模型，输入语言指令等，输出飞行动作。</li>
<li>实验分析表明OpenFly平台和OpenFly-Agent模型具有优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18041">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-84c2d6d19f35b95d872ca3e52bfaf5cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db52cdeac37cfba3a4d9e3fc0c3dd1eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a46ffe2d0d2b6f90c2da88684d1a3c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14171f3e22ed6bed2908260cfcdd1cc6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CityGaussianV2-Efficient-and-Geometrically-Accurate-Reconstruction-for-Large-Scale-Scenes"><a href="#CityGaussianV2-Efficient-and-Geometrically-Accurate-Reconstruction-for-Large-Scale-Scenes" class="headerlink" title="CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for   Large-Scale Scenes"></a>CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for   Large-Scale Scenes</h2><p><strong>Authors:Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang</strong></p>
<p>Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10$\times$ compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs. The project page is available at <a target="_blank" rel="noopener" href="https://dekuliutesla.github.io/CityGaussianV2/">https://dekuliutesla.github.io/CityGaussianV2/</a>. </p>
<blockquote>
<p>最近，3D高斯拼贴（3DGS）已经彻底改变了辐射场重建，展现出高效和高保真的新型视图合成。然而，由于3DGS的非结构化特性，准确表示表面，特别是在大型和复杂场景中，仍然是一个巨大的挑战。在本文中，我们提出了CityGaussianV2，这是一种针对大型场景重建的新方法，解决了与几何精度和效率相关的关键挑战。我们在二维高斯拼贴（2DGS）的有利泛化能力的基础上，解决了其收敛性和可扩展性问题。具体来说，我们实现了基于分解梯度的密集化和深度回归技术，以消除模糊伪影并加速收敛。为了扩展规模，我们引入了一个伸长过滤器，以缓解由2DGS退化引起的高斯计数爆炸。此外，我们对CityGaussian管道进行了优化，以实现并行训练，达到最高10倍的压缩率，至少节省25%的训练时间和减少一半的内存使用。我们还建立了大规模场景下的标准几何基准测试。实验结果表明，我们的方法在视觉质量、几何精度以及存储和训练成本之间取得了有希望的平衡。项目页面可在<a target="_blank" rel="noopener" href="https://dekuliutesla.github.io/CityGaussianV2/%E6%89%BE%E5%88%B0%E3%80%82">https://dekuliutesla.github.io/CityGaussianV2/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00771v2">PDF</a> Accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>基于三维高斯点云技术（3DGS）的局限性，本文提出CityGaussianV2，利用二维高斯点云（2DGS）的良好泛化能力进行大型场景重建。通过梯度分解密度增强和深度回归技术，解决了模糊伪影问题并加速了收敛。引入伸长滤波器缓解高斯计数爆炸问题，优化CityGaussian管道实现并行训练，达到压缩、节省训练时间和降低内存使用的效果。建立大型场景的标准几何基准测试，实现了视觉质量、几何准确性以及存储和训练成本的平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CityGaussianV2利用二维高斯点云（2DGS）的泛化能力进行大型场景重建，解决了三维高斯点云技术（3DGS）的挑战。</li>
<li>通过梯度分解密度增强和深度回归技术，解决了模糊伪影问题并加速了收敛。</li>
<li>引入伸长滤波器缓解高斯计数爆炸问题，优化并行训练管道。</li>
<li>实现高达10倍的压缩率，节省至少25%的训练时间和50%的内存使用。</li>
<li>建立了大型场景的标准几何基准测试。</li>
<li>实验结果表明，CityGaussianV2在视觉质量、几何准确性以及存储和训练成本之间达到了平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00771">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-20dfe09979bc5895f7916e164ecf8359.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c1bb1c9b9e78115c2bd8256c14f31ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86f2d09a4222beca5c4ff1f79f856cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17d41bfd9cc43a366695846be4eed01c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-190fb996ee21d987789559f1b5cf54f5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-03-06  2DGS-Avatar Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-acab252bf1c043182a59e0a064847666.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-03-06  2DGS-Avatar Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
