<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  Prompting Generative AI with Interaction-Augmented Instructions">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-55c83592273e96c08993adcd4e1ff6f7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-06-æ›´æ–°"><a href="#2025-03-06-æ›´æ–°" class="headerlink" title="2025-03-06 æ›´æ–°"></a>2025-03-06 æ›´æ–°</h1><h2 id="Prompting-Generative-AI-with-Interaction-Augmented-Instructions"><a href="#Prompting-Generative-AI-with-Interaction-Augmented-Instructions" class="headerlink" title="Prompting Generative AI with Interaction-Augmented Instructions"></a>Prompting Generative AI with Interaction-Augmented Instructions</h2><p><strong>Authors:Leixian Shen, Haotian Li, Yifang Wang, Xing Xie, Huamin Qu</strong></p>
<p>The emergence of generative AI (GenAI) models, including large language models and text-to-image models, has significantly advanced the synergy between humans and AI with not only their outstanding capability but more importantly, the intuitive communication method with text prompts. Though intuitive, text-based instructions suffer from natural languagesâ€™ ambiguous and redundant nature. To address the issue, researchers have explored augmenting text-based instructions with interactions that facilitate precise and effective human intent expression, such as direct manipulation. However, the design strategy of interaction-augmented instructions lacks systematic investigation, hindering our understanding and application. To provide a panorama of interaction-augmented instructions, we propose a framework to analyze related tools from why, when, who, what, and how interactions are applied to augment text-based instructions. Notably, we identify four purposes for applying interactions, including restricting, expanding, organizing, and refining text instructions. The design paradigms for each purpose are also summarized to benefit future researchers and practitioners. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æ¨¡å‹çš„å…´èµ·ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œä¸ä»…ä»¥å…¶å“è¶Šçš„èƒ½åŠ›ï¼Œè€Œä¸”ä»¥å…¶åŸºäºæ–‡æœ¬çš„ç›´è§‚æ²Ÿé€šæ–¹å¼ï¼Œæå¤§åœ°ä¿ƒè¿›äº†äººç±»ä¸äººå·¥æ™ºèƒ½ä¹‹é—´çš„ååŒã€‚å°½ç®¡åŸºäºæ–‡æœ¬çš„æŒ‡ä»¤æ˜¯ç›´è§‚çš„ï¼Œä½†å®ƒä»¬å´å—åˆ°è‡ªç„¶è¯­è¨€æ¨¡ç³Šå’Œå†—ä½™æ€§è´¨çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å·²ç»æ¢ç´¢äº†é€šè¿‡äº’åŠ¨å¢å¼ºæ–‡æœ¬æŒ‡ä»¤çš„æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›äººç±»æ„å›¾çš„ç²¾ç¡®å’Œæœ‰æ•ˆè¡¨è¾¾ï¼Œä¾‹å¦‚ç›´æ¥æ“ä½œã€‚ç„¶è€Œï¼Œäº’åŠ¨å¢å¼ºæŒ‡ä»¤çš„è®¾è®¡ç­–ç•¥ç¼ºä¹ç³»ç»Ÿç ”ç©¶ï¼Œé˜»ç¢äº†æˆ‘ä»¬å¯¹å®ƒçš„ç†è§£å’Œåº”ç”¨ã€‚ä¸ºäº†æä¾›ä¸€ä¸ªäº’åŠ¨å¢å¼ºæŒ‡ä»¤çš„å…¨æ™¯è§†å›¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œä»ä¸ºä»€ä¹ˆã€ä½•æ—¶ã€ä½•äººã€ä½•äº‹å’Œå¦‚ä½•åº”ç”¨äº’åŠ¨æ¥å¢å¼ºæ–‡æœ¬æŒ‡ä»¤çš„è§’åº¦åˆ†æç›¸å…³å·¥å…·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ç¡®å®šäº†å››ç§åº”ç”¨äº’åŠ¨çš„ç›®çš„ï¼ŒåŒ…æ‹¬é™åˆ¶ã€æ‰©å±•ã€ç»„ç»‡å’Œç²¾ç‚¼æ–‡æœ¬æŒ‡ä»¤ã€‚æ¯ç§ç›®çš„çš„è®¾è®¡èŒƒå¼ä¹Ÿè¿›è¡Œäº†æ€»ç»“ï¼Œä»¥é€ ç¦æœªæ¥çš„ç ”ç©¶äººå‘˜å’Œå®è·µè€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02874v1">PDF</a> accepted to CHI LBW 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆGenAIï¼‰çš„å‡ºç°ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬è½¬å›¾åƒæ¨¡å‹ï¼ŒäººæœºååŒå¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚å°½ç®¡åŸºäºæ–‡æœ¬çš„æŒ‡ä»¤ç›´è§‚æ˜“æ‡‚ï¼Œä½†ç”±äºè‡ªç„¶è¯­è¨€å›ºæœ‰çš„æ¨¡ç³Šæ€§å’Œå†—ä½™æ€§ï¼Œå…¶ä½¿ç”¨å—åˆ°æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å°è¯•é€šè¿‡å¢åŠ äº’åŠ¨æ¥å¢å¼ºæ–‡æœ¬æŒ‡ä»¤çš„ç²¾ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå…³äºäº’åŠ¨å¢å¼ºæŒ‡ä»¤çš„è®¾è®¡ç­–ç•¥ç¼ºä¹ç³»ç»Ÿç ”ç©¶ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªæ¡†æ¶ï¼Œä»ä¸ºä½•ã€ä½•æ—¶ã€ä½•äººã€ä½•äº‹å’Œå¦‚ä½•äº’åŠ¨ç­‰è§’åº¦ï¼Œå¯¹ç”¨äºå¢å¼ºæ–‡æœ¬æŒ‡ä»¤çš„äº’åŠ¨å·¥å…·è¿›è¡Œåˆ†æã€‚ç ”ç©¶ç¡®å®šäº†å››ç§äº’åŠ¨åº”ç”¨çš„ç›®çš„ï¼ŒåŒ…æ‹¬é™åˆ¶ã€æ‰©å±•ã€ç»„ç»‡å’Œç²¾ç‚¼æ–‡æœ¬æŒ‡ä»¤ã€‚ä¸ºæœªæ¥çš„ç ”ç©¶è€…å’Œå®è·µè€…æä¾›äº†è®¾è®¡èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆGenAIï¼‰æå‡äº†äººæœºååŒæ•ˆç‡ã€‚</li>
<li>åŸºäºæ–‡æœ¬çš„æŒ‡ä»¤å—é™äºè‡ªç„¶è¯­è¨€çš„æ¨¡ç³Šæ€§å’Œå†—ä½™æ€§ã€‚</li>
<li>ä¸ºæé«˜æŒ‡ä»¤çš„ç²¾ç¡®æ€§å’Œæœ‰æ•ˆæ€§ï¼Œç ”ç©¶äººå‘˜å°è¯•é€šè¿‡å¢åŠ äº’åŠ¨æ¥å¢å¼ºæ–‡æœ¬æŒ‡ä»¤ã€‚</li>
<li>ç¼ºä¹ç³»ç»Ÿç ”ç©¶å…³äºäº’åŠ¨å¢å¼ºæŒ‡ä»¤çš„è®¾è®¡ç­–ç•¥ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ä¸ªæ¡†æ¶åˆ†æäº’åŠ¨å¢å¼ºæŒ‡ä»¤çš„ç›¸å…³å·¥å…·ã€‚</li>
<li>ç¡®å®šäº†å››ç§äº’åŠ¨åº”ç”¨çš„ç›®çš„ï¼šé™åˆ¶ã€æ‰©å±•ã€ç»„ç»‡å’Œç²¾ç‚¼æ–‡æœ¬æŒ‡ä»¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-090c5b2f796dc403ec6e061e4fa8f9ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a99027dec2d5e8aa6145852d9596fff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8fb5bd5c65c75e317fe50ca1061365f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67ea0ecc789d1391ec0cce99f35d597f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FairSense-AI-Responsible-AI-Meets-Sustainability"><a href="#FairSense-AI-Responsible-AI-Meets-Sustainability" class="headerlink" title="FairSense-AI: Responsible AI Meets Sustainability"></a>FairSense-AI: Responsible AI Meets Sustainability</h2><p><strong>Authors:Shaina Raza, Mukund Sayeeganesh Chettiar, Matin Yousefabadi, Tahniat Khan, Marcelo Lotif</strong></p>
<p>In this paper, we introduce FairSense-AI: a multimodal framework designed to detect and mitigate bias in both text and images. By leveraging Large Language Models (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle forms of prejudice or stereotyping that can appear in content, providing users with bias scores, explanatory highlights, and automated recommendations for fairness enhancements. In addition, FairSense-AI integrates an AI risk assessment component that aligns with frameworks like the MIT AI Risk Repository and NIST AI Risk Management Framework, enabling structured identification of ethical and safety concerns. The platform is optimized for energy efficiency via techniques such as model pruning and mixed-precision computation, thereby reducing its environmental footprint. Through a series of case studies and applications, we demonstrate how FairSense-AI promotes responsible AI use by addressing both the social dimension of fairness and the pressing need for sustainability in large-scale AI deployments. <a target="_blank" rel="noopener" href="https://vectorinstitute.github.io/FairSense-AI">https://vectorinstitute.github.io/FairSense-AI</a>, <a target="_blank" rel="noopener" href="https://pypi.org/project/fair-sense-ai/">https://pypi.org/project/fair-sense-ai/</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†FairSense-AIï¼šä¸€ä¸ªæ—¨åœ¨æ£€æµ‹å’Œç¼“è§£æ–‡æœ¬å’Œå›¾åƒä¸­åè§çš„å¤šæ¨¡å¼æ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼ŒFairSense-AIæ­ç¤ºäº†å†…å®¹ä¸­å¯èƒ½å‡ºç°çš„å¾®å¦™å½¢å¼çš„åè§æˆ–åˆ»æ¿å°è±¡ï¼Œä¸ºç”¨æˆ·æä¾›åè§åˆ†æ•°ã€è§£é‡Šäº®ç‚¹å’Œå…¬å¹³æ€§å¢å¼ºçš„è‡ªåŠ¨åŒ–å»ºè®®ã€‚æ­¤å¤–ï¼ŒFairSense-AIè¿˜é›†æˆäº†ä¸€ä¸ªä¸MITäººå·¥æ™ºèƒ½é£é™©ä»“åº“å’ŒNISTäººå·¥æ™ºèƒ½é£é™©ç®¡ç†æ¡†æ¶ç­‰æ¡†æ¶ç›¸å»åˆçš„äººå·¥æ™ºèƒ½é£é™©è¯„ä¼°ç»„ä»¶ï¼Œèƒ½å¤Ÿå®ç°ä¼¦ç†å’Œå®‰å…¨é—®é¢˜çš„ç»“æ„åŒ–è¯†åˆ«ã€‚è¯¥å¹³å°é€šè¿‡æ¨¡å‹ä¿®å‰ªå’Œæ··åˆç²¾åº¦è®¡ç®—ç­‰æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæé«˜èƒ½æ•ˆï¼Œå‡å°‘å…¶ç¯å¢ƒè¶³è¿¹ã€‚é€šè¿‡ä¸€ç³»åˆ—æ¡ˆä¾‹ç ”ç©¶ä¸åº”ç”¨ï¼Œæˆ‘ä»¬å±•ç¤ºäº†FairSense-AIå¦‚ä½•é€šè¿‡è§£å†³å…¬å¹³çš„ç¤¾ä¼šç»´åº¦ä»¥åŠå¤§è§„æ¨¡äººå·¥æ™ºèƒ½éƒ¨ç½²ä¸­å¯¹å¯æŒç»­æ€§çš„è¿«åˆ‡éœ€æ±‚ï¼Œä¿ƒè¿›äººå·¥æ™ºèƒ½çš„è´Ÿè´£ä»»ä½¿ç”¨ã€‚<a target="_blank" rel="noopener" href="https://vectorinstitute.github.io/FairSense-AI%EF%BC%8Chttps://pypi.org/project/fair-sense-ai/%E3%80%82">https://vectorinstitute.github.io/FairSense-AIï¼Œhttps://pypi.org/project/fair-sense-ai/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02865v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†FairSense-AIï¼šä¸€ä¸ªç”¨äºæ£€æµ‹å¹¶å‡å°‘æ–‡æœ¬å’Œå›¾åƒä¸­åè§çš„å¤šæ¨¡æ€æ¡†æ¶ã€‚å®ƒé€šè¿‡è¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ­ç¤ºå†…å®¹ä¸­çš„åè§æˆ–åˆ»æ¿å°è±¡ï¼Œä¸ºç”¨æˆ·æä¾›åè§åˆ†æ•°ã€è§£é‡Šäº®ç‚¹å’Œå…¬å¹³å¢å¼ºçš„è‡ªåŠ¨åŒ–å»ºè®®ã€‚æ­¤å¤–ï¼ŒFairSense-AIèåˆäº†äººå·¥æ™ºèƒ½é£é™©è¯„ä¼°ç»„ä»¶ï¼Œéµå¾ªMITäººå·¥æ™ºèƒ½é£é™©åº“å’ŒNISTäººå·¥æ™ºèƒ½é£é™©ç®¡ç†æ¡†æ¶ç­‰æ¡†æ¶çš„è¦æ±‚ï¼Œä»¥ä¾¿ç³»ç»Ÿåœ°ç¡®å®šé“å¾·å’Œå®‰å…¨æ–¹é¢çš„å…³æ³¨ã€‚è¯¥å¹³å°é€šè¿‡æ¨¡å‹ä¿®å‰ªå’Œæ··åˆç²¾åº¦è®¡ç®—ç­‰æŠ€æœ¯ä¼˜åŒ–èƒ½æºæ•ˆç‡ï¼Œä»è€Œé™ä½å…¶ç¯å¢ƒè¶³è¿¹ã€‚ä¸€ç³»åˆ—æ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼ŒFairSense-AIä¸ä»…é€šè¿‡è§£å†³å…¬å¹³æ€§è¿™ä¸€ç¤¾ä¼šç»´åº¦é—®é¢˜ï¼Œè¿˜èƒ½æ»¡è¶³å¤§è§„æ¨¡äººå·¥æ™ºèƒ½éƒ¨ç½²ä¸­çš„ç´§è¿«å¯æŒç»­æ€§éœ€æ±‚ï¼Œæ¨åŠ¨è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ä½¿ç”¨ã€‚æœ‰å…³è¯¦æƒ…ï¼Œè¯·è®¿é—®ç›¸å…³ç½‘ç«™é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FairSense-AIæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹å¹¶å‡å°‘æ–‡æœ¬å’Œå›¾åƒä¸­çš„åè§ã€‚</li>
<li>å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹æ¥æ­ç¤ºå†…å®¹ä¸­çš„åè§æˆ–åˆ»æ¿å°è±¡ã€‚</li>
<li>FairSense-AIæä¾›åè§åˆ†æ•°ã€è§£é‡Šäº®ç‚¹å’Œå…¬å¹³å¢å¼ºçš„è‡ªåŠ¨åŒ–å»ºè®®ã€‚</li>
<li>è¯¥å¹³å°èåˆäº†äººå·¥æ™ºèƒ½é£é™©è¯„ä¼°ç»„ä»¶ï¼Œéµå¾ªå¤šä¸ªæ¡†æ¶çš„è¦æ±‚è¿›è¡Œé“å¾·å’Œå®‰å…¨è¯„ä¼°ã€‚</li>
<li>FairSense-AIé€šè¿‡ä¼˜åŒ–èƒ½æºæ•ˆç‡é™ä½ç¯å¢ƒè¶³è¿¹ã€‚</li>
<li>é€šè¿‡æ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼ŒFairSense-AIè§£å†³äº†å…¬å¹³æ€§çš„ç¤¾ä¼šç»´åº¦é—®é¢˜ï¼Œå¹¶æ»¡è¶³å¤§è§„æ¨¡äººå·¥æ™ºèƒ½éƒ¨ç½²çš„å¯æŒç»­æ€§éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-489f17b3c73a66366504143601cd9527.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4fde7d38e3d56562ea96eebd0aedeef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60d77cb7541762d9c606bcaa498006c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e5a910f1201eafdd58c65cdaebd2c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-083ad7d5739c287985ec93b537854d50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40fede344f9e79de122916e0372dceee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62761ba41b2bb2fee29a1671e909df53.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Calibrating-LLM-Confidence-with-Semantic-Steering-A-Multi-Prompt-Aggregation-Framework"><a href="#Calibrating-LLM-Confidence-with-Semantic-Steering-A-Multi-Prompt-Aggregation-Framework" class="headerlink" title="Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt   Aggregation Framework"></a>Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt   Aggregation Framework</h2><p><strong>Authors:Ziang Zhou, Tianyuan Jin, Jieming Shi, Qing Li</strong></p>
<p>Large Language Models (LLMs) often exhibit misaligned confidence scores, usually overestimating the reliability of their predictions. While verbalized confidence in Large Language Models (LLMs) has gained attention, prior work remains divided on whether confidence scores can be systematically steered through prompting. Recent studies even argue that such prompt-induced confidence shifts are negligible, suggesting LLMsâ€™ confidence calibration is rigid to linguistic interventions. Contrary to these claims, we first rigorously confirm the existence of directional confidence shifts by probing three models (including GPT3.5, LLAMA3-70b, GPT4) across 7 benchmarks, demonstrating that explicit instructions can inflate or deflate confidence scores in a regulated manner. Based on this observation, we propose a novel framework containing three components: confidence steering, steered confidence aggregation and steered answers selection, named SteeringConf. Our method, SteeringConf, leverages a confidence manipulation mechanism to steer the confidence scores of LLMs in several desired directions, followed by a summarization module that aggregates the steered confidence scores to produce a final prediction. We evaluate our method on 7 benchmarks and it consistently outperforms the baselines in terms of calibration metrics in task of confidence calibration and failure detection. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å¸¸è¡¨ç°å‡ºä¸åŒ¹é…çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼Œé€šå¸¸ä¼šé«˜ä¼°å…¶é¢„æµ‹çš„å¯ä¿¡åº¦ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç½®ä¿¡åº¦è¡¨è¾¾å·²å¼•èµ·å…³æ³¨ï¼Œä½†å…ˆå‰çš„ç ”ç©¶åœ¨æ˜¯å¦å¯ä»¥é€šè¿‡æç¤ºæ¥ç³»ç»Ÿåœ°å¼•å¯¼ç½®ä¿¡åº¦åˆ†æ•°ä¸Šå­˜åœ¨åˆ†æ­§ã€‚ç”šè‡³æœ€è¿‘çš„ç ”ç©¶è®¤ä¸ºï¼Œè¿™ç§æç¤ºå¼•èµ·çš„ç½®ä¿¡åº¦è½¬ç§»å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œè¿™è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ç½®ä¿¡åº¦æ ¡å‡†å¯¹è¯­è¨€å¹²é¢„æ˜¯åˆšæ€§çš„ã€‚ä¸è¿™äº›è¯´æ³•ç›¸åï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡æµ‹è¯•ä¸‰ä¸ªæ¨¡å‹ï¼ˆåŒ…æ‹¬GPT3.5ã€LLAMA3-70bã€GPT4ï¼‰åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ–¹å‘æ€§ç½®ä¿¡è½¬ç§»æ¥ä¸¥æ ¼éªŒè¯å…¶å­˜åœ¨æ€§ï¼Œè¯æ˜æ˜ç¡®çš„æŒ‡ä»¤å¯ä»¥ä»¥å—æ§çš„æ–¹å¼è†¨èƒ€æˆ–ç¼©å‡ç½®ä¿¡åº¦åˆ†æ•°ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŒ…å«ä¸‰ä¸ªç»„ä»¶çš„æ–¹æ³•ï¼šä¿¡å¿ƒå¼•å¯¼ã€å¼•å¯¼åçš„ä¿¡å¿ƒèšåˆå’Œå—å¼•å¯¼çš„ç­”æ¡ˆé€‰æ‹©ï¼Œåä¸ºSteeringConfã€‚æˆ‘ä»¬çš„SteeringConfæ–¹æ³•åˆ©ç”¨ä¿¡å¿ƒæ“çºµæœºåˆ¶æ¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¿¡å¿ƒåˆ†æ•°æœå‡ ä¸ªæœŸæœ›çš„æ–¹å‘å˜åŒ–ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ€»ç»“æ¨¡å—æ¥èšåˆè¿™äº›å¼•å¯¼åçš„ä¿¡å¿ƒåˆ†æ•°ä»¥äº§ç”Ÿæœ€ç»ˆé¢„æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®ƒåœ¨ä¿¡å¿ƒæ ¡å‡†å’Œä»»åŠ¡å¤±è´¥æ£€æµ‹æ–¹é¢çš„æ ¡å‡†æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02863v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å‡ºç°ä¿¡å¿ƒåˆ†æ•°é”™é…é—®é¢˜ï¼Œé€šå¸¸é«˜ä¼°å…¶é¢„æµ‹çš„å¯ä¿¡åº¦ã€‚å°½ç®¡LLMsçš„è‡ªä¿¡è¡¨è¾¾å·²å¼•èµ·å…³æ³¨ï¼Œä½†å…ˆå‰çš„ç ”ç©¶åœ¨æ˜¯å¦å¯ä»¥é€šè¿‡æç¤ºæ¥ç³»ç»Ÿå¼•å¯¼ä¿¡å¿ƒåˆ†æ•°ä¸Šå­˜åœ¨åˆ†æ­§ã€‚æœ€æ–°ç ”ç©¶ç”šè‡³è®¤ä¸ºï¼Œæç¤ºå¼•èµ·çš„ä¿¡å¿ƒå˜åŒ–å¾®ä¹å…¶å¾®ï¼Œæš—ç¤ºLLMsçš„ä¿¡å¿ƒæ ¡å‡†å¯¹è¯­è¨€å¹²é¢„å…·æœ‰åˆšæ€§ã€‚ä¸æ­¤ç›¸åï¼Œæˆ‘ä»¬é¦–æ¬¡åœ¨ä¸‰ä¸ªæ¨¡å‹ï¼ˆåŒ…æ‹¬GPT3.5ã€LLAMA3-70bã€GPT4ï¼‰å’Œä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¸¥æ ¼è¯å®äº†æ–¹å‘æ€§ä¿¡å¿ƒå˜åŒ–çš„å­˜åœ¨ï¼Œè¡¨æ˜æ˜ç¡®çš„æŒ‡ä»¤å¯ä»¥ä»¥å—æ§çš„æ–¹å¼è†¨èƒ€æˆ–æ”¶ç¼©ä¿¡å¿ƒåˆ†æ•°ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªç»„ä»¶çš„æ–°æ¡†æ¶ï¼šä¿¡å¿ƒå¼•å¯¼ã€å¼•å¯¼åçš„ä¿¡å¿ƒèšåˆå’Œé€‰æ‹©ç­”æ¡ˆï¼Œåä¸ºSteeringConfã€‚æˆ‘ä»¬çš„æ–¹æ³•SteeringConfåˆ©ç”¨ä¿¡å¿ƒæ“çºµæœºåˆ¶æ¥å¼•å¯¼LLMsçš„ä¿¡å¿ƒåˆ†æ•°æœå‘å¤šä¸ªæ–¹å‘ï¼Œç„¶åä½¿ç”¨ä¸€ä¸ªæ‘˜è¦æ¨¡å—æ¥èšåˆè¿™äº›å—æ§çš„ä¿¡å¿ƒåˆ†æ•°ä»¥äº§ç”Ÿæœ€ç»ˆé¢„æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®ƒåœ¨ä¿¡å¿ƒæ ¡å‡†å’Œæ•…éšœæ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸å‡ºç°ä¿¡å¿ƒåˆ†æ•°é”™é…é—®é¢˜ï¼Œå³æ¨¡å‹é¢„æµ‹çš„ä¿¡å¿ƒç¨‹åº¦ä¸å®é™…å‡†ç¡®æ€§ä¸åŒ¹é…ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨æ˜¯å¦å¯é€šè¿‡æç¤ºæ¥ç³»ç»Ÿå½±å“LLMsçš„ä¿¡å¿ƒåˆ†æ•°ä¸Šå­˜åœ¨åˆ†æ­§ã€‚</li>
<li>æˆ‘ä»¬ç¡®è®¤äº†é€šè¿‡ç‰¹å®šæç¤ºå¯ä»¥å®šå‘æ”¹å˜LLMsçš„ä¿¡å¿ƒåˆ†æ•°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶SteeringConfï¼ŒåŒ…å«ä¿¡å¿ƒå¼•å¯¼ã€å¼•å¯¼åçš„ä¿¡å¿ƒèšåˆå’Œç­”æ¡ˆé€‰æ‹©ä¸‰ä¸ªç»„ä»¶ã€‚</li>
<li>SteeringConfé€šè¿‡æ“çºµæœºåˆ¶å¼•å¯¼LLMsçš„ä¿¡å¿ƒåˆ†æ•°ï¼Œå¹¶ä½¿ç”¨æ‘˜è¦æ¨¡å—è¿›è¡Œèšåˆä»¥äº§ç”Ÿæ›´å‡†ç¡®é¢„æµ‹ã€‚</li>
<li>åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSteeringConfçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨ä¿¡å¿ƒæ ¡å‡†å’Œæ•…éšœæ£€æµ‹ä»»åŠ¡ä¸Šã€‚</li>
<li>è¿™ä¸€ç ”ç©¶ä¸ºæ”¹å–„LLMsçš„ä¿¡å¿ƒæ ¡å‡†æä¾›äº†ä¸€ä¸ªæ–°çš„ç ”ç©¶æ–¹å‘å’Œå®ç”¨æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-07534458add25f76a7f03addea062c78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55c83592273e96c08993adcd4e1ff6f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cafadf420974f8a80e14a8c922730e8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27422317e991c4cfba939e2888dee512.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Shakespearean-Sparks-The-Dance-of-Hallucination-and-Creativity-in-LLMsâ€™-Decoding-Layers"><a href="#Shakespearean-Sparks-The-Dance-of-Hallucination-and-Creativity-in-LLMsâ€™-Decoding-Layers" class="headerlink" title="Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMsâ€™   Decoding Layers"></a>Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMsâ€™   Decoding Layers</h2><p><strong>Authors:Zicong He, Boxuan Zhang, Lu Cheng</strong></p>
<p>Large language models (LLMs) are known to hallucinate, a phenomenon often linked to creativity. While previous research has primarily explored this connection through theoretical or qualitative lenses, our work takes a quantitative approach to systematically examine the relationship between hallucination and creativity in LLMs. Given the complex nature of creativity, we propose a narrow definition tailored to LLMs and introduce an evaluation framework, HCL, which quantifies Hallucination and Creativity across different Layers of LLMs during decoding. Our empirical analysis reveals a tradeoff between hallucination and creativity that is consistent across layer depth, model type, and model size. Notably, across different model architectures, we identify a specific layer at each model size that optimally balances this tradeoff. Additionally, the optimal layer tends to appear in the early layers of larger models, and the confidence of the model is also significantly higher at this layer. These findings provide a quantitative perspective that offers new insights into the interplay between LLM creativity and hallucination. The code and data for our experiments are available at <a target="_blank" rel="noopener" href="https://github.com/ZicongHe2002/HCL-Spark">https://github.com/ZicongHe2002/HCL-Spark</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šäº§ç”Ÿå¹»è§‰ï¼Œè¿™ä¸€ç°è±¡é€šå¸¸ä¸åˆ›é€ åŠ›æœ‰å…³ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é€šè¿‡ç†è®ºæˆ–å®šæ€§çš„è§’åº¦æ¢ç´¢è¿™ä¸€è”ç³»ï¼Œä½†æˆ‘ä»¬çš„å·¥ä½œé‡‡ç”¨å®šé‡æ–¹æ³•æ¥ç³»ç»Ÿåœ°ç ”ç©¶LLMä¸­å¹»è§‰å’Œåˆ›é€ åŠ›ä¹‹é—´çš„å…³ç³»ã€‚è€ƒè™‘åˆ°åˆ›é€ åŠ›çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬é’ˆå¯¹LLMæå‡ºäº†ä¸€ä¸ªç‹­éš˜çš„å®šä¹‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶HCLï¼Œè¯¥æ¡†æ¶åœ¨LLMè§£ç è¿‡ç¨‹ä¸­é‡åŒ–ä¸åŒå±‚çº§çš„å¹»è§‰å’Œåˆ›é€ åŠ›ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ­ç¤ºäº†å¹»è§‰å’Œåˆ›é€ åŠ›ä¹‹é—´çš„æƒè¡¡ï¼Œè¿™ç§æƒè¡¡åœ¨ä¸åŒå±‚çº§æ·±åº¦ã€æ¨¡å‹ç±»å‹å’Œæ¨¡å‹å¤§å°ä¹‹é—´éƒ½æ˜¯ä¸€è‡´çš„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†æ¯ä¸ªæ¨¡å‹å¤§å°ä¸­ç‰¹å®šå±‚çº§çš„å¹»è§‰å’Œåˆ›é€ åŠ›çš„æƒè¡¡è¾¾åˆ°æœ€ä¼˜å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæœ€ä½³å±‚çº§å¾€å¾€å‡ºç°åœ¨è¾ƒå¤§æ¨¡å‹çš„æ—©æœŸå±‚çº§ä¸­ï¼Œå¹¶ä¸”è¯¥å±‚çº§çš„æ¨¡å‹ç½®ä¿¡åº¦ä¹Ÿæ˜æ˜¾æ›´é«˜ã€‚è¿™äº›å‘ç°ä»å®šé‡è§’åº¦æä¾›äº†æ–°çš„è§è§£ï¼Œæ­ç¤ºäº†LLMåˆ›é€ åŠ›å’Œå¹»è§‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚æˆ‘ä»¬å®éªŒçš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZicongHe2002/HCL-Spark%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZicongHe2002/HCL-Sparkä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02851v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨å¹»è§‰ç°è±¡ï¼Œå¹¶ä¸åˆ›é€ åŠ›æœ‰å…³ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦ä»ç†è®ºæˆ–å®šæ€§è§’åº¦æ¢è®¨è¿™ç§è”ç³»ï¼Œè€Œæˆ‘ä»¬çš„å·¥ä½œé‡‡ç”¨å®šé‡æ–¹æ³•ç³»ç»Ÿåœ°ç ”ç©¶LLMä¸­å¹»è§‰ä¸åˆ›é€ åŠ›çš„å…³ç³»ã€‚é’ˆå¯¹åˆ›é€ åŠ›çš„å¤æ‚æ€§è´¨ï¼Œæˆ‘ä»¬ä¸ºLLMæå‡ºäº†ä¸€ä¸ªç‹­çª„çš„å®šä¹‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶HCLï¼Œè¯¥æ¡†æ¶åœ¨LLMè§£ç è¿‡ç¨‹ä¸­é‡åŒ–ä¸åŒå±‚çº§çš„å¹»è§‰å’Œåˆ›é€ åŠ›ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ­ç¤ºäº†å¹»è§‰å’Œåˆ›é€ åŠ›ä¹‹é—´çš„æƒè¡¡ï¼Œè¿™ç§æƒè¡¡åœ¨ä¸åŒå±‚æ·±åº¦ã€æ¨¡å‹ç±»å‹å’Œæ¨¡å‹å¤§å°ä¹‹é—´æ˜¯ä¸€è‡´çš„ã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ä¸­ï¼Œæˆ‘ä»¬å‘ç°æ¯ä¸ªæ¨¡å‹å¤§å°éƒ½æœ‰ä¸€ä¸ªç‰¹å®šå±‚çº§æœ€ä¼˜åœ°å¹³è¡¡äº†è¿™ç§æƒè¡¡ã€‚æ­¤å¤–ï¼Œåœ¨è¾ƒå¤§çš„æ¨¡å‹ä¸­ï¼Œæœ€ä½³å±‚çº§å¾€å¾€å‡ºç°åœ¨æ—©æœŸå±‚çº§ï¼Œå¹¶ä¸”æ¨¡å‹åœ¨æ­¤å±‚çº§çš„ç½®ä¿¡åº¦ä¹Ÿæ˜¾è‘—æé«˜ã€‚è¿™äº›å‘ç°ä»å®šé‡è§’åº¦æä¾›äº†æ–°çš„è§†è§’ï¼Œæ­ç¤ºäº†LLMåˆ›é€ åŠ›å’Œå¹»è§‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£ç è¿‡ç¨‹ä¸­ä¼šè¡¨ç°å‡ºå¹»è§‰ç°è±¡ï¼Œè¿™ä¸åˆ›é€ åŠ›æœ‰å…³ã€‚</li>
<li>é¦–æ¬¡é‡‡ç”¨å®šé‡æ–¹æ³•ç³»ç»Ÿåœ°ç ”ç©¶LLMä¸­å¹»è§‰ä¸åˆ›é€ åŠ›çš„å…³ç³»ã€‚</li>
<li>é’ˆå¯¹LLMæå‡ºäº†ä¸€ä¸ªç‹­çª„çš„å®šä¹‰åˆ›é€ åŠ›çš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥HCLè¯„ä¼°æ¡†æ¶é‡åŒ–å¹»è§‰å’Œåˆ›é€ åŠ›ã€‚</li>
<li>å®è¯åˆ†ææ˜¾ç¤ºå¹»è§‰å’Œåˆ›é€ åŠ›ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œè¿™ç§æƒè¡¡åœ¨ä¸åŒæ¨¡å‹ä¹‹é—´æ˜¯ä¸€è‡´çš„ã€‚</li>
<li>ä¸åŒæ¨¡å‹æ¶æ„ä¸­å­˜åœ¨ä¸€ä¸ªæœ€ä¼˜å±‚çº§ï¼Œèƒ½å¤Ÿå¹³è¡¡å¹»è§‰å’Œåˆ›é€ åŠ›çš„æƒè¡¡ã€‚</li>
<li>åœ¨è¾ƒå¤§çš„æ¨¡å‹ä¸­ï¼Œæœ€ä½³å±‚çº§å¾€å¾€å‡ºç°åœ¨æ—©æœŸå±‚çº§ï¼Œä¸”æ¨¡å‹åœ¨æ­¤å±‚çº§çš„ç½®ä¿¡åº¦è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a77acdc050e6effddd093c84ab7b2c2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c110c1fe1092f93a69956b7907633622.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a6a1ca3305a74bc760753ce6deb34dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8f8815a9f205a52193c7e205215342f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Mask-DPO-Generalizable-Fine-grained-Factuality-Alignment-of-LLMs"><a href="#Mask-DPO-Generalizable-Fine-grained-Factuality-Alignment-of-LLMs" class="headerlink" title="Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs"></a>Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs</h2><p><strong>Authors:Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen</strong></p>
<p>Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preference learning inevitably introduced noises during training. Therefore, this paper proposes a fine-grained factuality alignment method based on Direct Preference Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as mask signals, Mask-DPO only learns from factually correct sentences in the preferred samples and prevents the penalty on factual contents in the not preferred samples, which resolves the ambiguity in the preference learning. Extensive experimental results demonstrate that Mask-DPO can significantly improve the factuality of LLMs responses to questions from both in-domain and out-of-domain datasets, although these questions and their corresponding topics are unseen during training. Only trained on the ANAH train set, the score of Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%, even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its FactScore on the out-of-domain Biography dataset is also improved from 30.29% to 39.39%. We further study the generalization property of Mask-DPO using different training sample scaling strategies and find that scaling the number of topics in the dataset is more effective than the number of questions. We provide a hypothesis of what factual alignment is doing with LLMs, on the implication of this phenomenon, and conduct proof-of-concept experiments to verify it. We hope the method and the findings pave the way for future research on scaling factuality alignment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½œä¸ºä¸åŒé¢†åŸŸçš„AIåŠ©æ‰‹æ—¶ï¼Œä¼šå‡ºç°å¹»æƒ³ï¼ˆå³ä¸çœŸå®æˆ–æ— æ„ä¹‰çš„ä¿¡æ¯ï¼‰ã€‚ç”±äºLLMçš„å›åº”ä¸­å¹»æƒ³æ€»æ˜¯ä¼´éšç€çœŸå®å†…å®¹ï¼Œå› æ­¤ä¹‹å‰çš„åŸºäºå“åº”çº§åˆ«çš„çœŸå®æ€§å¯¹é½æ–¹æ³•ä¸å¯é¿å…åœ°ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥å™ªå£°ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„ç²¾ç»†çœŸå®æ€§å¯¹é½æ–¹æ³•ï¼Œç§°ä¸ºMask-DPOã€‚Mask-DPOå°†å¥å­çº§åˆ«çœŸå®æ€§ä½œä¸ºæ©ç ä¿¡å·ï¼Œä»…ä»é¦–é€‰æ ·æœ¬ä¸­çš„çœŸå®å¥å­ä¸­å­¦ä¹ ï¼Œå¹¶é˜²æ­¢å¯¹éé¦–é€‰æ ·æœ¬ä¸­çš„çœŸå®å†…å®¹æ–½åŠ æƒ©ç½šï¼Œè¿™è§£å†³äº†åå¥½å­¦ä¹ ä¸­çš„æ¨¡ç³Šæ€§ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMask-DPOå¯ä»¥æ˜¾è‘—æé«˜LLMå¯¹æ¥è‡ªåŸŸå†…å¤–æ•°æ®é›†çš„é—®é¢˜çš„å›åº”çš„çœŸå®æ€§ã€‚ä»…å¯¹ANAHè®­ç»ƒé›†è¿›è¡Œè®­ç»ƒï¼Œåœ¨ANAHæµ‹è¯•é›†ä¸Šï¼ŒLlama3.1-8B-Instructçš„å¾—åˆ†ä»49.19%æé«˜åˆ°77.53%ï¼Œç”šè‡³è¶…è¿‡äº†Llama3.1-70B-Instructçš„å¾—åˆ†ï¼ˆ53.44%ï¼‰ï¼Œè€Œå…¶åœ¨å¤–åŸŸä¼ è®°æ•°æ®é›†ä¸Šçš„FactScoreä¹Ÿä»30.29%æé«˜åˆ°39.39%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶äº†ä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ ·æœ¬ç¼©æ”¾ç­–ç•¥ä¸‹Mask-DPOçš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶å‘ç°å¢åŠ æ•°æ®é›†çš„è¯é¢˜æ•°é‡æ¯”å¢åŠ é—®é¢˜æ•°é‡æ›´æœ‰æ•ˆã€‚æˆ‘ä»¬å¯¹LLMçš„äº‹å®å¯¹é½æ–¹å¼æå‡ºäº†å‡è®¾ï¼Œæ¢è®¨äº†è¿™ä¸€ç°è±¡çš„å«ä¹‰ï¼Œå¹¶é€šè¿‡æ¦‚å¿µéªŒè¯å®éªŒè¿›è¡Œäº†éªŒè¯ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç§æ–¹æ³•å’Œç ”ç©¶ç»“æœèƒ½ä¸ºæœªæ¥å…³äºçœŸå®æ€§å¯¹é½çš„ç ”ç©¶é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02846v1">PDF</a> Accepted by ICLR 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/open-compass/ANAH">https://github.com/open-compass/ANAH</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºäººå·¥æ™ºèƒ½åŠ©æ‰‹æ—¶ä¼šå‡ºç°å¹»è§‰ï¼ˆå³ä¸çœŸå®æˆ–æ— æ„ä¹‰çš„ä¿¡æ¯ï¼‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„ç²¾ç»†äº‹å®æ€§å¯¹é½æ–¹æ³•ï¼Œç§°ä¸ºMask-DPOã€‚å®ƒé€šè¿‡åˆ©ç”¨å¥å­çº§åˆ«çš„äº‹å®æ€§ä½œä¸ºæ©è†œä¿¡å·ï¼Œä»…ä»é¦–é€‰æ ·æœ¬ä¸­çš„äº‹å®æ­£ç¡®çš„å¥å­ä¸­å­¦ä¹ ï¼Œå¹¶é˜²æ­¢å¯¹éé¦–é€‰æ ·æœ¬ä¸­çš„äº‹å®å†…å®¹çš„æƒ©ç½šï¼Œè§£å†³äº†åå¥½å­¦ä¹ ä¸­çš„æ¨¡ç³Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMask-DPOå¯ä»¥æ˜¾è‘—æé«˜LLMå¯¹åº”åˆ°åŸŸå’Œè·¨åŸŸæ•°æ®é›†çš„å“åº”äº‹å®æ€§ã€‚ä»…å¯¹ANAHè®­ç»ƒé›†è¿›è¡Œè®­ç»ƒï¼ŒLlama3.1-8B-Instructåœ¨ANAHæµ‹è¯•é›†ä¸Šçš„å¾—åˆ†ä»49.19%æé«˜åˆ°77.53%ï¼Œç”šè‡³è¶…è¿‡äº†Llama3.1-70B-Instructçš„å¾—åˆ†ï¼ˆ53.44%ï¼‰ï¼Œå…¶åœ¨è·¨åŸŸä¼ è®°æ•°æ®é›†ä¸Šçš„FactScoreä¹Ÿä»30.29%æé«˜åˆ°39.39%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½œä¸ºAIåŠ©æ‰‹æ—¶å¯èƒ½å‡ºç°å¹»è§‰ï¼Œå³ç”Ÿæˆä¸çœŸå®æˆ–æ— æ„ä¹‰çš„ä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„Mask-DPOæ–¹æ³•ï¼Œç”¨äºç²¾ç»†çš„äº‹å®æ€§å¯¹é½ã€‚</li>
<li>Mask-DPOé€šè¿‡å¥å­çº§åˆ«çš„äº‹å®æ€§ä½œä¸ºæ©è†œä¿¡å·ï¼Œä»…ä»é¦–é€‰æ ·æœ¬ä¸­çš„äº‹å®æ­£ç¡®å¥å­ä¸­å­¦ä¹ ã€‚</li>
<li>Mask-DPOèƒ½æé«˜LLMåœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†ä¸Šçš„å›åº”äº‹å®æ€§ã€‚</li>
<li>ä»…å¯¹ANAHè®­ç»ƒé›†è¿›è¡Œè®­ç»ƒçš„Llamaæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
<li>é€šè¿‡ä¸åŒçš„è®­ç»ƒæ ·æœ¬ç¼©æ”¾ç­–ç•¥ç ”ç©¶äº†Mask-DPOçš„æ³›åŒ–å±æ€§ï¼Œå‘ç°å¢åŠ æ•°æ®é›†çš„è¯é¢˜æ•°é‡æ¯”å¢åŠ é—®é¢˜æ•°é‡æ›´æœ‰æ•ˆã€‚</li>
<li>æä¾›äº†å…³äºLLMå¦‚ä½•è¿›è¡Œäº‹å®å¯¹é½çš„å‡è®¾ï¼Œå¹¶è¿›è¡Œäº†æ¦‚å¿µéªŒè¯å®éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39509b52b256a6b02a71f4cac89b8b74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b22ad8a91506703819560613357c1fb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba3d1c8c6750ce476f70b22dc4b400a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea8e1fbe650a2ca7a48c711c349c9b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1d734f7980d85143643306cde74b524.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AlignDistil-Token-Level-Language-Model-Alignment-as-Adaptive-Policy-Distillation"><a href="#AlignDistil-Token-Level-Language-Model-Alignment-as-Adaptive-Policy-Distillation" class="headerlink" title="AlignDistil: Token-Level Language Model Alignment as Adaptive Policy   Distillation"></a>AlignDistil: Token-Level Language Model Alignment as Adaptive Policy   Distillation</h2><p><strong>Authors:Songming Zhang, Xue Zhang, Tong Zhang, Bojie Hu, Yufeng Chen, Jinan Xu</strong></p>
<p>In modern large language models (LLMs), LLM alignment is of crucial importance and is typically achieved through methods such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO). However, in most existing methods for LLM alignment, all tokens in the response are optimized using a sparse, response-level reward or preference annotation. The ignorance of token-level rewards may erroneously punish high-quality tokens or encourage low-quality tokens, resulting in suboptimal performance and slow convergence speed. To address this issue, we propose AlignDistil, an RLHF-equivalent distillation method for token-level reward optimization. Specifically, we introduce the reward learned by DPO into the RLHF objective and theoretically prove the equivalence between this objective and a token-level distillation process, where the teacher distribution linearly combines the logits from the DPO model and a reference model. On this basis, we further bridge the accuracy gap between the reward from the DPO model and the pure reward model, by building a contrastive DPO reward with a normal and a reverse DPO model. Moreover, to avoid under- and over-optimization on different tokens, we design a token adaptive logit extrapolation mechanism to construct an appropriate teacher distribution for each token. Experimental results demonstrate the superiority of our AlignDistil over existing methods and showcase fast convergence due to its token-level distributional reward optimization. </p>
<blockquote>
<p>åœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼ŒLLMå¯¹é½è‡³å…³é‡è¦ï¼Œé€šå¸¸é€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰æ–¹æ³•å®ç°ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°ç°æœ‰çš„LLMå¯¹é½æ–¹æ³•ä¸­ï¼Œä½¿ç”¨ç¨€ç–çš„å“åº”çº§å¥–åŠ±æˆ–åå¥½æ³¨é‡Šæ¥ä¼˜åŒ–å“åº”ä¸­çš„æ‰€æœ‰ä»¤ç‰Œã€‚å¿½ç•¥ä»¤ç‰Œçº§å¥–åŠ±å¯èƒ½ä¼šé”™è¯¯åœ°æƒ©ç½šé«˜è´¨é‡ä»¤ç‰Œæˆ–é¼“åŠ±ä½è´¨é‡ä»¤ç‰Œï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³å’Œæ”¶æ•›é€Ÿåº¦æ…¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AlignDistilï¼Œè¿™æ˜¯ä¸€ç§ä¸RLHFç›¸å½“çš„ä»¤ç‰Œçº§å¥–åŠ±ä¼˜åŒ–è’¸é¦æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†DPOå­¦åˆ°çš„å¥–åŠ±å¼•å…¥RLHFç›®æ ‡ï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†è¯¥ç›®æ ‡ä¸ä»¤ç‰Œçº§è’¸é¦è¿‡ç¨‹çš„ç­‰ä»·æ€§ï¼Œå…¶ä¸­æ•™å¸ˆåˆ†å¸ƒçº¿æ€§ç»„åˆäº†DPOæ¨¡å‹å’Œå‚è€ƒæ¨¡å‹çš„æ—¥å¿—ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡æ„å»ºæ­£å¸¸çš„DPOå¥–åŠ±å’Œåå‘DPOå¥–åŠ±çš„å¯¹æ¯”ï¼Œè¿›ä¸€æ­¥ç¼©å°äº†DPOæ¨¡å‹å¥–åŠ±å’Œçº¯å¥–åŠ±æ¨¡å‹ä¹‹é—´çš„å‡†ç¡®æ€§å·®è·ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…ä¸åŒä»¤ç‰Œä¸Šçš„æ¬ ä¼˜åŒ–å’Œè¿‡åº¦ä¼˜åŒ–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä»¤ç‰Œè‡ªé€‚åº”æ—¥å¿—æ‰©å±•æœºåˆ¶ï¼Œä¸ºæ¯ä¸ªä»¤ç‰Œæ„å»ºé€‚å½“çš„æ•™å¸ˆåˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AlignDistilä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ç”±äºå…¶ä»¤ç‰Œçº§åˆ†å¸ƒå¥–åŠ±ä¼˜åŒ–è€Œå®ç°çš„å¿«é€Ÿæ”¶æ•›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02832v1">PDF</a> 15 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„LLMå¯¹é½é—®é¢˜ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½ä½¿ç”¨ç¨€ç–çš„å“åº”çº§å¥–åŠ±æˆ–åå¥½æ³¨é‡Šæ¥ä¼˜åŒ–å“åº”ä¸­çš„æ‰€æœ‰ä»¤ç‰Œï¼Œè¿™å¯èƒ½å¯¼è‡´é«˜è´¨é‡çš„ä»¤ç‰Œè¢«é”™è¯¯åœ°æƒ©ç½šæˆ–ä½è´¨é‡çš„ä»¤ç‰Œè¢«é¼“åŠ±ï¼Œä»è€Œæ€§èƒ½ä¸ä½³å’Œæ”¶æ•›é€Ÿåº¦æ…¢ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AlignDistilæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä»¤ç‰Œçº§å¥–åŠ±ä¼˜åŒ–çš„RLHFç­‰æ•ˆè’¸é¦æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†DPOå­¦åˆ°çš„å¥–åŠ±å¼•å…¥RLHFç›®æ ‡ä¸­ï¼Œå¹¶é€šè¿‡ç†è®ºè¯æ˜è¯¥ç›®æ ‡ä¸ä»¤ç‰Œçº§è’¸é¦è¿‡ç¨‹çš„ç­‰ä»·æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œé€šè¿‡æ„å»ºæ­£å¸¸çš„DPOæ¨¡å‹å’Œåå‘DPOæ¨¡å‹ä¹‹é—´çš„å¯¹æ¯”å¥–åŠ±ï¼Œç¼©å°äº†DPOæ¨¡å‹å¥–åŠ±ä¸çº¯å¥–åŠ±æ¨¡å‹ä¹‹é—´çš„ç²¾åº¦å·®è·ã€‚ä¸ºé¿å…å¯¹ä¸åŒä»¤ç‰Œçš„è¿‡åº¦å’Œä¸è¶³ä¼˜åŒ–ï¼Œè®¾è®¡äº†ä¸€ç§ä»¤ç‰Œè‡ªé€‚åº”å¯¹æ•°æ‰©å±•æœºåˆ¶ï¼Œä¸ºæ¯ä¸ªä»¤ç‰Œæ„å»ºé€‚å½“çš„æ•™å¸ˆåˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlignDistilä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ç”±äºå…¶ä»¤ç‰Œçº§åˆ†å¸ƒå¥–åŠ±ä¼˜åŒ–è€Œå…·æœ‰çš„å¿«é€Ÿæ”¶æ•›æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¯¹é½åœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è‡³å…³é‡è¦ï¼Œé€šå¸¸é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç­‰æ–¹æ³•å®ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šä½¿ç”¨ç¨€ç–çš„å“åº”çº§å¥–åŠ±æˆ–åå¥½æ³¨é‡Šæ¥ä¼˜åŒ–æ‰€æœ‰ä»¤ç‰Œï¼Œè¿™å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½å’Œæ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>AlignDistilæ˜¯ä¸€ç§æ–°çš„RLHFç­‰æ•ˆè’¸é¦æ–¹æ³•ï¼Œç”¨äºä»¤ç‰Œçº§å¥–åŠ±ä¼˜åŒ–ï¼Œç»“åˆäº†DPOçš„å¥–åŠ±å’ŒRLHFç›®æ ‡ã€‚</li>
<li>AlignDistilé€šè¿‡å¼•å…¥å¯¹æ¯”DPOå¥–åŠ±æœºåˆ¶æ¥ç¼©å°ä¸çº¯å¥–åŠ±æ¨¡å‹çš„ç²¾åº¦å·®è·ã€‚</li>
<li>AlignDistilè®¾è®¡äº†ä¸€ç§ä»¤ç‰Œè‡ªé€‚åº”å¯¹æ•°æ‰©å±•æœºåˆ¶ï¼Œä¸ºæ¯ä¸ªä»¤ç‰Œæ„å»ºé€‚å½“çš„æ•™å¸ˆåˆ†å¸ƒï¼Œé¿å…è¿‡åº¦å’Œä¸è¶³çš„ä¼˜åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜AlignDistilä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6844bf536df2202eca73d9cdc0be960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34663ef04c39e656a2ac47c6e89514ea.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RAAD-LLM-Adaptive-Anomaly-Detection-Using-LLMs-and-RAG-Integration"><a href="#RAAD-LLM-Adaptive-Anomaly-Detection-Using-LLMs-and-RAG-Integration" class="headerlink" title="RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration"></a>RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration</h2><p><strong>Authors:Alicia Russell-Gilbert, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jabour, Thomas Arnold, Joshua Church</strong></p>
<p>Anomaly detection in complex industrial environments poses unique challenges, particularly in contexts characterized by data sparsity and evolving operational conditions. Predictive maintenance (PdM) in such settings demands methodologies that are adaptive, transferable, and capable of integrating domain-specific knowledge. In this paper, we present RAAD-LLM, a novel framework for adaptive anomaly detection, leveraging large language models (LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach addresses the aforementioned PdM challenges. By effectively utilizing domain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time series data without requiring fine-tuning on specific datasets. The frameworkâ€™s adaptability mechanism enables it to adjust its understanding of normal operating conditions dynamically, thus increasing detection accuracy. We validate this methodology through a real-world application for a plastics manufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show significant improvements over our previous model with an accuracy increase from 70.7 to 89.1 on the real-world dataset. By allowing for the enriching of input series data with semantics, RAAD-LLM incorporates multimodal capabilities that facilitate more collaborative decision-making between the model and plant operators. Overall, our findings support RAAD-LLMâ€™s ability to revolutionize anomaly detection methodologies in PdM, potentially leading to a paradigm shift in how anomaly detection is implemented across various industries. </p>
<blockquote>
<p>åœ¨å¤æ‚çš„å·¥ä¸šç¯å¢ƒä¸­è¿›è¡Œå¼‚å¸¸æ£€æµ‹é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç–å’Œè¿è¥æ¡ä»¶ä¸æ–­å˜åŒ–çš„ç¯å¢ƒä¸­æ›´æ˜¯å¦‚æ­¤ã€‚åœ¨è¿™æ ·çš„ç¯å¢ƒä¸­ï¼Œé¢„æµ‹æ€§ç»´æŠ¤ï¼ˆPdMï¼‰éœ€è¦å…·æœ‰é€‚åº”æ€§ã€å¯è¿ç§»æ€§å¹¶èƒ½æ•´åˆç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„æ–¹æ³•è®ºã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RAAD-LLMï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆçš„è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¸Šè¿°PdMæŒ‘æˆ˜ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼ŒRAAD-LLMå¢å¼ºäº†æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ï¼Œè€Œæ— éœ€åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥æ¡†æ¶çš„é€‚åº”æ€§æœºåˆ¶ä½¿å…¶èƒ½å¤ŸåŠ¨æ€è°ƒæ•´å¯¹æ­£å¸¸æ“ä½œæ¡ä»¶çš„ç†è§£ï¼Œä»è€Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¡‘æ–™åˆ¶é€ å‚çš„å®é™…åº”ç”¨å’ŒSkoltechå¼‚å¸¸åŸºå‡†æµ‹è¯•ï¼ˆSKABï¼‰éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œä¸æˆ‘ä»¬çš„å‰æœŸæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä»70.7%æé«˜åˆ°äº†89.1%ã€‚é€šè¿‡å…è®¸ç”¨è¯­ä¹‰ä¿¡æ¯ä¸°å¯Œè¾“å…¥åºåˆ—æ•°æ®ï¼ŒRAAD-LLMç»“åˆäº†å¤šæ¨¡å¼åŠŸèƒ½ï¼Œä¿ƒè¿›äº†æ¨¡å‹å’Œå·¥å‚æ“ä½œå‘˜ä¹‹é—´çš„æ›´åä½œå†³ç­–ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ”¯æŒäº†RAAD-LLMåœ¨PdMä¸­é©æ–°å¼‚å¸¸æ£€æµ‹æ–¹æ³•è®ºçš„èƒ½åŠ›ï¼Œæœ‰å¯èƒ½å¯¼è‡´å„è¡Œä¸šå¼‚å¸¸æ£€æµ‹å®æ–½æ–¹å¼çš„èŒƒå¼è½¬å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02800v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2411.00914</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„RAADæ¡†æ¶èƒ½æœ‰æ•ˆåº”å¯¹å¤æ‚å·¥ä¸šç¯å¢ƒä¸­çš„å¼‚å¸¸æ£€æµ‹æŒ‘æˆ˜ã€‚å®ƒé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿé€‚åº”æ•°æ®ç¨€ç–å’Œè¿è¥æ¡ä»¶å¤šå˜çš„æƒ…å¢ƒã€‚é€šè¿‡åˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒRAADæ¡†æ¶æé«˜äº†æ—¶é—´åºåˆ—æ•°æ®çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚å…¶è‡ªé€‚åº”æœºåˆ¶èƒ½å¤ŸåŠ¨æ€è°ƒæ•´å¯¹æ­£å¸¸æ“ä½œæ¡ä»¶çš„ç†è§£ï¼Œä»è€Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚åœ¨å¡‘æ–™åˆ¶é€ å·¥å‚çš„å®é™…åº”ç”¨å’ŒSkoltechå¼‚å¸¸åŸºå‡†æµ‹è¯•ï¼ˆSKABï¼‰ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œå‡†ç¡®ç‡ä»ä¹‹å‰çš„70.7%æé«˜åˆ°89.1%ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶è¿˜ç»“åˆäº†å¤šæ¨¡æ€åŠŸèƒ½ï¼Œå…è®¸è¾“å…¥åºåˆ—æ•°æ®çš„è¯­ä¹‰å¢å¼ºï¼Œä¿ƒè¿›æ¨¡å‹ä¸å·¥å‚æ“ä½œäººå‘˜ä¹‹é—´çš„æ›´åä½œå†³ç­–ã€‚æ€»ä½“è€Œè¨€ï¼ŒRAADæ¡†æ¶æœ‰æœ›å½»åº•æ”¹å˜é¢„æµ‹ç»´æŠ¤ï¼ˆPdMï¼‰ä¸­çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œä¸ºå„è¡Œä¸šå¸¦æ¥èŒƒå¼è½¬å˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAAD-LLMæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œä¸ºå¤æ‚å·¥ä¸šç¯å¢ƒä¸­çš„å¼‚å¸¸æ£€æµ‹æä¾›äº†æ–°é¢–è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿé€‚åº”æ•°æ®ç¨€ç–å’Œè¿è¥æ¡ä»¶å¤šå˜çš„æŒ‘æˆ˜ã€‚</li>
<li>RAAD-LLMé€šè¿‡åˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œæé«˜äº†æ—¶é—´åºåˆ—æ•°æ®çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶å…·æœ‰è‡ªé€‚åº”æœºåˆ¶ï¼Œèƒ½åŠ¨æ€è°ƒæ•´å¯¹æ­£å¸¸æ“ä½œæ¡ä»¶çš„ç†è§£ï¼Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å®é™…å¡‘æ–™åˆ¶é€ å·¥å‚å’ŒSkoltechå¼‚å¸¸åŸºå‡†æµ‹è¯•ä¸­çš„ç»“æœè¡¨æ˜ï¼ŒRAAD-LLMçš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>RAAD-LLMæ¡†æ¶ç»“åˆäº†å¤šæ¨¡æ€åŠŸèƒ½ï¼Œä¿ƒè¿›æ¨¡å‹ä¸å·¥å‚æ“ä½œäººå‘˜ä¹‹é—´çš„åä½œå†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ade37635a2a0b992a410f91cd1632a71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a10eba53167cf88dc695912d62a98e3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8df499ffca869658fc8861b56ff84458.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multimodal-AI-predicts-clinical-outcomes-of-drug-combinations-from-preclinical-data"><a href="#Multimodal-AI-predicts-clinical-outcomes-of-drug-combinations-from-preclinical-data" class="headerlink" title="Multimodal AI predicts clinical outcomes of drug combinations from   preclinical data"></a>Multimodal AI predicts clinical outcomes of drug combinations from   preclinical data</h2><p><strong>Authors:Yepeng Huang, Xiaorui Su, Varun Ullanat, Ivy Liang, Lindsay Clegg, Damilola Olabode, Nicholas Ho, Bino John, Megan Gibbs, Marinka Zitnik</strong></p>
<p>Predicting clinical outcomes from preclinical data is essential for identifying safe and effective drug combinations. Current models rely on structural or target-based features to identify high-efficacy, low-toxicity drug combinations. However, these approaches fail to incorporate the multimodal data necessary for accurate, clinically-relevant predictions. Here, we introduce MADRIGAL, a multimodal AI model that learns from structural, pathway, cell viability, and transcriptomic data to predict drug combination effects across 953 clinical outcomes and 21842 compounds, including combinations of approved drugs and novel compounds in development. MADRIGAL uses a transformer bottleneck module to unify preclinical drug data modalities while handling missing data during training and inferenceâ€“a major challenge in multimodal learning. It outperforms single-modality methods and state-of-the-art models in predicting adverse drug interactions. MADRIGAL performs virtual screening of anticancer drug combinations and supports polypharmacy management for type II diabetes and metabolic dysfunction-associated steatohepatitis (MASH). It identifies transporter-mediated drug interactions. MADRIGAL predicts resmetirom, the first and only FDA-approved drug for MASH, among therapies with the most favorable safety profile. It supports personalized cancer therapy by integrating genomic profiles from cancer patients. Using primary acute myeloid leukemia samples and patient-derived xenograft models, it predicts the efficacy of personalized drug combinations. Integrating MADRIGAL with a large language model allows users to describe clinical outcomes in natural language, improving safety assessment by identifying potential adverse interactions and toxicity risks. MADRIGAL provides a multimodal approach for designing combination therapies with improved predictive accuracy and clinical relevance. </p>
<blockquote>
<p>ä»ä¸´åºŠå‰æ•°æ®é¢„æµ‹ä¸´åºŠç»“æœæ˜¯è¯†åˆ«å®‰å…¨å’Œæœ‰æ•ˆè¯ç‰©ç»„åˆçš„å…³é”®ã€‚å½“å‰æ¨¡å‹ä¾èµ–äºç»“æ„æˆ–ç›®æ ‡åŸºç¡€ç‰¹å¾æ¥è¯†åˆ«é«˜æ•ˆã€ä½æ¯’çš„è¯ç‰©ç»„åˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æœªèƒ½èå…¥å¯¹å‡†ç¡®ä¸”ä¸´åºŠç›¸å…³çš„é¢„æµ‹å¿…è¦çš„å¤šæ¨¡å¼æ•°æ®ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†MADRIGALï¼Œä¸€ä¸ªå¤šæ¨¡å¼äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œèƒ½å¤Ÿä»ç»“æ„ã€é€”å¾„ã€ç»†èƒæ´»æ€§å’Œè½¬å½•ç»„æ•°æ®å­¦ä¹ ï¼Œä»¥é¢„æµ‹æ¶‰åŠ953ç§ä¸´åºŠç»“æœå’Œ21842ç§åŒ–åˆç‰©çš„è¯ç‰©ç»„åˆæ•ˆæœï¼ŒåŒ…æ‹¬è·æ‰¹è¯ç‰©å’Œæ­£åœ¨å¼€å‘çš„æ–°åŒ–åˆç‰©çš„ç»„åˆã€‚MADRIGALé‡‡ç”¨å˜å‹å™¨ç“¶é¢ˆæ¨¡å—æ¥ç»Ÿä¸€ä¸´åºŠå‰è¯ç‰©æ•°æ®æ¨¡å¼ï¼Œå¹¶åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¤„ç†ç¼ºå¤±æ•°æ®â€”â€”è¿™æ˜¯å¤šæ¨¡å¼å­¦ä¹ ä¸­çš„ä¸»è¦æŒ‘æˆ˜ã€‚å®ƒåœ¨é¢„æµ‹è¯ç‰©ç›¸äº’ä½œç”¨çš„ä¸è‰¯ååº”æ–¹é¢ä¼˜äºå•æ¨¡å¼æ–¹æ³•å’Œæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚MADRIGALå¯¹æŠ—ç™Œè¯ç‰©ç»„åˆè¿›è¡Œè™šæ‹Ÿç­›é€‰ï¼Œå¹¶æ”¯æŒIIå‹ç³–å°¿ç—…å’Œä»£è°¢åŠŸèƒ½éšœç¢ç›¸å…³è„‚è‚ªæ€§è‚ç‚ï¼ˆMASHï¼‰çš„å¤šè¯æ²»ç–—ã€‚å®ƒç¡®å®šäº†è½¬è¿ä½“ä»‹å¯¼çš„è¯ç‰©ç›¸äº’ä½œç”¨ã€‚MADRIGALé¢„æµ‹é›·å¸ç¾ç‰¹ç½—æ˜¯MASHé¦–ä¸ªä¸”å”¯ä¸€ä¸€ç§FDAæ‰¹å‡†çš„è¯ç‰©ï¼Œåœ¨æ²»ç–—ä¸­å…·æœ‰æœ€å®‰å…¨çš„æ•ˆæœã€‚å®ƒé€šè¿‡æ•´åˆç™Œç—‡æ‚£è€…çš„åŸºå› ç»„å›¾è°±æ”¯æŒä¸ªæ€§åŒ–ç™Œç—‡æ²»ç–—ã€‚ä½¿ç”¨åŸå‘æ€§æ€¥æ€§é«“ç³»ç™½è¡€ç—…æ ·æœ¬å’Œæ‚£è€…è¡ç”Ÿçš„å¼‚ç§ç§»æ¤æ¨¡å‹ï¼Œå®ƒå¯é¢„æµ‹ä¸ªæ€§åŒ–è¯ç‰©ç»„åˆçš„æ•ˆæœã€‚å°†MADRIGALä¸å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆï¼Œä½¿ç”¨æˆ·å¯ä»¥ç”¨è‡ªç„¶è¯­è¨€æè¿°ä¸´åºŠç»“æœï¼Œé€šè¿‡è¯†åˆ«æ½œåœ¨çš„ä¸è‰¯ç›¸äº’ä½œç”¨å’Œæ¯’æ€§é£é™©æ¥æé«˜å®‰å…¨æ€§è¯„ä¼°ã€‚MADRIGALæä¾›äº†ä¸€ç§å¤šæ¨¡å¼æ–¹æ³•ï¼Œç”¨äºè®¾è®¡å…·æœ‰æ›´é«˜é¢„æµ‹å‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§çš„è”åˆç–—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02781v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMADRIGALçš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿèåˆç»“æ„ã€é€šè·¯ã€ç»†èƒæ´»åŠ›å’Œè½¬å½•ç»„æ•°æ®ï¼Œé¢„æµ‹è¯ç‰©ç»„åˆå¯¹953ç§ä¸´åºŠç»“å±€çš„å½±å“ã€‚å®ƒé‡‡ç”¨è½¬æ¢å™¨ç“¶é¢ˆæ¨¡å—æ¥å¤„ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¼ºå¤±æ•°æ®ï¼Œå¹¶åœ¨é¢„æµ‹è¯ç‰©ç›¸äº’ä½œç”¨æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚MADRIGALæ”¯æŒè™šæ‹Ÿç­›é€‰æŠ—ç™Œè¯ç‰©ç»„åˆå’Œå¤šè¯ç–—æ³•ç®¡ç†ï¼Œå¯è¯†åˆ«è¿è¾“ä»‹å¯¼çš„è¯ç‰©ç›¸äº’ä½œç”¨ï¼Œå¹¶é¢„æµ‹å…·æœ‰æœ€ä½³å®‰å…¨æ€§çš„è¯ç‰©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MADRIGALæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€AIæ¨¡å‹ï¼Œèƒ½å¤Ÿèåˆå¤šç§ç±»å‹çš„æ•°æ®æ¥é¢„æµ‹è¯ç‰©ç»„åˆçš„ä¸´åºŠæ•ˆæœã€‚</li>
<li>è¯¥æ¨¡å‹å¯ä»¥å¤„ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¼ºå¤±æ•°æ®ï¼Œè¿™æ˜¯å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>MADRIGALåœ¨é¢„æµ‹è¯ç‰©ç›¸äº’ä½œç”¨æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶å¯ç”¨äºè™šæ‹Ÿç­›é€‰æŠ—ç™Œè¯ç‰©ç»„åˆå’Œå¤šè¯ç–—æ³•ç®¡ç†ã€‚</li>
<li>MADRIGALèƒ½å¤Ÿè¯†åˆ«è¿è¾“ä»‹å¯¼çš„è¯ç‰©ç›¸äº’ä½œç”¨ï¼Œå¹¶é¢„æµ‹å…·æœ‰æœ€ä½³å®‰å…¨æ€§çš„è¯ç‰©ï¼Œå¦‚FDAæ‰¹å‡†çš„ç”¨äºæ²»ç–—MASHçš„resmetiromã€‚</li>
<li>è¯¥æ¨¡å‹æ”¯æŒä¸ªæ€§åŒ–ç™Œç—‡æ²»ç–—ï¼Œé€šè¿‡æ•´åˆç™Œç—‡æ‚£è€…çš„åŸºå› ç»„æ•°æ®è¿›è¡Œè¯ç‰©ç»„åˆé¢„æµ‹ã€‚</li>
<li>MADRIGALä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆä½¿ç”¨æˆ·å¯ä»¥ç”¨è‡ªç„¶è¯­è¨€æè¿°ä¸´åºŠç»“æœï¼Œä»è€Œæé«˜å®‰å…¨æ€§è¯„ä¼°çš„å‡†ç¡®æ€§å’Œè¯†åˆ«æ½œåœ¨çš„ä¸è‰¯ç›¸äº’ä½œç”¨å’Œæ¯’æ€§é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff4379348811740fcb2a0d9c4b93152e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="InSerter-Speech-Instruction-Following-with-Unsupervised-Interleaved-Pre-training"><a href="#InSerter-Speech-Instruction-Following-with-Unsupervised-Interleaved-Pre-training" class="headerlink" title="InSerter: Speech Instruction Following with Unsupervised Interleaved   Pre-training"></a>InSerter: Speech Instruction Following with Unsupervised Interleaved   Pre-training</h2><p><strong>Authors:Dingdong Wang, Jin Xu, Ruihang Chu, Zhifang Guo, Xiong Wang, Jincenzi Wu, Dongchao Yang, Shengpeng Ji, Junyang Lin</strong></p>
<p>Recent advancements in speech large language models (SpeechLLMs) have attracted considerable attention. Nonetheless, current methods exhibit suboptimal performance in adhering to speech instructions. Notably, the intelligence of models significantly diminishes when processing speech-form input as compared to direct text-form input. Prior work has attempted to mitigate this semantic inconsistency between speech and text representations through techniques such as representation and behavior alignment, which involve the meticulous design of data pairs during the post-training phase. In this paper, we introduce a simple and scalable training method called InSerter, which stands for Interleaved Speech-Text Representation Pre-training. InSerter is designed to pre-train large-scale unsupervised speech-text sequences, where the speech is synthesized from randomly selected segments of an extensive text corpus using text-to-speech conversion. Consequently, the model acquires the ability to generate textual continuations corresponding to the provided speech segments, obviating the need for intensive data design endeavors. To systematically evaluate speech instruction-following capabilities, we introduce SpeechInstructBench, the first comprehensive benchmark specifically designed for speech-oriented instruction-following tasks. Our proposed InSerter achieves SOTA performance in SpeechInstructBench and demonstrates superior or competitive results across diverse speech processing tasks. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSpeechLLMsï¼‰çš„è¿›å±•å¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨éµå¾ªè¯­éŸ³æŒ‡ä»¤æ–¹é¢è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç›´æ¥æ–‡æœ¬è¾“å…¥ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨å¤„ç†è¯­éŸ³å½¢å¼è¾“å…¥æ—¶çš„æ™ºèƒ½æ°´å¹³å¤§å¹…ä¸‹é™ã€‚æ—©æœŸçš„å·¥ä½œè¯•å›¾é€šè¿‡è¡¨ç¤ºå’Œè¡Œä¸ºå¯¹é½ç­‰æŠ€æœ¯æ¥ç¼“è§£è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„è¯­ä¹‰ä¸ä¸€è‡´æ€§ï¼Œè¿™éœ€è¦åœ¨åè®­ç»ƒé˜¶æ®µç²¾å¿ƒè®¾è®¡æ•°æ®å¯¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„è®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºInSerterï¼Œå³äº¤æ›¿è¯­éŸ³-æ–‡æœ¬è¡¨ç¤ºé¢„è®­ç»ƒã€‚InSerteræ—¨åœ¨é¢„è®­ç»ƒå¤§è§„æ¨¡æ— ç›‘ç£çš„è¯­éŸ³-æ–‡æœ¬åºåˆ—ï¼Œå…¶ä¸­è¯­éŸ³æ˜¯é€šè¿‡æ–‡æœ¬è½¬è¯­éŸ³è½¬æ¢ä»å¤§é‡æ–‡æœ¬è¯­æ–™åº“çš„éšæœºæ®µè½ä¸­åˆæˆçš„ã€‚å› æ­¤ï¼Œæ¨¡å‹è·å¾—äº†æ ¹æ®æä¾›çš„è¯­éŸ³æ®µè½ç”Ÿæˆæ–‡æœ¬å»¶ç»­çš„èƒ½åŠ›ï¼Œä»è€Œæ— éœ€è¿›è¡Œå¯†é›†çš„æ•°æ®è®¾è®¡åŠªåŠ›ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°éµå¾ªè¯­éŸ³æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SpeechInstructBenchï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºé¢å‘è¯­éŸ³çš„æŒ‡ä»¤éµå¾ªä»»åŠ¡è®¾è®¡çš„ç¬¬ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬æå‡ºçš„InSerteråœ¨SpeechInstructBenchä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œå¹¶åœ¨å¤šç§è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæˆ–å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02769v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSpeechLLMsï¼‰çš„è¿›å±•å¤‡å—å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¯­éŸ³æŒ‡ä»¤æ—¶è¡¨ç°æ¬ ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„è®­ç»ƒæ–¹æ³•â€”â€”InSerterï¼Œå³äº¤æ›¿è¯­éŸ³æ–‡æœ¬è¡¨ç¤ºé¢„è®­ç»ƒæ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé¢„è®­ç»ƒå¤§è§„æ¨¡æ— ç›‘ç£è¯­éŸ³æ–‡æœ¬åºåˆ—ï¼Œé€šè¿‡æ–‡æœ¬è½¬è¯­éŸ³è½¬æ¢æŠ€æœ¯åˆæˆéšæœºé€‰æ‹©çš„æ–‡æœ¬è¯­æ–™åº“ç‰‡æ®µçš„è¯­éŸ³ï¼Œä½¿æ¨¡å‹å­¦ä¼šæ ¹æ®æä¾›çš„è¯­éŸ³ç‰‡æ®µç”Ÿæˆæ–‡æœ¬å»¶ç»­ã€‚ä¸ºç³»ç»Ÿè¯„ä¼°è¯­éŸ³æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†SpeechInstructBenchåŸºå‡†æµ‹è¯•ï¼ŒInSerteråœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€ä¼˜æ€§èƒ½ï¼Œå¹¶åœ¨å¤šç§è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSpeechLLMsï¼‰åœ¨å¤„ç†è¯­éŸ³æŒ‡ä»¤æ—¶å­˜åœ¨æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å½“å‰ç ”ç©¶é€šè¿‡è¡¨ç¤ºå’Œè¡Œä¸ºå¯¹é½ç­‰æŠ€æœ¯å°è¯•è§£å†³è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„è¯­ä¹‰ä¸ä¸€è‡´æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®€å•ä¸”å¯æ‰©å±•çš„è®­ç»ƒæ–¹æ³•â€”â€”InSerterï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé¢„è®­ç»ƒå¤§è§„æ¨¡æ— ç›‘ç£è¯­éŸ³æ–‡æœ¬åºåˆ—ã€‚</li>
<li>InSerteråˆ©ç”¨æ–‡æœ¬è½¬è¯­éŸ³è½¬æ¢æŠ€æœ¯åˆæˆè¯­éŸ³ï¼Œä½¿æ¨¡å‹å­¦ä¼šæ ¹æ®è¯­éŸ³ç‰‡æ®µç”Ÿæˆæ–‡æœ¬å»¶ç»­ã€‚</li>
<li>å¼•å…¥äº†SpeechInstructBenchåŸºå‡†æµ‹è¯•ï¼Œä»¥ç³»ç»Ÿè¯„ä¼°è¯­éŸ³æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚</li>
<li>InSerteråœ¨SpeechInstructBenchåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä¼˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fcb877716fa531d0d55466670b46a534.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee555a2e01e5ccd9294112a41cbbb2fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cf824c46fd3314ae9e0c5b0089ba5d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d3e92b6c404a762f3e56a27afdfc6cc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BatchGEMBA-Token-Efficient-Machine-Translation-Evaluation-with-Batched-Prompting-and-Prompt-Compression"><a href="#BatchGEMBA-Token-Efficient-Machine-Translation-Evaluation-with-Batched-Prompting-and-Prompt-Compression" class="headerlink" title="BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched   Prompting and Prompt Compression"></a>BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched   Prompting and Prompt Compression</h2><p><strong>Authors:Daniil Larionov, Steffen Eger</strong></p>
<p>Recent advancements in Large Language Model (LLM)-based Natural Language Generation evaluation have largely focused on single-example prompting, resulting in significant token overhead and computational inefficiencies. In this work, we introduce BatchGEMBA-MQM, a framework that integrates batched prompting with the GEMBA-MQM metric for machine translation evaluation. Our approach aggregates multiple translation examples into a single prompt, reducing token usage by 2-4 times (depending on the batch size) relative to single-example prompting. Furthermore, we propose a batching-aware prompt compression model that achieves an additional token reduction of 13-15% on average while also showing ability to help mitigate batching-induced quality degradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral Small, Phi4, and CommandR7B) and varying batch sizes reveal that while batching generally negatively affects quality (but sometimes not substantially), prompt compression does not degrade further, and in some cases, recovers quality loss. For instance, GPT-4o retains over 90% of its baseline performance at a batch size of 4 when compression is applied, compared to a 44.6% drop without compression. We plan to release our code and trained models at <a target="_blank" rel="noopener" href="https://github.com/NL2G/batchgemba">https://github.com/NL2G/batchgemba</a> to support future research in this domain. </p>
<blockquote>
<p>è¿‘æœŸï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°è¿›å±•ä¸»è¦é›†ä¸­åœ¨å•ä¾‹æç¤ºä¸Šï¼Œè¿™å¯¼è‡´äº†æ˜¾è‘—çš„ä»¤ç‰Œå¼€é”€å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†BatchGEMBA-MQMæ¡†æ¶ï¼Œå®ƒå°†æ‰¹é‡æç¤ºä¸GEMBA-MQMæœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¤šä¸ªç¿»è¯‘ç¤ºä¾‹èšé›†åˆ°ä¸€ä¸ªæç¤ºä¸­ï¼Œä¸å•ä¾‹æç¤ºç›¸æ¯”ï¼Œå¯ä»¥å‡å°‘2-4å€çš„ä»¤ç‰Œä½¿ç”¨ï¼ˆå…·ä½“å–å†³äºæ‰¹é‡å¤§å°ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ„ŸçŸ¥æ‰¹å¤„ç†çš„æç¤ºå‹ç¼©æ¨¡å‹ï¼Œåœ¨å¹³å‡æƒ…å†µä¸‹å®ç°äº†é¢å¤–çš„ä»¤ç‰Œå‡å°‘13-15%ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºæœ‰åŠ©äºç¼“è§£æ‰¹é‡å¼•èµ·çš„è´¨é‡é€€åŒ–çš„èƒ½åŠ›ã€‚åœ¨å¤šä¸ªLLMï¼ˆGPT-4oã€GPT-4o-miniã€Mistral Smallã€Phi4å’ŒCommandR7Bï¼‰å’Œä¸åŒçš„æ‰¹é‡å¤§å°ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè™½ç„¶æ‰¹é‡å¤„ç†é€šå¸¸ä¼šå¯¹è´¨é‡äº§ç”Ÿè´Ÿé¢å½±å“ï¼ˆä½†æœ‰æ—¶å¹¶ä¸æ˜¾è‘—ï¼‰ï¼Œä½†æç¤ºå‹ç¼©å¹¶ä¸ä¼šè¿›ä¸€æ­¥é™ä½è´¨é‡ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹è¿˜èƒ½æ¢å¤æŸå¤±çš„è´¨é‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‰¹é‡å¤§å°ä¸º4æ—¶ï¼ŒGPT-4oåœ¨åº”ç”¨å‹ç¼©åä¿ç•™äº†è¶…è¿‡90%çš„åŸºçº¿æ€§èƒ½ï¼Œè€Œæ²¡æœ‰å‹ç¼©æ—¶æ€§èƒ½ä¸‹é™äº†44.6%ã€‚æˆ‘ä»¬è®¡åˆ’å°†ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/NL2G/batchgemba%E4%B8%8A%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E6%9C%AA%E6%9D%A5%E5%9C%A8%E8%BF%99%E4%B8%80%E9%A2%86%E5%9F%9F%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/NL2G/batchgembaä¸Šï¼Œä»¥æ”¯æŒæœªæ¥åœ¨è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02756v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°çš„æœ€æ–°è¿›å±•ä¸»è¦é›†ä¸­åœ¨å•ä¾‹æç¤ºä¸Šï¼Œå¯¼è‡´äº†æ˜¾è‘—çš„ä»¤ç‰Œå¼€é”€å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†BatchGEMBA-MQMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ‰¹é‡æç¤ºä¸GEMBA-MQMæŒ‡æ ‡ç›¸ç»“åˆï¼Œç”¨äºæœºå™¨ç¿»è¯‘è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¤šä¸ªç¿»è¯‘ç¤ºä¾‹èšé›†åˆ°ä¸€ä¸ªæç¤ºä¸­ï¼Œä¸å•ä¾‹æç¤ºç›¸æ¯”ï¼Œç›¸å¯¹å‡å°‘äº†2-4å€çš„ä»¤ç‰Œä½¿ç”¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ‰¹å¤„ç†æ„ŸçŸ¥çš„æç¤ºå‹ç¼©æ¨¡å‹ï¼Œåœ¨å¹³å‡å‡å°‘é¢å¤–çš„ä»¤ç‰Œä½¿ç”¨ç‡çš„åŒæ—¶æå‡äº†ç¿»è¯‘è´¨é‡ã€‚å¯¹å¤šä¸ªLLMå’Œä¸åŒçš„æ‰¹é‡å¤§å°çš„è¯„ä¼°è¡¨æ˜ï¼Œè™½ç„¶æ‰¹å¤„ç†é€šå¸¸ä¼šå¯¹è´¨é‡äº§ç”Ÿè´Ÿé¢å½±å“ï¼ˆä½†æœ‰æ—¶å½±å“å¹¶ä¸æ˜¾è‘—ï¼‰ï¼Œä½†æç¤ºå‹ç¼©å¹¶ä¸ä¼šè¿›ä¸€æ­¥é™ä½è´¨é‡ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹è¿˜èƒ½æ¢å¤æŸå¤±çš„è´¨é‡ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‰¹é‡å¤§å°ä¸º4çš„æƒ…å†µä¸‹ï¼ŒGPT-4oåœ¨ä½¿ç”¨å‹ç¼©æ—¶ä¿ç•™äº†è¶…è¿‡90%çš„åŸºçº¿æ€§èƒ½ï¼Œè€Œæ²¡æœ‰å‹ç¼©æ—¶æ€§èƒ½ä¸‹é™äº†44.6%ã€‚æˆ‘ä»¬è®¡åˆ’åœ¨<a target="_blank" rel="noopener" href="https://github.com/NL2G/batchgemba%E4%B8%8A%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E6%9C%AA%E6%9D%A5%E5%9C%A8%E6%AD%A4%E9%A2%86%E5%9F%9F%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/NL2G/batchgembaä¸Šå‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œè®­ç»ƒæ¨¡å‹ï¼Œä»¥æ”¯æŒæœªæ¥åœ¨æ­¤é¢†åŸŸçš„ç ”ç©¶ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€è¿‘LLMåœ¨NLPè¯„ä»·æ–¹é¢çš„è¿›å±•ä¸»è¦é›†ä¸­åœ¨å•ä¾‹æç¤ºä¸Šï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½å’Œä»¤ç‰Œå¼€é”€å¤§ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶BatchGEMBA-MQMæ¥æ”¹è¿›è¿™ä¸€çŠ¶å†µï¼Œå®ƒé€šè¿‡é›†æˆæ‰¹é‡æç¤ºæ¥æé«˜æ•ˆç‡ã€‚</li>
<li>è¯¥æ¡†æ¶å°†å¤šä¸ªç¿»è¯‘ç¤ºä¾‹èšé›†åˆ°ä¸€ä¸ªæç¤ºä¸­ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘ä»¤ç‰Œä½¿ç”¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ‰¹å¤„ç†æ„ŸçŸ¥çš„æç¤ºå‹ç¼©æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æé«˜äº†ç¿»è¯‘è´¨é‡å¹¶é™ä½äº†ä»¤ç‰Œä½¿ç”¨ç‡ã€‚</li>
<li>æ‰¹å¤„ç†å¯¹ç¿»è¯‘è´¨é‡æœ‰è´Ÿé¢å½±å“ï¼Œä½†æç¤ºå‹ç¼©æœ‰åŠ©äºç¼“è§£è¿™ç§å½±å“ã€‚</li>
<li>åœ¨æ‰¹é‡å¤§å°ä¸º4çš„æƒ…å†µä¸‹ï¼ŒGPT-4oåœ¨ä½¿ç”¨å‹ç¼©æ—¶ä¿æŒäº†è¾ƒé«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79e079d2940a3610e080502820065ce4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f80ea0322c13c79e22a10d27c3647897.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Evaluating-Knowledge-Generation-and-Self-Refinement-Strategies-for-LLM-based-Column-Type-Annotation"><a href="#Evaluating-Knowledge-Generation-and-Self-Refinement-Strategies-for-LLM-based-Column-Type-Annotation" class="headerlink" title="Evaluating Knowledge Generation and Self-Refinement Strategies for   LLM-based Column Type Annotation"></a>Evaluating Knowledge Generation and Self-Refinement Strategies for   LLM-based Column Type Annotation</h2><p><strong>Authors:Keti Korini, Christian Bizer</strong></p>
<p>Understanding the semantics of columns in relational tables is an important pre-processing step for indexing data lakes in order to provide rich data search. An approach to establishing such understanding is column type annotation (CTA) where the goal is to annotate table columns with terms from a given vocabulary. This paper experimentally compares different knowledge generation and self-refinement strategies for LLM-based column type annotation. The strategies include using LLMs to generate term definitions, error-based refinement of term definitions, self-correction, and fine-tuning using examples and term definitions. We evaluate these strategies along two dimensions: effectiveness measured as F1 performance and efficiency measured in terms of token usage and cost. Our experiments show that the best performing strategy depends on the model&#x2F;dataset combination. We find that using training data to generate label definitions outperforms using the same data as demonstrations for in-context learning for two out of three datasets using OpenAI models. The experiments further show that using the LLMs to refine label definitions brings an average increase of 3.9% F1 in 10 out of 12 setups compared to the performance of the non-refined definitions. Combining fine-tuned models with self-refined term definitions results in the overall highest performance, outperforming zero-shot prompting fine-tuned models by at least 3% in F1 score. The costs analysis shows that while reaching similar F1 score, self-refinement via prompting is more cost efficient for use cases requiring smaller amounts of tables to be annotated while fine-tuning is more efficient for large amounts of tables. </p>
<blockquote>
<p>ç†è§£å…³ç³»è¡¨ä¸­çš„åˆ—è¯­ä¹‰æ˜¯ç´¢å¼•æ•°æ®æ¹–ä»¥æä¾›ä¸°å¯Œæ•°æ®æœç´¢çš„é‡è¦é¢„å¤„ç†æ­¥éª¤ã€‚å»ºç«‹è¿™ç§ç†è§£çš„æ–¹æ³•ä¹‹ä¸€æ˜¯åˆ—ç±»å‹æ³¨é‡Šï¼ˆCTAï¼‰ï¼Œå…¶ç›®çš„æ˜¯ä½¿ç”¨ç»™å®šçš„è¯æ±‡å¯¹è¡¨åˆ—è¿›è¡Œæ³¨é‡Šã€‚æœ¬æ–‡å®éªŒæ€§åœ°æ¯”è¾ƒäº†åŸºäºLLMçš„åˆ—ç±»å‹æ³¨é‡Šçš„ä¸åŒçŸ¥è¯†ç”Ÿæˆå’Œè‡ªæˆ‘å®Œå–„ç­–ç•¥ã€‚è¿™äº›ç­–ç•¥åŒ…æ‹¬ä½¿ç”¨LLMç”Ÿæˆæœ¯è¯­å®šä¹‰ï¼ŒåŸºäºé”™è¯¯çš„æœ¯è¯­å®šä¹‰ä¼˜åŒ–ï¼Œè‡ªæˆ‘æ ¡æ­£ï¼Œä»¥åŠä½¿ç”¨ç¤ºä¾‹å’Œæœ¯è¯­å®šä¹‰è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬ä»ä¸¤ä¸ªç»´åº¦è¯„ä¼°è¿™äº›ç­–ç•¥ï¼šæœ‰æ•ˆæ€§ï¼Œä»¥F1æ€§èƒ½æ¥è¡¡é‡ï¼›ä»¥åŠæ•ˆç‡ï¼Œä»¥ä»¤ç‰Œä½¿ç”¨é‡å’Œæˆæœ¬æ¥è¡¡é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæœ€ä½³ç­–ç•¥å–å†³äºæ¨¡å‹å’Œæ•°æ®é›†ç»„åˆã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹äºä¸‰ä¸ªæ•°æ®é›†ä¸­çš„ä¸¤ä¸ªæ•°æ®é›†ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®ç”Ÿæˆæ ‡ç­¾å®šä¹‰çš„è¡¨ç°ä¼˜äºå°†ç›¸åŒæ•°æ®ç”¨ä½œä¸Šä¸‹æ–‡å­¦ä¹ çš„æ¼”ç¤ºæ¡ˆä¾‹ï¼Œä½¿ç”¨çš„æ˜¯OpenAIæ¨¡å‹ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œä¸æœªç²¾åŒ–çš„å®šä¹‰ç›¸æ¯”ï¼Œä½¿ç”¨LLMå®Œå–„æ ‡ç­¾å®šä¹‰åœ¨12ä¸ªè®¾ç½®ä¸­çš„10ä¸ªè®¾ç½®ä¸­å¹³å‡æé«˜äº†3.9%çš„F1åˆ†æ•°ã€‚ä¸é›¶æ ·æœ¬æç¤ºå¾®è°ƒæ¨¡å‹ç›¸æ¯”ï¼Œç»“åˆå¾®è°ƒæ¨¡å‹å’Œè‡ªæˆ‘å®Œå–„çš„æœ¯è¯­å®šä¹‰è¾¾åˆ°äº†æ•´ä½“æœ€ä½³æ€§èƒ½ï¼Œåœ¨F1åˆ†æ•°ä¸Šè‡³å°‘æé«˜äº†3%ã€‚æˆæœ¬åˆ†æè¡¨æ˜ï¼Œåœ¨è¾¾åˆ°ç±»ä¼¼F1å¾—åˆ†çš„æƒ…å†µä¸‹ï¼Œå¯¹äºéœ€è¦æ³¨é‡Šçš„è¡¨æ ¼æ•°é‡è¾ƒå°‘çš„ä½¿ç”¨æƒ…å†µï¼Œé€šè¿‡æç¤ºè¿›è¡Œè‡ªæˆ‘å®Œå–„æ›´å…·æˆæœ¬æ•ˆç›Šï¼Œè€Œå¾®è°ƒåˆ™æ›´é€‚åˆå¤§é‡è¡¨æ ¼çš„æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02718v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å®éªŒæ€§åœ°æ¯”è¾ƒäº†åŸºäºLLMçš„åˆ—ç±»å‹æ³¨è§£çš„ä¸åŒçŸ¥è¯†ç”Ÿæˆå’Œè‡ªæˆ‘å®Œå–„ç­–ç•¥ã€‚ç­–ç•¥åŒ…æ‹¬ä½¿ç”¨LLMç”Ÿæˆæœ¯è¯­å®šä¹‰ã€åŸºäºé”™è¯¯çš„æœ¯è¯­å®šä¹‰æ”¹è¿›ã€è‡ªæˆ‘æ ¡æ­£ä»¥åŠä½¿ç”¨ç¤ºä¾‹å’Œæœ¯è¯­å®šä¹‰è¿›è¡Œå¾®è°ƒã€‚è¯„ä¼°è¿™äº›ç­–ç•¥çš„ä¸¤ä¸ªç»´åº¦åŒ…æ‹¬æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³ç­–ç•¥å–å†³äºæ¨¡å‹å’Œæ•°æ®é›†ç»„åˆã€‚ä½¿ç”¨è®­ç»ƒæ•°æ®ç”Ÿæˆæ ‡ç­¾å®šä¹‰çš„è¡¨ç°ä¼˜äºä½¿ç”¨ç›¸åŒæ•°æ®ä½œä¸ºæ¼”ç¤ºè¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œä½¿ç”¨LLMæ”¹è¿›æ ‡ç­¾å®šä¹‰åœ¨12ä¸ªè®¾ç½®ä¸­çš„10ä¸ªä¸­ä½¿F1åˆ†æ•°å¹³å‡æé«˜äº†3.9%ã€‚ç»“åˆå¾®è°ƒæ¨¡å‹å’Œè‡ªæˆ‘å®Œå–„çš„æœ¯è¯­å®šä¹‰è¾¾åˆ°æ•´ä½“æœ€ä½³æ€§èƒ½ï¼Œåœ¨F1åˆ†æ•°ä¸Šè‡³å°‘ä¼˜äºé›¶æ ·æœ¬æç¤ºå¾®è°ƒæ¨¡å‹3%ã€‚æˆæœ¬åˆ†æè¡¨æ˜ï¼Œè™½ç„¶è¾¾åˆ°ç›¸ä¼¼çš„F1åˆ†æ•°ï¼Œä½†å¯¹äºéœ€è¦æ³¨é‡Šçš„è¡¨æ ¼æ•°é‡è¾ƒå°‘çš„æƒ…å†µï¼Œé€šè¿‡æç¤ºè¿›è¡Œè‡ªæˆ‘å®Œå–„æ›´ä¸ºæˆæœ¬æ•ˆç›Šï¼›è€Œå¯¹äºéœ€è¦å¤§é‡è¡¨æ ¼çš„æƒ…å†µï¼Œå¾®è°ƒæ›´ä¸ºé«˜æ•ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç†è§£å’Œå¤„ç†å…³ç³»è¡¨ä¸­çš„åˆ—è¯­ä¹‰æ˜¯æ•°æ®æ¹–ç´¢å¼•çš„é¢„å¤„ç†æ­¥éª¤ï¼Œä¸ºä¸°å¯Œæ•°æ®æœç´¢æä¾›æ”¯æŒã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºåˆ—ç±»å‹æ³¨è§£ï¼ˆCTAï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç»™è¡¨åˆ—è¿›è¡Œæœ¯è¯­æ ‡æ³¨ã€‚</li>
<li>å®éªŒæ¯”è¾ƒäº†åŸºäºLLMçš„CTAçš„ä¸åŒçŸ¥è¯†ç”Ÿæˆå’Œè‡ªæˆ‘å®Œå–„ç­–ç•¥ï¼ŒåŒ…æ‹¬æœ¯è¯­å®šä¹‰çš„ç”Ÿæˆã€åŸºäºé”™è¯¯çš„æœ¯è¯­å®šä¹‰æ”¹è¿›ç­‰ã€‚</li>
<li>è¯„ä¼°ç­–ç•¥çš„æœ‰æ•ˆæ€§é€šè¿‡F1æ€§èƒ½æ¥è¡¡é‡ï¼Œæ•ˆç‡åˆ™é€šè¿‡ä»¤ç‰Œä½¿ç”¨æƒ…å†µå’Œæˆæœ¬æ¥è¡¡é‡ã€‚</li>
<li>æœ€ä½³ç­–ç•¥çš„é€‰æ‹©å–å†³äºæ‰€ä½¿ç”¨çš„æ¨¡å‹å’Œæ•°æ®é›†ç»„åˆã€‚</li>
<li>åœ¨æŸäº›æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®ç”Ÿæˆæ ‡ç­¾å®šä¹‰çš„è¡¨ç°ä¼˜äºä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-00d46602bd0936287f4f4d52db621eb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13305087d6851b0a47d4242296a8956d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0eb4838f2cc46507ac585fec31c0c1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed70af54942bd381d7de5c8d41beb7b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-377b16aed24e7261bf6d67951bc2b570.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs"><a href="#Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs" class="headerlink" title="Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs"></a>Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs</h2><p><strong>Authors:Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have demonstrated significant progress in perceiving and reasoning over multimodal inquiries, ushering in a new research era for foundation models. However, vision-language misalignment in MLLMs has emerged as a critical challenge, where the textual responses generated by these models are not factually aligned with the given text-image inputs. Existing efforts to address vision-language misalignment have focused on developing specialized vision-language connectors or leveraging visual instruction tuning from diverse domains. In this paper, we tackle this issue from a fundamental yet unexplored perspective by revisiting the core architecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs consisting of a causal attention mechanism, which limits the ability of earlier modalities (e.g., images) to incorporate information from later modalities (e.g., text). To address this problem, we propose AKI, a novel MLLM that unlocks causal attention into modality-mutual attention (MMA) to enable image tokens to attend to text tokens. This simple yet effective design allows AKI to achieve superior performance in 12 multimodal understanding benchmarks (+7.2% on average) without introducing additional parameters and increasing training time. Our MMA design is intended to be generic, allowing for application across various modalities, and scalable to accommodate diverse multimodal scenarios. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/sony/aki">https://github.com/sony/aki</a>, and we will release our AKI-4B model to encourage further advancements in MLLMs across various directions. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥å’Œæ¨ç†å¤šæ¨¡æ€æŸ¥è¯¢æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºåŸºç¡€æ¨¡å‹å¼€å¯äº†ä¸€ä¸ªæ–°æ—¶ä»£ã€‚ç„¶è€Œï¼ŒMLLMsä¸­çš„è§†è§‰è¯­è¨€ä¸åŒ¹é…é—®é¢˜å·²æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå…¶ä¸­è¿™äº›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å“åº”ä¸ç»™å®šçš„æ–‡æœ¬å›¾åƒè¾“å…¥å¹¶ä¸ç›¸ç¬¦ã€‚ä¸ºè§£å†³è§†è§‰è¯­è¨€ä¸åŒ¹é…é—®é¢˜ï¼Œç°æœ‰çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨å¼€å‘ä¸“ç”¨çš„è§†è§‰è¯­è¨€è¿æ¥å™¨æˆ–ä»å¤šä¸ªé¢†åŸŸåˆ©ç”¨è§†è§‰æŒ‡ä»¤è°ƒæ•´ã€‚æœ¬æ–‡ä»ä¸€ä¸ªåŸºæœ¬ä¸”æœªè¢«æ¢ç´¢çš„è§†è§’æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡é‡æ–°è®¾è®¡MLLMçš„æ ¸å¿ƒæ¶æ„ã€‚å¤§å¤šæ•°MLLMé€šå¸¸å»ºç«‹åœ¨ä»…è§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼Œç”±å› æœæ³¨æ„åŠ›æœºåˆ¶ç»„æˆï¼Œè¿™é™åˆ¶äº†æ—©æœŸæ¨¡æ€ï¼ˆä¾‹å¦‚å›¾åƒï¼‰èå…¥åæœŸæ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬ï¼‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AKIï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹MLLMï¼Œå®ƒå°†å› æœæ³¨æ„åŠ›è§£é”ä¸ºæ¨¡æ€ç›¸äº’æ³¨æ„åŠ›ï¼ˆMMAï¼‰ï¼Œä½¿å›¾åƒæ ‡è®°èƒ½å¤Ÿå…³æ³¨æ–‡æœ¬æ ‡è®°ã€‚è¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ä½¿AKIèƒ½å¤Ÿåœ¨ä¸å¼•å…¥é¢å¤–å‚æ•°å’Œå¢åŠ è®­ç»ƒæ—¶é—´çš„æƒ…å†µä¸‹ï¼Œåœ¨12ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼ˆ+7.2%çš„å¹³å‡æå‡ï¼‰ã€‚æˆ‘ä»¬çš„MMAè®¾è®¡æ—¨åœ¨é€šç”¨åŒ–ï¼Œå¯ä»¥åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œå¹¶ä¸”å¯æ‰©å±•åˆ°é€‚åº”ä¸åŒçš„å¤šæ¨¡æ€åœºæ™¯ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/sony/aki%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%86%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84AKI-4B%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%BB%A5%E9%BC%93%E5%8A%B1%E5%9C%A8%E5%A4%9A%E4%B8%AA%E6%96%B9%E5%90%91%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%8E%A8%E5%8A%A8MLLM%E7%9A%84%E5%8F%91%E5%B1%95%E3%80%82">https://github.com/sony/akiï¼Œæˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„AKI-4Bæ¨¡å‹ï¼Œä»¥é¼“åŠ±åœ¨å¤šä¸ªæ–¹å‘è¿›ä¸€æ­¥æ¨åŠ¨MLLMçš„å‘å±•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02597v1">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9631bb198214146b9ebc74bab028622.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba860edccebed781a143e8644642aa31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-913470c87169155f8b7ee470a3116194.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c646ce9fb294b1e9730b899e36de328.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0c84f01c51ad0762dc24abedfc986e6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer"><a href="#Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer" class="headerlink" title="Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer"></a>Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer</h2><p><strong>Authors:Yujiao Yang, Jing Lian, Linhui Li</strong></p>
<p>Mixture-of-Experts (MoE) enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. However, expert in exist MoE paradigm works as an individual, thereby lacking high-quality expert interactions. Moreover, they have not been effectively extended to attention block, which constrains further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement dynamic routing on input data and experts. Our approach advances MoE design with three key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoEâ€™s routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the model employed with UoE surpass Full Attention, state-of-art MoEs and efficient transformers in several tasks across image and natural language domains. The source codes are available at <a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE">https://github.com/YujiaoYang-work/UoE</a>. </p>
<blockquote>
<p>ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¨¡å¼åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œéå¸¸é€‚åˆå¤§è§„æ¨¡åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰MoEæ¨¡å¼ä¸‹çš„ä¸“å®¶æ˜¯ç‹¬ç«‹å·¥ä½œçš„ï¼Œå› æ­¤ç¼ºä¹é«˜è´¨é‡çš„ä¸“å®¶äº¤äº’ã€‚æ­¤å¤–ï¼Œä»–ä»¬å°šæœªæœ‰æ•ˆåœ°æ‰©å±•åˆ°æ³¨æ„åŠ›å—ï¼Œè¿™é™åˆ¶äº†è¿›ä¸€æ­¥çš„æ•ˆç‡æ”¹è¿›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“å®¶è”åˆï¼ˆUoEï¼‰æ–¹æ³•ï¼Œå®ƒå°†å˜å‹å™¨åˆ†è§£ä¸ºç­‰ä»·çš„ä¸“å®¶ç»„ï¼Œç„¶åå¯¹è¾“å…¥æ•°æ®å’Œä¸“å®¶å®æ–½åŠ¨æ€è·¯ç”±ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹æ¨è¿›MoEè®¾è®¡ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬åœ¨MLPå—å’Œæ³¨æ„åŠ›å—ä¸Šè¿›è¡Œäº†åŸºäºå¼ é‡å¹¶è¡Œæ€§çš„ç­‰ä»·ä¸“å®¶åˆ†è§£ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸¤ç§è·¯ç”±èŒƒå¼ï¼šæ–‘å—çº§æ•°æ®é€‰æ‹©å’Œä¸“å®¶é€‰æ‹©ï¼Œä»¥åœ¨ä¸åŒçº§åˆ«ä¸Šåº”ç”¨è·¯ç”±ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬è®¾è®¡äº†UoEæ¨¡å‹çš„æ¶æ„ï¼ŒåŒ…æ‹¬é€‰æ‹©æ€§å¤šå¤´æ³¨æ„åŠ›ï¼ˆSMHAï¼‰å’Œè”åˆMLPä¸“å®¶ï¼ˆUoMEï¼‰ã€‚ï¼ˆ4ï¼‰æˆ‘ä»¬å¼€å‘äº†UoEè·¯ç”±å’Œè®¡ç®—æ“ä½œçš„å¹¶è¡Œå®ç°ï¼Œå¹¶åŸºäºç¡¬ä»¶å¤„ç†åˆ†æä¼˜åŒ–äº†æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨UoEçš„æ¨¡å‹åœ¨å›¾åƒå’Œè‡ªç„¶è¯­è¨€é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†å…¨æ³¨æ„åŠ›ã€æœ€æ–°çš„MoEå’Œé«˜æ•ˆå˜å‹å™¨ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YujiaoYang-work/UoEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02495v1">PDF</a> 17 pages, 6 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºMixture-of-Expertsï¼ˆMoEï¼‰çš„æ¨¡å‹åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚ç„¶è€Œï¼Œç°æœ‰MoEä¸­çš„ä¸“å®¶ä»¥ä¸ªäººèº«ä»½å·¥ä½œï¼Œç¼ºä¹é«˜è´¨é‡ä¸“å®¶äº¤äº’ï¼Œä¸”å°šæœªæœ‰æ•ˆåœ°æ‰©å±•åˆ°æ³¨æ„åŠ›å—ï¼Œé™åˆ¶äº†è¿›ä¸€æ­¥çš„æ•ˆç‡æå‡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Union-of-Expertsï¼ˆUoEï¼‰ï¼Œå®ƒå°†å˜å‹å™¨åˆ†è§£ä¸ºç­‰ä»·çš„ä¸“å®¶ç»„ï¼Œå¹¶åœ¨è¾“å…¥æ•°æ®å’Œä¸“å®¶ä¸Šå®ç°åŠ¨æ€è·¯ç”±ã€‚UoEçš„è®¾è®¡æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šå¯¹MLPå—å’Œæ³¨æ„åŠ›å—è¿›è¡Œç­‰ä»·ä¸“å®¶åˆ†è§£ã€å¼€å‘ä¸¤ç§è·¯ç”±èŒƒå¼ã€è®¾è®¡UoEæ¨¡å‹çš„æ¶æ„ï¼Œå¹¶åœ¨å›¾åƒå’Œè‡ªç„¶è¯­è¨€é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸­éªŒè¯äº†å…¶æ€§èƒ½è¶…è¶Šäº†å…¨æ³¨æ„åŠ›æ¨¡å‹å’Œé«˜æ•ˆçš„å˜å‹å™¨æ¨¡å‹ã€‚æºä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MoEæ¨¡å‹åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­å…·æœ‰è‰¯å¥½çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>MoEä¸­çš„ä¸“å®¶ä»¥ä¸ªäººèº«ä»½å·¥ä½œï¼Œç¼ºä¹é«˜è´¨é‡ä¸“å®¶äº¤äº’ã€‚</li>
<li>UoEé€šè¿‡åˆ†è§£ä¸ºç­‰ä»·çš„ä¸“å®¶ç»„æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¹¶å®ç°åŠ¨æ€è·¯ç”±ã€‚</li>
<li>UoEçš„è®¾è®¡æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šå¯¹MLPå—å’Œæ³¨æ„åŠ›å—çš„ç­‰ä»·ä¸“å®¶åˆ†è§£ã€å¼€å‘ä¸¤ç§è·¯ç”±èŒƒå¼ã€è®¾è®¡UoEæ¨¡å‹çš„æ¶æ„ã€‚</li>
<li>UoEæ¨¡å‹æ€§èƒ½åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†å…¨æ³¨æ„åŠ›æ¨¡å‹å’Œé«˜æ•ˆçš„å˜å‹å™¨æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96123587ca48d4e433b9806724203533.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19bba9e71a1326b4bda7584d14faac17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8be494f32600280076484d34d638802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11699562dda5bc611776b8c0548bc2be.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Network-Traffic-Classification-Using-Machine-Learning-Transformer-and-Large-Language-Models"><a href="#Network-Traffic-Classification-Using-Machine-Learning-Transformer-and-Large-Language-Models" class="headerlink" title="Network Traffic Classification Using Machine Learning, Transformer, and   Large Language Models"></a>Network Traffic Classification Using Machine Learning, Transformer, and   Large Language Models</h2><p><strong>Authors:Ahmad Antari, Yazan Abo-Aisheh, Jehad Shamasneh, Huthaifa I. Ashqar</strong></p>
<p>This study uses various models to address network traffic classification, categorizing traffic into web, browsing, IPSec, backup, and email. We collected a comprehensive dataset from Arbor Edge Defender (AED) devices, comprising of 30,959 observations and 19 features. Multiple models were evaluated, including Naive Bayes, Decision Tree, Random Forest, Gradient Boosting, XGBoost, Deep Neural Networks (DNN), Transformer, and two Large Language Models (LLMs) including GPT-4o and Gemini with zero- and few-shot learning. Transformer and XGBoost showed the best performance, achieving the highest accuracy of 98.95 and 97.56%, respectively. GPT-4o and Gemini showed promising results with few-shot learning, improving accuracy significantly from initial zero-shot performance. While Gemini Few-Shot and GPT-4o Few-Shot performed well in categories like Web and Email, misclassifications occurred in more complex categories like IPSec and Backup. The study highlights the importance of model selection, fine-tuning, and the balance between training data size and model complexity for achieving reliable classification results. </p>
<blockquote>
<p>æœ¬ç ”ç©¶é‡‡ç”¨å¤šç§æ¨¡å‹æ¥è§£å†³ç½‘ç»œæµé‡åˆ†ç±»é—®é¢˜ï¼Œå°†æµé‡åˆ†ä¸ºç½‘é¡µã€æµè§ˆã€IPSecã€å¤‡ä»½å’Œç”µå­é‚®ä»¶ç­‰ç±»åˆ«ã€‚æˆ‘ä»¬ä»Arbor Edge Defenderï¼ˆAEDï¼‰è®¾å¤‡æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®é›†ï¼ŒåŒ…å«30,959ä¸ªè§‚æµ‹å€¼å’Œ19ä¸ªç‰¹å¾ã€‚è¯„ä¼°äº†å¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬æœ´ç´ è´å¶æ–¯ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ¢¯åº¦æå‡ã€XGBoostã€æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ã€Transformerä»¥åŠä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒ…æ‹¬GPT-4oå’ŒGeminiï¼Œå¹¶è¿›è¡Œäº†é›¶æ¬¡å­¦ä¹ å’Œå°‘æ¬¡å­¦ä¹ ã€‚Transformerå’ŒXGBoostè¡¨ç°æœ€ä½³ï¼Œåˆ†åˆ«è¾¾åˆ°äº†æœ€é«˜çš„98.95%å’Œ97.56%çš„å‡†ç¡®ç‡ã€‚GPT-4oå’ŒGeminiåœ¨å°‘æ¬¡å­¦ä¹ çš„æƒ…å†µä¸‹è¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„ç»“æœï¼Œå‡†ç¡®ç‡ä»åˆå§‹çš„é›¶æ¬¡å­¦ä¹ æ€§èƒ½æ˜¾è‘—æé«˜ã€‚è™½ç„¶Geminiå°‘æ¬¡å­¦ä¹ å’ŒGPT-4oå°‘æ¬¡å­¦ä¹ åœ¨Webå’Œç”µå­é‚®ä»¶ç­‰ç±»åˆ«ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨IPSecå’Œå¤‡ä»½ç­‰æ›´å¤æ‚ç±»åˆ«ä¸­å‡ºç°äº†è¯¯åˆ†ç±»æƒ…å†µã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ¨¡å‹é€‰æ‹©ã€å¾®è°ƒä»¥åŠè®­ç»ƒæ•°æ®å¤§å°å’Œæ¨¡å‹å¤æ‚æ€§ä¹‹é—´çš„å¹³è¡¡å¯¹äºå®ç°å¯é åˆ†ç±»ç»“æœçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02141v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>ç½‘ç»œæµé‡åˆ†ç±»æŠ€æœ¯ä¸­æ¨¡å‹çš„æ¢è®¨å’Œåº”ç”¨ã€‚è¯¥ç ”ç©¶é‡‡ç”¨å¤šç§æ¨¡å‹å¯¹ç½‘ç»œæµé‡è¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬ç½‘ç»œæµé‡åˆ†ç±»å™¨åˆ†ä¸ºwebã€æµè§ˆã€IPSecã€å¤‡ä»½å’Œç”µå­é‚®ä»¶äº”å¤§ç±»ã€‚é‡‡ç”¨ç”±è¾¹ç¼˜è®¾å¤‡Arbor Edge Defenderé‡‡é›†çš„æ•°æ®é›†å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œäº†è¯„ä»·ã€‚æ¢è®¨äº†ä¸åŒçš„æ¨¡å‹ç®—æ³•æ€§èƒ½ä¼˜åŠ£æƒ…å†µã€‚å¦‚éšæœºæ£®æ—ã€æ¢¯åº¦æå‡æ ‘ç­‰æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œè€ŒGPT-4oå’ŒGeminiç­‰å¤§è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°çªå‡ºã€‚æ•´ä½“ç»“æœè¡¨æ˜ï¼Œé€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Œè°ƒæ•´å‚æ•°ï¼Œå¹³è¡¡è®­ç»ƒæ•°æ®é‡å’Œæ¨¡å‹å¤æ‚åº¦æ˜¯å®ç°å¯é åˆ†ç±»ç»“æœçš„å…³é”®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æ¢è®¨äº†å¤šç§æ¨¡å‹åœ¨ç½‘ç»œæµé‡åˆ†ç±»ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ–°å…´çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>æ•°æ®é›†ç”±Arbor Edge Defenderè®¾å¤‡æ”¶é›†ï¼ŒåŒ…å«å¤šä¸ªæµé‡ç±»åˆ«ï¼Œæä¾›äº†ä¸°å¯Œçš„åˆ†ææ ·æœ¬ã€‚</li>
<li>Naive Bayesã€Decision Treeç­‰ä¼ ç»Ÿæ¨¡å‹æœ‰ä¸€å®šçš„è¡¨ç°ã€‚è€ŒRandom Forestã€Gradient Boostingæ¨¡å‹ç­‰æ˜¾ç¤ºå‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>å¤§è¯­è¨€æ¨¡å‹GPT-4oå’ŒGeminiåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†é¢å¯¹å¤æ‚ç±»åˆ«å¦‚IPSecå’Œå¤‡ä»½æ—¶ä»å­˜åœ¨è¯¯åˆ†ç±»é£é™©ã€‚</li>
<li>Transformeræ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œè¾¾åˆ°98.95%çš„å‡†ç¡®ç‡ã€‚</li>
<li>æ¨¡å‹é€‰æ‹©ã€å‚æ•°è°ƒæ•´å¯¹åˆ†ç±»ç»“æœçš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87d508555433cc9106551319fb6b71f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-183dff6bd8954ff35c1334544fabc032.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19ffcb04c96746a4431d89905b6a2640.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e356351576ecad7a7f8315ef7702abc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-696f17b593216ec2f8be35fb966ba1ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74e8301376368994a7826ae4f3a3233c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4c3aed7036e702ede0946cfc059b51e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37cefc9e7264a58eca6d004a5b31e96c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fab96e842ee31f4a01f50c5f8f631062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa70ea9115467b9a2e8eda4b06a05071.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Forgetting-Transformer-Softmax-Attention-with-a-Forget-Gate"><a href="#Forgetting-Transformer-Softmax-Attention-with-a-Forget-Gate" class="headerlink" title="Forgetting Transformer: Softmax Attention with a Forget Gate"></a>Forgetting Transformer: Softmax Attention with a Forget Gate</h2><p><strong>Authors:Zhixuan Lin, Evgenii Nikishin, Xu Owen He, Aaron Courville</strong></p>
<p>An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformerâ€™s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a â€œProâ€ block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/forgetting-transformer">https://github.com/zhixuan-lin/forgetting-transformer</a>. </p>
<blockquote>
<p>ç°ä»£å¾ªç¯åºåˆ—æ¨¡å‹çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†æ˜¯é—å¿˜é—¨ã€‚è™½ç„¶Transformeræ²¡æœ‰æ˜ç¡®çš„é€’å½’å½¢å¼ï¼Œä½†æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§é€šè¿‡å°†æœªå½’ä¸€åŒ–çš„æ³¨æ„åŠ›å¾—åˆ†ä»¥æ•°æ®ç›¸å…³çš„æ–¹å¼è¿›è¡ŒåŠ æƒï¼Œè‡ªç„¶åœ°å°†é—å¿˜é—¨çº³å…¥Transformerçš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†è¿™ç§æ³¨æ„åŠ›æœºåˆ¶å‘½åä¸ºé—å¿˜æ³¨æ„åŠ›ï¼Œå¹¶å°†å¾—åˆ°çš„æ¨¡å‹å‘½åä¸ºé—å¿˜è½¬æ¢å™¨ï¼ˆFoXï¼‰ã€‚æˆ‘ä»¬å±•ç¤ºFoXåœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡ã€é•¿åº¦æ‰©å±•å’ŒçŸ­ä¸Šä¸‹æ–‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºTransformerï¼Œè€Œåœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸Transformerç›¸å½“ã€‚æ­¤å¤–ï¼Œå®ƒä¸FlashAttentionç®—æ³•å…¼å®¹ï¼Œä¸éœ€è¦ä»»ä½•ä½ç½®åµŒå…¥ã€‚åŒ…æ‹¬â€œæµ·åº•æé’ˆâ€æµ‹è¯•åœ¨å†…çš„å‡ é¡¹åˆ†æè¡¨æ˜ï¼ŒFoXåœ¨ä¿æŒTransformeråœ¨é•¿ä¸Šä¸‹æ–‡æ–¹é¢çš„ä¼˜åŠ¿çš„åŒæ—¶ï¼Œä¼˜äºå¾ªç¯åºåˆ—æ¨¡å‹å¦‚Mamba-2ã€HGRN2å’ŒDeltaNetç­‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ç»“åˆäº†å¾ªç¯åºåˆ—æ¨¡å‹ä¸­ä¸€äº›å¸¸è§æ¶æ„ç»„ä»¶çš„â€œProâ€å—è®¾è®¡ï¼Œå‘ç°å®ƒæ˜¾è‘—æé«˜äº†FoXå’ŒTransformerçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/forgetting-transformer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhixuan-lin/forgetting-transformeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02130v1">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨Transformeræ¨¡å‹ä¸­å¼•å…¥é—å¿˜é—¨æœºåˆ¶çš„æ–¹æ³•ï¼Œé€šè¿‡æ•°æ®ä¾èµ–æ€§åœ°é™ä½æœªå½’ä¸€åŒ–çš„æ³¨æ„åŠ›åˆ†æ•°æ¥å®ç°ã€‚æå‡ºäº†åä¸ºâ€œé—å¿˜æ³¨æ„åŠ›â€çš„æ–°æ³¨æ„åŠ›æœºåˆ¶å’Œç›¸åº”çš„é—å¿˜å˜å‹å™¨ï¼ˆFoXï¼‰æ¨¡å‹ã€‚åœ¨é•¿çŸ­ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡ã€é•¿åº¦å¤–æ¨å’ŒçŸ­ä¸Šä¸‹æ–‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼ŒFoXæ¨¡å‹è¡¨ç°ä¼˜äºTransformeræ¨¡å‹ï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ç›¸å½“ã€‚æ­¤å¤–ï¼ŒFoXä¸FlashAttentionç®—æ³•å…¼å®¹ï¼Œæ— éœ€ä½ç½®åµŒå…¥ã€‚é€šè¿‡ä¸€ç³»åˆ—åˆ†æè¯æ˜ï¼ŒFoXåœ¨é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›æ–¹é¢ä¼˜äºå…¶ä»–å¾ªç¯åºåˆ—æ¨¡å‹ï¼Œå¦‚Mamba-2ã€HGRN2å’ŒDeltaNetç­‰ã€‚æ­¤å¤–ï¼Œå¼•å…¥â€œProâ€å—è®¾è®¡ï¼Œèåˆäº†ä¸€äº›å¾ªç¯åºåˆ—æ¨¡å‹ä¸­çš„å¸¸è§ç»„ä»¶ï¼Œèƒ½æ˜¾è‘—æé«˜FoXå’ŒTransformerçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å°†é—å¿˜é—¨æœºåˆ¶è‡ªç„¶èå…¥Transformeræ¨¡å‹çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†â€œé—å¿˜æ³¨æ„åŠ›â€çš„æ–°æœºåˆ¶å’Œç›¸åº”çš„é—å¿˜å˜å‹å™¨ï¼ˆFoXï¼‰æ¨¡å‹ã€‚</li>
<li>åœ¨é•¿çŸ­ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡ã€é•¿åº¦å¤–æ¨å’ŒçŸ­ä¸Šä¸‹æ–‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼ŒFoXè¡¨ç°ä¼˜äºTransformerã€‚</li>
<li>FoXåœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸Transformerç›¸å½“ã€‚</li>
<li>FoXä¸FlashAttentionç®—æ³•å…¼å®¹ï¼Œæ— éœ€ä½¿ç”¨ä½ç½®åµŒå…¥ã€‚</li>
<li>åˆ†æè¡¨æ˜ï¼ŒFoXåœ¨é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›æ–¹é¢ä¼˜äºå…¶ä»–å¾ªç¯åºåˆ—æ¨¡å‹ã€‚</li>
<li>å¼•å…¥çš„â€œProâ€å—è®¾è®¡èƒ½æ˜¾è‘—æé«˜FoXå’ŒTransformerçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7115d483023217a1e5675d8c4edb71c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66cdcb70392f3a49eba2eef23bd0e870.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc12c8258b772edb4bb7df66467166af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c08028d52878a4d5930e048037b80bc5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Large-Scale-Data-Selection-for-Instruction-Tuning"><a href="#Large-Scale-Data-Selection-for-Instruction-Tuning" class="headerlink" title="Large-Scale Data Selection for Instruction Tuning"></a>Large-Scale Data Selection for Instruction Tuning</h2><p><strong>Authors:Hamish Ivison, Muru Zhang, Faeze Brahman, Pang Wei Koh, Pradeep Dasigi</strong></p>
<p>Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested â€“ all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at <a target="_blank" rel="noopener" href="https://github.com/hamishivi/automated-instruction-selection">https://github.com/hamishivi/automated-instruction-selection</a>. </p>
<blockquote>
<p>ä»è¾ƒå¤§çš„æ•°æ®æ± ä¸­é€‰å–é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®æ˜¯å¾®è°ƒè¯­è¨€æ¨¡å‹æ—¶çš„å…³é”®æ­¥éª¤ã€‚ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†é€šå¸¸ä¼šäº§ç”Ÿä¼˜äºåœ¨æ›´å¤§ã€æ›´å˜ˆæ‚çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚è‡ªåŠ¨æ•°æ®é€‰æ‹©æ–¹æ³•é€šå¸¸é€šè¿‡ä»å°å‹æ•°æ®æ± ä¸­é€‰æ‹©å°å‹æ•°æ®é›†ï¼ˆå¤§çº¦10kæ ·æœ¬ï¼‰æ¥è¿›è¡Œæµ‹è¯•ï¼ˆ10ä¸‡è‡³2ä¸‡æ ·æœ¬ï¼‰ã€‚ç„¶è€Œï¼Œæµè¡Œçš„éƒ¨ç½²æŒ‡ä»¤å¾®è°ƒæ¨¡å‹é€šå¸¸ä¼šåœ¨æ›´å¤§çš„æ•°æ®æ± ä¸­è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨æ•°ç™¾ä¸‡è‡³æ•°åƒä¸‡ä¸ªæ ·æœ¬ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†æ•°æ®é€‰æ‹©æ–¹æ³•å¦‚ä½•é€‚åº”è¿™äº›è®¾ç½®ï¼Œä»å¤šè¾¾58ä¸‡ä¸ªæ ·æœ¬çš„æ•°æ®æ± ä¸­é€‰æ‹©äº†å¤šè¾¾25ä¸‡ä¸ªæ ·æœ¬ï¼Œå¹¶åœ¨7ä¸ªä¸åŒçš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼Œè®¸å¤šæœ€è¿‘æå‡ºçš„æ–¹æ³•åœ¨æ­¤è®¾ç½®ä¸­çš„è¡¨ç°ä¸å¦‚éšæœºé€‰æ‹©ï¼ˆåŒæ—¶ä½¿ç”¨äº†æ›´å¤šçš„è®¡ç®—èµ„æºï¼‰ï¼Œå¹¶ä¸”åœ¨ç»™å®šçš„æ›´å¤§æ± ä¸­é€‰æ‹©æ•°æ®æ—¶æ€§èƒ½ç”šè‡³ä¸‹é™ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ä¸€ç§åŸºäºè¡¨ç¤ºçš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼ˆRDS+ï¼‰çš„å˜ä½“é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒLMéšè—çŠ¶æ€çš„åŠ æƒå¹³å‡æ± æ–¹æ³•è¡¨ç°æœ€å¥½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°å¼ºè°ƒåº”æ›´ä»”ç»†åœ°æ£€æŸ¥æå‡ºçš„è‡ªåŠ¨åŒ–é€‰æ‹©æ–¹æ³•çš„æ‰©å±•å±æ€§ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/hamishivi/automated-instruction-selection%E4%B8%8A%E3%80%82">https://github.com/hamishivi/automated-instruction-selectionä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01807v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä»å¤§å‹æ•°æ®æ± ä¸­é€‰å–é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®æ˜¯å¾®è°ƒè¯­è¨€æ¨¡å‹æ—¶çš„å…³é”®æ­¥éª¤ï¼Œç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†äº§ç”Ÿçš„æ¨¡å‹æ€§èƒ½é€šå¸¸ä¼˜äºåœ¨æ›´å¤§ã€æ›´å˜ˆæ‚çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†æ•°æ®é€‰æ‹©æ–¹æ³•å¦‚ä½•åœ¨æ›´å¤§è§„æ¨¡è®¾ç½®ä¸­è¿›è¡Œæ‰©å±•ï¼Œä»å¤šè¾¾58ä¸‡ä¸ªæ ·æœ¬çš„æ•°æ®æ± ä¸­é€‰å–å¤šè¾¾25ä¸‡ä¸ªæ ·æœ¬ï¼Œå¹¶åœ¨ä¸åŒçš„7é¡¹ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¸å¤šæœ€æ–°æå‡ºçš„æ–¹æ³•åœ¨è¿™ç§ç¯å¢ƒä¸‹ä¸åŠéšæœºé€‰æ‹©ï¼ˆåŒæ—¶ä½¿ç”¨æ›´å¤šè®¡ç®—èµ„æºï¼‰ï¼Œä¸”åœ¨é¢å¯¹æ›´å¤§çš„æ•°æ®æ± æ—¶æ€§èƒ½ç”šè‡³ä¸‹é™ã€‚ç„¶è€Œï¼Œä¸€ç§åŸºäºè¡¨ç¤ºçš„æ•°æ®é€‰æ‹©å˜ä½“ï¼ˆRDS+ï¼‰åœ¨æµ‹è¯•çš„æ‰€æœ‰è®¾ç½®ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®ƒä½¿ç”¨é¢„è®­ç»ƒLMéšè—çŠ¶æ€çš„åŠ æƒå‡å€¼æ± æŠ€æœ¯ï¼Œä¸”æ›´ä¸ºè®¡ç®—é«˜æ•ˆã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†åº”æ›´å¯†åˆ‡åœ°æ£€æŸ¥æè®®çš„è‡ªåŠ¨é€‰æ‹©æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€‰å–é«˜è´¨é‡è®­ç»ƒæ•°æ®å¯¹äºå¾®è°ƒè¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼Œç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†èƒ½å¤Ÿäº§ç”Ÿæ€§èƒ½æ›´ä¼˜çš„æ¨¡å‹ã€‚</li>
<li>ç›®å‰æ•°æ®é€‰æ‹©æ–¹æ³•åœ¨é¢å¯¹å¤§è§„æ¨¡æ•°æ®é›†æ—¶çš„è¡¨ç°éœ€è¦æ·±å…¥ç ”ç©¶ã€‚</li>
<li>è®¸å¤šä¼ ç»Ÿæ•°æ®é€‰æ‹©æ–¹æ³•åœ¨é¢å¯¹æ›´å¤§æ•°æ®æ± æ—¶æ€§èƒ½ä¸‹é™ï¼Œç”šè‡³ä¸åŠéšæœºé€‰æ‹©ã€‚</li>
<li>åŸºäºè¡¨ç¤ºçš„æ•°æ®é€‰æ‹©å˜ä½“ï¼ˆRDS+ï¼‰è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ›´ä¸ºè®¡ç®—é«˜æ•ˆã€‚</li>
<li>RDS+ä½¿ç”¨é¢„è®­ç»ƒLMéšè—çŠ¶æ€çš„åŠ æƒå‡å€¼æ± æŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶çš„å‘ç°å¼ºè°ƒäº†éœ€è¦æ›´å¯†åˆ‡åœ°æ£€æŸ¥æè®®çš„è‡ªåŠ¨é€‰æ‹©æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69d6cc1887c3b4ef0fd188472bb8ea8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-853e45752b1f5c1452cc720c660fed74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49126c783d625fbbd48290b9fde9936f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ade18eb4efe2647d0bf8c814fa9428.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MAPS-Motivation-Aware-Personalized-Search-via-LLM-Driven-Consultation-Alignment"><a href="#MAPS-Motivation-Aware-Personalized-Search-via-LLM-Driven-Consultation-Alignment" class="headerlink" title="MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation   Alignment"></a>MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation   Alignment</h2><p><strong>Authors:Weicong Qin, Yi Xu, Weijie Yu, Chenglei Shen, Ming He, Jianping Fan, Xiao Zhang, Jun Xu</strong></p>
<p>Personalized product search aims to retrieve and rank items that match usersâ€™ preferences and search intent. Despite their effectiveness, existing approaches typically assume that usersâ€™ query fully captures their real motivation. However, our analysis of a real-world e-commerce platform reveals that users often engage in relevant consultations before searching, indicating they refine intents through consultations based on motivation and need. The implied motivation in consultations is a key enhancing factor for personalized search. This unexplored area comes with new challenges including aligning contextual motivations with concise queries, bridging the category-text gap, and filtering noise within sequence history. To address these, we propose a Motivation-Aware Personalized Search (MAPS) method. It embeds queries and consultations into a unified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE) to prioritize critical semantics, and introduces dual alignment: (1) contrastive learning aligns consultations, reviews, and product features; (2) bidirectional attention integrates motivation-aware embeddings with user preferences. Extensive experiments on real and synthetic data show MAPS outperforms existing methods in both retrieval and ranking tasks. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–äº§å“æœç´¢æ—¨åœ¨æ£€ç´¢å’Œæ’åºä¸ç”¨æˆ·åå¥½å’Œæœç´¢æ„å›¾ç›¸åŒ¹é…çš„é¡¹ç›®ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•æœ‰æ•ˆï¼Œä½†å®ƒä»¬é€šå¸¸å‡è®¾ç”¨æˆ·çš„æŸ¥è¯¢å®Œå…¨æ•æ‰äº†ä»–ä»¬çš„çœŸå®åŠ¨æœºã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯¹ä¸€ä¸ªçœŸå®ç”µå­å•†åŠ¡å¹³å°çš„åˆ†æè¡¨æ˜ï¼Œç”¨æˆ·åœ¨æœç´¢ä¹‹å‰ç»å¸¸è¿›è¡Œç›¸å…³çš„å’¨è¯¢ï¼Œè¿™è¡¨æ˜ä»–ä»¬é€šè¿‡åŸºäºåŠ¨æœºå’Œéœ€æ±‚çš„å’¨è¯¢æ¥ç»†åŒ–æ„å›¾ã€‚å’¨è¯¢ä¸­çš„éšå«åŠ¨æœºæ˜¯ä¸ªæ€§åŒ–æœç´¢çš„å…³é”®å¢å¼ºå› ç´ ã€‚è¿™ä¸€æœªè¢«æ¢ç´¢çš„é¢†åŸŸå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å°†ä¸Šä¸‹æ–‡åŠ¨æœºä¸ç®€æ´æŸ¥è¯¢å¯¹é½ã€å¼¥åˆç±»åˆ«æ–‡æœ¬å·®è·ä»¥åŠè¿‡æ»¤åºåˆ—å†å²ä¸­çš„å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æœºæ„ŸçŸ¥ä¸ªæ€§åŒ–æœç´¢ï¼ˆMAPSï¼‰æ–¹æ³•ã€‚å®ƒé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å°†æŸ¥è¯¢å’Œå’¨è¯¢åµŒå…¥åˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ï¼Œåˆ©ç”¨æ³¨æ„åŠ›ä¸“å®¶æ··åˆä½“ï¼ˆMoAEï¼‰æ¥ä¼˜å…ˆå¤„ç†å…³é”®è¯­ä¹‰ï¼Œå¹¶å¼•å…¥åŒé‡å¯¹é½ï¼šï¼ˆ1ï¼‰å¯¹æ¯”å­¦ä¹ å¯¹é½å’¨è¯¢ã€è¯„è®ºå’Œäº§å“ç‰¹æ€§ï¼›ï¼ˆ2ï¼‰åŒå‘æ³¨æ„åŠ›å°†åŠ¨æœºæ„ŸçŸ¥åµŒå…¥ä¸ç”¨æˆ·åå¥½ç›¸ç»“åˆã€‚åœ¨çœŸå®å’Œåˆæˆæ•°æ®ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMAPSåœ¨æ£€ç´¢å’Œæ’åºä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01711v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    ä¸ªæ€§åŒ–äº§å“æœç´¢æ—¨åœ¨æ ¹æ®ç”¨æˆ·åå¥½å’Œæœç´¢æ„å›¾æ£€ç´¢å’Œæ’åºåŒ¹é…é¡¹ç›®ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸å‡å®šç”¨æˆ·æŸ¥è¯¢èƒ½å¤Ÿå®Œå…¨åæ˜ ä»–ä»¬çš„çœŸå®åŠ¨æœºï¼Œè¿™ä¸å®é™…ä¸ç¬¦ã€‚é€šè¿‡åˆ†æçœŸå®ç”µå•†å¹³å°æ•°æ®å‘ç°ï¼Œç”¨æˆ·åœ¨æœç´¢å‰ä¼šè¿›è¡Œç›¸å…³çš„å’¨è¯¢ï¼Œé€šè¿‡å’¨è¯¢åŸºäºåŠ¨æœºå’Œéœ€æ±‚æ¥ä¼˜åŒ–æ„å›¾ã€‚å’¨è¯¢ä¸­çš„éšå«åŠ¨æœºæ˜¯æå‡ä¸ªæ€§åŒ–æœç´¢çš„å…³é”®è¦ç´ ã€‚è¿™ä¸€æœªè¢«æ¢ç´¢çš„é¢†åŸŸå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å°†æƒ…å¢ƒåŠ¨æœºä¸ç®€æ´æŸ¥è¯¢å¯¹é½ã€ç¼©å°ç±»åˆ«ä¸æ–‡æœ¬ä¹‹é—´çš„å·®è·ä»¥åŠè¿‡æ»¤åºåˆ—å†å²ä¸­çš„å™ªå£°ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŠ¨æœºçš„ä¸ªæ€§åŒ–æœç´¢ï¼ˆMAPSï¼‰æ–¹æ³•ã€‚é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å°†æŸ¥è¯¢å’Œå’¨è¯¢åµŒå…¥åˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ï¼Œåˆ©ç”¨æ³¨æ„åŠ›ä¸“å®¶æ··åˆï¼ˆMoAEï¼‰æ¥ä¼˜å…ˆå¤„ç†å…³é”®è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å¼•å…¥åŒé‡å¯¹é½ï¼šï¼ˆ1ï¼‰å¯¹æ¯”å­¦ä¹ å¯¹é½å’¨è¯¢ã€è¯„è®ºå’Œäº§å“ç‰¹å¾ï¼›ï¼ˆ2ï¼‰åŒå‘æ³¨æ„åŠ›æ•´åˆåŸºäºåŠ¨æœºçš„åµŒå…¥å‘é‡å’Œç”¨æˆ·åå¥½ã€‚åœ¨çœŸå®å’Œåˆæˆæ•°æ®ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMAPSåœ¨æ£€ç´¢å’Œæ’åºä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¸ªæ€§åŒ–äº§å“æœç´¢æ—¨åœ¨æ ¹æ®ç”¨æˆ·åå¥½å’Œæœç´¢æ„å›¾åŒ¹é…å¹¶æ’åºäº§å“ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾ç”¨æˆ·æŸ¥è¯¢å¯ä»¥å®Œå…¨åæ˜ å…¶çœŸå®åŠ¨æœºï¼Œä½†è¿™å¹¶ä¸æ€»æ˜¯å‡†ç¡®ã€‚</li>
<li>ç”¨æˆ·åœ¨æœç´¢å‰ä¼šè¿›è¡Œç›¸å…³çš„å’¨è¯¢ï¼Œä»¥ä¼˜åŒ–åŸºäºåŠ¨æœºå’Œéœ€æ±‚çš„æ„å›¾ã€‚</li>
<li>å’¨è¯¢ä¸­çš„éšå«åŠ¨æœºæ˜¯æå‡ä¸ªæ€§åŒ–æœç´¢æ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
<li>è¯¥é¢†åŸŸé¢ä¸´å°†æƒ…å¢ƒåŠ¨æœºä¸ç®€æ´æŸ¥è¯¢å¯¹é½ã€ç¼©å°ç±»åˆ«ä¸æ–‡æœ¬é—´çš„å·®è·ä»¥åŠè¿‡æ»¤åºåˆ—å†å²ä¸­çš„å™ªå£°ç­‰æŒ‘æˆ˜ã€‚</li>
<li>MAPSæ–¹æ³•é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å°†æŸ¥è¯¢å’Œå’¨è¯¢åµŒå…¥ç»Ÿä¸€è¯­ä¹‰ç©ºé—´ï¼Œå¹¶å¼•å…¥åŒé‡å¯¹é½æœºåˆ¶æ¥æå‡æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMAPSåœ¨æ£€ç´¢å’Œæ’åºä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c24b03e580dab1511c49d766bfd0411.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9772c3816b2f0ff3bad594669bf16608.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08dc533aedf49bc08b0bca8a3e187d2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76dda317671b256efceb49e072d673a5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Llama-3-1-Sherkala-8B-Chat-An-Open-Large-Language-Model-for-Kazakh"><a href="#Llama-3-1-Sherkala-8B-Chat-An-Open-Large-Language-Model-for-Kazakh" class="headerlink" title="Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh"></a>Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh</h2><p><strong>Authors:Fajri Koto, Rituraj Joshi, Nurdaulet Mukhituly, Yuxia Wang, Zhuohan Xie, Rahul Pal, Daniil Orel, Parvez Mullah, Diana Turmakhan, Maiya Goloburda, Mohammed Kamran, Samujjwal Ghosh, Bokang Jia, Jonibek Mansurov, Mukhammed Togmanov, Debopriyo Banerjee, Nurkhan Laiyk, Akhmed Sakip, Xudong Han, Ekaterina Kochmar, Alham Fikri Aji, Aaryamonvikram Singh, Alok Anil Jadhav, Satheesh Katipomu, Samta Kamboj, Monojit Choudhury, Gurpreet Gosal, Gokul Ramakrishnan, Biswajit Mishra, Sarath Chandran, Avraham Sheinin, Natalia Vassilieva, Neha Sengupta, Larry Murray, Preslav Nakov</strong></p>
<p>Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8 billion parameters, it demonstrates strong knowledge and reasoning abilities in Kazakh, significantly outperforming existing open Kazakh and multilingual models of similar scale while achieving competitive performance in English. We release Sherkala-Chat (8B) as an open-weight instruction-tuned model and provide a detailed overview of its training, fine-tuning, safety alignment, and evaluation, aiming to advance research and support diverse real-world applications. </p>
<blockquote>
<p>Llama-3.1-Sherkala-8B-Chatï¼Œç®€ç§°Sherkala-Chatï¼ˆ8Bï¼‰ï¼Œæ˜¯ä¸€æ¬¾ç”¨äºå“ˆè¨å…‹è¯­çš„æœ€æ–°æŒ‡ä»¤è°ƒæ•´å‹å¼€æºç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚Sherkala-Chatï¼ˆ8Bï¼‰æ—¨åœ¨æé«˜å“ˆè¨å…‹è¯­ä½¿ç”¨è€…å¯¹LLMè¿›æ­¥çš„åŒ…å®¹æ€§ã€‚è¯¥æ¨¡å‹æ”¹ç¼–è‡ªLLaMA-3.1-8Bæ¨¡å‹ï¼Œç»è¿‡å“ˆè¨å…‹è¯­ã€è‹±è¯­ã€ä¿„è¯­å’ŒåœŸè€³å…¶è¯­å…±453äº¿è¯å…ƒçš„è®­ç»ƒã€‚æ‹¥æœ‰8äº¿å‚æ•°ï¼Œåœ¨å“ˆè¨å…‹è¯­æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨ç±»ä¼¼è§„æ¨¡çš„å…¬å¼€å“ˆè¨å…‹è¯­å’Œå¤šè¯­ç§æ¨¡å‹ä¸­è¡¨ç°å“è¶Šï¼ŒåŒæ—¶åœ¨è‹±è¯­æ–¹é¢ä¹Ÿæœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬å‘å¸ƒå¼€æºæŒ‡ä»¤è°ƒæ•´å‹æ¨¡å‹Sherkala-Chatï¼ˆ8Bï¼‰ï¼Œå¹¶å¯¹å…¶è®­ç»ƒã€å¾®è°ƒã€å®‰å…¨å¯¹é½å’Œè¯„ä¼°ç­‰æ–¹é¢æä¾›è¯¦ç»†ä»‹ç»ï¼Œæ—¨åœ¨æ¨åŠ¨ç ”ç©¶å¹¶æ”¯æŒå¤šæ ·åŒ–çš„å®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01493v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<pre><code>åŸºäºLLaMA-3.1-8Bæ¨¡å‹çš„Sherkala-Chatï¼ˆ8Bï¼‰æ˜¯ä¸€æ¬¾é’ˆå¯¹å“ˆè¨å…‹è¯­çš„å…ˆè¿›æŒ‡ä»¤è°ƒæ•´å¼€æºç”Ÿæˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ¨¡å‹æ—¨åœ¨æé«˜å“ˆè¨å…‹è¯­äººå£«å¯¹LLMè¿›å±•çš„åŒ…å®¹æ€§ã€‚ç»è¿‡åœ¨å“ˆè¨å…‹è¯­ã€è‹±è¯­ã€ä¿„è¯­å’ŒåœŸè€³å…¶è¯­å…±è®¡45.3Bä¸ªä»£å¸çš„è®­ç»ƒï¼Œæ‹¥æœ‰8äº¿å‚æ•°çš„Sherkala-Chatï¼ˆ8Bï¼‰å±•ç°äº†å¼ºå¤§çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨å“ˆè¨å…‹è¯­ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç±»ä¼¼è§„æ¨¡çš„å¼€æºå“ˆè¨å…‹è¯­å’Œå¤šè¯­ç§æ¨¡å‹ï¼ŒåŒæ—¶åœ¨è‹±è¯­ä¸Šä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚æˆ‘ä»¬å‘å¸ƒå¼€æºæŒ‡ä»¤è°ƒæ•´çš„Sherkala-Chatï¼ˆ8Bï¼‰æ¨¡å‹ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†å…¶è®­ç»ƒã€å¾®è°ƒã€å®‰å…¨å¯¹é½å’Œè¯„ä¼°çš„æ¦‚å†µï¼Œæ—¨åœ¨æ¨åŠ¨ç ”ç©¶å¹¶æ”¯æŒå„ç§ç°å®ä¸–ç•Œåº”ç”¨ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sherkala-Chatï¼ˆ8Bï¼‰æ˜¯åŸºäºLLaMA-3.1-8Bæ¨¡å‹çš„å…ˆè¿›æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹æ—¨åœ¨æé«˜å“ˆè¨å…‹è¯­äººå£«å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›å±•çš„åŒ…å®¹æ€§ã€‚</li>
<li>Sherkala-Chatï¼ˆ8Bï¼‰ç»è¿‡å¤šç§è¯­è¨€çš„è®­ç»ƒï¼ŒåŒ…æ‹¬å“ˆè¨å…‹è¯­ã€è‹±è¯­ã€ä¿„è¯­å’ŒåœŸè€³å…¶è¯­ã€‚</li>
<li>æ¨¡å‹æ‹¥æœ‰8äº¿å‚æ•°ï¼Œå±•ç°å‡ºå¼ºå¤§çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨å“ˆè¨å…‹è¯­æ€§èƒ½ä¸Šï¼ŒSherkala-Chatï¼ˆ8Bï¼‰æ˜¾è‘—ä¼˜äºç°æœ‰çš„ç±»ä¼¼è§„æ¨¡çš„å¼€æºå“ˆè¨å…‹è¯­å’Œå¤šè¯­ç§æ¨¡å‹ã€‚</li>
<li>Sherkala-Chatï¼ˆ8Bï¼‰åœ¨è‹±è¯­æ€§èƒ½ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cfba592207a0d460738bcd4a2d20b455.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5926d3a7771c32f1af7cd1e650c440af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8124f74e64b7e93f9a59efae592c7717.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Neural-ODE-Transformers-Analyzing-Internal-Dynamics-and-Adaptive-Fine-tuning"><a href="#Neural-ODE-Transformers-Analyzing-Internal-Dynamics-and-Adaptive-Fine-tuning" class="headerlink" title="Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive   Fine-tuning"></a>Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive   Fine-tuning</h2><p><strong>Authors:Anh Tong, Thanh Nguyen-Tang, Dongeun Lee, Duc Nguyen, Toan Tran, David Hall, Cheongwoong Kang, Jaesik Choi</strong></p>
<p>Recent advancements in large language models (LLMs) based on transformer architectures have sparked significant interest in understanding their inner workings. In this paper, we introduce a novel approach to modeling transformer architectures using highly flexible non-autonomous neural ordinary differential equations (ODEs). Our proposed model parameterizes all weights of attention and feed-forward blocks through neural networks, expressing these weights as functions of a continuous layer index. Through spectral analysis of the modelâ€™s dynamics, we uncover an increase in eigenvalue magnitude that challenges the weight-sharing assumption prevalent in existing theoretical studies. We also leverage the Lyapunov exponent to examine token-level sensitivity, enhancing model interpretability. Our neural ODE transformer demonstrates performance comparable to or better than vanilla transformers across various configurations and datasets, while offering flexible fine-tuning capabilities that can adapt to different architectural constraints. </p>
<blockquote>
<p>åŸºäºTransformeræ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å¼•å‘äº†äººä»¬å¯¹ç†è§£å…¶å†…éƒ¨å·¥ä½œåŸç†çš„æå¤§å…´è¶£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä½¿ç”¨é«˜åº¦çµæ´»çš„éè‡ªæ²»ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰å¯¹Transformeræ¶æ„è¿›è¡Œå»ºæ¨¡çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„æ¨¡å‹é€šè¿‡ç¥ç»ç½‘ç»œå¯¹æ‰€æœ‰æ³¨æ„åŠ›æƒé‡å’Œå‰é¦ˆå—è¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶å°†è¿™äº›æƒé‡è¡¨è¾¾ä¸ºè¿ç»­å±‚ç´¢å¼•çš„å‡½æ•°ã€‚é€šè¿‡å¯¹æ¨¡å‹åŠ¨æ€ç‰¹æ€§çš„è°±åˆ†æï¼Œæˆ‘ä»¬å‘ç°ç‰¹å¾å€¼çš„å¹…åº¦å¢åŠ ï¼Œè¿™æŒ‘æˆ˜äº†ç°æœ‰ç†è®ºç ”ç©¶ä¸­æ™®éå­˜åœ¨çš„æƒé‡å…±äº«å‡è®¾ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨LyapunovæŒ‡æ•°æ¥æ£€æŸ¥tokençº§åˆ«çš„æ•æ„Ÿæ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç¥ç»ODEå˜å‹å™¨åœ¨å„ç§é…ç½®å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸æ ‡å‡†å˜å‹å™¨ç›¸å½“æˆ–æ›´å¥½ï¼ŒåŒæ—¶æä¾›çµæ´»çš„å¾®è°ƒèƒ½åŠ›ï¼Œä»¥é€‚åº”ä¸åŒçš„æ¶æ„çº¦æŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01329v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–°ç ”ç©¶ã€‚è¯¥ç ”ç©¶ä½¿ç”¨éè‡ªä¸»ç¥ç»ODEså»ºæ¨¡transformeræ¶æ„ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå‚æ•°åŒ–æ³¨æ„åŠ›ä¸Feed Forwardå—æƒé‡ã€‚è¯¥æ¨¡å‹å®ç°æƒé‡ä½œä¸ºè¿ç»­å±‚ç´¢å¼•çš„å‡½æ•°ï¼Œä¸”æå‡äº†æ¨¡å‹çš„æ€§èƒ½ä½¿å…¶ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒæ—¶å…·æœ‰çµæ´»çš„å¾®è°ƒèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>åˆ©ç”¨ç¥ç»ç½‘ç»œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰å»ºæ¨¡transformeræ¶æ„ï¼Œæå‡ºä¸€ç§æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç¥ç»ç½‘ç»œå‚æ•°åŒ–æ³¨æ„åŠ›ä¸Feed Forwardå—çš„æƒé‡ï¼Œä½¿å…¶ä½œä¸ºè¿ç»­å±‚ç´¢å¼•çš„å‡½æ•°ã€‚</li>
<li>ç ”ç©¶é€šè¿‡è°±åˆ†æå‘ç°æ¨¡å‹åŠ¨åŠ›å­¦çš„ç‰¹å¾å€¼å¹…åº¦å¢åŠ ï¼ŒæŒ‘æˆ˜äº†ç°æœ‰ç†è®ºç ”ç©¶ä¸­æ™®éå­˜åœ¨çš„æƒé‡å…±äº«å‡è®¾ã€‚</li>
<li>åˆ©ç”¨LyapunovæŒ‡æ•°ç ”ç©¶æ¨¡å‹çš„tokençº§åˆ«æ•æ„Ÿæ€§ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef1365d88d627e2d38ff9cc943af640b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3290a43df62a7651a7ac83a31603966f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc4368ba170c370b60f8f64ca30e51da.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Cancer-Type-Stage-and-Prognosis-Assessment-from-Pathology-Reports-using-LLMs"><a href="#Cancer-Type-Stage-and-Prognosis-Assessment-from-Pathology-Reports-using-LLMs" class="headerlink" title="Cancer Type, Stage and Prognosis Assessment from Pathology Reports using   LLMs"></a>Cancer Type, Stage and Prognosis Assessment from Pathology Reports using   LLMs</h2><p><strong>Authors:Rachit Saluja, Jacob Rosenthal, Yoav Artzi, David J. Pisapia, Benjamin L. Liechty, Mert R. Sabuncu</strong></p>
<p>Large Language Models (LLMs) have shown significant promise across various natural language processing tasks. However, their application in the field of pathology, particularly for extracting meaningful insights from unstructured medical texts such as pathology reports, remains underexplored and not well quantified. In this project, we leverage state-of-the-art language models, including the GPT family, Mistral models, and the open-source Llama models, to evaluate their performance in comprehensively analyzing pathology reports. Specifically, we assess their performance in cancer type identification, AJCC stage determination, and prognosis assessment, encompassing both information extraction and higher-order reasoning tasks. Based on a detailed analysis of their performance metrics in a zero-shot setting, we developed two instruction-tuned models: Path-llama3.1-8B and Path-GPT-4o-mini-FT. These models demonstrated superior performance in zero-shot cancer type identification, staging, and prognosis assessment compared to the other models evaluated. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç—…ç†å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»ç—…ç†æŠ¥å‘Šç­‰æ— ç»“æ„åŒ»å­¦æ–‡æœ¬ä¸­æå–æœ‰æ„ä¹‰è§è§£æ–¹é¢ï¼Œä»è¢«è¾ƒå°‘æ¢ç´¢ä¸”æœªå¾—åˆ°å……åˆ†é‡åŒ–ã€‚åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTç³»åˆ—ã€Mistralæ¨¡å‹å’Œå¼€æºLlamaæ¨¡å‹ï¼Œè¯„ä¼°å…¶åœ¨ç»¼åˆåˆ†æç—…ç†æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å®ƒä»¬åœ¨ç™Œç—‡ç±»å‹è¯†åˆ«ã€AJCCåˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ä¿¡æ¯æå–å’Œé«˜çº§æ¨ç†ä»»åŠ¡ã€‚åŸºäºå¯¹é›¶æ ·æœ¬è®¾ç½®ä¸‹æ€§èƒ½æŒ‡æ ‡çš„è¯¦ç»†åˆ†æï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼šPath-llama3.1-8Bå’ŒPath-GPT-4o-mini-FTã€‚è¿™äº›æ¨¡å‹åœ¨é›¶æ ·æœ¬ç™Œç—‡ç±»å‹è¯†åˆ«ã€åˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸å…¶ä»–è¯„ä¼°çš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨ç—…ç†å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨ä»ç—…ç†æŠ¥å‘Šç­‰æ— ç»“æ„åŒ»å­¦æ–‡æœ¬ä¸­æå–æœ‰æ„ä¹‰ä¿¡æ¯æ–¹é¢ï¼Œä»ç ”ç©¶ä¸è¶³ä¸”æœªå……åˆ†é‡åŒ–ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTç³»åˆ—ã€Mistralæ¨¡å‹å’Œå¼€æºLlamaæ¨¡å‹ï¼Œè¯„ä¼°å…¶åœ¨ç—…ç†æŠ¥å‘Šç»¼åˆåˆ†æä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç™Œç—‡ç±»å‹è¯†åˆ«ã€AJCCåˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡åˆ†æé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œç ”ç©¶å¼€å‘äº†ä¸¤ä¸ªæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼šPath-llama3.1-8Bå’ŒPath-GPT-4o-mini-FTã€‚è¿™äº›æ¨¡å‹åœ¨é›¶æ ·æœ¬ç™Œç—‡ç±»å‹è¯†åˆ«ã€åˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>åœ¨ç—…ç†å­¦é¢†åŸŸï¼Œå°¤å…¶æ˜¯ä»ç—…ç†æŠ¥å‘Šä¸­æå–ä¿¡æ¯ï¼ŒLLMçš„åº”ç”¨ä»ç ”ç©¶ä¸è¶³ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠå¤šç§å…ˆè¿›è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTç³»åˆ—ã€Mistralæ¨¡å‹å’ŒLlamaæ¨¡å‹ã€‚</li>
<li>è¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨ç—…ç†æŠ¥å‘Šç»¼åˆåˆ†ææ–¹é¢çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ç™Œç—‡ç±»å‹è¯†åˆ«ã€AJCCåˆ†æœŸå’Œé¢„åè¯„ä¼°ã€‚</li>
<li>é€šè¿‡é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½æŒ‡æ ‡åˆ†æï¼Œç ”å‘å‡ºä¸¤ä¸ªè¡¨ç°å“è¶Šçš„æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ã€‚</li>
<li>Path-llama3.1-8Bå’ŒPath-GPT-4o-mini-FTæ¨¡å‹åœ¨ç™Œç—‡ç±»å‹è¯†åˆ«ã€åˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f9b477ec0e225601a11bd14523441e7e.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-06\./crop_LLM/2503.01194v1/page_2_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6198bf5d8c1936749922a1a3c2334ec5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73dea0ae000f5919209434f8ecfe944.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-33f5731f87983614ccc75d058787baab.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  Federated Learning for Privacy-Preserving Feedforward Control in   Multi-Agent Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-05/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0fc8eaf8d5cded4d6217ebce5f6378e3.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-05  CUIfy the XR An Open-Source Package to Embed LLM-powered Conversational   Agents in XR
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
