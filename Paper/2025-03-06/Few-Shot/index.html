<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  MX-Font++ Mixture of Heterogeneous Aggregation Experts for Few-shot   Font Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-54d7d93040223efbb147b43c5cb5a993.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    64 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-06-æ›´æ–°"><a href="#2025-03-06-æ›´æ–°" class="headerlink" title="2025-03-06 æ›´æ–°"></a>2025-03-06 æ›´æ–°</h1><h2 id="MX-Font-Mixture-of-Heterogeneous-Aggregation-Experts-for-Few-shot-Font-Generation"><a href="#MX-Font-Mixture-of-Heterogeneous-Aggregation-Experts-for-Few-shot-Font-Generation" class="headerlink" title="MX-Font++: Mixture of Heterogeneous Aggregation Experts for Few-shot   Font Generation"></a>MX-Font++: Mixture of Heterogeneous Aggregation Experts for Few-shot   Font Generation</h2><p><strong>Authors:Weihang Wang, Duolin Sun, Jielei Zhang, Longwen Gao</strong></p>
<p>Few-shot Font Generation (FFG) aims to create new font libraries using limited reference glyphs, with crucial applications in digital accessibility and equity for low-resource languages, especially in multilingual artificial intelligence systems. Although existing methods have shown promising performance, transitioning to unseen characters in low-resource languages remains a significant challenge, especially when font glyphs vary considerably across training sets. MX-Font considers the content of a character from the perspective of a local component, employing a Mixture of Experts (MoE) approach to adaptively extract the component for better transition. However, the lack of a robust feature extractor prevents them from adequately decoupling content and style, leading to sub-optimal generation results. To alleviate these problems, we propose Heterogeneous Aggregation Experts (HAE), a powerful feature extraction expert that helps decouple content and style downstream from being able to aggregate information in channel and spatial dimensions. Additionally, we propose a novel content-style homogeneity loss to enhance the untangling. Extensive experiments on several datasets demonstrate that our MX-Font++ yields superior visual results in FFG and effectively outperforms state-of-the-art methods. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/stephensun11/MXFontpp">https://github.com/stephensun11/MXFontpp</a>. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬å­—ä½“ç”Ÿæˆï¼ˆFFGï¼‰æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„å‚è€ƒå­—å½¢åˆ›å»ºæ–°çš„å­—ä½“åº“ï¼Œè¿™åœ¨ä½èµ„æºè¯­è¨€çš„å¤šè¯­è¨€äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­å¯¹æ•°å­—æ— éšœç¢å’Œå…¬å¹³æ€§æœ‰ç€è‡³å…³é‡è¦çš„åº”ç”¨ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€ä¸­è¿‡æ¸¡åˆ°æœªè§å­—ç¬¦ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒé›†ä¸­çš„å­—ä½“å­—å½¢å˜åŒ–å¾ˆå¤§æ—¶ã€‚MX-Fontä»å­—ç¬¦å†…å®¹çš„è§’åº¦è€ƒè™‘å±€éƒ¨ç»„ä»¶ï¼Œé‡‡ç”¨ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ–¹æ³•è‡ªé€‚åº”æå–ç»„ä»¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¿‡æ¸¡ã€‚ç„¶è€Œï¼Œç¼ºä¹ç¨³å¥çš„ç‰¹å¾æå–å™¨é˜»ç¢äº†å®ƒä»¬å……åˆ†åœ°è§£è€¦å†…å®¹å’Œé£æ ¼ï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆç»“æœä¸ç†æƒ³ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Heterogeneous Aggregation Expertsï¼ˆHAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„ç‰¹å¾æå–ä¸“å®¶ï¼Œæœ‰åŠ©äºä»ä¸‹æ¸¸è§£è€¦å†…å®¹å’Œé£æ ¼ï¼Œèƒ½å¤Ÿåœ¨é€šé“å’Œç©ºé—´ç»´åº¦ä¸Šèšåˆä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„å†…å®¹é£æ ¼åŒè´¨æ€§æŸå¤±ï¼Œä»¥å¢å¼ºè§£å¼€æ•ˆæœã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MX-Font++åœ¨FFGä¸­äº§ç”Ÿäº†å“è¶Šçš„è§†è§‰ç»“æœï¼Œå¹¶æœ‰æ•ˆåœ°è¶…è¶Šäº†æœ€æ–°æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stephensun11/MXFontpp%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/stephensun11/MXFontppè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02799v1">PDF</a> 4 pages, 4 figures, accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>FFGé¢ä¸´åœ¨ä½èµ„æºè¯­è¨€ä¸­å¯¹æœªè§å­—ç¬¦è¿›è¡Œå­—ä½“ç”Ÿæˆçš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•ä¸­çš„ç¼ºé™·ï¼Œä¾‹å¦‚ç¼ºä¹ç¨³å¥çš„ç‰¹å¾æå–å™¨ï¼Œç ”ç©¶è€…æå‡ºäº†Heterogeneous Aggregation Expertsï¼ˆHAEï¼‰ä¸æ–°çš„å†…å®¹é£æ ¼åŒè´¨æ€§æŸå¤±æœºåˆ¶ï¼Œå¼ºåŒ–äº†ç‰¹å¾æå–èƒ½åŠ›å¹¶ä¼˜åŒ–äº†å­—ä½“ç”Ÿæˆæ•ˆæœã€‚å…¶ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€äºgithubã€‚MX-Font++å±•ç°å‡ºå“è¶Šçš„è§†è§‰ç”Ÿæˆæ•ˆæœï¼Œæœ‰æ•ˆè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot Font Generationï¼ˆFFGï¼‰æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„å‚è€ƒå­—ç¬¦åˆ›å»ºæ–°çš„å­—ä½“åº“ï¼Œè¿™åœ¨å¤šè¯­è¨€äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­ä¸ºä½èµ„æºè¯­è¨€çš„æ•°å­—æ— éšœç¢è®¿é—®å’Œå…¬å¹³æ€§åº”ç”¨æä¾›äº†é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚ç„¶è€Œï¼Œå¤„ç†ä½èµ„æºè¯­è¨€ä¸­çš„æœªè§å­—ç¬¦ä»æ˜¯ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f53ae70c9828fbd700bffbb185f16ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db0176418af7806cf9ee9235dbf48579.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b6da6baea54e88bf3af3f9f68eaa76d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d27f12b519b0ff97eba40160d9d473e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8510c72c3eeaf7da85fddd86c869d87.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Use-Me-Wisely-AI-Driven-Assessment-for-LLM-Prompting-Skills-Development"><a href="#Use-Me-Wisely-AI-Driven-Assessment-for-LLM-Prompting-Skills-Development" class="headerlink" title="Use Me Wisely: AI-Driven Assessment for LLM Prompting Skills Development"></a>Use Me Wisely: AI-Driven Assessment for LLM Prompting Skills Development</h2><p><strong>Authors:Dimitri Ognibene, Gregor Donabauer, Emily Theophilou, Cansu Koyuturk, Mona Yavari, Sathya Bursic, Alessia Telari, Alessia Testa, Raffaele Boiano, Davide Taibi, Davinia Hernandez-Leo, Udo Kruschwitz, Martin Ruskov</strong></p>
<p>The use of large language model (LLM)-powered chatbots, such as ChatGPT, has become popular across various domains, supporting a range of tasks and processes. However, due to the intrinsic complexity of LLMs, effective prompting is more challenging than it may seem. This highlights the need for innovative educational and support strategies that are both widely accessible and seamlessly integrated into task workflows. Yet, LLM prompting is highly task- and domain-dependent, limiting the effectiveness of generic approaches. In this study, we explore whether LLM-based methods can facilitate learning assessments by using ad-hoc guidelines and a minimal number of annotated prompt samples. Our framework transforms these guidelines into features that can be identified within learnersâ€™ prompts. Using these feature descriptions and annotated examples, we create few-shot learning detectors. We then evaluate different configurations of these detectors, testing three state-of-the-art LLMs and ensembles. We run experiments with cross-validation on a sample of original prompts, as well as tests on prompts collected from task-naive learners. Our results show how LLMs perform on feature detection. Notably, GPT- 4 demonstrates strong performance on most features, while closely related models, such as GPT-3 and GPT-3.5 Turbo (Instruct), show inconsistent behaviors in feature classification. These differences highlight the need for further research into how design choices impact feature selection and prompt detection. Our findings contribute to the fields of generative AI literacy and computer-supported learning assessment, offering valuable insights for both researchers and practitioners. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„èŠå¤©æœºå™¨äººï¼Œå¦‚ChatGPTï¼Œå·²é€æ¸æˆä¸ºå„ä¸ªé¢†åŸŸçš„çƒ­é—¨è¶‹åŠ¿ï¼Œæ”¯æŒå„ç§ä»»åŠ¡å’Œæµç¨‹ã€‚ç„¶è€Œï¼Œç”±äºLLMçš„å†…åœ¨å¤æ‚æ€§ï¼Œæœ‰æ•ˆçš„æç¤ºæ¯”çœ‹èµ·æ¥æ›´å…·æŒ‘æˆ˜æ€§ã€‚è¿™çªå‡ºè¡¨æ˜éœ€è¦åˆ›æ–°å’Œå¹¿æ³›çš„æ”¯æŒç­–ç•¥ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ä»»åŠ¡å·¥ä½œæµç¨‹ä¸­ã€‚ç„¶è€Œï¼ŒLLMçš„æç¤ºé«˜åº¦ä¾èµ–äºä»»åŠ¡å’Œé¢†åŸŸï¼Œé™åˆ¶äº†é€šç”¨æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åŸºäºLLMçš„æ–¹æ³•æ˜¯å¦å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸“é—¨çš„æŒ‡å—å’Œå°‘é‡æ³¨é‡Šçš„æç¤ºæ ·æœ¬æ¥ä¿ƒè¿›å­¦ä¹ è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†è¿™äº›æŒ‡å—è½¬åŒ–ä¸ºå¯ä»¥åœ¨å­¦ä¹ è€…æç¤ºä¸­è¯†åˆ«çš„ç‰¹å¾ã€‚ä½¿ç”¨è¿™äº›ç‰¹å¾æè¿°å’Œæ³¨é‡Šç¤ºä¾‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å°‘æ•°å­¦ä¹ æ£€æµ‹å™¨ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¿™äº›æ£€æµ‹å™¨çš„ä¸åŒé…ç½®ï¼Œæµ‹è¯•äº†ä¸‰ç§æœ€å…ˆè¿›çš„LLMå’Œé›†æˆæ–¹æ³•ã€‚æˆ‘ä»¬åœ¨åŸå§‹æç¤ºçš„æ ·æœ¬ä¸Šè¿›è¡Œäº†äº¤å‰éªŒè¯å®éªŒï¼Œå¹¶å¯¹ä»ä»»åŠ¡æ— å…³å­¦ä¹ è€…æ”¶é›†çš„æç¤ºè¿›è¡Œäº†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†LLMåœ¨ç‰¹å¾æ£€æµ‹æ–¹é¢çš„è¡¨ç°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGPT-4åœ¨å¤§å¤šæ•°ç‰¹å¾ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè€Œä¸ä¹‹å¯†åˆ‡ç›¸å…³çš„æ¨¡å‹ï¼Œå¦‚GPT-3å’ŒGPT-3.5 Turboï¼ˆInstructï¼‰ï¼Œåœ¨ç‰¹å¾åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºä¸ä¸€è‡´çš„è¡Œä¸ºã€‚è¿™äº›å·®å¼‚è¿›ä¸€æ­¥çªå‡ºäº†è®¾è®¡é€‰æ‹©å¯¹ç‰¹å¾é€‰æ‹©å’Œæç¤ºæ£€æµ‹çš„å½±å“ï¼Œéœ€è¦è¿›è¡Œæ›´å¤šçš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç´ å…»å’Œè®¡ç®—æœºæ”¯æŒçš„å­¦ä¹ è¯„ä¼°é¢†åŸŸæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¯¹ç ”ç©¶è€…å’Œå®è·µè€…éƒ½æœ‰æ‰€è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02532v1">PDF</a> Preprint accepted for Publication in Educational Technology &amp; Society   (ET&amp;S)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„èŠå¤©æœºå™¨äººï¼Œå¦‚ChatGPTï¼Œå·²å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼Œæ”¯æŒå„ç§ä»»åŠ¡ä¸æµç¨‹ã€‚ä½†LLMçš„å¤æ‚æ€§ä½¿å¾—æœ‰æ•ˆæç¤ºæ›´å…·æŒ‘æˆ˜æ€§ï¼Œéœ€è¦åˆ›æ–°ä¸å¹¿æ³›æ¥å…¥çš„æ•™è‚²å’Œæ”¯æŒç­–ç•¥ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†åŸºäºLLMçš„æ–¹æ³•æ˜¯å¦å¯ä»¥é€šè¿‡å³å¸­æŒ‡å—å’Œå°‘é‡æ ‡æ³¨æç¤ºæ ·æœ¬æ¥ä¿ƒè¿›å­¦ä¹ è¯„ä¼°ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ç§å°‘æ ·æœ¬å­¦ä¹ æ£€æµ‹å™¨æ¥æ£€æµ‹å­¦ä¹ è€…çš„æç¤ºç‰¹å¾ï¼Œå¹¶å¯¹ä¸‰ç§å°–ç«¯LLMå’Œé›†æˆé…ç½®è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºGPT-4åœ¨ç‰¹å¾æ£€æµ‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€ŒGPT-3åŠå…¶æ›´æ–°ç‰ˆæœ¬åœ¨ç‰¹å¾åˆ†ç±»ä¸Šè¡¨ç°ä¸ä¸€ã€‚è¿™äº›å·®å¼‚å¼ºè°ƒäº†è®¾è®¡é€‰æ‹©å¯¹ç‰¹å¾é€‰æ‹©å’Œæç¤ºæ£€æµ‹çš„å½±å“ï¼Œä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç´ å…»å’Œè®¡ç®—æœºæ”¯æŒçš„å­¦ä¹ è¯„ä¼°é¢†åŸŸæä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé©±åŠ¨çš„èŠå¤©æœºå™¨äººå¹¿æ³›åº”ç”¨äºå„é¢†åŸŸã€‚</li>
<li>LLMçš„å¤æ‚æ€§å¯¼è‡´æœ‰æ•ˆæç¤ºé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>éœ€è¦åˆ›æ–°ä¸å¹¿æ³›æ¥å…¥çš„æ•™è‚²å’Œæ”¯æŒç­–ç•¥ä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å³å¸­æŒ‡å—å’Œå°‘é‡æ ‡æ³¨æç¤ºæ ·æœ¬æ¢ç´¢äº†åŸºäºLLMçš„æ–¹æ³•å¯¹å­¦ä¹ è¯„ä¼°çš„ä¿ƒè¿›ã€‚</li>
<li>åˆ›å»ºäº†å°‘æ ·æœ¬å­¦ä¹ æ£€æµ‹å™¨ä»¥æ£€æµ‹å­¦ä¹ è€…çš„æç¤ºç‰¹å¾ã€‚</li>
<li>GPT-4åœ¨ç‰¹å¾æ£€æµ‹ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-200becdf7c07267be10c346b553d97c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3491f62da2a033eda94bc7f6f3f0c775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1265ef6f1e1653b2e9e489a1ae78724c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-Intrinsic-Normal-Prototypes-within-a-Single-Image-for-Universal-Anomaly-Detection"><a href="#Exploring-Intrinsic-Normal-Prototypes-within-a-Single-Image-for-Universal-Anomaly-Detection" class="headerlink" title="Exploring Intrinsic Normal Prototypes within a Single Image for   Universal Anomaly Detection"></a>Exploring Intrinsic Normal Prototypes within a Single Image for   Universal Anomaly Detection</h2><p><strong>Authors:Wei Luo, Yunkang Cao, Haiming Yao, Xiaotian Zhang, Jianan Lou, Yuqi Cheng, Weiming Shen, Wenyong Yu</strong></p>
<p>Anomaly detection (AD) is essential for industrial inspection, yet existing methods typically rely on &#96;&#96;comparingâ€™â€™ test images to normal references from a training set. However, variations in appearance and positioning often complicate the alignment of these references with the test image, limiting detection accuracy. We observe that most anomalies manifest as local variations, meaning that even within anomalous images, valuable normal information remains. We argue that this information is useful and may be more aligned with the anomalies since both the anomalies and the normal information originate from the same image. Therefore, rather than relying on external normality from the training set, we propose INP-Former, a novel method that extracts Intrinsic Normal Prototypes (INPs) directly from the test image. Specifically, we introduce the INP Extractor, which linearly combines normal tokens to represent INPs. We further propose an INP Coherence Loss to ensure INPs can faithfully represent normality for the testing image. These INPs then guide the INP-Guided Decoder to reconstruct only normal tokens, with reconstruction errors serving as anomaly scores. Additionally, we propose a Soft Mining Loss to prioritize hard-to-optimize samples during training. INP-Former achieves state-of-the-art performance in single-class, multi-class, and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a versatile and universal solution for AD. Remarkably, INP-Former also demonstrates some zero-shot AD capability. Code is available at:<a target="_blank" rel="noopener" href="https://github.com/luow23/INP-Former">https://github.com/luow23/INP-Former</a>. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰åœ¨å·¥ä¸šæ£€æµ‹ä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå°†æµ‹è¯•å›¾åƒä¸è®­ç»ƒé›†ä¸­çš„æ­£å¸¸å‚è€ƒå›¾åƒè¿›è¡Œâ€œæ¯”è¾ƒâ€ã€‚ç„¶è€Œï¼Œå¤–è§‚å’Œä½ç½®çš„å˜åŒ–ç»å¸¸ä½¿è¿™äº›å‚è€ƒå›¾åƒä¸æµ‹è¯•å›¾åƒçš„å¯¹é½å˜å¾—å¤æ‚ï¼Œä»è€Œé™åˆ¶äº†æ£€æµ‹ç²¾åº¦ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¤§å¤šæ•°å¼‚å¸¸è¡¨ç°ä¸ºå±€éƒ¨å˜åŒ–ï¼Œè¿™æ„å‘³ç€å³ä½¿åœ¨å¼‚å¸¸å›¾åƒå†…éƒ¨ï¼Œä»ç„¶æœ‰å®è´µçš„æ­£å¸¸ä¿¡æ¯ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›ä¿¡æ¯æ˜¯æœ‰ç”¨çš„ï¼Œå¹¶ä¸”å¯èƒ½ä¸å¼‚å¸¸æ›´å¯¹é½ï¼Œå› ä¸ºå¼‚å¸¸å’Œæ­£å¸¸ä¿¡æ¯éƒ½æ¥è‡ªåŒä¸€å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•INP-Formerï¼Œå®ƒç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†INPæå–å™¨ï¼Œå®ƒçº¿æ€§ç»„åˆæ­£å¸¸ä»¤ç‰Œæ¥è¡¨ç¤ºINPsã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§INPä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ç¡®ä¿INPsèƒ½å¤Ÿå¿ å®åœ°ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚è¿™äº›INPsç„¶åå¼•å¯¼INPå¼•å¯¼è§£ç å™¨ä»…é‡å»ºæ­£å¸¸ä»¤ç‰Œï¼Œé‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è½¯æŒ–æ˜æŸå¤±ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†éš¾ä»¥ä¼˜åŒ–çš„æ ·æœ¬ã€‚INP-Formeråœ¨MVTec-ADã€VisAå’ŒReal-IADçš„å•ç±»ã€å¤šç±»å’Œå°‘æ•°å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæˆä¸ºäº†ä¸€ç§é€šç”¨ä¸”ä¸‡èƒ½çš„å¼‚å¸¸æ£€æµ‹è§£å†³æ–¹æ¡ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒINP-Formerè¿˜è¡¨ç°å‡ºä¸€äº›é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/luow23/INP-Former%E3%80%82">https://github.com/luow23/INP-Formerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02424v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºINP-Formerçš„æ–°å‹å·¥ä¸šæ£€æµ‹å¼‚å¸¸æ£€æµ‹ç®—æ³•ã€‚å®ƒæ— éœ€ä¾èµ–è®­ç»ƒé›†ä¸­çš„æ­£å¸¸å›¾åƒå‚ç…§ï¼Œè€Œæ˜¯ç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…éƒ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰ã€‚é€šè¿‡å¼•å…¥INPæå–å™¨å’ŒINPä¸€è‡´æ€§æŸå¤±ï¼Œç¡®ä¿INPsèƒ½å‡†ç¡®ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚åˆ©ç”¨è¿™äº›INPså¼•å¯¼è§£ç å™¨é‡å»ºæ­£å¸¸ä»¤ç‰Œï¼Œé‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†è½¯æŒ–æ˜æŸå¤±ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†éš¾ä»¥ä¼˜åŒ–çš„æ ·æœ¬ã€‚è¯¥ç®—æ³•åœ¨MVTec-ADã€VisAå’ŒReal-IADç­‰å¤šä¸ªå•ç±»ã€å¤šç±»å’Œå°‘ä¾‹å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œå…·å¤‡é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å·¥ä¸šæ£€æµ‹å¼‚å¸¸æ£€æµ‹æ–¹æ³•å¸¸ä¾èµ–è®­ç»ƒé›†ä¸­çš„æ­£å¸¸å›¾åƒå‚ç…§ï¼Œä½†å›¾åƒå¤–è§‚å’Œä½ç½®çš„å·®å¼‚ä¼šå½±å“æ£€æµ‹ç²¾åº¦ã€‚</li>
<li>INP-Formerç®—æ³•ç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…éƒ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨æ­£å¸¸å›¾åƒã€‚</li>
<li>å¼•å…¥çš„INPæå–å™¨é€šè¿‡çº¿æ€§ç»„åˆæ­£å¸¸ä»¤ç‰Œæ¥ä»£è¡¨INPsã€‚</li>
<li>INPä¸€è‡´æ€§æŸå¤±ç¡®ä¿INPsèƒ½å‡†ç¡®ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚</li>
<li>åˆ©ç”¨INPså¼•å¯¼è§£ç å™¨é‡å»ºæ­£å¸¸ä»¤ç‰Œï¼Œé‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ã€‚</li>
<li>è½¯æŒ–æ˜æŸå¤±ä¼˜å…ˆå¤„ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„éš¾ä»¥ä¼˜åŒ–æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-092a60ce2eee879d301a8b851aca3482.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c3dce207a988575dd0c6f8bd553cb24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0417778e274a04dc1572cd2f90926389.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b5055ad4cf5f50039e7d4fb9fba1a65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ffc2acef17e69c39582b2b0e4b7e0f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e38773309b637bba1c5cb30df3dc8b46.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="X2CT-CLIP-Enable-Multi-Abnormality-Detection-in-Computed-Tomography-from-Chest-Radiography-via-Tri-Modal-Contrastive-Learning"><a href="#X2CT-CLIP-Enable-Multi-Abnormality-Detection-in-Computed-Tomography-from-Chest-Radiography-via-Tri-Modal-Contrastive-Learning" class="headerlink" title="X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography   from Chest Radiography via Tri-Modal Contrastive Learning"></a>X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography   from Chest Radiography via Tri-Modal Contrastive Learning</h2><p><strong>Authors:Jianzhong You, Yuan Gao, Sangwook Kim, Chris Mcintosh</strong></p>
<p>Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible and safer, existing CXR foundation models focus primarily on detecting diseases that are readily visible on the CXR. Recently, works have explored training disease classification models on simulated CXRs, but they remain limited to recognizing a single disease type from CT. CT foundation models have also emerged with significantly improved detection of pathologies in CT. However, the generalized application of CT-derived labels on CXR has remained illusive. In this study, we propose X2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges the modality gap between CT and CXR while reducing the computational burden of model training. Our approach is the first work to enable multi-abnormality classification in CT, using CXR, by transferring knowledge from 3D CT volumes and associated radiology reports to a CXR encoder via a carefully designed tri-modal alignment mechanism in latent space. Extensive evaluations on three multi-label CT datasets demonstrate that our method outperforms state-of-the-art baselines in cross-modal retrieval, few-shot adaptation, and external validation. These results highlight the potential of CXR, enriched with knowledge derived from CT, as a viable efficient alternative for disease detection in resource-limited settings. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯è¯Šæ–­çš„å…³é”®æˆåƒæ–¹å¼ï¼Œä½†å…¶ä¸´åºŠåº”ç”¨å—åˆ°é«˜è¾å°„æš´éœ²å’Œé•¿æ—¶é—´ç­‰å¾…ç»“æœçš„é™åˆ¶ï¼Œé˜»ç¢äº†å…¶åœ¨å¤§è§„æ¨¡ç­›æŸ¥ä¸­çš„ä½¿ç”¨ã€‚è™½ç„¶èƒ¸éƒ¨æ”¾å°„æ‘„å½±ï¼ˆCXRï¼‰æ›´å®¹æ˜“è·å–ä¸”æ›´å®‰å…¨ï¼Œä½†ç°æœ‰çš„CXRåŸºç¡€æ¨¡å‹ä¸»è¦å…³æ³¨äºæ£€æµ‹åœ¨CXRä¸Šå®¹æ˜“çœ‹åˆ°çš„ç–¾ç—…ã€‚è¿‘æœŸï¼Œä¸€äº›ç ”ç©¶å¼€å§‹æ¢ç´¢åœ¨æ¨¡æ‹Ÿçš„CXRä¸Šè¿›è¡Œç–¾ç—…åˆ†ç±»æ¨¡å‹è®­ç»ƒï¼Œä½†å®ƒä»¬ä»…é™äºä»CTä¸­è¯†åˆ«å•ä¸€ç–¾ç—…ç±»å‹ã€‚åŒæ—¶ï¼ŒCTåŸºç¡€æ¨¡å‹ä¹Ÿå·²ç»å‡ºç°ï¼Œæ˜¾è‘—æé«˜äº†åœ¨CTä¸­çš„ç—…ç†æ£€æµ‹èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†CTè¡ç”Ÿçš„æ ‡ç­¾æ³›åŒ–åˆ°CXRä¸Šä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†X2CT-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰æ¨¡æ€çŸ¥è¯†è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œå®ƒç¼©å°äº†CTå’ŒCXRä¹‹é—´çš„æ¨¡æ€å·®è·ï¼ŒåŒæ—¶é™ä½äº†æ¨¡å‹è®­ç»ƒçš„è®¡ç®—è´Ÿæ‹…ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡ä»3D CTä½“ç§¯å’Œç›¸å…³æ”¾å°„å­¦æŠ¥å‘Šè½¬ç§»åˆ°CXRç¼–ç å™¨ï¼Œä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„æ½œåœ¨ç©ºé—´ä¸‰æ¨¡æ€å¯¹é½æœºåˆ¶ï¼Œå®ç°äº†ä½¿ç”¨CXRåœ¨CTä¸­è¿›è¡Œå¤šå¼‚å¸¸åˆ†ç±»ã€‚åœ¨ä¸‰ä¸ªå¤šæ ‡ç­¾CTæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨æ¨¡æ€æ£€ç´¢ã€å°æ ·æœ¬é€‚åº”å’Œå¤–éƒ¨éªŒè¯æ–¹é¢å‡ä¼˜äºæœ€æ–°åŸºçº¿ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†ä¸°å¯ŒçŸ¥è¯†çš„CXRçš„æ½œåŠ›ï¼Œå³å°†å…¶ä½œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­ç–¾ç—…æ£€æµ‹çš„å¯è¡Œé«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02162v1">PDF</a> 11 pages, 1 figure, 5 tables</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºX2CT-CLIPçš„ä¸‰æ¨¡æ€çŸ¥è¯†è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ç¼©å°CTå’ŒCXRä¹‹é—´çš„æ¨¡æ€å·®è·ï¼ŒåŒæ—¶é™ä½æ¨¡å‹è®­ç»ƒçš„è®¡ç®—è´Ÿæ‹…ã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾å¿ƒè®¾è®¡ä¸‰æ¨¡æ€å¯¹é½æœºåˆ¶ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­å®ç°äº†ä»ä¸‰ç»´CTä½“ç§¯å’Œå…³è”çš„æ”¾å°„å­¦æŠ¥å‘Šå‘CXRç¼–ç å™¨çš„çŸ¥è¯†è¿ç§»ï¼Œèƒ½å¤Ÿåˆ©ç”¨CXRè¿›è¡Œå¤šå¼‚å¸¸æ€§CTåˆ†ç±»ã€‚åœ¨ä¸‰ä¸ªå¤šæ ‡ç­¾CTæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è·¨æ¨¡æ€æ£€ç´¢ã€å°‘æ ·æœ¬é€‚åº”å’Œå¤–éƒ¨éªŒè¯æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚è¿™è¯æ˜äº†åˆ©ç”¨CTçŸ¥è¯†å¢å¼ºçš„CXRåœ¨èµ„æºå—é™ç¯å¢ƒä¸­è¿›è¡Œç–¾ç—…æ£€æµ‹çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>X2CT-CLIPæ¡†æ¶å®ç°äº†CTå’ŒCXRä¹‹é—´çš„çŸ¥è¯†è¿ç§»ï¼Œç¼©å°äº†æ¨¡æ€å·®è·ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä¸‰æ¨¡æ€å¯¹é½æœºåˆ¶ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­æ•´åˆäº†CTã€CXRå’Œæ”¾å°„å­¦æŠ¥å‘Šçš„ä¿¡æ¯ã€‚</li>
<li>X2CT-CLIPèƒ½å¤Ÿåœ¨CXRä¸Šè¿›è¡Œå¤šå¼‚å¸¸æ€§CTåˆ†ç±»ï¼Œè¿™æ˜¯ä»¥å¾€ç ”ç©¶æœªæ›¾å®ç°çš„ã€‚</li>
<li>åœ¨å¤šä¸ªå¤šæ ‡ç­¾CTæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒX2CT-CLIPåœ¨è·¨æ¨¡æ€æ£€ç´¢ã€å°‘æ ·æœ¬é€‚åº”å’Œå¤–éƒ¨éªŒè¯æ–¹é¢å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨CTçŸ¥è¯†å¢å¼ºçš„CXRåœ¨èµ„æºå—é™ç¯å¢ƒä¸­è¿›è¡Œç–¾ç—…æ£€æµ‹çš„æ½œåŠ›ã€‚</li>
<li>CTçš„é«˜è¾å°„æš´éœ²å’Œé•¿æ—¶é—´å¤„ç†é™åˆ¶äº†å…¶åœ¨å¤§è§„æ¨¡ç­›æŸ¥ä¸­çš„åº”ç”¨ï¼Œè€ŒX2CT-CLIPæä¾›äº†ä¸€ç§åˆ©ç”¨CXRè¿›è¡Œç–¾ç—…æ£€æµ‹çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5929e79dc6d2968e1163cd01e114aa98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca5a9fd9eb9e82ba594ec13d430ba23b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fb5dcf0b66a8a6fae22c18927754faf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d730010f2b7ee6dc2a7be69f95cffd32.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Malware-Classification-from-Memory-Dumps-Using-Machine-Learning-Transformers-and-Large-Language-Models"><a href="#Malware-Classification-from-Memory-Dumps-Using-Machine-Learning-Transformers-and-Large-Language-Models" class="headerlink" title="Malware Classification from Memory Dumps Using Machine Learning,   Transformers, and Large Language Models"></a>Malware Classification from Memory Dumps Using Machine Learning,   Transformers, and Large Language Models</h2><p><strong>Authors:Areej Dweib, Montaser Tanina, Shehab Alawi, Mohammad Dyab, Huthaifa I. Ashqar</strong></p>
<p>This study investigates the performance of various classification models for a malware classification task using different feature sets and data configurations. Six models-Logistic Regression, K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Trees, Random Forest (RF), and Extreme Gradient Boosting (XGB)-were evaluated alongside two deep learning models, Recurrent Neural Networks (RNN) and Transformers, as well as the Gemini zero-shot and few-shot learning methods. Four feature sets were tested including All Features, Literature Review Features, the Top 45 Features from RF, and Down-Sampled with Top 45 Features. XGB achieved the highest accuracy of 87.42% using the Top 45 Features, outperforming all other models. RF followed closely with 87.23% accuracy on the same feature set. In contrast, deep learning models underperformed, with RNN achieving 66.71% accuracy and Transformers reaching 71.59%. Down-sampling reduced performance across all models, with XGB dropping to 81.31%. Gemini zero-shot and few-shot learning approaches showed the lowest performance, with accuracies of 40.65% and 48.65%, respectively. The results highlight the importance of feature selection in improving model performance while reducing computational complexity. Traditional models like XGB and RF demonstrated superior performance, while deep learning and few-shot methods struggled to match their accuracy. This study underscores the effectiveness of traditional machine learning models for structured datasets and provides a foundation for future research into hybrid approaches and larger datasets. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶ä¸åŒåˆ†ç±»æ¨¡å‹åœ¨æ¶æ„è½¯ä»¶åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½è¡¨ç°ï¼Œé‡‡ç”¨å¤šç§ç‰¹å¾é›†å’Œæ•°æ®é…ç½®ã€‚å…±è¯„ä¼°äº†å…­ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬é€»è¾‘å›å½’ã€Kè¿‘é‚»ï¼ˆKNNï¼‰ã€æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ï¼ˆRFï¼‰å’Œæç«¯æ¢¯åº¦æå‡ï¼ˆXGBï¼‰ï¼Œä»¥åŠä¸¤ç§æ·±åº¦å­¦ä¹ æ¨¡å‹â€”â€”å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’ŒTransformerï¼Œä»¥åŠGeminié›¶æ ·æœ¬å­¦ä¹ å’Œå°æ ·æœ¬å­¦ä¹ æ–¹æ³•ã€‚æµ‹è¯•äº†å››ä¸ªç‰¹å¾é›†ï¼ŒåŒ…æ‹¬æ‰€æœ‰ç‰¹å¾ã€æ–‡çŒ®ç»¼è¿°ç‰¹å¾ã€æ¥è‡ªéšæœºæ£®æ—çš„å‰45ä¸ªç‰¹å¾å’Œé™ç»´åä¿ç•™çš„å‰45ä¸ªç‰¹å¾ã€‚ä½¿ç”¨å‰45ä¸ªç‰¹å¾æ—¶ï¼Œæç«¯æ¢¯åº¦æå‡è¾¾åˆ°äº†æœ€é«˜ç²¾åº¦87.42%ï¼Œè¶…è¿‡äº†å…¶ä»–æ‰€æœ‰æ¨¡å‹ã€‚éšæœºæ£®æ—åœ¨åŒä¸€ç‰¹å¾é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º87.23%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œçš„å‡†ç¡®ç‡ä¸º66.71%ï¼ŒTransformerçš„å‡†ç¡®ç‡ä¸º71.59%ã€‚é™ç»´å¯¼è‡´æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½ä¸‹é™ï¼Œæç«¯æ¢¯åº¦æå‡çš„å‡†ç¡®ç‡é™è‡³81.31%ã€‚Geminié›¶æ ·æœ¬å­¦ä¹ å’Œå°æ ·æœ¬å­¦ä¹ æ–¹æ³•çš„è¡¨ç°æœ€ä½ï¼Œå‡†ç¡®ç‡åˆ†åˆ«ä¸º40.65%å’Œ48.65%ã€‚ç»“æœå¼ºè°ƒäº†ç‰¹å¾é€‰æ‹©åœ¨æé«˜æ¨¡å‹æ€§èƒ½çš„åŒæ—¶é™ä½è®¡ç®—å¤æ‚åº¦çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹å¦‚æç«¯æ¢¯åº¦æå‡å’Œéšæœºæ£®æ—è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œè€Œæ·±åº¦å­¦ä¹ å’Œå°æ ·æœ¬æ–¹æ³•åˆ™åœ¨å‡†ç¡®æ€§æ–¹é¢é‡åˆ°äº†æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶çªæ˜¾äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®é›†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„æ··åˆæ–¹æ³•å’Œå¤§å‹æ•°æ®é›†ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02144v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†ä¸åŒåˆ†ç±»æ¨¡å‹åœ¨ç‰¹å®šæ¶æ„è½¯ä»¶åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚é€šè¿‡å¯¹æ¯”å¤šç§ç‰¹å¾é›†å’Œæ•°æ®é…ç½®æ–¹å¼ï¼ŒåŒ…æ‹¬é€»è¾‘å›å½’ã€Kè¿‘é‚»ã€æ”¯æŒå‘é‡æœºã€å†³ç­–æ ‘ã€éšæœºæ£®æ—å’Œæç«¯æ¢¯åº¦æå‡ç­‰ä¼ ç»Ÿæ¨¡å‹ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œå’ŒTransformerï¼‰ä»¥åŠGeminié›¶å°„å’Œå°‘å°„å­¦ä¹ æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å‰45ä¸ªç‰¹å¾çš„æç«¯æ¢¯åº¦æå‡æ¨¡å‹æ€§èƒ½æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º87.42%ï¼Œéšæœºæ£®æ—æ¨¡å‹ç´§éšå…¶åï¼Œå‡†ç¡®ç‡ä¸º87.23%ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œçš„å‡†ç¡®ç‡ä¸º66.71%ï¼ŒTransformerçš„å‡†ç¡®ç‡ä¸º71.59%ã€‚æ­¤å¤–ï¼Œç‰¹å¾é€‰æ‹©å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®é›†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥çš„æ··åˆæ–¹æ³•å’Œå¤§æ•°æ®é›†ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¯¹æ¯”äº†å¤šç§åˆ†ç±»æ¨¡å‹åœ¨æ¶æ„è½¯ä»¶åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æç«¯æ¢¯åº¦æå‡æ¨¡å‹ä½¿ç”¨ç‰¹å®šç‰¹å¾é›†è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>éšæœºæ£®æ—æ¨¡å‹åœ¨ç›¸åŒç‰¹å¾é›†ä¸Šå‡†ç¡®ç‡æ¥è¿‘æç«¯æ¢¯åº¦æå‡æ¨¡å‹ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œå°¤å…¶æ˜¯å¾ªç¯ç¥ç»ç½‘ç»œã€‚</li>
<li>ç‰¹å¾é€‰æ‹©å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>Geminié›¶å°„å’Œå°‘å°„å­¦ä¹ æ–¹æ³•è¡¨ç°æœ€ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bafeb3edf18b5c5cf948493fa5cc5fdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17585d0d7a157783e5f6592b2beee616.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80caf815a4a69f6ba49d8f2aeec9eb3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8d6d9b4d2e201e99d26ef65514b274c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Network-Traffic-Classification-Using-Machine-Learning-Transformer-and-Large-Language-Models"><a href="#Network-Traffic-Classification-Using-Machine-Learning-Transformer-and-Large-Language-Models" class="headerlink" title="Network Traffic Classification Using Machine Learning, Transformer, and   Large Language Models"></a>Network Traffic Classification Using Machine Learning, Transformer, and   Large Language Models</h2><p><strong>Authors:Ahmad Antari, Yazan Abo-Aisheh, Jehad Shamasneh, Huthaifa I. Ashqar</strong></p>
<p>This study uses various models to address network traffic classification, categorizing traffic into web, browsing, IPSec, backup, and email. We collected a comprehensive dataset from Arbor Edge Defender (AED) devices, comprising of 30,959 observations and 19 features. Multiple models were evaluated, including Naive Bayes, Decision Tree, Random Forest, Gradient Boosting, XGBoost, Deep Neural Networks (DNN), Transformer, and two Large Language Models (LLMs) including GPT-4o and Gemini with zero- and few-shot learning. Transformer and XGBoost showed the best performance, achieving the highest accuracy of 98.95 and 97.56%, respectively. GPT-4o and Gemini showed promising results with few-shot learning, improving accuracy significantly from initial zero-shot performance. While Gemini Few-Shot and GPT-4o Few-Shot performed well in categories like Web and Email, misclassifications occurred in more complex categories like IPSec and Backup. The study highlights the importance of model selection, fine-tuning, and the balance between training data size and model complexity for achieving reliable classification results. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä½¿ç”¨å¤šç§æ¨¡å‹æ¥è§£å†³ç½‘ç»œæµé‡åˆ†ç±»é—®é¢˜ï¼Œå°†æµé‡åˆ†ä¸ºç½‘é¡µã€æµè§ˆã€IPSecã€å¤‡ä»½å’Œç”µå­é‚®ä»¶ã€‚æˆ‘ä»¬ä»Arbor Edge Defender (AED)è®¾å¤‡æ”¶é›†äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«30,959ä¸ªè§‚æµ‹å€¼å’Œ19ä¸ªç‰¹å¾ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬æœ´ç´ è´å¶æ–¯ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ¢¯åº¦æå‡ã€XGBoostã€æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ã€Transformerä»¥åŠä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒåŒ…æ‹¬GPT-4oå’ŒGeminiï¼Œå®ç°é›¶æ¬¡å’Œå°‘æ¬¡å­¦ä¹ ã€‚Transformerå’ŒXGBoostè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡åˆ†åˆ«é«˜è¾¾98.95%å’Œ97.56%ã€‚GPT-4oå’ŒGeminiåœ¨å°‘æ¬¡å­¦ä¹ ä¸Šå±•ç°å‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œå‡†ç¡®ç‡è¾ƒåˆå§‹çš„é›¶æ¬¡å­¦ä¹ æ˜¾è‘—æé«˜ã€‚è™½ç„¶Geminiå°‘æ¬¡å­¦ä¹ å’ŒGPT-4oå°‘æ¬¡å­¦ä¹ åœ¨Webå’Œç”µå­é‚®ä»¶ç­‰ç±»åˆ«ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´å¤æ‚çš„ç±»åˆ«å¦‚IPSecå’Œå¤‡ä»½ä¸­å‡ºç°äº†è¯¯åˆ†ç±»ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ¨¡å‹é€‰æ‹©ã€å¾®è°ƒä»¥åŠè®­ç»ƒæ•°æ®å¤§å°ä¸æ¨¡å‹å¤æ‚æ€§ä¹‹é—´çš„å¹³è¡¡å¯¹äºå®ç°å¯é åˆ†ç±»ç»“æœçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02141v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç½‘ç»œæµé‡åˆ†ç±»é—®é¢˜ï¼Œé‡‡ç”¨å¤šç§æ¨¡å‹å¯¹æµé‡è¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬Naive Bayesã€Decision Treeç­‰æœºå™¨å­¦ä¹ æ¨¡å‹å’ŒGPT-4oç­‰è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTransformerå’ŒXGBoostè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡åˆ†åˆ«é«˜è¾¾98.95%å’Œ97.56%ã€‚GPT-4oå’ŒGeminiåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ä¹Ÿè¡¨ç°å‡ºæ½œåŠ›ï¼Œè™½åœ¨å¤æ‚ç±»åˆ«å¦‚IPSecå’Œå¤‡ä»½ä¸­å‡ºç°è¯¯åˆ†ç±»æƒ…å†µï¼Œä½†ç›¸æ¯”é›¶æ ·æœ¬æƒ…å†µä¸‹æœ‰æ˜æ˜¾æå‡ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†æ¨¡å‹é€‰æ‹©ã€ç²¾ç»†è°ƒæ•´å’Œè®­ç»ƒæ•°æ®é‡ä¸æ¨¡å‹å¤æ‚æ€§ä¹‹é—´çš„å¹³è¡¡å¯¹å¯é åˆ†ç±»ç»“æœçš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨å¤šç§æ¨¡å‹è¿›è¡Œç½‘ç»œæµé‡åˆ†ç±»ï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ã€‚</li>
<li>Transformerå’ŒXGBoostè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡è¶…è¿‡å…¶ä»–æ¨¡å‹ã€‚</li>
<li>GPT-4oå’ŒGeminiåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹å±•ç°å‡ºæ½œåŠ›ï¼Œå‡†ç¡®ç‡æœ‰æ‰€æå‡ã€‚</li>
<li>åœ¨å¤æ‚ç±»åˆ«å¦‚IPSecå’Œå¤‡ä»½ä¸­ï¼Œä¼šå‡ºç°ä¸€å®šçš„è¯¯åˆ†ç±»æƒ…å†µã€‚</li>
<li>æ¨¡å‹é€‰æ‹©å¯¹åˆ†ç±»ç»“æœå…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>ç²¾ç»†è°ƒæ•´æ˜¯å–å¾—å¯é åˆ†ç±»ç»“æœçš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-87d508555433cc9106551319fb6b71f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-183dff6bd8954ff35c1334544fabc032.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19ffcb04c96746a4431d89905b6a2640.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e356351576ecad7a7f8315ef7702abc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-696f17b593216ec2f8be35fb966ba1ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74e8301376368994a7826ae4f3a3233c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4c3aed7036e702ede0946cfc059b51e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37cefc9e7264a58eca6d004a5b31e96c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fab96e842ee31f4a01f50c5f8f631062.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa70ea9115467b9a2e8eda4b06a05071.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HoT-Highlighted-Chain-of-Thought-for-Referencing-Supportive-Facts-from-Inputs"><a href="#HoT-Highlighted-Chain-of-Thought-for-Referencing-Supportive-Facts-from-Inputs" class="headerlink" title="HoT: Highlighted Chain of Thought for Referencing Supportive Facts from   Inputs"></a>HoT: Highlighted Chain of Thought for Referencing Supportive Facts from   Inputs</h2><p><strong>Authors:Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen</strong></p>
<p>An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªå¼±ç‚¹æ˜¯å®ƒä»¬å€¾å‘äºäº§ç”Ÿéäº‹å®æ€§çš„é™ˆè¿°ã€‚ç”±äº‹å®å’Œè™šæ„é™ˆè¿°ç»„æˆçš„å›ç­”å¯¹äººç±»éªŒè¯å¹¶å‡†ç¡®åšå‡ºå†³ç­–æ„æˆäº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œçªå‡ºé‡ç‚¹æ€ç»´é“¾æç¤ºâ€ï¼ˆHoTï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§æç¤ºLLMç”Ÿæˆå¸¦æœ‰XMLæ ‡ç­¾çš„å“åº”çš„æ–¹æ³•ï¼Œè¿™äº›æ ‡ç­¾å°†äº‹å®ä¾æ®ä¸æŸ¥è¯¢ä¸­æä¾›çš„äº‹å®ç›¸è”ç³»ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥é—®é¢˜ï¼ŒLLMä¼šé¦–å…ˆé‡æ–°æ ¼å¼åŒ–é—®é¢˜ï¼Œæ·»åŠ çªå‡ºå…³é”®äº‹å®çš„XMLæ ‡ç­¾ï¼Œç„¶åç”ŸæˆåŒ…å«ä»è¾“å…¥ä¸­å¼•ç”¨çš„é‡ç‚¹äº‹å®çš„å“åº”ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒHoTåœ¨ç®—æœ¯ã€é˜…è¯»ç†è§£åˆ°é€»è¾‘æ¨ç†ç­‰17é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæ™®é€šæ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚å½“è¦æ±‚äººç±»éªŒè¯LLMçš„å“åº”æ—¶ï¼Œé‡ç‚¹æœ‰åŠ©äºæ—¶é—´æœ‰é™çš„å‚ä¸è€…æ›´å‡†ç¡®ã€é«˜æ•ˆåœ°è¯†åˆ«LLMçš„æ­£ç¡®æ€§ã€‚ç„¶è€Œï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå½“LLMé”™è¯¯æ—¶ï¼ŒHoTå¾€å¾€ä½¿ç”¨æˆ·è®¤ä¸ºç­”æ¡ˆæ˜¯æ­£ç¡®çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02003v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼±ç‚¹ä¹‹ä¸€æ˜¯å®ƒä»¬å®¹æ˜“ç”Ÿæˆéäº‹å®æ€§çš„é™ˆè¿°ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œHighlighted Chain-of-Thought Promptingâ€ï¼ˆHoTï¼‰çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡åœ¨ç”Ÿæˆå“åº”æ—¶æ·»åŠ XMLæ ‡ç­¾æ¥å°†äº‹å®ä¸æŸ¥è¯¢ä¸­çš„äº‹å®ç›¸å…³è”ã€‚åœ¨å°‘æ•°åœºæ™¯ä¸‹ï¼ŒHoTåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚å¯¹äºäººç±»éªŒè¯è€…è€Œè¨€ï¼Œè¿™ç§é«˜äº®æ˜¾ç¤ºæœ‰åŠ©äºæ›´å¿«é€Ÿå‡†ç¡®åœ°åˆ¤æ–­LLMçš„æ­£ç¡®æ€§ï¼Œä½†æœ‰è¶£çš„æ˜¯ï¼Œå½“LLMå‡ºé”™æ—¶ï¼ŒHoTæŠ€æœ¯å¯èƒ½ä¼šè®©ç”¨æˆ·è¯¯è®¤ä¸ºç­”æ¡ˆæ˜¯æ­£ç¡®çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨ç”Ÿæˆéäº‹å®æ€§é™ˆè¿°çš„é—®é¢˜ã€‚</li>
<li>Highlighted Chain-of-Thought Promptingï¼ˆHoTï¼‰æŠ€æœ¯é€šè¿‡æ·»åŠ XMLæ ‡ç­¾æ¥å…³è”äº‹å®ä¸æŸ¥è¯¢ã€‚</li>
<li>åœ¨å°‘æ•°åœºæ™¯ä¸‹ï¼ŒHoTåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚</li>
<li>é«˜äº®æ˜¾ç¤ºæœ‰åŠ©äºäººç±»éªŒè¯è€…æ›´å¿«é€Ÿå‡†ç¡®åœ°åˆ¤æ–­LLMçš„æ­£ç¡®æ€§ã€‚</li>
<li>HoTæŠ€æœ¯èƒ½æé«˜LLMåœ¨å„æ–¹é¢çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>HoTå¯¹äºæ—¶é—´é™åˆ¶çš„ç¯å¢ƒç‰¹åˆ«æœ‰å¸®åŠ©ï¼Œèƒ½æ˜¾è‘—æé«˜åˆ¤æ–­å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-229479dc80ff970af375cbacc72d64f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3254c77e3f0591fe1cfc853b1a9ce2b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc3f97fe94229286a57072ca68b7fe7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62ebcccdbbc90128c0abb04641c5768f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2abd555d82805b0a8a39aed90a1a1284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-596c5aa2e0a70fece1ad798fa9b7c3b2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Decision-Focused-Fine-Tuning-of-Time-Series-Foundation-Models-for-Dispatchable-Feeder-Optimization"><a href="#Decision-Focused-Fine-Tuning-of-Time-Series-Foundation-Models-for-Dispatchable-Feeder-Optimization" class="headerlink" title="Decision-Focused Fine-Tuning of Time Series Foundation Models for   Dispatchable Feeder Optimization"></a>Decision-Focused Fine-Tuning of Time Series Foundation Models for   Dispatchable Feeder Optimization</h2><p><strong>Authors:Maximilian Beichter, Nils Friederich, Janik Pinter, Dorina Werling, Kaleb Phipps, Sebastian Beichter, Oliver Neumann, Ralf Mikut, Veit Hagenmeyer, Benedikt Heidrich</strong></p>
<p>Time series foundation models provide a universal solution for generating forecasts to support optimization problems in energy systems. Those foundation models are typically trained in a prediction-focused manner to maximize forecast quality. In contrast, decision-focused learning directly improves the resulting value of the forecast in downstream optimization rather than merely maximizing forecasting quality. The practical integration of forecast values into forecasting models is challenging, particularly when addressing complex applications with diverse instances, such as buildings. This becomes even more complicated when instances possess specific characteristics that require instance-specific, tailored predictions to increase the forecast value. To tackle this challenge, we use decision-focused fine-tuning within time series foundation models to offer a scalable and efficient solution for decision-focused learning applied to the dispatchable feeder optimization problem. To obtain more robust predictions for scarce building data, we use Moirai as a state-of-the-art foundation model, which offers robust and generalized results with few-shot parameter-efficient fine-tuning. Comparing the decision-focused fine-tuned Moirai with a state-of-the-art classical prediction-focused fine-tuning Morai, we observe an improvement of 9.45% in average total daily costs. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ä¸ºç”Ÿæˆé¢„æµ‹æä¾›äº†é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œä»¥æ”¯æŒèƒ½æºç³»ç»Ÿä¸­çš„ä¼˜åŒ–é—®é¢˜ã€‚è¿™äº›åŸºç¡€æ¨¡å‹é€šå¸¸é‡‡ç”¨ä»¥é¢„æµ‹ä¸ºé‡ç‚¹çš„è®­ç»ƒæ–¹å¼ï¼Œä»¥æœ€å¤§åŒ–é¢„æµ‹è´¨é‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»¥å†³ç­–ä¸ºé‡ç‚¹çš„å­¦ä¹ ç›´æ¥æé«˜äº†ä¸‹æ¸¸ä¼˜åŒ–ä¸­é¢„æµ‹ç»“æœçš„ä»·å€¼ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€å¤§åŒ–é¢„æµ‹è´¨é‡ã€‚å°†é¢„æµ‹å€¼å®é™…é›†æˆåˆ°é¢„æµ‹æ¨¡å‹ä¸­æ˜¯æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å…·æœ‰å¤šç§å®ä¾‹çš„å¤æ‚åº”ç”¨ç¨‹åºæ—¶ï¼Œå¦‚å»ºç­‘ç‰©ã€‚å½“å®ä¾‹å…·æœ‰ç‰¹å®šç‰¹å¾ï¼Œéœ€è¦é’ˆå¯¹å®ä¾‹è¿›è¡Œç‰¹å®šé¢„æµ‹ä»¥æé«˜é¢„æµ‹ä»·å€¼æ—¶ï¼Œæƒ…å†µå˜å¾—æ›´åŠ å¤æ‚ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ä¸­ä½¿ç”¨ä»¥å†³ç­–ä¸ºé‡ç‚¹çš„å¾®è°ƒï¼Œä¸ºåº”ç”¨äºå¯è°ƒåº¦é¦ˆçº¿ä¼˜åŒ–é—®é¢˜çš„ä»¥å†³ç­–ä¸ºé‡ç‚¹çš„å­¦ä¹ æä¾›å¯ä¼¸ç¼©å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è·å¾—ç¨€ç¼ºå»ºç­‘æ•°æ®çš„æ›´ç¨³å¥é¢„æµ‹ï¼Œæˆ‘ä»¬ä½¿ç”¨Moiraiä½œä¸ºæœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹ï¼Œå®ƒæä¾›äº†ç¨³å¥å’Œé€šç”¨çš„ç»“æœï¼Œå¹¶å…·å¤‡å°‘é‡çš„å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚é€šè¿‡å°†é‡ç‚¹å†³ç­–çš„å¾®è°ƒMoiraiä¸ä»¥é¢„æµ‹ä¸ºé‡ç‚¹çš„å¸¸è§„å¾®è°ƒMoiraiè¿›è¡Œå¯¹æ¯”ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¹³å‡æ€»æ—¥å¸¸æˆæœ¬æé«˜äº†9.45%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01936v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ—¶é—´åºåˆ—è¡¨æ¨¡å‹ä¸ºèƒ½æºç³»ç»Ÿçš„ä¼˜åŒ–é—®é¢˜æä¾›äº†é€šç”¨çš„é¢„æµ‹è§£å†³æ–¹æ¡ˆã€‚é€šå¸¸ï¼Œè¿™äº›åŸºç¡€æ¨¡å‹ä»¥é¢„æµ‹ä¸ºé‡ç‚¹è¿›è¡Œè®­ç»ƒï¼Œä»¥æœ€å¤§åŒ–é¢„æµ‹è´¨é‡ã€‚ç„¶è€Œï¼Œå†³ç­–é‡ç‚¹å­¦ä¹ ç›´æ¥æé«˜é¢„æµ‹åœ¨ä¸‹æ¸¸ä¼˜åŒ–ä¸­çš„ä»·å€¼ï¼Œè€Œéä»…è¿½æ±‚æœ€å¤§åŒ–é¢„æµ‹è´¨é‡ã€‚å°†é¢„æµ‹å€¼å®é™…èå…¥é¢„æµ‹æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å…·æœ‰ä¸åŒå®ä¾‹çš„å¤æ‚åº”ç”¨æ—¶ï¼Œå¦‚å»ºç­‘ç‰©ã€‚é’ˆå¯¹å…·æœ‰ç‰¹å®šç‰¹æ€§çš„å®ä¾‹ï¼Œéœ€è¦å®šåˆ¶é¢„æµ‹ä»¥æé«˜é¢„æµ‹ä»·å€¼ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å†³ç­–é‡ç‚¹å¾®è°ƒæ—¶é—´åºåˆ—è¡¨æ¨¡å‹çš„æ–¹æ³•ï¼Œä¸ºè°ƒåº¦é¦ˆçº¿ä¼˜åŒ–é—®é¢˜æä¾›å¯ä¼¸ç¼©å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºè·å–ç¨€ç¼ºå»ºç­‘æ•°æ®çš„ç¨³å¥é¢„æµ‹ç»“æœï¼Œæˆ‘ä»¬é‡‡ç”¨Moiraiè¿™ä¸€å‰æ²¿åŸºç¡€æ¨¡å‹ï¼Œå®ƒå‡­å€Ÿå°‘å‚æ•°é«˜æ•ˆå¾®è°ƒå±•ç°å‡ºç¨³å¥å’Œæ³›åŒ–çš„ç»“æœã€‚ç›¸è¾ƒäºé‡‡ç”¨ä¼ ç»Ÿé¢„æµ‹é‡ç‚¹è°ƒæ•™çš„Moiraiæ¨¡å‹ï¼Œå†³ç­–é‡ç‚¹è°ƒæ•™çš„Moiraiæ¨¡å‹å¹³å‡æ€»æ—¥å¸¸æˆæœ¬é™ä½äº†9.45%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—è¡¨æ¨¡å‹ä¸ºèƒ½æºç³»ç»Ÿçš„ä¼˜åŒ–é—®é¢˜æä¾›é€šç”¨é¢„æµ‹è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä¼ ç»Ÿçš„æ—¶é—´åºåˆ—æ¨¡å‹ä»¥é¢„æµ‹è´¨é‡ä¸ºé‡ç‚¹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å†³ç­–é‡ç‚¹å­¦ä¹ æ—¨åœ¨æé«˜é¢„æµ‹åœ¨ä¸‹æ¸¸ä¼˜åŒ–ä¸­çš„ä»·å€¼ã€‚</li>
<li>å°†é¢„æµ‹å€¼èå…¥é¢„æµ‹æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚åº”ç”¨æ—¶ã€‚</li>
<li>ä¸ºåº”å¯¹å…·æœ‰ç‰¹å®šç‰¹æ€§çš„å®ä¾‹æŒ‘æˆ˜ï¼Œéœ€è¿›è¡Œå®šåˆ¶é¢„æµ‹ã€‚</li>
<li>é‡‡ç”¨å†³ç­–é‡ç‚¹å¾®è°ƒæ—¶é—´åºåˆ—è¡¨æ¨¡å‹çš„æ–¹æ³•è§£å†³è°ƒåº¦é¦ˆçº¿ä¼˜åŒ–é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-39920c8fcf5ea284753ecceb73e270ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f583c2361635dd5ad202589d90cbf876.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DOVE-A-Large-Scale-Multi-Dimensional-Predictions-Dataset-Towards-Meaningful-LLM-Evaluation"><a href="#DOVE-A-Large-Scale-Multi-Dimensional-Predictions-Dataset-Towards-Meaningful-LLM-Evaluation" class="headerlink" title="DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards   Meaningful LLM Evaluation"></a>DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards   Meaningful LLM Evaluation</h2><p><strong>Authors:Eliya Habba, Ofir Arviv, Itay Itzhak, Yotam Perlitz, Elron Bandel, Leshem Choshen, Michal Shmueli-Scheuer, Gabriel Stanovsky</strong></p>
<p>Recent work found that LLMs are sensitive to a wide range of arbitrary prompt dimensions, including the type of delimiters, answer enumerators, instruction wording, and more. This throws into question popular single-prompt evaluation practices. We present DOVE (Dataset Of Variation Evaluation) a large-scale dataset containing prompt perturbations of various evaluation benchmarks. In contrast to previous work, we examine LLM sensitivity from an holistic perspective, and assess the joint effects of perturbations along various dimensions, resulting in thousands of perturbations per instance. We evaluate several model families against DOVE, leading to several findings, including efficient methods for choosing well-performing prompts, observing that few-shot examples reduce sensitivity, and identifying instances which are inherently hard across all perturbations. DOVE consists of more than 250M prompt perturbations and model outputs, which we make publicly available to spur a community-wide effort toward meaningful, robust, and efficient evaluation.   Browse the data, contribute, and more: <a target="_blank" rel="noopener" href="https://slab-nlp.github.io/DOVE/">https://slab-nlp.github.io/DOVE/</a> </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¤šç§ä»»æ„çš„æç¤ºç»´åº¦éƒ½éå¸¸æ•æ„Ÿï¼ŒåŒ…æ‹¬åˆ†éš”ç¬¦çš„ç±»å‹ã€ç­”æ¡ˆæšä¸¾å™¨ã€æŒ‡ä»¤æªè¾ç­‰ç­‰ã€‚è¿™è´¨ç–‘äº†æµè¡Œçš„å•ä¸€æç¤ºè¯„ä¼°æ–¹æ³•ã€‚æˆ‘ä»¬æ¨å‡ºäº†DOVEï¼ˆå˜å¼‚è¯„ä¼°æ•°æ®é›†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼ŒåŒ…å«äº†å„ç§è¯„ä¼°åŸºå‡†æµ‹è¯•çš„æç¤ºæ‰°åŠ¨ã€‚ä¸ä»¥å‰çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬ä»æ•´ä½“çš„è§’åº¦æ¥å®¡è§†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•æ„Ÿæ€§ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒç»´åº¦æ‰°åŠ¨æ‰€äº§ç”Ÿçš„è”åˆæ•ˆåº”ï¼Œå¯¼è‡´æ¯ä¸ªå®ä¾‹éƒ½æœ‰æˆåƒä¸Šä¸‡çš„æ‰°åŠ¨ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªæ¨¡å‹å®¶æ—è¿›è¡Œäº†DOVEè¯„ä¼°ï¼Œå¾—åˆ°äº†å‡ é¡¹å‘ç°ï¼ŒåŒ…æ‹¬é€‰æ‹©è¡¨ç°è‰¯å¥½çš„æç¤ºçš„æœ‰æ•ˆæ–¹æ³•ã€è§‚å¯Ÿåˆ°å°‘é‡ç¤ºä¾‹å¯ä»¥å‡å°‘æ•æ„Ÿæ€§ä»¥åŠè¯†åˆ«å‡ºæ‰€æœ‰æ‰°åŠ¨éƒ½å›ºæœ‰çš„éš¾ä»¥åº”å¯¹çš„å®ä¾‹ã€‚DOVEåŒ…å«äº†è¶…è¿‡2.5äº¿ä¸ªæç¤ºæ‰°åŠ¨å’Œæ¨¡å‹è¾“å‡ºï¼Œæˆ‘ä»¬å°†å…¶å…¬å¼€æä¾›ï¼Œä»¥æ¿€å‘ç¤¾åŒºè¿›è¡Œæœ‰æ„ä¹‰ã€ç¨³å¥å’Œé«˜æ•ˆçš„è¯„ä¼°çš„åŠªåŠ›ã€‚è¯·æµè§ˆæ•°æ®ã€åšå‡ºè´¡çŒ®ç­‰ï¼š<a target="_blank" rel="noopener" href="https://slab-nlp.github.io/DOVE/">https://slab-nlp.github.io/DOVE/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01622v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æç¤ºç»´åº¦çš„æ•æ„Ÿæ€§ç ”ç©¶è¿‘æ—¥å—åˆ°å…³æ³¨ï¼ŒåŒ…æ‹¬åˆ†éš”ç¬¦ã€ç­”æ¡ˆæšä¸¾å™¨ã€æŒ‡ä»¤è¡¨è¿°ç­‰ã€‚è¿™è´¨ç–‘äº†æµè¡Œçš„å•ä¸€æç¤ºè¯„ä¼°æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºDOVEï¼ˆå˜å¼‚æ•°æ®é›†è¯„ä¼°ï¼‰å¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«å„ç§è¯„ä¼°åŸºå‡†æµ‹è¯•çš„æç¤ºæ‰°åŠ¨ã€‚æˆ‘ä»¬å…¨é¢è€ƒå¯ŸLLMæ•æ„Ÿæ€§ï¼Œå¹¶è¯„ä¼°å„ç»´åº¦æ‰°åŠ¨çš„è”åˆæ•ˆåº”ï¼Œæ¯ä¸ªå®ä¾‹äº§ç”Ÿæ•°åƒç§æ‰°åŠ¨ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªæ¨¡å‹å®¶æ—è¿›è¡ŒDOVEè¯„ä¼°ï¼Œå‘ç°æœ‰æ•ˆé€‰æ‹©è¡¨ç°è‰¯å¥½çš„æç¤ºæ–¹æ³•ï¼Œå‘ç°å°‘é‡ç¤ºä¾‹å¯ä»¥å‡å°‘æ•æ„Ÿæ€§ï¼Œå¹¶è¯†åˆ«å‡ºæ‰€æœ‰æ‰°åŠ¨éƒ½å›ºæœ‰çš„éš¾ä»¥å®ä¾‹åŒ–çš„å®ä¾‹ã€‚DOVEåŒ…å«è¶…è¿‡2.5äº¿ä¸ªæç¤ºæ‰°åŠ¨å’Œæ¨¡å‹è¾“å‡ºï¼Œæˆ‘ä»¬å°†å…¶å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ï¼Œä»¥æ¿€å‘æœ‰æ„ä¹‰çš„ã€ç¨³å¥çš„å’Œé«˜æ•ˆçš„è¯„ä¼°çš„ç¤¾åŒºåŠªåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æç¤ºç»´åº¦æ•æ„Ÿï¼ŒåŒ…æ‹¬åˆ†éš”ç¬¦å’ŒæŒ‡ä»¤è¡¨è¿°ç­‰ã€‚</li>
<li>å•ä¸€æç¤ºè¯„ä¼°æ–¹æ³•å—åˆ°è´¨ç–‘ï¼Œéœ€è¦æ›´å…¨é¢å’Œå¤šå…ƒçš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>ä»‹ç»äº†DOVEæ•°æ®é›†ï¼ŒåŒ…å«å¤šç§è¯„ä¼°åŸºå‡†æµ‹è¯•çš„æç¤ºæ‰°åŠ¨ï¼Œå¯ä»¥å…¨é¢è¯„ä¼°LLMçš„æ•æ„Ÿæ€§ã€‚</li>
<li>DOVEæ•°æ®é›†åŒ…å«è¶…è¿‡250Mçš„æç¤ºæ‰°åŠ¨å’Œæ¨¡å‹è¾“å‡ºï¼Œä¾›å…¬ä¼—ä½¿ç”¨å’Œè´¡çŒ®ã€‚</li>
<li>é€šè¿‡DOVEè¯„ä¼°ï¼Œå‘ç°äº†ä¸€äº›æœ‰æ•ˆçš„é€‰æ‹©è¡¨ç°è‰¯å¥½çš„æç¤ºæ–¹æ³•ã€‚</li>
<li>å°‘é‡ç¤ºä¾‹å¯ä»¥å‡å°‘LLMçš„æ•æ„Ÿæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-44c3fb9f0d20d0a947a49789efd56fb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1efd94dabee52daf0bc85e6e92a3466c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46455e42ab057a2a7ec4be97a4602d67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3200c071385b02410851ff3c286bda04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3982159f510b5b729c89804de4214e6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44f8a5fe19ea28e3e858d74241322c82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d138b4d27734f0bc55cda15ae905bc85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a536713a7479daf1f8847dff61e68fa.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MFM-DA-Instance-Aware-Adaptor-and-Hierarchical-Alignment-for-Efficient-Domain-Adaptation-in-Medical-Foundation-Models"><a href="#MFM-DA-Instance-Aware-Adaptor-and-Hierarchical-Alignment-for-Efficient-Domain-Adaptation-in-Medical-Foundation-Models" class="headerlink" title="MFM-DA: Instance-Aware Adaptor and Hierarchical Alignment for Efficient   Domain Adaptation in Medical Foundation Models"></a>MFM-DA: Instance-Aware Adaptor and Hierarchical Alignment for Efficient   Domain Adaptation in Medical Foundation Models</h2><p><strong>Authors:Jia-Xuan Jiang, Wenhui Lei, Yifeng Wu, Hongtao Wu, Furong Li, Yining Xie, Xiaofan Zhang, Zhong Wang</strong></p>
<p>Medical Foundation Models (MFMs), trained on large-scale datasets, have demonstrated superior performance across various tasks. However, these models still struggle with domain gaps in practical applications. Specifically, even after fine-tuning on source-domain data, task-adapted foundation models often perform poorly in the target domain. To address this challenge, we propose a few-shot unsupervised domain adaptation (UDA) framework for MFMs, named MFM-DA, which only leverages a limited number of unlabeled target-domain images. Our approach begins by training a Denoising Diffusion Probabilistic Model (DDPM), which is then adapted to the target domain using a proposed dynamic instance-aware adaptor and a distribution direction loss, enabling the DDPM to translate source-domain images into the target domain style. The adapted images are subsequently processed through the MFM, where we introduce a designed channel-spatial alignment Low-Rank Adaptation (LoRA) to ensure effective feature alignment. Extensive experiments on optic cup and disc segmentation tasks demonstrate that MFM-DA outperforms state-of-the-art methods. Our work provides a practical solution to the domain gap issue in real-world MFM deployment. Code will be available at here. </p>
<blockquote>
<p>åŒ»ç–—åŸºç¡€æ¨¡å‹ï¼ˆMFMsï¼‰ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶é¢ä¸´é¢†åŸŸå·®è·çš„æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œå³ä½¿åœ¨æºåŸŸæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œä»»åŠ¡é€‚åº”çš„åŸºç¡€æ¨¡å‹åœ¨ç›®æ ‡åŸŸä¸­çš„è¡¨ç°å¾€å¾€å¾ˆå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸ºMFMsæå‡ºäº†ä¸€ç§å°æ ·æœ¬çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ¡†æ¶ï¼Œåä¸ºMFM-DAï¼Œå®ƒä»…åˆ©ç”¨å°‘é‡æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸå›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œç„¶åä½¿ç”¨ä¸€ä¸ªæå‡ºçš„åŠ¨æ€å®ä¾‹æ„ŸçŸ¥é€‚é…å™¨å’Œåˆ†å¸ƒæ–¹å‘æŸå¤±æ¥é€‚åº”ç›®æ ‡åŸŸï¼Œä½¿DDPMèƒ½å¤Ÿå°†æºåŸŸå›¾åƒç¿»è¯‘æˆç›®æ ‡åŸŸé£æ ¼ã€‚é€‚åº”åçš„å›¾åƒéšåé€šè¿‡MFMè¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¼•å…¥äº†ä¸€ç§è®¾è®¡çš„é€šé“ç©ºé—´å¯¹é½ä½ç§©é€‚é…ï¼ˆLoRAï¼‰ä»¥ç¡®ä¿æœ‰æ•ˆçš„ç‰¹å¾å¯¹é½ã€‚åœ¨è§†ç¥ç»æ¯å’Œè§†ç›˜åˆ†å‰²ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMFM-DAä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè§£å†³ç°å®ä¸–ç•ŒMFMéƒ¨ç½²ä¸­çš„é¢†åŸŸå·®è·é—®é¢˜æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚ä»£ç å°†åœ¨æ­¤å¤„æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00802v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒçš„åŒ»å­¦åŸºç¡€æ¨¡å‹ï¼ˆMFMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»é¢ä¸´é¢†åŸŸå·®è·çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºMFMçš„å°‘æ ·æœ¬æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ¡†æ¶ï¼Œåä¸ºMFM-DAï¼Œä»…åˆ©ç”¨å°‘é‡æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸå›¾åƒã€‚é€šè¿‡è®­ç»ƒå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œå¹¶ç»“åˆåŠ¨æ€å®ä¾‹æ„ŸçŸ¥é€‚é…å™¨å’Œåˆ†å¸ƒæ–¹å‘æŸå¤±ï¼Œä½¿DDPMèƒ½å¤Ÿå°†æºåŸŸå›¾åƒè½¬åŒ–ä¸ºç›®æ ‡åŸŸé£æ ¼ã€‚é€‚åº”åçš„å›¾åƒå†é€šè¿‡MFMå¤„ç†ï¼Œæˆ‘ä»¬å¼•å…¥é€šé“ç©ºé—´å¯¹é½çš„ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œç¡®ä¿æœ‰æ•ˆç‰¹å¾å¯¹é½ã€‚åœ¨è§†æ¯å’Œè§†ç›˜åˆ†å‰²ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMFM-DAä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè§£å†³ç°å®ä¸–ç•Œä¸­MFMéƒ¨ç½²çš„åŸŸå·®è·é—®é¢˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MFMsåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨é¢†åŸŸå·®è·é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºMFM-DAçš„å°‘æ ·æœ¬æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œç”¨äºè§£å†³MFMåœ¨ç›®æ ‡åŸŸæ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡DDPMå°†æºåŸŸå›¾åƒè½¬åŒ–ä¸ºç›®æ ‡åŸŸé£æ ¼ã€‚</li>
<li>ä½¿ç”¨åŠ¨æ€å®ä¾‹æ„ŸçŸ¥é€‚é…å™¨å’Œåˆ†å¸ƒæ–¹å‘æŸå¤±æ¥å¢å¼ºDDPMçš„é€‚åº”æ€§ã€‚</li>
<li>å¼•å…¥é€šé“ç©ºé—´å¯¹é½çš„ä½ç§©é€‚é…æŠ€æœ¯ï¼ˆLoRAï¼‰æ¥ç¡®ä¿åœ¨MFMä¸­çš„æœ‰æ•ˆç‰¹å¾å¯¹é½ã€‚</li>
<li>åœ¨è§†æ¯å’Œè§†ç›˜åˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMFM-DAçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4425774a118a559f18e716fc0c5405d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa67d2f399aa25071412b1662169171e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e41572087f7321a91c65761e2c41a2a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Dynamic-Gradient-Sparsification-Training-for-Few-Shot-Fine-tuning-of-CT-Lymph-Node-Segmentation-Foundation-Model"><a href="#Dynamic-Gradient-Sparsification-Training-for-Few-Shot-Fine-tuning-of-CT-Lymph-Node-Segmentation-Foundation-Model" class="headerlink" title="Dynamic Gradient Sparsification Training for Few-Shot Fine-tuning of CT   Lymph Node Segmentation Foundation Model"></a>Dynamic Gradient Sparsification Training for Few-Shot Fine-tuning of CT   Lymph Node Segmentation Foundation Model</h2><p><strong>Authors:Zihao Luo, Zijun Gao, Wenjun Liao, Shichuan Zhang, Guotai Wang, Xiangde Luo</strong></p>
<p>Accurate lymph node (LN) segmentation is critical in radiotherapy treatment and prognosis analysis, but is limited by the need for large annotated datasets. While deep learning-based segmentation foundation models show potential in developing high-performing models with fewer samples, their medical adaptation faces LN domain-specific prior deficiencies and inefficient few-shot fine-tuning for complex clinical practices, highlighting the necessity of an LN segmentation foundation model. In this work, we annotated 36,106 visible LNs from 3,346 publicly available head-and-neck CT scans to establish a robust LN segmentation model (nnUNetv2). Building on this, we propose Dynamic Gradient Sparsification Training (DGST), a few-shot fine-tuning approach that preserves foundational knowledge while dynamically updating the most critical parameters of the LN segmentation model with few annotations. We validate it on two publicly available LN segmentation datasets: SegRap2023 and LNQ2023. The results show that DGST outperforms existing few-shot fine-tuning methods, achieving satisfactory performance with limited labeled data. We release the dataset, models and all implementations to facilitate relevant research: <a target="_blank" rel="noopener" href="https://github.com/Zihaoluoh/LN-Seg-FM">https://github.com/Zihaoluoh/LN-Seg-FM</a>. </p>
<blockquote>
<p>å‡†ç¡®çš„æ·‹å·´ç»“ï¼ˆLNï¼‰åˆ†å‰²å¯¹äºæ”¾å°„æ²»ç–—æ²»ç–—å’Œé¢„ååˆ†æè‡³å…³é‡è¦ï¼Œä½†å—é™äºéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²åŸºç¡€æ¨¡å‹åœ¨åˆ©ç”¨è¾ƒå°‘æ ·æœ¬å¼€å‘é«˜æ€§èƒ½æ¨¡å‹æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦é€‚åº”æ–¹é¢é¢ä¸´æ·‹å·´ç»“é¢†åŸŸç‰¹å®šå…ˆéªŒçŸ¥è¯†ä¸è¶³å’Œå¤æ‚ä¸´åºŠå®è·µä¸­çš„ä½æ•ˆå°æ ·æœ¬å¾®è°ƒé—®é¢˜ï¼Œè¿™çªå‡ºäº†éœ€è¦ä¸€ç§æ·‹å·´ç»“åˆ†å‰²åŸºç¡€æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»3,346ä¸ªå…¬å¼€å¯ç”¨çš„å¤´éƒ¨å’Œé¢ˆéƒ¨CTæ‰«æä¸­æ ‡æ³¨äº†36,106ä¸ªå¯è§æ·‹å·´ç»“ï¼Œä»¥å»ºç«‹ä¸€ä¸ªç¨³å¥çš„æ·‹å·´ç»“åˆ†å‰²æ¨¡å‹ï¼ˆnnUNetv2ï¼‰ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€æ¢¯åº¦ç¨€ç–è®­ç»ƒï¼ˆDGSTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°æ ·æœ¬çš„å¾®è°ƒæ–¹æ³•ï¼Œæ—¢ä¿ç•™äº†åŸºç¡€çŸ¥è¯†ï¼Œåˆèƒ½ç”¨å°‘é‡æ³¨é‡ŠåŠ¨æ€æ›´æ–°æ·‹å·´ç»“åˆ†å‰²æ¨¡å‹ä¸­æœ€å…³é”®çš„å‚æ•°ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å¼€çš„æ·‹å·´ç»“åˆ†å‰²æ•°æ®é›†SegRap2023å’ŒLNQ2023ä¸Šå¯¹å…¶è¿›è¡Œäº†éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒDGSTåœ¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸Šå–å¾—äº†ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å°æ ·ä¾‹å¾®è°ƒæ–¹æ³•ã€‚æˆ‘ä»¬å…¬å¼€äº†æ•°æ®é›†ã€æ¨¡å‹å’Œæ‰€æœ‰å®ç°ï¼Œä»¥ä¿ƒè¿›ç›¸å…³ç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://github.com/Zihaoluoh/LN-Seg-FM%E3%80%82">https://github.com/Zihaoluoh/LN-Seg-FMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00748v1">PDF</a> 10 pages, 3 figures, 2 tables, and the lymph node segmentation   foundation model code and pretrained model are available</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†æ·‹å·´èŠ‚ç‚¹ï¼ˆLNï¼‰åˆ†å‰²åœ¨æ”¾å°„æ²»ç–—ä¸é¢„ååˆ†æä¸­çš„é‡è¦æ€§ï¼Œä½†ç”±äºéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†è€Œå—åˆ°é™åˆ¶ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å…·æœ‰åœ¨å°‘é‡æ ·æœ¬ä¸‹å¼€å‘é«˜æ€§èƒ½æ¨¡å‹çš„æ½œåŠ›ï¼Œä½†åœ¨åŒ»å­¦é€‚åº”æ–¹é¢é¢ä¸´æ·‹å·´èŠ‚ç‚¹ç‰¹å®šå…ˆéªŒçŸ¥è¯†ä¸è¶³å’Œå¤æ‚ä¸´åºŠå®è·µä¸­çš„ä½æ•ˆå°‘æ ·æœ¬å¾®è°ƒé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æ ‡æ³¨äº†36,106ä¸ªå¯è§çš„æ·‹å·´èŠ‚ç‚¹ï¼Œå»ºç«‹äº†ç¨³å¥çš„åˆ†å‰²æ¨¡å‹nnUNetv2ï¼Œå¹¶æå‡ºåŠ¨æ€æ¢¯åº¦ç¨€ç–åŒ–è®­ç»ƒï¼ˆDGSTï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å°‘æ ·æœ¬å¾®è°ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿä¿ç•™åŸºç¡€çŸ¥è¯†å¹¶åŠ¨æ€æ›´æ–°æ·‹å·´èŠ‚ç‚¹åˆ†å‰²æ¨¡å‹çš„å…³é”®å‚æ•°ã€‚åœ¨å…¬å¼€æ•°æ®é›†SegRap2023å’ŒLNQ2023ä¸Šçš„éªŒè¯ç»“æœè¡¨æ˜ï¼ŒDGSTä¼˜äºç°æœ‰çš„å°‘æ ·æœ¬å¾®è°ƒæ–¹æ³•ï¼Œåœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹å®ç°äº†ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·‹å·´èŠ‚ç‚¹ï¼ˆLNï¼‰åˆ†å‰²åœ¨æ”¾å°„æ²»ç–—ä¸é¢„ååˆ†æä¸­è‡³å…³é‡è¦ï¼Œä½†å—é™äºç¼ºä¹å¤§é‡æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬ä¸‹å…·æœ‰å¼€å‘é«˜æ€§èƒ½æ¨¡å‹çš„æ½œåŠ›ï¼Œä½†åŒ»å­¦é€‚åº”é¢ä¸´ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„ä¸è¶³å’Œå¤æ‚ä¸´åºŠå®è·µä¸­çš„ä½æ•ˆå¾®è°ƒé—®é¢˜ã€‚</li>
<li>ç ”ç©¶å»ºç«‹äº†ç¨³å¥çš„æ·‹å·´èŠ‚ç‚¹åˆ†å‰²æ¨¡å‹nnUNetv2ï¼Œé€šè¿‡æ ‡æ³¨å¤§é‡æ·‹å·´èŠ‚ç‚¹æ•°æ®ã€‚</li>
<li>æå‡ºäº†åŠ¨æ€æ¢¯åº¦ç¨€ç–åŒ–è®­ç»ƒï¼ˆDGSTï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å°‘æ ·æœ¬å¾®è°ƒç­–ç•¥ï¼Œèƒ½å¤Ÿä¿ç•™åŸºç¡€çŸ¥è¯†å¹¶åŠ¨æ€æ›´æ–°æ¨¡å‹å…³é”®å‚æ•°ã€‚</li>
<li>DGSTæ–¹æ³•åœ¨å…¬å¼€æ•°æ®é›†SegRap2023å’ŒLNQ2023ä¸Šçš„éªŒè¯ç»“æœä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶æˆæœåŒ…æ‹¬æ•°æ®é›†ã€æ¨¡å‹å’Œæ‰€æœ‰å®ç°ï¼Œå·²å…¬å¼€å‘å¸ƒä»¥ä¿ƒè¿›ç›¸å…³ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aa53adb83a141104e3c75a961f8c159f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f06d04a0f6415e4dfc0175ef641d3ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-544001f59c6526aa9e5519f49323a248.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Transformer-Based-Self-Context-Aware-Prediction-for-Few-Shot-Anomaly-Detection-in-Videos"><a href="#Transformer-Based-Self-Context-Aware-Prediction-for-Few-Shot-Anomaly-Detection-in-Videos" class="headerlink" title="Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly   Detection in Videos"></a>Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly   Detection in Videos</h2><p><strong>Authors:Gargi V. Pillai, Ashish Verma, Debashis Sen</strong></p>
<p>Anomaly detection in videos is a challenging task as anomalies in different videos are of different kinds. Therefore, a promising way to approach video anomaly detection is by learning the non-anomalous nature of the video at hand. To this end, we propose a one-class few-shot learning driven transformer based approach for anomaly detection in videos that is self-context aware. Features from the first few consecutive non-anomalous frames in a video are used to train the transformer in predicting the non-anomalous feature of the subsequent frame. This takes place under the attention of a self-context learned from the input features themselves. After the learning, given a few previous frames, the video-specific transformer is used to infer if a frame is anomalous or not by comparing the feature predicted by it with the actual. The effectiveness of the proposed method with respect to the state-of-the-art is demonstrated through qualitative and quantitative results on different standard datasets. We also study the positive effect of the self-context used in our approach. </p>
<blockquote>
<p>è§†é¢‘ä¸­çš„å¼‚å¸¸æ£€æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºä¸åŒè§†é¢‘ä¸­çš„å¼‚å¸¸æ˜¯ä¸åŒçš„ã€‚å› æ­¤ï¼Œä¸€ç§æœ‰å‰æ™¯çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•æ˜¯é€šè¿‡å­¦ä¹ ç°æœ‰è§†é¢‘çš„éå¼‚å¸¸æ€§è´¨æ¥è¯†åˆ«å¼‚å¸¸ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¸€ç±»å°‘æ ·æœ¬å­¦ä¹ çš„è‡ªæˆ‘ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è½¬æ¢å™¨æ–¹æ³•æ¥è¿›è¡Œè§†é¢‘å¼‚å¸¸æ£€æµ‹ã€‚åˆ©ç”¨è§†é¢‘ç‰‡æ®µä¸­çš„å‰å‡ å¸§è¿ç»­çš„éå¼‚å¸¸ç‰¹å¾æ¥è®­ç»ƒè½¬æ¢å™¨é¢„æµ‹åç»­å¸§çš„éå¼‚å¸¸ç‰¹å¾ã€‚è¿™æ˜¯åœ¨è¾“å…¥ç‰¹å¾æœ¬èº«çš„è‡ªæˆ‘ä¸Šä¸‹æ–‡å…³æ³¨ä¸‹è¿›è¡Œçš„ã€‚å­¦ä¹ å®Œæˆåï¼Œé€šè¿‡æ¯”è¾ƒç”±è§†é¢‘ç‰¹å®šè½¬æ¢å™¨é¢„æµ‹çš„ç‰¹å¾ä¸å®é™…ç‰¹å¾ï¼Œåˆ©ç”¨å‰å‡ å¸§å³å¯æ¨æ–­æŸä¸€å¸§æ˜¯å¦ä¸ºå¼‚å¸¸å¸§ã€‚é€šè¿‡åœ¨ä¸åŒæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®šæ€§å’Œå®šé‡ç»“æœï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•ç›¸å¯¹äºæœ€æ–°æŠ€æœ¯å‰æ²¿çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ‰€ç”¨è‡ªæˆ‘ä¸Šä¸‹æ–‡åœ¨æ–¹æ³•ä¸­çš„ç§¯æå½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00670v1">PDF</a> Copyright 2022 IEEE. Personal use of this material is permitted.   Permission from IEEE must be obtained for all other uses, in any current or   future media, including reprinting&#x2F;republishing this material for advertising   or promotional purposes, creating new collective works, for resale or   redistribution to servers or lists, or reuse of any copyrighted component of   this work in other works</p>
<p><strong>Summary</strong><br>è§†é¢‘å¼‚å¸¸æ£€æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºä¸åŒè§†é¢‘ä¸­çš„å¼‚å¸¸äº‹ä»¶ç±»å‹å„å¼‚ã€‚ä¸€ç§å¯è¡Œçš„æ–¹æ³•æ˜¯å­¦ä¹ è§†é¢‘çš„éå¼‚å¸¸ç‰¹æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¸€ç±»å°æ ·æœ¬å­¦ä¹ çš„è‡ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹Transformeræ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è§†é¢‘ä¸­çš„å‰å‡ å¸§éå¼‚å¸¸ç‰¹å¾æ¥è®­ç»ƒTransformeré¢„æµ‹åç»­å¸§çš„éå¼‚å¸¸ç‰¹å¾ã€‚ç»è¿‡è®­ç»ƒåï¼Œä½¿ç”¨é’ˆå¯¹è§†é¢‘çš„Transformeré€šè¿‡æ¯”è¾ƒå…¶é¢„æµ‹çš„ç‰¹å¾ä¸å®é™…ç‰¹å¾æ¥æ¨æ–­æŸä¸€å¸§æ˜¯å¦ä¸ºå¼‚å¸¸å¸§ã€‚æœ¬æ–‡æ‰€ææ–¹æ³•åœ¨ä¸åŒæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®šæ€§å’Œå®šé‡ç»“æœå‡éªŒè¯äº†å…¶ç›¸å¯¹äºæœ€æ–°æŠ€æœ¯å‰æ²¿çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¯¹è‡ªä¸Šä¸‹æ–‡åœ¨æ–¹æ³•ä¸­çš„ç§¯æä½œç”¨è¿›è¡Œäº†ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å¼‚å¸¸æ£€æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¸åŒè§†é¢‘ä¸­çš„å¼‚å¸¸äº‹ä»¶å¤šæ ·ã€‚</li>
<li>å­¦ä¹ è§†é¢‘çš„éå¼‚å¸¸ç‰¹æ€§æ˜¯æ£€æµ‹å¼‚å¸¸äº‹ä»¶çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä¸€ç±»å°æ ·æœ¬å­¦ä¹ çš„è‡ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹Transformeræ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨è§†é¢‘ä¸­çš„å‰å‡ å¸§éå¼‚å¸¸ç‰¹å¾æ¥è®­ç»ƒTransformeré¢„æµ‹åç»­å¸§çš„éå¼‚å¸¸ç‰¹å¾ã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒé¢„æµ‹ç‰¹å¾å’Œå®é™…ç‰¹å¾æ¥æ¨æ–­æŸä¸€å¸§æ˜¯å¦ä¸ºå¼‚å¸¸å¸§ã€‚</li>
<li>æ‰€ææ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„ç»“æœéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-54d7d93040223efbb147b43c5cb5a993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4ea6b6e1645d5a6e5611051584d101e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54e95a9d5ed44b01ee1307ab77ea04d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c693e21176ae442d8cb73dd112bacfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74a009e3399166f79b08d147b057633a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-are-Powerful-EHR-Encoders"><a href="#Large-Language-Models-are-Powerful-EHR-Encoders" class="headerlink" title="Large Language Models are Powerful EHR Encoders"></a>Large Language Models are Powerful EHR Encoders</h2><p><strong>Authors:Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild</strong></p>
<p>Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸ºä¸´åºŠé¢„æµ‹æä¾›äº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å›ºæœ‰çš„å¤æ‚æ€§å’Œå¼‚è´¨æ€§ç»™ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚åŸºäºå¤§é‡æœªæ ‡è®°çš„EHRæ•°æ®è®­ç»ƒçš„ç‰¹å®šé¢†åŸŸçš„EHRåŸºç¡€æ¨¡å‹åœ¨é¢„æµ‹ç²¾åº¦å’Œé€šç”¨æ€§æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ”¹è¿›ï¼›ç„¶è€Œï¼Œå®ƒä»¬çš„è®­ç»ƒå—åˆ°å¤šæ ·ã€é«˜è´¨é‡æ•°æ®é›†è®¿é—®å—é™ä»¥åŠç¼–ç æ ‡å‡†å’ŒåŒ»ç–—ä¿å¥å®è·µä¸ä¸€è‡´çš„åˆ¶çº¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä½¿ç”¨åŸºäºé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åµŒå…¥æ–¹æ³•ä½œä¸ºEHRç¼–ç å™¨çš„å¯èƒ½æ€§ã€‚é€šè¿‡å°†æ‚£è€…è®°å½•åºåˆ—åŒ–ä¸ºç»“æ„åŒ–Markdownæ–‡æœ¬ï¼Œå°†ä»£ç è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æè¿°ç¬¦ï¼Œæˆ‘ä»¬å……åˆ†åˆ©ç”¨äº†LLMsåœ¨å¤§é‡å…¬å…±è¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒäº§ç”Ÿçš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œç»•è¿‡äº†å¯¹ä¸“æœ‰åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸¤ç§æœ€å…ˆè¿›çš„LLMåµŒå…¥æ¨¡å‹ï¼ŒGTE-Qwen2-7B-Instructå’ŒLLM2Vec-Llama3.1-8B-Instructï¼Œåœ¨EHRSHOTåŸºå‡†æµ‹è¯•çš„15ä¸ªä¸åŒä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸Šï¼Œä¸EHRç‰¹å®šçš„åŸºç¡€æ¨¡å‹CLIMBR-T-Baseå’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºå‡†è¿›è¡Œäº†æ€§èƒ½æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„åµŒå…¥åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ç»å¸¸ä¸ä¸“ç”¨æ¨¡å‹çš„è¡¨ç°ç›¸åŒ¹é…ç”šè‡³è¶…è¿‡å®ƒä»¬ï¼Œå…¶æœ‰æ•ˆæ€§éšç€åº•å±‚LLMçš„å¤§å°å’Œå¯ç”¨ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°è€Œæé«˜ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†LLMsé‡æ–°ç”¨äºEHRç¼–ç æä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œæœ‰æ•ˆçš„ä¸´åºŠé¢„æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿå…‹æœä¼ ç»ŸEHRå»ºæ¨¡çš„é™åˆ¶ï¼Œä¿ƒè¿›æ›´äº’è”å’Œæ›´é€šç”¨çš„åŒ»ç–—ä¿å¥åº”ç”¨ç¨‹åºçš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17403v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰åœ¨ä¸´åºŠé¢„æµ‹ä¸­çš„ä¸°å¯Œæ½œåŠ›ï¼Œä»¥åŠä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶ä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åµŒå…¥æ–¹æ³•ä½œä¸ºEHRç¼–ç å™¨ï¼Œé€šè¿‡åºåˆ—åŒ–æ‚£è€…è®°å½•ä¸ºç»“æ„åŒ–Markdownæ–‡æœ¬ï¼Œå°†ä»£ç è½¬æ¢ä¸ºäººç±»å¯è¯»æè¿°ç¬¦ï¼Œåˆ©ç”¨LLMsåœ¨å¤§é‡å…¬å…±è¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒèƒ½åŠ›ï¼Œä»è€Œç»•è¿‡å¯¹ä¸“æœ‰åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ã€‚ç ”ç©¶è¯„ä¼°äº†ä¸¤ç§å…ˆè¿›çš„LLMåµŒå…¥æ¨¡å‹ï¼Œå¹¶è¯æ˜å…¶æ€§èƒ½ä¸EHRç‰¹å®šåŸºç¡€æ¨¡å‹å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºå‡†ç›¸å½“æˆ–æ›´ä¼˜ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨LLMsè¿›è¡ŒEHRç¼–ç æä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œæœ‰æ•ˆçš„ä¸´åºŠé¢„æµ‹æ–¹æ³•ï¼Œèƒ½å¤Ÿå…‹æœä¼ ç»ŸEHRå»ºæ¨¡çš„é™åˆ¶ï¼Œä¿ƒè¿›æ›´äº’é€šå’Œé€šç”¨çš„åŒ»ç–—ä¿å¥åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰åœ¨ä¸´åºŠé¢„æµ‹ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•é¢ä¸´å¤æ‚æ€§å’Œå¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç‰¹å®šé¢†åŸŸçš„EHRåŸºç¡€æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å·²æ˜¾ç¤ºå‡ºæ”¹è¿›ï¼Œä½†å­˜åœ¨æ•°æ®é›†è®¿é—®æœ‰é™å’Œç¼–ç æ ‡å‡†ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºEHRç¼–ç å™¨ï¼Œé€šè¿‡åºåˆ—åŒ–æ‚£è€…è®°å½•å¹¶åˆ©ç”¨LLMsçš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ç»•è¿‡ä¸“æœ‰åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ã€‚</li>
<li>LLMåµŒå…¥æ¨¡å‹åœ¨å¤šæ ·åŒ–çš„ä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬ä¸EHRç‰¹å®šåŸºç¡€æ¨¡å‹å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºå‡†ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>LLMæ¨¡å‹åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä¸”å…¶æœ‰æ•ˆæ€§éšç€æ¨¡å‹å¤§å°å’Œå¯ç”¨ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°è€Œå¢åŠ ã€‚</li>
<li>ä½¿ç”¨LLMsè¿›è¡ŒEHRç¼–ç æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå…‹æœä¼ ç»ŸEHRå»ºæ¨¡çš„é™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4355b0d1618c4c913e6fa57980b5ddff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfedd0a7cb65190d72a64a7503e8e23c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89591a01aaa7b4088fef629c9c365d1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61db9bbf53dcfc812bf0a801e742f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf61cdd8860ed37ba16bc3d176e4343a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction"><a href="#Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction" class="headerlink" title="Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction"></a>Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction</h2><p><strong>Authors:Nils Constantin Hellwig, Jakob Fehle, Udo Kruschwitz, Christian Wolff</strong></p>
<p>Aspect sentiment quadruple prediction (ASQP) facilitates a detailed understanding of opinions expressed in a text by identifying the opinion term, aspect term, aspect category and sentiment polarity for each opinion. However, annotating a full set of training examples to fine-tune models for ASQP is a resource-intensive process. In this study, we explore the capabilities of large language models (LLMs) for zero- and few-shot learning on the ASQP task across five diverse datasets. We report F1 scores slightly below those obtained with state-of-the-art fine-tuned models but exceeding previously reported zero- and few-shot performance. In the 40-shot setting on the Rest16 restaurant domain dataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the best-performing fine-tuned method MVP. Additionally, we report the performance of LLMs in target aspect sentiment detection (TASD), where the F1 scores were also close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot setting, compared to 72.76 with MVP. While human annotators remain essential for achieving optimal performance, LLMs can reduce the need for extensive manual annotation in ASQP tasks. </p>
<blockquote>
<p>é¢å‘æ–¹é¢çš„æƒ…æ„Ÿå››é‡é¢„æµ‹ï¼ˆASQPï¼‰èƒ½å¤Ÿé€šè¿‡è¯†åˆ«æ¯ä¸ªæ„è§çš„è§‚ç‚¹è¯ã€æ–¹é¢è¯ã€æ–¹é¢ç±»åˆ«å’Œæƒ…æ„Ÿææ€§ï¼Œä»è€Œä¿ƒè¿›å¯¹æ–‡æœ¬ä¸­æ‰€è¡¨è¾¾æ„è§çš„æ·±å…¥ç†è§£ã€‚ç„¶è€Œï¼Œå¯¹å…¨å¥—è®­ç»ƒç¤ºä¾‹è¿›è¡Œæ ‡æ³¨ä»¥å¾®è°ƒASQPæ¨¡å‹æ˜¯ä¸€ä¸ªèµ„æºå¯†é›†å‹çš„æµç¨‹ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ åœ¨ASQPä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚æˆ‘ä»¬æŠ¥å‘Šçš„F1åˆ†æ•°ç•¥ä½äºä½¿ç”¨æœ€æ–°å¾®è°ƒæ¨¡å‹æ‰€è·å¾—çš„åˆ†æ•°ï¼Œä½†è¶…è¿‡äº†ä¹‹å‰æŠ¥å‘Šçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚åœ¨Rest16é¤å…é¢†åŸŸæ•°æ®é›†çš„40ä¸ªæ ·æœ¬è®¾ç½®ä¸­ï¼Œä¸è¡¨ç°æœ€ä½³çš„MVPå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒLLMså–å¾—äº†F1åˆ†æ•°ä¸º52.46çš„ä¸šç»©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†LLMåœ¨ç›®æ ‡æ–¹é¢æƒ…æ„Ÿæ£€æµ‹ï¼ˆTASDï¼‰æ–¹é¢çš„è¡¨ç°ï¼Œå…¶F1åˆ†æ•°ä¹Ÿæ¥è¿‘ç»è¿‡å¾®è°ƒçš„æ¨¡å‹ã€‚åœ¨Rest16çš„40ä¸ªæ ·æœ¬è®¾ç½®ä¸­ï¼Œä¸MVPçš„72.76ç›¸æ¯”ï¼ŒLLMså–å¾—äº†66.03çš„F1åˆ†æ•°ã€‚è™½ç„¶äººç±»æ³¨é‡Šè€…å¯¹äºå®ç°æœ€ä½³æ€§èƒ½ä»ç„¶è‡³å…³é‡è¦ï¼Œä½†LLMå¯ä»¥å‡å°‘ASQPä»»åŠ¡ä¸­å¯¹å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13044v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å‘æ–¹é¢çš„æƒ…æ„Ÿå››é‡é¢„æµ‹ï¼ˆASQPï¼‰ä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šï¼ŒLLMsçš„æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œè™½ç„¶ç•¥ä½äºç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹ï¼Œä½†åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ä¼˜äºå…ˆå‰æŠ¥å‘Šã€‚åœ¨Rest16é¤å…é¢†åŸŸæ•°æ®é›†çš„40ä¸ªæ ·æœ¬ä¸­ï¼ŒLLMsçš„F1åˆ†æ•°è¾¾åˆ°52.46%ï¼Œè€Œæœ€ä½³ç²¾ç»†è°ƒæ•´æ–¹æ³•MVPçš„F1åˆ†æ•°ä¸º60.39ã€‚æ­¤å¤–ï¼ŒLLMsåœ¨ç›®æ ‡æ–¹é¢æƒ…æ„Ÿæ£€æµ‹ï¼ˆTASDï¼‰æ–¹é¢çš„æ€§èƒ½ä¹Ÿæ¥è¿‘ç²¾ç»†è°ƒæ•´æ¨¡å‹ã€‚è™½ç„¶äººç±»æ³¨é‡Šè€…å¯¹äºå®ç°æœ€ä½³æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†LLMså¯ä»¥å‡å°‘å¯¹ASQPä»»åŠ¡ä¸­å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å‘æ–¹é¢çš„æƒ…æ„Ÿå››é‡é¢„æµ‹ï¼ˆASQPï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šï¼ŒLLMsçš„æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå…¶F1åˆ†æ•°ç•¥ä½äºç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹ã€‚</li>
<li>åœ¨Rest16é¤å…é¢†åŸŸæ•°æ®é›†çš„40ä¸ªæ ·æœ¬ä¸­ï¼ŒLLMsçš„F1åˆ†æ•°è¾¾åˆ°52.46%ï¼Œä¸æœ€ä½³ç²¾ç»†è°ƒæ•´æ–¹æ³•MVPç›¸æ¯”å…·æœ‰ä¸€å®šç«äº‰åŠ›ã€‚</li>
<li>LLMsåœ¨ç›®æ ‡æ–¹é¢æƒ…æ„Ÿæ£€æµ‹ï¼ˆTASDï¼‰æ–¹é¢çš„æ€§èƒ½æ¥è¿‘ç²¾ç»†è°ƒæ•´æ¨¡å‹ï¼Œå…¶F1åˆ†æ•°åœ¨Rest16æ•°æ®é›†ä¸Šè¾¾åˆ°66.03%ã€‚</li>
<li>è™½ç„¶äººç±»æ³¨é‡Šè€…å¯¹äºä¼˜åŒ–ASQPä»»åŠ¡æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†LLMsçš„åº”ç”¨å¯ä»¥å‡å°‘å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„éœ€æ±‚ã€‚</li>
<li>LLMsåœ¨ASQPä»»åŠ¡ä¸­çš„è¡¨ç°è¡¨æ˜å®ƒä»¬å…·æœ‰æ½œåœ¨çš„å®ç”¨ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7279e84c186ba6ea312367d1999b5b07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebcf0d1a8b222d8171b948897bc904cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a29bab5da6966b85e087ce3267b2775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e45fe3397ae5f5b1cfb3f4dad94d853.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Disentanglement-of-Content-and-Style-via-Variance-Invariance-Constraints"><a href="#Unsupervised-Disentanglement-of-Content-and-Style-via-Variance-Invariance-Constraints" class="headerlink" title="Unsupervised Disentanglement of Content and Style via   Variance-Invariance Constraints"></a>Unsupervised Disentanglement of Content and Style via   Variance-Invariance Constraints</h2><p><strong>Authors:Yuxuan Wu, Ziyu Wang, Bhiksha Raj, Gus Xia</strong></p>
<p>We contribute an unsupervised method that effectively learns disentangled content and style representations from sequences of observations. Unlike most disentanglement algorithms that rely on domain-specific labels or knowledge, our method is based on the insight of domain-general statistical differences between content and style â€“ content varies more among different fragments within a sample but maintains an invariant vocabulary across data samples, whereas style remains relatively invariant within a sample but exhibits more significant variation across different samples. We integrate such inductive bias into an encoder-decoder architecture and name our method after V3 (variance-versus-invariance). Experimental results show that V3 generalizes across multiple domains and modalities, successfully learning disentangled content and style representations, such as pitch and timbre from music audio, digit and color from images of hand-written digits, and action and character appearance from simple animations. V3 demonstrates strong disentanglement performance compared to existing unsupervised methods, along with superior out-of-distribution generalization under few-shot adaptation compared to supervised counterparts. Lastly, symbolic-level interpretability emerges in the learned content codebook, forging a near one-to-one alignment between machine representation and human knowledge. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä»è§‚å¯Ÿåºåˆ—ä¸­å­¦ä¹ åˆ†ç¦»çš„å†…å®¹å’Œé£æ ¼è¡¨ç¤ºã€‚ä¸å¤§å¤šæ•°ä¾èµ–äºç‰¹å®šé¢†åŸŸæ ‡ç­¾æˆ–çŸ¥è¯†çš„è§£çº ç¼ ç®—æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŸºäºå†…å®¹å’Œé£æ ¼ä¹‹é—´çš„ä¸€èˆ¬ç»Ÿè®¡å·®å¼‚ï¼šå†…å®¹åœ¨åŒä¸€æ ·æœ¬çš„ä¸åŒç‰‡æ®µä¹‹é—´å˜åŒ–è¾ƒå¤§ï¼Œä½†åœ¨ä¸åŒçš„æ•°æ®æ ·æœ¬ä¹‹é—´ä¿æŒä¸å˜çš„è¯æ±‡è¡¨ï¼Œè€Œé£æ ¼åœ¨åŒä¸€æ ·æœ¬å†…ç›¸å¯¹ä¿æŒä¸å˜ï¼Œä½†åœ¨ä¸åŒçš„æ ·æœ¬ä¹‹é—´è¡¨ç°å‡ºæ›´å¤§çš„å˜åŒ–ã€‚æˆ‘ä»¬å°†è¿™ç§å½’çº³åè§æ•´åˆåˆ°ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­ï¼Œå¹¶å°†æˆ‘ä»¬çš„æ–¹æ³•å‘½åä¸ºV3ï¼ˆæ–¹å·®ä¸ä¸å˜æ€§ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒV3å¯ä»¥è·¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡å¼è¿›è¡Œæ¨å¹¿ï¼ŒæˆåŠŸå­¦ä¹ è§£çº ç¼ çš„å†…å®¹å’Œé£æ ¼è¡¨ç¤ºï¼Œä¾‹å¦‚éŸ³ä¹éŸ³é¢‘ä¸­çš„éŸ³é«˜å’ŒéŸ³è‰²ï¼Œæ‰‹å†™æ•°å­—å›¾åƒä¸­çš„æ•°å­—å’Œé¢œè‰²ï¼Œä»¥åŠç®€å•åŠ¨ç”»ä¸­çš„åŠ¨ä½œå’Œè§’è‰²å¤–è§‚ã€‚ä¸ç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼ŒV3è¡¨ç°å‡ºå¼ºå¤§çš„è§£çº ç¼ æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å°‘é‡é€‚åº”çš„æƒ…å†µä¸‹å…·æœ‰å‡ºè‰²çš„åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºæœ‰ç›‘ç£çš„åŒç±»æ–¹æ³•ã€‚æœ€åï¼Œåœ¨å­¦ä¹ çš„å†…å®¹ä»£ç åº“ä¸­å‡ºç°äº†ç¬¦å·çº§çš„å¯è§£é‡Šæ€§ï¼Œä¸ºæœºå™¨è¡¨ç¤ºå’Œäººç±»çŸ¥è¯†ä¹‹é—´å»ºç«‹äº†ä¸€å¯¹ä¸€çš„å¯¹åº”å…³ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03824v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†ä¸€ç§èƒ½å¤Ÿä»åºåˆ—è§‚å¯Ÿä¸­å­¦ä¹ åˆ†ç¦»å†…å®¹å’Œé£æ ¼è¡¨ç¤ºçš„æ— ç›‘ç£æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºå†…å®¹å’Œé£æ ¼åœ¨ç»Ÿè®¡ä¸Šçš„å·®å¼‚ï¼Œé€šè¿‡å¼•å…¥æ–¹å·®ä¸ä¸å˜æ€§çš„è§‚ç‚¹ï¼ŒæˆåŠŸåœ°åœ¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡æ€ä¸­å­¦ä¹ åˆ°åˆ†ç¦»çš„å†…å®¹å’Œé£æ ¼è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨éŸ³é¢‘ã€å›¾åƒå’Œç®€å•åŠ¨ç”»ç­‰é¢†åŸŸå…·æœ‰è‰¯å¥½çš„è¡¨ç°ï¼Œä¸å…¶ä»–æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰è¾ƒå¼ºçš„è§£è€¦æ€§èƒ½ï¼Œå¹¶åœ¨å°‘æ•°é€‚åº”æƒ…å†µä¸‹å±•ç°å‡ºå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå­¦åˆ°çš„å†…å®¹ä»£ç æœ¬å…·æœ‰ç¬¦å·çº§çš„å¯è§£é‡Šæ€§ï¼Œå®ç°äº†æœºå™¨è¡¨ç¤ºå’Œäººç±»çŸ¥è¯†ä¹‹é—´è¿‘ä¹ä¸€å¯¹ä¸€çš„å¯¹åº”å…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿä»åºåˆ—è§‚å¯Ÿä¸­å­¦ä¹ åˆ†ç¦»å†…å®¹å’Œé£æ ¼è¡¨ç¤ºã€‚</li>
<li>åŸºäºå†…å®¹å’Œé£æ ¼åœ¨ç»Ÿè®¡ä¸Šçš„å·®å¼‚æ¥åŒºåˆ†ä¸¤è€…ã€‚</li>
<li>é€šè¿‡æ–¹å·®ä¸ä¸å˜æ€§çš„è§‚ç‚¹æ•´åˆè¿™ç§å½’çº³åè§ã€‚</li>
<li>åœ¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡æ€ä¸­æˆåŠŸåº”ç”¨ï¼Œå¦‚éŸ³ä¹éŸ³é¢‘ã€æ‰‹å†™æ•°å­—å›¾åƒå’Œç®€å•åŠ¨ç”»ã€‚</li>
<li>ä¸ç°æœ‰æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰è¾ƒå¼ºçš„è§£è€¦æ€§èƒ½ã€‚</li>
<li>åœ¨å°‘æ•°é€‚åº”æƒ…å†µä¸‹å±•ç°å‡ºå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.03824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-214a70beac4683ff8d402f334663ba05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2632b7aa85c4074631880babc828e20b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b40e2b76b455d9d9f072a81b3a40301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1d83febcd43ec38ac706fd78087ea1f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Few-shot-Personalization-of-LLMs-with-Mis-aligned-Responses"><a href="#Few-shot-Personalization-of-LLMs-with-Mis-aligned-Responses" class="headerlink" title="Few-shot Personalization of LLMs with Mis-aligned Responses"></a>Few-shot Personalization of LLMs with Mis-aligned Responses</h2><p><strong>Authors:Jaehyung Kim, Yiming Yang</strong></p>
<p>As the diversity of users increases, the capability of providing personalized responses by large language models (LLMs) has become increasingly important. Existing approaches have only limited successes in LLM personalization, due to the absence of personalized learning or the reliance on shared personal data. This paper proposes a new approach for a few-shot personalization of LLMs with their mis-aligned responses (Fermi). Our key idea is to learn a set of personalized prompts for each user by progressively improving the prompts using LLMs, based on user profile (e.g., demographic information) and a few examples of previous opinions. During an iterative process of prompt improvement, we incorporate the contexts of mis-aligned responses by LLMs, which are especially crucial for the effective personalization of LLMs. In addition, we develop an effective inference method to further leverage the context of the test query and the personalized prompts. Our experimental results demonstrate that Fermi significantly improves performance across various benchmarks, compared to best-performing baselines. </p>
<blockquote>
<p>éšç€ç”¨æˆ·å¤šæ ·æ€§çš„å¢åŠ ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›ä¸ªæ€§åŒ–å“åº”çš„èƒ½åŠ›å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç”±äºç¼ºå°‘ä¸ªæ€§åŒ–å­¦ä¹ æˆ–ä¾èµ–å…±äº«ä¸ªäººæ•°æ®ï¼Œç°æœ‰æ–¹æ³•åœ¨LLMä¸ªæ€§åŒ–æ–¹é¢çš„æˆæœæœ‰é™ã€‚æœ¬æ–‡é’ˆå¯¹LLMçš„è¯¯å¯¹é½å“åº”ï¼ˆè´¹ç±³ï¼‰æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬ä¸ªæ€§åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä¸»è¦æ€æƒ³æ˜¯åŸºäºç”¨æˆ·é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚äººå£ç»Ÿè®¡ä¿¡æ¯ï¼‰å’Œå…ˆå‰çš„å‡ ä¸ªè§‚ç‚¹çš„ç¤ºä¾‹ï¼Œé€šè¿‡é€æ­¥æ”¹è¿›æç¤ºæ¥å­¦ä¹ æ¯ä¸ªç”¨æˆ·çš„ä¸ªæ€§åŒ–æç¤ºé›†ã€‚åœ¨é€æ­¥æ”¹è¿›æç¤ºçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬èå…¥äº†LLMäº§ç”Ÿçš„è¯¯å¯¹é½å“åº”çš„ä¸Šä¸‹æ–‡ï¼Œè¿™å¯¹äºå®ç°LLMçš„æœ‰æ•ˆä¸ªæ€§åŒ–å°¤ä¸ºé‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„æ¨ç†æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥åˆ©ç”¨æµ‹è¯•æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡å’Œä¸ªæ€§åŒ–æç¤ºã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè´¹ç±³åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸æœ€ä½³åŸºçº¿ç›¸æ¯”æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.18678v2">PDF</a> NAACL 25 (main, long), 32 pages</p>
<p><strong>Summary</strong></p>
<p>éšç€ç”¨æˆ·å¤šæ ·æ€§çš„å¢åŠ ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›ä¸ªæ€§åŒ–å“åº”çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºç”¨æˆ·é…ç½®æ–‡ä»¶å’Œä¹‹å‰æ„è§çš„å‡ ä¸ªç¤ºä¾‹æ¥é€æ­¥æ”¹è¿›æç¤ºçš„æ–¹æ³•ï¼Œä»¥å®ç°LLMçš„å°‘é‡ä¸ªæ€§åŒ–å“åº”ã€‚åœ¨è¿­ä»£æ”¹è¿›æç¤ºçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç»“åˆäº†LLMçš„è¯¯å¯¹é½å“åº”çš„ä¸Šä¸‹æ–‡ï¼Œè¿™å¯¹äºLLMçš„æœ‰æ•ˆä¸ªæ€§åŒ–è‡³å…³é‡è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”¨æˆ·å¤šæ ·æ€§çš„å¢åŠ ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸ªæ€§åŒ–å“åº”èƒ½åŠ›å˜å¾—é‡è¦ã€‚</li>
<li>ç°æœ‰LLMä¸ªæ€§åŒ–æ–¹æ³•çš„å±€é™æ€§åœ¨äºç¼ºä¹ä¸ªæ€§åŒ–å­¦ä¹ æˆ–ä¾èµ–å…±äº«ä¸ªäººæ•°æ®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”¨æˆ·é…ç½®æ–‡ä»¶å’Œä¹‹å‰æ„è§çš„å‡ ä¸ªç¤ºä¾‹æ¥é€æ­¥æ”¹è¿›æç¤ºçš„æ–°æ–¹æ³•ã€‚</li>
<li>ç»“åˆLLMè¯¯å¯¹é½å“åº”çš„ä¸Šä¸‹æ–‡å¯¹äºæœ‰æ•ˆä¸ªæ€§åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„æ¨ç†æ–¹æ³•ï¼Œè¿›ä¸€æ­¥åˆ©ç”¨æµ‹è¯•æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡å’Œä¸ªæ€§åŒ–çš„æç¤ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.18678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76b99873090deda3b32991c269530d9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af57436bf29503117c30f979d6984ddb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b89af0f8045fb7806cacc487f4d29576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d11a64a0dba45426474aa80d0c10fe9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9190738800205caea1287c5b9299d89c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-94b36f42f65d577f56b683ea17670759.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  DarkDeblur Learning single-shot image deblurring in low-light condition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-33f5731f87983614ccc75d058787baab.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-06  Federated Learning for Privacy-Preserving Feedforward Control in   Multi-Agent Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
