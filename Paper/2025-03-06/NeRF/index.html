<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2025-03-06  2DGS-Avatar Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-190fb996ee21d987789559f1b5cf54f5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-06-更新"><a href="#2025-03-06-更新" class="headerlink" title="2025-03-06 更新"></a>2025-03-06 更新</h1><h2 id="2DGS-Avatar-Animatable-High-fidelity-Clothed-Avatar-via-2D-Gaussian-Splatting"><a href="#2DGS-Avatar-Animatable-High-fidelity-Clothed-Avatar-via-2D-Gaussian-Splatting" class="headerlink" title="2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting"></a>2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting</h2><p><strong>Authors:Qipeng Yan, Mingyang Sun, Lihua Zhang</strong></p>
<p>Real-time rendering of high-fidelity and animatable avatars from monocular videos remains a challenging problem in computer vision and graphics. Over the past few years, the Neural Radiance Field (NeRF) has made significant progress in rendering quality but behaves poorly in run-time performance due to the low efficiency of volumetric rendering. Recently, methods based on 3D Gaussian Splatting (3DGS) have shown great potential in fast training and real-time rendering. However, they still suffer from artifacts caused by inaccurate geometry. To address these problems, we propose 2DGS-Avatar, a novel approach based on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars with high-fidelity and fast training performance. Given monocular RGB videos as input, our method generates an avatar that can be driven by poses and rendered in real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the advantages of fast training and rendering while also capturing detailed, dynamic, and photo-realistic appearances. We conduct abundant experiments on popular datasets such as AvatarRex and THuman4.0, demonstrating impressive performance in both qualitative and quantitative metrics. </p>
<blockquote>
<p>从单目视频中实时渲染高保真和可动画的化身仍是计算机视觉和图形学中的一个具有挑战性的问题。过去几年里，神经辐射场（NeRF）在渲染质量方面取得了重大进展，但由于体积渲染的低效率，其在运行时性能上表现不佳。最近，基于三维高斯贴片技术（3DGS）的方法在快速训练和实时渲染方面显示出巨大潜力。然而，它们仍然受到由不准确几何形状引起的人工效应的影响。为了解决这些问题，我们提出了基于二维高斯贴片技术（2DGS）的2DGS化身方法，用于建模高保真和快速训练性能的可动画服装化身。我们的方法以单目RGB视频为输入，生成可以通过姿态驱动并在实时中呈现的化身。与基于三维高斯贴片技术的方法相比，我们的二维高斯贴片技术化身保留了快速训练和渲染的优点，同时捕捉到了详细、动态和逼真的外观。我们在流行的数据集如AvatarRex和THuman4.0上进行了大量实验，在定性和定量指标上都取得了令人印象深刻的表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02452v1">PDF</a> ICVRV 2024</p>
<p><strong>Summary</strong></p>
<p>基于单目视频实时渲染高质量动画人物仍是计算机视觉和图形学领域的一个挑战性问题。过去几年，神经辐射场（NeRF）在提高渲染质量方面取得了显著进展，但由于体积渲染的低效率，在运行时性能上表现不佳。最近，基于3D高斯描画（3DGS）的方法在快速训练和实时渲染方面显示出巨大潜力，但仍存在由几何不准确导致的伪影问题。为解决这些问题，本文提出一种基于二维高斯描画（2DGS）的新型方法——2DGS-Avatar，用于建模高质量、可动画的服装人物，具有快速训练性能。以单目RGB视频为输入，我们的方法生成的人物可以通过姿态驱动，实现实时渲染。相较于基于3DGS的方法，2DGS-Avatar既保留了快速训练和渲染的优势，又能捕捉详细、动态、逼真的外观。在流行的数据集如AvatarRex和THuman4.0上的实验表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Real-time rendering of high-fidelity and animatable avatars from monocular videos remains a challenging problem.</li>
<li>Neural Radiance Field (NeRF)虽能提高渲染质量，但运行时性能不佳。</li>
<li>基于3D Gaussian Splatting (3DGS)的方法虽可实现快速训练和实时渲染，但存在由几何不准确导致的伪影问题。</li>
<li>2DGS-Avatar方法基于二维高斯描画（2DGS），可建模高质量、可动画的服装人物，兼具快速训练性能。</li>
<li>2DGS-Avatar能捕捉详细、动态、逼真的外观。</li>
<li>2DGS-Avatar在多个数据集上的实验表现出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02452">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9a0336f847a70a7629aed33f587cd9ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91123f4413f9ef763a86ae6b011a81c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25f24955015d37bba5118c43e9f4117a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-559fdd72341d50e48a308055502b432a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e63a28c3b242df567bcd9e689685eaee.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Sim-to-Real-Visual-Quadrotor-Control-with-Hard-Constraints"><a href="#Zero-Shot-Sim-to-Real-Visual-Quadrotor-Control-with-Hard-Constraints" class="headerlink" title="Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints"></a>Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints</h2><p><strong>Authors:Yan Miao, Will Shen, Sayan Mitra</strong></p>
<p>We present the first framework demonstrating zero-shot sim-to-real transfer of visual control policies learned in a Neural Radiance Field (NeRF) environment for quadrotors to fly through racing gates. Robust transfer from simulation to real flight poses a major challenge, as standard simulators often lack sufficient visual fidelity. To address this, we construct a photorealistic simulation environment of quadrotor racing tracks, called FalconGym, which provides effectively unlimited synthetic images for training. Within FalconGym, we develop a pipelined approach for crossing gates that combines (i) a Neural Pose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor poses from single-frame RGB images and IMU data, and (ii) a self-attention-based multi-modal controller that adaptively integrates visual features and pose estimation. This multi-modal design compensates for perception noise and intermittent gate visibility. We train this controller purely in FalconGym with imitation learning and deploy the resulting policy to real hardware with no additional fine-tuning. Simulation experiments on three distinct tracks (circle, U-turn and figure-8) demonstrate that our controller outperforms a vision-only state-of-the-art baseline in both success rate and gate-crossing accuracy. In 30 live hardware flights spanning three tracks and 120 gates, our controller achieves a 95.8% success rate and an average error of just 10 cm when flying through 38 cm-radius gates. </p>
<blockquote>
<p>我们首次提出了一种框架，该框架展示了在神经网络辐射场（NeRF）环境中学习的视觉控制策略，用于无射击模拟到真实飞行的四旋翼无人机穿越竞速门时的零射击模拟到现实转移。从模拟到真实飞行的稳健转移是一个巨大的挑战，因为标准模拟器通常缺乏足够的视觉逼真度。为了解决这个问题，我们构建了名为FalconGym的四旋翼竞速轨道的逼真模拟环境，它提供了用于训练的有效无限合成图像。在FalconGym中，我们开发了一种穿越大门的管道方法，它结合了（i）与卡尔曼滤波器耦合的神经网络姿态估计器（NPE），可以从单帧RGB图像和IMU数据中可靠地推断出四旋翼的姿态；（ii）基于自注意力的多模态控制器，自适应地融合了视觉特征和姿态估计。这种多模态设计可以补偿感知噪声和间歇性大门可见性。我们仅使用模仿学习在FalconGym中训练此控制器，并将结果策略部署到实际硬件上，无需进行任何额外的微调。在三个不同轨道（圆形、U形转弯和图形数字-信道人投为青睐被下注者们也为接受的代位用语统计俱乐部反馈拟把八大域的共同体语义做近似处本位专题搜索曲线、花环轨迹等）上的模拟实验表明，我们的控制器在成功率和穿越大门准确性方面都优于仅使用视觉的最新技术基线。在跨越三个轨道和穿过涵盖竞赛排门的实战硬件飞行测试中（总共穿过高达三次合计人数），我们的控制器成功率达到了高达95.8%，并且在穿越直径仅为约三十八厘米的赛道门时平均误差仅为十厘米。这是一个巨大的成功！</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02198v1">PDF</a> </p>
<p><strong>摘要</strong><br>    本研究首次展示了在Neural Radiance Field（NeRF）环境中学习的视觉控制策略实现零样本模拟到真实场景的迁移，应用于四旋翼飞行器穿越竞速门。针对从模拟到真实飞行的稳健迁移所面临的视觉逼真度不足的问题，研究构建了名为FalconGym的高保真模拟环境，用于四旋翼竞速赛道的训练，并提供无限合成图像。在FalconGym中，研究开发出一种穿越门框的流水线方法，包括（i）结合卡尔曼滤波器的神经网络姿态估计器（NPE），可从单帧RGB图像和IMU数据中可靠推断四旋翼的姿态；（ii）基于自注意力机制的多模态控制器，能自适应地融合视觉特征和姿态估计。这种多模态设计弥补了感知噪声和间歇性门框可见性的问题。研究仅使用FalconGym模拟环境进行模仿学习训练该控制器，并直接应用于真实硬件，无需额外微调。在三个不同赛道（圆圈、U型转弯和八字形）的模拟实验表明，该控制器在成功率和穿越门框的准确性方面均优于仅使用视觉的最新技术基线。在跨越三个赛道、穿越120个门框的30次实际硬件飞行测试中，该控制器的成功率达到95.8%，平均误差仅为10厘米。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究实现了首个在NeRF环境中学习视觉控制策略的零样本模拟到真实迁移应用，针对四旋翼飞行器穿越竞速门。</li>
<li>构建名为FalconGym的高保真模拟环境，以应对从模拟到真实飞行中的视觉逼真度挑战。</li>
<li>开发出结合神经网络姿态估计器和卡尔曼滤波器的管线方法，能从单帧图像和IMU数据中推断四旋翼的姿态。</li>
<li>采用基于自注意力机制的多模态控制器设计，融合视觉特征和姿态估计，以应对感知噪声和间歇性门框可见性问题。</li>
<li>在模拟环境中进行广泛实验，证明所提出控制器在成功率和穿越门框准确性方面优于现有视觉基线技术。</li>
<li>在实际硬件飞行测试中，控制器表现出高成功率和低误差，平均误差仅为10厘米。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02198">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-713636ed097c1ef5c407c14ef55315fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a8fc78040175ecc6e6c68b6a944f4ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3504a548604311c18555a5b3056a350.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0e4f386f14d4765256587a337009108.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86fa7ee50d6379600ebb218ee28dd6de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff190394fb9efca0e39b3db3af9fe35d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbdc24a41590df2f72ecdf47291e7c41.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Data-Augmentation-for-NeRFs-in-the-Low-Data-Limit"><a href="#Data-Augmentation-for-NeRFs-in-the-Low-Data-Limit" class="headerlink" title="Data Augmentation for NeRFs in the Low Data Limit"></a>Data Augmentation for NeRFs in the Low Data Limit</h2><p><strong>Authors:Ayush Gaggar, Todd D. Murphey</strong></p>
<p>Current methods based on Neural Radiance Fields fail in the low data limit, particularly when training on incomplete scene data. Prior works augment training data only in next-best-view applications, which lead to hallucinations and model collapse with sparse data. In contrast, we propose adding a set of views during training by rejection sampling from a posterior uncertainty distribution, generated by combining a volumetric uncertainty estimator with spatial coverage. We validate our results on partially observed scenes; on average, our method performs 39.9% better with 87.5% less variability across established scene reconstruction benchmarks, as compared to state of the art baselines. We further demonstrate that augmenting the training set by sampling from any distribution leads to better, more consistent scene reconstruction in sparse environments. This work is foundational for robotic tasks where augmenting a dataset with informative data is critical in resource-constrained, a priori unknown environments. Videos and source code are available at <a target="_blank" rel="noopener" href="https://murpheylab.github.io/low-data-nerf/">https://murpheylab.github.io/low-data-nerf/</a>. </p>
<blockquote>
<p>当前基于神经辐射场的方法在数据稀缺的情况下表现不佳，特别是在对不完整场景数据进行训练时。早期的工作仅在最佳后续视图应用程序中扩充训练数据，这导致在稀疏数据情况下出现幻觉和模型崩溃。相比之下，我们提出通过从后验不确定性分布中进行拒绝采样来在训练过程中增加一组视图，该分布是通过结合体积不确定性估计器和空间覆盖率生成的。我们在部分观测场景上验证了我们的结果；平均而言，我们的方法在建立的场景重建基准测试上比最新技术基准高出39.9%，并且具有较低的87.5%变异性。我们进一步证明，通过从任何分布中采样来扩充训练集，可以在稀疏环境中实现更好、更一致的场景重建。这项工作在机器人任务中是基础性的，因为在资源受限、事先未知的环境中，用信息丰富的数据扩充数据集是至关重要的。相关视频和源代码可在<a target="_blank" rel="noopener" href="https://murpheylab.github.io/low-data-nerf/%E6%89%BE%E5%88%B0%E3%80%82">https://murpheylab.github.io/low-data-nerf/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02092v1">PDF</a> To be published in 2025 IEEE International Conference on Robotics and   Automation (ICRA 2025)</p>
<p><strong>Summary</strong></p>
<p>基于神经辐射场（NeRF）的当前方法在数据有限时表现不佳，特别是在训练不完整场景数据时。以往的工作仅在最佳视角应用中进行数据增强，导致在稀疏数据时产生幻象和模型崩溃。相反，我们提出通过从后验不确定性分布中进行拒绝采样来添加一组视图，该分布是通过结合体积不确定性估计器和空间覆盖率生成的。我们在部分观测场景上验证了我们的方法，平均而言，与最新技术基准相比，我们的方法在建立的场景重建基准测试上表现更好，平均高出39.9%，且变异度降低了87.5%。我们还证明，从任何分布中对训练集进行增强，可以在稀疏环境中实现更好、更一致的场景重建。这项工作对于机器人任务至关重要，在资源受限、先验未知的的环境中，数据增强具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前基于NeRF的方法在数据有限、尤其是场景数据不完整时表现不佳。</li>
<li>以往的数据增强方法仅在最佳视角应用中使用，这可能导致幻象和模型崩溃在稀疏数据时。</li>
<li>提出了一种新的数据增强方法，通过从后验不确定性分布进行拒绝采样来添加更多的视图。</li>
<li>方法在部分观测场景上的表现优于现有技术基准，平均性能提升39.9%，且变异度降低了87.5%。</li>
<li>从任何分布中对训练集进行增强可以在稀疏环境中实现更好的场景重建。</li>
<li>该研究对于机器人任务具有重要意义，特别是在资源受限、先验未知的的环境中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02092">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6696be0a07de9c5f56d283293e93933e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2b165d4759d7071a960dcafaec33d06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5909c8304224d0620e9f715c194ca6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc0e908e9e00fa4b005ddb50820d4248.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-156bdf231e1009692897e0971eef7fce.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Difix3D-Improving-3D-Reconstructions-with-Single-Step-Diffusion-Models"><a href="#Difix3D-Improving-3D-Reconstructions-with-Single-Step-Diffusion-Models" class="headerlink" title="Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models"></a>Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models</h2><p><strong>Authors:Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling</strong></p>
<p>Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2$\times$ improvement in FID score over baselines while maintaining 3D consistency. </p>
<blockquote>
<p>神经辐射场和三维高斯描绘技术已经彻底改变了三维重建和新型视图合成任务。然而，从极端新视角实现逼真渲染仍然是一个挑战，因为不同表示形式之间存在持久性的伪影。在这项工作中，我们引入了Difix3D+，这是一种新型管道设计，旨在通过单步扩散模型增强三维重建和新型视图合成。我们方法的核心是Difix，这是一种单步图像扩散模型，经过训练可增强并消除由于三维表示中的约束不足而在渲染新视图中产生的伪影。Difix在我们的管道中扮演两个关键角色。首先，它用于重建阶段，清理从重建生成的伪训练视图，然后将其蒸馏回三维空间。这极大地增强了约束不足的区域并提高了整体的三维表示质量。更重要的是，在推理过程中，Difix还充当神经增强器，有效地消除了由于不完美的三维监督和当前重建模型的有限容量而产生的残余伪影。Difix3D+是一种通用解决方案，一个兼容NeRF和3DGS表示的单一模型，在基线基础上实现了FID得分的平均两倍提升，同时保持了三维一致性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01774v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong>：神经辐射场和三维高斯绘制在三维重建和视角合成任务中起到了革命性的作用。然而，从极端全新视角实现逼真的渲染仍然存在挑战，因为表示中的伪影仍然存在。在这项工作中，我们引入了Difix3D+，这是一种通过单步扩散模型设计的增强三维重建和视角合成的新管道。我们的方法的核心是Difix，这是一种单步图像扩散模型，经过训练，旨在增强并消除由于三维表示的欠约束区域导致的渲染新视图中的伪影。Difix在我们的管道中扮演了两个关键角色。首先，它在重建阶段用于清理从重建生成的伪训练视图，然后将其蒸馏回三维。这极大地增强了欠约束区域并提高了整体的三维表示质量。更重要的是，在推理过程中，Difix还充当神经增强器，有效地消除了由不完美的三维监督和当前重建模型的有限容量引起的残余伪影。Difix3D+是一种通用解决方案，一个与NeRF和3DGS表示兼容的单一模型，在基线的基础上实现了FID分数的平均两倍改进，同时保持了三维一致性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>神经辐射场和三维高斯绘制已对三维重建和视角合成产生重大影响。</li>
<li>从极端全新视角进行逼真渲染仍存在挑战，主要因为表示中的伪影问题。</li>
<li>引入的Difix3D+管道通过使用单步扩散模型增强三维重建和视角合成。</li>
<li>核心模型Difix经过训练，能增强并消除由于三维表示欠约束区域导致的渲染新视图中的伪影。</li>
<li>Difix在管道中扮演了关键角色，既在重建阶段清理伪训练视图，也在推理过程中充当神经增强器。</li>
<li>Difix3D+对基线进行了改进，实现了FID分数的平均两倍提升，同时保持了三维一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7a75e326b590d50fda10ba198bf2d16c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-386bba37085eb54dd2daee0af8dfe5c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47cfbf3b63feeb9d4654954352571447.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04dbcc58e6d6212b602cf89b86e2c3ad.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Direct-Discriminative-Optimization-Your-Likelihood-Based-Visual-Generative-Model-is-Secretly-a-GAN-Discriminator"><a href="#Direct-Discriminative-Optimization-Your-Likelihood-Based-Visual-Generative-Model-is-Secretly-a-GAN-Discriminator" class="headerlink" title="Direct Discriminative Optimization: Your Likelihood-Based Visual   Generative Model is Secretly a GAN Discriminator"></a>Direct Discriminative Optimization: Your Likelihood-Based Visual   Generative Model is Secretly a GAN Discriminator</h2><p><strong>Authors:Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang</strong></p>
<p>While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that bridges likelihood-based generative training and the GAN objective to bypass this fundamental constraint. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79&#x2F;1.58 to new records of 1.30&#x2F;0.97 on CIFAR-10&#x2F;ImageNet-64 datasets, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256$\times$256. </p>
<blockquote>
<p>基于概率的生成模型，特别是扩散和自回归模型，在视觉生成方面取得了显著的保真度。然而，最大似然估计（MLE）目标本质上存在模式覆盖的倾向，这在有限的模型容量下限制了生成质量。在这项工作中，我们提出了直接判别优化（DDO）作为一个统一的框架，它结合了基于概率的生成训练和GAN目标，以规避这一基本约束。我们的关键见解是，使用一个判别器来隐含地参数化可学习目标模型与固定参考模型之间的概率比率，这与直接偏好优化（DPO）的理念相类似。不同于GAN，这种参数化方式消除了对生成器和判别器网络联合训练的需要，允许对预训练良好的模型进行直接、高效和有效的微调，充分发挥其潜力，超越MLE的限制。DDO可以自我迭代的方式进行自我完善模型优化，每一轮所需的训练时间不到预训练周期的百分之一。我们的实验通过显著改进先前的最佳扩散模型EDM，在CIFAR-10和ImageNet-64数据集上将FID分数从原来的1.79&#x2F;1.58降低到新的记录水平1.30&#x2F;0.97，并且持续改进无指导和CFG增强的ImageNet 256×256的视觉自回归模型的FID分数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01103v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/dir/ddo/">https://research.nvidia.com/labs/dir/ddo/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了Direct Discriminative Optimization（DDO）框架，旨在解决基于可能性的生成模型在面对有限模型容量时的模式覆盖倾向问题。通过结合可能性的生成训练和GAN目标，DDO能够绕过这一基本限制。实验结果显示，DDO能有效提升现有扩散模型和自回归模型的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Direct Discriminative Optimization（DDO）框架解决了基于可能性的生成模型在有限模型容量下的模式覆盖问题。</li>
<li>DDO结合了可能性的生成训练和GAN目标。</li>
<li>DDO通过参数化判别器，利用目标模型与固定参考模型之间的可能性比率，与Direct Preference Optimization（DPO）理念相似。</li>
<li>DDO不需要联合训练生成器和判别器网络，可直接、高效、有效地对预训练模型进行微调，充分发挥其潜力。</li>
<li>DDO可以通过自我对抗的方式进行迭代，实现模型的渐进改进，每次迭代所需的预训练周期不到1%。</li>
<li>实验结果显示，DDO在CIFAR-10和ImageNet-64数据集上显著提升了之前的最佳扩散模型EDM的性能，FID分数降至新低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01103">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e7865d484bf50a914b1493ef8c180745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-190fb996ee21d987789559f1b5cf54f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-106c67a686408df6757f42068767415f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd407872a40938a4fd6f1a89cb03a10e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7222e3ee9ddaec9041160f832ae4f6fe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Glad-A-Streaming-Scene-Generator-for-Autonomous-Driving"><a href="#Glad-A-Streaming-Scene-Generator-for-Autonomous-Driving" class="headerlink" title="Glad: A Streaming Scene Generator for Autonomous Driving"></a>Glad: A Streaming Scene Generator for Autonomous Driving</h2><p><strong>Authors:Bin Xie, Yingfei Liu, Tiancai Wang, Jiale Cao, Xiangyu Zhang</strong></p>
<p>The generation and simulation of diverse real-world scenes have significant application value in the field of autonomous driving, especially for the corner cases. Recently, researchers have explored employing neural radiance fields or diffusion models to generate novel views or synthetic data under driving scenes. However, these approaches suffer from unseen scenes or restricted video length, thus lacking sufficient adaptability for data generation and simulation. To address these issues, we propose a simple yet effective framework, named Glad, to generate video data in a frame-by-frame style. To ensure the temporal consistency of synthetic video, we introduce a latent variable propagation module, which views the latent features of previous frame as noise prior and injects it into the latent features of current frame. In addition, we design a streaming data sampler to orderly sample the original image in a video clip at continuous iterations. Given the reference frame, our Glad can be viewed as a streaming simulator by generating the videos for specific scenes. Extensive experiments are performed on the widely-used nuScenes dataset. Experimental results demonstrate that our proposed Glad achieves promising performance, serving as a strong baseline for online video generation. We will release the source code and models publicly. </p>
<blockquote>
<p>现实世界场景的生成与模拟在自动驾驶领域具有重要的应用价值，尤其对于一些特殊场景。近期，研究者尝试采用神经辐射场或扩散模型来生成驾驶场景下的新型视角或合成数据。然而，这些方法在面临未知场景或视频长度受限时，其在数据生成和模拟方面的适应性不足。为了解决这些问题，我们提出了一种简单有效的框架，名为Glad，以逐帧的方式生成视频数据。为了保证合成视频的时空一致性，我们引入了潜在变量传播模块，将前一帧的潜在特征视为噪声先验，并将其注入当前帧的潜在特征。此外，我们设计了一种流式数据采样器，以有序的方式在连续迭代中对视频片段中的原始图像进行采样。给定参考帧，我们的Glad可被视为一种流式模拟器，为特定场景生成视频。我们在广泛使用的nuScenes数据集上进行了大量实验。实验结果表明，我们提出的Glad取得了有前景的性能，成为在线视频生成的一个强有力的基准模型。我们将公开源代码和模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00045v1">PDF</a> Accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种名为Glad的框架，用于以逐帧的方式生成视频数据，用于自主驾驶领域的场景生成和模拟。该框架解决了现有方法在面对未见场景或视频长度受限时的适应能力不足的问题。通过引入潜在变量传播模块和流式数据采样器，确保合成视频的时空一致性。在广泛使用的nuScenes数据集上进行的实验表明，Glad框架取得了良好的性能，成为在线视频生成的强大基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Glad框架被用于生成和模拟自主驾驶中的多样现实世界场景，特别是针对角落情况的场景。</li>
<li>现有方法在面对未见场景或视频长度受限时存在适应能力不足的问题。</li>
<li>Glad框架采用逐帧的方式生成视频数据。</li>
<li>潜在变量传播模块确保合成视频的时空一致性，将前一帧的潜在特征视为噪声先验，并注入当前帧的潜在特征中。</li>
<li>流式数据采样器能够有序地采样视频片段中的原始图像。</li>
<li>Glad框架在nuScenes数据集上取得了良好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-05a296f4ffc6747c839aace5dfe97b45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe7c07e4f0673f290cb49f5dc056b064.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c151ff7bc09f39e674e4dce19167ae5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5728590cc44f2a83bddbf0835bcae53e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Learning-With-Sine-Activated-Low-rank-Matrices"><a href="#Efficient-Learning-With-Sine-Activated-Low-rank-Matrices" class="headerlink" title="Efficient Learning With Sine-Activated Low-rank Matrices"></a>Efficient Learning With Sine-Activated Low-rank Matrices</h2><p><strong>Authors:Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey</strong></p>
<p>Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition’s rank, thereby enhancing model performance. Our method proves to be a plug in enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling. </p>
<blockquote>
<p>低秩分解已成为提高神经网络架构参数效率的重要工具，在机器学习的各种应用中受到广泛关注。这些技术显著减少了参数数量，在紧凑性和性能之间取得了平衡。然而，一个常见的挑战是参数效率与模型准确性之间的权衡，减少的参数往往会导致与全秩模型相比准确性降低。在这项工作中，我们提出了一种新的理论框架，该框架将正弦函数整合到低秩分解过程中。这种方法不仅保留了低秩方法参数效率的优点，还提高了分解的秩，从而增强了模型性能。我们的方法被证明是对现有低秩模型的增强插件，其在视觉转换器（ViT）、大型语言模型（LLM）、神经辐射场（NeRF）和3D形状建模中的应用成功证实了这一点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.19243v4">PDF</a> The first two authors contributed equally. Paper accepted at ICLR   2025</p>
<p><strong>Summary</strong><br>低秩分解在提升神经网络架构的参数效率方面发挥了重要作用，广泛应用于机器学习中的不同应用。然而，参数效率的降低往往会导致模型准确性的下降。本研究提出了一种新的理论框架，该框架在低秩分解过程中融入了正弦函数，旨在保留低秩方法的参数效率优势的同时提高模型性能。此方法成功应用于Vision Transformers（ViT）、大型语言模型（LLMs）、神经辐射场（NeRF）和三维建模等领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>低秩分解是提高神经网络参数效率的重要工具，广泛应用于机器学习各个领域。</li>
<li>现有低秩方法常在参数效率和模型准确性之间做出妥协。</li>
<li>本研究提出了一种新的理论框架，结合正弦函数在低秩分解中，旨在提高模型性能。</li>
<li>该方法旨在保留低秩方法的参数效率优势。</li>
<li>此方法成功应用于Vision Transformers（ViT）、大型语言模型（LLMs）、神经辐射场（NeRF）和三维建模等领域。</li>
<li>该方法可作为现有低秩模型的增强插件。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.19243">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-56afd16b620c6ac0e32c24594078482f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-706824ac4158f3ca45116adc3b827c62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a757148268a0337590d8be5f70e3729.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1a1bcbf373db4512e06d05230780e20.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-06/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-32398b30f5a321f06b6bf6909b04c372.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-06  StageDesigner Artistic Stage Generation for Scenography via Theater   Scripts
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-06/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-767ab7fa30e8ec72218ec87b79691de4.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-03-06  2DGS-Avatar Animatable High-fidelity Clothed Avatar via 2D Gaussian   Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16355.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
