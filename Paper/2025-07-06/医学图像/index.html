<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-06  No time to train! Training-Free Reference-Based Instance Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5e80d9ec9d5814f4e933db0dc9aebc73.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-06-æ›´æ–°"><a href="#2025-07-06-æ›´æ–°" class="headerlink" title="2025-07-06 æ›´æ–°"></a>2025-07-06 æ›´æ–°</h1><h2 id="No-time-to-train-Training-Free-Reference-Based-Instance-Segmentation"><a href="#No-time-to-train-Training-Free-Reference-Based-Instance-Segmentation" class="headerlink" title="No time to train! Training-Free Reference-Based Instance Segmentation"></a>No time to train! Training-Free Reference-Based Instance Segmentation</h2><p><strong>Authors:Miguel Espinosa, Chenhongyi Yang, Linus Ericsson, Steven McDonagh, Elliot J. Crowley</strong></p>
<p>The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP). </p>
<blockquote>
<p>å†å²ä¸Šï¼Œå›¾åƒåˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ä¸€ç›´å—åˆ°æ”¶é›†å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„é«˜æˆæœ¬çš„é™åˆ¶ã€‚Segment Anything Modelï¼ˆSAMï¼‰é€šè¿‡å¯æç¤ºçš„ã€ä¸è¯­ä¹‰æ— å…³çš„åˆ†å‰²èŒƒå¼ç¼“è§£äº†è¿™ä¸€åŸå§‹é—®é¢˜ï¼Œä½†å¤„ç†æ–°å›¾åƒæ—¶ä»éœ€è¦æ‰‹åŠ¨è§†è§‰æç¤ºæˆ–å¤æ‚çš„åŸŸç›¸å…³æç¤ºç”Ÿæˆè§„åˆ™ã€‚ä¸ºäº†å‡è½»è¿™ä¸€æ–°è´Ÿæ‹…ï¼Œæˆ‘ä»¬çš„å·¥ä½œç ”ç©¶äº†åœ¨ä»…æä¾›ä¸€å°éƒ¨åˆ†å‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹è¿›è¡Œå¯¹è±¡åˆ†å‰²çš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ©ç”¨åŸºç¡€æ¨¡å‹å­¦åˆ°çš„å¼ºå¤§è¯­ä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œæ¥è¯†åˆ«å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´çš„ç›¸åº”åŒºåŸŸã€‚æˆ‘ä»¬å‘ç°è¿™ç§å¯¹åº”å…³ç³»èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„å®ä¾‹çº§åˆ†å‰²æ©è†œï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå¤šé˜¶æ®µã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•å®ç°æˆ‘ä»¬çš„æƒ³æ³•ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰æ„å»ºå†…å­˜é“¶è¡Œï¼›ï¼ˆ2ï¼‰è¡¨ç¤ºèšåˆå’Œï¼ˆ3ï¼‰è¯­ä¹‰æ„ŸçŸ¥ç‰¹å¾åŒ¹é…ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨åˆ†å‰²æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œè¾¾åˆ°äº†COCO FSODçš„å…ˆè¿›æ°´å¹³ï¼ˆ36.8% nAPï¼‰ï¼ŒPASCAL VOC Few-Shotï¼ˆ71.2% nAP50ï¼‰ï¼Œå¹¶åœ¨Cross-Domain FSODåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æ— è®­ç»ƒæ–¹æ³•ï¼ˆ22.4% nAPï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02798v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå°‘é‡å‚è€ƒå›¾åƒè¿›è¡Œå›¾åƒåˆ†å‰²çš„æ–¹æ³•ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹å­¦ä¹ åˆ°çš„è¯­ä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œåœ¨å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒä¹‹é—´è‡ªåŠ¨åŒ¹é…å¯¹åº”åŒºåŸŸï¼Œå®ç°ä¸‹æ¸¸ä»»åŠ¡çš„å®ä¾‹çº§åˆ†å‰²æ©ç ç”Ÿæˆã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ— è®­ç»ƒçš„å¤šé˜¶æ®µæ–¹æ³•ï¼ŒåŒ…æ‹¬æ„å»ºè®°å¿†åº“ã€è¡¨ç¤ºèšåˆå’Œè¯­ä¹‰æ„ŸçŸ¥ç‰¹å¾åŒ¹é…ç­‰æ­¥éª¤ï¼Œå®éªŒç»“æœæ˜¾ç¤ºåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„åˆ†å‰²åº¦é‡æŒ‡æ ‡æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Segment Anything Model (SAM) é€šè¿‡æç¤ºæ€§åˆ†å‰²æ¨¡å¼è§£å†³å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é«˜æˆæœ¬é—®é¢˜ã€‚ä½†ä»éœ€äººå·¥è§†è§‰æç¤ºæˆ–å¤æ‚è§„åˆ™æ¥å¤„ç†æ–°å›¾åƒã€‚</li>
<li>ç ”ç©¶è€…æå‡ºåˆ©ç”¨å°‘é‡å‚è€ƒå›¾åƒè¿›è¡Œå¯¹è±¡åˆ†å‰²çš„æ–°ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡åŸºç¡€æ¨¡å‹å­¦ä¹ çš„è¯­ä¹‰å…ˆéªŒçŸ¥è¯†åœ¨å‚è€ƒå›¾åƒå’Œç›®æ ‡å›¾åƒé—´è¯†åˆ«å¯¹åº”åŒºåŸŸã€‚</li>
<li>å¯¹åº”åŒºåŸŸåŒ¹é…å¯å®ç°ä¸‹æ¸¸ä»»åŠ¡çš„å®ä¾‹çº§åˆ†å‰²æ©ç è‡ªåŠ¨ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨æ— è®­ç»ƒçš„å¤šé˜¶æ®µæ–¹æ³•ï¼ŒåŒ…æ‹¬æ„å»ºè®°å¿†åº“ã€è¡¨ç¤ºèšåˆå’Œè¯­ä¹‰æ„ŸçŸ¥ç‰¹å¾åŒ¹é…ç­‰æ­¥éª¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74e6e77ec342ce3937f6f5fbad310cf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a43ed67f609bc84c9ae609956c3b7949.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34ef610aa985554711ae9ea98c2158ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a286991d15d5b81e95cb8d8dcb2551e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prompt-learning-with-bounding-box-constraints-for-medical-image-segmentation"><a href="#Prompt-learning-with-bounding-box-constraints-for-medical-image-segmentation" class="headerlink" title="Prompt learning with bounding box constraints for medical image   segmentation"></a>Prompt learning with bounding box constraints for medical image   segmentation</h2><p><strong>Authors:MÃ©lanie Gaillochet, Mehrdad Noori, Sahar Dastani, Christian Desrosiers, HervÃ© Lombaert</strong></p>
<p>Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotations-much easier to acquire-offer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multimodal datasets reveal that our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Minimel/box-prompt-learning-VFM.git">https://github.com/Minimel/box-prompt-learning-VFM.git</a> </p>
<blockquote>
<p>åŒ»å­¦é¢†åŸŸä¸­ï¼Œé€åƒç´ æ ‡æ³¨çš„è·å–å‘æ¥æ—¢è´¹æ—¶åˆæ˜‚è´µã€‚ä¸ºäº†å‡è½»è¿™ä¸€è´Ÿæ‹…ï¼ŒåŸºäºè¾¹ç•Œæ¡†æ ‡æ³¨çš„å¼±ç›‘ç£æ–¹æ³•æä¾›äº†ä¸€ç§å®ç”¨æ›¿ä»£æ–¹æ¡ˆï¼Œå› ä¸ºè¾¹ç•Œæ¡†æ ‡æ³¨æ›´å®¹æ˜“è·å–ã€‚å½“æä¾›æç¤ºï¼ˆå¦‚ç‚¹æˆ–è¾¹ç•Œæ¡†ï¼‰æ—¶ï¼Œè§†è§‰åŸºç¡€æ¨¡å‹æœ€è¿‘æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„åˆ†å‰²æ€§èƒ½ã€‚æç¤ºå­¦ä¹ é€šè¿‡é€‚åº”ä¸‹æ¸¸ä»»åŠ¡å’Œè‡ªåŠ¨åŒ–åˆ†å‰²æ¥åˆ©ç”¨è¿™äº›æ¨¡å‹ï¼Œä»è€Œå‡å°‘ç”¨æˆ·å¹²é¢„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æç¤ºå­¦ä¹ æ–¹æ³•ä¾èµ–äºå®Œå…¨æ³¨é‡Šçš„åˆ†å‰²æ©è†œã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªç»“åˆåŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ä¸å¼±ç›‘ç£åˆ†å‰²çš„æ ‡æ³¨æ•ˆç‡çš„æ–°æ¡†æ¶ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨è¾¹ç•Œæ¡†æ ‡æ³¨æ¥è‡ªåŠ¨ä¸ºåŸºç¡€æ¨¡å‹ç”Ÿæˆæç¤ºã€‚æˆ‘ä»¬æå‡ºçš„ä¼˜åŒ–æ–¹æ¡ˆå°†è¾¹ç•Œæ¡†æ ‡æ³¨ç”Ÿæˆçš„å¤šä¸ªçº¦æŸä¸ç”±åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾ç›¸ç»“åˆã€‚åœ¨å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨æœ‰é™æ•°æ®è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬çš„å¼±ç›‘ç£æ–¹æ³•å®ç°äº†å¹³å‡84.90%çš„Diceå¾—åˆ†ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å…¨ç›‘ç£å’Œå¼±ç›‘ç£æ–¹æ³•ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Minimel/box-prompt-learning-VFM.git">https://github.com/Minimel/box-prompt-learning-VFM.git</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02743v1">PDF</a> Accepted to IEEE Transactions on Biomedical Engineering (TMBE), 14   pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåƒç´ æ ‡æ³¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ ‡æ³¨åŠ³åŠ¨å¼ºåº¦å¤§ä¸”æˆæœ¬é«˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡åˆ©ç”¨è¾¹ç•Œæ¡†æ ‡æ³¨ç­‰æ›´æ˜“äºè·å–çš„æ•°æ®æ¥å‡è½»è¿™ä¸€è´Ÿæ‹…ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆåŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ä¸å¼±ç›‘ç£åˆ†å‰²çš„æ ‡æ³¨æ•ˆç‡çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨åŒ–ç”ŸæˆåŸºç¡€æ¨¡å‹çš„æç¤ºï¼Œä»…ä½¿ç”¨è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œå¹¶é€šè¿‡æ•´åˆå¤šä¸ªç”±è¾¹ç•Œæ¡†æ ‡æ³¨ç”Ÿæˆçš„çº¦æŸæ¡ä»¶å’Œä¼ªæ ‡ç­¾è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æœ‰é™æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å¹³å‡Diceå¾—åˆ†ä¸º84.9%ï¼Œä¼˜äºç°æœ‰çš„å…¨ç›‘ç£å’Œå¼±ç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åƒç´ çº§æ ‡æ³¨åŠ³åŠ¨å¼ºåº¦å¤§ä¸”æˆæœ¬é«˜ã€‚</li>
<li>å¼±ç›‘ç£æ–¹æ³•é€šè¿‡ä½¿ç”¨æ›´æ˜“äºè·å–çš„æ ‡æ³¨æ•°æ®ï¼ˆå¦‚è¾¹ç•Œæ¡†æ ‡æ³¨ï¼‰ä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰æç¤ºå­¦ä¹ æ–¹æ³•ä¾èµ–äºå®Œå…¨æ³¨é‡Šçš„åˆ†å‰²æ©è†œã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ä¸å¼±ç›‘ç£åˆ†å‰²çš„æ ‡æ³¨æ•ˆç‡ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨åŒ–ç”ŸæˆåŸºç¡€æ¨¡å‹çš„æç¤ºï¼Œä»…ä½¿ç”¨è¾¹ç•Œæ¡†æ ‡æ³¨ã€‚</li>
<li>é€šè¿‡æ•´åˆå¤šä¸ªç”±è¾¹ç•Œæ¡†æ ‡æ³¨ç”Ÿæˆçš„çº¦æŸæ¡ä»¶å’Œä¼ªæ ‡ç­¾è¿›è¡Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d9c46211e435d6944cb5eab07736a89c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e77419c7406a431d9dfdf91b64745eab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44591fd6ca8b4e1991ee8b3e64581adb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-652c57fe992bc7234e1d1099182f15e9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Structure-aware-Semantic-Discrepancy-and-Consistency-for-3D-Medical-Image-Self-supervised-Learning"><a href="#Structure-aware-Semantic-Discrepancy-and-Consistency-for-3D-Medical-Image-Self-supervised-Learning" class="headerlink" title="Structure-aware Semantic Discrepancy and Consistency for 3D Medical   Image Self-supervised Learning"></a>Structure-aware Semantic Discrepancy and Consistency for 3D Medical   Image Self-supervised Learning</h2><p><strong>Authors:Tan Pan, Zhaorui Tan, Kaiyu Guo, Dongli Xu, Weidi Xu, Chen Jiang, Xin Guo, Yuan Qi, Yuan Cheng</strong></p>
<p>3D medical image self-supervised learning (mSSL) holds great promise for medical analysis. Effectively supporting broader applications requires considering anatomical structure variations in location, scale, and morphology, which are crucial for capturing meaningful distinctions. However, previous mSSL methods partition images with fixed-size patches, often ignoring the structure variations. In this work, we introduce a novel perspective on 3D medical images with the goal of learning structure-aware representations. We assume that patches within the same structure share the same semantics (semantic consistency) while those from different structures exhibit distinct semantics (semantic discrepancy). Based on this assumption, we propose an mSSL framework named $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency in two steps. First, $S^2DC$ enforces distinct representations for different patches to increase semantic discrepancy by leveraging an optimal transport strategy. Second, $S^2DC$ advances semantic consistency at the structural level based on neighborhood similarity distribution. By bridging patch-level and structure-level representations, $S^2DC$ achieves structure-aware representations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3 modalities, our proposed method consistently outperforms the state-of-the-art methods in mSSL. </p>
<blockquote>
<p>åœ¨åŒ»å­¦åˆ†æé¢†åŸŸï¼Œä¸‰ç»´åŒ»å­¦å›¾åƒè‡ªç›‘ç£å­¦ä¹ ï¼ˆmSSLï¼‰å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°æ”¯æŒæ›´å¹¿æ³›çš„åº”ç”¨ï¼Œéœ€è¦è€ƒè™‘ä½ç½®ã€å°ºåº¦å’Œå½¢æ€ä¸Šçš„è§£å‰–ç»“æ„å˜å¼‚ï¼Œè¿™å¯¹äºæ•æ‰æœ‰æ„ä¹‰çš„å·®å¼‚è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„mSSLæ–¹æ³•ä½¿ç”¨å›ºå®šå¤§å°çš„æ–‘å—å¯¹å›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œç»å¸¸å¿½ç•¥ç»“æ„å˜å¼‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…³äºä¸‰ç»´åŒ»å­¦å›¾åƒçš„æ–°è§†è§’ï¼Œç›®æ ‡æ˜¯å­¦ä¹ ç»“æ„æ„ŸçŸ¥è¡¨ç¤ºã€‚æˆ‘ä»¬å‡è®¾åŒä¸€ç»“æ„å†…çš„æ–‘å—å…·æœ‰ç›¸åŒçš„è¯­ä¹‰ï¼ˆè¯­ä¹‰ä¸€è‡´æ€§ï¼‰ï¼Œè€Œä¸åŒç»“æ„çš„æ–‘å—åˆ™è¡¨ç°å‡ºä¸åŒçš„è¯­ä¹‰ï¼ˆè¯­ä¹‰å·®å¼‚ï¼‰ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º$S^2DC$çš„mSSLæ¡†æ¶ï¼Œåˆ†ä¸¤æ­¥å®ç°ç»“æ„æ„ŸçŸ¥è¯­ä¹‰å·®å¼‚å’Œä¸€è‡´æ€§ã€‚é¦–å…ˆï¼Œ$S^2DC$å¼ºåˆ¶ä¸åŒæ–‘å—æœ‰ä¸åŒçš„è¡¨ç¤ºï¼Œä»¥å¢åŠ è¯­ä¹‰å·®å¼‚ï¼Œå¹¶åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ç­–ç•¥ã€‚å…¶æ¬¡ï¼Œ$S^2DC$åŸºäºé‚»åŸŸç›¸ä¼¼æ€§åˆ†å¸ƒåœ¨ç»“æ„å±‚é¢ä¸Šæ¨è¿›è¯­ä¹‰ä¸€è‡´æ€§ã€‚é€šè¿‡æ¡¥æ¥æ–‘å—å±‚é¢å’Œç»“æ„å±‚é¢çš„è¡¨ç¤ºï¼Œ$S^2DC$å®ç°äº†ç»“æ„æ„ŸçŸ¥è¡¨ç¤ºã€‚ç»è¿‡åœ¨10ä¸ªæ•°æ®é›†ã€4é¡¹ä»»åŠ¡å’Œ3ç§æ¨¡æ€ä¸Šçš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æ‰€æå‡ºçš„æ–¹æ³•åœ¨mSSLä¸­å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02581v1">PDF</a> Accepted by ICCV25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆmSSLï¼‰æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†ç»“æ„å˜åŒ–æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§†è§’ï¼Œæ—¨åœ¨å­¦ä¹ ç»“æ„æ„ŸçŸ¥è¡¨ç¤ºã€‚é€šè¿‡å‡è®¾åŒä¸€ç»“æ„å†…çš„è¡¥ä¸å…·æœ‰ç›¸åŒçš„è¯­ä¹‰ï¼Œè€Œä¸åŒç»“æ„çš„è¡¥ä¸å…·æœ‰ä¸åŒçš„è¯­ä¹‰ï¼Œæå‡ºäº†åä¸º$S^2DC$çš„mSSLæ¡†æ¶ï¼Œå®ç°ç»“æ„æ„ŸçŸ¥è¯­ä¹‰å·®å¼‚å’Œä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–ä¼ è¾“ç­–ç•¥å¢å¼ºä¸åŒè¡¥ä¸ä¹‹é—´çš„è¯­ä¹‰å·®å¼‚ï¼Œå¹¶åŸºäºé‚»åŸŸç›¸ä¼¼æ€§åˆ†å¸ƒæé«˜ç»“æ„çº§åˆ«çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚é€šè¿‡å¯¹10ä¸ªæ•°æ®é›†ã€4ä¸ªä»»åŠ¡å’Œ3ç§æ¨¡æ€çš„è¯„ä¼°ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨mSSLä¸­å§‹ç»ˆä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè‡ªç›‘ç£å­¦ä¹ ï¼ˆmSSLï¼‰åœ¨åŒ»å­¦åˆ†æä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†éœ€è¦è€ƒè™‘è§£å‰–ç»“æ„çš„å˜å¼‚ã€‚</li>
<li>ä¼ ç»ŸmSSLæ–¹æ³•é‡‡ç”¨å›ºå®šå¤§å°çš„å›¾åƒåŒºå—åˆ’åˆ†ï¼Œå®¹æ˜“å¿½ç•¥ç»“æ„å˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†è§’æ¥å­¦ä¹ ç»“æ„æ„ŸçŸ¥è¡¨ç¤ºï¼Œå‡è®¾åŒä¸€ç»“æ„å†…çš„è¡¥ä¸å…·æœ‰ç›¸åŒè¯­ä¹‰ï¼Œä¸åŒç»“æ„çš„è¡¥ä¸å…·æœ‰ä¸åŒè¯­ä¹‰ã€‚</li>
<li>æå‡ºäº†åä¸º$S^2DC$çš„mSSLæ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–ä¼ è¾“ç­–ç•¥å¢å¼ºä¸åŒè¡¥ä¸ä¹‹é—´çš„è¯­ä¹‰å·®å¼‚ã€‚</li>
<li>$S^2DC$æ¡†æ¶åŸºäºé‚»åŸŸç›¸ä¼¼æ€§åˆ†å¸ƒæé«˜ç»“æ„çº§åˆ«çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>$S^2DC$å®ç°äº†è·¨ç»“æ„æ„ŸçŸ¥çš„è¡¨ç¤ºå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8683195bacce850ad6d0bc10a3dfeb9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57fd0e8a0572a86ec9f64bc45002b7ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef73ccb7f1d1f415b78219a97f03fd84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0783d553bd837c7add0f11d7edb8196.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eecd6ac84e39c43a187af153cfde33cc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="F-2TTA-Free-Form-Test-Time-Adaptation-on-Cross-Domain-Medical-Image-Classification-via-Image-Level-Disentangled-Prompt-Tuning"><a href="#F-2TTA-Free-Form-Test-Time-Adaptation-on-Cross-Domain-Medical-Image-Classification-via-Image-Level-Disentangled-Prompt-Tuning" class="headerlink" title="F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image   Classification via Image-Level Disentangled Prompt Tuning"></a>F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image   Classification via Image-Level Disentangled Prompt Tuning</h2><p><strong>Authors:Wei Li, Jingyang Zhang, Lihao Liu, Guoan Wang, Junjun He, Yang Chen, Lixu Gu</strong></p>
<p>Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a source model to unseen medical sites using unlabeled test data, due to the high cost of data annotation. Existing TTA methods consider scenarios where data from one or multiple domains arrives in complete domain units. However, in clinical practice, data usually arrives in domain fragments of arbitrary lengths and in random arrival orders, due to resource constraints and patient variability. This paper investigates a practical Free-Form Test-Time Adaptation (F$^{2}$TTA) task, where a source model is adapted to such free-form domain fragments, with shifts occurring between fragments unpredictably. In this setting, these shifts could distort the adaptation process. To address this problem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT) framework. I-DiPT employs an image-invariant prompt to explore domain-invariant representations for mitigating the unpredictable shifts, and an image-specific prompt to adapt the source model to each test image from the incoming fragments. The prompts may suffer from insufficient knowledge representation since only one image is available for training. To overcome this limitation, we first introduce Uncertainty-oriented Masking (UoM), which encourages the prompts to extract sufficient information from the incoming image via masked consistency learning driven by the uncertainty of the source model representations. Then, we further propose a Parallel Graph Distillation (PGD) method that reuses knowledge from historical image-specific and image-invariant prompts through parallel graph networks. Experiments on breast cancer and glaucoma classification demonstrate the superiority of our method over existing TTA approaches in F$^{2}$TTA. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mar-cry/F2TTA">https://github.com/mar-cry/F2TTA</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆTTAï¼‰å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºåˆ©ç”¨æ— æ ‡ç­¾çš„æµ‹è¯•æ•°æ®å°†æºæ¨¡å‹é€‚åº”äºæœªè§è¿‡çš„åŒ»ç–—ç«™ç‚¹ï¼Œä»¥é™ä½æ•°æ®æ ‡æ³¨çš„é«˜æˆæœ¬ã€‚ç°æœ‰çš„TTAæ–¹æ³•è€ƒè™‘çš„æ˜¯æ¥è‡ªä¸€ä¸ªæˆ–å¤šä¸ªåŸŸçš„æ•°æ®ä»¥å®Œæ•´çš„åŸŸå•å…ƒå½¢å¼åˆ°è¾¾çš„åœºæ™¯ã€‚ç„¶è€Œï¼Œåœ¨ä¸´åºŠå®è·µä¸­ï¼Œç”±äºèµ„æºçº¦æŸå’Œæ‚£è€…å·®å¼‚æ€§ï¼Œæ•°æ®é€šå¸¸ä»¥ä»»æ„é•¿åº¦çš„åŸŸç‰‡æ®µå½¢å¼éšæœºåˆ°è¾¾ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ä¸ªå®ç”¨çš„è‡ªç”±å½¢å¼æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆF$^{2}$TTAï¼‰ä»»åŠ¡ï¼Œå…¶ä¸­æºæ¨¡å‹è¢«é€‚åº”äºè¿™æ ·çš„è‡ªç”±å½¢å¼åŸŸç‰‡æ®µï¼Œç‰‡æ®µä¹‹é—´çš„å˜åŒ–æ˜¯ä¸å¯é¢„æµ‹çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™äº›å˜åŒ–å¯èƒ½ä¼šæ‰­æ›²é€‚åº”è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å›¾åƒçº§è§£è€¦æç¤ºè°ƒæ•´ï¼ˆI-DiPTï¼‰æ¡†æ¶ã€‚I-DiPTé‡‡ç”¨å›¾åƒä¸å˜æç¤ºæ¥æ¢ç´¢åŸŸä¸å˜è¡¨ç¤ºï¼Œä»¥ç¼“è§£ä¸å¯é¢„æµ‹çš„å˜åŒ–ï¼Œå¹¶é‡‡ç”¨å›¾åƒç‰¹å®šæç¤ºæ¥å°†æºæ¨¡å‹é€‚åº”äºæ¥è‡ªä¼ å…¥ç‰‡æ®µçš„æ¯ä¸ªæµ‹è¯•å›¾åƒã€‚ç”±äºåªæœ‰ä¸€å¼ å›¾åƒå¯ç”¨äºè®­ç»ƒï¼Œæç¤ºå¯èƒ½é¢ä¸´çŸ¥è¯†è¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†é¢å‘ä¸ç¡®å®šæ€§çš„æ©è”½ï¼ˆUoMï¼‰ï¼Œå®ƒé€šè¿‡æºæ¨¡å‹è¡¨ç¤ºçš„ä¸ç¡®å®šæ€§æ¥é¼“åŠ±æç¤ºä»ä¼ å…¥çš„å›¾åƒä¸­æå–è¶³å¤Ÿçš„ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å¹¶è¡Œå›¾è’¸é¦ï¼ˆPGDï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¹¶è¡Œå›¾ç½‘ç»œé‡ç”¨æ¥è‡ªå†å²å›¾åƒç‰¹å®šå’Œå›¾åƒä¸å˜æç¤ºçš„çŸ¥è¯†ã€‚å¯¹ä¹³è…ºç™Œå’Œé’å…‰çœ¼åˆ†ç±»çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨F$^{2}$TTAä¸­ä¼˜äºç°æœ‰çš„TTAæ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mar-cry/F2TTA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mar-cry/F2TTAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02437v1">PDF</a> This paper has been submitted to relevant journals</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆTest-Time Adaptationï¼ŒTTAï¼‰åœ¨æ— æ ‡ç­¾æµ‹è¯•æ•°æ®çš„æƒ…å†µä¸‹å¦‚ä½•è‡ªé€‚åº”åœ°åº”ç”¨è‡³ä¸åŒçš„åŒ»ç–—ç«™ç‚¹çš„é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯åœ¨å®è·µä¸­å¸¸å¸¸é¢ä¸´ä¸åŒåŸŸç‰‡æ®µï¼ˆdomain fragmentsï¼‰ä»»æ„é•¿åº¦ä»¥åŠéšæœºé¡ºåºåˆ°è¾¾çš„æƒ…å†µï¼Œè¿™å¢åŠ äº†è‡ªé€‚åº”è¿‡ç¨‹çš„éš¾åº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å®ç”¨çš„è‡ªç”±å½¢å¼æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆF^{2}TTAï¼‰ä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„å›¾åƒçº§è§£è€¦æç¤ºè°ƒæ•´ï¼ˆI-DiPTï¼‰æ¡†æ¶æ¥é€‚åº”è¿™ç§æƒ…å†µã€‚è¯¥æ¡†æ¶åŒ…å«æ¢ç´¢ä¸å˜æ€§åŸŸçš„å›¾åƒä¸å˜æç¤ºæ¥è§£å†³æœªçŸ¥çš„å˜åŒ–ä»¥åŠé€šè¿‡ç‰¹å®šçš„å›¾åƒæç¤ºå»é€‚åº”æ¨¡å‹åœ¨å…¥ç»„ç‰‡æ®µä¸­å¯¹æµ‹è¯•å›¾åƒçš„å»ºæ¨¡é—®é¢˜ã€‚ä¸ºäº†æé«˜åœ¨åªæ¥å—å•ä¸ªå›¾åƒè®­ç»ƒçš„æ¡ä»¶ä¸‹æ‰€éšå«çŸ¥è¯†ä»£è¡¨èƒ½åŠ›çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºä¸ç¡®å®šæ€§å¯¼å‘æ©ç›–çš„ç­–ç•¥ä»¥é€šè¿‡æ¨¡å‹çš„æœªç¡®å®šæ€§æ‰€é©±åŠ¨çš„é®æ©ä¸€è‡´æ€§å­¦ä¹ å¼•å¯¼æç¤ºè¿›è¡Œå……è¶³çš„ä¿¡æ¯æŠ½å–ã€‚åŒæ—¶æå‡ºä¸€ç§å¹¶è¡Œå›¾è’¸é¦æ–¹æ³•ä»¥åˆ©ç”¨å›¾åƒç‰¹å®šçš„ä»¥åŠå›¾åƒä¸å˜çš„æç¤ºå†å²çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜åœ¨ä¹³è…ºç™Œå’Œé’å…‰çœ¼åˆ†ç±»ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„TTAæ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡å…¬å¼€é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>å…³é”®å‘ç°</strong></p>
<ul>
<li>è‡ªç”±å½¢å¼çš„æµ‹è¯•æ—¶é—´è‡ªé€‚åº”æ˜¯ä¸€ä¸ªè€ƒè™‘å®é™…åœºæ™¯ä¸­åŒ»ç–—æ•°æ®éšæœºåˆ°è¾¾çš„æ–°é—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä»»æ„é•¿åº¦çš„åŸŸç‰‡æ®µå’Œä¸å¯é¢„æµ‹çš„è½¬ç§»å˜åŒ–ã€‚</li>
<li>å›¾åƒçº§è§£è€¦æç¤ºè°ƒæ•´æ¡†æ¶ç»“åˆäº†å›¾åƒä¸å˜æç¤ºå’Œç‰¹å®šå›¾åƒæç¤ºæ¥åº”å¯¹ä¸Šè¿°é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69bba76f5499909a2197724c876cf86b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df80940d64e266827eb7487a3f7ae3b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e3b30f0de40bab5b49b9525008cd70a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35678f54b83998f41cd452d3581e2389.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97fc0b58389db129da2e6dff2f77b82b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TABNet-A-Triplet-Augmentation-Self-Recovery-Framework-with-Boundary-Aware-Pseudo-Labels-for-Medical-Image-Segmentation"><a href="#TABNet-A-Triplet-Augmentation-Self-Recovery-Framework-with-Boundary-Aware-Pseudo-Labels-for-Medical-Image-Segmentation" class="headerlink" title="TABNet: A Triplet Augmentation Self-Recovery Framework with   Boundary-Aware Pseudo-Labels for Medical Image Segmentation"></a>TABNet: A Triplet Augmentation Self-Recovery Framework with   Boundary-Aware Pseudo-Labels for Medical Image Segmentation</h2><p><strong>Authors:Peilin Zhang, Shaouxan Wua, Jun Feng, Zhuo Jin, Zhizezhang Gao, Jingkun Chen, Yaqiong Xing, Xiao Zhang</strong></p>
<p>Background and objective: Medical image segmentation is a core task in various clinical applications. However, acquiring large-scale, fully annotated medical image datasets is both time-consuming and costly. Scribble annotations, as a form of sparse labeling, provide an efficient and cost-effective alternative for medical image segmentation. However, the sparsity of scribble annotations limits the feature learning of the target region and lacks sufficient boundary supervision, which poses significant challenges for training segmentation networks. Methods: We propose TAB Net, a novel weakly-supervised medical image segmentation framework, consisting of two key components: the triplet augmentation self-recovery (TAS) module and the boundary-aware pseudo-label supervision (BAP) module. The TAS module enhances feature learning through three complementary augmentation strategies: intensity transformation improves the modelâ€™s sensitivity to texture and contrast variations, cutout forces the network to capture local anatomical structures by masking key regions, and jigsaw augmentation strengthens the modeling of global anatomical layout by disrupting spatial continuity. By guiding the network to recover complete masks from diverse augmented inputs, TAS promotes a deeper semantic understanding of medical images under sparse supervision. The BAP module enhances pseudo-supervision accuracy and boundary modeling by fusing dual-branch predictions into a loss-weighted pseudo-label and introducing a boundary-aware loss for fine-grained contour refinement. Results: Experimental evaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB Net significantly outperforms state-of-the-art methods for scribble-based weakly supervised segmentation. Moreover, it achieves performance comparable to that of fully supervised methods. </p>
<blockquote>
<p>èƒŒæ™¯ä¸ç›®æ ‡ï¼šåŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯å„ç§ä¸´åºŠåº”ç”¨ä¸­çš„æ ¸å¿ƒä»»åŠ¡ã€‚ç„¶è€Œï¼Œè·å–å¤§è§„æ¨¡ã€å®Œå…¨æ ‡æ³¨çš„åŒ»å­¦å›¾åƒæ•°æ®é›†æ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚æ¶‚é¸¦æ ‡æ³¨ä½œä¸ºä¸€ç§ç¨€ç–æ ‡æ³¨å½¢å¼ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†é«˜æ•ˆä¸”ç»æµå®æƒ çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ¶‚é¸¦æ ‡æ³¨çš„ç¨€ç–æ€§é™åˆ¶äº†ç›®æ ‡åŒºåŸŸçš„ç‰¹å¾å­¦ä¹ ï¼Œå¹¶ä¸”ç¼ºä¹è¶³å¤Ÿçš„è¾¹ç•Œç›‘ç£ï¼Œè¿™ä¸ºè®­ç»ƒåˆ†å‰²ç½‘ç»œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†TAB Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸‰å…ƒç»„å¢å¼ºè‡ªæ¢å¤ï¼ˆTASï¼‰æ¨¡å—å’Œè¾¹ç•Œæ„ŸçŸ¥ä¼ªæ ‡ç­¾ç›‘ç£ï¼ˆBAPï¼‰æ¨¡å—ã€‚TASæ¨¡å—é€šè¿‡ä¸‰ç§äº’è¡¥çš„å¢å¼ºç­–ç•¥å¼ºåŒ–ç‰¹å¾å­¦ä¹ ï¼šå¼ºåº¦è½¬æ¢æé«˜æ¨¡å‹å¯¹çº¹ç†å’Œå¯¹æ¯”åº¦å˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œåˆ‡æ–­å…³é”®åŒºåŸŸè¿«ä½¿ç½‘ç»œæ•è·å±€éƒ¨è§£å‰–ç»“æ„ï¼Œæ‹¼å›¾å¢å¼ºé€šè¿‡ç ´åç©ºé—´è¿ç»­æ€§æ¥åŠ å¼ºå…¨å±€è§£å‰–å¸ƒå±€çš„å»ºæ¨¡ã€‚é€šè¿‡å¼•å¯¼ç½‘ç»œä»å„ç§å¢å¼ºè¾“å…¥ä¸­æ¢å¤å®Œæ•´çš„æ©è†œï¼ŒTASåœ¨ç¨€ç–ç›‘ç£ä¸‹ä¿ƒè¿›äº†å¯¹åŒ»å­¦å›¾åƒçš„æ›´æ·±å±‚æ¬¡è¯­ä¹‰ç†è§£ã€‚BAPæ¨¡å—é€šè¿‡èåˆåŒåˆ†æ”¯é¢„æµ‹ç»“æœç”Ÿæˆä¸€ä¸ªæŸå¤±åŠ æƒä¼ªæ ‡ç­¾ï¼Œå¹¶å¼•å…¥è¾¹ç•Œæ„ŸçŸ¥æŸå¤±æ¥è¿›è¡Œç²¾ç»†è½®å»“ä¿®æ•´ï¼Œä»è€Œæé«˜ä¼ªç›‘ç£çš„å‡†ç¡®æ€§å’Œè¾¹ç•Œå»ºæ¨¡ã€‚</p>
<p>ç»“æœï¼šåœ¨ACDCå’ŒMSCMR segä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒTAB Netåœ¨åŸºäºæ¶‚é¸¦çš„å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•ä¸­æ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶æ€§èƒ½ä¸å…¨ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02399v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTAB Netçš„å¼±ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ ‡æ³¨æ•°æ®è·å–å›°éš¾ä¸”æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸‰å…ƒç»„å¢å¼ºè‡ªæ¢å¤ï¼ˆTASï¼‰æ¨¡å—å’Œè¾¹ç•Œæ„ŸçŸ¥ä¼ªæ ‡ç­¾ç›‘ç£ï¼ˆBAPï¼‰æ¨¡å—ã€‚é€šè¿‡å¢å¼ºç‰¹å¾å­¦ä¹ å’Œç²¾ç»†çš„è¾¹ç•Œå»ºæ¨¡ï¼ŒTAB Netåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºäºæ¶‚é¸¦çš„å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œå¹¶ä¸å…¨ç›‘ç£æ–¹æ³•çš„è¡¨ç°ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TAB Netæ˜¯ä¸€ç§å¼±ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®è·å–å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šTASæ¨¡å—å’ŒBAPæ¨¡å—ã€‚</li>
<li>TASæ¨¡å—é€šè¿‡ä¸‰ç§å¢å¼ºç­–ç•¥ä¿ƒè¿›ç‰¹å¾å­¦ä¹ ï¼šå¼ºåº¦å˜æ¢ã€é®æŒ¡å’Œæ‹¼å›¾å¢å¼ºã€‚</li>
<li>BAPæ¨¡å—é€šè¿‡èåˆåŒåˆ†æ”¯é¢„æµ‹å’Œå¼•å…¥è¾¹ç•Œæ„ŸçŸ¥æŸå¤±æ¥æé«˜ä¼ªç›‘ç£å‡†ç¡®æ€§å’Œè¾¹ç•Œå»ºæ¨¡ç²¾åº¦ã€‚</li>
<li>åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ACDCå’ŒMSCMR segä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTAB Netåœ¨æ¶‚é¸¦åŸºç¡€ä¸Šçš„å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>TAB Netçš„è¡¨ç°ä¸å…¨ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”æˆæœ¬æ•ˆç›Šé«˜çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-027b7b5f4f8fda48ef94102729ea721e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e163b4edbf160d7a2dc5496394c9d947.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e80d9ec9d5814f4e933db0dc9aebc73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-680b4af977f3703e8357092fdd614007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-564aa04a1683f0d2692658c467b245eb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Continual-Multiple-Instance-Learning-with-Enhanced-Localization-for-Histopathological-Whole-Slide-Image-Analysis"><a href="#Continual-Multiple-Instance-Learning-with-Enhanced-Localization-for-Histopathological-Whole-Slide-Image-Analysis" class="headerlink" title="Continual Multiple Instance Learning with Enhanced Localization for   Histopathological Whole Slide Image Analysis"></a>Continual Multiple Instance Learning with Enhanced Localization for   Histopathological Whole Slide Image Analysis</h2><p><strong>Authors:Byung Hyun Lee, Wongi Jeong, Woojae Han, Kyoungbun Lee, Se Young Chun</strong></p>
<p>Multiple instance learning (MIL) significantly reduced annotation costs via bag-level weak labels for large-scale images, such as histopathological whole slide images (WSIs). However, its adaptability to continual tasks with minimal forgetting has been rarely explored, especially on instance classification for localization. Weakly incremental learning for semantic segmentation has been studied for continual localization, but it focused on natural images, leveraging global relationships among hundreds of small patches (e.g., $16 \times 16$) using pre-trained models. This approach seems infeasible for MIL localization due to enormous amounts ($\sim 10^5$) of large patches (e.g., $256 \times 256$) and no available global relationships such as cancer cells. To address these challenges, we propose Continual Multiple Instance Learning with Enhanced Localization (CoMEL), an MIL framework for both localization and adaptability with minimal forgetting. CoMEL consists of (1) Grouped Double Attention Transformer (GDAT) for efficient instance encoding, (2) Bag Prototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling, and (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting in both bag and instance classification. Extensive experiments on three public WSI datasets demonstrate superior performance of CoMEL, outperforming the prior arts by up to $11.00%$ in bag-level accuracy and up to $23.4%$ in localization accuracy under the continual MIL setup. </p>
<blockquote>
<p>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰é€šè¿‡å¤§è§„æ¨¡å›¾åƒï¼ˆå¦‚ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼‰çš„è¢‹çº§å¼±æ ‡ç­¾æ˜¾è‘—é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚ç„¶è€Œï¼Œå…¶åœ¨è¿ç»­ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ä»¥åŠæœ€å°åŒ–é—å¿˜çš„ç ”ç©¶å¾ˆå°‘æ¶‰åŠï¼Œç‰¹åˆ«æ˜¯åœ¨å®šä½å®ä¾‹åˆ†ç±»æ–¹é¢ã€‚å¯¹äºè¿ç»­å®šä½çš„å¼±å¢é‡å­¦ä¹ è¯­ä¹‰åˆ†å‰²å·²æœ‰æ‰€ç ”ç©¶ï¼Œä½†ä¸»è¦åº”ç”¨äºè‡ªç„¶å›¾åƒï¼Œåˆ©ç”¨æ•°ç™¾ä¸ªå°è¡¥ä¸ä¹‹é—´çš„å…¨å±€å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œçš„ $16 \times 16$ è¡¥ä¸ï¼‰ã€‚ç”±äºå­˜åœ¨å¤§é‡ï¼ˆ$\sim 10^5$ ä¸ªï¼‰å¤§å‹è¡¥ä¸ï¼ˆä¾‹å¦‚ï¼Œ$256 \times 256$ï¼‰å¹¶ä¸”ä¸å­˜åœ¨è¯¸å¦‚ç™Œç»†èƒä¹‹ç±»çš„å…¨å±€å…³ç³»ï¼Œè¿™ç§æ–¹æ³•ä¼¼ä¹ä¸é€‚ç”¨äº MIL å®šä½ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰æœ€å°é—å¿˜é€‚åº”æ€§çš„æŒç»­å¤šå®ä¾‹å­¦ä¹ å¢å¼ºå®šä½ï¼ˆCoMELï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå®šä½å’Œé€‚åº”æ€§çš„ MIL æ¡†æ¶ã€‚CoMEL åŒ…æ‹¬ï¼ˆ1ï¼‰ç”¨äºé«˜æ•ˆå®ä¾‹ç¼–ç çš„åˆ†ç»„åŒé‡æ³¨æ„åŠ›è½¬æ¢å™¨ï¼ˆGDATï¼‰ï¼Œï¼ˆ2ï¼‰åŸºäºè¢‹åŸå‹ä¼ªæ ‡ç­¾çš„å¯é å®ä¾‹ä¼ªæ ‡ç­¾ï¼ˆBPPLï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰æ­£äº¤åŠ æƒä½ç§©è‡ªé€‚åº”æŠ€æœ¯ï¼ˆOWLoRAï¼‰æ¥å‡è½»åœ¨è¿ç»­è®­ç»ƒæœŸé—´çš„åŒ…åˆ†ç±»å’Œå®ä¾‹åˆ†ç±»ä¸­å¯èƒ½å‘ç”Ÿçš„é—å¿˜ç°è±¡ã€‚åœ¨ä¸‰ä¸ªå…¬å…± WSI æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoMEL çš„æ€§èƒ½ä¼˜äºå…ˆå‰æŠ€æœ¯ï¼Œåœ¨è¢‹çº§å‡†ç¡®åº¦ä¸Šé«˜å‡º $11.00%$ ï¼Œåœ¨å®šä½å‡†ç¡®åº¦ä¸Šé«˜å‡º $23.4%$ ã€‚åœ¨è¿ç»­ MIL è®¾ç½®ä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02395v1">PDF</a> Accepted at ICCV 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰çš„æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡å›¾åƒï¼ˆå¦‚ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼‰çš„è¢‹çº§å¼±æ ‡ç­¾æ˜¾è‘—é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚ç„¶è€Œï¼Œå®ƒåœ¨è¿ç»­ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ä»¥åŠæœ€å°åŒ–é—å¿˜çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ä¾‹åˆ†ç±»å®šä½æ–¹é¢çš„åº”ç”¨ã€‚è™½ç„¶å·²æœ‰ç ”ç©¶æ¢è®¨äº†è¿ç»­è¯­ä¹‰åˆ†å‰²çš„å¼±å¢é‡å­¦ä¹ ï¼Œä½†è¿™ç§æ–¹æ³•ä¸»è¦é€‚ç”¨äºè‡ªç„¶å›¾åƒï¼Œä¾èµ–äºæ•°ç™¾ä¸ªå°è¡¥ä¸ä¹‹é—´çš„å…¨å±€å…³ç³»å¹¶ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚å¯¹äºå…·æœ‰å¤§é‡å¤§è§„æ¨¡è¡¥ä¸çš„MILå®šä½æ¥è¯´ï¼Œè¿™ç§æ–¹æ³•çš„é€‚ç”¨æ€§ä»¤äººè´¨ç–‘ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæŒç»­çš„å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶â€”â€”å¢å¼ºçš„å®šä½èƒ½åŠ›ï¼ˆCoMELï¼‰ã€‚CoMELç»“åˆäº†åˆ†ç»„åŒæ³¨æ„åŠ›è½¬æ¢å™¨ï¼ˆGDATï¼‰è¿›è¡Œé«˜æ•ˆå®ä¾‹ç¼–ç ã€åŸºäºè¢‹åŸå‹ä¼ªæ ‡ç­¾ï¼ˆBPPLï¼‰çš„å¯é å®ä¾‹ä¼ªæ ‡ç­¾ä»¥åŠæ­£äº¤åŠ æƒä½ç§©é€‚åº”ï¼ˆOWLoRAï¼‰ä»¥ç¼“è§£é—å¿˜é—®é¢˜ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€çš„å…¨åˆ‡ç‰‡å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œåœ¨è¿ç»­å¤šå®ä¾‹å­¦ä¹ çš„ç¯å¢ƒä¸­ï¼ŒCoMELçš„è¢‹çº§å‡†ç¡®ç‡å’Œå®šä½å‡†ç¡®ç‡å‡æ˜¾è‘—ä¼˜äºå…ˆå‰æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åˆ©ç”¨å¤§è§„æ¨¡å›¾åƒçš„å¼±æ ‡ç­¾æ˜¾è‘—é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚ä½†å¯¹äºè¿ç»­ä»»åŠ¡å’Œæœ€å°åŒ–é—å¿˜çš„åº”ç”¨ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡è¡¥ä¸çš„åœºæ™¯ä¸­ï¼ˆå¦‚ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼‰ã€‚æˆ‘ä»¬æå‡ºæŒç»­å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶â€”â€”å¢å¼ºçš„å®šä½èƒ½åŠ›ï¼ˆCoMELï¼‰ï¼Œç”¨äºè§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>CoMELé‡‡ç”¨ä¸‰ç§ç­–ç•¥æ¥æé«˜æ•ˆç‡å’Œæ€§èƒ½ï¼šï¼ˆaï¼‰åˆ†ç»„åŒæ³¨æ„åŠ›è½¬æ¢å™¨ï¼ˆGDATï¼‰å®ç°é«˜æ•ˆå®ä¾‹ç¼–ç ï¼›ï¼ˆbï¼‰åŸºäºè¢‹åŸå‹ä¼ªæ ‡ç­¾ï¼ˆBPPLï¼‰è¿›è¡Œå¯é çš„å®ä¾‹ä¼ªæ ‡ç­¾ï¼›ï¼ˆcï¼‰æ­£äº¤åŠ æƒä½ç§©é€‚åº”ï¼ˆOWLoRAï¼‰ä»¥ç¼“è§£é—å¿˜é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a1c1317eb2677b85788362c7d083a30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce80285f8f19fdbc397a2c8e639d397f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c91cd515a55ec54a3a28abf5040c3576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2611ff4095521b9d64a369ffe9f1ee09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f12a2412220c9653945a241cd318447.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Lightweight-Shrimp-Disease-Detection-Research-Based-on-YOLOv8n"><a href="#Lightweight-Shrimp-Disease-Detection-Research-Based-on-YOLOv8n" class="headerlink" title="Lightweight Shrimp Disease Detection Research Based on YOLOv8n"></a>Lightweight Shrimp Disease Detection Research Based on YOLOv8n</h2><p><strong>Authors:Fei Yuhuan, Wang Gengchen, Liu Fenghao, Zang Ran, Sun Xufei, Chang Hao</strong></p>
<p>Shrimp diseases are one of the primary causes of economic losses in shrimp aquaculture. To prevent disease transmission and enhance intelligent detection efficiency in shrimp farming, this paper proposes a lightweight network architecture based on YOLOv8n. First, by designing the RLDD detection head and C2f-EMCM module, the model reduces computational complexity while maintaining detection accuracy, improving computational efficiency. Subsequently, an improved SegNext_Attention self-attention mechanism is introduced to further enhance the modelâ€™s feature extraction capability, enabling more precise identification of disease characteristics. Extensive experiments, including ablation studies and comparative evaluations, are conducted on a self-constructed shrimp disease dataset, with generalization tests extended to the URPC2020 dataset. Results demonstrate that the proposed model achieves a 32.3% reduction in parameters compared to the original YOLOv8n, with a <a href="mailto:&#x6d;&#x41;&#x50;&#x40;&#48;&#x2e;&#x35;">&#x6d;&#x41;&#x50;&#x40;&#48;&#x2e;&#x35;</a> of 92.7% (3% improvement over YOLOv8n). Additionally, the model outperforms other lightweight YOLO-series models in <a href="mailto:&#x6d;&#x41;&#x50;&#x40;&#x30;&#46;&#53;">&#x6d;&#x41;&#x50;&#x40;&#x30;&#46;&#53;</a>, parameter count, and model size. Generalization experiments on the URPC2020 dataset further validate the modelâ€™s robustness, showing a 4.1% increase in <a href="mailto:&#109;&#65;&#x50;&#64;&#48;&#46;&#x35;">&#109;&#65;&#x50;&#64;&#48;&#46;&#x35;</a> compared to YOLOv8n. The proposed method achieves an optimal balance between accuracy and efficiency, providing reliable technical support for intelligent disease detection in shrimp aquaculture. </p>
<blockquote>
<p>è™¾ç±»ç–¾ç—…æ˜¯å¯¼è‡´è™¾ç±»å…»æ®–ä¸šç»æµæŸå¤±çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚ä¸ºäº†é¢„é˜²ç–¾ç—…ä¼ æ’­ï¼Œæé«˜è™¾ç±»å…»æ®–ä¸­çš„æ™ºèƒ½æ£€æµ‹æ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºYOLOv8nçš„è½»é‡åŒ–ç½‘ç»œæ¶æ„ã€‚é¦–å…ˆï¼Œé€šè¿‡è®¾è®¡RLDDæ£€æµ‹å¤´å’ŒC2f-EMCMæ¨¡å—ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒæ£€æµ‹ç²¾åº¦çš„åŒæ—¶é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚æ¥ç€ï¼Œå¼•å…¥æ”¹è¿›çš„SegNext_Attentionè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œä½¿ç–¾ç—…ç‰¹å¾è¯†åˆ«æ›´åŠ ç²¾ç¡®ã€‚åœ¨è‡ªå»ºçš„è™¾ç–¾ç—…æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬æ¶ˆèç ”ç©¶å’Œæ¯”è¾ƒè¯„ä¼°ï¼Œå¹¶æ‰©å±•äº†å¯¹URPC2020æ•°æ®é›†çš„æ³›åŒ–æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹YOLOv8nç›¸æ¯”ï¼Œæ‰€ææ¨¡å‹å‚æ•°å‡å°‘äº†32.3%ï¼Œåœ¨<a href="mailto:&#109;&#65;&#80;&#64;&#x30;&#x2e;&#x35;">&#109;&#65;&#80;&#64;&#x30;&#x2e;&#x35;</a>æŒ‡æ ‡ä¸Šè¾¾åˆ°92.7%ï¼ˆè¾ƒYOLOv8næé«˜3%ï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨<a href="mailto:&#x6d;&#65;&#x50;&#64;&#48;&#46;&#x35;">&#x6d;&#65;&#x50;&#64;&#48;&#46;&#x35;</a>ã€å‚æ•°è®¡æ•°å’Œæ¨¡å‹å¤§å°ç­‰æ–¹é¢å‡ä¼˜äºå…¶ä»–è½»é‡çº§YOLOç³»åˆ—æ¨¡å‹ã€‚åœ¨URPC2020æ•°æ®é›†ä¸Šçš„æ³›åŒ–å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œè¾ƒYOLOv8nåœ¨<a href="mailto:&#x6d;&#65;&#x50;&#64;&#x30;&#x2e;&#53;">&#x6d;&#65;&#x50;&#64;&#x30;&#x2e;&#53;</a>ä¸Šæé«˜äº†4.1%ã€‚æ‰€ææ–¹æ³•å®ç°äº†ç²¾åº¦ä¸æ•ˆç‡ä¹‹é—´çš„æœ€ä¼˜å¹³è¡¡ï¼Œä¸ºè™¾ç±»å…»æ®–ä¸šä¸­çš„æ™ºèƒ½ç–¾ç—…æ£€æµ‹æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02354v1">PDF</a> in Chinese language</p>
<p><strong>Summary</strong></p>
<p>åŸºäºYOLOv8nç½‘ç»œæ¶æ„çš„è½»é‡åŒ–æ¨¡å‹è¢«æå‡ºç”¨äºæå‡è™¾ç–¾ç—…æ£€æµ‹çš„æ™ºèƒ½åŒ–æ•ˆç‡ã€‚é€šè¿‡è®¾è®¡RLDDæ£€æµ‹å¤´å’ŒC2f-EMCMæ¨¡å—ï¼Œè¯¥æ¨¡å‹æé«˜äº†è®¡ç®—æ•ˆç‡å¹¶ä¿æŒæ£€æµ‹ç²¾åº¦ã€‚è¿›ä¸€æ­¥å¼•å…¥æ”¹è¿›çš„SegNext_Attentionè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„ç—…ç—‡ç‰¹å¾è¯†åˆ«ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºåŸå§‹YOLOv8næ¨¡å‹ï¼Œæ–°æ¨¡å‹åœ¨å‚æ•°ä¸Šå‡å°‘äº†32.3%ï¼ŒåŒæ—¶<a href="mailto:&#x6d;&#x41;&#x50;&#64;&#x30;&#46;&#53;">&#x6d;&#x41;&#x50;&#64;&#x30;&#46;&#53;</a>æé«˜äº†92.7%ï¼ˆè¾ƒYOLOv8næé«˜äº†3%ï¼‰ã€‚æ­¤æ¨¡å‹åœ¨é€šç”¨æ€§å®éªŒä¸­ä¹Ÿå±•ç°å‡ºäº†è‰¯å¥½çš„ç¨³å¥æ€§ã€‚æ­¤æ¨¡å‹ä¸ºè™¾å…»æ®–ä¸šä¸­çš„æ™ºèƒ½ç–¾ç—…æ£€æµ‹æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºåŸºäºYOLOv8nçš„è½»é‡åŒ–æ¨¡å‹ä»¥æé«˜è™¾ç–¾ç—…æ£€æµ‹çš„æ™ºèƒ½åŒ–æ•ˆç‡ã€‚</li>
<li>é€šè¿‡è®¾è®¡RLDDæ£€æµ‹å¤´å’ŒC2f-EMCMæ¨¡å—ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡å¹¶ä¿æŒæ£€æµ‹ç²¾åº¦ã€‚</li>
<li>å¼•å…¥SegNext_Attentionè‡ªæ³¨æ„åŠ›æœºåˆ¶å¢å¼ºäº†æ¨¡å‹çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹ç›¸è¾ƒäºYOLOv8nåœ¨å‚æ•°ä¸Šæœ‰æ‰€å‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„æ£€æµ‹å‡†ç¡®ç‡ã€‚</li>
<li>æ¨¡å‹åœ¨è‡ªæ„å»ºçš„è™¾ç–¾ç—…æ•°æ®é›†å’ŒURPC2020æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å®ç°äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„ä¼˜åŒ–å¹³è¡¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c95e76276f937e9aa4ab0aa8f7852c6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4e3961004d6cb0e8931c3ca8e44a964.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adc8e897be7989ff9a811bde7ef4d979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c251d6d809d6d057c0199edd2b499f6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bee25c3bbf68bc0c67f2068f496f41b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c470cbc334e1f7291d0a3bff311f610.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-computationally-frugal-open-source-foundation-model-for-thoracic-disease-detection-in-lung-cancer-screening-programs"><a href="#A-computationally-frugal-open-source-foundation-model-for-thoracic-disease-detection-in-lung-cancer-screening-programs" class="headerlink" title="A computationally frugal open-source foundation model for thoracic   disease detection in lung cancer screening programs"></a>A computationally frugal open-source foundation model for thoracic   disease detection in lung cancer screening programs</h2><p><strong>Authors:NiccolÃ² McConnell, Pardeep Vasudev, Daisuke Yamada, Daryl Cheng, Mehran Azimbagirad, John McCabe, Shahab Aslani, Ahmed H. Shahin, Yukun Zhou, The SUMMIT Consortium, Andre Altmann, Yipeng Hu, Paul Taylor, Sam M. Janes, Daniel C. Alexander, Joseph Jacob</strong></p>
<p>Low-dose computed tomography (LDCT) imaging employed in lung cancer screening (LCS) programs is increasing in uptake worldwide. LCS programs herald a generational opportunity to simultaneously detect cancer and non-cancer-related early-stage lung disease. Yet these efforts are hampered by a shortage of radiologists to interpret scans at scale. Here, we present TANGERINE, a computationally frugal, open-source vision foundation model for volumetric LDCT analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can be fine-tuned off the shelf for a wide range of disease-specific tasks with limited computational resources and training data. Relative to models trained from scratch, TANGERINE demonstrates fast convergence during fine-tuning, thereby requiring significantly fewer GPU hours, and displays strong label efficiency, achieving comparable or superior performance with a fraction of fine-tuning data. Pretrained using self-supervised learning on over 98,000 thoracic LDCTs, including the UKâ€™s largest LCS initiative to date and 27 public datasets, TANGERINE achieves state-of-the-art performance across 14 disease classification tasks, including lung cancer and multiple respiratory diseases, while generalising robustly across diverse clinical centres. By extending a masked autoencoder framework to 3D imaging, TANGERINE offers a scalable solution for LDCT analysis, departing from recent closed, resource-intensive models by combining architectural simplicity, public availability, and modest computational requirements. Its accessible, open-source lightweight design lays the foundation for rapid integration into next-generation medical imaging tools that could transform LCS initiatives, allowing them to pivot from a singular focus on lung cancer detection to comprehensive respiratory disease management in high-risk populations. </p>
<blockquote>
<p>ä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰æˆåƒåœ¨è‚ºç™Œç­›æŸ¥ï¼ˆLCSï¼‰è®¡åˆ’ä¸­çš„åº”ç”¨åœ¨å…¨çƒèŒƒå›´å†…ä¸æ–­å¢åŠ ã€‚LCSè®¡åˆ’é¢„ç¤ºç€ä¸€ä¸ªå¯ä»¥åŒæ—¶æ£€æµ‹ç™Œç—‡å’Œéç™Œç—‡ç›¸å…³æ—©æœŸè‚ºéƒ¨ç–¾ç—…çš„æœºä¼šã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡è§£è¯»æ‰«æç»“æœçš„æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè¿™äº›åŠªåŠ›å—åˆ°äº†é˜»ç¢ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†TANGERINEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä½“ç§¯LDCTåˆ†æçš„è®¡ç®—èŠ‚çº¦å‹å¼€æºè§†è§‰åŸºç¡€æ¨¡å‹ã€‚TANGERINEçš„è®¾è®¡æ—¨åœ¨å®ç°å¹¿æ³›çš„å¯è®¿é—®æ€§å’Œå¿«é€Ÿé€‚åº”ï¼Œå¯ä»¥ç°æˆå¾®è°ƒä»¥æ‰§è¡Œå¤šç§ç‰¹å®šç–¾ç—…ä»»åŠ¡ï¼Œä¸”è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®æœ‰é™ã€‚ç›¸å¯¹äºä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ï¼ŒTANGERINEåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ˜¾ç¤ºå‡ºå¿«é€Ÿæ”¶æ•›ï¼Œå› æ­¤éœ€è¦çš„GPUå°æ—¶æ•°å¤§å¤§å‡å°‘ï¼Œå¹¶ä¸”è¡¨ç°å‡ºå¼ºå¤§çš„æ ‡ç­¾æ•ˆç‡ï¼Œä½¿ç”¨å°‘é‡çš„å¾®è°ƒæ•°æ®å³å¯å®ç°ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚TANGERINEä½¿ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ åœ¨è¶…è¿‡98000ä¸ªèƒ¸éƒ¨LDCTä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒ…æ‹¬è¿„ä»Šä¸ºæ­¢è‹±å›½æœ€å¤§çš„LCSå€¡è®®å’Œ27ä¸ªå…¬å…±æ•°æ®é›†ï¼Œåœ¨14ä¸ªç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼ŒåŒ…æ‹¬è‚ºç™Œå’Œå¤šç§å‘¼å¸é“ç–¾ç—…ï¼ŒåŒæ—¶åœ¨å¤šç§ä¸´åºŠä¸­å¿ƒå®ç°äº†ç¨³å¥çš„æ¨å¹¿ã€‚é€šè¿‡å°†æ©ç è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶æ‰©å±•åˆ°3Dæˆåƒï¼ŒTANGERINEä¸ºLDCTåˆ†ææä¾›äº†å¯è§„æ¨¡åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡ç»“åˆç»“æ„ç®€æ´ã€å…¬ä¼—å¯è®¿é—®å’Œé€‚åº¦çš„è®¡ç®—è¦æ±‚ï¼Œä¸åŒäºæœ€è¿‘å°é—­çš„ã€èµ„æºå¯†é›†å‹çš„æ¨¡å‹ã€‚å…¶å¯è®¿é—®çš„å¼€æºè½»ä¾¿è®¾è®¡å¥ å®šäº†è¿…é€Ÿèå…¥ä¸‹ä¸€ä»£åŒ»å­¦å½±åƒå·¥å…·çš„åŸºç¡€ï¼Œæœ‰å¯èƒ½æ”¹å˜LCSè®¡åˆ’ï¼Œä½¿å®ƒä»¬ä»å•ä¸€å…³æ³¨è‚ºç™Œæ£€æµ‹è½¬å‘é«˜é£é™©äººç¾¤çš„ç»¼åˆå‘¼å¸é“ç–¾ç—…ç®¡ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01881v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>TANGERINEæ˜¯ä¸€ç§ç”¨äºä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰åˆ†æçš„è®¡ç®—è½»é‡å‹å¼€æºè§†è§‰åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹å¯åœ¨å¹¿æ³›çš„ç–¾ç—…ç‰¹å®šä»»åŠ¡ä¸­è¿›è¡Œå¾®è°ƒï¼Œæ— éœ€å¤§é‡è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®ã€‚ç›¸è¾ƒäºä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ï¼ŒTANGERINEåœ¨å¾®è°ƒæ—¶èƒ½å¤Ÿå¿«é€Ÿæ”¶æ•›ï¼Œæ˜¾è‘—å‡å°‘GPUå°æ—¶æ•°ï¼Œå¹¶æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ ‡ç­¾æ•ˆç‡ï¼Œç”¨å°‘é‡çš„å¾®è°ƒæ•°æ®å³å¯å®ç°ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨14ä¸ªç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒ…æ‹¬è‚ºç™Œå’Œå¤šç§å‘¼å¸ç³»ç»Ÿç–¾ç—…ï¼Œå¹¶åœ¨ä¸åŒçš„ä¸´åºŠä¸­å¿ƒå®ç°ç¨³å¥çš„é€šç”¨åŒ–ã€‚é€šè¿‡æ‰©å±•3Dæˆåƒçš„æ©ç è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶ï¼ŒTANGERINEä¸ºLDCTåˆ†ææä¾›äº†å¯è§„æ¨¡åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶å¼€æ”¾ã€è½»é‡çº§çš„è®¾è®¡å¯ä¸ºä¸‹ä¸€ä»£åŒ»å­¦å½±åƒå·¥å…·æä¾›å¿«é€Ÿé›†æˆçš„åŸºç¡€ï¼Œä»è€Œæ¨åŠ¨è‚ºç™Œç­›æŸ¥è®¡åˆ’ä»å•ä¸€çš„è‚ºç™Œæ£€æµ‹è½¬å‘é«˜é£é™©äººç¾¤çš„å…¨é¢å‘¼å¸ç–¾ç—…ç®¡ç†ã€‚

**Key Takeaways**

1. TANGERINEæ˜¯ä¸€ä¸ªç”¨äºä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰åˆ†æçš„å¼€æºè§†è§‰åŸºç¡€æ¨¡å‹ã€‚
2. TANGERINEå¯åœ¨å¹¿æ³›çš„ç–¾ç—…ç‰¹å®šä»»åŠ¡ä¸­è¿›è¡Œå¾®è°ƒï¼Œä¸”è®¡ç®—èµ„æºéœ€æ±‚è¾ƒå°ã€‚
3. ç›¸è¾ƒäºä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ï¼ŒTANGERINEåœ¨å¾®è°ƒæ—¶èƒ½å¤Ÿå¿«é€Ÿæ”¶æ•›ï¼Œå¹¶æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ ‡ç­¾æ•ˆç‡ã€‚
4. TANGERINEåœ¨å¤šä¸ªç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚
5. TANGERINEåœ¨å¤šç§ä¸´åºŠç¯å¢ƒä¸­å®ç°ç¨³å¥çš„é€šç”¨åŒ–æ€§èƒ½ã€‚
6. TANGERINEé€šè¿‡æ‰©å±•æ©ç è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶è‡³3Dæˆåƒï¼Œä¸ºLDCTåˆ†ææä¾›äº†å¯è§„æ¨¡åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8464410354e9f71e67c921d07616e7f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84b28edc9a62fc5d0e365ebd1c881ce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ad73daec828019ea3f9bd89ae5d475f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d73ff10042b0f15bc6c60b88ef7144.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Autoadaptive-Medical-Segment-Anything-Model"><a href="#Autoadaptive-Medical-Segment-Anything-Model" class="headerlink" title="Autoadaptive Medical Segment Anything Model"></a>Autoadaptive Medical Segment Anything Model</h2><p><strong>Authors:Tyler Ward, Meredith K. Owen, Oâ€™Kira Coleman, Brian Noehren, Abdullah-Al-Zubaer Imran</strong></p>
<p>Medical image segmentation is a key task in the imaging workflow, influencing many image-based decisions. Traditional, fully-supervised segmentation models rely on large amounts of labeled training data, typically obtained through manual annotation, which can be an expensive, time-consuming, and error-prone process. This signals a need for accurate, automatic, and annotation-efficient methods of training these models. We propose ADA-SAM (automated, domain-specific, and adaptive segment anything model), a novel multitask learning framework for medical image segmentation that leverages class activation maps from an auxiliary classifier to guide the predictions of the semi-supervised segmentation branch, which is based on the Segment Anything (SAM) framework. Additionally, our ADA-SAM model employs a novel gradient feedback mechanism to create a learnable connection between the segmentation and classification branches by using the segmentation gradients to guide and improve the classification predictions. We validate ADA-SAM on real-world clinical data collected during rehabilitation trials, and demonstrate that our proposed method outperforms both fully-supervised and semi-supervised baselines by double digits in limited label settings. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/tbwa233/ADA-SAM">https://github.com/tbwa233/ADA-SAM</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯æˆåƒå·¥ä½œæµç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå½±å“ç€è®¸å¤šåŸºäºå›¾åƒçš„å†³ç­–ã€‚ä¼ ç»Ÿçš„å®Œå…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–äºå¤§é‡çš„æ ‡è®°è®­ç»ƒæ•°æ®ï¼Œé€šå¸¸é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨è·å¾—ï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜‚è´µã€è€—æ—¶ä¸”æ˜“å‡ºé”™çš„è¿‡ç¨‹ã€‚è¿™å‡¸æ˜¾äº†å¯¹å‡†ç¡®ã€è‡ªåŠ¨å’Œæ ‡æ³¨æ•ˆç‡é«˜çš„è®­ç»ƒè¿™äº›æ¨¡å‹çš„æ–¹æ³•çš„éœ€æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ADA-SAMï¼ˆè‡ªåŠ¨ã€ç‰¹å®šé¢†åŸŸå’Œè‡ªé€‚åº”åˆ†å‰²ä»»ä½•äº‹ç‰©æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è¾…åŠ©åˆ†ç±»å™¨çš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼åŠç›‘ç£åˆ†å‰²åˆ†æ”¯çš„é¢„æµ‹ï¼Œè¯¥æ¡†æ¶åŸºäºåˆ†å‰²ä»»ä½•äº‹ç‰©ï¼ˆSAMï¼‰æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ADA-SAMæ¨¡å‹é‡‡ç”¨äº†ä¸€ç§æ–°å‹æ¢¯åº¦åé¦ˆæœºåˆ¶ï¼Œé€šè¿‡åˆ†å‰²æ¢¯åº¦æ¥å¼•å¯¼å’Œæ”¹è¿›åˆ†ç±»é¢„æµ‹ï¼Œåœ¨åˆ†å‰²å’Œåˆ†ç±»åˆ†æ”¯ä¹‹é—´å»ºç«‹å¯å­¦ä¹ çš„è¿æ¥ã€‚æˆ‘ä»¬åœ¨åº·å¤è¯•éªŒæœŸé—´æ”¶é›†çš„çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®ä¸ŠéªŒè¯äº†ADA-SAMï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰é™æ ‡ç­¾æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å®Œå…¨ç›‘ç£å’ŒåŠç›‘ç£åŸºå‡†æµ‹è¯•æ–¹é¢éƒ½è¡¨ç°å‡ºä¸¤ä½æ•°çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/tbwa233/ADA-SAM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tbwa233/ADA-SAMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01828v1">PDF</a> 11 pages, 2 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯æˆåƒå·¥ä½œæµç¨‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œå½±å“è®¸å¤šåŸºäºå›¾åƒçš„å†³å®šã€‚ä¼ ç»Ÿå…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–äºå¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œä¸”å®¹æ˜“å‡ºé”™ã€‚å› æ­¤ï¼Œéœ€è¦å‡†ç¡®ã€è‡ªåŠ¨ã€æ ‡æ³¨æ•ˆç‡é«˜çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºADA-SAMï¼ˆè‡ªåŠ¨ã€ç‰¹å®šé¢†åŸŸã€è‡ªé€‚åº”åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤šä»»åŠ¡å­¦ä¹ æ–°æ¡†æ¶ï¼Œåˆ©ç”¨è¾…åŠ©åˆ†ç±»å™¨çš„ç±»æ¿€æ´»å›¾æ¥æŒ‡å¯¼åŠç›‘ç£åˆ†å‰²åˆ†æ”¯çš„é¢„æµ‹ï¼ŒåŸºäºSegment Anythingï¼ˆSAMï¼‰æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ADA-SAMæ¨¡å‹é‡‡ç”¨æ–°å‹æ¢¯åº¦åé¦ˆæœºåˆ¶ï¼Œé€šè¿‡åˆ†å‰²æ¢¯åº¦æ¥æŒ‡å¯¼å’Œæ”¹è¿›åˆ†ç±»é¢„æµ‹ï¼Œåœ¨åˆ†å‰²å’Œåˆ†ç±»åˆ†æ”¯ä¹‹é—´å»ºç«‹å¯å­¦ä¹ çš„è”ç³»ã€‚æˆ‘ä»¬åœ¨åº·å¤è¯•éªŒæœŸé—´æ”¶é›†çš„çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®ä¸ŠéªŒè¯äº†ADA-SAMï¼Œè¯æ˜åœ¨æœ‰é™æ ‡ç­¾è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¨ç›‘ç£å’ŒåŠç›‘ç£åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡è¶…å‡ºä¸¤ä½æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨æˆåƒå·¥ä½œæµç¨‹ä¸­è‡³å…³é‡è¦ï¼Œå½±å“åŸºäºå›¾åƒçš„å†³ç­–ã€‚</li>
<li>ä¼ ç»Ÿå…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ä¾èµ–å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢ç¹çåˆè€—æ—¶ã€‚</li>
<li>éœ€è¦æ›´å‡†ç¡®ã€è‡ªåŠ¨ã€æ ‡æ³¨æ•ˆç‡é«˜çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ADA-SAMæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤šä»»åŠ¡å­¦ä¹ æ–°æ¡†æ¶ã€‚<br>5.ADA-SAMåˆ©ç”¨ç±»æ¿€æ´»å›¾å’Œæ¢¯åº¦åé¦ˆæœºåˆ¶æ¥æŒ‡å¯¼é¢„æµ‹å¹¶è¿æ¥åˆ†å‰²å’Œåˆ†ç±»åˆ†æ”¯ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®ä¸ŠéªŒè¯äº†ADA-SAMæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a0fa7c35078ce4f34ae24f38a720e1e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d68178e7e51e72fd3adf490b0306107a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8971aa4c8e2a4793226874d0c99e1c1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Are-Vision-Transformer-Representations-Semantically-Meaningful-A-Case-Study-in-Medical-Imaging"><a href="#Are-Vision-Transformer-Representations-Semantically-Meaningful-A-Case-Study-in-Medical-Imaging" class="headerlink" title="Are Vision Transformer Representations Semantically Meaningful? A Case   Study in Medical Imaging"></a>Are Vision Transformer Representations Semantically Meaningful? A Case   Study in Medical Imaging</h2><p><strong>Authors:Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu</strong></p>
<p>Vision transformers (ViTs) have rapidly gained prominence in medical imaging tasks such as disease classification, segmentation, and detection due to their superior accuracy compared to conventional deep learning models. However, due to their size and complex interactions via the self-attention mechanism, they are not well understood. In particular, it is unclear whether the representations produced by such models are semantically meaningful. In this paper, using a projected gradient-based algorithm, we show that their representations are not semantically meaningful and they are inherently vulnerable to small changes. Images with imperceptible differences can have very different representations; on the other hand, images that should belong to different semantic classes can have nearly identical representations. Such vulnerability can lead to unreliable classification results; for example, unnoticeable changes cause the classification accuracy to be reduced by over 60%. %. To the best of our knowledge, this is the first work to systematically demonstrate this fundamental lack of semantic meaningfulness in ViT representations for medical image classification, revealing a critical challenge for their deployment in safety-critical systems. </p>
<blockquote>
<p>è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰å› å…¶ç›¸å¯¹äºä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å“è¶Šå‡†ç¡®æ€§ï¼Œåœ¨åŒ»å­¦æˆåƒä»»åŠ¡ï¼ˆå¦‚ç–¾ç—…åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ï¼‰ä¸­è¿…é€Ÿå´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œç”±äºå…¶è§„æ¨¡å’Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶äº§ç”Ÿçš„å¤æ‚äº¤äº’ï¼Œäººä»¬å¯¹å…¶ç†è§£å¹¶ä¸å……åˆ†ã€‚å°¤å…¶ä¸æ¸…æ¥šçš„æ˜¯ï¼Œæ­¤ç±»æ¨¡å‹äº§ç”Ÿçš„è¡¨ç¤ºæ˜¯å¦å…·æœ‰è¯­ä¹‰æ„ä¹‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºæŠ•å½±çš„æ¢¯åº¦ç®—æ³•è¯æ˜ï¼Œå®ƒä»¬çš„è¡¨ç¤ºä¸å…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼Œå¹¶ä¸”æœ¬è´¨ä¸Šæ˜“äºå—åˆ°å¾®å°å˜åŒ–çš„å½±å“ã€‚å…·æœ‰å‡ ä¹æ— æ³•å¯Ÿè§‰å·®å¼‚çš„å›¾åƒå¯ä»¥å…·æœ‰æˆªç„¶ä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼›ç›¸åï¼Œæœ¬åº”å±äºä¸åŒè¯­ä¹‰ç±»çš„å›¾åƒå´å¯èƒ½æœ‰å‡ ä¹ç›¸åŒçš„è¡¨ç¤ºå½¢å¼ã€‚è¿™ç§è„†å¼±æ€§å¯èƒ½å¯¼è‡´åˆ†ç±»ç»“æœä¸å¯é ï¼›ä¾‹å¦‚ï¼Œå‡ ä¹æ— æ³•å¯Ÿè§‰çš„å˜åŒ–å¯¼è‡´åˆ†ç±»ç²¾åº¦é™ä½äº†è¶…è¿‡60%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹ç³»ç»Ÿè¯æ˜ViTåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„è¡¨ç¤ºç¼ºä¹åŸºæœ¬è¯­ä¹‰æ„ä¹‰çš„å·¥ä½œï¼Œè¿™æ­ç¤ºäº†å°†å…¶éƒ¨ç½²åœ¨å®‰å…¨å…³é”®ç³»ç»Ÿä¸­é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01788v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>ViTæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šå‡†ç¡®æ€§ï¼Œä½†å…¶å¤æ‚çš„ç»“æ„å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´å¯¹è¿™ç±»æ¨¡å‹çš„ç†è§£ä»ä¸å¤Ÿæ·±å…¥ã€‚æœ¬æ–‡åˆ©ç”¨æ¢¯åº¦æŠ•å½±ç®—æ³•è¯æ˜ViTæ¨¡å‹ç”Ÿæˆçš„è¡¨å¾ä¸å…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼Œå®¹æ˜“å—å¾®å°å˜åŒ–å½±å“ã€‚å…·æœ‰ç»†å¾®å·®å¼‚çš„å›¾ç‰‡åœ¨æ¨¡å‹ä¸­å¯èƒ½æœ‰æˆªç„¶ä¸åŒçš„è¡¨å¾ï¼Œè€Œæœ¬åº”å±äºä¸åŒè¯­ä¹‰ç±»åˆ«çš„å›¾ç‰‡åˆ™å¯èƒ½æœ‰å‡ ä¹ç›¸åŒçš„è¡¨å¾ã€‚è¿™ç§ç°è±¡å¯èƒ½å¯¼è‡´åˆ†ç±»ç»“æœä¸å¯é ï¼Œå¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´åˆ†ç±»å‡†ç¡®ç‡ä¸‹é™è¶…è¿‡60%ã€‚è¿™æ˜¯é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å±•ç¤ºViTæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ç¼ºä¹è¯­ä¹‰æ„ä¹‰çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºéƒ¨ç½²åœ¨å®‰å…¨å…³é”®ç³»ç»Ÿä¸­çš„é£é™©æä¾›äº†é‡è¦æ­ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision transformers (ViTs) å·²å¹¿æ³›åº”ç”¨äºåŒ»å­¦æˆåƒä»»åŠ¡ï¼Œå› å…¶å‡†ç¡®ç‡ä¼˜äºä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>ViTæ¨¡å‹çš„å¤æ‚æ€§å¯¼è‡´å…¶è¯­ä¹‰ç†è§£ä¸è¶³ã€‚</li>
<li>ä½¿ç”¨æ¢¯åº¦æŠ•å½±ç®—æ³•å‘ç°ViTæ¨¡å‹ç”Ÿæˆçš„è¡¨å¾ç¼ºä¹è¯­ä¹‰æ„ä¹‰ã€‚</li>
<li>ViTæ¨¡å‹å¯¹å¾®å°å˜åŒ–æ•æ„Ÿï¼Œå¯èƒ½å¯¼è‡´ä¸ç¨³å®šçš„åˆ†ç±»ç»“æœã€‚</li>
<li>ç»†å¾®å·®å¼‚çš„å›¾ç‰‡åœ¨æ¨¡å‹ä¸­å¯èƒ½æœ‰æˆªç„¶ä¸åŒçš„è¡¨å¾ã€‚</li>
<li>æœ¬åº”å±äºä¸åŒè¯­ä¹‰ç±»åˆ«çš„å›¾ç‰‡åœ¨ViTæ¨¡å‹ä¸­å¯èƒ½æœ‰ç›¸ä¼¼è¡¨å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01788">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ac1d07105f37e318f85b15abe6dba51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09acbc79a6bd2bc9414b76584bf89fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-206b0f82c0cb822ebfdaa8a10e431a29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae2a53d0e4b1c20d126622245d3b15e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da34302d17d91f6375031b4a3691437.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c694f96cd523005fa085639a6ae81b7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb644755d46dd562349f35fe2c753e7d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DeRIS-Decoupling-Perception-and-Cognition-for-Enhanced-Referring-Image-Segmentation-through-Loopback-Synergy"><a href="#DeRIS-Decoupling-Perception-and-Cognition-for-Enhanced-Referring-Image-Segmentation-through-Loopback-Synergy" class="headerlink" title="DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image   Segmentation through Loopback Synergy"></a>DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image   Segmentation through Loopback Synergy</h2><p><strong>Authors:Ming Dai, Wenxuan Cheng, Jiang-jiang Liu, Sen Yang, Wenxiao Cai, Yanpeng Sun, Wankou Yang</strong></p>
<p>Referring Image Segmentation (RIS) is a challenging task that aims to segment objects in an image based on natural language expressions. While prior studies have predominantly concentrated on improving vision-language interactions and achieving fine-grained localization, a systematic analysis of the fundamental bottlenecks in existing RIS frameworks remains underexplored. To bridge this gap, we propose DeRIS, a novel framework that decomposes RIS into two key components: perception and cognition. This modular decomposition facilitates a systematic analysis of the primary bottlenecks impeding RIS performance. Our findings reveal that the predominant limitation lies not in perceptual deficiencies, but in the insufficient multi-modal cognitive capacity of current models. To mitigate this, we propose a Loopback Synergy mechanism, which enhances the synergy between the perception and cognition modules, thereby enabling precise segmentation while simultaneously improving robust image-text comprehension. Additionally, we analyze and introduce a simple non-referent sample conversion data augmentation to address the long-tail distribution issue related to target existence judgement in general scenarios. Notably, DeRIS demonstrates inherent adaptability to both non- and multi-referents scenarios without requiring specialized architectural modifications, enhancing its general applicability. The codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/Dmmm1997/DeRIS">https://github.com/Dmmm1997/DeRIS</a>. </p>
<blockquote>
<p>å›¾åƒå¼•ç”¨åˆ†å‰²ï¼ˆRISï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼å¯¹å›¾åƒä¸­çš„å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚å°½ç®¡å…ˆå‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›è§†è§‰ä¸è¯­è¨€çš„äº¤äº’ä½œç”¨å’Œå®ç°ç²¾ç»†å®šä½ä¸Šï¼Œä½†å¯¹ç°æœ‰RISæ¡†æ¶ä¸­åŸºæœ¬ç“¶é¢ˆçš„ç³»ç»Ÿæ€§åˆ†æä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†DeRISè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†RISåˆ†è§£ä¸ºä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šæ„ŸçŸ¥å’Œè®¤çŸ¥ã€‚è¿™ç§æ¨¡å—åŒ–åˆ†è§£æœ‰åŠ©äºç³»ç»Ÿåœ°åˆ†æé˜»ç¢RISæ€§èƒ½çš„ä¸»è¦ç“¶é¢ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä¸»è¦å±€é™ä¸åœ¨äºæ„ŸçŸ¥ç¼ºé™·ï¼Œè€Œåœ¨äºå½“å‰æ¨¡å‹çš„å¤šæ¨¡å¼è®¤çŸ¥èƒ½åŠ›ä¸è¶³ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LoopbackååŒæœºåˆ¶ï¼Œå®ƒå¢å¼ºäº†æ„ŸçŸ¥å’Œè®¤çŸ¥æ¨¡å—ä¹‹é—´çš„ååŒä½œç”¨ï¼Œä»è€Œå®ç°äº†ç²¾ç¡®åˆ†å‰²ï¼ŒåŒæ—¶æé«˜äº†å›¾åƒæ–‡æœ¬ç†è§£çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æå’Œå¼•å…¥äº†ä¸€ç§ç®€å•çš„éå‚ç…§æ ·æœ¬è½¬æ¢æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä»¥è§£å†³ä¸€èˆ¬åœºæ™¯ä¸­ä¸ç›®æ ‡å­˜åœ¨åˆ¤æ–­ç›¸å…³çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDeRISå¯¹éå‚ç…§å’Œå¤šå‚ç…§åœºæ™¯å…·æœ‰å›ºæœ‰çš„é€‚åº”æ€§ï¼Œæ— éœ€è¿›è¡Œä¸“é—¨çš„æ¶æ„ä¿®æ”¹ï¼Œæé«˜äº†å…¶é€šç”¨æ€§ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Dmmm1997/DeRIS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Dmmm1997/DeRISè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01738v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDeRISçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè§£å†³å›¾åƒå‚ç…§åˆ†å‰²ï¼ˆRISï¼‰ä»»åŠ¡ä¸­çš„ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ¡†æ¶å°†RISåˆ†è§£ä¸ºæ„ŸçŸ¥å’Œè®¤çŸ¥ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œå¹¶å‘ç°ä¸»è¦é™åˆ¶ä¸åœ¨äºæ„ŸçŸ¥ç¼ºé™·ï¼Œè€Œåœ¨äºå½“å‰æ¨¡å‹çš„å¤šæ¨¡æ€è®¤çŸ¥èƒ½åŠ›ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒDeRISæå‡ºäº†Loopback Synergyæœºåˆ¶ï¼Œå¢å¼ºäº†æ„ŸçŸ¥å’Œè®¤çŸ¥æ¨¡å—ä¹‹é—´çš„ååŒä½œç”¨ï¼Œå®ç°äº†ç²¾ç¡®åˆ†å‰²ï¼Œå¹¶æé«˜äº†å›¾åƒæ–‡æœ¬ç†è§£ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼•å…¥äº†ä¸€ç§ç®€å•çš„éå‚ç…§æ ·æœ¬è½¬æ¢æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä»¥è§£å†³ä¸€èˆ¬åœºæ™¯ä¸­ç›®æ ‡å­˜åœ¨åˆ¤æ–­çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ã€‚DeRISå…·æœ‰é€‚åº”éå‚ç…§å’Œå¤šå‚ç…§åœºæ™¯çš„èƒ½åŠ›ï¼Œå¢å¼ºäº†å…¶é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeRISæ¡†æ¶æ—¨åœ¨è§£å†³å›¾åƒå‚ç…§åˆ†å‰²ï¼ˆRISï¼‰ä»»åŠ¡ä¸­çš„ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶å°†RISåˆ†è§£ä¸ºæ„ŸçŸ¥å’Œè®¤çŸ¥ä¸¤ä¸ªç»„ä»¶è¿›è¡Œç³»ç»Ÿåˆ†æã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œä¸»è¦é™åˆ¶åœ¨äºå½“å‰æ¨¡å‹çš„å¤šæ¨¡æ€è®¤çŸ¥èƒ½åŠ›ä¸è¶³ã€‚</li>
<li>æå‡ºLoopback Synergyæœºåˆ¶ï¼Œå¢å¼ºæ„ŸçŸ¥å’Œè®¤çŸ¥æ¨¡å—çš„ååŒä½œç”¨ã€‚</li>
<li>DeRISèƒ½å®ç°ç²¾ç¡®åˆ†å‰²ï¼Œå¹¶æé«˜å›¾åƒæ–‡æœ¬ç†è§£ã€‚</li>
<li>å¼•å…¥éå‚ç…§æ ·æœ¬è½¬æ¢æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè§£å†³ç›®æ ‡å­˜åœ¨åˆ¤æ–­çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-149d17b28845bc6e79369061d9cb2706.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5492811c0a0ac746ebf63253ceb99db6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a71934c15ba8c11c623a8dec95743a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac18c185c198c179599fbb984e2ad66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d7638f991a64c4706f64559f022afd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62e24181c8671f5c9869ca9c316c8356.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Learning-from-Random-Subspace-Exploration-Generalized-Test-Time-Augmentation-with-Self-supervised-Distillation"><a href="#Learning-from-Random-Subspace-Exploration-Generalized-Test-Time-Augmentation-with-Self-supervised-Distillation" class="headerlink" title="Learning from Random Subspace Exploration: Generalized Test-Time   Augmentation with Self-supervised Distillation"></a>Learning from Random Subspace Exploration: Generalized Test-Time   Augmentation with Self-supervised Distillation</h2><p><strong>Authors:Andrei Jelea, Ahmed Nabil Belbachir, Marius Leordeanu</strong></p>
<p>We introduce Generalized Test-Time Augmentation (GTTA), a highly effective method for improving the performance of a trained model, which unlike other existing Test-Time Augmentation approaches from the literature is general enough to be used off-the-shelf for many vision and non-vision tasks, such as classification, regression, image segmentation and object detection. By applying a new general data transformation, that randomly perturbs multiple times the PCA subspace projection of a test input, GTTA forms robust ensembles at test time in which, due to sound statistical properties, the structural and systematic noises in the initial input data is filtered out and final estimator errors are reduced. Different from other existing methods, we also propose a final self-supervised learning stage in which the ensemble output, acting as an unsupervised teacher, is used to train the initial single student model, thus reducing significantly the test time computational cost, at no loss in accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on various vision and non-vision well-known datasets and tasks, such as image classification and segmentation, speech recognition and house price prediction, validate the generality of the proposed GTTA. Furthermore, we also prove its effectiveness on the more specific real-world task of salmon segmentation and detection in low-visibility underwater videos, for which we introduce DeepSalmon, the largest dataset of its kind in the literature. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†å¹¿ä¹‰æµ‹è¯•æ—¶é—´å¢å¼ºï¼ˆGTTAï¼‰è¿™ä¸€é«˜åº¦æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºæå‡å·²è®­ç»ƒæ¨¡å‹çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„æµ‹è¯•æ—¶é—´å¢å¼ºæ–¹æ³•ä¸åŒï¼ŒGTTAå…·æœ‰è¶³å¤Ÿçš„é€šç”¨æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºè®¸å¤šè§†è§‰å’Œéè§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚åˆ†ç±»ã€å›å½’ã€å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ã€‚é€šè¿‡åº”ç”¨æ–°çš„é€šç”¨æ•°æ®è½¬æ¢ï¼Œå¤šæ¬¡éšæœºæ‰°åŠ¨æµ‹è¯•è¾“å…¥çš„PCAå­ç©ºé—´æŠ•å½±ï¼ŒGTTAåœ¨æµ‹è¯•æ—¶å½¢æˆç¨³å¥çš„é›†åˆã€‚ç”±äºå…·æœ‰è‰¯å¥½çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œåˆå§‹è¾“å…¥æ•°æ®ä¸­çš„ç»“æ„å’Œç³»ç»Ÿå™ªå£°è¢«è¿‡æ»¤æ‰ï¼Œæœ€ç»ˆä¼°è®¡å™¨è¯¯å·®å¾—ä»¥é™ä½ã€‚ä¸å…¶ä»–ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæœ€ç»ˆçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ é˜¶æ®µï¼Œå…¶ä¸­é›†åˆè¾“å‡ºä½œä¸ºæ— ç›‘ç£æ•™å¸ˆï¼Œç”¨äºè®­ç»ƒåˆå§‹å•ä¸€çš„å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œåœ¨ä¸å½±å“ç²¾åº¦çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†æµ‹è¯•æ—¶é—´çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬åœ¨å„ç§è§†è§‰å’Œéè§†è§‰çš„çŸ¥åæ•°æ®é›†å’Œä»»åŠ¡ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶ä¸å¼ºå¤§çš„TTAæ–¹æ³•å’Œæœ€æ–°æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ã€è¯­éŸ³è¯†åˆ«å’Œæˆ¿å±‹ä»·æ ¼é¢„æµ‹ç­‰ï¼ŒéªŒè¯äº†æ‰€æGTTAçš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„å®é™…ä»»åŠ¡â€”â€”ä½å¯è§åº¦æ°´ä¸‹è§†é¢‘çš„é²‘é±¼åˆ†å‰²å’Œæ£€æµ‹ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepSalmonï¼Œè¿™æ˜¯æ–‡çŒ®ä¸­åŒç±»æœ€å¤§çš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01347v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†å¹¿ä¹‰æµ‹è¯•æ—¶å¢å¼ºï¼ˆGTTAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡è®­ç»ƒæ¨¡å‹æ€§èƒ½ã€‚ä¸å…¶ä»–æµ‹è¯•æ—¶å¢å¼ºæ–¹æ³•ä¸åŒï¼ŒGTTAå…·æœ‰é€šç”¨æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºè§†è§‰å’Œéè§†è§‰ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€å›å½’ã€å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ã€‚é€šè¿‡æ–°çš„é€šç”¨æ•°æ®è½¬æ¢ï¼ŒGTTAåœ¨æµ‹è¯•æ—¶å½¢æˆç¨³å¥é›†åˆï¼Œè¿‡æ»¤æ‰åˆå§‹è¾“å…¥æ•°æ®çš„ç»“æ„å’Œç³»ç»Ÿå™ªå£°ï¼Œé™ä½æœ€ç»ˆä¼°è®¡å™¨è¯¯å·®ã€‚æ­¤å¤–ï¼ŒGTTAè¿˜é‡‡ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ é˜¶æ®µï¼Œåˆ©ç”¨é›†åˆè¾“å‡ºä½œä¸ºæ— ç›‘ç£æ•™å¸ˆï¼Œå¯¹åˆå§‹å•ä¸€å­¦ç”Ÿæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—é™ä½æµ‹è¯•æ—¶é—´è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªè§†è§‰å’Œéè§†è§‰æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†GTTAçš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥å¹¿ä¹‰æµ‹è¯•æ—¶å¢å¼ºï¼ˆGTTAï¼‰æ–¹æ³•ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€å›å½’ã€å›¾åƒåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ã€‚</li>
<li>é€šè¿‡æ–°çš„æ•°æ®è½¬æ¢æŠ€æœ¯ï¼Œéšæœºæ‰°åŠ¨æµ‹è¯•è¾“å…¥æ•°æ®çš„PCAå­ç©ºé—´æŠ•å½±ï¼Œå½¢æˆç¨³å¥é›†åˆã€‚</li>
<li>é›†åˆä¸­çš„ç¨³å¥æ€§é€šè¿‡è¿‡æ»¤åˆå§‹è¾“å…¥æ•°æ®çš„ç»“æ„å’Œç³»ç»Ÿå™ªå£°ï¼Œé™ä½æœ€ç»ˆä¼°è®¡å™¨è¯¯å·®ã€‚</li>
<li>æå‡ºè‡ªæˆ‘ç›‘ç£å­¦ä¹ é˜¶æ®µï¼Œåˆ©ç”¨é›†åˆè¾“å‡ºä½œä¸ºæ— ç›‘ç£æ•™å¸ˆï¼Œå¯¹åˆå§‹æ¨¡å‹è¿›è¡Œå†è®­ç»ƒï¼Œé™ä½æµ‹è¯•æ—¶é—´æˆæœ¬ã€‚</li>
<li>å®éªŒè¯æ˜GTTAåœ¨å¤šä¸ªè§†è§‰å’Œéè§†è§‰æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>å¼•å…¥DeepSalmonæ•°æ®é›†ï¼Œä¸ºæ°´ä¸‹è§†é¢‘ä¸­çš„ä¸‰æ–‡é±¼åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡æä¾›å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01347">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d94aac69cf1c9d4bf58495b2408c0b5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Structure-and-Smoothness-Constrained-Dual-Networks-for-MR-Bias-Field-Correction"><a href="#Structure-and-Smoothness-Constrained-Dual-Networks-for-MR-Bias-Field-Correction" class="headerlink" title="Structure and Smoothness Constrained Dual Networks for MR Bias Field   Correction"></a>Structure and Smoothness Constrained Dual Networks for MR Bias Field   Correction</h2><p><strong>Authors:Dong Liang, Xingyu Qiu, Yuzhen Li, Wei Wang, Kuanquan Wang, Suyu Dong, Gongning Luo</strong></p>
<p>MR imaging techniques are of great benefit to disease diagnosis. However, due to the limitation of MR devices, significant intensity inhomogeneity often exists in imaging results, which impedes both qualitative and quantitative medical analysis. Recently, several unsupervised deep learning-based models have been proposed for MR image improvement. However, these models merely concentrate on global appearance learning, and neglect constraints from image structures and smoothness of bias field, leading to distorted corrected results. In this paper, novel structure and smoothness constrained dual networks, named S2DNets, are proposed aiming to self-supervised bias field correction. S2DNets introduce piece-wise structural constraints and smoothness of bias field for network training to effectively remove non-uniform intensity and retain much more structural details. Extensive experiments executed on both clinical and simulated MR datasets show that the proposed model outperforms other conventional and deep learning-based models. In addition to comparison on visual metrics, downstream MR image segmentation tasks are also used to evaluate the impact of the proposed model. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/LeongDong/S2DNets%7D%7Bhttps://github.com/LeongDong/S2DNets">https://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒæŠ€æœ¯å¯¹äºç–¾ç—…è¯Šæ–­å…·æœ‰æå¤§çš„ç›Šå¤„ã€‚ç„¶è€Œï¼Œç”±äºç£å…±æŒ¯è®¾å¤‡çš„é™åˆ¶ï¼Œæˆåƒç»“æœä¸­å¸¸å¸¸å­˜åœ¨æ˜¾è‘—çš„å¼ºåº¦ä¸å‡åŒ€æ€§ï¼Œè¿™é˜»ç¢äº†å®šæ€§å’Œå®šé‡åŒ»å­¦åˆ†æã€‚æœ€è¿‘ï¼Œæå‡ºäº†å‡ ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ— ç›‘ç£æ¨¡å‹æ¥æ”¹å–„ç£å…±æŒ¯å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»…å…³æ³¨å…¨å±€å¤–è§‚å­¦ä¹ ï¼Œå¿½è§†äº†å›¾åƒç»“æ„å’Œåç½®åœºçš„å¹³æ»‘æ€§çº¦æŸï¼Œå¯¼è‡´æ ¡æ­£ç»“æœå¤±çœŸã€‚æœ¬æ–‡æå‡ºäº†æ–°å‹ç»“æ„å’Œå¹³æ»‘åº¦çº¦æŸåŒç½‘ç»œï¼Œåä¸ºS2DNetsï¼Œæ—¨åœ¨è¿›è¡Œè‡ªç›‘ç£åç½®åœºæ ¡æ­£ã€‚S2DNetså¼•å…¥åˆ†æ®µç»“æ„çº¦æŸå’Œåç½®åœºçš„å¹³æ»‘åº¦ç”¨äºç½‘ç»œè®­ç»ƒï¼Œä»¥æœ‰æ•ˆåœ°å»é™¤éå‡åŒ€å¼ºåº¦å¹¶ä¿ç•™æ›´å¤šçš„ç»“æ„ç»†èŠ‚ã€‚åœ¨ä¸´åºŠå’Œæ¨¡æ‹Ÿç£å…±æŒ¯æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹ä¼˜äºå…¶ä»–ä¼ ç»Ÿå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹ã€‚é™¤äº†è§†è§‰æŒ‡æ ‡çš„å¯¹æ¯”ï¼Œè¿˜ä½¿ç”¨ä¸‹æ¸¸ç£å…±æŒ¯å›¾åƒåˆ†å‰²ä»»åŠ¡æ¥è¯„ä¼°æ‰€æå‡ºæ¨¡å‹çš„å½±å“ã€‚æºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/LeongDong/S2DNets">https://github.com/LeongDong/S2DNets</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01326v1">PDF</a> 11 pages, 3 figures, accepted by MICCAI</p>
<p><strong>Summary</strong></p>
<p>MRå½±åƒæŠ€æœ¯å¯¹ç–¾ç—…è¯Šæ–­æœ‰é‡è¦æ„ä¹‰ï¼Œä½†è®¾å¤‡é™åˆ¶å¯¼è‡´çš„å½±åƒç»“æœå¼ºåº¦ä¸å‡åŒ€æ€§å½±å“äº†å®šæ€§å’Œå®šé‡åˆ†æã€‚ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸»è¦å…³æ³¨å…¨å±€å¤–è§‚å­¦ä¹ ï¼Œå¿½ç•¥äº†å›¾åƒç»“æ„å’Œåç½®åœºçš„å¹³æ»‘æ€§çº¦æŸï¼Œå¯¼è‡´æ ¡æ­£ç»“æœå¤±çœŸã€‚æœ¬æ–‡æå‡ºæ–°å‹ç»“æ„å’Œå¹³æ»‘çº¦æŸåŒç½‘ç»œï¼ˆS2DNetsï¼‰ï¼Œæ—¨åœ¨å®ç°è‡ªæˆ‘ç›‘ç£åç½®åœºæ ¡æ­£ã€‚S2DNetså¼•å…¥åˆ†æ®µç»“æ„çº¦æŸå’Œåç½®åœºå¹³æ»‘æ€§ï¼Œæœ‰æ•ˆæ¶ˆé™¤éå‡åŒ€å¼ºåº¦ï¼Œä¿ç•™æ›´å¤šç»“æ„ç»†èŠ‚ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¸´åºŠåŒ»å­¦å’Œæ¨¡æ‹ŸMRæ•°æ®é›†ä¸Šä¼˜äºä¼ ç»Ÿå’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚é™¤è§†è§‰æŒ‡æ ‡å¯¹æ¯”å¤–ï¼Œè¿˜é€šè¿‡ä¸‹æ¸¸MRå›¾åƒåˆ†å‰²ä»»åŠ¡è¯„ä¼°æ¨¡å‹å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRå½±åƒæŠ€æœ¯å¯¹ç–¾ç—…è¯Šæ–­æœ‰é‡è¦ä½œç”¨ï¼Œä½†è®¾å¤‡é™åˆ¶å¯¼è‡´çš„å½±åƒç»“æœå¼ºåº¦ä¸å‡åŒ€æ€§æ˜¯åŒ»å­¦åˆ†æçš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨MRå›¾åƒæ”¹è¿›æ–¹é¢ä¸»è¦å…³æ³¨å…¨å±€å¤–è§‚å­¦ä¹ ï¼Œå¿½ç•¥äº†å›¾åƒç»“æ„å’Œåç½®åœºçš„å¹³æ»‘æ€§çº¦æŸã€‚</li>
<li>S2DNetsæ¨¡å‹æ—¨åœ¨é€šè¿‡è‡ªæˆ‘ç›‘ç£å®ç°åç½®åœºæ ¡æ­£ï¼Œå¼•å…¥åˆ†æ®µç»“æ„çº¦æŸå’Œåç½®åœºå¹³æ»‘æ€§ã€‚</li>
<li>S2DNetsæ¨¡å‹èƒ½æœ‰æ•ˆæ¶ˆé™¤å½±åƒç»“æœä¸­çš„éå‡åŒ€å¼ºåº¦ï¼ŒåŒæ—¶ä¿ç•™æ›´å¤šç»“æ„ç»†èŠ‚ã€‚</li>
<li>ä¸´åºŠè¯•éªŒå’Œæ¨¡æ‹Ÿæ•°æ®é›†çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒS2DNetsæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿå’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>é™¤äº†é€šè¿‡è§†è§‰æŒ‡æ ‡å¯¹æ¯”æ¨¡å‹æ€§èƒ½å¤–ï¼Œè¿˜é€šè¿‡ä¸‹æ¸¸MRå›¾åƒåˆ†å‰²ä»»åŠ¡æ¥è¯„ä¼°æ¨¡å‹çš„å®é™…å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5f2e00cbf5ae0e8e87d3be8f18a28b5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd404f9db6d17896fe2a64163a288080.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a2b90bf64ba82e970cf335ce607206f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Classification-based-deep-learning-models-for-lung-cancer-and-disease-using-medical-images"><a href="#Classification-based-deep-learning-models-for-lung-cancer-and-disease-using-medical-images" class="headerlink" title="Classification based deep learning models for lung cancer and disease   using medical images"></a>Classification based deep learning models for lung cancer and disease   using medical images</h2><p><strong>Authors:Ahmad Chaddad, Jihao Peng, Yihang Wu</strong></p>
<p>The use of deep learning (DL) in medical image analysis has significantly improved the ability to predict lung cancer. In this study, we introduce a novel deep convolutional neural network (CNN) model, named ResNet+, which is based on the established ResNet framework. This model is specifically designed to improve the prediction of lung cancer and diseases using the images. To address the challenge of missing feature information that occurs during the downsampling process in CNNs, we integrate the ResNet-D module, a variant designed to enhance feature extraction capabilities by modifying the downsampling layers, into the traditional ResNet model. Furthermore, a convolutional attention module was incorporated into the bottleneck layers to enhance model generalization by allowing the network to focus on relevant regions of the input images. We evaluated the proposed model using five public datasets, comprising lung cancer (LC2500 $n$&#x3D;3183, IQ-OTH&#x2F;NCCD $n$&#x3D;1336, and LCC $n$&#x3D;25000 images) and lung disease (ChestXray $n$&#x3D;5856, and COVIDx-CT $n$&#x3D;425024 images). To address class imbalance, we used data augmentation techniques to artificially increase the representation of underrepresented classes in the training dataset. The experimental results show that ResNet+ model demonstrated remarkable accuracy&#x2F;F1, reaching 98.14&#x2F;98.14% on the LC25000 dataset and 99.25&#x2F;99.13% on the IQ-OTH&#x2F;NCCD dataset. Furthermore, the ResNet+ model saved computational cost compared to the original ResNet series in predicting lung cancer images. The proposed model outperformed the baseline models on publicly available datasets, achieving better performance metrics. Our codes are publicly available at <a target="_blank" rel="noopener" href="https://github.com/AIPMLab/Graduation-2024/tree/main/Peng">https://github.com/AIPMLab/Graduation-2024/tree/main/Peng</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨å¤§å¤§æé«˜äº†é¢„æµ‹è‚ºç™Œçš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå·²å»ºç«‹çš„ResNetæ¡†æ¶çš„æ–°å‹æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹ï¼Œåä¸ºResNet+ã€‚è¯¥æ¨¡å‹ä¸“é—¨è®¾è®¡ç”¨äºåˆ©ç”¨å›¾åƒé¢„æµ‹è‚ºç™Œå’Œè‚ºéƒ¨ç–¾ç—…ã€‚ä¸ºäº†è§£å†³CNNåœ¨ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­ç¼ºå¤±ç‰¹å¾ä¿¡æ¯çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†ResNet-Dæ¨¡å—é›†æˆåˆ°ä¼ ç»ŸResNetæ¨¡å‹ä¸­ã€‚è¿™ç§å˜ä½“æ—¨åœ¨é€šè¿‡ä¿®æ”¹ä¸‹é‡‡æ ·å±‚å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜åœ¨ç“¶é¢ˆå±‚ä¸­åŠ å…¥äº†å·ç§¯æ³¨æ„åŠ›æ¨¡å—ï¼Œé€šè¿‡å…è®¸ç½‘ç»œå…³æ³¨è¾“å…¥å›¾åƒçš„ç›¸å…³åŒºåŸŸï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨äº”ä¸ªå…¬å…±æ•°æ®é›†å¯¹æå‡ºçš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬è‚ºç™Œï¼ˆLC2500 n&#x3D;3183ã€IQ-OTH&#x2F;NCCD n&#x3D;1336å’ŒLCC n&#x3D;25000å›¾åƒï¼‰å’Œè‚ºéƒ¨ç–¾ç—…ï¼ˆChestXray n&#x3D;5856å’ŒCOVIDx-CT n&#x3D;425024å›¾åƒï¼‰ã€‚ä¸ºäº†è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯äººä¸ºå¢åŠ è®­ç»ƒæ•°æ®é›†ä¸­ä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«çš„è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒResNet+æ¨¡å‹åœ¨LC25000æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„å‡†ç¡®ç‡å’ŒF1åˆ†æ•°ï¼Œè¾¾åˆ°98.14%&#x2F;98.14%ï¼Œåœ¨IQ-OTH&#x2F;NCCDæ•°æ®é›†ä¸Šè¾¾åˆ°99.25%&#x2F;99.13%ã€‚æ­¤å¤–ï¼ŒResNet+æ¨¡å‹åœ¨é¢„æµ‹è‚ºç™Œå›¾åƒæ—¶æ¯”åŸå§‹ResNetç³»åˆ—èŠ‚çœäº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ¨¡å‹åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå–å¾—äº†æ›´å¥½çš„æ€§èƒ½æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/AIPMLab/Graduation-2024/tree/main/Peng%E3%80%82">https://github.com/AIPMLab/Graduation-2024/tree/main/Pengã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01279v1">PDF</a> Accepted in IEEE Transactions on Radiation and Plasma Medical   Sciences</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„åŒ»å­¦å›¾åƒåˆ†æåœ¨é¢„æµ‹è‚ºç™Œæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹â€”â€”ResNet+ï¼Œè¯¥æ¨¡å‹åŸºäºæˆç†Ÿçš„ResNetæ¡†æ¶æ„å»ºï¼Œæ—¨åœ¨æé«˜åˆ©ç”¨å›¾åƒé¢„æµ‹è‚ºç™ŒåŠå…¶ä»–ç–¾ç—…çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³CNNä¸­ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­ç‰¹å¾ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ï¼Œç ”ç©¶è€…å°†ResNet-Dæ¨¡å—é›†æˆåˆ°ä¼ ç»ŸResNetæ¨¡å‹ä¸­ï¼Œä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜åœ¨ç“¶é¢ˆå±‚å¼•å…¥äº†å·ç§¯æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿå…³æ³¨è¾“å…¥å›¾åƒçš„ç›¸å…³åŒºåŸŸã€‚ä½¿ç”¨äº”ä¸ªå…¬å¼€æ•°æ®é›†å¯¹æå‡ºçš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬è‚ºç™Œæ•°æ®é›†å’Œè‚ºéƒ¨ç–¾ç—…æ•°æ®é›†ã€‚ä¸ºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œé‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯æ¥äººä¸ºå¢åŠ è®­ç»ƒæ•°æ®é›†ä¸­ä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«çš„è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒResNet+æ¨¡å‹åœ¨LC2500å’ŒIQ-OTH&#x2F;NCCDæ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†98.14%çš„å‡†ç¡®ç‡å’ŒF1å€¼ï¼Œä»¥åŠåœ¨ChestXrayå’ŒCOVIDx-CTæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†å¾ˆé«˜çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒResNet+æ¨¡å‹åœ¨é¢„æµ‹è‚ºç™Œå›¾åƒæ—¶ï¼Œç›¸è¾ƒäºåŸå§‹ResNetç³»åˆ—ï¼ŒèŠ‚çœäº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ¨¡å‹åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå–å¾—äº†æ›´å¥½çš„æ€§èƒ½æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼•å…¥äº†åä¸ºResNet+çš„æ–°å‹æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºæ”¹è¿›è‚ºç™ŒåŠè‚ºéƒ¨ç–¾ç—…çš„é¢„æµ‹ã€‚</li>
<li>ä¸ºè§£å†³CNNä¸­ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­çš„ç‰¹å¾ä¿¡æ¯ä¸¢å¤±é—®é¢˜ï¼Œé›†æˆäº†ResNet-Dæ¨¡å—ã€‚</li>
<li>åœ¨æ¨¡å‹ä¸­åŠ å…¥äº†å·ç§¯æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æé«˜å¯¹è¾“å…¥å›¾åƒç›¸å…³åŒºåŸŸçš„å…³æ³¨ï¼Œè¿›è€Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨äº”ä¸ªå…¬å¼€æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬è‚ºç™Œå’Œè‚ºéƒ¨ç–¾ç—…æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ResNet+æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡å’ŒF1å€¼ï¼Œä¸”ç›¸è¾ƒäºåŸå§‹ResNetç³»åˆ—ï¼Œè®¡ç®—æˆæœ¬æ›´ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c3eca06d9fb0977372f08a26101ab49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc0e9e2ca7d4f051a7fa51085845dd58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00227fae96f6c9b0205480fa69e2e718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d68616d437db0bd9769e4d5a0170a708.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef3399eb39f0ffa16bba46e58bc26cfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe0176153750e29bfbf264e4a3a8062d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24955520c27ce07394817c40736e5e5c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DMCIE-Diffusion-Model-with-Concatenation-of-Inputs-and-Errors-to-Improve-the-Accuracy-of-the-Segmentation-of-Brain-Tumors-in-MRI-Images"><a href="#DMCIE-Diffusion-Model-with-Concatenation-of-Inputs-and-Errors-to-Improve-the-Accuracy-of-the-Segmentation-of-Brain-Tumors-in-MRI-Images" class="headerlink" title="DMCIE: Diffusion Model with Concatenation of Inputs and Errors to   Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images"></a>DMCIE: Diffusion Model with Concatenation of Inputs and Errors to   Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images</h2><p><strong>Authors:Sara Yavari, Rahul Nitin Pandya, Jacob Furst</strong></p>
<p>Accurate segmentation of brain tumors in MRI scans is essential for reliable clinical diagnosis and effective treatment planning. Recently, diffusion models have demonstrated remarkable effectiveness in image generation and segmentation tasks. This paper introduces a novel approach to corrective segmentation based on diffusion models. We propose DMCIE (Diffusion Model with Concatenation of Inputs and Errors), a novel framework for accurate brain tumor segmentation in multi-modal MRI scans. We employ a 3D U-Net to generate an initial segmentation mask, from which an error map is generated by identifying the differences between the prediction and the ground truth. The error map, concatenated with the original MRI images, are used to guide a diffusion model. Using multimodal MRI inputs (T1, T1ce, T2, FLAIR), DMCIE effectively enhances segmentation accuracy by focusing on misclassified regions, guided by the original inputs. Evaluated on the BraTS2020 dataset, DMCIE outperforms several state-of-the-art diffusion-based segmentation methods, achieving a Dice Score of 93.46 and an HD95 of 5.94 mm. These results highlight the effectiveness of error-guided diffusion in producing precise and reliable brain tumor segmentations. </p>
<blockquote>
<p>åœ¨MRIæ‰«æä¸­å¯¹è„‘è‚¿ç˜¤è¿›è¡Œå‡†ç¡®çš„åˆ†å‰²å¯¹äºå¯é çš„ä¸´åºŠè¯Šæ–­å’Œæœ‰æ•ˆçš„æ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ä¿®æ­£åˆ†å‰²æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†DMCIEï¼ˆè¾“å…¥å’Œè¯¯å·®ç»„åˆçš„æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ¨¡æ€MRIæ‰«æä¸­è„‘è‚¿ç˜¤ç²¾ç¡®åˆ†å‰²çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬é‡‡ç”¨3D U-Netç”Ÿæˆåˆå§‹åˆ†å‰²æ©è†œï¼Œé€šè¿‡è¯†åˆ«é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚æ¥ç”Ÿæˆè¯¯å·®å›¾ã€‚è¯¯å·®å›¾ä¸åŸå§‹MRIå›¾åƒç»„åˆï¼Œç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ã€‚ä½¿ç”¨å¤šæ¨¡æ€MRIè¾“å…¥ï¼ˆT1ã€T1ceã€T2ã€FLAIRï¼‰ï¼ŒDMCIEé€šè¿‡å…³æ³¨è¯¯åˆ†ç±»åŒºåŸŸï¼Œåœ¨åŸå§‹è¾“å…¥çš„å¼•å¯¼ä¸‹ï¼Œæœ‰æ•ˆåœ°æé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚åœ¨BraTS2020æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒDMCIEä¼˜äºå‡ ç§å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„åˆ†å‰²æ–¹æ³•ï¼Œå®ç°äº†Diceå¾—åˆ†ä¸º93.46ï¼ŒHD95ä¸º5.94æ¯«ç±³ã€‚è¿™äº›ç»“æœçªæ˜¾äº†è¯¯å·®å¼•å¯¼æ‰©æ•£åœ¨äº§ç”Ÿç²¾ç¡®å¯é çš„è„‘è‚¿ç˜¤åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹è„‘è‚¿ç˜¤çŸ«æ­£åˆ†å‰²æ–¹æ³•â€”â€”DMCIEã€‚è¯¥æ–¹æ³•é‡‡ç”¨3D U-Netç”Ÿæˆåˆå§‹åˆ†å‰²æ©è†œï¼Œé€šè¿‡è¯†åˆ«é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ç”Ÿæˆè¯¯å·®å›¾ã€‚è¯¯å·®å›¾ä¸åŸå§‹MRIå›¾åƒç»“åˆï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„è¿è¡Œã€‚ä½¿ç”¨å¤šæ¨¡æ€MRIè¾“å…¥ï¼ˆT1ã€T1ceã€T2ã€FLAIRï¼‰ï¼ŒDMCIEèƒ½å¤Ÿä¸“æ³¨äºè¯¯åˆ†ç±»åŒºåŸŸï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒDMCIEä¼˜äºå…¶ä»–å…ˆè¿›çš„æ‰©æ•£åˆ†å‰²æ–¹æ³•ï¼ŒDiceå¾—åˆ†ä¸º93.46ï¼ŒHD95ä¸º5.94mmï¼Œå‡¸æ˜¾äº†è¯¯å·®å¼•å¯¼æ‰©æ•£åœ¨ç²¾ç¡®å¯é çš„è„‘è‚¿ç˜¤åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DMCIEæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹è„‘è‚¿ç˜¤çŸ«æ­£åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨3D U-Netç”Ÿæˆåˆå§‹åˆ†å‰²æ©è†œã€‚</li>
<li>é€šè¿‡è¯†åˆ«é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ç”Ÿæˆè¯¯å·®å›¾ã€‚</li>
<li>è¯¯å·®å›¾ä¸åŸå§‹MRIå›¾åƒç»“åˆï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„è¿è¡Œã€‚</li>
<li>å¤šæ¨¡æ€MRIè¾“å…¥ï¼ˆT1ã€T1ceã€T2ã€FLAIRï¼‰ç”¨äºæé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>DMCIEåœ¨BraTS2020æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›çš„æ‰©æ•£åˆ†å‰²æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e37cca82601f29b1b4fb2e624606581a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba99c782d6346a1a976b5c91b4be5266.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MTCNet-Motion-and-Topology-Consistency-Guided-Learning-for-Mitral-Valve-Segmentationin-4D-Ultrasound"><a href="#MTCNet-Motion-and-Topology-Consistency-Guided-Learning-for-Mitral-Valve-Segmentationin-4D-Ultrasound" class="headerlink" title="MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve   Segmentationin 4D Ultrasound"></a>MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve   Segmentationin 4D Ultrasound</h2><p><strong>Authors:Rusi Chen, Yuanting Yang, Jiezhi Yao, Hongning Song, Ji Zhang, Yongsong Zhou, Yuhao Huang, Ronghao Yang, Dan Jia, Yuhan Zhang, Xing Tao, Haoran Dou, Qing Zhou, Xin Yang, Dong Ni</strong></p>
<p>Mitral regurgitation is one of the most prevalent cardiac disorders. Four-dimensional (4D) ultrasound has emerged as the primary imaging modality for assessing dynamic valvular morphology. However, 4D mitral valve (MV) analysis remains challenging due to limited phase annotations, severe motion artifacts, and poor imaging quality. Yet, the absence of inter-phase dependency in existing methods hinders 4D MV analysis. To bridge this gap, we propose a Motion-Topology guided consistency network (MTCNet) for accurate 4D MV ultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only sparse end-diastolic and end-systolic annotations. First, we design a cross-phase motion-guided consistency learning strategy, utilizing a bi-directional attention memory bank to propagate spatio-temporal features. This enables MTCNet to achieve excellent performance both per- and inter-phase. Second, we devise a novel topology-guided correlation regularization that explores physical prior knowledge to maintain anatomically plausible. Therefore, MTCNet can effectively leverage structural correspondence between labeled and unlabeled phases. Extensive evaluations on the first largest 4D MV dataset, with 1408 phases from 160 patients, show that MTCNet performs superior cross-phase consistency compared to other advanced methods (Dice: 87.30%, HD: 1.75mm). Both the code and the dataset are available at <a target="_blank" rel="noopener" href="https://github.com/crs524/MTCNet">https://github.com/crs524/MTCNet</a>. </p>
<blockquote>
<p>äºŒå°–ç“£åæµæ˜¯æœ€å¸¸è§çš„å¿ƒè„ç–¾ç—…ä¹‹ä¸€ã€‚å››ç»´ï¼ˆ4Dï¼‰è¶…å£°å·²æˆä¸ºè¯„ä¼°åŠ¨æ€ç“£è†œå½¢æ€çš„ä¸»è¦æˆåƒæ–¹å¼ã€‚ç„¶è€Œï¼Œç”±äºç›¸ä½æ³¨é‡Šæœ‰é™ã€è¿åŠ¨ä¼ªå½±ä¸¥é‡ä»¥åŠæˆåƒè´¨é‡ä¸ä½³ï¼Œ4DäºŒå°–ç“£ï¼ˆMVï¼‰åˆ†æä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹ç›¸ä½é—´çš„ä¾èµ–æ€§ï¼Œé˜»ç¢äº†4D MVåˆ†æã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿åŠ¨æ‹“æ‰‘å¼•å¯¼çš„ä¸€è‡´æ€§ç½‘ç»œï¼ˆMTCNetï¼‰ï¼Œç”¨äºåŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä¸­å‡†ç¡®çš„4D MVè¶…å£°åˆ†å‰²ã€‚MTCNetä»…éœ€è¦ç¨€ç–çš„èˆ’å¼ æœ«æœŸå’Œæ”¶ç¼©æœ«æœŸæ³¨é‡Šã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è·¨ç›¸ä½è¿åŠ¨å¼•å¯¼çš„ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨åŒå‘æ³¨æ„åŠ›å­˜å‚¨åº“ä¼ æ’­æ—¶ç©ºç‰¹å¾ã€‚è¿™ä½¿å¾—MTCNetåœ¨è·¨ç›¸ä½å†…å’Œè·¨ç›¸ä½é—´éƒ½èƒ½å®ç°å“è¶Šçš„æ€§èƒ½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹æ‹“æ‰‘å¼•å¯¼ç›¸å…³æ€§æ­£åˆ™åŒ–ï¼Œåˆ©ç”¨ç‰©ç†å…ˆéªŒçŸ¥è¯†æ¥ä¿æŒè§£å‰–ä¸Šçš„åˆç†æ€§ã€‚å› æ­¤ï¼ŒMTCNetå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾ç›¸ä½ä¹‹é—´çš„ç»“æ„å¯¹åº”å…³ç³»ã€‚åœ¨é¦–ä¸ªæœ€å¤§çš„4D MVæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª160åæ‚£è€…çš„1408ä¸ªç›¸ä½ï¼Œç»“æœè¡¨æ˜MTCNetåœ¨è·¨ç›¸ä½ä¸€è‡´æ€§æ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ˆDiceï¼š87.30%ï¼ŒHDï¼š1.75mmï¼‰ã€‚ä»£ç å’Œæ•°æ®é›†å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/crs524/MTCNet">https://github.com/crs524/MTCNet</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00660v2">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹4Dè¶…å£°å¿ƒåŠ¨å›¾åˆ†æä¸­MVåˆ†æçš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åä¸ºMTCNetçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿åŠ¨æ‹“æ‰‘å¼•å¯¼çš„ä¸€è‡´æ€§ç½‘ç»œï¼Œå®ç°äº†åœ¨åŠç›‘ç£å­¦ä¹ ä¸‹çš„å‡†ç¡®4D MVè¶…å£°åˆ†å‰²ã€‚æ¨¡å‹åˆ©ç”¨ç¨€ç–çš„èˆ’å¼ æœ«æœŸå’Œæ”¶ç¼©æœ«æœŸæ³¨é‡Šï¼Œé€šè¿‡è·¨ç›¸ä½è¿åŠ¨å¼•å¯¼çš„ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥å®ç°ä¼˜ç§€æ€§èƒ½ã€‚åŒæ—¶ï¼Œæ¨¡å‹ç»“åˆäº†ç‰©ç†å…ˆéªŒçŸ¥è¯†çš„æ‹“æ‰‘å¼•å¯¼ç›¸å…³æ€§æ­£åˆ™åŒ–ï¼Œä»¥å®ç°è§£å‰–ä¸Šçš„åˆç†åˆ†å‰²ã€‚åœ¨å¤§å‹4D MVæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMTCNetç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•å…·æœ‰æ›´å¥½çš„è·¨ç›¸ä½ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å››ç»´è¶…å£°å¿ƒåŠ¨å›¾ï¼ˆ4Dè¶…å£°ï¼‰æ˜¯ç›®å‰è¯„ä¼°åŠ¨æ€ç“£è†œå½¢æ€çš„ä¸»è¦æˆåƒæ–¹å¼ã€‚</li>
<li>4DäºŒå°–ç“£ï¼ˆMVï¼‰åˆ†æé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚ç›¸ä½æ³¨é‡Šæœ‰é™ã€è¿åŠ¨ä¼ªå½±å’Œæˆåƒè´¨é‡ä¸ä½³ç­‰ã€‚</li>
<li>MTCNetæ¨¡å‹é€šè¿‡è¿åŠ¨æ‹“æ‰‘å¼•å¯¼çš„ä¸€è‡´æ€§ç½‘ç»œè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œå®ç°äº†åœ¨åŠç›‘ç£å­¦ä¹ ä¸‹çš„å‡†ç¡®4D MVè¶…å£°åˆ†å‰²ã€‚</li>
<li>MTCNetä»…éœ€è¦ç¨€ç–çš„èˆ’å¼ æœ«æœŸå’Œæ”¶ç¼©æœ«æœŸæ³¨é‡Šã€‚</li>
<li>MTCNeté‡‡ç”¨è·¨ç›¸ä½è¿åŠ¨å¼•å¯¼çš„ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨åŒå‘æ³¨æ„åŠ›è®°å¿†åº“ä¼ æ’­æ—¶ç©ºç‰¹å¾ï¼Œå®ç°ä¼˜ç§€æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†ç‰©ç†å…ˆéªŒçŸ¥è¯†çš„æ‹“æ‰‘å¼•å¯¼ç›¸å…³æ€§æ­£åˆ™åŒ–ï¼Œç¡®ä¿è§£å‰–ä¸Šçš„åˆç†åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be0ba723776bd92f8c6e38c3b32b5a45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e1055bd04b4914fd18d63b74a498110.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ccd5efe24ee06042b754bad32365a5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62eef599675e7913a5b83eea69b97a91.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Similarity-Memory-Prior-is-All-You-Need-for-Medical-Image-Segmentation"><a href="#Similarity-Memory-Prior-is-All-You-Need-for-Medical-Image-Segmentation" class="headerlink" title="Similarity Memory Prior is All You Need for Medical Image Segmentation"></a>Similarity Memory Prior is All You Need for Medical Image Segmentation</h2><p><strong>Authors:Tang Hao, Guo ZhiQing, Wang LieJun, Liu Chao</strong></p>
<p>In recent years, it has been found that â€œgrandmother cellsâ€ in the primary visual cortex (V1) of macaques can directly recognize visual input with complex shapes. This inspires us to examine the value of these cells in promoting the research of medical image segmentation. In this paper, we design a Similarity Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically, we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and remembers the category features of specific lesions or organs in medical images through the similarity memory prior in the prototype memory bank, thus helping the network to learn subtle texture changes between categories. DMW-LA also dynamically updates the similarity memory prior in reverse through Weight-Loss Dynamic (W-LD) update strategy, effectively assisting the network directly extract category features. In addition, we propose the Double-Similarity Global Internal Enhancement Module (DS-GIM) to deeply explore the internal differences in the feature distribution of input data through cosine similarity and euclidean distance. Extensive experiments on four public datasets show that Sim-MPNet has better segmentation performance than other state-of-the-art methods. Our code is available on <a target="_blank" rel="noopener" href="https://github.com/vpsg-research/Sim-MPNet">https://github.com/vpsg-research/Sim-MPNet</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç ”ç©¶å‘ç°çŒ•çŒ´åˆçº§è§†è§‰çš®å±‚ï¼ˆV1ï¼‰ä¸­çš„â€œç¥–æ¯ç»†èƒâ€èƒ½ç›´æ¥è¯†åˆ«å…·æœ‰å¤æ‚å½¢çŠ¶çš„è§†è§‰è¾“å…¥ï¼Œè¿™æ¿€å‘äº†æˆ‘ä»¬æ¢ç´¢è¿™äº›ç»†èƒåœ¨æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²ç ”ç©¶ä¸­çš„ä»·å€¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡äº†ä¸€ç§ç›¸ä¼¼æ€§è®°å¿†å…ˆéªŒç½‘ç»œï¼ˆSim-MPNetï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€è®°å¿†æƒé‡æŸå¤±æ³¨æ„åŠ›ï¼ˆDMW-LAï¼‰ï¼Œå®ƒé€šè¿‡åŸå‹è®°å¿†åº“ä¸­çš„ç›¸ä¼¼æ€§è®°å¿†å…ˆéªŒæ¥åŒ¹é…å’Œè®°å¿†åŒ»å­¦å›¾åƒä¸­ç‰¹å®šç—…å˜æˆ–å™¨å®˜çš„åˆ†ç±»ç‰¹å¾ï¼Œä»è€Œå¸®åŠ©ç½‘ç»œå­¦ä¹ ç±»åˆ«ä¹‹é—´çš„ç»†å¾®çº¹ç†å˜åŒ–ã€‚DMW-LAè¿˜é€šè¿‡æƒé‡æŸå¤±åŠ¨æ€ï¼ˆW-LDï¼‰æ›´æ–°ç­–ç•¥åå‘åŠ¨æ€æ›´æ–°ç›¸ä¼¼æ€§è®°å¿†å…ˆéªŒï¼Œæœ‰æ•ˆåœ°å¸®åŠ©ç½‘ç»œç›´æ¥æå–ç±»åˆ«ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åŒç›¸ä¼¼æ€§å…¨å±€å†…éƒ¨å¢å¼ºæ¨¡å—ï¼ˆDS-GIMï¼‰ï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦å’Œæ¬§å‡ é‡Œå¾—è·ç¦»æ·±å…¥æ¢ç´¢è¾“å…¥æ•°æ®ç‰¹å¾åˆ†å¸ƒçš„å†…éƒ¨å·®å¼‚ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSim-MPNetçš„åˆ†å‰²æ€§èƒ½ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vpsg-research/Sim-MPNet%E4%B8%8A%E8%8E%B7%E3%80%82">https://github.com/vpsg-research/Sim-MPNetä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00585v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å…³äºçŒ•çŒ´åˆçº§è§†è§‰çš®å±‚V1ä¸­çš„â€œç¥–æ¯ç»†èƒâ€å¯¹å¤æ‚å½¢çŠ¶è§†è§‰è¾“å…¥çš„è¯†åˆ«èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œç ”ç©¶äººå‘˜è®¾è®¡äº†ä¸€ç§åä¸ºSim-MPNetçš„åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œï¼Œå¹¶å¼•å…¥äº†Dynamic Memory Weights-Loss Attentionï¼ˆDMW-LAï¼‰å’ŒDouble-Similarity Global Internal Enhancement Moduleï¼ˆDS-GIMï¼‰ä¸¤å¤§æ¨¡å—ï¼Œä»¥é€šè¿‡ç›¸ä¼¼æ€§è®°å¿†å…ˆéªŒæ¥åŒ¹é…å’Œè®°å¿†åŒ»å­¦å›¾åƒä¸­ç‰¹å®šç—…å˜æˆ–å™¨å®˜çš„ç‰¹å¾ç±»åˆ«ï¼Œä»è€Œæé«˜ç½‘ç»œå¯¹ç±»åˆ«é—´ç»†å¾®çº¹ç†å˜åŒ–çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶æ·±å…¥æ¢ç´¢è¾“å…¥æ•°æ®ç‰¹å¾åˆ†å¸ƒçš„å†…éƒ¨å·®å¼‚ã€‚å®éªŒè¡¨æ˜ï¼ŒSim-MPNetåœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„åˆ†å‰²æ€§èƒ½ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>â€œç¥–æ¯ç»†èƒâ€åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ç ”ç©¶ä¸­å…·æœ‰å¯ç¤ºä½œç”¨ï¼Œèƒ½å¤Ÿç›´æ¥è¯†åˆ«å…·æœ‰å¤æ‚å½¢çŠ¶çš„è§†è§‰è¾“å…¥ã€‚</li>
<li>æå‡ºäº†åä¸ºSim-MPNetçš„åŒ»å­¦å›¾åƒåˆ†å‰²ç½‘ç»œã€‚</li>
<li>DMW-LAæ¨¡å—é€šè¿‡ç›¸ä¼¼æ€§è®°å¿†å…ˆéªŒåŒ¹é…å’Œè®°å¿†åŒ»å­¦å›¾åƒä¸­çš„ç‰¹å®šç—…å˜æˆ–å™¨å®˜çš„ç‰¹å¾ç±»åˆ«ã€‚</li>
<li>DMW-LAé‡‡ç”¨Weight-Loss Dynamicï¼ˆW-LDï¼‰æ›´æ–°ç­–ç•¥ï¼Œå¯åŠ¨æ€æ›´æ–°ç›¸ä¼¼æ€§è®°å¿†å…ˆéªŒã€‚</li>
<li>DS-GIMæ¨¡å—ç”¨äºæ·±å…¥æ¢ç´¢è¾“å…¥æ•°æ®ç‰¹å¾åˆ†å¸ƒçš„å†…éƒ¨å·®å¼‚ã€‚</li>
<li>Sim-MPNetåœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3888e366a24d9fcf8d197d2314897873.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a141f46d08d3306c7ff02fa9df54290.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3728bf2b4781ef5897204dfc15e654e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-878863d1775997702f5c7ff096bd9293.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03623ce2555efcb668489c7185055423.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7522946d8ad6af48335e690a843a3432.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Bridging-Classical-and-Learning-based-Iterative-Registration-through-Deep-Equilibrium-Models"><a href="#Bridging-Classical-and-Learning-based-Iterative-Registration-through-Deep-Equilibrium-Models" class="headerlink" title="Bridging Classical and Learning-based Iterative Registration through   Deep Equilibrium Models"></a>Bridging Classical and Learning-based Iterative Registration through   Deep Equilibrium Models</h2><p><strong>Authors:Yi Zhang, Yidong Zhao, Qian Tao</strong></p>
<p>Deformable medical image registration is traditionally formulated as an optimization problem. While classical methods solve this problem iteratively, recent learning-based approaches use recurrent neural networks (RNNs) to mimic this process by unrolling the prediction of deformation fields in a fixed number of steps. However, classical methods typically converge after sufficient iterations, but learning-based unrolling methods lack a theoretical convergence guarantee and show instability empirically. In addition, unrolling methods have a practical bottleneck at training time: GPU memory usage grows linearly with the unrolling steps due to backpropagation through time (BPTT). To address both theoretical and practical challenges, we propose DEQReg, a novel registration framework based on Deep Equilibrium Models (DEQ), which formulates registration as an equilibrium-seeking problem, establishing a natural connection between classical optimization and learning-based unrolling methods. DEQReg maintains constant memory usage, enabling theoretically unlimited iteration steps. Through extensive evaluation on the public brain MRI and lung CT datasets, we show that DEQReg can achieve competitive registration performance, while substantially reducing memory consumption compared to state-of-the-art unrolling methods. We also reveal an intriguing phenomenon: the performance of existing unrolling methods first increases slightly then degrades irreversibly when the inference steps go beyond the training configuration. In contrast, DEQReg achieves stable convergence with its inbuilt equilibrium-seeking mechanism, bridging the gap between classical optimization-based and modern learning-based registration methods. </p>
<blockquote>
<p>å¯å˜å½¢åŒ»å­¦å›¾åƒé…å‡†ä¼ ç»Ÿä¸Šè¢«åˆ¶å®šä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ã€‚è™½ç„¶ç»å…¸æ–¹æ³•é€šè¿‡è¿­ä»£æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æœ€è¿‘çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ¥æ¨¡æ‹Ÿè¿™ä¸ªè¿‡ç¨‹ï¼Œé€šè¿‡åœ¨å›ºå®šæ•°é‡çš„æ­¥éª¤ä¸­å±•å¼€å˜å½¢åœºçš„é¢„æµ‹ã€‚ç„¶è€Œï¼Œç»å…¸æ–¹æ³•é€šå¸¸åœ¨è¶³å¤Ÿçš„è¿­ä»£åæ”¶æ•›ï¼Œä½†åŸºäºå±•å¼€çš„å­¦ä¹ æ–¹æ³•ç¼ºä¹ç†è®ºæ”¶æ•›ä¿è¯ï¼Œå¹¶ä¸”åœ¨ç»éªŒä¸Šæ˜¾ç¤ºå‡ºä¸ç¨³å®šã€‚æ­¤å¤–ï¼Œå±•å¼€æ–¹æ³•åœ¨è®­ç»ƒæ—¶é—´æ–¹é¢å­˜åœ¨å®é™…ç“¶é¢ˆï¼šç”±äºæ—¶é—´åå‘ä¼ æ’­ï¼ˆBPTTï¼‰ï¼ŒGPUå†…å­˜ä½¿ç”¨é‡éšå±•å¼€æ­¥éª¤çº¿æ€§å¢é•¿ã€‚ä¸ºäº†è§£å†³ç†è®ºä¸Šçš„æŒ‘æˆ˜å’Œå®è·µä¸Šçš„å›°éš¾ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ·±åº¦å¹³è¡¡æ¨¡å‹ï¼ˆDEQï¼‰çš„æ–°å‹é…å‡†æ¡†æ¶DEQRegã€‚å®ƒå°†é…å‡†å…¬å¼åŒ–ä¸ºå¹³è¡¡å¯»æ‰¾é—®é¢˜ï¼Œåœ¨ç»å…¸ä¼˜åŒ–æ–¹æ³•å’ŒåŸºäºå­¦ä¹ çš„å±•å¼€æ–¹æ³•ä¹‹é—´å»ºç«‹äº†è‡ªç„¶çš„è”ç³»ã€‚DEQRegä¿æŒæ’å®šçš„å†…å­˜ä½¿ç”¨ï¼Œå®ç°ç†è®ºä¸Šæ— é™è¿­ä»£æ­¥éª¤ã€‚é€šè¿‡å¯¹å…¬å…±è„‘MRIå’Œè‚ºéƒ¨CTæ•°æ®é›†è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†DEQRegåœ¨è¾¾åˆ°ç«äº‰æ€§çš„é…å‡†æ€§èƒ½çš„åŒæ—¶ï¼Œä¸æœ€å…ˆè¿›çš„å±•å¼€æ–¹æ³•ç›¸æ¯”å¤§å¤§å‡å°‘äº†å†…å­˜æ¶ˆè€—ã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šç°æœ‰å±•å¼€æ–¹æ³•çš„æ€§èƒ½åœ¨æ¨æ–­æ­¥éª¤è¶…å‡ºè®­ç»ƒé…ç½®æ—¶é¦–å…ˆä¼šç•¥æœ‰æé«˜ï¼Œç„¶åä¸å¯é€†åœ°ä¸‹é™ã€‚ç›¸åï¼ŒDEQRegå‡­å€Ÿå…¶å†…ç½®çš„å‡è¡¡å¯»æ‰¾æœºåˆ¶å®ç°äº†ç¨³å®šçš„æ”¶æ•›ï¼Œå¼¥åˆäº†åŸºäºç»å…¸ä¼˜åŒ–çš„å’ŒåŸºäºç°ä»£å­¦ä¹ çš„é…å‡†æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00582v1">PDF</a> Submitted version. Accepted by MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ·±åº¦å‡è¡¡æ¨¡å‹ï¼ˆDEQï¼‰çš„æ–°å‹åŒ»å­¦å›¾åƒæ³¨å†Œæ¡†æ¶DEQRegã€‚ä¼ ç»ŸåŒ»å­¦å›¾åƒæ³¨å†Œè¢«åˆ¶å®šä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œè€ŒDEQRegå°†å…¶åˆ¶å®šä¸ºå¹³è¡¡å¯»æ±‚é—®é¢˜ï¼Œå»ºç«‹äº†ç»å…¸ä¼˜åŒ–ä¸åŸºäºå­¦ä¹ çš„æ»šåŠ¨æ–¹æ³•ä¹‹é—´çš„è‡ªç„¶è”ç³»ã€‚ç›¸è¾ƒäºå½“å‰æµè¡Œçš„æ»šåŠ¨æ–¹æ³•ï¼ŒDEQRegåœ¨ç†è®ºä¸Šå®ç°äº†æ’å®šçš„å†…å­˜ä½¿ç”¨å¹¶å¯ä»¥æ”¯æŒç†è®ºä¸Šçš„æ— é™è¿­ä»£æ­¥éª¤ï¼Œåœ¨ä¿è¯æ³¨å†Œæ€§èƒ½çš„åŒæ—¶æå¤§åœ°é™ä½äº†å†…å­˜æ¶ˆè€—ã€‚åœ¨å…¬å…±è„‘MRIå’Œè‚ºéƒ¨CTæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜DEQRegçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ­ç¤ºäº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šå½“æ¨ç†æ­¥éª¤è¶…å‡ºè®­ç»ƒé…ç½®æ—¶ï¼Œç°æœ‰æ»šåŠ¨æ–¹æ³•çš„æ€§èƒ½ä¼šå…ˆç•¥æœ‰æé«˜ç„¶åä¸å¯é€†åœ°ä¸‹é™ï¼Œè€ŒDEQRegå‡­å€Ÿå…¶å†…ç½®çš„å‡è¡¡å¯»æ±‚æœºåˆ¶å®ç°äº†ç¨³å®šçš„æ”¶æ•›ï¼Œç¼©å°äº†åŸºäºç»å…¸ä¼˜åŒ–å’Œç°ä»£åŸºäºå­¦ä¹ çš„æ³¨å†Œæ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¼ ç»ŸåŒ»å­¦å›¾åƒæ³¨å†Œæ˜¯é€šè¿‡ä¼˜åŒ–é—®é¢˜æ¥è§£å†³çš„ï¼Œè€Œæœ€è¿‘çš„å­¦ä¹ å‹æ–¹æ³•æ˜¯åŸºäºé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰æ¨¡æ‹Ÿè¿™ä¸€è¿‡ç¨‹çš„ã€‚</li>
<li>å­¦ä¹ å‹å±•å¼€æ–¹æ³•ç¼ºä¹ç†è®ºæ”¶æ•›ä¿è¯ï¼Œå¹¶åœ¨å®è·µä¸­æ˜¾ç¤ºå‡ºä¸ç¨³å®šæ€§å’ŒGPUå†…å­˜ä½¿ç”¨éšç€å±•å¼€æ­¥éª¤çš„å¢åŠ è€Œçº¿æ€§å¢é•¿çš„ç“¶é¢ˆã€‚</li>
<li>DEQRegæ˜¯ä¸€ç§æ–°å‹çš„åŸºäºæ·±åº¦å‡è¡¡æ¨¡å‹ï¼ˆDEQï¼‰çš„æ³¨å†Œæ¡†æ¶ï¼Œè§£å†³äº†ä¸Šè¿°ç†è®ºä¸Šçš„æŒ‘æˆ˜å¹¶ç»´æŒäº†æ’å®šçš„å†…å­˜ä½¿ç”¨ã€‚</li>
<li>DEQRegå®ç°äº†ç†è®ºä¸Šæ— é™çš„è¿­ä»£æ­¥éª¤ï¼Œå®ç°äº†ç¨³å®šçš„æ”¶æ•›ã€‚</li>
<li>åœ¨å…¬å…±è„‘MRIå’Œè‚ºéƒ¨CTæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºDEQRegå…·æœ‰ç«äº‰åŠ›çš„æ³¨å†Œæ€§èƒ½å¹¶æ˜¾è‘—å‡å°‘äº†ä¸æœ€æ–°å±•å¼€æ–¹æ³•çš„å†…å­˜æ¶ˆè€—ã€‚</li>
<li>ç°æœ‰å±•å¼€æ–¹æ³•çš„æ€§èƒ½åœ¨æ¨ç†æ­¥éª¤è¶…å‡ºè®­ç»ƒé…ç½®æ—¶ä¼šå‘ç”Ÿä¸å¯é€†çš„ä¸‹é™ï¼Œè€ŒDEQRegé€šè¿‡å…¶å‡è¡¡å¯»æ±‚æœºåˆ¶é¿å…äº†è¿™ä¸€é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0887a36a42388235cd5f7ae52d3924b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0e7bb0286ecc7dd5ce68ddff8dca304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ed80a53b1c8445273e77bc9c107fcd9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Generation-of-Indoor-Open-Street-Maps-for-Robot-Navigation-from-CAD-Files"><a href="#Generation-of-Indoor-Open-Street-Maps-for-Robot-Navigation-from-CAD-Files" class="headerlink" title="Generation of Indoor Open Street Maps for Robot Navigation from CAD   Files"></a>Generation of Indoor Open Street Maps for Robot Navigation from CAD   Files</h2><p><strong>Authors:Jiajie Zhang, Shenrui Wu, Xu Ma, SÃ¶ren Schwertfeger</strong></p>
<p>The deployment of autonomous mobile robots is predicated on the availability of environmental maps, yet conventional generation via SLAM (Simultaneous Localization and Mapping) suffers from significant limitations in time, labor, and robustness, particularly in dynamic, large-scale indoor environments where map obsolescence can lead to critical localization failures. To address these challenges, this paper presents a complete and automated system for converting architectural Computer-Aided Design (CAD) files into a hierarchical topometric OpenStreetMap (OSM) representation, tailored for robust life-long robot navigation. Our core methodology involves a multi-stage pipeline that first isolates key structural layers from the raw CAD data and then employs an AreaGraph-based topological segmentation to partition the building layout into a hierarchical graph of navigable spaces. This process yields a comprehensive and semantically rich map, further enhanced by automatically associating textual labels from the CAD source and cohesively merging multiple building floors into a unified, topologically-correct model. By leveraging the permanent structural information inherent in CAD files, our system circumvents the inefficiencies and fragility of SLAM, offering a practical and scalable solution for deploying robots in complex indoor spaces. The software is encapsulated within an intuitive Graphical User Interface (GUI) to facilitate practical use. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/jiajiezhang7/osmAG-from-cad">https://github.com/jiajiezhang7/osmAG-from-cad</a>. </p>
<blockquote>
<p>è‡ªä¸»ç§»åŠ¨æœºå™¨äººçš„éƒ¨ç½²ä¾èµ–äºç¯å¢ƒåœ°å›¾çš„å¯ç”¨æ€§ï¼Œç„¶è€Œï¼Œä¼ ç»Ÿçš„é€šè¿‡SLAMï¼ˆåŒæ—¶å®šä½ä¸åœ°å›¾æ„å»ºï¼‰ç”Ÿæˆåœ°å›¾çš„æ–¹æ³•åœ¨æ—¶é—´ã€åŠ³åŠ¨åŠ›å’Œç¨³å¥æ€§æ–¹é¢å­˜åœ¨é‡å¤§å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€ã€å¤§è§„æ¨¡å®¤å†…ç¯å¢ƒä¸­ï¼Œåœ°å›¾å¤±æ•ˆå¯èƒ½å¯¼è‡´å…³é”®å®šä½å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å®Œæ•´ã€è‡ªåŠ¨åŒ–çš„ç³»ç»Ÿï¼Œå°†å»ºç­‘è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ–‡ä»¶è½¬æ¢ä¸ºåˆ†å±‚çš„æ‹“æ‰‘OpenStreetMapï¼ˆOSMï¼‰è¡¨ç¤ºå½¢å¼ï¼Œé€‚ç”¨äºç¨³å¥çš„ç»ˆèº«æœºå™¨äººå¯¼èˆªã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ–¹æ³•æ¶‰åŠä¸€ä¸ªå¤šé˜¶æ®µç®¡é“ï¼Œé¦–å…ˆä»åŸå§‹CADæ•°æ®ä¸­éš”ç¦»å‡ºå…³é”®ç»“æ„å±‚ï¼Œç„¶åé‡‡ç”¨åŸºäºAreaGraphçš„æ‹“æ‰‘åˆ†å‰²å°†å»ºç­‘å¸ƒå±€åˆ†å‰²æˆå¯å¯¼èˆªç©ºé—´çš„å±‚æ¬¡å›¾ã€‚è¿™ä¸€è¿‡ç¨‹äº§ç”Ÿäº†å…¨é¢ä¸”è¯­ä¹‰ä¸°å¯Œçš„åœ°å›¾ï¼Œé€šè¿‡è‡ªåŠ¨å…³è”æ¥è‡ªCADæºçš„æ–‡æœ¬æ ‡ç­¾ï¼Œå¹¶å°†å¤šä¸ªæ¥¼å±‚åˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€ã€æ‹“æ‰‘æ­£ç¡®çš„æ¨¡å‹ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†åœ°å›¾çš„åŠŸèƒ½ã€‚é€šè¿‡åˆ©ç”¨CADæ–‡ä»¶ä¸­å›ºæœ‰çš„æ°¸ä¹…ç»“æ„ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿé¿å…äº†SLAMçš„ä¸æ•ˆç‡å’Œè„†å¼±æ€§ï¼Œä¸ºåœ¨å¤æ‚å®¤å†…ç©ºé—´éƒ¨ç½²æœºå™¨äººæä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥è½¯ä»¶è¢«å°è£…åœ¨ä¸€ä¸ªç›´è§‚çš„ç”¨æˆ·å›¾å½¢ç•Œé¢ï¼ˆGUIï¼‰ä¸­ï¼Œä»¥ä¾¿äºå®é™…åº”ç”¨ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jiajiezhang7/osmAG-from-cad%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jiajiezhang7/osmAG-from-cadæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00552v1">PDF</a> 8 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªä¸»ç§»åŠ¨æœºå™¨äººçš„éƒ¨ç½²å¯¹åœ°å›¾ä¿¡æ¯çš„ä¾èµ–ä»¥åŠä¼ ç»Ÿçš„SLAMåœ°å›¾ç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†åŠ¨æ€ã€å¤§è§„æ¨¡å®¤å†…ç¯å¢ƒæ—¶çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”±å»ºç­‘è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ–‡ä»¶è‡ªåŠ¨åŒ–è½¬æ¢ä¸ºå±‚æ¬¡åŒ–çš„OpenStreetMapï¼ˆOSMï¼‰åœ°å›¾è¡¨ç¤ºçš„ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¤šé˜¶æ®µç®¡é“å¤„ç†ï¼Œä»CADæ•°æ®ä¸­éš”ç¦»å…³é”®ç»“æ„å±‚ï¼Œå¹¶åˆ©ç”¨AreaGraphè¿›è¡Œæ‹“æ‰‘åˆ†å‰²ï¼Œä¸ºæœºå™¨äººå¯¼èˆªç”Ÿæˆä¸°å¯Œçš„è¯­ä¹‰åœ°å›¾ã€‚å€ŸåŠ©CADæ–‡ä»¶ä¸­çš„æ°¸ä¹…ç»“æ„ä¿¡æ¯ï¼Œè¯¥ç³»ç»Ÿå…‹æœäº†SLAMæ–¹æ³•çš„ä¸æ•ˆç‡å’Œè„†å¼±æ€§ï¼Œä¸ºå¤æ‚å®¤å†…ç©ºé—´çš„æœºå™¨äººéƒ¨ç½²æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»ç§»åŠ¨æœºå™¨äººçš„éƒ¨ç½²ä¾èµ–äºç¯å¢ƒåœ°å›¾çš„å¯ç”¨æ€§ã€‚</li>
<li>ä¼ ç»ŸSLAMåœ°å›¾ç”Ÿæˆæ–¹æ³•åœ¨åŠ¨æ€ã€å¤§è§„æ¨¡å®¤å†…ç¯å¢ƒä¸­æœ‰æ—¶é—´ã€åŠ³åŠ¨åŠ›å’Œç¨³å¥æ€§çš„é™åˆ¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”±å»ºç­‘CADæ–‡ä»¶è‡ªåŠ¨åŒ–è½¬æ¢ä¸ºOSMåœ°å›¾çš„ç³»ç»Ÿï¼Œç”¨äºæœºå™¨äººå¯¼èˆªã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡å¤šé˜¶æ®µç®¡é“å¤„ç†ï¼ŒåŒ…æ‹¬éš”ç¦»å…³é”®ç»“æ„å±‚å’Œåˆ©ç”¨AreaGraphè¿›è¡Œæ‹“æ‰‘åˆ†å‰²ã€‚</li>
<li>ç³»ç»Ÿèƒ½ç”Ÿæˆä¸°å¯Œçš„è¯­ä¹‰åœ°å›¾ï¼Œå¹¶è‡ªåŠ¨å…³è”CADæºä¸­çš„æ–‡æœ¬æ ‡ç­¾ã€‚</li>
<li>åˆ©ç”¨CADæ–‡ä»¶ä¸­çš„æ°¸ä¹…ç»“æ„ä¿¡æ¯ï¼Œç³»ç»Ÿå…‹æœäº†SLAMçš„ä¸æ•ˆç‡å’Œè„†å¼±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a45dce7034e62ee0a2a9e9a26962ed38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adcd26b8522dfdb2a4c469cf80ec1b31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67c294ff38ac82dd6749b7a9094059d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9322e68bad3bf83cab8fe208435be3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcb82dfde7c8932bb269bb5e7f22b9b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0762e59e8f69db7307a5172aa31f889.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1a2bd89874b34b963e0d7d4174e132a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49e15d0bb3c76d6b9b0893c30646e965.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Medical-Image-Segmentation-Using-Advanced-Unet-VMSE-Unet-and-VM-Unet-CBAM"><a href="#Medical-Image-Segmentation-Using-Advanced-Unet-VMSE-Unet-and-VM-Unet-CBAM" class="headerlink" title="Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet   CBAM+"></a>Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet   CBAM+</h2><p><strong>Authors:Sayandeep Kanrar, Raja Piyush, Qaiser Razi, Debanshi Chakraborty, Vikas Hassija, GSS Chalapathi</strong></p>
<p>In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two cutting-edge deep learning architectures designed to enhance medical image segmentation. Our approach integrates Squeeze-and-Excitation (SE) and Convolutional Block Attention Module (CBAM) techniques into the traditional VM U-Net framework, significantly improving segmentation accuracy, feature localization, and computational efficiency. Both models show superior performance compared to the baseline VM-Unet across multiple datasets. Notably, VMSEUnet achieves the highest accuracy, IoU, precision, and recall while maintaining low loss values. It also exhibits exceptional computational efficiency with faster inference times and lower memory usage on both GPU and CPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a valuable tool for medical image analysis. These findings highlight its potential for real-world clinical applications, emphasizing the importance of further research to optimize accuracy, robustness, and computational efficiency. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†VMSE U-Netå’ŒVM-Unet CBAM+æ¨¡å‹ï¼Œè¿™ä¸¤ç§å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¶æ„æ—¨åœ¨å¢å¼ºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å‹ç¼©å’Œæ¿€åŠ±ï¼ˆSEï¼‰å’Œå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰æŠ€æœ¯é›†æˆåˆ°ä¼ ç»Ÿçš„VM U-Netæ¡†æ¶ä¸­ï¼Œæ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦ã€ç‰¹å¾å®šä½å’Œè®¡ç®—æ•ˆç‡ã€‚ä¸åŸºçº¿VM-Unetç›¸æ¯”ï¼Œä¸¤ä¸ªæ¨¡å‹åœ¨å¤šæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVMSEUnetåœ¨ä¿æŒä½æŸå¤±å€¼çš„åŒæ—¶ï¼Œå®ç°äº†æœ€é«˜çš„å‡†ç¡®åº¦ã€IoUã€ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨GPUå’ŒCPUä¸Šéƒ½è¡¨ç°å‡ºå‡ºè‰²çš„è®¡ç®—æ•ˆç‡ï¼Œæ¨ç†æ—¶é—´æ›´å¿«ï¼Œå†…å­˜ä½¿ç”¨æ›´ä½ã€‚æ€»ä½“è€Œè¨€ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¢å¼ºçš„VMSE-Unetæ¶æ„æ˜¯åŒ»å­¦å›¾åƒåˆ†æçš„æœ‰ä»·å€¼çš„å·¥å…·ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å…¶åœ¨ç°å®ä¸–ç•Œä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†è¿›ä¸€æ­¥ä¼˜åŒ–å‡†ç¡®åº¦ã€ç¨³å¥æ€§å’Œè®¡ç®—æ•ˆç‡çš„ç ”ç©¶çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00511v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VMSE U-Netå’ŒVM-Unet CBAM+ä¸¤ä¸ªå…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå®ƒä»¬è¢«è®¾è®¡ç”¨äºæé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚é€šè¿‡é›†æˆSqueeze-and-Excitationï¼ˆSEï¼‰å’Œå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰æŠ€æœ¯åˆ°ä¼ ç»Ÿçš„VM U-Netæ¡†æ¶ä¸­ï¼Œæ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦ã€ç‰¹å¾å®šä½èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚å…¶ä¸­ï¼ŒVMSE U-Netåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºå‡†VM-Unetæ¨¡å‹ï¼Œå®ç°äº†æœ€é«˜çš„å‡†ç¡®æ€§ã€IoUã€ç²¾ç¡®åº¦å’Œå¬å›ç‡ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„æŸå¤±å€¼ã€‚å®ƒè¿˜åœ¨GPUå’ŒCPUä¸Šè¡¨ç°å‡ºå“è¶Šçš„è®¡ç®—æ•ˆç‡ï¼Œå…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ç‡ã€‚æ€»ä½“è€Œè¨€ï¼Œä¼˜åŒ–åçš„VMSE U-Netæ¶æ„å¯¹äºåŒ»å­¦å›¾åƒåˆ†æå…·æœ‰é‡è¦ä»·å€¼ï¼Œå¹¶æœ‰æœ›åœ¨çœŸå®ä¸´åºŠç¯å¢ƒä¸­å¾—åˆ°åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VMSE U-Netå’ŒVM-Unet CBAM+æ˜¯ä¸“ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚</li>
<li>è¿™äº›æ¨¡å‹é€šè¿‡é›†æˆSEå’ŒCBAMæŠ€æœ¯ï¼Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€ç‰¹å¾å®šä½å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>VMSE U-Netåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†VM-Unetæ¨¡å‹ã€‚</li>
<li>VMSE U-Netå®ç°äº†é«˜å‡†ç¡®æ€§ã€IoUã€ç²¾ç¡®åº¦å’Œå¬å›ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½æŸå¤±å€¼ã€‚</li>
<li>VMSE U-Netåœ¨GPUå’ŒCPUä¸Šå±•ç°å‡ºå“è¶Šçš„è®¡ç®—æ•ˆç‡ï¼Œå…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ã€‚</li>
<li>æ•´ä½“è€Œè¨€ï¼ŒVMSE U-Netæ¶æ„å¯¹äºåŒ»å­¦å›¾åƒåˆ†æå…·æœ‰é‡è¦ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01d1322911218834f774854cabb4d299.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-084968aa2c56e186772b8e72690c106c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb2ae97f6eecf0cc70214a69e8a720f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-279aaa8202d90a7385d80cf25005b568.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39ee2e730dd0afc94ae3de3a3063c8b5.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-06/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b391931e453b7ba19cf4412331be5b26.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-06  JoyTTS LLM-based Spoken Chatbot With Voice Cloning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ab458c94402aada830bcbeebf033d843.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-06  MAD Makeup All-in-One with Cross-Domain Diffusion Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
