<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-06  Large Reasoning Models are not thinking straight on the unreliability   of thinking trajectories">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c5bd301c013f4c6653fb04aefd7c77ab.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-06-æ›´æ–°"><a href="#2025-07-06-æ›´æ–°" class="headerlink" title="2025-07-06 æ›´æ–°"></a>2025-07-06 æ›´æ–°</h1><h2 id="Large-Reasoning-Models-are-not-thinking-straight-on-the-unreliability-of-thinking-trajectories"><a href="#Large-Reasoning-Models-are-not-thinking-straight-on-the-unreliability-of-thinking-trajectories" class="headerlink" title="Large Reasoning Models are not thinking straight: on the unreliability   of thinking trajectories"></a>Large Reasoning Models are not thinking straight: on the unreliability   of thinking trajectories</h2><p><strong>Authors:Jhouben Cuesta-Ramirez, Samuel Beaussant, Mehdi Mounsif</strong></p>
<p>Large Language Models (LLMs) trained via Reinforcement Learning (RL) have recently achieved impressive results on reasoning benchmarks. Yet, growing evidence shows that these models often generate longer but ineffective chains of thought (CoTs), calling into question whether benchmark gains reflect real reasoning improvements. We present new evidence of overthinking, where models disregard correct solutions even when explicitly provided, instead continuing to generate unnecessary reasoning steps that often lead to incorrect conclusions. Experiments on three state-of-the-art models using the AIME2024 math benchmark reveal critical limitations in these models ability to integrate corrective information, posing new challenges for achieving robust and interpretable reasoning. </p>
<blockquote>
<p>é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œè¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¼šäº§ç”Ÿæ›´é•¿ä½†æ— æ•ˆçš„æ¨ç†é“¾ï¼ˆCoTsï¼‰ï¼Œäººä»¬è´¨ç–‘åŸºå‡†æµ‹è¯•æˆç»©çš„æé«˜æ˜¯å¦åæ˜ äº†çœŸå®æ¨ç†èƒ½åŠ›çš„æ”¹è¿›ã€‚æˆ‘ä»¬æä¾›äº†æ–°çš„è¿‡åº¦æ€è€ƒçš„è¯æ®ï¼Œå³æ¨¡å‹ä¼šå¿½è§†æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå³ä½¿æ˜ç¡®æä¾›ï¼Œä¹Ÿä¼šç»§ç»­ç”Ÿæˆä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ï¼Œé€šå¸¸å¯¼è‡´é”™è¯¯çš„ç»“è®ºã€‚åœ¨AIME2024æ•°å­¦åŸºå‡†ä¸Šå¯¹ä¸‰ç§å…ˆè¿›æ¨¡å‹è¿›è¡Œçš„å®éªŒæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨æ•´åˆçº æ­£ä¿¡æ¯æ–¹é¢çš„å…³é”®å±€é™ï¼Œä¸ºå®ç°ç¨³å¥å’Œå¯è§£é‡Šçš„æ¨ç†æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00711v1">PDF</a> Accepted to KONVENS 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œè¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€äº§ç”Ÿå†—é•¿è€Œæ— ç”¨çš„æ¨ç†é“¾ï¼Œä½¿å¾—åŸºå‡†æµ‹è¯•æˆç»©çš„æé«˜å¹¶ä¸èƒ½çœŸæ­£åæ˜ æ¨ç†èƒ½åŠ›çš„æé«˜ã€‚æ–°çš„è¯æ®è¡¨æ˜ï¼Œæ¨¡å‹ä¼šè¿‡åº¦æ€è€ƒï¼Œå¿½è§†æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œç»§ç»­ç”Ÿæˆä¸å¿…è¦çš„æ¨ç†æ­¥éª¤ï¼Œå¯¼è‡´é”™è¯¯çš„ç»“è®ºã€‚å¯¹ä½¿ç”¨AIME2024æ•°å­¦åŸºå‡†çš„ä¸‰ä¸ªå…ˆè¿›æ¨¡å‹è¿›è¡Œçš„å®éªŒæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨æ•´åˆçº æ­£ä¿¡æ¯æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œä¸ºå®ç°ç¨³å¥å’Œå¯è§£é‡Šçš„æ¨ç†æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¿™ç±»æ¨¡å‹æœ‰æ—¶ç”Ÿæˆå†—é•¿ä¸”æ— æ•ˆçš„æ¨ç†é“¾ã€‚</li>
<li>æ¨¡å‹å­˜åœ¨è¿‡åº¦æ€è€ƒç°è±¡ï¼Œä¼šå¿½è§†æ­£ç¡®è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•´åˆçº æ­£ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ­¥éª¤å¸¸å¯¼è‡´é”™è¯¯ç»“è®ºã€‚</li>
<li>è¿™äº›å…³é”®é—®é¢˜ä¸ºå®ç°ç¨³å¥å’Œå¯è§£é‡Šçš„æ¨ç†å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e7366864e3496aa30656219ef9aae7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dea4b43d1870621149beadbea0b8267c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da4957a589228b2c13a3105b719b976e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8800beedeac6cfe8a7c778391c8e8b07.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SAFER-Probing-Safety-in-Reward-Models-with-Sparse-Autoencoder"><a href="#SAFER-Probing-Safety-in-Reward-Models-with-Sparse-Autoencoder" class="headerlink" title="SAFER: Probing Safety in Reward Models with Sparse Autoencoder"></a>SAFER: Probing Safety in Reward Models with Sparse Autoencoder</h2><p><strong>Authors:Sihang Li, Wei Shi, Ziyuan Xie, Tao Liang, Guojun Ma, Xiang Wang</strong></p>
<p>Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/xzy-101/SAFER-code">https://github.com/xzy-101/SAFER-code</a>. \textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.} </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å…³é”®èŒƒå¼ï¼Œä½†å…¶æ ¸å¿ƒçš„å¥–åŠ±æ¨¡å‹ä»ç„¶å¤§å¤šä¸é€æ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSAFERï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æœºæ¢°åˆ†ææ¥è§£é‡Šå’Œæ”¹è¿›å¥–åŠ±æ¨¡å‹çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œæ­ç¤ºäº†å¥–åŠ±æ¨¡å‹æ¿€æ´»ä¸­çš„äººå¯è§£é‡Šç‰¹å¾ï¼Œä»è€Œæ·±å…¥äº†è§£ä¸å®‰å…¨ç›¸å…³çš„å†³ç­–ã€‚æˆ‘ä»¬å°†SAFERåº”ç”¨äºé¢å‘å®‰å…¨çš„åå¥½æ•°æ®é›†ï¼Œå¹¶é€šè¿‡é€‰æ‹©å“åº”å’Œæ‹’ç»å“åº”ä¹‹é—´çš„æ¿€æ´»å·®å¼‚æ¥é‡åŒ–å•ä¸ªç‰¹å¾çš„é‡è¦æ€§ã€‚ä½¿ç”¨è¿™äº›ç‰¹å¾çº§ä¿¡å·ï¼Œæˆ‘ä»¬è®¾è®¡äº†æœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ±¡æŸ“å’Œå»å™ªç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒSAFERå¯ä»¥åœ¨ä¸ç‰ºç‰²ä¸€èˆ¬èŠå¤©æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æœ€å°çš„æ•°æ®ä¿®æ”¹ç²¾ç¡®åœ°é™ä½æˆ–æé«˜å®‰å…¨å¯¹é½æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºè§£é‡Šã€å®¡è®¡å’Œå®Œå–„é«˜é£é™©è¯­è¨€æ¨¡å‹å¯¹é½ä»»åŠ¡ä¸­çš„å¥–åŠ±æ¨¡å‹åšå‡ºäº†è´¡çŒ®ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xzy-101/SAFER-code%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82%E6%9C%AC%E6%9E%9C%E8%AE%A8%E8%AE%BA%E4%BA%86%E4%B8%8E%E5%A4%9A%E5%BC%BA%E8%AF%AD%E8%A8%80%E6%A8%A%E6%A8%A1%E5%85%B7odel%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3%E7%9A%84%E8%AF%9D%E9%A2%98%EF%BC%8C%E5%B9%B6%E5%8F%AF%E8%83%BD%E5%8C%85%E5%90%AB%E7%AA%81%E5%87%BA%E6%BD%9C%E5%9C%A8%E9%A3%8E%E9%99%A9%E6%88%96%E4%B8%8D%E5%BD%93%E8%A1%8C%E4%B8%BA%E7%9A%84%E8%AE%A8%E8%AE%BA%E6%88%96%E7%A4%BA%E4%BE%8B%E3%80%82">https://github.com/xzy-101/SAFER-codeä¸Šæ‰¾åˆ°ã€‚æœ¬æ–‡è®¨è®ºäº†ä¸å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨ç›¸å…³çš„è¯é¢˜ï¼Œå¹¶å¯èƒ½åŒ…å«çªå‡ºæ½œåœ¨é£é™©æˆ–ä¸å®‰å…¨ç»“æœçš„è®¨è®ºæˆ–ç¤ºä¾‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00665v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆçš„ç»“åˆæ˜¯å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„å…³é”®æ–¹æ³•ï¼Œä½†å…¶æ ¸å¿ƒçš„å¥–åŠ±æ¨¡å‹ä»ç›¸å¯¹æ¨¡ç³Šã€‚æœ¬ç ”ç©¶æå‡ºåä¸ºâ€œSAFERâ€çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè§£è¯»å’Œæ”¹è¿›å¥–åŠ±æ¨¡å‹ã€‚åˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencodersï¼‰ï¼Œæˆ‘ä»¬ä»å¥–åŠ±æ¨¡å‹æ¿€æ´»ä¸­å‘ç°äººç±»å¯è§£é‡Šçš„ç‰¹å¾ï¼Œä»è€Œæ´å¯Ÿå®‰å…¨ç›¸å…³çš„å†³ç­–è¿‡ç¨‹ã€‚å¯¹é¢å‘å®‰å…¨æ€§çš„åå¥½æ•°æ®é›†çš„åº”ç”¨å’Œå®éªŒä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ¿€æ´»æ‰€é€‰å’Œæ‹’ç»å“åº”ä¹‹é—´çš„å·®å¼‚é‡åŒ–ä¸ªä½“ç‰¹å¾çš„é‡è¦æ€§ã€‚æ ¹æ®è¿™äº›ç‰¹å¾çº§ä¿¡å·ï¼Œæˆ‘ä»¬è®¾è®¡äº†æœ‰é’ˆå¯¹æ€§çš„æ•°æ®ä¸­æ¯’å’Œå»å™ªç­–ç•¥ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSAFERèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²é€šç”¨èŠå¤©æ€§èƒ½çš„å‰æä¸‹ï¼Œç²¾ç¡®è°ƒæ•´å®‰å…¨å¯¹é½åº¦æˆ–å¢å¼ºå®‰å…¨å¯¹é½åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºè§£è¯»ã€å®¡è®¡å’Œæ”¹è¿›é«˜é£é™©å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½ä»»åŠ¡ä¸­çš„å¥–åŠ±æ¨¡å‹åšå‡ºäº†è´¡çŒ®ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/xzy-101/SAFER-code">é“¾æ¥</a>è·å–ã€‚è¯¥è®ºæ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨ç›¸å…³é—®é¢˜ï¼Œå¹¶å¯èƒ½åŒ…å«çªå‡ºæ½œåœ¨é£é™©æˆ–ä¸å®‰å…¨ç»“æœçš„è®¨è®ºæˆ–ç¤ºä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚å¯¹é½çš„å…³é”®æ–¹æ³•ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹æ˜¯RLHFçš„æ ¸å¿ƒï¼Œä½†ç›¸å¯¹æ¨¡ç³Šä¸”ç¼ºä¹é€æ˜åº¦ã€‚</li>
<li>â€œSAFERâ€æ¡†æ¶ç”¨äºè§£è¯»å’Œæ”¹è¿›å¥–åŠ±æ¨¡å‹ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨å‘ç°äººç±»å¯è§£é‡Šçš„ç‰¹å¾ã€‚</li>
<li>é€šè¿‡æ´å¯Ÿå¥–åŠ±æ¨¡å‹çš„æ¿€æ´»çŠ¶æ€ï¼Œå¯ä»¥äº†è§£å®‰å…¨ç›¸å…³çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>åº”ç”¨é¢å‘å®‰å…¨æ€§çš„åå¥½æ•°æ®é›†ï¼Œé‡åŒ–ä¸ªä½“ç‰¹å¾åœ¨å¥–åŠ±æ¨¡å‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>åŸºäºç‰¹å¾çº§ä¿¡å·è®¾è®¡æ•°æ®ä¸­æ¯’å’Œå»å™ªç­–ç•¥ï¼Œç²¾ç¡®è°ƒæ•´å®‰å…¨å¯¹é½åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9e71caa23ff1f27045cbadc7c151653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5bd301c013f4c6653fb04aefd7c77ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a78d2c2754a5ea35ce71a168a51ef874.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c94edd6cbf348159d85baa5d203abf13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af121c77dee697de4177d3a651f9a705.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf416b60c182ae0d8a00df77fa1172f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a56807defaedc8e037bce00e137b482a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning"><a href="#MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning" class="headerlink" title="MiCo: Multi-image Contrast for Reinforcement Visual Reasoning"></a>MiCo: Multi-image Contrast for Reinforcement Visual Reasoning</h2><p><strong>Authors:Xi Chen, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu, Yu Liu, Xiang Bai, Hengshuang Zhao</strong></p>
<p>This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å®ç°Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†ï¼Œä»¥åœ¨å¤šä¸ªå›¾åƒä¹‹é—´å»ºç«‹è§†è§‰çº¿ç´¢è”ç³»ã€‚ä¸€ç§ç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€‚åº”åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ã€‚ç„¶è€Œï¼Œæ­¤ç±»æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨æ•´ç†çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œåœ¨å¤„ç†ç²¾ç»†çš„è§†è§‰ç»†èŠ‚å’Œå›¾åƒä¹‹é—´çš„å¤æ‚é€»è¾‘æ—¶ï¼Œå¯èƒ½ä¼šé¢ä¸´ç‰¹åˆ«æŒ‘æˆ˜ã€‚å—è‡ªæˆ‘ç›‘ç£çš„è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„å¯å‘ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å›¾åƒåŒ…å«å¯ä»¥ä½œä¸ºç›‘ç£çš„å›ºæœ‰çº¦æŸã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æ„å»ºäº†å›¾åƒä¸‰å…ƒç»„ï¼ŒåŒ…æ‹¬åŒä¸€å›¾åƒçš„ä¸¤ä¸ªå¢å¼ºè§†å›¾å’Œç¬¬ä¸‰å¼ ç›¸ä¼¼ä½†ä¸åŒçš„å›¾åƒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹è¢«æç¤ºç”Ÿæˆä¸€ä¸ªæ¨ç†è¿‡ç¨‹æ¥æ¯”è¾ƒè¿™äº›å›¾åƒï¼ˆå³ç¡®å®šç›¸åŒæˆ–ä¸åŒï¼‰ã€‚ç„¶åæˆ‘ä»¬ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ã€‚ç”±äºé«˜åº¦è§†è§‰ç›¸ä¼¼æ€§å’Œå¢å¼ºæŠ€æœ¯çš„å­˜åœ¨ï¼Œæ¨¡å‹å¿…é¡»å…³æ³¨ç»†å¾®çš„è§†è§‰å˜åŒ–ï¼Œå¹¶è¿›è¡Œé€»è¾‘æ¨ç†æ‰èƒ½æˆåŠŸã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ä»…é€šè¿‡è§†è§‰æ¯”è¾ƒä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œä½†æ‰€å­¦çš„æ¨ç†èƒ½åŠ›å¯¹ä¸€ç³»åˆ—é—®é¢˜æœ‰æ•ˆåœ°æ³›åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–ä»»ä½•äººå·¥æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œåœ¨è·¨å›¾åƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22434v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†é€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†é“¾æ¥è·¨å¤šä¸ªå›¾åƒè§†è§‰çº¿ç´¢çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªæˆ‘ç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„ç­–ç•¥ï¼Œåˆ©ç”¨å›¾åƒä¸­çš„å›ºæœ‰çº¦æŸä½œä¸ºç›‘ç£ä¿¡æ¯ã€‚é€šè¿‡æ„å»ºåŒ…å«ä¸¤ä¸ªç›¸åŒå›¾åƒçš„å¢å¼ºè§†å›¾å’Œä¸€ä¸ªç›¸ä¼¼ä½†ä¸åŒçš„å›¾åƒçš„ä¸‰å…ƒç»„ï¼Œè®­ç»ƒæ¨¡å‹å¯¹å›¾åƒè¿›è¡Œæ¯”è¾ƒå¹¶ç”Ÿæˆæ¨ç†è¿‡ç¨‹ã€‚æœ€ç»ˆä¼˜åŒ–æ¨¡å‹é‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ã€‚æ¨¡å‹åœ¨è§†è§‰ç›¸ä¼¼æ€§é«˜å’Œå­˜åœ¨å¢å¼ºçš„æ¡ä»¶ä¸‹ï¼Œå¿…é¡»å…³æ³¨ç»†å¾®çš„è§†è§‰å˜åŒ–å¹¶è¿›è¡Œé€»è¾‘æ¨ç†æ‰èƒ½æˆåŠŸã€‚å®éªŒè¡¨æ˜ï¼Œè™½ç„¶æ¨¡å‹ä»…ç»è¿‡è§†è§‰æ¯”è¾ƒä»»åŠ¡çš„è®­ç»ƒï¼Œä½†å…¶å­¦ä¹ èƒ½åŠ›èƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºå„ç§é—®é¢˜ã€‚å¹¶ä¸”ï¼Œè¯¥ç ”ç©¶çš„æ–¹æ³•ä¸ä¾èµ–ä»»ä½•äººå·¥æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œåœ¨è·¨å›¾åƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä½œæ¢ç´¢äº†Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†åœ¨è·¨å¤šä¸ªå›¾åƒè§†è§‰çº¿ç´¢ä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè‡ªæˆ‘ç›‘ç£è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„ç­–ç•¥ï¼Œåˆ©ç”¨å›¾åƒä¸­çš„å›ºæœ‰çº¦æŸä½œä¸ºç›‘ç£ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡æ„å»ºå›¾åƒä¸‰å…ƒç»„æ¥è®­ç»ƒæ¨¡å‹è¿›è¡Œå›¾åƒæ¯”è¾ƒå’Œæ¨ç†ã€‚</li>
<li>é‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å¿…é¡»å…³æ³¨ç»†å¾®çš„è§†è§‰å˜åŒ–å¹¶è¿›è¡Œé€»è¾‘æ¨ç†ï¼Œä»¥åº”å¯¹é«˜åº¦è§†è§‰ç›¸ä¼¼æ€§å’Œå­˜åœ¨çš„å¢å¼ºæ¡ä»¶ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹åœ¨è·¨å›¾åƒæ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c393203fc7c0a94f5bd878d651ebb1c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-018e0fde7deb4a78e29f0945d5245a5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b03185cace3b2a9390b8b3b6e30f434.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8db4750d9664b50155d0321261db691.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="EFRame-Deeper-Reasoning-via-Exploration-Filter-Replay-Reinforcement-Learning-Framework"><a href="#EFRame-Deeper-Reasoning-via-Exploration-Filter-Replay-Reinforcement-Learning-Framework" class="headerlink" title="EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement   Learning Framework"></a>EFRame: Deeper Reasoning via Exploration-Filter-Replay Reinforcement   Learning Framework</h2><p><strong>Authors:Chen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yue Wang, Yuzhi Zhang</strong></p>
<p>Recent advances in reinforcement learning (RL) have significantly enhanced the reasoning capabilities of large language models (LLMs). Group Relative Policy Optimization (GRPO), an efficient variant of PPO that lowers RLâ€™s computational cost, still faces limited exploration, low sample efficiency and instability, constraining its performance on complex reasoning tasks. To address these limitations, we introduce EFRame, an Exploration-Filtering-Replay framework that systematically augments GRPO along three critical dimensions. EFRame performs additional rollouts to explore high-quality trajectories, applies online filtering to eliminate low-quality samples that introduce noise and variance, and leverages experience replay to repeatedly exploit rare but informative samples. EFRame establishes a complete and stable learning cycle, guiding the model through a structured transition from exploration to convergence. Our experiments across a variety of reasoning benchmarks demonstrate that EFRame not only improves the robustness and efficiency of training, but also enables access to deeper reasoning capabilities that remain unattainable under vanilla GRPO. Furthermore, EFRame enables a more fine-grained categorization of training samples, allowing for a deeper analysis of how different types of samples contribute to the learning process in RL. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/597358816/EFRame">https://github.com/597358816/EFRame</a>. </p>
<blockquote>
<p>éšç€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ€æ–°è¿›å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ˜¯PPOçš„ä¸€ç§é«˜æ•ˆå˜ä½“ï¼Œé™ä½äº†RLçš„è®¡ç®—æˆæœ¬ï¼Œä½†ä»é¢ä¸´æ¢ç´¢æœ‰é™ã€æ ·æœ¬æ•ˆç‡è¾ƒä½å’Œä¸ç¨³å®šçš„é—®é¢˜ï¼Œä½¿å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†EFRameï¼Œè¿™æ˜¯ä¸€ä¸ªæ¢ç´¢-è¿‡æ»¤-å›æ”¾æ¡†æ¶ï¼Œç³»ç»Ÿåœ°å¢å¼ºäº†GRPOçš„ä¸‰ä¸ªå…³é”®ç»´åº¦ã€‚EFRameæ‰§è¡Œé¢å¤–çš„è¿­ä»£æ¥æ¢ç´¢é«˜è´¨é‡çš„è½¨è¿¹ï¼Œåº”ç”¨åœ¨çº¿è¿‡æ»¤æ¥æ¶ˆé™¤å¼•å…¥å™ªå£°å’Œæ–¹å·®çš„ä½è´¨é‡æ ·æœ¬ï¼Œå¹¶åˆ©ç”¨ç»éªŒå›æ”¾æ¥åå¤åˆ©ç”¨ç¨€æœ‰ä½†å…·æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ã€‚EFRameå»ºç«‹äº†ä¸€ä¸ªå®Œæ•´ç¨³å®šçš„å­¦ä¹ å‘¨æœŸï¼Œå¼•å¯¼æ¨¡å‹ä»æ¢ç´¢åˆ°æ”¶æ•›è¿›è¡Œç»“æ„åŒ–è½¬æ¢ã€‚æˆ‘ä»¬åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEFRameä¸ä»…æé«˜äº†è®­ç»ƒçš„ç¨³å¥æ€§å’Œæ•ˆç‡ï¼Œè¿˜ä½¿æ¨¡å‹èƒ½å¤Ÿè®¿é—®æ›´æ·±å…¥çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›æ˜¯æ™®é€šGRPOæ— æ³•è¾¾åˆ°çš„ã€‚æ­¤å¤–ï¼ŒEFRameèƒ½å¤Ÿå¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œæ›´ç²¾ç»†çš„åˆ†ç±»ï¼Œä»è€Œæ›´æ·±å…¥åœ°åˆ†æä¸åŒç±»å‹æ ·æœ¬å¯¹RLå­¦ä¹ è¿‡ç¨‹çš„ä¸åŒè´¡çŒ®ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/597358816/EFRame%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/597358816/EFRameæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22200v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿‘æœŸè¿›å±•æå¤§åœ°æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹GRPOï¼ˆä¸€ç§é™ä½RLè®¡ç®—æˆæœ¬çš„PPOå˜ä½“ï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†EFRameæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ¢ç´¢ã€è¿‡æ»¤å’Œå›æ”¾ä¸‰ä¸ªå…³é”®ç»´åº¦æ¥å¢å¼ºGRPOã€‚å®éªŒè¡¨æ˜ï¼ŒEFRameä¸ä»…æé«˜äº†è®­ç»ƒçš„ç¨³å¥æ€§å’Œæ•ˆç‡ï¼Œè¿˜å®ç°äº†æ›´æ·±å…¥çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GRPOé¢ä¸´æ¢ç´¢å±€é™æ€§ã€ä½æ ·æœ¬æ•ˆç‡å’Œä¸ç¨³å®šæ€§çš„é—®é¢˜ã€‚</li>
<li>EFRameæ¡†æ¶é€šè¿‡æ¢ç´¢ã€è¿‡æ»¤å’Œå›æ”¾å¢å¼ºGRPOã€‚</li>
<li>EFRameæé«˜è®­ç»ƒç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</li>
<li>EFRameä½¿æ¨¡å‹å®ç°æ›´æ·±å…¥çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>EFRameä½¿è®­ç»ƒæ ·æœ¬åˆ†ç±»æ›´ç²¾ç»†ï¼Œä¾¿äºåˆ†æä¸åŒç±»å‹æ ·æœ¬å¯¹å­¦ä¹ è¿‡ç¨‹çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-98981727656fc0f28e0b528f0fa266f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20da48605d2c268c0ae14d8df293786e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81639a2048fe9265d82e4fa27324f4f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f30139609dbb662684573dbaea8d27e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LeanConjecturer-Automatic-Generation-of-Mathematical-Conjectures-for-Theorem-Proving"><a href="#LeanConjecturer-Automatic-Generation-of-Mathematical-Conjectures-for-Theorem-Proving" class="headerlink" title="LeanConjecturer: Automatic Generation of Mathematical Conjectures for   Theorem Proving"></a>LeanConjecturer: Automatic Generation of Mathematical Conjectures for   Theorem Proving</h2><p><strong>Authors:Naoto Onda, Kazumi Kasaura, Yuta Oriike, Masaya Taniguchi, Akiyoshi Sannai, Sho Sonoda</strong></p>
<p>We introduce LeanConjecturer, a pipeline for automatically generating university-level mathematical conjectures in Lean 4 using Large Language Models (LLMs). Our hybrid approach combines rule-based context extraction with LLM-based theorem statement generation, addressing the data scarcity challenge in formal theorem proving. Through iterative generation and evaluation, LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with 3,776 identified as syntactically valid and non-trivial, that is, cannot be proven by \texttt{aesop} tactic. We demonstrate the utility of these generated conjectures for reinforcement learning through Group Relative Policy Optimization (GRPO), showing that targeted training on domain-specific conjectures can enhance theorem proving capabilities. Our approach generates 103.25 novel conjectures per seed file on average, providing a scalable solution for creating training data for theorem proving systems. Our system successfully verified several non-trivial theorems in topology, including properties of semi-open, alpha-open, and pre-open sets, demonstrating its potential for mathematical discovery beyond simple variations of existing results. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†LeanConjecturerï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨Large Language Models (LLM)åœ¨Lean 4ä¸­è‡ªåŠ¨ç”Ÿæˆå¤§å­¦çº§åˆ«æ•°å­¦çŒœæƒ³çš„æµæ°´çº¿ã€‚æˆ‘ä»¬çš„æ··åˆæ–¹æ³•ç»“åˆäº†åŸºäºè§„åˆ™ä¸Šä¸‹æ–‡æå–ä¸åŸºäºLLMçš„å®šç†é™ˆè¿°ç”Ÿæˆï¼Œè§£å†³äº†å½¢å¼åŒ–å®šç†è¯æ˜ä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚é€šè¿‡è¿­ä»£ç”Ÿæˆå’Œè¯„ä¼°ï¼ŒLeanConjecturerä»40ä¸ªMathlibç§å­æ–‡ä»¶ä¸­äº§ç”Ÿäº†12,289ä¸ªçŒœæƒ³ï¼Œå…¶ä¸­3,776ä¸ªè¢«è¯†åˆ«ä¸ºè¯­æ³•æœ‰æ•ˆä¸”éå¹³å‡¡ï¼ˆå³æ— æ³•é€šè¿‡aesopç­–ç•¥è¯æ˜ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡Group Relative Policy Optimization (GRPO)å±•ç¤ºäº†è¿™äº›ç”ŸæˆçŒœæƒ³åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„å®ç”¨æ€§ï¼Œè¡¨æ˜é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„çŒœæƒ³è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒå¯ä»¥å¢å¼ºå®šç†è¯æ˜èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¹³å‡æ¯ä¸ªç§å­æ–‡ä»¶ç”Ÿæˆ103.25ä¸ªæ–°çŒœæƒ³ï¼Œä¸ºå®šç†è¯æ˜ç³»ç»Ÿåˆ›å»ºè®­ç»ƒæ•°æ®æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç³»ç»ŸæˆåŠŸéªŒè¯äº†æ‹“æ‰‘ä¸­çš„å‡ ä¸ªéå¹³å‡¡å®šç†ï¼ŒåŒ…æ‹¬åŠå¼€é›†ã€alphaå¼€é›†å’Œé¢„å¼€é›†çš„å±æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•°å­¦å‘ç°ä¸Šçš„æ½œåŠ›ï¼Œè€Œä¸ä»…ä»…æ˜¯ç°æœ‰ç»“æœçš„ç®€å•å˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22005v1">PDF</a> 15 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨ç”Ÿæˆæ•°å­¦çŒœæƒ³ç³»ç»ŸLeanConjecturerçš„ç®€ä»‹ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨Lean 4ç¯å¢ƒä¸­æ„å»ºæ•°å­¦çŒœæƒ³ç”Ÿæˆç®¡é“ã€‚é€šè¿‡ç»“åˆè§„åˆ™åŸºç¡€ä¸Šä¸‹æ–‡æå–å’ŒåŸºäºLLMçš„å®šç†é™ˆè¿°ç”Ÿæˆï¼Œåº”å¯¹å½¢å¼åŒ–å®šç†è¯æ˜ä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚ä»40ä¸ªMathlibç§å­æ–‡ä»¶ä¸­ç”Ÿæˆäº†12,289ä¸ªçŒœæƒ³ï¼Œå…¶ä¸­3,776ä¸ªè¢«è¯†åˆ«ä¸ºè¯­æ³•ä¸Šæœ‰æ•ˆä¸”éå¹³å‡¡ï¼ˆå³ä¸èƒ½ç”¨aesopç­–ç•¥è¯æ˜ï¼‰ã€‚é€šè¿‡ç¾¤ä½“ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰éªŒè¯äº†çŒœæƒ³åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„å®ç”¨æ€§ã€‚ç³»ç»Ÿå¹³å‡æ¯ä¸ªç§å­æ–‡ä»¶ç”Ÿæˆ103.25ä¸ªæ–°çŒœæƒ³ï¼Œä¸ºå®šç†è¯æ˜ç³»ç»Ÿåˆ›å»ºè®­ç»ƒæ•°æ®æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥ç³»ç»ŸæˆåŠŸéªŒè¯äº†æ‹“æ‰‘ä¸­çš„éå¹³å‡¡å®šç†ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•°å­¦å‘ç°æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LeanConjectureræ˜¯ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆå¤§å­¦çº§åˆ«æ•°å­¦çŒœæƒ³çš„ç³»ç»Ÿï¼Œä½¿ç”¨Large Language Models (LLMs)åœ¨Lean 4ç¯å¢ƒä¸­æ„å»ºã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡è§„åˆ™åŸºç¡€ä¸Šä¸‹æ–‡æå–ä¸LLMç»“åˆçš„å®šç†é™ˆè¿°ç”Ÿæˆæ¥åº”å¯¹æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>ä»40ä¸ªMathlibç§å­æ–‡ä»¶ä¸­ç”Ÿæˆäº†12,289ä¸ªçŒœæƒ³ï¼Œå…¶ä¸­éƒ¨åˆ†çŒœæƒ³çš„éªŒè¯å¯¹äºå¼ºåŒ–å­¦ä¹ å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç³»ç»ŸæˆåŠŸéªŒè¯äº†å¤šä¸ªéå¹³å‡¡å®šç†ï¼ŒåŒ…æ‹¬æ‹“æ‰‘å­¦ä¸­åŠå¼€é›†ã€alphaå¼€é›†å’Œé¢„å¼€é›†çš„æ€§è´¨ç­‰ã€‚</li>
<li>é€šè¿‡Group Relative Policy Optimization (GRPO)å±•ç¤ºäº†ç›®æ ‡è®­ç»ƒå¯¹é¢†åŸŸç‰¹å®šçŒœæƒ³çš„å¼ºåŒ–å­¦ä¹ èƒ½åŠ›æå‡ã€‚</li>
<li>ç³»ç»Ÿå¹³å‡æ¯ä¸ªç§å­æ–‡ä»¶ç”Ÿæˆå¤§é‡æ–°çŒœæƒ³ï¼Œä¸ºå®šç†è¯æ˜ç³»ç»Ÿçš„è®­ç»ƒæ•°æ®æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d450b6cba609bf7e2ab3d4fbf18fd345.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-600bf18fa57f21f6b911c7aa6d2f9af9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d4bfc2f6b89c2007fc7d83ab9174752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-320d8b2dc40159cb51332d1cccba6168.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9de9b3765d6b8c2b665612f55359b5a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ec5e77ebbd5be1f3574049a7392e838.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-878a3de3e7620dfc6bcd46c2a35ef37a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SEEA-R1-Tree-Structured-Reinforcement-Fine-Tuning-for-Self-Evolving-Embodied-Agents"><a href="#SEEA-R1-Tree-Structured-Reinforcement-Fine-Tuning-for-Self-Evolving-Embodied-Agents" class="headerlink" title="SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving   Embodied Agents"></a>SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving   Embodied Agents</h2><p><strong>Authors:Wanxin Tian, Shijie Zhang, Kevin Zhang, Xiaowei Chi, Yulin Luo, Junyu Lu, Chunkai Fan, Qiang Zhou, Yiming Zhao, Ning Liu Siyu Lin, Zhiyuan Qin, Xiaozhu Ju, Shanghang Zhang, Jian Tang</strong></p>
<p>Self-evolution, the ability of agents to autonomously improve their reasoning and behavior, is essential for the embodied domain with long-horizon, real-world tasks. Despite current advancements in reinforcement fine-tuning (RFT) showing strong performance in enhancing reasoning in LLMs, its potential to enable self-evolving embodied intelligence with multi-modal interactions remains largely unexplored. Specifically, reinforcement fine-tuning faces two fundamental obstacles in embodied settings: (i) the lack of accessible intermediate rewards in multi-step reasoning tasks limits effective learning signals, and (ii) reliance on hand-crafted reward functions restricts generalization to novel tasks and environments. To address these challenges, we present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework designed for enabling the self-evolving capabilities of embodied agents. Specifically, to convert sparse delayed rewards into denser intermediate signals that improve multi-step reasoning, we propose Tree-based group relative policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into GRPO. To generalize reward estimation across tasks and scenes, supporting autonomous adaptation and reward-driven self-evolution, we further introduce Multi-modal Generative Reward Model (MGRM). To holistically evaluate the effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing state-of-the-art methods with scores of 85.07% (textual) and 36.19% (multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also achieves scores of 80.3% without environmental reward, surpassing all open-source baselines and highlighting its scalability as a self-evolving embodied agent. Additional experiments and qualitative analysis further support the potential of SEEA-R1 for future research in scalable embodied intelligence. </p>
<blockquote>
<p>è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›ï¼Œå³ä»£ç†èƒ½å¤Ÿè‡ªä¸»æé«˜å…¶æ¨ç†å’Œè¡Œä¸ºçš„èƒ½åŠ›ï¼Œå¯¹äºå…·æœ‰é•¿è¿œè§†é‡å’Œç°å®ä¸–ç•Œä»»åŠ¡çš„èº«ä½“åŒ–é¢†åŸŸè‡³å…³é‡è¦ã€‚å°½ç®¡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å·²å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨æ”¯æŒå¤šæ¨¡å¼äº¤äº’çš„è‡ªæˆ‘è¿›åŒ–æ™ºèƒ½æ–¹é¢ä»å­˜åœ¨å¤§é‡æœªå¼€å‘æ½œåŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œå¼ºåŒ–å¾®è°ƒåœ¨é¢å¯¹ç°å®ç¯å¢ƒæ—¶é¢ä¸´ä¸¤ä¸ªæ ¹æœ¬éšœç¢ï¼šé¦–å…ˆï¼Œç¼ºä¹å¯è·å–çš„ä¸­é—´å¥–åŠ±å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡é™åˆ¶äº†æœ‰æ•ˆçš„å­¦ä¹ ä¿¡å·ï¼›å…¶æ¬¡ï¼Œå¯¹äººå·¥å¥–åŠ±å‡½æ•°çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨æ–°ä»»åŠ¡å’Œç¯å¢ƒä¸­æ³›åŒ–çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Self-Evolving Embodied Agents-R1ï¼ˆç®€ç§°SEEA-R1ï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªä¸ºå¢å¼ºä»£ç†çš„è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›è€Œè®¾è®¡çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å°†ç¨€ç–å»¶è¿Ÿå¥–åŠ±è½¬åŒ–ä¸ºæ›´å¯†é›†çš„ä¸­é—´ä¿¡å·ä»¥æé«˜å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ‘çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTree-GRPOï¼‰ï¼Œå®ƒå°†è’™ç‰¹å¡æ´›æ ‘æœç´¢é›†æˆåˆ°GRPOä¸­ã€‚ä¸ºäº†è·¨ä»»åŠ¡å’Œåœºæ™¯è¿›è¡Œå¥–åŠ±ä¼°è®¡æ³›åŒ–ï¼Œä»¥æ”¯æŒè‡ªä¸»é€‚åº”å’Œå¥–åŠ±é©±åŠ¨çš„è‡ªæˆ‘è¿›åŒ–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šæ¨¡å¼ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆMGRMï¼‰ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°SEEA-R1çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ALFWorldåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¾—åˆ†åˆ†åˆ«ä¸ºæ–‡æœ¬æ¨¡å¼ä¸‹çš„85.07%å’Œå¤šæ¨¡å¼ä¸‹çš„36.19%ï¼Œè¶…è¿‡äº†åŒ…æ‹¬GPT-4oåœ¨å†…çš„å…ˆå‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨ä¸ä½¿ç”¨ç¯å¢ƒå¥–åŠ±çš„æƒ…å†µä¸‹ï¼ŒSEEA-R1è¾¾åˆ°äº†80.3%çš„å¾—åˆ†ï¼Œè¶…è¿‡äº†æ‰€æœ‰å¼€æºåŸºå‡†æµ‹è¯•ï¼Œçªæ˜¾äº†å…¶ä½œä¸ºè‡ªæˆ‘è¿›åŒ–å®ä½“ä»£ç†çš„å¯æ‰©å±•æ€§ã€‚å…¶ä»–å®éªŒå’Œå®šæ€§åˆ†æè¿›ä¸€æ­¥æ”¯æŒäº†SEEA-R1åœ¨æœªæ¥å¯æ‰©å±•æ™ºèƒ½ç ”ç©¶ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21669v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« æ¢è®¨è‡ªæˆ‘è¿›åŒ–åœ¨å®ä½“é¢†åŸŸä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰é•¿è¿œè§†è§’å’ŒçœŸå®ä»»åŠ¡çš„ç¯å¢ƒä¸­ã€‚è™½ç„¶å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å®ä½“è‡ªæˆ‘è¿›åŒ–æ™ºèƒ½æ–¹é¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€äº¤äº’æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ–‡ç« æå‡ºSelf-Evolving Embodied Agents-R1ï¼ˆSEEA-R1ï¼‰æ¡†æ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å®ç°å®ä½“æ™ºèƒ½çš„è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›ã€‚é€šè¿‡æ ‘çŠ¶ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTree-GRPOï¼‰å°†ç¨€ç–å»¶è¿Ÿå¥–åŠ±è½¬åŒ–ä¸ºæ›´å¯†é›†çš„ä¸­é—´ä¿¡å·ä»¥æé«˜å¤šæ­¥æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼•å…¥å¤šæ¨¡æ€ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆMGRMï¼‰æ¥æ¨å¹¿è·¨ä»»åŠ¡å’Œåœºæ™¯çš„å¥–åŠ±ä¼°è®¡ã€‚åœ¨ALFWorldåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºSEEA-R1è¶…è¶Šç°æœ‰æŠ€æœ¯æ–¹æ³•ï¼Œå¹¶åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€æ¨¡å¼ä¸‹çš„å¾—åˆ†åˆ†åˆ«ä¸º85.07%å’Œ36.19%ï¼Œä¼˜äºåŒ…æ‹¬GPT-4oåœ¨å†…çš„å…ˆå‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨æ²¡æœ‰ç¯å¢ƒå¥–åŠ±çš„æƒ…å†µä¸‹ï¼ŒSEEA-R1çš„å¾—åˆ†è¾¾åˆ°80.3%ï¼Œè¶…è¶Šäº†æ‰€æœ‰å¼€æºåŸºçº¿ï¼Œå‡¸æ˜¾å…¶ä½œä¸ºè‡ªæˆ‘è¿›åŒ–å®ä½“ä»£ç†çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªæˆ‘è¿›åŒ–å¯¹äºå®ä½“é¢†åŸŸä¸­çš„é•¿è¿œè§†è§’å’ŒçœŸå®ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è™½åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨å®ç°å®ä½“è‡ªæˆ‘è¿›åŒ–æ™ºèƒ½æ–¹é¢æ½œåŠ›å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚</li>
<li>SEEA-R1æ¡†æ¶æ—¨åœ¨è§£å†³å®ä½“æ™ºèƒ½çš„è‡ªæˆ‘è¿›åŒ–é—®é¢˜ï¼Œå¹¶ä¸ºæ­¤æå‡ºTree-GRPOå’Œå¤šæ¨¡æ€ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆMGRMï¼‰ã€‚</li>
<li>Tree-GRPOèƒ½å°†ç¨€ç–å»¶è¿Ÿå¥–åŠ±è½¬åŒ–ä¸ºä¸­é—´ä¿¡å·ï¼Œæé«˜å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MGRMæ¨¡å‹èƒ½æ¨å¹¿è·¨ä»»åŠ¡å’Œåœºæ™¯çš„å¥–åŠ±ä¼°è®¡ï¼Œæ”¯æŒè‡ªä¸»é€‚åº”å’Œå¥–åŠ±é©±åŠ¨çš„è‡ªæˆ‘è¿›åŒ–ã€‚</li>
<li>åœ¨ALFWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSEEA-R1è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60c76ea773340d5fd9714881e7e260bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3016059c50d569d35d6b6e82a168a37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc555e125eecffec549895d38980abdd.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Preference-Optimization-Improves-Spatial-Reasoning-in-VLMs"><a href="#Fine-Grained-Preference-Optimization-Improves-Spatial-Reasoning-in-VLMs" class="headerlink" title="Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs"></a>Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs</h2><p><strong>Authors:Yifan Shen, Yuanzhe Liu, Jingyuan Zhu, Xu Cao, Xiaofeng Zhang, Yixiao He, Wenming Ye, James Matthew Rehg, Ismini Lourentzou</strong></p>
<p>Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks. </p>
<blockquote>
<p>å½“å‰çš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç²¾ç»†çš„ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šæ­¥éª¤é€»è¾‘å’Œç²¾ç¡®ç©ºé—´å¯¹é½çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpatialReasoner-R1ï¼Œè¿™æ˜¯ä¸€ç§è®¾è®¡ç”¨äºè§£å†³è¿™äº›å±€é™æ€§çš„è§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹ã€‚ä¸ºäº†æ„å»ºé«˜è´¨é‡çš„ç©ºé—´æ¨ç†ç›‘ç£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šæ¨¡å‹è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆM3CTSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆå¤šæ ·åŒ–ä¸”é€»è¾‘ä¸€è‡´çš„Long CoTï¼ˆé•¿æ€è€ƒé“¾ï¼‰æ¨ç†è½¨è¿¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç²¾ç»†çš„Direct Preference Optimizationï¼ˆfDPOï¼‰ï¼Œå®ƒå¼•å…¥äº†åˆ†æ®µç‰¹å®šçš„åå¥½ç²’åº¦ï¼Œç”¨äºæè¿°æ€§å®šä½å’Œé€»è¾‘æ¨ç†ï¼Œç”±ç©ºé—´å¥–åŠ±æœºåˆ¶å¼•å¯¼ï¼Œè¯¥æœºåˆ¶æ ¹æ®è§†è§‰ä¸€è‡´æ€§ã€ç©ºé—´å®šä½å’Œé€»è¾‘è¿è´¯æ€§æ¥è¯„ä¼°å€™é€‰ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç©ºé—´è´¨é‡ä»»åŠ¡ä¸Šï¼ŒfDPOè¾ƒæ ‡å‡†DPOå¹³å‡æé«˜äº†4.1%ï¼Œåœ¨ç©ºé—´æ•°é‡ä»»åŠ¡ä¸Šæé«˜äº†9.0%ã€‚ä½¿ç”¨fDPOè®­ç»ƒçš„SpatialReasoner-R1åœ¨SPATIALRGPT-Benchä¸Šåˆ›ä¸‹äº†æ–°çš„æœ€ä½³æˆç»©ï¼Œå¹³å‡å‡†ç¡®ç‡è¾ƒæœ€å¼ºåŸºçº¿æé«˜äº†9.8%ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šä¿æŒäº†ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21656v1">PDF</a> 29 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç²¾ç»†ç©ºé—´æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šæ­¥éª¤é€»è¾‘å’Œç²¾ç¡®ç©ºé—´å¯¹é½çš„æƒ…å†µä¸‹ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¼•å…¥äº†SpatialReasoner-R1è§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹ï¼Œå¹¶é€šè¿‡è®¾è®¡å¤šæ¨¡å¼è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆM3CTSï¼‰æ–¹æ³•æ¥æ„å»ºé«˜è´¨é‡çš„ç©ºé—´æ¨ç†ç›‘ç£ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ç²¾ç»†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆfDPOï¼‰ï¼Œè¯¥æ–¹æ³•å¼•å…¥åˆ†æ®µç‰¹å®šåå¥½ç²’åº¦ï¼Œç”¨äºæè¿°æ€§å®šä½å’Œé€»è¾‘æ¨ç†ï¼Œç”±ç©ºé—´å¥–åŠ±æœºåˆ¶è¯„ä¼°å€™é€‰å“åº”çš„è§†è§‰ä¸€è‡´æ€§ã€ç©ºé—´å®šä½å’Œé€»è¾‘è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒfDPOåœ¨æ ‡å‡†DPOçš„åŸºç¡€ä¸Šï¼Œåœ¨ç©ºé—´è´¨é‡ä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†4.1%ï¼Œåœ¨ç©ºé—´æ•°é‡ä»»åŠ¡ä¸Šæé«˜äº†9.0%ã€‚ä½¿ç”¨fDPOè®­ç»ƒçš„SpatialReasoner-R1åœ¨SPATIALRGPT-Benchä¸Šè¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹³å‡å‡†ç¡®ç‡è¾ƒæœ€å¼ºåŸºçº¿æé«˜äº†9.8%ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šä¿æŒç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šæ­¥éª¤é€»è¾‘å’Œç²¾ç¡®ç©ºé—´å¯¹é½çš„åœºæ™¯ä¸­ã€‚</li>
<li>SpatialReasoner-R1æ¨¡å‹è¢«è®¾è®¡æ¥è§£å†³è¿™äº›é™åˆ¶ï¼Œé€šè¿‡å¼•å…¥å¤šæ¨¡å¼è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆM3CTSï¼‰æ–¹æ³•æ„å»ºé«˜è´¨é‡çš„ç©ºé—´æ¨ç†ç›‘ç£ã€‚</li>
<li>æå‡ºäº†ç²¾ç»†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆfDPOï¼‰ï¼Œé€šè¿‡å¼•å…¥åˆ†æ®µç‰¹å®šåå¥½ç²’åº¦ï¼Œç”¨äºæè¿°æ€§å®šä½å’Œé€»è¾‘æ¨ç†ã€‚</li>
<li>fDPOé€šè¿‡ç©ºé—´å¥–åŠ±æœºåˆ¶è¯„ä¼°å€™é€‰å“åº”çš„è§†è§‰ä¸€è‡´æ€§ã€ç©ºé—´å®šä½å’Œé€»è¾‘è¿è´¯æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç©ºé—´è´¨é‡ä»»åŠ¡ä¸Šï¼ŒfDPOè¾ƒæ ‡å‡†DPOå¹³å‡æé«˜äº†4.1%çš„æ€§èƒ½ï¼›åœ¨ç©ºé—´æ•°é‡ä»»åŠ¡ä¸Šï¼Œè¿™ä¸€æ•°å­—è¾¾åˆ°äº†9.0%ã€‚</li>
<li>SpatialReasoner-R1åœ¨SPATIALRGPT-Benchä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¾ƒæœ€å¼ºåŸºçº¿å¹³å‡æé«˜äº†9.8%çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9da91887513004bc5c461a23d51243aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9006b235521c518e345b6881ddfdbcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-277aa0998ab591af17227f914e7f45e6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="APO-Enhancing-Reasoning-Ability-of-MLLMs-via-Asymmetric-Policy-Optimization"><a href="#APO-Enhancing-Reasoning-Ability-of-MLLMs-via-Asymmetric-Policy-Optimization" class="headerlink" title="APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy   Optimization"></a>APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy   Optimization</h2><p><strong>Authors:Minjie Hong, Zirun Guo, Yan Xia, Zehan Wang, Ziang Zhang, Tao Jin, Zhou Zhao</strong></p>
<p>Multimodal Large Language Models (MLLMs) are powerful at integrating diverse data, but they often struggle with complex reasoning. While Reinforcement learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky. Common issues include a drop in performance on general tasks and the generation of overly detailed or â€œoverthinkingâ€ reasoning. Our work investigates how the KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric Policy Optimization (APO) to address these issues, which divides the sampled responses into positive and negative groups. For positive samples, Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically adjust the KL divergence weight based on their difficulty. This method prevents policy entropy from dropping sharply, improves training stability, utilizes samples better, and preserves the modelâ€™s existing knowledge. For negative samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to penalize overly long responses. This helps mitigate overthinking and encourages more concise reasoning while preserving the modelâ€™s explorative capacity. We apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B significantly enhances reasoning capabilities, showing an average 7% gain over the base model and outperforming larger MLLMs (7-11B) on various reasoning benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade on general tasks, View-R1-3B maintains consistent improvement, demonstrating superior generalization. These results highlight the effectiveness and broad applicability of our DADS and STCR techniques for advancing complex multimodal reasoning in MLLMs. The code will be made available at <a target="_blank" rel="noopener" href="https://github.com/Indolent-Kawhi/View-R1">https://github.com/Indolent-Kawhi/View-R1</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•´åˆå¤šæ ·åŒ–æ•°æ®æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚æ¨ç†æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æé«˜LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å°†å…¶åº”ç”¨äºMLLMså´å¾ˆéš¾ã€‚å¸¸è§çš„é—®é¢˜åŒ…æ‹¬ä¸€èˆ¬ä»»åŠ¡æ€§èƒ½ä¸‹é™ä»¥åŠç”Ÿæˆè¿‡äºè¯¦ç»†æˆ–â€œè¿‡åº¦æ€è€ƒâ€çš„æ¨ç†ã€‚æˆ‘ä»¬çš„å·¥ä½œç ”ç©¶äº†KLæƒ©ç½šå’Œè¿‡åº¦æ€è€ƒå¦‚ä½•å½±å“MLLMsä¸­çš„RLè®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸å¯¹ç§°ç­–ç•¥ä¼˜åŒ–ï¼ˆAPOï¼‰ï¼Œå°†é‡‡æ ·å“åº”åˆ†ä¸ºæ­£è´Ÿä¸¤ç»„ã€‚å¯¹äºæ­£æ ·æœ¬ï¼Œæˆ‘ä»¬å¼•å…¥äº†éš¾åº¦è‡ªé€‚åº”å‘æ•£å½¢çŠ¶ï¼ˆDADSï¼‰ï¼Œæ ¹æ®éš¾åº¦åŠ¨æ€è°ƒæ•´KLå‘æ•£æƒé‡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥é˜²æ­¢ç­–ç•¥ç†µæ€¥å‰§ä¸‹é™ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œæ›´å¥½åœ°åˆ©ç”¨æ ·æœ¬ï¼Œå¹¶ä¿ç•™æ¨¡å‹ç°æœ‰çš„çŸ¥è¯†ã€‚å¯¹äºè´Ÿæ ·æœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†æ¬¡ä¼˜è½¨è¿¹å¤æ‚æ€§æ­£åˆ™åŒ–ï¼ˆSTCRï¼‰æ¥æƒ©ç½šè¿‡äºå†—é•¿çš„å›åº”ã€‚è¿™æœ‰åŠ©äºå‡è½»è¿‡åº¦æ€è€ƒï¼Œé¼“åŠ±æ›´ç®€æ´çš„æ¨ç†ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºQwen2.5-VL-3Bï¼Œåˆ›å»ºäº†View-R1-3Bã€‚View-R1-3Bæ˜¾è‘—å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œåœ¨åŸºå‡†æ¨¡å‹çš„åŸºç¡€ä¸Šå¹³å‡æé«˜äº†7%ï¼Œå¹¶åœ¨å„ç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæ›´å¤§çš„MLLMsï¼ˆ7-11Bï¼‰ã€‚é‡è¦çš„æ˜¯ï¼Œä¸å…¶ä»–ç»å¸¸é™ä½é€šç”¨ä»»åŠ¡æ€§èƒ½çš„é’ˆå¯¹æ¨ç†ä¼˜åŒ–çš„MLLMsä¸åŒï¼ŒView-R1-3Bä¿æŒäº†æŒç»­çš„æ”¹è¿›ï¼Œæ˜¾ç¤ºå‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœçªå‡ºäº†æˆ‘ä»¬çš„DADSå’ŒSTCRæŠ€æœ¯åœ¨æ¨è¿›MLLMsä¸­å¤æ‚å¤šæ¨¡æ€æ¨ç†çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›åº”ç”¨æ€§ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Indolent-Kawhi/View-R1%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Indolent-Kawhi/View-R1ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21655v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡ä¸å¯¹ç§°ç­–ç•¥ä¼˜åŒ–ï¼ˆAPOï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æ–‡ç« ä»‹ç»äº†éš¾åº¦è‡ªé€‚åº”å‘æ•£å½¢çŠ¶ï¼ˆDADSï¼‰å’Œæ¬¡ä¼˜è½¨è¿¹å¤æ‚æ€§æ­£åˆ™åŒ–ï¼ˆSTCRï¼‰ä¸¤ç§æ–¹æ³•ï¼Œåˆ†åˆ«ç”¨äºä¼˜åŒ–æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„è®­ç»ƒè¿‡ç¨‹ã€‚æœ€ç»ˆé€šè¿‡åº”ç”¨è¿™äº›æ–¹æ³•äºç‰¹å®šæ¨¡å‹ä¸Šï¼Œå®ç°äº†å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æå‡ï¼Œä¸”åœ¨å„ç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¹¶ç»´æŒäº†å¯¹ä¸€èˆ¬ä»»åŠ¡çš„æ”¹å–„æ•ˆæœã€‚å…¶ä»£ç å°†å…¬å¼€å‘å¸ƒåœ¨æŒ‡å®šç½‘å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMsåœ¨å¤æ‚æ¨ç†ä¸Šè¡¨ç°ä¸è¶³ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åº”ç”¨ä¹Ÿå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸å¯¹ç§°ç­–ç•¥ä¼˜åŒ–ï¼ˆAPOï¼‰ä»¥è§£å†³MLLMsåœ¨RLè®­ç»ƒä¸­çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥éš¾åº¦è‡ªé€‚åº”å‘æ•£å½¢çŠ¶ï¼ˆDADSï¼‰ä¼˜åŒ–æ­£æ ·æœ¬è®­ç»ƒï¼ŒåŠ¨æ€è°ƒæ•´KLå‘æ•£æƒé‡ã€‚</li>
<li>æå‡ºæ¬¡ä¼˜è½¨è¿¹å¤æ‚æ€§æ­£åˆ™åŒ–ï¼ˆSTCRï¼‰ä»¥æƒ©ç½šè¿‡é•¿çš„è´Ÿæ ·æœ¬å“åº”ï¼Œå‡è½»â€œè¿‡åº¦æ€è€ƒâ€ã€‚</li>
<li>åº”ç”¨è¿™äº›æ–¹æ³•äºç‰¹å®šæ¨¡å‹ï¼Œå®ç°æ¨ç†èƒ½åŠ›å¹³å‡æå‡7%ï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æ¨¡å‹åœ¨ä¿æŒæ¨ç†èƒ½åŠ›æå‡çš„åŒæ—¶ï¼Œä¹Ÿç»´æŒäº†å¯¹ä¸€èˆ¬ä»»åŠ¡çš„æ”¹å–„æ•ˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04c71992c7e9ddf722befe67c65c0aef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-380a5cdae663fc631b777a5af4707638.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebcf27ade513014f7814a1c7540499ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d920bf8de417f31822658372bb1992c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-074ecd175ff86345c582541e9ffc0b96.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Refine-POI-Reinforcement-Fine-Tuned-Large-Language-Models-for-Next-Point-of-Interest-Recommendation"><a href="#Refine-POI-Reinforcement-Fine-Tuned-Large-Language-Models-for-Next-Point-of-Interest-Recommendation" class="headerlink" title="Refine-POI: Reinforcement Fine-Tuned Large Language Models for Next   Point-of-Interest Recommendation"></a>Refine-POI: Reinforcement Fine-Tuned Large Language Models for Next   Point-of-Interest Recommendation</h2><p><strong>Authors:Peibo Li, Shuang Ao, Hao Xue, Yang Song, Maarten de Rijke, Johan BarthÃ©lemy, Tomasz Bednarz, Flora D. Salim</strong></p>
<p>Large language models (LLMs) have been adopted for next point-of-interest (POI) recommendation tasks. Typical LLM-based recommenders fall into two categories: prompt-based and supervised fine-tuning (SFT)-based models. Prompt-based models generally offer greater output flexibility but deliver lower accuracy, whereas SFT-based models achieve higher performance yet face a fundamental mismatch: next POI recommendation data does not naturally suit supervised fine-tuning. In SFT, the model is trained to reproduce the exact ground truth, but each training example provides only a single target POI, so there is no ground truth for producing a top-k list.   To address this, we propose Refine-POI, a reinforcement fine-tuning framework for next POI recommendation. We introduce recommendation-driven rewards that enable LLMs to learn to generate top-k recommendation lists using only one ground-truth POI per example. Experiments on real-world datasets demonstrate that Refine-POI achieves state-of-the-art top-k recommendation performance. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨èç³»ç»Ÿå·²è¢«åº”ç”¨äºä¸‹ä¸€ä¸ªå…´è¶£ç‚¹ï¼ˆPOIï¼‰æ¨èä»»åŠ¡ã€‚å…¸å‹çš„åŸºäºLLMçš„æ¨èç³»ç»Ÿåˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºæç¤ºçš„å’ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ¨¡å‹ã€‚åŸºäºæç¤ºçš„æ¨¡å‹é€šå¸¸å…·æœ‰æ›´å¤§çš„è¾“å‡ºçµæ´»æ€§ï¼Œä½†å‡†ç¡®æ€§è¾ƒä½ï¼Œè€ŒåŸºäºSFTçš„æ¨¡å‹è™½ç„¶æ€§èƒ½è¾ƒé«˜ï¼Œä½†é¢ä¸´ä¸€ä¸ªåŸºæœ¬çš„ä¸åŒ¹é…é—®é¢˜ï¼šä¸‹ä¸€ä¸ªPOIæ¨èæ•°æ®å¹¶ä¸é€‚åˆè¿›è¡Œç›‘ç£å¾®è°ƒã€‚åœ¨SFTä¸­ï¼Œæ¨¡å‹è¢«è®­ç»ƒä»¥å¤åˆ¶ç²¾ç¡®çš„çœŸç›¸æ ‡ç­¾ï¼Œä½†ç”±äºæ¯ä¸ªè®­ç»ƒæ ·æœ¬åªæä¾›ä¸€ä¸ªç›®æ ‡POIï¼Œå› æ­¤ä¸å­˜åœ¨äº§ç”ŸTop-kåˆ—è¡¨çš„çœŸå®æ ‡ç­¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Refine-POIï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä¸‹ä¸€ä¸ªPOIæ¨èçš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¨èé©±åŠ¨å¥–åŠ±ï¼Œä½¿LLMèƒ½å¤Ÿå­¦ä¹ ä½¿ç”¨æ¯ä¸ªç¤ºä¾‹ä»…ä¸€ä¸ªçœŸå®POIæ¥ç”ŸæˆTop-kæ¨èåˆ—è¡¨ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRefine-POIåœ¨Top-kæ¨èæ€§èƒ½æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21599v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åº”ç”¨äºä¸‹ä¸€ä¸ªå…´è¶£ç‚¹ï¼ˆPOIï¼‰æ¨èä»»åŠ¡ï¼Œä¸»è¦åˆ†ä¸ºåŸºäºæç¤ºå’ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ¨¡å‹ä¸¤ç±»ã€‚åŸºäºæç¤ºçš„æ¨¡å‹è¾“å‡ºçµæ´»ä½†å‡†ç¡®åº¦è¾ƒä½ï¼Œè€ŒSFTæ¨¡å‹è™½æ€§èƒ½è¾ƒé«˜ï¼Œä½†å­˜åœ¨æ•°æ®ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºRefine-POIï¼Œä¸€ä¸ªé’ˆå¯¹ä¸‹ä¸€ä¸ªPOIæ¨èçš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œå¼•å…¥æ¨èé©±åŠ¨å¥–åŠ±ï¼Œä½¿LLMèƒ½å¤Ÿå­¦ä¹ ç”Ÿæˆå‰kä¸ªæ¨èåˆ—è¡¨ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRefine-POIå®ç°äº†æœ€å…ˆè¿›çš„top-kæ¨èæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²åº”ç”¨äºPOIæ¨èä»»åŠ¡ã€‚</li>
<li>åŸºäºæç¤ºçš„æ¨¡å‹å’ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ¨¡å‹å„æœ‰ä¼˜ç¼ºç‚¹ã€‚</li>
<li>åŸºäºæç¤ºçš„æ¨¡å‹è¾“å‡ºçµæ´»ä½†å‡†ç¡®åº¦è¾ƒä½ã€‚</li>
<li>SFTæ¨¡å‹è™½ç„¶æ€§èƒ½é«˜ï¼Œä½†å­˜åœ¨æ•°æ®ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>æå‡ºäº†Refine-POIæ¡†æ¶ï¼Œä¸€ä¸ªé’ˆå¯¹ä¸‹ä¸€ä¸ªPOIæ¨èçš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ã€‚</li>
<li>Refine-POIé€šè¿‡å¼•å…¥æ¨èé©±åŠ¨å¥–åŠ±ï¼Œä½¿LLMèƒ½å¤Ÿå­¦ä¹ ç”Ÿæˆå‰kä¸ªæ¨èåˆ—è¡¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c69acba472cc9aa0225a06cb610da76b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60fbf05c7b5eb32d73adb94dc84347ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c6246d48f15f523a07f421e73cb0f1c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Is-DeepSeek-a-New-Voice-Among-LLMs-in-Public-Opinion-Simulation"><a href="#Is-DeepSeek-a-New-Voice-Among-LLMs-in-Public-Opinion-Simulation" class="headerlink" title="Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?"></a>Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?</h2><p><strong>Authors:Weihong Qi, Fan Huang, Jisun An, Haewoon Kwak</strong></p>
<p>This study evaluates the ability of DeepSeek, an open-source large language model (LLM), to simulate public opinions in comparison to LLMs developed by major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5, GPT-4o, and Llama-3.3 and utilizing survey data from the American National Election Studies (ANES) and the Zuobiao dataset of China, we assess these modelsâ€™ capacity to predict public opinions on social issues in both China and the United States, highlighting their comparative capabilities between countries. Our findings indicate that DeepSeek-V3 performs best in simulating U.S. opinions on the abortion issue compared to other topics such as climate change, gun control, immigration, and services for same-sex couples, primarily because it more accurately simulates responses when provided with Democratic or liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating opinions on foreign aid and individualism but shows limitations in modeling views on capitalism, particularly failing to capture the stances of low-income and non-college-educated individuals. It does not exhibit significant differences from other models in simulating opinions on traditionalism and the free market. Further analysis reveals that all LLMs exhibit the tendency to overgeneralize a single perspective within demographic groups, often defaulting to consistent responses within groups. These findings highlight the need to mitigate cultural and demographic biases in LLM-driven public opinion modeling, calling for approaches such as more inclusive training methodologies. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è¯„ä¼°äº†DeepSeekè¿™ä¸€å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿå…¬ä¼—æ„è§çš„èƒ½åŠ›ï¼Œå¹¶ä¸ç”±å¤§å‹ç§‘æŠ€å…¬å¸å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚é€šè¿‡æ¯”è¾ƒDeepSeek-R1å’ŒDeepSeek-V3ä¸Qwen2.5ã€GPT-4oå’ŒLlama-3.3ï¼Œå¹¶åˆ©ç”¨ç¾å›½å…¨å›½é€‰ä¸¾ç ”ç©¶ï¼ˆANESï¼‰å’Œä¸­å›½çš„ã€Šå·¦æ ‡ã€‹æ•°æ®é›†çš„è°ƒæŸ¥æ•°æ®ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨ä¸­å›½å’Œç¾å›½å°±ç¤¾ä¼šé—®é¢˜ä¸Šé¢„æµ‹å…¬ä¼—æ„è§çš„èƒ½åŠ›ï¼Œå¹¶çªå‡ºäº†å®ƒä»¬åœ¨ä¸åŒå›½å®¶ä¹‹é—´çš„æ¯”è¾ƒèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ¨¡æ‹Ÿç¾å›½å…³äºå •èƒé—®é¢˜çš„æ„è§æ–¹é¢ï¼ŒDeepSeek-V3è¡¨ç°å¾—æœ€å¥½ï¼Œä¸å…¶ä»–ä¸»é¢˜å¦‚æ°”å€™å˜åŒ–ã€æªæ”¯ç®¡åˆ¶ã€ç§»æ°‘å’Œä¸ºåŒæ€§å¤«å¦‡æä¾›çš„æœåŠ¡ç›¸æ¯”ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒåœ¨æä¾›æ°‘ä¸»å…šäººæˆ–è‡ªç”±æ´¾äººç‰©æ—¶æ›´èƒ½å‡†ç¡®åœ°æ¨¡æ‹Ÿå›åº”ã€‚å¯¹äºä¸­å›½çš„æ ·æœ¬ï¼ŒDeepSeek-V3åœ¨æ¨¡æ‹Ÿå…³äºå¯¹å¤–æ´åŠ©å’Œä¸ªäººä¸»ä¹‰æ–¹é¢çš„æ„è§æ–¹é¢è¡¨ç°æœ€å¥½ï¼Œä½†åœ¨æ¨¡æ‹Ÿå¯¹èµ„æœ¬ä¸»ä¹‰çš„çœ‹æ³•æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æœªèƒ½æ•æ‰åˆ°ä½æ”¶å…¥å’Œéå¤§å­¦å­¦å†ä¸ªä½“çš„ç«‹åœºã€‚åœ¨æ¨¡æ‹Ÿå…³äºä¼ ç»Ÿä¸»ä¹‰å’Œè‡ªç”±å¸‚åœºçš„æ„è§æ–¹é¢ï¼Œä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”æ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæ‰€æœ‰å¤§å‹è¯­è¨€æ¨¡å‹éƒ½å€¾å‘äºåœ¨äººå£ç¾¤ä½“ä¸­æ¦‚æ‹¬å•ä¸€è§†è§’ï¼Œç»å¸¸åœ¨ç¾¤ä½“å†…éƒ¨äº§ç”Ÿä¸€è‡´çš„ååº”ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å…¬ä¼—æ„è§æ¨¡å‹ä¸­å‡è½»æ–‡åŒ–å’Œäººå£åè§çš„å¿…è¦æ€§ï¼Œå‘¼åé‡‡ç”¨æ›´åŒ…å®¹çš„è®­ç»ƒæ–¹æ³•ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21587v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬ç ”ç©¶è¯„ä¼°äº†DeepSeekè¿™ä¸€å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿå…¬ä¼—æ„è§çš„èƒ½åŠ›ï¼Œå¹¶ä¸ç”±å¤§å‹ç§‘æŠ€å…¬å¸å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚é€šè¿‡å¯¹æ¯”DeepSeek-R1å’ŒDeepSeek-V3ä¸Qwen2.5ã€GPT-4oå’ŒLlama-3.3ï¼Œå¹¶åˆ©ç”¨ç¾å›½å…¨å›½é€‰ä¸¾ç ”ç©¶ï¼ˆANESï¼‰å’Œä¸­å›½çš„ä½æ ‡æ•°æ®é›†è¿›è¡Œè°ƒæŸ¥æ•°æ®ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ä¸­å›½å’Œç¾å›½å°±ç¤¾ä¼šçƒ­ç‚¹é—®é¢˜æ¨¡æ‹Ÿå…¬ä¼—æ„è§çš„èƒ½åŠ›ï¼Œä»¥åŠå›½å®¶é—´çš„æ¯”è¾ƒèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°åœ¨ç¾å›½ï¼ŒDeepSeek-V3åœ¨æ¨¡æ‹Ÿå •èƒé—®é¢˜çš„æ„è§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œåœ¨æ°”å€™å˜åŒ–ã€æªæ”¯æ§åˆ¶ã€ç§»æ°‘å’Œä¸ºåŒæ€§å¤«å¦‡æä¾›æœåŠ¡ç­‰å…¶ä»–ä¸»é¢˜æ–¹é¢ä¸å¦‚å…¶ä»–æ¨¡å‹ã€‚å¯¹äºä¸­å›½çš„æ ·æœ¬ï¼ŒDeepSeek-V3åœ¨æ¨¡æ‹Ÿå…³äºå¯¹å¤–æ´åŠ©å’Œä¸ªäººä¸»ä¹‰çš„è§‚ç‚¹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä½†åœ¨æ¨¡æ‹Ÿå…³äºèµ„æœ¬ä¸»ä¹‰çš„è§‚ç‚¹æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•æ‰ä½æ”¶å…¥å’Œéå¤§å­¦æ•™è‚²è€…çš„ç«‹åœºæ–¹é¢ã€‚åœ¨æ¨¡æ‹Ÿå…³äºä¼ ç»Ÿä¸»ä¹‰å’Œè‡ªç”±å¸‚åœºçš„è§‚ç‚¹æ–¹é¢ï¼Œä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”æ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºï¼Œæ‰€æœ‰å¤§å‹è¯­è¨€æ¨¡å‹éƒ½æœ‰åœ¨äººå£ç¾¤ä½“å†…éƒ¨è¿‡åº¦æ¦‚æ‹¬å•ä¸€è§†è§’çš„å€¾å‘ï¼Œç»å¸¸å¯¼è‡´ç¾¤ä½“å†…éƒ¨çš„ä¸€è‡´å›åº”ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å‡è½»å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨å…¬ä¼—æ„è§å»ºæ¨¡ä¸­çš„æ–‡åŒ–å’Œäººå£åå·®çš„éœ€è¦ï¼Œå‘¼åé‡‡ç”¨æ›´å…·åŒ…å®¹æ€§çš„è®­ç»ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>DeepSeekä¸å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿå…¬ä¼—æ„è§æ–¹é¢è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>DeepSeek-V3åœ¨æ¨¡æ‹Ÿç¾å›½å…³äºå •èƒé—®é¢˜çš„æ„è§ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>DeepSeek-V3åœ¨ä¸­å›½å…³äºå¯¹å¤–æ´åŠ©å’Œä¸ªäººä¸»ä¹‰çš„è§‚ç‚¹æ¨¡æ‹Ÿä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå…³äºèµ„æœ¬ä¸»ä¹‰çš„è§‚ç‚¹æ–¹é¢ï¼ŒDeepSeek-V3å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹æœ‰è¿‡åº¦æ¦‚æ‹¬äººå£ç¾¤ä½“å†…éƒ¨å•ä¸€è§†è§’çš„å€¾å‘ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹åœ¨æ¨¡æ‹Ÿå…³äºä¼ ç»Ÿä¸»ä¹‰å’Œè‡ªç”±å¸‚åœºçš„è§‚ç‚¹æ–¹é¢æ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9dd8b5e4f1de29c71f890cc347f2aca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b5b74fbe916954ba788b6bf7e591093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-154269876b8961fad5d1903c23536494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d2ee3df3fd4d4d24bb54c97c3fd3f69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f17e92b8bdb0d4df497e9de1af49495e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40aade566feb12cad78cbf835e51a1fd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ShotBench-Expert-Level-Cinematic-Understanding-in-Vision-Language-Models"><a href="#ShotBench-Expert-Level-Cinematic-Understanding-in-Vision-Language-Models" class="headerlink" title="ShotBench: Expert-Level Cinematic Understanding in Vision-Language   Models"></a>ShotBench: Expert-Level Cinematic Understanding in Vision-Language   Models</h2><p><strong>Authors:Hongbo Liu, Jingwen He, Yi Jin, Dian Zheng, Yuhao Dong, Fan Zhang, Ziqi Huang, Yinan He, Yangguang Li, Weichao Chen, Yu Qiao, Wanli Ouyang, Shengjie Zhao, Ziwei Liu</strong></p>
<p>Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce ShotBench, a comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct ShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new state-of-the-art performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation. </p>
<blockquote>
<p>ç”µå½±æ‘„å½±ä½œä¸ºç”µå½±çš„åŸºæœ¬è§†è§‰è¯­è¨€ï¼Œå¯¹äºä¼ è¾¾å™äº‹ã€æƒ…æ„Ÿå’Œå®¡ç¾è´¨é‡è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å±•ç¤ºäº†å¼ºå¤§çš„é€šç”¨è§†è§‰ç†è§£èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ç†è§£å•ä¸ªé•œå¤´ä¸­åµŒå…¥çš„å¾®å¦™ç”µå½±è¯­æ³•æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ï¼Œä¹Ÿç¼ºä¹ç¨³å¥çš„è¯„ä¼°ã€‚è¿™ä¸€å…³é”®å·®è·é™åˆ¶äº†ç²¾ç»†ç²’åº¦çš„è§†è§‰ç†è§£å’ŒAIè¾…åŠ©è§†é¢‘ç”Ÿæˆçš„ç²¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ShotBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºç”µå½±è¯­è¨€ç†è§£è®¾è®¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«æ¥è‡ªè¶…è¿‡200éƒ¨å¤‡å—èµèª‰ï¼ˆä¸»è¦æ˜¯å¥¥æ–¯å¡æåï¼‰ç”µå½±çš„è¶…è¿‡3.5kä¸“å®¶æ³¨é‡Šçš„QAå¯¹ï¼Œæ¶µç›–äº†å…«ä¸ªå…³é”®çš„ç”µå½±æ‘„å½±ç»´åº¦ã€‚æˆ‘ä»¬å¯¹24ä¸ªé¢†å…ˆçš„VLMåœ¨ShotBenchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºå‡ºäº†å®ƒä»¬çš„é‡å¤§å±€é™æ€§ï¼šå³ä½¿è¡¨ç°æœ€ä½³çš„æ¨¡å‹å¹³å‡å‡†ç¡®ç‡ä¹Ÿä½äº60%ï¼Œå°¤å…¶éš¾ä»¥æŒæ¡ç»†å¾®çš„è§†è§‰çº¿ç´¢å’Œå¤æ‚çš„ç©ºé—´æ¨ç†ã€‚ä¸ºäº†å‚¬åŒ–è¯¥é¢†åŸŸçš„è¿›æ­¥ï¼Œæˆ‘ä»¬æ„å»ºäº†ShotQAï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…å«å¤§çº¦7ä¸‡å¯¹ç”µå½±QAã€‚é€šè¿‡åˆ©ç”¨ShotQAè¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒä»¥åŠé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œæˆ‘ä»¬å¼€å‘äº†ShotVLã€‚ShotVLåœ¨ShotBenchä¸Šæ˜¾è‘—è¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œåˆ›é€ äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚ä¸ºäº†ä¿ƒè¿›AIé©±åŠ¨çš„ç”µå½±ç†è§£ç”Ÿæˆçš„è¿™ä¸€å…³é”®é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21356v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”µå½±è§†è§‰è¯­è¨€çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç”µå½±é•œå¤´ä¸­çš„ç»†å¾®è¯­æ³•æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ShotBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹å¯¹ç”µå½±è¯­è¨€çš„ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ShotQAæ•°æ®é›†å’ŒShotVLæ¨¡å‹çš„å¼€å‘æƒ…å†µï¼Œè¯¥æ¨¡å‹åœ¨ShotBenchä¸Šçš„è¡¨ç°è¾¾åˆ°äº†æ–°çš„æ°´å¹³ã€‚æœ€åï¼Œæ–‡ç« å¼€æºäº†æ¨¡å‹ã€æ•°æ®å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›AIé©±åŠ¨çš„ç”µå½±ç†è§£çš„å¿«é€Ÿå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå½±è§†è§‰è¯­è¨€çš„é‡è¦æ€§åŠå…¶å¯¹äºå™äº‹ã€æƒ…æ„Ÿå’Œç¾å­¦è´¨é‡çš„ä¼ è¾¾ä½œç”¨ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£ç”µå½±ç»†å¾®è¯­æ³•æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å•ä¸ªé•œå¤´ä¸­çš„ç»†å¾®å·®åˆ«å’Œå¤æ‚ç©ºé—´æ¨ç†æ–¹é¢ã€‚</li>
<li>ShotBenchåŸºå‡†æµ‹è¯•å¹³å°çš„ä»‹ç»ï¼Œè¯¥å¹³å°æ—¨åœ¨è¯„ä¼°æ¨¡å‹å¯¹ç”µå½±è¯­è¨€çš„ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«ä»å›¾åƒå’Œè§†é¢‘å‰ªè¾‘ä¸­ç²¾å¿ƒæŒ‘é€‰çš„è¶…è¿‡3.5kçš„ä¸“å®¶æ³¨é‡Šé—®ç­”å¯¹ã€‚</li>
<li>å¯¹24æ¬¾é¢†å…ˆè§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ç»“æœæ­ç¤ºäº†å®ƒä»¬çš„æ˜¾è‘—å±€é™æ€§ã€‚</li>
<li>ShotQAæ•°æ®é›†çš„æ„å»ºï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼ç”µå½±é—®ç­”æ•°æ®é›†ï¼ŒåŒ…å«å¤§çº¦70kçš„ç”µå½±é—®ç­”å¯¹ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒåˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œå¼€å‘äº†ShotVLæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ShotBenchä¸Šçš„è¡¨ç°è¶…è¶Šäº†æ‰€æœ‰ç°æœ‰çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90fe04d1de06d5fa3c19e0311c00ddbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-568f2aef1f52a64ab8b92ecda762d653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d4f5dcb667d98acfd01a263a69c4d01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73a79725965f33a75a3fdc1cd26ba5b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01c6cc23c7c4a5bf59db2f0d1f8e17df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d4c1ab37832c32770d9b96cbeedfe59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9f1cdc76727af2b28eb6e9cf2373841.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Mobile-R1-Towards-Interactive-Reinforcement-Learning-for-VLM-Based-Mobile-Agent-via-Task-Level-Rewards"><a href="#Mobile-R1-Towards-Interactive-Reinforcement-Learning-for-VLM-Based-Mobile-Agent-via-Task-Level-Rewards" class="headerlink" title="Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based   Mobile Agent via Task-Level Rewards"></a>Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based   Mobile Agent via Task-Level Rewards</h2><p><strong>Authors:Jihao Gu, Qihang Ai, Yingyao Wang, Pi Bu, Jingxuan Xing, Zekun Zhu, Wei Jiang, Ziming Wang, Yingxiu Zhao, Ming-Liang Zhang, Jun Song, Yuning Jiang, Bo Zheng</strong></p>
<p>Vision-language model-based mobile agents have gained the ability to not only understand complex instructions and mobile screenshots, but also optimize their action outputs via thinking and reasoning, benefiting from reinforcement learning, such as Group Relative Policy Optimization (GRPO). However, existing research centers on offline reinforcement learning training or online optimization using action-level rewards, which limits the agentâ€™s dynamic interaction with the environment. This often results in agents settling into local optima, thereby weakening their ability for exploration and error action correction. To address these challenges, we introduce an approach called Mobile-R1, which employs interactive multi-turn reinforcement learning with task-level rewards for mobile agents. Our training framework consists of three stages: initial format finetuning, single-step online training via action-level reward, followed by online training via task-level reward based on multi-turn trajectories. This strategy is designed to enhance the exploration and error correction capabilities of Mobile-R1, leading to significant performance improvements. Moreover, we have collected a dataset covering 28 Chinese applications with 24,521 high-quality manual annotations and established a new benchmark with 500 trajectories. We will open source all resources, including the dataset, benchmark, model weight, and codes: <a target="_blank" rel="noopener" href="https://mobile-r1.github.io/Mobile-R1/">https://mobile-r1.github.io/Mobile-R1/</a>. </p>
<blockquote>
<p>åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç§»åŠ¨ä»£ç†ä¸ä»…è·å¾—äº†ç†è§£å¤æ‚æŒ‡ä»¤å’Œç§»åŠ¨æˆªå›¾çš„èƒ½åŠ›ï¼Œè€Œä¸”å¾—ç›Šäºå¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼‰ï¼Œå®ƒä»¬è¿˜èƒ½å¤Ÿé€šè¿‡æ€è€ƒå’Œæ¨ç†ä¼˜åŒ–å…¶è¡ŒåŠ¨è¾“å‡ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶ä¸­å¿ƒä¸»è¦å…³æ³¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ–åˆ©ç”¨åŠ¨ä½œçº§å¥–åŠ±è¿›è¡Œåœ¨çº¿ä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†ä»£ç†ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’ã€‚è¿™é€šå¸¸å¯¼è‡´ä»£ç†é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œä»è€Œå‰Šå¼±å…¶æ¢ç´¢èƒ½åŠ›å’Œçº æ­£é”™è¯¯è¡ŒåŠ¨çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç§°ä¸ºMobile-R1çš„æ–¹æ³•ï¼Œå®ƒä¸ºç§»åŠ¨ä»£ç†é‡‡ç”¨äº¤äº’å¼å¤šå›åˆå¼ºåŒ–å­¦ä¹ å’Œä»»åŠ¡çº§å¥–åŠ±ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šåˆå§‹æ ¼å¼å¾®è°ƒã€é€šè¿‡åŠ¨ä½œçº§å¥–åŠ±è¿›è¡Œå•æ­¥åœ¨çº¿è®­ç»ƒï¼Œç„¶åæ˜¯åŸºäºå¤šå›åˆè½¨è¿¹çš„ä»»åŠ¡çº§å¥–åŠ±åœ¨çº¿è®­ç»ƒã€‚è¿™ä¸€ç­–ç•¥æ—¨åœ¨å¢å¼ºMobile-R1çš„æ¢ç´¢å’Œé”™è¯¯çº æ­£èƒ½åŠ›ï¼Œä»è€Œå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†æ¶µç›–28ä¸ªä¸­æ–‡åº”ç”¨ç¨‹åºçš„æ•°æ®é›†ï¼ŒåŒ…å«24521ä¸ªé«˜è´¨é‡çš„æ‰‹åŠ¨æ³¨é‡Šï¼Œå¹¶å»ºç«‹äº†åŒ…å«500ä¸ªè½¨è¿¹çš„æ–°åŸºå‡†ã€‚æˆ‘ä»¬å°†å…¬å¼€æ‰€æœ‰èµ„æºï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•ã€æ¨¡å‹æƒé‡å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://mobile-r1.github.io/Mobile-R1/">https://mobile-r1.github.io/Mobile-R1/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20332v2">PDF</a> 14 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>ç§»åŠ¨æ™ºèƒ½ä½“åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡ç†è§£å¤æ‚æŒ‡ä»¤å’Œç§»åŠ¨æˆªå›¾çš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–åŠ¨ä½œè¾“å‡ºã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶é›†ä¸­åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ–åŠ¨ä½œçº§åˆ«çš„åœ¨çº¿ä¼˜åŒ–ä¸Šï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„åŠ¨æ€äº¤äº’ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºMobile-R1æ–¹æ³•ï¼Œé‡‡ç”¨äº¤äº’å¼å¤šå›åˆå¼ºåŒ–å­¦ä¹ ä¸ä»»åŠ¡çº§åˆ«å¥–åŠ±ç›¸ç»“åˆçš„è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬åˆå§‹æ ¼å¼å¾®è°ƒã€å•æ­¥åœ¨çº¿åŠ¨ä½œçº§åˆ«å¥–åŠ±è®­ç»ƒå’ŒåŸºäºå¤šå›åˆè½¨è¿¹çš„åœ¨çº¿ä»»åŠ¡çº§åˆ«å¥–åŠ±è®­ç»ƒä¸‰ä¸ªé˜¶æ®µã€‚è¿™æœ‰åŠ©äºæé«˜Mobile-R1çš„æ¢ç´¢å’Œé”™è¯¯çº æ­£èƒ½åŠ›ï¼Œå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªåŒ…å«28ä¸ªåº”ç”¨ç¨‹åºçš„æ•°æ®é›†ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨æ™ºèƒ½ä½“å·²å…·å¤‡é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç†è§£å¤æ‚æŒ‡ä»¤å’Œç§»åŠ¨æˆªå›¾çš„èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è¢«ç”¨äºä¼˜åŒ–ç§»åŠ¨æ™ºèƒ½ä½“çš„åŠ¨ä½œè¾“å‡ºã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ–åŠ¨ä½œçº§åˆ«çš„åœ¨çº¿ä¼˜åŒ–ä¸Šï¼Œé™åˆ¶äº†æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’ã€‚</li>
<li>Mobile-R1æ–¹æ³•é‡‡ç”¨äº¤äº’å¼å¤šå›åˆå¼ºåŒ–å­¦ä¹ ä¸ä»»åŠ¡çº§åˆ«å¥–åŠ±æ¥å¢å¼ºæ™ºèƒ½ä½“çš„æ¢ç´¢èƒ½åŠ›å’Œé”™è¯¯çº æ­£èƒ½åŠ›ã€‚</li>
<li>Mobile-R1çš„è®­ç»ƒæ¡†æ¶åŒ…æ‹¬åˆå§‹æ ¼å¼å¾®è°ƒã€å•æ­¥åœ¨çº¿åŠ¨ä½œçº§åˆ«å¥–åŠ±è®­ç»ƒå’Œåœ¨çº¿ä»»åŠ¡çº§åˆ«å¥–åŠ±è®­ç»ƒä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>æ”¶é›†äº†ä¸€ä¸ªåŒ…å«28ä¸ªåº”ç”¨ç¨‹åºçš„æ•°æ®é›†ï¼Œå¹¶å»ºç«‹äº†æ–°çš„åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1c72ae8b3d4985250edac51d6b6f90f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4f11634f38e1adf173487f146e8d128.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ac899d0e0cae2b41e94d2554d2a555a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f48800b37ff69204ffefe5d608ce8de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f462316ef48c2506828a89724016e12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d24431eea05581b90288ae4c006193c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents"><a href="#From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents" class="headerlink" title="From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents"></a>From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents</h2><p><strong>Authors:Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu</strong></p>
<p>Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in <a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a>. </p>
<blockquote>
<p>ä¿¡æ¯æ£€ç´¢æ˜¯ç°ä»£çŸ¥è¯†è·å–çš„æ ¸å¿ƒåŸºçŸ³ï¼Œæ¯å¤©å¯ä»¥åœ¨ä¸åŒé¢†åŸŸå¤„ç†æ•°åäº¿çš„æŸ¥è¯¢ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“è¶Šæ¥è¶Šéš¾ä»¥æ»¡è¶³å¤æ‚çš„ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚ã€‚æˆ‘ä»¬çš„è§‚ç‚¹æ˜¯ï¼Œæ‹¥æœ‰æ¨ç†å’Œæ™ºèƒ½èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨å¼•é¢†ä¸€ç§æ–°çš„åä¸ºæ™ºèƒ½æ·±åº¦ç ”ç©¶ï¼ˆAgentic Deep Researchï¼‰çš„èŒƒå¼ã€‚è¿™äº›ç³»ç»Ÿé€šè¿‡ç´§å¯†é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆåˆ°ä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯ä¸­ï¼Œä»è€Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¿¡æ¯æœç´¢æŠ€æœ¯ã€‚æˆ‘ä»¬è¿½æº¯äº†ä»é™æ€ç½‘é¡µæœç´¢åˆ°äº¤äº’å¼ã€åŸºäºæ™ºèƒ½ç³»ç»Ÿçš„æ¼”å˜è¿‡ç¨‹ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥è§„åˆ’ã€æ¢ç´¢å’Œå­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæµ‹è¯•æ—¶é—´ç¼©æ”¾å®šå¾‹æ¥æ­£å¼è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚åœ¨åŸºå‡†æµ‹è¯•ç»“æœå’Œå¼€æºå®ç°çš„æ¨åŠ¨ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†æ™ºèƒ½æ·±åº¦ç ”ç©¶ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”å·²æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚æ‰€æœ‰ç›¸å…³èµ„æºï¼ŒåŒ…æ‹¬å·¥ä¸šäº§å“ã€ç ”ç©¶è®ºæ–‡ã€åŸºå‡†æ•°æ®é›†å’Œå¼€æºå®ç°ï¼Œéƒ½åœ¨ <a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a> ä¸­ä¸ºç¤¾åŒºæ”¶é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18959v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡æ¨ç†å’Œä»£ç†èƒ½åŠ›ï¼Œæ­£åœ¨æ¨åŠ¨ä¸€ç§åä¸ºä»£ç†æ·±åº¦ç ”ç©¶çš„æ–°èŒƒå¼ã€‚è¿™ç§æ–°èŒƒå¼é€šè¿‡ç´§å¯†ç»“åˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆï¼Œçªç ´äº†ä¼ ç»Ÿä¿¡æ¯æœç´¢æŠ€æœ¯çš„é™åˆ¶ã€‚å…¶åœ¨æµ‹è¯•æ—¶çš„è§„æ¨¡å®šå¾‹æ­£å¼ç¡®å®šäº†è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚ä»£ç†æ·±åº¦ç ”ç©¶ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”æœ‰æœ›æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚ç›¸å…³èµ„æºå‡å·²æ”¶é›†ï¼Œä¾›ç¤¾åŒºå‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡æ¨ç†å’Œä»£ç†èƒ½åŠ›ï¼Œæ¨åŠ¨ä¿¡æ¯æœç´¢æ–°èŒƒå¼â€”â€”ä»£ç†æ·±åº¦ç ”ç©¶çš„å‡ºç°ã€‚</li>
<li>ä»£ç†æ·±åº¦ç ”ç©¶é€šè¿‡ç»“åˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆï¼Œè¶…è¶Šäº†ä¼ ç»Ÿä¿¡æ¯æœç´¢æŠ€æœ¯ã€‚</li>
<li>æµ‹è¯•æ—¶çš„è§„æ¨¡å®šå¾‹è¡¨æ˜è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢æœ‰é‡è¦å½±å“ã€‚</li>
<li>ä»£ç†æ·±åº¦ç ”ç©¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ‰æœ›æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚</li>
<li>è¯¥é¢†åŸŸçš„ç›¸å…³èµ„æºï¼ŒåŒ…æ‹¬å·¥ä¸šäº§å“ã€ç ”ç©¶è®ºæ–‡ã€åŸºå‡†æ•°æ®é›†å’Œå¼€æºå®ç°ï¼Œå‡å·²æ”¶é›†ä¾›ç¤¾åŒºå‚è€ƒã€‚</li>
<li>ä»£ç†æ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„è®¡åˆ’ã€æ¢ç´¢å’Œå­¦ä¹ èƒ½åŠ›æ˜¯ä¼ ç»Ÿæœç´¢å¼•æ“æ— æ³•æ¯”æ‹Ÿçš„ã€‚</li>
<li>å¼€æºå®ç°çš„å…´èµ·å¯¹ä»£ç†æ·±åº¦ç ”ç©¶çš„æ¨å¹¿å’Œåº”ç”¨èµ·åˆ°äº†ç§¯æä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f9a57a6730e21115b5e3be79c0c73400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76a584b347a5f8a48347111db296ccc0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-155280869ad2bd126b54c3da5ebc7cd6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Reasoning-about-Uncertainty-Do-Reasoning-Models-Know-When-They-Donâ€™t-Know"><a href="#Reasoning-about-Uncertainty-Do-Reasoning-Models-Know-When-They-Donâ€™t-Know" class="headerlink" title="Reasoning about Uncertainty: Do Reasoning Models Know When They Donâ€™t   Know?"></a>Reasoning about Uncertainty: Do Reasoning Models Know When They Donâ€™t   Know?</h2><p><strong>Authors:Zhiting Mei, Christina Zhang, Tenny Yin, Justin Lidard, Ola Shorinwa, Anirudha Majumdar</strong></p>
<p>Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First, are reasoning models well-calibrated? Second, does deeper reasoning improve model calibration? Finally, inspired by humansâ€™ innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask: can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces? We introduce introspective uncertainty quantification (UQ) to explore this direction. In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models. </p>
<blockquote>
<p>æ¨ç†è¯­è¨€æ¨¡å‹å·²åœ¨è®¸å¤šå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰ï¼Œè¿™æ˜¯é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¤šæ­¥æ¨ç†å®ç°çš„ã€‚ç„¶è€Œï¼Œä¸ä¹‹å‰çš„è¯­è¨€æ¨¡å‹ä¸€æ ·ï¼Œæ¨ç†æ¨¡å‹å®¹æ˜“äº§ç”Ÿè‡ªä¿¡ä¸”çœ‹ä¼¼åˆç†çš„é”™è¯¯ç­”æ¡ˆï¼ˆå¹»è§‰ï¼‰ã€‚çŸ¥é“ä½•æ—¶ä»¥åŠå¤šå°‘ä¿¡ä»»è¿™äº›æ¨¡å‹å¯¹äºåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å®‰å…¨éƒ¨ç½²æ¨ç†æ¨¡å‹è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨å·¥ä½œä¸­æ¢è®¨äº†æ¨ç†æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªåŸºæœ¬é—®é¢˜ï¼šé¦–å…ˆï¼Œæ¨ç†æ¨¡å‹æ˜¯å¦ç»è¿‡è‰¯å¥½æ ¡å‡†ï¼Ÿå…¶æ¬¡ï¼Œæ›´æ·±å…¥çš„æ¨ç†èƒ½å¦æé«˜æ¨¡å‹çš„æ ¡å‡†åº¦ï¼Ÿæœ€åï¼Œå—äººç±»å¤©ç”Ÿèƒ½å¤Ÿå›é¡¾è‡ªå·±çš„æ€è€ƒè¿‡ç¨‹ä»¥éªŒè¯ç­”æ¡ˆçš„æœ‰æ•ˆæ€§å’Œè‡ªä¿¡ç¨‹åº¦çš„å¯å‘ï¼Œæˆ‘ä»¬é—®ï¼šæ¨ç†æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿé€šè¿‡æ˜ç¡®æ€è€ƒå…¶æ€ç»´è½¨è¿¹æ¥æé«˜å…¶æ ¡å‡†åº¦ï¼Ÿæˆ‘ä»¬å¼•å…¥å†…çœä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æ¥æ¢ç´¢è¿™ä¸ªæ–¹å‘ã€‚åœ¨å¯¹ä¸€ç³»åˆ—æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ¨ç†æ¨¡å‹è¿›è¡Œå¹¿æ³›è¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹é€šå¸¸è¿‡äºè‡ªä¿¡ï¼Œè‡ªæˆ‘å£å¤´è¡¨è¿°çš„ä¿¡å¿ƒä¼°è®¡å€¼ç»å¸¸è¶…è¿‡85%ï¼Œç‰¹åˆ«æ˜¯åœ¨é”™è¯¯ç­”æ¡ˆçš„æƒ…å†µä¸‹ï¼›å®ƒä»¬åœ¨è¿›è¡Œæ›´æ·±å…¥æ¨ç†æ—¶ä¼šå˜å¾—æ›´åŠ è¿‡äºè‡ªä¿¡ï¼›è€Œä¸”èƒ½å¤Ÿé€šè¿‡å†…çœå˜å¾—æ›´å¥½æ ¡å‡†ï¼ˆä¾‹å¦‚ï¼Œo3-Miniå’ŒDeepSeek R1ï¼‰ï¼Œä½†å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½å¦‚æ­¤ï¼ˆä¾‹å¦‚ï¼ŒClaude 3.7 Sonnetçš„æ ¡å‡†åº¦å˜å¾—æ›´å·®ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†è®¾è®¡å¿…è¦çš„ä¸ç¡®å®šæ€§é‡åŒ–åŸºå‡†å¹¶æ”¹å–„æ¨ç†æ¨¡å‹æ ¡å‡†çš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18183v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¯±å¯¼çš„å¤šæ­¥æ¨ç†è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–é—®é¢˜ã€‚æ–‡ç« å¯¹ä¸‰ä¸ªå…³é”®é—®é¢˜è¿›è¡Œäº†æ¢ç´¢ï¼šè¯­è¨€æ¨¡å‹çš„æ ¡å‡†æƒ…å†µï¼›æ›´æ·±å…¥çš„æ¨ç†æ˜¯å¦æ”¹å–„æ¨¡å‹æ ¡å‡†ï¼›ä»¥åŠè¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½é€šè¿‡å¯¹å…¶æ€ç»´è¿‡ç¨‹è¿›è¡Œå†…çœæ¥æé«˜æ ¡å‡†ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­è¨€æ¨¡å‹é€šå¸¸æœ‰è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ï¼Œä¸”é”™è¯¯çš„å›ç­”ä¸­è‡ªä¿¡åº¦è¾ƒé«˜ï¼›æ·±åº¦æ¨ç†ä¼šä½¿æ¨¡å‹æ›´åŠ è¿‡åº¦è‡ªä¿¡ï¼›éƒ¨åˆ†æ¨¡å‹é€šè¿‡å†…çœå¯ä»¥æ›´å¥½åœ°æ ¡å‡†ï¼Œä½†ä¹Ÿæœ‰ä¾‹å¤–ã€‚æœ€åï¼Œæ–‡ç« æå‡ºäº†è®¾è®¡ä¸ç¡®å®šæ€§é‡åŒ–åŸºå‡†æµ‹è¯•å’Œæ”¹è¿›è¯­è¨€æ¨¡å‹æ ¡å‡†çš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥æ¨ç†ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å­˜åœ¨ç”Ÿæˆè‡ªä¿¡ä½†é”™è¯¯çš„å›ç­”ï¼ˆhallucinationsï¼‰çš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹æ ¡å‡†å¯¹äºå°†è¯­è¨€æ¨¡å‹éƒ¨ç½²åˆ°å®é™…åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶å‘ç°è¯­è¨€æ¨¡å‹æ™®éå­˜åœ¨è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é”™è¯¯çš„å›ç­”ã€‚</li>
<li>æ·±åº¦æ¨ç†ä½¿è¯­è¨€æ¨¡å‹æ›´åŠ è¿‡åº¦è‡ªä¿¡ã€‚</li>
<li>é€šè¿‡å†…çœï¼Œéƒ¨åˆ†è¯­è¨€æ¨¡å‹çš„æ ¡å‡†æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†ä¹Ÿæœ‰ä¾‹å¤–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7846692c4dd3323906f461f298422c37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85108b4cc391ab764cd59473a6e97f5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6de958e088c45ab303d691ad724ffd40.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-LLM-Interaction-with-Socratic-Chain-of-Thought-Reasoning-and-Homomorphically-Encrypted-Vector-Databases"><a href="#Privacy-Preserving-LLM-Interaction-with-Socratic-Chain-of-Thought-Reasoning-and-Homomorphically-Encrypted-Vector-Databases" class="headerlink" title="Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought   Reasoning and Homomorphically Encrypted Vector Databases"></a>Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought   Reasoning and Homomorphically Encrypted Vector Databases</h2><p><strong>Authors:Yubeen Bae, Minchan Kim, Jaejin Lee, Sangbum Kim, Jaehyung Kim, Yejin Choi, Niloofar Mireshghallah</strong></p>
<p>Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single userâ€™s private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œä¸ªäººä»£ç†ï¼Œè®¿é—®æ•æ„Ÿç”¨æˆ·æ•°æ®ï¼Œå¦‚æ—¥å†ã€ç”µå­é‚®ä»¶å’ŒåŒ»ç–—è®°å½•ã€‚ç›®å‰ï¼Œç”¨æˆ·é¢ä¸´ä¸€ç§æƒè¡¡ï¼šä»–ä»¬å¯ä»¥å°†å­˜å‚¨åœ¨è¿œç¨‹æ•°æ®åº“ä¸­çš„ç§äººè®°å½•å‘é€ç»™åŠŸèƒ½å¼ºå¤§ä½†ä¸å—ä¿¡ä»»çš„è¯­è¨€æ¨¡å‹æä¾›å•†ï¼Œä»è€Œå¢åŠ å…¶æš´éœ²é£é™©ã€‚æˆ–è€…ï¼Œä»–ä»¬å¯ä»¥åœ¨å¯é çš„è®¾å¤‡ä¸Šè¿è¡ŒåŠŸèƒ½è¾ƒå¼±çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼¥è¡¥äº†è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬çš„è‹æ ¼æ‹‰åº•æ€ç»´é“¾æ¨ç†é¦–å…ˆå‘åŠŸèƒ½å¼ºå¤§ã€ä¸å—ä¿¡ä»»çš„è¯­è¨€æ¨¡å‹å‘é€ä¸€ä¸ªé€šç”¨çš„éç§æœ‰ç”¨æˆ·æŸ¥è¯¢ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆä¸€ä¸ªæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå’Œè¯¦ç»†çš„å­æŸ¥è¯¢ï¼Œæ— éœ€è®¿é—®ç”¨æˆ·æ•°æ®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº›å­æŸ¥è¯¢åµŒå…¥å¹¶åœ¨æˆ‘ä»¬çš„åŒæ€åŠ å¯†å‘é‡æ•°æ®åº“ä¸­ä½¿ç”¨åŠ å¯†å­ç§’è¯­ä¹‰æœç´¢åŠŸèƒ½ï¼Œåœ¨å•ä¸ªç”¨æˆ·çš„ç§äººæ•°æ®çš„ä¸€ç™¾ä¸‡æ¡æ¡ç›®ä¸­è¿›è¡Œæœç´¢ã€‚è¿™ä»£è¡¨äº†å¤šå¹´æ¥ç§¯ç´¯çš„ä¸ªäººæ–‡æ¡£ã€ç”µå­é‚®ä»¶å’Œè®°å½•çš„ç°å®è§„æ¨¡ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ€ç»´é“¾æç¤ºå’Œè§£å¯†çš„è®°å½•è¾“å…¥æœ¬åœ°è¯­è¨€æ¨¡å‹ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚åœ¨LoCoMoé•¿æ–‡æœ¬é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬ç»“åˆGPT-4oå’Œæœ¬åœ°Llama-3.2-1Bæ¨¡å‹çš„æ··åˆæ¡†æ¶ï¼Œæ¯”ä»…ä½¿ç”¨GPT-4oé«˜å‡º7.1ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™æœç€ä»»åŠ¡åœ¨ä¸å—ä¿¡ä»»çš„å¼ºå¤§è¯­è¨€æ¨¡å‹å’Œè¾ƒå¼±çš„æœ¬åœ°è¯­è¨€æ¨¡å‹ä¹‹é—´åˆ†è§£å’Œæ‹†åˆ†ã€åŒæ—¶ä¿ç•™ç”¨æˆ·éšç§çš„ç³»ç»Ÿè¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17336v2">PDF</a> 29 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä¸ªäººä»£ç†ï¼Œè®¿é—®ç”¨æˆ·æ•æ„Ÿæ•°æ®ï¼Œå¦‚æ—¥å†ã€ç”µå­é‚®ä»¶å’ŒåŒ»ç–—è®°å½•ã€‚ç”¨æˆ·é¢ä¸´ä¸¤éš¾é€‰æ‹©ï¼šå°†ç§æœ‰è®°å½•å‘é€åˆ°å¼ºå¤§çš„ä½†ä¸å—ä¿¡ä»»çš„è¯­è¨€æ¨¡å‹æä¾›å•†ï¼Œæˆ–å°†è®°å½•ä¿å­˜åœ¨æœ¬åœ°æ¨¡å‹ä¸­ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è‹æ ¼æ‹‰åº•å¼æ€ç»´é“¾æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†ç”¨æˆ·æŸ¥è¯¢å‘é€åˆ°å¼ºå¤§çš„ä¸å—ä¿¡ä»»çš„è¯­è¨€æ¨¡å‹ç”Ÿæˆæ€ç»´é“¾æç¤ºå’Œè¯¦ç»†çš„å­æŸ¥è¯¢ï¼Œç„¶ååˆ©ç”¨åŒæ€åŠ å¯†å‘é‡æ•°æ®åº“åœ¨ç”¨æˆ·çš„ç§æœ‰æ•°æ®ä¸­æ‰§è¡ŒåŠ å¯†è¯­ä¹‰æœç´¢ã€‚æœ€åï¼Œå°†æ€ç»´é“¾æç¤ºå’Œå·²è§£å¯†çš„è®°å½•è¾“å…¥æœ¬åœ°è¯­è¨€æ¨¡å‹ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚åœ¨LoCoMoé•¿æ–‡æœ¬é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œç»“åˆäº†GPT-4oå’Œæœ¬åœ°Llama-3.2-1Bæ¨¡å‹çš„æ··åˆæ¡†æ¶æ¯”ä»…ä½¿ç”¨GPT-4oé«˜å‡º7.1ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™è¯æ˜äº†åœ¨ä¸ç‰ºç‰²ç”¨æˆ·éšç§çš„æƒ…å†µä¸‹ï¼Œä»»åŠ¡å¯ä»¥åœ¨å¼ºå¤§çš„ä¸å—ä¿¡ä»»çš„è¯­è¨€æ¨¡å‹å’Œè¾ƒå¼±çš„åœ°æ–¹æ¨¡å‹ä¹‹é—´åˆ†è§£å’Œåˆ’åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºä¸ªäººä»£ç†å¹¿æ³›åº”ç”¨äºæ•æ„Ÿæ•°æ®è®¿é—®ï¼Œæ¶‰åŠæ•°æ®éšç§ä¸å®‰å…¨çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>è‹æ ¼æ‹‰åº•å¼æ€ç»´é“¾æŠ€æœ¯ç»“åˆäº†å¼ºå¤§çš„è¯­è¨€æ¨¡å‹å’Œæœ¬åœ°ä¿¡ä»»æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä¸ºè¿™ä¸ªé—®é¢˜æä¾›äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ€ç»´é“¾æŠ€æœ¯å…ˆé€šè¿‡ç”Ÿæˆé€šç”¨éç§äººæŸ¥è¯¢è®¿é—®å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œç„¶åç”Ÿæˆè¯¦ç»†çš„å­æŸ¥è¯¢ä»¥æ‰§è¡ŒåŠ å¯†è¯­ä¹‰æœç´¢ç”¨æˆ·çš„ç§æœ‰æ•°æ®ã€‚</li>
<li>è‹æ ¼æ‹‰åº•å¼æ€ç»´é“¾æŠ€æœ¯åœ¨LoCoMoé•¿æ–‡æœ¬é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>æ··åˆæ¡†æ¶ç»“åˆäº†è¿œç¨‹å’Œæœ¬åœ°æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæé«˜äº†æ€§èƒ½å¹¶ä¿æŠ¤äº†ç”¨æˆ·éšç§ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¸ºæœªæ¥çš„ä»»åŠ¡åˆ†è§£å’Œæ¨¡å‹åˆ†å·¥æä¾›äº†èŒƒä¾‹ï¼Œåœ¨ä¸ç‰ºç‰²ç”¨æˆ·éšç§çš„å‰æä¸‹å®ç°äº†é«˜æ•ˆçš„è®¡ç®—èµ„æºåˆ©ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c1cb21d8acda60b62154794317cbc86f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="BIS-Reasoning-1-0-The-First-Large-Scale-Japanese-Benchmark-for-Belief-Inconsistent-Syllogistic-Reasoning"><a href="#BIS-Reasoning-1-0-The-First-Large-Scale-Japanese-Benchmark-for-Belief-Inconsistent-Syllogistic-Reasoning" class="headerlink" title="BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for   Belief-Inconsistent Syllogistic Reasoning"></a>BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for   Belief-Inconsistent Syllogistic Reasoning</h2><p><strong>Authors:Ha-Thanh Nguyen, Chaoran Liu, Qianying Liu, Hideyuki Tachibana, Su Myat Noe, Yusuke Miyao, Koichi Takeda, Sadao Kurohashi</strong></p>
<p>We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†BIS Reasoning 1.0ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡æ—¥è¯­æ•°æ®é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¿¡å¿µä¸ä¸€è‡´æ¨ç†ã€‚ä¸ä¹‹å‰çš„NeuBAROCOå’ŒJFLDæ•°æ®é›†ä¸åŒï¼Œå®ƒä»¬ä¾§é‡äºä¸€èˆ¬æˆ–ä¿¡å¿µä¸€è‡´çš„æ¨ç†ï¼ŒBIS Reasoning 1.0å¼•å…¥äº†é€»è¾‘ä¸Šæœ‰æ•ˆä½†ä¿¡å¿µä¸ä¸€è‡´çš„æ¨ç†é¢˜ï¼Œä»¥æ­ç¤ºåœ¨åŸºäºäººç±»è¯­æ–™åº“è®­ç»ƒçš„LLMä¸­å­˜åœ¨çš„æ¨ç†åè§ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬GPTæ¨¡å‹ã€Claudeæ¨¡å‹å’Œé¢†å…ˆçš„æ—¥æœ¬LLMï¼Œç»“æœæ˜¾ç¤ºæ€§èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼ŒGPT-4oçš„å‡†ç¡®ç‡è¾¾åˆ°äº†79.54%ã€‚æˆ‘ä»¬çš„åˆ†æç¡®å®šäº†å½“å‰LLMåœ¨å¤„ç†é€»è¾‘ä¸Šæœ‰æ•ˆä½†ä¿¡å¿µç›¸å†²çªè¾“å…¥æ—¶çš„å…³é”®å¼±ç‚¹ã€‚è¿™äº›å‘ç°å¯¹äºåœ¨è¯¸å¦‚æ³•å¾‹ã€åŒ»ç–—ä¿å¥å’Œç§‘å­¦æ–‡çŒ®ç­‰é«˜é£é™©é¢†åŸŸéƒ¨ç½²LLMå…·æœ‰é‡è¦çš„å½±å“æ„ä¹‰ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼ŒçœŸç›¸å¿…é¡»è¶…è¶Šç›´è§‰ä¿¡å¿µï¼Œä»¥ç¡®ä¿å®Œæ•´æ€§å’Œå®‰å…¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06955v3">PDF</a> This version includes typo corrections, added logit lens analysis for   open models, and an updated author list</p>
<p><strong>Summary</strong></p>
<p>BIS Reasoning 1.0çš„æ¨å‡ºï¼Œæ ‡å¿—ç€é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¿¡å¿µä¸ä¸€è‡´æ¨ç†çš„å¤§å‹æ—¥è¯­æ•°æ®é›†è¯ç”Ÿã€‚è¯¥æ•°æ®é›†å¼•å…¥é€»è¾‘ä¸Šåˆç†ä½†ä¿¡å¿µä¸ä¸€è‡´çš„æ¨ç†é—®é¢˜ï¼Œä»¥æ­ç¤ºè®­ç»ƒäºäººç±»è¯­æ–™åº“çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†åè§ã€‚æœ€æ–°æ¨¡å‹çš„è¡¨ç°å‚å·®ä¸é½ï¼ŒGPT-4oå‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°79.54%ã€‚åˆ†ææŒ‡å‡ºï¼Œå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é€»è¾‘ä¸Šåˆç†ä½†ä¿¡å¿µå†²çªçš„è¾“å…¥æ—¶å­˜åœ¨é‡å¤§å¼±ç‚¹ï¼Œè¿™åœ¨æ³•å¾‹ã€åŒ»ç–—å’Œç§‘å­¦æ–‡çŒ®ç­‰é«˜é£é™©é¢†åŸŸçš„éƒ¨ç½²ä¸­å…·æœ‰é‡è¦å«ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BIS Reasoning 1.0æ˜¯é¦–ä¸ªä¸“é—¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¿¡å¿µä¸ä¸€è‡´æ¨ç†çš„æ—¥è¯­æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†å¼•å…¥äº†é€»è¾‘ä¸Šåˆç†ä½†ä¿¡å¿µä¸ä¸€è‡´çš„æ¨ç†é—®é¢˜ã€‚</li>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ­¤ç±»é—®é¢˜æ—¶å­˜åœ¨æ¨ç†åè§ã€‚</li>
<li>GPT-4oåœ¨BIS Reasoning 1.0æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º79.54%ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é€»è¾‘ä¸ä¿¡å¿µå†²çªæ—¶å­˜åœ¨å¼±ç‚¹ã€‚</li>
<li>è¿™ä¸€å‘ç°å¯¹äºåœ¨é«˜é£é™©é¢†åŸŸéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¦‚æ³•å¾‹ã€åŒ»ç–—å’Œç§‘å­¦æ–‡çŒ®é¢†åŸŸã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64ef21971102060fce4a0acd650a43ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbdd550e465882b24d992a9e84cbc064.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60709b7a434fc511677607cf40483f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5111e14c64e0ff4d2f5b3d29ca8a45c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8b41c40e14f0d2a65f0a2594d7ac1f4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AudioTrust-Benchmarking-the-Multifaceted-Trustworthiness-of-Audio-Large-Language-Models"><a href="#AudioTrust-Benchmarking-the-Multifaceted-Trustworthiness-of-Audio-Large-Language-Models" class="headerlink" title="AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large   Language Models"></a>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large   Language Models</h2><p><strong>Authors:Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang, Hanjun Luo, Yingbin Jin, Xinxin Xing, Ziyang Ma, Yue Liu, Xiaojun Jia, Yifan Zhang, Junfeng Fang, Kun Wang, Yibo Yan, Haoyang Li, Yiming Li, Xiaobin Zhuang, Yang Liu, Haibo Hu, Zhizheng Wu, Xiaolin Hu, Eng-Siong Chng, XiaoFeng Wang, Wenyuan Xu, Wei Dong, Xinfeng Li</strong></p>
<p>The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio&#x2F;text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at <a target="_blank" rel="noopener" href="https://github.com/JusperLee/AudioTrust">https://github.com/JusperLee/AudioTrust</a>. </p>
<blockquote>
<p>éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰çš„å¿«é€Ÿè¿›æ­¥å’Œå¹¿æ³›åº”ç”¨è¦æ±‚æˆ‘ä»¬å¯¹å…¶å¯é æ€§æœ‰ä¸¥æ ¼çš„ç†è§£ã€‚ç„¶è€Œï¼Œå…³äºè¯„ä¼°è¿™äº›æ¨¡å‹çš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éŸ³é¢‘æ¨¡æ€ç‰¹æœ‰é£é™©çš„ç ”ç©¶ï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨æ–‡æœ¬æ¨¡æ€æˆ–åªè§£å†³æœ‰é™çš„å®‰å…¨ç»´åº¦ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘éŸ³é¢‘æ¨¡æ€çš„å›ºæœ‰ç‰¹æ€§å’Œåº”ç”¨åœºæ™¯ã€‚æˆ‘ä»¬å¼•å…¥AudioTrustâ€”â€”é¦–ä¸ªä¸“é—¨é’ˆå¯¹éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹çš„å¤šå…ƒåŒ–å¯é æ€§è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ã€‚AudioTrustä¿ƒè¿›äº†åœ¨å…­ä¸ªå…³é”®ç»´åº¦ä¸Šçš„è¯„ä¼°ï¼šå…¬å¹³æ€§ã€å¹»è§‰ã€å®‰å…¨æ€§ã€éšç§ã€ç¨³å¥æ€§å’Œè®¤è¯ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›ç»´åº¦ï¼ŒAudioTrustå›´ç»•18ä¸ªç‹¬ç‰¹çš„å®éªŒè®¾ç½®æ„å»ºã€‚å…¶æ ¸å¿ƒæ˜¯ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡4420ä¸ªéŸ³é¢‘&#x2F;æ–‡æœ¬æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æ¥è‡ªç°å®ä¸–ç•Œåœºæ™¯ï¼ˆä¾‹å¦‚æ—¥å¸¸å¯¹è¯ã€ç´§æ€¥å‘¼å«ã€è¯­éŸ³åŠ©æ‰‹äº¤äº’ï¼‰ï¼Œä¸“é—¨ç”¨äºæ¢ç´¢éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹çš„å¤šå…ƒåŒ–å¯é æ€§ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼ŒåŸºå‡†æµ‹è¯•ç²¾å¿ƒè®¾è®¡äº†9ä¸ªéŸ³é¢‘ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ç®¡é“å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œå®¢è§‚å’Œå¯é‡åŒ–çš„è¯„åˆ†ã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰å…ˆè¿›å¼€æºå’Œä¸“æœ‰éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹å„ç§é«˜é£é™©éŸ³é¢‘åœºæ™¯æ—¶çš„å¯é æ€§è¾¹ç•Œå’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥éŸ³é¢‘æ¨¡å‹çš„å®‰å…¨å¯ä¿¡éƒ¨ç½²æä¾›äº†å®è´µè§è§£ã€‚æˆ‘ä»¬çš„å¹³å°å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JusperLee/AudioTrust">https://github.com/JusperLee/AudioTrust</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16211v2">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰çš„å¤šç»´åº¦ä¿¡ä»»è¯„ä¼°æ¡†æ¶â€”â€”AudioTrustã€‚è¯¥æ¡†æ¶æ—¨åœ¨å…¨é¢è¯„ä¼°éŸ³é¢‘æ¨¡å‹çš„ä¿¡ä»»åº¦ï¼Œæ¶µç›–äº†å…¬å¹³æ€§ã€å¹»è±¡åº¦ã€å®‰å…¨æ€§ã€éšç§æ€§ã€ç¨³å¥æ€§å’Œè®¤è¯ç­‰å…­ä¸ªå…³é”®ç»´åº¦ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å®éªŒå’ŒåŒ…å«çœŸå®åœºæ™¯æ•°æ®çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒAudioTrustèƒ½å¤Ÿå®¢è§‚é‡åŒ–è¯„ä¼°ä¸åŒé£é™©åœºæ™¯ä¸­æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶æ­ç¤ºå½“å‰ä¸»æµæ¨¡å‹çš„ä¿¡ä»»åº¦è¾¹ç•Œä¸å±€é™æ€§ã€‚æ—¨åœ¨ä¸ºéŸ³é¢‘æ¨¡å‹çš„å¯é éƒ¨ç½²æä¾›æœ‰ä»·å€¼çš„å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AudioTrustæ˜¯é¦–ä¸ªé’ˆå¯¹éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰çš„å¤šç»´åº¦ä¿¡ä»»è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶æ¶µç›–äº†å…¬å¹³æ€§ã€å¹»è±¡åº¦ã€å®‰å…¨æ€§ã€éšç§æ€§ã€ç¨³å¥æ€§å’Œè®¤è¯ç­‰å…­ä¸ªå…³é”®è¯„ä¼°ç»´åº¦ã€‚</li>
<li>AudioTrustä½¿ç”¨äº†18ä¸ªä¸åŒçš„å®éªŒè®¾ç½®æ¥å…¨é¢è¯„ä¼°è¿™äº›ç»´åº¦ã€‚</li>
<li>æ¡†æ¶ä½¿ç”¨äº†ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªçœŸå®åœºæ™¯ï¼ˆå¦‚æ—¥å¸¸å¯¹è¯ã€ç´§æ€¥å‘¼å«ã€è¯­éŸ³åŠ©æ‰‹äº¤äº’ï¼‰çš„4420ä¸ªéŸ³é¢‘&#x2F;æ–‡æœ¬æ ·æœ¬ã€‚</li>
<li>è¯„ä¼°ä¸­é‡‡ç”¨äº†9ä¸ªé’ˆå¯¹éŸ³é¢‘çš„ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨äº†å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œæ¨¡å‹è¾“å‡ºçš„å®¢è§‚å’Œå¯ä¼¸ç¼©è¯„åˆ†ã€‚</li>
<li>å®éªŒç»“æœæ­ç¤ºäº†å½“å‰ä¸»æµéŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹é«˜é£é™©åœºæ™¯æ—¶çš„ä¿¡ä»»åº¦è¾¹ç•Œå’Œå±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-965a3c6c3182e37cf0fb1d8561887476.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54dda451559d9c58e799f8a262e64784.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d346566c2faa744dfd923d0964abbe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63c7c018d38cdd0a2c9fd1f3d8429317.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59513d5791662d028a87b020cb84cc40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-628cedea84d9c16b05cd0b4e5b33f78e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Code2Logic-Game-Code-Driven-Data-Synthesis-for-Enhancing-VLMs-General-Reasoning"><a href="#Code2Logic-Game-Code-Driven-Data-Synthesis-for-Enhancing-VLMs-General-Reasoning" class="headerlink" title="Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General   Reasoning"></a>Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General   Reasoning</h2><p><strong>Authors:Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang</strong></p>
<p>Visual-language Chain-of-Thought (CoT) data resources are relatively scarce compared to text-only counterparts, limiting the improvement of reasoning capabilities in Vision Language Models (VLMs). However, high-quality vision-language reasoning data is expensive and labor-intensive to annotate. To address this issue, we leverage a promising resource: game code, which naturally contains logical structures and state transition processes. Therefore, we propose Code2Logic, a novel game-code-driven approach for multimodal reasoning data synthesis. Our approach leverages Large Language Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning processes and results through code execution. Using the Code2Logic approach, we developed the GameQA dataset to train and evaluate VLMs. GameQA is cost-effective and scalable, offers controllable difficulty gradation and is diverse with 30 games and 158 tasks. Surprisingly, despite training solely on game data, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language benchmarks. Our code, dataset and models are available at <a target="_blank" rel="noopener" href="https://github.com/tongjingqi/Code2Logic">https://github.com/tongjingqi/Code2Logic</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®èµ„æºç›¸è¾ƒäºä»…æœ‰æ–‡æœ¬çš„æ•°æ®èµ„æºè€Œè¨€è¾ƒä¸ºç¨€ç¼ºï¼Œè¿™é™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¨ç†èƒ½åŠ›çš„æå‡ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡è§†è§‰è¯­è¨€æ¨ç†æ•°æ®çš„æ ‡æ³¨æˆæœ¬é«˜ä¸”åŠ³åŠ¨å¯†é›†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ä¸€ä¸ªå¾ˆæœ‰å‰æ™¯çš„èµ„æºï¼šæ¸¸æˆä»£ç ï¼Œå…¶å¤©ç„¶åŒ…å«é€»è¾‘ç»“æ„å’ŒçŠ¶æ€è½¬æ¢è¿‡ç¨‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Code2Logicï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ¸¸æˆä»£ç é©±åŠ¨çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®åˆæˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥é€‚åº”æ¸¸æˆä»£ç ï¼Œé€šè¿‡ä»£ç æ‰§è¡Œè‡ªåŠ¨è·å–æ¨ç†è¿‡ç¨‹å’Œç»“æœã€‚ä½¿ç”¨Code2Logicæ–¹æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†GameQAæ•°æ®é›†æ¥è®­ç»ƒå’Œè¯„ä¼°VLMã€‚GameQAæˆæœ¬ä½ä¸”å¯æ‰©å±•ï¼Œæä¾›å¯æ§çš„éš¾åº¦æ¢¯åº¦ï¼Œå¹¶ä¸”å…·æœ‰30ä¸ªæ¸¸æˆå’Œ158ä¸ªä»»åŠ¡çš„å¤šæ ·æ€§ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå°½ç®¡ä»…åœ¨æ¸¸æˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒVLMè¡¨ç°å‡ºäº†è·¨åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯Qwen2.5-VL-7Båœ¨7ä¸ªä¸åŒçš„è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡äº†2.33%ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tongjingqi/Code2Logic%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tongjingqi/Code2Logicæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13886v2">PDF</a> 63 pages, 23 figures, submitted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€é“¾æ€ç»´ï¼ˆCoTï¼‰æ•°æ®èµ„æºç›¸è¾ƒäºæ–‡æœ¬åªæœ‰èµ„æºè¾ƒä¸ºåŒ®ä¹ï¼Œåˆ¶çº¦äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¨ç†èƒ½åŠ›çš„æå‡ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†Code2Logicâ€”â€”ä¸€ç§åˆ©ç”¨æ¸¸æˆä»£ç é©±åŠ¨çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®åˆæˆæ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”æ¸¸æˆä»£ç ï¼Œå®ç°é€šè¿‡ä»£ç æ‰§è¡Œè‡ªåŠ¨è·å–æ¨ç†è¿‡ç¨‹å’Œç»“æœã€‚åŸºäºæ­¤æ–¹æ³•ï¼Œå›¢é˜Ÿå¼€å‘äº†GameQAæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°VLMsã€‚GameQAå…·æœ‰æˆæœ¬æ•ˆç›Šé«˜ã€å¯è§„æ¨¡åŒ–ã€éš¾åº¦å¯æ§ä»¥åŠä»»åŠ¡å¤šæ ·æ€§ç­‰ç‰¹ç‚¹ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿åœ¨ä»…ä½¿ç”¨æ¸¸æˆæ•°æ®è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒVLMsä»å±•ç°å‡ºè·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå…¶ä¸­Qwen2.5-VL-7Båœ¨7ç§ä¸åŒçš„è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½æå‡äº†2.33%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€é“¾æ€ç»´ï¼ˆCoTï¼‰æ•°æ®èµ„æºçš„ç¨€ç¼ºæ€§åˆ¶çº¦äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†èƒ½åŠ›æå‡ã€‚</li>
<li>Code2Logicæ–¹æ³•åˆ©ç”¨æ¸¸æˆä»£ç ä¸­çš„é€»è¾‘ç»“æ„å’ŒçŠ¶æ€è½¬æ¢è¿‡ç¨‹ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†æ•°æ®åˆæˆæä¾›æ–°æ€è·¯ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”æ¸¸æˆä»£ç ï¼Œå¯è‡ªåŠ¨è·å–æ¨ç†è¿‡ç¨‹å’Œç»“æœã€‚</li>
<li>GameQAæ•°æ®é›†å…·æœ‰æˆæœ¬æ•ˆç›Šé«˜ã€å¯è§„æ¨¡åŒ–ã€éš¾åº¦å¯æ§ä»¥åŠä»»åŠ¡å¤šæ ·æ€§ç­‰ç‰¹ç‚¹ã€‚</li>
<li>GameQAæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°VLMsã€‚</li>
<li>ä»…ä½¿ç”¨æ¸¸æˆæ•°æ®è¿›è¡Œè®­ç»ƒçš„VLMså±•ç°å‡ºè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-222445b92e5dd147e80e7a2ffd7232aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21e6704457998e75ad0cbe0b9811e9e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df6268abb7364ad36b920df2727e6d81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c18fa5fff01bcc866b9f8ebc0c34894a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b17819eff357c29a67fe45ce6a580981.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b66e3f8995718b28aae0d5a39d1814bd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CSC-SQL-Corrective-Self-Consistency-in-Text-to-SQL-via-Reinforcement-Learning"><a href="#CSC-SQL-Corrective-Self-Consistency-in-Text-to-SQL-via-Reinforcement-Learning" class="headerlink" title="CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement   Learning"></a>CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement   Learning</h2><p><strong>Authors:Lei Sheng, Shuai-Shuai Xu</strong></p>
<p>Large language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particular, test-time scaling techniques such as Self-Consistency and Self-Correction can enhance SQL generation accuracy by increasing computational effort during inference. However, these methods have notable limitations: Self-Consistency may select suboptimal outputs despite majority votes, while Self-Correction typically addresses only syntactic errors. To leverage the strengths of both approaches, we propose CSC-SQL, a novel method that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two most frequently occurring outputs from parallel sampling and feeds them into a merge revision model for correction. Additionally, we employ the Group Relative Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and revision models via reinforcement learning, significantly enhancing output quality. Experimental results confirm the effectiveness and generalizability of CSC-SQL. On the BIRD private test set, our 7B model achieves 71.72% execution accuracy, while the 32B model achieves 73.67%. The code has been open sourced at <a target="_blank" rel="noopener" href="https://github.com/CycloneBoy/csc_sql">https://github.com/CycloneBoy/csc_sql</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°†å…³äºå…³ç³»æ•°æ®åº“çš„è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆSQLæŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæµ‹è¯•æ—¶çš„ç¼©æ”¾æŠ€æœ¯ï¼Œå¦‚è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSelf-Consistencyï¼‰å’Œè‡ªæˆ‘ä¿®æ­£ï¼ˆSelf-Correctionï¼‰ï¼Œå¯ä»¥é€šè¿‡å¢åŠ æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å·¥ä½œé‡æ¥æé«˜SQLç”Ÿæˆå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æœ‰æ˜æ˜¾çš„å±€é™æ€§ï¼šè‡ªæˆ‘ä¸€è‡´æ€§å¯èƒ½ä¼šåœ¨å¤šæ•°æŠ•ç¥¨çš„æƒ…å†µä¸‹é€‰æ‹©å‡ºæ¬¡ä¼˜è¾“å‡ºï¼Œè€Œè‡ªæˆ‘ä¿®æ­£é€šå¸¸åªé’ˆå¯¹è¯­æ³•é”™è¯¯ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†CSC-SQLï¼Œä¸€ç§å°†è‡ªæˆ‘ä¸€è‡´æ€§å’Œè‡ªæˆ‘ä¿®æ­£ç›¸ç»“åˆçš„æ–°æ–¹æ³•ã€‚CSC-SQLé€‰æ‹©å¹¶è¡Œé‡‡æ ·ä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„ä¸¤ä¸ªè¾“å‡ºï¼Œå¹¶å°†å…¶è¾“å…¥åˆ°ä¸€ä¸ªä¿®æ­£åˆå¹¶æ¨¡å‹ä¸­è¿›è¡Œä¿®æ­£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹SQLç”Ÿæˆå’Œä¿®è®¢æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†è¾“å‡ºè´¨é‡ã€‚å®éªŒç»“æœè¯å®äº†CSC-SQLçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚åœ¨BIRDç§æœ‰æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„70äº¿æ¨¡å‹è¾¾åˆ°äº†71.72%çš„æ‰§è¡Œå‡†ç¡®ç‡ï¼Œè€Œ320äº¿æ¨¡å‹è¾¾åˆ°äº†73.67%ã€‚ä»£ç å·²å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/CycloneBoy/csc_sql%E3%80%82">https://github.com/CycloneBoy/csc_sqlã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13271v2">PDF</a> 25 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆSQLæŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯ï¼Œå¦‚è‡ªæˆ‘ä¸€è‡´æ€§åŠè‡ªæˆ‘æ ¡æ­£ï¼Œå¯é€šè¿‡å¢åŠ æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å·¥ä½œé‡æ¥æé«˜SQLç”Ÿæˆå‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æœ‰å±€é™æ€§ï¼šè‡ªæˆ‘ä¸€è‡´æ€§å¯èƒ½ä¼šé€‰æ‹©æ¬¡ä¼˜è¾“å‡ºï¼Œè€Œè‡ªæˆ‘æ ¡æ­£ä¸»è¦è§£å†³çš„æ˜¯è¯­æ³•é”™è¯¯ã€‚ä¸ºç»“åˆè¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†CSC-SQLæ–°æ–¹æ³•ï¼Œå®ƒç»“åˆäº†è‡ªæˆ‘ä¸€è‡´æ€§ä¸è‡ªæˆ‘æ ¡æ­£ã€‚CSC-SQLä»å¹¶è¡Œé‡‡æ ·ä¸­é€‰æ‹©å‡ºç°æœ€é¢‘ç¹çš„ä¸¤ä¸ªè¾“å‡ºï¼Œå¹¶å°†å…¶è¾“å…¥ä¿®æ­£åˆå¹¶æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ç®—æ³•å¯¹SQLç”Ÿæˆå’Œä¿®è®¢æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜è¾“å‡ºè´¨é‡ã€‚å®éªŒç»“æœè¯å®äº†CSC-SQLçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºSQLæŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•æ—¶æ‰©å±•æŠ€æœ¯å¦‚è‡ªæˆ‘ä¸€è‡´æ€§åŠè‡ªæˆ‘æ ¡æ­£èƒ½æå‡SQLç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚</li>
<li>è‡ªæˆ‘ä¸€è‡´æ€§å’Œè‡ªæˆ‘æ ¡æ­£æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œåˆ†åˆ«å¯èƒ½é€‰æ‹©æ¬¡ä¼˜è¾“å‡ºå’Œä¸»è¦è§£å†³è¯­æ³•é”™è¯¯ã€‚</li>
<li>CSC-SQLç»“åˆäº†è‡ªæˆ‘ä¸€è‡´æ€§å’Œè‡ªæˆ‘æ ¡æ­£ï¼Œé€šè¿‡é€‰æ‹©æœ€é¢‘ç¹çš„è¾“å‡ºå¹¶è¿›è¡Œä¿®æ­£æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ç®—æ³•å¯¹SQLç”Ÿæˆå’Œä¿®è®¢æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜è¾“å‡ºè´¨é‡ã€‚</li>
<li>CSC-SQLåœ¨BIRDç§äººæµ‹è¯•é›†ä¸Šå–å¾—äº†71.72%å’Œ73.67%çš„æ‰§è¡Œå‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6055af19bb838a537259524d05e72da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f42c5a1deea0635f3e811600fcd32ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a79bbc7cd0980927b53e9756da69c3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4210cdd7bb5bfe10a869313a846b1c6c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Reasoning-by-Superposition-A-Theoretical-Perspective-on-Chain-of-Continuous-Thought"><a href="#Reasoning-by-Superposition-A-Theoretical-Perspective-on-Chain-of-Continuous-Thought" class="headerlink" title="Reasoning by Superposition: A Theoretical Perspective on Chain of   Continuous Thought"></a>Reasoning by Superposition: A Theoretical Perspective on Chain of   Continuous Thought</h2><p><strong>Authors:Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable performance in many applications, including challenging reasoning problems via chain-of-thoughts (CoTs) techniques that generate &#96;&#96;thinking tokensâ€™â€™ before answering the questions. While existing theoretical works demonstrate that CoTs with discrete tokens boost the capability of LLMs, recent work on continuous CoTs lacks a theoretical understanding of why it outperforms discrete counterparts in various reasoning tasks such as directed graph reachability, a fundamental graph reasoning problem that includes many practical domain applications as special cases. In this paper, we prove that a two-layer transformer with $D$ steps of continuous CoTs can solve the directed graph reachability problem, where $D$ is the diameter of the graph, while the best known result of constant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps where $n$ is the number of vertices ($D&lt;n$). In our construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoTs must choose a single path sampled from the superposition state, which leads to sequential search that requires many more steps and may be trapped into local solutions. We also performed extensive experiments to verify that our theoretical construction aligns well with the empirical solution obtained via training dynamics. Notably, encoding of multiple search frontiers as a superposition state automatically emerges in training continuous CoTs, without explicit supervision to guide the model to explore multiple paths simultaneously. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šåº”ç”¨ä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é€šè¿‡æ€ç»´é“¾ï¼ˆCoTsï¼‰æŠ€æœ¯è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†é—®é¢˜ï¼Œåœ¨å›ç­”é—®é¢˜ä¹‹å‰ä¼šç”Ÿæˆâ€œæ€ç»´æ ‡è®°â€ã€‚è™½ç„¶ç°æœ‰çš„ç†è®ºå·¥ä½œè¡¨æ˜ï¼Œä½¿ç”¨ç¦»æ•£æ ‡è®°çš„CoTså¯ä»¥å¢å¼ºLLMçš„åŠŸèƒ½ï¼Œä½†å…³äºè¿ç»­CoTsçš„æœ€æ–°å·¥ä½œç¼ºä¹å¯¹å…¶åœ¨è¯¸å¦‚å®šå‘å›¾å¯è¾¾æ€§ä¹‹ç±»çš„å„ç§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç¦»æ•£å¯¹åº”ç‰©çš„ç†è®ºç†è§£ã€‚å®šå‘å›¾å¯è¾¾æ€§æ˜¯ä¸€ä¸ªåŸºæœ¬çš„å›¾å½¢æ¨ç†é—®é¢˜ï¼ŒåŒ…æ‹¬è®¸å¤šå®é™…åº”ç”¨é¢†åŸŸçš„ç‰¹æ®Šæƒ…å†µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªå…·æœ‰è¿ç»­CoTsçš„ä¸¤å±‚transformerå¯ä»¥è§£å†³å®šå‘å›¾å¯è¾¾æ€§é—®é¢˜ï¼Œå…¶ä¸­Dæ˜¯å›¾çš„ç›´å¾„ï¼Œè€Œå…·æœ‰ç¦»æ•£CoTsçš„æ’å®šæ·±åº¦å˜å‹å™¨å·²çŸ¥çš„æœ€ä½³ç»“æœéœ€è¦O(n^2)è§£ç æ­¥éª¤ï¼Œå…¶ä¸­næ˜¯é¡¶ç‚¹æ•°ï¼ˆD&lt;nï¼‰ã€‚åœ¨æˆ‘ä»¬çš„æ„å»ºä¸­ï¼Œæ¯ä¸ªè¿ç»­çš„æ€ç»´å‘é‡éƒ½æ˜¯ä¸€ä¸ªå åŠ æ€ï¼Œå¯ä»¥åŒæ—¶ç¼–ç å¤šä¸ªæœç´¢å‰æ²¿ï¼ˆå³å¹¶è¡Œå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰ï¼‰ï¼Œè€Œç¦»æ•£CoTså¿…é¡»ä»å åŠ æ€ä¸­é€‰æ‹©ä¸€æ¡è·¯å¾„ï¼Œè¿™å¯¼è‡´äº†é¡ºåºæœç´¢éœ€è¦æ›´å¤šçš„æ­¥éª¤ï¼Œå¹¶ä¸”å¯èƒ½é™·å…¥å±€éƒ¨è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯æˆ‘ä»¬çš„ç†è®ºæ„å»ºä¸é€šè¿‡è®­ç»ƒåŠ¨æ€è·å¾—çš„ç»éªŒè§£å†³æ–¹æ¡ˆå»åˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®­ç»ƒè¿ç»­CoTsè¿‡ç¨‹ä¸­ï¼Œå°†å¤šä¸ªæœç´¢å‰æ²¿ç¼–ç ä¸ºå åŠ æ€ä¼šè‡ªåŠ¨å‡ºç°ï¼Œè€Œæ— éœ€æ˜ç¡®çš„ç›‘ç£æ¥æŒ‡å¯¼æ¨¡å‹åŒæ—¶æ¢ç´¢å¤šæ¡è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12514v2">PDF</a> 26 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTsï¼‰æŠ€æœ¯å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è§£å†³æ¨ç†é—®é¢˜æ–¹é¢ã€‚ç¦»æ•£æ€ç»´é“¾å·²æœ‰ç†è®ºç ”ç©¶åŸºç¡€ï¼Œè€Œè¿ç»­æ€ç»´é“¾åˆ™ç¼ºä¹å¯¹å…¶ç†è®ºä¼˜åŠ¿çš„ç†è§£ã€‚æœ¬ç ”ç©¶è¯æ˜äº†è¿ç»­æ€ç»´é“¾åœ¨ä¸¤å±‚çš„transformerç»“æ„ä¸­çš„ä¼˜è¶Šæ€§ï¼Œèƒ½æ›´é«˜æ•ˆåœ°è§£å†³å®šå‘å›¾å¯è¾¾æ€§é—®é¢˜ã€‚æœ¬ç ”ç©¶è¿˜å‘ç°è¿ç»­æ€ç»´å‘é‡æ˜¯å åŠ çŠ¶æ€ï¼Œèƒ½åŒæ—¶ç¼–ç å¤šä¸ªæœç´¢è¾¹ç•Œï¼Œè€Œç¦»æ•£æ€ç»´é“¾åˆ™éœ€ä»å åŠ çŠ¶æ€ä¸­é€‰å–å•ä¸€è·¯å¾„ï¼Œå¯¼è‡´æœç´¢æ­¥éª¤å¢å¤šä¸”å¯èƒ½é™·å…¥å±€éƒ¨è§£ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç†è®ºæ„é€ ä¸é€šè¿‡è®­ç»ƒåŠ¨æ€è·å¾—çš„å®è¯è§£å†³æ–¹æ¡ˆé«˜åº¦ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTsï¼‰æŠ€æœ¯è§£å†³æ¨ç†é—®é¢˜è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç¦»æ•£æ€ç»´é“¾å·²æœ‰ç†è®ºç ”ç©¶åŸºç¡€ï¼Œä½†è¿ç»­æ€ç»´é“¾çš„ç†è®ºä¼˜åŠ¿å°šä¸æ¸…æ¥šã€‚</li>
<li>ä¸¤å±‚transformerç»“æ„ä¸­çš„è¿ç»­æ€ç»´é“¾èƒ½æ›´é«˜æ•ˆåœ°è§£å†³å®šå‘å›¾å¯è¾¾æ€§é—®é¢˜ã€‚</li>
<li>è¿ç»­æ€ç»´å‘é‡æ˜¯å åŠ çŠ¶æ€ï¼Œèƒ½åŒæ—¶ç¼–ç å¤šä¸ªæœç´¢è¾¹ç•Œã€‚</li>
<li>ç¦»æ•£æ€ç»´é“¾éœ€è¦ä»å åŠ çŠ¶æ€ä¸­é€‰å–å•ä¸€è·¯å¾„ï¼Œå¯¼è‡´æœç´¢æ­¥éª¤å¢å¤šä¸”å¯èƒ½é™·å…¥å±€éƒ¨è§£ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œç†è®ºæ„é€ ä¸é€šè¿‡è®­ç»ƒåŠ¨æ€è·å¾—çš„å®è¯è§£å†³æ–¹æ¡ˆä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-358bc4991e42e1f01dcd64d6f479c742.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3ba3101064cad0058560df95f34c248.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d52b9346f498b5109a03cc68cbf06d7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1925bbc90b98e577800cb23125d17d4.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-06/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-06/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-06/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f057b78d1fd5aa808ab7d2d292286af9.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-06  Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge   Graph Guided Distractor Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-05/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d0c2a7cc29fee5319b749d83188c03e7.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-05  AnyI2V Animating Any Conditional Image with Motion Control
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
