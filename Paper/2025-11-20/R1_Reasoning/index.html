<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  UniGen-1.5 Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b2103e0612f304f8fda470659851e2bf')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-20-æ›´æ–°"><a href="#2025-11-20-æ›´æ–°" class="headerlink" title="2025-11-20 æ›´æ–°"></a>2025-11-20 æ›´æ–°</h1><h2 id="UniGen-1-5-Enhancing-Image-Generation-and-Editing-through-Reward-Unification-in-Reinforcement-Learning"><a href="#UniGen-1-5-Enhancing-Image-Generation-and-Editing-through-Reward-Unification-in-Reinforcement-Learning" class="headerlink" title="UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning"></a>UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</h2><p><strong>Authors:Rui Tian, Mingfei Gao, Haiming Gang, Jiasen Lu, Zhe Gan, Yinfei Yang, Zuxuan Wu, Afshin Dehghan</strong></p>
<p>We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†UniGen-1.5ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç”¨äºé«˜çº§å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚åŸºäºUniGenï¼Œæˆ‘ä»¬å…¨é¢å¢å¼ºäº†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹ï¼ŒåŠ å¼ºäº†å›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶è§£é”äº†å¼ºå¤§çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥ï¼Œé€šè¿‡å…±äº«å¥–åŠ±æ¨¡å‹ï¼Œå…±åŒæé«˜å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå›¾åƒç¼–è¾‘æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†è½»é‡çº§çš„ç¼–è¾‘æŒ‡ä»¤å¯¹é½é˜¶æ®µï¼Œè¿™å¤§å¤§æé«˜äº†ç¼–è¾‘æŒ‡ä»¤çš„ç†è§£ï¼Œå¯¹äºRLè®­ç»ƒçš„æˆåŠŸè‡³å…³é‡è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniGen-1.5åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒUniGen-1.5åœ¨GenEvalå’ŒImgEditä¸Šçš„æ€»ä½“å¾—åˆ†åˆ†åˆ«ä¸º0.89å’Œ4.31ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›æ¨¡å‹BAGELï¼Œè¾¾åˆ°äº†ç±»ä¼¼ä¸“æœ‰æ¨¡å‹GPT-Image-1çš„æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14760v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>UniGen-1.5æ˜¯ä¸€æ¬¾ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé«˜çº§å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚è¯¥æ¨¡å‹åŸºäºUniGenæ„å»ºï¼Œå…¨é¢å¢å¼ºäº†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹ï¼Œæé«˜äº†å›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶è§£é”äº†å¼ºå¤§çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚é€šè¿‡å…±äº«å¥–åŠ±æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œå¯ä»¥åŒæ—¶æé«˜å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§è½»é‡çº§çš„ç¼–è¾‘æŒ‡ä»¤å¯¹é½é˜¶æ®µï¼Œå¯ä»¥æ˜¾è‘—æé«˜å¯¹ç¼–è¾‘æŒ‡ä»¤çš„ç†è§£ï¼Œè¿™æ˜¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆåŠŸçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniGen-1.5åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚ç‰¹åˆ«åœ°ï¼ŒUniGen-1.5åœ¨GenEvalå’ŒImgEditä¸Šçš„æ€»ä½“å¾—åˆ†åˆ†åˆ«ä¸º0.89å’Œ4.31ï¼Œè¶…è¿‡äº†ç›®å‰å…ˆè¿›çš„æ¨¡å‹å¦‚BAGELï¼Œå¹¶è¾¾åˆ°äº†ä¸ä¸“æœ‰æ¨¡å‹å¦‚GPT-Image-1ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniGen-1.5æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé«˜çº§å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚</li>
<li>è¯¥æ¨¡å‹å¢å¼ºäº†å¯¹å›¾åƒçš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„å›¾åƒç¼–è¾‘åŠŸèƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å…±äº«å¥–åŠ±æ¨¡å‹ï¼ŒåŒæ—¶æé«˜å›¾åƒç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„ç¼–è¾‘æŒ‡ä»¤å¯¹é½é˜¶æ®µï¼Œæé«˜äº†å¯¹ç¼–è¾‘æŒ‡ä»¤çš„ç†è§£ã€‚</li>
<li>UniGen-1.5åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼ŒGenEvalå’ŒImgEditæ€»ä½“å¾—åˆ†åˆ†åˆ«ä¸º0.89å’Œ4.31ã€‚</li>
<li>UniGen-1.5è¶…è¿‡äº†ç›®å‰å…ˆè¿›çš„æ¨¡å‹å¦‚BAGELã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bcffe7166321ee2720fe0d6d9ccde13" align="middle">
<img src="https://picx.zhimg.com/v2-5f214815f78c95d785997a2538e94d6a" align="middle">
<img src="https://picx.zhimg.com/v2-c5d46179b29a289b72800d25edf60694" align="middle">
<img src="https://picx.zhimg.com/v2-db2b0ca4fdc88f292d1625b6280c4acb" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Look-Ahead-Reasoning-on-Learning-Platforms"><a href="#Look-Ahead-Reasoning-on-Learning-Platforms" class="headerlink" title="Look-Ahead Reasoning on Learning Platforms"></a>Look-Ahead Reasoning on Learning Platforms</h2><p><strong>Authors:Haiqing Zhu, Tijana Zrnic, Celestine Mendler-DÃ¼nner</strong></p>
<p>On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes, effectively contesting the platformâ€™s predictions. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and â€“ at scale â€“ impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learnerâ€™s and the usersâ€™ utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action.</p>
<blockquote>
<p>åœ¨è®¸å¤šå­¦ä¹ å¹³å°ä¸Šï¼Œå¼•å¯¼æ¨¡å‹è®­ç»ƒçš„ä¼˜åŒ–æ ‡å‡†åæ˜ çš„æ˜¯è®¾è®¡è€…çš„ä¼˜å…ˆäº‹é¡¹ï¼Œè€Œéå…¶æ‰€å½±å“ä¸ªä½“çš„ä¼˜å…ˆäº‹é¡¹ã€‚å› æ­¤ï¼Œç”¨æˆ·å¯èƒ½ä¼šé‡‡å–ç­–ç•¥æ€§è¡Œä¸ºä»¥è·å¾—æ›´æœ‰åˆ©çš„ç»“æœï¼Œä»è€Œæœ‰æ•ˆåœ°è´¨ç–‘å¹³å°çš„é¢„æµ‹ã€‚å°½ç®¡è¿‡å»çš„ç ”ç©¶å·²ç»ç ”ç©¶äº†å­¦ä¹ å¹³å°ä¸Šçš„ç­–ç•¥æ€§è¡Œä¸ºï¼Œä½†é‡ç‚¹å¤§å¤šæ”¾åœ¨å¯¹éƒ¨ç½²æ¨¡å‹çš„æˆ˜ç•¥ååº”ä¸Šï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°å…¶ä»–ç”¨æˆ·çš„è¡Œä¸ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå‰ç»æ€§æ¨ç†è€ƒè™‘åˆ°äº†ç”¨æˆ·è¡Œä¸ºæ˜¯ç›¸äº’å…³è”çš„ï¼Œè€Œä¸”åœ¨å¤§è§„æ¨¡ä¸Šä¼šå½±å“æœªæ¥çš„é¢„æµ‹ã€‚åœ¨æœ¬æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ­£å¼æå‡ºè¡Œä¸ºç»æµå­¦ä¸­çš„kçº§æ€ç»´æ¦‚å¿µï¼Œç”¨æˆ·é€šè¿‡å‰ç»æ€§æ€è€ƒæ¥è¶…è¶ŠåŒé¾„äººã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè™½ç„¶è¾¾åˆ°å‡è¡¡çš„æ”¶æ•›é€Ÿåº¦åŠ å¿«äº†ï¼Œä½†å‡è¡¡çŠ¶æ€ä»ç„¶ä¿æŒä¸å˜ï¼Œä»é•¿è¿œæ¥çœ‹ï¼Œå¹¶æ²¡æœ‰ç»™ä¸ªäººå¸¦æ¥é«˜æ°´å¹³æ€è€ƒçš„å¥½å¤„ã€‚ç„¶åï¼Œæˆ‘ä»¬å…³æ³¨é›†ä½“æ¨ç†ï¼Œç”¨æˆ·é€šè¿‡ä¼˜åŒ–å…¶è”åˆè¡ŒåŠ¨å¯¹æ¨¡å‹äº§ç”Ÿçš„å½±å“æ¥åè°ƒè¡ŒåŠ¨ã€‚é€šè¿‡å¯¹é›†ä½“è¡Œä¸ºä¸è‡ªç§è¡Œä¸ºçš„å¯¹æ¯”ï¼Œæˆ‘ä»¬åˆ†æäº†åè°ƒçš„åˆ©å¼Šï¼›åœ¨å­¦ä¹ è€…å’Œç”¨æˆ·æ•ˆç”¨ä¹‹é—´å‡ºç°äº†ä¸€ç§æ–°çš„å¯¹é½æ¦‚å¿µï¼Œè¿™æ˜¯å…³é”®ã€‚æˆ‘ä»¬è®¨è®ºäº†ä¸æˆ˜ç•¥åˆ†ç±»ã€ç»©æ•ˆé¢„æµ‹å’Œç®—æ³•é›†ä½“è¡ŒåŠ¨ç­‰å‡ ä¸ªç›¸å…³æ•°å­¦æ¡†æ¶çš„è”ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14745v1">PDF</a> accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨å­¦ä¹ å¹³å°ä¸Šçš„ç”¨æˆ·è¡Œä¸ºç­–ç•¥ï¼ŒæŒ‡å‡ºå¹³å°ä¼˜åŒ–æ ‡å‡†å¾€å¾€åæ˜ è®¾è®¡è€…è€Œéå—å½±å“ä¸ªä½“çš„ä¼˜å…ˆäº‹é¡¹ã€‚ç”¨æˆ·ä¸ºè·å¾—æ›´æœ‰åˆ©çš„ç»“æœï¼Œå¯èƒ½ä¼šé‡‡å–æˆ˜ç•¥æ€§è¡Œä¸ºæ¥å¯¹æŠ—å¹³å°é¢„æµ‹ã€‚æ–‡ç« å¼ºè°ƒå‰ç»æ€§æ¨ç†çš„é‡è¦æ€§ï¼Œå³ç”¨æˆ·è¡ŒåŠ¨é—´çš„å…³è”æ€§åŠå…¶å¯¹æœªæ¥é¢„æµ‹çš„å½±å“ã€‚æ–‡ä¸­æå‡ºäº†â€œlevel-$k$æ€è€ƒâ€æ¦‚å¿µï¼Œä½†å³ä¾¿åŠ å¿«è¾¾åˆ°å¹³è¡¡ï¼Œé•¿è¿œçœ‹å¯¹ä¸ªäººå¹¶æ— ç›Šå¤„ã€‚éšåèšç„¦äºé›†ä½“æ¨ç†ï¼Œç”¨æˆ·é€šè¿‡ä¼˜åŒ–è”åˆå½±å“æ¨¡å‹é‡‡å–åè°ƒè¡ŒåŠ¨ã€‚å¯¹æ¯”é›†ä½“ä¸è‡ªç§è¡Œä¸ºï¼Œæ­ç¤ºåè°ƒçš„åˆ©å¼Šï¼Œå¹¶å‡ºç°å­¦ä¹ è€…ä¸ç”¨æˆ·éœ€æ±‚é—´çš„æ–°å¯¹é½æ¦‚å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ å¹³å°çš„ä¼˜åŒ–æ ‡å‡†å¾€å¾€åæ˜ è®¾è®¡è€…è€Œéå—å½±å“ä¸ªä½“çš„ä¼˜å…ˆæƒã€‚</li>
<li>ç”¨æˆ·ä¸ºè·å¾—æœ‰åˆ©ç»“æœï¼Œä¼šé‡‡å–æˆ˜ç•¥æ€§è¡Œä¸ºå¯¹æŠ—å¹³å°é¢„æµ‹ã€‚</li>
<li>ç”¨æˆ·çš„è¡ŒåŠ¨æ˜¯ç›¸äº’å…³è”çš„ï¼Œå¹¶å¯¹æœªæ¥é¢„æµ‹äº§ç”Ÿå½±å“ï¼Œè¿™è¢«ç§°ä¸ºå‰ç»æ€§æ¨ç†ã€‚</li>
<li>â€œlevel-$k$æ€è€ƒâ€è™½æœ‰åŠ©äºåŠ é€Ÿè¾¾åˆ°å¹³è¡¡ï¼Œä½†å¯¹ä¸ªä½“é•¿è¿œæ¥çœ‹æ²¡æœ‰å®é™…å¥½å¤„ã€‚</li>
<li>é›†ä½“æ¨ç†é‡è§†ç”¨æˆ·é—´çš„åè°ƒè¡ŒåŠ¨ï¼Œé€šè¿‡ä¼˜åŒ–å¯¹æ¨¡å‹çš„è”åˆå½±å“æ¥å®ç°ã€‚</li>
<li>å¯¹æ¯”é›†ä½“ä¸è‡ªç§è¡Œä¸ºæ­ç¤ºäº†åè°ƒçš„åˆ©å¼Šï¼Œå¼•å‡ºæ–°çš„å­¦ä¹ è€…ä¸ç”¨æˆ·éœ€æ±‚å¯¹é½æ¦‚å¿µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5b4bcb6b2a23450a65310e2461ecbc2" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="From-Random-Determinants-to-the-Ground-State"><a href="#From-Random-Determinants-to-the-Ground-State" class="headerlink" title="From Random Determinants to the Ground State"></a>From Random Determinants to the Ground State</h2><p><strong>Authors:Hao Zhang, Matthew Otten</strong></p>
<p>Accurate quantum many-body calculations often depend on reliable reference states or good human-designed ansÃ¤tze, yet these sources of knowledge can become unreliable in hard problems like strongly correlated systems. We introduce the Trimmed Configuration Interaction (TrimCI) method, a prior-knowledge-free algorithm that builds accurate ground states directly from random Slater determinants. TrimCI iteratively expands the variational space and trims away unimportant states, allowing a random initial core to self-refine into an accurate approximation of exact ground state. Across challenging benchmarks, TrimCI achieves state-of-the-art accuracy with strikingly efficiency gains of several orders of magnitude. For [4Fe-4S] cluster, it matches recent quantum computing results with $10^6$-fold fewer determinants and CPU-hours. For the nitrogenase P-cluster, it matches selected-CI accuracy using $10^5$-fold fewer determinants. For $8\times8$ Hubbard model, it recovers over $99%$ of the ground-state energy using only $10^{-28}$ of the Hilbert space. In some regimes, TrimCI attains orders-of-magnitude higher accuracy than AFQMC method. These results demonstrate that high-accuracy many-body ground states can be discovered directly from random determinants, establishing TrimCI as a prior-knowledge-free, accurate and highly efficient framework for quantum many-body systems. The compact explicit wavefunctions it produces further enable direct and rapid evaluation of observables.</p>
<blockquote>
<p>ç²¾ç¡®çš„é‡å­å¤šä½“è®¡ç®—å¾€å¾€ä¾èµ–äºå¯é çš„å‚è€ƒæ€æˆ–è‰¯å¥½çš„äººå·¥è®¾è®¡çš„è¯•æ¢å‡½æ•°ï¼Œä½†åœ¨å¼ºå…³è”ç³»ç»Ÿç­‰éš¾é¢˜ä¸­ï¼Œè¿™äº›çŸ¥è¯†æ¥æºå¯èƒ½å˜å¾—ä¸å¯é ã€‚æˆ‘ä»¬å¼•å…¥äº†Trimmedé…ç½®äº¤äº’ï¼ˆTrimCIï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€å…ˆéªŒçŸ¥è¯†çš„ç®—æ³•ï¼Œå®ƒç›´æ¥ä»éšæœºSlaterè¡Œåˆ—å¼æ„å»ºå‡†ç¡®çš„åŸºæ€ã€‚TrimCIé€šè¿‡è¿­ä»£æ‰©å±•å˜åˆ†ç©ºé—´å¹¶å‰”é™¤ä¸é‡è¦çŠ¶æ€ï¼Œä½¿å¾—éšæœºåˆå§‹æ ¸å¿ƒèƒ½å¤Ÿè‡ªæˆ‘å®Œå–„ï¼Œæˆä¸ºç²¾ç¡®åŸºæ€çš„å‡†ç¡®è¿‘ä¼¼ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTrimCIå®ç°äº†æœ€æ–°æŠ€æœ¯çš„å‡†ç¡®æ€§ï¼Œå¹¶å®ç°äº†å‡ ä¸ªæ•°é‡çº§çš„æ•ˆç‡æå‡ã€‚å¯¹äº[4Fe-4S]ç°‡ï¼Œå®ƒä¸æœ€æ–°çš„é‡å­è®¡ç®—ç»“æœç›¸åŒ¹é…ï¼Œä½¿ç”¨çš„è¡Œåˆ—å¼å’ŒCPUå°æ—¶æ•°å‡å°‘äº†$10^6$å€ã€‚å¯¹äºæ°®é…¶Pç°‡ï¼Œå®ƒä½¿ç”¨è¾ƒå°‘çš„è¡Œåˆ—å¼è¾¾åˆ°äº†é€‰å®šCIçš„å‡†ç¡®æ€§ã€‚å¯¹äº$8\times8$çš„Hubbardæ¨¡å‹ï¼Œå®ƒæ¢å¤äº†99%ä»¥ä¸Šçš„åŸºæ€èƒ½é‡ï¼Œä»…ä½¿ç”¨äº†Hilbertç©ºé—´çš„$10^{-28}$éƒ¨åˆ†ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒTrimCIçš„ç²¾åº¦æ¯”AFQMCæ–¹æ³•é«˜å‡ºæ•°ä¸ªæ•°é‡çº§ã€‚è¿™äº›ç»“æœè¯æ˜ï¼Œå¯ä»¥ç›´æ¥ä»éšæœºè¡Œåˆ—å¼ä¸­å‘ç°é«˜å‡†ç¡®åº¦çš„å¤šä½“åŸºæ€ï¼Œç¡®ç«‹äº†TrimCIä½œä¸ºä¸€ä¸ªæ— éœ€å…ˆéªŒçŸ¥è¯†ã€å‡†ç¡®ä¸”é«˜æ•ˆçš„é‡å­å¤šä½“ç³»ç»Ÿæ¡†æ¶ã€‚å®ƒäº§ç”Ÿçš„ç´§å‡‘æ˜¾å¼æ³¢å‡½æ•°å¯ä»¥è¿›ä¸€æ­¥å®ç°è§‚æµ‹é‡çš„ç›´æ¥å’Œå¿«é€Ÿè¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14734v1">PDF</a> 13 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼šæ— å…ˆéªŒçŸ¥è¯†çš„TrimCIæ–¹æ³•é€šè¿‡ç›´æ¥åˆ©ç”¨éšæœºSlaterè¡Œåˆ—å¼æ„å»ºç²¾ç¡®åŸºæ€ï¼Œè§£å†³äº†å¼ºå…³è”ç³»ç»Ÿç­‰éš¾é¢˜ä¸­çš„å¯é å‚è€ƒçŠ¶æ€æˆ–è‰¯å¥½äººä¸ºè®¾è®¡ansÃ¤tzeå¯èƒ½å¸¦æ¥çš„é—®é¢˜ã€‚TrimCIé€šè¿‡è¿­ä»£æ‰©å±•å˜ç©ºé—´å¹¶å‰”é™¤ä¸é‡è¦çŠ¶æ€ï¼Œä½¿å¾—éšæœºåˆå§‹æ ¸å¿ƒèƒ½å¤Ÿè‡ªæˆ‘å®Œå–„ï¼Œä»è€Œè¿‘ä¼¼å¾—åˆ°ç²¾ç¡®çš„åŸºæ€ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTrimCIå®ç°äº†é«˜ç²¾åº¦ï¼Œæ•ˆç‡æé«˜äº†å‡ ä¸ªæ•°é‡çº§ã€‚å¯¹äºä¸åŒçš„ç³»ç»Ÿï¼ŒTrimCIå®ç°äº†ä¸é‡å­è®¡ç®—ç›¸åŒ¹é…çš„ç»“æœï¼Œä½¿ç”¨æ›´å°‘çš„ç¡®å®šå› å­å’Œè®¡ç®—æ—¶é—´ï¼›å¯¹äºæ°®é…¶Pç°‡ï¼Œå®ƒä¸æ‰€é€‰CIçš„ç²¾åº¦ç›¸åŒ¹é…ï¼Œä½¿ç”¨æ›´å°‘çš„ç¡®å®šå› å­ï¼›å¯¹äºHubbardæ¨¡å‹ï¼Œå®ƒæ¢å¤äº†è¶…è¿‡99%çš„åŸºæ€èƒ½é‡ï¼Œä»…ä½¿ç”¨Hilbertç©ºé—´çš„æå°éƒ¨åˆ†ã€‚TrimCIåœ¨ä¸€äº›æƒ…å†µä¸‹æ¯”AFQMCæ–¹æ³•å…·æœ‰æ›´é«˜çš„ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>TrimCIæ˜¯ä¸€ç§æ— å…ˆéªŒçŸ¥è¯†çš„ç®—æ³•ï¼Œå¯ä»¥ç›´æ¥ä»éšæœºSlaterè¡Œåˆ—å¼æ„å»ºç²¾ç¡®åŸºæ€ã€‚</li>
<li>TrimCIé€šè¿‡è¿­ä»£æ‰©å±•å˜ç©ºé—´å¹¶å‰”é™¤ä¸é‡è¦çŠ¶æ€ï¼Œå…è®¸éšæœºåˆå§‹æ ¸å¿ƒè‡ªæˆ‘å®Œå–„ã€‚</li>
<li>TrimCIåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜ç²¾åº¦å’Œæ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚</li>
<li>å¯¹äºä¸åŒçš„ç³»ç»Ÿï¼ŒTrimCIè¾¾åˆ°äº†ä¸é‡å­è®¡ç®—ç›¸åŒ¹é…çš„ç»“æœï¼Œä½¿ç”¨çš„ç¡®å®šå› å­å’Œè®¡ç®—æ—¶é—´å¤§å¤§å‡å°‘ã€‚</li>
<li>TrimCIåœ¨ä¸æ‰€é€‰CIçš„ç²¾åº¦ç›¸åŒ¹é…çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„ç¡®å®šå› å­å¤§å¹…å‡å°‘ã€‚</li>
<li>TrimCIåœ¨Hubbardæ¨¡å‹ä¸­æ¢å¤è¶…è¿‡99%çš„åŸºæ€èƒ½é‡ï¼Œä»…ä½¿ç”¨Hilbertç©ºé—´çš„æå°éƒ¨åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51d9cac4b4cb602ba3c5c6ba4954e6f8" align="middle">
<img src="https://picx.zhimg.com/v2-bc29e7d598d3f8c07ce10df8130bf437" align="middle">
<img src="https://picx.zhimg.com/v2-b2103e0612f304f8fda470659851e2bf" align="middle">
<img src="https://picx.zhimg.com/v2-01814adf92bbc23e50ede9b9b6b822fb" align="middle">
<img src="https://picx.zhimg.com/v2-1734e9b60d3f1eb5514cde184d0f5df3" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Seeing-Beyond-the-Image-ECG-and-Anatomical-Knowledge-Guided-Myocardial-Scar-Segmentation-from-Late-Gadolinium-Enhanced-Images"><a href="#Seeing-Beyond-the-Image-ECG-and-Anatomical-Knowledge-Guided-Myocardial-Scar-Segmentation-from-Late-Gadolinium-Enhanced-Images" class="headerlink" title="Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images"></a>Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images</h2><p><strong>Authors:Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna, Meryem Jabrane, Vicente Grau, Shahnaz Jamil-Copley, Richard H. Clayton,  Chen,  Chen</strong></p>
<p>Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to â€œsee beyond the imageâ€, setting a new direction for robust and physiologically grounded cardiac scar segmentation.</p>
<blockquote>
<p>å¯¹æ™šæœŸé’†å¢å¼ºï¼ˆLGEï¼‰å¿ƒè„ç£å…±æŒ¯æˆåƒä¸­å¿ƒè‚Œç˜¢ç—•çš„ç²¾ç¡®åˆ†å‰²å¯¹äºè¯„ä¼°ç»„ç»‡æ´»åŠ›è‡³å…³é‡è¦ï¼Œä½†ç”±äºå¯¹æ¯”åº¦å˜åŒ–å’Œæˆåƒä¼ªå½±ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¿ƒç”µå›¾ï¼ˆECGï¼‰ä¿¡å·æä¾›äº†è¡¥å……çš„ç”Ÿç†ä¿¡æ¯ï¼Œå› ä¸ºä¼ å¯¼å¼‚å¸¸å¯ä»¥å¸®åŠ©å®šä½æˆ–æç¤ºå¿ƒè‚Œç˜¢ç—•åŒºåŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¿ƒç”µå›¾è¡ç”Ÿçš„ç”Ÿç†ä¿¡æ¯ä¸AHA-17å›¾è°±çš„è§£å‰–å…ˆéªŒçŸ¥è¯†ï¼Œä»¥è¿›è¡ŒåŸºäºLGEçš„ç”Ÿç†ä¸€è‡´æ€§ç˜¢ç—•åˆ†å‰²ã€‚ç”±äºå¿ƒç”µå›¾å’ŒLGE-MRIå¹¶éåŒæ—¶é‡‡é›†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ—¶é—´æ„ŸçŸ¥ç‰¹å¾èåˆï¼ˆTAFFï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®é‡‡é›†æ—¶é—´å·®å¼‚åŠ¨æ€åŠ æƒå¹¶èåˆç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ä»…ä½¿ç”¨å›¾åƒçš„æœ€æ–°åŸºçº¿ï¼ˆnnU-Netï¼‰ç›¸æ¯”ï¼Œå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç˜¢ç—•çš„å¹³å‡Diceå¾—åˆ†ä»0.6149æé«˜åˆ°0.8463ï¼Œåœ¨ç²¾ç¡®åº¦ï¼ˆ0.9115ï¼‰å’Œçµæ•åº¦ï¼ˆ0.9043ï¼‰æ–¹é¢éƒ½å®ç°äº†é«˜æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ•´åˆç”Ÿç†å’Œè§£å‰–çŸ¥è¯†å¯ä»¥ä½¿æ¨¡å‹â€œè¶…è¶Šå›¾åƒâ€ï¼Œä¸ºç¨³å¥å’ŒåŸºäºç”Ÿç†çš„å¿ƒè„ç˜¢ç—•åˆ†å‰²è®¾å®šäº†ä¸€ä¸ªæ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14702v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¯¥æ–‡æœ¬ä»‹ç»äº†ä½¿ç”¨å¿ƒç”µå›¾è¡ç”Ÿä¿¡æ¯å’Œå¤šæ¨¡æ€æ¡†æ¶èåˆçš„æ–¹æ³•æ¥è¿›è¡Œå¿ƒè‚Œç–¤ç—•åˆ†å‰²çš„æŒ‘æˆ˜ä¸æœ€æ–°ç ”ç©¶ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªèåˆå¿ƒç”µå›¾ä¿¡æ¯ä¸AHA-17å›¾è°±è§£å‰–å…ˆéªŒçŸ¥è¯†çš„æ–°æ¡†æ¶ï¼Œå¹¶é‡‡ç”¨æ—¶é—´æ„ŸçŸ¥ç‰¹å¾èåˆæœºåˆ¶ï¼Œå®ç°äº†åœ¨æ™šæœŸé’†å¢å¼ºå¿ƒè„MRIå›¾åƒä¸Šçš„å‡†ç¡®åˆ†å‰²ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨ä¸´åºŠè¯•éªŒæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¿ƒç”µå›¾ï¼ˆECGï¼‰ä¿¡å·æä¾›äº†å¿ƒè‚Œç–¤ç—•åŒºåŸŸçš„é‡è¦ç”Ÿç†ä¿¡æ¯ï¼Œå¯ç”¨äºå®šä½ç–¤ç—•åŒºåŸŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œèåˆäº†ECGä¿¡æ¯å’ŒAHA-17å›¾è°±çš„è§£å‰–å…ˆéªŒçŸ¥è¯†ï¼Œç”¨äºåŸºäºè§£å‰–å­¦çš„ç–¤ç—•åˆ†å‰²ã€‚</li>
<li>ç”±äºå¿ƒç”µå›¾å’Œæ™šæœŸé’†å¢å¼ºMRIä¸æ˜¯åŒæ—¶è·å–çš„ï¼Œå¼•å…¥äº†æ—¶é—´æ„ŸçŸ¥ç‰¹å¾èåˆï¼ˆTAFFï¼‰æœºåˆ¶ï¼Œæ ¹æ®é‡‡é›†æ—¶é—´å·®å¼‚åŠ¨æ€èåˆç‰¹å¾ã€‚</li>
<li>ä¸ä»…ä½¿ç”¨å›¾åƒçš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæé«˜äº†ç–¤ç—•çš„Diceå¾—åˆ†ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œè¿˜æé«˜äº†ç²¾ç¡®åº¦å’Œçµæ•åº¦ã€‚</li>
<li>èåˆç”Ÿç†å’Œè§£å‰–çŸ¥è¯†ä½¿æ¨¡å‹èƒ½å¤Ÿâ€œè¶…è¶Šå›¾åƒâ€ï¼Œä¸ºå¿ƒè„ç–¤ç—•åˆ†å‰²æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ed8edc112df8c1ce29f56a2f9ca33c6" align="middle">
<img src="https://picx.zhimg.com/v2-b9d1a4acb24bc59c4d4775766c0bd177" align="middle">
<img src="https://picx.zhimg.com/v2-448edf2a1ab59be064a1068892a4d294" align="middle">
<img src="https://picx.zhimg.com/v2-24d2bfbfc730a4b061ba512cab05c41a" align="middle">
<img src="https://picx.zhimg.com/v2-3173b07fdcd4cbe1ca0ed5b060ce280f" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HyMAD-A-Hybrid-Multi-Activity-Detection-Approach-for-Border-Surveillance-and-Monitoring"><a href="#HyMAD-A-Hybrid-Multi-Activity-Detection-Approach-for-Border-Surveillance-and-Monitoring" class="headerlink" title="HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring"></a>HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring</h2><p><strong>Authors:Sriram Srinivasan, Srinivasan Aruchamy, Siva Ram Krisha Vadali</strong></p>
<p>Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.</p>
<blockquote>
<p>åœ°éœ‡æ„ŸçŸ¥æŠ€æœ¯å·²ä½œä¸ºè¾¹å¢ƒç›‘è§†å’Œç›‘æ§çš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆå‡ºç°ã€‚é€šå¸¸åŸ‹äºåœ°ä¸‹çš„åœ°éœ‡ä¼ æ„Ÿå™¨ä½“ç§¯å°å·§ï¼Œéš¾ä»¥å¯Ÿè§‰ï¼Œä½¿å¾—å…¥ä¾µè€…éš¾ä»¥å‘ç°ã€é¿å¼€æˆ–ç ´åå®ƒä»¬ã€‚ä¸é«˜åº¦å¯è§çš„æ‘„åƒå¤´æˆ–å›´æ ç›¸æ¯”ï¼Œè¿™æ˜¾è‘—æé«˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå‡†ç¡®æ£€æµ‹å’ŒåŒºåˆ†åŒæ—¶å‘ç”Ÿçš„é‡å æ´»åŠ¨ï¼Œå¦‚äººç±»å…¥ä¾µã€åŠ¨ç‰©ç§»åŠ¨å’Œè½¦è¾†éœ‡åŠ¨ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå› ä¸ºåœ°éœ‡ä¿¡å·å¤æ‚ä¸”å……æ»¡å™ªéŸ³ã€‚æ­£ç¡®è¯†åˆ«åŒæ—¶å‘ç”Ÿçš„æ´»åŠ¨è‡³å…³é‡è¦ï¼Œå› ä¸ºæ— æ³•å°†å®ƒä»¬åˆ†å¼€å¯èƒ½ä¼šå¯¼è‡´è¯¯åˆ†ç±»ã€æ¼æ£€ä»¥åŠæƒ…å†µäº†è§£ä¸å®Œæ•´ï¼Œä»è€Œé™ä½ç›‘æ§ç³»ç»Ÿçš„å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HyMADï¼ˆæ··åˆå¤šæ´»åŠ¨æ£€æµ‹ï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ—¶ç©ºç‰¹å¾èåˆçš„æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é€šè¿‡SincNetæå–çš„é¢‘è°±ç‰¹å¾å’Œé€šè¿‡å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å»ºæ¨¡çš„æ—¶é—´ä¾èµ–æ€§ã€‚æ­¤å¤–ï¼ŒHyMADè¿˜é‡‡ç”¨è‡ªæ³¨æ„åŠ›å±‚æ¥åŠ å¼ºå†…éƒ¨æ¨¡æ€è¡¨ç¤ºå’Œè·¨æ¨¡æ€èåˆæ¨¡å—ï¼Œä»¥å®ç°åœ°éœ‡äº‹ä»¶çš„ç¨³å¥å¤šæ ‡ç­¾åˆ†ç±»ã€‚æˆ‘ä»¬åœ¨è¾¹å¢ƒç›‘è§†å’Œç›‘æ§èƒŒæ™¯ä¸‹æ”¶é›†çš„æ¥è‡ªçœŸå®ä¸–ç•Œç°åœºå½•éŸ³çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æ¶‰åŠäººç±»ã€åŠ¨ç‰©å’Œè½¦è¾†çš„å¤æ‚åŒæ—¶æ´»åŠ¨åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå¯æ‰©å±•åˆ°åŸºäºåœ°éœ‡çš„å®æ—¶å®‰å…¨åº”ç”¨ä¸­çš„æ´»åŠ¨è¯†åˆ«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14698v1">PDF</a> Multi-label seismic signal classification using novel attention-based feature fusion. Submitting to cs.CV due to relevance to general pattern recognition and time-frequency (spectrogram) analysis</p>
<p><strong>Summary</strong><br>åœ°éœ‡æ„ŸçŸ¥æŠ€æœ¯å·²æˆä¸ºè¾¹å¢ƒç›‘æ§å’Œç›‘è§†çš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚åœ°éœ‡ä¼ æ„Ÿå™¨é€šå¸¸åŸ‹äºåœ°ä¸‹ï¼Œéš¾ä»¥è¢«å…¥ä¾µè€…å¯Ÿè§‰ã€é¿å…æˆ–ç ´åï¼Œä»è€Œæé«˜å…¶æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œç”±äºåœ°éœ‡ä¿¡å·çš„å¤æ‚æ€§å’Œå™ªå£°ï¼Œå‡†ç¡®æ£€æµ‹å’ŒåŒºåˆ†åŒæ—¶å‘ç”Ÿçš„é‡å æ´»åŠ¨ï¼ˆå¦‚äººç±»å…¥ä¾µã€åŠ¨ç‰©ç§»åŠ¨å’Œè½¦è¾†éš†éš†å£°ï¼‰ä»æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†HyMADï¼ˆæ··åˆå¤šæ´»åŠ¨æ£€æµ‹ï¼‰æ–¹æ³•ï¼Œä¸€ç§åŸºäºæ—¶ç©ºç‰¹å¾èåˆçš„æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ã€‚è¯¥æ–¹æ³•èåˆäº†é€šè¿‡SincNetæå–çš„é¢‘è°±ç‰¹å¾å’Œç”±å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å»ºæ¨¡çš„æ—¶é—´ä¾èµ–æ€§ã€‚HyMADè¿˜é‡‡ç”¨è‡ªæ³¨æ„åŠ›å±‚åŠ å¼ºå†…éƒ¨æ¨¡æ€è¡¨ç¤ºå’Œè·¨æ¨¡æ€èåˆæ¨¡å—ï¼Œå®ç°ç¨³å¥çš„åœ°éœ‡äº‹ä»¶å¤šæ ‡ç­¾åˆ†ç±»ã€‚åœ¨è¾¹å¢ƒç›‘æ§å’Œç›‘è§†èƒŒæ™¯ä¸‹æ”¶é›†çš„å®åœ°å½•éŸ³æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æ¶‰åŠäººç±»ã€åŠ¨ç‰©å’Œè½¦è¾†çš„å¤æ‚åŒæ—¶æ´»åŠ¨åœºæ™¯ä¸­çš„é€šç”¨æ€§ã€‚è¯¥æ–¹æ³•å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºåŸºäºåœ°éœ‡çš„æ´»åŠ¨è¯†åˆ«åœ¨ç°å®å®‰å…¨åº”ç”¨ä¸­çš„æ‰©å±•æä¾›äº†æ¨¡å—åŒ–æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ°éœ‡æ„ŸçŸ¥æŠ€æœ¯å› å…¶éš¾ä»¥å¯Ÿè§‰çš„ç‰¹æ€§è€Œæˆä¸ºè¾¹å¢ƒç›‘æ§çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>åŒæ—¶æ´»åŠ¨åŒºåˆ†æ˜¯åœ°éœ‡æ„ŸçŸ¥æŠ€æœ¯çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>HyMADæ–¹æ³•ç»“åˆäº†å¤šç§æŠ€æœ¯æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ—¶ç©ºç‰¹å¾èåˆã€é¢‘è°±ç‰¹å¾æå–ã€RNNå»ºæ¨¡æ—¶é—´ä¾èµ–æ€§ç­‰ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›å±‚å’Œè·¨æ¨¡æ€èåˆæ¨¡å—å¼ºåŒ–äº†å¤šæ ‡ç­¾åˆ†ç±»çš„ç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è¾¹å¢ƒç›‘æ§é¢†åŸŸçš„å®é™…æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§å’Œç«äº‰åŠ›ã€‚</li>
<li>HyMADæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå¯æ‰©å±•åˆ°ç°å®å®‰å…¨åº”ç”¨ä¸­çš„åœ°éœ‡æ´»åŠ¨è¯†åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fc8667060582e6f99d1ce7c4c974f99" align="middle">
<img src="https://picx.zhimg.com/v2-c73b80f1473fcdec1ad0a1b12888f43d" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Talk-Snap-Complain-Validation-Aware-Multimodal-Expert-Framework-for-Fine-Grained-Customer-Grievances"><a href="#Talk-Snap-Complain-Validation-Aware-Multimodal-Expert-Framework-for-Fine-Grained-Customer-Grievances" class="headerlink" title="Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances"></a>Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances</h2><p><strong>Authors:Rishu Kumar Singh, Navneet Shreya, Sarmistha Das, Apoorva Singh, Sriparna Saha</strong></p>
<p>Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: <a target="_blank" rel="noopener" href="https://github.com/sarmistha-D/VALOR">https://github.com/sarmistha-D/VALOR</a></p>
<blockquote>
<p>ç°æœ‰çš„æŠ•è¯‰åˆ†ææ–¹å¼ä¸»è¦ä¾èµ–äºå•æ¨¡æ€çš„ç®€çŸ­å†…å®¹ï¼Œå¦‚æ¨ç‰¹æˆ–äº§å“è¯„è®ºã€‚è¿™é¡¹å·¥ä½œé€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€ã€å¤šå›åˆçš„å®¢æˆ·æ”¯æŒå¯¹è¯æ¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ï¼Œç”¨æˆ·é€šå¸¸ä¼šåœ¨å…¶ä¸­åˆ†äº«æ–‡æœ¬æŠ•è¯‰å’Œè§†è§‰è¯æ®ï¼ˆå¦‚æˆªå›¾ã€äº§å“ç…§ç‰‡ï¼‰ï¼Œä»¥å®ç°æŠ•è¯‰æ–¹é¢çš„ç²¾ç»†åˆ†ç±»å’Œä¸¥é‡ç¨‹åº¦åˆ¤æ–­ã€‚æˆ‘ä»¬å¼•å…¥äº†VALORï¼Œä¸€ä¸ªå¸¦æœ‰ä¸“å®¶è·¯ç”±çš„éªŒè¯æ„è¯†å­¦ä¹ è€…ï¼Œä¸“ä¸ºè¿™ç§å¤šæ¨¡æ€ç¯å¢ƒé‡èº«å®šåˆ¶ã€‚å®ƒé‡‡ç”¨å¤šä¸“å®¶æ¨ç†è®¾ç½®ï¼Œä½¿ç”¨å…·æœ‰æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºçš„å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹è¿›è¡Œç»†å¾®çš„å†³ç­–ã€‚ä¸ºäº†ç¡®ä¿å„æ¨¡æ€ä¹‹é—´çš„è¿è´¯æ€§ï¼Œè®¡ç®—äº†è¯­ä¹‰å¯¹é½å¾—åˆ†ï¼Œå¹¶å°†å…¶é€šè¿‡å…ƒèåˆç­–ç•¥é›†æˆåˆ°æœ€ç»ˆåˆ†ç±»ä¸­ã€‚ç¬¦åˆè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡ï¼ˆUN SDGsï¼‰ï¼Œæ‰€ææ¡†æ¶æ”¯æŒSDG9ï¼ˆå·¥ä¸šã€åˆ›æ–°å’ŒåŸºç¡€è®¾æ–½ï¼‰ï¼Œé€šè¿‡æ¨åŠ¨AIé©±åŠ¨å·¥å…·å®ç°ç¨³å¥ã€å¯æ‰©å±•å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æœåŠ¡åŸºç¡€è®¾æ–½ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯ç”¨æŠ•è¯‰å™äº‹å’Œè§†è§‰ä¸Šä¸‹æ–‡çš„ç»“æ„åŒ–åˆ†æï¼Œå®ƒå¯¹SDG12ï¼ˆè´Ÿè´£ä»»çš„æ¶ˆè´¹å’Œç”Ÿäº§ï¼‰åšå‡ºè´¡çŒ®ï¼Œä¿ƒè¿›æ›´å…·å“åº”æ€§çš„äº§å“è®¾è®¡ä»¥åŠæ¶ˆè´¹è€…æœåŠ¡ä¸­çš„æ”¹è¿›é—®è´£åˆ¶ã€‚æˆ‘ä»¬åœ¨ç²¾é€‰çš„å¤šæ¨¡æ€æŠ•è¯‰æ•°æ®é›†ä¸Šè¯„ä¼°VALORï¼Œè¯¥æ•°æ®é›†å¸¦æœ‰ç²¾ç»†çš„æ–¹é¢å’Œä¸¥é‡ç¨‹åº¦æ ‡ç­¾ï¼Œè¡¨æ˜å®ƒå§‹ç»ˆä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯åˆ†å¸ƒå¤æ‚çš„æŠ•è¯‰åœºæ™¯ä¸­ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€äº¤äº’å’Œä¸“å®¶éªŒè¯åœ¨å®é™…æŠ•è¯‰ç†è§£ç³»ç»Ÿä¸­çš„ä»·å€¼ã€‚æœ‰å…³æ•°æ®å’Œä»£ç çš„èµ„æºå¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/sarmistha-D/VALOR">https://github.com/sarmistha-D/VALOR</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14693v1">PDF</a> To be published in the Proceedings of the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026 Special Track on AI for Social Impact )</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰çš„æŠ•è¯‰åˆ†ææ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€ã€å¤šå›åˆçš„å®¢æˆ·æ”¯æŒå¯¹è¯æ¥æé«˜æŠ•è¯‰åˆ†ç±»çš„ç²¾ç»†åº¦ã€‚æå‡ºVALORæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šä¸“å®¶æ¨ç†è®¾ç½®å¤§å‹ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡é“¾æ¡æ€ç»´æç¤ºè¿›è¡Œå¾®å¦™å†³ç­–ã€‚VALORæ¡†æ¶é€šè¿‡è®¡ç®—è¯­ä¹‰å¯¹é½å¾—åˆ†å¹¶æ•´åˆåˆ°æœ€ç»ˆåˆ†ç±»ä¸­ï¼Œç¡®ä¿æ¨¡æ€ä¹‹é—´çš„è¿è´¯æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡ä¸­çš„ä¸¤ä¸ªç›®æ ‡â€”â€”äº§ä¸šã€åˆ›æ–°å’ŒåŸºç¡€è®¾æ–½ï¼ˆSDG 9ï¼‰å’Œè´£ä»»æ¶ˆè´¹å’Œç”Ÿäº§ï¼ˆSDG 12ï¼‰ï¼Œé€šè¿‡æ¨è¿›AIé©±åŠ¨å·¥å…·æ¥å®ç°ç¨³å¥ã€å¯æ‰©å±•å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æœåŠ¡åŸºç¡€è®¾æ–½ï¼Œä¿ƒè¿›æ›´å…·å“åº”æ€§çš„äº§å“è®¾è®¡å’Œæé«˜æ¶ˆè´¹è€…æœåŠ¡çš„é—®è´£åˆ¶ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒVALORåœ¨ç²¾ç»†ç²’åº¦çš„æŠ•è¯‰æ–¹é¢å’Œæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æŠ•è¯‰åˆ†ææ–¹æ³•ä¸»è¦ä¾èµ–å•æ¨¡æ€çŸ­å†…å®¹ï¼Œå¦‚æ¨ç‰¹æˆ–äº§å“è¯„è®ºï¼Œè€Œæ–°æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€ã€å¤šå›åˆå®¢æˆ·æ”¯æŒå¯¹è¯æå‡æŠ•è¯‰åˆ†ç±»ç²¾ç»†åº¦ã€‚</li>
<li>VALORæ¡†æ¶å¼•å…¥å¤šä¸“å®¶æ¨ç†è®¾ç½®ï¼Œä½¿ç”¨å¤§å‹ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®å¦™å†³ç­–ã€‚</li>
<li>VALORé€šè¿‡è®¡ç®—è¯­ä¹‰å¯¹é½å¾—åˆ†ç¡®ä¿ä¸åŒæ¨¡æ€é—´çš„è¿è´¯æ€§ã€‚</li>
<li>VALORæ”¯æŒè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡ä¸­çš„SDG 9å’ŒSDG 12ï¼Œæ¨è¿›AIå·¥å…·å®ç°ç¨³å¥ã€å¯æ‰©å±•å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æœåŠ¡åŸºç¡€è®¾æ–½ã€‚</li>
<li>VALORåœ¨ç²¾ç»†ç²’åº¦çš„æŠ•è¯‰æ–¹é¢å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•å°¤å…¶åœ¨å¤æ‚æŠ•è¯‰åœºæ™¯ä¸­ï¼Œå½“ä¿¡æ¯åˆ†å¸ƒåœ¨æ–‡æœ¬å’Œå›¾åƒä¸­æ—¶ï¼Œè¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a41cf84558e1c9643f5395cb6f8b7dd8" align="middle">
<img src="https://picx.zhimg.com/v2-100916113831c5c3bb21d418f6f0293c" align="middle">
<img src="https://picx.zhimg.com/v2-9c3ee3724199abec1507efea34133070" align="middle">
<img src="https://picx.zhimg.com/v2-3976107173ab0ab1f2d08ef3d7f505eb" align="middle">
<img src="https://picx.zhimg.com/v2-bf4f0998ee70540e12cdcc3985914871" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SMRC-Aligning-Large-Language-Models-with-Student-Reasoning-for-Mathematical-Error-Correction"><a href="#SMRC-Aligning-Large-Language-Models-with-Student-Reasoning-for-Mathematical-Error-Correction" class="headerlink" title="SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction"></a>SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction</h2><p><strong>Authors:Biaojie Zeng, Min Zhang, Juan Zhou, Fengrui Liu, Ruiyang Huang, Xin Lin</strong></p>
<p>Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the <code>teacher-style</code> correction required in educational settings, \textit{i.e.}, systematically guiding and revising a studentâ€™s problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Mind-Lab-ECNU/SMRC">https://github.com/Mind-Lab-ECNU/SMRC</a>.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶å¸¸å¸¸å‡ºç°æ¨ç†é”™è¯¯ï¼Œå¦‚ä½•è‡ªåŠ¨æ£€æµ‹å’Œçº æ­£è¿™äº›é”™è¯¯å·²æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹çš„è‡ªæˆ‘ä¿®æ­£ä¸Šï¼Œè¿™æ— æ³•è¾¾åˆ°æ•™è‚²ç¯å¢ƒä¸­æ‰€éœ€çš„â€œæ•™å¸ˆå¼â€ä¿®æ­£ï¼Œå³ç³»ç»Ÿåœ°æŒ‡å¯¼å’Œä¿®è®¢å­¦ç”Ÿçš„é—®é¢˜è§£å†³è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†<code>SMRC</code>ï¼ˆå­¦ç”Ÿæ•°å­¦æ¨ç†ä¿®æ­£æ–¹æ³•ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿LLMä¸å­¦ç”Ÿæ¨ç†ç›¸ä¸€è‡´çš„æ–°å‹æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒSMRCå°†å­¦ç”Ÿæ¨ç†åˆ¶å®šä¸ºä¸€ä¸ªå¤šæ­¥éª¤çš„åºåˆ—å†³ç­–é—®é¢˜ï¼Œå¹¶å¼•å…¥è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥æ¢ç´¢æœ€ä½³çš„ä¿®æ­£è·¯å¾„ã€‚ä¸ºäº†å‡å°‘æ ‡æ³¨è¿‡ç¨‹å±‚é¢å¥–åŠ±çš„ä»£ä»·ï¼Œæˆ‘ä»¬åˆ©ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰åœ¨LLMçš„æŒ‡å¯¼ä¸‹è¿›è¡Œæœç´¢ï¼Œå¹¶é€šè¿‡æœ€ç»ˆç­”æ¡ˆçš„è¯„ä¼°æ¥ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œè¿™äº›å¥–åŠ±ä¿¡å·é€šè¿‡åå‘ä¼ æ’­æœºåˆ¶è¢«åˆ†é…åˆ°ä¸­é—´çš„æ¨ç†æ­¥éª¤ä¸­ï¼Œå®ç°äº†ç²¾ç»†çš„è¿‡ç¨‹ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜ä¸­æ•°å­¦åŸºå‡†æµ‹è¯•MSEBï¼ˆå¤šè§£è¯¯å·®åŸºå‡†æµ‹è¯•ï¼‰ï¼Œå…¶ä¸­åŒ…æ‹¬158ä¸ªå®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹éƒ½åŒ…å«é—®é¢˜é™ˆè¿°ã€å­¦ç”Ÿè§£ç­”å’Œæ­£ç¡®çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä»¥â€œè§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§â€å’Œâ€œæ­£ç¡®æ­¥éª¤ä¿ç•™ç‡â€ä¸ºä¸­å¿ƒçš„åŒè¯„ä¼°åè®®ï¼Œä¸ºæ•™è‚²é€‚ç”¨æ€§æä¾›äº†å…¨é¢çš„è¡¡é‡æ ‡å‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒSMRCåœ¨å…¬å¼€æ•°æ®é›†ï¼ˆProcessBenchå’ŒMR-GSM8Kï¼‰ä»¥åŠæˆ‘ä»¬çš„MSEBåŸºå‡†æµ‹è¯•ä¸Šï¼Œå…¶æœ‰æ•ˆæ€§å’Œæ•´ä½“æ€§èƒ½å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Mind-Lab-ECNU/SMRC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Mind-Lab-ECNU/SMRCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14684v1">PDF</a> 13 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶çš„æ¨ç†é”™è¯¯è‡ªåŠ¨æ£€æµ‹å’Œçº æ­£æ–¹æ³•æˆä¸ºé‡è¦ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹å†…çš„è‡ªæˆ‘çº æ­£ï¼Œç¼ºä¹æ•™è‚²ç¯å¢ƒä¸­æ‰€éœ€çš„â€œæ•™å¸ˆå¼â€çº æ­£ï¼Œå³ç³»ç»Ÿåœ°æŒ‡å¯¼å’Œä¿®è®¢å­¦ç”Ÿçš„è§£é¢˜è¿‡ç¨‹ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ–¹æ³•\texttt{SMRC}ï¼Œä½¿LLMsä¸å­¦ç”Ÿæ¨ç†ç›¸ä¸€è‡´ã€‚å…·ä½“è€Œè¨€ï¼Œ\texttt{SMRC}å°†å­¦ç”Ÿæ¨ç†å½¢å¼åŒ–ä¸ºå¤šæ­¥éª¤çš„åºåˆ—å†³ç­–é—®é¢˜ï¼Œå¹¶å¼•å…¥è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä»¥æ¢ç´¢æœ€ä¼˜ä¿®æ­£è·¯å¾„ã€‚åŒæ—¶åˆ©ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰é™ä½è¿‡ç¨‹çº§å¥–åŠ±çš„æ ‡æ³¨æˆæœ¬ï¼Œé€šè¿‡LLMsçš„æœ€ç»ˆç­”æ¡ˆè¯„ä¼°ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œå¹¶é€šè¿‡åå‘ä¼ æ’­æœºåˆ¶å°†å…¶åˆ†å¸ƒåˆ°ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå®ç°ç²¾ç»†çš„è¿‡ç¨‹ç›‘ç£ã€‚æ­¤å¤–ï¼Œæ„å»ºé«˜ä¸­æ•°å­¦çš„MSEBåŸºå‡†æµ‹è¯•é›†ï¼Œå¹¶æå‡ºä»¥è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§å’Œæ­£ç¡®æ­¥éª¤ä¿ç•™ç‡ä¸ºä¸­å¿ƒçš„åŒé‡è¯„ä¼°åè®®ã€‚å®éªŒè¡¨æ˜ï¼Œ\texttt{SMRC}åœ¨å…¬å¼€æ•°æ®é›†å’Œè‡ªåˆ¶æµ‹è¯•é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶å­˜åœ¨æ¨ç†é”™è¯¯çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹å†…éƒ¨çš„è‡ªæˆ‘çº æ­£ï¼Œç¼ºä¹åƒæ•™è‚²ç¯å¢ƒä¸­æ•™å¸ˆé‚£æ ·çš„ç³»ç»Ÿæ€§æŒ‡å¯¼å’Œä¿®è®¢ã€‚</li>
<li>\texttt{SMRC}æ–¹æ³•æ—¨åœ¨å°†LLMsä¸å­¦ç”Ÿæ¨ç†ç›¸ç»“åˆï¼Œå°†å…¶å½¢å¼åŒ–ä¸ºå¤šæ­¥éª¤çš„åºåˆ—å†³ç­–é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¢ç´¢æœ€ä¼˜ä¿®æ­£è·¯å¾„ï¼Œå¹¶åˆ©ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰é™ä½æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>é€šè¿‡LLMsçš„æœ€ç»ˆç­”æ¡ˆè¯„ä¼°ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œå¹¶é€šè¿‡åå‘ä¼ æ’­æœºåˆ¶ç²¾ç»†åœ°ç›‘ç£ä¸­é—´æ¨ç†æ­¥éª¤ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªé’ˆå¯¹é«˜ä¸­æ•°å­¦çš„åŸºå‡†æµ‹è¯•é›†MSEBï¼Œå¹¶æå‡ºäº†ä»¥è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§å’Œæ­£ç¡®æ­¥éª¤ä¿ç•™ç‡ä¸ºä¸»çš„åŒé‡è¯„ä¼°åè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d911db7e2a2f9930ac83b75d73226170" align="middle">
<img src="https://picx.zhimg.com/v2-ca6532240cce89f6a10b7e7a53a19440" align="middle">
<img src="https://picx.zhimg.com/v2-f92e1a20bdc2b891e88bb6a96c8aff53" align="middle">
<img src="https://picx.zhimg.com/v2-0c41c02d963cb9cd30af7a6ce7695246" align="middle">
<img src="https://picx.zhimg.com/v2-1a9d13a59b7380eb7ee23a00a2d5caf2" align="middle">
<img src="https://picx.zhimg.com/v2-676bc2ec0dee967b24232c622752071c" align="middle">
<img src="https://picx.zhimg.com/v2-252e9aeee641c03ca4c6665fa52035b4" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AutoTool-Efficient-Tool-Selection-for-Large-Language-Model-Agents"><a href="#AutoTool-Efficient-Tool-Selection-for-Large-Language-Model-Agents" class="headerlink" title="AutoTool: Efficient Tool Selection for Large Language Model Agents"></a>AutoTool: Efficient Tool Selection for Large Language Model Agents</h2><p><strong>Authors:Jingyi Jia, Qinbin Li</strong></p>
<p>Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å·²ç»æ¶Œç°ä¸ºå¼ºå¤§çš„å·¥å…·ï¼Œèƒ½å¤Ÿé€šè¿‡åˆ©ç”¨LLMçš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›æ¥è‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰ä»£ç†æ¡†æ¶çš„ä¸»è¦ç“¶é¢ˆåœ¨äºå·¥å…·é€‰æ‹©çš„é«˜æ¨ç†æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒReActè¿™æ ·çš„æ–¹æ³•ä¸­ï¼Œå®ƒåå¤è°ƒç”¨LLMæ¥ç¡®å®šæ¯ä¸€æ­¥åº”ä½¿ç”¨å“ªä¸ªå·¥å…·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AutoToolï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå›¾çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä¸€ä¸ªå…³é”®çš„ç»éªŒè§‚å¯Ÿç»“æœæ¥ç»•è¿‡é‡å¤çš„LLMæ¨ç†ï¼šå·¥å…·ä½¿ç”¨æƒ¯æ€§â€”â€”å·¥å…·è°ƒç”¨çš„è¶‹åŠ¿éµå¾ªå¯é¢„æµ‹çš„é¡ºåºæ¨¡å¼ã€‚AutoToolä»å†å²ä»£ç†è½¨è¿¹æ„å»ºæœ‰å‘å›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å·¥å…·ï¼Œè¾¹æ•æ‰è½¬æ¢æ¦‚ç‡ï¼Œæœ‰æ•ˆåœ°å¯¹å·¥å…·é€‰æ‹©ä¸­çš„æƒ¯æ€§è¿›è¡Œå»ºæ¨¡ã€‚å®ƒè¿›ä¸€æ­¥æ•´åˆå‚æ•°çº§åˆ«çš„ä¿¡æ¯æ¥ä¼˜åŒ–å·¥å…·è¾“å…¥ç”Ÿæˆã€‚é€šè¿‡éå†è¿™ç§ç»“æ„åŒ–è¡¨ç¤ºï¼ŒAutoToolèƒ½å¤Ÿé«˜æ•ˆåœ°é€‰æ‹©å·¥å…·å’Œå‚æ•°ï¼Œå¯¹LLMæ¨ç†çš„ä¾èµ–åº¦é™åˆ°æœ€ä½ã€‚åœ¨å¤šç§ä»£ç†ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAutoToolå°†æ¨ç†æˆæœ¬é™ä½äº†é«˜è¾¾30%ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰æ€§çš„ä»»åŠ¡å®Œæˆç‡ï¼Œä¸ºæ¨ç†å¯†é›†å‹æ¡†æ¶æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„å¢å¼ºã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†å°†ç»Ÿè®¡ç»“æ„æ•´åˆåˆ°LLMä»£ç†è®¾è®¡ä¸­ä»¥æé«˜æ•ˆç‡è€Œä¸ç‰ºç‰²æ€§èƒ½çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14650v1">PDF</a> Accepted by AAAI 2026, 18 pages, 11 figures, Code: <a target="_blank" rel="noopener" href="https://github.com/jiajingyyyyyy/AutoTool">https://github.com/jiajingyyyyyy/AutoTool</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†é€šè¿‡åˆ©ç”¨LLMçš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›è‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡ï¼Œå·²æˆä¸ºå¼ºå¤§çš„å·¥å…·ã€‚ç„¶è€Œï¼Œå½“å‰ä»£ç†æ¡†æ¶çš„ä¸»è¦ç“¶é¢ˆåœ¨äºå·¥å…·é€‰æ‹©çš„é«˜æ¨ç†æˆæœ¬ï¼Œå°¤å…¶æ˜¯åœ¨ReActç­‰æ–¹æ³•ä¸­ï¼Œä¼šåå¤è°ƒç”¨LLMæ¥ç¡®å®šæ¯ä¸€æ­¥ä½¿ç”¨å“ªä¸ªå·¥å…·ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºAutoToolï¼Œä¸€ç§åŸºäºå›¾çš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å·¥å…·ä½¿ç”¨æƒ¯æ€§è¿™ä¸€å…³é”®å®è¯è§‚å¯Ÿæ¥ç»•è¿‡åå¤çš„LLMæ¨ç†ã€‚AutoToolé€šè¿‡æ„å»ºæœ‰å‘å›¾æ¥æ•æ‰å·¥å…·ä½¿ç”¨çš„é¡ºåºæ¨¡å¼ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å·¥å…·ï¼Œè¾¹æ•æ‰è½¬æ¢æ¦‚ç‡ã€‚é€šè¿‡éå†è¿™ç§ç»“æ„åŒ–è¡¨ç¤ºï¼ŒAutoToolèƒ½å¤Ÿé«˜æ•ˆé€‰æ‹©å·¥å…·åŠå…¶å‚æ•°ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘å¯¹LLMæ¨ç†çš„ä¾èµ–ã€‚å®éªŒè¡¨æ˜ï¼ŒAutoToolå°†æ¨ç†æˆæœ¬é™ä½äº†30%ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰æ€§çš„ä»»åŠ¡å®Œæˆç‡ï¼Œä¸ºæ¨ç†å¯†é›†å‹æ¡†æ¶æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä»£ç†å·²æˆä¸ºè‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡çš„é‡è¦å·¥å…·ï¼Œä½†å­˜åœ¨é«˜æ¨ç†æˆæœ¬çš„ç“¶é¢ˆã€‚</li>
<li>å½“å‰æ–¹æ³•å¦‚ReActåå¤è°ƒç”¨LLMè¿›è¡Œå·¥å…·é€‰æ‹©ï¼Œå¯¼è‡´æ•ˆç‡é™ä½ã€‚</li>
<li>AutoToolæå‡ºä¸€ç§åŸºäºå›¾çš„æ¡†æ¶ï¼Œé€šè¿‡æ•æ‰å·¥å…·ä½¿ç”¨çš„é¡ºåºæ¨¡å¼æ¥ç»•è¿‡åå¤LLMæ¨ç†ã€‚</li>
<li>AutoToolæ„å»ºçš„æœ‰å‘å›¾åŒ…æ‹¬èŠ‚ç‚¹ä»£è¡¨å·¥å…·å’Œè¾¹ä»£è¡¨è½¬æ¢æ¦‚ç‡ï¼Œæœ‰æ•ˆå»ºæ¨¡å·¥å…·ä½¿ç”¨æƒ¯æ€§ã€‚</li>
<li>AutoToolé€šè¿‡éå†ç»“æ„åŒ–è¡¨ç¤ºé«˜æ•ˆé€‰æ‹©å·¥å…·åŠå…¶å‚æ•°ï¼Œå‡å°‘å¯¹LLMæ¨ç†çš„ä¾èµ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒAutoToolé™ä½äº†æ¨ç†æˆæœ¬30%ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡å®Œæˆç‡ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fefa04a2c46e19fbbee469599850beb" align="middle">
<img src="https://picx.zhimg.com/v2-81a51b25ce91875cad1a2b89c7c3ec7e" align="middle">
<img src="https://picx.zhimg.com/v2-3496eb1fbf801efbb3bf9b66489da9bf" align="middle">
<img src="https://picx.zhimg.com/v2-c8f075ee0c424f7d79e66d53b89f8c88" align="middle">
<img src="https://picx.zhimg.com/v2-42c76b6b578f95415469e7baa94f5e72" align="middle">
<img src="https://picx.zhimg.com/v2-d209a3f04482afdefedee98d5ccc760c" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RepAir-A-Framework-for-Airway-Segmentation-and-Discontinuity-Correction-in-CT"><a href="#RepAir-A-Framework-for-Airway-Segmentation-and-Discontinuity-Correction-in-CT" class="headerlink" title="RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT"></a>RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT</h2><p><strong>Authors:John M. Oyer, Ali Namvar, Benjamin A. Hoff, Wassim W. Labaki, Ella A. Kazerooni, Charles R. Hatt, Fernando J. Martinez, MeiLan K. Han, Craig J. GalbÃ¡n, Sundaresh Ram</strong></p>
<p>Accurate airway segmentation from chest computed tomography (CT) scans is essential for quantitative lung analysis, yet manual annotation is impractical and many automated U-Net-based methods yield disconnected components that hinder reliable biomarker extraction. We present RepAir, a three-stage framework for robust 3D airway segmentation that combines an nnU-Net-based network with anatomically informed topology correction. The segmentation network produces an initial airway mask, after which a skeleton-based algorithm identifies potential discontinuities and proposes reconnections. A 1D convolutional classifier then determines which candidate links correspond to true anatomical branches versus false or obstructed paths. We evaluate RepAir on two distinct datasets: ATMâ€™22, comprising annotated CT scans from predominantly healthy subjects and AeroPath, encompassing annotated scans with severe airway pathology. Across both datasets, RepAir outperforms existing 3D U-Net-based approaches such as Bronchinet and NaviAirway on both voxel-level and topological metrics, and produces more complete and anatomically consistent airway trees while maintaining high segmentation accuracy.</p>
<blockquote>
<p>ä»èƒ¸éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­å‡†ç¡®çš„æ°”é“åˆ†å‰²å¯¹å®šé‡è‚ºåˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æ ‡æ³¨å¹¶ä¸å®ç”¨ï¼Œè®¸å¤šåŸºäºU-Netçš„è‡ªåŠ¨åŒ–æ–¹æ³•ä¼šäº§ç”Ÿæ–­è£‚çš„ç»„ä»¶ï¼Œé˜»ç¢å¯é çš„ç”Ÿç‰©æ ‡å¿—ç‰©æå–ã€‚æˆ‘ä»¬æå‡ºäº†RepAirï¼Œè¿™æ˜¯ä¸€ä¸ªç¨³å¥çš„3Dæ°”é“åˆ†å‰²ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œå®ƒå°†åŸºäºnnU-Netçš„ç½‘ç»œä¸è§£å‰–ä¿¡æ¯æ‹“æ‰‘æ ¡æ­£ç›¸ç»“åˆã€‚åˆ†å‰²ç½‘ç»œç”Ÿæˆåˆå§‹æ°”é“æ©è†œï¼Œä¹‹ååŸºäºéª¨æ¶çš„ç®—æ³•è¯†åˆ«æ½œåœ¨çš„ä¸è¿ç»­ç‚¹å¹¶æå‡ºé‡æ–°è¿æ¥ã€‚ç„¶åï¼Œä¸€ç»´å·ç§¯åˆ†ç±»å™¨ç¡®å®šå“ªäº›å€™é€‰é“¾æ¥å¯¹åº”äºçœŸæ­£çš„è§£å‰–åˆ†æ”¯ï¼Œå“ªäº›å¯¹åº”äºé”™è¯¯æˆ–å—é˜»çš„è·¯å¾„ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šå¯¹RepAirè¿›è¡Œäº†è¯„ä¼°ï¼šATMâ€™22ä¸»è¦ç”±å¥åº·å—è¯•è€…çš„æ³¨é‡ŠCTæ‰«æç»„æˆï¼ŒAeroPathåŒ…å«å¸¦æœ‰ä¸¥é‡æ°”é“ç—…ç†çš„æ³¨é‡Šæ‰«æã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­ï¼ŒRepAiråœ¨ä½“ç´ çº§å’Œæ‹“æ‰‘åº¦é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„3D U-Netæ–¹æ³•ï¼ˆå¦‚Bronchinetå’ŒNaviAirwayï¼‰ï¼Œå¹¶äº§ç”Ÿæ›´å®Œæ•´å’Œè§£å‰–ä¸Šä¸€è‡´çš„æ°”é“æ ‘ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„åˆ†å‰²ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14649v1">PDF</a> 4 pages, 3 figures, 1 table. Preprint submitted to SSIAI 2026 Conference on November 17, 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRepAirçš„ç¨³å¥ä¸‰ç»´æ°”é“åˆ†å‰²æ¡†æ¶ï¼Œç»“åˆäº†nnU-NetåŸºç¡€ç½‘ç»œå’Œè§£å‰–ä¿¡æ¯æ‹“æ‰‘æ ¡æ­£ã€‚è¯¥æ¡†æ¶ç”¨äºä»èƒ¸éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­å‡†ç¡®åˆ†å‰²æ°”é“ï¼Œä¸ºå®šé‡è‚ºåˆ†ææä¾›å¯é åŸºç¡€ã€‚é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„å¤„ç†ï¼ŒRepAirèƒ½å¤Ÿäº§å‡ºå®Œæ•´ä¸”è§£å‰–ä¸Šä¸€è‡´çš„æ°”é“æ ‘ï¼ŒåŒæ—¶ä¿æŒé«˜åˆ†å‰²ç²¾åº¦ã€‚åœ¨ATMâ€™22å’ŒAeroPathä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–ä¸‰ç»´U-Netæ–¹æ³•ï¼Œå¦‚Bronchinetå’ŒNaviAirwayã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RepAiræ˜¯ä¸€ä¸ªç”¨äºä»èƒ¸éƒ¨CTæ‰«æä¸­ç¨³å¥è¿›è¡Œä¸‰ç»´æ°”é“åˆ†å‰²çš„æ¡†æ¶ã€‚</li>
<li>ç»“åˆäº†nnU-NetåŸºç¡€ç½‘ç»œå’Œè§£å‰–ä¿¡æ¯æ‹“æ‰‘æ ¡æ­£ï¼Œä»¥æé«˜åˆ†å‰²ç²¾åº¦å’Œå®Œæ•´æ€§ã€‚</li>
<li>RepAiré€šè¿‡ä¸‰ä¸ªé˜¶æ®µå¤„ç†ï¼šåˆå§‹æ°”é“é®ç½©ç”Ÿæˆã€åŸºäºéª¨æ¶çš„ç®—æ³•è¯†åˆ«æ½œåœ¨æ–­è£‚å¹¶æè®®é‡æ–°è¿æ¥ï¼Œä»¥åŠä½¿ç”¨ä¸€ç»´å·ç§¯åˆ†ç±»å™¨ç¡®å®šçœŸæ­£çš„è§£å‰–åˆ†æ”¯ä¸é”™è¯¯æˆ–é˜»å¡è·¯å¾„ã€‚</li>
<li>åœ¨ATMâ€™22å’ŒAeroPathæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRepAiråœ¨ä½“ç´ çº§åˆ«å’Œæ‹“æ‰‘æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„ä¸‰ç»´U-Netæ–¹æ³•ï¼Œå¦‚Bronchinetå’ŒNaviAirwayã€‚</li>
<li>RepAirèƒ½å¤Ÿäº§å‡ºæ›´å®Œæ•´ä¸”è§£å‰–ä¸Šä¸€è‡´çš„æ°”é“æ ‘ï¼ŒåŒæ—¶ä¿æŒé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>æ¡†æ¶çš„è®¾è®¡è€ƒè™‘äº†æ°”é“çš„å¤æ‚æ€§å’Œæ‹“æ‰‘ç»“æ„ï¼Œä½¿å…¶æ›´é€‚åˆäºå®šé‡è‚ºåˆ†æå’Œç”Ÿç‰©æ ‡å¿—ç‰©æå–ç­‰åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14649">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16741b6a4e806216f5837e532b4ef25b" align="middle">
<img src="https://picx.zhimg.com/v2-72a890e3b2d3a0e01d9c872377e80095" align="middle">
<img src="https://picx.zhimg.com/v2-fb7f4056de7a61158fecebb4dae1c88c" align="middle">
<img src="https://picx.zhimg.com/v2-337e4fd9ae032fe6fab4bc822fdfa3f8" align="middle">
<img src="https://picx.zhimg.com/v2-697157c4b7bd46fc7edbc515374c5e6d" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SLAM-AGS-Slide-Label-Aware-Multi-Task-Pretraining-Using-Adaptive-Gradient-Surgery-in-Computational-Cytology"><a href="#SLAM-AGS-Slide-Label-Aware-Multi-Task-Pretraining-Using-Adaptive-Gradient-Surgery-in-Computational-Cytology" class="headerlink" title="SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology"></a>SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology</h2><p><strong>Authors:Marco Acerbis, Swarnadip Chatterjee, Christophe Avenel, Joakim Lindblad</strong></p>
<p>Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: <a target="_blank" rel="noopener" href="https://github.com/Ace95/SLAM-AGS">https://github.com/Ace95/SLAM-AGS</a>.</p>
<blockquote>
<p>è®¡ç®—ç»†èƒå­¦é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯å®ä¾‹çº§åˆ«çš„æ ‡ç­¾ä¸å¯é ä¸”è·å–æˆæœ¬é«˜æ˜‚ï¼ŒäºŒæ˜¯ç›®å‡»ç‡æä½ã€‚æˆ‘ä»¬æå‡ºäº†SLAM-AGSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¹»ç¯ç‰‡æ ‡ç­¾æ„ŸçŸ¥çš„å¤šä»»åŠ¡é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒè”åˆä¼˜åŒ–äº†ï¼ˆiï¼‰å¹»ç¯ç‰‡è´Ÿè¡¥ä¸ä¸Šçš„å¼±ç›‘ç£ç›¸ä¼¼æ€§ç›®æ ‡å’Œï¼ˆiiï¼‰å¹»ç¯ç‰‡æ­£è¡¥ä¸ä¸Šçš„è‡ªç›‘ç£å¯¹æ¯”ç›®æ ‡ï¼Œä»è€Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°æ›´å¼ºçš„æ€§èƒ½ã€‚ä¸ºäº†ç¨³å®šå­¦ä¹ ï¼Œæˆ‘ä»¬åº”ç”¨è‡ªé€‚åº”æ¢¯åº¦æ‰‹æœ¯æ¥è§£å†³å†²çªçš„ä»»åŠ¡æ¢¯åº¦å¹¶é˜²æ­¢æ¨¡å‹å´©æºƒã€‚æˆ‘ä»¬å°†é¢„è®­ç»ƒçš„ç¼–ç å™¨é›†æˆåˆ°åŸºäºæ³¨æ„åŠ›çš„å¤šæ¬¡å®ä¾‹å­¦ä¹ èšåˆå™¨ä¸­ï¼Œç”¨äºåŒ…çº§åˆ«é¢„æµ‹å’Œæ³¨æ„åŠ›å¼•å¯¼çš„æ£€ç´¢åŒ…ä¸­æœ€å¼‚å¸¸çš„å®ä¾‹ã€‚åœ¨å…¬å¼€çš„éª¨é«“ç»†èƒæ•°æ®é›†ä¸Šï¼Œé€šè¿‡ä»10%æ¨¡æ‹Ÿç›®å‡»ç‡é™è‡³0.5%ï¼ŒSLAM-AGSåœ¨åŒ…çº§F1åˆ†æ•°å’Œå‰400ä¸ªé˜³æ€§ç»†èƒæ£€ç´¢æ–¹é¢è¶…è¿‡äº†å…¶ä»–é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä½ç›®å‡»ç‡ä¸‹è·å¾—äº†æœ€å¤§çš„æ”¶ç›Šï¼Œè¿™è¡¨æ˜è§£å†³æ¢¯åº¦å¹²æ‰°å¯å®ç°ç¨³å®šçš„é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ›´å¥½æ€§èƒ½ã€‚ä¸ºäº†ä¾¿äºå¤ç°ï¼Œæˆ‘ä»¬å°†å®Œæ•´çš„å®ç°å’Œè¯„ä¼°æ¡†æ¶ä½œä¸ºå¼€æºåˆ†äº«ï¼š<a target="_blank" rel="noopener" href="https://github.com/Ace95/SLAM-AGS%E3%80%82">https://github.com/Ace95/SLAM-AGSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14639v1">PDF</a> 5 pages, 2 figures, Submitted to ISBI2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è®¡ç®—ç»†èƒå­¦é¢†åŸŸæ‰€é¢ä¸´çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå®ä¾‹æ ‡ç­¾ä¸å¯é ä¸”è·å–æˆæœ¬é«˜æ˜‚ï¼Œç›®å‡»ç‡æä½ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSLAM-AGSçš„å¹»ç¯ç‰‡æ ‡ç­¾æ„ŸçŸ¥å¤šä»»åŠ¡é¢„è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–å¼±ç›‘ç£ç›¸ä¼¼åº¦ç›®æ ‡å’Œè‡ªæˆ‘ç›‘ç£å¯¹æ¯”ç›®æ ‡ï¼Œæé«˜äº†ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ä¸ºç¨³å®šå­¦ä¹ ï¼Œé‡‡ç”¨è‡ªé€‚åº”æ¢¯åº¦æ‰‹æœ¯è§£å†³ä»»åŠ¡æ¢¯åº¦å†²çªï¼Œé˜²æ­¢æ¨¡å‹å´©æºƒã€‚å°†é¢„è®­ç»ƒç¼–ç å™¨é›†æˆåˆ°åŸºäºæ³¨æ„åŠ›çš„å¤šå®ä¾‹å­¦ä¹ èšåˆå™¨ä¸­ï¼Œç”¨äºè¿›è¡Œè¢‹çº§é¢„æµ‹å’Œæ³¨æ„åŠ›å¼•å¯¼çš„æœ€å¼‚å¸¸å®ä¾‹æ£€ç´¢ã€‚åœ¨å…¬å¼€å¯ç”¨çš„éª¨é«“ç»†èƒå­¦æ•°æ®é›†ä¸Šï¼Œå½“æ¨¡æ‹Ÿç›®å‡»ç‡ä»10%ä¸‹é™åˆ°0.5%æ—¶ï¼ŒSLAM-AGSåœ¨è¢‹çº§F1åˆ†æ•°å’Œå‰400ä¸ªé˜³æ€§ç»†èƒæ£€ç´¢æ–¹é¢ä¼˜äºå…¶ä»–é¢„è®­ç»ƒæ–¹æ³•ï¼Œåœ¨ä½ç›®å‡»ç‡ä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºã€‚è¿™è¡¨æ˜è§£å†³æ¢¯åº¦å¹²æ‰°å¯å®ç°ç¨³å®šé¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—ç»†èƒå­¦é¢ä¸´å®ä¾‹æ ‡ç­¾ä¸å¯é å’Œç›®å‡»ç‡ä½çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºSLAM-AGSï¼šä¸€ç§è”åˆä¼˜åŒ–å¼±ç›‘ç£ç›¸ä¼¼åº¦ç›®æ ‡å’Œè‡ªæˆ‘ç›‘ç£å¯¹æ¯”ç›®æ ‡çš„é¢„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>é‡‡ç”¨è‡ªé€‚åº”æ¢¯åº¦æ‰‹æœ¯ç¨³å®šå­¦ä¹ å¹¶é˜²æ­¢æ¨¡å‹å´©æºƒã€‚</li>
<li>é›†æˆé¢„è®­ç»ƒç¼–ç å™¨è¿›è¡Œè¢‹çº§é¢„æµ‹å’Œæ³¨æ„åŠ›å¼•å¯¼çš„æœ€å¼‚å¸¸å®ä¾‹æ£€ç´¢ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šï¼ŒSLAM-AGSåœ¨ä½ç›®å‡»ç‡ä¸‹è¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨è¢‹çº§F1åˆ†æ•°å’Œå‰400ä¸ªé˜³æ€§ç»†èƒæ£€ç´¢æ–¹é¢ã€‚</li>
<li>SLAM-AGSé€šè¿‡è§£å†³æ¢¯åº¦å¹²æ‰°å®ç°ç¨³å®šé¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7fc6baf1876254e663d8cd8786c4a68" align="middle">
<img src="https://picx.zhimg.com/v2-c802ee06233946d70ab90addf3018e1b" align="middle">
<img src="https://picx.zhimg.com/v2-0a593b90984a669e2f30509b2abfb6d4" align="middle">
<img src="https://picx.zhimg.com/v2-564ee4c89933be589ec9003c4e7dd166" align="middle">
<img src="https://picx.zhimg.com/v2-01042d2185854e43f2b6cc02c9272be3" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Specialized-Large-Language-Model-for-Clinical-Reasoning-and-Diagnosis-in-Rare-Diseases"><a href="#A-Specialized-Large-Language-Model-for-Clinical-Reasoning-and-Diagnosis-in-Rare-Diseases" class="headerlink" title="A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases"></a>A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases</h2><p><strong>Authors:Tao Yang, Dandan Huang, Yunting Lin, Pengfei Wu, Zhikun Wu, Gangyuan Ma, Yulan Lu, Xinran Dong, Dingpeng Li, Junshuang Ge, Zhiyan Zhang, Xuanzhao Huang, Wenyan Nong, Yao Zhou, Hui Tang, Hongxi Yang, Shijie Zhang, Juan Li, Xiaojun Cao, Lin Yang, Xia Gao, Kaishou Xu, Xiaoqiong Gu, Wen Zhang, Huimin Xia, Li Liu, Wenhao Zhou, Mulin Jun Li</strong></p>
<p>Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general&#x2F;medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.</p>
<blockquote>
<p>ç½•è§ç–¾ç—…å½±å“å…¨çƒæ•°äº¿äººï¼Œä½†è¯Šæ–­è¿‡ç¨‹å¾€å¾€é•¿è¾¾æ•°å¹´ã€‚ä¼ ç»Ÿç®¡é“å°†å˜ˆæ‚çš„è¯æ®æå–ä¸ä¸‹æ¸¸æ¨æ–­æ€§è¯Šæ–­ç›¸åˆ†ç¦»ï¼Œè€Œé€šç”¨&#x2F;åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´ç°å®ä¸–ç•Œç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰çš„ç¨€ç¼ºã€é¢†åŸŸçŸ¥è¯†çš„é™ˆæ—§ä»¥åŠå¹»è§‰ç­‰é—®é¢˜ã€‚æˆ‘ä»¬æ±‡é›†äº†å¤§é‡ä¸“ä¸šé¢†åŸŸç‰¹å®šçš„ä¸´åºŠè¯­æ–™åº“å’Œç»ä¸´åºŠåŒ»ç”ŸéªŒè¯çš„æ¨ç†é›†ï¼Œå¹¶é€šè¿‡åˆ†é˜¶æ®µæŒ‡ä»¤è°ƒæ•´ã€æ€ç»´é“¾å­¦ä¹ å’ŒåŸºäºå›¾çš„æ£€ç´¢å¼€å‘å‡ºäº†RareSeek R1ã€‚åœ¨å¤šä¸­å¿ƒEHRå™äº‹å’Œå…¬å…±åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRareSeek R1è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€ç¨³å¥çš„æ¨å¹¿æ€§å’Œåœ¨å˜ˆæ‚æˆ–é‡å è¡¨å‹ä¸‹çš„ç¨³å®šæ€§ã€‚å½“å™äº‹ä¸ä¼˜å…ˆå˜ä½“é…å¯¹æ—¶ï¼Œå¢å¼ºæ£€ç´¢è§£å†³äº†æ­§ä¹‰ï¼Œå¹¶å°†å€™é€‰äººä¸æœºåˆ¶å¯¹é½ï¼Œä»è€Œè·å¾—æœ€å¤§çš„æ”¶ç›Šã€‚äººç±»ç ”ç©¶è¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¸ç»éªŒä¸°å¯Œçš„åŒ»ç”Ÿç›¸å½“ï¼Œå¹¶ä¸”åœ¨è¾…åŠ©ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºæŒç»­çš„å¢ç›Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€æ˜çš„æ¨ç†çªå‡ºäº†è®¸å¤šæ­£ç¡®è¯Šæ–­æ‰€ä¾èµ–çš„å†³å®šæ€§éè¡¨å‹è¯æ®ï¼ˆå 23.1%ï¼Œä¾‹å¦‚æˆåƒã€å¹²é¢„ã€åŠŸèƒ½æµ‹è¯•ï¼‰ã€‚è¿™é¡¹å·¥ä½œæ¨è¿›äº†ä¸€ç§ä»¥å™äº‹ä¸ºä¸»ã€çŸ¥è¯†æ•´åˆçš„æ¨ç†èŒƒå¼ï¼Œç¼©çŸ­äº†è¯Šæ–­æ—…ç¨‹ï¼Œå¹¶å®ç°äº†å¯å®¡æ ¸çš„ã€ä¸´åºŠä¸Šå¯è½¬åŒ–çš„å†³ç­–æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14638v1">PDF</a> 50 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼šç½•è§ç–¾ç—…å½±å“å…¨çƒæ•°äº¿äººï¼Œè¯Šæ–­è¿‡ç¨‹è€—æ—¶å¤šå¹´ã€‚ä¸ºè§£å†³ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•å­˜åœ¨çš„è¯æ®æå–ä¸æ¨ç†åˆ†ç¦»ã€å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†ç°å®ä¸–ç•Œç”µå­å¥åº·è®°å½•ä¸è¶³ã€é¢†åŸŸçŸ¥è¯†æ»ååŠè™šæ„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡ã€ä¸“ä¸šåŒ–çš„ä¸´åºŠè¯­æ–™åº“å’Œç»è¿‡åŒ»ç”ŸéªŒè¯çš„æ¨ç†é›†ï¼Œé€šè¿‡åˆ†é˜¶æ®µæŒ‡ä»¤è°ƒæ•´ã€æ€ç»´é“¾å­¦ä¹ å’Œå›¾æ£€ç´¢æŠ€æœ¯ï¼Œç ”å‘å‡ºåä¸ºâ€œRareSeek R1â€çš„ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåœ¨å¤šä¸­å¿ƒç”µå­å¥åº·è®°å½•å™è¿°å’Œå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œåœ¨å™ªå£°æˆ–é‡å ç°è±¡ä¸‹çš„ç¨³å®šæ€§ã€‚ç‰¹åˆ«æ˜¯ç»“åˆå™äº‹ä¼˜å…ˆå˜ä½“è§£å†³æ­§ä¹‰å’Œå¯¹åº”æœºåˆ¶ï¼Œè¾…åŠ©æ£€ç´¢å¢ç›Šæœ€ä¸ºæ˜¾è‘—ã€‚äººç±»å®éªŒè¡¨æ˜å…¶è¡¨ç°ä¸èµ„æ·±åŒ»å¸ˆç›¸å½“ï¼Œå¹¶åœ¨è¾…åŠ©ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºæŒç»­ä¼˜åŠ¿ã€‚è¯¥ç³»ç»Ÿå¼ºè°ƒéç°è±¡è¯æ®çš„é‡è¦æ€§ï¼ˆå¦‚å½±åƒã€å¹²é¢„ã€åŠŸèƒ½æµ‹è¯•ç­‰ï¼‰ï¼Œæ¨åŠ¨äº†ä¸€ç§ä»¥å™äº‹ä¸ºå…ˆã€çŸ¥è¯†æ•´åˆçš„æ¨ç†æ¨¡å¼ï¼Œç¼©çŸ­è¯Šæ–­æ—…ç¨‹ï¼Œå®ç°å¯å®¡è®¡ã€å¯ä¸´åºŠè½¬åŒ–çš„å†³ç­–æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç½•è§ç–¾ç—…å¯¹å…¨çƒæ•°ç™¾ä¸‡äººäº§ç”Ÿå½±å“ï¼Œè¯Šæ–­è¿‡ç¨‹å…·æœ‰æŒ‘æˆ˜æ€§å’Œè€—æ—¶æ€§ã€‚</li>
<li>å­˜åœ¨è¯æ®æå–ä¸è¯Šæ–­æ¨ç†ä¹‹é—´çš„è„±èŠ‚é—®é¢˜ä»¥åŠç°æœ‰æ¨¡å‹å¤„ç†ç°å®å¥åº·è®°å½•çš„æŒ‘æˆ˜ã€‚</li>
<li>RareSeek R1ç³»ç»Ÿé€šè¿‡å¤§è§„æ¨¡ä¸´åºŠè¯­æ–™åº“å’ŒåŒ»ç”ŸéªŒè¯çš„æ¨ç†é›†å¼€å‘è€Œæˆï¼Œå…·æœ‰å…ˆè¿›çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>RareSeek R1åœ¨å¤šä¸­å¿ƒç”µå­å¥åº·è®°å½•å™è¿°å’Œå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚</li>
<li>è¾…åŠ©æ£€ç´¢ç»“åˆå™äº‹ä¼˜å…ˆå˜ä½“è§£å†³äº†æ­§ä¹‰å’Œå¯¹åº”æœºåˆ¶é—®é¢˜ï¼Œå–å¾—äº†æœ€å¤§çš„æ”¶ç›Šã€‚</li>
<li>äººç±»å®éªŒè¯æ˜å…¶è¡¨ç°ä¸èµ„æ·±åŒ»å¸ˆç›¸å½“ï¼Œå¹¶åœ¨è¾…åŠ©ä½¿ç”¨æ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a0eee5adb50714d453a4492a537ad56" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Agentic-Autonomous-Scientific-Discovery-with-Vision-Language-Model-Capabilities"><a href="#Enhancing-Agentic-Autonomous-Scientific-Discovery-with-Vision-Language-Model-Capabilities" class="headerlink" title="Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities"></a>Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities</h2><p><strong>Authors:Kahaan Gandhi, Boris Bolliet, Inigo Zubeldia</strong></p>
<p>We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: <a target="_blank" rel="noopener" href="https://github.com/CMBAgents/cmbagent">https://github.com/CMBAgents/cmbagent</a></p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†ç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼•å¯¼çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¦‚ä½•æ”¹è¿›ç«¯åˆ°ç«¯çš„è‡ªä¸»ç§‘å­¦å‘ç°ã€‚é€šè¿‡å°†å›¾è¡¨è§†ä¸ºå¯éªŒè¯çš„æ£€æŸ¥ç‚¹ï¼ŒVLMä½œä¸ºæ³•å®˜æ ¹æ®åŠ¨æ€ç”Ÿæˆçš„é¢†åŸŸç‰¹å®šè§„åˆ™å¯¹å›¾è¡¨è¿›è¡Œè¯„ä¼°ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿçº æ­£è‡ªå·±çš„é”™è¯¯ï¼Œå¹¶å®æ—¶å¼•å¯¼æ¢ç´¢æ€§æ•°æ®åˆ†æã€‚å®‡å®™å­¦å’Œå¤©ä½“åŒ–å­¦çš„æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†ä»é”™è¯¯çš„æ¨ç†è·¯å¾„ä¸­æ¢å¤ä»¥åŠé€‚åº”æ–°æ•°æ®é›†çš„èƒ½åŠ›ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚åœ¨é¢å‘æ•°æ®é©±åŠ¨å‘ç°çš„10é¡¹ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ä»…ä½¿ç”¨ä»£ç çš„0.2-0.3åˆ†å’Œä»£ç ä¸æ–‡æœ¬åŸºå‡†çº¿çš„0.4-0.5åˆ†ç›¸æ¯”ï¼Œå¢å¼ºå‹VLMç³»ç»Ÿçš„é€šè¿‡ç‡é«˜è¾¾0.7-0.8ï¼ŒåŒæ—¶æä¾›å¯å®¡æ ¸çš„æ¨ç†è½¨è¿¹ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ã€‚ä»£ç è¯¦æƒ…å‚è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/CMBAgents/cmbagent">https://github.com/CMBAgents/cmbagent</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14631v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼•å¯¼çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿæ”¹è¿›ç«¯åˆ°ç«¯çš„è‡ªä¸»ç§‘å­¦å‘ç°ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†å›¾è¡¨è§†ä¸ºå¯éªŒè¯çš„æ£€æŸ¥ç‚¹ï¼Œåˆ©ç”¨VLMä½œä¸ºè¯„ä¼°å™¨å¯¹å›¾è¡¨è¿›è¡ŒåŠ¨æ€ç”Ÿæˆçš„é¢†åŸŸç‰¹å®šè§„åˆ™è¯„ä¼°ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿçº æ­£è‡ªå·±çš„é”™è¯¯å¹¶åœ¨å®æ—¶ä¸­å¼•å¯¼æ¢ç´¢æ€§æ•°æ®åˆ†æã€‚åœ¨å®‡å®™å­¦å’Œå¤©ä½“åŒ–å­¦çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæ¼”ç¤ºäº†ä»é”™è¯¯çš„æ¨ç†è·¯å¾„ä¸­æ¢å¤ä»¥åŠé€‚åº”æ–°æ•°æ®é›†çš„èƒ½åŠ›ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚åœ¨æ•°æ®é©±åŠ¨å‘ç°çš„10é¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨VLMå¢å¼ºçš„ç³»ç»Ÿå®ç°äº†é€šè¿‡ç‡ä¸º0.7-0.8çš„åˆ†æ•°ï¼Œè€Œä»…ä½¿ç”¨ä»£ç å’Œä»£ç ä¸æ–‡æœ¬çš„åŸºçº¿åˆ†åˆ«ä¸º0.2-0.3å’Œ0.4-0.5ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè¿˜æä¾›å¯å®¡æ ¸çš„æ¨ç†è½¨è¿¹ï¼Œæé«˜å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰èƒ½æé«˜è‡ªä¸»ç§‘å­¦å‘ç°çš„æ•ˆç‡ã€‚</li>
<li>VLMä½œä¸ºè¯„ä¼°å™¨ï¼Œæ ¹æ®åŠ¨æ€ç”Ÿæˆçš„é¢†åŸŸç‰¹å®šè§„åˆ™å¯¹å›¾è¡¨è¿›è¡ŒéªŒè¯ã€‚</li>
<li>æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªæˆ‘çº æ­£é”™è¯¯å¹¶åœ¨å®æ—¶ä¸­è°ƒæ•´æ¢ç´¢æ€§æ•°æ®åˆ†æè·¯å¾„ã€‚</li>
<li>åœ¨å®‡å®™å­¦å’Œå¤©ä½“åŒ–å­¦é¢†åŸŸï¼Œè¯¥ç³»ç»Ÿèƒ½åœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹æ¢å¤ä»é”™è¯¯æ¨ç†è·¯å¾„å¹¶é€‚åº”æ–°æ•°æ®é›†ã€‚</li>
<li>åœ¨æ•°æ®é©±åŠ¨å‘ç°çš„10é¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVLMå¢å¼ºç³»ç»Ÿçš„é€šè¿‡ç‡æ˜¾è‘—é«˜äºä»…ä½¿ç”¨ä»£ç æˆ–ä»£ç ä¸æ–‡æœ¬çš„åŸºçº¿ã€‚</li>
<li>ç³»ç»Ÿæä¾›å¯å®¡æ ¸çš„æ¨ç†è½¨è¿¹ï¼Œæé«˜å†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e6eb89a8dff32a8ff27fcdf88974051" align="middle">
<img src="https://picx.zhimg.com/v2-a7b4389f8fba64a1caeab09da05cb04b" align="middle">
<img src="https://picx.zhimg.com/v2-c13f0d251fe3d600e67d029d1575d574" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Seer-Online-Context-Learning-for-Fast-Synchronous-LLM-Reinforcement-Learning"><a href="#Seer-Online-Context-Learning-for-Fast-Synchronous-LLM-Reinforcement-Learning" class="headerlink" title="Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning"></a>Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning</h2><p><strong>Authors:Ruoyu Qin, Weiran He, Weixiao Huang, Yangkun Zhang, Yikai Zhao, Bo Pang, Xinran Xu, Yingdi Shan, Yongwei Wu, Mingxing Zhang</strong></p>
<p>Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.</p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹äºæ¨åŠ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„åŒæ­¥RLç³»ç»Ÿé¢ä¸´ä¸¥é‡çš„æ€§èƒ½ç“¶é¢ˆã€‚æ»šåŠ¨å±•å¼€é˜¶æ®µåœ¨ç«¯åˆ°ç«¯è¿­ä»£æ—¶é—´ä¸­å ä¸»å¯¼åœ°ä½ï¼Œç”±äºå›ºæœ‰çš„å·¥ä½œé‡ä¸å¹³è¡¡ï¼Œå¯¼è‡´é•¿å°¾å»¶è¿Ÿä¸¥é‡ä¸”èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†Seerï¼Œä¸€ç§æ–°å‹åœ¨çº¿ä¸Šä¸‹æ–‡å­¦ä¹ ç³»ç»Ÿï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä¹‹å‰è¢«å¿½è§†çš„ç›¸åŒæç¤ºä¸‹è¯·æ±‚è¾“å‡ºé•¿åº¦å’Œç”Ÿæˆæ¨¡å¼ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚Seerå¼•å…¥äº†ä¸‰ç§å…³é”®æŠ€æœ¯ï¼šç”¨äºåŠ¨æ€è´Ÿè½½å‡è¡¡çš„åˆ†å‰²æ»šåŠ¨ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥è°ƒåº¦å’Œè‡ªé€‚åº”åˆ†ç»„æŠ•æœºè§£ç ã€‚è¿™äº›æœºåˆ¶å…±åŒæ˜¾è‘—å‡å°‘äº†é•¿å°¾å»¶è¿Ÿï¼Œæé«˜äº†æ»šåŠ¨è¿‡ç¨‹ä¸­çš„èµ„æºæ•ˆç‡ã€‚åœ¨ç”Ÿäº§çº§RLå·¥ä½œé‡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€æ–°çš„åŒæ­¥RLç³»ç»Ÿç›¸æ¯”ï¼ŒSeerçš„ç«¯åˆ°ç«¯æ»šåŠ¨ååé‡æé«˜äº†74%åˆ°97%ï¼Œé•¿å°¾å»¶è¿Ÿå‡å°‘äº†75%åˆ°93%ï¼Œæ˜¾è‘—åŠ é€Ÿäº†RLè®­ç»ƒè¿­ä»£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14617v1">PDF</a> 16 pages, 12 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†Seerï¼Œä¸€ä¸ªè§£å†³å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨ä¸­é¢ä¸´æ€§èƒ½ç“¶é¢ˆçš„æ–°é¢–åœ¨çº¿ä¸Šä¸‹æ–‡å­¦ä¹ ç³»ç»Ÿã€‚é’ˆå¯¹ç°æœ‰åŒæ­¥RLç³»ç»Ÿå­˜åœ¨çš„é•¿å°¾å»¶è¿Ÿå’Œèµ„æºåˆ©ç”¨ç‡ä½ä¸‹çš„é—®é¢˜ï¼ŒSeeré€šè¿‡åˆ©ç”¨ç›¸åŒæç¤ºçš„è¯·æ±‚è¾“å‡ºé•¿åº¦å’Œç”Ÿæˆæ¨¡å¼çš„ç›¸ä¼¼æ€§ï¼Œæå‡ºäº†åŠ¨æ€è´Ÿè½½å‡è¡¡çš„åˆ†å‰²æ»šåŠ¨ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥è°ƒåº¦å’Œè‡ªé€‚åº”åˆ†ç»„æ¨æµ‹è§£ç ä¸‰é¡¹å…³é”®æŠ€æœ¯ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒSeeråœ¨ç”Ÿäº§çº§çš„RLå·¥ä½œè´Ÿè½½ä¸Šèƒ½æ˜¾è‘—æé«˜ç«¯åˆ°ç«¯çš„æ»šåŠ¨ååé‡å¹¶å¤§å¹…åº¦å‡å°‘é•¿å°¾å»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ç°ä»£å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰åŒæ­¥RLç³»ç»Ÿå­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>Seeræ˜¯ä¸€ä¸ªè§£å†³è¿™ä¸€é—®é¢˜çš„æ–°é¢–åœ¨çº¿ä¸Šä¸‹æ–‡å­¦ä¹ ç³»ç»Ÿã€‚</li>
<li>Seeré€šè¿‡åˆ©ç”¨ç›¸åŒæç¤ºçš„è¯·æ±‚è¾“å‡ºé•¿åº¦å’Œç”Ÿæˆæ¨¡å¼çš„ç›¸ä¼¼æ€§æ¥æå‡æ€§èƒ½ã€‚</li>
<li>Seeræå‡ºäº†ä¸‰é¡¹å…³é”®æŠ€æœ¯ï¼šåŠ¨æ€è´Ÿè½½å‡è¡¡çš„åˆ†å‰²æ»šåŠ¨ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥è°ƒåº¦å’Œè‡ªé€‚åº”åˆ†ç»„æ¨æµ‹è§£ç ã€‚</li>
<li>Seerèƒ½æ˜¾è‘—æé«˜ç«¯åˆ°ç«¯çš„æ»šåŠ¨ååé‡ï¼Œæ”¹å–„èµ„æºåˆ©ç”¨ç‡ã€‚</li>
<li>ç›¸è¾ƒäºç°æœ‰åŒæ­¥RLç³»ç»Ÿï¼ŒSeeråœ¨é•¿å°¾å»¶è¿Ÿæ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-050cdfe88c5f2d96d91cb89299906060" align="middle">
<img src="https://picx.zhimg.com/v2-dae76b1c7c07bd50fe054d3319c91e4f" align="middle">
<img src="https://picx.zhimg.com/v2-a45f55e609cc1f161672778e0b3a2eac" align="middle">
<img src="https://picx.zhimg.com/v2-a8b252426e677ad5b607f16fd6e5ec18" align="middle">
<img src="https://picx.zhimg.com/v2-187f980c3f03e7e97f7902c8b586af99" align="middle">
<img src="https://picx.zhimg.com/v2-8bd2ec6e9de434fa72d09146d5024292" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Bridging-Human-and-Model-Perspectives-A-Comparative-Analysis-of-Political-Bias-Detection-in-News-Media-Using-Large-Language-Models"><a href="#Bridging-Human-and-Model-Perspectives-A-Comparative-Analysis-of-Political-Bias-Detection-in-News-Media-Using-Large-Language-Models" class="headerlink" title="Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models"></a>Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models</h2><p><strong>Authors:Shreya Adrita Banik, Niaz Nafi Rahman, Tahsina Moiukh, Farig Sadeque</strong></p>
<p>Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.</p>
<blockquote>
<p>æ£€æµ‹æ–°é—»åª’ä½“ä¸­çš„æ”¿æ²»åè§æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦è§£é‡Šå¾®å¦™çš„è¯­è¨€å’Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚å°½ç®¡è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„æœ€æ–°è¿›å±•å·²ç»å®ç°äº†è‡ªåŠ¨åè§åˆ†ç±»ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åˆ¤æ–­çš„å¯¹é½ç¨‹åº¦ä»ç„¶ç›¸å¯¹æœªè¢«å……åˆ†æ¢ç´¢å’Œç†è§£ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ä¸ªæ¯”è¾ƒæ¡†æ¶ï¼Œä»¥è¯„ä¼°äººç±»æ³¨é‡Šå’Œå¤šç§LLMï¼ˆåŒ…æ‹¬GPTã€BERTã€RoBERTaå’ŒFLANï¼‰å¯¹æ”¿æ²»åè§çš„æ£€æµ‹ã€‚æˆ‘ä»¬æ„å»ºäº†æ–°é—»æ–‡ç« çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†ï¼Œå¹¶è¯„ä¼°æ³¨é‡Šä¸€è‡´æ€§ã€åè§ææ€§å’Œæ¨¡å‹é—´åè®®ï¼Œä»¥é‡åŒ–äººç±»å’Œæ¨¡å‹å¯¹åè§æ„ŸçŸ¥ä¹‹é—´çš„å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¼ ç»Ÿçš„åŸºäºå˜æ¢å™¨æ¨¡å‹ä¸­ï¼ŒRoBERTaä¸äººæ ‡ç­¾çš„å¯¹é½åº¦æœ€é«˜ï¼Œè€ŒGPTç­‰ç”Ÿæˆæ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ä¸äººç±»æ³¨é‡Šçš„æ•´ä½“åè®®æœ€å¼ºã€‚åœ¨æ‰€æœ‰åŸºäºå˜æ¢å™¨çš„åŸºçº¿ä¸­ï¼Œæˆ‘ä»¬å¾®è°ƒçš„RoBERTaæ¨¡å‹è·å¾—äº†æœ€é«˜çš„å‡†ç¡®æ€§å’Œä¸äººç±»æ³¨é‡Šæ ‡ç­¾çš„æœ€å¼ºå¯¹é½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œäººç±»å’ŒLLMåœ¨æ„ŸçŸ¥æ”¿æ²»å€¾å‘æ–¹é¢å­˜åœ¨ç³»ç»Ÿå·®å¼‚ï¼Œå¼ºè°ƒéœ€è¦åœ¨è‡ªåŠ¨åŒ–åª’ä½“åè§æ£€æµ‹ä¸­ç»“åˆäººç±»è§£é‡Šæ€§å’Œæ¨¡å‹å¯æ‰©å±•æ€§çš„æ··åˆè¯„ä¼°æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14606v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–°é—»åª’ä½“çš„æ”¿æ²»å€¾å‘æ€§æ£€æµ‹æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦è§£è¯»è¯­è¨€å’Œä¸Šä¸‹æ–‡ä¸­çš„å¾®å¦™çº¿ç´¢ã€‚å°½ç®¡è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„è¿‘æœŸè¿›å±•å·²ç»å®ç°äº†è‡ªåŠ¨åè§åˆ†ç±»ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åˆ¤æ–­ä¹‹é—´çš„å¥‘åˆç¨‹åº¦ä»ç„¶ç›¸å¯¹è¾ƒä½ä¸”å°šæœªå¾—åˆ°å……åˆ†ç†è§£ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ä¸ªæ¯”è¾ƒæ¡†æ¶ï¼Œä»¥è¯„ä¼°äººç±»æ³¨é‡Šå’Œå¤šç§LLMså¯¹æ”¿æ²»å€¾å‘æ€§çš„æ£€æµ‹æ•ˆæœï¼ŒåŒ…æ‹¬GPTã€BERTã€RoBERTaå’ŒFLANã€‚æˆ‘ä»¬æ„å»ºäº†æ–°é—»æ–‡ç« çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†ï¼Œå¹¶è¯„ä¼°æ³¨é‡Šä¸€è‡´æ€§ã€åè§ææ€§å’Œæ¨¡å‹é—´åè®®ï¼Œä»¥é‡åŒ–äººç±»å’Œæ¨¡å‹å¯¹åè§æ„ŸçŸ¥ä¹‹é—´çš„å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¼ ç»Ÿçš„åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ä¸­ï¼ŒRoBERTaä¸äººç±»æ ‡ç­¾çš„å¥‘åˆåº¦æœ€é«˜ï¼Œè€Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œç”Ÿæˆæ¨¡å‹å¦‚GPTä¸äººç±»æ³¨é‡Šçš„å¥‘åˆåº¦æœ€å¼ºã€‚åœ¨æ‰€æœ‰åŸºäºè½¬æ¢å™¨çš„åŸºçº¿æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¾®è°ƒè¿‡çš„RoBERTaæ¨¡å‹è·å¾—äº†æœ€é«˜çš„å‡†ç¡®æ€§å’Œä¸äººç±»æ³¨é‡Šçš„æœ€å¼ºå¥‘åˆåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†äººç±»å’ŒLLMsåœ¨æ„ŸçŸ¥æ”¿æ²»å€¾å‘æ€§æ–¹é¢çš„ç³»ç»Ÿå·®å¼‚ï¼Œå¼ºè°ƒäº†åœ¨è‡ªåŠ¨åŒ–åª’ä½“åè§æ£€æµ‹ä¸­ï¼Œéœ€è¦ç»“åˆäººç±»å¯è§£é‡Šæ€§å’Œæ¨¡å‹å¯æ‰©å±•æ€§çš„æ··åˆè¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¿æ²»åè§æ£€æµ‹éœ€è§£è¯»è¯­è¨€å’Œä¸Šä¸‹æ–‡ä¸­çš„å¾®å¦™çº¿ç´¢ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åè§åˆ†ç±»æ–¹é¢çš„åº”ç”¨å·²æ˜¾ç°ï¼Œä½†ä¸äººåˆ¤æ–­çš„å¥‘åˆç¨‹åº¦ä»éœ€æ·±å…¥ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶é€šè¿‡æ¯”è¾ƒæ¡†æ¶è¯„ä¼°äº†å¤šç§LLMsåœ¨æ£€æµ‹æ”¿æ²»å€¾å‘æ€§æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>æ‰‹åŠ¨æ³¨é‡Šçš„æ–°é—»æ–‡ç« æ•°æ®é›†è¢«æ„å»ºä»¥è¯„ä¼°äººç±»ä¸æ¨¡å‹å¯¹åè§æ„ŸçŸ¥çš„å·®å¼‚ã€‚</li>
<li>RoBERTaæ¨¡å‹åœ¨ä¼ ç»Ÿçš„åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ä¸­ä¸äººç±»æ ‡ç­¾å¥‘åˆåº¦æœ€é«˜ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹å¦‚GPTåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä¸äººç±»æ³¨é‡Šå¥‘åˆåº¦å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e389c6d14a25295c9b708b9c0066612f" align="middle">
<img src="https://picx.zhimg.com/v2-0aa04939facaf8346ce5e9bbed174022" align="middle">
<img src="https://picx.zhimg.com/v2-3e365113404eed3ee4d137e14e8c1d3f" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="XAttn-BMD-Multimodal-Deep-Learning-with-Cross-Attention-for-Femoral-Neck-Bone-Mineral-Density-Estimation"><a href="#XAttn-BMD-Multimodal-Deep-Learning-with-Cross-Attention-for-Femoral-Neck-Bone-Mineral-Density-Estimation" class="headerlink" title="XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation"></a>XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation</h2><p><strong>Authors:Yilin Zhang, Leo D. Westbury, Elaine M. Dennison, Nicholas C. Harvey, Nicholas R. Fuggle, Rahman Attar</strong></p>
<p>Poor bone health is a significant public health concern, and low bone mineral density (BMD) leads to an increased fracture risk, a key feature of osteoporosis. We present XAttn-BMD (Cross-Attention BMD), a multimodal deep learning framework that predicts femoral neck BMD from hip X-ray images and structured clinical metadata. It utilizes a novel bidirectional cross-attention mechanism to dynamically integrate image and metadata features for cross-modal mutual reinforcement. A Weighted Smooth L1 loss is tailored to address BMD imbalance and prioritize clinically significant cases. Extensive experiments on the data from the Hertfordshire Cohort Study show that our model outperforms the baseline models in regression generalization and robustness. Ablation studies confirm the effectiveness of both cross-attention fusion and the customized loss function. Experimental results show that the integration of multimodal data via cross-attention outperforms naive feature concatenation without cross-attention, reducing MSE by 16.7%, MAE by 6.03%, and increasing the R2 score by 16.4%, highlighting the effectiveness of the approach for femoral neck BMD estimation. Furthermore, screening performance was evaluated using binary classification at clinically relevant femoral neck BMD thresholds, demonstrating the modelâ€™s potential in real-world scenarios.</p>
<blockquote>
<p>ä¸è‰¯çš„éª¨éª¼å¥åº·æ˜¯ä¸€ä¸ªé‡è¦çš„å…¬å…±å«ç”Ÿé—®é¢˜ï¼Œä½éª¨çŸ¿ç‰©è´¨å¯†åº¦ï¼ˆBMDï¼‰ä¼šå¢åŠ éª¨æŠ˜é£é™©ï¼Œè¿™æ˜¯éª¨è´¨ç–æ¾ç—‡çš„ä¸»è¦ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†XAttn-BMDï¼ˆè·¨æ³¨æ„åŠ›BMDï¼‰ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥ä»é«‹å…³èŠ‚Xå°„çº¿å›¾åƒå’Œç»“æ„åŒ–çš„ä¸´åºŠå…ƒæ•°æ®ä¸­é¢„æµ‹è‚¡éª¨é¢ˆBMDã€‚å®ƒåˆ©ç”¨ä¸€ç§æ–°é¢–çš„åŒå‘è·¨æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ¨æ€èåˆå›¾åƒå’Œå…ƒæ•°æ®ç‰¹å¾ï¼Œä»¥å®ç°è·¨æ¨¡æ€ç›¸äº’å¢å¼ºã€‚é’ˆå¯¹BMDä¸å¹³è¡¡é—®é¢˜ï¼Œå®šåˆ¶äº†åŠ æƒå¹³æ»‘L1æŸå¤±ï¼Œå¹¶ä¼˜å…ˆè€ƒè™‘å…·æœ‰ä¸´åºŠæ„ä¹‰çš„æƒ…å†µã€‚åœ¨èµ«ç‰¹ç¦å¾·éƒ¡é˜Ÿåˆ—ç ”ç©¶çš„æ•°æ®ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å›å½’æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ¶ˆèç ”ç©¶è¯å®äº†è·¨æ³¨æ„åŠ›èåˆå’Œå®šåˆ¶æŸå¤±å‡½æ•°çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æ•´åˆå¤šæ¨¡æ€æ•°æ®ä¼˜äºæ²¡æœ‰è·¨æ³¨æ„åŠ›çš„ç®€å•ç‰¹å¾æ‹¼æ¥ï¼Œå‡å°‘äº†å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰16.7%ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰å‡å°‘6.03%ï¼ŒR2å¾—åˆ†æé«˜16.4%ï¼Œå‡¸æ˜¾äº†è¯¥æ–¹æ³•åœ¨è‚¡éª¨é¢ˆBMDä¼°è®¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ä¸´åºŠä¸Šç›¸å…³çš„è‚¡éª¨é¢ˆBMDé˜ˆå€¼è¿›è¡ŒäºŒå…ƒåˆ†ç±»è¯„ä¼°ç­›æŸ¥æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†è¯¥æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14604v1">PDF</a> 11 figures, 10 tables, 38 pages. Submitted to Artificial Intelligence in Medicine (currently with editor)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºXAttn-BMDçš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»é«‹å…³èŠ‚Xå°„çº¿å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠå…ƒæ•°æ®ä¸­é¢„æµ‹è‚¡éª¨é¢ˆéª¨å¯†åº¦ã€‚å®ƒé‡‡ç”¨ä¸€ç§æ–°é¢–çš„åŒå‘è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ¨æ€é›†æˆå›¾åƒå’Œå…ƒæ•°æ®ç‰¹å¾ï¼Œå®ç°è·¨æ¨¡æ€ç›¸äº’å¢å¼ºã€‚åœ¨èµ«ç‰¹ç¦å¾·éƒ¡é˜Ÿåˆ—ç ”ç©¶çš„æ•°æ®ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å›å½’æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>XAttn-BMDæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä»é«‹å…³èŠ‚Xå°„çº¿å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠå…ƒæ•°æ®ä¸­é¢„æµ‹è‚¡éª¨é¢ˆéª¨å¯†åº¦ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŒå‘è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ¨æ€é›†æˆå›¾åƒå’Œå…ƒæ•°æ®ç‰¹å¾ï¼Œå®ç°è·¨æ¨¡æ€ç›¸äº’å¢å¼ºã€‚</li>
<li>å®šåˆ¶äº†åŠ æƒå¹³æ»‘L1æŸå¤±æ¥è§£å†³éª¨å¯†åº¦ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶ä¼˜å…ˆå¤„ç†å…·æœ‰ä¸´åºŠæ„ä¹‰çš„æƒ…å†µã€‚</li>
<li>åœ¨èµ«ç‰¹ç¦å¾·éƒ¡é˜Ÿåˆ—ç ”ç©¶çš„æ•°æ®ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒXAttn-BMDæ¨¡å‹åœ¨å›å½’æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†è·¨æ³¨æ„åŠ›èåˆå’Œå®šåˆ¶æŸå¤±å‡½æ•°çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é€šè¿‡è·¨æ³¨æ„åŠ›æ•´åˆå¤šæ¨¡æ€æ•°æ®çš„æ–¹æ³•ä¼˜äºç®€å•çš„ç‰¹å¾æ‹¼æ¥æ–¹æ³•ï¼Œé™ä½äº†å‡æ–¹è¯¯å·®å’Œå¹³å‡ç»å¯¹è¯¯å·®ï¼Œæé«˜äº†R2åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f78ee0cbf957fab8dfc96c95625f26d" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Masked-IRL-LLM-Guided-Reward-Disambiguation-from-Demonstrations-and-Language"><a href="#Masked-IRL-LLM-Guided-Reward-Disambiguation-from-Demonstrations-and-Language" class="headerlink" title="Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language"></a>Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language</h2><p><strong>Authors:Minyoung Hwang, Alexandra Forsey-Smerek, Nathaniel Dennler, Andreea Bobu</strong></p>
<p>Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: <a target="_blank" rel="noopener" href="https://mit-clear-lab.github.io/Masked-IRL">https://MIT-CLEAR-Lab.github.io/Masked-IRL</a> and Code: <a target="_blank" rel="noopener" href="https://github.com/MIT-CLEAR-Lab/Masked-IRL">https://github.com/MIT-CLEAR-Lab/Masked-IRL</a></p>
<blockquote>
<p>æœºå™¨äººå¯ä»¥é€šè¿‡ä»æ¼”ç¤ºä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°æ¥é€‚åº”ç”¨æˆ·åå¥½ï¼Œä½†åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¥–åŠ±æ¨¡å‹å¾€å¾€ä¼šè¿‡åº¦é€‚åº”å¶ç„¶çš„ç›¸å…³æ€§ï¼Œæ— æ³•æ¨å¹¿ã€‚è¿™æ˜¯å› ä¸ºæ¼”ç¤ºå‘Šè¯‰æœºå™¨äººå¦‚ä½•å®Œæˆä»»åŠ¡ï¼Œä½†æ²¡æœ‰å‘Šè¯‰å®ƒä»¬è¿™ä¸ªä»»åŠ¡çš„é‡ç‚¹æ˜¯ä»€ä¹ˆï¼Œå¯¼è‡´æ¨¡å‹å…³æ³¨äºæ— å…³ç´§è¦çš„çŠ¶æ€ç»†èŠ‚ã€‚è‡ªç„¶è¯­è¨€å¯ä»¥æ›´ç›´æ¥åœ°æŒ‡å®šæœºå™¨äººåº”è¯¥å…³æ³¨ä»€ä¹ˆï¼Œå¹¶ä¸”ç†è®ºä¸Šå¯ä»¥åœ¨ä¸æ¼”ç¤ºä¸€è‡´çš„è®¸å¤šå¥–åŠ±å‡½æ•°ä¸­æ¶ˆé™¤æ­§ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­è¨€æ¡ä»¶å¥–åŠ±å­¦ä¹ æ–¹æ³•é€šå¸¸å°†æŒ‡ä»¤è§†ä¸ºç®€å•çš„æ¡ä»¶ä¿¡å·ï¼Œæ²¡æœ‰å……åˆ†åˆ©ç”¨å®ƒä»¬è§£å†³æ­§ä¹‰æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒçœŸå®æŒ‡ä»¤é€šå¸¸æœ¬èº«å…·æœ‰æ¨¡ç³Šæ€§ï¼Œå› æ­¤ç®€å•çš„æ¡ä»¶è®¾ç½®å¹¶ä¸å¯é ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯è¿™ä¸¤ç§è¾“å…¥ç±»å‹æºå¸¦äº†äº’è¡¥ä¿¡æ¯ï¼šæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•è¡ŒåŠ¨ï¼Œè€Œè¯­è¨€åˆ™è¯´æ˜äº†é‡ç‚¹ã€‚æˆ‘ä»¬æå‡ºäº†Masked Inverse Reinforcement Learningï¼ˆMasked IRLï¼‰æ¡†æ¶ï¼Œå®ƒä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆä¸¤ç§è¾“å…¥ç±»å‹çš„ä¼˜åŠ¿ã€‚Masked IRLä»è¯­è¨€æŒ‡ä»¤ä¸­æ¨æ–­çŠ¶æ€ç›¸å…³æ€§æ©ç ï¼Œå¹¶å¼ºåˆ¶å¯¹æ— å…³çŠ¶æ€ç»„ä»¶å…·æœ‰ä¸å˜æ€§ã€‚å½“æŒ‡ä»¤æ¨¡ç³Šæ—¶ï¼Œå®ƒä¼šä½¿ç”¨LLMæ¨ç†åœ¨æ¼”ç¤ºçš„ä¸Šä¸‹æ–‡ä¸­æ¾„æ¸…å®ƒä»¬ã€‚åœ¨æ¨¡æ‹Ÿå’Œå®é™…æœºå™¨äººä¸Šï¼ŒMasked IRLåœ¨è¯­è¨€è°ƒèŠ‚çš„IRLæ–¹æ³•ä¸Šè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œæé«˜äº†é«˜è¾¾15%ï¼ŒåŒæ—¶ä½¿ç”¨æ•°æ®é«˜è¾¾4.7å€å°‘ï¼Œè¯æ˜äº†å…¶åœ¨æ ·æœ¬æ•ˆç‡ã€æ¨å¹¿èƒ½åŠ›ä»¥åŠå¤„ç†æ¨¡ç³Šè¯­è¨€æ–¹é¢çš„ç¨³å¥æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mit-clear-lab.github.io/Masked-IRL">https://MIT-CLEAR-Lab.github.io/Masked-IRL</a> å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/MIT-CLEAR-Lab/Masked-IRL">https://github.com/MIT-CLEAR-Lab/Masked-IRL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14565v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœºå™¨äººå¯é€šè¿‡å­¦ä¹ æ¼”ç¤ºä¸­çš„å¥–åŠ±å‡½æ•°æ¥é€‚åº”ç”¨æˆ·åå¥½ï¼Œä½†åœ¨æœ‰é™æ•°æ®æƒ…å†µä¸‹ï¼Œå¥–åŠ±æ¨¡å‹å¸¸å› è¿‡åº¦æ‹Ÿåˆè€Œå¤±å»æ³›åŒ–èƒ½åŠ›ã€‚åŸå› æ˜¯æ¼”ç¤ºå±•ç¤ºäº†æœºå™¨äººçš„ä»»åŠ¡æ‰§è¡Œæ–¹å¼ï¼Œä½†æœªè¯´æ˜ä»»åŠ¡çš„æ ¸å¿ƒæ‰€åœ¨ï¼Œå¯¼è‡´æ¨¡å‹å…³æ³¨äºæ— å…³çš„çŠ¶æ€ç»†èŠ‚ã€‚è‡ªç„¶è¯­è¨€èƒ½æ›´ç›´æ¥åœ°æŒ‡å®šæœºå™¨äººåº”å…³æ³¨çš„å†…å®¹ï¼Œå¹¶åœ¨åŸåˆ™ä¸Šæ¶ˆé™¤ä¸æ¼”ç¤ºä¸€è‡´çš„å¤šä¸ªå¥–åŠ±å‡½æ•°ä¹‹é—´çš„æ­§ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­è¨€è°ƒèŠ‚å¥–åŠ±å­¦ä¹ æ–¹æ³•é€šå¸¸å°†æŒ‡ä»¤è§†ä¸ºç®€å•çš„æ¡ä»¶ä¿¡å·ï¼Œæœªå……åˆ†åˆ©ç”¨å…¶è§£å†³æ­§ä¹‰æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒçœŸå®æŒ‡ä»¤é€šå¸¸å…·æœ‰æ­§ä¹‰æ€§ï¼Œå› æ­¤ç®€å•çš„æ¡ä»¶åŒ–å¹¶ä¸å¯é ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯è¿™ä¸¤ç§è¾“å…¥ç±»å‹æºå¸¦äº†äº’è¡¥ä¿¡æ¯ï¼šæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•è¡ŒåŠ¨ï¼Œè€Œè¯­è¨€åˆ™è¯´æ˜äº†é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Masked Inverse Reinforcement Learningï¼ˆMasked IRLï¼‰æ¡†æ¶ï¼Œç»“åˆä¸¤ç§è¾“å…¥çš„ä¼˜åŠ¿ã€‚Masked IRLä»è¯­è¨€æŒ‡ä»¤æ¨æ–­çŠ¶æ€ç›¸å…³æ©ç ï¼Œå¹¶æ‰§è¡Œå¯¹æ— å…³çŠ¶æ€åˆ†é‡çš„ä¸å˜æ€§ã€‚å½“æŒ‡ä»¤æ¨¡ç³Šæ—¶ï¼Œå®ƒä¼šåˆ©ç”¨LLMåœ¨æ¼”ç¤ºçš„è¯­å¢ƒä¸­æ˜ç¡®æŒ‡ä»¤ã€‚åœ¨æ¨¡æ‹Ÿå’Œå®é™…æœºå™¨äººä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMasked IRLåœ¨ä»…ä½¿ç”¨15%æ•°æ®çš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºå…ˆå‰çš„è¯­è¨€è°ƒèŠ‚IRLæ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›å’Œå¯¹æ¨¡ç³Šè¯­è¨€çš„ç¨³å¥æ€§ã€‚æ›´å¤šä¿¡æ¯å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://mit-clear-lab.github.io/Masked-IRL">https://MIT-CLEAR-Lab.github.io/Masked-IRL</a> å¹¶æŸ¥é˜…ä»£ç  <a target="_blank" rel="noopener" href="https://github.com/MIT-CLEAR-Lab/Masked-IRL%E3%80%82">https://github.com/MIT-CLEAR-Lab/Masked-IRLã€‚</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æœºå™¨äººé€šè¿‡å¥–åŠ±å‡½æ•°å­¦ä¹ é€‚åº”ç”¨æˆ·åå¥½æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æœ‰é™æ•°æ®æ¡ä»¶ä¸‹æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›é—®é¢˜ã€‚</li>
<li>è‡ªç„¶è¯­è¨€èƒ½æ›´ç›´æ¥åœ°è¯´æ˜ä»»åŠ¡çš„æ ¸å¿ƒå†…å®¹ï¼Œæœ‰åŠ©äºæœºå™¨äººç†è§£å¥–åŠ±å‡½æ•°çš„çœŸæ­£æ„å›¾ã€‚</li>
<li>ç°æœ‰è¯­è¨€è°ƒèŠ‚å¥–åŠ±å­¦ä¹ æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è¯­è¨€è§£å†³å¥–åŠ±å‡½æ•°ä¸­çš„æ­§ä¹‰æ€§ã€‚</li>
<li>çœŸå®æŒ‡ä»¤å¾€å¾€å…·æœ‰æ¨¡ç³Šæ€§ï¼Œç®€å•çš„æ¡ä»¶åŒ–æ–¹æ³•å¹¶ä¸è¶³ä»¥å¤„ç†è¿™ç§æ¨¡ç³Šæ€§ã€‚</li>
<li>Masked IRLæ¡†æ¶ç»“åˆæ¼”ç¤ºå’Œè¯­è¨€çš„ä¼˜åŠ¿ï¼Œé€šè¿‡è¯­è¨€æ¨¡å‹ç†è§£çŠ¶æ€çš„é‡è¦æ€§å¹¶æ¶ˆé™¤æ¼”ç¤ºä¸­çš„æ¨¡ç³Šæ€§ã€‚</li>
<li>Masked IRLæ¡†æ¶èƒ½æé«˜æ ·æœ¬æ•ˆç‡ã€æ³›åŒ–èƒ½åŠ›å’Œå¯¹æ¨¡ç³Šè¯­è¨€çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd628a4553f78c1d671c8d17e07b04f5" align="middle">
<img src="https://picx.zhimg.com/v2-f4c975221d333b338d395ffefcf417fe" align="middle">
<img src="https://picx.zhimg.com/v2-a1e2198ee1d4077cbe856bb6726172b7" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="O3SLM-Open-Weight-Open-Data-and-Open-Vocabulary-Sketch-Language-Model"><a href="#O3SLM-Open-Weight-Open-Data-and-Open-Vocabulary-Sketch-Language-Model" class="headerlink" title="O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model"></a>O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model</h2><p><strong>Authors:Rishi Gupta, Mukilan Karuppasamy, Shyam Marjit, Aditay Tripathi, Anirban Chakraborty</strong></p>
<p>While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.</p>
<blockquote>
<p>éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œå®ƒä»¬å¯¹æŠ½è±¡è§†è§‰è¾“å…¥çš„è§£è¯»èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»¬åœ¨ç†è§£æ‰‹ç»˜è‰å›¾æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œè‰å›¾æ˜¯ä¸€ç§è¡¨è¾¾éš¾ä»¥ç”¨æ–‡å­—æè¿°çš„æ¦‚å¿µçš„ç›´è§‚æ‰‹æ®µã€‚æˆ‘ä»¬å°†ä¸»è¦ç“¶é¢ˆç¡®å®šä¸ºç¼ºä¹ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å»ºæ¨¡è‰å›¾ã€å†™å®å›¾åƒå’Œç›¸åº”è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªå…¨æ–°çš„å¤§è§„æ¨¡å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿›é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„O3SLMå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å¯¹å¤šä¸ªåŸºäºè‰å›¾çš„ä»»åŠ¡çš„ç»¼åˆè¯„ä¼°åŒ…æ‹¬ï¼šï¼ˆaï¼‰ç›®æ ‡å®šä½ï¼Œï¼ˆbï¼‰è®¡æ•°ï¼Œï¼ˆcï¼‰å›¾åƒæ£€ç´¢ï¼ˆå³SBIRå’Œç²¾ç»†ç²’åº¦SBIRï¼‰ï¼Œä»¥åŠï¼ˆdï¼‰è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼›åœ¨èå…¥QuickDraw!ã€Sketchyå’ŒTu Berlinè¿™ä¸‰ä¸ªç°æœ‰çš„è‰å›¾æ•°æ®é›†ä»¥åŠæˆ‘ä»¬ç”Ÿæˆçš„SketchVCLæ•°æ®é›†çš„åŒæ—¶ï¼Œè¡¨æ˜O3SLMå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è‰å›¾ç†è§£å’Œæ¨ç†æ–¹é¢å¤§å¹…è¶…è¶Šäº†ç°æœ‰çš„LVLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14368v1">PDF</a> Accepted to AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨è§£é‡ŠæŠ½è±¡è§†è§‰è¾“å…¥æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£æ‰‹ç»˜è‰å›¾æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼šä¸€æ˜¯åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œç”¨äºä¿ƒè¿›é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ï¼›äºŒæ˜¯åŸºäºè¯¥æ•°æ®é›†è®­ç»ƒäº†O3SLMæ¨¡å‹ã€‚åœ¨å¤šä¸ªè‰å›¾ç›¸å…³ä»»åŠ¡ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒO3SLMå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨è‰å›¾ç†è§£å’Œæ¨ç†æ–¹é¢å¤§å¹…è¶…è¶Šäº†ç°æœ‰çš„LVLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMåœ¨è§£é‡ŠæŠ½è±¡è§†è§‰è¾“å…¥ï¼Œå°¤å…¶æ˜¯æ‰‹ç»˜è‰å›¾æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>å½“å‰ç¼ºä¹ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å»ºæ¨¡è‰å›¾ã€ç…§ç‰‡çº§å›¾åƒå’Œç›¸åº”è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>è®ºæ–‡è´¡çŒ®ä¹‹ä¸€æ˜¯åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œç”¨äºä¿ƒè¿›é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>è®ºæ–‡è¿˜ä»‹ç»äº†O3SLMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¸Šè¿°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨å¤šä¸ªè‰å›¾ç›¸å…³ä»»åŠ¡ä¸Šï¼ŒO3SLMå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>O3SLMåœ¨è‰å›¾ç†è§£å’Œæ¨ç†æ–¹é¢å¤§å¹…è¶…è¶Šäº†ç°æœ‰çš„LVLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f93a04671f7cc60ec478111406a84703" align="middle">
<img src="https://picx.zhimg.com/v2-2eeff7fa697da6d186e2a2db364b8db0" align="middle">
<img src="https://picx.zhimg.com/v2-e59a29298cac3b1340e089b68e60d5e7" align="middle">
<img src="https://picx.zhimg.com/v2-b25e24627b42829d6d9b6a19465880f9" align="middle">
<img src="https://picx.zhimg.com/v2-1873d8f09b93d300b4918a7a87905468" align="middle">
<img src="https://picx.zhimg.com/v2-4b3c740b21db62bb37cfdbd6062f0629" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DataSage-Multi-agent-Collaboration-for-Insight-Discovery-with-External-Knowledge-Retrieval-Multi-role-Debating-and-Multi-path-Reasoning"><a href="#DataSage-Multi-agent-Collaboration-for-Insight-Discovery-with-External-Knowledge-Retrieval-Multi-role-Debating-and-Multi-path-Reasoning" class="headerlink" title="DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning"></a>DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning</h2><p><strong>Authors:Xiaochuan Liu, Yuanfeng Song, Xiaoming Yin, Xing Chen</strong></p>
<p>In todayâ€™s data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.</p>
<blockquote>
<p>åœ¨å¦‚ä»Šçš„æ•°æ®é©±åŠ¨æ—¶ä»£ï¼Œå®Œå…¨è‡ªåŠ¨åŒ–çš„ç«¯åˆ°ç«¯æ•°æ®åˆ†æï¼Œå°¤å…¶æ˜¯å‘ç°æ´å¯ŸåŠ›ï¼Œå¯¹äºå‘ç°å¯æ“ä½œçš„è§è§£ä»¥ååŠ©ç»„ç»‡åšå‡ºæœ‰æ•ˆå†³ç­–è‡³å…³é‡è¦ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒLLMé©±åŠ¨çš„ä»£ç†å‡ºç°äº†ä¸€ç§æœ‰å‰æ™¯çš„è‡ªåŠ¨åŒ–æ•°æ®åˆ†æå’Œè§è§£å‘ç°çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®è§è§£ä»£ç†åœ¨æŸäº›å…³é”®æ–¹é¢ä»ç„¶å­˜åœ¨å±€é™æ€§ï¼Œå¸¸å¸¸å› ä»¥ä¸‹åŸå› æ— æ³•æä¾›ä»¤äººæ»¡æ„çš„ç»“æœï¼šï¼ˆ1ï¼‰å¯¹é¢†åŸŸçŸ¥è¯†çš„åˆ©ç”¨ä¸è¶³ï¼›ï¼ˆ2ï¼‰åˆ†ææ·±åº¦ä¸å¤Ÿï¼›ï¼ˆ3ï¼‰åœ¨äº§ç”Ÿè§è§£æ—¶é”™è¯¯å€¾å‘çš„ä»£ç ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DataSageï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šä»£ç†æ¡†æ¶ï¼Œç»“åˆäº†ä¸‰ä¸ªåˆ›æ–°åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ä»¥ä¸°å¯Œåˆ†æä¸Šä¸‹æ–‡ã€å¤šè§’è‰²è¾©è®ºæœºåˆ¶æ¨¡æ‹Ÿå¤šç§åˆ†æè§†è§’å¹¶æ·±åŒ–åˆ†ææ·±åº¦ï¼Œä»¥åŠå¤šè·¯å¾„æ¨ç†ä»¥æé«˜ç”Ÿæˆä»£ç å’Œè§è§£çš„å‡†ç¡®æ€§ã€‚åœ¨InsightBenchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDataSageåœ¨æ‰€æœ‰éš¾åº¦çº§åˆ«ä¸Šå‡ä¼˜äºç°æœ‰æ•°æ®è§è§£ä»£ç†ï¼Œä¸ºè‡ªåŠ¨åŒ–æ•°æ®è§è§£å‘ç°æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14299v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>åœ¨æ•°æ®é©±åŠ¨çš„æ—¶ä»£ï¼Œå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®åˆ†æï¼ˆå°¤å…¶æ˜¯æ´å¯Ÿå‘ç°ï¼‰å¯¹äºå‘ç°å¯æ“ä½œçš„è§è§£è‡³å…³é‡è¦ï¼Œè¿™äº›è§è§£æœ‰åŠ©äºç»„ç»‡åšå‡ºæœ‰æ•ˆå†³ç­–ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒLLMé©±åŠ¨çš„ä»£ç†æˆä¸ºè‡ªåŠ¨åŒ–æ•°æ®åˆ†æå’Œæ´å¯Ÿå‘ç°çš„æ½œåŠ›æ¨¡å¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®æ´å¯Ÿä»£ç†åœ¨å¤šä¸ªå…³é”®æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œé€šå¸¸ç”±äºï¼ˆ1ï¼‰æœªèƒ½å……åˆ†åˆ©ç”¨é¢†åŸŸçŸ¥è¯†ï¼Œï¼ˆ2ï¼‰åˆ†ææ·±åº¦ä¸è¶³ï¼Œä»¥åŠï¼ˆ3ï¼‰åœ¨ç”Ÿæˆè§è§£æ—¶å‡ºç°é”™è¯¯ä»£ç è€Œå¯¼è‡´ç»“æœä¸å°½äººæ„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DataSageï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šä»£ç†æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªåˆ›æ–°åŠŸèƒ½ï¼šå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ä»¥ä¸°å¯Œåˆ†æä¸Šä¸‹æ–‡ï¼Œå¤šè§’è‰²è¾©è®ºæœºåˆ¶æ¨¡æ‹Ÿå¤šç§åˆ†æè§†è§’å¹¶æ·±åŒ–åˆ†ææ·±åº¦ï¼Œä»¥åŠå¤šè·¯å¾„æ¨ç†ä»¥æé«˜ç”Ÿæˆä»£ç å’Œè§è§£çš„å‡†ç¡®æ€§ã€‚åœ¨InsightBenchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDataSageåœ¨æ‰€æœ‰éš¾åº¦çº§åˆ«ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰æ•°æ®æ´å¯Ÿä»£ç†ï¼Œä¸ºè‡ªåŠ¨åŒ–æ•°æ®æ´å¯Ÿå‘ç°æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åœ¨æ•°æ®é©±åŠ¨çš„æ—¶ä»£ï¼Œå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®åˆ†æå¯¹äºç»„ç»‡å†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºè‡ªåŠ¨åŒ–æ•°æ®åˆ†ææä¾›äº†æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ•°æ®æ´å¯Ÿä»£ç†å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç¼ºä¹é¢†åŸŸçŸ¥è¯†åˆ©ç”¨ã€åˆ†ææ·±åº¦ä¸è¶³å’Œé”™è¯¯ä»£ç ç”Ÿæˆã€‚</li>
<li>DataSageæ˜¯ä¸€ä¸ªæ–°çš„å¤šä»£ç†æ¡†æ¶ï¼Œé€šè¿‡å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ã€å¤šè§’è‰²è¾©è®ºæœºåˆ¶å’Œå¤šè·¯å¾„æ¨ç†è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>DataSageåœ¨InsightBenchä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ•°æ®æ´å¯Ÿä»£ç†ã€‚</li>
<li>DataSageèƒ½æœ‰æ•ˆæé«˜æ•°æ®æ´å¯Ÿçš„å‡†ç¡®æ€§å’Œæ·±åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acbb01d891d094eee60426433afdd90d" align="middle">
<img src="https://picx.zhimg.com/v2-6121cfec39687fcb6c8186d7773c7265" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PathMind-A-Retrieve-Prioritize-Reason-Framework-for-Knowledge-Graph-Reasoning-with-Large-Language-Models"><a href="#PathMind-A-Retrieve-Prioritize-Reason-Framework-for-Knowledge-Graph-Reasoning-with-Large-Language-Models" class="headerlink" title="PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models"></a>PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models</h2><p><strong>Authors:Yu Liu, Xixun Lin, Yanmin Shang, Yangxi Li, Shi Wang, Yanan Cao</strong></p>
<p>Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a â€œRetrieve-Prioritize-Reasonâ€ paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.</p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆKGRï¼‰æ˜¯é€šè¿‡åœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œé€»è¾‘æ¨æ–­æ¥æ¨æ–­æ–°çŸ¥è¯†çš„ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚å°½ç®¡å‰æ™¯å……æ»¡å¸Œæœ›ï¼Œä½†ç›®å‰çš„åŸºäºLLMçš„KGRæ–¹æ³•ä»ç„¶é¢ä¸´ä¸¤ä¸ªå…³é”®çš„å±€é™æ€§ã€‚é¦–å…ˆï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸ä¸åŠ åŒºåˆ«åœ°æå–æ¨ç†è·¯å¾„ï¼Œæ²¡æœ‰è¯„ä¼°å®ƒä»¬çš„ä¸åŒé‡è¦æ€§ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥æ— å…³çš„å™ªå£°ï¼Œè¯¯å¯¼LLMã€‚å…¶æ¬¡ï¼Œè™½ç„¶è®¸å¤šæ–¹æ³•åˆ©ç”¨LLMæ¥åŠ¨æ€æ¢ç´¢æ½œåœ¨çš„æ¨ç†è·¯å¾„ï¼Œä½†å®ƒä»¬éœ€è¦å¾ˆé«˜çš„æ£€ç´¢éœ€æ±‚å’Œé¢‘ç¹çš„LLMè°ƒç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PathMindï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é€‰æ‹©é‡è¦çš„æ¨ç†è·¯å¾„æ¥æŒ‡å¯¼LLMï¼Œä»¥å¢å¼ºå¿ å®å’Œå¯è§£é‡Šçš„æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒPathMindéµå¾ªâ€œæ£€ç´¢-ä¼˜å…ˆæ’åº-æ¨ç†â€çš„æ¨¡å¼ã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡æ£€ç´¢æ¨¡å—ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢æŸ¥è¯¢å­å›¾ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªè·¯å¾„ä¼˜å…ˆçº§æœºåˆ¶ï¼Œä½¿ç”¨è¯­ä¹‰æ„ŸçŸ¥çš„è·¯å¾„ä¼˜å…ˆçº§å‡½æ•°æ¥è¯†åˆ«é‡è¦çš„æ¨ç†è·¯å¾„ï¼ŒåŒæ—¶è€ƒè™‘ç´¯ç§¯æˆæœ¬å’Œåˆ°è¾¾ç›®æ ‡çš„é¢„ä¼°æœªæ¥æˆæœ¬ã€‚æœ€åï¼ŒPathMindé€šè¿‡åŒé˜¶æ®µè®­ç»ƒç­–ç•¥ç”Ÿæˆå‡†ç¡®ä¸”é€»è¾‘ä¸€è‡´çš„å“åº”ï¼ŒåŒ…æ‹¬ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´å’Œè·¯å¾„åå¥½å¯¹é½ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPathMindå§‹ç»ˆä¼˜äºç«äº‰åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾“å…¥ä»¤ç‰Œè¾ƒå°‘çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šï¼Œå®ƒé€šè¿‡è¯†åˆ«å…³é”®æ¨ç†è·¯å¾„æ¥å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14256v1">PDF</a> AAAI 2026, Long Paper, Oral</p>
<p><strong>Summary</strong></p>
<p>åŸºäºçŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆKGRï¼‰çš„ä»»åŠ¡æ˜¯é€šè¿‡åœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œé€»è¾‘æ¨æ¼”æ¥æ¨æ–­æ–°çŸ¥è¯†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLM-based KGRæ–¹æ³•é¢ä¸´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¸€æ˜¯ç°æœ‰æ–¹æ³•å¸¸å¸¸ä¸åŠ åŒºåˆ«åœ°æå–æ¨ç†è·¯å¾„ï¼Œæ²¡æœ‰è¯„ä¼°å®ƒä»¬çš„é‡è¦æ€§ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥æ— å…³å™ªå£°è¯¯å¯¼LLMsï¼›äºŒæ˜¯è®¸å¤šæ–¹æ³•è™½ç„¶åˆ©ç”¨LLMsåŠ¨æ€æ¢ç´¢æ½œåœ¨æ¨ç†è·¯å¾„ï¼Œä½†éœ€è¦è¾ƒé«˜çš„æ£€ç´¢éœ€æ±‚å’Œé¢‘ç¹çš„LLMè°ƒç”¨ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PathMindæ¡†æ¶ï¼Œå®ƒé€šè¿‡é€‰æ‹©é‡è¦çš„æ¨ç†è·¯å¾„æ¥å¢å¼ºå¿ å®å’Œå¯è§£é‡Šçš„æ¨ç†ã€‚PathMindéµå¾ªâ€œæ£€ç´¢-ä¼˜å…ˆæ’åº-æ¨ç†â€çš„æ¨¡å¼ï¼Œé€šè¿‡æ£€ç´¢æ¨¡å—ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢å­å›¾ï¼Œå¼•å…¥è·¯å¾„ä¼˜å…ˆçº§æœºåˆ¶æ¥è¯†åˆ«é‡è¦çš„æ¨ç†è·¯å¾„ï¼Œå¹¶è€ƒè™‘ç´¯ç§¯æˆæœ¬å’Œåˆ°è¾¾ç›®æ ‡çš„é¢„ä¼°æœªæ¥æˆæœ¬ã€‚æœ€åï¼ŒPathMindé€šè¿‡åŒé˜¶æ®µè®­ç»ƒç­–ç•¥ç”Ÿæˆå‡†ç¡®ä¸”é€»è¾‘ä¸€è‡´çš„å›åº”ï¼ŒåŒ…æ‹¬ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´å’Œè·¯å¾„åå¥½å¯¹é½ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPathMindåœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾“å…¥ä»¤ç‰Œè¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è¯†åˆ«å…³é”®æ¨ç†è·¯å¾„ï¼ŒæŒç»­ä¼˜äºç«äº‰å¯¹æ‰‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆKGRï¼‰æ˜¯é€šè¿‡é€»è¾‘æ¨æ¼”åœ¨çŸ¥è¯†å›¾è°±ä¸­æ¨æ–­æ–°çŸ¥è¯†ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>å½“å‰LLM-based KGRæ–¹æ³•é¢ä¸´æå–æ¨ç†è·¯å¾„æ— å·®åˆ«å’Œæ£€ç´¢éœ€æ±‚è¿‡é«˜çš„é—®é¢˜ã€‚</li>
<li>PathMindæ¡†æ¶é€šè¿‡é€‰æ‹©é‡è¦çš„æ¨ç†è·¯å¾„æ¥å¢å¼ºå¿ å®å’Œå¯è§£é‡Šçš„æ¨ç†ã€‚</li>
<li>PathMindéµå¾ªâ€œæ£€ç´¢-ä¼˜å…ˆæ’åº-æ¨ç†â€æ¨¡å¼ï¼Œè¯†åˆ«é‡è¦æ¨ç†è·¯å¾„å¹¶è€ƒè™‘ç´¯ç§¯å’Œé¢„ä¼°æœªæ¥æˆæœ¬ã€‚</li>
<li>PathMindé€šè¿‡åŒé˜¶æ®µè®­ç»ƒç­–ç•¥æé«˜å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPathMindåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æŒç»­ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾“å…¥ä»¤ç‰Œè¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f957be03788fa4df051462c1b765a16" align="middle">
<img src="https://picx.zhimg.com/v2-91050cfc7d5ac6aab4273a82cabc9a8b" align="middle">
<img src="https://picx.zhimg.com/v2-08ef6a313387840fcf85be14ed3c1438" align="middle">
<img src="https://picx.zhimg.com/v2-28acd1fd68f69c3ffe5d1740c506b905" align="middle">
<img src="https://picx.zhimg.com/v2-238951dfa5d60e2667f2acf65e156b8f" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e59a29298cac3b1340e089b68e60d5e7" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  UniGen-1.5 Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-19/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b942947f17f32787af52c3e88c21c9b1" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-19  Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
