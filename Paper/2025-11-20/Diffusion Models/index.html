<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  NERD Network-Regularized Diffusion Sampling For 3D Computed Tomography">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-86164ea73a937d56afc364da2b25f47d')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-20-æ›´æ–°"><a href="#2025-11-20-æ›´æ–°" class="headerlink" title="2025-11-20 æ›´æ–°"></a>2025-11-20 æ›´æ–°</h1><h2 id="NERD-Network-Regularized-Diffusion-Sampling-For-3D-Computed-Tomography"><a href="#NERD-Network-Regularized-Diffusion-Sampling-For-3D-Computed-Tomography" class="headerlink" title="NERD: Network-Regularized Diffusion Sampling For 3D Computed Tomography"></a>NERD: Network-Regularized Diffusion Sampling For 3D Computed Tomography</h2><p><strong>Authors:Shijun Liang, Ismail Alkhouri, Qing Qu, Rongrong Wang, Saiprasad Ravishankar</strong></p>
<p>Numerous diffusion model (DM)-based methods have been proposed for solving inverse imaging problems. Among these, a recent line of work has demonstrated strong performance by formulating sampling as an optimization procedure that enforces measurement consistency, forward diffusion consistency, and both step-wise and backward diffusion consistency. However, these methods have only considered 2D reconstruction tasks and do not directly extend to 3D image reconstruction problems, such as in Computed Tomography (CT). To bridge this gap, we propose NEtwork-Regularized diffusion sampling for 3D CT (NERD) by incorporating an L1 regularization into the optimization objective. This regularizer encourages spatial continuity across adjacent slices, reducing inter-slice artifacts and promoting coherent volumetric reconstructions. Additionally, we introduce two efficient optimization strategies to solve the resulting objective: one based on the Alternating Direction Method of Multipliers (ADMM) and another based on the Primal-Dual Hybrid Gradient (PDHG) method. Experiments on medical 3D CT data demonstrate that our approach achieves either state-of-the-art or highly competitive results.</p>
<blockquote>
<p>é’ˆå¯¹é€†å‘æˆåƒé—®é¢˜ï¼Œå·²ç»æå‡ºäº†è®¸å¤šåŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰çš„æ–¹æ³•ã€‚å…¶ä¸­ï¼Œæœ€è¿‘çš„ä¸€é¡¹å·¥ä½œé€šè¿‡åˆ¶å®šé‡‡æ ·ä½œä¸ºä¼˜åŒ–è¿‡ç¨‹ï¼Œå¼ºåˆ¶æ‰§è¡Œæµ‹é‡ä¸€è‡´æ€§ã€æ­£å‘æ‰©æ•£ä¸€è‡´æ€§ä»¥åŠé€æ­¥å’Œåå‘æ‰©æ•£ä¸€è‡´æ€§ï¼Œå±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…è€ƒè™‘äº†2Dé‡å»ºä»»åŠ¡ï¼Œå¹¶ä¸èƒ½ç›´æ¥æ‰©å±•åˆ°3Då›¾åƒé‡å»ºé—®é¢˜ï¼Œä¾‹å¦‚åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é€šè¿‡å°†L1æ­£åˆ™åŒ–èå…¥ä¼˜åŒ–ç›®æ ‡ï¼Œæå‡ºäº†ç”¨äº3D CTçš„ç½‘ç»œæ­£åˆ™åŒ–æ‰©æ•£é‡‡æ ·ï¼ˆNERDï¼‰ã€‚è¿™ç§æ­£åˆ™åŒ–é¼“åŠ±ç›¸é‚»åˆ‡ç‰‡ä¹‹é—´çš„ç©ºé—´è¿ç»­æ€§ï¼Œå‡å°‘äº†åˆ‡ç‰‡é—´çš„ä¼ªå½±ï¼Œä¿ƒè¿›äº†è¿è´¯çš„ä½“ç§¯é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ç§æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥æ¥è§£å†³ç”±æ­¤äº§ç”Ÿçš„ç›®æ ‡ï¼šä¸€ç§åŸºäºäº¤æ›¿æ–¹å‘æ³•ä¹˜æ•°ï¼ˆADMMï¼‰ï¼Œå¦ä¸€ç§åŸºäºåŸå§‹-å¯¹å¶æ··åˆæ¢¯åº¦ï¼ˆPDHGï¼‰æ–¹æ³•ã€‚åœ¨åŒ»ç–—3D CTæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å‰æ²¿æˆ–é«˜åº¦ç«äº‰çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14680v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹ä¸‰ç»´è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒé‡å»ºé—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºç½‘ç»œæ­£åˆ™åŒ–çš„æ‰©æ•£é‡‡æ ·æ–¹æ³•ï¼ˆNERDï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥L1æ­£åˆ™åŒ–é¡¹ï¼Œä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œå¢å¼ºç©ºé—´è¿ç»­æ€§ï¼Œå‡å°‘åˆ‡ç‰‡é—´ä¼ªå½±ï¼Œå®ç°æ›´è¿è´¯çš„ä¸‰ç»´é‡å»ºã€‚åŒæ—¶ï¼Œé‡‡ç”¨äº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰å’ŒåŸå§‹-å¯¹å¶æ··åˆæ¢¯åº¦æ³•ï¼ˆPDHGï¼‰ä¸¤ç§ä¼˜åŒ–ç­–ç•¥æ±‚è§£ç›®æ ‡å‡½æ•°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»å­¦ä¸‰ç»´CTæ•°æ®ä¸Šå–å¾—äº†å…ˆè¿›æˆ–é«˜åº¦ç«äº‰çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨è§£å†³é€†å‘æˆåƒé—®é¢˜ä¸Šæœ‰å¤šç§æ–¹æ³•ï¼Œå…¶ä¸­ä¸€äº›é€šè¿‡åˆ¶å®šé‡‡æ ·ä½œä¸ºä¼˜åŒ–ç¨‹åºæ¥å¼ºåˆ¶æ‰§è¡Œæµ‹é‡ä¸€è‡´æ€§ã€å‰å‘æ‰©æ•£ä¸€è‡´æ€§å’Œæ­¥è¿›ä¸åå‘æ‰©æ•£ä¸€è‡´æ€§ï¼Œè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨äºŒç»´é‡å»ºä»»åŠ¡ä¸Šï¼Œæ— æ³•ç›´æ¥åº”ç”¨äºä¸‰ç»´å›¾åƒé‡å»ºé—®é¢˜ï¼Œå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºNERDçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥L1æ­£åˆ™åŒ–é¡¹æ¥å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä¼˜åŒ–ç›®æ ‡å‡½æ•°ä»¥é¼“åŠ±ç©ºé—´è¿ç»­æ€§ï¼Œå‡å°‘åˆ‡ç‰‡é—´ä¼ªå½±ã€‚</li>
<li>NERDé‡‡ç”¨ä¸¤ç§ä¼˜åŒ–ç­–ç•¥ï¼šåŸºäºäº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰å’ŒåŸå§‹-å¯¹å¶æ··åˆæ¢¯åº¦æ³•ï¼ˆPDHGï¼‰ã€‚</li>
<li>NERDæ–¹æ³•åœ¨åŒ»å­¦ä¸‰ç»´CTæ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨ç°å‡ºå…ˆè¿›æˆ–é«˜åº¦ç«äº‰çš„æ€§èƒ½ã€‚</li>
<li>L1æ­£åˆ™åŒ–é¡¹æœ‰åŠ©äºå®ç°æ›´è¿è´¯çš„ä¸‰ç»´é‡å»ºï¼Œå¢å¼ºå›¾åƒçš„ç©ºé—´è¿ç»­æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b442a708b43495bfae680857b9f75606" align="middle">
<img src="https://picx.zhimg.com/v2-af17de49d36e2683f624599da9881579" align="middle">
<img src="https://picx.zhimg.com/v2-c20a8e90f14156077108de4904cf720e" align="middle">
<img src="https://picx.zhimg.com/v2-b2733f8e40242c04c451438cf599129d" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Segmentation-Aware-Latent-Diffusion-for-Satellite-Image-Super-Resolution-Enabling-Smallholder-Farm-Boundary-Delineation"><a href="#Segmentation-Aware-Latent-Diffusion-for-Satellite-Image-Super-Resolution-Enabling-Smallholder-Farm-Boundary-Delineation" class="headerlink" title="Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation"></a>Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation</h2><p><strong>Authors:Aditi Agarwal, Anjali Jain, Nikita Saxena, Ishan Deshpande, Michal Kazmierski, Abigail Annkah, Nadav Sherman, Karthikeyan Shanmugam, Alok Talekar, Vaibhav Rajan</strong></p>
<p>Delineating farm boundaries through segmentation of satellite images is a fundamental step in many agricultural applications. The task is particularly challenging for smallholder farms, where accurate delineation requires the use of high resolution (HR) imagery which are available only at low revisit frequencies (e.g., annually). To support more frequent (sub-) seasonal monitoring, HR images could be combined as references (ref) with low resolution (LR) images â€“ having higher revisit frequency (e.g., weekly) â€“ using reference-based super-resolution (Ref-SR) methods. However, current Ref-SR methods optimize perceptual quality and smooth over crucial features needed for downstream tasks, and are unable to meet the large scale-factor requirements for this task. Further, previous two-step approaches of SR followed by segmentation do not effectively utilize diverse satellite sources as inputs. We address these problems through a new approach, $\textbf{SEED-SR}$, which uses a combination of conditional latent diffusion models and large-scale multi-spectral, multi-source geo-spatial foundation models. Our key innovation is to bypass the explicit SR task in the pixel space and instead perform SR in a segmentation-aware latent space. This unique approach enables us to generate segmentation maps at an unprecedented 20$\times$ scale factor, and rigorous experiments on two large, real datasets demonstrate up to $\textbf{25.5}$ and $\textbf{12.9}$ relative improvement in instance and semantic segmentation metrics respectively over approaches based on state-of-the-art Ref-SR methods.</p>
<blockquote>
<p>é€šè¿‡å«æ˜Ÿå›¾åƒçš„åˆ†å‰²æ¥åˆ’å®šå†œç”°è¾¹ç•Œæ˜¯è®¸å¤šå†œä¸šåº”ç”¨ä¸­çš„åŸºæœ¬æ­¥éª¤ã€‚å¯¹äºå°è§„æ¨¡å†œåœºï¼Œè¿™é¡¹ä»»åŠ¡å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå‡†ç¡®åˆ’å®šè¾¹ç•Œéœ€è¦ä½¿ç”¨é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å›¾åƒï¼Œè€Œè¿™äº›å›¾åƒåªèƒ½åœ¨è¾ƒä½çš„å›è®¿é¢‘ç‡ï¼ˆä¾‹å¦‚ï¼Œæ¯å¹´ï¼‰ä¸Šè·å¾—ã€‚ä¸ºäº†æ”¯æŒæ›´é¢‘ç¹ï¼ˆäºšï¼‰å­£èŠ‚æ€§çš„ç›‘æµ‹ï¼Œå¯ä»¥å°†é«˜åˆ†è¾¨ç‡å›¾åƒä¸å…·æœ‰æ›´é«˜å›è®¿é¢‘ç‡ï¼ˆä¾‹å¦‚ï¼Œæ¯å‘¨ï¼‰çš„ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒç›¸ç»“åˆï¼Œä½¿ç”¨åŸºäºå‚è€ƒçš„è¶…åˆ†è¾¨ç‡ï¼ˆRef-SRï¼‰æ–¹æ³•ä½œä¸ºå‚è€ƒã€‚ç„¶è€Œï¼Œå½“å‰çš„Ref-SRæ–¹æ³•ä¼˜åŒ–äº†æ„ŸçŸ¥è´¨é‡ï¼Œå¹³æ»‘äº†ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„å…³é”®ç‰¹å¾ï¼Œå¹¶ä¸”æ— æ³•æ»¡è¶³æ­¤é¡¹ä»»åŠ¡çš„å¤§è§„æ¨¡å› å­è¦æ±‚ã€‚æ­¤å¤–ï¼Œå…ˆå‰å…ˆæ‰§è¡Œè¶…åˆ†è¾¨ç‡å†åˆ†å‰²çš„ä¸¤æ­¥æ–¹æ³•æœªèƒ½æœ‰æ•ˆåœ°åˆ©ç”¨ä¸åŒçš„å«æ˜Ÿæ•°æ®æºä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°æ–¹æ³•SEED-SRæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ä»¥åŠå¤§è§„æ¨¡çš„å¤šå…‰è°±ã€å¤šæºåœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºç»•è¿‡åƒç´ ç©ºé—´ä¸­çš„æ˜¾å¼SRä»»åŠ¡ï¼Œè€Œæ˜¯åœ¨æ„ŸçŸ¥åˆ†å‰²çš„æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡ŒSRã€‚è¿™ç§ç‹¬ç‰¹çš„æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥å‰æ‰€æœªæœ‰çš„20å€è§„æ¨¡å› å­ç”Ÿæˆåˆ†å‰²åœ°å›¾ï¼Œå¹¶ä¸”åœ¨ä¸¤ä¸ªå¤§å‹çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œä¸¥æ ¼å®éªŒï¼Œç›¸å¯¹äºåŸºäºæœ€æ–°Ref-SRæ–¹æ³•çš„æ–¹æ¡ˆï¼Œåœ¨å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²æŒ‡æ ‡ä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾25.5%å’Œ12.9%çš„ç›¸å¯¹æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14481v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºå«æ˜Ÿå›¾åƒçš„å†œåœºè¾¹ç•Œåˆ†å‰²åœ¨è®¸å¤šå†œä¸šåº”ç”¨ä¸­æ˜¯åŸºç¡€æ­¥éª¤ï¼Œå¯¹å°å‹å†œåœºå°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†æ”¯æŒæ›´é¢‘ç¹çš„ï¼ˆäºšï¼‰å­£èŠ‚æ€§ç›‘æµ‹ï¼Œå¯ä»¥ä½¿ç”¨åŸºäºå‚è€ƒçš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼ˆRef-SRï¼‰ï¼Œç»“åˆä½åˆ†è¾¨ç‡å›¾åƒçš„é«˜é‡è®¿é¢‘ç‡ä¸å¯åˆ©ç”¨çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„Ref-SRæ–¹æ³•ä¼˜åŒ–æ„ŸçŸ¥è´¨é‡å¹¶å¿½ç•¥äº†ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„å…³é”®ç‰¹å¾ï¼Œæ— æ³•æ»¡è¶³å¤§è§„æ¨¡å› å­è¦æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•SEED-SRï¼Œç»“åˆæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå¤šå…‰è°±ã€å¤šæºåœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç»•è¿‡æ˜¾å¼è¶…åˆ†è¾¨ç‡ä»»åŠ¡ç›´æ¥åœ¨åˆ†å‰²æ„ŸçŸ¥æ½œåœ¨ç©ºé—´è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥æ–¹æ³•èƒ½åœ¨å‰æ‰€æœªæœ‰çš„20å€å°ºåº¦å› å­ä¸‹ç”Ÿæˆåˆ†å‰²å›¾ï¼Œå¹¶åœ¨ä¸¤ä¸ªå¤§å‹çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºäºæœ€æ–°Ref-SRçš„æ–¹æ³•ï¼Œå…¶åœ¨å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²æŒ‡æ ‡ä¸Šåˆ†åˆ«æœ‰é«˜è¾¾25.5å’Œ12.9çš„ç›¸å¯¹æ”¹è¿›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å«æ˜Ÿå›¾åƒä¸­çš„å†œåœºè¾¹ç•Œåˆ†å‰²åœ¨å†œä¸šåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å°å‹å†œåœºä¸­ã€‚</li>
<li>ç»“åˆé«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡å›¾åƒä»¥æ”¯æŒæ›´é¢‘ç¹çš„ç›‘æµ‹æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç­–ç•¥ã€‚</li>
<li>å½“å‰Ref-SRæ–¹æ³•åœ¨ä¼˜åŒ–æ„ŸçŸ¥è´¨é‡æ—¶å¿½è§†äº†ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„å…³é”®ç‰¹å¾ï¼Œéš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡åº”ç”¨çš„éœ€æ±‚ã€‚</li>
<li>SEED-SRæ–¹æ³•é€šè¿‡æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå¤šæºåœ°ç†ç©ºé—´æ¨¡å‹ç»“åˆï¼Œåœ¨åˆ†å‰²æ„ŸçŸ¥æ½œåœ¨ç©ºé—´è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œæ˜¯ä¸€ç§åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚</li>
<li>SEED-SRæ–¹æ³•èƒ½å¤Ÿåœ¨é«˜å°ºåº¦å› å­ï¼ˆ20å€ï¼‰ä¸‹ç”Ÿæˆåˆ†å‰²å›¾ï¼Œä¸ºè¿™ä¸€é¢†åŸŸå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„è¿›æ­¥ã€‚</li>
<li>åœ¨å¤§å‹çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSEED-SRæ–¹æ³•åœ¨å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²æŒ‡æ ‡ä¸Šç›¸å¯¹äºç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed0f43bf838a23d325d28330fbf12c24" align="middle">
<img src="https://picx.zhimg.com/v2-d9a740d50a7634ebdbe4748ab36a807f" align="middle">
<img src="https://picx.zhimg.com/v2-e2ee861d3156516c774bfe50519d1fd9" align="middle">
<img src="https://picx.zhimg.com/v2-4fe0be79ac8b3fb754c3d541970c0a14" align="middle">
<img src="https://picx.zhimg.com/v2-addbc5718c0ba810dd0774b3c12090e2" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Iterative-Diffusion-Refined-Neural-Attenuation-Fields-for-Multi-Source-Stationary-CT-Reconstruction-NAF-Meets-Diffusion-Model"><a href="#Iterative-Diffusion-Refined-Neural-Attenuation-Fields-for-Multi-Source-Stationary-CT-Reconstruction-NAF-Meets-Diffusion-Model" class="headerlink" title="Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model"></a>Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model</h2><p><strong>Authors:Jiancheng Fang, Shaoyu Wang, Junlin Wang, Weiwen Wu, Yikun Zhang, Qiegen Liu</strong></p>
<p>Multi-source stationary computed tomography (CT) has recently attracted attention for its ability to achieve rapid image reconstruction, making it suitable for time-sensitive clinical and industrial applications. However, practical systems are often constrained by ultra-sparse-view sampling, which significantly degrades reconstruction quality. Traditional methods struggle under ultra-sparse-view settings, where interpolation becomes inaccurate and the resulting reconstructions are unsatisfactory. To address this challenge, this study proposes Diffusion-Refined Neural Attenuation Fields (Diff-NAF), an iterative framework tailored for multi-source stationary CT under ultra-sparse-view conditions. Diff-NAF combines a Neural Attenuation Field representation with a dual-branch conditional diffusion model. The process begins by training an initial NAF using ultra-sparse-view projections. New projections are then generated through an Angle-Prior Guided Projection Synthesis strategy that exploits inter view priors, and are subsequently refined by a Diffusion-driven Reuse Projection Refinement Module. The refined projections are incorporated as pseudo-labels into the training set for the next iteration. Through iterative refinement, Diff-NAF progressively enhances projection completeness and reconstruction fidelity under ultra-sparse-view conditions, ultimately yielding high-quality CT reconstructions. Experimental results on multiple simulated 3D CT volumes and real projection data demonstrate that Diff-NAF achieves the best performance under ultra-sparse-view conditions.</p>
<blockquote>
<p>å¤šæºé™æ€è®¡ç®—å±‚ææˆåƒï¼ˆCTï¼‰å› å…¶å¿«é€Ÿå›¾åƒé‡å»ºèƒ½åŠ›è€Œè¿‘æœŸå¤‡å—å…³æ³¨ï¼Œä½¿å…¶æˆä¸ºé€‚ç”¨äºæ—¶é—´æ•æ„Ÿæ€§ä¸´åºŠå’Œå·¥ä¸šåº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚ç„¶è€Œï¼Œå®é™…ç³»ç»Ÿé€šå¸¸å—åˆ°è¶…ç¨€ç–è§†å›¾é‡‡æ ·çš„é™åˆ¶ï¼Œè¿™ä¼šæ˜¾è‘—é™çº§é‡å»ºè´¨é‡ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨è¶…ç¨€ç–è§†å›¾è®¾ç½®ä¸‹è¡¨ç°æŒ£æ‰ï¼Œæ’å€¼å˜å¾—ä¸å‡†ç¡®ï¼Œå¯¼è‡´é‡å»ºç»“æœä¸å°½äººæ„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†Diffusion-Refined Neural Attenuation Fieldsï¼ˆDiff-NAFï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è¶…ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹çš„å¤šæºé™æ€CTé‡èº«å®šåˆ¶çš„è¿­ä»£æ¡†æ¶ã€‚Diff-NAFç»“åˆäº†ç¥ç»è¡°å‡åœºè¡¨ç¤ºæ³•å’ŒåŒåˆ†æ”¯æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚æµç¨‹å§‹äºä½¿ç”¨è¶…ç¨€ç–è§†å›¾æŠ•å½±è®­ç»ƒåˆå§‹NAFã€‚ç„¶åé€šè¿‡è§’åº¦ä¼˜å…ˆå¼•å¯¼æŠ•å½±åˆæˆç­–ç•¥ç”Ÿæˆæ–°çš„æŠ•å½±ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨è§†å›¾é—´å…ˆéªŒï¼Œéšåé€šè¿‡æ‰©æ•£é©±åŠ¨çš„é‡ç”¨æŠ•å½±ç»†åŒ–æ¨¡å—è¿›è¡Œç»†åŒ–ã€‚ç»†åŒ–åçš„æŠ•å½±è¢«ä½œä¸ºä¼ªæ ‡ç­¾çº³å…¥è®­ç»ƒé›†ï¼Œç”¨äºä¸‹ä¸€æ¬¡è¿­ä»£ã€‚é€šè¿‡è¿­ä»£ç»†åŒ–ï¼ŒDiff-NAFåœ¨è¶…ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹é€æ­¥æé«˜äº†æŠ•å½±çš„å®Œæ•´æ€§å’Œé‡å»ºçš„ä¿çœŸåº¦ï¼Œæœ€ç»ˆäº§ç”Ÿé«˜è´¨é‡çš„CTé‡å»ºç»“æœã€‚åœ¨å¤šä¸ªæ¨¡æ‹Ÿçš„3D CTä½“ç§¯å’ŒçœŸå®æŠ•å½±æ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDiff-NAFåœ¨è¶…ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14310v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæºé™æ€è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å› èƒ½å¿«é€Ÿå®ç°å›¾åƒé‡å»ºï¼Œåœ¨æ—¶é—´å’Œæ•æ„Ÿçš„ä¸´åºŠå’Œå·¥ä¸šåº”ç”¨ä¸­å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œå®é™…ç³»ç»Ÿä¸­å¸¸å—åˆ°è¶…ç¨€ç–è§†å›¾é‡‡æ ·çš„é™åˆ¶ï¼Œå¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºDiffusion-Refined Neural Attenuation Fieldsï¼ˆDiff-NAFï¼‰æ–¹æ³•ï¼Œé€‚ç”¨äºè¶…ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹çš„å¤šæºé™æ€CTã€‚Diff-NAFç»“åˆç¥ç»è¡°å‡åœºè¡¨ç¤ºå’ŒåŒåˆ†æ”¯æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–ï¼Œé€æ­¥æé«˜æŠ•å½±çš„å®Œæ•´æ€§å’Œé‡å»ºçš„ä¿çœŸåº¦ï¼Œå®ç°é«˜è´¨é‡çš„CTé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæºé™æ€CTå…·æœ‰å¿«é€Ÿå›¾åƒé‡å»ºèƒ½åŠ›ï¼Œé€‚ç”¨äºæ—¶é—´æ•æ„Ÿçš„ä¸´åºŠå’Œå·¥ä¸šåº”ç”¨ã€‚</li>
<li>å®é™…ç³»ç»Ÿä¸­å¸¸é¢ä¸´è¶…ç¨€ç–è§†å›¾é‡‡æ ·é—®é¢˜ï¼Œå¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ã€‚</li>
<li>Diff-NAFæ–¹æ³•ç»“åˆç¥ç»è¡°å‡åœºè¡¨ç¤ºå’ŒåŒåˆ†æ”¯æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œé€‚ç”¨äºè¶…ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹çš„å¤šæºé™æ€CTã€‚</li>
<li>Diff-NAFé€šè¿‡è¿­ä»£ä¼˜åŒ–ï¼Œé€æ­¥æé«˜æŠ•å½±çš„å®Œæ•´æ€§å’Œé‡å»ºçš„ä¿çœŸåº¦ã€‚</li>
<li>Angle-Prior Guided Projection Synthesisç­–ç•¥ç”¨äºç”Ÿæˆæ–°æŠ•å½±ï¼Œåˆ©ç”¨ä¸åŒè§†å›¾é—´çš„å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>Diffusion-driven Reuse Projection Refinement Moduleç”¨äºç»†åŒ–æŠ•å½±ï¼Œå¹¶ä½œä¸ºä¼ªæ ‡ç­¾èå…¥è®­ç»ƒé›†è¿›è¡Œä¸‹ä¸€æ¬¡è¿­ä»£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-951038796f4969da80c4d4aefb02276f" align="middle">
<img src="https://picx.zhimg.com/v2-4c183dfc0bbd477957b81fc5cd3a9711" align="middle">
<img src="https://picx.zhimg.com/v2-afa2603eadd86e7e992b9c9559f70656" align="middle">
<img src="https://picx.zhimg.com/v2-b9ea1ca2db0175d352829370449f6a6f" align="middle">
<img src="https://picx.zhimg.com/v2-d69fb156eb3c5406c54289594722c497" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="InstantViR-Real-Time-Video-Inverse-Problem-Solver-with-Distilled-Diffusion-Prior"><a href="#InstantViR-Real-Time-Video-Inverse-Problem-Solver-with-Distilled-Diffusion-Prior" class="headerlink" title="InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior"></a>InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior</h2><p><strong>Authors:Weimin Bai, Suzhe Xu, Yiwei Ren, Jinhua Hao, Ming Sun, Wenzheng Chen, He Sun</strong></p>
<p>Video inverse problems are fundamental to streaming, telepresence, and AR&#x2F;VR, where high perceptual quality must coexist with tight latency constraints. Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use. We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior. We distill a powerful bidirectional video diffusion model (teacher) into a causal autoregressive student that maps a degraded video directly to its restored version in a single forward pass, inheriting the teacherâ€™s strong temporal modeling while completely removing iterative test-time optimization. The distillation is prior-driven: it only requires the teacher diffusion model and known degradation operators, and does not rely on externally paired clean&#x2F;noisy video data. To further boost throughput, we replace the video-diffusion backbone VAE with a high-efficiency LeanVAE via an innovative teacher-space regularized distillation scheme, enabling low-latency latent-space processing. Across streaming random inpainting, Gaussian deblurring and super-resolution, InstantViR matches or surpasses the reconstruction quality of diffusion-based baselines while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100 times speedups over iterative video diffusion solvers. These results show that diffusion-based video reconstruction is compatible with real-time, interactive, editable, streaming scenarios, turning high-quality video restoration into a practical component of modern vision systems.</p>
<blockquote>
<p>è§†é¢‘é€†é—®é¢˜åœ¨æµåª’ä½“ã€è¿œç¨‹å­˜åœ¨ã€å¢å¼ºç°å®&#x2F;è™šæ‹Ÿç°å®ç­‰é¢†åŸŸä¸­è‡³å…³é‡è¦ï¼Œè¿™äº›é¢†åŸŸè¦æ±‚é«˜æ„ŸçŸ¥è´¨é‡ä¸ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶å…±å­˜ã€‚åŸºäºæ‰©æ•£çš„å…ˆéªŒç›®å‰æä¾›äº†æœ€å…ˆè¿›çš„é‡å»ºæŠ€æœ¯ï¼Œä½†ç°æœ‰æ–¹æ³•è¦ä¹ˆé‡‡ç”¨ä¸´æ—¶è°ƒèŠ‚å™¨é€‚åº”å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¯¼è‡´å‡ºç°æ—¶é—´ä¸Šçš„ä¼ªå½±ï¼Œè¦ä¹ˆä¾èµ–äºåŸç”Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå…¶è¿­ä»£åéªŒé‡‡æ ·å¯¹äºå®æ—¶ä½¿ç”¨æ¥è¯´å¤ªæ…¢ã€‚æˆ‘ä»¬æ¨å‡ºäº†InstantViRï¼Œè¿™æ˜¯ä¸€ä¸ªç”±é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£å…ˆéªŒé©±åŠ¨çš„è¶…é«˜é€Ÿè§†é¢‘é‡å»ºæ‘Šé”€æ¨ç†æ¡†æ¶ã€‚æˆ‘ä»¬å°†å¼ºå¤§çš„åŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰æç‚¼ä¸ºå› æœè‡ªå›å½’å­¦ç”Ÿæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­å°†é€€åŒ–è§†é¢‘ç›´æ¥æ˜ å°„åˆ°å…¶æ¢å¤ç‰ˆæœ¬ï¼Œç»§æ‰¿äº†æ•™å¸ˆçš„å¼ºå¤§æ—¶é—´å»ºæ¨¡èƒ½åŠ›ï¼ŒåŒæ—¶å®Œå…¨æ¶ˆé™¤äº†è¿­ä»£æµ‹è¯•æ—¶é—´ä¼˜åŒ–ã€‚æç‚¼æ˜¯ä¼˜å…ˆé©±åŠ¨çš„ï¼šå®ƒåªéœ€è¦æ•™å¸ˆæ‰©æ•£æ¨¡å‹å’Œå·²çŸ¥çš„é€€åŒ–ç®—å­ï¼Œå¹¶ä¸ä¾èµ–äºå¤–éƒ¨é…å¯¹çš„å¹²å‡€&#x2F;å˜ˆæ‚çš„è§†é¢‘æ•°æ®ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ååé‡ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªé«˜æ•ˆçš„LeanVAEæ›¿ä»£äº†è§†é¢‘æ‰©æ•£ä¸»å¹²VAEï¼Œé€šè¿‡åˆ›æ–°çš„æ•™å¸ˆç©ºé—´æ­£åˆ™åŒ–æç‚¼æ–¹æ¡ˆï¼Œå®ç°åœ¨æ½œåœ¨ç©ºé—´çš„ä½å»¶è¿Ÿå¤„ç†ã€‚åœ¨éšæœºå¡«å……ã€é«˜æ–¯å»æ¨¡ç³Šå’Œè¶…åˆ†è¾¨ç‡å¢å¼ºæ–¹é¢ï¼ŒInstantViRåŒ¹é…æˆ–è¶…è¶Šäº†åŸºäºæ‰©æ•£çš„åŸºçº¿é‡å»ºè´¨é‡ï¼ŒåŒæ—¶åœ¨NVIDIA A100 GPUä¸Šä»¥è¶…è¿‡35å¸§çš„é€Ÿåº¦è¿è¡Œï¼Œç›¸å¯¹äºè¿­ä»£è§†é¢‘æ‰©æ•£æ±‚è§£å™¨å®ç°äº†é«˜è¾¾100å€çš„é€Ÿåº¦æå‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„è§†é¢‘é‡å»ºä¸å®æ—¶ã€äº¤äº’ã€å¯ç¼–è¾‘çš„æµåª’ä½“åœºæ™¯å…¼å®¹ï¼Œå°†é«˜è´¨é‡è§†é¢‘æ¢å¤è½¬å˜ä¸ºç°ä»£è§†è§‰ç³»ç»Ÿçš„å®ç”¨ç»„ä»¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14208v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘é€†é—®é¢˜åœ¨æµåª’ä½“ã€è¿œç¨‹å‡ºå¸­ä»¥åŠAR&#x2F;VRç­‰é¢†åŸŸä¸­è‡³å…³é‡è¦ï¼Œå…¶ä¸­é«˜æ„ŸçŸ¥è´¨é‡å¿…é¡»ä¸ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶å…±å­˜ã€‚è™½ç„¶æ‰©æ•£å…ˆéªŒç›®å‰æä¾›äº†æœ€å…ˆè¿›çš„é‡å»ºæ•ˆæœï¼Œä½†ç°æœ‰æ–¹æ³•è¦ä¹ˆé‡‡ç”¨ä¸´æ—¶è°ƒèŠ‚å™¨é€‚åº”å›¾åƒæ‰©æ•£æ¨¡å‹å¯¼è‡´å‡ºç°æ—¶é—´ä¸Šçš„ç‘•ç–µï¼Œè¦ä¹ˆä¾èµ–äºåŸå§‹è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå…¶è¿­ä»£åéªŒé‡‡æ ·å¯¹äºå®æ—¶ä½¿ç”¨æ¥è¯´è¿‡äºç¼“æ…¢ã€‚æˆ‘ä»¬å¼•å…¥äº†InstantViRï¼Œè¿™æ˜¯ä¸€ä¸ªç”±é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£å…ˆéªŒé©±åŠ¨çš„è¶…å¿«é€Ÿè§†é¢‘é‡å»ºæ‘Šé”€æ¨ç†æ¡†æ¶ã€‚æˆ‘ä»¬å°†å¼ºå¤§çš„åŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰æç‚¼ä¸ºå› æœè‡ªå›å½’å­¦ç”Ÿï¼Œå•æ¬¡å‰å‘ä¼ é€’å³å¯å°†é€€åŒ–è§†é¢‘ç›´æ¥æ˜ å°„åˆ°å…¶æ¢å¤ç‰ˆæœ¬ï¼Œç»§æ‰¿äº†æ•™å¸ˆçš„å¼ºå¤§æ—¶é—´å»ºæ¨¡èƒ½åŠ›ï¼ŒåŒæ—¶å®Œå…¨æ¶ˆé™¤äº†è¿­ä»£æµ‹è¯•æ—¶é—´ä¼˜åŒ–ã€‚æç‚¼æ˜¯ç”±å…ˆéªŒé©±åŠ¨çš„ï¼šå®ƒåªéœ€è¦æ•™å¸ˆæ‰©æ•£æ¨¡å‹å’Œå·²çŸ¥çš„é€€åŒ–è¿ç®—ç¬¦ï¼Œå¹¶ä¸ä¾èµ–äºå¤–éƒ¨æ¸…æ´&#x2F;å™ªå£°è§†é¢‘çš„é…å¯¹æ•°æ®ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ååé‡ï¼Œæˆ‘ä»¬é€šè¿‡åˆ›æ–°çš„æ•™å¸ˆç©ºé—´æ­£åˆ™åŒ–æç‚¼æ–¹æ¡ˆï¼Œç”¨é«˜æ•ˆç‡çš„LeanVAEæ›¿æ¢è§†é¢‘æ‰©æ•£ä¸»å¹²VAEï¼Œå®ç°ä½å»¶è¿Ÿçš„æ½œåœ¨ç©ºé—´å¤„ç†ã€‚åœ¨éšæœºå¡«å……ã€é«˜æ–¯å»æ¨¡ç³Šå’Œè¶…åˆ†è¾¨ç‡ç­‰åº”ç”¨ä¸­ï¼ŒInstantViRåŒ¹é…æˆ–è¶…è¶Šäº†æ‰©æ•£åŸºå‡†çš„é‡å»ºè´¨é‡ï¼ŒåŒæ—¶åœ¨NVIDIA A100 GPUä¸Šä»¥è¶…è¿‡35 FPSè¿è¡Œï¼Œå®ç°äº†é«˜è¾¾100å€çš„è¿­ä»£è§†é¢‘æ‰©æ•£æ±‚è§£å™¨åŠ é€Ÿã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„è§†é¢‘é‡å»ºä¸å®æ—¶ã€äº¤äº’ã€å¯ç¼–è¾‘çš„æµåª’ä½“åœºæ™¯å…¼å®¹ï¼Œå°†é«˜è´¨é‡è§†é¢‘æ¢å¤è½¬å˜ä¸ºç°ä»£è§†è§‰ç³»ç»Ÿçš„å®ç”¨ç»„ä»¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘é€†é—®é¢˜åœ¨å¤šä¸ªé¢†åŸŸçš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚æ—¶é—´ç‘•ç–µå’Œè¾ƒä½çš„å®æ—¶æ€§èƒ½ã€‚</li>
<li>InstantViRæ¡†æ¶çš„å¼•å…¥ï¼Œç»“åˆé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£å…ˆéªŒè¿›è¡Œè¶…å¿«é€Ÿè§†é¢‘é‡å»ºã€‚</li>
<li>é€šè¿‡è’¸é¦æŠ€æœ¯å°†å¼ºå¤§çš„åŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰è½¬åŒ–ä¸ºå› æœè‡ªå›å½’æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰ï¼Œå®ç°å•æ¬¡å‰å‘ä¼ é€’æ¢å¤è§†é¢‘ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿç»§æ‰¿æ•™å¸ˆçš„ä¼˜ç§€æ—¶é—´å»ºæ¨¡èƒ½åŠ›ï¼ŒåŒæ—¶æ¶ˆé™¤è¿­ä»£æµ‹è¯•æ—¶é—´ä¼˜åŒ–ã€‚</li>
<li>æ¡†æ¶çš„å…ˆéªŒé©±åŠ¨è’¸é¦è¿‡ç¨‹ä»…éœ€è¦æ•™å¸ˆæ‰©æ•£æ¨¡å‹å’Œå·²çŸ¥çš„é€€åŒ–è¿ç®—ç¬¦ï¼Œæ— éœ€é…å¯¹æ¸…æ´&#x2F;å™ªå£°è§†é¢‘æ•°æ®ã€‚</li>
<li>é€šè¿‡é«˜æ•ˆLeanVAEçš„ä½¿ç”¨å’Œåˆ›æ–°æ•™å¸ˆç©ºé—´æ­£åˆ™åŒ–è’¸é¦æ–¹æ¡ˆï¼Œå®ç°ä½å»¶è¿Ÿçš„æ½œåœ¨ç©ºé—´å¤„ç†å’Œé«˜æ€§èƒ½çš„è§†é¢‘é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9edc75607f1463fb40485b584b9c4aa" align="middle">
<img src="https://picx.zhimg.com/v2-39d74828ec8e0ae5615cd935e9690606" align="middle">
<img src="https://picx.zhimg.com/v2-584313e91badddafc66442aeed587bdb" align="middle">
<img src="https://picx.zhimg.com/v2-ca1bbaf175d14dff4f7daa6a6b9b0b73" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Coffee-Controllable-Diffusion-Fine-tuning"><a href="#Coffee-Controllable-Diffusion-Fine-tuning" class="headerlink" title="Coffee: Controllable Diffusion Fine-tuning"></a>Coffee: Controllable Diffusion Fine-tuning</h2><p><strong>Authors:Ziyao Zeng, Jingcheng Ni, Ruyi Liu, Alex Wong</strong></p>
<p>Text-to-image diffusion models can generate diverse content with flexible prompts, which makes them well-suited for customization through fine-tuning with a small amount of user-provided data. However, controllable fine-tuning that prevents models from learning undesired concepts present in the fine-tuning data, and from entangling those concepts with user prompts, remains an open challenge. It is crucial for downstream tasks like bias mitigation, preventing malicious adaptation, attribute disentanglement, and generalizable fine-tuning of diffusion policy. We propose Coffee that allows using language to specify undesired concepts to regularize the adaptation process. The crux of our method lies in keeping the embeddings of the user prompt from aligning with undesired concepts. Crucially, Coffee requires no additional training and enables flexible modification of undesired concepts by modifying textual descriptions. We evaluate Coffee by fine-tuning on images associated with user prompts paired with undesired concepts. Experimental results demonstrate that Coffee can prevent text-to-image models from learning specified undesired concepts during fine-tuning and outperforms existing methods. Code will be released upon acceptance.</p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå…·æœ‰çµæ´»æç¤ºçš„å¤šæ ·åŒ–å†…å®¹ï¼Œè¿™ä½¿å¾—å®ƒä»¬éå¸¸é€‚åˆé€šè¿‡å¾®è°ƒå°‘é‡ç”¨æˆ·æä¾›çš„æ•°æ®æ¥è¿›è¡Œä¸ªæ€§åŒ–å®šåˆ¶ã€‚ç„¶è€Œï¼Œå¯æ§çš„å¾®è°ƒä»¥é˜²æ­¢æ¨¡å‹å­¦ä¹ å¾®è°ƒæ•°æ®ä¸­çš„ä¸è‰¯æ¦‚å¿µï¼Œå¹¶é˜²æ­¢è¿™äº›æ¦‚å¿µä¸ç”¨æˆ·æç¤ºç›¸äº’çº ç¼ ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚è¿™å¯¹äºä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå¦‚åå·®ç¼“è§£ã€é˜²æ­¢æ¶æ„é€‚åº”ã€å±æ€§è§£è€¦å’Œæ‰©æ•£æ”¿ç­–çš„å¯æ³›åŒ–å¾®è°ƒã€‚æˆ‘ä»¬æå‡ºäº†Coffeeï¼Œå®ƒå…è®¸ä½¿ç”¨è¯­è¨€æ¥æŒ‡å®šä¸è‰¯æ¦‚å¿µä»¥è§„èŒƒé€‚åº”è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åœ¨äºé˜²æ­¢ç”¨æˆ·æç¤ºçš„åµŒå…¥ä¸ä¸è‰¯æ¦‚å¿µå¯¹é½ã€‚å…³é”®çš„æ˜¯ï¼ŒCoffeeä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä¿®æ”¹æ–‡æœ¬æè¿°æ¥çµæ´»åœ°ä¿®æ”¹ä¸è‰¯æ¦‚å¿µã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒä¸ç”¨æˆ·æç¤ºå’Œä¸è‰¯æ¦‚å¿µç›¸å…³çš„å›¾åƒæ¥è¯„ä¼°Coffeeã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoffeeå¯ä»¥é˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å­¦ä¹ æŒ‡å®šçš„ä¸è‰¯æ¦‚å¿µï¼Œå¹¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è®ºæ–‡è¢«æ¥å—åå°†å‘å¸ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14113v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šè¿‡çµæ´»çš„æç¤ºç”Ÿæˆå¤šæ ·åŒ–å†…å®¹ï¼Œé€‚åˆé€šè¿‡å°‘é‡ç”¨æˆ·æä¾›çš„æ•°æ®è¿›è¡Œå¾®è°ƒä»¥å®ç°å®šåˆ¶ã€‚ç„¶è€Œï¼Œå¦‚ä½•å¯æ§åœ°å¾®è°ƒæ¨¡å‹ä»¥é¿å…å­¦ä¹ å¾®è°ƒæ•°æ®ä¸­çš„ä¸è‰¯æ¦‚å¿µï¼Œä»¥åŠé˜²æ­¢è¿™äº›æ¦‚å¿µä¸ç”¨æˆ·æç¤ºçº ç¼ åœ¨ä¸€èµ·ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™å¯¹äºä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå¦‚åå·®ç¼“è§£ã€é˜²æ­¢æ¶æ„é€‚åº”ã€å±æ€§è§£è€¦å’Œæ‰©æ•£æ”¿ç­–çš„é€šç”¨å¾®è°ƒã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCoffeeçš„æ–¹æ³•ï¼Œå…è®¸ä½¿ç”¨è¯­è¨€æ¥æŒ‡å®šä¸è‰¯æ¦‚å¿µä»¥è§„èŒƒé€‚åº”è¿‡ç¨‹ã€‚Coffeeçš„æ ¸å¿ƒåœ¨äºé˜²æ­¢ç”¨æˆ·æç¤ºçš„åµŒå…¥ä¸ä¸è‰¯æ¦‚å¿µå¯¹é½ã€‚å…³é”®çš„æ˜¯ï¼ŒCoffeeæ— éœ€é¢å¤–çš„è®­ç»ƒï¼Œé€šè¿‡ä¿®æ”¹æ–‡æœ¬æè¿°å³å¯çµæ´»ä¿®æ”¹ä¸è‰¯æ¦‚å¿µã€‚é€šè¿‡å®éªŒè¯„ä¼°ï¼ŒCoffeeåœ¨å›¾åƒä¸ç”¨æˆ·æç¤ºå’Œä¸è‰¯æ¦‚å¿µé…å¯¹ä¸Šè¿›è¡Œå¾®è°ƒï¼Œç»“æœè¡¨æ˜å®ƒèƒ½é˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å­¦ä¹ æŒ‡å®šçš„ä¸è‰¯æ¦‚å¿µï¼Œå¹¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å…·å¤‡ç”Ÿæˆå¤šæ ·åŒ–å†…å®¹çš„èƒ½åŠ›ï¼Œä¸”èƒ½é€šè¿‡å¾®è°ƒé€‚åº”ä¸ªæ€§åŒ–éœ€æ±‚ã€‚</li>
<li>å¯æ§çš„å¾®è°ƒæ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œéœ€è¦é˜²æ­¢æ¨¡å‹å­¦ä¹ ä¸è‰¯æ¦‚å¿µå’Œä¸ç”¨æˆ·æç¤ºæ··æ·†ã€‚</li>
<li>ä¸‹æ¸¸ä»»åŠ¡å¦‚åå·®ç¼“è§£ã€é˜²æ­¢æ¶æ„é€‚åº”ç­‰å¯¹äºæ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†åä¸ºCoffeeçš„æ–¹æ³•ï¼Œå…è®¸ä½¿ç”¨è¯­è¨€æ¥æŒ‡å®šä¸è‰¯æ¦‚å¿µï¼Œä»è€Œè§„èŒƒæ¨¡å‹çš„é€‚åº”è¿‡ç¨‹ã€‚</li>
<li>Coffeeé€šè¿‡é˜²æ­¢ç”¨æˆ·æç¤ºä¸ä¸è‰¯æ¦‚å¿µçš„åµŒå…¥å¯¹é½æ¥å®ç°å…¶æ ¸å¿ƒåŠŸèƒ½ã€‚</li>
<li>Coffeeå…·æœ‰çµæ´»æ€§ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹æ–‡æœ¬æè¿°æ¥è½»æ¾ä¿®æ”¹ä¸è‰¯æ¦‚å¿µï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCoffeeèƒ½æœ‰æ•ˆé˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å­¦ä¹ æŒ‡å®šçš„ä¸è‰¯æ¦‚å¿µï¼Œä¸”å…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85f67aff24eeee10871495a9c4a23f53" align="middle">
<img src="https://picx.zhimg.com/v2-4dfb106e0f75725d793300d32e949d5e" align="middle">
<img src="https://picx.zhimg.com/v2-f164134ec06d939174e9eb99a6b55fa6" align="middle">
<img src="https://picx.zhimg.com/v2-e5f1cdfc3b92755244915059d84dd55e" align="middle">
<img src="https://picx.zhimg.com/v2-0f8a2ea196938fc98417c08589be7af5" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FAPE-IR-Frequency-Aware-Planning-and-Execution-Framework-for-All-in-One-Image-Restoration"><a href="#FAPE-IR-Frequency-Aware-Planning-and-Execution-Framework-for-All-in-One-Image-Restoration" class="headerlink" title="FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration"></a>FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration</h2><p><strong>Authors:Jingren Liu, Shuning Xu, Qirui Yang, Yun Wang, Xiangyu Chen, Zhong Ji</strong></p>
<p>All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.</p>
<blockquote>
<p>å…¨æ™¯å›¾åƒæ¢å¤ï¼ˆAIO-IRï¼‰çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤æ‚æ¡ä»¶ä¸‹å¤„ç†å¤šç§é€€åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„è®¾è®¡æˆ–æ½œåœ¨è·¯ç”±ç­–ç•¥ï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥é€‚åº”å…·æœ‰å„ç§é€€åŒ–çš„ç°å®ä¸–ç•Œåœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†FAPE-IRï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå›¾åƒæ¢å¤çš„é¢‘ç‡æ„ŸçŸ¥è§„åˆ’å’Œæ‰§è¡Œæ¡†æ¶ã€‚å®ƒä½¿ç”¨ä¸€ä¸ªå†»ç»“çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºè§„åˆ’å™¨ï¼Œåˆ†æé€€åŒ–å›¾åƒå¹¶ç”Ÿæˆç®€æ´çš„é¢‘ç‡æ„ŸçŸ¥æ¢å¤è®¡åˆ’ã€‚è¿™äº›è®¡åˆ’æŒ‡å¯¼åŸºäºLoRAçš„æ··åˆä¸“å®¶ï¼ˆLoRA-MoEï¼‰æ¨¡å—åœ¨æ‰©æ•£æ‰§è¡Œå™¨ä¸­å·¥ä½œï¼Œè¯¥æ¨¡å—æ ¹æ®è¾“å…¥å›¾åƒé¢‘ç‡ç‰¹å¾åŠ¨æ€é€‰æ‹©é«˜é¢‘æˆ–ä½é¢‘ä¸“å®¶è¿›è¡Œè¡¥å……ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¢å¤è´¨é‡å’Œå‡å°‘ä¼ªå½±ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹æŠ—æ€§è®­ç»ƒå’Œé¢‘ç‡æ­£åˆ™åŒ–æŸå¤±ã€‚é€šè¿‡å°†è¯­ä¹‰è§„åˆ’ä¸åŸºäºé¢‘ç‡çš„æ¢å¤ç›¸ç»“åˆï¼ŒFAPE-IRä¸ºå…¨æ™¯å›¾åƒæ¢å¤æä¾›äº†ç»Ÿä¸€ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFAPE-IRåœ¨ä¸ƒä¸ªæ¢å¤ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ··åˆé€€åŒ–æƒ…å†µä¸‹è¡¨ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14099v1">PDF</a> </p>
<p><strong>Summary</strong><br>    FAPE-IRæå‡ºä¸€ä¸ªé¢‘ç‡æ„ŸçŸ¥çš„è§„åˆ’æ‰§è¡Œæ¡†æ¶ï¼Œç”¨äºå›¾åƒä¿®å¤ã€‚å®ƒç»“åˆå†»ç»“çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºè§„åˆ’å™¨ï¼Œç”Ÿæˆç®€æ´çš„é¢‘ç‡æ„ŸçŸ¥ä¿®å¤è®¡åˆ’ï¼Œå¹¶å¼•å¯¼åŸºäºLoRAçš„ä¸“å®¶æ··åˆæ¨¡å—ï¼ˆLoRA-MoEï¼‰åœ¨æ‰©æ•£æ‰§è¡Œå™¨ä¸­åŠ¨æ€é€‰æ‹©é«˜é¢‘æˆ–ä½é¢‘ä¸“å®¶ï¼Œè¾…ä»¥è¾“å…¥å›¾åƒçš„é¢‘ç‡ç‰¹å¾ã€‚é€šè¿‡è¯­ä¹‰è§„åˆ’ä¸é¢‘ç‡ä¿®å¤çš„ç»“åˆï¼ŒFAPE-IRä¸ºå…¨æ–¹ä½å›¾åƒä¿®å¤æä¾›äº†ç»Ÿä¸€ä¸”å¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIO-IRç›®æ ‡å¼€å‘ç»Ÿä¸€æ¨¡å‹ï¼Œå¤„ç†å¤æ‚æ¡ä»¶ä¸‹çš„å¤šç§é€€åŒ–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–ç‰¹å®šä»»åŠ¡è®¾è®¡æˆ–æ½œåœ¨è·¯ç”±ç­–ç•¥ï¼Œéš¾ä»¥é€‚åº”å¤šç§é€€åŒ–åœºæ™¯ã€‚</li>
<li>FAPE-IRé‡‡ç”¨é¢‘ç‡æ„ŸçŸ¥çš„è§„åˆ’æ‰§è¡Œæ¡†æ¶è¿›è¡Œå›¾åƒä¿®å¤ã€‚</li>
<li>åˆ©ç”¨å†»ç»“çš„MLLMä½œä¸ºè§„åˆ’å™¨ï¼Œç”Ÿæˆç®€æ´çš„é¢‘ç‡æ„ŸçŸ¥ä¿®å¤è®¡åˆ’ã€‚</li>
<li>LoRA-MoEæ¨¡å—åœ¨æ‰©æ•£æ‰§è¡Œå™¨ä¸­åŠ¨æ€é€‰æ‹©é«˜é¢‘æˆ–ä½é¢‘ä¸“å®¶ã€‚</li>
<li>å¯¹æŠ—è®­ç»ƒå’Œé¢‘ç‡æ­£åˆ™åŒ–æŸå¤±æé«˜äº†ä¿®å¤è´¨é‡å’Œå‡å°‘äº†ä¼ªå½±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1be4bcd6c654542367779f85b05282f9" align="middle">
<img src="https://picx.zhimg.com/v2-9f061d98551f858a66f397584975e551" align="middle">
<img src="https://picx.zhimg.com/v2-2563ec37b4976d202280d162a24f38da" align="middle">
<img src="https://picx.zhimg.com/v2-de19bd8ce665bba3e8ce445d792e08d2" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FashionMAC-Deformation-Free-Fashion-Image-Generation-with-Fine-Grained-Model-Appearance-Customization"><a href="#FashionMAC-Deformation-Free-Fashion-Image-Generation-with-Fine-Grained-Model-Appearance-Customization" class="headerlink" title="FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization"></a>FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization</h2><p><strong>Authors:Rong Zhang, Jinxiao Li, Jingnan Wang, Zhiwen Zuo, Jianfeng Dong, Wei Li, Chi Wang, Weiwei Xu, Xun Wang</strong></p>
<p>Garment-centric fashion image generation aims to synthesize realistic and controllable human models dressing a given garment, which has attracted growing interest due to its practical applications in e-commerce. The key challenges of the task lie in two aspects: (1) faithfully preserving the garment details, and (2) gaining fine-grained controllability over the modelâ€™s appearance. Existing methods typically require performing garment deformation in the generation process, which often leads to garment texture distortions. Also, they fail to control the fine-grained attributes of the generated models, due to the lack of specifically designed mechanisms. To address these issues, we propose FashionMAC, a novel diffusion-based deformation-free framework that achieves high-quality and controllable fashion showcase image generation. The core idea of our framework is to eliminate the need for performing garment deformation and directly outpaint the garment segmented from a dressed person, which enables faithful preservation of the intricate garment details. Moreover, we propose a novel region-adaptive decoupled attention (RADA) mechanism along with a chained mask injection strategy to achieve fine-grained appearance controllability over the synthesized human models. Specifically, RADA adaptively predicts the generated regions for each fine-grained text attribute and enforces the text attribute to focus on the predicted regions by a chained mask injection strategy, significantly enhancing the visual fidelity and the controllability. Extensive experiments validate the superior performance of our framework compared to existing state-of-the-art methods.</p>
<blockquote>
<p>æœè£…å¯¼å‘çš„æ—¶å°šå›¾åƒç”Ÿæˆæ—¨åœ¨åˆæˆç©¿ç€ç»™å®šæœè£…çš„ç°å®ä¸”å¯æ§çš„äººä½“æ¨¡å‹ï¼Œè¿™å› å…¶ç”µå­å•†åŠ¡ç­‰å®é™…åº”ç”¨è€Œæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚è¯¥ä»»åŠ¡çš„å…³é”®æŒ‘æˆ˜åœ¨äºä¸¤ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰å¿ å®ä¿ç•™æœè£…ç»†èŠ‚ï¼›ï¼ˆ2ï¼‰å®ç°å¯¹æ¨¡å‹å¤–è§‚çš„ç²¾ç»†æ§åˆ¶ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œæœè£…å˜å½¢ï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´æœè£…çº¹ç†å¤±çœŸã€‚æ­¤å¤–ï¼Œç”±äºç¼ºä¹ä¸“é—¨è®¾è®¡çš„æœºåˆ¶ï¼Œå®ƒä»¬æ— æ³•æ§åˆ¶ç”Ÿæˆæ¨¡å‹çš„ç²¾ç»†å±æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FashionMACï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ— å˜å½¢æ¡†æ¶ï¼Œå¯å®ç°é«˜è´¨é‡ä¸”å¯æ§çš„æ—¶å°šå±•ç¤ºå›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¸éœ€è¦æ‰§è¡Œæœè£…å˜å½¢ï¼Œè€Œæ˜¯ç›´æ¥å¤–æ¨ä»ç©¿ç€è€…èº«ä¸Šåˆ†å‰²å‡ºæ¥çš„æœè£…ï¼Œè¿™èƒ½å¤Ÿå¿ å®ä¿ç•™å¤æ‚çš„æœè£…ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒºåŸŸè‡ªé€‚åº”è§£è€¦æ³¨æ„åŠ›ï¼ˆRADAï¼‰æœºåˆ¶ï¼Œå¹¶ç»“åˆé“¾å¼æ©è†œæ³¨å…¥ç­–ç•¥ï¼Œä»¥å®ç°åˆæˆäººä½“æ¨¡å‹çš„ç²¾ç»†å¤–è§‚æ§åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒRADAè‡ªé€‚åº”åœ°é¢„æµ‹æ¯ä¸ªç²¾ç»†æ–‡æœ¬å±æ€§çš„ç”ŸæˆåŒºåŸŸï¼Œå¹¶é€šè¿‡é“¾å¼æ©è†œæ³¨å…¥ç­–ç•¥å¼ºåˆ¶æ–‡æœ¬å±æ€§é›†ä¸­åœ¨é¢„æµ‹åŒºåŸŸï¼Œè¿™æ˜¾è‘—æé«˜äº†è§†è§‰é€¼çœŸåº¦å’Œå¯æ§æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14031v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— éœ€å˜å½¢çš„æ—¶è£…å±•ç¤ºå›¾åƒç”Ÿæˆæ¡†æ¶FashionMACï¼Œå¯é«˜è´¨é‡ä¸”å¯æ§åœ°ç”Ÿæˆæ—¶è£…å±•ç¤ºå›¾åƒã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä¸éœ€è¦æ‰§è¡Œè¡£ç‰©å˜å½¢ï¼Œè€Œæ˜¯ç›´æ¥å¤–æ¨ä»ç©¿ç€è€…èº«ä¸Šåˆ†å‰²å‡ºæ¥çš„è¡£ç‰©ï¼Œå®ç°è¡£ç‰©ç»†èŠ‚çš„å¿ å®ä¿ç•™ã€‚åŒæ—¶ï¼Œç»“åˆåŒºåŸŸè‡ªé€‚åº”è§£è€¦æ³¨æ„åŠ›ï¼ˆRADAï¼‰æœºåˆ¶å’Œé“¾å¼æ©è†œæ³¨å…¥ç­–ç•¥ï¼Œå®ç°å¯¹åˆæˆäººç‰©æ¨¡å‹çš„ç»†ç²’åº¦å¤–è§‚æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœè£…ç»†èŠ‚çš„çœŸå®ä¿ç•™ï¼šé€šè¿‡æ¶ˆé™¤è¡£ç‰©å˜å½¢çš„éœ€æ±‚ï¼Œç›´æ¥å¤–æ¨åˆ†å‰²çš„è¡£ç‰©éƒ¨åˆ†ï¼Œå¿ å®ä¿ç•™è¡£ç‰©çš„ç»†èŠ‚ã€‚</li>
<li>é«˜è´¨é‡ä¸”å¯æ§çš„æ—¶è£…å±•ç¤ºå›¾åƒç”Ÿæˆï¼šæå‡ºçš„FashionMACæ¡†æ¶èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„æ—¶è£…å±•ç¤ºå›¾åƒç”Ÿæˆï¼Œå¹¶ä¸”å…·æœ‰å¯æ§æ€§ã€‚</li>
<li>å¼•å…¥åŒºåŸŸè‡ªé€‚åº”è§£è€¦æ³¨æ„åŠ›ï¼ˆRADAï¼‰æœºåˆ¶ï¼šRADAæœºåˆ¶èƒ½å¤Ÿè‡ªé€‚åº”é¢„æµ‹æ¯ä¸ªç»†ç²’åº¦æ–‡æœ¬å±æ€§çš„ç”ŸæˆåŒºåŸŸã€‚</li>
<li>é“¾å¼æ©è†œæ³¨å…¥ç­–ç•¥ï¼šé€šè¿‡è¯¥ç­–ç•¥ï¼Œæ–‡æœ¬å±æ€§èƒ½å¤Ÿä¸“æ³¨äºé¢„æµ‹çš„ç”ŸæˆåŒºåŸŸï¼Œæé«˜äº†è§†è§‰ä¿çœŸåº¦å’Œå¯æ§æ€§ã€‚</li>
<li>æ¡†æ¶è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼šç»è¿‡å¹¿æ³›å®éªŒéªŒè¯ï¼ŒFashionMACæ¡†æ¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›æŠ€æœ¯ã€‚</li>
<li>æ ¸å¿ƒæŒ‘æˆ˜åŒ…æ‹¬ç»†èŠ‚ä¿ç•™å’Œæ¨¡å‹å¤–è§‚çš„ç²¾ç»†æ§åˆ¶ï¼šæ—¶è£…å›¾åƒç”Ÿæˆçš„å…³é”®æŒ‘æˆ˜åœ¨äºå¦‚ä½•çœŸå®åœ°ä¿ç•™æœè£…ç»†èŠ‚ï¼Œä»¥åŠå®ç°å¯¹æ¨¡å‹å¤–è§‚çš„ç²¾ç»†æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b67a9e09839c6a650f6d5a3dd2b73eb" align="middle">
<img src="https://picx.zhimg.com/v2-1f7e8147c272a286253d15aadae7364b" align="middle">
<img src="https://picx.zhimg.com/v2-31d6e581bd9c884243ba518d302649df" align="middle">
<img src="https://picx.zhimg.com/v2-b710edeb568b3d845f67a7da51e7dd35" align="middle">
<img src="https://picx.zhimg.com/v2-92aebab37ba72693c0ee2840232350dc" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Scene-Graph-Guided-Generative-AI-Framework-for-Synthesizing-and-Evaluating-Industrial-Hazard-Scenarios"><a href="#Scene-Graph-Guided-Generative-AI-Framework-for-Synthesizing-and-Evaluating-Industrial-Hazard-Scenarios" class="headerlink" title="Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios"></a>Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios</h2><p><strong>Authors:Sanjay Acharjee, Abir Khan Ratul, Diego Patino, Md Nazmus Sakib</strong></p>
<p>Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.</p>
<blockquote>
<p>è®­ç»ƒè§†è§‰æ¨¡å‹ä»¥å‡†ç¡®æ£€æµ‹å·¥ä½œåœºæ‰€å±é™©éœ€è¦ç°å®çš„ä¸å®‰å…¨çŠ¶å†µå›¾åƒï¼Œè¿™äº›å›¾åƒå¯èƒ½å¯¼è‡´äº‹æ•…ã€‚ç„¶è€Œï¼Œè·å–æ­¤ç±»æ•°æ®é›†å¾ˆå›°éš¾ï¼Œå› ä¸ºæ•æ‰å‘ç”Ÿçš„å¯¼è‡´äº‹æ•…çš„åœºæ™¯å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯å›¾å¼•å¯¼ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†èŒä¸šå®‰å…¨ä¸å¥åº·ç®¡ç†å±€ï¼ˆOSHAï¼‰çš„å†å²äº‹æ•…æŠ¥å‘Šï¼Œåˆæˆé€¼çœŸçš„å±é™©åœºæ™¯å›¾åƒã€‚ä½¿ç”¨GPT-4oåˆ†æOSHAçš„å™è¿°ï¼Œæå–ç»“æ„åŒ–å±é™©æ¨ç†ï¼Œå°†å…¶è½¬æ¢ä¸ºæ•è·ç©ºé—´å’Œä¸Šä¸‹æ–‡å…³ç³»ç†è§£é£é™©çš„å¯¹è±¡çº§åœºæ™¯å›¾ã€‚è¿™äº›å›¾å½¢å¼•å¯¼æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆç»“æ„ä¸Šå‡†ç¡®çš„å±é™©åœºæ™¯ã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆæ•°æ®çš„çœŸå®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦ï¼Œå¼•å…¥äº†ä¸€ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¡†æ¶ã€‚åœ¨å››ç§æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæ‰€æå‡ºçš„åŸºäºVQAå›¾çš„è¯„åˆ†æ–¹æ³•ä¼˜äºåŸºäºç†µéªŒè¯çš„CLIPå’ŒBLIPæŒ‡æ ‡ï¼Œè¯æ˜äº†å…¶æ›´é«˜çš„é‰´åˆ«æ•æ„Ÿæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13970v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåœºæ™¯å›¾å¼•å¯¼çš„æ–°å‹ç”Ÿæˆå¼AIæ¡†æ¶ï¼Œç”¨äºåˆæˆåŸºäºå†å²èŒä¸šå®‰å…¨ä¸å«ç”Ÿç®¡ç†å±€ï¼ˆOSHAï¼‰äº‹æ•…æŠ¥å‘Šçš„é€¼çœŸå±é™©åœºæ™¯å›¾åƒã€‚è¯¥ç ”ç©¶ä½¿ç”¨GPT-4oåˆ†æOSHAæŠ¥å‘Šï¼Œæå–ç»“æ„åŒ–å±é™©æ¨ç†ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæ•æ‰ç©ºé—´å’Œä¸Šä¸‹æ–‡å…³ç³»çš„å¯¹è±¡çº§åœºæ™¯å›¾ï¼Œä»¥ç†è§£é£é™©ã€‚è¿™äº›åœºæ™¯å›¾æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆç»„åˆå‡†ç¡®çš„å±é™©åœºæ™¯ã€‚è¯„ä¼°ç”Ÿæˆæ•°æ®çœŸå®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦æ—¶ï¼Œå¼•å…¥äº†ä¸€ç§è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¡†æ¶ã€‚ç›¸è¾ƒäºCLIPå’ŒBLIPæŒ‡æ ‡ï¼Œè¯¥ç ”ç©¶çš„VQA Graph Scoreè¯„ä»·æŒ‡æ ‡è¡¨ç°å‡ºæ›´é«˜çš„é‰´åˆ«æ•æ„Ÿæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒè§†è§‰æ¨¡å‹æ£€æµ‹å·¥ä½œåœºæ‰€å±é™©éœ€è¦çœŸå®çš„å±é™©å›¾åƒæ•°æ®é›†ï¼Œä½†è·å–è¿™äº›æ•°æ®é›†ååˆ†å›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåœºæ™¯å›¾çš„ç”Ÿæˆå¼AIæ¡†æ¶ï¼Œåˆæˆé€¼çœŸçš„å±é™©åœºæ™¯å›¾åƒã€‚</li>
<li>ä½¿ç”¨GPT-4oä»OSHAäº‹æ•…æŠ¥å‘Šä¸­æå–ç»“æ„åŒ–å±é™©æ¨ç†ã€‚</li>
<li>å°†å±é™©æ¨ç†è½¬åŒ–ä¸ºå¯¹è±¡çº§åœºæ™¯å›¾ï¼Œæ•æ‰ç©ºé—´å’Œä¸Šä¸‹æ–‡å…³ç³»ä»¥ç†è§£é£é™©ã€‚</li>
<li>åœºæ™¯å›¾æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå±é™©åœºæ™¯ã€‚</li>
<li>å¼•å…¥VQAæ¡†æ¶è¯„ä¼°ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13970">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cad42ebc680166a318286c640781906" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Crossing-Borders-A-Multimodal-Challenge-for-Indian-Poetry-Translation-and-Image-Generation"><a href="#Crossing-Borders-A-Multimodal-Challenge-for-Indian-Poetry-Translation-and-Image-Generation" class="headerlink" title="Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation"></a>Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation</h2><p><strong>Authors:Sofia Jamil, Kotla Sai Charan, Sriparna Saha, Koustava Goswami, Joseph K J</strong></p>
<p>Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the readerâ€™s experience.</p>
<blockquote>
<p>å°åº¦è¯—æ­Œä»¥å…¶è¯­è¨€å¤æ‚æ€§å’Œæ·±åšçš„æ–‡åŒ–å…±é¸£è€Œé—»åï¼Œæ‹¥æœ‰è·¨è¶Šæ•°åƒå¹´çš„ä¸°å¯Œè€Œå¤šæ ·çš„ä¼ ç»Ÿã€‚ç„¶è€Œï¼Œå…¶å±‚æ¬¡åŒ–çš„æ„ä¹‰ã€æ–‡åŒ–å…¸æ•…å’Œå¤æ‚çš„è¯­æ³•ç»“æ„å¾€å¾€æ„æˆç†è§£ä¸Šçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéæ¯è¯­è€…æˆ–å¯¹å…¶è¯­å¢ƒå’Œè¯­è¨€ä¸ç†Ÿæ‚‰çš„è¯»è€…ã€‚å°½ç®¡å…¶åœ¨æ–‡åŒ–ä¸Šå…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†ç°æœ‰å…³äºè¯—æ­Œçš„ä½œå“å¤§å¤šå¿½ç•¥äº†å°åº¦è¯­è¨€è¯—æ­Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¿»è¯‘ä¸å›¾åƒç”Ÿæˆï¼ˆTAIï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é€‚å½“çš„æç¤ºè°ƒæ•´ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ”¯æŒè”åˆå›½å¯æŒç»­å‘å±•ç›®æ ‡ä¸­çš„ä¼˜è´¨æ•™è‚²ï¼ˆSDG 4ï¼‰å’Œå‡å°‘ä¸å¹³ç­‰ï¼ˆSDG 10ï¼‰ï¼Œé€šè¿‡æé«˜å¯Œå«æ–‡åŒ–çš„å°åº¦è¯­è¨€è¯—æ­Œçš„å¯è¾¾æ€§ï¼Œä¸ºå…¨çƒå—ä¼—æœåŠ¡ã€‚å®ƒåŒ…æ‹¬ï¼ˆ1ï¼‰ä¸€ä¸ªä½¿ç”¨å‡ ç‡æ¯”ç‡åå¥½å¯¹é½ç®—æ³•çš„ç¿»è¯‘æ¨¡å—ï¼Œè¯¥ç®—æ³•å¯å°†å½¢æ€ä¸°å¯Œçš„è¯—æ­Œå‡†ç¡®åœ°ç¿»è¯‘æˆè‹±è¯­ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—é‡‡ç”¨è¯­ä¹‰å›¾æ¥æ•è·æ ‡è®°ã€ä¾èµ–å…³ç³»å’Œéšå–»åŠå…¶æ„ä¹‰ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»¥åˆ›å»ºå°åº¦è¯—æ­Œçš„è§†è§‰æœ‰æ„ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å…¨é¢å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬äººç±»å’Œå®šé‡è¯„ä¼°ï¼Œè¯æ˜äº†TAIæ‰©æ•£åœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³å°åº¦è¯­è¨€è¯—æ­Œèµ„æºåŒ®ä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†å½¢æ€ä¸°å¯Œå°åº¦è¯­è¨€è¯—æ­ŒMorphoVerseæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«1570é¦–è·¨è¶Š21ç§ä½èµ„æºå°åº¦è¯­è¨€çš„è¯—æ­Œã€‚é€šè¿‡è§£å†³è¯—æ­Œç¿»è¯‘å’Œè§†è§‰ç†è§£ä¹‹é—´çš„å·®è·ï¼Œè¿™é¡¹å·¥ä½œæ—¨åœ¨æé«˜å¯åŠæ€§å¹¶ä¸°å¯Œè¯»è€…çš„ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13689v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆç¿»è¯‘ä¸å›¾åƒç”Ÿæˆçš„æ¡†æ¶ï¼ˆTAIï¼‰ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡é€‚å½“çš„æç¤ºè°ƒæ•´ï¼Œæ”¯æŒå°åº¦è¯­è¨€è¯—æ­Œçš„å…¨çƒåŒ–ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ç¿»è¯‘æ¨¡å—å’Œå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œå‰è€…ä½¿ç”¨Odds Ratio Preference Alignment Algorithmå°†å½¢æ€ä¸°å¯Œçš„è¯—æ­Œç¿»è¯‘æˆè‹±è¯­ï¼Œåè€…åˆ©ç”¨è¯­ä¹‰å›¾æ•æ‰è¯—æ­Œä¸­çš„ç¬¦å·ã€ä¾èµ–å…³ç³»å’Œéšå–»ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä¸ºå°åº¦è¯—æ­Œåˆ›å»ºè§†è§‰ä¸Šæœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒTAIåœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³å°åº¦è¯­è¨€è¯—æ­Œèµ„æºåŒ®ä¹çš„é—®é¢˜ï¼Œè¿˜æ¨å‡ºäº†MorphoVerseæ•°æ®é›†ï¼ŒåŒ…å«1570é¦–è·¨è¶Š21ç§ä½èµ„æºå°åº¦è¯­è¨€çš„è¯—æ­Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°åº¦è¯—æ­Œå…·æœ‰æ‚ ä¹…å†å²å’Œä¸°å¯Œå¤šæ ·çš„é—äº§ï¼Œä½†å…¶å¤æ‚çš„è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯å¯¹éæ¯è¯­è€…æˆ–ä¸äº†è§£å…¶æ–‡åŒ–å’Œè¯­è¨€èƒŒæ™¯çš„è¯»è€…æ¥è¯´ï¼Œç†è§£èµ·æ¥å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰å…³äºè¯—æ­Œçš„ç ”ç©¶å¤§å¤šå¿½ç•¥äº†å°åº¦è¯­è¨€è¯—æ­Œã€‚</li>
<li>æå‡ºçš„Translation and Image Generation (TAI)æ¡†æ¶ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºå°åº¦è¯­è¨€è¯—æ­Œçš„å…¨çƒæ€§ã€‚</li>
<li>TAIæ¡†æ¶åŒ…æ‹¬ç¿»è¯‘æ¨¡å—å’Œå›¾åƒç”Ÿæˆæ¨¡å—ï¼Œå‰è€…é‡‡ç”¨Odds Ratio Preference Alignment Algorithmè¿›è¡Œç¿»è¯‘ï¼Œåè€…åˆ©ç”¨è¯­ä¹‰å›¾æ•æ‰è¯—æ­Œçš„è¯­ä¹‰å…³ç³»ï¼Œç”Ÿæˆæœ‰æ„ä¹‰çš„å›¾åƒè¡¨ç¤ºã€‚</li>
<li>ç»¼åˆå®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒTAIåœ¨è¯—æ­Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>é’ˆå¯¹å°åº¦è¯­è¨€è¯—æ­Œèµ„æºçš„åŒ®ä¹ï¼Œæ¨å‡ºäº†Morphologically Rich Indian Language Poems MorphoVerseæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5081cff53890101b3bec859e9232d49" align="middle">
<img src="https://picx.zhimg.com/v2-fcf6490258e970f26ba6a174333559a7" align="middle">
<img src="https://picx.zhimg.com/v2-b7b13598440fc3707843a91a6a8c965d" align="middle">
<img src="https://picx.zhimg.com/v2-2f02d6dcf4b57a639f1aa397320af375" align="middle">
<img src="https://picx.zhimg.com/v2-9d5028d10aa948a9debf3fbabe5f23d7" align="middle">
<img src="https://picx.zhimg.com/v2-2569534174f2934f2d2a780b02a6d8ef" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Generalized-Denoising-Diffusion-Codebook-Models-gDDCM-Tokenizing-images-using-a-pre-trained-diffusion-model"><a href="#Generalized-Denoising-Diffusion-Codebook-Models-gDDCM-Tokenizing-images-using-a-pre-trained-diffusion-model" class="headerlink" title="Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model"></a>Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model</h2><p><strong>Authors:Fei Kong</strong></p>
<p>Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.</p>
<blockquote>
<p>æœ€è¿‘ï¼Œæå‡ºäº†å»å™ªæ‰©æ•£ç¼–ç æœ¬æ¨¡å‹ï¼ˆDDCMï¼‰ã€‚DDCMåˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œæ ¹æ®é¢„è®¾è§„åˆ™ï¼Œå°†åå‘è¿‡ç¨‹ä¸­çš„éšæœºå™ªå£°æ›¿æ¢ä¸ºä»ç‰¹å®šé›†åˆä¸­é‡‡æ ·çš„å™ªå£°ï¼Œä»è€Œå®ç°å›¾åƒå‹ç¼©ã€‚ç„¶è€Œï¼ŒDDCMæ— æ³•åº”ç”¨äºé™¤DDPMä¹‹å¤–çš„å…¶ä»–æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¹¿ä¹‰å»å™ªæ‰©æ•£å‹ç¼©æ¨¡å‹ï¼ˆgDDCMï¼‰ï¼Œå®ƒå°†DDCMæ‰©å±•åˆ°ä¸»æµæ‰©æ•£æ¨¡å‹åŠå…¶å˜ä½“ï¼ŒåŒ…æ‹¬DDPMã€åŸºäºåˆ†æ•°çš„æ¨¡å‹ã€ä¸€è‡´æ€§æ¨¡å‹å’Œä¿®æ­£æµã€‚æˆ‘ä»¬åœ¨CIFAR-10å’ŒLSUNå§å®¤æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å°†DDCMæ¨å¹¿åˆ°äº†ä¸Šè¿°æ¨¡å‹ï¼Œå¹¶å®ç°äº†æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13387v2">PDF</a> in Chinese language</p>
<p><strong>Summary</strong><br>     è¿‘æœŸæå‡ºäº†å»å™ªæ‰©æ•£ç¼–ç æœ¬æ¨¡å‹ï¼ˆDDCMï¼‰ï¼Œå®ƒåˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰å¹¶å°†åå‘è¿‡ç¨‹ä¸­çš„éšæœºå™ªå£°æ›¿æ¢ä¸ºç‰¹å®šé›†åˆä¸­çš„å™ªå£°æ ·æœ¬ï¼Œä»è€Œå®ç°å›¾åƒå‹ç¼©ã€‚ç„¶è€Œï¼ŒDDCMä»…é€‚ç”¨äºDDPMæ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†å¹¿ä¹‰å»å™ªæ‰©æ•£å‹ç¼©æ¨¡å‹ï¼ˆgDDCMï¼‰ï¼Œå°†DDCMæ‰©å±•åˆ°ä¸»æµæ‰©æ•£æ¨¡å‹åŠå…¶å˜ä½“ï¼ŒåŒ…æ‹¬DDPMã€åŸºäºåˆ†æ•°çš„æ¨¡å‹ã€ä¸€è‡´æ€§æ¨¡å‹å’Œæ ¡æ­£æµã€‚æˆ‘ä»¬åœ¨CIFAR-10å’ŒLSUNå§å®¤æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸåœ°å°†DDCMæ¨å¹¿åˆ°äº†ä¸Šè¿°æ¨¡å‹ï¼Œå¹¶å®ç°äº†æ€§èƒ½çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDCMåˆ©ç”¨DDPMå¹¶å°†åå‘è¿‡ç¨‹ä¸­çš„éšæœºå™ªå£°æ›¿æ¢ä¸ºç‰¹å®šé›†åˆé‡‡æ ·çš„å™ªå£°ï¼Œå®ç°å›¾åƒå‹ç¼©ã€‚</li>
<li>DDCMä»…é€‚ç”¨äºDDPMï¼Œç¼ºä¹é€šç”¨æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†å¹¿ä¹‰å»å™ªæ‰©æ•£å‹ç¼©æ¨¡å‹ï¼ˆgDDCMï¼‰ï¼Œä»¥æ‰©å±•DDCMçš„åº”ç”¨èŒƒå›´ã€‚</li>
<li>gDDCMå¯ä»¥åº”ç”¨äºä¸»æµæ‰©æ•£æ¨¡å‹åŠå…¶å˜ä½“ï¼ŒåŒ…æ‹¬DDPMã€åŸºäºåˆ†æ•°çš„æ¨¡å‹ã€ä¸€è‡´æ€§æ¨¡å‹å’Œæ ¡æ­£æµã€‚</li>
<li>gDDCMåœ¨CIFAR-10å’ŒLSUNå§å®¤æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒgDDCMæˆåŠŸæ¨å¹¿äº†DDCMçš„åº”ç”¨ï¼Œå¹¶æé«˜äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9f270504587e4f3eb8e2ef20e7e6270" align="middle">
<img src="https://picx.zhimg.com/v2-ed515e6490ffa03dc1745d7a682e9a65" align="middle">
<img src="https://picx.zhimg.com/v2-3a04df4ee769f58838d7027a23d412c4" align="middle">
<img src="https://picx.zhimg.com/v2-a9bd143b2e8ca03340ecf8aefc81ac6c" align="middle">
<img src="https://picx.zhimg.com/v2-34a334b32a9bb53aef0fb4fcd4751123" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-3D-Object-Centric-Feature-Learning-for-Semantic-Scene-Completion"><a href="#Towards-3D-Object-Centric-Feature-Learning-for-Semantic-Scene-Completion" class="headerlink" title="Towards 3D Object-Centric Feature Learning for Semantic Scene Completion"></a>Towards 3D Object-Centric Feature Learning for Semantic Scene Completion</h2><p><strong>Authors:Weihua Wang, Yubo Cui, Xiangru Lin, Zhiheng Li, Zheng Fang</strong></p>
<p>Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.</p>
<blockquote>
<p>åŸºäºè§†è§‰çš„3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰å› å…¶åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨æ½œåŠ›è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é‡‡ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒï¼ˆego-centricï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ•´ä¸ªåœºæ™¯ä¸Šèšåˆå’Œæ‰©æ•£ç‰¹å¾ï¼Œä½†å®ƒä»¬å¸¸å¸¸å¿½ç•¥ç²¾ç»†çš„ç‰©ä½“çº§ç»†èŠ‚ï¼Œå¯¼è‡´è¯­ä¹‰å’Œå‡ ä½•ä¸Šçš„æ¨¡ç³Šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Oceanï¼Œä¸€ä¸ªä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„é¢„æµ‹æ¡†æ¶ï¼Œå®ƒå°†åœºæ™¯åˆ†è§£ä¸ºå•ç‹¬çš„ç‰©ä½“å®ä¾‹ï¼Œä»¥å®ç°æ›´ç²¾ç¡®çš„è¯­ä¹‰å ç”¨é¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨è½»é‡çº§åˆ†å‰²æ¨¡å‹MobileSAMä»è¾“å…¥å›¾åƒä¸­æå–å®ä¾‹æ©è†œã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ª3Dè¯­ä¹‰ç»„æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒåˆ©ç”¨çº¿æ€§æ³¨æ„åŠ›åœ¨3Dç©ºé—´ä¸­èšåˆç‰©ä½“ä¸­å¿ƒç‰¹å¾ã€‚ä¸ºäº†å¤„ç†åˆ†å‰²é”™è¯¯å’Œç¼ºå¤±çš„å®ä¾‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªå…¨å±€ç›¸ä¼¼æ€§å¼•å¯¼æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒåˆ©ç”¨åˆ†å‰²ç‰¹å¾è¿›è¡Œå…¨å±€äº¤äº’ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®ä¾‹æ„ŸçŸ¥å±€éƒ¨æ‰©æ•£æ¨¡å—ï¼Œå®ƒé€šè¿‡ç”Ÿæˆè¿‡ç¨‹æ”¹è¿›å®ä¾‹ç‰¹å¾ï¼Œéšååœ¨BEVç©ºé—´ç»†åŒ–åœºæ™¯è¡¨ç¤ºã€‚åœ¨SemanticKITTIå’ŒSSCBench-KITTI360åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOceanè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒmIoUå¾—åˆ†åˆ†åˆ«ä¸º17.40å’Œ20.28ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13031v2">PDF</a> Accepted to AAAI-2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åŸºäºè§†è§‰çš„3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½ç•¥å¯¹è±¡çº§åˆ«çš„ç»†èŠ‚å¯¼è‡´çš„è¯­ä¹‰å’Œå‡ ä½•æ¨¡ç³Šé—®é¢˜ï¼Œæå‡ºäº†Oceanæ¡†æ¶ã€‚å®ƒé‡‡ç”¨å¯¹è±¡ä¸­å¿ƒé¢„æµ‹ï¼Œå°†åœºæ™¯åˆ†è§£ä¸ºç‹¬ç«‹å¯¹è±¡å®ä¾‹ï¼Œå®ç°æ›´å‡†ç¡®çš„è¯­ä¹‰å ç”¨é¢„æµ‹ã€‚é€šè¿‡æ¨¡å—è®¾è®¡ï¼ŒåŒ…æ‹¬MobileSAMåˆ†å‰²æ¨¡å‹ã€3Dè¯­ä¹‰ç»„æ³¨æ„åŠ›æ¨¡å—ã€å…¨å±€ç›¸ä¼¼æ€§å¼•å¯¼æ³¨æ„åŠ›å’Œå®ä¾‹æ„ŸçŸ¥å±€éƒ¨æ‰©æ•£æ¨¡å—ï¼Œä»¥æé«˜æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚åœ¨SemanticKITTIå’ŒSSCBench-KITTI360åŸºå‡†æµ‹è¯•ä¸Šï¼ŒOceanå®ç°äº†æœ€ä½³æ€§èƒ½ï¼ŒmIoUå¾—åˆ†åˆ†åˆ«ä¸º17.40å’Œ20.28ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶ä¸­ï¼ŒåŸºäºè§†è§‰çš„3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰å—åˆ°å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤šé‡‡ç”¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„é¢„æµ‹èŒƒå¼ï¼Œå¯¼è‡´è¯­ä¹‰å’Œå‡ ä½•æ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>Oceanæ¡†æ¶é‡‡ç”¨å¯¹è±¡ä¸­å¿ƒé¢„æµ‹ï¼Œåˆ†è§£åœºæ™¯ä¸ºç‹¬ç«‹å¯¹è±¡å®ä¾‹ï¼Œæé«˜è¯­ä¹‰å ç”¨é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>OceanåŒ…æ‹¬MobileSAMåˆ†å‰²æ¨¡å‹ã€3Dè¯­ä¹‰ç»„æ³¨æ„åŠ›æ¨¡å—ç­‰å…³é”®æ¨¡å—ã€‚</li>
<li>Oceané€šè¿‡å…¨å±€ç›¸ä¼¼æ€§å¼•å¯¼æ³¨æ„åŠ›å¤„ç†åˆ†å‰²é”™è¯¯å’Œç¼ºå¤±å®ä¾‹ã€‚</li>
<li>Oceané€šè¿‡å®ä¾‹æ„ŸçŸ¥å±€éƒ¨æ‰©æ•£æ¨¡å—æé«˜å®ä¾‹ç‰¹å¾å¹¶ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c822bf482e1eeb465948fd01cf65c81" align="middle">
<img src="https://picx.zhimg.com/v2-d7decc4adf777792327f5f72d97cd86f" align="middle">
<img src="https://picx.zhimg.com/v2-982fd138e42c45b00c22f4be4173e13a" align="middle">
<img src="https://picx.zhimg.com/v2-ec76fe343ee1fe4328f17e45b77c057c" align="middle">
<img src="https://picx.zhimg.com/v2-3217193d7bd07426126f2d26363fda65" align="middle">
<img src="https://picx.zhimg.com/v2-8a424d82c4a6bb5bc3fe144c40f52759" align="middle">
<img src="https://picx.zhimg.com/v2-cae2a10d2e29f65cde48735b3a300840" align="middle">
<img src="https://picx.zhimg.com/v2-4ca4097146be43768f80bb2ea7a82b02" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PFAvatar-Pose-Fusion-3D-Personalized-Avatar-Reconstruction-from-Real-World-Outfit-of-the-Day-Photos"><a href="#PFAvatar-Pose-Fusion-3D-Personalized-Avatar-Reconstruction-from-Real-World-Outfit-of-the-Day-Photos" class="headerlink" title="PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos"></a>PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos</h2><p><strong>Authors:Dianbing Xi, Guoyuan An, Jingsen Zhu, Zhijian Liu, Yuan Liu, Ruiyuan Zhang, Jiayuan Lu, Yuchi Huo, Rui Wang</strong></p>
<p>We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from Outfit of the Day(OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48x speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions&#x2F;truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†PFAvatarï¼ˆå§¿æ€èåˆåŒ–èº«ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥ä»æ—¥å¸¸ç©¿æ­ï¼ˆOOTDï¼‰ç…§ç‰‡é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´åŒ–èº«ã€‚OOTDç…§ç‰‡å±•ç¤ºäº†å„ç§å§¿åŠ¿ã€é®æŒ¡å’Œå¤æ‚çš„èƒŒæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰åˆ©ç”¨å°‘é‡OOTDç¤ºä¾‹å¯¹å§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰é€šè¿‡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤ºä¸‰ç»´åŒ–èº«å¹¶è¿›è¡Œæç‚¼ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä¸ä»¥å¾€å°†å›¾åƒåˆ†å‰²ä¸ºèµ„äº§ï¼ˆå¦‚æœè£…ã€é…é¥°ï¼‰è¿›è¡Œä¸‰ç»´ç»„è£…çš„æ–¹æ³•ä¸åŒï¼Œè¿™ç§æ–¹æ³•å®¹æ˜“å¼•å‘ä¸ä¸€è‡´çš„é—®é¢˜ã€‚æˆ‘ä»¬é¿å…åˆ†è§£ï¼Œç›´æ¥å¯¹å…¨èº«å¤–è§‚è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡é›†æˆé¢„è®­ç»ƒçš„ControlNetè¿›è¡Œå§¿æ€ä¼°è®¡å’Œæ–°é¢–çš„æ¡ä»¶å…ˆéªŒä¿ç•™æŸå¤±ï¼ˆCPPLï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç«¯åˆ°ç«¯çš„å­¦ä¹ ä¸­æ•æ‰ç²¾ç»†ç»†èŠ‚ï¼ŒåŒæ—¶å‡è½»å°‘é‡è®­ç»ƒä¸­çš„è¯­è¨€æ¼‚ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…ä»…5åˆ†é’Ÿå†…å®Œæˆä¸ªæ€§åŒ–è®¾ç½®ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†48å€ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºNeRFçš„åŒ–èº«è¡¨ç¤ºï¼Œé€šè¿‡æ ‡å‡†SMPL-Xç©ºé—´é‡‡æ ·å’Œå¤šåˆ†è¾¨ç‡3D-SDSè¿›è¡Œä¼˜åŒ–ã€‚ä¸åŸºäºç½‘æ ¼çš„è¡¨ç¤ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è¿ç»­è¾å°„åœºå¯ä»¥ä¿ç•™é«˜é¢‘çº¹ç†ï¼ˆä¾‹å¦‚å¤´å‘ï¼‰ï¼Œå¹¶é€šè¿‡é€å°„ç‡æ­£ç¡®åœ°å¤„ç†é®æŒ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒPFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™ä»¥åŠé®æŒ¡&#x2F;æˆªæ–­é²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæ¨åŠ¨äº†ä»ç°å®ä¸–ç•ŒOOTDç›¸å†Œç”Ÿæˆå®ç”¨ä¸‰ç»´åŒ–èº«çš„å‘å±•ã€‚æ­¤å¤–ï¼Œé‡å»ºçš„ä¸‰ç»´åŒ–èº«æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚è™šæ‹Ÿè¯•ç©¿ã€åŠ¨ç”»å’Œäººç±»è§†é¢‘é‡æ¼”ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„é€šç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12935v2">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong><br>    æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºPFAvatarï¼ˆå§¿æ€èåˆåŒ–èº«ï¼‰çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿä»æ—¥å¸¸ç©¿æ­ï¼ˆOOTDï¼‰ç…§ç‰‡ä¸­é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´åŒ–èº«ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šä¸€æ˜¯ç”¨å°‘é‡OOTDç¤ºä¾‹å¾®è°ƒå§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼ŒäºŒæ˜¯é€šè¿‡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤ºåŒ–èº«å¹¶è¿›è¡Œä¼˜åŒ–ã€‚æ­¤æ–¹æ³•é¿å…äº†åˆ†è§£å›¾åƒèµ„äº§ï¼ˆå¦‚æœè£…ã€é…é¥°ï¼‰è¿›è¡Œä¸‰ç»´è£…é…çš„ä¸ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯å­¦ä¹ ç²¾ç»†ç»†èŠ‚å¹¶å‡è½»å°‘é‡è®­ç»ƒä¸­çš„è¯­è¨€æ¼‚ç§»ã€‚ç¬¬äºŒé˜¶æ®µçš„NeRFåŒ–èº«è¡¨ç¤ºæ³•ä¼˜åŒ–ï¼Œé€šè¿‡æ ‡å‡†SMPL-Xç©ºé—´é‡‡æ ·å’Œå¤šåˆ†è¾¨ç‡3D-SDSæŠ€æœ¯ï¼Œèƒ½ä¿å­˜é«˜é¢‘çº¹ç†ï¼Œæ­£ç¡®å¤„ç†é®æŒ¡é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒPFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿å­˜ä»¥åŠå¯¹é®æŒ¡å’Œæˆªæ–­æƒ…å†µçš„ç¨³å¥æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ¨åŠ¨äº†å®é™…åº”ç”¨çš„OOTDä¸“è¾‘çš„ä¸‰ç»´åŒ–èº«ç”Ÿæˆã€‚é‡å»ºçš„åŒ–èº«è¿˜æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚è™šæ‹Ÿè¯•ç©¿ã€åŠ¨ç”»å’Œäººä½“è§†é¢‘é‡æ–°æ¼”ç»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PFAvatarèƒ½å¤Ÿä»OOTDç…§ç‰‡ä¸­é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´åŒ–èº«ã€‚</li>
<li>æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¾®è°ƒå§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹å’Œé€šè¿‡NeRFè¡¨ç¤ºä¼˜åŒ–åŒ–èº«ã€‚</li>
<li>é¿å…åˆ†è§£å›¾åƒèµ„äº§è¿›è¡Œä¸‰ç»´è£…é…çš„ä¸ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿç«¯åˆ°ç«¯å­¦ä¹ ç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>é€šè¿‡CPPLå’ŒControlNetè¾…åŠ©å§¿æ€ä¼°è®¡ï¼Œå‡å°‘è¯­è¨€æ¼‚ç§»ã€‚</li>
<li>NeRFåŒ–èº«è¡¨ç¤ºæ³•èƒ½å¤Ÿä¿å­˜é«˜é¢‘çº¹ç†ï¼Œæ­£ç¡®å¤„ç†é®æŒ¡é—®é¢˜ã€‚</li>
<li>PFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿å­˜å’Œå¯¹é®æŒ¡&#x2F;æˆªæ–­æƒ…å†µçš„ç¨³å¥æ€§æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44a47f9f5267362bc79d4a4b8cf362dd" align="middle">
<img src="https://picx.zhimg.com/v2-f69f246eef6a79116dd8a1c3c0437772" align="middle">
<img src="https://picx.zhimg.com/v2-dc4cce51cb10c3588fd127745a8e1a41" align="middle">
<img src="https://picx.zhimg.com/v2-5c8c04429049eeef36d71747b460ea83" align="middle">
<img src="https://picx.zhimg.com/v2-79f18b03f7c81325d79e755ba0fe6749" align="middle">
<img src="https://picx.zhimg.com/v2-87c8e6052af98dbaef6dec80e7c738af" align="middle">
<img src="https://picx.zhimg.com/v2-a8d8f82dc104053190c27ed1c2c01c14" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="cryoSENSE-Compressive-Sensing-Enables-High-throughput-Microscopy-with-Sparse-and-Generative-Priors-on-the-Protein-Cryo-EM-Image-Manifold"><a href="#cryoSENSE-Compressive-Sensing-Enables-High-throughput-Microscopy-with-Sparse-and-Generative-Priors-on-the-Protein-Cryo-EM-Image-Manifold" class="headerlink" title="cryoSENSE: Compressive Sensing Enables High-throughput Microscopy with Sparse and Generative Priors on the Protein Cryo-EM Image Manifold"></a>cryoSENSE: Compressive Sensing Enables High-throughput Microscopy with Sparse and Generative Priors on the Protein Cryo-EM Image Manifold</h2><p><strong>Authors:Zain Shabeeb, Daniel Saeedi, Darin Tsui, Vida Jamali, Amirali Aghazadeh</strong></p>
<p>Cryo-electron microscopy (cryo-EM) enables the atomic-resolution visualization of biomolecules; however, modern direct detectors generate data volumes that far exceed the available storage and transfer bandwidth, thereby constraining practical throughput. We introduce cryoSENSE, the computational realization of a hardware-software co-designed framework for compressive cryo-EM sensing and acquisition. We show that cryo-EM images of proteins lie on low-dimensional manifolds that can be independently represented using sparse priors in predefined bases and generative priors captured by a denoising diffusion model. cryoSENSE leverages these low-dimensional manifolds to enable faithful image reconstruction from spatial and Fourier-domain undersampled measurements while preserving downstream structural resolution. In experiments, cryoSENSE increases acquisition throughput by up to 2.5$\times$ while retaining the original 3D resolution, offering controllable trade-offs between the number of masked measurements and the level of downsampling. Sparse priors favor faithful reconstruction from Fourier-domain measurements and moderate compression, whereas generative diffusion priors achieve accurate recovery from pixel-domain measurements and more severe undersampling. Project website: <a target="_blank" rel="noopener" href="https://cryosense.github.io/">https://cryosense.github.io</a>.</p>
<blockquote>
<p>å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰èƒ½å¤Ÿå®ç°ç”Ÿç‰©åˆ†å­çš„åŸå­åˆ†è¾¨ç‡å¯è§†åŒ–ï¼›ç„¶è€Œï¼Œç°ä»£ç›´æ¥æ¢æµ‹å™¨ç”Ÿæˆçš„æ•°æ®é‡è¿œè¿œè¶…è¿‡äº†å¯ç”¨çš„å­˜å‚¨å’Œä¼ è¾“å¸¦å®½ï¼Œä»è€Œé™åˆ¶äº†å®é™…ååé‡ã€‚æˆ‘ä»¬å¼•å…¥äº†cryoSENSEï¼Œè¿™æ˜¯ç¡¬ä»¶å’Œè½¯ä»¶ååŒè®¾è®¡æ¡†æ¶çš„è®¡ç®—å®ç°ï¼Œç”¨äºå‹ç¼©å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰çš„æ„ŸçŸ¥å’Œé‡‡é›†ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè›‹ç™½è´¨å†·å†»ç”µå­æ˜¾å¾®é•œå›¾åƒä½äºä½ç»´æµå½¢ä¸Šï¼Œå¯ä»¥ä½¿ç”¨é¢„å®šä¹‰åŸºå…ƒä¸­çš„ç¨€ç–å…ˆéªŒå’Œç”±é™å™ªæ‰©æ•£æ¨¡å‹æ•è·çš„ç”Ÿæˆå…ˆéªŒè¿›è¡Œç‹¬ç«‹è¡¨ç¤ºã€‚cryoSENSEåˆ©ç”¨è¿™äº›ä½ç»´æµå½¢ï¼Œèƒ½å¤Ÿä»ç©ºé—´å’Œå‚…ç«‹å¶åŸŸæ¬ é‡‡æ ·æµ‹é‡ä¸­è¿›è¡Œå¿ å®çš„å›¾åƒé‡å»ºï¼ŒåŒæ—¶ä¿ç•™ä¸‹æ¸¸ç»“æ„åˆ†è¾¨ç‡ã€‚åœ¨å®éªŒä¸Šï¼ŒcryoSENSEåœ¨æé«˜é‡‡é›†é€Ÿåº¦çš„åŒæ—¶ä¿æŒåŸå§‹çš„ä¸‰ç»´åˆ†è¾¨ç‡ï¼Œå¯æ§åˆ¶æµ‹é‡æ©ç çš„æ•°é‡å’Œæ¬ é‡‡æ ·çš„ç¨‹åº¦ä¹‹é—´çš„æƒè¡¡ã€‚ç¨€ç–å…ˆéªŒæœ‰åˆ©äºä»å‚…ç«‹å¶åŸŸæµ‹é‡å’Œé€‚åº¦å‹ç¼©ä¸­é‡å»ºå¿ å®å›¾åƒï¼Œè€Œç”Ÿæˆæ‰©æ•£å…ˆéªŒåˆ™å¯å®ç°ä»åƒç´ åŸŸæµ‹é‡å’Œæ›´ä¸¥é‡çš„æ¬ é‡‡æ ·ä¸­å‡†ç¡®æ¢å¤ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://cryosense.github.io./">https://cryosense.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12931v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†cryoSENSEçš„è®¡ç®—å®ç°ï¼Œè¿™æ˜¯ä¸€ç§ç¡¬ä»¶å’Œè½¯ä»¶ååŒè®¾è®¡çš„æ¡†æ¶ï¼Œç”¨äºå‹ç¼©å†·å†»ç”µå­æ˜¾å¾®é•œï¼ˆcryo-EMï¼‰æ„ŸçŸ¥å’Œé‡‡é›†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè›‹ç™½è´¨å†·å†»ç”µå­æ˜¾å¾®é•œå›¾åƒä½äºä½ç»´æµå½¢ä¸Šï¼Œå¯ä»¥ä½¿ç”¨é¢„å®šä¹‰çš„ç¨€ç–å…ˆéªŒå’Œç”±å»å™ªæ‰©æ•£æ¨¡å‹æ•è·çš„ç”Ÿæˆå…ˆéªŒè¿›è¡Œç‹¬ç«‹è¡¨ç¤ºã€‚cryoSENSEåˆ©ç”¨è¿™äº›ä½ç»´æµå½¢å®ç°äº†ä»ç©ºé—´å’Œå‚…ç«‹å¶åŸŸæ¬ é‡‡æ ·æµ‹é‡å€¼çš„å¿ å®å›¾åƒé‡å»ºï¼ŒåŒæ—¶ä¿æŒä¸‹æ¸¸ç»“æ„åˆ†è¾¨ç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒcryoSENSEåœ¨æé«˜é‡‡é›†é€Ÿåº¦çš„åŒæ—¶ä¿ç•™äº†åŸå§‹3Dåˆ†è¾¨ç‡ï¼Œå¯ä»¥åœ¨æ©è†œæµ‹é‡æ•°é‡å’Œæ¬ é‡‡æ ·ç¨‹åº¦ä¹‹é—´å®ç°å¯æ§çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>cryo-EMå…è®¸åŸå­åˆ†è¾¨ç‡çš„è›‹ç™½è´¨å¯è§†åŒ–ï¼Œä½†ç°ä»£ç›´æ¥æ£€æµ‹å™¨ç”Ÿæˆçš„æ•°æ®é‡è¶…è¿‡äº†å¯ç”¨çš„å­˜å‚¨å’Œä¼ è¾“å¸¦å®½ï¼Œé™åˆ¶äº†å®é™…ååé‡ã€‚</li>
<li>cryoSENSEæ˜¯ç¡¬ä»¶å’Œè½¯ä»¶ååŒè®¾è®¡çš„æ¡†æ¶çš„è®¡ç®—å®ç°ï¼Œç”¨äºå‹ç¼©cryo-EMæ„ŸçŸ¥å’Œé‡‡é›†ã€‚</li>
<li>è›‹ç™½è´¨å†·å†»ç”µå­æ˜¾å¾®é•œå›¾åƒä½äºä½ç»´æµå½¢ä¸Šï¼Œå¯ç‹¬ç«‹ä½¿ç”¨ç¨€ç–å…ˆéªŒå’Œç”Ÿæˆå…ˆéªŒè¡¨ç¤ºã€‚</li>
<li>cryoSENSEåˆ©ç”¨è¿™äº›ä½ç»´æµå½¢å®ç°ä»ç©ºé—´å’Œå‚…ç«‹å¶åŸŸæ¬ é‡‡æ ·æµ‹é‡çš„å¿ å®å›¾åƒé‡å»ºã€‚</li>
<li>åœ¨å®éªŒä¸­ï¼ŒcryoSENSEé€šè¿‡æé«˜é‡‡é›†é€Ÿåº¦åŒæ—¶ä¿ç•™åŸå§‹3Dåˆ†è¾¨ç‡ï¼Œåœ¨æ©è†œæµ‹é‡æ•°é‡å’Œæ¬ é‡‡æ ·ç¨‹åº¦ä¹‹é—´æä¾›äº†å¯æ§çš„æƒè¡¡ã€‚</li>
<li>ç¨€ç–å…ˆéªŒæœ‰åˆ©äºä»å‚…ç«‹å¶åŸŸæµ‹é‡å€¼å’Œé€‚åº¦å‹ç¼©ä¸­å®ç°å¿ å®é‡å»ºã€‚</li>
<li>ç”Ÿæˆæ‰©æ•£å…ˆéªŒåœ¨åƒç´ åŸŸæµ‹é‡å’Œæ›´ä¸¥é‡çš„æ¬ é‡‡æ ·ä¸­å®ç°å‡†ç¡®æ¢å¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f71c01cd15cd2a6436a0c0d93b9be23" align="middle">
<img src="https://picx.zhimg.com/v2-038b5e44f1910f0ec2af4ff16417eab1" align="middle">
<img src="https://picx.zhimg.com/v2-8ef69c11ab792ed08de9fa9f8b104593" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GeoMVD-Geometry-Enhanced-Multi-View-Generation-Model-Based-on-Geometric-Information-Extraction"><a href="#GeoMVD-Geometry-Enhanced-Multi-View-Generation-Model-Based-on-Geometric-Information-Extraction" class="headerlink" title="GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction"></a>GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction</h2><p><strong>Authors:Jiaqi Wu, Yaosen Chen, Shuyuan Zhu</strong></p>
<p>Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: <a target="_blank" rel="noopener" href="https://sobeymil.github.io/GeoMVD.com">https://sobeymil.github.io/GeoMVD.com</a>.</p>
<blockquote>
<p>å¤šè§†è§’å›¾åƒç”Ÿæˆåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨3Dé‡å»ºã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºå•å¼ å›¾åƒçš„æ‰©å±•ï¼Œä½†åœ¨ä¿æŒè·¨è§†å›¾ä¸€è‡´æ€§å’Œç”Ÿæˆé«˜åˆ†è¾¨ç‡è¾“å‡ºæ–¹é¢é¢ä¸´é‡å¤§çš„è®¡ç®—æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å‡ ä½•å¼•å¯¼çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æå–å¤šè§†å›¾å‡ ä½•ä¿¡æ¯å’Œè°ƒæ•´å‡ ä½•ç‰¹å¾å¼ºåº¦çš„æœºåˆ¶ï¼Œä»¥ç”Ÿæˆæ—¢è·¨è§†å›¾ä¸€è‡´åˆç»†èŠ‚ä¸°å¯Œçš„å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šè§†å›¾å‡ ä½•ä¿¡æ¯æå–æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨æ·±åº¦å›¾ã€æ³•çº¿å›¾å’Œå‰æ™¯åˆ†å‰²æ©è†œæ¥æ„å»ºå…±äº«å‡ ä½•ç»“æ„ï¼Œç¡®ä¿ä¸åŒè§†å›¾ä¹‹é—´çš„å½¢çŠ¶å’Œç»“æ„ä¸€è‡´æ€§ã€‚ä¸ºäº†å¢å¼ºç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§å’Œç»†èŠ‚æ¢å¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è§£è€¦çš„å‡ ä½•å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åŠ å¼ºäº†å…³é”®å‡ ä½•ç»†èŠ‚çš„ç‰¹å¾å…³æ³¨ï¼Œä»è€Œæé«˜äº†æ•´ä½“å›¾åƒè´¨é‡å’Œç»†èŠ‚ä¿ç•™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç­–ç•¥ï¼Œå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ›´å¥½åœ°æ•æ‰ç”Ÿæˆè§†å›¾ä¹‹é—´çš„ç©ºé—´å…³ç³»å’Œè§†è§‰è¿è´¯æ€§ï¼Œç¡®ä¿ç»“æœçš„çœŸå®æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¿˜ç»“åˆäº†ä¸€ç§è¿­ä»£ç»†åŒ–è¿‡ç¨‹ï¼Œé€šè¿‡å¤šä¸ªé˜¶æ®µçš„å›¾åƒç”Ÿæˆé€æ­¥æ”¹è¿›è¾“å‡ºè´¨é‡ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€å‡ ä½•ä¿¡æ¯å¼ºåº¦è°ƒæ•´æœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°è°ƒèŠ‚å‡ ä½•æ•°æ®çš„å½±å“ï¼Œä¼˜åŒ–æ•´ä½“è´¨é‡ï¼ŒåŒæ—¶ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è‡ªç„¶æ€§ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è§é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sobeymil.github.io/GeoMVD.com%E3%80%82">https://sobeymil.github.io/GeoMVD.comã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12204v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå‡ ä½•å¼•å¯¼çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼ˆGeometry-guided Multi-View Diffusion Modelï¼‰ï¼Œç”¨äºç”Ÿæˆå¤šè§†è§’å›¾åƒã€‚è¯¥æ¨¡å‹é€šè¿‡æå–å’Œåˆ©ç”¨å¤šè§†è§’å‡ ä½•ä¿¡æ¯ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—é‡å¤§ã€è§†è§’ä¸€è‡´æ€§ä¿æŒå·®ã€é«˜åˆ†è¾¨ç‡è¾“å‡ºç”Ÿæˆå›°éš¾ç­‰é—®é¢˜ã€‚é€šè¿‡æ·±åº¦å›¾ã€æ³•çº¿å›¾ã€å‰æ™¯åˆ†å‰²æ©è†œç­‰æ„å»ºå…±äº«å‡ ä½•ç»“æ„ï¼Œç¡®ä¿ä¸åŒè§†è§’çš„å½¢çŠ¶å’Œç»“æ„ä¸€è‡´æ€§ã€‚é‡‡ç”¨å»è€¦çš„å‡ ä½•å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶æé«˜å›¾åƒè´¨é‡åŠç»†èŠ‚ä¿ç•™ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜å…·å¤‡è‡ªé€‚åº”å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ•æ‰ç©ºé—´å…³ç³»å’Œè§†è§‰è¿è´¯æ€§ï¼Œç”Ÿæˆæ›´çœŸå®çš„å›¾åƒã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œé€æ­¥æ”¹å–„è¾“å‡ºè´¨é‡ã€‚æ¨¡å‹è¿˜æå‡ºäº†åŠ¨æ€è°ƒæ•´å‡ ä½•ä¿¡æ¯å¼ºåº¦æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–æ•´ä½“è´¨é‡å’Œä¿è¯å›¾åƒçš„è‡ªç„¶æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºåŸºäºå‡ ä½•å¼•å¯¼çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼ˆGeometry-guided Multi-View Diffusion Modelï¼‰ï¼Œè§£å†³å¤šè§†è§’å›¾åƒç”Ÿæˆä¸­çš„è®¡ç®—é‡å¤§ã€è§†è§’ä¸€è‡´æ€§å·®ç­‰é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ·±åº¦å›¾ã€æ³•çº¿å›¾å’Œå‰æ™¯åˆ†å‰²æ©è†œæ„å»ºå…±äº«å‡ ä½•ç»“æ„ï¼Œç¡®ä¿ä¸åŒè§†è§’çš„å½¢çŠ¶å’Œç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨å»è€¦çš„å‡ ä½•å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜å›¾åƒè´¨é‡å’Œç»†èŠ‚ä¿ç•™ã€‚</li>
<li>æ¨¡å‹å…·å¤‡è‡ªé€‚åº”å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ•æ‰ç©ºé—´å…³ç³»å’Œè§†è§‰è¿è´¯æ€§ï¼Œç”Ÿæˆæ›´çœŸå®çš„å›¾åƒã€‚</li>
<li>é€šè¿‡è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹é€æ­¥æ”¹å–„è¾“å‡ºè´¨é‡ã€‚</li>
<li>æ¨¡å‹å…·å¤‡åŠ¨æ€è°ƒæ•´å‡ ä½•ä¿¡æ¯å¼ºåº¦æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–æ•´ä½“å›¾åƒè´¨é‡å¹¶ä¿éšœè‡ªç„¶æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e840d21d2bf4a0178309719d37eb60b" align="middle">
<img src="https://picx.zhimg.com/v2-b3050f6f7d03089a6ba89c9b2a882b65" align="middle">
<img src="https://picx.zhimg.com/v2-9243bc0f7d49e18cc659a64a3879cc91" align="middle">
<img src="https://picx.zhimg.com/v2-c99716f7264398c29a214bf44211f8ea" align="middle">
<img src="https://picx.zhimg.com/v2-eec0fd49ffc63130a5d1c123eb8afeaf" align="middle">
<img src="https://picx.zhimg.com/v2-e4a2da202b51edf2ffed30b22064bfd5" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MMaDA-Parallel-Multimodal-Large-Diffusion-Language-Models-for-Thinking-Aware-Editing-and-Generation"><a href="#MMaDA-Parallel-Multimodal-Large-Diffusion-Language-Models-for-Thinking-Aware-Editing-and-Generation" class="headerlink" title="MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation"></a>MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</h2><p><strong>Authors:Ye Tian, Ling Yang, Jiongfan Yang, Anran Wang, Yu Tian, Jiani Zheng, Haochen Wang, Zhiyang Teng, Zhuochen Wang, Yinjie Wang, Yunhai Tong, Mengdi Wang, Xiangtai Li</strong></p>
<p>While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/tyfeld/MMaDA-Parallel">https://github.com/tyfeld/MMaDA-Parallel</a></p>
<blockquote>
<p>åœ¨è€ƒè™‘æ„ŸçŸ¥ç”Ÿæˆçš„æ”¹è¿›æ˜¯ä¸ºäº†æé«˜å¤æ‚ä»»åŠ¡çš„æ€§èƒ½æ—¶ï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰çš„ä¸€äº›åºåˆ—å¼ã€è‡ªå›å½’çš„æ–¹æ³•ä¼šå› é”™è¯¯ä¼ æ’­è€Œæ„å¤–åœ°é™ä½æ€§èƒ½ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³é”®çš„å¤±è´¥æ¨¡å¼ã€‚ä¸ºäº†ç³»ç»Ÿåœ°åˆ†æè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ParaBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬å’Œå›¾åƒè¾“å‡ºæ¨¡å¼ã€‚æˆ‘ä»¬ä½¿ç”¨ParaBenchçš„åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ€§èƒ½ä¸‹é™ä¸ç”Ÿæˆæ¨ç†å’Œæœ€ç»ˆå›¾åƒä¹‹é—´çš„å¯¹é½ä¸ä½³æœ‰å¼ºç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09611v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://tyfeld.github.io/mmadaparellel.github.io/">https://tyfeld.github.io/mmadaparellel.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æŒ‡å‡ºäº†ç°æœ‰åºåˆ—è‡ªå›å½’æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å­˜åœ¨çš„ä¸€ä¸ªå…³é”®å¤±è´¥æ¨¡å¼ï¼Œå³è¯¯å·®ä¼ æ’­å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ParaBenchè¿™ä¸€æ–°åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°æ—¢å¯ä»¥è¯„ä¼°æ–‡æœ¬è¾“å‡ºä¹Ÿå¯ä»¥è¯„ä¼°å›¾åƒè¾“å‡ºã€‚åˆ†ææ˜¾ç¤ºæ€§èƒ½ä¸‹é™ä¸ç”Ÿæˆæ¨ç†å’Œæœ€ç»ˆå›¾åƒä¹‹é—´çš„å¯¹é½ä¸ä½³å¯†åˆ‡ç›¸å…³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œä½œè€…æå‡ºäº†MMaDA-Parallelè¿™ä¸€å¹¶è¡Œå¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶ï¼Œå®ƒèƒ½åœ¨æ•´ä¸ªå»å™ªè½¨è¿¹ä¸­å®ç°æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„æŒç»­åŒå‘äº¤äº’ã€‚æ­¤å¤–ï¼Œç»“åˆç›‘ç£å¾®è°ƒè®­ç»ƒï¼Œé‡‡ç”¨å¹³è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆParaRLï¼‰ç­–ç•¥è¿›ä¸€æ­¥è°ƒä¼˜ï¼Œé€šè¿‡åœ¨è½¨è¿¹ä¸Šåº”ç”¨è¯­ä¹‰å¥–åŠ±æ¥åŠ å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—æé«˜äº†è·¨æ¨¡æ€å¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œåœ¨ParaBenchä¸Šçš„è¾“å‡ºå¯¹é½åº¦ç›¸æ¯”ç›®å‰æœ€å…ˆè¿›çš„Bagelæ¨¡å‹æé«˜äº†6.9%ï¼Œä¸ºæ€è€ƒæ„ŸçŸ¥å›¾åƒåˆæˆå»ºç«‹äº†æ›´ç¨³å¥çš„èŒƒå¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰è‡ªå›å½’æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å­˜åœ¨è¯¯å·®ä¼ æ’­å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>ParaBenchåŸºå‡†æµ‹è¯•å¹³å°è¢«è®¾è®¡ç”¨æ¥è¯„ä¼°æ–‡æœ¬å’Œå›¾åƒè¾“å‡ºï¼Œæ­ç¤ºç”Ÿæˆæ¨ç†ä¸æœ€ç»ˆå›¾åƒä¹‹é—´çš„å¯¹é½é—®é¢˜ã€‚</li>
<li>MMaDA-Parallelæ¡†æ¶å®ç°äº†æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„æŒç»­åŒå‘äº¤äº’ï¼Œåœ¨æ•´ä¸ªå»å™ªè½¨è¿¹ä¸­ä¼˜åŒ–è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>MMaDA-Parallelç»“åˆç›‘ç£å¾®è°ƒè®­ç»ƒå¹¶é‡‡ç”¨å¹³è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆParaRLï¼‰ç­–ç•¥è¿›ä¸€æ­¥è°ƒä¼˜æ¨¡å‹ã€‚</li>
<li>ParaRLé€šè¿‡è½¨è¿¹ä¸Šçš„è¯­ä¹‰å¥–åŠ±æ¥åŠ å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMMaDA-Parallelåœ¨ParaBenchä¸Šçš„è¾“å‡ºå¯¹é½åº¦ç›¸æ¯”ç›®å‰æœ€å…ˆè¿›çš„æ¨¡å‹æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>å¼€æºä»£ç ä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªæ›´ç¨³å¥çš„æ€è€ƒæ„ŸçŸ¥å›¾åƒåˆæˆèŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a90830141447711757742224098f248d" align="middle">
<img src="https://picx.zhimg.com/v2-d60d55e15b807652b0a47e5cd811faf4" align="middle">
<img src="https://picx.zhimg.com/v2-70ffe8d2396f7497ad5b9b4ef1899d44" align="middle">
<img src="https://picx.zhimg.com/v2-23b5121eaf775eb73b369eb65d47504e" align="middle">
<img src="https://picx.zhimg.com/v2-f32b92308ede5f0449e0f4317581be18" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Learning-few-step-posterior-samplers-by-unfolding-and-distillation-of-diffusion-models"><a href="#Learning-few-step-posterior-samplers-by-unfolding-and-distillation-of-diffusion-models" class="headerlink" title="Learning few-step posterior samplers by unfolding and distillation of diffusion models"></a>Learning few-step posterior samplers by unfolding and distillation of diffusion models</h2><p><strong>Authors:Charlesquin Kemajou Mbakam, Jonathan Spence, Marcelo Pereyra</strong></p>
<p>Diffusion models (DMs) have emerged as powerful image priors in Bayesian computational imaging. Two primary strategies have been proposed for leveraging DMs in this context: Plug-and-Play methods, which are zero-shot and highly flexible but rely on approximations; and specialized conditional DMs, which achieve higher accuracy and faster inference for specific tasks through supervised training. In this work, we introduce a novel framework that integrates deep unfolding and model distillation to transform a DM image prior into a few-step conditional model for posterior sampling. A central innovation of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm - specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et al., 2025) - representing the first known instance of deep unfolding applied to a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and distilled samplers through extensive experiments and comparisons with the state of the art, where they achieve excellent accuracy and computational efficiency, while retaining the flexibility to adapt to variations in the forward model at inference time.</p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨è´å¶æ–¯è®¡ç®—æˆåƒä¸­å·²ä½œä¸ºå¼ºå¤§çš„å›¾åƒå…ˆéªŒæ¶Œç°å‡ºæ¥ã€‚åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ï¼Œå·²ç»æå‡ºäº†ä¸¤ç§ä¸»è¦ç­–ç•¥æ¥åˆ©ç”¨DMsï¼šå³æ’å³ç”¨ï¼ˆPlug-and-Playï¼‰æ–¹æ³•ï¼Œå®ƒæ˜¯é›¶å°„å‡»ï¼ˆzero-shotï¼‰å’Œé«˜åº¦çš„çµæ´»æ€§ï¼Œä½†ä¾èµ–äºè¿‘ä¼¼å€¼ï¼›ä»¥åŠé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ä¸“ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£è®­ç»ƒå®ç°æ›´é«˜çš„ç²¾åº¦å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†æ·±åº¦å±•å¼€å’Œæ¨¡å‹è’¸é¦ï¼Œå°†DMå›¾åƒå…ˆéªŒè½¬æ¢ä¸ºä¸€ä¸ªç”¨äºåé‡‡æ ·ï¼ˆposterior samplingï¼‰çš„å‡ æ­¥æ¡ä»¶æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸€ä¸ªæ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºå±•å¼€é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ï¼ˆMCMCï¼‰ç®—æ³•â€”â€”å…·ä½“æ¥è¯´ï¼Œæ˜¯æœ€è¿‘æå‡ºçš„LATINO Langeviné‡‡æ ·å™¨ï¼ˆSpagnolettiç­‰äººï¼Œ2025ï¼‰â€”â€”è¿™ä»£è¡¨äº†é¦–æ¬¡å°†æ·±åº¦å±•å¼€åº”ç”¨äºè’™ç‰¹å¡ç½—é‡‡æ ·æ–¹æ¡ˆçš„å®ä¾‹ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œä¸æœ€æ–°æŠ€æœ¯çš„æ¯”è¾ƒæ¥éªŒè¯æˆ‘ä»¬æå‡ºçš„å±•å¼€å’Œè’¸é¦é‡‡æ ·å™¨ï¼Œå®ƒä»¬åœ¨ä¿æŒæ¨ç†æ—¶æ­£å‘æ¨¡å‹å˜åŒ–çµæ´»æ€§çš„åŒæ—¶ï¼Œå®ç°äº†å‡ºè‰²çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02686v2">PDF</a> 34 pages, 18 figures, 11 tables</p>
<p><strong>æ‘˜è¦</strong><br>    æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²æˆä¸ºè´å¶æ–¯è®¡ç®—æˆåƒä¸­çš„å¼ºå¤§å›¾åƒå…ˆéªŒã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒèåˆäº†æ·±åº¦å±•å¼€å’Œæ¨¡å‹è’¸é¦æŠ€æœ¯ï¼Œå°†æ‰©æ•£æ¨¡å‹å›¾åƒå…ˆéªŒè½¬åŒ–ä¸ºç”¨äºåéªŒé‡‡æ ·çš„å‡ æ­¥æ¡ä»¶æ¨¡å‹ã€‚è¯¥æ¡†æ¶çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå±•å¼€é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ç®—æ³•ï¼ˆMCMCï¼‰ï¼Œç‰¹åˆ«æ˜¯æœ€è¿‘æå‡ºçš„LATINO Langeviné‡‡æ ·å™¨ï¼Œä»£è¡¨äº†æ·±åº¦å±•å¼€é¦–æ¬¡åº”ç”¨äºè’™ç‰¹å¡æ´›é‡‡æ ·æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼Œæ‰€æå‡ºçš„å±•å¼€å’Œè’¸é¦é‡‡æ ·å™¨åœ¨ä¿æŒå¯¹å‰å‘æ¨¡å‹æ¨ç†æ—¶å˜å¼‚çš„é€‚åº”æ€§çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†å¾ˆé«˜çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œå¹¶ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²æˆä¸ºè´å¶æ–¯è®¡ç®—æˆåƒä¸­çš„å¼ºå¤§å›¾åƒå…ˆéªŒã€‚</li>
<li>æå‡ºäº†ç»“åˆæ·±åº¦å±•å¼€å’Œæ¨¡å‹è’¸é¦çš„æ–°å‹æ¡†æ¶ï¼Œå°†DMå›¾åƒå…ˆéªŒè½¬åŒ–ä¸ºæ¡ä»¶æ¨¡å‹ã€‚</li>
<li>åˆ›æ–°åœ°å±•å¼€é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMCMCï¼‰ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯LATINO Langeviné‡‡æ ·å™¨çš„åº”ç”¨ã€‚</li>
<li>æ¡†æ¶é¦–æ¬¡å°†æ·±åº¦å±•å¼€åº”ç”¨äºè’™ç‰¹å¡æ´›é‡‡æ ·æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒå’Œä¸æœ€æ–°æŠ€æœ¯çš„æ¯”è¾ƒï¼Œå±•ç¤ºæ‰€æå‡ºçš„é‡‡æ ·å™¨çš„é«˜å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>æå‡ºçš„é‡‡æ ·å™¨åœ¨ä¿æŒæ¨ç†æ—¶é€‚åº”å‰å‘æ¨¡å‹å˜å¼‚çš„èƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ¡†æ¶å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ac904dd7ae62f6fdf4d79db003a16bd" align="middle">
<img src="https://picx.zhimg.com/v2-7ed3e98d4e88833335c4f04080222141" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="OG-VLA-Orthographic-Image-Generation-for-3D-Aware-Vision-Language-Action-Model"><a href="#OG-VLA-Orthographic-Image-Generation-for-3D-Aware-Vision-Language-Action-Model" class="headerlink" title="OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model"></a>OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model</h2><p><strong>Authors:Ishika Singh, Ankit Goyal, Stan Birchfield, Dieter Fox, Animesh Garg, Valts Blukis</strong></p>
<p>We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and one or more RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA unprojects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at <a target="_blank" rel="noopener" href="https://og-vla.github.io/">https://og-vla.github.io/</a></p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†OG-VLAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„å’Œå­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†Vision Language Actionæ¨¡å‹ï¼ˆVLAï¼‰çš„æ¨å¹¿ä¼˜åŠ¿å’Œ3Dæ„ŸçŸ¥ç­–ç•¥çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬è§£å†³äº†å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œä¸€ä¸ªæˆ–å¤šä¸ªRGBDè§‚å¯Ÿç»“æœæ˜ å°„åˆ°é™æ€æœºå™¨äººåŠ¨ä½œçš„æŒ‘æˆ˜ã€‚3Dæ„ŸçŸ¥æœºå™¨äººç­–ç•¥åœ¨ç²¾ç¡®çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ï¼Œä½†åœ¨æ¨å¹¿åˆ°æœªè§è¿‡çš„æŒ‡ä»¤ã€åœºæ™¯å’Œå¯¹è±¡æ—¶é¢ä¸´å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼ŒVLAåœ¨æŒ‡ä»¤å’Œåœºæ™¯æ–¹é¢çš„æ¨å¹¿èƒ½åŠ›å¾ˆå¼ºï¼Œä½†å¯èƒ½å¯¹ç›¸æœºå’Œæœºå™¨äººå§¿æ€å˜åŒ–æ•æ„Ÿã€‚æˆ‘ä»¬åˆ©ç”¨åµŒå…¥åœ¨è¯­è¨€ä¸è§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜3Dæ„ŸçŸ¥å…³é”®å¸§ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚OG-VLAå°†ä»ä¸åŒè§†è§’è¾“å…¥çš„è§‚æµ‹å€¼æŠ•å½±åˆ°ç‚¹äº‘ä¸­ï¼Œç„¶åä»è§„èŒƒçš„æ­£äº¤è§†è§’è¿›è¡Œæ¸²æŸ“ï¼Œç¡®ä¿è¾“å…¥è§†è§’çš„ä¸å˜æ€§å’Œè¾“å…¥ä¸è¾“å‡ºç©ºé—´ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚è¿™äº›è§„èŒƒè§†è§’é€šè¿‡è§†è§‰ä¸»å¹²ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆç¼–ç è¾“å…¥åœºæ™¯ä¸Šæœ«ç«¯æ‰§è¡Œå™¨ä¸‹ä¸€ä½ç½®å’Œæ–¹å‘çš„å›¾åƒã€‚åœ¨Arnoldå’ŒColosseumåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¯æ˜ï¼Œåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸Šæ¨å¹¿å…·æœ‰æœ€æ–°çŠ¶æ€ï¼Œç›¸å¯¹æ”¹è¿›è¶…è¿‡40%ï¼ŒåŒæ—¶åœ¨å·²çŸ¥è®¾ç½®ä¸­ä¹Ÿä¿æŒç¨³å¥æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨3åˆ°5æ¬¡æ¼”ç¤ºä¸­çš„ç°å®é€‚åº”æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³è§†é¢‘å’Œèµ„æºè¯·è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://og-vla.github.io/]">https://og-vla.github.io/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01196v2">PDF</a> 13 pages</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„å’Œå­¦ä¹ æ¡†æ¶OG-VLAï¼Œå®ƒå°†Vision Language Actionæ¨¡å‹ï¼ˆVLAï¼‰çš„æ³›åŒ–èƒ½åŠ›ä¸3Dæ„ŸçŸ¥ç­–ç•¥çš„ç¨³å¥æ€§ç›¸ç»“åˆã€‚OG-VLAè§£å†³äº†å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œä¸€ä¸ªæˆ–å¤šä¸ªRGBDè§‚æµ‹ç»“æœæ˜ å°„åˆ°é™æ€æœºå™¨äººåŠ¨ä½œçš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨Arnoldå’ŒColosseumåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒOG-VLAåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­æœ‰æœ€å…ˆè¿›çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨å·²çŸ¥åœºæ™¯ä¸­ä¿æŒç¨³å¥æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨å°‘é‡çœŸå®ä¸–ç•Œç¤ºèŒƒä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>OG-VLAæ˜¯ä¸€ä¸ªç»“åˆäº†Vision Language Actionæ¨¡å‹ï¼ˆVLAï¼‰æ³›åŒ–èƒ½åŠ›ä¸3Dæ„ŸçŸ¥ç­–ç•¥ç¨³å¥æ€§çš„æ–°å‹æ¶æ„å’Œå­¦ä¹ æ¡†æ¶ã€‚</li>
<li>OG-VLAè§£å†³äº†å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’ŒRGBDè§‚æµ‹ç»“æœæ˜ å°„åˆ°é™æ€æœºå™¨äººåŠ¨ä½œçš„æŒ‘æˆ˜ã€‚</li>
<li>OG-VLAé€šè¿‡åˆ©ç”¨è¯­è¨€å’Œè§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜äº†3Dæ„ŸçŸ¥å…³é”®å¸§ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>OG-VLAé€šè¿‡å°†è¾“å…¥è§‚å¯Ÿä»ä¸åŒè§†è§’æŠ•å½±åˆ°ç‚¹äº‘ï¼Œç„¶åä»å…¶è§„èŒƒçš„æ­£äº¤è§†è§’è¿›è¡Œæ¸²æŸ“ï¼Œç¡®ä¿äº†è¾“å…¥è§†è§’çš„ä¸å˜æ€§å’Œè¾“å…¥è¾“å‡ºç©ºé—´çš„ä¸€è‡´æ€§ã€‚</li>
<li>OG-VLAä½¿ç”¨è§†è§‰ä¸»å¹²ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå›¾åƒæ‰©æ•£æ¨¡å‹æ¥å¤„ç†è¿™äº›è§„èŒƒè§†å›¾ï¼Œç”Ÿæˆç¼–ç è¾“å…¥åœºæ™¯ä¸Šæœ€ç»ˆæ‰§è¡Œå™¨ä¸‹ä¸€æ­¥ä½ç½®å’Œæ–¹å‘çš„å›¾åƒã€‚</li>
<li>åœ¨Arnoldå’ŒColosseumåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒOG-VLAåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­æœ‰æœ€å…ˆè¿›çš„æ³›åŒ–èƒ½åŠ›ï¼Œç›¸å¯¹æ”¹è¿›è¶…è¿‡40%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-480a5f172f445b02e0fc39968bad4b2c" align="middle">
<img src="https://picx.zhimg.com/v2-86164ea73a937d56afc364da2b25f47d" align="middle">
<img src="https://picx.zhimg.com/v2-cf89853096a0f8f86af317da2ff712c9" align="middle">
<img src="https://picx.zhimg.com/v2-c887ed10dff82d078726b59b4886afad" align="middle">
<img src="https://picx.zhimg.com/v2-c2c1b6a04f615e6d437e88790d09ee04" align="middle">
<img src="https://picx.zhimg.com/v2-cc697aa9da717e28031d481485f16d0b" align="middle">
<img src="https://picx.zhimg.com/v2-84700919fe4dfef44fc30f0313949c06" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Is-Noise-Conditioning-Necessary-for-Denoising-Generative-Models"><a href="#Is-Noise-Conditioning-Necessary-for-Denoising-Generative-Models" class="headerlink" title="Is Noise Conditioning Necessary for Denoising Generative Models?"></a>Is Noise Conditioning Necessary for Denoising Generative Models?</h2><p><strong>Authors:Qiao Sun, Zhicheng Jiang, Hanhong Zhao, Kaiming He</strong></p>
<p>It is widely believed that noise conditioning is indispensable for denoising diffusion models to work successfully. This work challenges this belief. Motivated by research on blind image denoising, we investigate a variety of denoising-based generative models in the absence of noise conditioning. To our surprise, most models exhibit graceful degradation, and in some cases, they even perform better without noise conditioning. We provide a theoretical analysis of the error caused by removing noise conditioning and demonstrate that our analysis aligns with empirical observations. We further introduce a noise-unconditional model that achieves a competitive FID of 2.23 on CIFAR-10, significantly narrowing the gap to leading noise-conditional models. We hope our findings will inspire the community to revisit the foundations and formulations of denoising generative models.</p>
<blockquote>
<p>æ™®éè®¤ä¸ºå™ªå£°è°ƒèŠ‚å¯¹äºé™å™ªæ‰©æ•£æ¨¡å‹çš„æˆåŠŸè¿è¡Œæ˜¯å¿…ä¸å¯å°‘çš„ã€‚æœ¬ç ”ç©¶å¯¹è¿™ä¸€è§‚ç‚¹æå‡ºäº†è´¨ç–‘ã€‚å—ç›²å›¾åƒå»å™ªç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨æ²¡æœ‰å™ªå£°è°ƒèŠ‚çš„æƒ…å†µä¸‹ï¼ŒåŸºäºå»å™ªçš„å¤šç§ç”Ÿæˆæ¨¡å‹ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå¤§å¤šæ•°æ¨¡å‹è¡¨ç°å‡ºä¼˜é›…çš„é€€åŒ–æ€§èƒ½ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç”šè‡³åœ¨æ— éœ€å™ªå£°è°ƒèŠ‚çš„æƒ…å†µä¸‹è¡¨ç°æ›´ä½³ã€‚æˆ‘ä»¬å¯¹å› å»é™¤å™ªå£°è°ƒèŠ‚è€Œäº§ç”Ÿçš„è¯¯å·®è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„åˆ†æä¸å®é™…è§‚å¯Ÿç›¸å»åˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§æ— éœ€å™ªå£°è°ƒèŠ‚çš„æ¨¡å‹ï¼Œåœ¨CIFAR-10ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„FIDåˆ†æ•°2.23ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸é¢†å…ˆçš„å™ªå£°æ¡ä»¶æ¨¡å‹çš„å·®è·ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½æ¿€åŠ±ç¤¾åŒºé‡æ–°æ€è€ƒå»å™ªç”Ÿæˆæ¨¡å‹çš„åŸºç¡€å’Œå…¬å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13129v2">PDF</a> Update ImageNet experiments (SiT with CFG). Update Appendix</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‘æˆ˜äº†å¹¿æ³›è®¤ä¸ºçš„å™ªå£°è°ƒèŠ‚å¯¹é™å™ªæ‰©æ•£æ¨¡å‹æˆåŠŸå·¥ä½œçš„ä¸å¯æˆ–ç¼ºæ€§ã€‚å—ç›²å›¾åƒå»å™ªç ”ç©¶å¯å‘ï¼Œæœ¬æ–‡ç ”ç©¶äº†åœ¨æ— å™ªå£°è°ƒèŠ‚çš„æƒ…å†µä¸‹å„ç§åŸºäºå»å™ªçš„ç”Ÿæˆæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨æ²¡æœ‰å™ªå£°è°ƒèŠ‚çš„æƒ…å†µä¸‹æ€§èƒ½ä»ç„¶ç¨³å¥ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´ä½³ã€‚æœ¬æ–‡æä¾›äº†å»é™¤å™ªå£°è°ƒèŠ‚æ‰€å¸¦æ¥çš„è¯¯å·®çš„ç†è®ºåˆ†æï¼Œå¹¶å±•ç¤ºäº†è¯¥åˆ†æä¸å®è¯è§‚å¯Ÿçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ— éœ€å™ªå£°è°ƒèŠ‚çš„æ¨¡å‹ï¼Œåœ¨CIFAR-10ä¸Šå®ç°äº†ç«äº‰æ€§çš„FIDåˆ†æ•°ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸é¢†å…ˆçš„å™ªå£°æ¡ä»¶æ¨¡å‹çš„å·®è·ã€‚æœ¬æ–‡çš„å‘ç°æœ‰æœ›æ¿€åŠ±ç¤¾åŒºé‡æ–°æ€è€ƒé™å™ªç”Ÿæˆæ¨¡å‹çš„åŸºç¡€å’Œå…¬å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æŒ‘æˆ˜äº†å¹¿æ³›è®¤ä¸ºå™ªå£°è°ƒèŠ‚å¯¹äºé™å™ªæ‰©æ•£æ¨¡å‹æˆåŠŸçš„å¿…è¦æ€§ã€‚</li>
<li>åœ¨æ— å™ªå£°è°ƒèŠ‚çš„æƒ…å†µä¸‹ï¼Œç ”ç©¶äº†å¤šç§åŸºäºå»å™ªçš„ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>å¤§å¤šæ•°æ¨¡å‹åœ¨æ— å™ªå£°è°ƒèŠ‚æ—¶è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œç”šè‡³æœ‰äº›æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚</li>
<li>æä¾›äº†å»é™¤å™ªå£°è°ƒèŠ‚çš„ç†è®ºåˆ†æï¼Œå¹¶ä¸å®è¯è§‚å¯Ÿç›¸ä¸€è‡´ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ— éœ€å™ªå£°è°ƒèŠ‚çš„æ¨¡å‹ï¼Œåœ¨CIFAR-10ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„FIDåˆ†æ•°ã€‚</li>
<li>è¯¥æ¨¡å‹æ˜¾è‘—ç¼©å°äº†ä¸é¢†å…ˆçš„å™ªå£°æ¡ä»¶æ¨¡å‹çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd1ba0d3de14c959d5961bd41ec36a23" align="middle">
<img src="https://picx.zhimg.com/v2-9679dd95a2e1e42450575c27d8036a02" align="middle">
<img src="https://picx.zhimg.com/v2-192e534cd29bb2671ba5802842b92051" align="middle">
<img src="https://picx.zhimg.com/v2-db1b6690d3e203c1dd6e1acf1735092c" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8b01a4c3575c585038ad9e8a5ea54b9b" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  Dental3R Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-79f18b03f7c81325d79e755ba0fe6749" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  iGaussian Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
