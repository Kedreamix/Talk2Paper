<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  Mitigating Label Length Bias in Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f69f246eef6a79116dd8a1c3c0437772')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-20-æ›´æ–°"><a href="#2025-11-20-æ›´æ–°" class="headerlink" title="2025-11-20 æ›´æ–°"></a>2025-11-20 æ›´æ–°</h1><h2 id="Mitigating-Label-Length-Bias-in-Large-Language-Models"><a href="#Mitigating-Label-Length-Bias-in-Large-Language-Models" class="headerlink" title="Mitigating Label Length Bias in Large Language Models"></a>Mitigating Label Length Bias in Large Language Models</h2><p><strong>Authors:Mario Sanz-Guerrero, Katharina von der Wense</strong></p>
<p>Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ è€…ã€‚ç„¶è€Œï¼Œåœ¨é¢„æµ‹ä¸€ç»„å€™é€‰é€‰é¡¹æ—¶ï¼ŒLLMä¼šå—åˆ°æ ‡ç­¾åè§çš„å½±å“ï¼Œè€Œç°æœ‰çš„æ ¡å‡†æ–¹æ³•å¿½è§†äº†ç”±å¤šä»¤ç‰Œç±»æ ‡ç­¾äº§ç”Ÿçš„åè§ã€‚æˆ‘ä»¬è§£å†³äº†ä¸€ä¸ªæˆ‘ä»¬ç§°ä¸ºæ ‡ç­¾é•¿åº¦åè§çš„é—®é¢˜ï¼Œå³ä¸åŒé•¿åº¦çš„æ ‡ç­¾åœ¨æ ‡å‡†é•¿åº¦å½’ä¸€åŒ–åä»ç„¶å¤„ç†ä¸ä¸€è‡´ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å½’ä¸€åŒ–ä¸Šä¸‹æ–‡æ ¡å‡†ï¼ˆNCCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨å…¨æ ‡ç­¾çº§åˆ«å¯¹é¢„æµ‹è¿›è¡Œå½’ä¸€åŒ–å’Œæ ¡å‡†ã€‚NCCåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šå®ç°äº†å¯¹å…ˆå‰æ–¹æ³•çš„ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ”¹è¿›ï¼ŒF1å¾—åˆ†æé«˜äº†é«˜è¾¾10%ã€‚æ­¤å¤–ï¼ŒNCCå°†åè§ç¼“è§£æ‰©å±•åˆ°æ›´å¹¿æ³›çš„ä»»åŠ¡ï¼Œå¦‚å¤šé¡¹é€‰æ‹©é¢˜å›ç­”ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œç»“åˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒNCCå¯¹å°‘æ ·æœ¬ç¤ºä¾‹é€‰æ‹©ä¸å¤ªæ•æ„Ÿï¼Œéœ€è¦æ›´å°‘çš„ç¤ºä¾‹å³å¯å®ç°ç«äº‰æ€§èƒ½ï¼Œå¹¶äº§ç”Ÿæ›´å¯é çš„ä¿¡å¿ƒä¼°è®¡ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å‡è½»å…¨æ ‡ç­¾åè§çš„é‡è¦æ€§ï¼Œä»¥æé«˜åŸºäºLLMçš„æ–¹æ³•çš„æ€§èƒ½å’Œç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç±»æ ‡ç­¾è‡ªç„¶åŒ…å«å¤šä¸ªä»¤ç‰Œçš„å®é™…åº”ç”¨ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14385v1">PDF</a> Accepted to AACL 2025 (Main)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨é¢„æµ‹å€™é€‰é€‰é¡¹é›†æ—¶ï¼Œæ˜“å—æ ‡ç­¾åå·®å½±å“ã€‚ç°æœ‰æ ¡å‡†æ–¹æ³•å¿½ç•¥äº†ç”±å¤šä»¤ç‰Œç±»åˆ«æ ‡ç­¾äº§ç”Ÿçš„åå·®ã€‚é’ˆå¯¹æ ‡ç­¾é•¿åº¦åå·®é—®é¢˜ï¼Œå³ä½¿åœ¨è¿›è¡Œæ ‡å‡†é•¿åº¦å½’ä¸€åŒ–åï¼Œä¸åŒé•¿åº¦çš„æ ‡ç­¾å¤„ç†ä»ä¸ä¸€è‡´ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†å½’ä¸€åŒ–ä¸Šä¸‹æ–‡æ ¡å‡†ï¼ˆNCCï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å…¨é•¿æ ‡ç­¾çº§åˆ«å¯¹é¢„æµ‹è¿›è¡Œå½’ä¸€åŒ–å’Œæ ¡å‡†ã€‚NCCåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šå®ç°äº†å¯¹å…ˆå‰æ–¹æ³•çš„ç»Ÿè®¡æ˜¾è‘—æ”¹è¿›ï¼ŒF1å¾—åˆ†æé«˜é«˜è¾¾10%ã€‚æ­¤å¤–ï¼ŒNCCå°†åå·®ç¼“è§£æ‰©å±•åˆ°æ›´å¹¿æ³›çš„ä»»åŠ¡ï¼Œå¦‚å¤šé¡¹é€‰æ‹©é¢˜å›ç­”ã€‚åˆ†æè¡¨æ˜ï¼Œç»“åˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒNCCå¯¹å°‘æ ·æœ¬ç¤ºä¾‹é€‰æ‹©ä¸é‚£ä¹ˆæ•æ„Ÿï¼Œç«äº‰æ€§èƒ½æ‰€éœ€çš„ç¤ºä¾‹æ›´å°‘ï¼Œå¹¶äº§ç”Ÿæ›´å¯é çš„ä¿¡å¿ƒä¼°è®¡ã€‚è¿™äº›å‘ç°å¼ºè°ƒå‡è½»å…¨é•¿æ ‡ç­¾åå·®çš„é‡è¦æ€§ï¼Œæœ‰åŠ©äºæé«˜LLMæ–¹æ³•æ€§èƒ½å’Œç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç±»åˆ«æ ‡ç­¾è‡ªç„¶åŒ…å«å¤šä¸ªä»¤ç‰Œçš„å®é™…åº”ç”¨ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢„æµ‹å€™é€‰é€‰é¡¹æ—¶æ˜“å—æ ‡ç­¾åå·®å½±å“ã€‚</li>
<li>ç°æœ‰æ ¡å‡†æ–¹æ³•å¿½ç•¥ç”±å¤šä»¤ç‰Œç±»åˆ«æ ‡ç­¾äº§ç”Ÿçš„åå·®ã€‚</li>
<li>æå‡ºäº†å½’ä¸€åŒ–ä¸Šä¸‹æ–‡æ ¡å‡†ï¼ˆNCCï¼‰æ–¹æ³•ä»¥è§£å†³æ ‡ç­¾é•¿åº¦åå·®é—®é¢˜ã€‚</li>
<li>NCCåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šå®ç°å¯¹å…ˆå‰æ–¹æ³•çš„æ”¹è¿›ï¼ŒF1å¾—åˆ†æé«˜é«˜è¾¾10%ã€‚</li>
<li>NCCå°†åå·®ç¼“è§£æ‰©å±•åˆ°å¤šé¡¹é€‰æ‹©é¢˜å›ç­”ç­‰æ›´å¹¿æ³›çš„ä»»åŠ¡ã€‚</li>
<li>ç»“åˆä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒNCCå¯¹å°‘æ ·æœ¬ç¤ºä¾‹é€‰æ‹©æ›´åŠ ç¨³å¥ï¼Œå¹¶äº§ç”Ÿæ›´å¯é ä¿¡å¿ƒä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e060e3c9f8fc4298a972e598d7f0c713" align="middle">
<img src="https://picx.zhimg.com/v2-f58720c9f33b31a8b80cd630adebaa30" align="middle">
<img src="https://picx.zhimg.com/v2-28919b635cec2fba8645aa816e038292" align="middle">
<img src="https://picx.zhimg.com/v2-cb3d9e0608acbd092215f81a2e31c717" align="middle">
<img src="https://picx.zhimg.com/v2-2d4574b674be9c058e7b731ebf8db015" align="middle">
<img src="https://picx.zhimg.com/v2-c7b8cc40fc7beb017a09de76b98aa4f1" align="middle">
<img src="https://picx.zhimg.com/v2-2986a34f1c299a48e6c397269938e3d8" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Free-Lunch-to-Meet-the-Gap-Intermediate-Domain-Reconstruction-for-Cross-Domain-Few-Shot-Learning"><a href="#Free-Lunch-to-Meet-the-Gap-Intermediate-Domain-Reconstruction-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Free Lunch to Meet the Gap: Intermediate Domain Reconstruction for Cross-Domain Few-Shot Learning"></a>Free Lunch to Meet the Gap: Intermediate Domain Reconstruction for Cross-Domain Few-Shot Learning</h2><p><strong>Authors:Tong Zhang, Yifan Zhao, Liangyu Wang, Jia Li</strong></p>
<p>Cross-Domain Few-Shot Learning (CDFSL) endeavors to transfer generalized knowledge from the source domain to target domains using only a minimal amount of training data, which faces a triplet of learning challenges in the meantime, i.e., semantic disjoint, large domain discrepancy, and data scarcity. Different from predominant CDFSL works focused on generalized representations, we make novel attempts to construct Intermediate Domain Proxies (IDP) with source feature embeddings as the codebook and reconstruct the target domain feature with this learned codebook. We then conduct an empirical study to explore the intrinsic attributes from perspectives of visual styles and semantic contents in intermediate domain proxies. Reaping benefits from these attributes of intermediate domains, we develop a fast domain alignment method to use these proxies as learning guidance for target domain feature transformation. With the collaborative learning of intermediate domain reconstruction and target feature transformation, our proposed model is able to surpass the state-of-the-art models by a margin on 8 cross-domain few-shot learning benchmarks.</p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰è‡´åŠ›äºä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®ï¼Œä»æºåŸŸè½¬ç§»é€šç”¨çŸ¥è¯†åˆ°ç›®æ ‡åŸŸï¼Œä¸æ­¤åŒæ—¶ï¼Œå®ƒé¢ä¸´ç€ä¸‰ä¸ªå­¦ä¹ æŒ‘æˆ˜ï¼Œå³è¯­ä¹‰ä¸ç›¸å…³ã€åŸŸå·®å¼‚å¤§å’Œæ•°æ®ç¨€ç¼ºã€‚ä¸ä¸»æµçš„CDFSLå·¥ä½œé›†ä¸­åœ¨é€šç”¨è¡¨ç¤ºä¸Šä¸åŒï¼Œæˆ‘ä»¬æ–°é¢–åœ°å°è¯•æ„å»ºä»¥æºç‰¹å¾åµŒå…¥ä½œä¸ºä»£ç æœ¬çš„ä¸­ä»‹åŸŸä»£ç†ï¼ˆIDPï¼‰ï¼Œå¹¶ç”¨è¿™ä¸ªå­¦ä¹ åˆ°çš„ä»£ç æœ¬é‡å»ºç›®æ ‡åŸŸç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»ä¸­ä»‹åŸŸä»£ç†çš„è§†è§‰é£æ ¼å’Œè¯­ä¹‰å†…å®¹ç­‰è§’åº¦è¿›è¡Œå®è¯ç ”ç©¶ï¼Œæ¢ç´¢å…¶å†…åœ¨å±æ€§ã€‚å—ç›Šäºä¸­ä»‹åŸŸçš„è¿™äº›å±æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¿«é€ŸåŸŸå¯¹é½æ–¹æ³•ï¼Œåˆ©ç”¨è¿™äº›ä»£ç†ä½œä¸ºç›®æ ‡åŸŸç‰¹å¾è½¬æ¢çš„å­¦ä¹ æŒ‡å—ã€‚é€šè¿‡ä¸­ä»‹åŸŸé‡å»ºå’Œç›®æ ‡ç‰¹å¾è½¬æ¢çš„ååŒå­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹èƒ½å¤Ÿåœ¨8ä¸ªè·¨åŸŸå°æ ·æœ¬å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šæœ€æ–°æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14279v1">PDF</a> Accepted to IJCV 2025</p>
<p><strong>Summary</strong><br>è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰æ—¨åœ¨åˆ©ç”¨å°‘é‡è®­ç»ƒæ•°æ®ï¼Œä»æºåŸŸè½¬ç§»é€šç”¨çŸ¥è¯†åˆ°ç›®æ ‡åŸŸã€‚é¢å¯¹è¯­ä¹‰åˆ†ç¦»ã€é¢†åŸŸå·®å¼‚å¤§å’Œç¼ºå°‘æ•°æ®ç­‰å­¦ä¹ æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°æ„å»ºäº†ä¸­é—´åŸŸä»£ç†ï¼ˆIDPï¼‰ï¼Œä½¿ç”¨æºç‰¹å¾åµŒå…¥ä½œä¸ºä»£ç æœ¬ï¼Œé‡å»ºç›®æ ‡åŸŸç‰¹å¾ã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸­é—´åŸŸä»£ç†çš„å†…åœ¨å±æ€§ï¼Œå¦‚è§†è§‰é£æ ¼å’Œè¯­ä¹‰å†…å®¹ã€‚åˆ©ç”¨è¿™äº›ä¸­é—´åŸŸçš„å±æ€§ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¿«é€ŸåŸŸå¯¹é½æ–¹æ³•ï¼Œå°†è¿™äº›ä»£ç†ä½œä¸ºç›®æ ‡åŸŸç‰¹å¾è½¬æ¢çš„å­¦ä¹ æŒ‡å—ã€‚é€šè¿‡ä¸­é—´åŸŸé‡å»ºå’Œç›®æ ‡ç‰¹å¾è½¬æ¢çš„ååŒå­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨8ä¸ªè·¨åŸŸå°æ ·æœ¬å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰ä¸“æ³¨äºä»æºåŸŸè½¬ç§»çŸ¥è¯†åˆ°ç›®æ ‡åŸŸï¼Œä½¿ç”¨æå°‘çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>é¢ä¸´è¯­ä¹‰åˆ†ç¦»ã€é¢†åŸŸå·®å¼‚å¤§å’Œç¼ºå°‘æ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆ›æ–°æ„å»ºä¸­é—´åŸŸä»£ç†ï¼ˆIDPï¼‰ï¼Œåˆ©ç”¨æºç‰¹å¾åµŒå…¥ä½œä¸ºä»£ç æœ¬ã€‚</li>
<li>é€šè¿‡å®è¯ç ”ç©¶ï¼Œæ¢ç´¢äº†ä¸­é—´åŸŸä»£ç†çš„å†…åœ¨å±æ€§ï¼Œå¦‚è§†è§‰é£æ ¼å’Œè¯­ä¹‰å†…å®¹ã€‚</li>
<li>åˆ©ç”¨ä¸­é—´åŸŸçš„å±æ€§ä¼˜åŠ¿ï¼Œå¼€å‘äº†ä¸€ç§å¿«é€ŸåŸŸå¯¹é½æ–¹æ³•ã€‚</li>
<li>å°†ä¸­é—´åŸŸä»£ç†ä½œä¸ºç›®æ ‡åŸŸç‰¹å¾è½¬æ¢çš„å­¦ä¹ æŒ‡å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e60afbd255dbf1fe6558e02ef89e091" align="middle">
<img src="https://picx.zhimg.com/v2-710280f78bad43de86ae5dfc02328971" align="middle">
<img src="https://picx.zhimg.com/v2-930b1df309501b26d4885b69c9d96c3d" align="middle">
<img src="https://picx.zhimg.com/v2-3fc0f50afd6e88c7bc4e293c4f9f0a01" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Let-Language-Constrain-Geometry-Vision-Language-Models-as-Semantic-and-Spatial-Critics-for-3D-Generation"><a href="#Let-Language-Constrain-Geometry-Vision-Language-Models-as-Semantic-and-Spatial-Critics-for-3D-Generation" class="headerlink" title="Let Language Constrain Geometry: Vision-Language Models as Semantic and Spatial Critics for 3D Generation"></a>Let Language Constrain Geometry: Vision-Language Models as Semantic and Spatial Critics for 3D Generation</h2><p><strong>Authors:Weimin Bai, Yubo Li, Weijian Luo, Zeqiang Lai, Yequan Wang, Wenzheng Chen, He Sun</strong></p>
<p>Text-to-3D generation has advanced rapidly, yet state-of-the-art models, encompassing both optimization-based and feed-forward architectures, still face two fundamental limitations. First, they struggle with coarse semantic alignment, often failing to capture fine-grained prompt details. Second, they lack robust 3D spatial understanding, leading to geometric inconsistencies and catastrophic failures in part assembly and spatial relationships. To address these challenges, we propose VLM3D, a general framework that repurposes large vision-language models (VLMs) as powerful, differentiable semantic and spatial critics. Our core contribution is a dual-query critic signal derived from the VLMâ€™s Yes or No log-odds, which assesses both semantic fidelity and geometric coherence. We demonstrate the generality of this guidance signal across two distinct paradigms: (1) As a reward objective for optimization-based pipelines, VLM3D significantly outperforms existing methods on standard benchmarks. (2) As a test-time guidance module for feed-forward pipelines, it actively steers the iterative sampling process of SOTA native 3D models to correct severe spatial errors. VLM3D establishes a principled and generalizable path to inject the VLMâ€™s rich, language-grounded understanding of both semantics and space into diverse 3D generative pipelines.</p>
<blockquote>
<p>æ–‡æœ¬åˆ°3Dç”Ÿæˆçš„è½¬æ¢æŠ€æœ¯å·²ç»è¿…é€Ÿå‘å±•ï¼Œç„¶è€Œï¼ŒåŒ…æ‹¬åŸºäºä¼˜åŒ–å’Œå‰é¦ˆæ¶æ„çš„æœ€å…ˆè¿›æ¨¡å‹ä»ç„¶é¢ä¸´ä¸¤ä¸ªåŸºæœ¬å±€é™ã€‚é¦–å…ˆï¼Œå®ƒä»¬åœ¨è¿›è¡Œç²—ç•¥è¯­ä¹‰å¯¹é½æ—¶é‡åˆ°å›°éš¾ï¼Œå¾€å¾€æ— æ³•æ•æ‰ç²¾ç»†çš„æç¤ºç»†èŠ‚ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬ç¼ºä¹å¼ºå¤§çš„3Dç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´å‡ ä½•ä¸ä¸€è‡´ä»¥åŠåœ¨éƒ¨ä»¶ç»„è£…å’Œç©ºé—´å…³ç³»æ–¹é¢çš„ç¾éš¾æ€§å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†VLM3Dï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå®ƒé‡æ–°åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä½œä¸ºå¼ºå¤§ä¸”å¯åŒºåˆ†çš„è¯­ä¹‰å’Œç©ºé—´æ‰¹åˆ¤è€…ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒè´¡çŒ®æ˜¯æºäºVLMçš„â€œæ˜¯â€æˆ–â€œå¦â€å¯¹æ•°å‡ ç‡çš„åŒæŸ¥è¯¢æ‰¹åˆ¤ä¿¡å·ï¼Œå®ƒè¯„ä¼°è¯­ä¹‰ä¿çœŸåº¦å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†è¿™ç§æŒ‡å¯¼ä¿¡å·åœ¨ä¸¤ç§ä¸åŒèŒƒå¼ä¸‹çš„æ™®éæ€§ï¼šï¼ˆ1ï¼‰ä½œä¸ºåŸºäºä¼˜åŒ–çš„ç®¡é“å¥–åŠ±ç›®æ ‡æ—¶ï¼ŒVLM3Dåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼›ï¼ˆ2ï¼‰ä½œä¸ºå‰é¦ˆç®¡é“çš„æµ‹è¯•æ—¶é—´æŒ‡å¯¼æ¨¡å—æ—¶ï¼Œå®ƒä¸»åŠ¨å¼•å¯¼æœ€æ–°æœ¬åœ°3Dæ¨¡å‹çš„è¿­ä»£é‡‡æ ·è¿‡ç¨‹ä»¥çº æ­£ä¸¥é‡çš„ç©ºé—´é”™è¯¯ã€‚VLM3Då»ºç«‹äº†ä¸€æ¡æœ‰åŸåˆ™ä¸”å¯æ¨å¹¿çš„é€”å¾„ï¼Œå°†VLMå¯¹è¯­ä¹‰å’Œç©ºé—´çš„ä¸°å¯Œã€åŸºäºè¯­è¨€çš„ç†è§£æ³¨å…¥åˆ°å¤šæ ·åŒ–çš„3Dç”Ÿæˆç®¡é“ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14271v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æåˆ°å½“å‰æ–‡æœ¬åˆ°ä¸‰ç»´ç”ŸæˆæŠ€æœ¯çš„ä¸¤å¤§å±€é™æ€§ï¼šè¯­ä¹‰å¯¹é½ç²—ç³™å’Œç©ºé—´ç†è§£ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†VLM3Dæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„å¯åŒºåˆ†è¯­ä¹‰å’Œç©ºé—´è¯„è®ºå®¶ã€‚å…¶æ ¸å¿ƒè´¡çŒ®æ˜¯é€šè¿‡VLMçš„Yesæˆ–No log-oddsæ´¾ç”Ÿå‡ºåŒæŸ¥è¯¢è¯„è®ºå®¶ä¿¡å·ï¼Œè¯„ä¼°è¯­ä¹‰ä¿çœŸåº¦å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚è¯¥æŒ‡å¯¼ä¿¡å·åœ¨ä¼˜åŒ–ç®¡é“å’ŒåŸºäºå‰é¦ˆç®¡é“çš„ä¸¤ä¸ªä¸åŒèŒƒå¼ä¸­å‡è¡¨ç°å‡ºä¸€èˆ¬æ€§ã€‚åœ¨ä¼˜åŒ–ç®¡é“ä¸­ä½œä¸ºå¥–åŠ±ç›®æ ‡ï¼ŒVLM3Dåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨åŸºäºå‰é¦ˆç®¡é“ä¸­ä½œä¸ºæµ‹è¯•æ—¶é—´æŒ‡å¯¼æ¨¡å—ï¼Œå®ƒä¸»åŠ¨å¼•å¯¼å½“å‰ä¸‰ç»´æ¨¡å‹çš„è¿­ä»£é‡‡æ ·è¿‡ç¨‹ï¼Œçº æ­£ä¸¥é‡çš„ç©ºé—´é”™è¯¯ã€‚VLM3Dä¸ºå°†ä¸°å¯Œçš„è¯­è¨€åŸºç¡€è¯­ä¹‰å’Œç©ºé—´ç†è§£æ³¨å…¥å¤šç§ä¸‰ç»´ç”Ÿæˆç®¡é“æä¾›äº†åŸåˆ™æ€§å’Œé€šç”¨æ€§çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬åˆ°ä¸‰ç»´ç”ŸæˆæŠ€æœ¯é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šè¯­ä¹‰å¯¹é½ç²—ç³™å’Œç©ºé—´ç†è§£ä¸è¶³ã€‚</li>
<li>VLM3Dæ¡†æ¶åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„å¯åŒºåˆ†è¯­ä¹‰å’Œç©ºé—´è¯„è®ºå®¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>VLM3Dé€šè¿‡VLMçš„Yesæˆ–No log-oddsæ´¾ç”Ÿå‡ºåŒæŸ¥è¯¢è¯„è®ºå®¶ä¿¡å·ï¼Œè¯„ä¼°è¯­ä¹‰å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>VLM3Dåœ¨ä¼˜åŒ–ç®¡é“å’ŒåŸºäºå‰é¦ˆç®¡é“çš„ä¸¤ä¸ªä¸åŒèŒƒå¼ä¸­éƒ½è¡¨ç°å‡ºä¸€èˆ¬æ€§ã€‚</li>
<li>åœ¨ä¼˜åŒ–ç®¡é“ä¸­ï¼ŒVLM3Dä½œä¸ºå¥–åŠ±ç›®æ ‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨åŸºäºå‰é¦ˆç®¡é“ä¸­ï¼ŒVLM3Dèƒ½ä¸»åŠ¨å¼•å¯¼å½“å‰ä¸‰ç»´æ¨¡å‹çš„è¿­ä»£é‡‡æ ·è¿‡ç¨‹ï¼Œçº æ­£ç©ºé—´é”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15b7ebc137a2cbb2d468848a06b6483d" align="middle">
<img src="https://picx.zhimg.com/v2-e9704939f9833b1aee1eccb252c2ef70" align="middle">
<img src="https://picx.zhimg.com/v2-8c8a11aacf5b5be69d2ba0ef3859f434" align="middle">
<img src="https://picx.zhimg.com/v2-405be6a7e995488d32350ce82664fa98" align="middle">
<img src="https://picx.zhimg.com/v2-10818125fd015e78fc2e5cd6191612e1" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Authentic-Movie-Dubbing-with-Retrieve-Augmented-Director-Actor-Interaction-Learning"><a href="#Towards-Authentic-Movie-Dubbing-with-Retrieve-Augmented-Director-Actor-Interaction-Learning" class="headerlink" title="Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning"></a>Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning</h2><p><strong>Authors:Rui Liu, Yuan Zhao, Zhenqi Jia</strong></p>
<p>The automatic movie dubbing model generates vivid speech from given scripts, replicating a speakerâ€™s timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actorâ€™s final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at <a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/Authentic-Dubber">https://github.com/AI-S2-Lab/Authentic-Dubber</a>.</p>
<blockquote>
<p>è‡ªåŠ¨ç”µå½±é…éŸ³æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç»™å®šçš„è„šæœ¬ç”Ÿæˆç”ŸåŠ¨é€¼çœŸçš„è¯­éŸ³ï¼Œä»ç®€çŸ­çš„éŸ³è‰²æç¤ºä¸­å¤åˆ¶æ¼”è®²è€…çš„éŸ³è‰²ï¼ŒåŒæ—¶ç¡®ä¿ä¸æ— å£°è§†é¢‘çš„å”‡åŒæ­¥ã€‚ç°æœ‰æ–¹æ³•æ¨¡æ‹Ÿäº†ä¸€ä¸ªç®€åŒ–çš„å·¥ä½œæµç¨‹ï¼Œå³æ¼”å‘˜ç›´æ¥è¿›è¡Œé…éŸ³è€Œæ— éœ€å‡†å¤‡ï¼Œä»è€Œå¿½è§†äº†å…³é”®çš„å¯¼æ¼”ä¸æ¼”å‘˜ä¹‹é—´çš„äº’åŠ¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒçœŸå®çš„å·¥ä½œæµç¨‹æ¶‰åŠåŠ¨æ€çš„åä½œï¼šå¯¼æ¼”ç§¯æä¸æ¼”å‘˜åˆä½œï¼ŒæŒ‡å¯¼ä»–ä»¬ç†è§£ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨è¡¨æ¼”å‰çš„æƒ…ç»ªã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ£€ç´¢çš„å¯¼æ¼”-æ¼”å‘˜äº¤äº’å­¦ä¹ æ–¹æ¡ˆï¼Œä»¥å®ç°çœŸå®çš„ç”µå½±é…éŸ³ï¼Œç§°ä¸ºâ€œçœŸå®é…éŸ³å™¨â€ï¼Œå…¶ä¸­åŒ…å«ä¸‰ç§æ–°é¢–æœºåˆ¶ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å‚è€ƒç´ æåº“ï¼Œä»¥æ¨¡æ‹Ÿå¯¼æ¼”æä¾›çš„å­¦ä¹ ç´ æã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æ•´åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥å®ç°è·¨å¤šæ¨¡æ€ä¿¡å·çš„æ·±å±‚æƒ…æ„Ÿè¡¨ç¤ºç†è§£ã€‚ï¼ˆ2ï¼‰ä¸ºäº†æ¨¡æ‹Ÿæ¼”å‘˜åœ¨é…éŸ³è¿‡ç¨‹ä¸­å¦‚ä½•æœ‰æ•ˆä¸”å…¨é¢åœ°å†…åŒ–å¯¼æ¼”æä¾›çš„ç´ æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæƒ…æ„Ÿç›¸ä¼¼æ€§çš„æ£€ç´¢å¢å¼ºç­–ç•¥ã€‚è¯¥ç­–ç•¥æ£€ç´¢ä¸ç›®æ ‡æ— å£°è§†é¢‘æœ€ç›¸å…³çš„å¤šæ¨¡æ€ä¿¡æ¯ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæ¸è¿›å›¾è°±çš„è¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼šé€æ­¥èå…¥æ£€ç´¢åˆ°çš„å¤šæ¨¡æ€æƒ…æ„ŸçŸ¥è¯†ï¼Œä»è€Œæ¨¡æ‹Ÿæ¼”å‘˜çš„æœ€ç»ˆé…éŸ³è¿‡ç¨‹ã€‚ä¸Šè¿°æœºåˆ¶ä½¿çœŸå®é…éŸ³å™¨èƒ½å¤Ÿå¿ å®åœ°å¤åˆ¶çœŸå®çš„é…éŸ³å·¥ä½œæµç¨‹ï¼Œåœ¨æƒ…æ„Ÿè¡¨è¾¾æ–¹é¢å–å¾—äº†å…¨é¢çš„æ”¹è¿›ã€‚åœ¨V2C AnimationåŸºå‡†æ•°æ®é›†ä¸Šçš„ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°éƒ½éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/Authentic-Dubber%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/Authentic-Dubberä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14249v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAuthentic-Dubberçš„è‡ªåŠ¨ç”µå½±é…éŸ³æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ„å»ºå¤šæ¨¡æ€å‚è€ƒç´ æåº“ã€é‡‡ç”¨åŸºäºæƒ…æ„Ÿç›¸ä¼¼åº¦çš„æ£€ç´¢å¢å¼ºç­–ç•¥ï¼Œä»¥åŠæ¸è¿›å›¾åŸºè¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œæ¨¡æ‹ŸçœŸå®ç”µå½±é…éŸ³å·¥ä½œæµç¨‹ï¼Œæé«˜æƒ…æ„Ÿè¡¨è¾¾çš„çœŸå®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Authentic-Dubberæ¨¡å‹ç”Ÿæˆç”ŸåŠ¨è¯­éŸ³ï¼Œä»ç»™å®šå‰§æœ¬å¤åˆ¶æ¼”è®²è€…éŸ³è‰²ï¼Œå¹¶ç¡®ä¿ä¸æ— å£°è§†é¢‘åŒæ­¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†å¯¼æ¼”ä¸æ¼”å‘˜ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œè€ŒçœŸå®çš„é…éŸ³å·¥ä½œæµç¨‹æ˜¯åŠ¨æ€çš„ï¼Œå¯¼æ¼”ä¸æ¼”å‘˜ä¹‹é—´æœ‰æ·±å…¥çš„åä½œã€‚</li>
<li>Authentic-Dubberæå‡ºäº†ä¸€ä¸ªRetrieve-Augmented Director-Actor Interaction Learningæ–¹æ¡ˆæ¥æ¨¡æ‹ŸçœŸå®çš„ç”µå½±é…éŸ³ã€‚</li>
<li>è¯¥æ¨¡å‹åŒ…å«ä¸‰ä¸ªæ–°æœºåˆ¶ï¼šæ„å»ºå¤šæ¨¡æ€å‚è€ƒç´ æåº“ã€é‡‡ç”¨åŸºäºæƒ…æ„Ÿç›¸ä¼¼åº¦çš„æ£€ç´¢å¢å¼ºç­–ç•¥ï¼Œä»¥åŠæ¸è¿›å›¾åŸºè¯­éŸ³ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>å¤šæ¨¡æ€å‚è€ƒç´ æåº“é€šè¿‡æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹å®ç°è·¨æ¨¡æ€æƒ…æ„Ÿè¡¨ç¤ºçš„æ·±å…¥ç†è§£ã€‚</li>
<li>åŸºäºæƒ…æ„Ÿç›¸ä¼¼åº¦çš„æ£€ç´¢å¢å¼ºç­–ç•¥å¯æœ‰æ•ˆåœ°æ¨¡ä»¿æ¼”å‘˜å¦‚ä½•å…¨é¢å†…åŒ–å¯¼æ¼”æä¾›çš„ç´ æè¿›è¡Œé…éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9276ef2e38bdf17c2987f21986d220d3" align="middle">
<img src="https://picx.zhimg.com/v2-7b73273f49e9b9eeebcddabf882fd574" align="middle">
<img src="https://picx.zhimg.com/v2-a2a83f5b971a4370786c560bfbf3fd84" align="middle">
<img src="https://picx.zhimg.com/v2-9565bb7cec9e83c800d1ded69e344e5e" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EBind-a-practical-approach-to-space-binding"><a href="#EBind-a-practical-approach-to-space-binding" class="headerlink" title="EBind: a practical approach to space binding"></a>EBind: a practical approach to space binding</h2><p><strong>Authors:Jim Broadbent, Felix Cohen, Frederik HvilshÃ¸j, Eric Landau, Eren Sasoglu</strong></p>
<p>We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.</p>
<blockquote>
<p>æˆ‘ä»¬é€šè¿‡å¯¹ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶çš„ä¸“æ³¨æ¥ç®€åŒ–ç©ºé—´ç»‘å®šï¼šæ¯ä¸ªæ¨¡æ€çš„å•ç¼–ç å™¨å’Œé«˜è´¨é‡æ•°æ®ã€‚è¿™ä½¿å¾—åœ¨å•ä¸ªGPUä¸Šè®­ç»ƒæœ€å…ˆè¿›çš„æ¨¡å‹çš„æ—¶é—´ä»å¤šå¤©ç¼©çŸ­åˆ°å‡ ä¸ªå°æ—¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†EBindï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ã€ä»¥æ•°æ®ä¸ºä¸­å¿ƒã€å‚æ•°æ•ˆç‡é«˜çš„æ–¹æ³•ï¼Œç”¨äºç»‘å®šå¤šä¸ªå¯¹æ¯”æ¨¡å‹çš„åµŒå…¥ç©ºé—´ã€‚æˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªç®€å•çš„1.8Bå‚æ•°çš„å›¾åƒ-æ–‡æœ¬-è§†é¢‘-éŸ³é¢‘-3Dæ¨¡å‹å¯ä»¥è¶…è¶Šè§„æ¨¡å¤§4åˆ°17å€çš„æ¨¡å‹ã€‚å®ç°è¿™ä¸€ç‚¹çš„å…³é”®æ˜¯ç²¾å¿ƒç¼–åˆ¶çš„ä¸‰ç±»äº’è¡¥æ•°æ®æºï¼šä¸€ã€ä½¿ç”¨æœ€å…ˆè¿›æ£€ç´¢æ¨¡å‹çš„è‡ªåŠ¨æœç´¢äº§ç”Ÿçš„çº¦è¾¾è¿‘ä¸ƒç™¾å…­åƒä¸‡å…¨è‡ªåŠ¨åŒ–å¤šåª’ä½“ä¿¡æ¯å›¾åƒï¼ˆå›¾ç‰‡ä¿¡æ¯å¤šä¸ºä¸‰å…ƒç»„å½¢å¼çš„å•ä¸€æƒ…æ™¯çŠ¶æ€æ¶µç›–å„ä¸ªé€šé“ä¿¡æ¯ç­‰ï¼‰, äºŒç™¾ä¸‡ä¸°å¯Œæ ·æœ¬ä»ä¸åŒæ•°æ®æºæ”¶é›†å¹¶æ ‡æ³¨ä¸ºè´Ÿé¢ã€éƒ¨åˆ†åŒ¹é…æˆ–æ­£é¢åŒ¹é…çš„ä¸‰å…ƒç»„ï¼Œä¸‰ã€è¿‘ä¸‰ç™¾å››åä¸‡å·²æœ‰çš„å¸¦å­—å¹•çš„æ•°æ®æ¡ç›®ã€‚æˆ‘ä»¬ä½¿ç”¨å¤šç§ä¸åŒçš„è¯„ä¼°æ–¹æ³•æ¥è¯´æ˜æ¯ä¸ªæ•°æ®æºçš„ä»·å€¼ã€‚ç”±äºç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¨å‡ºé¦–ä¸ªé«˜è´¨é‡çš„å…±è¯†æ ‡æ³¨çš„é›¶å°„å‡»åˆ†ç±»åŸºå‡†æµ‹è¯•æ¥æ¯”å¯¹éŸ³é¢‘ä¸ç”µè„‘äº¤äº’çš„è¿‡ç¨‹ä»¥åŠå³æ—¶è¾“å…¥å›ç­”ç›¸å…³æ–‡æœ¬ç»“æœçš„ç²¾åº¦æƒ…å†µä½œä¸ºç»ˆç«¯è‡ªåŠ¨åŒ–ç»ˆç«¯ç”¨æˆ·ç•Œé¢å®¡æ ¸å‚ç…§æ¯”å¯¹ä»¥è¿›è¡Œæµ‹è¯•çš„ç»“æœæä¾›å¯¹æ¯”åˆ†ææŠ¥å‘Šè€Œéç®€è€Œæ“ä½œé€šç”¨çš„ç”¨æˆ·ä½“éªŒçš„è¯„ä»·æ–¹å¼çš„èŒƒå›´ç¯å¢ƒé€‚ç”¨äºæ­å»ºå®¶åº­æœåŠ¡å’Œå®‰é˜²ç›‘å¬ä¹‹ä¸­å‘æ™®åŠçš„éœ€æ±‚çš„äº§ç‰©ç­‰éæš‚æ—¶è·¨å¢ƒç‰¹å®šçš„å¹¿å‘Šå®£ä¼ å·¥å…·çš„èŒƒæœ¬è®¾è®¡çš„ç›¸å…³æ“ä½œè¡Œä¸ºé¢†åŸŸå·¥ä½œå†…å®¹çš„è‡ªåŠ¨åŒ–æœåŠ¡å¤„ç†æ•°æ®ä»¥åˆ†ææ¶ˆè´¹è€…è¡Œä¸ºå’Œæ€åº¦ç ”ç©¶ç­‰çš„é€šç”¨éœ€æ±‚ï¼›ä¸ä¹‹ç›¸å…³çš„ç ”ç©¶è®ºæ–‡å¯¹æ¯”å…¶ä»–åŒç±»ç›¸å…³ç ”ç©¶ä¸åŒä¹‹å¤„æ˜¯æœªæ¥å°†å¼€æ”¾æºä»£ç ç¨‹åºï¼ˆä»¥ç®€åŒ–å’Œå¯æ¥å…¥åŒ–çš„ç¨‹åº¦è¶Šå®Œå–„ä¹Ÿç­‰åŒäºæ¶µç›–çš„å†…å®¹å’Œé€”å¾„çš„å¹¿æ³›æ€§æ›´é«˜ä¸ºä¼˜é€‰ï¼‰ï¼Œæ¨¡å‹æƒé‡ä»¥åŠæ•°æ®é›†å¼€æºå…¬å¼€ç»™å…¬ä¼—å…±åŒç ”ç©¶è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14229v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†EBindæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ã€ä»¥æ•°æ®ä¸ºä¸­å¿ƒå’Œå‚æ•°é«˜æ•ˆçš„ç»‘å®šå¤šä¸ªå¯¹æ¯”æ¨¡å‹åµŒå…¥ç©ºé—´çš„æ–¹æ³•ã€‚é€šè¿‡ä¸“æ³¨äºä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ¯æ¨¡æ€çš„å•ç¼–ç å™¨å’Œé«˜è´¨é‡æ•°æ®ï¼Œä½¿å¾—åœ¨å•ä¸ªGPUä¸Šè®­ç»ƒæœ€å…ˆè¿›çš„æ¨¡å‹çš„æ—¶é—´ä»æ•°å¤©ç¼©çŸ­åˆ°æ•°å°æ—¶ã€‚å®éªŒè¡¨æ˜ï¼Œä¸€ä¸ªç®€å•çš„1.8Bå‚æ•°çš„å›¾åƒ-æ–‡æœ¬-è§†é¢‘-éŸ³é¢‘-3Dæ¨¡å‹å¯ä»¥ä¼˜äº4åˆ°17å€çš„æ¨¡å‹ã€‚å…³é”®çš„æˆåŠŸå› ç´ åœ¨äºä½¿ç”¨äº†ä¸‰ç§äº’è¡¥æ•°æ®æºï¼šå…¨è‡ªåŠ¨çš„å¤šæ¨¡æ€äº”å…ƒç»„ã€äººç±»æ ‡æ³¨çš„è´Ÿé¢ã€éƒ¨åˆ†æˆ–æ­£é¢åŒ¹é…çš„ä¸‰å…ƒç»„å’Œé¢„å­˜çš„å¸¦è¯´æ˜çš„æ•°æ®é¡¹ã€‚æ¯ä¸ªæ•°æ®æºçš„ä»·å€¼éƒ½é€šè¿‡å¤šä¸ªè¯„ä¼°æ¥è¯æ˜ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ¨å‡ºäº†é¦–ä¸ªé«˜è´¨é‡ã€å…±è¯†æ ‡æ³¨çš„é›¶æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ï¼Œç”¨äºéŸ³é¢‘å’ŒPCä¹‹é—´çš„å¯¹æ¯”ã€‚æœ¬æ–‡å…¬å¼€äº†ä»£ç ã€æ¨¡å‹æƒé‡å’Œæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EBindæ–¹æ³•èšç„¦äºæ¯æ¨¡æ€çš„å•ç¼–ç å™¨å’Œé«˜è´¨é‡æ•°æ®ï¼Œç®€åŒ–äº†å¤šæ¨¡æ€ç©ºé—´ç»‘å®šçš„å¤æ‚æ€§ã€‚</li>
<li>é€šè¿‡EBindï¼Œè®­ç»ƒå…ˆè¿›çš„æ¨¡å‹å¯ä»¥åœ¨æ•°å°æ—¶å†…å®Œæˆï¼Œè€Œä¸æ˜¯æ•°å¤©ï¼Œå¤§å¤§æé«˜äº†æ•ˆç‡ã€‚</li>
<li>ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„æ¨¡å‹ï¼ˆ1.8Bå‚æ•°ï¼‰èƒ½å¤Ÿè¶…è¶Šè§„æ¨¡æ›´å¤§çš„æ¨¡å‹ï¼ˆ4è‡³17å€ï¼‰ã€‚</li>
<li>æˆåŠŸçš„å…³é”®åœ¨äºä½¿ç”¨ä¸‰ç§äº’è¡¥æ•°æ®æºï¼ŒåŒ…æ‹¬å…¨è‡ªåŠ¨çš„å¤šæ¨¡æ€æ•°æ®ã€äººç±»æ ‡æ³¨çš„ä¸‰å…ƒç»„å’Œé¢„å­˜çš„å¸¦è¯´æ˜çš„æ•°æ®é¡¹ã€‚</li>
<li>å¤šç§è¯„ä¼°æ–¹æ³•è¯æ˜äº†æ¯ä¸ªæ•°æ®æºçš„ä»·å€¼ã€‚</li>
<li>æ¨å‡ºäº†é¦–ä¸ªé«˜è´¨é‡ã€å…±è¯†æ ‡æ³¨çš„é›¶æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ï¼Œç”¨äºéŸ³é¢‘å’Œä¸ªäººç”µè„‘ï¼ˆPCï¼‰ä¹‹é—´çš„å¯¹æ¯”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-561dac7739ce86d142281f4324385282" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="KTester-Leveraging-Domain-and-Testing-Knowledge-for-More-Effective-LLM-based-Test-Generation"><a href="#KTester-Leveraging-Domain-and-Testing-Knowledge-for-More-Effective-LLM-based-Test-Generation" class="headerlink" title="KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation"></a>KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation</h2><p><strong>Authors:Anji Li, Mingwei Liu, Zhenxi Chen, Zheng Pei, Zike Li, Dekun Dai, Yanlin Wang, Zibin Zheng</strong></p>
<p>Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.</p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•ç”Ÿæˆå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†åœ¨ç”Ÿæˆæ—¢æ­£ç¡®åˆå¯åœ¨å®é™…é¡¹ç›®ä¸­ç»´æŠ¤çš„æµ‹è¯•æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†KTesterï¼Œä¸€ä¸ªç»“åˆç‰¹å®šé¡¹ç›®çŸ¥è¯†å’Œæµ‹è¯•é¢†åŸŸçŸ¥è¯†çš„æ–°å‹æ¡†æ¶ï¼Œä»¥å¢å¼ºåŸºäºLLMçš„æµ‹è¯•ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡é™æ€åˆ†ææå–é¡¹ç›®ç»“æ„å’Œä½¿ç”¨çŸ¥è¯†ï¼Œä¸ºæ¨¡å‹æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ã€‚ç„¶åï¼Œå®ƒé‡‡ç”¨æµ‹è¯•é¢†åŸŸçŸ¥è¯†æŒ‡å¯¼çš„æµ‹è¯•æ¡ˆä¾‹è®¾è®¡å’Œæµ‹è¯•æ–¹æ³•ç”Ÿæˆçš„åˆ†ç¦»ï¼Œç»“åˆå¤šè§’åº¦æç¤ºç­–ç•¥ï¼Œå¼•å¯¼LLMè€ƒè™‘å„ç§æµ‹è¯•å¯å‘å¼æ–¹æ³•ã€‚ç”Ÿæˆçš„æµ‹è¯•éµå¾ªç»“æ„åŒ–æ¨¡æ¿ï¼Œæé«˜äº†æ¸…æ™°åº¦å’Œå¯ç»´æŠ¤æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¼€æºé¡¹ç›®ä¸Šè¯„ä¼°äº†KTesterï¼Œä¸æœ€æ–°çš„LLMåŸºçº¿ä½¿ç”¨è‡ªåŠ¨æ­£ç¡®æ€§å’Œè¦†ç›–ç‡æŒ‡æ ‡è¿›è¡Œæ¯”è¾ƒï¼Œä»¥åŠä¸€é¡¹è¯„ä¼°å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§çš„äººç±»ç ”ç©¶ã€‚ç»“æœè¡¨æ˜ï¼ŒKTesteråœ¨å…­ä¸ªå…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æœ€å¼ºåŸºçº¿ä¸Šæé«˜äº†æ‰§è¡Œé€šè¿‡ç‡5.6 ç™¾åˆ†æ¯”å’Œä»£ç è¦†ç›–ç‡æé«˜äº† 8.8 ç™¾åˆ†æ¯” ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„æ—¶é—´å’Œç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ›´å°‘ã€‚äººç±»è¯„ä¼°è€…ä¹Ÿå¯¹KTesteräº§ç”Ÿçš„æµ‹è¯•åœ¨æ­£ç¡®æ€§ã€å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§æ–¹é¢ç»™äºˆäº†æ›´é«˜çš„è¯„ä»·ï¼Œè¿™è¯å®äº†æˆ‘ä»¬çš„çŸ¥è¯†é©±åŠ¨æ¡†æ¶çš„å®é™…ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14224v1">PDF</a> 13 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•ç”Ÿæˆå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†åœ¨ç°å®ä¸–ç•Œé¡¹ç›®ä¸­ç”Ÿæˆæ—¢æ­£ç¡®åˆæ˜“äºç»´æŠ¤çš„æµ‹è¯•æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºKTesteræ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆé¡¹ç›®ç‰¹å®šçŸ¥è¯†å’Œæµ‹è¯•é¢†åŸŸçŸ¥è¯†ä»¥å¢å¼ºLLMçš„æµ‹è¯•ç”Ÿæˆèƒ½åŠ›ã€‚KTesteré¦–å…ˆé€šè¿‡é™æ€åˆ†ææå–é¡¹ç›®ç»“æ„å’Œä½¿ç”¨çŸ¥è¯†ï¼Œä¸ºæ¨¡å‹æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ã€‚ç„¶åé‡‡ç”¨æµ‹è¯•é¢†åŸŸçŸ¥è¯†æŒ‡å¯¼çš„æµ‹è¯•æ¡ˆä¾‹è®¾è®¡å’Œæµ‹è¯•æ–¹æ³•ç”Ÿæˆåˆ†ç¦»ï¼Œç»“åˆå¤šè§’åº¦æç¤ºç­–ç•¥ï¼Œå¼•å¯¼LLMè€ƒè™‘å¤šç§æµ‹è¯•å¯å‘å¼æ–¹æ³•ã€‚ç”Ÿæˆçš„æµ‹è¯•éµå¾ªç»“æ„åŒ–æ¨¡æ¿ï¼Œæé«˜æ¸…æ™°åº¦å’Œå¯ç»´æŠ¤æ€§ã€‚åœ¨å¤šä¸ªå¼€æºé¡¹ç›®ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒKTesteråœ¨å…­ä¸ªå…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ‰§è¡Œé€šè¿‡ç‡æé«˜5.69%ï¼Œä»£ç è¦†ç›–ç‡æé«˜8.83%ï¼ŒåŒæ—¶æ‰€éœ€æ—¶é—´å’Œç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ›´å°‘ã€‚äººç±»è¯„ä¼°è€…ä¹Ÿå¯¹KTesterç”Ÿæˆçš„æµ‹è¯•åœ¨æ­£ç¡®æ€§ã€å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§æ–¹é¢ç»™äºˆäº†æ›´é«˜çš„è¯„ä»·ï¼Œè¯å®äº†æˆ‘ä»¬çš„çŸ¥è¯†é©±åŠ¨æ¡†æ¶çš„å®é™…ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KTesteræ˜¯ä¸€ä¸ªç»“åˆé¡¹ç›®ç‰¹å®šçŸ¥è¯†å’Œæµ‹è¯•é¢†åŸŸçŸ¥è¯†çš„æ¡†æ¶ï¼Œç”¨äºå¢å¼ºLLMåœ¨æµ‹è¯•ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>KTesteré€šè¿‡é™æ€åˆ†ææå–é¡¹ç›®ç»“æ„å’ŒçŸ¥è¯†ï¼Œä¸ºLLMæä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨æµ‹è¯•é¢†åŸŸçŸ¥è¯†æŒ‡å¯¼çš„æµ‹è¯•æ¡ˆä¾‹è®¾è®¡å’Œæµ‹è¯•æ–¹æ³•ç”Ÿæˆåˆ†ç¦»ã€‚</li>
<li>å¤šè§’åº¦æç¤ºç­–ç•¥å¼•å¯¼LLMè€ƒè™‘å¤šç§æµ‹è¯•å¯å‘å¼æ–¹æ³•ã€‚</li>
<li>ç”Ÿæˆçš„æµ‹è¯•éµå¾ªç»“æ„åŒ–æ¨¡æ¿ï¼Œæé«˜æ¸…æ™°åº¦å’Œå¯ç»´æŠ¤æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªå¼€æºé¡¹ç›®ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒKTesteråœ¨å¤šä¸ªå…³é”®æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f41fedafa6b5001a27a6832b3ece4da" align="middle">
<img src="https://picx.zhimg.com/v2-d354344dd8a654ff842db332cba30619" align="middle">
<img src="https://picx.zhimg.com/v2-2b8b9d31342c1243c5b2b4a66a6925c3" align="middle">
<img src="https://picx.zhimg.com/v2-5568e2a66eee4b25161ef45267098d37" align="middle">
<img src="https://picx.zhimg.com/v2-40b26a88ebe1b412c50bfa18fdd092a0" align="middle">
<img src="https://picx.zhimg.com/v2-f955afc3af86b83c87e992ec6352137a" align="middle">
<img src="https://picx.zhimg.com/v2-5ef8c0080e859b8f935fe67c36131901" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LLM-Aligned-Geographic-Item-Tokenization-for-Local-Life-Recommendation"><a href="#LLM-Aligned-Geographic-Item-Tokenization-for-Local-Life-Recommendation" class="headerlink" title="LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation"></a>LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation</h2><p><strong>Authors:Hao Jiang, Guoquan Wang, Donglin Zhou, Sheng Yu, Yang Zeng, Wencong Zeng, Kun Gai, Guorui Zhou</strong></p>
<p>Recent advances in Large Language Models (LLMs) have enhanced text-based recommendation by enriching traditional ID-based methods with semantic generalization capabilities. Text-based methods typically encode item textual information via prompt design and generate discrete semantic IDs through item tokenization. However, in domain-specific tasks such as local-life services, simply injecting location information into prompts fails to capture fine-grained spatial characteristics and real-world distance awareness among items. To address this, we propose LGSID, an LLM-Aligned Geographic Item Tokenization Framework for Local-life Recommendation. This framework consists of two key components: (1) RL-based Geographic LLM Alignment, and (2) Hierarchical Geographic Item Tokenization. In the RL-based alignment module, we initially train a list-wise reward model to capture real-world spatial relationships among items. We then introduce a novel G-DPO algorithm that uses pre-trained reward model to inject generalized spatial knowledge and collaborative signals into LLMs while preserving their semantic understanding. Furthermore, we propose a hierarchical geographic item tokenization strategy, where primary tokens are derived from discrete spatial and content attributes, and residual tokens are refined using the aligned LLMâ€™s geographic representation vectors. Extensive experiments on real-world Kuaishou industry datasets show that LGSID consistently outperforms state-of-the-art discriminative and generative recommendation models. Ablation studies, visualizations, and case studies further validate its effectiveness.</p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥é€šè¿‡ä¸ºä¼ ç»ŸåŸºäºIDçš„æ–¹æ³•èµ‹äºˆè¯­ä¹‰æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå¢å¼ºäº†åŸºäºæ–‡æœ¬çš„æ¨èã€‚åŸºäºæ–‡æœ¬çš„æ–¹æ³•é€šå¸¸é€šè¿‡æç¤ºè®¾è®¡å¯¹ç‰©å“æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç¼–ç ï¼Œå¹¶é€šè¿‡ç‰©å“æ ‡è®°åŒ–ç”Ÿæˆç¦»æ•£è¯­ä¹‰IDã€‚ç„¶è€Œï¼Œåœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ï¼ˆå¦‚æœ¬åœ°ç”Ÿæ´»æœåŠ¡ï¼‰ä¸­ï¼Œä»…é€šè¿‡æç¤ºæ³¨å…¥ä½ç½®ä¿¡æ¯æ— æ³•æ•è·ç²¾ç»†çš„ç©ºé—´ç‰¹å¾å’Œç‰©å“ä¹‹é—´çš„ç°å®ä¸–ç•Œè·ç¦»æ„ŸçŸ¥ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LGSIDï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æœ¬åœ°ç”Ÿæ´»æ¨èçš„LLMå¯¹é½åœ°ç†é¡¹ç›®æ ‡è®°åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„åœ°ç†LLMå¯¹é½ï¼Œï¼ˆ2ï¼‰åˆ†å±‚åœ°ç†é¡¹ç›®æ ‡è®°åŒ–ã€‚åœ¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹é½æ¨¡å—ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªåˆ—è¡¨çº§å¥–åŠ±æ¨¡å‹æ¥æ•è·ç‰©å“ä¹‹é—´ç°å®ä¸–ç•Œçš„ç©ºé—´å…³ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„G-DPOç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨é¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹å°†æ³›åŒ–çš„ç©ºé—´çŸ¥è¯†å’ŒååŒä¿¡å·æ³¨å…¥LLMä¸­ï¼ŒåŒæ—¶ä¿æŒå…¶è¯­ä¹‰ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚åœ°ç†é¡¹ç›®æ ‡è®°åŒ–ç­–ç•¥ï¼Œå…¶ä¸­ä¸»è¦æ ‡è®°æ¥è‡ªç¦»æ•£çš„ç©ºé—´å’Œå†…å®¹å±æ€§ï¼Œå‰©ä½™æ ‡è®°åˆ™ä½¿ç”¨å¯¹é½çš„LLMåœ°ç†è¡¨ç¤ºå‘é‡è¿›è¡Œç»†åŒ–ã€‚åœ¨å¿«æ‰‹è¡Œä¸šçœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLGSIDå§‹ç»ˆä¼˜äºæœ€æ–°çš„åˆ¤åˆ«å’Œç”Ÿæˆæ¨èæ¨¡å‹ã€‚æ¶ˆèç ”ç©¶ã€å¯è§†åŒ–å’Œæ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•é€šè¿‡è¯­ä¹‰æ³›åŒ–èƒ½åŠ›ä¸°å¯Œäº†ä¼ ç»Ÿçš„åŸºäºIDçš„æ–¹æ³•ï¼Œä»è€Œæé«˜äº†åŸºäºæ–‡æœ¬çš„æ¨èã€‚é’ˆå¯¹é¢†åŸŸç‰¹å®šä»»åŠ¡å¦‚æœ¬åœ°ç”Ÿæ´»æœåŠ¡ï¼Œæå‡ºLGSIDæ¡†æ¶ï¼ŒåŒ…æ‹¬RLåŸºäºåœ°ç†çš„LLMå¯¹é½å’Œåˆ†å±‚åœ°ç†é¡¹ç›®ä»¤ç‰ŒåŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒLGSIDåœ¨å¿«æ‰‹è¡Œä¸šæ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºæœ€æ–°çš„åˆ¤åˆ«å’Œç”Ÿæˆæ¨èæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„è¿›å±•é€šè¿‡è¯­ä¹‰æ³›åŒ–èƒ½åŠ›å¢å¼ºäº†æ–‡æœ¬æ¨èã€‚</li>
<li>æ–‡æœ¬æ¨èæ–¹æ³•é€šè¿‡ç¼–ç é¡¹ç›®æ–‡æœ¬ä¿¡æ¯å’Œç”Ÿæˆç¦»æ•£è¯­ä¹‰IDè¿›è¡Œã€‚</li>
<li>åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­ï¼Œä»…æ³¨å…¥ä½ç½®ä¿¡æ¯åˆ°æç¤ºä¸­æ— æ³•æ•æ‰ç²¾ç»†çš„ç©ºé—´ç‰¹æ€§å’Œç°å®ä¸–ç•Œä¸­çš„è·ç¦»æ„ŸçŸ¥ã€‚</li>
<li>LGSIDæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šRLåŸºäºåœ°ç†çš„LLMå¯¹é½å’Œåˆ†å±‚åœ°ç†é¡¹ç›®ä»¤ç‰ŒåŒ–ã€‚</li>
<li>RL-basedå¯¹é½æ¨¡å—é€šè¿‡è®­ç»ƒåˆ—è¡¨å¥–åŠ±æ¨¡å‹æ¥æ•è·é¡¹ç›®ä¹‹é—´çš„ç°å®ç©ºé—´å…³ç³»ï¼Œå¹¶å¼•å…¥G-DPOç®—æ³•å°†ç©ºé—´çŸ¥è¯†å’Œåä½œä¿¡å·æ³¨å…¥LLMä¸­ã€‚</li>
<li>åˆ†å±‚åœ°ç†é¡¹ç›®ä»¤ç‰ŒåŒ–ç­–ç•¥åŒ…æ‹¬ä¸»è¦ä»¤ç‰Œå’Œå‰©ä½™ä»¤ç‰Œï¼Œå‰è€…æ¥æºäºç¦»æ•£çš„ç©ºé—´å’Œå†…å®¹å±æ€§ï¼Œåè€…é€šè¿‡LLMåœ°ç†è¡¨ç¤ºå‘é‡è¿›è¡Œç²¾ç‚¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-434d737fd2161c80ed46d103602118ec" align="middle">
<img src="https://picx.zhimg.com/v2-eea8378981cec48b985f7f83bffa3110" align="middle">
<img src="https://picx.zhimg.com/v2-dc1252e5818c4b3da60a3cddd47abf07" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Precise-Event-Spotting-via-Unified-Multi-Entity-Graph-and-Distillation"><a href="#Few-Shot-Precise-Event-Spotting-via-Unified-Multi-Entity-Graph-and-Distillation" class="headerlink" title="Few-Shot Precise Event Spotting via Unified Multi-Entity Graph and Distillation"></a>Few-Shot Precise Event Spotting via Unified Multi-Entity Graph and Distillation</h2><p><strong>Authors:Zhaoyu Liu, Kan Jiang, Murong Ma, Zhe Hou, Yun Lin, Jin Song Dong</strong></p>
<p>Precise event spotting (PES) aims to recognize fine-grained events at exact moments and has become a key component of sports analytics. This task is particularly challenging due to rapid succession, motion blur, and subtle visual differences. Consequently, most existing methods rely on domain-specific, end-to-end training with large labeled datasets and often struggle in few-shot conditions due to their dependence on pixel- or pose-based inputs alone. However, obtaining large labeled datasets is practically hard. We propose a Unified Multi-Entity Graph Network (UMEG-Net) for few-shot PES. UMEG-Net integrates human skeletons and sport-specific object keypoints into a unified graph and features an efficient spatio-temporal extraction module based on advanced GCN and multi-scale temporal shift. To further enhance performance, we employ multimodal distillation to transfer knowledge from keypoint-based graphs to visual representations. Our approach achieves robust performance with limited labeled data and significantly outperforms baseline models in few-shot settings, providing a scalable and effective solution for few-shot PES. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/LZYAndy/UMEG-Net">https://github.com/LZYAndy/UMEG-Net</a>.</p>
<blockquote>
<p>ç²¾ç¡®äº‹ä»¶å®šä½ï¼ˆPESï¼‰æ—¨åœ¨ç²¾ç¡®æ—¶åˆ»è¯†åˆ«ç²¾ç»†ç²’åº¦çš„äº‹ä»¶ï¼Œå·²æˆä¸ºä½“è‚²åˆ†æçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç”±äºäº‹ä»¶å¿«é€Ÿè¿ç»­å‘ç”Ÿã€è¿åŠ¨æ¨¡ç³Šå’Œç»†å¾®çš„è§†è§‰å·®å¼‚ï¼Œæ­¤ä»»åŠ¡é¢‡å…·æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½ä¾èµ–äºç‰¹å®šé¢†åŸŸçš„ç«¯åˆ°ç«¯è®­ç»ƒä»¥åŠå¤§é‡æ ‡è®°æ•°æ®é›†ï¼Œå¹¶ä¸”ç”±äºå®ƒä»¬ä»…ä¾èµ–äºåƒç´ æˆ–åŸºäºå§¿åŠ¿çš„è¾“å…¥ï¼Œå› æ­¤åœ¨å°æ ·æœ¬æ¡ä»¶ä¸‹å¾€å¾€è¡¨ç°æŒ£æ‰ã€‚ç„¶è€Œï¼Œè·å–å¤§é‡æ ‡è®°æ•°æ®é›†åœ¨å®é™…ä¸­æ˜¯å¾ˆå›°éš¾çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå°æ ·æœ¬PESçš„ç»Ÿä¸€å¤šå®ä½“å›¾ç½‘ç»œï¼ˆUMEG-Netï¼‰ã€‚UMEG-Netå°†äººä½“éª¨éª¼å’Œè¿åŠ¨ç‰¹å®šå¯¹è±¡å…³é”®ç‚¹é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€å›¾ä¸­ï¼Œå¹¶åŸºäºé«˜çº§GCNå’Œå¤šå°ºåº¦æ—¶é—´ç§»ä½åŠŸèƒ½ï¼Œå…·æœ‰é«˜æ•ˆçš„ç©ºé—´æ—¶é—´æå–æ¨¡å—ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ¨¡æ€è’¸é¦æŠ€æœ¯ï¼Œå°†åŸºäºå…³é”®ç‚¹çš„å›¾å½¢çš„çŸ¥è¯†è½¬ç§»åˆ°è§†è§‰è¡¨ç¤ºä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸Šè¡¨ç°ç¨³å¥ï¼Œåœ¨å°æ ·æœ¬è®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œä¸ºå°æ ·æœ¬ç§‘æä¾›äº†å¯æ‰©å±•å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/LZYAndy/UMEG-Net%E3%80%82">https://github.com/LZYAndy/UMEG-Netã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14186v1">PDF</a> The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç²¾ç»†åŠ¨ä½œäº‹ä»¶æ£€æµ‹çš„å°‘æ ·æœ¬å­¦ä¹ çš„æ–¹æ³•ï¼Œå³ç»Ÿä¸€å¤šå®ä½“å›¾ç½‘ç»œï¼ˆUMEG-Netï¼‰ã€‚è¯¥æ–¹æ³•ç»“åˆäº†äººä½“éª¨æ¶å’Œè¿åŠ¨ç‰¹å®šå¯¹è±¡å…³é”®ç‚¹ï¼Œé‡‡ç”¨åŸºäºå›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰å’Œå¤šå°ºåº¦æ—¶é—´ç§»ä½çš„é«˜æ•ˆæ—¶ç©ºæå–æ¨¡å—ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨å¤šæ¨¡æ€è’¸é¦æŠ€æœ¯ä»å…³é”®ç‚¹å›¾å‘è§†è§‰è¡¨ç¤ºè½¬ç§»çŸ¥è¯†ã€‚è¯¥æ–¹æ³•åœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸‹è¡¨ç°å‡ºç¨³å¥æ€§èƒ½ï¼Œå¹¶åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PESï¼ˆç²¾ç¡®äº‹ä»¶è¯†åˆ«ï¼‰æ˜¯ä½“è‚²åˆ†æä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œæ—¨åœ¨è¯†åˆ«ç²¾ç¡®æ—¶åˆ»çš„ç²¾ç»†äº‹ä»¶ã€‚</li>
<li>ç”±äºå¿«é€Ÿè¿ç»­åŠ¨ä½œã€è¿åŠ¨æ¨¡ç³Šå’Œç»†å¾®çš„è§†è§‰å·®å¼‚ï¼Œæ­¤ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•å¤§å¤šä¾èµ–äºç‰¹å®šé¢†åŸŸçš„ç«¯åˆ°ç«¯è®­ç»ƒå’Œå¤§é‡æ ‡è®°æ•°æ®é›†ï¼Œä½†åœ¨å°‘æ ·æœ¬æ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ã€‚</li>
<li>UMEG-Netç»“åˆäº†äººä½“éª¨æ¶å’Œè¿åŠ¨ç‰¹å®šå¯¹è±¡å…³é”®ç‚¹ï¼Œé‡‡ç”¨ç»Ÿä¸€å›¾è¡¨ç¤ºã€‚</li>
<li>UMEG-Netä½¿ç”¨åŸºäºGCNå’Œå¤šå°ºåº¦æ—¶é—´ç§»ä½çš„æ—¶ç©ºæå–æ¨¡å—ã€‚</li>
<li>å¤šæ¨¡æ€è’¸é¦æŠ€æœ¯ç”¨äºä»å…³é”®ç‚¹å›¾å‘è§†è§‰è¡¨ç¤ºè½¬ç§»çŸ¥è¯†ã€‚</li>
<li>æ–¹æ³•åœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸‹è¡¨ç°ç¨³å¥ï¼Œå°‘æ ·æœ¬è®¾ç½®ä¸‹æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5610d009ce994d1d6903dd87d235c95" align="middle">
<img src="https://picx.zhimg.com/v2-b86202bb94048ef3602bb0da5036a7cd" align="middle">
<img src="https://picx.zhimg.com/v2-67b1e47df6746463db75067c247e62f6" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SMART-Shot-Aware-Multimodal-Video-Moment-Retrieval-with-Audio-Enhanced-MLLM"><a href="#SMART-Shot-Aware-Multimodal-Video-Moment-Retrieval-with-Audio-Enhanced-MLLM" class="headerlink" title="SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM"></a>SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM</h2><p><strong>Authors:An Yu, Weiheng Lu, Jian Li, Zhenfei Zhang, Yunhang Shen, Felix X. -F. Ye, Ming-Ching Chang</strong></p>
<p>Video Moment Retrieval is a task in video understanding that aims to localize a specific temporal segment in an untrimmed video based on a natural language query. Despite recent progress in moment retrieval from videos using both traditional techniques and Multimodal Large Language Models (MLLM), most existing methods still rely on coarse temporal understanding and a single visual modality, limiting performance on complex videos. To address this, we introduce \textit{S}hot-aware \textit{M}ultimodal \textit{A}udio-enhanced \textit{R}etrieval of \textit{T}emporal \textit{S}egments (SMART), an MLLM-based framework that integrates audio cues and leverages shot-level temporal structure. SMART enriches multimodal representations by combining audio and visual features while applying \textbf{Shot-aware Token Compression}, which selectively retains high-information tokens within each shot to reduce redundancy and preserve fine-grained temporal details. We also refine prompt design to better utilize audio-visual cues. Evaluations on Charades-STA and QVHighlights show that SMART achieves significant improvements over state-of-the-art methods, including a 1.61% increase in <a href="mailto:&#x52;&#49;&#64;&#48;&#x2e;&#x35;">&#x52;&#49;&#64;&#48;&#x2e;&#x35;</a> and 2.59% gain in <a href="mailto:&#x52;&#x31;&#x40;&#x30;&#46;&#55;">&#x52;&#x31;&#x40;&#x30;&#46;&#55;</a> on Charades-STA.</p>
<blockquote>
<p>è§†é¢‘æ—¶åˆ»æ£€ç´¢æ˜¯è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„ä¸€ç§ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢åœ¨æœªä¿®å‰ªçš„è§†é¢‘ä¸­å®šä½ç‰¹å®šçš„æ—¶é—´ç‰‡æ®µã€‚å°½ç®¡æœ€è¿‘åœ¨ä¼ ç»ŸæŠ€æœ¯å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ–¹é¢çš„è§†é¢‘æ—¶åˆ»æ£€ç´¢å–å¾—äº†è¿›å±•ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»ç„¶ä¾èµ–äºç²—ç•¥çš„æ—¶é—´ç†è§£å’Œå•ä¸€è§†è§‰æ¨¡å¼ï¼Œè¿™åœ¨å¤„ç†å¤æ‚è§†é¢‘æ—¶é™åˆ¶äº†æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SMARTï¼ˆåŸºäºé•œå¤´æ„ŸçŸ¥çš„å¤šæ¨¡æ€éŸ³é¢‘å¢å¼ºæ—¶é—´ç‰‡æ®µæ£€ç´¢ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºMLLMçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†éŸ³é¢‘çº¿ç´¢å¹¶åˆ©ç”¨äº†é•œå¤´çº§çš„æ—¶é—´ç»“æ„ã€‚SMARTé€šè¿‡ç»“åˆéŸ³é¢‘å’Œè§†è§‰ç‰¹å¾æ¥ä¸°å¯Œå¤šæ¨¡æ€è¡¨ç¤ºï¼ŒåŒæ—¶åº”ç”¨â€œé•œå¤´æ„ŸçŸ¥ä»¤ç‰Œå‹ç¼©â€ï¼Œæœ‰é€‰æ‹©åœ°ä¿ç•™æ¯ä¸ªé•œå¤´ä¸­çš„é«˜ä¿¡æ¯ä»¤ç‰Œï¼Œä»¥å‡å°‘å†—ä½™å¹¶ä¿ç•™ç²¾ç»†çš„æ—¶é—´ç»†èŠ‚ã€‚æˆ‘ä»¬è¿˜å¯¹æç¤ºè®¾è®¡è¿›è¡Œäº†æ”¹è¿›ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨è§†å¬çº¿ç´¢ã€‚åœ¨Charades-STAå’ŒQVHighlightsä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSMARTç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬åœ¨Charades-STAä¸Š<a href="mailto:&#x52;&#x31;&#64;&#x30;&#46;&#x35;">&#x52;&#x31;&#64;&#x30;&#46;&#x35;</a>æé«˜äº†1.61%ï¼Œ<a href="mailto:&#x52;&#x31;&#64;&#x30;&#46;&#55;">&#x52;&#x31;&#64;&#x30;&#46;&#55;</a>æé«˜äº†2.59%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14143v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘æ—¶åˆ»æ£€ç´¢æ˜¯è§†é¢‘ç†è§£é¢†åŸŸä¸­çš„ä¸€é¡¹ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢åœ¨æœªå‰ªè¾‘çš„è§†é¢‘ä¸­å®šä½ç‰¹å®šçš„æ—¶é—´ç‰‡æ®µã€‚å°½ç®¡è¿‘æœŸé‡‡ç”¨ä¼ ç»ŸæŠ€æœ¯å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–¹æ³•æœ‰æ‰€è¿›å±•ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»ä¾èµ–äºç²—ç•¥çš„æ—¶é—´ç†è§£å’Œå•ä¸€è§†è§‰æ¨¡å¼ï¼Œè¿™åœ¨å¤æ‚è§†é¢‘ä¸Šçš„æ€§èƒ½å—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥SMARTæ¡†æ¶ï¼Œå®ƒç»“åˆäº†éŸ³é¢‘çº¿ç´¢ï¼Œåˆ©ç”¨é•œå¤´çº§åˆ«çš„ä¸´æ—¶ç»“æ„ï¼Œå¹¶åŸºäºMLLMã€‚SMARTé€šè¿‡ç»“åˆéŸ³é¢‘å’Œè§†è§‰ç‰¹å¾æ¥ä¸°å¯Œå¤šæ¨¡å¼è¡¨ç¤ºï¼ŒåŒæ—¶åº”ç”¨é•œå¤´æ„ŸçŸ¥æ ‡è®°å‹ç¼©ï¼Œé€‰æ‹©æ€§ä¿ç•™æ¯ä¸ªé•œå¤´ä¸­çš„é«˜ä¿¡æ¯æ ‡è®°ï¼Œä»¥å‡å°‘å†—ä½™å¹¶ä¿ç•™ç²¾ç»†çš„æ—¶é—´ç»†èŠ‚ã€‚åœ¨Charades-STAå’ŒQVHighlightsä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSMARTç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œå…¶ä¸­åœ¨Charades-STAä¸Š<a href="mailto:&#82;&#49;&#x40;&#48;&#x2e;&#x35;">&#82;&#49;&#x40;&#48;&#x2e;&#x35;</a>æé«˜äº†1.61%ï¼Œ<a href="mailto:&#82;&#49;&#x40;&#x30;&#46;&#x37;">&#82;&#49;&#x40;&#x30;&#46;&#x37;</a>æé«˜äº†2.59%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ—¶åˆ»æ£€ç´¢æ—¨åœ¨å®šä½æœªå‰ªè¾‘è§†é¢‘ä¸­çš„ç‰¹å®šæ—¶é—´ç‰‡æ®µï¼ŒåŸºäºè‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç²—ç•¥çš„æ—¶é—´ç†è§£å’Œå•ä¸€è§†è§‰æ¨¡å¼ï¼Œæ€§èƒ½å—é™ã€‚</li>
<li>SMARTæ¡†æ¶ç»“åˆéŸ³é¢‘çº¿ç´¢å’Œé•œå¤´çº§åˆ«çš„ä¸´æ—¶ç»“æ„ã€‚</li>
<li>SMARTåˆ©ç”¨å¤šæ¨¡æ€è¡¨ç¤ºï¼Œç»“åˆéŸ³é¢‘å’Œè§†è§‰ç‰¹å¾ã€‚</li>
<li>é•œå¤´æ„ŸçŸ¥æ ‡è®°å‹ç¼©æŠ€æœ¯è¢«ç”¨äºSMARTä¸­ï¼Œä»¥ä¿ç•™é«˜ä¿¡æ¯æ ‡è®°å¹¶å‡å°‘å†—ä½™ã€‚</li>
<li>SMARTåœ¨Charades-STAå’ŒQVHighlightsä¸Šçš„è¯„ä¼°ç»“æœä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00685be9e1ca914ca052f5390c7b0269" align="middle">
<img src="https://picx.zhimg.com/v2-e17c8fdb48df91b910d0fb76f5b2a87e" align="middle">
<img src="https://picx.zhimg.com/v2-1510c420aa4f50667020a8683e72a1d2" align="middle">
<img src="https://picx.zhimg.com/v2-15dac372b6fbd62dd747a060997756d5" align="middle">
<img src="https://picx.zhimg.com/v2-f5fdb83395b51236a86048fcfbb47d71" align="middle">
<img src="https://picx.zhimg.com/v2-6bd5aad16b198920729a96451c3e896f" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FxSearcher-gradient-free-text-driven-audio-transformation"><a href="#FxSearcher-gradient-free-text-driven-audio-transformation" class="headerlink" title="FxSearcher: gradient-free text-driven audio transformation"></a>FxSearcher: gradient-free text-driven audio transformation</h2><p><strong>Authors:Hojoon Ki, Jongsuk Kim, Minchan Kwon, Junmo Kim</strong></p>
<p>Achieving diverse and high-quality audio transformations from text prompts remains challenging, as existing methods are fundamentally constrained by their reliance on a limited set of differentiable audio effects. This paper proposes \textbf{FxSearcher}, a novel gradient-free framework that discovers the optimal configuration of audio effects (FX) to transform a source signal according to a text prompt. Our method employs Bayesian Optimization and CLAP-based score function to perform this search efficiently. Furthermore, a guiding prompt is introduced to prevent undesirable artifacts and enhance human preference. To objectively evaluate our method, we propose an AI-based evaluation framework. The results demonstrate that the highest scores achieved by our method on these metrics align closely with human preferences. Demos are available at <a target="_blank" rel="noopener" href="https://hojoonki.github.io/FxSearcher/">https://hojoonki.github.io/FxSearcher/</a></p>
<blockquote>
<p>å®ç°å¤šæ ·åŒ–å’Œé«˜è´¨é‡åŸºäºæ–‡æœ¬æç¤ºçš„éŸ³é¢‘è½¬æ¢ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•ä»æ ¹æœ¬ä¸Šå—åˆ°å…¶ä¾èµ–äºæœ‰é™çš„å¯å¾®éŸ³é¢‘æ•ˆæœçš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°å‹æ— æ¢¯åº¦æ¡†æ¶â€”â€”FxSearcherï¼Œå®ƒå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºå‘ç°éŸ³é¢‘æ•ˆæœçš„æœ€ä½³é…ç½®ï¼Œä»¥è½¬æ¢æºä¿¡å·ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è´å¶æ–¯ä¼˜åŒ–å’ŒåŸºäºCLAPçš„è¯„åˆ†å‡½æ•°ï¼Œä»¥é«˜æ•ˆæ‰§è¡Œæœç´¢æ“ä½œã€‚æ­¤å¤–ï¼Œå¼•å…¥æŒ‡å¯¼æç¤ºï¼Œä»¥é¿å…ä¸è‰¯äº§ç‰©å¹¶æé«˜äººç±»åå¥½åº¦ã€‚ä¸ºäº†å®¢è§‚åœ°è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºAIçš„è¯„ä¼°æ¡†æ¶ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿™äº›æŒ‡æ ‡ä¸Šè·å¾—çš„æœ€é«˜åˆ†æ•°ä¸äººç±»åå¥½ç´§å¯†å¯¹é½ã€‚æ¼”ç¤ºå†…å®¹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://hojoonki.github.io/FxSearcher/%E8%AE%BF%E9%97%AE%E3%80%82">https://hojoonki.github.io/FxSearcher/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14138v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºäº†ä¸€ä¸ªåä¸ºFxSearcherçš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ— æ¢¯åº¦ä¾èµ–çš„æ–¹å¼ï¼Œå®ç°åŸºäºæ–‡æœ¬æç¤ºçš„é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„éŸ³é¢‘è½¬æ¢ã€‚é€šè¿‡å¼•å…¥è´å¶æ–¯ä¼˜åŒ–å’ŒåŸºäºCLAPçš„è¯„åˆ†å‡½æ•°ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé«˜æ•ˆæœç´¢æœ€ä½³çš„éŸ³é¢‘æ•ˆæœé…ç½®æ¥è½¬æ¢æºä¿¡å·ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†æŒ‡å¯¼æç¤ºæ¥é˜²æ­¢ä¸å¸Œæœ›çš„ä¼ªè¿¹å¹¶å¢å¼ºäººç±»åå¥½ã€‚é‡‡ç”¨åŸºäºAIçš„è¯„ä¼°æ¡†æ¶è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œç»“æœè¯æ˜äº†è¯¥æ–¹æ³•ä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FxSearcheræ˜¯ä¸€ä¸ªæ— æ¢¯åº¦ä¾èµ–çš„åˆ›æ–°æ¡†æ¶ï¼Œç”¨äºéŸ³é¢‘è½¬æ¢ä»»åŠ¡ã€‚å®ƒé€šè¿‡ä¼˜åŒ–éŸ³é¢‘æ•ˆæœé…ç½®æ¥è½¬æ¢æºä¿¡å·ä»¥åŒ¹é…æ–‡æœ¬æç¤ºã€‚</li>
<li>FxSearcheré€šè¿‡è´å¶æ–¯ä¼˜åŒ–å’ŒåŸºäºCLAPçš„è¯„åˆ†å‡½æ•°å®ç°é«˜æ•ˆæœç´¢éŸ³é¢‘æ•ˆæœé…ç½®ã€‚è¿™ç§æ–¹æ³•è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•çš„é™åˆ¶ï¼Œä¸ºéŸ³é¢‘è½¬æ¢æä¾›äº†æ›´å¹¿æ³›çš„å¯èƒ½æ€§ã€‚</li>
<li>å¼•å…¥äº†æŒ‡å¯¼æç¤ºï¼Œæœ‰æ•ˆé˜²æ­¢è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°ä¸å¸Œæœ›çš„ä¼ªè¿¹ï¼ŒåŒæ—¶å¢å¼ºäº†äººç±»åå¥½ã€‚è¿™æé«˜äº†éŸ³é¢‘è½¬æ¢çš„è´¨é‡å’Œç”¨æˆ·ä½“éªŒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e9bc39eb896cb30a439d815733ec985" align="middle">
<img src="https://picx.zhimg.com/v2-e2361069eee70f661bf6b37af1a447f6" align="middle">
<img src="https://picx.zhimg.com/v2-a7477fb6a9951a663fdae4026de17193" align="middle">
<img src="https://picx.zhimg.com/v2-1a98329c8e3ec87b7fedce8e115f9f35" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Run-Ruminate-and-Regulate-A-Dual-process-Thinking-System-for-Vision-and-Language-Navigation"><a href="#Run-Ruminate-and-Regulate-A-Dual-process-Thinking-System-for-Vision-and-Language-Navigation" class="headerlink" title="Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation"></a>Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation</h2><p><strong>Authors:Yu Zhong, Zihao Zhang, Rui Zhang, Lingdong Huang, Haihan Gao, Shuo Wang, Da Li, Ruijian Han, Jiaming Guo, Shaohui Peng, Di Huang, Yunji Chen</strong></p>
<p>Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMsâ€™ generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.</p>
<blockquote>
<p>è§†è§‰ä¸è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰è¦æ±‚æ™ºèƒ½ä½“æ ¹æ®äººç±»æŒ‡ä»¤åŠ¨æ€æ¢ç´¢å¤æ‚çš„3Dç¯å¢ƒã€‚æœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒäº†åœ¨è§†è§‰ä¸è¯­è¨€å¯¼èˆªä¸­åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ½œåŠ›ï¼Œå› ä¸ºå®ƒä»¬å…·å¤‡å¸¸è¯†çŸ¥è¯†å’Œé€šç”¨æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡LLMå…·æœ‰ä¼˜åŠ¿ï¼Œä½†åŸºäºLLMçš„æ–¹æ³•å’Œé¢†åŸŸä¸“å®¶ä¹‹é—´åœ¨ä»»åŠ¡å®Œæˆæ€§èƒ½ä¸Šä»å­˜åœ¨è¾ƒå¤§å·®è·ï¼Œå› ä¸ºLLMåœ¨æœ¬è´¨ä¸Šéš¾ä»¥ç²¾ç¡®åœ°ç†è§£ç°å®ä¸–ç•Œçš„ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œå¼•å…¥LLMè¿˜ä¼´éšç€å·¨å¤§çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºR3çš„æ–°å‹åŒè¿‡ç¨‹æ€ç»´æ¡†æ¶ï¼Œä»¥é›¶æ ·æœ¬çš„æ–¹å¼å°†LLMçš„é€šç”¨èƒ½åŠ›ä¸VLNç‰¹å®šä¸“ä¸šçŸ¥è¯†ç›¸ç»“åˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šRunnerã€Ruminatorå’ŒRegulatorã€‚Runneræ˜¯ä¸€ä¸ªåŸºäºè½»é‡çº§å˜å‹å™¨çš„ä¸“å®¶æ¨¡å‹ï¼Œå¯ç¡®ä¿åœ¨å¸¸è§„æƒ…å†µä¸‹å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„å¯¼èˆªã€‚Ruminatoråˆ™é‡‡ç”¨å¼ºå¤§çš„å¤šæ¨¡æ€LLMä½œä¸ºéª¨å¹²ï¼Œå¹¶é‡‡ç”¨æ€ç»´é“¾æç¤ºæ¥æ¿€å‘ç»“æ„åŒ–æ¨ç†ã€‚Regulatorç›‘æ§å¯¼èˆªè¿›åº¦ï¼Œå¹¶æ ¹æ®ä¸‰ä¸ªæ ‡å‡†æ§åˆ¶é€‚å½“çš„æ€ç»´æ¨¡å¼ï¼Œä½¿Runnerå’ŒRuminatorèƒ½å¤Ÿå’Œè°é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜¾ç¤ºï¼ŒR3åœ¨REVERIEåŸºå‡†æµ‹è¯•ä¸Šçš„SPLå’ŒRGSPLåˆ†åˆ«è¶…è¿‡äº†å…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œè¾¾åˆ°äº†3.28%å’Œ3.30%ã€‚è¿™ä¸€æ˜¾è‘—çš„æå‡çªæ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨åº”å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„VLNä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14131v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºè§†è§‰å’Œè¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰çš„ä»»åŠ¡éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºä¸€ä¸ªæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸VLNç‰¹å®šä¸“é•¿çš„é›¶æ ·æœ¬æ€è€ƒæ¡†æ¶R3ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šRunnerã€Ruminatorå’ŒRegulatorã€‚Runnerç¡®ä¿é«˜æ•ˆå‡†ç¡®å¯¼èˆªï¼ŒRuminatoråˆ©ç”¨å¼ºå¤§çš„å¤šæ¨¡æ€LLMè¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼ŒRegulatoræ ¹æ®ä¸‰ä¸ªæ ‡å‡†ç›‘æ§å¯¼èˆªè¿›åº¦å¹¶æ§åˆ¶é€‚å½“çš„æ€è€ƒæ¨¡å¼ã€‚åœ¨REVERIEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒR3æ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œæé«˜äº†3.28%å’Œ3.30%çš„SPLå’ŒRGSPLã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>VLNä»»åŠ¡éœ€è¦åŠ¨æ€æ¢ç´¢å¤æ‚3Dç¯å¢ƒå¹¶éµå¾ªäººç±»æŒ‡ä»¤ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¸¸è¯†çŸ¥è¯†å’Œé€šç”¨æ¨ç†èƒ½åŠ›ï¼Œåœ¨VLNä¸­æœ‰æ½œåŠ›ã€‚</li>
<li>LLMåœ¨ç†è§£ç°å®ä¸–ç•Œç©ºé—´å…³è”æ–¹é¢å­˜åœ¨ç²¾ç¡®æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºR3çš„æ–°å‹åŒæ€ç»´è¿‡ç¨‹æ¡†æ¶ï¼Œç»“åˆäº†LLMçš„é€šç”¨èƒ½åŠ›ä¸VLNç‰¹å®šä¸“é•¿ã€‚</li>
<li>R3æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šRunnerã€Ruminatorå’ŒRegulatorï¼Œå„æœ‰å…¶åŠŸèƒ½å’Œä½œç”¨ã€‚</li>
<li>åœ¨REVERIEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒR3æ˜¾è‘—æé«˜äº†VLNä»»åŠ¡å®Œæˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ffb36975b5c305975a3bdbc22d065a9" align="middle">
<img src="https://picx.zhimg.com/v2-6e09084fbfeee9c7817207c54822d81e" align="middle">
<img src="https://picx.zhimg.com/v2-358c7a7c4e6703860373cd57d6dbf18e" align="middle">
<img src="https://picx.zhimg.com/v2-df63dc9bd46568e9bbc0c71b632e6e52" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PRISM-Prompt-Refined-In-Context-System-Modelling-for-Financial-Retrieval"><a href="#PRISM-Prompt-Refined-In-Context-System-Modelling-for-Financial-Retrieval" class="headerlink" title="PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval"></a>PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval</h2><p><strong>Authors:Chun Chet Ng, Jia Yu Lim, Wei Zeng Low</strong></p>
<p>With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at <a target="_blank" rel="noopener" href="https://bit.ly/prism-ailens">https://bit.ly/prism-ailens</a>.</p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œé‡‘èä¿¡æ¯æ£€ç´¢å·²æˆä¸ºä¸€é¡¹é‡è¦çš„å·¥ä¸šåº”ç”¨ã€‚ä»å†—é•¿çš„è´¢åŠ¡æ–‡ä»¶ä¸­æå–ä¸ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯æ˜¯æ“ä½œå’Œå†³ç­–åˆ†æçš„å…³é”®ã€‚FinAgentBenchæ•°æ®é›†é€šè¿‡ä¸¤ä¸ªä»»åŠ¡ï¼šæ–‡æ¡£æ’åå’Œå—æ’åï¼Œæ­£å¼è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†PRISMï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ç²¾ç»†çš„ç³»ç»Ÿæç¤ºã€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œè½»é‡çº§çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚æˆ‘ä»¬å¯¹æ¯ä¸ªç»„ä»¶è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä»¥æ­ç¤ºå®ƒä»¬çš„ååŒä½œç”¨ï¼šæç¤ºå·¥ç¨‹æä¾›ç²¾ç¡®çš„ä»»åŠ¡æŒ‡ä»¤ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ æä¾›è¯­ä¹‰ä¸Šç›¸å…³çš„å°‘é‡æ ·æœ¬ï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¯¹åè°ƒè¯„åˆ†è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æœ€ä½³é…ç½®åœ¨å—é™çš„éªŒè¯é›†ä¸Šå®ç°äº†NDCG@5ä¸º0.71818çš„æˆç»©ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜PRISMå¯¹äºç”Ÿäº§è§„æ¨¡çš„é‡‘èæ£€ç´¢æ˜¯å¯è¡Œå’Œç¨³å¥çš„ã€‚å…¶æ¨¡å—åŒ–ã€ä»…æ¨ç†çš„è®¾è®¡ä½¿å…¶æˆä¸ºçœŸå®ä¸–ç•Œç”¨ä¾‹çš„å®é™…é€‰æ‹©ã€‚æºä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://bit.ly/prism-ailens%E3%80%82">https://bit.ly/prism-ailensã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14130v1">PDF</a> 3rd-place solution for the ACM ICAIF 2025 Agentic Retrieval Grand Challenge</p>
<p><strong>Summary</strong></p>
<p>é‡‘èä¿¡æ¯æ£€ç´¢å·²æˆä¸ºå…³é”®å·¥ä¸šåº”ç”¨ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ã€‚PRISMæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒã€æ•´åˆç²¾ç»†åŒ–ç³»ç»Ÿæç¤ºã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè½»é‡çº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³é‡‘èæ–‡ä»¶æ£€ç´¢ä¸­çš„æ–‡æ¡£æ’åºå’Œç‰‡æ®µæ’åºä»»åŠ¡ã€‚è¯¥æ¡†æ¶å®ç°è‰¯å¥½ï¼Œå¹¶é€‚åˆå®é™…åº”ç”¨åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èä¿¡æ¯æ£€ç´¢å·²æˆä¸ºé‡è¦å·¥ä¸šåº”ç”¨ï¼Œå¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ã€‚</li>
<li>PRISMæ¡†æ¶ç”¨äºè§£å†³é‡‘èæ–‡ä»¶æ£€ç´¢ä¸­çš„æ–‡æ¡£æ’åºå’Œç‰‡æ®µæ’åºä»»åŠ¡ã€‚</li>
<li>PRISMé›†æˆäº†ç²¾ç»†åŒ–ç³»ç»Ÿæç¤ºã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè½»é‡çº§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚</li>
<li>ç²¾ç»†åŒ–ç³»ç»Ÿæç¤ºæä¾›ç²¾ç¡®ä»»åŠ¡æŒ‡ä»¤ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ æä¾›è¯­ä¹‰ä¸Šç›¸å…³çš„å°‘é‡æ ·æœ¬ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¨¡æ‹ŸååŒè¯„åˆ†è¡Œä¸ºã€‚</li>
<li>PRISMåœ¨é™å®šéªŒè¯é›†ä¸Šå–å¾—NDCG@5ä¸º0.71818çš„æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed076afd7a1912309e3d704d0ecb0964" align="middle">
<img src="https://picx.zhimg.com/v2-6351c38ad660eef01a430be7bfa2923f" align="middle">
<img src="https://picx.zhimg.com/v2-ce48bd79fa7ea5ba8f2743a8687d98be" align="middle">
<img src="https://picx.zhimg.com/v2-7bc422fb8dace9de2716ea4ac7e462b9" align="middle">
<img src="https://picx.zhimg.com/v2-c18fde4f323d63d15bc11a090bfc9128" align="middle">
<img src="https://picx.zhimg.com/v2-50b1f277997f9fa7c930eda927c7f02c" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="APD-Agents-A-Large-Language-Model-Driven-Multi-Agents-Collaborative-Framework-for-Automated-Page-Design"><a href="#APD-Agents-A-Large-Language-Model-Driven-Multi-Agents-Collaborative-Framework-for-Automated-Page-Design" class="headerlink" title="APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design"></a>APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design</h2><p><strong>Authors:Xinpeng Chen, Xiaofeng Han, Kaihao Zhang, Guochao Ren, Yujie Wang, Wenhao Cao, Yang Zhou, Jianfeng Lu, Zhenbo Song</strong></p>
<p>Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the userâ€™s description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish usersâ€™ design task. To be specific, the SemanticParserAgent is responsible for converting usersâ€™ descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.</p>
<blockquote>
<p>ç§»åŠ¨åº”ç”¨é¡µé¢è®¾è®¡å¸ƒå±€æ˜¯å¼€å‘è¿‡ç¨‹ä¸­çš„å…³é”®æ­¥éª¤ã€‚ç„¶è€Œï¼Œå¯¹äºè®¾è®¡å¸ˆæ¥è¯´ï¼Œè®¾è®¡ä»¤äººæ»¡æ„çš„è®¾è®¡éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´ï¼Œå› ä¸ºä»–ä»¬éœ€è¦è€ƒè™‘åœ¨é¡µé¢ä¸Šå‘ˆç°å“ªäº›æ§ä»¶å’Œå†…å®¹ï¼Œå¹¶åå¤è°ƒæ•´å…¶å¤§å°ã€ä½ç½®å’Œæ ·å¼ä»¥è¾¾åˆ°æ›´å¥½çš„å¤–è§‚å’Œç»“æ„ã€‚å°½ç®¡ç°åœ¨å¾ˆå¤šè®¾è®¡è½¯ä»¶éƒ½èƒ½å¸®åŠ©å®Œæˆè¿™äº›é‡å¤çš„ä»»åŠ¡ï¼Œä½†è¦æƒ³æœ‰æ•ˆåœ°ä½¿ç”¨å®ƒä»¬ä»ç„¶éœ€è¦è¿›è¡Œå¤§é‡çš„åŸ¹è®­ã€‚æ­¤å¤–ï¼Œè·¨åº”ç”¨é¡µé¢çš„åä½œè®¾è®¡éœ€è¦é¢å¤–çš„æ—¶é—´æ¥å¯¹é½æ ‡å‡†å’Œç¡®ä¿æ ·å¼çš„ä¸€è‡´æ€§ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†APD-agentsï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”¨äºç§»åŠ¨åº”ç”¨è‡ªåŠ¨åŒ–é¡µé¢è®¾è®¡çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«OrchestratorAgentã€SemanticParserAgentã€PrimaryLayoutAgentã€TemplateRetrievalAgentå’ŒRecursiveComponentAgentã€‚åœ¨æ¥æ”¶åˆ°ç”¨æˆ·å¯¹é¡µé¢çš„æè¿°åï¼ŒOrchestratorAgentå¯ä»¥åŠ¨æ€åœ°æŒ‡å¯¼å…¶ä»–æ™ºèƒ½ä½“å®Œæˆç”¨æˆ·çš„è®¾è®¡ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒSemanticParserAgentè´Ÿè´£å°†ç”¨æˆ·å¯¹é¡µé¢å†…å®¹çš„æè¿°è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®ã€‚PrimaryLayoutAgentå¯ä»¥ç”Ÿæˆè¯¥é¡µé¢çš„åˆå§‹ç²—ç•¥å¸ƒå±€ã€‚TemplateRetrievalAgentå¯ä»¥æ£€ç´¢è¯­ä¹‰ä¸Šç›¸å…³çš„å°‘æ•°ç¤ºä¾‹å¹¶æå‡å¸ƒå±€ç”Ÿæˆçš„è´¨é‡ã€‚æ­¤å¤–ï¼ŒRecursiveComponentAgentå¯ä»¥ç”¨æ¥å†³å®šå¦‚ä½•é€’å½’ç”Ÿæˆå¸ƒå±€ä¸­æ¯ä¸ªå…ƒç´ æ‰€åŒ…å«çš„ç²¾ç»†ç²’åº¦å­å…ƒç´ ã€‚æˆ‘ä»¬çš„å·¥ä½œå……åˆ†åˆ©ç”¨äº†å¤§å‹æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªåŠ¨åä½œèƒ½åŠ›ã€‚åœ¨RICOæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„APD-agentsè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14101v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šä»£ç†æ¡†æ¶APD-agentsï¼Œç”¨äºç§»åŠ¨åº”ç”¨ç¨‹åºçš„è‡ªåŠ¨åŒ–é¡µé¢è®¾è®¡ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªä»£ç†ï¼Œèƒ½å¤Ÿåä½œå®Œæˆé¡µé¢è®¾è®¡ä»»åŠ¡ï¼Œå¦‚å†…å®¹ç»“æ„åŒ–ã€åˆå§‹å¸ƒå±€ç”Ÿæˆã€æ¨¡æ¿æ£€ç´¢å’Œé€’å½’ç»„ä»¶ç”Ÿæˆç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨RICOæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>APD-agentsæ˜¯ä¸€ä¸ªå¤šä»£ç†æ¡†æ¶ï¼Œç”¨äºç§»åŠ¨åº”ç”¨ç¨‹åºçš„è‡ªåŠ¨åŒ–é¡µé¢è®¾è®¡ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªä»£ç†ï¼Œå¦‚OrchestratorAgentã€SemanticParserAgentã€PrimaryLayoutAgentã€TemplateRetrievalAgentå’ŒRecursiveComponentAgentã€‚</li>
<li>APD-agentsèƒ½å¤Ÿè‡ªåŠ¨å®Œæˆé¡µé¢è®¾è®¡ä»»åŠ¡ï¼Œå¦‚å†…å®¹ç»“æ„åŒ–ã€åˆå§‹å¸ƒå±€ç”Ÿæˆç­‰ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åä½œèƒ½åŠ›ï¼Œæé«˜äº†è®¾è®¡æ•ˆç‡ã€‚</li>
<li>APD-agentsé€šè¿‡å°‘é‡ç¤ºä¾‹è¿›è¡Œè¯­ä¹‰ç›¸å…³çš„æ¨¡æ¿æ£€ç´¢ï¼Œæé«˜äº†å¸ƒå±€ç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒAPD-agentsåœ¨RICOæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6992ecb3c77279930f5f747d4816520" align="middle">
<img src="https://picx.zhimg.com/v2-0f84f08d4f2c474bfb93a9bba8e76aff" align="middle">
<img src="https://picx.zhimg.com/v2-9481c5091064728c7f5466eff4645f15" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Zero-Training-Task-Specific-Model-Synthesis-for-Few-Shot-Medical-Image-Classification"><a href="#Zero-Training-Task-Specific-Model-Synthesis-for-Few-Shot-Medical-Image-Classification" class="headerlink" title="Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification"></a>Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification</h2><p><strong>Authors:Yao Qin, Yangyang Yan, YuanChao Yang, Jinhua Pang, Huanyong Bi, Yuan Liu, HaiHua Wang</strong></p>
<p>Deep learning models have achieved remarkable success in medical image analysis but are fundamentally constrained by the requirement for large-scale, meticulously annotated datasets. This dependency on â€œbig dataâ€ is a critical bottleneck in the medical domain, where patient data is inherently difficult to acquire and expert annotation is expensive, particularly for rare diseases where samples are scarce by definition. To overcome this fundamental challenge, we propose a novel paradigm: Zero-Training Task-Specific Model Synthesis (ZS-TMS). Instead of adapting a pre-existing model or training a new one, our approach leverages a large-scale, pre-trained generative engine to directly synthesize the entire set of parameters for a task-specific classifier. Our framework, the Semantic-Guided Parameter Synthesizer (SGPS), takes as input minimal, multi-modal task information as little as a single example image (1-shot) and a corresponding clinical text description to directly synthesize the entire set of parameters for a task-specific classifier.   The generative engine interprets these inputs to generate the weights for a lightweight, efficient classifier (e.g., an EfficientNet-V2), which can be deployed for inference immediately without any task-specific training or fine-tuning. We conduct extensive evaluations on challenging few-shot classification benchmarks derived from the ISIC 2018 skin lesion dataset and a custom rare disease dataset. Our results demonstrate that SGPS establishes a new state-of-the-art, significantly outperforming advanced few-shot and zero-shot learning methods, especially in the ultra-low data regimes of 1-shot and 5-shot classification. This work paves the way for the rapid development and deployment of AI-powered diagnostic tools, particularly for the long tail of rare diseases where data is critically limited.</p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ä»æ ¹æœ¬ä¸Šå—åˆ°å¤§è§„æ¨¡ç²¾ç»†æ ‡æ³¨æ•°æ®é›†éœ€æ±‚çš„åˆ¶çº¦ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œå¯¹â€œå¤§æ•°æ®â€çš„ä¾èµ–æ˜¯ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆï¼Œå› ä¸ºæ‚£è€…æ•°æ®æœ¬è´¨ä¸Šéš¾ä»¥è·å–ï¼Œä¸“å®¶æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œå°¤å…¶æ˜¯å¯¹äºå®šä¹‰ä¸Šæ ·æœ¬ç¨€ç¼ºçš„ç½•è§ç–¾ç—…ã€‚ä¸ºäº†å…‹æœè¿™ä¸€åŸºæœ¬æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹èŒƒå¼ï¼šé›¶è®­ç»ƒä»»åŠ¡ç‰¹å®šæ¨¡å‹åˆæˆï¼ˆZS-TMSï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯é€‚åº”ç°æœ‰æ¨¡å‹æˆ–è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒç”Ÿæˆå¼•æ“ç›´æ¥åˆæˆé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„åˆ†ç±»å™¨çš„æ•´ä¸ªå‚æ•°é›†ã€‚æˆ‘ä»¬çš„æ¡†æ¶ï¼Œè¯­ä¹‰å¼•å¯¼å‚æ•°åˆæˆå™¨ï¼ˆSGPSï¼‰ï¼Œä»¥æœ€å°‘çš„å¤šæ¨¡å¼ä»»åŠ¡ä¿¡æ¯ä½œä¸ºè¾“å…¥ï¼Œåªéœ€è¦ä¸€ä¸ªç¤ºä¾‹å›¾åƒï¼ˆä¸€æ¬¡æ‹æ‘„ï¼‰å’Œç›¸åº”çš„ä¸´åºŠæ–‡æœ¬æè¿°ï¼Œå°±å¯ä»¥ç›´æ¥åˆæˆé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„åˆ†ç±»å™¨çš„æ•´ä¸ªå‚æ•°é›†ã€‚ç”Ÿæˆå¼•æ“ä¼šè§£é‡Šè¿™äº›è¾“å…¥ä»¥ç”Ÿæˆè½»é‡çº§é«˜æ•ˆåˆ†ç±»å™¨çš„æƒé‡ï¼ˆä¾‹å¦‚EfficientNet-V2ï¼‰ï¼Œè¯¥åˆ†ç±»å™¨å¯ä»¥ç«‹å³éƒ¨ç½²è¿›è¡Œæ¨ç†ï¼Œæ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚æˆ‘ä»¬åœ¨ISIC 2018çš®è‚¤ç—…å˜æ•°æ®é›†å’Œè‡ªå®šä¹‰ç½•è§ç–¾ç—…æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…·æœ‰æŒ‘æˆ˜æ€§çš„å°‘é‡æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒSGPSå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…ˆè¿›çš„å°‘é‡æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œå°¤å…¶åœ¨1ä¸ªå’Œ5ä¸ªæ ·æœ¬åˆ†ç±»çš„è¶…ä½æ•°æ®çŠ¶æ€ä¸‹ã€‚è¿™é¡¹å·¥ä½œä¸ºAIé©±åŠ¨çš„è¯Šæ–­å·¥å…·çš„å¿«é€Ÿå¼€å‘å’Œéƒ¨ç½²é“ºå¹³äº†é“è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æä¸ºæœ‰é™çš„ç½•è§ç–¾ç—…é•¿å°¾é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸Šå–å¾—æ˜¾è‘—æˆæ•ˆï¼Œä½†å¯¹å¤§è§„æ¨¡ç²¾ç»†æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–æˆä¸ºå…³é”®ç“¶é¢ˆã€‚é’ˆå¯¹åŒ»å­¦é¢†åŸŸæ‚£è€…æ•°æ®è·å–å›°éš¾ã€ä¸“å®¶æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ç½•è§ç–¾ç—…æ ·æœ¬ç¨€ç¼ºçš„é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°å‹è§£å†³æ–¹æ¡ˆï¼šé›¶è®­ç»ƒä»»åŠ¡ç‰¹å®šæ¨¡å‹åˆæˆï¼ˆZS-TMSï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒç”Ÿæˆå¼•æ“ï¼Œç›´æ¥åˆæˆé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å‚æ•°é›†ï¼Œåªéœ€å°‘é‡æ¨¡æ€ä»»åŠ¡ä¿¡æ¯å’Œç¤ºä¾‹å›¾åƒï¼Œå³å¯å¿«é€Ÿéƒ¨ç½²è¿›è¡Œæ¨ç†ã€‚åœ¨ISIC 2018çš®è‚¤ç—…å˜æ•°æ®é›†å’Œè‡ªå®šä¹‰ç½•è§ç–¾ç—…æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åˆ›å»ºæ–°æ ‡å‡†ï¼Œå°¤å…¶åœ¨è¶…ä½æ•°æ®ç¯å¢ƒä¸‹çš„1-shotå’Œ5-shotåˆ†ç±»ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¿™ä¸ºAIè¯Šæ–­å·¥å…·å°¤å…¶æ˜¯é’ˆå¯¹æ•°æ®ä¸¥é‡å—é™çš„ç½•è§ç–¾ç—…çš„å¿«é€Ÿå¼€å‘å’Œéƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨åŠå…¶å¯¹æ•°æ®é›†çš„ä¾èµ–ã€‚</li>
<li>åŒ»å­¦é¢†åŸŸæ‚£è€…æ•°æ®è·å–å’Œä¸“å®¶æ ‡æ³¨çš„å›°éš¾ï¼Œç‰¹åˆ«æ˜¯ç½•è§ç–¾ç—…æ ·æœ¬çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>æå‡ºé›¶è®­ç»ƒä»»åŠ¡ç‰¹å®šæ¨¡å‹åˆæˆï¼ˆZS-TMSï¼‰æ–¹æ³•ä»¥è§£å†³æ•°æ®ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆå¼•æ“ç›´æ¥åˆæˆä»»åŠ¡ç‰¹å®šåˆ†ç±»å™¨çš„å‚æ•°é›†ã€‚</li>
<li>è¯­ä¹‰å¼•å¯¼å‚æ•°åˆæˆå™¨ï¼ˆSGPSï¼‰åªéœ€å°‘é‡æ¨¡æ€ä»»åŠ¡ä¿¡æ¯å’Œç¤ºä¾‹å›¾åƒå³å¯å·¥ä½œã€‚</li>
<li>æ–¹æ³•åœ¨ISIC 2018çš®è‚¤ç—…å˜æ•°æ®é›†å’Œè‡ªå®šä¹‰ç½•è§ç–¾ç—…æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97e9f108bde6e1be2fcec377ca561061" align="middle">
<img src="https://picx.zhimg.com/v2-0d70533e591c58d9de6625e70e25b1c6" align="middle">
<img src="https://picx.zhimg.com/v2-19e256eecfef319e3427048f2afc4e25" align="middle">
<img src="https://picx.zhimg.com/v2-b2eface14ac9ce48aabfcdcc2cb6a5c7" align="middle">
<img src="https://picx.zhimg.com/v2-14a3dc02c070a6cef17b73e72fdb66e4" align="middle">
<img src="https://picx.zhimg.com/v2-423db59cc494298f795618d71d4a6699" align="middle">
<img src="https://picx.zhimg.com/v2-047e79ea2a177ccc5f4023cef03fd614" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Start-Small-Think-Big-Curriculum-based-Relative-Policy-Optimization-for-Visual-Grounding"><a href="#Start-Small-Think-Big-Curriculum-based-Relative-Policy-Optimization-for-Visual-Grounding" class="headerlink" title="Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding"></a>Start Small, Think Big: Curriculum-based Relative Policy Optimization for Visual Grounding</h2><p><strong>Authors:Qingyang Yan, Guangyao Chen, Yixiong Zou</strong></p>
<p>Chain-of-Thought (CoT) prompting has recently shown significant promise across various NLP and computer vision tasks by explicitly generating intermediate reasoning steps. However, we find that reinforcement learning (RL)-based fine-tuned CoT reasoning can paradoxically degrade performance in Visual Grounding tasks, particularly as CoT outputs become lengthy or complex. Additionally, our analysis reveals that increased dataset size does not always enhance performance due to varying data complexities. Motivated by these findings, we propose Curriculum-based Relative Policy Optimization (CuRPO), a novel training strategy that leverages CoT length and generalized Intersection over Union (gIoU) rewards as complexity indicators to progressively structure training data from simpler to more challenging examples. Extensive experiments on RefCOCO, RefCOCO+, RefCOCOg, and LISA datasets demonstrate the effectiveness of our approach. CuRPO consistently outperforms existing methods, including Visual-RFT, with notable improvements of up to +12.52 mAP on RefCOCO. Moreover, CuRPO exhibits exceptional efficiency and robustness, delivering strong localization performance even in few-shot learning scenarios, particularly benefiting tasks characterized by ambiguous and intricate textual descriptions.The code is released on <a target="_blank" rel="noopener" href="https://github.com/qyoung-yan/CuRPO">https://github.com/qyoung-yan/CuRPO</a>.</p>
<blockquote>
<p>â€œé“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æç¤ºåœ¨å¤šç§NLPå’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œé€šè¿‡æ˜ç¡®ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¾®è°ƒCoTæ¨ç†åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­ä¼šå‡ºç°æ€§èƒ½æ‚–è®ºæ€§ä¸‹é™çš„æƒ…å†µï¼Œç‰¹åˆ«æ˜¯åœ¨CoTè¾“å‡ºå˜å¾—å†—é•¿æˆ–å¤æ‚æ—¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¢åŠ æ•°æ®é›†çš„å¤§å°å¹¶ä¸æ€»æ˜¯èƒ½æé«˜æ€§èƒ½ï¼Œå› ä¸ºæ•°æ®å¤æ‚æ€§å„ä¸ç›¸åŒã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¯¾ç¨‹çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆCuRPOï¼‰è¿™ä¸€æ–°é¢–çš„è®­ç»ƒç­–ç•¥ï¼Œå®ƒåˆ©ç”¨CoTé•¿åº¦å’Œå¹¿ä¹‰äº¤å¹¶æ¯”ï¼ˆgIoUï¼‰å¥–åŠ±ä½œä¸ºå¤æ‚æ€§æŒ‡æ ‡ï¼Œä»¥ä»ç®€å•åˆ°å¤æ‚ç¤ºä¾‹çš„æ–¹å¼é€æ­¥æ„å»ºè®­ç»ƒæ•°æ®ã€‚åœ¨RefCOCOã€RefCOCO+ã€RefCOCOgå’ŒLISAæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚CuRPOæŒç»­è¶…è¶Šç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬Visual-RFTï¼Œåœ¨RefCOCOä¸Šçš„æ”¹è¿›è¾¾åˆ°äº†+12.52 mAPã€‚æ­¤å¤–ï¼ŒCuRPOè¡¨ç°å‡ºæƒŠäººçš„æ•ˆç‡å’Œç¨³å¥æ€§ï¼Œå³ä½¿åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ä¹Ÿèƒ½æä¾›å‡ºè‰²çš„å®šä½æ€§èƒ½ï¼Œå°¤å…¶æœ‰åˆ©äºæ¶‰åŠæ¨¡ç³Šå’Œå¤æ‚æ–‡æœ¬æè¿°çš„ä»»åŠ¡ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/qyoung-yan/CuRPO%E3%80%82">https://github.com/qyoung-yan/CuRPOã€‚</a>â€œ</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13924v1">PDF</a> AAAI 2026 (Oral)</p>
<p><strong>Summary</strong></p>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æç¤ºæ³•åœ¨å„NLPå’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡æ˜ç¡®ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ï¼Œèƒ½æœ‰æ•ˆä¿ƒè¿›æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼Œç®€ç§°RLï¼‰è¿›è¡Œfine-tuningçš„CoTæ¨ç†åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­ä¼šè¡¨ç°å‡ºæ€§èƒ½é™çº§ï¼Œå°¤å…¶åœ¨CoTè¾“å‡ºå¤æ‚ä¸”è¯¦ç»†æ—¶å°¤ä¸ºæ˜æ˜¾ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¯¾ç¨‹è¡¨çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆCurriculum-based Relative Policy Optimizationï¼Œç®€ç§°CuRPOï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨CoTé•¿åº¦å’Œå¹¿ä¹‰äº¤é›†è”åˆæ³•ï¼ˆGeneralized Intersection over Unionï¼Œç®€ç§°gIoUï¼‰å¥–åŠ±ä½œä¸ºå¤æ‚æ€§æŒ‡æ ‡ï¼Œæ¥ç»“æ„åŒ–ä»ç®€å•åˆ°å¤æ‚çš„æ¸è¿›å¼è®­ç»ƒæ•°æ®ã€‚åœ¨RefCOCOã€RefCOCO+ã€RefCOCOgå’ŒLISAæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚CuRPOæ–¹æ³•ç›¸è¾ƒäºç°æœ‰æ–¹æ³•å¦‚Visual-RFTæœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œåœ¨RefCOCOæ•°æ®é›†ä¸Šæå‡æœ€é«˜å¯è¾¾+12.52 mAPã€‚æ­¤å¤–ï¼ŒCuRPOå±•ç°å‡ºå“è¶Šçš„æ•ˆç‡ä¸ç¨³å¥æ€§ï¼Œå³ä½¿åœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ä¹Ÿèƒ½å®ç°å‡ºè‰²çš„å®šä½æ€§èƒ½ï¼Œå°¤å…¶é€‚ç”¨äºæ–‡æœ¬æè¿°æ¨¡ç³Šä¸”å¤æ‚çš„ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ³•å¯¹äºNLPå’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰fine-tuningçš„CoTæ¨ç†åœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>CuRPOæ˜¯ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡åˆ©ç”¨CoTé•¿åº¦å’ŒgIoUå¥–åŠ±ä½œä¸ºå¤æ‚æ€§æŒ‡æ ‡æ¥æ¸è¿›å¼ç»“æ„åŒ–è®­ç»ƒæ•°æ®ã€‚</li>
<li>CuRPOåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>CuRPOåœ¨RefCOCOæ•°æ®é›†ä¸Šçš„æ”¹è¿›æœ€é«˜å¯è¾¾+12.52 mAPã€‚</li>
<li>CuRPOåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ•ˆç‡ä¸ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43ae996782fde3ebddf4c94da1e13691" align="middle">
<img src="https://picx.zhimg.com/v2-4970b07bf6096646632838f6129005e4" align="middle">
<img src="https://picx.zhimg.com/v2-7e682066a1c22b65d366cea66249c2f1" align="middle">
<img src="https://picx.zhimg.com/v2-71f6d0afe8eea8db91c89fccd434b5df" align="middle">
<img src="https://picx.zhimg.com/v2-6518f7fd3a49356525fa871841ea0053" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SQL-to-Text-Generation-with-Weighted-AST-Few-Shot-Prompting"><a href="#SQL-to-Text-Generation-with-Weighted-AST-Few-Shot-Prompting" class="headerlink" title="SQL-to-Text Generation with Weighted-AST Few-Shot Prompting"></a>SQL-to-Text Generation with Weighted-AST Few-Shot Prompting</h2><p><strong>Authors:Sriom Chakrabarti, Chuangtao Ma, Arijit Khan, Sebastian Link</strong></p>
<p>SQL-to-Text generation aims at translating structured SQL queries into natural language descriptions, thereby facilitating comprehension of complex database operations for non-technical users. Although large language models (LLMs) have recently demonstrated promising results, current methods often fail to maintain the exact semantics of SQL queries, particularly when there are multiple possible correct phrasings. To address this problem, our work proposes Weighted-AST retrieval with prompting, an architecture that integrates structural query representations and LLM prompting. This method retrieves semantically relevant examples as few-shot prompts using a similarity metric based on an Abstract Syntax Tree (AST) with learned feature weights. Our structure-aware prompting technique ensures that generated descriptions are both fluent and faithful to the original query logic. Numerous experiments on three benchmark datasets - Spider, SParC, and CoSQL show that our method outperforms the current baselines by up to +17.24% in execution Accuracy (EX), performs superior in Exact Match (EM) and provides more consistent semantic fidelity when evaluated by humans, all while preserving competitive runtime performance. These results demonstrate that Weighted-AST prompting is a scalable and effective method for deriving natural language explanations from structured database queries.</p>
<blockquote>
<p>SQL-to-Textç”Ÿæˆæ—¨åœ¨å°†ç»“æ„åŒ–SQLæŸ¥è¯¢ç¿»è¯‘ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œä»è€Œå¸®åŠ©éæŠ€æœ¯ç”¨æˆ·ç†è§£å¤æ‚çš„æ•°æ®åº“æ“ä½œã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘è¡¨ç°å‡ºäº†ä»¤äººé¼“èˆçš„ç»“æœï¼Œä½†å½“å‰çš„æ–¹æ³•å¾€å¾€æ— æ³•ä¿æŒSQLæŸ¥è¯¢çš„ç¡®åˆ‡è¯­ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰å¤šç§å¯èƒ½çš„æ­£ç¡®è¡¨è¿°æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†åŠ æƒæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰æ£€ç´¢ä¸æç¤ºæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†ç»“æ„åŒ–æŸ¥è¯¢è¡¨ç¤ºå’ŒLLMæç¤ºçš„æ¶æ„ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºäºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰çš„ç›¸ä¼¼åº¦åº¦é‡ä»¥åŠå­¦ä¹ åˆ°çš„ç‰¹å¾æƒé‡ï¼Œæ£€ç´¢å°‘é‡è¯­ä¹‰ç›¸å…³çš„ç¤ºä¾‹ä½œä¸ºæç¤ºã€‚æˆ‘ä»¬çš„ç»“æ„æ„ŸçŸ¥æç¤ºæŠ€æœ¯ç¡®ä¿ç”Ÿæˆçš„æè¿°æ—¢æµç•…åˆå¿ äºåŸå§‹æŸ¥è¯¢é€»è¾‘ã€‚åœ¨Spiderã€SParcå’ŒCoSQLä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰§è¡Œå‡†ç¡®æ€§ï¼ˆEXï¼‰æ–¹é¢æ¯”å½“å‰åŸºçº¿é«˜å‡º+17.24%ï¼Œåœ¨ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰æ–¹é¢è¡¨ç°æ›´ä¼˜ï¼ŒåŒæ—¶åœ¨äººç±»è¯„ä¼°æ—¶æä¾›æ›´ä¸€è‡´çš„è¯­ä¹‰ä¿çœŸåº¦ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›æ°´å¹³çš„è¿è¡Œæ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŠ æƒASTæç¤ºæ˜¯ä¸€ç§å¯ä»ç»“æ„åŒ–æ•°æ®åº“æŸ¥è¯¢ä¸­å¯¼å‡ºè‡ªç„¶è¯­è¨€è§£é‡Šçš„å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13907v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SQL-to-Textç”Ÿæˆæ—¨åœ¨å°†ç»“æ„åŒ–SQLæŸ¥è¯¢ç¿»è¯‘ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œå¸®åŠ©éæŠ€æœ¯ç”¨æˆ·ç†è§£å¤æ‚çš„æ•°æ®åº“æ“ä½œã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æœ‰è¾ƒå¥½è¡¨ç°ï¼Œä½†å½“å‰æ–¹æ³•å¾€å¾€æ— æ³•ä¿æŒSQLæŸ¥è¯¢çš„ç¡®åˆ‡è¯­ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰å¤šç§å¯èƒ½æ­£ç¡®è¡¨è¿°æ—¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºWeighted-ASTæ£€ç´¢ä¸æç¤ºæ¶æ„ï¼Œç»“åˆç»“æ„åŒ–æŸ¥è¯¢è¡¨ç¤ºä¸LLMæç¤ºã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºäºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰çš„ç›¸ä¼¼åº¦åº¦é‡å’Œå­¦ä¹ çš„ç‰¹å¾æƒé‡ï¼Œæ£€ç´¢å°‘é‡ç›¸å…³ç¤ºä¾‹ä½œä¸ºæç¤ºã€‚ç»“æ„æ„ŸçŸ¥æç¤ºæŠ€æœ¯ç¡®ä¿ç”Ÿæˆçš„æè¿°æ—¢æµç•…åˆå¿ å®äºåŸå§‹æŸ¥è¯¢é€»è¾‘ã€‚åœ¨Spiderã€SParCå’ŒCoSQLä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰§è¡Œå‡†ç¡®åº¦ï¼ˆEXï¼‰ä¸Šè¾ƒå½“å‰åŸºçº¿é«˜å‡º+17.24%ï¼Œåœ¨ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œäººç±»è¯„ä¼°çš„è¯­ä¹‰ä¿çœŸåº¦ä¹Ÿæ›´é«˜ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›è¿è¡Œæ—¶é—´æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒWeighted-ASTæç¤ºæ˜¯ä¸€ç§ä»ç»“æ„åŒ–æ•°æ®åº“æŸ¥è¯¢ä¸­æ´¾ç”Ÿå‡ºè‡ªç„¶è¯­è¨€è§£é‡Šçš„æœ‰æ•ˆä¸”å¯æ‰©å±•æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SQL-to-Textç”Ÿæˆæ—¨åœ¨å°†SQLæŸ¥è¯¢è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œä¾¿äºéæŠ€æœ¯ç”¨æˆ·ç†è§£ã€‚</li>
<li>å½“å‰æ–¹æ³•éš¾ä»¥ä¿æŒSQLæŸ¥è¯¢çš„ç¡®åˆ‡è¯­ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰å¤šç§è¡¨è¾¾æ–¹å¼æ—¶ã€‚</li>
<li>Weighted-ASTæ£€ç´¢ä¸æç¤ºæ¶æ„ç»“åˆäº†ç»“æ„åŒ–æŸ¥è¯¢è¡¨ç¤ºå’ŒLLMæç¤ºæŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨åŸºäºASTçš„ç›¸ä¼¼åº¦åº¦é‡åŠå­¦ä¹ çš„ç‰¹å¾æƒé‡è¿›è¡Œè¯­ä¹‰ç›¸å…³ç¤ºä¾‹æ£€ç´¢ã€‚</li>
<li>ç»“æ„æ„ŸçŸ¥æç¤ºæŠ€æœ¯ç¡®ä¿æè¿°æ—¢æµç•…åˆå¿ å®äºåŸå§‹æŸ¥è¯¢é€»è¾‘ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ‰§è¡Œå‡†ç¡®åº¦ã€ç²¾ç¡®åŒ¹é…å’Œè¯­ä¹‰ä¿çœŸåº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-56e2a95d1e05d202bfd98f1ff602e4ed" align="middle">
<img src="https://picx.zhimg.com/v2-097402965843178eae0ccb4e2c635443" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Temporal-Object-Aware-Vision-Transformer-for-Few-Shot-Video-Object-Detection"><a href="#Temporal-Object-Aware-Vision-Transformer-for-Few-Shot-Video-Object-Detection" class="headerlink" title="Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection"></a>Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection</h2><p><strong>Authors:Yogesh Kumar, Anand Mishra</strong></p>
<p>Few-shot Video Object Detection (FSVOD) addresses the challenge of detecting novel objects in videos with limited labeled examples, overcoming the constraints of traditional detection methods that require extensive training data. This task presents key challenges, including maintaining temporal consistency across frames affected by occlusion and appearance variations, and achieving novel object generalization without relying on complex region proposals, which are often computationally expensive and require task-specific training. Our novel object-aware temporal modeling approach addresses these challenges by incorporating a filtering mechanism that selectively propagates high-confidence object features across frames. This enables efficient feature progression, reduces noise accumulation, and enhances detection accuracy in a few-shot setting. By utilizing few-shot trained detection and classification heads with focused feature propagation, we achieve robust temporal consistency without depending on explicit object tube proposals. Our approach achieves performance gains, with AP improvements of 3.7% (FSVOD-500), 5.3% (FSYTV-40), 4.3% (VidOR), and 4.5 (VidVRD) in the 5-shot setting. Further results demonstrate improvements in 1-shot, 3-shot, and 10-shot configurations. We make the code public at: <a target="_blank" rel="noopener" href="https://github.com/yogesh-iitj/fs-video-vit">https://github.com/yogesh-iitj/fs-video-vit</a></p>
<blockquote>
<p>å°‘æ ·æœ¬è§†é¢‘ç›®æ ‡æ£€æµ‹ï¼ˆFSVODï¼‰è§£å†³äº†åœ¨æœ‰é™æ ‡è®°æ ·æœ¬ä¸­æ£€æµ‹æ–°è§†é¢‘ç›®æ ‡çš„æŒ‘æˆ˜ï¼Œå…‹æœäº†ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„é™åˆ¶ã€‚æ­¤ä»»åŠ¡é¢ä¸´å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç»´æŒå—é®æŒ¡å’Œå¤–è§‚å˜åŒ–å½±å“çš„å¸§ä¹‹é—´çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œä»¥åŠåœ¨ä¸éœ€è¦å¤æ‚åŒºåŸŸææ¡ˆçš„æƒ…å†µä¸‹å®ç°æ–°å¯¹è±¡æ³›åŒ–ã€‚å¤æ‚çš„åŒºåŸŸææ¡ˆé€šå¸¸è®¡ç®—é‡å¤§ï¼Œéœ€è¦ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–°å‹ç›®æ ‡æ„ŸçŸ¥æ—¶é—´å»ºæ¨¡æ–¹æ³•é€šè¿‡å¼•å…¥è¿‡æ»¤æœºåˆ¶æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥æœºåˆ¶å¯é€‰æ‹©æ€§åœ°åœ¨å¸§ä¹‹é—´ä¼ æ’­é«˜ç½®ä¿¡åº¦çš„ç›®æ ‡ç‰¹å¾ã€‚è¿™å®ç°äº†æœ‰æ•ˆçš„ç‰¹å¾è¿›å±•ï¼Œå‡å°‘äº†å™ªå£°ç´¯ç§¯ï¼Œå¹¶åœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸­æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§ã€‚é€šè¿‡åˆ©ç”¨å°‘é‡æ ·æœ¬è®­ç»ƒçš„æ£€æµ‹å’Œåˆ†ç±»å¤´ä»¥åŠæœ‰é’ˆå¯¹æ€§çš„ç‰¹å¾ä¼ æ’­ï¼Œæˆ‘ä»¬åœ¨ä¸ä¾èµ–æ˜¾å¼å¯¹è±¡ç®¡ææ¡ˆçš„æƒ…å†µä¸‹å®ç°äº†ç¨³å¥çš„æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨FSVOD-500ã€FSYTV-40ã€VidORå’ŒVidVRDçš„5æ¬¡å°„å‡»è®¾ç½®ä¸Šåˆ†åˆ«æé«˜äº†APå€¼3.7%ã€5.3%ã€4.3%å’Œ4.5%ã€‚è¿›ä¸€æ­¥çš„ç»“æœå±•ç¤ºäº†åœ¨å•æ¬¡å°„å‡»ã€ä¸‰æ¬¡å°„å‡»å’Œåæ¬¡å°„å‡»é…ç½®ä¸­çš„æ”¹è¿›ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€äº†ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/yogesh-iitj/fs-video-vit">https://github.com/yogesh-iitj/fs-video-vit</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.13784v1">PDF</a> Accepted at AAAI 2026 Main Track</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Few-shot Video Object Detectionï¼ˆFSVODï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯è§£å†³äº†åœ¨è§†é¢‘ä¸­å¯¹æ–°å‹å¯¹è±¡è¿›è¡Œæœ‰é™æ ‡æ³¨æ ·æœ¬æ£€æµ‹çš„æŒ‘æˆ˜ã€‚é€šè¿‡é‡‡ç”¨å¯¹è±¡æ„ŸçŸ¥çš„æ—¶é—´å»ºæ¨¡æ–¹æ³•ï¼Œç»“åˆè¿‡æ»¤æœºåˆ¶é€‰æ‹©æ€§ä¼ æ’­é«˜ç½®ä¿¡åº¦çš„å¯¹è±¡ç‰¹å¾ï¼Œå®ç°äº†é«˜æ•ˆç‰¹å¾è¿›å±•ã€å‡å°‘å™ªå£°ç´¯ç§¯ï¼Œå¹¶åœ¨å°æ ·æœ¬è®¾ç½®ä¸‹æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§ã€‚é€šè¿‡åˆ©ç”¨å°æ ·æœ¬è®­ç»ƒçš„æ£€æµ‹å’Œåˆ†ç±»å¤´ä»¥åŠæœ‰é’ˆå¯¹æ€§çš„ç‰¹å¾ä¼ æ’­ï¼Œå®ç°äº†ç¨³å¥çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæ— éœ€ä¾èµ–æ˜¾å¼å¯¹è±¡ç®¡é“ææ¡ˆã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot Video Object Detection (FSVOD) æŠ€æœ¯è§£å†³äº†åœ¨æœ‰é™æ ‡æ³¨æ ·æœ¬ä¸‹æ£€æµ‹è§†é¢‘ä¸­çš„æ–°å‹å¯¹è±¡çš„æŒ‘æˆ˜ã€‚</li>
<li>FSVODé¢ä¸´çš„å…³é”®æŒ‘æˆ˜åŒ…æ‹¬ï¼šç»´æŒè·¨å¸§çš„æš‚æ—¶ä¸€è‡´æ€§ã€å¤„ç†é®æŒ¡å’Œå¤–è§‚å˜åŒ–ä»¥åŠå®ç°æ–°å‹å¯¹è±¡çš„æ³›åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¯¹è±¡æ„ŸçŸ¥çš„æ—¶é—´å»ºæ¨¡æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§ä¼ æ’­é«˜ç½®ä¿¡åº¦çš„å¯¹è±¡ç‰¹å¾æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>é«˜æ•ˆç‰¹å¾è¿›å±•ã€å‡å°‘å™ªå£°ç´¯ç§¯åœ¨å°æ ·æœ¬è®¾ç½®ä¸‹æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨å°æ ·æœ¬è®­ç»ƒçš„æ£€æµ‹å’Œåˆ†ç±»å¤´ä»¥åŠæœ‰é’ˆå¯¹æ€§çš„ç‰¹å¾ä¼ æ’­ï¼Œå®ç°äº†ç¨³å¥çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæ— éœ€ä¾èµ–å¤æ‚çš„åŒºåŸŸææ¡ˆã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨5-shotè®¾ç½®åŠå…¶ä»–è®¾ç½®ä¸­å‡å–å¾—äº†æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84d7e6445ed20bb25441baeb19e964e7" align="middle">
<img src="https://picx.zhimg.com/v2-b66f37c565abaa93d2e05e25d185d1b9" align="middle">
<img src="https://picx.zhimg.com/v2-ab505f97b4d772a051a251870aaff8a6" align="middle">
<img src="https://picx.zhimg.com/v2-c038e5d09e34fd0ac8878d4601fbb7cf" align="middle">
<img src="https://picx.zhimg.com/v2-618a559aaf4b3d8911727d6b8b5b39b5" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="PFAvatar-Pose-Fusion-3D-Personalized-Avatar-Reconstruction-from-Real-World-Outfit-of-the-Day-Photos"><a href="#PFAvatar-Pose-Fusion-3D-Personalized-Avatar-Reconstruction-from-Real-World-Outfit-of-the-Day-Photos" class="headerlink" title="PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos"></a>PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos</h2><p><strong>Authors:Dianbing Xi, Guoyuan An, Jingsen Zhu, Zhijian Liu, Yuan Liu, Ruiyuan Zhang, Jiayuan Lu, Yuchi Huo, Rui Wang</strong></p>
<p>We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from Outfit of the Day(OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48x speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions&#x2F;truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†PFAvatarï¼ˆå§¿æ€èåˆåŒ–èº«ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥ä»æ—¥å¸¸ç©¿æ­ï¼ˆOOTDï¼‰ç…§ç‰‡é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´åŒ–èº«ã€‚è¿™äº›ç…§ç‰‡å±•ç¤ºäº†å¤šç§å§¿æ€ã€é®æŒ¡å’Œå¤æ‚èƒŒæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰é€šè¿‡å°‘é‡OOTDç¤ºä¾‹å¯¹å§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›ï¼ˆ2ï¼‰é€šè¿‡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤º3DåŒ–èº«å¹¶è¿›è¡Œè’¸é¦ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä¸åŒäºä»¥å¾€å°†å›¾åƒåˆ†å‰²æˆèµ„äº§ï¼ˆå¦‚æœè£…ã€é…é¥°ï¼‰è¿›è¡Œ3Dç»„è£…çš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•å®¹æ˜“å¯¼è‡´ä¸ä¸€è‡´æ€§ã€‚æˆ‘ä»¬é¿å…åˆ†è§£ï¼Œç›´æ¥å¯¹å…¨èº«å¤–è§‚è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡é›†æˆé¢„è®­ç»ƒçš„ControlNetè¿›è¡Œå§¿æ€ä¼°è®¡å’Œæ–°é¢–çš„æ¡ä»¶å…ˆéªŒä¿ç•™æŸå¤±ï¼ˆCPPLï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç«¯åˆ°ç«¯å­¦ä¹ ä¸­ç²¾ç»†ç»†èŠ‚ï¼ŒåŒæ—¶å‡è½»å°‘é‡è®­ç»ƒä¸­çš„è¯­è¨€æ¼‚ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…5åˆ†é’Ÿå†…å®Œæˆä¸ªæ€§åŒ–è®¾ç½®ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†48å€ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºNeRFçš„åŒ–èº«è¡¨ç¤ºï¼Œé€šè¿‡æ ‡å‡†SMPL-Xç©ºé—´é‡‡æ ·å’Œå¤šåˆ†è¾¨ç‡3D-SDSè¿›è¡Œä¼˜åŒ–ã€‚ä¸åŸºäºç½‘æ ¼çš„è¡¨ç¤ºç›¸æ¯”ï¼Œåè€…å—åˆ°åˆ†è¾¨ç‡ä¾èµ–çš„ç¦»æ•£åŒ–å’Œé®æŒ¡å‡ ä½•é”™è¯¯çš„å½±å“ï¼Œæˆ‘ä»¬çš„è¿ç»­è¾å°„åœºèƒ½å¤Ÿä¿ç•™é«˜é¢‘çº¹ç†ï¼ˆä¾‹å¦‚å¤´å‘ï¼‰å¹¶é€šè¿‡é€å…‰ç‡æ­£ç¡®å¤„ç†é®æŒ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒPFAvataråœ¨é‡å»ºä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™å’ŒæŠ—é®æŒ¡&#x2F;æˆªæ–­æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ¨åŠ¨äº†ä»ç°å®ä¸–ç•ŒOOTDç›¸å†Œä¸­ç”Ÿæˆå®ç”¨ä¸‰ç»´åŒ–èº«çš„å‘å±•ã€‚æ­¤å¤–ï¼Œé‡å»ºçš„3DåŒ–èº«æ”¯æŒä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚è™šæ‹Ÿè¯•ç©¿ã€åŠ¨ç”»å’Œäººç±»è§†é¢‘å¤ç°ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„é€šç”¨æ€§å’Œå®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.12935v2">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§åä¸ºPFAvatarçš„æ–°æ–¹æ³•ï¼Œä»æ—¥å¸¸ç©¿æ­ç…§ç‰‡é‡å»ºé«˜è´¨é‡3Då¤´åƒã€‚æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡å¾®è°ƒå§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹è¿›è¡Œå°‘æ•°æ—¥å¸¸ç©¿æ­ç¤ºä¾‹å­¦ä¹ ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡ç¥ç»è¾å°„åœºè¡¨ç¤º3Då¤´åƒã€‚è¯¥æ–¹æ³•é¿å…äº†èµ„äº§åˆ†è§£çš„ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œç›´æ¥å»ºæ¨¡å…¨èº«å¤–è§‚ï¼Œå®ç°ä¸ªæ€§åŒ–ä»…éœ€5åˆ†é’Ÿï¼Œé€Ÿåº¦æå‡48å€ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨åŸºäºNeRFçš„å¤´åƒè¡¨ç¤ºæ³•ï¼Œé€šè¿‡è§„èŒƒSMPL-Xç©ºé—´é‡‡æ ·å’Œå¤šåˆ†è¾¨ç‡3D-SDSè¿›è¡Œä¼˜åŒ–ï¼Œèƒ½ä¿ç•™é«˜é¢‘çº¹ç†å¹¶æ­£ç¡®å¤„ç†é®æŒ¡ã€‚PFAvataræ–¹æ³•æé«˜äº†é‡å»ºçš„ä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™ä»¥åŠå¯¹é®æŒ¡çš„é²æ£’æ€§ï¼Œæ¨åŠ¨äº†å®ç”¨3Då¤´åƒä»ç°å®ä¸–ç•Œçš„æ—¥å¸¸ç©¿æ­ç›¸å†Œç”Ÿæˆçš„æŠ€æœ¯è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PFAvataræ˜¯ä¸€ç§ä»æ—¥å¸¸ç©¿æ­ç…§ç‰‡é‡å»º3Då¤´åƒçš„æ–°æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¾®è°ƒå§¿æ€æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹å’Œé‡‡ç”¨ç¥ç»è¾å°„åœºè¡¨ç¤º3Då¤´åƒã€‚</li>
<li>é¿å…èµ„äº§åˆ†è§£çš„ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œç›´æ¥å»ºæ¨¡å…¨èº«å¤–è§‚ã€‚</li>
<li>ä¸ªæ€§åŒ–å®Œæˆä»…éœ€5åˆ†é’Ÿï¼Œé€Ÿåº¦æå‡æ˜¾è‘—ã€‚</li>
<li>é‡‡ç”¨NeRF-basedå¤´åƒè¡¨ç¤ºæ³•ï¼Œèƒ½ä¿ç•™é«˜é¢‘çº¹ç†å¹¶æ­£ç¡®å¤„ç†é®æŒ¡ã€‚</li>
<li>PFAvataråœ¨é‡å»ºçš„ä¿çœŸåº¦ã€ç»†èŠ‚ä¿ç•™ä»¥åŠå¯¹é®æŒ¡çš„é²æ£’æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44a47f9f5267362bc79d4a4b8cf362dd" align="middle">
<img src="https://picx.zhimg.com/v2-f69f246eef6a79116dd8a1c3c0437772" align="middle">
<img src="https://picx.zhimg.com/v2-dc4cce51cb10c3588fd127745a8e1a41" align="middle">
<img src="https://picx.zhimg.com/v2-5c8c04429049eeef36d71747b460ea83" align="middle">
<img src="https://picx.zhimg.com/v2-79f18b03f7c81325d79e755ba0fe6749" align="middle">
<img src="https://picx.zhimg.com/v2-87c8e6052af98dbaef6dec80e7c738af" align="middle">
<img src="https://picx.zhimg.com/v2-a8d8f82dc104053190c27ed1c2c01c14" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VULPO-Context-Aware-Vulnerability-Detection-via-On-Policy-LLM-Optimization"><a href="#VULPO-Context-Aware-Vulnerability-Detection-via-On-Policy-LLM-Optimization" class="headerlink" title="VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization"></a>VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization</h2><p><strong>Authors:Youpeng Li, Fuxun Yu, Xinda Wang</strong></p>
<p>The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context.   This paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution. Extensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.</p>
<blockquote>
<p>å¯¹å¼€æºè½¯ä»¶çš„å¹¿æ³›ä¾èµ–æ˜¾è‘—å¢åŠ äº†æ¼æ´åˆ©ç”¨çš„é£é™©ï¼Œè¿™çªæ˜¾äº†æœ‰æ•ˆä¸”å¯æ‰©å±•çš„æ¼æ´æ£€æµ‹ï¼ˆVDï¼‰çš„å¿…è¦æ€§ã€‚ç°æœ‰çš„VDæŠ€æœ¯ï¼Œæ— è®ºæ˜¯åŸºäºä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„æ–¹æ³•è¿˜æ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œå¦‚æç¤ºå·¥ç¨‹ã€ç›‘ç£å¾®è°ƒæˆ–ç¦»çº¿åå¥½ä¼˜åŒ–ï¼Œå®ƒä»¬åœ¨æ‰§è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†ææ–¹é¢çš„èƒ½åŠ›ä»æ ¹æœ¬ä¸Šå—åˆ°é™åˆ¶ï¼šå®ƒä»¬ä¾èµ–äºå›ºå®šè¾“å…¥æˆ–é™æ€åå¥½æ•°æ®é›†ï¼Œæ— æ³•è‡ªé€‚åº”åœ°æ¢ç´¢ä»“åº“çº§åˆ«çš„ä¾èµ–å…³ç³»ï¼Œå¹¶å—åˆ°åŠŸèƒ½çº§åˆ«åŸºå‡†æµ‹è¯•çš„çº¦æŸï¼Œè¿™äº›åŸºå‡†æµ‹è¯•å¿½ç•¥äº†å…³é”®çš„æ¼æ´ä¸Šä¸‹æ–‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11896v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼€æºè½¯ä»¶çš„å¹¿æ³›åº”ç”¨å¢åŠ äº†æ¼æ´åˆ©ç”¨çš„é£é™©ï¼Œå‡¸æ˜¾äº†æœ‰æ•ˆä¸”å¯æ‰©å±•çš„æ¼æ´æ£€æµ‹ï¼ˆVDï¼‰çš„é‡è¦æ€§ã€‚ç°æœ‰VDæŠ€æœ¯å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æ‰§è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç­–ç•¥çš„LLMå¼ºåŒ–å­¦ä¹ æ¡†æ¶VULPOï¼Œç”¨äºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„VDã€‚é€šè¿‡æ„å»ºContextVulæ•°æ®é›†å’Œæ”¯æŒå¤šç»´å¥–åŠ±ç»“æ„è®¾è®¡ï¼ŒVULPOå®ç°äº†é¢„æµ‹æ­£ç¡®æ€§ã€æ¼æ´å®šä½å‡†ç¡®æ€§å’Œè¯­ä¹‰ç›¸å…³æ€§çš„ç»¼åˆè€ƒé‡ï¼Œå¹¶è®¾è®¡äº†éš¾åº¦è‡ªé€‚åº”çš„å¥–åŠ±ç¼©æ”¾æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒVULPOæ¡†æ¶åœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥VDæ–¹é¢çš„è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æºè½¯ä»¶çš„å¹¿æ³›åº”ç”¨å¢åŠ äº†æ¼æ´åˆ©ç”¨çš„é£é™©ï¼Œå‡¸æ˜¾äº†æ”¹è¿›æ¼æ´æ£€æµ‹ï¼ˆVDï¼‰æŠ€æœ¯çš„å¿…è¦æ€§ã€‚</li>
<li>ç°æœ‰VDæŠ€æœ¯å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æ‰§è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>VULPOæ˜¯ä¸€ç§åŸºäºç­–ç•¥çš„LLMå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„VDã€‚</li>
<li>ContextVulæ•°æ®é›†çš„æ„å»ºæ”¯æŒVULPOçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>VULPOé€šè¿‡å¤šç»´å¥–åŠ±ç»“æ„è®¾è®¡ï¼Œå®ç°äº†é¢„æµ‹æ­£ç¡®æ€§ã€æ¼æ´å®šä½å‡†ç¡®æ€§å’Œè¯­ä¹‰ç›¸å…³æ€§çš„ç»¼åˆè€ƒé‡ã€‚</li>
<li>VULPOé‡‡ç”¨äº†éš¾åº¦è‡ªé€‚åº”çš„å¥–åŠ±ç¼©æ”¾æœºåˆ¶ï¼Œä»¥å¤„ç†ä¸åŒçš„æ¼æ´æ¡ˆä¾‹çš„ä¸å¯¹ç§°éš¾åº¦å¹¶é˜²æ­¢å¥–åŠ±ä½œå¼Šã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒVULPOæ¡†æ¶åœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥VDæ–¹é¢çš„è¡¨ç°å“è¶Šï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72cd555314d0c70fe7c584b9f4a75a23" align="middle">
<img src="https://picx.zhimg.com/v2-79231456553a380ccf6ce3449b8bf409" align="middle">
<img src="https://picx.zhimg.com/v2-4d024aed536014d38329ce790b4f68ca" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Efficient-Reinforcement-Learning-for-Zero-Shot-Coordination-in-Evolving-Games"><a href="#Efficient-Reinforcement-Learning-for-Zero-Shot-Coordination-in-Evolving-Games" class="headerlink" title="Efficient Reinforcement Learning for Zero-Shot Coordination in Evolving Games"></a>Efficient Reinforcement Learning for Zero-Shot Coordination in Evolving Games</h2><p><strong>Authors:Bingyu Hui, Lebin Yu, Quanming Yao, Yunpeng Qu, Xudong Zhang, Jian Wang</strong></p>
<p>Zero-shot coordination(ZSC), a key challenge in multi-agent game theory, has become a hot topic in reinforcement learning (RL) research recently, especially in complex evolving games. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators from a diverse, potentially evolving, pool of partners that are not seen before without any fine-tuning. Population-based training, which approximates such an evolving partner pool, has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient RL training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi cooperative game and confirms its superiority.</p>
<blockquote>
<p>é›¶é•œå¤´åè°ƒï¼ˆZSCï¼‰æ˜¯å¤šæ™ºèƒ½ä½“åšå¼ˆç†è®ºä¸­çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œæœ€è¿‘å·²æˆä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç ”ç©¶ä¸­çš„çƒ­ç‚¹è¯é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ¼”åŒ–æ¸¸æˆä¸­ã€‚å®ƒå…³æ³¨æ™ºèƒ½ä½“çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¦æ±‚å®ƒä»¬ä¸æ¥è‡ªå¤šæ ·åŒ–çš„ã€å¯èƒ½ä¸æ–­æ¼”åŒ–çš„ä¼™ä¼´æ± ä¸­çš„åˆä½œè€…è¿›è¡Œè‰¯å¥½åè°ƒï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•å¾®è°ƒå³å¯é€‚åº”ä¹‹å‰æœªè§è¿‡çš„ä¼™ä¼´ã€‚åŸºäºç§ç¾¤è®­ç»ƒçš„æ–¹æ³•è¿‘ä¼¼äºè¿™ç§ä¸æ–­æ¼”å˜çš„ä¼™ä¼´æ± ï¼Œå·²è¢«è¯æ˜å¯ä»¥æä¾›è‰¯å¥½çš„é›¶é•œå¤´åè°ƒæ€§èƒ½ï¼›ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å—é™äºè®¡ç®—èµ„æºï¼Œä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–å°ç§ç¾¤ä¸­çš„å¤šæ ·æ€§ï¼Œè€Œå¿½ç•¥äº†æ‰©å¤§ç§ç¾¤è§„æ¨¡å¯èƒ½å¸¦æ¥çš„æ½œåœ¨æ€§èƒ½æå‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯æ‰©å±•ç§ç¾¤è®­ç»ƒï¼ˆScaPTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„RLè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šä¸€ç§å…ƒæ™ºèƒ½ä½“ï¼Œå®ƒé€šè¿‡é€‰æ‹©æ€§å…±äº«å‚æ•°æœ‰æ•ˆåœ°å®ç°ç§ç¾¤ï¼Œä»¥åŠä¸€ä¸ªä¿è¯ç§ç¾¤å¤šæ ·æ€§çš„äº’ä¿¡æ¯è°ƒèŠ‚å™¨ã€‚ä¸ºäº†å®è¯éªŒè¯ScaPTçš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡åœ¨Hanabiåˆä½œæ¸¸æˆä¸­å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸å…¶ä»–ä»£è¡¨æ€§æ¡†æ¶è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯å®äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11083v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶ä¸­ï¼Œé›¶æ ·æœ¬åè°ƒï¼ˆZSCï¼‰æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å¤šå˜çš„åšå¼ˆç¯å¢ƒä¸­ã€‚æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¯æ‰©å±•äººå£è®­ç»ƒï¼ˆScaPTï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬å…ƒä»£ç†å’Œç›¸äº’ä¿¡æ¯è°ƒèŠ‚å™¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•è®¡ç®—èµ„æºæœ‰é™çš„é—®é¢˜ï¼Œé€šè¿‡é€‰æ‹©æ€§å…±äº«å‚æ•°å’Œä¿è¯äººå£å¤šæ ·æ€§æ¥æé«˜é›¶æ ·æœ¬åè°ƒæ€§èƒ½ã€‚é€šè¿‡æ±‰å¸åˆä½œæ¸¸æˆä¸­çš„å®éªŒéªŒè¯ï¼Œè¯æ˜äº†ScaPTçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬åè°ƒï¼ˆZSCï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨å¤æ‚å¤šå˜çš„åšå¼ˆç¯å¢ƒä¸­ã€‚</li>
<li>äººå£è®­ç»ƒå¯æ¨¡æ‹Ÿé›¶æ ·æœ¬åè°ƒçš„ä¼™ä¼´æ± ï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨å°ç§ç¾¤å¤šæ ·æ€§çš„ä¼˜åŒ–ã€‚</li>
<li>æœ¬æ–‡æå‡ºå¯æ‰©å±•äººå£è®­ç»ƒï¼ˆScaPTï¼‰æ¡†æ¶ä»¥è§£å†³è®¡ç®—èµ„æºé—®é¢˜ã€‚åŒ…æ‹¬é€‰æ‹©æ€§å…±äº«å‚æ•°çš„å…ƒä»£ç†å’Œä¿è¯ç§ç¾¤å¤šæ ·æ€§çš„ç›¸äº’ä¿¡æ¯è°ƒèŠ‚å™¨ã€‚</li>
<li>å…ƒä»£ç†èƒ½é«˜æ•ˆå®ç°ç§ç¾¤æ„å»ºã€‚é€šè¿‡å‚æ•°å…±äº«é™ä½è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>ç›¸äº’ä¿¡æ¯è°ƒèŠ‚å™¨ç¡®ä¿ç§ç¾¤å¤šæ ·æ€§ï¼Œæœ‰åŠ©äºæé«˜é›¶æ ·æœ¬åè°ƒæ€§èƒ½ã€‚</li>
<li>åœ¨æ±‰å¸åˆä½œæ¸¸æˆä¸­éªŒè¯äº†ScaPTæ¡†æ¶çš„ä¼˜è¶Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜å…¶åœ¨æé«˜é›¶æ ·æœ¬åè°ƒæ€§èƒ½æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-115cd73954d6ce06613cb706f3673a01" align="middle">
<img src="https://picx.zhimg.com/v2-27c16ba70813abf8c73158c59f1f293e" align="middle">
<img src="https://picx.zhimg.com/v2-6b100073b26ae92f9905510ef811e944" align="middle">
<img src="https://picx.zhimg.com/v2-4df97403e54454925bd1b0f08c119df0" align="middle">
<img src="https://picx.zhimg.com/v2-e101cdb8defb0ba21569098a2ccb8379" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a5081cff53890101b3bec859e9232d49" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-339068b970dc33b0f6b4becd581e7bde" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  MAVias Mitigate any Visual Bias
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
