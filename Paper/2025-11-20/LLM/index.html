<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  UniGen-1.5 Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e59a29298cac3b1340e089b68e60d5e7')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-20-æ›´æ–°"><a href="#2025-11-20-æ›´æ–°" class="headerlink" title="2025-11-20 æ›´æ–°"></a>2025-11-20 æ›´æ–°</h1><h2 id="UniGen-1-5-Enhancing-Image-Generation-and-Editing-through-Reward-Unification-in-Reinforcement-Learning"><a href="#UniGen-1-5-Enhancing-Image-Generation-and-Editing-through-Reward-Unification-in-Reinforcement-Learning" class="headerlink" title="UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning"></a>UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning</h2><p><strong>Authors:Rui Tian, Mingfei Gao, Haiming Gang, Jiasen Lu, Zhe Gan, Yinfei Yang, Zuxuan Wu, Afshin Dehghan</strong></p>
<p>We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.</p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†UniGen-1.5ï¼Œè¿™æ˜¯ä¸€æ¬¾ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç”¨äºé«˜çº§å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚åŸºäºUniGenï¼Œæˆ‘ä»¬å…¨é¢å¢å¼ºäº†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹ï¼ŒåŠ å¼ºäº†å›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶è§£é”äº†å¼ºå¤§çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥ï¼Œé€šè¿‡å…±äº«å¥–åŠ±æ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶æé«˜å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘çš„èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›å›¾åƒç¼–è¾‘æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†è½»é‡çº§çš„ç¼–è¾‘æŒ‡ä»¤å¯¹é½é˜¶æ®µï¼Œè¿™å¤§å¤§æé«˜äº†ç¼–è¾‘æŒ‡ä»¤çš„ç†è§£ï¼Œå¯¹äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æˆåŠŸè‡³å…³é‡è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniGen-1.5åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒUniGen-1.5åœ¨GenEvalå’ŒImgEditä¸Šçš„æ€»ä½“å¾—åˆ†åˆ†åˆ«ä¸º0.89å’Œ4.31ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚BAGELï¼Œå¹¶è¾¾åˆ°äº†ä¸ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-Image-1ï¼‰ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14760v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>UniGen-1.5æ˜¯ä¸€æ¬¾ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé«˜çº§å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚è¯¥æ¨¡å‹åŸºäºUniGenæ„å»ºï¼Œå…¨é¢å¢å¼ºäº†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹ï¼Œæé«˜äº†å›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶è§£é”äº†å¼ºå¤§çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚æå‡ºä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å…±äº«å¥–åŠ±æ¨¡å‹ï¼ŒåŒæ—¶æé«˜å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒUniGen-1.5åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨GenEvalå’ŒImgEditä¸Šçš„æ€»ä½“å¾—åˆ†è¾¾åˆ°äº†è¶…è¶ŠBAGELç­‰æœ€æ–°æ¨¡å‹çš„æ°´å¹³ï¼Œä¸GPT-Image-1ç­‰ä¸“æœ‰æ¨¡å‹çš„è¡¨ç°ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniGen-1.5æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé«˜çº§å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ã€‚</li>
<li>åŸºäºUniGenæ¶æ„ï¼Œå…¨é¢å¢å¼ºäº†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹ã€‚</li>
<li>æå‡ºä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å…±äº«å¥–åŠ±æ¨¡å‹ï¼ŒåŒæ—¶æé«˜å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥è½»é‡çº§çš„ç¼–è¾‘æŒ‡ä»¤å¯¹é½é˜¶æ®µï¼Œæ˜¾è‘—æé«˜äº†å¯¹ç¼–è¾‘æŒ‡ä»¤çš„ç†è§£ï¼Œè¿™æ˜¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆåŠŸçš„å…³é”®ã€‚</li>
<li>UniGen-1.5åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œæ€»ä½“å¾—åˆ†è¶…è¶Šäº†ä¸€äº›æœ€æ–°æ¨¡å‹ã€‚</li>
<li>ä¸ä¸“æœ‰æ¨¡å‹å¦‚GPT-Image-1ç›¸æ¯”ï¼ŒUniGen-1.5çš„æ€§èƒ½è¡¨ç°ç›¸å½“ã€‚</li>
<li>UniGen-1.5åœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢æœ‰ç€å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bcffe7166321ee2720fe0d6d9ccde13" align="middle">
<img src="https://picx.zhimg.com/v2-5f214815f78c95d785997a2538e94d6a" align="middle">
<img src="https://picx.zhimg.com/v2-c5d46179b29a289b72800d25edf60694" align="middle">
<img src="https://picx.zhimg.com/v2-db2b0ca4fdc88f292d1625b6280c4acb" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Vision-Large-Language-Models-Are-Good-Noise-Handlers-in-Engagement-Analysis"><a href="#Vision-Large-Language-Models-Are-Good-Noise-Handlers-in-Engagement-Analysis" class="headerlink" title="Vision Large Language Models Are Good Noise Handlers in Engagement Analysis"></a>Vision Large Language Models Are Good Noise Handlers in Engagement Analysis</h2><p><strong>Authors:Alexander Vedernikov, Puneet Kumar, Haoyu Chen, Tapio SeppÃ¤nen, Xiaobai Li</strong></p>
<p>Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS &#x2F; PAFE with F1 gains of +0.22 &#x2F; +0.06.</p>
<blockquote>
<p>è§†é¢‘æ•°æ®é›†çš„æƒ…æ„Ÿè¯†åˆ«ä¸ä¼ ç»Ÿçš„å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸åŒï¼Œå®ƒå—åˆ°ä¸»è§‚æ ‡ç­¾å’Œå™ªå£°å¯¹æ¨¡å‹æ€§èƒ½çš„é™åˆ¶è€Œé¢ä¸´ç‰¹åˆ«æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœä¸»è§‚å’Œå™ªå£°æƒ…æ„Ÿæ ‡ç­¾å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥ä¼˜åŒ–æ³¨é‡Šå¹¶å¼•å¯¼è®­ç»ƒè¿‡ç¨‹çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨é—®å·æ¥æå–è¡Œä¸ºçº¿ç´¢å¹¶å°†æ•°æ®åˆ’åˆ†ä¸ºé«˜å¯é æ€§å’Œä½å¯é æ€§å­é›†ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§ç»“åˆè¯¾ç¨‹å­¦ä¹ ä¸è½¯æ ‡ç­¾ä¼˜åŒ–çš„è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥å¼•å…¥æ¨¡ç³Šæ ·æœ¬ï¼ŒåŒæ—¶è°ƒæ•´ç›‘ç£ä»¥åæ˜ ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨ä¼˜åŒ–åçš„é«˜å¯é æ€§å­é›†ä¸Šè®­ç»ƒçš„ç»å…¸è®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼Œå¹¶ç»“åˆæˆ‘ä»¬çš„è¯¾ç¨‹ç­–ç•¥è¿›è¡Œå¢å¼ºï¼Œè¡¨ç°å‡ºäº†æ”¹è¿›ï¼Œçªå‡ºäº†ä½¿ç”¨VLMsè§£å†³æ ‡ç­¾ä¸»è§‚æ€§çš„å¥½å¤„ã€‚è¯¥æ–¹æ³•åœ¨è¯¸å¦‚EngageNetï¼ˆå…­ä¸ªåŠŸèƒ½è®¾ç½®ä¸­çš„ä¸‰ä¸ªï¼Œæœ€é«˜æå‡+1.21%ï¼‰å’ŒDREAMS&#x2F;PAFEç­‰æƒ…æ„ŸåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³ï¼ŒF1å¾—åˆ†åˆ†åˆ«æé«˜äº†+0.22&#x2F;+0.06ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14749v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨è§†é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œå‚ä¸åº¦è¯†åˆ«ä¸ä¼ ç»Ÿçš„å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸åŒï¼Œé¢ä¸´ç€ä¸»è§‚æ ‡ç­¾å’Œå™ªå£°å¯¹æ¨¡å‹æ€§èƒ½çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¡†æ¶ï¼Œä»¥ä¼˜åŒ–æ³¨é‡Šå¹¶å¼•å¯¼è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡é—®å·è°ƒæŸ¥æå–è¡Œä¸ºçº¿ç´¢ï¼Œå¹¶å°†æ•°æ®åˆ†ä¸ºé«˜å¯é æ€§å’Œä½å¯é æ€§å­é›†ã€‚æˆ‘ä»¬è¿˜ç»“åˆäº†è¯¾ç¨‹å­¦ä¹ ä¸è½¯æ ‡ç­¾ä¼˜åŒ–ç­–ç•¥ï¼Œé€æ­¥å¼•å…¥æ¨¡ç³Šæ ·æœ¬ï¼ŒåŒæ—¶è°ƒæ•´ç›‘ç£ä»¥åæ˜ ä¸ç¡®å®šæ€§ã€‚åœ¨ä¼˜åŒ–åçš„é«˜å¯é æ€§å­é›†ä¸Šè®­ç»ƒçš„ç»å…¸è®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼Œç»“åˆæˆ‘ä»¬çš„è¯¾ç¨‹ç­–ç•¥ï¼Œè¡¨ç°å‡ºæ”¹è¿›çš„æ•ˆæœï¼Œçªæ˜¾äº†ä½¿ç”¨VLMsè§£å†³æ ‡ç­¾ä¸»è§‚æ€§çš„å¥½å¤„ã€‚è¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„å‚ä¸åº¦åŸºå‡†æµ‹è¯•æ°´å¹³ï¼Œå¦‚EngageNetï¼ˆå…­ä¸ªç‰¹å¾è®¾ç½®ä¸­çš„ä¸‰ä¸ªï¼Œæœ€é«˜æ”¹è¿›+1.21%ï¼‰ï¼Œå¹¶åœ¨DREAMS&#x2F;PAFEä¸Šå®ç°äº†F1å¾—åˆ†çš„+0.22&#x2F;+0.06çš„å¢ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ•°æ®é›†çš„å‚ä¸åº¦è¯†åˆ«é¢ä¸´ä¸»è§‚æ ‡ç­¾å’Œå™ªå£°çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºåˆ©ç”¨è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¡†æ¶æ¥è§£å†³æ ‡ç­¾é—®é¢˜ã€‚</li>
<li>é€šè¿‡é—®å·è°ƒæŸ¥æå–è¡Œä¸ºçº¿ç´¢ï¼Œå°†æ•°æ®åˆ†ä¸ºé«˜å¯é æ€§å’Œä½å¯é æ€§å­é›†ã€‚</li>
<li>ç»“åˆè¯¾ç¨‹å­¦ä¹ ä¸è½¯æ ‡ç­¾ä¼˜åŒ–ç­–ç•¥ï¼Œé€æ­¥å¼•å…¥æ¨¡ç³Šæ ·æœ¬å¹¶è°ƒæ•´ç›‘ç£ã€‚</li>
<li>åœ¨é«˜å¯é æ€§å­é›†ä¸Šè®­ç»ƒçš„ç»å…¸è®¡ç®—æœºè§†è§‰æ¨¡å‹è¡¨ç°æ”¹è¿›ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„å‚ä¸åº¦åŸºå‡†æµ‹è¯•æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db6136119af3da05827455d90f3dd8ab" align="middle">
<img src="https://picx.zhimg.com/v2-120eb48edc37e217b5bd0ccb53a4150d" align="middle">
<img src="https://picx.zhimg.com/v2-b91af1f9d382321e1c46edb4ad7cd9fc" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="When-AI-Democratizes-Exploitation-LLM-Assisted-Strategic-Manipulation-of-Fair-Division-Algorithms"><a href="#When-AI-Democratizes-Exploitation-LLM-Assisted-Strategic-Manipulation-of-Fair-Division-Algorithms" class="headerlink" title="When AI Democratizes Exploitation: LLM-Assisted Strategic Manipulation of Fair Division Algorithms"></a>When AI Democratizes Exploitation: LLM-Assisted Strategic Manipulation of Fair Division Algorithms</h2><p><strong>Authors:Priyanka Verma, Balagopal Unnikrishnan</strong></p>
<p>Fair resource division algorithms, like those implemented in Spliddit platform, have traditionally been considered difficult for the end users to manipulate due to its complexities. This paper demonstrates how Large Language Models (LLMs) can dismantle these protective barriers by democratizing access to strategic expertise. Through empirical analysis of rent division scenarios on Spliddit algorithms, we show that users can obtain actionable manipulation strategies via simple conversational queries to AI assistants. We present four distinct manipulation scenarios: exclusionary collusion where majorities exploit minorities, defensive counterstrategies that backfire, benevolent subsidization of specific participants, and cost minimization coalitions. Our experiments reveal that LLMs can explain algorithmic mechanics, identify profitable deviations, and generate specific numerical inputs for coordinated preference misreportingâ€“capabilities previously requiring deep technical knowledge. These findings extend algorithmic collective action theory from classification contexts to resource allocation scenarios, where coordinated preference manipulation replaces feature manipulation. The implications reach beyond rent division to any domain using algorithmic fairness mechanisms for resource division. While AI-enabled manipulation poses risks to system integrity, it also creates opportunities for preferential treatment of equity deserving groups. We argue that effective responses must combine algorithmic robustness, participatory design, and equitable access to AI capabilities, acknowledging that strategic sophistication is no longer a scarce resource.</p>
<blockquote>
<p>èµ„æºå…¬å¹³åˆ†é…ç®—æ³•ï¼Œå¦‚Splidditå¹³å°æ‰€å®ç°çš„ç®—æ³•ï¼Œç”±äºå…¶å¤æ‚æ€§ï¼Œä¼ ç»Ÿä¸Šè¢«è®¤ä¸ºéš¾ä»¥ä¾›æœ€ç»ˆç”¨æˆ·ä½¿ç”¨ã€‚æœ¬æ–‡å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•é€šè¿‡æ™®åŠæˆ˜ç•¥ä¸“ä¸šçŸ¥è¯†æ¥æ¶ˆé™¤è¿™äº›ä¿æŠ¤å£å’ã€‚é€šè¿‡å¯¹Splidditç®—æ³•ä¸­çš„ç§Ÿé‡‘åˆ†é…æƒ…æ™¯è¿›è¡Œå®è¯åˆ†æï¼Œæˆ‘ä»¬è¡¨æ˜ç”¨æˆ·å¯ä»¥é€šè¿‡å¯¹äººå·¥æ™ºèƒ½åŠ©ç†è¿›è¡Œç®€å•çš„æŸ¥è¯¢æ¥è·å¾—å¯è¡Œçš„æ“çºµç­–ç•¥ã€‚æˆ‘ä»¬æå‡ºäº†å››ç§ä¸åŒçš„æ“çºµæƒ…æ™¯ï¼šå¤šæ•°äººå¯¹å°‘æ•°äººçš„æ’æ–¥æ€§å‹¾ç»“ã€é€‚å¾—å…¶åçš„é˜²å¾¡æ€§åå‡»ç­–ç•¥ã€å¯¹ç‰¹å®šå‚ä¸è€…çš„ä»æ…ˆè¡¥è´´ä»¥åŠæˆæœ¬æœ€å°åŒ–è”ç›Ÿã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLLMå¯ä»¥è§£é‡Šç®—æ³•æœºåˆ¶ã€å‘ç°æœ‰åˆ©åå·®ï¼Œå¹¶ç”Ÿæˆåè°ƒåå¥½è¯¯æŠ¥çš„ç‰¹å®šæ•°å€¼è¾“å…¥â€”â€”è¿™äº›èƒ½åŠ›ä»¥å‰éœ€è¦æ·±åšçš„æŠ€æœ¯çŸ¥è¯†ã€‚è¿™äº›å‘ç°å°†ç®—æ³•é›†ä½“è¡ŒåŠ¨ç†è®ºä»åˆ†ç±»èƒŒæ™¯æ‰©å±•åˆ°èµ„æºåˆ†é…åœºæ™¯ï¼Œå…¶ä¸­åè°ƒåå¥½æ“çºµå–ä»£äº†ç‰¹å¾æ“çºµã€‚å…¶å½±å“ä¸ä»…ä»…å±€é™äºç§Ÿé‡‘åˆ†é…ï¼Œè€Œä¸”å»¶ä¼¸åˆ°ä½¿ç”¨ç®—æ³•å…¬å¹³æœºåˆ¶è¿›è¡Œèµ„æºåˆ†é…çš„ä»»ä½•é¢†åŸŸã€‚è™½ç„¶äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ“çºµå¯¹ç³»ç»Ÿå®Œæ•´æ€§æ„æˆé£é™©ï¼Œä½†å®ƒä¹Ÿä¸ºåº”å¾—åˆ°å…¬å¹³å¾…é‡çš„ç¾¤ä½“æä¾›äº†ä¼˜å¾…çš„æœºä¼šã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæœ‰æ•ˆçš„åº”å¯¹æªæ–½å¿…é¡»ç»“åˆç®—æ³•ç¨³å¥æ€§ã€å‚ä¸æ€§è®¾è®¡å’Œå…¬å¹³çš„AIèƒ½åŠ›è®¿é—®ï¼ŒåŒæ—¶è®¤è¯†åˆ°æˆ˜ç•¥ç²¾ç»†ä¸å†æ˜¯ä¸€ç§ç¨€ç¼ºèµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14722v1">PDF</a> submitted to NeurIPS 2025 workshop on Algorithmic Collective Action</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç®€åŒ–å¤æ‚èµ„æºåˆ†é…ç®—æ³•çš„ä½¿ç”¨éš¾åº¦ï¼Œå¹¶æ­ç¤ºç”¨æˆ·é€šè¿‡ç®€å•çš„å¯¹è¯æŸ¥è¯¢å³å¯è·å¾—ç­–ç•¥æ€§æ“ä½œç­–ç•¥ã€‚é€šè¿‡å¯¹Splidditç®—æ³•ç§Ÿé‡‘åˆ†é…åœºæ™¯çš„å®è¯åˆ†æï¼Œè®ºæ–‡å±•ç¤ºäº†å››ç§ä¸åŒçš„æ“çºµåœºæ™¯ï¼Œå¹¶æŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè§£é‡Šç®—æ³•æœºåˆ¶ã€è¯†åˆ«æœ‰åˆ©åå·®ä»¥åŠç”Ÿæˆåè°ƒåå¥½è¯¯æŠ¥çš„ç‰¹å®šæ•°å€¼è¾“å…¥ã€‚è¿™äº›å‘ç°å°†ç®—æ³•é›†ä½“è¡ŒåŠ¨ç†è®ºä»åˆ†ç±»æƒ…å¢ƒæ‰©å±•åˆ°èµ„æºåˆ†é…åœºæ™¯ï¼Œå¹¶å¯¹ç³»ç»Ÿå®Œæ•´æ€§æå‡ºäº†é£é™©ä¸æœºé‡å¹¶å­˜çš„é—®é¢˜ã€‚è®ºæ–‡å¼ºè°ƒäº†ç»“åˆç®—æ³•ç¨³å¥æ€§ã€å‚ä¸æ€§è®¾è®¡å’Œå…¬å¹³è®¿é—®AIèƒ½åŠ›çš„å¿…è¦å“åº”æ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥æ¶ˆé™¤èµ„æºåˆ†é…ç®—æ³•çš„å¤æ‚æ€§å£å’ï¼Œä½¿ç”¨æˆ·æ›´å®¹æ˜“æ“çºµè¿™äº›ç®—æ³•ã€‚</li>
<li>ç”¨æˆ·é€šè¿‡ç®€å•çš„å¯¹è¯æŸ¥è¯¢å¯ä»¥è·å¾—æˆ˜ç•¥æ€§çš„æ“ä½œç­–ç•¥ã€‚</li>
<li>å››ç§æ“çºµåœºæ™¯è¢«æ­ç¤ºï¼ŒåŒ…æ‹¬å¤šæ•°äººçš„æ’æ–¥å‹¾ç»“ã€é˜²å¾¡ç­–ç•¥çš„é€‚å¾—å…¶åã€ç‰¹å®šå‚ä¸è€…çš„ä»æ…ˆè¡¥è´´ä»¥åŠæˆæœ¬æœ€å°åŒ–è”ç›Ÿã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè§£é‡Šç®—æ³•æœºåˆ¶ï¼Œè¯†åˆ«æœ‰åˆ©åå·®ï¼Œç”Ÿæˆåè°ƒåå¥½è¯¯æŠ¥çš„ç‰¹å®šæ•°å€¼è¾“å…¥ã€‚</li>
<li>è¿™äº›å‘ç°å°†ç®—æ³•é›†ä½“è¡ŒåŠ¨ç†è®ºæ‰©å±•åˆ°èµ„æºåˆ†é…åœºæ™¯ï¼Œå³åè°ƒåå¥½æ“çºµå–ä»£ç‰¹å¾æ“çºµã€‚</li>
<li>AIé©±åŠ¨çš„æ“çºµå¯¹ç³»ç»Ÿå®Œæ•´æ€§æå‡ºäº†é£é™©ï¼Œä½†åŒæ—¶ä¹Ÿä¸ºéœ€è¦å…¬å¹³å¯¹å¾…çš„ç¾¤ä½“æä¾›äº†æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c31e92bb550929465fb7b8d01405372" align="middle">
<img src="https://picx.zhimg.com/v2-09cf786da8dea6350e726757384edf45" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Encoding-and-Understanding-Astrophysical-Information-in-Large-Language-Model-Generated-Summaries"><a href="#Encoding-and-Understanding-Astrophysical-Information-in-Large-Language-Model-Generated-Summaries" class="headerlink" title="Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries"></a>Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries</h2><p><strong>Authors:Kiera McCormick, Rafael MartÃ­nez-Galarza</strong></p>
<p>Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºåœ¨å¤šé¢†åŸŸã€å¤šæ¨¡æ€çš„å¤šä¸ªå±‚é¢ä¸Šçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œç”šè‡³å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚è¿™ä½¿å¾—å…³äºå¦‚ä½•åˆ©ç”¨å®ƒä»¬æ¥ç¼–ç é€šå¸¸ä»…é€šè¿‡ç§‘å­¦æµ‹é‡è·å¾—ä¸”æ¾æ•£åœ°ç¼–ç åœ¨æ–‡æœ¬æè¿°ä¸­çš„ç‰©ç†ä¿¡æ¯çš„ç ”ç©¶é—®é¢˜æµ®å‡ºæ°´é¢ã€‚æˆ‘ä»¬ä»¥å¤©ä½“ç‰©ç†å­¦ä¸ºæµ‹è¯•å¹³å°ï¼Œé€šè¿‡ä¸¤ä¸ªé—®é¢˜æ¥ç ”ç©¶LLMåµŒå…¥æ˜¯å¦èƒ½å¤Ÿç¼–çº‚ä»ç§‘å­¦æµ‹é‡ä¸­è·å¾—çš„ç‰©ç†æ‘˜è¦ç»Ÿè®¡ä¿¡æ¯ï¼š1ï¼‰æç¤ºåœ¨LLMå¦‚ä½•ç¼–çº‚è¿™äº›æ•°é‡æ–¹é¢æ˜¯å¦å‘æŒ¥ä½œç”¨ï¼Ÿä»¥åŠ2ï¼‰åœ¨ç¼–ç æµ‹é‡æ‰€ä»£è¡¨çš„ç‰©ç†å­¦æ–¹é¢ï¼Œè¯­è¨€æ–¹é¢çš„å“ªäº›å› ç´ æœ€ä¸ºé‡è¦ï¼Ÿæˆ‘ä»¬é€šè¿‡ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ä»æ–‡æœ¬ä¸­æå–å¯è§£é‡Šçš„ç‰¹å¾æ¥ç ”ç©¶è¿™ä¸ªé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14685v1">PDF</a> Accepted to the Machine Learning and the Physical Sciences Workshop at NeurIPS 2025, 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºè·¨é¢†åŸŸã€è·¨æ¨¡æ€çš„å¤šå±‚æ¬¡æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å…·å¤‡ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ¢è®¨å¦‚ä½•åˆ©ç”¨è¿™äº›æ¨¡å‹ç¼–ç é€šå¸¸ä»…é€šè¿‡ç§‘å­¦å®éªŒæµ‹é‡å¾—åˆ°çš„ç‰©ç†ä¿¡æ¯ï¼Œä»¥åŠè¿™äº›ä¿¡æ¯çš„æ–‡æœ¬æè¿°æ–¹å¼ã€‚ä»¥å¤©æ–‡å­¦ä¸ºå®éªŒå¹³å°ï¼Œç ”ç©¶ä¸¤ä¸ªé—®é¢˜ï¼šä¸€æ˜¯æç¤ºå¯¹æ¨¡å‹ç¼–ç è¿™äº›ç‰©ç†é‡çš„å½±å“ï¼›äºŒæ˜¯æµ‹é‡æ‰€ä»£è¡¨ç‰©ç†è¯­è¨€çš„å“ªäº›æ–¹é¢æœ€é‡è¦ï¼Ÿæœ¬ç ”ç©¶ä½¿ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ä»æ–‡æœ¬ä¸­æå–å¯è§£é‡Šç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡è·¨é¢†åŸŸã€è·¨æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¼–ç ç‰©ç†ä¿¡æ¯æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åˆ©ç”¨æ–‡æœ¬æè¿°ä¸­çš„ç§‘å­¦æµ‹é‡ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶ä»¥å¤©æ–‡å­¦ä¸ºå®éªŒå¹³å°ï¼Œé€šè¿‡ä¸¤ä¸ªé—®é¢˜å±•å¼€ç ”ç©¶ï¼šæç¤ºå¯¹æ¨¡å‹ç¼–ç ç‰©ç†é‡çš„å½±å“ä»¥åŠæµ‹é‡ç‰©ç†è¯­è¨€çš„æ–¹é¢é‡è¦æ€§ã€‚</li>
<li>ä½¿ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨æ¥æå–æ–‡æœ¬ä¸­çš„å¯è§£é‡Šç‰¹å¾ï¼Œä¸ºç†è§£æ¨¡å‹å¦‚ä½•ç¼–ç ç‰©ç†ä¿¡æ¯æä¾›é‡è¦å·¥å…·ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½é€šè¿‡æ–‡æœ¬æè¿°ä¸­çš„æŸäº›è¯­è¨€ç‰¹å¾æ¥ç¼–ç ç‰©ç†ä¿¡æ¯ï¼Œå¦‚ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›çš„åº”ç”¨ã€‚</li>
<li>æ­¤ç ”ç©¶å¯¹äºå¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®å¤„ç†å’Œè§£æé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨ç§‘å­¦ä¸æŠ€æœ¯é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-879de3a03bb7d092a8f1f79f036b841f" align="middle">
<img src="https://picx.zhimg.com/v2-96b779685952ecb66c0dcb7214c34c7d" align="middle">
<img src="https://picx.zhimg.com/v2-9da800f883a0c46c1c40c8fc4e1f08f4" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SMRC-Aligning-Large-Language-Models-with-Student-Reasoning-for-Mathematical-Error-Correction"><a href="#SMRC-Aligning-Large-Language-Models-with-Student-Reasoning-for-Mathematical-Error-Correction" class="headerlink" title="SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction"></a>SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction</h2><p><strong>Authors:Biaojie Zeng, Min Zhang, Juan Zhou, Fengrui Liu, Ruiyang Huang, Xin Lin</strong></p>
<p>Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the <code>teacher-style</code> correction required in educational settings, \textit{i.e.}, systematically guiding and revising a studentâ€™s problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Mind-Lab-ECNU/SMRC">https://github.com/Mind-Lab-ECNU/SMRC</a>.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶å¾€å¾€ä¼šå‡ºç°æ¨ç†é”™è¯¯ï¼Œå¦‚ä½•è‡ªåŠ¨æ£€æµ‹å’Œçº æ­£è¿™äº›é”™è¯¯å·²æˆä¸ºä¸€ä¸ªé‡è¦ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹çš„è‡ªæˆ‘ä¿®æ­£ä¸Šï¼Œè¿™æ— æ³•è¾¾åˆ°æ•™è‚²ç¯å¢ƒä¸­æ‰€éœ€çš„â€œæ•™å¸ˆå¼â€ä¿®æ­£ï¼Œå³ç³»ç»Ÿåœ°æŒ‡å¯¼å’Œä¿®è®¢å­¦ç”Ÿçš„é—®é¢˜è§£å†³è¿‡ç¨‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†<code>SMRC</code>ï¼ˆå­¦ç”Ÿæ•°å­¦æ¨ç†ä¿®æ­£ï¼Œ\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrectionï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿LLMä¸å­¦ç”Ÿæ¨ç†ç›¸ä¸€è‡´çš„æ–°æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼ŒSMRCå°†å­¦ç”Ÿæ¨ç†åˆ¶å®šä¸ºä¸€ä¸ªå¤šæ­¥éª¤çš„åºåˆ—å†³ç­–é—®é¢˜ï¼Œå¹¶å¼•å…¥è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥æ¢ç´¢æœ€ä½³çš„ä¿®æ­£è·¯å¾„ã€‚ä¸ºäº†é™ä½å¯¹è¿‡ç¨‹çº§å¥–åŠ±è¿›è¡Œæ ‡æ³¨çš„æˆæœ¬ï¼Œæˆ‘ä»¬åˆ©ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰ä»¥LLMä¸ºæŒ‡å¯¼å¹¶ç»“åˆæœ€ç»ˆç­”æ¡ˆè¯„ä¼°æ¥ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œç„¶åé€šè¿‡åå‘ä¼ æ’­æœºåˆ¶å°†è¿™äº›å¥–åŠ±ä¿¡å·åˆ†é…åˆ°ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œå®ç°ç²¾ç»†çš„è¿‡ç¨‹ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜ä¸­æ•°å­¦åŸºå‡†æµ‹è¯•MSEBï¼ˆå¤šè§£è¯¯å·®åŸºå‡†ï¼‰ï¼ŒåŒ…å«158ä¸ªå®ä¾‹ï¼ŒåŒ…æ‹¬é—®é¢˜æè¿°ã€å­¦ç”Ÿè§£å†³æ–¹æ¡ˆå’Œæ­£ç¡®çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä»¥è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§å’Œæ­£ç¡®æ­¥éª¤ä¿ç•™ç‡ä¸ºä¸­å¿ƒçš„åŒé‡è¯„ä¼°åè®®ï¼Œä¸ºæ•™è‚²é€‚ç”¨æ€§æä¾›äº†å…¨é¢çš„è¡¡é‡æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒSMRCåœ¨å…¬å¼€æ•°æ®é›†ï¼ˆProcessBenchå’ŒMR-GSM8Kï¼‰ä»¥åŠæˆ‘ä»¬çš„MSEBä¸Šçš„æœ‰æ•ˆæ€§å’Œæ•´ä½“æ€§èƒ½å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Mind-Lab-ECNU/SMRC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Mind-Lab-ECNU/SMRCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14684v1">PDF</a> 13 pages, 3 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶å­˜åœ¨æ¨ç†é”™è¯¯çš„é—®é¢˜ï¼Œå¦‚ä½•è‡ªåŠ¨æ£€æµ‹å’Œçº æ­£è¿™äº›é”™è¯¯å·²æˆä¸ºé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹å†…éƒ¨çš„è‡ªæˆ‘çº æ­£ï¼Œç¼ºä¹æ•™è‚²ç¯å¢ƒä¸­æ‰€éœ€çš„â€œæ•™å¸ˆå¼â€çº æ­£ï¼Œå³ç³»ç»Ÿåœ°æŒ‡å¯¼å’Œä¿®è®¢å­¦ç”Ÿçš„é—®é¢˜è§£å†³è¿‡ç¨‹ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†\texttt{SMRC}æ–¹æ³•ï¼Œä½¿LLMsä¸å­¦ç”Ÿæ¨ç†ç›¸ä¸€è‡´ã€‚å…·ä½“è€Œè¨€ï¼Œ\texttt{SMRC}å°†å­¦ç”Ÿæ¨ç†åˆ¶å®šä¸ºä¸€ä¸ªå¤šæ­¥éª¤çš„åºåˆ—å†³ç­–é—®é¢˜ï¼Œå¹¶å¼•å…¥è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥æ¢ç´¢æœ€ä½³çš„çº æ­£è·¯å¾„ã€‚ä¸ºå‡å°‘æ ‡æ³¨è¿‡ç¨‹çº§å¥–åŠ±çš„æˆæœ¬ï¼Œæˆ‘ä»¬åˆ©ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰ç”±LLMså¼•å¯¼å¹¶ç»“åˆæœ€ç»ˆç­”æ¡ˆè¯„ä¼°æ¥ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œè¿™äº›ä¿¡å·é€šè¿‡åå‘ä¼ æ’­æœºåˆ¶è¢«åˆ†é…åˆ°ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œå®ç°ç²¾ç»†çš„è¿‡ç¨‹ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†é«˜ä¸­æ•°å­¦çš„åŸºå‡†æµ‹è¯•MSEBï¼ˆå¤šè§£è¯¯å·®åŸºå‡†ï¼‰ï¼ŒåŒ…å«158ä¸ªåŒ…å«é—®é¢˜é™ˆè¿°ã€å­¦ç”Ÿè§£å†³æ–¹æ¡ˆå’Œæ­£ç¡®æ¨ç†æ­¥éª¤çš„å®ä¾‹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä»¥è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§å’Œæ­£ç¡®æ­¥éª¤ä¿ç•™ç‡ä¸ºä¸­å¿ƒçš„åŒé‡è¯„ä¼°åè®®ï¼Œä¸ºæ•™è‚²åº”ç”¨æä¾›äº†å…¨é¢çš„è¡¡é‡æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œ\texttt{SMRC}åœ¨å…¬å¼€æ•°æ®é›†ï¼ˆProcessBenchå’ŒMR-GSM8Kï¼‰ä»¥åŠæˆ‘ä»¬çš„MSEBåŸºå‡†æµ‹è¯•ä¸Šï¼Œå…¶æœ‰æ•ˆæ€§å’Œæ•´ä½“æ€§èƒ½å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Mind-Lab-ECNU/SMRC%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Mind-Lab-ECNU/SMRCä¸Šè·å¾—ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMsåœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶å­˜åœ¨æ¨ç†é”™è¯¯çš„é—®é¢˜ï¼Œè‡ªåŠ¨æ£€æµ‹å’Œçº æ­£è¿™äº›é”™è¯¯æ˜¯é‡è¦ç ”ç©¶æ–¹å‘ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ¨¡å‹å†…éƒ¨çš„è‡ªæˆ‘çº æ­£ï¼Œç¼ºä¹æ•™è‚²ç¯å¢ƒä¸­éœ€è¦çš„æ•™å¸ˆå¼çº æ­£ã€‚</li>
<li>\texttt{SMRC}æ–¹æ³•é€šè¿‡å°†å­¦ç”Ÿæ¨ç†åˆ¶å®šä¸ºåºåˆ—å†³ç­–é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¢ç´¢æœ€ä½³çº æ­£è·¯å¾„æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>\texttt{SMRC}åˆ©ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢å¼•å¯¼å¹¶ç»“åˆæœ€ç»ˆç­”æ¡ˆè¯„ä¼°æ¥ç”Ÿæˆå¥–åŠ±ä¿¡å·ï¼Œå®ç°ç²¾ç»†çš„è¿‡ç¨‹ç›‘ç£ã€‚</li>
<li>æˆ‘ä»¬æ„å»ºäº†é«˜ä¸­æ•°å­¦çš„MSEBåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§å’Œæ­£ç¡®æ€§ã€‚</li>
<li>\texttt{SMRC}åœ¨å…¬å¼€æ•°æ®é›†å’ŒMSEBåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d911db7e2a2f9930ac83b75d73226170" align="middle">
<img src="https://picx.zhimg.com/v2-ca6532240cce89f6a10b7e7a53a19440" align="middle">
<img src="https://picx.zhimg.com/v2-f92e1a20bdc2b891e88bb6a96c8aff53" align="middle">
<img src="https://picx.zhimg.com/v2-0c41c02d963cb9cd30af7a6ce7695246" align="middle">
<img src="https://picx.zhimg.com/v2-1a9d13a59b7380eb7ee23a00a2d5caf2" align="middle">
<img src="https://picx.zhimg.com/v2-676bc2ec0dee967b24232c622752071c" align="middle">
<img src="https://picx.zhimg.com/v2-252e9aeee641c03ca4c6665fa52035b4" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AutoTool-Efficient-Tool-Selection-for-Large-Language-Model-Agents"><a href="#AutoTool-Efficient-Tool-Selection-for-Large-Language-Model-Agents" class="headerlink" title="AutoTool: Efficient Tool Selection for Large Language Model Agents"></a>AutoTool: Efficient Tool Selection for Large Language Model Agents</h2><p><strong>Authors:Jingyi Jia, Qinbin Li</strong></p>
<p>Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å·¥å…·é€šè¿‡åˆ©ç”¨LLMçš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œå·²é€æ¸å‘å±•ä¸ºè‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå½“å‰ä»£ç†æ¡†æ¶çš„ä¸»è¦ç“¶é¢ˆåœ¨äºå·¥å…·é€‰æ‹©çš„é«˜æ¨ç†æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒReActè¿™æ ·çš„æ–¹æ³•ä¸­ï¼Œéœ€è¦åå¤è°ƒç”¨LLMæ¥ç¡®å®šæ¯ä¸€æ­¥åº”ä½¿ç”¨å“ªä¸ªå·¥å…·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AutoToolï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå›¾çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä¸€ä¸ªå…³é”®çš„ç»éªŒè§‚å¯Ÿæ¥ç»•è¿‡é‡å¤çš„LLMæ¨ç†ï¼šå·¥å…·ä½¿ç”¨æƒ¯æ€§â€”â€”å·¥å…·è°ƒç”¨çš„è¶‹åŠ¿éµå¾ªå¯é¢„æµ‹çš„é¡ºåºæ¨¡å¼ã€‚AutoToolä»å†å²ä»£ç†è½¨è¿¹æ„å»ºä¸€ä¸ªæœ‰å‘å›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å·¥å…·ï¼Œè¾¹æ•æ‰è½¬æ¢æ¦‚ç‡ï¼Œæœ‰æ•ˆåœ°å¯¹å·¥å…·é€‰æ‹©ä¸­çš„æƒ¯æ€§è¿›è¡Œå»ºæ¨¡ã€‚å®ƒè¿›ä¸€æ­¥æ•´åˆå‚æ•°çº§åˆ«çš„ä¿¡æ¯æ¥ä¼˜åŒ–å·¥å…·è¾“å…¥ç”Ÿæˆã€‚é€šè¿‡éå†è¿™ç§ç»“æ„åŒ–è¡¨ç¤ºï¼ŒAutoToolèƒ½å¤Ÿé«˜æ•ˆé€‰æ‹©å·¥å…·å’Œå‚æ•°ï¼Œå¯¹LLMæ¨ç†çš„ä¾èµ–åº¦é™åˆ°æœ€ä½ã€‚åœ¨å¤šç§ä»£ç†ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAutoToolå°†æ¨ç†æˆæœ¬é™ä½äº†é«˜è¾¾30%ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„ä»»åŠ¡å®Œæˆç‡ï¼Œä¸ºæ¨ç†ç¹é‡çš„æ¡†æ¶æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„å¢å¼ºã€‚æˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†å°†ç»Ÿè®¡ç»“æ„æ•´åˆåˆ°LLMä»£ç†è®¾è®¡ä¸­ä»¥æé«˜æ•ˆç‡è€Œä¸ç‰ºç‰²æ€§èƒ½çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14650v1">PDF</a> Accepted by AAAI 2026, 18 pages, 11 figures, Code: <a target="_blank" rel="noopener" href="https://github.com/jiajingyyyyyy/AutoTool">https://github.com/jiajingyyyyyy/AutoTool</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†é€šè¿‡åˆ©ç”¨LLMçš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›è‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡ï¼Œå·²æˆä¸ºå¼ºå¤§çš„å·¥å…·ã€‚ç„¶è€Œï¼Œå½“å‰ä»£ç†æ¡†æ¶ä¸­å­˜åœ¨çš„ä¸»è¦ç“¶é¢ˆåœ¨äºå·¥å…·é€‰æ‹©çš„é«˜æ¨ç†æˆæœ¬ï¼Œå°¤å…¶æ˜¯åœ¨ReActç­‰æ–¹æ³•ä¸­ï¼Œä¼šåå¤è°ƒç”¨LLMæ¥ç¡®å®šæ¯ä¸€æ­¥åº”ä½¿ç”¨å“ªä¸ªå·¥å…·ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºAutoToolï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå›¾çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å·¥å…·ä½¿ç”¨æƒ¯æ€§è¿™ä¸€å…³é”®è§‚å¯Ÿç»“æœæ¥ç»•è¿‡é‡å¤çš„LLMæ¨ç†ã€‚AutoToolé€šè¿‡æ„å»ºæœ‰å‘å›¾æ¥æ•æ‰å·¥å…·é€‰æ‹©çš„é¡ºåºæ¨¡å¼ï¼Œå…¶ä¸­èŠ‚ç‚¹è¡¨ç¤ºå·¥å…·ï¼Œè¾¹è¡¨ç¤ºè½¬æ¢æ¦‚ç‡ã€‚é€šè¿‡éå†æ­¤ç»“æ„åŒ–è¡¨ç¤ºï¼ŒAutoToolå¯é«˜æ•ˆé€‰æ‹©å·¥å…·å’Œå‚æ•°ï¼Œå¯¹LLMæ¨ç†çš„ä¾èµ–åº¦é™åˆ°æœ€ä½ã€‚åœ¨å¤šç§ä»£ç†ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAutoToolå°†æ¨ç†æˆæœ¬é™ä½äº†30%ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰æ€§çš„ä»»åŠ¡å®Œæˆç‡ï¼Œä¸ºæ¨ç†å¯†é›†å‹æ¡†æ¶æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„å¢å¼ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä»£ç†å·²æˆä¸ºè‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡çš„é‡è¦å·¥å…·ï¼Œä½†å­˜åœ¨å·¥å…·é€‰æ‹©çš„é«˜æ¨ç†æˆæœ¬é—®é¢˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å¦‚ReActåå¤è°ƒç”¨LLMæ¥ç¡®å®šå·¥å…·é€‰æ‹©ï¼Œå¯¼è‡´æ•ˆç‡è¾ƒä½ã€‚</li>
<li>AutoToolæ˜¯ä¸€ä¸ªåŸºäºå›¾çš„æ¡†æ¶ï¼Œé€šè¿‡æ•æ‰å·¥å…·ä½¿ç”¨çš„é¡ºåºæ¨¡å¼æ¥ç»•è¿‡é‡å¤çš„LLMæ¨ç†ã€‚</li>
<li>AutoToolé€šè¿‡æ„å»ºæœ‰å‘å›¾æ¥è¡¨ç¤ºå·¥å…·å’Œå‚æ•°ï¼Œæœ‰æ•ˆå»ºæ¨¡å·¥å…·é€‰æ‹©æƒ¯æ€§ã€‚</li>
<li>é€šè¿‡éå†ç»“æ„åŒ–è¡¨ç¤ºï¼ŒAutoToolå¯é«˜æ•ˆé€‰æ‹©å·¥å…·å’Œå‚æ•°ï¼Œå‡å°‘å¯¹LLMæ¨ç†çš„ä¾èµ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒAutoToolèƒ½æ˜¾è‘—é™ä½æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡å®Œæˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fefa04a2c46e19fbbee469599850beb" align="middle">
<img src="https://picx.zhimg.com/v2-81a51b25ce91875cad1a2b89c7c3ec7e" align="middle">
<img src="https://picx.zhimg.com/v2-3496eb1fbf801efbb3bf9b66489da9bf" align="middle">
<img src="https://picx.zhimg.com/v2-c8f075ee0c424f7d79e66d53b89f8c88" align="middle">
<img src="https://picx.zhimg.com/v2-42c76b6b578f95415469e7baa94f5e72" align="middle">
<img src="https://picx.zhimg.com/v2-d209a3f04482afdefedee98d5ccc760c" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Specialized-Large-Language-Model-for-Clinical-Reasoning-and-Diagnosis-in-Rare-Diseases"><a href="#A-Specialized-Large-Language-Model-for-Clinical-Reasoning-and-Diagnosis-in-Rare-Diseases" class="headerlink" title="A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases"></a>A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases</h2><p><strong>Authors:Tao Yang, Dandan Huang, Yunting Lin, Pengfei Wu, Zhikun Wu, Gangyuan Ma, Yulan Lu, Xinran Dong, Dingpeng Li, Junshuang Ge, Zhiyan Zhang, Xuanzhao Huang, Wenyan Nong, Yao Zhou, Hui Tang, Hongxi Yang, Shijie Zhang, Juan Li, Xiaojun Cao, Lin Yang, Xia Gao, Kaishou Xu, Xiaoqiong Gu, Wen Zhang, Huimin Xia, Li Liu, Wenhao Zhou, Mulin Jun Li</strong></p>
<p>Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general&#x2F;medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.</p>
<blockquote>
<p>ç½•è§ç–¾ç—…å½±å“å…¨çƒæ•°äº¿äººï¼Œä½†è¯Šæ–­è¿‡ç¨‹å¾€å¾€é•¿è¾¾æ•°å¹´ã€‚ä¼ ç»Ÿç®¡é“å°†å˜ˆæ‚çš„è¯æ®æå–ä¸ä¸‹æ¸¸æ¨ç†è¯Šæ–­è§£è€¦ï¼Œè€Œé€šç”¨&#x2F;åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´ç°å®ä¸–ç•Œç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ç¨€ç¼ºã€é¢†åŸŸçŸ¥è¯†è¿‡æ—¶ä»¥åŠè™šæ„ç­‰é—®é¢˜ã€‚æˆ‘ä»¬æ”¶é›†äº†å¤§é‡é¢†åŸŸä¸“ä¸šåŒ–çš„ä¸´åºŠè¯­æ–™åº“å’Œç»ä¸´åºŠåŒ»ç”ŸéªŒè¯çš„æ¨ç†é›†ï¼Œå¹¶é€šè¿‡åˆ†é˜¶æ®µæŒ‡ä»¤è°ƒæ•´ã€æ€ç»´é“¾å­¦ä¹ å’Œå›¾åŸºæ£€ç´¢å¼€å‘å‡ºäº†RareSeek R1ã€‚åœ¨å¤šä¸­å¿ƒç”µå­å¥åº·è®°å½•å™è¿°å’Œå…¬å…±åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRareSeek R1è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€ç¨³å¥çš„é€šç”¨æ€§å’Œåœ¨å˜ˆæ‚æˆ–é‡å è¡¨å‹ä¸‹çš„ç¨³å®šæ€§ã€‚å½“å™äº‹ä¸ä¼˜å…ˆå˜ä½“é…å¯¹æ—¶ï¼Œå¢å¼ºæ£€ç´¢ä¼šè·å¾—æœ€å¤§æ”¶ç›Šï¼Œé€šè¿‡è§£å†³æ­§ä¹‰å’Œå¯¹å€™é€‰è€…çš„æœºåˆ¶å¯¹é½ã€‚äººç±»ç ”ç©¶è¡¨æ˜ï¼Œå…¶è¡¨ç°ä¸ç»éªŒä¸°å¯Œçš„åŒ»ç”Ÿç›¸å½“ï¼Œå¹¶ä¸”åœ¨è¾…åŠ©ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºæŒç»­çš„ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€æ˜çš„æ¨ç†çªå‡ºäº†è®¸å¤šæ­£ç¡®è¯Šæ–­æ‰€ä¾èµ–çš„å†³å®šæ€§éè¡¨å‹è¯æ®ï¼ˆä¸­ä½æ•°ä¸º23.1%ï¼Œä¾‹å¦‚æˆåƒã€å¹²é¢„ã€åŠŸèƒ½æµ‹è¯•ï¼‰ã€‚è¿™é¡¹å·¥ä½œæ¨è¿›äº†ä¸€ç§ä»¥å™äº‹ä¸ºä¸»ã€çŸ¥è¯†æ•´åˆçš„æ¨ç†èŒƒå¼ï¼Œç¼©çŸ­äº†è¯Šæ–­è¿‡ç¨‹ï¼Œå¹¶å®ç°äº†å¯å®¡æ ¸çš„ã€ä¸´åºŠä¸Šå¯è½¬åŒ–çš„å†³ç­–æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14638v1">PDF</a> 50 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRareSeek R1çš„æ–°ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡å¤§å‹ä¸“æœ‰ä¸´åºŠè¯­æ–™åº“å’ŒåŒ»ç”ŸéªŒè¯çš„æ¨ç†é›†çš„å¼€å‘ï¼Œé‡‡ç”¨åˆ†é˜¶æ®µæŒ‡ä»¤è°ƒæ•´ã€æ€ç»´é“¾å­¦ä¹ å’Œå›¾æ£€ç´¢æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ç½•è§ç–¾ç—…è¯Šæ–­éš¾é¢˜ã€‚è¯¥ç³»ç»Ÿåœ¨å¤šä¸­å¿ƒç”µå­å¥åº·è®°å½•å…¬å…±åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œèƒ½å¤Ÿåœ¨æœ‰å™ªå£°æˆ–é‡å ç°è±¡çš„æ¡ä»¶ä¸‹å®ç°å‡†ç¡®çš„è¯Šæ–­ã€‚äººç±»ç ”ç©¶è¯æ˜ï¼Œè¯¥ç³»ç»Ÿä¸ç»éªŒä¸°å¯Œçš„åŒ»ç”Ÿçš„æ€§èƒ½ç›¸å½“ï¼Œå¹¶ä¸”åœ¨è¾…åŠ©ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºä¸€è‡´çš„å¢ç›Šã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿå…·æœ‰é€æ˜çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿçªå‡ºè®¸å¤šæ­£ç¡®è¯Šæ–­æ‰€ä¾æ®çš„éå†³å®šæ€§è¯æ®ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œæ¨è¿›äº†ä¸€ç§ä»¥å™äº‹ä¸ºä¸»ã€æ•´åˆçŸ¥è¯†çš„æ¨ç†æ¨¡å¼ï¼Œç¼©çŸ­äº†è¯Šæ–­è¿‡ç¨‹ï¼Œå¹¶ä¸ºå¯å®¡è®¡çš„ã€å¯ä¸´åºŠè½¬åŒ–çš„å†³ç­–æ”¯æŒæä¾›äº†å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RareSeek R1ç³»ç»Ÿæ—¨åœ¨è§£å†³ç½•è§ç–¾ç—…çš„è¯Šæ–­éš¾é¢˜ï¼Œé€šè¿‡å¤„ç†ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰æ¥å®ç°é«˜æ•ˆä¸”å‡†ç¡®çš„è¯Šæ–­ã€‚</li>
<li>è¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è§„æ¨¡ä¸“æœ‰ä¸´åºŠè¯­æ–™åº“å’ŒåŒ»ç”ŸéªŒè¯çš„æ¨ç†é›†è¿›è¡Œå¼€å‘ï¼Œä»¥å¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚</li>
<li>RareSeek R1é‡‡ç”¨äº†åˆ†é˜¶æ®µæŒ‡ä»¤è°ƒæ•´ã€æ€ç»´é“¾å­¦ä¹ å’Œå›¾æ£€ç´¢æŠ€æœ¯ï¼Œæé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>åœ¨å¤šä¸­å¿ƒEHRå™äº‹å’Œå…¬å…±åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRareSeek R1è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„è¡¨ç°ã€‚</li>
<li>äººç±»ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿçš„æ€§èƒ½ä¸ç»éªŒä¸°å¯Œçš„åŒ»ç”Ÿç›¸å½“ï¼Œå¹¶åœ¨è¾…åŠ©ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºä¸€è‡´çš„å¢ç›Šã€‚</li>
<li>è¯¥ç³»ç»Ÿå…·æœ‰é€æ˜çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¼ºè°ƒéå†³å®šæ€§è¯æ®åœ¨è¯Šæ–­è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a0eee5adb50714d453a4492a537ad56" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Failure-to-Mix-Large-language-models-struggle-to-answer-according-to-desired-probability-distributions"><a href="#Failure-to-Mix-Large-language-models-struggle-to-answer-according-to-desired-probability-distributions" class="headerlink" title="Failure to Mix: Large language models struggle to answer according to desired probability distributions"></a>Failure to Mix: Large language models struggle to answer according to desired probability distributions</h2><p><strong>Authors:Ivy Yuqian Yang, David Yu Zhang</strong></p>
<p>Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of â€œ1â€ 49% of the time produces an answer of â€œ0â€ nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.</p>
<blockquote>
<p>ç§‘å­¦åˆ›æ„çš„äº§ç”Ÿå’Œé€‰æ‹©éœ€è¦éµå¾ªç›®æ ‡æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œæ¢ç´¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½“å‰çš„AIåŸºå‡†æµ‹è¯•æ‹¥æœ‰å®¢è§‚æ­£ç¡®ç­”æ¡ˆï¼Œé€šè¿‡é’ˆå¯¹è¿™äº›åŸºå‡†æµ‹è¯•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šæŠ‘åˆ¶æ¦‚ç‡æ€§æ¢ç´¢ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿå®éªŒï¼Œè¦æ±‚LLMéµå¾ªç®€å•æ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿè¾“å‡ºï¼Œå¹¶å‘ç°æ‰€æœ‰æµ‹è¯•è¿‡çš„ç°ä»£LLMéƒ½ä¸¥é‡æ— æ³•éµå¾ªè¿™äº›åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œè¦æ±‚äº§ç”Ÿè¾“å‡ºâ€œ1â€çš„æ¦‚ç‡ä¸º49%ï¼Œä½†å®é™…ä¸Šäº§ç”Ÿç­”æ¡ˆâ€œ0â€çš„æ¦‚ç‡æ¥è¿‘100%ã€‚è¿™ç§ç±»ä¼¼äºé˜¶è·ƒå‡½æ•°çš„è¡Œä¸ºï¼Œå³ä½¿äº§ç”Ÿå…·æœ‰ç¨å¾®æœ€é«˜æ¦‚ç‡çš„è¾“å‡ºï¼Œä¹Ÿä¼šè¿‘ä¹ä¸“å±åœ°ç”Ÿæˆè¾“å‡ºï¼Œç”šè‡³ä¼šè¦†ç›–æ‰LLMå†…éƒ¨å­˜åœ¨çš„å¼ºçƒˆåè§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14630v1">PDF</a> 13 pages, 6 figures. Code and reproducibility package: <a target="_blank" rel="noopener" href="https://github.com/BiostateAIresearch/failure-to-mix">https://github.com/BiostateAIresearch/failure-to-mix</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†ç§‘å­¦æ€æƒ³äº§ç”Ÿä¸é€‰æ‹©è¿‡ç¨‹ä¸­æ¦‚ç‡æ€§æ¢ç´¢çš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºå½“å‰AIåŸºå‡†æµ‹è¯•ä¸å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•å¿½ç•¥äº†è¿™ä¸€ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼Œç°ä»£LLMsæ— æ³•éµå¾ªç®€å•çš„æ¦‚ç‡åˆ†å¸ƒæŒ‡ä»¤ï¼Œå¾€å¾€ç”Ÿæˆå…·æœ‰æœ€é«˜æ¦‚ç‡çš„è¾“å‡ºï¼Œå¿½ç•¥äº†å…¶ä»–å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç§‘å­¦æ€æƒ³ç”Ÿæˆä¸é€‰æ‹©éœ€è¦éµå¾ªç›®æ ‡æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œæ¢ç´¢ã€‚</li>
<li>å½“å‰AIåŸºå‡†æµ‹è¯•é€šå¸¸å…·æœ‰å®¢è§‚æ­£ç¡®ç­”æ¡ˆï¼Œè¿™å¯¼è‡´LLMè®­ç»ƒè¿‡ç¨‹ä¸­ç¼ºä¹æ¦‚ç‡æ€§æ¢ç´¢ã€‚</li>
<li>LLMsåœ¨éµå¾ªç®€å•æ¦‚ç‡åˆ†å¸ƒæŒ‡ä»¤æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>LLMså€¾å‘äºç”Ÿæˆå…·æœ‰æœ€é«˜æ¦‚ç‡çš„è¾“å‡ºï¼Œå¿½ç•¥å…¶ä»–å¯èƒ½æ€§ã€‚</li>
<li>è¿™ç§è¡Œä¸ºç”šè‡³èƒ½è¦†ç›–LLMçš„å†…ç½®åè§ã€‚</li>
<li>å¿½è§†æ¦‚ç‡æ€§æ¢ç´¢å¯èƒ½å½±å“LLMåœ¨ç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4440015c2c8c4c6755e9291cc27c65ce" align="middle">
<img src="https://picx.zhimg.com/v2-d113b20218405ffd8d7f98d9aaa1dddc" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="O3SLM-Open-Weight-Open-Data-and-Open-Vocabulary-Sketch-Language-Model"><a href="#O3SLM-Open-Weight-Open-Data-and-Open-Vocabulary-Sketch-Language-Model" class="headerlink" title="O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model"></a>O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model</h2><p><strong>Authors:Rishi Gupta, Mukilan Karuppasamy, Shyam Marjit, Aditay Tripathi, Anirban Chakraborty</strong></p>
<p>While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.</p>
<blockquote>
<p>éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œå®ƒä»¬å¯¹æŠ½è±¡è§†è§‰è¾“å…¥çš„è§£è¯»èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®ƒä»¬åœ¨ç†è§£æ‰‹ç»˜è‰å›¾æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œè‰å›¾æ˜¯ä¸€ç§ç›´è§‚çš„è¡¨è¾¾æ¦‚å¿µçš„æ‰‹æ®µï¼Œè¿™äº›æ¦‚å¿µå¾ˆéš¾ç”¨æ–‡å­—æ¥æè¿°ã€‚æˆ‘ä»¬å°†ä¸»è¦çš„ç“¶é¢ˆç¡®å®šä¸ºç¼ºä¹ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å»ºæ¨¡è‰å›¾ã€å†™å®å›¾åƒå’Œç›¸åº”è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®è´¡çŒ®ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªå…¨æ–°çš„å¤§è§„æ¨¡å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œæ—¨åœ¨ä¿ƒè¿›é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„O3SLMå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚åœ¨å¤šä¸ªåŸºäºè‰å›¾çš„ä»»åŠ¡ä¸Šçš„ç»¼åˆè¯„ä¼°åŒ…æ‹¬ï¼šï¼ˆaï¼‰ç›®æ ‡å®šä½ï¼Œï¼ˆbï¼‰è®¡æ•°ï¼Œï¼ˆcï¼‰å›¾åƒæ£€ç´¢ï¼ˆå³SBIRå’Œç²¾ç»†ç²’åº¦SBIRï¼‰ï¼Œä»¥åŠï¼ˆdï¼‰è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼›åœ¨èå…¥QuickDraw!ã€Sketchyå’ŒTu Berlinè¿™ä¸‰ä¸ªç°æœ‰çš„è‰å›¾æ•°æ®é›†ä»¥åŠæˆ‘ä»¬ç”Ÿæˆçš„SketchVCLæ•°æ®é›†çš„åŒæ—¶ï¼Œè¡¨æ˜O3SLMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è‰å›¾ç†è§£å’Œæ¨ç†æ–¹é¢å¤§å¹…è¶…è¶Šäº†ç°æœ‰çš„LVLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14368v1">PDF</a> Accepted to AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨è§£è¯»æŠ½è±¡è§†è§‰è¾“å…¥æ–¹é¢ä»å­˜åœ¨å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£æ‰‹ç»˜è‰å›¾æ–¹é¢ã€‚æœ¬æ–‡è¯†åˆ«å‡ºä¸»è¦ç“¶é¢ˆåœ¨äºç¼ºä¹ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å»ºæ¨¡è‰å›¾ã€å†™å®å›¾åƒå’Œç›¸åº”è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡åšå‡ºäº†ä¸¤é¡¹é‡è¦è´¡çŒ®ï¼šä¸€æ˜¯æ¨å‡ºäº†æ–°çš„å¤§è§„æ¨¡å›¾åƒ-è‰å›¾-æŒ‡ä»¤ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œç”¨äºä¿ƒè¿›é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ï¼›äºŒæ˜¯åŸºäºè¯¥æ•°æ®é›†è®­ç»ƒäº†O3SLMæ¨¡å‹ã€‚åœ¨å¤šä¸ªåŸºäºè‰å›¾çš„ä»»åŠ¡ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒO3SLMåœ¨å®ç°è‰å›¾ç†è§£ã€è®¡æ•°ã€å›¾åƒæ£€ç´¢å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰LVLMæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨è§£è¯»æŠ½è±¡è§†è§‰è¾“å…¥ï¼Œå°¤å…¶æ˜¯æ‰‹ç»˜è‰å›¾æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ä¸»è¦ç“¶é¢ˆåœ¨äºç¼ºä¹èƒ½å¤ŸåŒæ—¶å»ºæ¨¡è‰å›¾ã€å†™å®å›¾åƒå’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†æ–°çš„å¤§è§„æ¨¡å›¾åƒ-è‰å›¾-æŒ‡ä»¤æ•°æ®é›†ã€‚</li>
<li>O3SLMæ¨¡å‹åŸºäºè¯¥æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨æé«˜LVLMåœ¨è‰å›¾ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºäºè‰å›¾çš„ä»»åŠ¡ä¸Šï¼ŒO3SLMè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>O3SLMæ¨¡å‹åœ¨è‰å›¾ç†è§£ã€è®¡æ•°ã€å›¾åƒæ£€ç´¢å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰LVLMæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f93a04671f7cc60ec478111406a84703" align="middle">
<img src="https://picx.zhimg.com/v2-2eeff7fa697da6d186e2a2db364b8db0" align="middle">
<img src="https://picx.zhimg.com/v2-e59a29298cac3b1340e089b68e60d5e7" align="middle">
<img src="https://picx.zhimg.com/v2-b25e24627b42829d6d9b6a19465880f9" align="middle">
<img src="https://picx.zhimg.com/v2-1873d8f09b93d300b4918a7a87905468" align="middle">
<img src="https://picx.zhimg.com/v2-4b3c740b21db62bb37cfdbd6062f0629" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PathMind-A-Retrieve-Prioritize-Reason-Framework-for-Knowledge-Graph-Reasoning-with-Large-Language-Models"><a href="#PathMind-A-Retrieve-Prioritize-Reason-Framework-for-Knowledge-Graph-Reasoning-with-Large-Language-Models" class="headerlink" title="PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models"></a>PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models</h2><p><strong>Authors:Yu Liu, Xixun Lin, Yanmin Shang, Yangxi Li, Shi Wang, Yanan Cao</strong></p>
<p>Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a â€œRetrieve-Prioritize-Reasonâ€ paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.</p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆKGRï¼‰æ˜¯é€šè¿‡åœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œé€»è¾‘æ¨æ–­æ¥æ¨æ–­æ–°çŸ¥è¯†çš„ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚å°½ç®¡å‰æ™¯å¹¿é˜”ï¼Œä½†åŸºäºLLMçš„KGRæ–¹æ³•ä»ç„¶é¢ä¸´ä¸¤ä¸ªå…³é”®çš„å±€é™æ€§ã€‚é¦–å…ˆï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¼šä¸åŠ åŒºåˆ«åœ°æå–æ¨ç†è·¯å¾„ï¼Œè€Œä¸è¯„ä¼°å®ƒä»¬çš„ä¸åŒé‡è¦æ€§ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥æ— å…³çš„å™ªå£°ï¼Œè¯¯å¯¼LLMã€‚å…¶æ¬¡ï¼Œè™½ç„¶è®¸å¤šæ–¹æ³•åˆ©ç”¨LLMæ¥åŠ¨æ€æ¢ç´¢æ½œåœ¨çš„æ¨ç†è·¯å¾„ï¼Œä½†éœ€è¦å¤§é‡çš„æ£€ç´¢è¦æ±‚å’Œé¢‘ç¹çš„LLMè°ƒç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PathMindï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é€‰æ‹©æ€§å¼•å¯¼LLMèµ°é‡è¦çš„æ¨ç†è·¯å¾„ï¼Œæ¥æå‡å¿ å®å’Œå¯è§£é‡Šçš„æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒPathMindéµå¾ªâ€œæ£€ç´¢-ä¼˜å…ˆæ’åº-æ¨ç†â€çš„æ¨¡å¼ã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡æ£€ç´¢æ¨¡å—ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢æŸ¥è¯¢å­å›¾ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªè·¯å¾„ä¼˜å…ˆçº§æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä½¿ç”¨ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥çš„è·¯å¾„ä¼˜å…ˆçº§å‡½æ•°æ¥è¯†åˆ«é‡è¦çš„æ¨ç†è·¯å¾„ï¼ŒåŒæ—¶è€ƒè™‘ç´¯ç§¯æˆæœ¬å’Œåˆ°è¾¾ç›®æ ‡çš„é¢„ä¼°æœªæ¥æˆæœ¬ã€‚æœ€åï¼ŒPathMindé€šè¿‡åŒé˜¶æ®µè®­ç»ƒç­–ç•¥ç”Ÿæˆå‡†ç¡®ä¸”é€»è¾‘ä¸€è‡´çš„å“åº”ï¼ŒåŒ…æ‹¬ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´å’Œè·¯å¾„åå¥½å¯¹é½ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPathMindå§‹ç»ˆä¼˜äºç«äº‰åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾“å…¥ä»¤ç‰Œè¾ƒå°‘çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œé€šè¿‡è¯†åˆ«å…³é”®æ¨ç†è·¯å¾„è¡¨ç°æ›´ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14256v1">PDF</a> AAAI 2026, Long Paper, Oral</p>
<p><strong>æ€»ç»“</strong></p>
<p>çŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆKGRï¼‰æ˜¯åœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œé€»è¾‘æ¨æ–­ä»¥è·å–æ–°çŸ¥è¯†çš„ä»»åŠ¡ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åŸºäºLLMçš„KGRæ–¹æ³•ä»é¢ä¸´ä¸¤ä¸ªå…³é”®é—®é¢˜ã€‚ä¸€æ˜¯ç°æœ‰æ–¹æ³•å¸¸å¸¸æ— æ³•åŒºåˆ†æ¨ç†è·¯å¾„çš„é‡è¦æ€§ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥æ— å…³å™ªå£°è¯¯å¯¼LLMã€‚äºŒæ˜¯è®¸å¤šæ–¹æ³•è™½ç„¶åˆ©ç”¨LLMåŠ¨æ€æ¢ç´¢æ½œåœ¨æ¨ç†è·¯å¾„ï¼Œä½†éœ€æ±‚è¿‡é«˜ï¼Œé¢‘ç¹è°ƒç”¨LLMã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†PathMindæ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©é‡è¦çš„æ¨ç†è·¯å¾„ï¼Œæé«˜å¿ å®åº¦å’Œå¯è§£é‡Šæ€§çš„æ¨ç†ã€‚PathMindéµå¾ªâ€œæ£€ç´¢-ä¼˜å…ˆæ’åº-æ¨ç†â€çš„æ¨¡å¼ï¼Œé€šè¿‡æ£€ç´¢æ¨¡å—ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢æŸ¥è¯¢å­å›¾ï¼Œå¼•å…¥è·¯å¾„ä¼˜å…ˆçº§æœºåˆ¶ï¼Œä½¿ç”¨è¯­ä¹‰æ„ŸçŸ¥çš„è·¯å¾„ä¼˜å…ˆçº§å‡½æ•°è¯†åˆ«é‡è¦æ¨ç†è·¯å¾„ï¼ŒåŒæ—¶è€ƒè™‘ç´¯ç§¯æˆæœ¬å’Œåˆ°è¾¾ç›®æ ‡çš„é¢„ä¼°æœªæ¥æˆæœ¬ã€‚é€šè¿‡åŒé˜¶æ®µè®­ç»ƒç­–ç•¥ç”Ÿæˆå‡†ç¡®ä¸”é€»è¾‘ä¸€è‡´çš„å“åº”ï¼ŒåŒ…æ‹¬ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´å’Œè·¯å¾„åå¥½å¯¹é½ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPathMindåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œç‰¹åˆ«æ˜¯è¾“å…¥ä»¤ç‰Œè¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è¯†åˆ«å…³é”®æ¨ç†è·¯å¾„ï¼Œå§‹ç»ˆä¼˜äºç«äº‰å¯¹æ‰‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆKGRï¼‰æ˜¯é€šè¿‡å¯¹çŸ¥è¯†å›¾è°±è¿›è¡Œé€»è¾‘æ¨æ–­æ¥æ¨æ–­æ–°çŸ¥è¯†çš„ä»»åŠ¡ã€‚</li>
<li>å½“å‰LLMåœ¨KGRä¸­é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šæ— æ³•åŒºåˆ†æ¨ç†è·¯å¾„çš„é‡è¦æ€§å’Œé«˜æ£€ç´¢éœ€æ±‚ã€‚</li>
<li>PathMindæ¡†æ¶æ—¨åœ¨é€šè¿‡é€‰æ‹©é‡è¦çš„æ¨ç†è·¯å¾„æ¥æé«˜å¿ å®åº¦å’Œå¯è§£é‡Šæ€§çš„æ¨ç†ã€‚</li>
<li>PathMindéµå¾ªâ€œæ£€ç´¢-ä¼˜å…ˆæ’åº-æ¨ç†â€çš„æ¨¡å¼æ¥å¤„ç†çŸ¥è¯†å›¾è°±ä¸­çš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>PathMindä½¿ç”¨è¯­ä¹‰æ„ŸçŸ¥çš„è·¯å¾„ä¼˜å…ˆçº§å‡½æ•°æ¥è¯†åˆ«é‡è¦æ¨ç†è·¯å¾„ã€‚</li>
<li>PathMindé‡‡ç”¨åŒé˜¶æ®µè®­ç»ƒç­–ç•¥æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPathMindåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾“å…¥ä»¤ç‰Œè¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f957be03788fa4df051462c1b765a16" align="middle">
<img src="https://picx.zhimg.com/v2-91050cfc7d5ac6aab4273a82cabc9a8b" align="middle">
<img src="https://picx.zhimg.com/v2-08ef6a313387840fcf85be14ed3c1438" align="middle">
<img src="https://picx.zhimg.com/v2-28acd1fd68f69c3ffe5d1740c506b905" align="middle">
<img src="https://picx.zhimg.com/v2-238951dfa5d60e2667f2acf65e156b8f" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MalRAG-A-Retrieval-Augmented-LLM-Framework-for-Open-set-Malicious-Traffic-Identification"><a href="#MalRAG-A-Retrieval-Augmented-LLM-Framework-for-Open-set-Malicious-Traffic-Identification" class="headerlink" title="MalRAG: A Retrieval-Augmented LLM Framework for Open-set Malicious Traffic Identification"></a>MalRAG: A Retrieval-Augmented LLM Framework for Open-set Malicious Traffic Identification</h2><p><strong>Authors:Xiang Luo, Chang Liu, Gang Xiong, Chen Yang, Gaopeng Gou, Yaochen Ren, Zhen Li</strong></p>
<p>Fine-grained identification of IDS-flagged suspicious traffic is crucial in cybersecurity. In practice, cyber threats evolve continuously, making the discovery of novel malicious traffic a critical necessity as well as the identification of known classes. Recent studies have advanced this goal with deep models, but they often rely on task-specific architectures that limit transferability and require per-dataset tuning. In this paper we introduce MalRAG, the first LLM driven retrieval-augmented framework for open-set malicious traffic identification. MalRAG freezes the LLM and operates via comprehensive traffic knowledge construction, adaptive retrieval, and prompt engineering. Concretely, we construct a multi-view traffic database by mining prior malicious traffic from content, structural, and temporal perspectives. Furthermore, we introduce a Coverage-Enhanced Retrieval Algorithm that queries across these views to assemble the most probable candidates, thereby improving the inclusion of correct evidence. We then employ Traffic-Aware Adaptive Pruning to select a variable subset of these candidates based on traffic-aware similarity scores, suppressing incorrect matches and yielding reliable retrieved evidence. Moreover, we develop a suite of guidance prompts where task instruction, evidence referencing, and decision guidance are integrated with the retrieved evidence to improve LLM performance. Across diverse real-world datasets and settings, MalRAG delivers state-of-the-art results in both fine-grained identification of known classes and novel malicious traffic discovery. Ablation and deep-dive analyses further show that MalRAG effective leverages LLM capabilities yet achieves open-set malicious traffic identification without relying on a specific LLM.</p>
<blockquote>
<p>åœ¨ç½‘ç»œå®‰å…¨ä¸­ï¼Œå¯¹IDSæ ‡è®°çš„å¯ç–‘æµé‡è¿›è¡Œç²¾ç»†ç²’åº¦çš„è¯†åˆ«è‡³å…³é‡è¦ã€‚å®é™…ä¸Šï¼Œç½‘ç»œå¨èƒæ˜¯ä¸æ–­æ¼”å˜çš„ï¼Œå› æ­¤å¯¹æ–°å‹æ¶æ„æµé‡çš„å‘ç°ä»¥åŠå¯¹å·²çŸ¥ç±»åˆ«çš„è¯†åˆ«éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚å°½ç®¡æœ€è¿‘çš„ç ”ç©¶å·²ç»ä½¿ç”¨æ·±åº¦æ¨¡å‹æ¨åŠ¨äº†è¿™ä¸€ç›®æ ‡ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ¶æ„ï¼Œè¿™äº›æ¶æ„é™åˆ¶äº†å¯è¿ç§»æ€§å¹¶éœ€è¦é’ˆå¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MalRAGï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå¼€æ”¾é›†æ¶æ„æµé‡è¯†åˆ«çš„LLMé©±åŠ¨æ£€ç´¢å¢å¼ºæ¡†æ¶ã€‚MalRAGå†»ç»“äº†LLMï¼Œå¹¶é€šè¿‡å…¨é¢çš„æµé‡çŸ¥è¯†æ„å»ºã€è‡ªé€‚åº”æ£€ç´¢å’Œæç¤ºå·¥ç¨‹è¿›è¡Œæ“ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ä»å†…å®¹ã€ç»“æ„å’Œæ—¶é—´ç­‰å¤šä¸ªè§’åº¦æŒ–æ˜å…ˆå‰çš„æ¶æ„æµé‡ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤šè§†è§’æµé‡æ•°æ®åº“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¢å¼ºè¦†ç›–ç‡çš„æ£€ç´¢ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥åœ¨è¿™äº›è§†è§’ä¸­è¿›è¡ŒæŸ¥è¯¢ï¼Œä»¥ç»„åˆæœ€å¯èƒ½çš„å€™é€‰è€…ï¼Œä»è€Œæé«˜æ­£ç¡®è¯æ®çš„å¯åŒ…å«æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨æµé‡æ„ŸçŸ¥è‡ªé€‚åº”ä¿®å‰ªæ³•ï¼ŒåŸºäºæµé‡æ„ŸçŸ¥ç›¸ä¼¼åº¦å¾—åˆ†é€‰æ‹©è¿™äº›å€™é€‰è€…çš„å¯å˜å­é›†ï¼Œä»è€ŒæŠ‘åˆ¶é”™è¯¯çš„åŒ¹é…å¹¶äº§ç”Ÿå¯é çš„æ£€ç´¢è¯æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—æŒ‡å¯¼æç¤ºï¼Œå°†ä»»åŠ¡æŒ‡ä»¤ã€è¯æ®å¼•ç”¨å’Œå†³ç­–æŒ‡å¯¼ä¸æ£€ç´¢åˆ°çš„è¯æ®ç›¸ç»“åˆï¼Œä»¥æé«˜LLMçš„æ€§èƒ½ã€‚åœ¨å¤šç§çœŸå®ä¸–ç•Œçš„æ•°æ®é›†å’Œç¯å¢ƒä¸­ï¼ŒMalRAGåœ¨å·²çŸ¥ç±»åˆ«çš„ç²¾ç»†ç²’åº¦è¯†åˆ«å’Œæ–°å‹æ¶æ„æµé‡å‘ç°æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°ç»“æœã€‚æ¶ˆå»åˆ†æå’Œæ·±å…¥æ¢ç´¢è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒMalRAGæœ‰æ•ˆåœ°åˆ©ç”¨äº†LLMçš„èƒ½åŠ›ï¼ŒåŒæ—¶å®ç°äº†å¼€æ”¾é›†æ¶æ„æµé‡è¯†åˆ«ï¼Œè€Œä¸ä¾èµ–äºç‰¹å®šçš„LLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14129v1">PDF</a> 13 pages, 13 figures. Intended for submission to IEEE Transactions on Information Forensics and Security (TIFS)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ç½‘ç»œå®‰å…¨é¢†åŸŸä¸­IDSæ ‡è®°çš„ç–‘ä¼¼æµé‡ç²¾ç»†ç²’åº¦è¯†åˆ«çš„é‡è¦æ€§ã€‚éšç€ç½‘ç»œå¨èƒçš„ä¸æ–­æ¼”å˜ï¼Œå‘ç°æ–°å‹æ¶æ„æµé‡å’Œè¯†åˆ«å·²çŸ¥ç±»åˆ«åŒæ ·å…³é”®ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶ä½¿ç”¨æ·±åº¦æ¨¡å‹æ¨è¿›è¿™ä¸€ç›®æ ‡ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æ¶æ„ï¼Œé™åˆ¶äº†å¯è¿ç§»æ€§å¹¶éœ€è¦é’ˆå¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œè°ƒæ•´ã€‚æœ¬æ–‡å¼•å…¥äº†MalRAGï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå¼€æ”¾é›†æ¶æ„æµé‡è¯†åˆ«çš„LLMé©±åŠ¨æ£€ç´¢å¢å¼ºæ¡†æ¶ã€‚MalRAGæ„å»ºäº†ä¸€ä¸ªå¤šè§†å›¾æµé‡æ•°æ®åº“ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç®—æ³•å’Œæµé‡æ„ŸçŸ¥è‡ªé€‚åº”å‰ªææ–¹æ³•ï¼Œæé«˜LLMåœ¨æ¶æ„æµé‡è¯†åˆ«æ–¹é¢çš„æ€§èƒ½ã€‚åœ¨å¤šç§çœŸå®æ•°æ®é›†å’Œè®¾ç½®ä¸‹ï¼ŒMalRAGåœ¨å·²çŸ¥ç±»åˆ«çš„ç²¾ç»†ç²’åº¦è¯†åˆ«å’Œæ–°å‹æ¶æ„æµé‡å‘ç°æ–¹é¢å‡å–å¾—äº†æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç½‘ç»œå®‰å…¨ä¸­IDSæ ‡è®°çš„ç–‘ä¼¼æµé‡ç²¾ç»†ç²’åº¦è¯†åˆ«è‡³å…³é‡è¦ã€‚</li>
<li>æ¶æ„æµé‡çš„å‘ç°å’Œå·²çŸ¥ç±»åˆ«çš„è¯†åˆ«éƒ½æ˜¯å…³é”®éœ€æ±‚ã€‚</li>
<li>MalRAGæ˜¯é¦–ä¸ªç”¨äºå¼€æ”¾é›†æ¶æ„æµé‡è¯†åˆ«çš„LLMé©±åŠ¨æ£€ç´¢å¢å¼ºæ¡†æ¶ã€‚</li>
<li>MalRAGé€šè¿‡æ„å»ºå¤šè§†å›¾æµé‡æ•°æ®åº“ã€ä½¿ç”¨è¦†ç›–å¢å¼ºæ£€ç´¢ç®—æ³•å’Œæµé‡æ„ŸçŸ¥è‡ªé€‚åº”å‰ªææ–¹æ³•æé«˜LLMæ€§èƒ½ã€‚</li>
<li>MalRAGåœ¨å¤šç§çœŸå®æ•°æ®é›†å’Œè®¾ç½®ä¸‹å–å¾—äº†æœ€æ–°ç»“æœï¼Œåœ¨å·²çŸ¥ç±»åˆ«çš„ç²¾ç»†ç²’åº¦è¯†åˆ«å’Œæ–°å‹æ¶æ„æµé‡å‘ç°æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>MalRAGé€šè¿‡å¼•å…¥æŒ‡å¯¼æç¤ºï¼Œå°†ä»»åŠ¡æŒ‡ä»¤ã€è¯æ®å¼•ç”¨å’Œå†³ç­–æŒ‡å¯¼ä¸æ£€ç´¢åˆ°çš„è¯æ®ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜äº†LLMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b16920f03ac073306180fc5ebffac47d" align="middle">
<img src="https://picx.zhimg.com/v2-97de53afaec1abe14638897a101debe5" align="middle">
<img src="https://picx.zhimg.com/v2-ce405eae5ec3b14dbfbce891929ff37f" align="middle">
<img src="https://picx.zhimg.com/v2-8d9087eac6db9164d150dcf53b36ca27" align="middle">
<img src="https://picx.zhimg.com/v2-18e49454803762a5fe9f2a97d3a93d1d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="HiEAG-Evidence-Augmented-Generation-for-Out-of-Context-Misinformation-Detection"><a href="#HiEAG-Evidence-Augmented-Generation-for-Out-of-Context-Misinformation-Detection" class="headerlink" title="HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection"></a>HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection</h2><p><strong>Authors:Junjie Wu, Yumeng Fu, Nan Yu, Guohong Fu</strong></p>
<p>Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.</p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤šæ¨¡æ€è„±ç¦»ä¸Šä¸‹æ–‡ï¼ˆOOCï¼‰è™šå‡ä¿¡æ¯æ£€æµ‹é¢†åŸŸçš„è¿›å±•ï¼Œåœ¨æ£€æŸ¥ä¸åŒæ¨¡æ€ä¹‹é—´çš„ä¸€è‡´æ€§ä»¥æ”¯æŒæˆ–åé©³å›¾æ–‡å¯¹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„OOCè™šå‡ä¿¡æ¯æ£€æµ‹æ–¹æ³•å¾€å¾€å¼ºè°ƒå†…éƒ¨ä¸€è‡´æ€§çš„ä½œç”¨ï¼Œå¿½ç•¥äº†å›¾æ–‡å¯¹ä¸å¤–éƒ¨è¯æ®ä¹‹é—´çš„å¤–éƒ¨ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HiEAGï¼Œä¸€ä¸ªæ–°å‹çš„åˆ†å±‚æ¬¡è¯æ®å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¹¿æ³›çŸ¥è¯†æ¥å®Œå–„å¤–éƒ¨ä¸€è‡´æ€§æ£€æŸ¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¤–éƒ¨ä¸€è‡´æ€§æ£€æŸ¥åˆ†è§£ä¸ºä¸€ä¸ªç»¼åˆçš„å¼•æ“ç®¡é“ï¼Œé™¤äº†æ£€ç´¢ä¹‹å¤–ï¼Œè¿˜ç»“åˆäº†é‡æ–°æ’åºå’Œé‡å†™ã€‚è¯æ®é‡æ–°æ’åºæ¨¡å—åˆ©ç”¨è‡ªåŠ¨è¯æ®é€‰æ‹©æç¤ºï¼ˆAESPï¼‰ä»è¯æ®æ£€ç´¢çš„äº§å“ä¸­è·å¾—ç›¸å…³çš„è¯æ®é¡¹ã€‚éšåï¼Œè¯æ®é‡å†™æ¨¡å—åˆ©ç”¨è‡ªåŠ¨è¯æ®ç”Ÿæˆæç¤ºï¼ˆAEGPï¼‰æ¥æé«˜åŸºäºMLLMçš„OOCè™šå‡ä¿¡æ¯æ£€æµ‹å™¨çš„ä»»åŠ¡é€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜ä¸ºåˆ¤æ–­æä¾›äº†è§£é‡Šï¼Œå¹¶é€šè¿‡æŒ‡ä»¤è°ƒæ•´å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚åœ¨ä¸åŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„HiEAGåœ¨æ‰€æœ‰æ ·æœ¬çš„å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†å…ˆå‰æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14027v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€è„±ç¦»ä¸Šä¸‹æ–‡ï¼ˆOOCï¼‰çš„è¯¯æ£€æŠ€æœ¯åœ¨ä¸åŒæ¨¡æ€é—´ä¸€è‡´æ€§æ£€æµ‹æ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œç°æœ‰æ–¹æ³•è¿‡äºå¼ºè°ƒå†…éƒ¨ä¸€è‡´æ€§ï¼Œå¿½è§†äº†å›¾åƒæ–‡æœ¬å¯¹ä¸å¤–éƒ¨è¯æ®é—´å¤–éƒ¨ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚æœ¬æ–‡æå‡ºHiEAGï¼Œä¸€ç§æ–°å‹åˆ†å±‚è¯æ®å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¸°å¯ŒçŸ¥è¯†æ¥æ”¹è¿›å¤–éƒ¨ä¸€è‡´æ€§æ£€æŸ¥ã€‚HiEAGå°†å¤–éƒ¨ä¸€è‡´æ€§æ£€æŸ¥åˆ†è§£ä¸ºåŒ…æ‹¬é‡æ’å’Œé‡å†™åœ¨å†…çš„ç»¼åˆå¼•æ“ç®¡é“ï¼Œé™¤æ£€ç´¢å¤–ï¼Œè¿˜é›†æˆäº†è¯æ®é‡æ’æ¨¡å—å’Œè¯æ®é‡å†™æ¨¡å—ã€‚é€šè¿‡è‡ªåŠ¨è¯æ®é€‰æ‹©æç¤ºï¼ˆAESPï¼‰å’Œè‡ªåŠ¨è¯æ®ç”Ÿæˆæç¤ºï¼ˆAEGPï¼‰ï¼ŒHiEAGæé«˜äº†å¯¹OOCè¯¯æ£€æ£€æµ‹å™¨çš„ä»»åŠ¡é€‚åº”æ€§ï¼Œå¹¶å®ç°äº†åˆ¤æ–­è§£é‡Šï¼Œä¸”åœ¨ä¸åŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHiEAGçš„å‡†ç¡®ç‡è¶…è¿‡äº†ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰OOCè¯¯æ£€æ£€æµ‹æ³•ä¾§é‡äºå†…éƒ¨ä¸€è‡´æ€§ï¼Œå¿½ç•¥äº†ä¸å¤–éƒ¨è¯æ®çš„ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºHiEAGæ¡†æ¶ï¼Œåˆ©ç”¨MLLMsçš„ä¸°å¯ŒçŸ¥è¯†æ¥æ”¹è¿›å¤–éƒ¨ä¸€è‡´æ€§æ£€æŸ¥ã€‚</li>
<li>HiEAGå°†å¤–éƒ¨ä¸€è‡´æ€§æ£€æŸ¥åˆ†è§£ä¸ºåŒ…æ‹¬é‡æ’å’Œé‡å†™åœ¨å†…çš„ç»¼åˆå¼•æ“ç®¡é“ã€‚</li>
<li>è¯æ®é‡æ’æ¨¡å—é€šè¿‡AESPé€‰æ‹©ç›¸å…³è¯æ®ã€‚</li>
<li>è¯æ®é‡å†™æ¨¡å—é€šè¿‡AEGPæé«˜ä»»åŠ¡é€‚åº”æ€§ã€‚</li>
<li>HiEAGèƒ½å¤Ÿè§£é‡Šåˆ¤æ–­ï¼Œå¹¶å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e1e2112725bf0e006bea69df5ee0d38" align="middle">
<img src="https://picx.zhimg.com/v2-4df3aeec05e3742a595f917dd700b9a4" align="middle">
<img src="https://picx.zhimg.com/v2-38e2e5e8961e8dd7de51a998067da6ae" align="middle">
<img src="https://picx.zhimg.com/v2-451d08001daac36ceb6c60e49a911e9c" align="middle">
<img src="https://picx.zhimg.com/v2-f0756f55a82ec3bd405c8d9f51c065ba" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VULPO-Context-Aware-Vulnerability-Detection-via-On-Policy-LLM-Optimization"><a href="#VULPO-Context-Aware-Vulnerability-Detection-via-On-Policy-LLM-Optimization" class="headerlink" title="VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization"></a>VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization</h2><p><strong>Authors:Youpeng Li, Fuxun Yu, Xinda Wang</strong></p>
<p>The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context.   This paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution. Extensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.</p>
<blockquote>
<p>å¯¹å¼€æºè½¯ä»¶çš„å¹¿æ³›ä¾èµ–æ˜¾è‘—å¢åŠ äº†æ¼æ´åˆ©ç”¨çš„é£é™©ï¼Œè¿™å‡¸æ˜¾äº†æœ‰æ•ˆä¸”å¯æ‰©å±•çš„æ¼æ´æ£€æµ‹ï¼ˆVDï¼‰çš„å¿…è¦æ€§ã€‚ç°æœ‰çš„VDæŠ€æœ¯ï¼Œæ— è®ºæ˜¯åŸºäºä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„è¿˜æ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œå¦‚æç¤ºå·¥ç¨‹ã€ç›‘ç£å¾®è°ƒæˆ–ç¦»ç­–ç•¥åå¥½ä¼˜åŒ–ï¼Œå®ƒä»¬åœ¨æ‰§è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†ææ–¹é¢çš„èƒ½åŠ›éƒ½å—åˆ°æ ¹æœ¬æ€§é™åˆ¶ã€‚å®ƒä»¬ä¾èµ–äºå›ºå®šè¾“å…¥æˆ–é™æ€åå¥½æ•°æ®é›†ï¼Œæ— æ³•è‡ªé€‚åº”åœ°æ¢ç´¢ä»“åº“çº§åˆ«çš„ä¾èµ–å…³ç³»ï¼Œå¹¶å—åˆ°åŠŸèƒ½çº§åˆ«åŸºå‡†æµ‹è¯•çš„é™åˆ¶ï¼Œè€Œè¿™äº›æµ‹è¯•å¿½ç•¥äº†å…³é”®çš„æ¼æ´ä¸Šä¸‹æ–‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.11896v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼€æºè½¯ä»¶çš„å¹¿æ³›åº”ç”¨å¢åŠ äº†æ¼æ´åˆ©ç”¨çš„é£é™©ï¼Œå‡¸æ˜¾äº†å¯¹æœ‰æ•ˆä¸”å¯æ‰©å±•çš„æ¼æ´æ£€æµ‹ï¼ˆVDï¼‰çš„éœ€æ±‚ã€‚ç°æœ‰VDæŠ€æœ¯å­˜åœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æçš„å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç­–ç•¥çš„æ¼æ´è‡ªé€‚åº”ä¼˜åŒ–ï¼ˆVULPOï¼‰æ¡†æ¶ï¼Œç»“åˆä¸Šä¸‹æ–‡æ„ŸçŸ¥VDã€‚ä¸ºæ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œæ„å»ºäº†ContextVulæ•°æ®é›†ï¼Œè®¾è®¡å¤šç»´å¥–åŠ±ç»“æ„ï¼Œå¹¶å¼•å…¥æ ‡ç­¾çº§åˆ«å’Œæ ·æœ¬çº§åˆ«çš„éš¾åº¦è‡ªé€‚åº”å¥–åŠ±ç¼©æ”¾ã€‚å®éªŒè¯æ˜ï¼ŒVULPOæ¡†æ¶åœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥VDä¸­è¡¨ç°å“è¶Šï¼Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æºè½¯ä»¶çš„å¹¿æ³›åº”ç”¨å¢åŠ äº†æ¼æ´é£é™©ï¼Œéœ€è¦æœ‰æ•ˆå’Œå¯æ‰©å±•çš„æ¼æ´æ£€æµ‹æŠ€æœ¯ï¼ˆVDï¼‰ã€‚</li>
<li>ç°æœ‰VDæŠ€æœ¯é¢ä¸´ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†æçš„å±€é™æ€§ï¼Œéš¾ä»¥åº”å¯¹å˜åŒ–çš„ç¯å¢ƒå’Œéœ€æ±‚ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºç­–ç•¥çš„æ¼æ´è‡ªé€‚åº”ä¼˜åŒ–ï¼ˆVULPOï¼‰æ¡†æ¶ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡æ„ŸçŸ¥VDçš„èƒ½åŠ›ã€‚</li>
<li>VULPOåˆ©ç”¨ContextVulæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œè¯¥æ•°æ®é›†èåˆäº†å‡½æ•°çº§åˆ«æ ·æœ¬å’Œä»“åº“çº§åˆ«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>VULPOè®¾è®¡å¤šç»´å¥–åŠ±ç»“æ„ï¼Œç»¼åˆè€ƒè™‘é¢„æµ‹æ­£ç¡®æ€§ã€æ¼æ´å®šä½å‡†ç¡®æ€§å’Œè¯­ä¹‰ç›¸å…³æ€§ã€‚</li>
<li>VULPOå¼•å…¥æ ‡ç­¾çº§åˆ«å’Œæ ·æœ¬çº§åˆ«çš„éš¾åº¦è‡ªé€‚åº”å¥–åŠ±ç¼©æ”¾ï¼Œä»¥åº”å¯¹ä¸åŒçš„æ¼æ´æƒ…å†µå¹¶å¹³è¡¡å¥–åŠ±åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72cd555314d0c70fe7c584b9f4a75a23" align="middle">
<img src="https://picx.zhimg.com/v2-79231456553a380ccf6ce3449b8bf409" align="middle">
<img src="https://picx.zhimg.com/v2-4d024aed536014d38329ce790b4f68ca" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VSPO-Validating-Semantic-Pitfalls-in-Ontology-via-LLM-Based-CQ-Generation"><a href="#VSPO-Validating-Semantic-Pitfalls-in-Ontology-via-LLM-Based-CQ-Generation" class="headerlink" title="VSPO: Validating Semantic Pitfalls in Ontology via LLM-Based CQ Generation"></a>VSPO: Validating Semantic Pitfalls in Ontology via LLM-Based CQ Generation</h2><p><strong>Authors:Hyojun Choi, Seokju Hwang, Kyong-Ho Lee</strong></p>
<p>Competency Questions (CQs) play a crucial role in validating ontology design. While manually crafting CQs can be highly time-consuming and costly for ontology engineers, recent studies have explored the use of large language models (LLMs) to automate this process. However, prior approaches have largely evaluated generated CQs based on their similarity to existing datasets, which often fail to verify semantic pitfalls such as â€œMisusing allValuesFromâ€. Since such pitfalls cannot be reliably detected through rule-based methods, we propose a novel dataset and model of Validating Semantic Pitfalls in Ontology (VSPO) for CQ generation specifically designed to verify the semantic pitfalls. To simulate missing and misused axioms, we use LLMs to generate natural language definitions of classes and properties and introduce misalignments between the definitions and the ontology by removing axioms or altering logical operators (e.g., substituting union with intersection). We then fine-tune LLaMA-3.1-8B-Instruct to generate CQs that validate these semantic discrepancies between the provided definitions and the corresponding axioms. The resulting CQs can detect a broader range of modeling errors compared to existing public datasets. Our fine-tuned model demonstrates superior performance over baselines, showing 26% higher precision and 28.2% higher recall than GPT-4.1 in generating CQs for pitfall validation. This research enables automatic generation of TBox-validating CQs using LLMs, significantly reducing manual effort while improving semantic alignment between ontologies and expert knowledge. To the best of our knowledge, this is the first study to target semantic pitfall validation in CQ generation using LLMs.</p>
<blockquote>
<p>èƒ½åŠ›é—®é¢˜ï¼ˆCQsï¼‰åœ¨éªŒè¯æœ¬ä½“è®¾è®¡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶æ‰‹å·¥ç¼–åˆ¶CQså¯¹äºæœ¬ä½“å·¥ç¨‹å¸ˆæ¥è¯´å¯èƒ½æ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„æ–¹æ³•å¤§å¤šåŸºäºç”Ÿæˆçš„CQsä¸ç°æœ‰æ•°æ®é›†çš„ç›¸ä¼¼æ€§è¿›è¡Œè¯„ä¼°ï¼Œè¿™å¾€å¾€æ— æ³•éªŒè¯è¯­ä¹‰é™·é˜±ï¼Œä¾‹å¦‚â€œè¯¯ç”¨allValuesFromâ€ã€‚ç”±äºè¿™ç§é™·é˜±æ— æ³•å¯é åœ°é€šè¿‡åŸºäºè§„åˆ™çš„æ–¹æ³•è¿›è¡Œæ£€æµ‹ï¼Œæˆ‘ä»¬é’ˆå¯¹CQç”Ÿæˆæå‡ºäº†ä¸€ä¸ªä¸“é—¨ç”¨äºéªŒè¯è¯­ä¹‰é™·é˜±çš„æ–°æ•°æ®é›†å’Œæ¨¡å‹â€”â€”æœ¬ä½“è¯­ä¹‰é™·é˜±éªŒè¯ï¼ˆVSPOï¼‰ã€‚ä¸ºäº†æ¨¡æ‹Ÿç¼ºå¤±å’Œè¯¯ç”¨çš„å…¬ç†ï¼Œæˆ‘ä»¬ä½¿ç”¨LLMç”Ÿæˆç±»å’Œå±æ€§çš„è‡ªç„¶è¯­è¨€å®šä¹‰ï¼Œå¹¶é€šè¿‡ç§»é™¤å…¬ç†æˆ–æ”¹å˜é€»è¾‘è¿ç®—ç¬¦ï¼ˆä¾‹å¦‚ï¼Œç”¨äº¤é›†æ›¿æ¢å¹¶é›†ï¼‰æ¥å¼•å…¥å®šä¹‰ä¸æœ¬ä½“ä¹‹é—´çš„ä¸åŒ¹é…ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹LLaMA-3.1-8B-Instructè¿›è¡Œå¾®è°ƒï¼Œç”ŸæˆéªŒè¯è¿™äº›è¯­ä¹‰å·®å¼‚ï¼ˆå­˜åœ¨äºæä¾›çš„å®šä¹‰å’Œç›¸åº”å…¬ç†ä¹‹é—´ï¼‰çš„CQsã€‚ç”Ÿæˆçš„CQsèƒ½å¤Ÿæ£€æµ‹æ¯”ç°æœ‰å…¬å…±æ•°æ®é›†æ›´å¹¿æ³›çš„å»ºæ¨¡é”™è¯¯ã€‚æˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹ç›¸å¯¹äºåŸºçº¿è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨ç”Ÿæˆç”¨äºé™·é˜±éªŒè¯çš„CQsæ–¹é¢ï¼Œç›¸æ¯”GPT-4.1ï¼Œå…·æœ‰26%çš„æ›´é«˜ç²¾åº¦å’Œ28.2%çš„æ›´é«˜å¬å›ç‡ã€‚è¿™é¡¹ç ”ç©¶èƒ½å¤Ÿä½¿ç”¨LLMè‡ªåŠ¨ç”ŸæˆéªŒè¯TBoxçš„CQsï¼Œæ˜¾è‘—å‡å°‘æ‰‹åŠ¨å·¥ä½œï¼ŒåŒæ—¶æé«˜æœ¬ä½“ä¸ä¸“ä¸šçŸ¥è¯†ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ä½¿ç”¨LLMé’ˆå¯¹CQç”Ÿæˆä¸­çš„è¯­ä¹‰é™·é˜±éªŒè¯è¿›è¡Œç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07991v2">PDF</a> Accepted at AAAI 2026 oral</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨éªŒè¯æœ¬ä½“è®¾è®¡è¯­ä¹‰é™·äº•ä¸­çš„æ½œåœ¨ä»·å€¼å’Œåº”ç”¨å—åˆ°å…³æ³¨ã€‚é’ˆå¯¹ä»¥å¾€ç ”ç©¶ä¸­ä»…ä¾èµ–æ•°æ®é›†ç›¸ä¼¼æ€§è¯„ä¼°ç”Ÿæˆçš„é—®é¢˜ï¼ˆCQsï¼‰çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œæ¨¡å‹â€”â€”ç”¨äºéªŒè¯æœ¬ä½“çš„è¯­ä¹‰é™·äº•ï¼ˆVSPOï¼‰ã€‚é€šè¿‡æ¨¡æ‹Ÿç¼ºå¤±å’Œè¯¯ç”¨çš„å…¬ç†ï¼Œåˆ©ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­è¨€çš„ç±»å’Œå±æ€§å®šä¹‰ï¼Œå¹¶é€šè¿‡å¾®è°ƒLLaMAæ¨¡å‹ç”ŸæˆéªŒè¯è¯­ä¹‰å·®å¼‚çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•èƒ½æ›´å¹¿æ³›åœ°æ£€æµ‹å»ºæ¨¡é”™è¯¯ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°ä¼˜è¶Šã€‚æœ¬ç ”ç©¶é™ä½äº†æ‰‹åŠ¨ç”ŸæˆCQsçš„éš¾åº¦ï¼Œæé«˜äº†æœ¬ä½“ä¸ä¸“å®¶çŸ¥è¯†çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤ä¸ºé’ˆå¯¹LLMåœ¨CQç”Ÿæˆä¸­éªŒè¯è¯­ä¹‰é™·äº•çš„é¦–ä¸ªç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ç”ŸæˆéªŒè¯æœ¬ä½“çš„èƒ½åŠ›é—®é¢˜ï¼ˆCQsï¼‰ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå‡å°‘äº†æ‰‹åŠ¨å·¥ä½œçš„éœ€æ±‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºæ•°æ®é›†ç›¸ä¼¼æ€§è¯„ä¼°ç”Ÿæˆçš„CQsï¼Œä½†æ— æ³•æœ‰æ•ˆéªŒè¯è¯­ä¹‰é™·äº•å¦‚â€œè¯¯ç”¨allValuesFromâ€ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œæ¨¡å‹â€”â€”ç”¨äºéªŒè¯æœ¬ä½“çš„è¯­ä¹‰é™·äº•ï¼ˆVSPOï¼‰ï¼Œä¸“æ³¨äºæ£€æµ‹è¯­ä¹‰å·®å¼‚ã€‚</li>
<li>åˆ©ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­è¨€çš„ç±»å’Œå±æ€§å®šä¹‰ï¼Œæ¨¡æ‹Ÿç¼ºå¤±å’Œè¯¯ç”¨çš„å…¬ç†ã€‚</li>
<li>é€šè¿‡å¾®è°ƒLLaMAæ¨¡å‹ï¼Œç”Ÿæˆçš„CQsèƒ½æ›´å¹¿æ³›åœ°æ£€æµ‹å»ºæ¨¡é”™è¯¯ã€‚</li>
<li>ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥ç ”ç©¶çš„æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„ç²¾ç¡®æ€§å’Œå¬å›ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af125fa7369cbed358f6ab7365af2761" align="middle">
<img src="https://picx.zhimg.com/v2-0fe24f7635239c131021b2cb6d6ee426" align="middle">
<img src="https://picx.zhimg.com/v2-5e49da9d02c492cb3917a73b15183c62" align="middle">
<img src="https://picx.zhimg.com/v2-aec1e559ccbcc11a35a110f6b0d3192b" align="middle">
<img src="https://picx.zhimg.com/v2-9eedeec18a363a81eb92b575918b2ce6" align="middle">
<img src="https://picx.zhimg.com/v2-c230f9215fb001153f74c7025241e6c2" align="middle">
<img src="https://picx.zhimg.com/v2-08d3e8dd50ca9a32317f152532775b85" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SLICE-SLO-Driven-Scheduling-for-LLM-Inference-on-Edge-Computing-Devices"><a href="#SLICE-SLO-Driven-Scheduling-for-LLM-Inference-on-Edge-Computing-Devices" class="headerlink" title="SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices"></a>SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices</h2><p><strong>Authors:Will Chow</strong></p>
<p>Large Language Models (LLMs), as the foundational architecture for next-generation interactive AI applications, not only power intelligent dialogue systems but also drive the evolution of embodied intelligence on edge devices, including humanoid robots, smart vehicles, and other scenarios. The applications running on these edge devices impose differentiated Service Level Objectives (SLO) requirements on LLM services, specifically manifested as distinct constraints on Time to First Token (TTFT) and Time Per Output Token (TPOT) as well as end-to-end latency. Notably, edge devices typically handle real-time tasks that are extremely sensitive to latency, such as machine control and navigation planning. However, existing scheduling service systems still prioritize maximizing output token throughput as the sole optimization objective, failing to adequately address the diversity of SLO requirements. This ultimately results in persistently high violation rates for end-to-end latency or TPOT related SLOs.   This paper proposes SLICE, an innovative scheduling solution designed for edge computing scenarios with differentiated SLO requirements. By combining a utility-maximizing request scheduling algorithm with a dynamic iterative control mechanism for generation rates, SLICE significantly improves LLM inference service SLO attainment. Experimental results demonstrate that compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to 35x higher SLO attainment and 3.4x advantage in task completion time than the other two solutions. This version is temporarily hosted anonymously for double-blind review.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä¸‹ä¸€ä»£äº¤äº’å¼AIåº”ç”¨çš„åŸºç¡€æ¶æ„ï¼Œä¸ä»…åŠ©åŠ›æ™ºèƒ½å¯¹è¯ç³»ç»Ÿçš„å‘å±•ï¼Œè¿˜æ¨åŠ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„ä½“ç°æ™ºèƒ½è¿›åŒ–ï¼ŒåŒ…æ‹¬äººå½¢æœºå™¨äººã€æ™ºèƒ½è½¦è¾†å’Œå…¶ä»–åœºæ™¯ã€‚åœ¨è¿™äº›è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œçš„åº”ç”¨å¯¹LLMæœåŠ¡æå‡ºäº†å·®å¼‚åŒ–çš„æœåŠ¡çº§åˆ«ç›®æ ‡ï¼ˆSLOï¼‰è¦æ±‚ï¼Œå…·ä½“è¡¨ç°ä¸ºé¦–å­—èŠ‚æ—¶é—´ï¼ˆTTFTï¼‰ã€æ¯è¾“å‡ºä»¤ç‰Œæ—¶é—´ï¼ˆTPOTï¼‰ä»¥åŠç«¯åˆ°ç«¯å»¶è¿Ÿçš„ä¸åŒçº¦æŸã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¾¹ç¼˜è®¾å¤‡é€šå¸¸å¤„ç†å¯¹å»¶è¿Ÿæå…¶æ•æ„Ÿçš„ä»»åŠ¡ï¼Œå¦‚æœºå™¨æ§åˆ¶å’Œå¯¼èˆªè§„åˆ’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è°ƒåº¦æœåŠ¡ç³»ç»Ÿä»ç„¶å°†æœ€å¤§åŒ–è¾“å‡ºä»¤ç‰Œååé‡ä½œä¸ºå”¯ä¸€çš„ä¼˜åŒ–ç›®æ ‡ï¼Œæœªèƒ½å……åˆ†åº”å¯¹å¤šæ ·åŒ–çš„SLOè¦æ±‚ã€‚è¿™æœ€ç»ˆå¯¼è‡´ç«¯åˆ°ç«¯å»¶è¿Ÿæˆ–TPOTç›¸å…³çš„SLOæŒç»­é«˜è¿è§„ç‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å…·æœ‰å·®å¼‚åŒ–SLOè¦æ±‚çš„è¾¹ç¼˜è®¡ç®—åœºæ™¯çš„åˆ›æ–°è°ƒåº¦è§£å†³æ–¹æ¡ˆSLICEã€‚é€šè¿‡ç»“åˆæ•ˆç”¨æœ€å¤§åŒ–è¯·æ±‚è°ƒåº¦ç®—æ³•å’Œç”Ÿæˆé€Ÿç‡çš„åŠ¨æ€è¿­ä»£æ§åˆ¶æœºåˆ¶ï¼ŒSLICEæ˜¾è‘—æé«˜äº†LLMæ¨ç†æœåŠ¡SLOçš„è¾¾æˆåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°è§£å†³æ–¹æ¡ˆOrcaå’ŒFastServeç›¸æ¯”ï¼ŒSLICEçš„SLOè¾¾æˆåº¦æé«˜äº†é«˜è¾¾35å€ï¼Œä»»åŠ¡å®Œæˆæ—¶é—´æ¯”å…¶ä»–ä¸¤ä¸ªè§£å†³æ–¹æ¡ˆç¼©çŸ­äº†3.4å€ã€‚æœ¬ç‰ˆæœ¬æš‚æ—¶åŒ¿åæ‰˜ç®¡ä»¥ä¾›åŒé‡ç›²å®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18544v3">PDF</a> This work has been submitted to the IEEE for possible publication. This version is temporarily hosted anonymously for double-blind review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä¸‹ä¸€ä»£äº¤äº’äººå·¥æ™ºèƒ½åº”ç”¨çš„åŸºç¡€æ¶æ„ï¼Œä¸ä»…åŠ©åŠ›æ™ºèƒ½å¯¹è¯ç³»ç»Ÿï¼Œè¿˜æ¨åŠ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„ä½“ç°æ™ºèƒ½çš„å‘å±•ï¼Œå¦‚äººå½¢æœºå™¨äººã€æ™ºèƒ½è½¦è¾†ç­‰ã€‚è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨ç¨‹åºå¯¹LLMæœåŠ¡æœ‰ä¸åŒçš„æœåŠ¡çº§åˆ«ç›®æ ‡ï¼ˆSLOï¼‰è¦æ±‚ï¼Œè¡¨ç°ä¸ºæ—¶é—´è‡³é¦–ä»¤ç‰Œï¼ˆTTFTï¼‰ã€æ¯è¾“å‡ºä»¤ç‰Œæ—¶é—´ï¼ˆTPOTï¼‰ä»¥åŠç«¯åˆ°ç«¯å»¶è¿Ÿçš„ç‰¹å®šçº¦æŸã€‚ç„¶è€Œï¼Œç°æœ‰è°ƒåº¦æœåŠ¡ç³»ç»Ÿä»æŠŠä¼˜åŒ–ç›®æ ‡å•ä¸€åœ°å®šåœ¨æœ€å¤§åŒ–è¾“å‡ºä»¤ç‰Œååé‡ä¸Šï¼Œæ— æ³•å……åˆ†åº”å¯¹å¤šæ ·çš„SLOè¦æ±‚ï¼Œå¯¼è‡´ç«¯åˆ°ç«¯å»¶è¿Ÿæˆ–TPOTç›¸å…³SLOçš„è¿åç‡æŒç»­åé«˜ã€‚æœ¬æ–‡æå‡ºäº†SLICEï¼Œè¿™æ˜¯ä¸€ç§ä¸ºå…·æœ‰å·®å¼‚åŒ–SLOè¦æ±‚çš„è¾¹ç¼˜è®¡ç®—åœºæ™¯è®¾è®¡çš„åˆ›æ–°è°ƒåº¦è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ç»“åˆæ•ˆç”¨æœ€å¤§åŒ–çš„è¯·æ±‚è°ƒåº¦ç®—æ³•å’Œç”Ÿæˆé€Ÿç‡çš„åŠ¨æ€è¿­ä»£æ§åˆ¶æœºåˆ¶ï¼ŒSLICEæ˜¾è‘—æé«˜äº†LLMæ¨ç†æœåŠ¡SLOçš„è¾¾æˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsä½œä¸ºä¸‹ä¸€ä»£AIåº”ç”¨çš„åŸºç¡€æ¶æ„ï¼Œæ¨åŠ¨äº†æ™ºèƒ½å¯¹è¯ç³»ç»Ÿå’Œè¾¹ç¼˜è®¾å¤‡ä¸Šçš„ä½“ç°æ™ºèƒ½çš„å‘å±•ã€‚</li>
<li>è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨ç¨‹åºå¯¹LLMæœåŠ¡æœ‰å·®å¼‚åŒ–çš„SLOè¦æ±‚ï¼ŒåŒ…æ‹¬TTFTã€TPOTå’Œç«¯åˆ°ç«¯å»¶è¿Ÿç­‰ã€‚</li>
<li>ç°æœ‰è°ƒåº¦æœåŠ¡ç³»ç»Ÿä¸»è¦ä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å¤§åŒ–è¾“å‡ºä»¤ç‰Œååé‡ï¼Œæ— æ³•æ»¡è¶³å¤šæ ·åŒ–çš„SLOè¦æ±‚ã€‚</li>
<li>SLICEè°ƒåº¦è§£å†³æ–¹æ¡ˆé’ˆå¯¹å·®å¼‚åŒ–SLOè¦æ±‚çš„è¾¹ç¼˜è®¡ç®—åœºæ™¯è¿›è¡Œè®¾è®¡ï¼Œç»“åˆäº†è¯·æ±‚è°ƒåº¦ç®—æ³•å’Œç”Ÿæˆé€Ÿç‡çš„æ§åˆ¶æœºåˆ¶ã€‚</li>
<li>SLICEæ˜¾è‘—æé«˜LLMæ¨ç†æœåŠ¡SLOè¾¾æˆç‡ï¼Œç›¸è¾ƒäºç°æœ‰è§£å†³æ–¹æ¡ˆæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>SLICEåœ¨ä»»åŠ¡å®Œæˆæ—¶é—´ä¸Šç›¸æ¯”å…¶ä»–è§£å†³æ–¹æ¡ˆæœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec2097b33d1a4163b3f83f39967f2f0b" align="middle">
<img src="https://picx.zhimg.com/v2-b0e7681ef3ebe1c090e4518ad9871807" align="middle">
<img src="https://picx.zhimg.com/v2-51c4a4336368a59783de839f6f79aa0d" align="middle">
<img src="https://picx.zhimg.com/v2-6107335eb20452394e660afc9d4079a2" align="middle">
<img src="https://picx.zhimg.com/v2-0facc3d1fa8530f35a0fd2e9100267b4" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Attention-Surgery-An-Efficient-Recipe-to-Linearize-Your-Video-Diffusion-Transformer"><a href="#Attention-Surgery-An-Efficient-Recipe-to-Linearize-Your-Video-Diffusion-Transformer" class="headerlink" title="Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer"></a>Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer</h2><p><strong>Authors:Mohsen Ghafoorian, Denis Korzhenkov, Amirhossein Habibian</strong></p>
<p>Transformer-based video diffusion models (VDMs) deliver state-of-the-art video generation quality but are constrained by the quadratic cost of self-attention, making long sequences and high resolutions computationally expensive. While linear attention offers sub-quadratic complexity, previous approaches have failed to match the expressiveness of softmax attention unless retrained at significant computational cost. We introduce Attention Surgery, an efficient framework that enables linear or hybrid attention in pretrained VDMs, eliminating the need for training from scratch. Inspired by recent advances in language models, our method combines a novel hybrid attention mechanism-mixing softmax and linear tokens-with a lightweight distillation and fine-tuning pipeline requiring only a few GPU-days. Additionally, we incorporate a cost-aware block-rate strategy to balance expressiveness and efficiency across layers. Applied to Wan2.1 1.3B, a state-of-the-art efficient transformer VDM and evaluated on VBench, VBench2.0 and a human preference study, Attention Surgery achieves competitive results. Furthermore, measurements of on-mobile latency, memory usage, and FLOPs demonstrate notable improvements in scaling behavior for longer videos. Project page is available at: <a target="_blank" rel="noopener" href="https://qualcomm-ai-research.github.io/attention-surgery">https://qualcomm-ai-research.github.io/attention-surgery</a>.</p>
<blockquote>
<p>åŸºäºTransformerçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰æä¾›äº†æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œä½†ç”±äºè‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡æˆæœ¬è€Œå—åˆ°é™åˆ¶ï¼Œä½¿å¾—é•¿åºåˆ—å’Œé«˜åˆ†è¾¨ç‡çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚è™½ç„¶çº¿æ€§æ³¨æ„åŠ›æä¾›äº†æ¬¡äºŒæ¬¡å¤æ‚æ€§ï¼Œä½†ä»¥å‰çš„æ–¹æ³•åœ¨åŒ¹é…softmaxæ³¨æ„åŠ›çš„è¡¨ç°åŠ›æ–¹é¢å¤±è´¥äº†ï¼Œé™¤éä»¥é‡å¤§çš„è®¡ç®—æˆæœ¬é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬å¼•å…¥äº†Attention Surgeryï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨é¢„è®­ç»ƒçš„VDMä¸­ä½¿ç”¨çº¿æ€§æˆ–æ··åˆæ³¨æ„åŠ›ï¼Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°æœ€è¿‘è¯­è¨€æ¨¡å‹è¿›å±•çš„å¯å‘ï¼Œç»“åˆäº†ä¸€ç§æ–°å‹æ··åˆæ³¨æ„åŠ›æœºåˆ¶â€”â€”æ··åˆsoftmaxå’Œçº¿æ€§ä»¤ç‰Œâ€”â€”ä»¥åŠä¸€ä¸ªè½»é‡çº§çš„è’¸é¦å’Œå¾®è°ƒç®¡é“ï¼Œåªéœ€å‡ å¤©çš„GPUæ—¶é—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æˆæœ¬æ„ŸçŸ¥çš„å—ç‡ç­–ç•¥ï¼Œä»¥å¹³è¡¡å„å±‚çš„è¡¨ç°åŠ›å’Œæ•ˆç‡ã€‚åº”ç”¨äºWan2.1 1.3Bè¿™ä¸€å…ˆè¿›çš„Transformer VDMï¼Œå¹¶åœ¨VBenchã€VBench2.0å’Œä¸€é¡¹äººç±»åå¥½ç ”ç©¶ä¸­è¿›è¡Œè¯„ä¼°ï¼ŒAttention Surgeryå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼Œå¯¹ç§»åŠ¨ç«¯çš„å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨å’ŒFLOPsçš„æµ‹é‡æ˜¾ç¤ºï¼Œå¯¹äºè¾ƒé•¿çš„è§†é¢‘ï¼Œå…¶åœ¨æ‰©å±•è¡Œä¸ºæ–¹é¢æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚é¡¹ç›®é¡µé¢å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://qualcomm-ai-research.github.io/attention-surgery%E8%AE%BF%E9%97%AE%E3%80%82">https://qualcomm-ai-research.github.io/attention-surgeryè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24899v3">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºTransformerçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰è™½ç„¶ç”Ÿæˆè§†é¢‘è´¨é‡ä¸€æµï¼Œä½†ç”±äºè‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡æˆæœ¬ï¼Œå¤„ç†é•¿åºåˆ—å’Œé«˜åˆ†è¾¨ç‡æ—¶è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶å¼•å…¥Attention Surgeryæ¡†æ¶ï¼Œé€šè¿‡çº¿æ€§æˆ–æ··åˆæ³¨æ„åŠ›ï¼Œè®©é¢„è®­ç»ƒçš„VDMæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ··åˆæ³¨æ„åŠ›æœºåˆ¶å’Œè½»é‡çº§è’¸é¦å¾®è°ƒç®¡é“ï¼Œä»…éœ€è¦å°‘é‡GPUå¤©æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æˆæœ¬æ„ŸçŸ¥çš„å—ç‡ç­–ç•¥æ¥å¹³è¡¡å„å±‚çš„è¡¨è¾¾æ€§å’Œæ•ˆç‡ã€‚åº”ç”¨äºWan2.1 1.3Bè¿™ä¸€é«˜æ•ˆçš„Transformer VDMå¹¶åœ¨VBenchç­‰å¹³å°ä¸Šè¿›è¡Œè¯„æµ‹æ—¶ï¼ŒAttention Surgeryè¡¨ç°ä¼˜ç§€ã€‚Attention Surgeryè¿˜æ˜¾è‘—æé«˜äº†ç§»åŠ¨è®¾å¤‡ä¸Šå¤„ç†é•¿è§†é¢‘çš„å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨å’Œæµ®ç‚¹è¿ç®—æ€§èƒ½ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://qualcomm-ai-research.github.io/attention-surgery">https://qualcomm-ai-research.github.io/attention-surgery</a>ã€‚<br>     ï¼ˆæ³¨ï¼šç”±äºæä¾›çš„æ‘˜è¦é•¿åº¦æœªè¾¾åˆ°æ‰€éœ€é•¿åº¦é™åˆ¶ï¼Œå·²å°†éƒ¨åˆ†å†…å®¹åˆ æ”¹ç¡®ä¿ç®€æ´ä¸”è¡¨è¾¾å®Œæ•´ã€‚ï¼‰</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-basedè§†é¢‘æ‰©æ•£æ¨¡å‹é¢ä¸´è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿åºåˆ—å’Œé«˜åˆ†è¾¨ç‡æ—¶ã€‚</li>
<li>Attention Surgeryæ¡†æ¶å¼•å…¥äº†ä¸€ç§æ··åˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆäº†softmaxå’Œçº¿æ€§æ³¨æ„åŠ›ä»¤ç‰Œï¼Œä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>Attention Surgeryæ¡†æ¶é€šè¿‡è½»é‡çº§è’¸é¦å’Œå¾®è°ƒç®¡é“å®ç°äº†å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŒ–ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æˆæœ¬æ„ŸçŸ¥çš„å—ç‡ç­–ç•¥å¹³è¡¡äº†è¡¨è¾¾æ€§å’Œæ•ˆç‡ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Attention Surgeryåœ¨å¤šä¸ªå¹³å°ä¸Šè¿›è¡Œäº†è¯„æµ‹å¹¶è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>Attention Surgeryæ˜¾è‘—æé«˜äº†ç§»åŠ¨è®¾å¤‡ä¸Šå¤„ç†é•¿è§†é¢‘çš„å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨å’Œæµ®ç‚¹è¿ç®—æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3a080ab45ab56c08ebff18e290e2697" align="middle">
<img src="https://picx.zhimg.com/v2-d22d3d70f3ad6a65a48a4d1057f5b0e3" align="middle">
<img src="https://picx.zhimg.com/v2-d25c322293eed1e9e170735082266a6f" align="middle">
<img src="https://picx.zhimg.com/v2-f713fd0141266ced2edf31c59cfddcc2" align="middle">
<img src="https://picx.zhimg.com/v2-963569a31187361476fd2db2b5a81b4a" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Guided-Reasoning-in-LLM-Driven-Penetration-Testing-Using-Structured-Attack-Trees"><a href="#Guided-Reasoning-in-LLM-Driven-Penetration-Testing-Using-Structured-Attack-Trees" class="headerlink" title="Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees"></a>Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees</h2><p><strong>Authors:Katsuaki Nakano, Reza Fayyazi, Shanchieh Jay Yang, Michael Zuzak</strong></p>
<p>Recent advances in Large Language Models (LLMs) have driven interest in automating cybersecurity penetration testing workflows, offering the promise of faster and more consistent vulnerability assessment for enterprise systems. Existing LLM agents for penetration testing primarily rely on self-guided reasoning, which can produce inaccurate or hallucinated procedural steps. As a result, the LLM agent may undertake unproductive actions, such as exploiting unused software libraries or generating cyclical responses that repeat prior tactics. In this work, we propose a guided reasoning pipeline for penetration testing LLM agents that incorporates a deterministic task tree built from the MITRE ATT&amp;CK Matrix, a proven penetration testing kll chain, to constrain the LLMâ€™s reaoning process to explicitly defined tactics, techniques, and procedures. This anchors reasoning in proven penetration testing methodologies and filters out ineffective actions by guiding the agent towards more productive attack procedures. To evaluate our approach, we built an automated penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with 103 discrete subtasks representing real-world cyberattack scenarios. Our proposed reasoning pipeline guided the LLM agent through 71.8%, 72.8%, and 78.6% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively. Comparatively, the state-of-the-art LLM penetration testing tool using self-guided reasoning completed only 13.5%, 16.5%, and 75.7% of subtasks and required 86.2%, 118.7%, and 205.9% more model queries. This suggests that incorporating a deterministic task tree into LLM reasoning pipelines can enhance the accuracy and efficiency of automated cybersecurity assessments</p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¿€å‘äº†è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨æ¸—é€æµ‹è¯•å·¥ä½œæµçš„å…´è¶£ï¼Œä¸ºä¼ä¸šç³»ç»Ÿçš„æ¼æ´è¯„ä¼°æä¾›äº†æ›´å¿«æ›´ä¸€è‡´çš„å¸Œæœ›ã€‚ç°æœ‰çš„æ¸—é€æµ‹è¯•LLMä»£ç†ä¸»è¦ä¾èµ–äºè‡ªæˆ‘å¼•å¯¼æ¨ç†ï¼Œè¿™å¯èƒ½ä¼šäº§ç”Ÿä¸å‡†ç¡®æˆ–è™šæ„çš„æ“ä½œæ­¥éª¤ã€‚å› æ­¤ï¼ŒLLMä»£ç†å¯èƒ½ä¼šæ‰§è¡Œæ— æ•ˆæ“ä½œï¼Œå¦‚åˆ©ç”¨æœªä½¿ç”¨çš„è½¯ä»¶åº“æˆ–ç”Ÿæˆé‡å¤ç­–ç•¥çš„å¾ªç¯å“åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ¸—é€æµ‹è¯•LLMä»£ç†çš„å¼•å¯¼æ¨ç†ç®¡é“ï¼Œå®ƒç»“åˆäº†ç”±MITRE ATTï¼†CKçŸ©é˜µæ„å»ºçš„ç¡®å®šæ€§ä»»åŠ¡æ ‘ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡éªŒè¯çš„æ¸—é€æµ‹è¯•æ€ä¼¤é“¾ï¼Œå°†LLMçš„æ¨ç†è¿‡ç¨‹é™åˆ¶åœ¨æ˜ç¡®å®šä¹‰çš„ç­–ç•¥ã€æŠ€æœ¯å’Œç¨‹åºå†…ã€‚è¿™ä½¿æ¨ç†é”šå®šåœ¨æˆç†Ÿçš„æ¸—é€æµ‹è¯•æ–¹æ³•ä¸Šï¼Œå¹¶é€šè¿‡å¼•å¯¼ä»£ç†æ‰§è¡Œæ›´æœ‰æ•ˆçš„æ”»å‡»ç¨‹åºæ¥è¿‡æ»¤æ‰æ— æ•ˆæ“ä½œã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªLLMï¼ˆLlama-3-8Bã€Gemini-1.5å’ŒGPT-4ï¼‰æ„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ¸—é€æµ‹è¯•LLMä»£ç†ï¼Œå¹¶å°†å…¶åº”ç”¨äºå¯¼èˆªHackTheBoxç½‘ç»œå®‰å…¨æ¼”ç»ƒä¸­çš„åä¸ªç½‘ç»œå®‰å…¨æ¼”ç»ƒä¸­çš„ä¸€ç™¾é›¶ä¸‰ä¸ªç¦»æ•£å­ä»»åŠ¡ä»£è¡¨çœŸå®ä¸–ç•Œç½‘ç»œæ”»å‡»åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºçš„æ¨ç†ç®¡é“ä½¿ç”¨Llama-3-8Bã€Gemini-1.5å’ŒGPT-4åˆ†åˆ«å¼•å¯¼äº†ç™¾åˆ†ä¹‹ä¸ƒåä¸€ã€ç™¾åˆ†ä¹‹ä¸ƒåäºŒå’Œç™¾åˆ†ä¹‹ä¸ƒåå…«çš„å­ä»»åŠ¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€å…ˆè¿›çš„è‡ªæˆ‘å¼•å¯¼æ¨ç†LLMæ¸—é€æµ‹è¯•å·¥å…·ä»…å®Œæˆäº†ç™¾åˆ†ä¹‹åä¸‰ã€ç™¾åˆ†ä¹‹åå…­å’Œç™¾åˆ†ä¹‹ä¸ƒåäº”çš„å­ä»»åŠ¡å¹¶éœ€è¦é¢å¤–ä½¿ç”¨ç™¾åˆ†ä¹‹å…«åå…­ç‚¹äºŒã€ç™¾åˆ†ä¹‹ä¸€ç™¾ä¸€åå…«ç‚¹ä¸ƒå’Œç™¾åˆ†ä¹‹äºŒç™¾é›¶äº”ç‚¹ä¹çš„æ¨¡å‹æŸ¥è¯¢é‡ã€‚è¿™è¡¨æ˜å°†ç¡®å®šæ€§ä»»åŠ¡æ ‘çº³å…¥LLMæ¨ç†ç®¡é“å¯ä»¥æé«˜è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07939v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨ç½‘ç»œå®‰å…¨æ¸—é€æµ‹è¯•è‡ªåŠ¨åŒ–æ–¹é¢çš„åº”ç”¨å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰LLMä»£ç†ä¸»è¦ä¾èµ–è‡ªæˆ‘å¼•å¯¼æ¨ç†ï¼Œå­˜åœ¨ä¸å‡†ç¡®å’Œäº§ç”Ÿå¾ªç¯å“åº”çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºMITRE ATT&amp;CKçŸ©é˜µçš„å¼•å¯¼å¼æ¨ç†ç®¡é“ï¼Œå°†LLMçš„æ¨ç†è¿‡ç¨‹çº¦æŸåœ¨æ˜ç¡®çš„æˆ˜æœ¯ã€æŠ€æœ¯å’Œç¨‹åºå†…ï¼Œä»è€Œæé«˜è‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨ç†ç®¡é“å¼•å¯¼çš„LLMä»£ç†åœ¨å®Œæˆå­ä»»åŠ¡æ–¹é¢çš„è¡¨ç°ä¼˜äºè‡ªæˆ‘å¼•å¯¼å¼LLMæ¸—é€æµ‹è¯•å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªåŠ¨åŒ–ç½‘ç»œå®‰å…¨æ¸—é€æµ‹è¯•å·¥ä½œæµç¨‹ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰LLMä»£ç†åœ¨æ¸—é€æµ‹è¯•ä¸­ä¸»è¦ä¾èµ–è‡ªæˆ‘å¼•å¯¼æ¨ç†ï¼Œå­˜åœ¨ä¸å‡†ç¡®å’Œäº§ç”Ÿå¾ªç¯å“åº”çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„å¼•å¯¼å¼æ¨ç†ç®¡é“ç»“åˆMITRE ATT&amp;CKçŸ©é˜µï¼Œæé«˜äº†è‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>ç¡®å®šçš„æ¨ç†ç®¡é“çº¦æŸLLMçš„æ¨ç†è¿‡ç¨‹ï¼Œä½¿å…¶éµå¾ªæ˜ç¡®çš„æˆ˜æœ¯ã€æŠ€æœ¯å’Œç¨‹åºã€‚</li>
<li>ä¸è‡ªæˆ‘å¼•å¯¼å¼LLMæ¸—é€æµ‹è¯•å·¥å…·ç›¸æ¯”ï¼Œè¯¥æ¨ç†ç®¡é“åœ¨å®Œæˆä»»åŠ¡æ–¹é¢çš„è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>ä½¿ç”¨ä¸‰ç§LLMï¼ˆLlama-3-8Bã€Gemini-1.5å’ŒGPT-4ï¼‰æ„å»ºçš„è‡ªåŠ¨åŒ–æ¸—é€æµ‹è¯•LLMä»£ç†åœ¨çœŸå®ç½‘ç»œå®‰å…¨åœºæ™¯ä¸­æœ‰è¾ƒé«˜æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed0958295176577ebab6cad8c889f227" align="middle">
<img src="https://picx.zhimg.com/v2-d0b9cf2e29dcb1c633a74a82dee3530b" align="middle">
<img src="https://picx.zhimg.com/v2-8bf7c88db034c56dd34d746312c78c87" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LENS-Learning-to-Segment-Anything-with-Unified-Reinforced-Reasoning"><a href="#LENS-Learning-to-Segment-Anything-with-Unified-Reinforced-Reasoning" class="headerlink" title="LENS: Learning to Segment Anything with Unified Reinforced Reasoning"></a>LENS: Learning to Segment Anything with Unified Reinforced Reasoning</h2><p><strong>Authors:Lianghui Zhu, Bin Ouyang, Yuxuan Zhang, Tianheng Cheng, Rui Hu, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Li Yu, Wenyu Liu, Xinggang Wang</strong></p>
<p>Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning significantly enhances text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models (SAM). Code is available at <a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS">https://github.com/hustvl/LENS</a>.</p>
<blockquote>
<p>æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²èƒ½å¤Ÿå®ç°ç²¾ç»†çš„è§†è§‰ç†è§£ï¼Œå¯¹äºäººæœºäº¤äº’å’Œæœºå™¨äººç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•é€šå¸¸åœ¨æµ‹è¯•æ—¶å¿½ç•¥æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹æœªè§æç¤ºå’ŒåŸŸè¿›è¡Œæ³›åŒ–çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LENSï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è”åˆä¼˜åŒ–æ¨ç†è¿‡ç¨‹å’Œåˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œæ¶µç›–å¥å­ã€ç›’å­å’Œåˆ†æ®µçº§åˆ«çš„çº¿ç´¢ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ç»†åŒ–æ©è†œè´¨é‡çš„åŒæ—¶ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„CoTæ¨ç†ã€‚æˆ‘ä»¬ä½¿ç”¨å…¬å¼€å¯ç”¨çš„3äº¿å‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå³Qwen2.5-VL-3B-Instructï¼‰è¿›è¡Œè¯•éªŒï¼ŒLENSåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å¹³å‡å®Œå…¨äº¤å¹¶æ¯”ï¼ˆcIoUï¼‰81.2%ï¼Œè¶…è¿‡äº†å¼ºå¤§çš„å¾®è°ƒæ–¹æ³•GLaMMï¼Œæœ€å¤šé«˜å‡º5.6%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„CoTæ¨ç†æ˜¾è‘—æé«˜äº†æ–‡æœ¬æç¤ºçš„åˆ†å‰²æ•ˆæœï¼Œå¹¶ä¸ºå®ç°æ›´é€šç”¨çš„ä»»æ„åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰æä¾›äº†å®é™…é€”å¾„ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hustvl/LENSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14153v2">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS">https://github.com/hustvl/LENS</a></p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†æ¡†æ¶LENSï¼Œå®ç°äº†æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²å’Œæ¨ç†è¿‡ç¨‹çš„è”åˆä¼˜åŒ–ã€‚LENSé‡‡ç”¨ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ï¼Œæ¶µç›–å¥å­ã€æ¡†å’Œåˆ†å‰²çº§åˆ«çš„çº¿ç´¢ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„æ¨ç†ç†ç”±ï¼ŒåŒæ—¶æé«˜æ©æ¨¡è´¨é‡ã€‚ä½¿ç”¨å…¬å¼€å¯ç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VL-3B-Instructï¼Œåœ¨RefCOCOç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¾ƒå¥½çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²å¯¹äºç²¾ç»†è§†è§‰ç†è§£å’Œäººæœºäº¤äº’ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•å¿½ç•¥äº†æµ‹è¯•æ—¶çš„æ˜¾å¼é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œé™åˆ¶äº†å…¶åœ¨æœªè§æç¤ºå’Œé¢†åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LENSæ¡†æ¶æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†å’Œåˆ†å‰²è”åˆä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LENSé‡‡ç”¨ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ï¼Œæ¶µç›–ä¸åŒçº§åˆ«çš„çº¿ç´¢ï¼Œæé«˜æ¨¡å‹ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„æ¨ç†ç†ç”±çš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨Qwen2.5-VL-3B-Instructè§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒLENSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡cIoUä¸º81.2%çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†ç²¾ç»†å¾®è°ƒæ–¹æ³•GLaMMã€‚</li>
<li>ç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„CoTæ¨ç†æ˜¾è‘—æé«˜äº†æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚</li>
<li>LENSä¸ºå¼€å‘æ›´å…·æ³›åŒ–èƒ½åŠ›çš„ä»»æ„åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰æä¾›äº†å®ç”¨é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4fca4251b68ca6ce0dc45c54b899635" align="middle">
<img src="https://picx.zhimg.com/v2-9147d7c00529068e3b43ddb4bbeec526" align="middle">
<img src="https://picx.zhimg.com/v2-db86b8ee37cee871c61f68aad1cf8aa0" align="middle">
<img src="https://picx.zhimg.com/v2-cc7b9c3ef02cbae7df1db236731e49f2" align="middle">
<img src="https://picx.zhimg.com/v2-1cb736a9f87c026bbd615866bb71839f" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-Medical-Reasoning-with-Minimal-Fine-Tuning-Data"><a href="#Towards-Efficient-Medical-Reasoning-with-Minimal-Fine-Tuning-Data" class="headerlink" title="Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data"></a>Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data</h2><p><strong>Authors:Xinlin Zhuang, Feilong Tang, Haolin Yang, Xiwei Liu, Ming Hu, Huifa Li, Haochen Xue, Junjun He, Zongyuan Ge, Yichen Li, Ying Qian, Imran Razzak</strong></p>
<p>Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language Models (LLMs) to specialized domains such as medical reasoning. However, existing SFT practices often rely on unfiltered datasets that contain redundant and low-quality samples, leading to substantial computational costs and suboptimal performance. Although existing methods attempt to alleviate this problem by selecting data based on sample difficulty, defined by knowledge and reasoning complexity, they overlook each sampleâ€™s optimization utility reflected in its gradient. Interestingly, we find that gradient-based influence alone favors easy-to-optimize samples that cause large parameter shifts but lack deep reasoning chains, while difficulty alone selects noisy or overly complex cases that fail to guide stable optimization. Based on this observation, we propose a data selection strategy, Difficulty-Influence Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence quadrant to balance complex clinical reasoning with substantial gradient influence, enabling efficient medical reasoning with minimal fine-tuning data. Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected subsets demonstrate higher data quality and generate clinical reasoning that is more aligned with expert practices in differential diagnosis, safety check, and evidence citation, as DIQ emphasizes samples that foster expert-like reasoning patterns. Extensive experiments on medical reasoning benchmarks demonstrate that DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms baseline methods, highlighting the superiority of principled data selection over brute-force scaling. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/mihara-bot/DIQ">https://github.com/mihara-bot/DIQ</a>.</p>
<blockquote>
<p>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”äºåŒ»ç–—æ¨ç†ç­‰ç‰¹å®šé¢†åŸŸæ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SFTå®è·µé€šå¸¸ä¾èµ–äºåŒ…å«å†—ä½™å’ŒåŠ£è´¨æ ·æœ¬çš„æœªè¿‡æ»¤æ•°æ®é›†ï¼Œå¯¼è‡´å·¨å¤§çš„è®¡ç®—æˆæœ¬å’Œæ€§èƒ½ä¸ä½³ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡æ ¹æ®æ ·æœ¬çš„å¤æ‚åº¦å’ŒçŸ¥è¯†å«é‡æ¥é€‰æ‹©æ•°æ®æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†åæ˜ åœ¨æ¯ä¸ªæ ·æœ¬æ¢¯åº¦ä¸Šçš„ä¼˜åŒ–æ•ˆç”¨ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä»…åŸºäºæ¢¯åº¦çš„é€‰æ‹©å€¾å‘äºé€‰æ‹©å®¹æ˜“ä¼˜åŒ–çš„æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬ä¼šå¼•èµ·è¾ƒå¤§çš„å‚æ•°å˜åŒ–ï¼Œä½†ç¼ºä¹æ·±åº¦æ¨ç†é“¾ï¼Œè€Œä»…ä¾é éš¾åº¦é€‰æ‹©åˆ™ä¼šäº§ç”Ÿå™ªå£°æˆ–è¿‡äºå¤æ‚çš„æ¡ˆä¾‹ï¼Œæ— æ³•æŒ‡å¯¼ç¨³å®šçš„ä¼˜åŒ–ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå³éš¾åº¦å½±å“è±¡é™ï¼ˆDIQï¼‰ï¼Œè¯¥ç­–ç•¥ä¼˜å…ˆé€‰æ‹©åœ¨é«˜éš¾åº¦å’Œé«˜å½±å“è±¡é™ä¸­çš„æ ·æœ¬ï¼Œä»¥å¹³è¡¡å¤æ‚çš„ä¸´åºŠæ¨ç†å’Œæ˜¾è‘—çš„æ¢¯åº¦å½±å“ï¼Œä»è€Œå®ç°ä½¿ç”¨æœ€å°‘å¾®è°ƒæ•°æ®çš„æœ‰æ•ˆåŒ»ç–—æ¨ç†ã€‚æ­¤å¤–ï¼Œäººç±»å’ŒLLMä½œä¸ºæ³•å®˜çš„è¯„ä¼°è¡¨æ˜ï¼ŒDIQé€‰æ‹©çš„å­é›†æ˜¾ç¤ºå‡ºæ›´é«˜çš„æ•°æ®è´¨é‡ï¼Œå¹¶äº§ç”Ÿä¸ä¸“å®¶å®è·µæ›´ä¸€è‡´çš„ä¸´åºŠæ¨ç†ï¼ŒåŒ…æ‹¬é‰´åˆ«è¯Šæ–­ã€å®‰å…¨æ£€æŸ¥å’Œè¯æ®å¼•ç”¨ç­‰æ–¹é¢ã€‚å› ä¸ºDIQå¼ºè°ƒé€‰æ‹©é‚£äº›ä¿ƒè¿›ä¸“å®¶å¼æ¨ç†æ¨¡å¼çš„æ ·æœ¬ã€‚åœ¨åŒ»ç–—æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨1%ç²¾é€‰æ•°æ®çš„DIQæ¨¡å‹å°±èƒ½è¾¾åˆ°å…¨æ•°æ®é›†çš„æ€§èƒ½ï¼Œè€Œä½¿ç”¨10%çš„æ•°æ®æ—¶åˆ™å§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œè¿™å‡¸æ˜¾äº†åŸåˆ™æ€§æ•°æ®é€‰æ‹©ä¼˜äºç²—æš´æ‰©å±•çš„ä¼˜åŠ¿ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mihara-bot/DIQ%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mihara-bot/DIQè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01450v2">PDF</a> preprint, under review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—æ¨ç†ç­‰ä¸“ä¸šé¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰èµ·åˆ°å…³é”®ä½œç”¨ã€‚ä½†ç°æœ‰SFTå®è·µå¸¸ä½¿ç”¨åŒ…å«å†—ä½™å’ŒåŠ£è´¨æ ·æœ¬çš„æœªç­›é€‰æ•°æ®é›†ï¼Œå¯¼è‡´å·¨å¤§çš„è®¡ç®—æˆæœ¬åŠæ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ•°æ®é€‰æ‹©ç­–ç•¥â€”â€”éš¾åº¦å½±å“è±¡é™ï¼ˆDIQï¼‰ï¼Œä¼˜å…ˆé€‰å–é«˜éš¾åº¦é«˜å½±å“åº¦çš„æ ·æœ¬ï¼Œå¹³è¡¡å¤æ‚ä¸´åºŠæ¨ç†ä¸æ˜¾è‘—çš„æ¢¯åº¦å½±å“ï¼Œå®ç°é«˜æ•ˆçš„åŒ»ç–—æ¨ç†å¹¶æœ€å°åŒ–å¾®è°ƒæ•°æ®ã€‚ç»äººç±»å’ŒLLM-as-a-judgeè¯„ä¼°ï¼ŒDIQé€‰å®šçš„å­é›†æ•°æ®è´¨é‡æ›´é«˜ï¼Œç”Ÿæˆçš„ä¸´åºŠæ¨ç†ä¸ä¸“å®¶å®è·µæ›´ä¸€è‡´ã€‚ä»…åœ¨1%çš„é€‰å®šæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå³å¯è¾¾åˆ°å…¨æ•°æ®é›†çš„æ€§èƒ½ï¼Œå‡¸æ˜¾äº†åŸåˆ™æ€§æ•°æ®é€‰æ‹©ä¼˜äºç²—æš´çš„è§„æ¨¡æ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”åŒ»ç–—æ¨ç†ç­‰ç‰¹å®šé¢†åŸŸæ—¶å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>ç°æœ‰SFTå®è·µä½¿ç”¨æœªç­›é€‰æ•°æ®é›†ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚å’Œæ€§èƒ½ä¸ä½³ã€‚</li>
<li>éš¾åº¦å½±å“è±¡é™ï¼ˆDIQï¼‰ç­–ç•¥ç»“åˆæ ·æœ¬çš„éš¾åº¦å’Œæ¢¯åº¦å½±å“åŠ›è¿›è¡Œæ•°æ®é€‰æ‹©ã€‚</li>
<li>DIQç­–ç•¥å¹³è¡¡äº†å¤æ‚ä¸´åºŠæ¨ç†ä¸æ˜¾è‘—çš„æ¢¯åº¦å½±å“ï¼Œæé«˜äº†æ•°æ®è´¨é‡å’Œæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>DIQé€‰å®šçš„å­é›†åœ¨åŒ»ç–—æ¨ç†æ–¹é¢ä¸ä¸“å®¶å®è·µæ›´ä¸€è‡´ã€‚</li>
<li>ä»…ä½¿ç”¨1%çš„é€‰å®šæ•°æ®è¿›è¡Œå¾®è°ƒå³å¯è¾¾åˆ°å…¨æ•°æ®é›†æ€§èƒ½ï¼Œå‡¸æ˜¾åŸåˆ™æ€§æ•°æ®é€‰æ‹©çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fcd24b9ccfc1a5263719ba39703ea93f" align="middle">
<img src="https://picx.zhimg.com/v2-3d3a0ee5a715cfd59f9de1112adae266" align="middle">
<img src="https://picx.zhimg.com/v2-053f763c56f267259b03676be7633f7f" align="middle">
<img src="https://picx.zhimg.com/v2-017ea1ffe4816767f657fdc81ac7ec7c" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GMAT-Grounded-Multi-Agent-Clinical-Description-Generation-for-Text-Encoder-in-Vision-Language-MIL-for-Whole-Slide-Image-Classification"><a href="#GMAT-Grounded-Multi-Agent-Clinical-Description-Generation-for-Text-Encoder-in-Vision-Language-MIL-for-Whole-Slide-Image-Classification" class="headerlink" title="GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification"></a>GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification</h2><p><strong>Authors:Ngoc Bui Lam Quang, Nam Le Nguyen Binh, Thanh-Huy Nguyen, Le Thien Phuc Nguyen, Quan Nguyen, Ulas Bagci</strong></p>
<p>Multiple Instance Learning (MIL) is the leading approach for whole slide image (WSI) classification, enabling efficient analysis of gigapixel pathology slides. Recent work has introduced vision-language models (VLMs) into MIL pipelines to incorporate medical knowledge through text-based class descriptions rather than simple class names. However, when these methods rely on large language models (LLMs) to generate clinical descriptions or use fixed-length prompts to represent complex pathology concepts, the limited token capacity of VLMs often constrains the expressiveness and richness of the encoded class information. Additionally, descriptions generated solely by LLMs may lack domain grounding and fine-grained medical specificity, leading to suboptimal alignment with visual features. To address these challenges, we propose a vision-language MIL framework with two key contributions: (1) A grounded multi-agent description generation system that leverages curated pathology textbooks and agent specialization (e.g., morphology, spatial context) to produce accurate and diverse clinical descriptions; (2) A text encoding strategy using a list of descriptions rather than a single prompt, capturing fine-grained and complementary clinical signals for better alignment with visual features. Integrated into a VLM-MIL pipeline, our approach shows improved performance over single-prompt class baselines and achieves results comparable to state-of-the-art models, as demonstrated on renal and lung cancer datasets.</p>
<blockquote>
<p>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ˜¯å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»çš„é¢†å…ˆæ–¹æ³•ï¼Œå¯å®ç°é«˜æ•ˆçš„å¯¹åƒå…†åƒç´ ç—…ç†åˆ‡ç‰‡çš„å…¨é¢åˆ†æã€‚æœ€è¿‘çš„å·¥ä½œå°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼•å…¥åˆ°MILç®¡é“ä¸­ï¼Œé€šè¿‡åŸºäºæ–‡æœ¬çš„ç±»åˆ«æè¿°è€Œéç®€å•çš„ç±»åˆ«åç§°æ¥èå…¥åŒ»å­¦çŸ¥è¯†ã€‚ç„¶è€Œï¼Œå½“è¿™äº›æ–¹æ³•ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆä¸´åºŠæè¿°æˆ–ä½¿ç”¨å›ºå®šé•¿åº¦çš„æç¤ºæ¥è¡¨ç¤ºå¤æ‚çš„ç—…ç†å­¦æ¦‚å¿µæ—¶ï¼ŒVLMçš„æœ‰é™ä»¤ç‰Œå®¹é‡é€šå¸¸é™åˆ¶äº†ç¼–ç ç±»åˆ«ä¿¡æ¯çš„è¡¨è¾¾ä¸°å¯Œæ€§ã€‚æ­¤å¤–ï¼Œä»…ç”±LLMç”Ÿæˆçš„ä¸´åºŠæè¿°å¯èƒ½ç¼ºä¹é¢†åŸŸåŸºç¡€å’Œç²¾ç»†çš„åŒ»å­¦ç‰¹å¼‚æ€§ï¼Œå¯¼è‡´ä¸è§†è§‰ç‰¹å¾çš„æ¬¡ä¼˜å¯¹é½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªä¸»è¦è´¡çŒ®çš„è§†è§‰è¯­è¨€MILæ¡†æ¶ï¼šä¸€æ˜¯åŸºäºç—…ç†æ•™ç§‘ä¹¦å’Œä»£ç†ä¸“ä¸šåŒ–ï¼ˆå¦‚å½¢æ€å­¦ã€ç©ºé—´ä¸Šä¸‹æ–‡ï¼‰çš„åœ°åŸºå¤šä»£ç†æè¿°ç”Ÿæˆç³»ç»Ÿï¼Œèƒ½å¤Ÿäº§ç”Ÿå‡†ç¡®å’Œå¤šæ ·åŒ–çš„ä¸´åºŠæè¿°ï¼›äºŒæ˜¯ä½¿ç”¨ä¸€ç³»åˆ—æè¿°è€Œéå•ä¸€æç¤ºçš„æ–‡æœ¬ç¼–ç ç­–ç•¥ï¼Œæ•è·ç²¾ç»†å’Œè¡¥å……çš„ä¸´åºŠä¿¡å·ï¼Œä»¥æ›´å¥½åœ°ä¸è§†è§‰ç‰¹å¾å¯¹é½ã€‚èå…¥VLM-MILç®¡é“åï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å•æç¤ºç±»åˆ«åŸºçº¿çš„åŸºç¡€ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶åœ¨è‚¾è„å’Œè‚ºç™Œæ•°æ®é›†ä¸Šå®ç°äº†ä¸æœ€æ–°æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01293v2">PDF</a> Acccepted in MICCAI Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨ç—…ç†å­¦æ£€æµ‹çš„å…¨æ»‘å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ˜¯ç›®å‰ä¸»æµçš„æ–¹æ³•ã€‚æœ€æ–°ç ”ç©¶å¼•å…¥äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œé€šè¿‡åŸºäºæ–‡æœ¬çš„ç±»åˆ«æè¿°èå…¥åŒ»å­¦çŸ¥è¯†ï¼Œè€Œéä»…ä½¿ç”¨ç®€å•çš„ç±»åˆ«åç§°ã€‚ç„¶è€Œï¼Œå½“è¿™äº›æ–¹æ³•ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä¸´åºŠæè¿°æˆ–ä½¿ç”¨å›ºå®šé•¿åº¦çš„æç¤ºæ¥è¡¨ç¤ºå¤æ‚çš„ç—…ç†å­¦æ¦‚å¿µæ—¶ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æœ‰é™ä»¤ç‰Œå®¹é‡é€šå¸¸é™åˆ¶äº†ç¼–ç ç±»åˆ«ä¿¡æ¯çš„è¡¨è¾¾ä¸°å¯Œæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¸¦æœ‰ä¸¤ä¸ªå…³é”®è´¡çŒ®çš„è§†è§‰è¯­è¨€MILæ¡†æ¶ï¼šä¸€æ˜¯åŸºäºç—…ç†æ•™æå’Œä»£ç†ä¸“ä¸šçŸ¥è¯†çš„å¤šä»£ç†æè¿°ç”Ÿæˆç³»ç»Ÿï¼›äºŒæ˜¯ä½¿ç”¨ä¸€ç³»åˆ—æè¿°è€Œéå•ä¸€æç¤ºçš„æ–‡æœ¬ç¼–ç ç­–ç•¥ï¼Œä»¥æ•æ‰ç²¾ç»†ä¸”äº’è¡¥çš„ä¸´åºŠä¿¡å·ï¼Œæ›´å¥½åœ°ä¸è§†è§‰ç‰¹å¾å¯¹é½ã€‚è¯¥æ¡†æ¶åœ¨è‚¾è„å’Œè‚ºç™Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå•æç¤ºç±»åˆ«åŸºçº¿ï¼Œå¹¶è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ˜¯ç—…ç†å­¦æ£€æµ‹å…¨æ»‘å›¾åƒåˆ†ç±»çš„ä¸»æµæ–¹æ³•ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡åŒ»å­¦çŸ¥è¯†èå…¥è·¯å¾„å­¦åˆ†ç±»ä¸­ï¼Œä¸ä»…ä½¿ç”¨ç®€å•çš„ç±»åˆ«åç§°ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä¸´åºŠæè¿°æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç¼ºä¹é¢†åŸŸçŸ¥è¯†å’Œç²¾ç»†åŒ»å­¦ç‰¹å¼‚æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è§†è§‰è¯­è¨€MILæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šå¤šä»£ç†æè¿°ç”Ÿæˆç³»ç»Ÿå’Œæ–‡æœ¬ç¼–ç ç­–ç•¥ã€‚</li>
<li>å¤šä»£ç†æè¿°ç”Ÿæˆç³»ç»ŸåŸºäºç—…ç†æ•™æå’Œä»£ç†ä¸“ä¸šçŸ¥è¯†ï¼Œèƒ½ç”Ÿæˆå‡†ç¡®ä¸”å¤šæ ·çš„ä¸´åºŠæè¿°ã€‚</li>
<li>æ–‡æœ¬ç¼–ç ç­–ç•¥ä½¿ç”¨ä¸€ç³»åˆ—æè¿°è€Œéå•ä¸€æç¤ºï¼Œä»¥æ•æ‰æ›´ç²¾ç»†ä¸”äº’è¡¥çš„ä¸´åºŠä¿¡å·ï¼Œä¸è§†è§‰ç‰¹å¾æ›´å¥½åœ°å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f1c1b241464e40b39de9c0c0738246c" align="middle">
<img src="https://picx.zhimg.com/v2-e25081b512f7089049f380d6f22c0e4a" align="middle">
<img src="https://picx.zhimg.com/v2-0f39b9c2673410ea1b792069e560f8e5" align="middle">
<img src="https://picx.zhimg.com/v2-852d98b4d1aa4c17caf38f993cc6f2cf" align="middle">
<img src="https://picx.zhimg.com/v2-b93ab6f194da96772302793b999f0111" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d67a74f417dab498877371d362b371ae" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  AutoTool Efficient Tool Selection for Large Language Model Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b2103e0612f304f8fda470659851e2bf" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  UniGen-1.5 Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
