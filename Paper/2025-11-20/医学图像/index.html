<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  Seeing Beyond the Image ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d9a740d50a7634ebdbe4748ab36a807f')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-20-æ›´æ–°"><a href="#2025-11-20-æ›´æ–°" class="headerlink" title="2025-11-20 æ›´æ–°"></a>2025-11-20 æ›´æ–°</h1><h2 id="Seeing-Beyond-the-Image-ECG-and-Anatomical-Knowledge-Guided-Myocardial-Scar-Segmentation-from-Late-Gadolinium-Enhanced-Images"><a href="#Seeing-Beyond-the-Image-ECG-and-Anatomical-Knowledge-Guided-Myocardial-Scar-Segmentation-from-Late-Gadolinium-Enhanced-Images" class="headerlink" title="Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images"></a>Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images</h2><p><strong>Authors:Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna, Meryem Jabrane, Vicente Grau, Shahnaz Jamil-Copley, Richard H. Clayton,  Chen,  Chen</strong></p>
<p>Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to â€œsee beyond the imageâ€, setting a new direction for robust and physiologically grounded cardiac scar segmentation.</p>
<blockquote>
<p>ç²¾ç¡®åˆ†å‰²æ™šæœŸé’†å¢å¼ºï¼ˆLGEï¼‰å¿ƒè„MRIä¸­çš„å¿ƒè‚Œç˜¢ç—•å¯¹äºè¯„ä¼°ç»„ç»‡æ´»åŠ›è‡³å…³é‡è¦ï¼Œä½†ç”±äºå¯¹æ¯”åº¦å˜åŒ–å’Œæˆåƒä¼ªå½±ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¿ƒç”µå›¾ï¼ˆECGï¼‰ä¿¡å·æä¾›äº†è¡¥å……çš„ç”Ÿç†ä¿¡æ¯ï¼Œå› ä¸ºä¼ å¯¼å¼‚å¸¸æœ‰åŠ©äºå®šä½æˆ–æç¤ºç˜¢ç—•å¿ƒè‚ŒåŒºåŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å¤šæ¨¡å¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ECGè¡ç”Ÿçš„ç”Ÿç†ä¿¡æ¯ä¸AHA-17å›¾è°±è§£å‰–å…ˆéªŒçŸ¥è¯†ç›¸ç»“åˆï¼Œç”¨äºåŸºäºç”Ÿç†ä¸€è‡´çš„LGEç˜¢ç—•åˆ†å‰²ã€‚ç”±äºå¿ƒç”µå›¾å’ŒLGE-MRIå¹¶éåŒæ—¶é‡‡é›†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ—¶é—´æ„ŸçŸ¥ç‰¹å¾èåˆï¼ˆTAFFï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®é‡‡é›†æ—¶é—´å·®å¼‚åŠ¨æ€åŠ æƒå’Œèåˆç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸´åºŠè¯•éªŒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„ä»…å›¾åƒåŸºçº¿ï¼ˆnnU-Netï¼‰å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œç˜¢ç—•çš„å¹³å‡Diceå¾—åˆ†ä»0.6149æé«˜åˆ°0.8463ï¼Œå¹¶ä¸”åœ¨ç²¾ç¡®åº¦ï¼ˆ0.9115ï¼‰å’Œçµæ•åº¦ï¼ˆ0.9043ï¼‰æ–¹é¢éƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ•´åˆç”Ÿç†å’Œè§£å‰–çŸ¥è¯†ä½¿æ¨¡å‹èƒ½å¤Ÿâ€œè¶…è¶Šå›¾åƒçœ‹åˆ°æ›´å¤šâ€ï¼Œä¸ºç¨³å¥ä¸”åŸºäºç”Ÿç†çš„å¿ƒè„ç˜¢ç—•åˆ†å‰²è®¾å®šäº†ä¸€ä¸ªæ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¿ƒç”µå›¾è¡ç”Ÿçš„ç”Ÿç†ä¿¡æ¯ä¸AHA-17å›¾è°±æä¾›çš„è§£å‰–å­¦å…ˆéªŒä¿¡æ¯ï¼Œç”¨äºåŸºäºæ™šæœŸé’†å¢å¼ºï¼ˆLGEï¼‰å¿ƒè„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„ç”Ÿç†ä¸€è‡´æ€§ç˜¢ç—•åˆ†å‰²ã€‚é€šè¿‡å¼•å…¥åŸºäºæ—¶é—´æ„ŸçŸ¥ç‰¹å¾èåˆï¼ˆTAFFï¼‰æœºåˆ¶ï¼Œæ ¹æ®é‡‡é›†æ—¶é—´å·®å¼‚åŠ¨æ€åŠ æƒå’Œèåˆç‰¹å¾ï¼Œå®ç°äº†å¿ƒç”µå›¾å’ŒLGE-MRIçš„éåŒæ­¥æ•°æ®èåˆã€‚ç›¸è¾ƒäºä»…ä¾èµ–å›¾åƒçš„å‰ç»æ€§æ–¹æ³•ï¼ˆå¦‚nnU-Netï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨ä¸´åºŠæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾è‘—æå‡äº†ç˜¢ç—•çš„ç‹„å…‹ç³»æ•°ï¼Œä»0.6149æå‡è‡³0.8463ï¼ŒåŒæ—¶åœ¨ç²¾ç¡®åº¦å’Œçµæ•åº¦æ–¹é¢å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚è¿™è¡¨æ˜ç»“åˆç”Ÿç†å’Œè§£å‰–å­¦çŸ¥è¯†å¯ä½¿æ¨¡å‹â€œè¶…è¶Šå›¾åƒâ€ï¼Œä¸ºç¨³å¥ä¸”åŸºäºç”Ÿç†çš„å¿ƒè„ç˜¢ç—•åˆ†å‰²è®¾å®šæ–°æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡ç»“åˆå¿ƒç”µå›¾ï¼ˆECGï¼‰å’Œæ™šæœŸé’†å¢å¼ºï¼ˆLGEï¼‰å¿ƒè„MRIæ•°æ®ï¼Œå®ç°å¿ƒè‚Œç˜¢ç—•çš„å‡†ç¡®åˆ†å‰²ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€æ¡†æ¶ï¼Œé›†æˆäº†ECGçš„ç”Ÿç†ä¿¡æ¯ä¸AHA-17å›¾è°±çš„è§£å‰–å­¦å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>é€šè¿‡Temporal Aware Feature Fusionï¼ˆTAFFï¼‰æœºåˆ¶ï¼Œå®ç°äº†éåŒæ­¥æ•°æ®èåˆï¼Œæ ¹æ®é‡‡é›†æ—¶é—´å·®å¼‚åŠ¨æ€è°ƒæ•´ç‰¹å¾æƒé‡ã€‚</li>
<li>ç›¸è¾ƒäºä»…ä¾èµ–å›¾åƒçš„æ–¹æ³•ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æå‡äº†ç˜¢ç—•åˆ†å‰²çš„ç‹„å…‹ç³»æ•°ï¼Œä»0.6149æå‡è‡³0.8463ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ç²¾ç¡®åº¦å’Œçµæ•åº¦æ–¹é¢å‡å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ï¼Œåˆ†åˆ«è¾¾åˆ°äº†0.9115å’Œ0.9043ã€‚</li>
<li>ç»“åˆç”Ÿç†å’Œè§£å‰–å­¦çŸ¥è¯†ä½¿æ¨¡å‹èƒ½å¤Ÿâ€œè¶…è¶Šå›¾åƒâ€ï¼Œä¸ºå¿ƒè„ç˜¢ç—•åˆ†å‰²æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ed8edc112df8c1ce29f56a2f9ca33c6" align="middle">
<img src="https://picx.zhimg.com/v2-b9d1a4acb24bc59c4d4775766c0bd177" align="middle">
<img src="https://picx.zhimg.com/v2-448edf2a1ab59be064a1068892a4d294" align="middle">
<img src="https://picx.zhimg.com/v2-24d2bfbfc730a4b061ba512cab05c41a" align="middle">
<img src="https://picx.zhimg.com/v2-3173b07fdcd4cbe1ca0ed5b060ce280f" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="NERD-Network-Regularized-Diffusion-Sampling-For-3D-Computed-Tomography"><a href="#NERD-Network-Regularized-Diffusion-Sampling-For-3D-Computed-Tomography" class="headerlink" title="NERD: Network-Regularized Diffusion Sampling For 3D Computed Tomography"></a>NERD: Network-Regularized Diffusion Sampling For 3D Computed Tomography</h2><p><strong>Authors:Shijun Liang, Ismail Alkhouri, Qing Qu, Rongrong Wang, Saiprasad Ravishankar</strong></p>
<p>Numerous diffusion model (DM)-based methods have been proposed for solving inverse imaging problems. Among these, a recent line of work has demonstrated strong performance by formulating sampling as an optimization procedure that enforces measurement consistency, forward diffusion consistency, and both step-wise and backward diffusion consistency. However, these methods have only considered 2D reconstruction tasks and do not directly extend to 3D image reconstruction problems, such as in Computed Tomography (CT). To bridge this gap, we propose NEtwork-Regularized diffusion sampling for 3D CT (NERD) by incorporating an L1 regularization into the optimization objective. This regularizer encourages spatial continuity across adjacent slices, reducing inter-slice artifacts and promoting coherent volumetric reconstructions. Additionally, we introduce two efficient optimization strategies to solve the resulting objective: one based on the Alternating Direction Method of Multipliers (ADMM) and another based on the Primal-Dual Hybrid Gradient (PDHG) method. Experiments on medical 3D CT data demonstrate that our approach achieves either state-of-the-art or highly competitive results.</p>
<blockquote>
<p>é’ˆå¯¹é€†å‘æˆåƒé—®é¢˜ï¼Œå·²ç»æå‡ºäº†è®¸å¤šåŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰çš„æ–¹æ³•ã€‚å…¶ä¸­ï¼Œè¿‘æœŸçš„ä¸€é¡¹å·¥ä½œé€šè¿‡å°†é‡‡æ ·åˆ¶å®šä¸ºä¼˜åŒ–ç¨‹åºï¼Œå¼ºåˆ¶å®æ–½æµ‹é‡ä¸€è‡´æ€§ã€å‰å‘æ‰©æ•£ä¸€è‡´æ€§ä»¥åŠé€æ­¥å’Œåå‘æ‰©æ•£ä¸€è‡´æ€§ï¼Œä»è€Œå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…è€ƒè™‘äº†2Dé‡å»ºä»»åŠ¡ï¼Œå¹¶ä¸èƒ½ç›´æ¥æ‰©å±•åˆ°3Då›¾åƒé‡å»ºé—®é¢˜ï¼Œä¾‹å¦‚åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é€šè¿‡å°†L1æ­£åˆ™åŒ–çº³å…¥ä¼˜åŒ–ç›®æ ‡ï¼Œæå‡ºäº†ç”¨äº3D CTçš„ç½‘ç»œæ­£åˆ™åŒ–æ‰©æ•£é‡‡æ ·ï¼ˆNERDï¼‰ã€‚è¿™ç§æ­£åˆ™åŒ–é¼“åŠ±ç›¸é‚»åˆ‡ç‰‡ä¹‹é—´çš„ç©ºé—´è¿ç»­æ€§ï¼Œå‡å°‘äº†åˆ‡ç‰‡é—´çš„ä¼ªå½±ï¼Œä¿ƒè¿›äº†è¿è´¯çš„ä½“ç§¯é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸¤ç§æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥æ¥è§£å†³ç›®æ ‡é—®é¢˜ï¼šä¸€ç§åŸºäºäº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰ï¼Œå¦ä¸€ç§åŸºäºåŸå§‹-å¯¹å¶æ··åˆæ¢¯åº¦ï¼ˆPDHGï¼‰æ–¹æ³•ã€‚åœ¨åŒ»ç–—3D CTæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æ°´å¹³æˆ–æå…·ç«äº‰åŠ›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14680v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹ä¸‰ç»´CTé‡å»ºçš„åŸºäºç½‘ç»œæ­£åˆ™åŒ–æ‰©æ•£é‡‡æ ·æ–¹æ³•ï¼ˆNERDï¼‰ï¼Œç»“åˆäº†æ‰©æ•£æ¨¡å‹å’ŒL1æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥è§£å†³ä¸‰ç»´å›¾åƒé‡å»ºé—®é¢˜ã€‚é€šè¿‡å¼•å…¥ç©ºé—´è¿ç»­æ€§çº¦æŸï¼Œè¯¥æ–¹æ³•æé«˜äº†ç›¸é‚»åˆ‡ç‰‡ä¹‹é—´çš„è¿ç»­æ€§ï¼Œå‡å°‘äº†åˆ‡ç‰‡é—´ä¼ªå½±ï¼Œä¿ƒè¿›äº†ä½“ç§¯é‡å»ºçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†ä¸¤ç§æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ï¼ŒåŸºäºäº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰å’ŒåŸå§‹-å¯¹å¶æ··åˆæ¢¯åº¦æ³•ï¼ˆPDHGï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»å­¦ä¸‰ç»´CTæ•°æ®ä¸Šå–å¾—äº†æœ€å‰æ²¿æˆ–é«˜åº¦ç«äº‰çš„ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºç½‘ç»œæ­£åˆ™åŒ–çš„æ‰©æ•£é‡‡æ ·æ–¹æ³•ï¼ˆNERDï¼‰ï¼Œè§£å†³äº†ä¸‰ç»´CTé‡å»ºé—®é¢˜ã€‚</li>
<li>NERDæ–¹æ³•ç»“åˆäº†æ‰©æ•£æ¨¡å‹å’ŒL1æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œæé«˜äº†ç›¸é‚»åˆ‡ç‰‡ä¹‹é—´çš„ç©ºé—´è¿ç»­æ€§ã€‚</li>
<li>L1æ­£åˆ™åŒ–æœ‰åŠ©äºå‡å°‘åˆ‡ç‰‡é—´ä¼ªå½±ï¼Œä¿ƒè¿›ä½“ç§¯é‡å»ºçš„ä¸€è‡´æ€§ã€‚</li>
<li>ä»‹ç»äº†ä¸¤ç§ä¼˜åŒ–ç­–ç•¥ï¼šåŸºäºADMMå’ŒPDHGçš„æ–¹æ³•ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒNERDæ–¹æ³•åœ¨åŒ»å­¦ä¸‰ç»´CTæ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº†æœ€å‰æ²¿æˆ–é«˜åº¦ç«äº‰çš„ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºè§£å†³å…¶ä»–ä¸‰ç»´å›¾åƒé‡å»ºé—®é¢˜æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b442a708b43495bfae680857b9f75606" align="middle">
<img src="https://picx.zhimg.com/v2-af17de49d36e2683f624599da9881579" align="middle">
<img src="https://picx.zhimg.com/v2-c20a8e90f14156077108de4904cf720e" align="middle">
<img src="https://picx.zhimg.com/v2-b2733f8e40242c04c451438cf599129d" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-segmentation-of-retinal-arteries-and-veins-using-cardiac-signal-in-doppler-holograms"><a href="#Improving-segmentation-of-retinal-arteries-and-veins-using-cardiac-signal-in-doppler-holograms" class="headerlink" title="Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms"></a>Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms</h2><p><strong>Authors:Marius Dubosc, Yann Fischer, Zacharie Auray, Nicolas Boutry, Edwin Carlinet, Michael Atlan, Thierry Geraud</strong></p>
<p>Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/DigitalHolography/">https://huggingface.co/datasets/DigitalHolography/</a></p>
<blockquote>
<p>å¤šæ™®å‹’å…¨æ¯æœ¯æ˜¯ä¸€ç§æ–°å…´è§†ç½‘è†œæˆåƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿä»¥é«˜æ—¶é—´åˆ†è¾¨ç‡æ•æ‰è¡€æµçš„åŠ¨æ€è¡Œä¸ºï¼Œå®ç°å¯¹è§†ç½‘è†œè¡€æµåŠ¨åŠ›å­¦å®šé‡è¯„ä¼°ã€‚è¿™éœ€è¦å‡†ç¡®åœ°å¯¹è§†ç½‘è†œåŠ¨è„‰å’Œé™è„‰è¿›è¡Œåˆ†å‰²ï¼Œä½†ä¼ ç»Ÿåˆ†å‰²æ–¹æ³•åªå…³æ³¨ç©ºé—´ä¿¡æ¯ï¼Œå¿½ç•¥äº†å…¨æ¯æ•°æ®çš„ä¸°å¯Œæ—¶é—´ä¿¡æ¯ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºæ ‡å‡†åˆ†å‰²æ¶æ„çš„åŠ¨è„‰é™è„‰åˆ†å‰²æ–¹æ³•ã€‚é€šè¿‡ç»“åˆæ¥è‡ªä¸“ç”¨è„‰å†²åˆ†æç®¡é“çš„ç‰¹å¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸ä¼ ç»ŸU-Netåˆ©ç”¨æ—¶é—´åŠ¨æ€ä¿¡æ¯ï¼Œå¹¶å®ç°ä¸æ›´å¤æ‚åŸºäºæ³¨æ„åŠ›æˆ–è¿­ä»£æ¨¡å‹çš„æ€§èƒ½ç›¸å½“çš„æ•ˆæœã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ—¶é—´è§£æé¢„å¤„ç†å¯ä»¥è§£é”æ·±åº¦å­¦ä¹ åœ¨å¤šæ™®å‹’å…¨æ¯æœ¯ä¸­çš„æ½œåŠ›ï¼Œä¸ºè§†ç½‘è†œè¡€æµåŠ¨åŠ›å­¦å®šé‡ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚æ•°æ®é›†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/DigitalHolography/%E4%B8%8A%E5%8F%AF%E4%B8%8B%E8%BD%BD%E4%BD%BF%E7%94%A8%E3%80%82">https://huggingface.co/datasets/DigitalHolography/ä¸Šå¯ä¸‹è½½ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14654v1">PDF</a> 5 pages, 3 figures, 1 table. Submitted to ISBI2026</p>
<p><strong>Summary</strong></p>
<p>å¤šæ™®å‹’å…¨æ¯æœ¯æ˜¯ä¸€ç§æ–°å…´çš„è§†ç½‘è†œæˆåƒæŠ€æœ¯ï¼Œèƒ½æ•æ‰è¡€æµçš„åŠ¨æ€è¡Œä¸ºï¼Œå…·æœ‰é«˜çš„æ—¶é—´åˆ†è¾¨ç‡ï¼Œå¯ä»¥å¯¹è§†ç½‘è†œè¡€æµåŠ¨åŠ›å­¦è¿›è¡Œå®šé‡è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŠ¨è„‰é™è„‰åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆæ ‡å‡†åˆ†å‰²æ¶æ„å¹¶åˆ©ç”¨è„‰å†²åˆ†æç®¡é“çš„ç‰¹å¾ï¼Œå…è®¸ä¼ ç»Ÿçš„U-Netåˆ©ç”¨æ—¶é—´åŠ¨æ€ä¿¡æ¯ï¼Œå®ç°ä¸æ›´å¤æ‚åŸºäºæ³¨æ„åŠ›æˆ–è¿­ä»£æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚è¿™ä¸ºå¤šæ™®å‹’å…¨æ¯æœ¯æ·±åº¦å­¦ä¹ è§£é”äº†æ½œåŠ›ï¼Œä¸ºè§†ç½‘è†œè¡€æµåŠ¨åŠ›å­¦çš„å®šé‡ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™®å‹’å…¨æ¯æœ¯æ˜¯ä¸€ç§æ–°å…´è§†ç½‘è†œæˆåƒæŠ€æœ¯ï¼Œèƒ½æ•æ‰è¡€æµåŠ¨æ€è¡Œä¸ºå¹¶å…·æœ‰é«˜æ—¶é—´åˆ†è¾¨ç‡ã€‚</li>
<li>ä¼ ç»Ÿåˆ†å‰²æ–¹æ³•ä¸»è¦å…³æ³¨ç©ºé—´ä¿¡æ¯ï¼Œå¿½ç•¥äº†å…¨æ¯æ•°æ®çš„ä¸°å¯Œæ—¶é—´ä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ ‡å‡†åˆ†å‰²æ¶æ„çš„åŠ¨è„‰é™è„‰åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è„‰å†²åˆ†æçš„ç‰¹å¾ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨æ—¶é—´åŠ¨æ€ä¿¡æ¯ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¸æ›´å¤æ‚çš„æ³¨æ„åŠ›æˆ–è¿­ä»£æ¨¡å‹ç›¸å½“ã€‚</li>
<li>æ—¶é—´è§£æé¢„å¤„ç†å¯ä»¥è§£é”æ·±åº¦å­¦ä¹ åœ¨å¤šæ™®å‹’å…¨æ¯æœ¯ä¸­çš„æ½œåŠ›ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•å…¬å¼€äº†æ•°æ®é›†ä»¥ä¾›ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7860a17c5d876cfd1ef2bff01d2206bc" align="middle">
<img src="https://picx.zhimg.com/v2-f6d0b74b7dc8d5358e3a918cf281e9eb" align="middle">
<img src="https://picx.zhimg.com/v2-49f2a840751a109cd4cdd5d5b0e7be04" align="middle">
<img src="https://picx.zhimg.com/v2-ca0cd9b65970b0cb44db10e2dbbe4e31" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RepAir-A-Framework-for-Airway-Segmentation-and-Discontinuity-Correction-in-CT"><a href="#RepAir-A-Framework-for-Airway-Segmentation-and-Discontinuity-Correction-in-CT" class="headerlink" title="RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT"></a>RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT</h2><p><strong>Authors:John M. Oyer, Ali Namvar, Benjamin A. Hoff, Wassim W. Labaki, Ella A. Kazerooni, Charles R. Hatt, Fernando J. Martinez, MeiLan K. Han, Craig J. GalbÃ¡n, Sundaresh Ram</strong></p>
<p>Accurate airway segmentation from chest computed tomography (CT) scans is essential for quantitative lung analysis, yet manual annotation is impractical and many automated U-Net-based methods yield disconnected components that hinder reliable biomarker extraction. We present RepAir, a three-stage framework for robust 3D airway segmentation that combines an nnU-Net-based network with anatomically informed topology correction. The segmentation network produces an initial airway mask, after which a skeleton-based algorithm identifies potential discontinuities and proposes reconnections. A 1D convolutional classifier then determines which candidate links correspond to true anatomical branches versus false or obstructed paths. We evaluate RepAir on two distinct datasets: ATMâ€™22, comprising annotated CT scans from predominantly healthy subjects and AeroPath, encompassing annotated scans with severe airway pathology. Across both datasets, RepAir outperforms existing 3D U-Net-based approaches such as Bronchinet and NaviAirway on both voxel-level and topological metrics, and produces more complete and anatomically consistent airway trees while maintaining high segmentation accuracy.</p>
<blockquote>
<p>ä»èƒ¸éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­å‡†ç¡®åˆ†å‰²æ°”é“å¯¹äºå®šé‡è‚ºéƒ¨åˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æ ‡æ³¨æ˜¯ä¸åˆ‡å®é™…çš„ï¼Œè®¸å¤šåŸºäºU-Netçš„è‡ªåŠ¨åŒ–æ–¹æ³•ä¼šäº§ç”Ÿæ–­å¼€çš„ç»„ä»¶ï¼Œé˜»ç¢äº†å¯é çš„ç”Ÿç‰©æ ‡å¿—ç‰©æå–ã€‚æˆ‘ä»¬æå‡ºäº†RepAirï¼Œè¿™æ˜¯ä¸€ä¸ªç¨³å¥çš„3Dæ°”é“åˆ†å‰²ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œå®ƒå°†åŸºäºnnU-Netçš„ç½‘ç»œä¸è§£å‰–ä¿¡æ¯æ‹“æ‰‘æ ¡æ­£ç›¸ç»“åˆã€‚åˆ†å‰²ç½‘ç»œç”Ÿæˆåˆå§‹æ°”é“æ©è†œï¼Œä¹‹ååŸºäºéª¨æ¶çš„ç®—æ³•è¯†åˆ«æ½œåœ¨çš„ä¸è¿ç»­ç‚¹å¹¶æå‡ºé‡æ–°è¿æ¥ã€‚ç„¶åï¼Œä¸€ç»´å·ç§¯åˆ†ç±»å™¨ç¡®å®šå“ªäº›å€™é€‰é“¾æ¥å¯¹åº”äºçœŸæ­£çš„è§£å‰–åˆ†æ”¯ï¼Œå“ªäº›æ˜¯é”™è¯¯æˆ–å—é˜»çš„è·¯å¾„ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šå¯¹RepAirè¿›è¡Œäº†è¯„ä¼°ï¼šATMâ€™22ä¸»è¦ç”±å¥åº·å—è¯•è€…çš„æ³¨é‡ŠCTæ‰«æç»„æˆï¼ŒAeroPathåˆ™åŒ…å«å¸¦æœ‰ä¸¥é‡æ°”é“ç—…ç†çš„æ³¨é‡Šæ‰«æã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šï¼ŒRepAiråœ¨ä½“ç´ çº§å’Œæ‹“æ‰‘æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„3D U-Netæ–¹æ³•ï¼ˆå¦‚Bronchinetå’ŒNaviAirwayï¼‰ï¼Œå¹¶ä¸”äº§ç”Ÿçš„æ°”é“æ ‘æ›´åŠ å®Œæ•´ä¸”è§£å‰–ä¸Šä¸€è‡´ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„åˆ†å‰²ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14649v1">PDF</a> 4 pages, 3 figures, 1 table. Preprint submitted to SSIAI 2026 Conference on November 17, 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRepAirçš„ç¨³å¥ä¸‰ç»´æ°”é“åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†nnU-Netç½‘ç»œåŠè§£å‰–å­¦ä¿¡æ¯æ‹“æ‰‘ä¿®æ­£æŠ€æœ¯ã€‚å®ƒèƒ½ä»èƒ¸éƒ¨CTæ‰«æä¸­å‡†ç¡®åˆ†å‰²æ°”é“ï¼Œè§£å†³äº†æ‰‹åŠ¨æ ‡æ³¨ä¸å®ç”¨åŠç°æœ‰U-Netæ–¹æ³•äº§ç”Ÿçš„åˆ†å‰²ç»„ä»¶ä¸è¿ç»­ç­‰é—®é¢˜ã€‚RepAiré€šè¿‡ä¸‰ä¸ªé˜¶æ®µå®Œæˆåˆ†å‰²ï¼šé¦–å…ˆç”±åˆ†å‰²ç½‘ç»œç”Ÿæˆåˆå§‹æ°”é“æ©è†œï¼Œæ¥ç€é€šè¿‡åŸºäºéª¨æ¶çš„ç®—æ³•è¯†åˆ«æ½œåœ¨çš„ä¸è¿ç»­ç‚¹å¹¶å»ºè®®é‡æ–°è¿æ¥ï¼Œæœ€åé€šè¿‡ä¸€ç»´å·ç§¯åˆ†ç±»å™¨ç¡®å®šå“ªäº›å€™é€‰é“¾æ¥æ˜¯çœŸæ­£çš„è§£å‰–åˆ†æ”¯æˆ–å‡è·¯å¾„ã€‚åœ¨ATMâ€™22å’ŒAeroPathä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRepAiråœ¨ä½“ç´ çº§å’Œæ‹“æ‰‘åº¦é‡ä¸Šå‡ä¼˜äºç°æœ‰çš„ä¸‰ç»´U-Netæ–¹æ³•ï¼ˆå¦‚Bronchinetå’ŒNaviAirwayï¼‰ï¼Œèƒ½ç”Ÿæˆæ›´å®Œæ•´ä¸”è§£å‰–å­¦ä¸Šä¸€è‡´çš„æ°”é“æ ‘ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„åˆ†å‰²ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RepAiræ¡†æ¶æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸‰ç»´æ°”é“åˆ†å‰²çš„ç¨³å¥æ–¹æ³•ï¼Œç»“åˆäº†nnU-Netç½‘ç»œå’Œè§£å‰–å­¦ä¿¡æ¯æ‹“æ‰‘ä¿®æ­£æŠ€æœ¯ã€‚</li>
<li>æ°”é“åˆ†å‰²å­˜åœ¨å‡†ç¡®æ€§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ‰‹åŠ¨æ ‡æ³¨çš„ä¸å®ç”¨æ€§å’Œç°æœ‰U-Netæ–¹æ³•äº§ç”Ÿçš„ç»„ä»¶ä¸è¿ç»­é—®é¢˜ã€‚</li>
<li>RepAiré€šè¿‡ä¸‰ä¸ªé˜¶æ®µå®Œæˆåˆ†å‰²ï¼šåˆå§‹æ°”é“æ©è†œçš„ç”Ÿæˆã€æ½œåœ¨ä¸è¿ç»­ç‚¹çš„è¯†åˆ«åŠé‡æ–°è¿æ¥çš„å»ºè®®ã€ä»¥åŠé€šè¿‡å·ç§¯åˆ†ç±»å™¨ç¡®å®šè§£å‰–åˆ†æ”¯ä¸å‡è·¯å¾„ã€‚</li>
<li>å®éªŒåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ATMâ€™22å’ŒAeroPathæ•°æ®é›†ã€‚</li>
<li>RepAiråœ¨ä½“ç´ çº§å’Œæ‹“æ‰‘åº¦é‡ä¸Šä¼˜äºç°æœ‰çš„ä¸‰ç»´U-Netæ–¹æ³•ï¼ˆå¦‚Bronchinetå’ŒNaviAirwayï¼‰ã€‚</li>
<li>RepAirèƒ½ç”Ÿæˆæ›´å®Œæ•´ä¸”è§£å‰–å­¦ä¸Šä¸€è‡´çš„æ°”é“æ ‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14649">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16741b6a4e806216f5837e532b4ef25b" align="middle">
<img src="https://picx.zhimg.com/v2-72a890e3b2d3a0e01d9c872377e80095" align="middle">
<img src="https://picx.zhimg.com/v2-fb7f4056de7a61158fecebb4dae1c88c" align="middle">
<img src="https://picx.zhimg.com/v2-337e4fd9ae032fe6fab4bc822fdfa3f8" align="middle">
<img src="https://picx.zhimg.com/v2-697157c4b7bd46fc7edbc515374c5e6d" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SparseSurf-Sparse-View-3D-Gaussian-Splatting-for-Surface-Reconstruction"><a href="#SparseSurf-Sparse-View-3D-Gaussian-Splatting-for-Surface-Reconstruction" class="headerlink" title="SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction"></a>SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction</h2><p><strong>Authors:Meiying Gu, Jiawei Zhang, Jiahe Li, Xiaohan Yu, Haonan Luo, Jin Zheng, Xiao Bai</strong></p>
<p>Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.</p>
<blockquote>
<p>å…³äºä¼˜åŒ–é«˜æ–¯æ··æ¶‚ï¼ˆGaussian Splattingï¼‰ä»¥ç”¨äºåœºæ™¯å‡ ä½•çš„æœ€æ–°è¿›å±•ï¼Œå·²ç»èƒ½å¤Ÿå®ç°ä»å›¾åƒé«˜æ•ˆé‡å»ºç»†èŠ‚ä¸°å¯Œçš„è¡¨é¢ã€‚ç„¶è€Œï¼Œå½“è¾“å…¥è§†è§’è¾ƒä¸ºç¨€ç–æ—¶ï¼Œè¿™ç§ä¼˜åŒ–å®¹æ˜“å‡ºç°è¿‡åº¦æ‹Ÿåˆçš„æƒ…å†µï¼Œå¯¼è‡´é‡å»ºè´¨é‡ä¸å°½äººæ„ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡é‡‡ç”¨æ‰å¹³åŒ–é«˜æ–¯åŸºå…ƒä»¥æ›´å¥½åœ°é€‚åº”è¡¨é¢å‡ ä½•ç»“æ„ï¼Œå¹¶ç»“åˆæ·±åº¦æ­£åˆ™åŒ–æ¥ç¼“è§£æœ‰é™è§†è§’ä¸‹çš„å‡ ä½•æ­§ä¹‰æ€§æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œæ‰å¹³åŒ–é«˜æ–¯æ‰€å›ºæœ‰çš„å„å‘å¼‚æ€§å¢åŠ åœ¨ç¨€ç–è§†è§’åœºæ™¯ä¸­åŠ å‰§äº†è¿‡åº¦æ‹Ÿåˆé—®é¢˜ï¼Œé˜»ç¢äº†ç²¾ç¡®çš„è¡¨é¢æ‹Ÿåˆå’Œæ–°é¢–è§†è§’åˆæˆæ€§èƒ½çš„é™ä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†net{}æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé‡å»ºæ›´åŠ ç²¾ç¡®å’Œç»†èŠ‚ä¸°å¯Œçš„è¡¨é¢ï¼ŒåŒæ—¶ä¿ç•™é«˜è´¨é‡çš„æ–°é¢–è§†è§’æ¸²æŸ“ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯å¼•å…¥ç«‹ä½“å‡ ä½•çº¹ç†å¯¹é½ï¼ˆStereo Geometry-Texture Alignmentï¼‰ï¼Œå®ƒæ¶èµ·äº†æ¸²æŸ“è´¨é‡å’Œå‡ ä½•ä¼°è®¡ä¹‹é—´çš„æ¡¥æ¢ï¼Œä»è€Œè”åˆå¢å¼ºè¡¨é¢é‡å»ºå’Œè§†è§’åˆæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¼ªç‰¹å¾å¢å¼ºå‡ ä½•ä¸€è‡´æ€§ï¼ˆPseudo-Feature Enhanced Geometry Consistencyï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç»“åˆè®­ç»ƒè§†è§’å’Œæœªè§è¿‡çš„è§†è§’æ¥å¼ºåˆ¶æ‰§è¡Œå¤šè§†è§’å‡ ä½•ä¸€è‡´æ€§ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ç”±ç¨€ç–ç›‘ç£å¼•èµ·çš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚åœ¨DTUã€BlendedMVSå’ŒMip-NeRF360æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14633v1">PDF</a> Accepted at AAAI 2026. Project page: <a target="_blank" rel="noopener" href="https://miya-oi.github.io/SparseSurf-project">https://miya-oi.github.io/SparseSurf-project</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åœºæ™¯å‡ ä½•ä¼˜åŒ–çš„é«˜æ–¯æ··åˆæŠ€æœ¯çš„æ–°è¿›å±•ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿä»å›¾åƒä¸­é‡å»ºè¯¦ç»†çš„è¡¨é¢ã€‚ç„¶è€Œï¼Œå½“è¾“å…¥è§†è§’ç¨€ç–æ—¶ï¼Œè¯¥æŠ€æœ¯å®¹æ˜“è¿‡åº¦æ‹Ÿåˆï¼Œå¯¼è‡´é‡å»ºè´¨é‡ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•\net{}ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒé«˜è´¨é‡æ–°é¢–è§†å›¾æ¸²æŸ“çš„åŒæ—¶ï¼Œé‡å»ºæ›´å‡†ç¡®ã€æ›´è¯¦ç»†çš„è¡¨é¢ã€‚æœ¬æ–‡çš„å…³é”®è§è§£æ˜¯å¼•å…¥ç«‹ä½“å‡ ä½•çº¹ç†å¯¹é½æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿæ¡¥æ¥æ¸²æŸ“è´¨é‡å’Œå‡ ä½•ä¼°è®¡ï¼Œä»è€ŒåŒæ—¶æé«˜è¡¨é¢é‡å»ºå’Œè§†å›¾åˆæˆçš„è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ä¼ªç‰¹å¾å¢å¼ºå‡ ä½•ä¸€è‡´æ€§æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè®­ç»ƒè§†è§’å’Œæœªè§è§†è§’ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ç¨€ç–ç›‘ç£å¼•èµ·çš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚åœ¨DTUã€BlendedMVSå’ŒMip-NeRF360æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é«˜æ–¯æ··åˆæŠ€æœ¯åœ¨åœºæ™¯å‡ ä½•ä¼˜åŒ–ä¸­çš„æ–°è¿›å±•ï¼Œèƒ½å¤Ÿä»å›¾åƒé‡å»ºè¯¦ç»†è¡¨é¢ã€‚</li>
<li>åœ¨è¾“å…¥è§†è§’ç¨€ç–æ—¶ï¼Œç°æœ‰æŠ€æœ¯å®¹æ˜“è¿‡åº¦æ‹Ÿåˆï¼Œå¯¼è‡´é‡å»ºè´¨é‡ä¸ä½³ã€‚</li>
<li>å¼•å…¥\net{}æ–¹æ³•ï¼Œæé«˜äº†è¡¨é¢é‡å»ºçš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„æ–°é¢–è§†å›¾æ¸²æŸ“ã€‚</li>
<li>æå‡ºäº†ç«‹ä½“å‡ ä½•çº¹ç†å¯¹é½æŠ€æœ¯ï¼Œæ¡¥æ¥æ¸²æŸ“è´¨é‡å’Œå‡ ä½•ä¼°è®¡ï¼Œå¢å¼ºè¡¨é¢é‡å»ºå’Œè§†å›¾åˆæˆçš„è´¨é‡ã€‚</li>
<li>å¼•å…¥äº†ä¼ªç‰¹å¾å¢å¼ºå‡ ä½•ä¸€è‡´æ€§æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè®­ç»ƒè§†è§’å’Œæœªè§è§†è§’ï¼Œç¼“è§£ç¨€ç–ç›‘ç£å¼•èµ·çš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14bc74961559f38ea1dd0ea77b558f3f" align="middle">
<img src="https://picx.zhimg.com/v2-2fa832c45609bf74ca853b1abd44e173" align="middle">
<img src="https://picx.zhimg.com/v2-4a8b4de7dcb974acc87743792b9607b7" align="middle">
<img src="https://picx.zhimg.com/v2-8ca96a1349fb290f7ae81dfd3b87f9a0" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="XAttn-BMD-Multimodal-Deep-Learning-with-Cross-Attention-for-Femoral-Neck-Bone-Mineral-Density-Estimation"><a href="#XAttn-BMD-Multimodal-Deep-Learning-with-Cross-Attention-for-Femoral-Neck-Bone-Mineral-Density-Estimation" class="headerlink" title="XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation"></a>XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation</h2><p><strong>Authors:Yilin Zhang, Leo D. Westbury, Elaine M. Dennison, Nicholas C. Harvey, Nicholas R. Fuggle, Rahman Attar</strong></p>
<p>Poor bone health is a significant public health concern, and low bone mineral density (BMD) leads to an increased fracture risk, a key feature of osteoporosis. We present XAttn-BMD (Cross-Attention BMD), a multimodal deep learning framework that predicts femoral neck BMD from hip X-ray images and structured clinical metadata. It utilizes a novel bidirectional cross-attention mechanism to dynamically integrate image and metadata features for cross-modal mutual reinforcement. A Weighted Smooth L1 loss is tailored to address BMD imbalance and prioritize clinically significant cases. Extensive experiments on the data from the Hertfordshire Cohort Study show that our model outperforms the baseline models in regression generalization and robustness. Ablation studies confirm the effectiveness of both cross-attention fusion and the customized loss function. Experimental results show that the integration of multimodal data via cross-attention outperforms naive feature concatenation without cross-attention, reducing MSE by 16.7%, MAE by 6.03%, and increasing the R2 score by 16.4%, highlighting the effectiveness of the approach for femoral neck BMD estimation. Furthermore, screening performance was evaluated using binary classification at clinically relevant femoral neck BMD thresholds, demonstrating the modelâ€™s potential in real-world scenarios.</p>
<blockquote>
<p>éª¨å¥åº·ä¸è‰¯æ˜¯ä¸€ä¸ªé‡è¦çš„å…¬å…±å«ç”Ÿé—®é¢˜ï¼Œä½éª¨çŸ¿ç‰©è´¨å¯†åº¦ï¼ˆBMDï¼‰ä¼šå¢åŠ éª¨æŠ˜é£é™©ï¼Œè¿™æ˜¯éª¨è´¨ç–æ¾ç—‡çš„ä¸»è¦ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†XAttn-BMDï¼ˆè·¨æ³¨æ„åŠ›BMDï¼‰ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥ä»é«‹å…³èŠ‚Xå°„çº¿å›¾åƒå’Œç»“æ„åŒ–çš„ä¸´åºŠå…ƒæ•°æ®ä¸­é¢„æµ‹è‚¡éª¨é¢ˆBMDã€‚å®ƒåˆ©ç”¨ä¸€ç§æ–°å‹åŒå‘è·¨æ³¨æ„åŠ›æœºåˆ¶æ¥åŠ¨æ€èåˆå›¾åƒå’Œå…ƒæ•°æ®ç‰¹å¾ï¼Œå®ç°è·¨æ¨¡æ€ç›¸äº’å¢å¼ºã€‚é’ˆå¯¹BMDä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬å®šåˆ¶äº†åŠ æƒå¹³æ»‘L1æŸå¤±ï¼Œå¹¶ä¼˜å…ˆè€ƒè™‘ä¸´åºŠæ„ä¹‰é‡å¤§çš„ç—…ä¾‹ã€‚åœ¨èµ«ç‰¹ç¦å¾·éƒ¡é˜Ÿåˆ—ç ”ç©¶çš„æ•°æ®ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å›å½’æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ¶ˆèç ”ç©¶è¯å®äº†è·¨æ³¨æ„åŠ›èåˆå’Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›èåˆå¤šæ¨¡æ€æ•°æ®çš„æ–¹æ³•ä¼˜äºæ²¡æœ‰è·¨æ³¨æ„åŠ›çš„ç‰¹å¾ç®€å•æ‹¼æ¥æ–¹æ³•ï¼Œé™ä½äº†å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰16.7%ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½äº†6.03%ï¼Œå¹¶æé«˜äº†R2åˆ†æ•°è¾¾16.4%ï¼Œå‡¸æ˜¾äº†è¯¥æ–¹æ³•åœ¨è‚¡éª¨é¢ˆBMDä¼°è®¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ä¸´åºŠä¸Šè‚¡éª¨é¢ˆBMDç›¸å…³é˜ˆå€¼çš„äºŒå…ƒåˆ†ç±»å¯¹ç­›æŸ¥æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºäº†è¯¥æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14604v1">PDF</a> 11 figures, 10 tables, 38 pages. Submitted to Artificial Intelligence in Medicine (currently with editor)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†XAttn-BMDç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé¢„æµ‹è‚¡éª¨é¢ˆéª¨å¯†åº¦ï¼ˆBMDï¼‰çš„å¤šæ¨¡å¼æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚å®ƒé€šè¿‡æ•´åˆé«‹å…³èŠ‚Xå…‰å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠå…ƒæ•°æ®ï¼Œåˆ©ç”¨åŒå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡ŒåŠ¨æ€ç‰¹å¾èåˆã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åŠ æƒå¹³æ»‘L1æŸå¤±ä»¥è§£å†³BMDä¸å¹³è¡¡é—®é¢˜å¹¶é‡ç‚¹å…³æ³¨å…·æœ‰ä¸´åºŠæ„ä¹‰çš„æƒ…å†µã€‚åœ¨èµ«ç‰¹ç¦å¾·éƒ¡é˜Ÿåˆ—ç ”ç©¶çš„æ•°æ®ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å›å½’æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>XAttn-BMDæ˜¯ä¸€ä¸ªå¤šæ¨¡å¼æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é¢„æµ‹è‚¡éª¨é¢ˆéª¨å¯†åº¦ï¼ˆBMDï¼‰ã€‚</li>
<li>è¯¥ç³»ç»Ÿåˆ©ç”¨é«‹å…³èŠ‚Xå…‰å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠå…ƒæ•°æ®è¿›è¡ŒåŠ¨æ€ç‰¹å¾èåˆã€‚</li>
<li>åŒå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ç”¨äºå¢å¼ºå›¾åƒå’Œå…ƒæ•°æ®çš„ç›¸äº’ä½œç”¨ã€‚</li>
<li>åŠ æƒå¹³æ»‘L1æŸå¤±å‡½æ•°ç”¨äºå¤„ç†BMDä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶é‡ç‚¹å…³æ³¨ä¸´åºŠæ„ä¹‰æ˜¾è‘—çš„æ¡ˆä¾‹ã€‚</li>
<li>åœ¨èµ«ç‰¹ç¦å¾·éƒ¡é˜Ÿåˆ—ç ”ç©¶çš„æ•°æ®ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒXAttn-BMDåœ¨å›å½’æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
<li>é€šè¿‡äº¤å‰æ³¨æ„åŠ›èåˆå¤šæ¨¡å¼æ•°æ®çš„æ–¹æ³•æ¯”æ— äº¤å‰æ³¨æ„åŠ›çš„ç‰¹å¾æ‹¼æ¥æ–¹æ³•æ›´æœ‰æ•ˆï¼Œåœ¨å‡æ–¹è¯¯å·®ã€å¹³å‡ç»å¯¹è¯¯å·®å’ŒRÂ²åˆ†æ•°æ–¹é¢æœ‰æ‰€æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f78ee0cbf957fab8dfc96c95625f26d" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MRI-Embeddings-Complement-Clinical-Predictors-for-Cognitive-Decline-Modeling-in-Alzheimerâ€™s-Disease-Cohorts"><a href="#MRI-Embeddings-Complement-Clinical-Predictors-for-Cognitive-Decline-Modeling-in-Alzheimerâ€™s-Disease-Cohorts" class="headerlink" title="MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimerâ€™s Disease Cohorts"></a>MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimerâ€™s Disease Cohorts</h2><p><strong>Authors:Nathaniel Putera, Daniel Vilet RodrÃ­guez, Noah Videcrantz, Julia Machnio, Mostafa Mehdipour Ghazi</strong></p>
<p>Accurate modeling of cognitive decline in Alzheimerâ€™s disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.</p>
<blockquote>
<p>å¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…ä¸­çš„è®¤çŸ¥è¡°é€€è¿›è¡Œç²¾ç¡®å»ºæ¨¡å¯¹äºæ—©æœŸåˆ†å±‚å’Œä¸ªæ€§åŒ–ç®¡ç†è‡³å…³é‡è¦ã€‚è™½ç„¶è¡¨æ ¼é¢„æµ‹å› å­æä¾›äº†å…¨çƒé£é™©çš„ç¨³å¥æ ‡å¿—ï¼Œä½†å®ƒä»¬æ•æ‰ç»†å¾®çš„è„‘éƒ¨å˜åŒ–çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¡¨æ ¼å’ŒåŸºäºæˆåƒçš„è¡¨ç¤ºå½¢å¼çš„é¢„æµ‹è´¡çŒ®ï¼Œé‡ç‚¹å…³æ³¨ç”±è½¬æ¢å™¨æ´¾ç”Ÿçš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åµŒå…¥ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºåŠ¨æ€æ—¶é—´å¼¯æ›²èšç±»çš„è½¨è¿¹æ„ŸçŸ¥æ ‡ç­¾ç­–ç•¥ï¼Œä»¥æ•è·è®¤çŸ¥å˜åŒ–çš„ä¸åŒæ¨¡å¼ï¼Œå¹¶é€šè¿‡åœ¨è°ƒå’Œå’Œå¢å¼ºçš„MRIæ•°æ®ä¸Šè¿›è¡Œæ— ç›‘ç£é‡å»ºè®­ç»ƒ3Dè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œä»¥è·å¾—æ— éœ€è¿›å±•æ ‡ç­¾çš„è§£å‰–ä¿ç•™åµŒå…¥ã€‚éšåä½¿ç”¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ†ç±»å™¨å’Œæ·±åº¦å­¦ä¹ å¤´å¯¹é¢„è®­ç»ƒçš„ç¼–ç å™¨åµŒå…¥è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸è¡¨æ ¼è¡¨ç¤ºå’Œå·ç§¯ç½‘ç»œåŸºå‡†è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœçªå‡ºäº†è·¨æ¨¡å¼çš„äº’è¡¥ä¼˜åŠ¿ã€‚ä¸´åºŠå’Œä½“ç§¯ç‰¹å¾åœ¨é¢„æµ‹è½»åº¦è‡³é‡åº¦è¿›å±•æ–¹é¢è¾¾åˆ°äº†çº¦0.70çš„æœ€é«˜AUCå€¼ï¼Œè¿™çªæ˜¾äº†å®ƒä»¬åœ¨æ•æ‰å…¨çƒè¡°é€€è½¨è¿¹æ–¹é¢çš„æ•ˆç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¥è‡ªViTæ¨¡å‹çš„MRIåµŒå…¥åœ¨åŒºåˆ†è®¤çŸ¥ç¨³å®šä¸ªä½“æ–¹é¢æœ€ä¸ºæœ‰æ•ˆï¼ŒAUCä¸º0.71ã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ–¹æ³•åœ¨å¼‚è´¨çš„ä¸­åº¦ç»„ä¸­éƒ½é‡åˆ°äº†å›°éš¾ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä¸´åºŠç‰¹å¾åœ¨è¯†åˆ«é«˜é£é™©æç«¯æƒ…å†µæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒåŸºäºè½¬æ¢å™¨çš„MRIåµŒå…¥æ›´æ•æ„Ÿäºç¨³å®šæ€§çš„ç»†å¾®æ ‡å¿—ï¼Œè¿™æ¿€å‘äº†ä¸ºé˜¿å°”èŒ¨æµ·é»˜æ°ç—‡è¿›å±•å»ºæ¨¡çš„å¤šæ¨¡å¼èåˆç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14601v1">PDF</a> Accepted at SPIE - Medical Imaging Conference 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é˜¿å°”èŒ¨æµ·é»˜ç—…ä¸­çš„è®¤çŸ¥è¡°é€€å‡†ç¡®å»ºæ¨¡çš„é‡è¦æ€§ï¼Œå¹¶è¯„ä¼°äº†è¡¨æ ¼å’Œæˆåƒè¡¨å¾çš„é¢„æµ‹è´¡çŒ®ï¼Œé‡ç‚¹ä»‹ç»äº†åŸºäºTransformerçš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åµŒå…¥ã€‚ç ”ç©¶é‡‡ç”¨åŠ¨æ€æ—¶é—´è§„æ•´èšç±»æ³•æ„å»ºè½¨è¿¹æ„ŸçŸ¥æ ‡ç­¾ç­–ç•¥ï¼Œå¹¶ä½¿ç”¨æ— ç›‘ç£é‡å»ºæ–¹æ³•åœ¨è°ƒå’Œå’Œå¢å¼ºçš„MRIæ•°æ®ä¸Šè®­ç»ƒä¸‰ç»´è§†è§‰Transformerï¼ˆViTï¼‰ï¼Œè·å¾—æ— éœ€è¿›å±•æ ‡ç­¾çš„è§£å‰–ç»“æ„ä¿ç•™åµŒå…¥ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡æ€å…·æœ‰äº’è¡¥ä¼˜åŠ¿ï¼šä¸´åºŠå’Œä½“ç§¯ç‰¹å¾åœ¨é¢„æµ‹è½»åº¦è‡³é‡åº¦è¿›å±•æ–¹é¢è¡¨ç°æœ€ä½³ï¼ŒAUCçº¦ä¸º0.7ï¼›è€ŒViTæ¨¡å‹çš„MRIåµŒå…¥åœ¨åŒºåˆ†è®¤çŸ¥ç¨³å®šä¸ªä½“æ–¹é¢æœ€ä¸ºæœ‰æ•ˆï¼ŒAUCä¸º0.71ã€‚ä½†æ‰€æœ‰æ–¹æ³•åœ¨è¯†åˆ«ä¸­åº¦å¼‚è´¨æ€§è¿›å±•ç»„æ—¶éƒ½é¢ä¸´æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è®¤çŸ¥è¡°é€€å‡†ç¡®å»ºæ¨¡å¯¹äºæ—©æœŸåˆ†å±‚å’Œä¸ªæ€§åŒ–ç®¡ç†è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†è¡¨æ ¼é¢„æµ‹å› ç´ å’Œæˆåƒè¡¨å¾çš„é¢„æµ‹èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„MRIåµŒå…¥ã€‚</li>
<li>é‡‡ç”¨åŠ¨æ€æ—¶é—´è§„æ•´èšç±»æ³•çš„è½¨è¿¹æ„ŸçŸ¥æ ‡ç­¾ç­–ç•¥ç”¨äºæ•æ‰è®¤çŸ¥å˜åŒ–çš„å¼‚è´¨æ¨¡å¼ã€‚</li>
<li>ä½¿ç”¨æ— ç›‘ç£é‡å»ºè®­ç»ƒçš„ViTæ¨¡å‹ç”Ÿæˆè§£å‰–ç»“æ„ä¿ç•™çš„MRIåµŒå…¥ã€‚</li>
<li>ä¸´åºŠå’Œä½“ç§¯ç‰¹å¾åœ¨é¢„æµ‹æç«¯é£é™©æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€ŒåŸºäºTransformerçš„MRIåµŒå…¥æ›´æ“…é•¿è¯†åˆ«å¾®å¦™çš„ç¨³å®šæ€§æ ‡è®°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94956f818b904c3dec1764e186e85e43" align="middle">
<img src="https://picx.zhimg.com/v2-9de916ae1b6f7cb315d4e49c2f1a4ab1" align="middle">
<img src="https://picx.zhimg.com/v2-6b5f6d27323af43b1ba8b80a9da2c34e" align="middle">
<img src="https://picx.zhimg.com/v2-31e6cac26d64636618abe0b184c83f52" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CCSD-Cross-Modal-Compositional-Self-Distillation-for-Robust-Brain-Tumor-Segmentation-with-Missing-Modalities"><a href="#CCSD-Cross-Modal-Compositional-Self-Distillation-for-Robust-Brain-Tumor-Segmentation-with-Missing-Modalities" class="headerlink" title="CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities"></a>CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities</h2><p><strong>Authors:Dongqing Xie, Yonghuang Wu, Zisheng Ai, Jun Min, Zhencun Jiang, Shaojin Geng, Lei Wang</strong></p>
<p>The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.</p>
<blockquote>
<p>è„‘è‚¿ç˜¤çš„ç²¾å‡†åˆ†å‰²åœ¨å¤šæ¨¡æ€MRIçš„ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šä¸­è‡³å…³é‡è¦ã€‚è™½ç„¶èåˆå„ç§MRIåºåˆ—çš„äº’è¡¥ä¿¡æ¯æ˜¯ä¸€ç§å¸¸è§åšæ³•ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œä¸€ç§æˆ–å¤šç§æ¨¡æ€çš„é¢‘ç¹ç¼ºå¤±å¯¹æ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨æ¨¡æ€ç»„åˆè‡ªè’¸é¦ï¼ˆCCSDï¼‰æ¡†æ¶ï¼Œå®ƒå¯ä»¥çµæ´»åœ°å¤„ç†ä»»æ„ç»„åˆçš„è¾“å…¥æ¨¡æ€ã€‚CCSDé‡‡ç”¨å…±äº«ç‰¹å®šç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶èåˆä¸¤ç§è‡ªè’¸é¦ç­–ç•¥ï¼šï¼ˆiï¼‰ä¸€ç§åˆ†å±‚æ¨¡æ€è‡ªè’¸é¦æœºåˆ¶ï¼Œå®ƒé€šè¿‡è·¨æ¨¡æ€å±‚æ¬¡ç»“æ„è½¬ç§»çŸ¥è¯†ï¼Œä»¥å‡å°‘è¯­ä¹‰å·®å¼‚ï¼›ï¼ˆiiï¼‰ä¸€ç§æ¸è¿›å¼æ¨¡æ€ç»„åˆè’¸é¦æ–¹æ³•ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé€æ¸ä¸¢å¤±æ¨¡æ€ï¼Œå¢å¼ºäº†å¯¹ç¼ºå¤±æ¨¡æ€çš„é²æ£’æ€§ã€‚åœ¨å…¬å…±è„‘è‚¿ç˜¤åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCCSDåœ¨å„ç§ç¼ºå¤±æ¨¡æ€åœºæ™¯ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14599v1">PDF</a> 9 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºä¸€ç§è§£å†³å®é™…åº”ç”¨ä¸­MRIå¤šæ¨¡æ€èåˆçš„é—®é¢˜ï¼Œç”±äºç°å®æƒ…å†µä¸­æ¨¡æ€æ•°æ®ç¼ºå¤±å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å½±å“ã€‚é‡‡ç”¨è·¨æ¨¡æ€ç»„åˆè‡ªè’¸é¦ï¼ˆCCSDï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿçµæ´»å¤„ç†ä»»æ„ç»„åˆçš„è¾“å…¥æ¨¡æ€ã€‚CCSDåŒ…å«å…±äº«ç‰¹å®šç¼–ç å™¨è§£ç å™¨æ¶æ„å’Œä¸¤ç§è‡ªè’¸é¦ç­–ç•¥ï¼ŒåŒ…æ‹¬åˆ†å±‚æ¨¡æ€è‡ªè’¸é¦å’Œæ¸è¿›æ¨¡æ€ç»„åˆè’¸é¦ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨å…¬å¼€è„‘è‚¿ç˜¤åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCCSDåœ¨ä¸åŒç¼ºå¤±æ¨¡æ€åœºæ™¯ä¸‹å®ç°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®çš„å¤šæ¨¡æ€MRIè„‘è‚¿ç˜¤åˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>ç°å®ä¸´åºŠç¯å¢ƒä¸­ï¼ŒMRIåºåˆ—ä¸­ä¸€ç§æˆ–å¤šç§æ¨¡æ€çš„ç¼ºå¤±æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„CCSDæ¡†æ¶èƒ½çµæ´»å¤„ç†ä»»æ„ç»„åˆçš„è¾“å…¥æ¨¡æ€ã€‚</li>
<li>CCSDé‡‡ç”¨å…±äº«ç‰¹å®šç¼–ç å™¨è§£ç å™¨æ¶æ„ã€‚</li>
<li>CCSDåŒ…å«ä¸¤ç§è‡ªè’¸é¦ç­–ç•¥ï¼šåˆ†å±‚æ¨¡æ€è‡ªè’¸é¦å’Œæ¸è¿›æ¨¡æ€ç»„åˆè’¸é¦ã€‚</li>
<li>åˆ†å±‚æ¨¡æ€è‡ªè’¸é¦èƒ½å‡å°‘è¯­ä¹‰å·®å¼‚ï¼Œæ¸è¿›æ¨¡æ€ç»„åˆè’¸é¦å¢å¼ºæ¨¡å‹å¯¹ç¼ºå¤±æ¨¡æ€çš„é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60deb5dc3aa3c1075653ea2113ad179f" align="middle">
<img src="https://picx.zhimg.com/v2-daf3dcb45000eb0e82c0ef1a50716942" align="middle">
<img src="https://picx.zhimg.com/v2-38ad9971f9919468193b08d37eca3b5b" align="middle">
<img src="https://picx.zhimg.com/v2-0ac929afee41d8b03af43ce139265f71" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-comparison-of-time-dependent-Cloudy-astrophysical-code-simulations-with-experimental-X-ray-spectra-from-keV-laser-generated-argon-plasmas"><a href="#A-comparison-of-time-dependent-Cloudy-astrophysical-code-simulations-with-experimental-X-ray-spectra-from-keV-laser-generated-argon-plasmas" class="headerlink" title="A comparison of time-dependent Cloudy astrophysical code simulations with experimental X-ray spectra from keV laser-generated argon plasmas"></a>A comparison of time-dependent Cloudy astrophysical code simulations with experimental X-ray spectra from keV laser-generated argon plasmas</h2><p><strong>Authors:N. Rathee, F. P. Keenan, R. J. R. Williams, G. J. Ferland, S. J. Rose, S. White, D. Riley</strong></p>
<p>We have generated strongly photoionized Ar plasmas in experiments designed to use primarily X-ray L-shell line emission generated from Ag foils irradiated by the VULCAN high-power laser at the UK Central Laser Facility. The principle of the experiment is that use of line emission rather than the usual sub-keV quasi-blackbody source allows keV radiation to play a more dominant role compared to softer X-rays and thus mimic the effect of a blackbody with a higher effective spectral temperature. Our aim is to reproduce in the laboratory the extreme photoionization conditions found in accretion-powered astrophysical sources. In this paper, we compare the experimental results on K-$Î²$ X-ray Ar spectra with modelling using the time-dependent version of the Cloudy astrophysical code. The results indicate that photoionized laboratory plasmas can be successfully modelled with codes such as Cloudy that have been developed for application to astrophysical sources. Our comparison of simulation and experiment shows that the flux of sub-keV photons that photoionize the outer-shell electrons can have a significant effect, and that detailed measurements of the X-ray drive spectrum across all photon energy ranges are crucial for accurate modelling of experiments.</p>
<blockquote>
<p>æˆ‘ä»¬åœ¨è‹±å›½ä¸­å¤®æ¿€å…‰è®¾æ–½ä½¿ç”¨VULCANé«˜åŠŸç‡æ¿€å…‰å¯¹é“¶ç®”è¿›è¡Œè¾å°„ï¼Œä»¥æ­¤ç”Ÿæˆä¸»è¦ç”¨äºXå°„çº¿Lå£³å±‚çº¿å‘å°„çš„å®éªŒï¼Œä»è€Œäº§ç”Ÿäº†é«˜åº¦ç”µç¦»çš„æ°©ç­‰ç¦»å­ä½“ã€‚å®éªŒçš„åŸç†æ˜¯åˆ©ç”¨çº¿å‘å°„è€Œéé€šå¸¸çš„äºšåƒç”µå­ä¼å‡†é»‘ä½“æºï¼Œä½¿åƒç”µå­ä¼è¾å°„ç›¸è¾ƒäºè½¯Xå°„çº¿å‘æŒ¥æ›´é‡è¦çš„ä½œç”¨ï¼Œä»è€Œæ¨¡ä»¿å…·æœ‰è¾ƒé«˜æœ‰æ•ˆå…‰è°±æ¸©åº¦çš„é»‘ä½“æ•ˆåº”ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨å®éªŒå®¤ä¸­é‡ç°ç§¯ç›˜ç”µæºå¤©ä½“ç‰©ç†æºä¸­çš„æç«¯å…‰ç¦»å­åŒ–æ¡ä»¶ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†K-$Î²$ Xå°„çº¿æ°©å…‰è°±çš„å®éªŒç»“æœä¸ä½¿ç”¨Cloudyå¤©æ–‡ä»£ç çš„æ—¶é—´ä¾èµ–æ€§ç‰ˆæœ¬è¿›è¡Œçš„å»ºæ¨¡è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨Cloudyç­‰é’ˆå¯¹å¤©ä½“ç‰©ç†æºå¼€å‘çš„ä»£ç å¯ä»¥æˆåŠŸæ¨¡æ‹Ÿå®éªŒå®¤ä¸­çš„å…‰ç¦»å­åŒ–ç­‰ç¦»å­ä½“ã€‚æˆ‘ä»¬å¯¹æ¨¡æ‹Ÿå’Œå®éªŒçš„æ¯”è¾ƒè¡¨æ˜ï¼Œäºšåƒç”µå­ä¼å…‰å­é€šé‡å¯¹å¤–éƒ¨ç”µå­çš„å…‰ç”µç¦»å¯ä»¥äº§ç”Ÿé‡å¤§å½±å“ï¼Œä¸”å¯¹æ‰€æœ‰å…‰å­èƒ½é‡èŒƒå›´å†…çš„Xå°„çº¿é©±åŠ¨å…‰è°±çš„è¯¦ç»†æµ‹é‡å¯¹äºå®éªŒçš„å‡†ç¡®å»ºæ¨¡è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14538v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬å®éªŒåˆ©ç”¨è‹±å›½ä¸­å¤®æ¿€å…‰è®¾æ–½çš„é«˜åŠŸç‡æ¿€å…‰å¯¹é“¶ç®”è¿›è¡Œè¾å°„ï¼Œç”Ÿæˆäº†å¼ºçƒˆçš„ç”µç¦»æ°©ç­‰ç¦»å­ä½“ã€‚å®éªŒåŸç†æ˜¯åˆ©ç”¨çº¿å‘å°„è€Œéå¸¸è§„çš„äºšåƒç”µå­ä¼ç‰¹å‡†é»‘ä½“æºï¼Œä½¿åƒç”µå­ä¼ç‰¹è¾å°„å‘æŒ¥æ›´ä¸»å¯¼ä½œç”¨ï¼Œæ¨¡ä»¿æ›´é«˜æœ‰æ•ˆè°±æ¸©é»‘ä½“çš„æ•ˆæœã€‚å®éªŒç›®çš„æ˜¯åœ¨å®éªŒå®¤ä¸­é‡ç°æç«¯çš„å…‰ç”µç¦»æ¡ä»¶ï¼Œè¿™äº›æ¡ä»¶å­˜åœ¨äºå¼•åŠ›æºäº§ç”Ÿçš„å¤©ä½“ç‰©ç†æºä¸­ã€‚æœ¬æ–‡å°†å®éªŒç»“æœçš„æ°©K-$Î²$å°„çº¿å…‰è°±ä¸äº‘æ¨¡ç³Šä»£ç çš„æ—¶å˜ç‰ˆæœ¬æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœæŒ‡ç¤ºæˆåŠŸçš„åˆ©ç”¨äº‘é›¾ä»£ç å¯¹å®éªŒå®¤å†…çš„å…‰è‡´ç­‰ç¦»å­ä½“è¿›è¡Œäº†å»ºæ¨¡ã€‚å¯¹æ¯”ä»¿çœŸå’Œå®éªŒè¡¨æ˜ï¼Œäºšåƒç”µå­ä¼ç‰¹å…‰å­çš„æµé‡å¯¹å¤–éƒ¨ç”µå­å±‚çš„å…‰ç”µç¦»ä½œç”¨æ˜æ˜¾ï¼Œè€Œè·¨è¶Šæ‰€æœ‰å…‰å­èƒ½é‡èŒƒå›´çš„Xå°„çº¿é©±åŠ¨å…‰è°±çš„ç²¾ç¡®æµ‹é‡å¯¹äºå®éªŒçš„ç²¾ç¡®å»ºæ¨¡è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®éªŒæˆåŠŸç”Ÿæˆäº†å¼ºçƒˆç”µç¦»çš„æ°©ç­‰ç¦»å­ä½“ï¼Œä½¿ç”¨è‹±å›½ä¸­å¤®æ¿€å…‰è®¾æ–½çš„é«˜åŠŸç‡æ¿€å…‰å’Œé“¶ç®”è¾å°„å®ç°ã€‚</li>
<li>å®éªŒåŸç†åœ¨äºåˆ©ç”¨çº¿å‘å°„æ¨¡ä»¿æ›´é«˜æœ‰æ•ˆè°±æ¸©é»‘ä½“çš„æ•ˆæœï¼Œä½¿åƒç”µå­ä¼ç‰¹è¾å°„å‘æŒ¥ä¸»å¯¼ä½œç”¨ã€‚</li>
<li>å®éªŒç›®çš„æ˜¯åœ¨å®éªŒå®¤ç¯å¢ƒä¸­é‡ç°å¤©ä½“ç‰©ç†æºä¸­çš„æç«¯å…‰ç”µç¦»æ¡ä»¶ã€‚</li>
<li>å®éªŒç»“æœçš„æ°©K-$Î²$å°„çº¿å…‰è°±ä¸äº‘æ¨¡ç³Šæ¨¡å‹çš„æ¯”è¾ƒæˆåŠŸã€‚</li>
<li>äº‘æ¨¡ç³Šä»£ç æˆåŠŸåœ°åº”ç”¨äºå®éªŒå®¤å…‰è‡´ç­‰ç¦»å­ä½“çš„å»ºæ¨¡ã€‚</li>
<li>å®éªŒä¸ä»¿çœŸå¯¹æ¯”æ˜¾ç¤ºï¼Œäºšåƒç”µå­ä¼ç‰¹å…‰å­æµé‡å¯¹å¤–éƒ¨ç”µå­å±‚çš„å…‰ç”µç¦»æœ‰æ˜¾è‘—å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14538">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2c328387adfd7e56c5f22c564117c3b" align="middle">
<img src="https://picx.zhimg.com/v2-cccfc2f5d3ef4ba407aa931deb62259d" align="middle">
<img src="https://picx.zhimg.com/v2-cc388dc2b83b66094fac324d20a49c5d" align="middle">
<img src="https://picx.zhimg.com/v2-b30fb5a72b2d1002a020f8d0cd75fb52" align="middle">
<img src="https://picx.zhimg.com/v2-3d13864b1e6c9b553dd75c5401513085" align="middle">
<img src="https://picx.zhimg.com/v2-0f6373f6a924208b6537b67997d282d9" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="D-PerceptCT-Deep-Perceptual-Enhancement-for-Low-Dose-CT-Images"><a href="#D-PerceptCT-Deep-Perceptual-Enhancement-for-Low-Dose-CT-Images" class="headerlink" title="D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images"></a>D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images</h2><p><strong>Authors:Taifour Yousra Nabila, Azeddine Beghdadi, Marie Luong, Zuheng Ming, Habib Zaidi, Faouzi Alaya Cheikh</strong></p>
<p>Low Dose Computed Tomography (LDCT) is widely used as an imaging solution to aid diagnosis and other clinical tasks. However, this comes at the price of a deterioration in image quality due to the low dose of radiation used to reduce the risk of secondary cancer development. While some efficient methods have been proposed to enhance LDCT quality, many overestimate noise and perform excessive smoothing, leading to a loss of critical details. In this paper, we introduce D-PerceptCT, a novel architecture inspired by key principles of the Human Visual System (HVS) to enhance LDCT images. The objective is to guide the model to enhance or preserve perceptually relevant features, thereby providing radiologists with CT images where critical anatomical structures and fine pathological details are perceptu- ally visible. D-PerceptCT consists of two main blocks: 1) a Visual Dual-path Extractor (ViDex), which integrates semantic priors from a pretrained DINOv2 model with local spatial features, allowing the network to incorporate semantic-awareness during enhancement; (2) a Global-Local State-Space block that captures long-range information and multiscale features to preserve the important structures and fine details for diagnosis. In addition, we propose a novel deep perceptual loss, designated as the Deep Perceptual Relevancy Loss Function (DPRLF), which is inspired by human contrast sensitivity, to further emphasize perceptually important features. Extensive experiments on the Mayo2016 dataset demonstrate the effectiveness of D-PerceptCT method for LDCT enhancement, showing better preservation of structural and textural information within LDCT images compared to SOTA methods.</p>
<blockquote>
<p>ä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰ä½œä¸ºä¸€ç§æˆåƒè§£å†³æ–¹æ¡ˆï¼Œå¹¿æ³›åº”ç”¨äºè¾…åŠ©è¯Šæ–­å’Œä¸´åºŠä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºé‡‡ç”¨ä½å‰‚é‡è¾å°„ä»¥é™ä½äºŒæ¬¡ç™Œç—‡é£é™©ï¼Œå…¶å›¾åƒè´¨é‡å¾€å¾€ä¼šå—åˆ°å½±å“ã€‚è™½ç„¶å·²æœ‰ä¸€äº›æœ‰æ•ˆçš„æ–¹æ³•æå‡ºæ”¹å–„LDCTçš„è´¨é‡ï¼Œä½†å®ƒä»¬æœ‰æ—¶ä¼šè¿‡åº¦ä¼°è®¡å™ªå£°å¹¶è¿‡åº¦å¹³æ»‘å¤„ç†ï¼Œå¯¼è‡´å…³é”®ç»†èŠ‚ä¸¢å¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†D-PerceptCTï¼Œè¿™æ˜¯ä¸€ç§å—äººç±»è§†è§‰ç³»ç»Ÿï¼ˆHVSï¼‰å…³é”®åŸç†å¯å‘çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨å¢å¼ºLDCTå›¾åƒã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼•å¯¼æ¨¡å‹å¢å¼ºæˆ–ä¿ç•™æ„ŸçŸ¥ç›¸å…³çš„ç‰¹å¾ï¼Œä»è€Œä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›CTå›¾åƒï¼Œå…¶ä¸­å…³é”®è§£å‰–ç»“æ„å’Œç²¾ç»†ç—…ç†ç»†èŠ‚åœ¨è§†è§‰ä¸Šæ¸…æ™°å¯è§ã€‚D-PerceptCTç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šä¸€æ˜¯è§†è§‰åŒè·¯å¾„æå–å™¨ï¼ˆViDexï¼‰ï¼Œå®ƒç»“åˆäº†æ¥è‡ªé¢„è®­ç»ƒçš„DINOv2æ¨¡å‹çš„è¯­ä¹‰å…ˆéªŒçŸ¥è¯†å’Œå±€éƒ¨ç©ºé—´ç‰¹å¾ï¼Œå…è®¸ç½‘ç»œåœ¨å¢å¼ºè¿‡ç¨‹ä¸­èå…¥è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ï¼›äºŒæ˜¯å…¨å±€-å±€éƒ¨çŠ¶æ€ç©ºé—´æ¨¡å—ï¼Œè¯¥æ¨¡å—æ•æ‰è¿œç¨‹ä¿¡æ¯å’Œå¤šå°ºåº¦ç‰¹å¾ï¼Œä»¥ä¿ç•™é‡è¦çš„ç»“æ„å’Œç²¾ç»†è¯Šæ–­ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ·±åº¦æ„ŸçŸ¥æŸå¤±ï¼Œè¢«ç§°ä¸ºæ·±åº¦æ„ŸçŸ¥ç›¸å…³æ€§æŸå¤±å‡½æ•°ï¼ˆDPRLFï¼‰ï¼Œå®ƒå—åˆ°äººç±»å¯¹æ¯”æ•æ„Ÿåº¦å¯å‘ï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥å¼ºè°ƒæ„ŸçŸ¥é‡è¦çš„ç‰¹å¾ã€‚åœ¨Mayo2016æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒD-PerceptCTæ–¹æ³•åœ¨LDCTå¢å¼ºæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®ƒåœ¨LDCTå›¾åƒä¸­æ›´å¥½åœ°ä¿ç•™äº†ç»“æ„å’Œçº¹ç†ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14518v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰ä½œä¸ºä¸€ç§æˆåƒè§£å†³æ–¹æ¡ˆï¼Œå¹¿æ³›åº”ç”¨äºè¯Šæ–­å’Œå…¶ä»–ä¸´åºŠä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºé‡‡ç”¨ä½å‰‚é‡è¾å°„ä»¥é™ä½äºŒæ¬¡ç™Œç—‡å‘å±•çš„é£é™©ï¼Œå›¾åƒè´¨é‡ä¼šä¸‹é™ã€‚è™½ç„¶å·²æœ‰ä¸€äº›æœ‰æ•ˆçš„æ–¹æ³•æ¥æé«˜LDCTçš„è´¨é‡ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šè¿‡åº¦ä¼°è®¡å™ªå£°ï¼Œè¿›è¡Œè¿‡åº¦å¹³æ»‘å¤„ç†ï¼Œå¯¼è‡´å…³é”®ç»†èŠ‚ä¸¢å¤±ã€‚æœ¬æ–‡ä»‹ç»äº†D-PerceptCTï¼Œä¸€ç§å—äººç±»è§†è§‰ç³»ç»Ÿï¼ˆHVSï¼‰å…³é”®åŸç†å¯å‘çš„å…¨æ–°æ¶æ„ï¼Œç”¨äºå¢å¼ºLDCTå›¾åƒã€‚ç›®æ ‡æ˜¯å¼•å¯¼æ¨¡å‹å¢å¼ºæˆ–ä¿ç•™æ„ŸçŸ¥ç›¸å…³çš„ç‰¹å¾ï¼Œä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›CTå›¾åƒï¼Œå…¶ä¸­å…³é”®è§£å‰–ç»“æ„å’Œç²¾ç»†ç—…ç†ç»†èŠ‚åœ¨æ„ŸçŸ¥ä¸Šæ˜¯å¯è§çš„ã€‚D-PerceptCTç”±ä¸¤ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼š1ï¼‰è§†è§‰åŒè·¯å¾„æå–å™¨ï¼ˆViDexï¼‰ï¼Œå®ƒæ•´åˆäº†é¢„è®­ç»ƒDINOv2æ¨¡å‹çš„è¯­ä¹‰å…ˆéªŒçŸ¥è¯†å’Œå±€éƒ¨ç©ºé—´ç‰¹å¾ï¼Œå…è®¸ç½‘ç»œåœ¨å¢å¼ºè¿‡ç¨‹ä¸­èå…¥è¯­ä¹‰æ„è¯†ï¼›2ï¼‰å…¨å±€-å±€éƒ¨çŠ¶æ€ç©ºé—´æ¨¡å—ï¼Œè¯¥æ¨¡å—æ•æ‰é•¿ç¨‹ä¿¡æ¯å’Œå¤šå°ºåº¦ç‰¹å¾ï¼Œä»¥ä¿ç•™é‡è¦ç»“æ„å’Œç²¾ç»†è¯Šæ–­ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦æ„ŸçŸ¥æŸå¤±ï¼Œç§°ä¸ºæ·±åº¦æ„ŸçŸ¥ç›¸å…³æ€§æŸå¤±å‡½æ•°ï¼ˆDPRLFï¼‰ï¼Œå®ƒå—åˆ°äººç±»å¯¹æ¯”æ•æ„Ÿåº¦å¯å‘ï¼Œä»¥è¿›ä¸€æ­¥å¼ºè°ƒæ„ŸçŸ¥ä¸Šé‡è¦çš„ç‰¹å¾ã€‚åœ¨Mayo2016æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒD-PerceptCTæ–¹æ³•åœ¨LDCTå¢å¼ºæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™LDCTå›¾åƒä¸­çš„ç»“æ„å’Œçº¹ç†ä¿¡æ¯ï¼Œä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LDCTç”±äºé‡‡ç”¨ä½å‰‚é‡è¾å°„ä»¥é™ä½ç™Œç—‡é£é™©è€Œå¹¿æ³›è¢«åº”ç”¨ï¼Œä½†ä¼šå¯¼è‡´å›¾åƒè´¨é‡ä¸‹é™ã€‚</li>
<li>å½“å‰å­˜åœ¨çš„æ–¹æ³•åœ¨å¢å¼ºLDCTå›¾åƒè´¨é‡æ—¶å¯èƒ½ä¼šè¿‡åº¦ä¼°è®¡å™ªå£°å’Œè¿‡åº¦å¹³æ»‘å¤„ç†ï¼Œå¯¼è‡´ä¸¢å¤±å…³é”®ç»†èŠ‚ã€‚</li>
<li>D-PerceptCTæ¶æ„è¢«æå‡ºä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå®ƒç»“åˆäººç±»è§†è§‰ç³»ç»Ÿçš„åŸç†æ¥å¢å¼ºLDCTå›¾åƒã€‚</li>
<li>D-PerceptCTåŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šè§†è§‰åŒè·¯å¾„æå–å™¨ï¼ˆViDexï¼‰å’Œå…¨å±€-å±€éƒ¨çŠ¶æ€ç©ºé—´æ¨¡å—ï¼Œæ—¨åœ¨å¢å¼ºæ„ŸçŸ¥ç›¸å…³ç‰¹å¾å¹¶ä¿ç•™å…³é”®è§£å‰–ç»“æ„å’Œç—…ç†ç»†èŠ‚ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æ·±åº¦æ„ŸçŸ¥æŸå¤±å‡½æ•°DPRLFï¼Œå®ƒåŸºäºäººç±»å¯¹æ¯”æ•æ„Ÿåº¦ï¼Œä»¥å¼ºè°ƒæ„ŸçŸ¥ä¸Šé‡è¦çš„ç‰¹å¾ã€‚</li>
<li>åœ¨Mayo2016æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒD-PerceptCTæ–¹æ³•åœ¨LDCTå›¾åƒå¢å¼ºæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ•ˆæœï¼Œæ¯”ç°æœ‰æŠ€æœ¯æ›´å¥½åœ°ä¿ç•™ç»“æ„å’Œçº¹ç†ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c034a852a4d3dcbc43d616f519e6ceab" align="middle">
<img src="https://picx.zhimg.com/v2-41c3e25f6ec0ce21b8e9f44fdeb7fa63" align="middle">
<img src="https://picx.zhimg.com/v2-62b69288ff782b0a06b35582522c3489" align="middle">
<img src="https://picx.zhimg.com/v2-9532f2ac25babcc9fdb06c564aadddd6" align="middle">
<img src="https://picx.zhimg.com/v2-76ca1f3700734ea809552108d6c79b8e" align="middle">
<img src="https://picx.zhimg.com/v2-84415f87676407e85b9abb6850ea1338" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Segmentation-Aware-Latent-Diffusion-for-Satellite-Image-Super-Resolution-Enabling-Smallholder-Farm-Boundary-Delineation"><a href="#Segmentation-Aware-Latent-Diffusion-for-Satellite-Image-Super-Resolution-Enabling-Smallholder-Farm-Boundary-Delineation" class="headerlink" title="Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation"></a>Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation</h2><p><strong>Authors:Aditi Agarwal, Anjali Jain, Nikita Saxena, Ishan Deshpande, Michal Kazmierski, Abigail Annkah, Nadav Sherman, Karthikeyan Shanmugam, Alok Talekar, Vaibhav Rajan</strong></p>
<p>Delineating farm boundaries through segmentation of satellite images is a fundamental step in many agricultural applications. The task is particularly challenging for smallholder farms, where accurate delineation requires the use of high resolution (HR) imagery which are available only at low revisit frequencies (e.g., annually). To support more frequent (sub-) seasonal monitoring, HR images could be combined as references (ref) with low resolution (LR) images â€“ having higher revisit frequency (e.g., weekly) â€“ using reference-based super-resolution (Ref-SR) methods. However, current Ref-SR methods optimize perceptual quality and smooth over crucial features needed for downstream tasks, and are unable to meet the large scale-factor requirements for this task. Further, previous two-step approaches of SR followed by segmentation do not effectively utilize diverse satellite sources as inputs. We address these problems through a new approach, $\textbf{SEED-SR}$, which uses a combination of conditional latent diffusion models and large-scale multi-spectral, multi-source geo-spatial foundation models. Our key innovation is to bypass the explicit SR task in the pixel space and instead perform SR in a segmentation-aware latent space. This unique approach enables us to generate segmentation maps at an unprecedented 20$\times$ scale factor, and rigorous experiments on two large, real datasets demonstrate up to $\textbf{25.5}$ and $\textbf{12.9}$ relative improvement in instance and semantic segmentation metrics respectively over approaches based on state-of-the-art Ref-SR methods.</p>
<blockquote>
<p>é€šè¿‡å«æ˜Ÿå›¾åƒçš„åˆ†å‰²æ¥åˆ’å®šå†œç”°è¾¹ç•Œæ˜¯è®¸å¤šå†œä¸šåº”ç”¨ä¸­çš„åŸºæœ¬æ­¥éª¤ã€‚è¯¥ä»»åŠ¡å¯¹äºå°è§„æ¨¡å†œåœºæ¥è¯´å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå‡†ç¡®åˆ’å®šè¾¹ç•Œéœ€è¦ä½¿ç”¨é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å›¾åƒï¼Œè€Œè¿™äº›å›¾åƒåªèƒ½åœ¨è¾ƒä½çš„å›è®¿é¢‘ç‡ï¼ˆä¾‹å¦‚æ¯å¹´ï¼‰ä¸‹è·å–ã€‚ä¸ºäº†æ”¯æŒæ›´é¢‘ç¹ï¼ˆäºšï¼‰å­£èŠ‚æ€§çš„ç›‘æµ‹ï¼Œå¯ä»¥å°†é«˜åˆ†è¾¨ç‡å›¾åƒä¸å…·æœ‰æ›´é«˜å›è®¿é¢‘ç‡çš„ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒç›¸ç»“åˆä½œä¸ºå‚è€ƒï¼ˆä¾‹å¦‚æ¯å‘¨ï¼‰ï¼Œå¹¶ä½¿ç”¨åŸºäºå‚è€ƒçš„è¶…åˆ†è¾¨ç‡ï¼ˆRef-SRï¼‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„Ref-SRæ–¹æ³•åœ¨ä¼˜åŒ–æ„ŸçŸ¥è´¨é‡æ—¶å¿½è§†äº†åç»­ä»»åŠ¡æ‰€éœ€çš„å…³é”®ç‰¹å¾ï¼Œæ— æ³•æ»¡è¶³æ­¤ä»»åŠ¡çš„å¤§è§„æ¨¡å› å­è¦æ±‚ã€‚æ­¤å¤–ï¼Œå…ˆå‰é‡‡å–çš„å…ˆè¶…åˆ†è¾¨ç‡å†åˆ†å‰²çš„ä¸¤æ­¥èµ°æ–¹æ³•å¹¶æœªæœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ ·çš„å«æ˜Ÿæ•°æ®æºä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°æ–¹æ³•SEED-SRæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ä»¥åŠå¤§è§„æ¨¡çš„å¤šå…‰è°±ã€å¤šæºåœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºç»•è¿‡åƒç´ ç©ºé—´çš„æ˜¾å¼è¶…åˆ†è¾¨ç‡ä»»åŠ¡ï¼Œè½¬è€Œæ‰§è¡Œæ„ŸçŸ¥åˆ†å‰²çš„æ½œåœ¨ç©ºé—´ä¸­çš„è¶…åˆ†è¾¨ç‡ã€‚è¿™ç§ç‹¬ç‰¹çš„æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å‰æ‰€æœªæœ‰çš„20å€æ”¾å¤§å€æ•°ä¸‹ç”Ÿæˆåˆ†å‰²å›¾ï¼Œå¯¹ä¸¤ä¸ªå¤§å‹çœŸå®æ•°æ®é›†è¿›è¡Œçš„ä¸¥æ ¼å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºåŸºäºæœ€å‰æ²¿Ref-SRæ–¹æ³•çš„æ–¹æ¡ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²æŒ‡æ ‡ä¸Šåˆ†åˆ«æœ‰é«˜è¾¾25.5%å’Œ12.9%çš„ç›¸å¯¹æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14481v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹å«æ˜Ÿå›¾åƒä¸­å†œåœºè¾¹ç•Œçš„åˆ’å®šé—®é¢˜å±•å¼€ï¼Œé’ˆå¯¹é«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒçš„ç‰¹ç‚¹è¿›è¡Œäº†èåˆåˆ©ç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨åº”å¯¹å…·æœ‰å‘¨æœŸæ€§è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ä¸‹å­˜åœ¨é—®é¢˜ï¼Œå½“å‰çš„æ–°æŠ€æœ¯å¦‚SEED-SRé€šè¿‡ç»“åˆæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå¤šæºåœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ï¼Œåœ¨åˆ†å‰²æ„ŸçŸ¥æ½œåœ¨ç©ºé—´ä¸­ç»•è¿‡æ˜¾å¼è¶…åˆ†è¾¨ç‡ä»»åŠ¡ï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„é«˜å°ºåº¦åˆ†å‰²æ˜ å°„ç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºåŸºäºå½“å‰ä¸»æµå‚è€ƒè¶…åˆ†è¾¨ç‡æ–¹æ³•çš„æ–¹æ³•ï¼ŒSEED-SRåœ¨å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„ç›¸å¯¹æå‡ã€‚è¿™ä¸ä»…æ¨åŠ¨äº†å†œä¸šåº”ç”¨çš„ç²¾å‡†å‘å±•ï¼Œä¹Ÿä½“ç°äº†æŠ€æœ¯çš„é©æ–°ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å«æ˜Ÿå›¾åƒå†œåœºè¾¹ç•Œçš„åˆ’å®šæ˜¯å†œä¸šåº”ç”¨ä¸­çš„åŸºç¡€æ­¥éª¤ï¼Œå¯¹é«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒçš„èåˆåˆ©ç”¨æ˜¯å…³é”®æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>å½“å‰é¢ä¸´çš„é—®é¢˜æ˜¯è¶…åˆ†è¾¨ç‡æŠ€æœ¯éœ€è¦é¢‘ç¹çš„å›é¡¾å’Œåˆ©ç”¨æ•°æ®ï¼ˆå¹´åº¦è®¿é—®ï¼‰ï¼Œå¯¼è‡´åœ¨æŸäº›åœºåˆçš„æ—¶æ•ˆæ€§å’Œå¯é æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç»“åˆäº†é«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡å›¾åƒçš„ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed0f43bf838a23d325d28330fbf12c24" align="middle">
<img src="https://picx.zhimg.com/v2-d9a740d50a7634ebdbe4748ab36a807f" align="middle">
<img src="https://picx.zhimg.com/v2-e2ee861d3156516c774bfe50519d1fd9" align="middle">
<img src="https://picx.zhimg.com/v2-4fe0be79ac8b3fb754c3d541970c0a14" align="middle">
<img src="https://picx.zhimg.com/v2-addbc5718c0ba810dd0774b3c12090e2" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cranio-ID-Graph-Based-Craniofacial-Identification-via-Automatic-Landmark-Annotation-in-2D-Multi-View-X-rays"><a href="#Cranio-ID-Graph-Based-Craniofacial-Identification-via-Automatic-Landmark-Annotation-in-2D-Multi-View-X-rays" class="headerlink" title="Cranio-ID: Graph-Based Craniofacial Identification via Automatic Landmark Annotation in 2D Multi-View X-rays"></a>Cranio-ID: Graph-Based Craniofacial Identification via Automatic Landmark Annotation in 2D Multi-View X-rays</h2><p><strong>Authors:Ravi Shankar Prasad, Nandani Sharma, Dinesh Singh</strong></p>
<p>In forensic craniofacial identification and in many biomedical applications, craniometric landmarks are important. Traditional methods for locating landmarks are time-consuming and require specialized knowledge and expertise. Current methods utilize superimposition and deep learning-based methods that employ automatic annotation of landmarks. However, these methods are not reliable due to insufficient large-scale validation studies. In this paper, we proposed a novel framework Cranio-ID: First, an automatic annotation of landmarks on 2D skulls (which are X-ray scans of faces) with their respective optical images using our trained YOLO-pose models. Second, cross-modal matching by formulating these landmarks into graph representations and then finding semantic correspondence between graphs of these two modalities using cross-attention and optimal transport framework. Our proposed framework is validated on the S2F and CUHK datasets (CUHK dataset resembles with S2F dataset). Extensive experiments have been conducted to evaluate the performance of our proposed framework, which demonstrates significant improvements in both reliability and accuracy, as well as its effectiveness in cross-domain skull-to-face and sketch-to-face matching in forensic science.</p>
<blockquote>
<p>åœ¨æ³•åŒ»å­¦é¢…é¢éƒ¨è¯†åˆ«ä»¥åŠè®¸å¤šç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­ï¼Œé¢…é¢æµ‹é‡åœ°æ ‡éå¸¸é‡è¦ã€‚ä¼ ç»Ÿåœ°æ ‡å®šä½æ–¹æ³•è€—æ—¶ï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œä¸“é—¨çŸ¥è¯†ã€‚ç›®å‰çš„æ–¹æ³•é‡‡ç”¨å åŠ å’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œå¯¹åœ°æ ‡è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡éªŒè¯ç ”ç©¶ï¼Œè¿™äº›æ–¹æ³•å¹¶ä¸å¯é ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶Cranio-IDï¼šé¦–å…ˆï¼Œä½¿ç”¨æˆ‘ä»¬è®­ç»ƒçš„YOLO-poseæ¨¡å‹å¯¹äºŒç»´é¢…éª¨ï¼ˆé¢éƒ¨Xå°„çº¿æ‰«æï¼‰åŠå…¶ç›¸åº”çš„å…‰å­¦å›¾åƒè¿›è¡Œåœ°æ ‡è‡ªåŠ¨æ ‡æ³¨ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å°†è¿™äº›åœ°æ ‡è½¬åŒ–ä¸ºå›¾å½¢è¡¨ç¤ºï¼Œç„¶ååœ¨ä¸¤ç§æ¨¡æ€çš„å›¾å½¢ä¹‹é—´æ‰¾åˆ°è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œä½¿ç”¨è·¨æ³¨æ„åŠ›å’Œæœ€ä¼˜ä¼ è¾“æ¡†æ¶å®ç°è·¨æ¨¡æ€åŒ¹é…ã€‚æˆ‘ä»¬çš„æè®®æ¡†æ¶åœ¨S2Få’ŒCUHKæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼ˆCUHKæ•°æ®é›†ä¸S2Fæ•°æ®é›†ç›¸ä¼¼ï¼‰ã€‚å·²ç»è¿›è¡Œäº†å¤§é‡å®éªŒæ¥è¯„ä¼°æˆ‘ä»¬æè®®æ¡†æ¶çš„æ€§èƒ½ï¼Œå®ƒåœ¨å¯é æ€§å’Œå‡†ç¡®æ€§æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾ç€æé«˜ï¼Œå¹¶ä¸”åœ¨æ³•åŒ»å­¦ä¸­çš„è·¨åŸŸé¢…éª¨å¯¹é¢éƒ¨ä»¥åŠè‰å›¾å¯¹é¢éƒ¨çš„åŒ¹é…ä¸­éå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14411v1">PDF</a> 11 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶Cranio-IDï¼Œç”¨äºè‡ªåŠ¨æ ‡æ³¨é¢…éª¨ä¸Šçš„ç‰¹å¾ç‚¹å¹¶è¿›è¡Œè·¨æ¨¡æ€åŒ¹é…ã€‚è¯¥æ¡†æ¶åˆ©ç”¨YOLO-poseæ¨¡å‹å¯¹äºŒç»´é¢…éª¨å›¾åƒè¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ï¼Œå¹¶é€šè¿‡å›¾å½¢è¡¨ç¤ºå’Œè·¨æ³¨æ„åŠ›æœºåˆ¶å®ç°è·¨æ¨¡æ€åŒ¹é…ã€‚å®éªŒéªŒè¯è¯¥æ¡†æ¶åœ¨S2Få’ŒCUHKæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼Œæœ‰æ•ˆåº”ç”¨äºæ³•åŒ»ç§‘å­¦çš„è·¨åŸŸé¢…éª¨åˆ°äººè„¸å’Œè‰å›¾åˆ°äººè„¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Craniometric landmarks are significant in forensic craniofacial identification and biomedical applications.</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¯»æ‰¾ç‰¹å¾ç‚¹è€—æ—¶ä¸”éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œç»éªŒã€‚</li>
<li>å½“å‰æ–¹æ³•ä½¿ç”¨å åŠ å’Œæ·±åº¦å­¦ä¹ è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ï¼Œä½†ç¼ºä¹å¤§è§„æ¨¡éªŒè¯ç ”ç©¶ï¼Œå¯é æ€§ä¸è¶³ã€‚</li>
<li>æ–°å‹æ¡†æ¶Cranio-IDæå‡ºè‡ªåŠ¨æ ‡æ³¨äºŒç»´é¢…éª¨å›¾åƒä¸Šçš„ç‰¹å¾ç‚¹ã€‚</li>
<li>é€šè¿‡å›¾å½¢è¡¨ç¤ºå’Œè·¨æ³¨æ„åŠ›æœºåˆ¶å®ç°è·¨æ¨¡æ€åŒ¹é…ã€‚</li>
<li>åœ¨S2Få’ŒCUHKæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œæ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b815ea2b5b74c08af96eec463e92d414" align="middle">
<img src="https://picx.zhimg.com/v2-a3223de8e2da05c17b2de53abc55180c" align="middle">
<img src="https://picx.zhimg.com/v2-ffb2cfa524bf0966a443c746f1bb0574" align="middle">
<img src="https://picx.zhimg.com/v2-9c2d9838a2c44870de4e546d9f7a4ced" align="middle">
<img src="https://picx.zhimg.com/v2-f96703e6a3c9e1dd7c049b2aef93211d" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="The-SRG-eROSITA-all-sky-survey-X-ray-scaling-relations-of-galaxy-groups-and-clusters-in-the-western-Galactic-hemisphere"><a href="#The-SRG-eROSITA-all-sky-survey-X-ray-scaling-relations-of-galaxy-groups-and-clusters-in-the-western-Galactic-hemisphere" class="headerlink" title="The SRG&#x2F;eROSITA all-sky survey: X-ray scaling relations of galaxy groups and clusters in the western Galactic hemisphere"></a>The SRG&#x2F;eROSITA all-sky survey: X-ray scaling relations of galaxy groups and clusters in the western Galactic hemisphere</h2><p><strong>Authors:M. E. Ramos-Ceja, L. Fiorino, E. Bulbul, V. Ghirardini, N. Clerc, A. Liu, J. S. Sanders, Y. E. Bahar, J. Dietl, M. Kluge, F. Pacaud, E. Artis, F. Balzer, J. Comparat, Z. Ding, N. Malavasi, A. Merloni, T. Mistele, K. Nandra, R. Seppi, S. Zelmer, X. Zhang</strong></p>
<p>The soft X-ray telescope on board the Spectrum-Roentgen-Gamma (SRG) mission, eROSITA (extended ROentgen Survey with an Imaging Telescope Array), has produced the largest sample to date of galaxy groups and clusters detected via their intracluster&#x2F;intragroup medium (ICM&#x2F;IGrM) emission. Scaling relations between the intrinsic properties of these systems provide valuable insight into their formation and evolution. In this work, we investigate the scaling relations between key physical properties, such as soft band X-ray luminosity, temperature, gas mass, and the low-scatter mass proxy $Y_{\rm X}$, for the galaxy groups and clusters detected in the first eROSITA All-Sky Survey (eRASS1). Our analysis fully accounts for selection effects and the redshift evolution of the observable distributions. We construct a high-purity sample of $3061$ galaxy groups and clusters spanning the redshift range $0.05&lt;z&lt;1.07$ and mass range of $1.1\times10^{13}&lt;M_{500}&#x2F;$M$<em>{\odot}&lt;1.6\times10^{15}$. This represents the largest sample to date used for scaling relation analysis. The selection function, derived from state-of-the-art simulations of the eROSITA sky, is rigorously incorporated into our modeling. We report best-fit parameters - normalization, slope, redshift evolution, and intrinsic scatter - for a set of scaling relations: $L</em>{\mathrm{X}}-T$, $L_{\mathrm{X}}-M_{\rm gas}$, $L_{\mathrm{X}}-Y_{\rm X}$, as well as the $M_{\rm gas}-T$ relation. Our best-fit models indicate that the slopes of the scaling relations deviate significantly from self-similar expectations, while the redshift evolution remains consistent with the self-similar model. The fits exhibit small statistical uncertainties, likely owing to the large sample size. Our results are in good agreement with previous observational studies that account for selection effects, as well as with simulations that incorporate non-gravitational physics.</p>
<blockquote>
<p>æ­è½½åœ¨Spectrum-Roentgen-Gammaï¼ˆSRGï¼‰ä»»åŠ¡ä¸Šçš„è½¯Xå°„çº¿æœ›è¿œé•œeROSITAï¼ˆæ‰©å±•çš„æˆåƒæœ›è¿œé•œé˜µåˆ—è¿›è¡Œçš„ROentgenè°ƒæŸ¥ï¼‰è‡³ä»Šå·²ç»äº§ç”Ÿäº†é€šè¿‡é›†ç¾¤å†…æˆ–ç»„å†…ä»‹è´¨ï¼ˆICM&#x2F;IGrMï¼‰å‘å°„æ£€æµ‹åˆ°çš„æœ€å¤§çš„æ˜Ÿç³»å›¢æ ·æœ¬ã€‚è¿™äº›ç³»ç»Ÿå†…åœ¨å±æ€§ä¹‹é—´çš„æ ‡åº¦å…³ç³»ä¸ºæ·±å…¥äº†è§£å®ƒä»¬çš„å½¢æˆå’Œæ¼”åŒ–æä¾›äº†å®è´µçš„è§è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è½¯Xå°„çº¿å…‰åº¦ã€æ¸©åº¦ã€æ°”ä½“è´¨é‡å’Œä½æ•£å°„è´¨é‡ä»£ç†Yxç­‰å…³é”®ç‰©ç†å±æ€§ä¹‹é—´çš„æ ‡åº¦å…³ç³»ï¼Œè¿™äº›æ˜Ÿç³»å›¢æ˜¯åœ¨ç¬¬ä¸€æ¬¡eROSITAå…¨å¤©ç©ºæ™®æŸ¥ï¼ˆeRASS1ï¼‰ä¸­æ£€æµ‹åˆ°çš„ã€‚æˆ‘ä»¬çš„åˆ†æå……åˆ†è€ƒè™‘äº†é€‰æ‹©æ•ˆåº”å’Œå¯è§‚æµ‹åˆ†å¸ƒçš„çº¢ç§»æ¼”åŒ–ã€‚æˆ‘ä»¬æ„å»ºäº†è·¨è¶Šçº¢ç§»èŒƒå›´0.05&lt;z&lt;1.07å’Œè´¨é‡èŒƒå›´1.1Ã—10^13&lt;M_{500}&#x2F;M_{\odot}&lt;1.6Ã—10^15çš„é«˜çº¯åº¦æ ·æœ¬ï¼ŒåŒ…å«3061ä¸ªæ˜Ÿç³»å›¢ã€‚è¿™æ˜¯è¿„ä»Šä¸ºæ­¢ç”¨äºæ ‡åº¦å…³ç³»åˆ†æçš„æœ€å¤§æ ·æœ¬ã€‚é€‰æ‹©å‡½æ•°æ˜¯ä»eROSITAå¤©ç©ºçš„å…ˆè¿›æ¨¡æ‹Ÿä¸­å¾—å‡ºçš„ï¼Œè¢«ä¸¥æ ¼åœ°çº³å…¥æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†ä¸€ç»„æ ‡åº¦å…³ç³»çš„æœ€ä½³æ‹Ÿåˆå‚æ•°ï¼ŒåŒ…æ‹¬Lx-Tã€Lx-Mgasã€Lx-Yxä»¥åŠMgas-Tå…³ç³»ã€‚æˆ‘ä»¬çš„æœ€ä½³æ‹Ÿåˆæ¨¡å‹è¡¨æ˜ï¼Œæ ‡åº¦å…³ç³»çš„æ–œç‡ä¸è‡ªç›¸ä¼¼é¢„æœŸå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè€Œçº¢ç§»æ¼”åŒ–ä¸è‡ªç›¸ä¼¼æ¨¡å‹ä¿æŒä¸€è‡´ã€‚æ‹Ÿåˆç»“æœå…·æœ‰å¾ˆå°çš„ç»Ÿè®¡ä¸ç¡®å®šæ€§ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ ·æœ¬é‡å¤§çš„åŸå› ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸è€ƒè™‘é€‰æ‹©æ•ˆåº”çš„å‰ç»æ€§è§‚æµ‹ç ”ç©¶ä»¥åŠè€ƒè™‘åˆ°éé‡åŠ›ä½œç”¨çš„æ¨¡æ‹Ÿç»“æœå»åˆè‰¯å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14356v1">PDF</a> 13 pages, 18 figures. Submitted to A&amp;A</p>
<p><strong>Summary</strong><br>    eROSITAæœ›è¿œé•œåœ¨SRGä»»åŠ¡ä¸Šè§‚å¯Ÿåˆ°è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„æ˜Ÿç³»å›¢é›†ç¾¤æ ·æœ¬ï¼Œé€šè¿‡å…¶å›¢å†…&#x2F;ç»„å†…ä»‹è´¨å‘å°„æ£€æµ‹å¾—åˆ°ã€‚åˆ†æè¿™äº›ç³»ç»Ÿçš„å†…åœ¨å±æ€§ä¹‹é—´çš„æ ‡åº¦å…³ç³»ï¼Œä¸ºç†è§£å®ƒä»¬çš„å½¢æˆå’Œæ¼”åŒ–æä¾›äº†å®è´µè§è§£ã€‚ç ”ç©¶å›¢é˜Ÿæ¢ç©¶äº†è½¯Xå°„çº¿å…‰åº¦ã€æ¸©åº¦ã€æ°”ä½“è´¨é‡ç­‰å…³é”®ç‰©ç†å±æ€§ä¹‹é—´çš„æ ‡åº¦å…³ç³»ï¼Œä»¥åŠç¬¬ä¸€ä¸ªeROSITAå…¨å¤©å·¡å¤©å¾—åˆ°çš„æ˜Ÿç³»å›¢æ ·æœ¬çš„ä½æ•£å°„è´¨é‡ä»£ç†Yxã€‚è¯¥æ ·æœ¬è·¨è¶Šäº†çº¢ç§»èŒƒå›´0.05&lt;z&lt;1.07å’Œè´¨é‡èŒƒå›´ï¼Œä»£è¡¨äº†è¿„ä»Šä¸ºæ­¢ç”¨äºæ ‡åº¦å…³ç³»åˆ†æçš„æœ€å¤§æ ·æœ¬ã€‚ç ”ç©¶å›¢é˜Ÿå°†é€‰æ‹©å‡½æ•°çº³å…¥æ¨¡å‹ä¸­ï¼Œå¹¶æŠ¥å‘Šäº†ä¸€ç³»åˆ—æœ€ä½³æ‹Ÿåˆå‚æ•°ã€‚ç»“æœè¡¨æ˜ï¼Œæ ‡åº¦å…³ç³»çš„æ–œç‡ä¸è‡ªç›¸ä¼¼é¢„æœŸå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè€Œçº¢ç§»æ¼”åŒ–åˆ™ä¸è‡ªç›¸ä¼¼æ¨¡å‹ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>eROSITAæœ›è¿œé•œè§‚æµ‹åˆ°äº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„æ˜Ÿç³»å›¢é›†ç¾¤æ ·æœ¬ã€‚</li>
<li>é€šè¿‡åˆ†æè¿™äº›æ˜Ÿç³»å›¢å’Œé›†ç¾¤çš„å†…åœ¨å±æ€§ä¹‹é—´çš„æ ‡åº¦å…³ç³»ï¼Œæ­ç¤ºäº†å®ƒä»¬å¯èƒ½çš„å½¢æˆå’Œæ¼”åŒ–è¿‡ç¨‹ã€‚</li>
<li>ç ”ç©¶äº†è½¯Xå°„çº¿å…‰åº¦ã€æ¸©åº¦ã€æ°”ä½“è´¨é‡å’ŒYxç­‰å…³é”®ç‰©ç†å±æ€§çš„æ ‡åº¦å…³ç³»ã€‚</li>
<li>æ ·æœ¬æ¶µç›–äº†å¹¿æ³›çš„çº¢ç§»èŒƒå›´å’Œè´¨é‡èŒƒå›´ï¼Œä¸ºæ ‡åº¦å…³ç³»åˆ†ææä¾›äº†ä¸°å¯Œçš„æ•°æ®ã€‚</li>
<li>ä¸¥æ ¼è€ƒè™‘äº†é€‰æ‹©æ•ˆåº”ï¼Œå¹¶å°†é€‰æ‹©å‡½æ•°çº³å…¥æ¨¡å‹ä¸­è¿›è¡Œåˆ†æã€‚</li>
<li>æ ‡åº¦å…³ç³»çš„æ–œç‡ä¸è‡ªç›¸ä¼¼é¢„æœŸå­˜åœ¨å·®å¼‚ï¼Œè€Œçº¢ç§»æ¼”åŒ–åˆ™ä¸è‡ªç›¸ä¼¼æ¨¡å‹ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f03be87724ef6bce78d58b46cf58b34" align="middle">
<img src="https://picx.zhimg.com/v2-c760dac4b85e19cb62d4588e5f5e9421" align="middle">
<img src="https://picx.zhimg.com/v2-371c5169ac9682b1fe82820f6a3f76a1" align="middle">
<img src="https://picx.zhimg.com/v2-e33b5bd6cddf1b22cea30e8dad90b8c3" align="middle">
<img src="https://picx.zhimg.com/v2-c3939406e0446f303397e6f61c9697d6" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Step-by-Step-Network"><a href="#Step-by-Step-Network" class="headerlink" title="Step by Step Network"></a>Step by Step Network</h2><p><strong>Authors:Dongchen Han, Tianzhu Ye, Zhuofan Xia, Kaiyi Chen, Yulin Wang, Hanting Chen, Gao Huang</strong></p>
<p>Scaling up network depth is a fundamental pursuit in neural architecture design, as theory suggests that deeper models offer exponentially greater capability. Benefiting from the residual connections, modern neural networks can scale up to more than one hundred layers and enjoy wide success. However, as networks continue to deepen, current architectures often struggle to realize their theoretical capacity improvements, calling for more advanced designs to further unleash the potential of deeper networks. In this paper, we identify two key barriers that obstruct residual models from scaling deeper: shortcut degradation and limited width. Shortcut degradation hinders deep-layer learning, while the inherent depth-width trade-off imposes limited width. To mitigate these issues, we propose a generalized residual architecture dubbed Step by Step Network (StepsNet) to bridge the gap between theoretical potential and practical performance of deep models. Specifically, we separate features along the channel dimension and let the model learn progressively via stacking blocks with increasing width. The resulting method mitigates the two identified problems and serves as a versatile macro design applicable to various models. Extensive experiments show that our method consistently outperforms residual models across diverse tasks, including image classification, object detection, semantic segmentation, and language modeling. These results position StepsNet as a superior generalization of the widely adopted residual architecture.</p>
<blockquote>
<p>æ‰©å¤§ç½‘ç»œæ·±åº¦æ˜¯ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡ä¸­çš„åŸºæœ¬è¿½æ±‚ï¼Œå› ä¸ºç†è®ºè¡¨æ˜ï¼Œæ›´æ·±çš„æ¨¡å‹èƒ½åŠ›å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚å¾—ç›Šäºæ®‹å·®è¿æ¥ï¼Œç°ä»£ç¥ç»ç½‘ç»œå¯ä»¥æ‰©å±•åˆ°ä¸€ç™¾å±‚ä»¥ä¸Šï¼Œå¹¶è·å¾—äº†å¹¿æ³›çš„æˆåŠŸã€‚ç„¶è€Œï¼Œéšç€ç½‘ç»œçš„ç»§ç»­æ·±åŒ–ï¼Œå½“å‰æ¶æ„å¾€å¾€éš¾ä»¥å®ç°å…¶ç†è®ºä¸Šçš„æ€§èƒ½æå‡ï¼Œè¿™éœ€è¦æ›´å…ˆè¿›çš„è®¾è®¡æ¥è¿›ä¸€æ­¥é‡Šæ”¾æ›´æ·±ç½‘ç»œçš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†é˜»ç¢æ®‹å·®æ¨¡å‹è¿›ä¸€æ­¥æ·±åŒ–çš„ä¸¤ä¸ªå…³é”®éšœç¢ï¼šæ·å¾„é€€åŒ–å’Œæœ‰é™å®½åº¦ã€‚æ·å¾„é€€åŒ–é˜»ç¢äº†æ·±å±‚å­¦ä¹ ï¼Œè€Œå›ºæœ‰çš„æ·±åº¦å®½åº¦æƒè¡¡å¯¼è‡´äº†å®½åº¦çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨æ®‹å·®æ¶æ„ï¼Œåä¸ºâ€œé€æ­¥ç½‘ç»œâ€ï¼ˆStepsNetï¼‰ï¼Œä»¥å¼¥è¡¥æ·±æ¨¡å‹ç†è®ºæ½œåŠ›ä¸å®é™…æ€§èƒ½ä¹‹é—´çš„é¸¿æ²Ÿã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ²¿é€šé“ç»´åº¦åˆ†ç¦»ç‰¹å¾ï¼Œå¹¶é€šè¿‡å †å å®½åº¦ä¸æ–­å¢åŠ çš„å—æ¥è®©æ¨¡å‹é€æ­¥å­¦ä¹ ã€‚ç”±æ­¤äº§ç”Ÿçš„æ–¹æ³•ç¼“è§£äº†è¿™ä¸¤ä¸ªå·²ç¡®å®šçš„é—®é¢˜ï¼Œå¹¶ä½œä¸ºä¸€ç§é€‚ç”¨äºå„ç§æ¨¡å‹çš„é€šç”¨å®è§‚è®¾è®¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„é¡¹ä»»åŠ¡ä¸Šå‡ä¼˜äºæ®‹å·®æ¨¡å‹ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œè¯­è¨€å»ºæ¨¡ã€‚è¿™äº›ç»“æœå°†StepsNetå®šä½ä¸ºå¹¿æ³›é‡‡ç”¨çš„æ®‹å·®æ¶æ„çš„ä¼˜è¶Šé€šç”¨åŒ–ç‰ˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14329v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†ç¥ç»ç½‘ç»œæ·±åº¦æ‰©å±•çš„é—®é¢˜ï¼ŒæŒ‡å‡ºæ®‹å·®æ¨¡å‹åœ¨æ·±åº¦æ‰©å±•æ—¶é¢ä¸´ä¸¤å¤§éšœç¢ï¼šshortcuté™è§£å’Œå®½åº¦å—é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºStepsNetçš„å¹¿ä¹‰æ®‹å·®æ¶æ„ï¼Œé€šè¿‡åˆ†ç¦»ç‰¹å¾é€šé“å¹¶é€æ­¥å¢åŠ å®½åº¦ï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œå¹¶åœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†å¯¹æ®‹å·®æ¨¡å‹çš„æ€§èƒ½è¶…è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œæ·±åº¦æ‰©å±•æ˜¯ç¥ç»æ¶æ„è®¾è®¡ä¸­çš„åŸºæœ¬è¿½æ±‚ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨æ·±åº¦å¢åŠ æ—¶éš¾ä»¥å®ç°ç†è®ºä¸Šçš„æ€§èƒ½æå‡ã€‚</li>
<li>æ®‹å·®æ¨¡å‹åœ¨æ·±åº¦æ‰©å±•æ—¶é¢ä¸´ä¸¤å¤§éšœç¢ï¼šshortcuté™è§£å’Œå®½åº¦å—é™ã€‚</li>
<li>Shortcuté™è§£é˜»ç¢æ·±å±‚å­¦ä¹ ï¼Œè€Œå®½åº¦å—é™åˆ™å—åˆ°æ·±åº¦ä¸å®½åº¦ä¹‹é—´çš„å›ºæœ‰æƒè¡¡é™åˆ¶ã€‚</li>
<li>StepsNetä½œä¸ºä¸€ç§å¹¿ä¹‰æ®‹å·®æ¶æ„ï¼Œæ—¨åœ¨å¼¥è¡¥æ·±å±‚æ¨¡å‹ç†è®ºæ½œåŠ›ä¸å®é™…åº”ç”¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚</li>
<li>StepsNeté€šè¿‡åˆ†ç¦»ç‰¹å¾é€šé“å¹¶é€æ¸å¢åŠ å®½åº¦ï¼Œè®©æ¨¡å‹é€æ­¥å­¦ä¹ ã€‚</li>
<li>StepsNetåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†å¯¹æ®‹å·®æ¨¡å‹çš„æ€§èƒ½è¶…è¶Šï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œè¯­è¨€å»ºæ¨¡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d1e16078d6311f6b1d13ae93f62a2be" align="middle">
<img src="https://picx.zhimg.com/v2-2ba82406f31b7e294cb9ca41b8270f80" align="middle">
<img src="https://picx.zhimg.com/v2-0f6b07b21f264dc658707fd9532aa6bf" align="middle">
<img src="https://picx.zhimg.com/v2-5e365176e924068c1db07488cd5f0658" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dental3R-Geometry-Aware-Pairing-for-Intraoral-3D-Reconstruction-from-Sparse-View-Photographs"><a href="#Dental3R-Geometry-Aware-Pairing-for-Intraoral-3D-Reconstruction-from-Sparse-View-Photographs" class="headerlink" title="Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs"></a>Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs</h2><p><strong>Authors:Yiyi Miao, Taoyu Wu, Tong Chen, Ji Jiang, Zhe Tang, Zhengyong Jiang, Angelos Stefanidis, Limin Yu, Jionglong Su</strong></p>
<p>Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery. While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging. The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation. Furthermore, sparse-view photometric supervision often induces a frequency bias, leading to over-smoothed reconstructions that lose critical diagnostic details. To address these limitations, we propose \textbf{Dental3R}, a pose-free, graph-guided pipeline for robust, high-fidelity reconstruction from sparse intraoral photographs. Our method first constructs a Geometry-Aware Pairing Strategy (GAPS) to intelligently select a compact subgraph of high-value image pairs. The GAPS focuses on correspondence matching, thereby improving the stability of the geometry initialization and reducing memory usage. Building on the recovered poses and point cloud, we train the 3DGS model with a wavelet-regularized objective. By enforcing band-limited fidelity using a discrete wavelet transform, our approach preserves fine enamel boundaries and interproximal edges while suppressing high-frequency artifacts. We validate our approach on a large-scale dataset of 950 clinical cases and an additional video-based test set of 195 cases. Experimental results demonstrate that Dental3R effectively handles sparse, unposed inputs and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art methods.</p>
<blockquote>
<p>å£è…”å†…3Dé‡å»ºæ˜¯æ•°å­—æ­£ç•¸çš„åŸºç¡€ï¼Œä½†ä¼ ç»Ÿçš„å¦‚å£è…”å†…æ‰«æç­‰æ–¹æ³•å¯¹äºè¿œç¨‹æ­£ç•¸æ²»ç–—å¹¶ä¸å¯è¡Œï¼Œè¿œç¨‹æ­£ç•¸é€šå¸¸ä¾èµ–äºç¨€ç–çš„æ™ºèƒ½æ‰‹æœºå›¾åƒã€‚è™½ç„¶3Dé«˜æ–¯æ‘Šé“ºï¼ˆ3DGSï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å°†å…¶åº”ç”¨äºæ ‡å‡†ä¸´åºŠä¸‰è”çš„å‰æ–¹å’ŒåŒä¾§é¢Šä¾§ç…§ç‰‡å´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å£è…”ç¯å¢ƒä¸­å¸¸è§çš„è§†é‡åŸºçº¿å¤§ã€ç…§æ˜ä¸ä¸€è‡´å’Œé•œé¢è¡¨é¢å¯èƒ½ä¼šç ´åå§¿æ€å’Œå‡ ä½•ä¼°è®¡çš„åŒæ—¶æ€§ã€‚æ­¤å¤–ï¼Œç¨€ç–è§†å›¾çš„å…‰åº¦ç›‘ç£é€šå¸¸ä¼šäº§ç”Ÿé¢‘ç‡åå·®ï¼Œå¯¼è‡´è¿‡åº¦å¹³æ»‘çš„é‡å»ºï¼Œä»è€Œä¸¢å¤±å…³é”®çš„è¯Šæ–­ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{Dental3R}ï¼Œè¿™æ˜¯ä¸€ç§æ— å§¿æ€ã€ä»¥å›¾å¼•å¯¼çš„ç®¡é“ï¼Œå¯ä»ç¨€ç–çš„å£è…”ç…§ç‰‡ä¸­è¿›è¡Œç¨³å¥ã€é«˜ä¿çœŸé‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå‡ ä½•æ„ŸçŸ¥é…å¯¹ç­–ç•¥ï¼ˆGAPSï¼‰ï¼Œä»¥æ™ºèƒ½é€‰æ‹©é«˜ä»·å€¼å›¾åƒå¯¹ç»„æˆçš„ç´§å‡‘å­å›¾ã€‚GAPSä¸“æ³¨äºå¯¹åº”åŒ¹é…ï¼Œä»è€Œæé«˜å‡ ä½•åˆå§‹åŒ–çš„ç¨³å®šæ€§å¹¶å‡å°‘å†…å­˜ä½¿ç”¨ã€‚åœ¨æ¢å¤çš„å§¿æ€å’Œç‚¹äº‘çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨å°æ³¢æ­£åˆ™åŒ–çš„ç›®æ ‡è®­ç»ƒäº†3DGSæ¨¡å‹ã€‚é€šè¿‡ç¦»æ•£å°æ³¢å˜æ¢å¼ºåˆ¶æ‰§è¡Œå¸¦é™ä¿çœŸåº¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä¿ç•™ç²¾ç»†çš„é‡‰è´¨è¾¹ç•Œå’Œé‚»æ¥è¾¹ç¼˜ï¼ŒåŒæ—¶æŠ‘åˆ¶é«˜é¢‘ä¼ªå½±ã€‚æˆ‘ä»¬åœ¨åŒ…å«950ä¸ªä¸´åºŠç—…ä¾‹çš„å¤§è§„æ¨¡æ•°æ®é›†å’Œé¢å¤–çš„åŸºäºè§†é¢‘çš„æµ‹è¯•é›†ï¼ˆåŒ…å«195ä¸ªç—…ä¾‹ï¼‰ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDental3Ræœ‰æ•ˆåœ°å¤„ç†äº†ç¨€ç–ä¸”æ— å§¿æ€çš„è¾“å…¥ï¼Œå¹¶åœ¨ç‰™é½¿å’¬åˆå¯è§†åŒ–æ–¹é¢å®ç°äº†å“è¶Šçš„æ–°å‹è§†å›¾åˆæˆè´¨é‡ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14315v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDental3Rçš„æ–¹æ³•ï¼Œç”¨äºä»ç¨€ç–çš„å£è…”å†…ç…§ç‰‡ä¸­è¿›è¡Œç¨³å¥ã€é«˜ä¿çœŸåº¦çš„é‡å»ºã€‚é€šè¿‡æ„é€ Geometry-Aware Pairing Strategy (GAPS)å’Œåˆ©ç”¨ç¦»æ•£å°æ³¢å˜æ¢å¼ºåˆ¶æœ‰é™å¸¦å®½ä¿çœŸï¼Œæé«˜äº†æ¨¡å‹åœ¨å¤„ç†ä¸ä¸€è‡´ç…§æ˜ã€å¤§çš„è§†ç‚¹åŸºå‡†çº¿ç­‰é—®é¢˜æ—¶çš„ç¨³å®šæ€§å’Œå‡ ä½•æ¢å¤çš„ç²¾åº¦ã€‚é€šè¿‡å¤§å‹ä¸´åºŠç—…ä¾‹æ•°æ®é›†çš„å®éªŒéªŒè¯ï¼ŒDental3Råœ¨ç¨€ç–ã€æœªå®šä½è¾“å…¥çš„æƒ…å†µä¸‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå®ç°äº†é«˜è´¨é‡çš„ç‰™é½¿å’¬åˆå¯è§†åŒ–ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†è¿œç¨‹æ­£ç•¸é¢ä¸´çš„é™åˆ¶å’ŒæŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹é«˜æ•ˆçš„3Dé‡å»ºæ–¹æ³•ç”¨äºè·å–é«˜è´¨é‡çš„ç‰™é½¿ä¿¡æ¯ã€‚</li>
<li>è¯¦ç»†æè¿°äº†å¦‚ä½•åˆ©ç”¨3D Gaussian Splatting (3DGS)æ–¹æ³•è¿›è¡Œç‰™ç§‘å›¾åƒé‡å»ºçš„æŒ‘æˆ˜åŠå…¶é‡è¦æ€§ã€‚</li>
<li>çªå‡ºäº†æ–°æå‡ºçš„Dental3Ræ–¹æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼ŒåŒ…æ‹¬Geometry-Aware Pairing Strategy (GAPS)å’ŒåŸºäºå°æ³¢æ­£åˆ™åŒ–çš„ç›®æ ‡å‡½æ•°ä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†çš„å®éªŒéªŒè¯ï¼Œè¯æ˜äº†Dental3Ræ–¹æ³•åœ¨ç¨€ç–ã€æœªå®šä½è¾“å…¥æ¡ä»¶ä¸‹å¯¹å£è…”å›¾åƒè¿›è¡Œç¨³å¥ã€é«˜ä¿çœŸé‡å»ºçš„èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c2d9ccb80b304d468fa615807224194" align="middle">
<img src="https://picx.zhimg.com/v2-2f837e6c5157f3a496aa8e685c2e114d" align="middle">
<img src="https://picx.zhimg.com/v2-3dab4dba46af8552b818f59bb47f87f8" align="middle">
<img src="https://picx.zhimg.com/v2-8b01a4c3575c585038ad9e8a5ea54b9b" align="middle">
<img src="https://picx.zhimg.com/v2-43031384229a2cab498613da4b91f8ce" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Iterative-Diffusion-Refined-Neural-Attenuation-Fields-for-Multi-Source-Stationary-CT-Reconstruction-NAF-Meets-Diffusion-Model"><a href="#Iterative-Diffusion-Refined-Neural-Attenuation-Fields-for-Multi-Source-Stationary-CT-Reconstruction-NAF-Meets-Diffusion-Model" class="headerlink" title="Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model"></a>Iterative Diffusion-Refined Neural Attenuation Fields for Multi-Source Stationary CT Reconstruction: NAF Meets Diffusion Model</h2><p><strong>Authors:Jiancheng Fang, Shaoyu Wang, Junlin Wang, Weiwen Wu, Yikun Zhang, Qiegen Liu</strong></p>
<p>Multi-source stationary computed tomography (CT) has recently attracted attention for its ability to achieve rapid image reconstruction, making it suitable for time-sensitive clinical and industrial applications. However, practical systems are often constrained by ultra-sparse-view sampling, which significantly degrades reconstruction quality. Traditional methods struggle under ultra-sparse-view settings, where interpolation becomes inaccurate and the resulting reconstructions are unsatisfactory. To address this challenge, this study proposes Diffusion-Refined Neural Attenuation Fields (Diff-NAF), an iterative framework tailored for multi-source stationary CT under ultra-sparse-view conditions. Diff-NAF combines a Neural Attenuation Field representation with a dual-branch conditional diffusion model. The process begins by training an initial NAF using ultra-sparse-view projections. New projections are then generated through an Angle-Prior Guided Projection Synthesis strategy that exploits inter view priors, and are subsequently refined by a Diffusion-driven Reuse Projection Refinement Module. The refined projections are incorporated as pseudo-labels into the training set for the next iteration. Through iterative refinement, Diff-NAF progressively enhances projection completeness and reconstruction fidelity under ultra-sparse-view conditions, ultimately yielding high-quality CT reconstructions. Experimental results on multiple simulated 3D CT volumes and real projection data demonstrate that Diff-NAF achieves the best performance under ultra-sparse-view conditions.</p>
<blockquote>
<p>å¤šæºé™æ­¢è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å› å…¶å¿«é€Ÿå›¾åƒé‡å»ºèƒ½åŠ›è€Œæœ€è¿‘å¤‡å—å…³æ³¨ï¼Œä½¿å…¶é€‚ç”¨äºæ—¶é—´æ•æ„Ÿçš„ä¸´åºŠå’Œå·¥ä¸šåº”ç”¨ã€‚ç„¶è€Œï¼Œå®é™…ç³»ç»Ÿé€šå¸¸å—åˆ°è¶…ç¨€ç–è§†å›¾é‡‡æ ·çš„é™åˆ¶ï¼Œè¿™æ˜¾è‘—é™ä½äº†é‡å»ºè´¨é‡ã€‚åœ¨è¶…ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹ï¼Œä¼ ç»Ÿæ–¹æ³•è¡¨ç°ä¸ä½³ï¼Œæ’å€¼å˜å¾—ä¸å‡†ç¡®ï¼Œé‡å»ºç»“æœä»¤äººä¸æ»¡æ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†æ‰©æ•£ç»†åŒ–ç¥ç»ç½‘ç»œè¡°å‡åœºï¼ˆDiff-NAFï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šæºé™æ­¢CTåœ¨è¶…ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹çš„è¿­ä»£æ¡†æ¶ã€‚Diff-NAFç»“åˆäº†ç¥ç»ç½‘ç»œè¡°å‡åœºè¡¨ç¤ºå’ŒåŒåˆ†æ”¯æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚è¿‡ç¨‹é¦–å…ˆä½¿ç”¨è¶…ç¨€ç–è§†å›¾æŠ•å½±è®­ç»ƒåˆå§‹NAFã€‚ç„¶åï¼Œé€šè¿‡åˆ©ç”¨è§†è§’å…ˆéªŒçš„è§’åº¦å¼•å¯¼æŠ•å½±åˆæˆç­–ç•¥ç”Ÿæˆæ–°çš„æŠ•å½±ï¼Œéšåé€šè¿‡æ‰©æ•£é©±åŠ¨çš„é‡ç”¨æŠ•å½±ç»†åŒ–æ¨¡å—è¿›è¡Œç»†åŒ–ã€‚ç»†åŒ–åçš„æŠ•å½±è¢«ä½œä¸ºä¼ªæ ‡ç­¾çº³å…¥ä¸‹ä¸€è½®è®­ç»ƒé›†ã€‚é€šè¿‡è¿­ä»£ç»†åŒ–ï¼ŒDiff-NAFåœ¨è¶…ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹é€æ­¥æé«˜äº†æŠ•å½±çš„å®Œæ•´æ€§å’Œé‡å»ºçš„å¿ å®åº¦ï¼Œæœ€ç»ˆäº§ç”Ÿé«˜è´¨é‡çš„CTé‡å»ºå›¾åƒã€‚åœ¨å¤šä¸ªæ¨¡æ‹Ÿçš„3D CTä½“ç§¯å’ŒçœŸå®æŠ•å½±æ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDiff-NAFåœ¨è¶…ç¨€ç–è§†å›¾æ¡ä»¶ä¸‹è¡¨ç°æœ€ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14310v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†å¤šæºå›ºå®šè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æŠ€æœ¯åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³è¶…ç¨€ç–è§†è§’é‡‡æ ·å¯¼è‡´çš„é‡å»ºè´¨é‡ä¸‹é™é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDiffusion-Refined Neural Attenuation Fieldsï¼ˆDiff-NAFï¼‰çš„è¿­ä»£æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è¶…ç¨€ç–è§†è§’æ¡ä»¶ä¸‹æé«˜å›¾åƒé‡å»ºçš„é€Ÿåº¦å’Œè´¨é‡ã€‚Diff-NAFç»“åˆäº†ç¥ç»è¡°å‡åœºè¡¨ç¤ºå’ŒåŒåˆ†æ”¯æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–é€æ­¥æé«˜äº†æŠ•å½±çš„å®Œæ•´æ€§å’Œé‡å»ºçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiff-NAFåœ¨è¶…ç¨€ç–è§†è§’æ¡ä»¶ä¸‹å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæºå›ºå®šCTæŠ€æœ¯å› å¿«é€Ÿå›¾åƒé‡å»ºè€Œå¤‡å—å…³æ³¨ï¼Œé€‚ç”¨äºæ—¶é—´æ•æ„Ÿçš„ä¸´åºŠå’Œå·¥ä¸šåº”ç”¨ã€‚</li>
<li>è¶…ç¨€ç–è§†è§’é‡‡æ ·æ˜¯å®è·µä¸­çš„å¸¸è§é—®é¢˜ï¼Œä¼šæ˜¾è‘—å½±å“é‡å»ºè´¨é‡ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨è¶…ç¨€ç–è§†è§’æ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ï¼Œæ’å€¼ä¸å‡†ç¡®ï¼Œé‡å»ºç»“æœä¸ç†æƒ³ã€‚</li>
<li>Diff-NAFæ¡†æ¶ç»“åˆç¥ç»è¡°å‡åœºè¡¨ç¤ºå’ŒåŒåˆ†æ”¯æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå¤šæºå›ºå®šCTçš„è¶…ç¨€ç–è§†è§’æ¡ä»¶ã€‚</li>
<li>Diff-NAFé€šè¿‡è§’åº¦å¼•å¯¼æŠ•å½±åˆæˆç­–ç•¥å’Œæ‰©æ•£é©±åŠ¨çš„é‡ç”¨æŠ•å½±ä¼˜åŒ–æ¨¡å—è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDiff-NAFåœ¨å¤šä¸ªæ¨¡æ‹Ÿçš„3D CTä½“ç§¯å’ŒçœŸå®æŠ•å½±æ•°æ®ä¸Šï¼Œåœ¨è¶…ç¨€ç–è§†è§’æ¡ä»¶ä¸‹å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-951038796f4969da80c4d4aefb02276f" align="middle">
<img src="https://picx.zhimg.com/v2-4c183dfc0bbd477957b81fc5cd3a9711" align="middle">
<img src="https://picx.zhimg.com/v2-afa2603eadd86e7e992b9c9559f70656" align="middle">
<img src="https://picx.zhimg.com/v2-b9ea1ca2db0175d352829370449f6a6f" align="middle">
<img src="https://picx.zhimg.com/v2-d69fb156eb3c5406c54289594722c497" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SAM-Fed-SAM-Guided-Federated-Semi-Supervised-Learning-for-Medical-Image-Segmentation"><a href="#SAM-Fed-SAM-Guided-Federated-Semi-Supervised-Learning-for-Medical-Image-Segmentation" class="headerlink" title="SAM-Fed: SAM-Guided Federated Semi-Supervised Learning for Medical Image Segmentation"></a>SAM-Fed: SAM-Guided Federated Semi-Supervised Learning for Medical Image Segmentation</h2><p><strong>Authors:Sahar Nasirihaghighi, Negin Ghamsarian, Yiping Li, Marcel Breeuwer, Raphael Sznitman, Klaus Schoeffmann</strong></p>
<p>Medical image segmentation is clinically important, yet data privacy and the cost of expert annotation limit the availability of labeled data. Federated semi-supervised learning (FSSL) offers a solution but faces two challenges: pseudo-label reliability depends on the strength of local models, and client devices often require compact or heterogeneous architectures due to limited computational resources. These constraints reduce the quality and stability of pseudo-labels, while large models, though more accurate, cannot be trained or used for routine inference on client devices. We propose SAM-Fed, a federated semi-supervised framework that leverages a high-capacity segmentation foundation model to guide lightweight clients during training. SAM-Fed combines dual knowledge distillation with an adaptive agreement mechanism to refine pixel-level supervision. Experiments on skin lesion and polyp segmentation across homogeneous and heterogeneous settings show that SAM-Fed consistently outperforms state-of-the-art FSSL methods.</p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå…·æœ‰é‡è¦æ„ä¹‰ï¼Œç„¶è€Œæ•°æ®éšç§å’Œä¸“å®¶æ ‡æ³¨çš„æˆæœ¬é™åˆ¶äº†æ ‡è®°æ•°æ®çš„å¯ç”¨æ€§ã€‚è”åˆåŠç›‘ç£å­¦ä¹ ï¼ˆFSSLï¼‰æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œä½†é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šä¼ªæ ‡ç­¾çš„å¯é æ€§å–å†³äºæœ¬åœ°æ¨¡å‹çš„å¼ºåº¦ï¼Œè€Œç”±äºè®¡ç®—èµ„æºæœ‰é™ï¼Œå®¢æˆ·ç«¯è®¾å¤‡é€šå¸¸éœ€è¦ç´§å‡‘æˆ–å¼‚æ„æ¶æ„ã€‚è¿™äº›çº¦æŸé™ä½äº†ä¼ªæ ‡ç­¾çš„è´¨é‡å’Œç¨³å®šæ€§ï¼Œè€Œå¤§å‹æ¨¡å‹è™½ç„¶æ›´å‡†ç¡®ï¼Œä½†ä¸èƒ½åœ¨å®¢æˆ·ç«¯è®¾å¤‡è¿›è¡Œå¸¸è§„æ¨ç†è®­ç»ƒæˆ–ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†SAM-Fedï¼Œè¿™æ˜¯ä¸€ä¸ªè”é‚¦åŠç›‘ç£æ¡†æ¶ï¼Œåˆ©ç”¨é«˜å®¹é‡çš„åˆ†å‰²åŸºç¡€æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å¯¼è½»é‡çº§å®¢æˆ·ç«¯ã€‚SAM-Fedç»“åˆåŒçŸ¥è¯†è’¸é¦å’Œè‡ªé€‚åº”åè®®æœºåˆ¶æ¥ä¼˜åŒ–åƒç´ çº§ç›‘ç£ã€‚åœ¨çš®è‚¤ç—…å˜å’Œæ¯è‚‰åˆ†å‰²çš„åŒè´¨å’Œå¼‚è´¨è®¾ç½®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAM-Fedå§‹ç»ˆä¼˜äºæœ€æ–°çš„FSSLæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14302v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†ç”±äºæ•°æ®éšç§å’Œä¸“å®¶æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œå¯¼è‡´æ ‡æ³¨æ•°æ®æœ‰é™ã€‚è”é‚¦åŠç›‘ç£å­¦ä¹ ï¼ˆFSSLï¼‰è™½ä¸ºæ­¤æä¾›è§£å†³æ–¹æ¡ˆï¼Œä½†é¢ä¸´ä¼ªæ ‡ç­¾å¯é æ€§å’Œå®¢æˆ·ç«¯è®¡ç®—èµ„æºé™åˆ¶ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSAM-Fedæ¡†æ¶ï¼Œåˆ©ç”¨é«˜å®¹é‡çš„åˆ†å‰²åŸºç¡€æ¨¡å‹æŒ‡å¯¼è½»é‡çº§å®¢æˆ·ç«¯è¿›è¡Œè®­ç»ƒï¼Œå¹¶ç»“åˆåŒé‡çŸ¥è¯†è’¸é¦å’Œè‡ªé€‚åº”å…±è¯†æœºåˆ¶è¿›è¡Œåƒç´ çº§ç›‘ç£ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒSAM-Fedåœ¨ä¸åŒè®¾ç½®ä¸‹å¯¹çš®è‚¤ç—…å˜å’Œæ¯è‚‰åˆ†å‰²çš„åˆ†å‰²æ•ˆæœå‡ä¼˜äºç°æœ‰FSSLæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹ä¸´åºŠé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ•°æ®éšç§å’Œä¸“å®¶æ ‡æ³¨æˆæœ¬é™åˆ¶æ ‡æ³¨æ•°æ®çš„å¯ç”¨æ€§ã€‚</li>
<li>è”é‚¦åŠç›‘ç£å­¦ä¹ ï¼ˆFSSLï¼‰æ˜¯è§£å†³è¯¥é—®é¢˜çš„ä¸€ç§æ–¹æ³•ã€‚</li>
<li>FSSLé¢ä¸´ä¼ªæ ‡ç­¾å¯é æ€§å’Œå®¢æˆ·ç«¯è®¡ç®—èµ„æºé™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>SAM-Fedæ¡†æ¶åˆ©ç”¨é«˜å®¹é‡åˆ†å‰²åŸºç¡€æ¨¡å‹æŒ‡å¯¼å®¢æˆ·ç«¯è®­ç»ƒã€‚</li>
<li>SAM-Fedé€šè¿‡åŒé‡çŸ¥è¯†è’¸é¦å’Œè‡ªé€‚åº”å…±è¯†æœºåˆ¶ä¼˜åŒ–åƒç´ çº§ç›‘ç£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1214aa76e3668de11346ec5786cd9132" align="middle">
<img src="https://picx.zhimg.com/v2-6ba59ef40c9c56c0a64b69f3d978ad79" align="middle">
<img src="https://picx.zhimg.com/v2-ba70a15c1085fe2abb654bfd1ed3f370" align="middle">
<img src="https://picx.zhimg.com/v2-79d4f9f12559e9265874138041853806" align="middle">
<img src="https://picx.zhimg.com/v2-94633ae0bf3a191d12593c6aa99280a0" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="NeuralBoneReg-A-Novel-Self-Supervised-Method-for-Robust-and-Accurate-Multi-Modal-Bone-Surface-Registration"><a href="#NeuralBoneReg-A-Novel-Self-Supervised-Method-for-Robust-and-Accurate-Multi-Modal-Bone-Surface-Registration" class="headerlink" title="NeuralBoneReg: A Novel Self-Supervised Method for Robust and Accurate Multi-Modal Bone Surface Registration"></a>NeuralBoneReg: A Novel Self-Supervised Method for Robust and Accurate Multi-Modal Bone Surface Registration</h2><p><strong>Authors:Luohong Wu, Matthias Seibold, Nicola A. Cavalcanti, Yunke Ao, Roman Flepp, Aidana Massalimova, Lilian Calvet, Philipp FÃ¼rnstahl</strong></p>
<p>In computer- and robot-assisted orthopedic surgery (CAOS), patient-specific surgical plans derived from preoperative imaging define target locations and implant trajectories. During surgery, these plans must be accurately transferred, relying on precise cross-registration between preoperative and intraoperative data. However, substantial modality heterogeneity across imaging modalities makes this registration challenging and error-prone. Robust, automatic, and modality-agnostic bone surface registration is therefore clinically important. We propose NeuralBoneReg, a self-supervised, surface-based framework that registers bone surfaces using 3D point clouds as a modality-agnostic representation. NeuralBoneReg includes two modules: an implicit neural unsigned distance field (UDF) that learns the preoperative bone model, and an MLP-based registration module that performs global initialization and local refinement by generating transformation hypotheses to align the intraoperative point cloud with the neural UDF. Unlike SOTA supervised methods, NeuralBoneReg operates in a self-supervised manner, without requiring inter-subject training data. We evaluated NeuralBoneReg against baseline methods on two publicly available multi-modal datasets: a CT-ultrasound dataset of the fibula and tibia (UltraBones100k) and a CT-RGB-D dataset of spinal vertebrae (SpineDepth). The evaluation also includes a newly introduced CTâ€“ultrasound dataset of cadaveric subjects containing femur and pelvis (UltraBones-Hip), which will be made publicly available. NeuralBoneReg matches or surpasses existing methods across all datasets, achieving mean RRE&#x2F;RTE of 1.68Â°&#x2F;1.86 mm on UltraBones100k, 1.88Â°&#x2F;1.89 mm on UltraBones-Hip, and 3.79Â°&#x2F;2.45 mm on SpineDepth. These results demonstrate strong generalizability across anatomies and modalities, providing robust and accurate cross-modal alignment for CAOS.</p>
<blockquote>
<p>åœ¨è®¡ç®—æœºè¾…åŠ©éª¨ç§‘æ‰‹æœ¯ï¼ˆCAOSï¼‰ä¸­ï¼Œæ ¹æ®æœ¯å‰æˆåƒåˆ¶å®šçš„æ‚£è€…ç‰¹å¼‚æ€§æ‰‹æœ¯è®¡åˆ’å®šä¹‰äº†ç›®æ ‡ä½ç½®å’Œæ¤å…¥ç‰©è½¨è¿¹ã€‚åœ¨æ‰‹æœ¯è¿‡ç¨‹ä¸­ï¼Œå¿…é¡»å‡†ç¡®ä¼ è¾“è¿™äº›è®¡åˆ’ï¼Œè¿™ä¾èµ–äºæœ¯å‰å’Œæœ¯ä¸­æ•°æ®ä¹‹é—´çš„ç²¾ç¡®è·¨æ³¨å†Œã€‚ç„¶è€Œï¼Œä¸åŒæˆåƒæ–¹å¼ä¹‹é—´çš„æ¨¡æ€å¼‚è´¨æ€§ä½¿å¾—è¿™ç§æ³¨å†Œå…·æœ‰æŒ‘æˆ˜æ€§å’Œæ˜“å‡ºé”™ã€‚å› æ­¤ï¼Œç¨³å¥ã€è‡ªåŠ¨ã€ä¸æ¨¡æ€æ— å…³çš„éª¨è¡¨é¢æ³¨å†Œåœ¨ä¸´åºŠä¸Šå…·æœ‰é‡è¦æ„ä¹‰ã€‚æˆ‘ä»¬æå‡ºäº†NeuralBoneRegï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªç›‘ç£ã€åŸºäºè¡¨é¢çš„æ¡†æ¶ï¼Œä½¿ç”¨3Dç‚¹äº‘ä½œä¸ºä¸æ¨¡æ€æ— å…³çš„è¡¨ç¤ºæ¥æ³¨å†Œéª¨è¡¨é¢ã€‚NeuralBoneRegåŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šä¸€ä¸ªéšå¼ç¥ç»ç½‘ç»œæ— ç¬¦å·è·ç¦»åœºï¼ˆUDFï¼‰ï¼Œç”¨äºå­¦ä¹ æœ¯å‰éª¨æ¨¡å‹ï¼Œä¸€ä¸ªåŸºäºå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„æ³¨å†Œæ¨¡å—ï¼Œé€šè¿‡ç”Ÿæˆè½¬æ¢å‡è®¾æ¥æ‰§è¡Œå…¨å±€åˆå§‹åŒ–å’Œå±€éƒ¨ç»†åŒ–ï¼Œä»¥å°†æœ¯ä¸­ç‚¹äº‘ä¸ç¥ç»UDFå¯¹é½ã€‚ä¸æœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•ä¸åŒï¼ŒNeuralBoneRegé‡‡ç”¨è‡ªç›‘ç£æ–¹å¼è¿è¡Œï¼Œæ— éœ€è·¨ä¸»ä½“è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„å¤šæ¨¡å¼æ•°æ®é›†ä¸Šå¯¹NeuralBoneRegä¸åŸºå‡†æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼šè…“éª¨å’Œèƒ«éª¨çš„CT-è¶…å£°æ•°æ®é›†ï¼ˆUltraBones100kï¼‰å’Œè„Šæ¤éª¨çš„CT-RGB-Dæ•°æ®é›†ï¼ˆSpineDepthï¼‰ã€‚è¯„ä¼°è¿˜åŒ…æ‹¬æ–°å¼•å…¥çš„åŒ…å«è‚¡éª¨å’Œéª¨ç›†çš„å°¸æ£€CT-è¶…å£°æ•°æ®é›†ï¼ˆUltraBones-Hipï¼‰ï¼Œè¯¥æ•°æ®é›†å°†å…¬å¼€æä¾›ã€‚NeuralBoneRegåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå‡è¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨UltraBones100kä¸Šçš„å¹³å‡RRE&#x2F;RTEä¸º1.68Â°&#x2F;1.86æ¯«ç±³ï¼Œåœ¨UltraBones-Hipä¸Šä¸º1.88Â°&#x2F;1.89æ¯«ç±³ï¼Œåœ¨SpineDepthä¸Šä¸º3.79Â°&#x2F;2.45æ¯«ç±³ã€‚è¿™äº›ç»“æœè¯æ˜äº†åœ¨ä¸åŒè§£å‰–ç»“æ„å’Œæ¨¡æ€ä¸‹çš„å¼ºå¤§é€šç”¨æ€§ï¼Œä¸ºCAOSæä¾›äº†ç¨³å¥è€Œå‡†ç¡®çš„è·¨æ¨¡æ€å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14286v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨éª¨ç§‘è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ï¼ˆCAOSï¼‰ä¸­ï¼ŒåŸºäºæœ¯å‰æˆåƒåˆ¶å®šæ‚£è€…ç‰¹å¼‚æ€§æ‰‹æœ¯è®¡åˆ’çš„é‡è¦æ€§ã€‚æ–‡ç« æå‡ºäº†NeuralBoneRegï¼Œä¸€ç§è‡ªç›‘ç£çš„åŸºäºè¡¨é¢çš„æ¡†æ¶ï¼Œç”¨äºä½¿ç”¨3Dç‚¹äº‘ä½œä¸ºæ¨¡æ€æ— å…³è¡¨ç¤ºè¿›è¡Œéª¨éª¼è¡¨é¢æ³¨å†Œã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šå­¦ä¹ æœ¯å‰éª¨éª¼æ¨¡å‹çš„éšå¼ç¥ç»æ— ç¬¦å·è·ç¦»åœºï¼ˆUDFï¼‰å’ŒåŸºäºMLPçš„æ³¨å†Œæ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡ç”Ÿæˆè½¬æ¢å‡è®¾æ¥æ‰§è¡Œå…¨å±€åˆå§‹åŒ–å’Œå±€éƒ¨ç»†åŒ–ï¼Œä½¿æœ¯ä¸­ç‚¹äº‘ä¸ç¥ç»UDFå¯¹é½ã€‚åœ¨å¤šä¸ªå…¬å…±å¯ç”¨å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒNeuralBoneRegåœ¨è·¨æ¨¡æ€å¯¹é½æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºCAOSæä¾›äº†ç¨³å¥å’Œå‡†ç¡®çš„äº¤å‰æ¨¡æ€å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©éª¨ç§‘æ‰‹æœ¯ï¼ˆCAOSï¼‰ä¸­ï¼Œéœ€æ ¹æ®æœ¯å‰æˆåƒåˆ¶å®šæ‚£è€…ç‰¹å¼‚æ€§æ‰‹æœ¯è®¡åˆ’ï¼Œæ˜ç¡®ç›®æ ‡ä½ç½®å’Œæ¤å…¥è½¨è¿¹ã€‚</li>
<li>æœ¯ä¸­éœ€è¦å‡†ç¡®è½¬æ¢è¿™äº›è®¡åˆ’ï¼Œè¿™ä¾èµ–äºæœ¯å‰å’Œæœ¯ä¸­çš„æ•°æ®ä¹‹é—´çš„ç²¾ç¡®è·¨æ³¨å†Œã€‚</li>
<li>ä¸åŒçš„æˆåƒæ¨¡å¼ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ¨¡æ€å¼‚è´¨æ€§ï¼Œä½¿å¾—æ³¨å†Œå…·æœ‰æŒ‘æˆ˜æ€§å’Œæ˜“å‡ºé”™æ€§ã€‚</li>
<li>æå‡ºäº†NeuralBoneRegï¼Œä¸€ç§è‡ªç›‘ç£çš„åŸºäºè¡¨é¢çš„æ¡†æ¶ï¼Œç”¨äºä½¿ç”¨3Dç‚¹äº‘è¿›è¡Œéª¨éª¼è¡¨é¢æ³¨å†Œï¼Œå…·æœ‰æ¨¡æ€æ— å…³æ€§ã€‚</li>
<li>NeuralBoneRegåŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šéšå¼ç¥ç»æ— ç¬¦å·è·ç¦»åœºï¼ˆUDFï¼‰å’ŒåŸºäºMLPçš„æ³¨å†Œæ¨¡å—ã€‚</li>
<li>NeuralBoneRegåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ed3390b5904c3caeee4fa9218e4c7f1" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Unveiling-the-Sources-of-X-ray-Luminosity-in-DESI-Galaxy-Groups-Insights-from-the-SRG-eROSITA-All-Sky-Survey"><a href="#Unveiling-the-Sources-of-X-ray-Luminosity-in-DESI-Galaxy-Groups-Insights-from-the-SRG-eROSITA-All-Sky-Survey" class="headerlink" title="Unveiling the Sources of X-ray Luminosity in DESI Galaxy Groups: Insights from the SRG&#x2F;eROSITA All-Sky Survey"></a>Unveiling the Sources of X-ray Luminosity in DESI Galaxy Groups: Insights from the SRG&#x2F;eROSITA All-Sky Survey</h2><p><strong>Authors:YunLiang Zheng, Xiaohu Yang, Teng Liu, Shijiang Chen, Esra Bulbul, Ang Liu, Yi Zhang, Dawei Li, Xi Kang, Yizhou Gu, Yirong Wang, Qingyang Li, Jiaqi Wang</strong></p>
<p>We use the first eROSITA all-sky survey (eRASS1) to investigate the contributions of AGN and extended gas to the total X-ray luminosity ($L_X$) of galaxy groups with different halo masses ($M_h$) at different redshifts. The presence of AGN in their central galaxies is identified using multi-wavelength catalogs, including the X-ray counterparts, the ASKAP radio catalog, and the DESI spectroscopic measurements. We apply the stacking method to obtain sufficient statistics for the X-ray surface brightness profile and the $L_X$ for groups with different central AGN properties. We find that the X-ray groups exhibit the highest $L_X$, followed by groups with QSO, radio, BPT-AGN, and non-AGN centrals. Moreover, the $L_X$ of the $M_h \lesssim 10^{13}h^{-1}M_\odot$ groups is dominated by the central AGN, while the X-ray emission from extended gas tends to be more prominent in the $M_h \gtrsim 10^{13}h^{-1}M_\odot$ groups. In groups where the AGN play a major role in X-ray emission, the contribution from extended gas is minor, resulting in significant uncertainties concerning the extended X-ray emission. When the subset containing the X-ray detected counterparts is excluded, the extended gas component becomes easier to obtain. A correlation has been identified between the X-ray luminosity of the central AGN and extended gas. However, once we account for the positional offset, their correlation becomes less prominent. Currently, the results are not conclusive enough to confirm whether there is a connection between the AGN feedback and extended gas. However, they provide a new perspective on the feedback processes in the history of group assembly.</p>
<blockquote>
<p>æˆ‘ä»¬åˆ©ç”¨eROSITAé¦–æ¬¡å…¨å¤©ç©ºè°ƒæŸ¥ï¼ˆeRASS1ï¼‰æ¥ç ”ç©¶æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰å’Œå»¶å±•æ°”ä½“å¯¹ä¸åŒçº¢ç§»ä¸‹ä¸åŒæ™•è´¨é‡ï¼ˆMhï¼‰æ˜Ÿç³»ç¾¤æ€»Xå°„çº¿å…‰åº¦ï¼ˆLxï¼‰çš„è´¡çŒ®ã€‚é€šè¿‡å¤šæ³¢æ®µç›®å½•ï¼ŒåŒ…æ‹¬Xå°„çº¿å¯¹åº”ä½“ã€ASKAPå°„ç”µç›®å½•å’ŒDESIå…‰è°±æµ‹é‡ï¼Œç¡®å®šäº†å…¶ä¸­å¿ƒæ˜Ÿç³»ä¸­æ´»åŠ¨æ˜Ÿç³»æ ¸çš„å­˜åœ¨ã€‚æˆ‘ä»¬åº”ç”¨å åŠ æ–¹æ³•è·å¾—ä¸åŒä¸­å¿ƒæ´»åŠ¨æ˜Ÿç³»æ ¸å±æ€§çš„æ˜Ÿç³»çš„Xå°„çº¿è¡¨é¢äº®åº¦åˆ†å¸ƒå’ŒLxçš„è¶³å¤Ÿç»Ÿè®¡æ•°æ®ã€‚æˆ‘ä»¬å‘ç°ï¼ŒXå°„çº¿æ˜Ÿç³»çš„Lxæœ€é«˜ï¼Œå…¶æ¬¡æ˜¯å…·æœ‰ç±»æ˜Ÿä½“ã€å°„ç”µã€BPT-AGNå’Œéæ´»åŠ¨æ˜Ÿç³»æ ¸çš„ä¸­å¿ƒæ˜Ÿç³»ã€‚æ­¤å¤–ï¼Œå¯¹äºMhå°äºæˆ–ç­‰äº10^13h^-1M_âŠ™çš„æ˜Ÿç³»ç¾¤æ¥è¯´ï¼Œä¸­å¿ƒæ´»åŠ¨æ˜Ÿç³»æ ¸å¯¹Lxçš„è´¡çŒ®å ä¸»å¯¼åœ°ä½ï¼Œè€Œå»¶å±•æ°”ä½“çš„Xå°„çº¿å‘å°„åœ¨Mhå¤§äºæˆ–ç­‰äº10^13h^-1M_âŠ™çš„æ˜Ÿç³»ç¾¤ä¸­æ›´ä¸ºçªå‡ºã€‚åœ¨ä¸­å¿ƒæ´»åŠ¨æ˜Ÿç³»æ ¸å¯¹Xå°„çº¿å‘å°„èµ·ä¸»è¦ä½œç”¨çš„æ˜Ÿç³»ç¾¤ä¸­ï¼Œå»¶å±•æ°”ä½“çš„è´¡çŒ®è¾ƒå°ï¼Œå¯¼è‡´å»¶å±•Xå°„çº¿å‘å°„å­˜åœ¨é‡å¤§ä¸ç¡®å®šæ€§ã€‚å½“æ’é™¤åŒ…å«Xå°„çº¿æ£€æµ‹å¯¹åº”ä½“çš„å­é›†æ—¶ï¼Œæ›´å®¹æ˜“è·å¾—å»¶å±•æ°”ä½“æˆåˆ†ã€‚ä¸­å¤®æ´»åŠ¨æ˜Ÿç³»æ ¸çš„Xå°„çº¿å…‰åº¦å’Œå»¶å±•æ°”ä½“ä¹‹é—´å·²ç»å‘ç°äº†ä¸€å®šçš„ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œä¸€æ—¦è€ƒè™‘åˆ°ä½ç½®åç§»ï¼Œå®ƒä»¬ä¹‹é—´çš„å…³è”å°±å˜å¾—ä¸é‚£ä¹ˆæ˜¾è‘—ã€‚ç›®å‰çš„ç»“æœå°šä¸è¶³ä»¥ç¡®å®šæ´»åŠ¨æ˜Ÿç³»æ ¸åé¦ˆä¸å»¶å±•æ°”ä½“ä¹‹é—´æ˜¯å¦å­˜åœ¨è”ç³»ã€‚ç„¶è€Œï¼Œè¿™ä¸ºç ”ç©¶æ˜Ÿç³»å›¢å½¢æˆè¿‡ç¨‹ä¸­çš„åé¦ˆè¿‡ç¨‹æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14216v1">PDF</a> 29 pages, 14 figures, ApJ accepted</p>
<p><strong>Summary</strong><br>     åˆ©ç”¨eROSITAå…¨å¤©å›¾é¦–æ¬¡è°ƒæŸ¥ï¼ˆeRASS1ï¼‰ç ”ç©¶ä¸åŒä¸­å¿ƒæ˜Ÿç³»æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰çš„è´¡çŒ®ä»¥åŠä¸åŒè´¨é‡æš—ç‰©è´¨æ™•ï¼ˆMhï¼‰æ˜Ÿç³»å›¢ä¸­å»¶å±•æ°”ä½“å¯¹æ€»Xå°„çº¿å…‰åº¦ï¼ˆLxï¼‰çš„è´¡çŒ®ã€‚é€šè¿‡å¤šæ³¢æ®µç›®å½•è¯†åˆ«ä¸­å¤®æ˜Ÿç³»ä¸­çš„æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ŒåŒ…æ‹¬Xå°„çº¿å¯¹åº”ç‰©ã€ASKAPæ— çº¿ç”µç›®å½•å’ŒDESIå…‰è°±æµ‹é‡ã€‚åº”ç”¨å †å æ–¹æ³•è·å¾—ä¸åŒä¸­å¤®æ´»åŠ¨æ˜Ÿç³»æ ¸ç‰¹æ€§çš„æ˜Ÿç³»çš„Xå°„çº¿è¡¨é¢äº®åº¦åˆ†å¸ƒå’ŒLxã€‚å‘ç°Xå°„çº¿æ˜Ÿå›¢å…·æœ‰æœ€é«˜çš„Lxï¼Œå…¶æ¬¡æ˜¯QSOã€æ— çº¿ç”µã€BPT- AGNå’Œéæ´»åŠ¨æ˜Ÿç³»æ ¸æ˜Ÿå›¢ã€‚å¯¹äºMhå°äºç­‰äº10^13h^-1MâŠ™çš„æ˜Ÿå›¢ï¼Œä¸­å¤®æ´»åŠ¨æ˜Ÿç³»æ ¸å¯¹Lxçš„è´¡çŒ®å ä¸»å¯¼åœ°ä½ï¼Œè€Œå»¶å±•æ°”ä½“çš„Xå°„çº¿å‘å°„åœ¨Mhå¤§äºæˆ–ç­‰äº10^13h^-1MâŠ™çš„æ˜Ÿå›¢ä¸­æ›´ä¸ºçªå‡ºã€‚æ´»åŠ¨æ˜Ÿç³»æ ¸å¯¹Xå°„çº¿å‘å°„èµ·ä¸»è¦ä½œç”¨çš„æ˜Ÿå›¢ä¸­ï¼Œå»¶å±•æ°”ä½“çš„è´¡çŒ®è¾ƒå°ï¼Œå¯¼è‡´å»¶å±•Xå°„çº¿å‘å°„å­˜åœ¨é‡å¤§ä¸ç¡®å®šæ€§ã€‚å½“æ’é™¤åŒ…å«Xå°„çº¿æ£€æµ‹å¯¹åº”ç‰©çš„å­é›†æ—¶ï¼Œæ›´å®¹æ˜“è·å¾—å»¶å±•æ°”ä½“æˆåˆ†ã€‚ä¸­å¤®æ´»åŠ¨æ˜Ÿç³»æ ¸çš„Xå°„çº¿å…‰åº¦å’Œå»¶å±•æ°”ä½“ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œä½†è€ƒè™‘åˆ°ä½ç½®åç§»ï¼Œå…¶ç›¸å…³æ€§å˜å¾—ä¸é‚£ä¹ˆæ˜æ˜¾ã€‚ç›®å‰çš„ç»“æœå°šä¸è¶³ä»¥è¯å®æ´»åŠ¨æ˜Ÿç³»æ ¸åé¦ˆä¸å»¶å±•æ°”ä½“ä¹‹é—´çš„è”ç³»ï¼Œä½†å®ƒä»¬ä¸ºç¾¤ä½“ç»„è£…è¿‡ç¨‹ä¸­çš„åé¦ˆè¿‡ç¨‹æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨eROSITAå…¨å¤©å›¾é¦–æ¬¡è°ƒæŸ¥ç ”ç©¶äº†ä¸åŒä¸­å¿ƒæ˜Ÿç³»çš„æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰å’Œå»¶å±•æ°”ä½“å¯¹æ˜Ÿç³»å›¢æ€»Xå°„çº¿å…‰åº¦çš„è´¡çŒ®ã€‚</li>
<li>é€šè¿‡å¤šæ³¢æ®µç›®å½•è¯†åˆ«ä¸­å¤®æ˜Ÿç³»ä¸­çš„æ´»åŠ¨æ˜Ÿç³»æ ¸ã€‚</li>
<li>åº”ç”¨å †å æ–¹æ³•åˆ†æä¸åŒä¸­å¤®æ´»åŠ¨æ˜Ÿç³»æ ¸ç‰¹æ€§çš„æ˜Ÿç³»çš„Xå°„çº¿è¡¨é¢äº®åº¦åˆ†å¸ƒå’ŒLxã€‚</li>
<li>å‘ç°ä¸åŒç±»å‹çš„æ˜Ÿå›¢åœ¨Lxä¸Šå­˜åœ¨å·®å¼‚ï¼Œå…¶ä¸­Xå°„çº¿æ˜Ÿå›¢å…·æœ‰æœ€é«˜çš„Lxå€¼ã€‚</li>
<li>åœ¨ä½è´¨é‡æ˜Ÿå›¢ä¸­ï¼Œä¸­å¤®æ´»åŠ¨æ˜Ÿç³»æ ¸å¯¹Lxçš„è´¡çŒ®æ˜¾è‘—ï¼›è€Œåœ¨é«˜è´¨é‡æ˜Ÿå›¢ä¸­ï¼Œå»¶å±•æ°”ä½“çš„Xå°„çº¿å‘å°„æ›´ä¸ºçªå‡ºã€‚</li>
<li>æ´»åŠ¨æ˜Ÿç³»æ ¸å¯¹Xå°„çº¿å‘å°„èµ·ä¸»å¯¼ä½œç”¨çš„æ˜Ÿå›¢ä¸­ï¼Œå»¶å±•æ°”ä½“çš„è´¡çŒ®å­˜åœ¨è¾ƒå¤§çš„ä¸ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b468d0c26d86de4034d49ac70ef47e47" align="middle">
<img src="https://picx.zhimg.com/v2-6409c5d2894ef8086f3347bd91d488b4" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GCA-ResUNet-Image-segmentation-in-medical-images-using-grouped-coordinate-attention"><a href="#GCA-ResUNet-Image-segmentation-in-medical-images-using-grouped-coordinate-attention" class="headerlink" title="GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention"></a>GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention</h2><p><strong>Authors:Jun Ding, Shang Gao</strong></p>
<p>Medical image segmentation underpins computer-aided diagnosis and therapy by supporting clinical diagnosis, preoperative planning, and disease monitoring. While U-Net style convolutional neural networks perform well due to their encoder-decoder structures with skip connections, they struggle to capture long-range dependencies. Transformer-based variants address global context but often require heavy computation and large training datasets. This paper proposes GCA-ResUNet, an efficient segmentation network that integrates Grouped Coordinate Attention (GCA) into ResNet-50 residual blocks. GCA uses grouped coordinate modeling to jointly encode global dependencies across channels and spatial locations, strengthening feature representation and boundary delineation while adding minimal parameter and FLOP overhead compared with self-attention. On the Synapse dataset, GCA-ResUNet achieves a Dice score of 86.11%, and on the ACDC dataset, it reaches 92.64%, surpassing several state-of-the-art baselines while maintaining fast inference and favorable computational efficiency. These results indicate that GCA offers a practical way to enhance convolutional architectures with global modeling capability, enabling high-accuracy and resource-efficient medical image segmentation.</p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—çš„åŸºçŸ³ï¼Œå®ƒæ”¯æŒä¸´åºŠè¯Šæ–­ã€æœ¯å‰è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹ã€‚è™½ç„¶U-Neté£æ ¼çš„å·ç§¯ç¥ç»ç½‘ç»œç”±äºå…¶å¸¦æœ‰è·³è·ƒè¿æ¥çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„è€Œè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ•æ‰é•¿æœŸä¾èµ–å…³ç³»æ–¹é¢ä»æœ‰å›°éš¾ã€‚åŸºäºTransformerçš„å˜ä½“è§£å†³äº†å…¨å±€ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå¤§å‹è®­ç»ƒæ•°æ®é›†ã€‚æœ¬æ–‡æå‡ºäº†GCA-ResUNetï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„åˆ†å‰²ç½‘ç»œï¼Œå®ƒå°†åˆ†ç»„åæ ‡æ³¨æ„åŠ›ï¼ˆGCAï¼‰é›†æˆåˆ°ResNet-50æ®‹å·®å—ä¸­ã€‚GCAä½¿ç”¨åˆ†ç»„åæ ‡å»ºæ¨¡æ¥è”åˆç¼–ç é€šé“å’Œç©ºé—´ä½ç½®ä¹‹é—´çš„å…¨å±€ä¾èµ–å…³ç³»ï¼ŒåŠ å¼ºäº†ç‰¹å¾è¡¨ç¤ºå’Œè¾¹ç•Œæç»˜ï¼Œä¸è‡ªæ³¨æ„åŠ›ç›¸æ¯”å¢åŠ äº†æå°‘çš„å‚æ•°å’ŒFLOPå¼€é”€ã€‚åœ¨Synapseæ•°æ®é›†ä¸Šï¼ŒGCA-ResUNetçš„Diceç³»æ•°ä¸º86.11%ï¼Œåœ¨ACDCæ•°æ®é›†ä¸Šè¾¾åˆ°äº†92.64%ï¼Œè¶…è¶Šäº†å¤šä¸ªæœ€å…ˆè¿›çš„åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒäº†å¿«é€Ÿæ¨ç†å’Œè®¡ç®—æ•ˆç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒGCAæä¾›äº†ä¸€ç§å®ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨å·ç§¯æ¶æ„ä¸­å¢å¼ºå…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼Œä»è€Œå®ç°é«˜å‡†ç¡®æ€§å’Œèµ„æºé«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.14087v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—çš„åŸºçŸ³ï¼Œä¸ºä¸´åºŠè¯Šæ–­ã€æœ¯å‰è§„åˆ’å’Œç–¾ç—…ç›‘æµ‹æä¾›æ”¯æŒã€‚æœ¬æ–‡æå‡ºä¸€ç§é«˜æ•ˆçš„åˆ†å‰²ç½‘ç»œGCA-ResUNetï¼Œå®ƒå°†Grouped Coordinate Attentionï¼ˆGCAï¼‰èå…¥ResNet-50æ®‹å·®å—ä¸­ã€‚GCAèƒ½å¤Ÿåœ¨ç¼–ç å…¨å±€ä¾èµ–å…³ç³»çš„åŒæ—¶å¢å¼ºç‰¹å¾è¡¨ç¤ºå’Œè¾¹ç•Œæç»˜ï¼Œä¸”å‚æ•°å’ŒFLOPså¼€é”€è¾ƒå°ã€‚åœ¨Synapseå’ŒACDCæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGCA-ResUNetå®ç°äº†è¾ƒé«˜çš„åˆ†å‰²ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ï¼Œä¸ºå·ç§¯æ¶æ„æä¾›äº†å®ç”¨çš„å…¨å±€å»ºæ¨¡å¢å¼ºæ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è¾…åŠ©ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è¿‡ç¨‹ä¸­å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>U-Neté£æ ¼çš„å·ç§¯ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†éš¾ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>Transformerå˜ä½“èƒ½å¤Ÿå¤„ç†å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½†éœ€è¦å¤§é‡è®¡ç®—å’Œè®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>GCA-ResUNetæ˜¯ä¸€ä¸ªç»“åˆGrouped Coordinate Attentionï¼ˆGCAï¼‰å’ŒResNet-50çš„åˆ†å‰²ç½‘ç»œã€‚</li>
<li>GCAèƒ½å¤Ÿåœ¨ç¼–ç å…¨å±€ä¾èµ–å…³ç³»çš„åŒæ—¶å¼ºåŒ–ç‰¹å¾è¡¨ç¤ºå’Œè¾¹ç•Œæç»˜ï¼Œä¸”å¼€é”€è¾ƒå°ã€‚</li>
<li>GCA-ResUNetåœ¨Synapseå’ŒACDCæ•°æ®é›†ä¸Šå®ç°äº†é«˜åˆ†å‰²ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.14087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52bb933a338cfb456ee88d75656e65e0" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-60cf10f3828651d48453811db52eb1ab" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  TTA Transcribe, Translate and Alignment for Cross-lingual Speech Representation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-20/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8b01a4c3575c585038ad9e8a5ea54b9b" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-20  Dental3R Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
