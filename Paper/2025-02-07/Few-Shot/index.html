<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-02-07  CAPE Covariate-Adjusted Pre-Training for Epidemic Time Series   Forecasting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a5f0f3e191c20353e3c9c955c5d1542a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-07-更新"><a href="#2025-02-07-更新" class="headerlink" title="2025-02-07 更新"></a>2025-02-07 更新</h1><h2 id="CAPE-Covariate-Adjusted-Pre-Training-for-Epidemic-Time-Series-Forecasting"><a href="#CAPE-Covariate-Adjusted-Pre-Training-for-Epidemic-Time-Series-Forecasting" class="headerlink" title="CAPE: Covariate-Adjusted Pre-Training for Epidemic Time Series   Forecasting"></a>CAPE: Covariate-Adjusted Pre-Training for Epidemic Time Series   Forecasting</h2><p><strong>Authors:Zewen Liu, Juntong Ni, Max S. Y. Lau, Wei Jin</strong></p>
<p>Accurate forecasting of epidemic infection trajectories is crucial for safeguarding public health. However, limited data availability during emerging outbreaks and the complex interaction between environmental factors and disease dynamics present significant challenges for effective forecasting. In response, we introduce CAPE, a novel epidemic pre-training framework designed to harness extensive disease datasets from diverse regions and integrate environmental factors directly into the modeling process for more informed decision-making on downstream diseases. Based on a covariate adjustment framework, CAPE utilizes pre-training combined with hierarchical environment contrasting to identify universal patterns across diseases while estimating latent environmental influences. We have compiled a diverse collection of epidemic time series datasets and validated the effectiveness of CAPE under various evaluation scenarios, including full-shot, few-shot, zero-shot, cross-location, and cross-disease settings, where it outperforms the leading baseline by an average of 9.9% in full-shot and 14.3% in zero-shot settings. The code will be released upon acceptance. </p>
<blockquote>
<p>流行病感染轨迹的准确预测对于保障公共卫生至关重要。然而，新兴疫情期间的有限数据以及环境因素与疾病动态之间的复杂相互作用为有效预测带来了重大挑战。为了应对这些挑战，我们引入了CAPE，这是一个新型的流行病预训练框架，旨在利用来自不同地区的广泛疾病数据集，并将环境因素直接融入建模过程，以为下游疾病的决策提供更有依据的决策支持。基于协变量调整框架，CAPE利用预训练结合分层环境对比，在估计潜在环境影响的同时，识别疾病之间的通用模式。我们收集了各种各样的流行病时间序列数据集，并在各种评估场景下验证了CAPE的有效性，包括全数据、小数据、零数据、跨地点和跨疾病设置。CAPE在全数据场景下较领先基线高出平均9.9%，在零数据场景下高出平均14.3%。代码将在接受后发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03393v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CAPE是一个新型传染病预训练框架，利用广泛的疾病数据集和环境因素信息来预测传染病的轨迹，提高了决策的可靠性。通过采用协变量调整框架、层次环境对比和预训练技术，CAPE能够在各种疾病背景下识别通用模式并估算潜在环境影响。在多种评估场景下，CAPE的表现优于现有基线模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAPE是一个针对传染病预测的预训练框架，用于利用广泛的疾病数据集和环境因素进行建模。</li>
<li>CAPE采用协变量调整框架来整合环境数据，提高预测准确性。</li>
<li>通过层次环境对比技术，CAPE能够识别不同疾病间的通用模式。</li>
<li>CAPE通过预训练技术来优化模型性能。</li>
<li>在多种评估场景中，CAPE表现优于现有基线模型，特别是在少样本和无样本场景下表现更突出。</li>
<li>CAPE能够处理有限数据下的传染病预测挑战，为公共卫生决策提供更可靠的信息支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03393">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0697a327b6144f92d5e444c62bf70d5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30a476abf2aa710d3be00549ba98f438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0adbab8d823026deb62259306b368ac9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62195b8b58dcf8508ce8dd592480fecc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ALPET-Active-Few-shot-Learning-for-Citation-Worthiness-Detection-in-Low-Resource-Wikipedia-Languages"><a href="#ALPET-Active-Few-shot-Learning-for-Citation-Worthiness-Detection-in-Low-Resource-Wikipedia-Languages" class="headerlink" title="ALPET: Active Few-shot Learning for Citation Worthiness Detection in   Low-Resource Wikipedia Languages"></a>ALPET: Active Few-shot Learning for Citation Worthiness Detection in   Low-Resource Wikipedia Languages</h2><p><strong>Authors:Aida Halitaj, Arkaitz Zubiaga</strong></p>
<p>Citation Worthiness Detection (CWD) consists in determining which sentences, within an article or collection, should be backed up with a citation to validate the information it provides. This study, introduces ALPET, a framework combining Active Learning (AL) and Pattern-Exploiting Training (PET), to enhance CWD for languages with limited data resources. Applied to Catalan, Basque, and Albanian Wikipedia datasets, ALPET outperforms the existing CCW baseline while reducing the amount of labeled data in some cases above 80%. ALPET’s performance plateaus after 300 labeled samples, showing it suitability for low-resource scenarios where large, labeled datasets are not common. While specific active learning query strategies, like those employing K-Means clustering, can offer advantages, their effectiveness is not universal and often yields marginal gains over random sampling, particularly with smaller datasets. This suggests that random sampling, despite its simplicity, remains a strong baseline for CWD in constraint resource environments. Overall, ALPET’s ability to achieve high performance with fewer labeled samples makes it a promising tool for enhancing the verifiability of online content in low-resource language settings. </p>
<blockquote>
<p>引用价值检测（CWD）的目的是确定文章或集合中的哪些句子应该通过引用验证其提供的信息。本研究介绍了ALPET，这是一个结合主动学习（AL）和模式挖掘训练（PET）的框架，以提高对数据资源有限的语言的CWD能力。在加泰罗尼亚语、巴斯克语和阿尔巴尼亚语维基百科数据集上应用时，ALPET的表现超过了现有的CCW基线，在某些情况下减少了超过80%的标注数据。ALPET的性能在300个标注样本后达到平稳，这显示出它在资源匮乏的场景下的适用性，在那里大型标注数据集并不常见。虽然特定的主动学习查询策略，如使用K-Means聚类等，可能会带来优势，但其有效性并非普遍适用，并且往往只带来相对于随机采样的微小收益，特别是在较小的数据集上。这表明，尽管随机采样方法简单，但在受限资源环境中仍然是CWD的强基线。总体而言，ALPET能够在较少的标注样本上实现高性能，使其成为提高低资源语言环境中在线内容可验证性的有前途的工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03292v1">PDF</a> 24 pages, 8 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Citation Worthiness Detection（CWD）的概念，并提出了一种结合主动学习和模式挖掘训练（ALPET）的新框架，以提高在低资源语言环境中的CWD性能。研究表明，ALPET在加泰罗尼亚语、巴斯克语和阿尔巴尼亚语Wikipedia数据集上的表现优于现有CCW基线，并且在某些情况下可以减少80%以上的标注数据量。此外，ALPET在300个标注样本后性能趋于稳定，适合资源匮乏的场景。虽然特定的主动学习和查询策略如K-Means聚类在某些情况下具有优势，但其效果并非普遍有效，且通常仅在大型数据集上获得微小优势。因此，随机采样在资源受限环境中仍是CWD的强基线。总体而言，ALPET在减少标注样本的同时实现高性能，是增强低资源语言环境中在线内容可验证性的有前途的工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Citation Worthiness Detection (CWD)旨在确定文章中哪些句子应该通过引用验证其提供的信息。</li>
<li>ALPET框架结合了主动学习和模式挖掘训练，旨在提高低资源语言环境中的CWD性能。</li>
<li>ALPET在加泰罗尼亚语、巴斯克语和阿尔巴尼亚语Wikipedia数据集上的表现优于现有基线。</li>
<li>ALPET能够在减少标注数据量的同时保持高性能，有时减少量超过80%。</li>
<li>在低资源环境下，随机采样作为CWD的基线策略仍然有效。</li>
<li>特定主动学习和查询策略的效果并非普遍显著，通常在大型数据集上才能获得微小优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03292">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-901055292620b1aec7c0f644add20e05.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Scalable-In-Context-Learning-on-Tabular-Data-via-Retrieval-Augmented-Large-Language-Models"><a href="#Scalable-In-Context-Learning-on-Tabular-Data-via-Retrieval-Augmented-Large-Language-Models" class="headerlink" title="Scalable In-Context Learning on Tabular Data via Retrieval-Augmented   Large Language Models"></a>Scalable In-Context Learning on Tabular Data via Retrieval-Augmented   Large Language Models</h2><p><strong>Authors:Xumeng Wen, Shun Zheng, Zhen Xu, Yiming Sun, Jiang Bian</strong></p>
<p>Recent studies have shown that large language models (LLMs), when customized with post-training on tabular data, can acquire general tabular in-context learning (TabICL) capabilities. These models are able to transfer effectively across diverse data schemas and different task domains. However, existing LLM-based TabICL approaches are constrained to few-shot scenarios due to the sequence length limitations of LLMs, as tabular instances represented in plain text consume substantial tokens. To address this limitation and enable scalable TabICL for any data size, we propose retrieval-augmented LLMs tailored to tabular data. Our approach incorporates a customized retrieval module, combined with retrieval-guided instruction-tuning for LLMs. This enables LLMs to effectively leverage larger datasets, achieving significantly improved performance across 69 widely recognized datasets and demonstrating promising scaling behavior. Extensive comparisons with state-of-the-art tabular models reveal that, while LLM-based TabICL still lags behind well-tuned numeric models in overall performance, it uncovers powerful algorithms under limited contexts, enhances ensemble diversity, and excels on specific datasets. These unique properties underscore the potential of language as a universal and accessible interface for scalable tabular data learning. </p>
<blockquote>
<p>最近的研究表明，大型语言模型（LLM）在针对表格数据进行后训练定制后，可以获得通用的上下文内表格学习（TabICL）能力。这些模型能够跨不同的数据架构和任务领域进行有效迁移。然而，由于LLM的序列长度限制，现有的基于LLM的TabICL方法仅限于小样本场景，以纯文本形式表示的表格实例会消耗大量令牌。为了解决这一限制，实现任何数据规模的可扩展TabICL，我们提出了针对表格数据定制的检索增强LLM。我们的方法结合了一个定制的检索模块，并结合检索指导的LLM指令调整。这使得LLM能够有效利用更大的数据集，在69个广泛认可的数据集上实现了显著的性能提升，并表现出了有前景的可扩展性行为。与最先进表格模型的广泛比较表明，虽然基于LLM的TabICL在总体性能上仍落后于经过良好调整的数值模型，但它在有限上下文下揭示了强大的算法，增强了组合多样性，并在特定数据集上表现出色。这些独特的特点突显了语言作为通用和可访问界面用于可扩展表格数据学习的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03147v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过针对表格数据进行后训练，可以获取通用的表格上下文学习能力（TabICL）。然而，由于文本表示的表格实例消耗大量令牌，现有LLM-based TabICL方法受限于小样本文档。为应对这一限制并实现对任意数据规模的TabICL进行扩展，我们提出定制的检索增强型LLM以用于表格数据。我们的方法结合了定制检索模块和检索引导LLM指令调整，使得LLM能够有效利用大规模数据集。对比结果显示，尽管相较于调优的数字模型，LLM在总体性能上仍有不足，但在特定情境下揭示了强大的算法，增强了集成多样性并在特定数据集上表现出卓越性能。这表明语言作为通用和可访问的接口具有可扩展的表格数据学习潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM通过定制的后训练可以获得通用的表格上下文学习能力（TabICL）。</li>
<li>现有LLM-based TabICL方法受限于小样本文档场景，因为表格数据转换为文本消耗大量令牌。</li>
<li>为解决此限制，提出了检索增强型LLM方法，结合定制检索模块和检索引导指令调整。</li>
<li>该方法使LLM能够利用大规模数据集进行有效学习。</li>
<li>对比实验表明，LLM在总体性能上相较于调优的数字模型仍有不足，但在特定情境下展现出强大的算法性能。</li>
<li>LLM增强了集成多样性并在特定数据集上表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03147">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-794fdada3b907cf49bc40bb60210cafc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f81fb15aeafda82dad10e1b9acd52ea0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e90f30928366d0a06b083e5cdfb68385.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0eed5782f6af179339c23167d7abc94.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OmniRL-In-Context-Reinforcement-Learning-by-Large-Scale-Meta-Training-in-Randomized-Worlds"><a href="#OmniRL-In-Context-Reinforcement-Learning-by-Large-Scale-Meta-Training-in-Randomized-Worlds" class="headerlink" title="OmniRL: In-Context Reinforcement Learning by Large-Scale Meta-Training   in Randomized Worlds"></a>OmniRL: In-Context Reinforcement Learning by Large-Scale Meta-Training   in Randomized Worlds</h2><p><strong>Authors:Fan Wang, Pengtao Shao, Yiming Zhang, Bo Yu, Shaoshan Liu, Ning Ding, Yang Cao, Yu Kang, Haifeng Wang</strong></p>
<p>We introduce OmniRL, a highly generalizable in-context reinforcement learning (ICRL) model that is meta-trained on hundreds of thousands of diverse tasks. These tasks are procedurally generated by randomizing state transitions and rewards within Markov Decision Processes. To facilitate this extensive meta-training, we propose two key innovations: 1. An efficient data synthesis pipeline for ICRL, which leverages the interaction histories of diverse behavior policies; and 2. A novel modeling framework that integrates both imitation learning and reinforcement learning (RL) within the context, by incorporating prior knowledge. For the first time, we demonstrate that in-context learning (ICL) alone, without any gradient-based fine-tuning, can successfully tackle unseen Gymnasium tasks through imitation learning, online RL, or offline RL. Additionally, we show that achieving generalized ICRL capabilities-unlike task identification-oriented few-shot learning-critically depends on long trajectories generated by variant tasks and diverse behavior policies. By emphasizing the potential of ICL and departing from pre-training focused on acquiring specific skills, we further underscore the significance of meta-training aimed at cultivating the ability of ICL itself. </p>
<blockquote>
<p>我们介绍了OmniRL，这是一个高度通用的上下文强化学习（ICRL）模型，它在数十万个不同的任务上进行元训练。这些任务是通过在马尔可夫决策过程中随机化状态转换和奖励来程序生成的。为了促进这种广泛的元训练，我们提出了两项关键创新：1.为ICRL设计的有效数据合成管道，它利用了各种行为策略的交互历史；2.一种新型建模框架，通过融入先验知识，将模仿学习和强化学习（RL）结合在上下文之中。我们首次证明，仅通过上下文学习（ICL），无需任何基于梯度的微调，就可以通过模仿学习、在线强化学习或离线强化学习成功解决未见过的Gym任务。此外，我们表明，与面向任务识别的少样本学习不同，实现通用的ICRL功能严重依赖于由不同任务和多样行为策略产生的长轨迹。我们强调ICL的潜力，并侧重于摆脱以获取特定技能为重点的预训练，进一步突显以培养ICL能力为目标的元训练的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02869v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>     奥咪RL模型是一种高度通用的上下文强化学习（ICRL）模型，它通过百万级别的多样化任务进行元训练。该模型利用马尔可夫决策过程中的状态转换和奖励的随机性来程序化生成这些任务。为了实现广泛的元训练，该论文提出了两个关键的创新点：一是为ICRL设计的有效数据合成管道，它利用各种行为策略的交互历史；二是整合模仿学习和强化学习（RL）的新型建模框架。该论文首次证明，仅通过上下文学习（ICL），无需基于梯度的微调，就能成功解决未见的Gymnasium任务，包括模仿学习、在线RL或离线RL。实现广义ICRL能力关键在于长轨迹的生成依赖于多样化任务和多种行为策略，而不是面向任务识别的少样本学习。该论文强调了ICL的潜力，并强调偏离以获取特定技能为重点的预训练，进一步突出了以培养ICL能力为目标的元训练的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniRL是一个高度通用的上下文强化学习（ICRL）模型，通过大量多样化任务进行元训练。</li>
<li>OmniRL利用马尔可夫决策过程中的状态转换和奖励的随机性程序化生成任务。</li>
<li>OmniRL提出了两个关键创新点：一个高效的数据合成管道用于ICRL，以及一个整合模仿学习和强化学习的建模框架。</li>
<li>仅通过上下文学习（ICL）就能解决未见的Gymnasium任务，包括模仿学习、在线RL或离线RL。</li>
<li>实现广义ICRL能力需要依赖于多样化任务和多种行为策略生成的长轨迹，而不是面向任务识别的少样本学习。</li>
<li>该论文强调以获取ICL能力为目标的元训练的重要性，而非以获取特定技能为重点的预训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02869">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a5f0f3e191c20353e3c9c955c5d1542a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-634d8e168980bbd41b757618e3a0c14b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-411f433bea10c1704ecb7cc989c58306.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Transformers-Boost-the-Performance-of-Decision-Trees-on-Tabular-Data-across-Sample-Sizes"><a href="#Transformers-Boost-the-Performance-of-Decision-Trees-on-Tabular-Data-across-Sample-Sizes" class="headerlink" title="Transformers Boost the Performance of Decision Trees on Tabular Data   across Sample Sizes"></a>Transformers Boost the Performance of Decision Trees on Tabular Data   across Sample Sizes</h2><p><strong>Authors:Mayuka Jayawardhana, Renbo Tu, Samuel Dooley, Valeriia Cherepanova, Andrew Gordon Wilson, Frank Hutter, Colin White, Tom Goldstein, Micah Goldblum</strong></p>
<p>Large language models (LLMs) perform remarkably well on tabular datasets in zero- and few-shot settings, since they can extract meaning from natural language column headers that describe features and labels. Similarly, TabPFN, a recent non-LLM transformer pretrained on numerous tables for in-context learning, has demonstrated excellent performance for dataset sizes up to a thousand samples. In contrast, gradient-boosted decision trees (GBDTs) are typically trained from scratch on each dataset without benefiting from pretraining data and must learn the relationships between columns from their entries alone since they lack natural language understanding. LLMs and TabPFN excel on small tabular datasets where a strong prior is essential, yet they are not competitive with GBDTs on medium or large datasets, since their context lengths are limited. In this paper, we propose a simple and lightweight approach for fusing large language models and TabPFN with gradient-boosted decision trees, which allows scalable GBDTs to benefit from the natural language capabilities and pretraining of transformers. We name our fusion methods LLM-Boost and PFN-Boost, respectively. While matching or surpassing the performance of the transformer at sufficiently small dataset sizes and GBDTs at sufficiently large sizes, LLM-Boost and PFN-Boost outperform both standalone components on a wide range of dataset sizes in between. We demonstrate state-of-the-art performance against numerous baselines and ensembling algorithms. We find that PFN-Boost achieves the best average performance among all methods we test for all but very small dataset sizes. We release our code at <a target="_blank" rel="noopener" href="http://github.com/MayukaJ/LLM-Boost">http://github.com/MayukaJ/LLM-Boost</a> . </p>
<blockquote>
<p>大型语言模型（LLM）在零样本和少样本设置下的表格数据集上表现非常出色，因为它们可以从描述特征和标签的自然语言列标题中提取意义。类似地，TabPFN作为一个最近的非LLM转换器，在大量表格上进行预训练以进行上下文学习，已显示出在多达一千个样本的数据集上表现出色。相比之下，梯度提升决策树（GBDT）通常会在每个数据集上进行从头训练，而无法受益于预训练数据，并且它们仅能从条目本身学习列之间的关系，因为它们缺乏自然语言理解能力。LLM和TabPFN在小型表格数据集中表现出色，其中强大的先验知识至关重要，但在中等或大型数据集上，它们无法与GBDT竞争，因为它们的上下文长度有限。在本文中，我们提出了一种简单而轻量级的方法，用于将大型语言模型和TabPFN与梯度提升决策树融合，这使得可扩展的GBDT可以受益于转换器的自然语言能力和预训练。我们将我们的融合方法分别命名为LLM-Boost和PFN-Boost。尽管在小数据集上LLM-Boost和PFN-Boost的性能与转换器相匹配或超过它，并且在足够大的数据集上GBDT的性能相匹配或超过它，但它们在各种中间数据集大小上的表现均优于这两个独立组件。我们与众多基准线和集成算法相比，展现了最佳性能。我们发现，除了非常小的数据集外，PFN-Boost在所有测试方法中取得了最佳的平均性能。我们在<a target="_blank" rel="noopener" href="http://github.com/MayukaJ/LLM-Boost">http://github.com/MayukaJ/LLM-Boost</a>上发布了我们的代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02672v1">PDF</a> 12 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在零样本和少样本环境下处理表格数据集时表现出色，因为它们可以从描述特征和标签的自然语言列标题中提取意义。相比之下，梯度提升决策树（GBDTs）通常针对每个数据集从头开始训练，无法从预训练数据中受益，并且必须仅从条目本身中学习列之间的关系。LLMs和TabPFN在小型表格数据集上表现出色，其中强先验知识至关重要，但在中等或大型数据集上无法与GBDTs竞争。本文提出了一种简单而轻量级的方法，将大型语言模型和TabPFN与梯度提升决策树融合，使可扩展的GBDTs受益于变换器的自然语言能力和预训练。我们分别将融合方法命名为LLM-Boost和PFN-Boost。当数据集大小足够小和足够大时，LLM-Boost和PFN-Boost分别匹配或超越变换器和GBDTs的性能，但在两者之间的一系列数据集大小上表现出优于两者的性能。我们在众多基准测试和集成算法上展示了卓越的性能。我们发现，除非常小的数据集大小外，PFN-Boost在所有方法中实现了最佳的平均性能。我们的代码已发布在<a target="_blank" rel="noopener" href="http://github.com/MayukaJ/LLM-Boost%E3%80%82">http://github.com/MayukaJ/LLM-Boost。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在零样本和少样本环境下处理表格数据集时具有优势，得益于它们对自然语言的深刻理解。</li>
<li>TabPFN预训练的表格理解能力在非大型数据集上表现优秀。</li>
<li>梯度提升决策树（GBDTs）通常不依赖预训练数据，但在处理中等或大型数据集时表现良好。</li>
<li>LLM-Boost和PFN-Boost方法融合了LLMs和TabPFN的优点，提升了GBDTs的性能。</li>
<li>LLM-Boost和PFN-Boost在不同数据集大小上均表现出卓越性能，超越了单纯的LLMs和GBDTs。</li>
<li>PFN-Boost在大多数数据集大小上实现了最佳的平均性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02672">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-155ab9ea96b16aa1bf38a99036944a13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-557f160475fc9ac7bb65d414f94dda85.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation"><a href="#BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation" class="headerlink" title="BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation"></a>BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation</h2><p><strong>Authors:Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Boris Hanin, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia</strong></p>
<p>As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT. </p>
<blockquote>
<p>随着模型训练中对高质量数据的需求不断增长，研究者和开发者们正在越来越多地生成合成数据来调整和优化大型语言模型（LLMs）。关于合成数据的常见假设是，从指令优化模型中采样就足够了；然而，这些模型在产生多样化输出方面存在困难，这是泛化的一个关键要求。尽管有各种提示方法，但在这项工作中，我们表明从指令优化模型中实现有意义的多样性仍然具有挑战性。相比之下，我们发现没有经过后训练的基准模型表现出更大的多样性，但在遵循指令方面的能力较差，因此质量较低。我们利用这一见解，提出了Base-Refine（BARE）方法，这是一种合成数据生成方法，它通过两阶段过程结合基准模型的多样性和指令优化模型的质量。通过少量的示例和筛选，BARE可以生成多样且高质量的数据集，提高下游任务性能。我们展示使用仅1000个BARE生成样本进行微调时，在LiveCodeBench任务上的性能可与类似大小的最佳模型相当。此外，使用BARE生成的数据进行微调在GSM8K上实现了比仅使用指令数据提高101%的性能，并在RAFT上实现了比最先进方法提高18.4%的效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01697v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>生成高质量数据对于模型训练至关重要，研究者利用合成数据来调整和优化大型语言模型（LLMs）。尽管存在各种提示方法，但发现从指令调整模型中实现有意义的多样性仍然具有挑战性。相反，研究发现未经训练的基准模型展现出更大的多样性，但在指令遵循方面能力较低。基于此，提出了结合基准模型的多样性和指令调整模型的质量的合成数据生成方法——Base-Refine（BARE）。通过最少的几次示例和筛选，BARE生成了多样且高质量的数据集，提高了下游任务性能。在LiveCodeBench任务上，使用BARE生成的样本进行微调达到了出色的性能表现。相较于指令仅涉及的数据，使用BARE生成的微调数据在GSM8K上实现了101%的提升，并在RAFT任务上比最先进的处理方法提升了约达至有特定的表达目的的智能和思维方式已经明确的文化准则相近的个人所处的当下所面临的政治环境等要素都使得翻译变得更为复杂和困难。翻译不仅需要语言层面的转换，还需要对文化语境、社会背景、历史背景等进行深入理解。因此，翻译是一项需要高度专业知识和技能的工作。随着全球化的不断发展，翻译行业的前景广阔，对于具备专业技能和经验的翻译人才的需求也在不断增加。同时，随着人工智能技术的不断进步，机器翻译也在不断发展，但机器翻译仍然无法完全取代人工翻译的地位。未来翻译行业将朝着更加专业化和精细化的方向发展，对于翻译人才的要求也将不断提高。在此基础上推动行业的发展，不断挑战自我技能与适应环境变化成为未来译者职业发展的重要方向之一。此次对话表明了一个共同的认识：机器虽然强大但仍无法取代人的智能与创造力这是机器翻译未来发展中需要面对的重要挑战之一。，还取得了显著的提升效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成合成数据对于训练LLMs至关重要。</li>
<li>指令调整模型在生成多样输出方面存在挑战。</li>
<li>未经训练的基准模型展现出更大的多样性但在指令遵循方面能力较低。</li>
<li>BARE方法结合了基准模型的多样性和指令调整模型的质量。</li>
<li>使用BARE生成的少量样本进行微调可达到出色的性能表现。</li>
<li>BARE方法在多个任务上实现了显著的性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01697">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5d21c6b0ee4ae4a31251f77c5ca955e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2aedf490ff7ee0cbb3ff7b1063b933f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ecc96ecda174238e70bc680753559e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81d93849a1a7b4bb94a319e97c5973c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a2a1fce24198ceb9f7f7f2a168d9850.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Aggregation-Artifacts-in-Subjective-Tasks-Collapse-Large-Language-Models’-Posteriors"><a href="#Aggregation-Artifacts-in-Subjective-Tasks-Collapse-Large-Language-Models’-Posteriors" class="headerlink" title="Aggregation Artifacts in Subjective Tasks Collapse Large Language   Models’ Posteriors"></a>Aggregation Artifacts in Subjective Tasks Collapse Large Language   Models’ Posteriors</h2><p><strong>Authors:Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan</strong></p>
<p>In-context Learning (ICL) has become the primary method for performing natural language tasks with Large Language Models (LLMs). The knowledge acquired during pre-training is crucial for this few-shot capability, providing the model with task priors. However, recent studies have shown that ICL predominantly relies on retrieving task priors rather than “learning” to perform tasks. This limitation is particularly evident in complex subjective domains such as emotion and morality, where priors significantly influence posterior predictions. In this work, we examine whether this is the result of the aggregation used in corresponding datasets, where trying to combine low-agreement, disparate annotations might lead to annotation artifacts that create detrimental noise in the prompt. Moreover, we evaluate the posterior bias towards certain annotators by grounding our study in appropriate, quantitative measures of LLM priors. Our results indicate that aggregation is a confounding factor in the modeling of subjective tasks, and advocate focusing on modeling individuals instead. However, aggregation does not explain the entire gap between ICL and the state of the art, meaning other factors in such tasks also account for the observed phenomena. Finally, by rigorously studying annotator-level labels, we find that it is possible for minority annotators to both better align with LLMs and have their perspectives further amplified. </p>
<blockquote>
<p>上下文学习（ICL）已成为使用大型语言模型（LLM）执行自然语言任务的主要方法。在预训练期间获得的知识对于这种小样本能力至关重要，为模型提供任务先验。然而，最近的研究表明，ICL主要依赖于检索任务先验，而不是“学习”执行任务。在情感、道德等复杂的主观领域，这种局限性尤为明显，先验知识对后验预测有重大影响。在这项工作中，我们调查这是否是由于相应数据集中使用的聚合方法造成的。尝试将低同意度、分散的注释结合起来可能会导致注释伪像，在提示中产生有害的噪音。此外，我们通过基于定量的大型语言模型先验知识衡量方法，评估了对某些注释器的后验偏见。我们的结果表明，聚合是主观任务建模中的混淆因素，并主张重点对个体进行建模。然而，聚合并不能完全解释ICL与最新技术之间的差距，这意味着此类任务中的其他因素也导致了观察到的现象。最后，通过对注释器级别的标签进行严格研究，我们发现少数注释器与大型语言模型更加吻合，并且他们的观点得到了进一步的放大。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13776v3">PDF</a> 16 pages, 12 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）进行自然语言任务时主要依赖上下文学习（ICL），其关键在于预训练阶段获取的知识，为模型提供了任务先验信息。但研究表明，ICL主要依赖检索任务先验信息而非“学习”执行任务。在情感、道德等复杂主观领域，这种局限性尤为明显。本文探讨了这一现象是否由数据集使用的聚合方法导致，尝试将低一致性、分散的注释结合起来可能产生注释伪影，给提示带来有害噪声。同时评估了模型对特定注释者的后验偏见。研究结果表明，聚合是模拟主观任务中的混淆因素，并主张将重点放在模拟个人上。然而，聚合并不能完全解释ICL与当前技术水平之间的差距，意味着还有其他因素也影响了观察到的现象。通过深入研究注释器级别的标签，发现少数注释器与LLM更匹配，他们的观点也得到了进一步的放大。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICL成为LLM执行自然语言任务的主要方法，依赖于预训练知识提供的任务先验信息。</li>
<li>ICL在复杂主观任务中主要依赖检索任务先验而非学习过程。</li>
<li>数据集的聚合方法是影响模型表现的因素之一，尝试结合低一致性、分散的注释可能产生注释伪影。</li>
<li>聚合方法对模拟主观任务有混淆作用，应关注模拟个人层面的方法。</li>
<li>后验偏向特定注释者可能影响模型表现，需要对此进行深入研究。</li>
<li>聚合并不能完全解释ICL与当前技术水平的差距，其他因素也影响任务表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13776">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-50cea568155414300ef195f257bce149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea476b55613b4b8cfe6ac91c0d847156.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa137b1732bdc12bced30de02e261e79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77c7eebb2a86fccc78d221fd28cc4a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c63541f6606e0f528601e2a68b0ec509.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching"><a href="#Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching" class="headerlink" title="Agent-OM: Leveraging LLM Agents for Ontology Matching"></a>Agent-OM: Leveraging LLM Agents for Ontology Matching</h2><p><strong>Authors:Zhangcheng Qiang, Weiqing Wang, Kerry Taylor</strong></p>
<p>Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks. </p>
<blockquote>
<p>本体匹配（OM）通过使不同本体之间实现语义互操作性，并通过对齐相关实体解决其概念上的异质性。目前，OM系统主要有两种流行的设计范式：传统的基于知识的专家系统和较新的基于机器学习的预测系统。虽然大型语言模型（LLM）和LLM代理已经彻底改变了数据工程，并在许多领域得到了创造性的应用，但它们在OM中的潜力仍未得到充分探索。本研究引入了一种新型基于LLM的代理驱动OM系统设计范式。考虑到利用LLM代理进行OM面临的若干特定挑战，我们提出了一个通用框架，即Agent-OM（用于本体匹配的代理），它由两个用于检索和匹配的Siamese代理和一组OM工具组成。我们的框架在一个概念验证系统中实现。对三个本体对齐评估倡议（OAEI）赛道上的最新OM系统的评估表明，我们的系统在简单OM任务上的结果非常接近长期以来的最佳性能，并且在复杂和少量射击OM任务上可以显著提高性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00326v8">PDF</a> 19 pages, 12 figures, 3 tables</p>
<p><strong>Summary</strong><br>     本研究提出一种基于新型智能体辅助的大型语言模型（LLM）设计范式，用于本体匹配（OM）系统。通过考虑利用LLM智能体进行OM所面临的特定挑战，研究团队提出了一个通用框架Agent-OM（用于本体匹配的智能体），包含两个用于检索和匹配的Siamese智能体以及一套OM工具。在概念验证系统中实现了该框架，评估结果显示，该系统在简单OM任务上的表现接近最佳，且在复杂和少量样本OM任务上能显著提高性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究提出了一种新的基于智能体的LLM设计范式用于OM系统。</li>
<li>LLM和智能体在本体匹配中的潜力尚未得到充分探索。</li>
<li>Agent-OM框架包含两个Siamese智能体，用于检索和匹配，以及一套OM工具。</li>
<li>该框架解决了利用LLM智能体进行OM的特定挑战。</li>
<li>系统在简单OM任务上的表现接近最佳。</li>
<li>系统在复杂和少量样本OM任务上能显著提高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00326">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c794a59ff02345828f7dbc90674bd4ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6afc0a809c7cca7c74b8fb85ea0018f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96ae714ecca0dbe9a8c8ed1e5d36d634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd619db47abc4ecf8d65df2589e3fbcd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-07/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-07/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-07/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ce03319fdca1ee60bc62215e26679ccf.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-02-07  Secure & Personalized Music-to-Video Generation via CHARCHA
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-07/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c5b4b80c3220979c320af49a82fc2356.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-02-07  Secure & Personalized Music-to-Video Generation via CHARCHA
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
